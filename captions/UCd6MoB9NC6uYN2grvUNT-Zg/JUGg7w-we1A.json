[
  {
    "text": "I I think we'll go ahead and get started here my name is Spence nyla I'm a Solutions Architect with AWS",
    "start": "0",
    "end": "6140"
  },
  {
    "text": "in this session we're going to be covering data lakes how to build data",
    "start": "6140",
    "end": "11190"
  },
  {
    "text": "lakes on AWS what a data like means and really some patterns and best practices that we've seen customers take when",
    "start": "11190",
    "end": "17580"
  },
  {
    "text": "building data lakes so how many folks in the room are familiar with data lakes",
    "start": "17580",
    "end": "22800"
  },
  {
    "text": "already okay a good number excellent how many of those folks are currently",
    "start": "22800",
    "end": "28410"
  },
  {
    "text": "implementing add a lake for themselves okay maybe half the room or so so really data",
    "start": "28410",
    "end": "36030"
  },
  {
    "text": "lakes really really popular for customers to start building on AWS and",
    "start": "36030",
    "end": "41160"
  },
  {
    "text": "if we take a step back and we look at big data there's different ways of defining big data but it really boils",
    "start": "41160",
    "end": "47370"
  },
  {
    "text": "down to a couple core tenets or you know B's that the industry calls the most",
    "start": "47370",
    "end": "53190"
  },
  {
    "text": "common way of thinking of big data is volume but it doesn't have to be volume based so you don't have to have",
    "start": "53190",
    "end": "58559"
  },
  {
    "text": "petabytes exabytes you know that sort of size of data to have big data sometimes",
    "start": "58559",
    "end": "64140"
  },
  {
    "text": "you might just have a lot of messy data what if you have a lot of unstructured data semi structured data you might have",
    "start": "64140",
    "end": "70200"
  },
  {
    "text": "a lot of different data sets that need to be combined together and all of those sorts of use cases actually fit into the",
    "start": "70200",
    "end": "77369"
  },
  {
    "text": "big data space really really well so you know even though it says big data you might have relatively small data but",
    "start": "77369",
    "end": "83670"
  },
  {
    "text": "they fit these other characteristics and these sorts of techniques still really apply to that that sort of use case and",
    "start": "83670",
    "end": "91159"
  },
  {
    "text": "you know if you really look at it there's a couple of industry reports screenshots that we see here it really",
    "start": "91159",
    "end": "97770"
  },
  {
    "text": "comes down to this rapid data growth if you really look at today's data over 90%",
    "start": "97770",
    "end": "103049"
  },
  {
    "text": "of today's data have really been gener in the last two years so if you actually look at the rate of data being generated",
    "start": "103049",
    "end": "108570"
  },
  {
    "text": "it's growing growing very very quickly and customers like yourself Swan and",
    "start": "108570",
    "end": "114000"
  },
  {
    "text": "really be able to take that data and drive insights from that data but not only that the type of data that is being",
    "start": "114000",
    "end": "120840"
  },
  {
    "text": "generated is different it's a lot of IOT data a lot of sensor data is getting",
    "start": "120840",
    "end": "125850"
  },
  {
    "text": "generated a lot of unstructured and semi-structured data sets from social media and other data sources so the",
    "start": "125850",
    "end": "132840"
  },
  {
    "text": "variety of day sets that are getting generated or constantly growing so it's not just that that rapid growth that high volume of",
    "start": "132840",
    "end": "140069"
  },
  {
    "text": "data but also the types of data again generated so what data likes I want to",
    "start": "140069",
    "end": "148049"
  },
  {
    "text": "spend a ton of time introducing what a data Lake is since a lot of folks in the room are familiar with data lakes but",
    "start": "148049",
    "end": "154859"
  },
  {
    "text": "with the data like it's all about being able to quickly ingest your data so if you have a data set and you want to",
    "start": "154859",
    "end": "160769"
  },
  {
    "text": "really get it into the lake that the amount of work that is needed to be able to do that should be very very seamless",
    "start": "160769",
    "end": "166379"
  },
  {
    "text": "it should be very easy get the data into the data Lakes not only that but also",
    "start": "166379",
    "end": "171480"
  },
  {
    "text": "different types of data so you know if you have all this unstructured data semi structured data even if it's structured",
    "start": "171480",
    "end": "177329"
  },
  {
    "text": "being able to rapidly ingest that data into the day lake it's really critical so that you could get the data in there",
    "start": "177329",
    "end": "183450"
  },
  {
    "text": "and start running tools and running analytics on that data and the key part about running analytics is being able to",
    "start": "183450",
    "end": "190169"
  },
  {
    "text": "really future-proof what we mean by that is as you get your data into your data Lake you want to be able to have a suite",
    "start": "190169",
    "end": "196560"
  },
  {
    "text": "of tools to be able to run on that data if it's actually stove-piped siloed into one sort of tool then it's",
    "start": "196560",
    "end": "203430"
  },
  {
    "text": "really hard to start experimenting using other tools on that data so the architecture really needs to be able to",
    "start": "203430",
    "end": "209010"
  },
  {
    "text": "adapt and be able to run different sorts of analytics on it on that data to be able to drive those insights out of that",
    "start": "209010",
    "end": "215639"
  },
  {
    "text": "content and of course security you know with all this data it really needs to be",
    "start": "215639",
    "end": "221430"
  },
  {
    "text": "very very secure controlled audited fine grain access to all that other information that you or things that you",
    "start": "221430",
    "end": "228689"
  },
  {
    "text": "would expect because data lakes are really fabulous at driving insights but you also have to have them very very",
    "start": "228689",
    "end": "234840"
  },
  {
    "text": "protected at a protection layer because it's a lot of sensitive data most often",
    "start": "234840",
    "end": "240919"
  },
  {
    "text": "so if we take a look at you know a general architecture and we're gonna dive into how to dive into each of these",
    "start": "240919",
    "end": "247169"
  },
  {
    "text": "pieces what the best practices of those pieces are but if we take a look at the architecture we really see some key",
    "start": "247169",
    "end": "253199"
  },
  {
    "text": "components the first in the middle is you know most people think of just storage right storage by itself really",
    "start": "253199",
    "end": "261269"
  },
  {
    "text": "really great but isn't really the only component that makes up what data like another really",
    "start": "261269",
    "end": "266760"
  },
  {
    "text": "aspects to a data Lake is cataloging up your data being able understand what data you have where is it what does the",
    "start": "266760",
    "end": "274230"
  },
  {
    "text": "schema look like what is the format how can I make it so all these different tools could really process and and",
    "start": "274230",
    "end": "280640"
  },
  {
    "text": "ingest that data you really need to have a catalog that supports that in addition to where that data is being stored so",
    "start": "280640",
    "end": "288060"
  },
  {
    "text": "those two things really make up that that core component it's not just the storage and it's just not the catalog",
    "start": "288060",
    "end": "293330"
  },
  {
    "text": "but also you have to get the data into the data like so there one of the key",
    "start": "293330",
    "end": "298620"
  },
  {
    "text": "takeaways and the the things I like to talk to customers about is you don't necessarily have to use one tool for all",
    "start": "298620",
    "end": "304560"
  },
  {
    "text": "your ingestion the way you ingest data from a relational database into a data Lake may be doing CDC changed you know",
    "start": "304560",
    "end": "311550"
  },
  {
    "text": "change capturing is very different than the way you might capture clickstream data from your users which is very",
    "start": "311550",
    "end": "317910"
  },
  {
    "text": "different than the way you might capture satellite imagery for example you know digital globe has 100 petabytes of data",
    "start": "317910",
    "end": "323190"
  },
  {
    "text": "that they ingest it through like a snowmobile so all the ways of ingesting your data",
    "start": "323190",
    "end": "328580"
  },
  {
    "text": "oftentimes is very very specific to what type of data you have and how that data is getting generated but the key thing",
    "start": "328580",
    "end": "335400"
  },
  {
    "text": "is after it gets into your data like the way it got into your data Lake doesn't really matter as much so you might have",
    "start": "335400",
    "end": "341940"
  },
  {
    "text": "different data sets coming in and one data set might be use one method of getting into the lake while another data",
    "start": "341940",
    "end": "349400"
  },
  {
    "text": "data set might be coming in and through a different means and then of course the",
    "start": "349400",
    "end": "354660"
  },
  {
    "text": "analytics the driving insights data lakes are really great at powering analytics machine learning that sort of",
    "start": "354660",
    "end": "361650"
  },
  {
    "text": "capability on top of it and you'll notice this is John as decoupled right",
    "start": "361650",
    "end": "366720"
  },
  {
    "text": "so what that means is the way you get your data in doesn't influence and isn't",
    "start": "366720",
    "end": "371970"
  },
  {
    "text": "really dependent on the way your story which isn't really as dependent on the way you analyze it so having that decoupled architecture is another way",
    "start": "371970",
    "end": "379470"
  },
  {
    "text": "that you actually feature proof and you can also start small that way so",
    "start": "379470",
    "end": "385400"
  },
  {
    "text": "patterns and some lessons learns and some things to think about as you're looking at your data lakes the the first",
    "start": "385400",
    "end": "391380"
  },
  {
    "text": "thing is what data do I have you know storing your data in a data lake in s3 for example and not being able to know",
    "start": "391380",
    "end": "397410"
  },
  {
    "text": "where the data is what the format is it's almost like you don't have it there anymore right you really have to know where your data is to be",
    "start": "397410",
    "end": "404000"
  },
  {
    "text": "able to process analyze drive insights from it Gartner has two nice quotes that I like",
    "start": "404000",
    "end": "410210"
  },
  {
    "text": "to steal a lot of times the first is in 2018 eighty percent of the data lakes are going to be ineffective because of",
    "start": "410210",
    "end": "417230"
  },
  {
    "text": "poor metadata management what that you know what they're trying to elevate here is the importance of being able to have",
    "start": "417230",
    "end": "423350"
  },
  {
    "text": "that catalog and and good metadata and management on your data Lake the other quote that I kind of like is really",
    "start": "423350",
    "end": "429050"
  },
  {
    "text": "taking that data Lake analogy another step is it's the fish finder it's letting you find your data that you care",
    "start": "429050",
    "end": "435410"
  },
  {
    "text": "about for that particular type of analytic so there's a couple ways of",
    "start": "435410",
    "end": "440600"
  },
  {
    "text": "doing your your metadata management on AWS one of the first ones we're going to",
    "start": "440600",
    "end": "445610"
  },
  {
    "text": "talk about is using the glue data catalog how many folks are familiar with glue already okay maybe a third of the room",
    "start": "445610",
    "end": "453370"
  },
  {
    "text": "so glue has two key components to it one of them is the ETL portion that allows",
    "start": "453370",
    "end": "459440"
  },
  {
    "text": "you to run server lists ETL through either PI spark Scala spark to be able",
    "start": "459440",
    "end": "465050"
  },
  {
    "text": "to run transform loads the other is the data catalog and these things could be",
    "start": "465050",
    "end": "471020"
  },
  {
    "text": "used together or they might be used separately for example Dow Jones presented at the New York summit a few",
    "start": "471020",
    "end": "477320"
  },
  {
    "text": "weeks ago how they're heavily using the glue data catalog but they're using things like EMR to do there ETL versus",
    "start": "477320",
    "end": "483860"
  },
  {
    "text": "using glue so you don't have to use the entire suite even though they're very very nicely integrated together so this",
    "start": "483860",
    "end": "491510"
  },
  {
    "text": "data catalog is hive compliant what that why you care about that why I want to mention that is in order to have a lot",
    "start": "491510",
    "end": "497960"
  },
  {
    "text": "of different tools to be able to process and understand the data it first has to look up what data do I have and where",
    "start": "497960",
    "end": "503960"
  },
  {
    "text": "does it live and having it hive compliant that's one of the de facto standards ways of defining metadata",
    "start": "503960",
    "end": "510950"
  },
  {
    "text": "stores on-premises that people are running on Hadoop platforms having that",
    "start": "510950",
    "end": "516530"
  },
  {
    "text": "allows it so it's very easy to integrate with Hadoop style analytics but it doesn't have to be you could run within",
    "start": "516530",
    "end": "523669"
  },
  {
    "text": "Sage maker and query your data cataloging and do tensorflow MX net you know cafe the different types of machine",
    "start": "523669",
    "end": "530480"
  },
  {
    "text": "learning algorithms or deep learning algorithms on your data as well the other really important aspect is you know what really",
    "start": "530480",
    "end": "538100"
  },
  {
    "text": "makes a metadata store or data catalog very very effective is not necessarily populating it once and forgetting about",
    "start": "538100",
    "end": "544310"
  },
  {
    "text": "it because your datasets change your formats might change being able to really have that data and and have a way",
    "start": "544310",
    "end": "551450"
  },
  {
    "text": "of governing or maintaining that data set over time and that's really where",
    "start": "551450",
    "end": "556940"
  },
  {
    "text": "these crawlers come in crawlers aren't the only way of doing this we'll talk about a few other ways but crawlers are",
    "start": "556940",
    "end": "563780"
  },
  {
    "text": "really great if you don't know which data sets are in your data like so let's say you may know you may know a little",
    "start": "563780",
    "end": "571010"
  },
  {
    "text": "bit about the data set but you don't know the field names you don't know the variable names those sorts of things",
    "start": "571010",
    "end": "576320"
  },
  {
    "text": "what these crawlers allow you to do is it allows you to really plug it in into",
    "start": "576320",
    "end": "582110"
  },
  {
    "text": "your data Lake or your relational databases your your data warehouses usually you know the schemas of those",
    "start": "582110",
    "end": "587840"
  },
  {
    "text": "because they're schema on on right but essentially what you can do is point it",
    "start": "587840",
    "end": "593600"
  },
  {
    "text": "to those data sets and these crawlers will look at the actual data and derive oh this is a JSON data this is XML this",
    "start": "593600",
    "end": "601100"
  },
  {
    "text": "is a CSV this is park' and then as it's deriving that information it will be looking at the fields and give you all",
    "start": "601100",
    "end": "608240"
  },
  {
    "text": "the different fields you have along with what the data type is of those various fields so one example is you know a lot",
    "start": "608240",
    "end": "615410"
  },
  {
    "text": "of times I like to give demonstrations processing New York City Taxi data it's about eight years of data it's about",
    "start": "615410",
    "end": "621350"
  },
  {
    "text": "three billion entries really nice nice public data set to play around with and",
    "start": "621350",
    "end": "628520"
  },
  {
    "text": "what's interesting is you know when you're actually calling that data set and analyzing it and looking at it you",
    "start": "628520",
    "end": "635450"
  },
  {
    "text": "could actually use these crawlers to be able to derive all the different fields and even though it's CSV data and CSV",
    "start": "635450",
    "end": "642530"
  },
  {
    "text": "data is naturally all ASCII right there's no data types in in a CSV file it derives oh this is a longitude",
    "start": "642530",
    "end": "649250"
  },
  {
    "text": "it needs to be a double this is a date these are the different fields so even though the the data source itself",
    "start": "649250",
    "end": "656840"
  },
  {
    "text": "doesn't define those fields all the time these crawlers will actually discover those for you the crawlers do great but",
    "start": "656840",
    "end": "664340"
  },
  {
    "text": "you know they're not perfect all the time right there are automated process to be able to discover these schemas so if you",
    "start": "664340",
    "end": "671370"
  },
  {
    "text": "actually crawl it on on a data set and you have a lot of optional fields and you don't have all the fields in the",
    "start": "671370",
    "end": "676440"
  },
  {
    "text": "data set that crawl is you might actually have new data come in and it has new fields because it's very sparsely populated as one example or",
    "start": "676440",
    "end": "682770"
  },
  {
    "text": "maybe it found a dataset for example the flight data it saw a flight number it",
    "start": "682770",
    "end": "687840"
  },
  {
    "text": "treats it as a string but maybe you want to treat it or it found it as an integer but maybe you want to treat it as a",
    "start": "687840",
    "end": "693120"
  },
  {
    "text": "stream because there's no numeric you don't want to average flight numbers for certain carrier right it's more of a string field so what you could do after",
    "start": "693120",
    "end": "700260"
  },
  {
    "text": "it populates your catalog is you could go in there and modify the different data types or add fields that sort of",
    "start": "700260",
    "end": "707190"
  },
  {
    "text": "thing so these crawlers are really really great at being able to populate your data set but you could run them",
    "start": "707190",
    "end": "712860"
  },
  {
    "text": "over time as well so as your data is changing it could discover that as new partitions come into your your data lake",
    "start": "712860",
    "end": "720480"
  },
  {
    "text": "it will discover those partitions to be able to maintain all that information there it you know it's it's interesting",
    "start": "720480",
    "end": "727830"
  },
  {
    "text": "I was working with one customer that had a very very unique log format it was",
    "start": "727830",
    "end": "733350"
  },
  {
    "text": "some-some telco equipment it was it was emitting a very very interesting log",
    "start": "733350",
    "end": "738480"
  },
  {
    "text": "format that wasn't really CSV it wasn't tab it had like all these parentheses with different units that sort of thing",
    "start": "738480",
    "end": "744570"
  },
  {
    "text": "and they wanted to be able to use glue they want to use these things and that's where these custom classifiers come in",
    "start": "744570",
    "end": "751350"
  },
  {
    "text": "so these custom classifiers really allow you to define your own rules our processing your data and those come in",
    "start": "751350",
    "end": "759450"
  },
  {
    "text": "different forms so if I have data sets and one record is always separated by a new line but the way that's encoded is",
    "start": "759450",
    "end": "767040"
  },
  {
    "text": "very very unique what you could do is you could use a crock format what crock will do it's an extension on top of reg",
    "start": "767040",
    "end": "773070"
  },
  {
    "text": "X that has these patterns built into it and what you could do is you could say oh here's an IP address there might be a",
    "start": "773070",
    "end": "778710"
  },
  {
    "text": "dash here here's the day here's a comma and then here's some more fields where the delimiters might be very very unique",
    "start": "778710",
    "end": "785370"
  },
  {
    "text": "as you're processing the line but you could define that expression in that grok formula and then what the crawlers",
    "start": "785370",
    "end": "793680"
  },
  {
    "text": "will do is they'll actually use that to discover all the data and then really really nice aspect of all this is that",
    "start": "793680",
    "end": "799529"
  },
  {
    "text": "it's abstracted away in your catalog so as you use tools to be able to analyze your data maybe it's using sequel based",
    "start": "799529",
    "end": "805470"
  },
  {
    "text": "queries on top of your data using Athena or maybe you're bringing it in on EMR using various tools on top of it that",
    "start": "805470",
    "end": "813230"
  },
  {
    "text": "custom format is actually abstracted away from the people writing the analytics so each person doesn't have to",
    "start": "813230",
    "end": "819449"
  },
  {
    "text": "figure out what that reg X or that crock format is that grok expression it's actually in the catalog with the",
    "start": "819449",
    "end": "825029"
  },
  {
    "text": "rightsir D and then they could use those tools on top of it so that they just see the logical view not necessarily the",
    "start": "825029",
    "end": "830579"
  },
  {
    "text": "concrete mess of whatever log format that was so really really great to be able to do that and bring bring your",
    "start": "830579",
    "end": "837120"
  },
  {
    "text": "datasets in we have xml and json class surprises as well to be able to plot the",
    "start": "837120",
    "end": "842189"
  },
  {
    "text": "different fields of those datasets and be able to process that so some other",
    "start": "842189",
    "end": "850319"
  },
  {
    "text": "ways of popping your catalogue and then we're gonna start going through the ecosystem so right now we're talking about cataloging but we're going to talk",
    "start": "850319",
    "end": "856379"
  },
  {
    "text": "about ingestion analytics machine learning so we're really going to step on all these aspects we're not going to",
    "start": "856379",
    "end": "861629"
  },
  {
    "text": "just talk about necessarily just one part of the data like but there's other",
    "start": "861629",
    "end": "867240"
  },
  {
    "text": "ways of populating your catalog so let's say you're actually running a hive metadata store maybe on RDS or maybe",
    "start": "867240",
    "end": "872699"
  },
  {
    "text": "on-premises that sort of thing what you could do is you can actually use import/export scripts so you can take",
    "start": "872699",
    "end": "878069"
  },
  {
    "text": "your hive metadata store you can run these scripts and populate the glue data catalog or if you want to go the other",
    "start": "878069",
    "end": "883230"
  },
  {
    "text": "way you could take the glue Datak alak and populate your hive metadata stores so if you're doing a migration may be",
    "start": "883230",
    "end": "888329"
  },
  {
    "text": "from on-premises you can actually both migrate your HDFS data through HDFS endpoints through s33 snoball as well as",
    "start": "888329",
    "end": "896670"
  },
  {
    "text": "you can migrate that hive metadata stores that sort of thing using a lot of these techniques there's some really",
    "start": "896670",
    "end": "902009"
  },
  {
    "text": "good talks about just migrating Hadoop workloads you know hour-long talks just",
    "start": "902009",
    "end": "907110"
  },
  {
    "text": "focused on that some really good patterns if if that's what you're looking at doing so some metadata",
    "start": "907110",
    "end": "914309"
  },
  {
    "text": "management what about how to get the data into into your leg and you know this is really where it's a",
    "start": "914309",
    "end": "922800"
  },
  {
    "text": "pretty common theme but this is really where it's important that you use the right tool for the right job so if I",
    "start": "922800",
    "end": "928319"
  },
  {
    "text": "have you know for example if I'm using Kinesis or Kafka and I'm trying to do streaming data and pop it in my",
    "start": "928319",
    "end": "934919"
  },
  {
    "text": "data like if I try to start sending like all these big images over that or all these other large data sets it becomes",
    "start": "934919",
    "end": "941879"
  },
  {
    "text": "very very messy to manage it's it's a lot of times is a lot better to use different ingestion techniques for the",
    "start": "941879",
    "end": "947489"
  },
  {
    "text": "different data sets and how those data's are getting that data sets are getting generated so if we take a look at some",
    "start": "947489",
    "end": "955289"
  },
  {
    "text": "of these so on the real-time flow definitely IOT IOT core has some really",
    "start": "955289",
    "end": "962009"
  },
  {
    "text": "good adapters to be able to take IOT data coming from MQTT or coming through",
    "start": "962009",
    "end": "967470"
  },
  {
    "text": "WebSocket different channels and getting that into lake or Kinesis we're really",
    "start": "967470",
    "end": "972839"
  },
  {
    "text": "going to talk about Kinesis and more depth here so Kinesis really really",
    "start": "972839",
    "end": "979379"
  },
  {
    "text": "great at being able to handle streaming data so this could be the IOT data coming from IOT core it could be sensors",
    "start": "979379",
    "end": "987989"
  },
  {
    "text": "out kind of out in the wild however you're getting streaming data Kinesis",
    "start": "987989",
    "end": "993209"
  },
  {
    "text": "data streams is really really great you can start small with one shard you can scale it up scale it back down as you",
    "start": "993209",
    "end": "998579"
  },
  {
    "text": "need to what's really great is you could create kind of this big data lambda",
    "start": "998579",
    "end": "1004129"
  },
  {
    "text": "architecture a little bit different than AWS lambda but this architecture to be",
    "start": "1004129",
    "end": "1009289"
  },
  {
    "text": "able to handle both real-time analytics off of your real time feed for example",
    "start": "1009289",
    "end": "1015259"
  },
  {
    "text": "you could do Kinesis data analytics you could use lambda you could use spark on",
    "start": "1015259",
    "end": "1021529"
  },
  {
    "text": "EMR and this float this top flow up here actually is the real-time processing",
    "start": "1021529",
    "end": "1026959"
  },
  {
    "text": "flow but what's really nice is you could set up this flow and then plug it into a Kinesis firehose to then take your data",
    "start": "1026959",
    "end": "1033620"
  },
  {
    "text": "and populate your data like with all the data as well so you could both do the real-time analytics kind of using the",
    "start": "1033620",
    "end": "1040188"
  },
  {
    "text": "tool of your choice there's only a list of four up there just because I didn't want to clutter the slide but you could",
    "start": "1040189",
    "end": "1045438"
  },
  {
    "text": "use fling you could use a lot of the other tools you could use storm to be able to process the data off of that",
    "start": "1045439",
    "end": "1051919"
  },
  {
    "text": "real-time feed and then you could also then populate your data like with that data and the important part of this is",
    "start": "1051919",
    "end": "1059769"
  },
  {
    "text": "with with AWS with s3 as well as if you're doing this on premises kind of",
    "start": "1059769",
    "end": "1064909"
  },
  {
    "text": "one of the big Achilles heels of of data leaks is really really small files right",
    "start": "1064909",
    "end": "1069919"
  },
  {
    "text": "you don't want really really small files in your date data like a lot of times because those end up you know a lot of",
    "start": "1069919",
    "end": "1075860"
  },
  {
    "text": "overhead even if it's just HDFS on premises a lot of overhead as you're running your jobs as you're running your",
    "start": "1075860",
    "end": "1081500"
  },
  {
    "text": "analytics this flow right here actually optimizes against that so it really",
    "start": "1081500",
    "end": "1087470"
  },
  {
    "text": "enforces the best practice because firehose will let you take all that data batch it up and then store those bashed",
    "start": "1087470",
    "end": "1094370"
  },
  {
    "text": "objects they could be encrypted with kms you could you could actually compress",
    "start": "1094370",
    "end": "1099409"
  },
  {
    "text": "them as well and and it batches them up into the larger block sizes and writes them out but then you can still have",
    "start": "1099409",
    "end": "1105409"
  },
  {
    "text": "your real time real time flows up at the top to be able to have your dashboards that sort of thing",
    "start": "1105409",
    "end": "1111850"
  },
  {
    "text": "so Kinesis really great at real-time data you might be getting your data in",
    "start": "1111850",
    "end": "1117200"
  },
  {
    "text": "through Kinesis you might be getting the data in through s3 adapters you know many different ways one of the really",
    "start": "1117200",
    "end": "1124909"
  },
  {
    "text": "really thing key takeaways that I would say is regardless of how you gain your data in your data Lake don't try to",
    "start": "1124909",
    "end": "1131630"
  },
  {
    "text": "transform it right away don't try to do a whole bunch of ETL on the data as part of your ingestion flow a really really",
    "start": "1131630",
    "end": "1138770"
  },
  {
    "text": "good pattern to ingest your data in the raw form CSV Park Hae some custom log",
    "start": "1138770",
    "end": "1144140"
  },
  {
    "text": "format that it's really hard to parse ingest the data in that form and then we'll talk about ways of prepping that",
    "start": "1144140",
    "end": "1151520"
  },
  {
    "text": "data for the various analytics and the reason why that is is a lot of times when you're dealing with these data",
    "start": "1151520",
    "end": "1157880"
  },
  {
    "text": "lakes and these big data systems it's really hard to recover from errors and recover from any issue that you might",
    "start": "1157880",
    "end": "1164029"
  },
  {
    "text": "have and if you're always storing the raw data you always have that source of truth to all to go back on to you might",
    "start": "1164029",
    "end": "1171679"
  },
  {
    "text": "want to do things like set up these lifecycle policies to be able to move it to glacier and pay for tenths of a cent",
    "start": "1171679",
    "end": "1179090"
  },
  {
    "text": "per gigabyte on your data but always have that raw data and in that data like to be able to even if it's in the cold",
    "start": "1179090",
    "end": "1186590"
  },
  {
    "text": "part of it and you need to rehydrate it in worst case scenarios if they're if you don't do this and you have some ETL",
    "start": "1186590",
    "end": "1192500"
  },
  {
    "text": "script that's transforming the data and then there's some air something goes wrong in that ETL script it's really",
    "start": "1192500",
    "end": "1199220"
  },
  {
    "text": "hard to recover in these systems depending on the size of them kind of related to the tiered",
    "start": "1199220",
    "end": "1206659"
  },
  {
    "text": "storage there's really these different classes in s3 these different object classes a lot of times a lot of the data",
    "start": "1206659",
    "end": "1215209"
  },
  {
    "text": "will fit in s3 standard that's what you generally think of when you just talk s3",
    "start": "1215209",
    "end": "1220399"
  },
  {
    "text": "in general it has the 1198 durability it has you know very high availability that",
    "start": "1220399",
    "end": "1225499"
  },
  {
    "text": "sort of thing what you could do is you can actually in your lake especially for this raw data you can set up these lifecycle policies so that as you",
    "start": "1225499",
    "end": "1233029"
  },
  {
    "text": "process your data or maybe you say it's gonna stay in hot storage for 30 days",
    "start": "1233029",
    "end": "1238639"
  },
  {
    "text": "but then I want to move it to infrequent access or cold storage you can set up the lifecycle policy to do this automatically for you that way you could",
    "start": "1238639",
    "end": "1245450"
  },
  {
    "text": "drive your cost down you know to be very very low you know four tenths of a cent per gig per month but then you always",
    "start": "1245450",
    "end": "1252379"
  },
  {
    "text": "have that source of record to go back on to if something goes wrong so a really really good pattern to be able to have",
    "start": "1252379",
    "end": "1258679"
  },
  {
    "text": "that raw data but have that lifecycle policy to drive drive those cost them you'll notice it's a fifth or maybe a",
    "start": "1258679",
    "end": "1266570"
  },
  {
    "text": "sixth of the price of kind of s3 standard which is only 2.3 cents but so",
    "start": "1266570",
    "end": "1272769"
  },
  {
    "text": "kind of shifting back to those best practices have that raw data sets coming in and have that immutable what that",
    "start": "1272769",
    "end": "1279379"
  },
  {
    "text": "means is if you're getting data from a database don't necessarily update your records have the original record and if",
    "start": "1279379",
    "end": "1286879"
  },
  {
    "text": "a new record if that record gets updated have another the time entry saying it's almost your event journal you know this",
    "start": "1286879",
    "end": "1292549"
  },
  {
    "text": "record was updated with this field at this time so it's really really good to have at least in the raw data set that's",
    "start": "1292549",
    "end": "1298759"
  },
  {
    "text": "all immutable you know you're always appending even if you're setting up those cold storage rules to be able to",
    "start": "1298759",
    "end": "1306229"
  },
  {
    "text": "grow that back truth a lot of times that data is really interesting for data scientists to see how data changes over",
    "start": "1306229",
    "end": "1312139"
  },
  {
    "text": "time not just necessarily the the most recent view of your data but what did that data look like over time and then",
    "start": "1312139",
    "end": "1318619"
  },
  {
    "text": "you know that's really really great but how do we optimize how do we really make this powerful for people to run",
    "start": "1318619",
    "end": "1325279"
  },
  {
    "text": "analytics machine learning on that sort of thing and that's where these curated data sets so you know I mentioned Dow",
    "start": "1325279",
    "end": "1331339"
  },
  {
    "text": "Jones Dow Jones is doing this had could talk a New York summit we have examples like FINRA FINRA ingests 70 to",
    "start": "1331339",
    "end": "1338739"
  },
  {
    "text": "90 billion events every day of various stock trading and identifies market",
    "start": "1338739",
    "end": "1344369"
  },
  {
    "text": "manipulators people try and do fraudulent activities on the market stock markets and I'll you know very",
    "start": "1344369",
    "end": "1351100"
  },
  {
    "text": "very common pattern to have that raw look staging location and that curated data set for your your analytics your",
    "start": "1351100",
    "end": "1357940"
  },
  {
    "text": "machine learning your data scientists one of the ways of doing that is you",
    "start": "1357940",
    "end": "1365019"
  },
  {
    "text": "know it doesn't have to be different buckets person I drew them as you know different lists of buckets here but",
    "start": "1365019",
    "end": "1371710"
  },
  {
    "text": "having these different areas each of these areas might have different access controls right the people that have",
    "start": "1371710",
    "end": "1377019"
  },
  {
    "text": "access to your raw data might be much much finer grain or much more limited than the people that have access to the",
    "start": "1377019",
    "end": "1383769"
  },
  {
    "text": "created a sets so as the data's coming it goes into the sraw phase or raw",
    "start": "1383769",
    "end": "1389769"
  },
  {
    "text": "location and then oftentimes you want to do you know ELT ETL you know a lot of",
    "start": "1389769",
    "end": "1396009"
  },
  {
    "text": "times people refer this as ELT B's you're you know you're actually storing the data you're extracting it from the",
    "start": "1396009",
    "end": "1401889"
  },
  {
    "text": "store storing your data in s3 first and then transform it later on to create these curated data sets in this example",
    "start": "1401889",
    "end": "1409210"
  },
  {
    "text": "here I happen to be showing glue as that icon I didn't really call it out but you",
    "start": "1409210",
    "end": "1415539"
  },
  {
    "text": "could use other services like EMR you know we have customers that are using knife I and other open source projects",
    "start": "1415539",
    "end": "1421570"
  },
  {
    "text": "as well but kind of architectural II this is a really really good pattern to",
    "start": "1421570",
    "end": "1426669"
  },
  {
    "text": "be able to have your corrida data sets though those might be mutable mutating",
    "start": "1426669",
    "end": "1431830"
  },
  {
    "text": "datasets you might actually create these materialized views where it's always the most logic you know the most up-to-date",
    "start": "1431830",
    "end": "1437679"
  },
  {
    "text": "view of your data that these Analects are running on so kind of going through",
    "start": "1437679",
    "end": "1445179"
  },
  {
    "text": "the ecosystem ingesting kind of that ETL elt pipeline the data management you",
    "start": "1445179",
    "end": "1453009"
  },
  {
    "text": "know how can I start analyzing this data start driving insights from the data and",
    "start": "1453009",
    "end": "1458080"
  },
  {
    "text": "that's really where we kind of go up the stack here and talk to some about our analytic services and how to structure",
    "start": "1458080",
    "end": "1463749"
  },
  {
    "text": "some of these you know when to use some of the when not to use others for example I get",
    "start": "1463749",
    "end": "1469780"
  },
  {
    "text": "questions all the time should I use Athena versus Rachel spectrum those sorts of things so within the analytic",
    "start": "1469780",
    "end": "1477400"
  },
  {
    "text": "section you know one of the first things I like to ask the customer if they say they say been you know what service should I use for this one of the first",
    "start": "1477400",
    "end": "1484210"
  },
  {
    "text": "things I like to ask is you know what are you trying to solve and who are the users right who's using the system",
    "start": "1484210",
    "end": "1489810"
  },
  {
    "text": "because who the users are is and what types of problems you're solving is very",
    "start": "1489810",
    "end": "1494950"
  },
  {
    "text": "very a strong driver to the types of tools that you may want to use for",
    "start": "1494950",
    "end": "1500050"
  },
  {
    "text": "example you know if you're doing you know data science work data engineering analytics the tools that those",
    "start": "1500050",
    "end": "1506940"
  },
  {
    "text": "professionals use are going to be quite different than like a standard bi user that might not be very tech savvy all",
    "start": "1506940",
    "end": "1513820"
  },
  {
    "text": "all these are powered from the data like but there's different flows going into",
    "start": "1513820",
    "end": "1520180"
  },
  {
    "text": "the lake but they're all using the common storage the central s3 storage as well as the catalog on top of that so on",
    "start": "1520180",
    "end": "1527680"
  },
  {
    "text": "the data science and we're gonna spend more time talking about how to structure data science platforms I think we have a",
    "start": "1527680",
    "end": "1533380"
  },
  {
    "text": "couple examples for a FINRA they're unified data science platform and some others here in a moment but you",
    "start": "1533380",
    "end": "1539770"
  },
  {
    "text": "know what you'll see here is you know different UI so sage maker Cabana",
    "start": "1539770",
    "end": "1545560"
  },
  {
    "text": "Zeppelin Jupiter environments those are very development savvy so I would never",
    "start": "1545560",
    "end": "1551290"
  },
  {
    "text": "ask you know a business analyst who is in very tech savvy to use a Jupiter notebook right you know you need to be",
    "start": "1551290",
    "end": "1557290"
  },
  {
    "text": "able to write these code fragments that sort of thing but then you might have your business reporter you know business",
    "start": "1557290",
    "end": "1564130"
  },
  {
    "text": "analysts using a quick side tableau a MicroStrategy on top of your data warehouse and this is a really important",
    "start": "1564130",
    "end": "1571210"
  },
  {
    "text": "aspect you know a lot of times people will ask you know should I use a data",
    "start": "1571210",
    "end": "1577090"
  },
  {
    "text": "warehouse or should I use a data Lake and you might use one or the other but they do complement each other very very",
    "start": "1577090",
    "end": "1583570"
  },
  {
    "text": "very well and you know a lot of times if you have an application that needs to be",
    "start": "1583570",
    "end": "1588580"
  },
  {
    "text": "able to have very very strict SLA s for business reporting you know if you're",
    "start": "1588580",
    "end": "1594130"
  },
  {
    "text": "using tableau if you're using quick site MicroStrategy these sorts of tools and you have very very tight",
    "start": "1594130",
    "end": "1600029"
  },
  {
    "text": "essays on reporting that sort of thing a lot of folks are still leveraging the data warehouse for that red shift that",
    "start": "1600029",
    "end": "1606809"
  },
  {
    "text": "sort of thing if you really have you know this need to be able to run",
    "start": "1606809",
    "end": "1611999"
  },
  {
    "text": "advanced analytics machine learning it's scanning giant corpuses of documents to",
    "start": "1611999",
    "end": "1617190"
  },
  {
    "text": "do different sorts of analytics on the data that's really where we see a lot of folks leveraging the data like directly",
    "start": "1617190",
    "end": "1625909"
  },
  {
    "text": "so let's step through a couple of these so the first is Amazon Athena how many",
    "start": "1625909",
    "end": "1630959"
  },
  {
    "text": "folks are familiar with Amazon Athena okay maybe half maybe a little bit more so",
    "start": "1630959",
    "end": "1638249"
  },
  {
    "text": "Athena really really good compliment to your data like so it really provides",
    "start": "1638249",
    "end": "1643559"
  },
  {
    "text": "sequel level access into the lake integrates with things like the glue data catalog so what you could do is you",
    "start": "1643559",
    "end": "1651029"
  },
  {
    "text": "could actually sub these crawlers with the custom classifiers and then you could use Athena to be able to start",
    "start": "1651029",
    "end": "1656999"
  },
  {
    "text": "querying that data through sequel and the important aspect about this is remember the data actually is stored in",
    "start": "1656999",
    "end": "1663989"
  },
  {
    "text": "the raw form so that might be a JSON document it might be a CSV it might be whatever format it's in what you do with",
    "start": "1663989",
    "end": "1670379"
  },
  {
    "text": "Athena is as you're understanding your data you might want to CERN users to be able to query the raw data to better",
    "start": "1670379",
    "end": "1676199"
  },
  {
    "text": "understand it may be to populate a data warehouse or do something else with the data what what this allows you to do is",
    "start": "1676199",
    "end": "1682289"
  },
  {
    "text": "this considered schema on read what that means is the data gets stored in s3 in the raw form and whatever format it's in",
    "start": "1682289",
    "end": "1689579"
  },
  {
    "text": "and then the schemas actually applied the the logical table is applied as you're querying and you know really",
    "start": "1689579",
    "end": "1696869"
  },
  {
    "text": "really nice to be able to take a data set you know the New York City Taxi data a lot of times will demo within about a",
    "start": "1696869",
    "end": "1702239"
  },
  {
    "text": "couple minutes we bring the data in s3 we run the crawlers and we're able to start doing select statements on the",
    "start": "1702239",
    "end": "1707909"
  },
  {
    "text": "data set and analyze the data better so it really lets you start investigating",
    "start": "1707909",
    "end": "1713609"
  },
  {
    "text": "data really really well what's interesting though is you could also join datasets together that are in",
    "start": "1713609",
    "end": "1719579"
  },
  {
    "text": "different formats so if I have JSON data over here I have part ki f CSV and one statement I could",
    "start": "1719579",
    "end": "1725549"
  },
  {
    "text": "create a join criteria and start joining the data together it's not gonna be as performant like if you have two giant",
    "start": "1725549",
    "end": "1731969"
  },
  {
    "text": "fact tables that you traditionally put in a data warehousing try to do like joins across that it's not really meant for that sort of ad-hoc",
    "start": "1731969",
    "end": "1739200"
  },
  {
    "text": "query today that's really where like redshift and ratcheted spectrum would fit in great but really really great at",
    "start": "1739200",
    "end": "1745980"
  },
  {
    "text": "being able to do that ad hoc query and against your dataset the the other",
    "start": "1745980",
    "end": "1752190"
  },
  {
    "text": "service here that we see a ton of customers use and we'll talk about some of the ways of structuring this in a",
    "start": "1752190",
    "end": "1757440"
  },
  {
    "text": "moment is Amazon EMR so we're gonna dive a little bit more into EMR here on the",
    "start": "1757440",
    "end": "1763020"
  },
  {
    "text": "next couple slides EMR really provides like I like to say it's the Hadoop and",
    "start": "1763020",
    "end": "1768060"
  },
  {
    "text": "spark ecosystem as a service it's not just MapReduce is just it's not just HDFS it provides if you want to run",
    "start": "1768060",
    "end": "1774720"
  },
  {
    "text": "flink or presto or your spark or hive and pig and tez and and all types of",
    "start": "1774720",
    "end": "1780420"
  },
  {
    "text": "different applications there's 19 of them right now supported on EMR and we",
    "start": "1780420",
    "end": "1785490"
  },
  {
    "text": "have ways of bootstrapping cluster so if you have some really custom thing I've had customers want to run like open TS",
    "start": "1785490",
    "end": "1791520"
  },
  {
    "text": "DB which runs on top of HBase we provide HBase on top of EMR but they ran bootstrap actions to load open TS DB",
    "start": "1791520",
    "end": "1798480"
  },
  {
    "text": "which is a open source time series database that lives on top of HBase so it's really easy to take this platform",
    "start": "1798480",
    "end": "1805800"
  },
  {
    "text": "launch it a claw sure but then extend it it's interesting because this is actually one of the services where we do",
    "start": "1805800",
    "end": "1811170"
  },
  {
    "text": "a lot of the heavy lifting of running the Hadoop and spark for you but you actually have direct SSH access onto the",
    "start": "1811170",
    "end": "1816870"
  },
  {
    "text": "EMR cluster a little different than things like RDS or if you think of RDS you don't actually have SSH access onto",
    "start": "1816870",
    "end": "1823590"
  },
  {
    "text": "a database you just have that that logical JDBC ODBC access so you have a",
    "start": "1823590",
    "end": "1828660"
  },
  {
    "text": "lot of flexibility with EMR to be able to customize that Hadoop environment even though it's considered a higher",
    "start": "1828660",
    "end": "1834780"
  },
  {
    "text": "managed service and what these environments look like I spent before",
    "start": "1834780",
    "end": "1840450"
  },
  {
    "text": "joining AWS been here about three and a half years I actually built a few on-premise Hadoop clusters and you know",
    "start": "1840450",
    "end": "1847980"
  },
  {
    "text": "building on my premise was always a challenge you know we always did this capacity management and one of the key",
    "start": "1847980",
    "end": "1853770"
  },
  {
    "text": "concepts of on-premise Hadoop clusters is coupling storage with compute right when you're building cluster you want to",
    "start": "1853770",
    "end": "1860100"
  },
  {
    "text": "figure out how much storage do I need how much compute do I need try to predict that and have that as a as an",
    "start": "1860100",
    "end": "1866730"
  },
  {
    "text": "exercise and then if you want to grow it's really really hard to adjust that over time what we like to",
    "start": "1866730",
    "end": "1872759"
  },
  {
    "text": "recommend with an AWS is actually decoupling those leveraging s3 as as the",
    "start": "1872759",
    "end": "1879539"
  },
  {
    "text": "storage of your data Lake and then leveraging the Hadoop and spark on top of that and connecting directly into the",
    "start": "1879539",
    "end": "1885659"
  },
  {
    "text": "s3 is interesting because Netflix actually has a blog post where they tested this configuration versus local",
    "start": "1885659",
    "end": "1893490"
  },
  {
    "text": "HDFS on a Hadoop cluster and they actually found some you know very very",
    "start": "1893490",
    "end": "1898590"
  },
  {
    "text": "consistent results but interesting for people that are new to the cloud and Hadoop on the cloud where the way you",
    "start": "1898590",
    "end": "1905580"
  },
  {
    "text": "store your data and the way you write your analytics are gonna far supersede the performance of analytic than if it's",
    "start": "1905580",
    "end": "1912539"
  },
  {
    "text": "on HDFS or s3 there's a slight difference so if you look at their blog posts and their charts you know their",
    "start": "1912539",
    "end": "1918509"
  },
  {
    "text": "sequence files you know when they're doing their queries might have taken 60 seconds versus 55 or 53 or whatever it",
    "start": "1918509",
    "end": "1925080"
  },
  {
    "text": "is but what's really interesting is if you just structure your data slightly different that Delta drops drastically",
    "start": "1925080",
    "end": "1931320"
  },
  {
    "text": "so if you use park' or RC or these other optimized forms that they're both the",
    "start": "1931320",
    "end": "1937259"
  },
  {
    "text": "the the difference between sequence file on HTTPS and park' on HDFS dropped",
    "start": "1937259",
    "end": "1943500"
  },
  {
    "text": "drastically but the difference between this park' on s3 and HDFS was less than",
    "start": "1943500",
    "end": "1949019"
  },
  {
    "text": "a second so the reason I like to mention this is you know a lot of times if you're building this system and you use",
    "start": "1949019",
    "end": "1954690"
  },
  {
    "text": "an s3 as a day like if a customer comes to me and says I'm not really getting the performance I need out of it the",
    "start": "1954690",
    "end": "1960240"
  },
  {
    "text": "first thing I like to ask is ok 1 are you writing in the same region obviously you would want your s3 bucket in the",
    "start": "1960240",
    "end": "1966990"
  },
  {
    "text": "same region as your EMR cluster if it's not as going across region but the second question I like to ask is you",
    "start": "1966990",
    "end": "1973860"
  },
  {
    "text": "know what is your data format because if it if worst case scenario is like a a",
    "start": "1973860",
    "end": "1978899"
  },
  {
    "text": "gzip CSV file that's not splittable that is one that would be like horrible you",
    "start": "1978899",
    "end": "1985230"
  },
  {
    "text": "know but so very very performant if you architect it the right way and this is",
    "start": "1985230",
    "end": "1991320"
  },
  {
    "text": "exactly how you know Dow Jones is doing it FINRA a lot of our Nasdaq a lot of",
    "start": "1991320",
    "end": "1996690"
  },
  {
    "text": "our customers are doing this exact architecture with very very high loads and what this really buys you is not",
    "start": "1996690",
    "end": "2004140"
  },
  {
    "text": "only being able to scale those independently so you don't have to necessarily predict how much storage in",
    "start": "2004140",
    "end": "2009420"
  },
  {
    "text": "your day lake you need upfront you could just consume it as you need it but it also lets you set up multiple clusters",
    "start": "2009420",
    "end": "2015530"
  },
  {
    "text": "and this is important for a couple of reasons the first is you know when",
    "start": "2015530",
    "end": "2021060"
  },
  {
    "text": "you're building this sort of architecture on premises you don't really set up different clusters for different jobs right it's it's one big",
    "start": "2021060",
    "end": "2027120"
  },
  {
    "text": "cluster with a resource negotiator that runs all these different jobs and at times you know if you look at the",
    "start": "2027120",
    "end": "2032880"
  },
  {
    "text": "profiles oftentimes there's a large queue during peak peak hours of the day",
    "start": "2032880",
    "end": "2039120"
  },
  {
    "text": "very very low queue within Yarn those sorts of things during other times of",
    "start": "2039120",
    "end": "2044700"
  },
  {
    "text": "the day what you could actually do is you can actually set up these dedicated for example if I have a job that needs",
    "start": "2044700",
    "end": "2051330"
  },
  {
    "text": "to run every night I could spin that up point it to the same data on s3 point it to the same data Kellog so it has the",
    "start": "2051330",
    "end": "2057629"
  },
  {
    "text": "same logical view so these clusters see the same data regardless of which",
    "start": "2057630",
    "end": "2063480"
  },
  {
    "text": "cluster you go on with this architecture but then you could spin up these transient clusters maybe to do an ETL",
    "start": "2063480",
    "end": "2068850"
  },
  {
    "text": "job and leverage things like the spot market drive across town like 90 percent for these for these ETL jobs and then",
    "start": "2068850",
    "end": "2075300"
  },
  {
    "text": "you have these interactive clusters these streaming clusters that are very very specialized that aren't necessarily",
    "start": "2075300",
    "end": "2080460"
  },
  {
    "text": "competing resources for these other jobs to run the other thing that this really",
    "start": "2080460",
    "end": "2085649"
  },
  {
    "text": "lets you do is let's say you have groups that want to be able to run different",
    "start": "2085650",
    "end": "2090990"
  },
  {
    "text": "sorts of Hadoop jobs with different libraries it's interesting these many times when you actually look at what",
    "start": "2090990",
    "end": "2097620"
  },
  {
    "text": "libraries are needed for different versions of the analytical stack",
    "start": "2097620",
    "end": "2103110"
  },
  {
    "text": "sometimes the not to pick on Avro but maybe the Avro jar it changed and it's not compatible anymore",
    "start": "2103110",
    "end": "2108600"
  },
  {
    "text": "I remember that I think one time for one spark upgrade or you know maybe these different jars don't really play nicely",
    "start": "2108600",
    "end": "2115290"
  },
  {
    "text": "together it's really really easy to stand up another cluster have your old jobs running on the old cluster and do more",
    "start": "2115290",
    "end": "2122310"
  },
  {
    "text": "of a Bluegreen deployment to the new cluster and if you're doing this kind of in a traditional approach you'd actually have to do a lot more data syncing you",
    "start": "2122310",
    "end": "2129420"
  },
  {
    "text": "know disk copies that sort of thing to be able to see synchronize the data on the cluster but",
    "start": "2129420",
    "end": "2134730"
  },
  {
    "text": "you don't necessarily have to do that in this case because it's decoupled from that um so you know I could probably",
    "start": "2134730",
    "end": "2143730"
  },
  {
    "text": "spend about two hours talking about EMR and Hadoop systems I won't do that to you guys don't want everyone leaving the",
    "start": "2143730",
    "end": "2150330"
  },
  {
    "text": "room here so let's kind of shift a little bit for a date for data warehousing here so data warehousing",
    "start": "2150330",
    "end": "2156630"
  },
  {
    "text": "really really powerful with an AWS Amazon redshift lets you you know start",
    "start": "2156630",
    "end": "2162270"
  },
  {
    "text": "with relatively small data warehouse clusters so 160 gig warehouse cluster",
    "start": "2162270",
    "end": "2168000"
  },
  {
    "text": "and scale all our way up to the petabyte range and what you could do is you could launch this cluster you could you know",
    "start": "2168000",
    "end": "2175410"
  },
  {
    "text": "shut it down and you can launch a new one and you know it's fairly simple to actually plug in a very wide number of",
    "start": "2175410",
    "end": "2181830"
  },
  {
    "text": "BI tools on here and the reason why folks are doing this and I should have set this in context of data lakes is de",
    "start": "2181830",
    "end": "2188700"
  },
  {
    "text": "lakes are really great for those analytics for Hadoop for machine learning deep learning you wouldn't",
    "start": "2188700",
    "end": "2193980"
  },
  {
    "text": "necessarily have a business analyst talk directly to a data like you want them to talk to curated data sets that been",
    "start": "2193980",
    "end": "2200730"
  },
  {
    "text": "vetted you know maybe there's data biases in the data that you want to bet out that sort of thing bless you so you",
    "start": "2200730",
    "end": "2208500"
  },
  {
    "text": "know oftentimes what you'll have is you'll have your data like populating these data warehouses that could be very",
    "start": "2208500",
    "end": "2214770"
  },
  {
    "text": "very simple in terms of I have my data in s3 I have my data in glue my data catalog and glue and I'm just doing some",
    "start": "2214770",
    "end": "2221040"
  },
  {
    "text": "copies into my my rest shift cluster to be able to load load that cluster so it",
    "start": "2221040",
    "end": "2227280"
  },
  {
    "text": "could be you could start very very small with this sort of architecture and grow as your needs grow the other way of",
    "start": "2227280",
    "end": "2234090"
  },
  {
    "text": "getting the data in here is using things like glue ETL so let's say let's say in",
    "start": "2234090",
    "end": "2240060"
  },
  {
    "text": "my data like I have JSON data JSON data is really really great as a a file based",
    "start": "2240060",
    "end": "2245609"
  },
  {
    "text": "data but if you try to load JSON into like a normal relation one database not a document or a database it's quite hard",
    "start": "2245609",
    "end": "2252690"
  },
  {
    "text": "right you know it's very nested at these different fields what if you want to load some of that data into a data",
    "start": "2252690",
    "end": "2258570"
  },
  {
    "text": "warehouse or a database let's talk about redshift and data warehouse in this example this is really where you'd might",
    "start": "2258570",
    "end": "2264420"
  },
  {
    "text": "want to use something like glue or some sort of ETL process rather than just a straight copy because what you could do is you can",
    "start": "2264420",
    "end": "2270390"
  },
  {
    "text": "actually use functions called there's one in blue called relational lives what that does is it will take a nested",
    "start": "2270390",
    "end": "2276030"
  },
  {
    "text": "object for example a nested JSON or nested XML and it will create a set of tables for you to populate it your your",
    "start": "2276030",
    "end": "2283650"
  },
  {
    "text": "relational form and these tables actually have the IDS referencing the parents and child so if you have one",
    "start": "2283650",
    "end": "2290250"
  },
  {
    "text": "object that actually has three children and then those children have more children all the IDS match across the",
    "start": "2290250",
    "end": "2296100"
  },
  {
    "text": "data frames or the dynamic frames that are going to begin loaded into your your relational store so you know that's a",
    "start": "2296100",
    "end": "2303420"
  },
  {
    "text": "really good method if you have kind of complex structures that you want to load into your your your data warehouse again",
    "start": "2303420",
    "end": "2310320"
  },
  {
    "text": "you know with data warehousing the schema on schema on right so with data warehousing you define your schema",
    "start": "2310320",
    "end": "2316560"
  },
  {
    "text": "upfront and you're actually riding into that schema and that's why you know a lot of times people will park it in in",
    "start": "2316560",
    "end": "2322590"
  },
  {
    "text": "the lake first and then pre-process it you know transform it load it into their data warehouse so you know here's a nice",
    "start": "2322590",
    "end": "2330750"
  },
  {
    "text": "long slide comparing some of the data warehousing versus data lakes you know",
    "start": "2330750",
    "end": "2336870"
  },
  {
    "text": "one of the key differences really is the second line their schema on read versus schema on right you know we talked about",
    "start": "2336870",
    "end": "2343620"
  },
  {
    "text": "what that means but you know when you apply that logical data model or physical data model depending on what",
    "start": "2343620",
    "end": "2348960"
  },
  {
    "text": "what system it is the the other key thing I would mention is the use cases",
    "start": "2348960",
    "end": "2354930"
  },
  {
    "text": "you know a lot of times we're seeing you know bi generally is powered by your warehousing and then you know your data",
    "start": "2354930",
    "end": "2361290"
  },
  {
    "text": "science predictive analytics you know there are some bi use cases that talk directly to a data Lake so quick site",
    "start": "2361290",
    "end": "2368010"
  },
  {
    "text": "and tableau both have Athena adapters right so you can technically set up a tableau dashboard or a quick site",
    "start": "2368010",
    "end": "2374070"
  },
  {
    "text": "dashboard to query directly into your lake but if if somebody came to me and said they wanted to do that but they",
    "start": "2374070",
    "end": "2379740"
  },
  {
    "text": "want to scale and have also very very tight production SLA s that's that's",
    "start": "2379740",
    "end": "2386010"
  },
  {
    "text": "where the tipping point oftentimes is where you might want to use more of a data warehouse or more of a redshift but",
    "start": "2386010",
    "end": "2393990"
  },
  {
    "text": "you know how can we combine these a little bit more seamless this is where Wretch's spectrum comes in so",
    "start": "2393990",
    "end": "2401800"
  },
  {
    "text": "you wouldn't necessarily ever run wretches spectrum unless you're already running a redshift data warehouse",
    "start": "2401800",
    "end": "2406980"
  },
  {
    "text": "because wretches spectrum is actually a component of your redshift cluster it's",
    "start": "2406980",
    "end": "2412480"
  },
  {
    "text": "actually a piece that's inside your restive cluster and then there's a separate compute layer which I'll show",
    "start": "2412480",
    "end": "2417760"
  },
  {
    "text": "in a moment but what it allows you to do is really combine both the schema on",
    "start": "2417760",
    "end": "2422860"
  },
  {
    "text": "write and the schema on read together or to say differently your data warehouse and your data lake together and what",
    "start": "2422860",
    "end": "2428920"
  },
  {
    "text": "that means is what I could do is I can actually define for my bi users or my users in my data warehouse a star schema",
    "start": "2428920",
    "end": "2434920"
  },
  {
    "text": "certain schema that they're using to run their their reporting on but what I could do is if I'm bringing in new data",
    "start": "2434920",
    "end": "2440740"
  },
  {
    "text": "or if I have a really really time-sensitive day load that I want to",
    "start": "2440740",
    "end": "2445750"
  },
  {
    "text": "be able to access right away Nuvi ID is a really good example they actually use spectrum to be able to get the data to",
    "start": "2445750",
    "end": "2452020"
  },
  {
    "text": "their bi users very very quickly where it's in seconds now instead of minutes and you know what you're able to do is",
    "start": "2452020",
    "end": "2459520"
  },
  {
    "text": "you're able to define these external schemas and point them to your data in your data like that might be a glue s3",
    "start": "2459520",
    "end": "2467020"
  },
  {
    "text": "combination it might be some other hive store combination it doesn't have to be glue in this example so you can have as",
    "start": "2467020",
    "end": "2473710"
  },
  {
    "text": "long as it's s3 in some sort of metadata store that's hive compliant you can",
    "start": "2473710",
    "end": "2478750"
  },
  {
    "text": "actually configure your retro spectrum external schemas to actually take some of the data out of s3 in your data lake",
    "start": "2478750",
    "end": "2485860"
  },
  {
    "text": "and combine it with the data in your data warehouse you know the time-sensitive example that",
    "start": "2485860",
    "end": "2491680"
  },
  {
    "text": "Nuvi a does is one example of that another example is you know maybe you have some really cold data and then you",
    "start": "2491680",
    "end": "2498250"
  },
  {
    "text": "have some really hot data but you want both of those datasets exposed to the same tool to your bi users so you might",
    "start": "2498250",
    "end": "2504790"
  },
  {
    "text": "want you know you're bi users to be able to query data sets on the cluster that hot data and then for other sets of data",
    "start": "2504790",
    "end": "2512950"
  },
  {
    "text": "sets to be able to query directly down to the data like and that you know this is another really good example of where",
    "start": "2512950",
    "end": "2519850"
  },
  {
    "text": "retro spectrum gets used in this data flow and the way this works when you're",
    "start": "2519850",
    "end": "2525010"
  },
  {
    "text": "using spectrum is the query still comes through that leader node so redshift is actually structured with a leader node",
    "start": "2525010",
    "end": "2531580"
  },
  {
    "text": "and then it has a set of compute nodes underneath it and what happens is you're doing your querying let's say I'm",
    "start": "2531580",
    "end": "2538330"
  },
  {
    "text": "taking some of my hot data on my cluster and let's say I'm taking that data set I'm combining it with some cold data in",
    "start": "2538330",
    "end": "2544330"
  },
  {
    "text": "my data like I'm doing a joint across those data sets from the B ice perspective or the BI users perspective",
    "start": "2544330",
    "end": "2550410"
  },
  {
    "text": "all I'm doing is I'm querying two tables one of them happens to be external schema or external table so the user",
    "start": "2550410",
    "end": "2557980"
  },
  {
    "text": "experience from the B ice perspective is very seamless but under the covers what happens is the external schemas through",
    "start": "2557980",
    "end": "2565300"
  },
  {
    "text": "red spectrum gets delegated down to this spectrum compute layer that does a",
    "start": "2565300",
    "end": "2570700"
  },
  {
    "text": "couple things one it actually offloads a lot of compute so anything that's done at the spectrum layer actually isn't",
    "start": "2570700",
    "end": "2576310"
  },
  {
    "text": "consuming compute out of your actual cluster and then it's also then taking the data set oftentimes doing like",
    "start": "2576310",
    "end": "2582790"
  },
  {
    "text": "predicate push down and bringing back as limited data set into the cluster as it",
    "start": "2582790",
    "end": "2588610"
  },
  {
    "text": "can to be able to optimize that query you could actually do things like create",
    "start": "2588610",
    "end": "2593680"
  },
  {
    "text": "what's called late binding views what that allows you to do is create a logical view across the data set and",
    "start": "2593680",
    "end": "2599860"
  },
  {
    "text": "that actually might join some data out of your hot data on your cluster and join it with the data on your data like",
    "start": "2599860",
    "end": "2605920"
  },
  {
    "text": "to create one view for those bi users so some really really good techniques to be able to leverage that data lake and that",
    "start": "2605920",
    "end": "2613150"
  },
  {
    "text": "data warehouse technologies together to be able to provide those to those bi",
    "start": "2613150",
    "end": "2618490"
  },
  {
    "text": "users so you know here what we're seeing you know just as a higher level view is",
    "start": "2618490",
    "end": "2627220"
  },
  {
    "text": "you know taking that data out of the data lake populating it both on the compute nodes of your redshift cluster",
    "start": "2627220",
    "end": "2633100"
  },
  {
    "text": "but then also being able to query that data in place through things like",
    "start": "2633100",
    "end": "2638950"
  },
  {
    "text": "spectrum one other thing then the reason I want to add you know expand on this a",
    "start": "2638950",
    "end": "2644380"
  },
  {
    "text": "little bit is you actually might have cases where data flows into your data warehouse first it's not as common as",
    "start": "2644380",
    "end": "2651220"
  },
  {
    "text": "flowing into s3 you know even if it's flowing into your rush of cluster we like to have people stated in s3 and do",
    "start": "2651220",
    "end": "2657940"
  },
  {
    "text": "copies anyway but you actually might have data that's kin derived and generated within your warehouse and push",
    "start": "2657940",
    "end": "2664450"
  },
  {
    "text": "backed into your like maybe through a unload statement or something like that but these are really kind of",
    "start": "2664450",
    "end": "2670450"
  },
  {
    "text": "working together in conjunction with each other so you know we talked a",
    "start": "2670450",
    "end": "2676810"
  },
  {
    "text": "little bit about analytics ingestion metadata management kind of completing the circle across that architecture",
    "start": "2676810",
    "end": "2683350"
  },
  {
    "text": "we're looking at first is how do we start doing machine learning and machine learning and deep learning on this big",
    "start": "2683350",
    "end": "2688990"
  },
  {
    "text": "data set and really the data in our data Lake and why we like to talk about these things a lot of times together is",
    "start": "2688990",
    "end": "2695380"
  },
  {
    "text": "machine learning and deep learning is all about driving patterns and recognizing things in data you always",
    "start": "2695380",
    "end": "2702370"
  },
  {
    "text": "end up plugging data into some sort of machine learning algorithm it's all about being able to find what these",
    "start": "2702370",
    "end": "2707680"
  },
  {
    "text": "functions are or these formulas are and then being able to apply that to new data that you're seeing in the future so",
    "start": "2707680",
    "end": "2714670"
  },
  {
    "text": "we we oftentimes like to talk about these flywheel effects so as you get more and more data you are able to make",
    "start": "2714670",
    "end": "2720850"
  },
  {
    "text": "better decisions using a lot of the analytical based tools but then that's also driving better machine learning",
    "start": "2720850",
    "end": "2727300"
  },
  {
    "text": "better products that sort of thing which then drives the additional data so you",
    "start": "2727300",
    "end": "2732580"
  },
  {
    "text": "know we see this flywheel effect quite often both on our retail side as well as working with customers so on the machine",
    "start": "2732580",
    "end": "2740980"
  },
  {
    "text": "learning side you know you guys have probably caught a common theme it's all about being able to plug these tools",
    "start": "2740980",
    "end": "2748150"
  },
  {
    "text": "into the into the lake here and there's really these different classes of machine learning services and it really",
    "start": "2748150",
    "end": "2756550"
  },
  {
    "text": "kind of depends on what you need to get out of your data lake and what you need to get out of your algorithm so if",
    "start": "2756550",
    "end": "2763450"
  },
  {
    "text": "you're actually taking let's say you're taking video for mation or you're taking imagery and you want to be able to have",
    "start": "2763450",
    "end": "2769990"
  },
  {
    "text": "that come in but you also want to be able to make it searchable and and drive insights and extract metadata which then",
    "start": "2769990",
    "end": "2776710"
  },
  {
    "text": "becomes part of your data like itself you might want to use something like one of our application services these are",
    "start": "2776710",
    "end": "2782920"
  },
  {
    "text": "services that allow you to not necessarily worry about the training portion so you don't have to train on",
    "start": "2782920",
    "end": "2788230"
  },
  {
    "text": "all these tags all these different things maybe you have audio files that you want to transcribe that those sorts",
    "start": "2788230",
    "end": "2794140"
  },
  {
    "text": "of use cases what you could do is you could very easily set up various triggers in your lake to be able to",
    "start": "2794140",
    "end": "2799480"
  },
  {
    "text": "automatically run shock that information and then store it back in in there the other thing that we",
    "start": "2799480",
    "end": "2806650"
  },
  {
    "text": "see quite a bit is using things like sage maker so CH maker is one of our platform services sage maker allows you",
    "start": "2806650",
    "end": "2813249"
  },
  {
    "text": "to set this up so that when you're actually running your jobs it connects into your data catalog so we have lots",
    "start": "2813249",
    "end": "2820299"
  },
  {
    "text": "of customers that are using boto and various integrations to include data catalog and runs your jobs to be able to",
    "start": "2820299",
    "end": "2827039"
  },
  {
    "text": "extract that information and get it out one example I like to give is this",
    "start": "2827039",
    "end": "2832390"
  },
  {
    "text": "digital globe example so digital globe is a very very large customer on AWS they have over a hundred petabytes in",
    "start": "2832390",
    "end": "2838660"
  },
  {
    "text": "their in their data Lake they're very very satellite data driven but they run a lot of interesting and licks on it so",
    "start": "2838660",
    "end": "2845109"
  },
  {
    "text": "they work with various organizations to be able identify fire movement whenever",
    "start": "2845109",
    "end": "2851259"
  },
  {
    "text": "there's big fires or other natural disasters so they're constantly taking this geospatial data but also running",
    "start": "2851259",
    "end": "2857410"
  },
  {
    "text": "advancing analytics and machine learning on the data what's really interesting though is they actually used it in",
    "start": "2857410",
    "end": "2863349"
  },
  {
    "text": "another use case that isn't kind of the traditional machine learning or deep",
    "start": "2863349",
    "end": "2868390"
  },
  {
    "text": "learning example where they wanted to really instead optimize operations they wanted to be able to take that hundred",
    "start": "2868390",
    "end": "2874509"
  },
  {
    "text": "petabyte data lake and be able to how to be able to effectively cache that data",
    "start": "2874509",
    "end": "2879640"
  },
  {
    "text": "to be able to set different bucket classes or different classes for their different objects in their satellite",
    "start": "2879640",
    "end": "2884799"
  },
  {
    "text": "imagery and what they able to do and there's a really good blog post about this is they were able to use sage maker and machine learning algorithms and",
    "start": "2884799",
    "end": "2891130"
  },
  {
    "text": "change their storage classes on their data in their data Lake to be able to drop their cost in half and the way they",
    "start": "2891130",
    "end": "2898150"
  },
  {
    "text": "did that is they predicted what data would be accessed and as people were accessing the data I have a really cool",
    "start": "2898150",
    "end": "2904420"
  },
  {
    "text": "video here if it's going to play so what we see here is the different cache hits",
    "start": "2904420",
    "end": "2909849"
  },
  {
    "text": "over three months and which parts are being accessed and they are able to predict with a I think it was about a",
    "start": "2909849",
    "end": "2916479"
  },
  {
    "text": "90% accuracy of what data was going to be requested when and when they are able",
    "start": "2916479",
    "end": "2922299"
  },
  {
    "text": "to predict that well they're able to set up their cache more effectively have those caches smaller move more stuff to",
    "start": "2922299",
    "end": "2928449"
  },
  {
    "text": "cold storage that they needed to and that's really how through this architecture they're able to drive those",
    "start": "2928449",
    "end": "2933880"
  },
  {
    "text": "costs in half so you know we have tons of really interesting examples on you know satellite imagery other types of videos",
    "start": "2933880",
    "end": "2941740"
  },
  {
    "text": "that sort of thing I like to talk about this one because you know it's it's kind of that operational use case so we're",
    "start": "2941740",
    "end": "2946780"
  },
  {
    "text": "we're really seeing this being applied throughout the ecosystem not just you know imagery voice that sort of thing",
    "start": "2946780",
    "end": "2954030"
  },
  {
    "text": "another example here that we're going to talk about is FINRA and how they're doing machine learning on their day",
    "start": "2954030",
    "end": "2959110"
  },
  {
    "text": "awake they very very rich system FINRA actually developed their own metadata",
    "start": "2959110",
    "end": "2964630"
  },
  {
    "text": "layer they developed this before glue data catalog came out it's called herd so if you think of like hurting your",
    "start": "2964630",
    "end": "2970900"
  },
  {
    "text": "data herding cats it's an open-source tool called herd and they're running all that on AWS they're using our services",
    "start": "2970900",
    "end": "2978490"
  },
  {
    "text": "they're using our partner services things like data breaks EMR together that sort of thing and what they're",
    "start": "2978490",
    "end": "2984880"
  },
  {
    "text": "doing is they're taking all the stock information and running machine learning and deep learning on the data and what",
    "start": "2984880",
    "end": "2991420"
  },
  {
    "text": "their architecture consists of is you know strong slightly different because I think I stole this slide from when there are events or one of their summit",
    "start": "2991420",
    "end": "2998610"
  },
  {
    "text": "presentations but what you'll see is they actually have a very similar architecture right they have their",
    "start": "2998610",
    "end": "3004920"
  },
  {
    "text": "storage it's an s3 they have their metadata store its hive compliant they happen to be using heard because glue",
    "start": "3004920",
    "end": "3011820"
  },
  {
    "text": "data Kellogg wasn't out yet in various reasons they've been running this system for quite some time and improving on top",
    "start": "3011820",
    "end": "3017370"
  },
  {
    "text": "of it they're also running multiple clusters you'll notice it's not just one Hadoop cluster it's not just one lambda",
    "start": "3017370",
    "end": "3023040"
  },
  {
    "text": "it's not just one one compute layer they're running different types of",
    "start": "3023040",
    "end": "3028170"
  },
  {
    "text": "compute based on what type of job they're running on what that really",
    "start": "3028170",
    "end": "3033600"
  },
  {
    "text": "allows them to do through that architecture is not just use one one tool here so this is actually another",
    "start": "3033600",
    "end": "3039990"
  },
  {
    "text": "screenshot of their I think it's the unified data science platform is what that acronym stands for it's something",
    "start": "3039990",
    "end": "3046080"
  },
  {
    "text": "that FINRA caused their data science platform and through this platform they",
    "start": "3046080",
    "end": "3052380"
  },
  {
    "text": "actually provide their scientists access to our and shiny and cafe and and",
    "start": "3052380",
    "end": "3057570"
  },
  {
    "text": "tensorflow and and scikit-learn and lots of these different tools which is integrating with that catalog and",
    "start": "3057570",
    "end": "3063810"
  },
  {
    "text": "talking to s3 and I didn't show the slide here I don't know if it's",
    "start": "3063810",
    "end": "3069060"
  },
  {
    "text": "no it's not on this one they have a couple deep dive sessions on our previous reinvents in some it's talking",
    "start": "3069060",
    "end": "3076350"
  },
  {
    "text": "about that data creation process right they're ingesting the raw data a lot of times the market data coming from",
    "start": "3076350",
    "end": "3082230"
  },
  {
    "text": "different firms cause different values different things so they're storing all their raw data the crane the curated",
    "start": "3082230",
    "end": "3087960"
  },
  {
    "text": "data sets those curated data sets are really driving the scientists machine learning algorithms and driving those",
    "start": "3087960",
    "end": "3093710"
  },
  {
    "text": "you know both EMR jobs as well as the data that's getting provide the data bricks so you know if we actually take",
    "start": "3093710",
    "end": "3102210"
  },
  {
    "text": "another view of this this is another depiction down at the bottom we have the real-time flow coming in being able to",
    "start": "3102210",
    "end": "3109860"
  },
  {
    "text": "run the analytics on the real-time processing storing that data in s3 as",
    "start": "3109860",
    "end": "3114870"
  },
  {
    "text": "that historic or that that data posture over time and then having the different tools to be able to provide that",
    "start": "3114870",
    "end": "3120980"
  },
  {
    "text": "interactive and batch processing on top of it so you know last few kind of notes",
    "start": "3120980",
    "end": "3127320"
  },
  {
    "text": "as core tenants and core takeaways here the first is really data lakes and data",
    "start": "3127320",
    "end": "3133500"
  },
  {
    "text": "warehouses are complimentary you might have a case where you you want to use",
    "start": "3133500",
    "end": "3138600"
  },
  {
    "text": "both depending on what use case you have also you could be very very performant",
    "start": "3138600",
    "end": "3143880"
  },
  {
    "text": "and still has a loose coupling of storage and compute this is one of the things where you know in my past life",
    "start": "3143880",
    "end": "3149520"
  },
  {
    "text": "when I was building on premises Hadoop clusters out you know going into it I'd be like aren't you gonna lose",
    "start": "3149520",
    "end": "3155070"
  },
  {
    "text": "performance and you know the way you store your data is going too far supersede that try it out optimized for",
    "start": "3155070",
    "end": "3160860"
  },
  {
    "text": "the queries and you'll get quite good performance out of that we do have tools",
    "start": "3160860",
    "end": "3166440"
  },
  {
    "text": "to be able to cache data on your cluster there's something called s3 disk copy I'd say less than 1% of the customers",
    "start": "3166440",
    "end": "3171690"
  },
  {
    "text": "after they optimize we'd have to do something like that there are ways of doing it just for that those few",
    "start": "3171690",
    "end": "3177690"
  },
  {
    "text": "outliers choosing the best job for best tool for the excuse me best tool for the",
    "start": "3177690",
    "end": "3184290"
  },
  {
    "text": "best job so you're not picking one tool for ingestion one tool for analytics",
    "start": "3184290",
    "end": "3189530"
  },
  {
    "text": "what we find is the types of analytics you run today it's going to be very different from the types of analytics",
    "start": "3189530",
    "end": "3195270"
  },
  {
    "text": "you might run three years from now which is very different than what Analects existed five years ago so you want a",
    "start": "3195270",
    "end": "3201750"
  },
  {
    "text": "platform and you want the Ecosystem people evolved as your your business questions change as well as the",
    "start": "3201750",
    "end": "3207600"
  },
  {
    "text": "tooling changes under the covers and you know capacity management you know in big",
    "start": "3207600",
    "end": "3213060"
  },
  {
    "text": "data and in data lakes it's a really really prime example of how the cloud could help with capacity management",
    "start": "3213060",
    "end": "3219300"
  },
  {
    "text": "because it's all consumption based if you architected correctly s3 is all consumption based when you",
    "start": "3219300",
    "end": "3224550"
  },
  {
    "text": "spin up your compute you're spinning it based on your compute needs it's a really really good example of that and",
    "start": "3224550",
    "end": "3230820"
  },
  {
    "text": "then the kind of last key takeaways and I'll stick around here for for questions and comments but picking the right tool",
    "start": "3230820",
    "end": "3236760"
  },
  {
    "text": "for the right data structure for what you want to do you know is it a data set",
    "start": "3236760",
    "end": "3242040"
  },
  {
    "text": "that's constantly changing you know for example even in the Korea to do sets you know Parque is really good at some",
    "start": "3242040",
    "end": "3247290"
  },
  {
    "text": "things but maybe you want to use Avro or C or another format so all your data",
    "start": "3247290",
    "end": "3253740"
  },
  {
    "text": "doesn't have to be in your same format in your data like so you know think",
    "start": "3253740",
    "end": "3258900"
  },
  {
    "text": "about as you're looking at your data what are the characteristics of the data how much is it changing what's the",
    "start": "3258900",
    "end": "3264750"
  },
  {
    "text": "volumes that sort of thing as you're looking at the data structure if I was working with a customer and they had like CSV data and JSON data which are",
    "start": "3264750",
    "end": "3271650"
  },
  {
    "text": "query wise quite poor performing because they're very sequential but if they're",
    "start": "3271650",
    "end": "3277950"
  },
  {
    "text": "not dealing with large data sets they're querying it once and then taking the data and sending us somewhere they may",
    "start": "3277950",
    "end": "3283050"
  },
  {
    "text": "not want to go through the process of optimizing it into Park AOR C so just take a look at what the unique aspects",
    "start": "3283050",
    "end": "3290610"
  },
  {
    "text": "of that are and then you know really the last thing I would say is you know",
    "start": "3290610",
    "end": "3296040"
  },
  {
    "text": "looking at your system if you're costing it out if there's a really giant cost",
    "start": "3296040",
    "end": "3301200"
  },
  {
    "text": "for a lot of these systems it's usually something that you want to engage the",
    "start": "3301200",
    "end": "3306210"
  },
  {
    "text": "essays some of our team to look at your architecture you know oftentimes we price our services to be optimized for",
    "start": "3306210",
    "end": "3312780"
  },
  {
    "text": "how you want to use them so if you try to take streaming data and store s3 where each record is individual object",
    "start": "3312780",
    "end": "3320070"
  },
  {
    "text": "in s3 it's not going to perform well because you're gonna actually cost it out on your put costs for s3 are going",
    "start": "3320070",
    "end": "3325260"
  },
  {
    "text": "to be giant instead you want to bash that data up have a single put or small number of puts out there and that you",
    "start": "3325260",
    "end": "3332280"
  },
  {
    "text": "know that costing exercise is a really good validation on what the services are meant to do that sort of thing and",
    "start": "3332280",
    "end": "3337650"
  },
  {
    "text": "definitely engage your essays as you're as you're going through that exercise so you know definitely want to thank",
    "start": "3337650",
    "end": "3343710"
  },
  {
    "text": "everyone for attending um you know I'll stick around up here for four questions you know and thank you again",
    "start": "3343710",
    "end": "3351920"
  },
  {
    "text": "[Applause]",
    "start": "3351920",
    "end": "3358869"
  }
]