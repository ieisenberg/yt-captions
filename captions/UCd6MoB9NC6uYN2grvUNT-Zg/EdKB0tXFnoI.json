[
  {
    "start": "0",
    "end": "22000"
  },
  {
    "text": "(upbeat music)",
    "start": "788",
    "end": "3455"
  },
  {
    "text": "- [Instructor] You can use\nthe AWS Schema Conversion Tool",
    "start": "7740",
    "end": "10440"
  },
  {
    "text": "to convert BigQuery database objects",
    "start": "10440",
    "end": "12660"
  },
  {
    "text": "to a format compatible\nwith Amazon Redshift.",
    "start": "12660",
    "end": "16170"
  },
  {
    "text": "Then you can apply the converted code",
    "start": "16170",
    "end": "17850"
  },
  {
    "text": "to your target database and migrate data.",
    "start": "17850",
    "end": "20880"
  },
  {
    "text": "This video shows how to get started",
    "start": "20880",
    "end": "22710"
  },
  {
    "start": "22000",
    "end": "166000"
  },
  {
    "text": "with BigQuery to Amazon\nRedshift migrations",
    "start": "22710",
    "end": "25710"
  },
  {
    "text": "and covers the key steps of this process.",
    "start": "25710",
    "end": "28707"
  },
  {
    "text": "AWS SCT uses a service account to connect",
    "start": "28708",
    "end": "31620"
  },
  {
    "text": "to your BigQuery data warehouse.",
    "start": "31620",
    "end": "33900"
  },
  {
    "text": "You can create a service account",
    "start": "33900",
    "end": "35490"
  },
  {
    "text": "in the Google Cloud management console,",
    "start": "35490",
    "end": "37729"
  },
  {
    "text": "but first make sure that you\nturned on the BigQuery API,",
    "start": "37729",
    "end": "41547"
  },
  {
    "text": "then create a service account.",
    "start": "41547",
    "end": "43983"
  },
  {
    "text": "(upbeat music)",
    "start": "44888",
    "end": "47555"
  },
  {
    "text": "And grant the following privileges.",
    "start": "55153",
    "end": "58440"
  },
  {
    "text": "AWS SCT uses the BigQuery admin role",
    "start": "58440",
    "end": "61800"
  },
  {
    "text": "to load your BigQuery metadata\nin the migration project.",
    "start": "61800",
    "end": "65309"
  },
  {
    "text": "To extract data from a\nBigQuery data warehouse,",
    "start": "65310",
    "end": "68189"
  },
  {
    "text": "AWS SCT uses the Google\nCloud storage bucket.",
    "start": "68190",
    "end": "72300"
  },
  {
    "text": "To access this bucket,",
    "start": "72300",
    "end": "73680"
  },
  {
    "text": "you grant the storage admin\nrole to your service account.",
    "start": "73680",
    "end": "77149"
  },
  {
    "text": "(upbeat music)",
    "start": "77149",
    "end": "79732"
  },
  {
    "text": "Then you create a new JSON key",
    "start": "84390",
    "end": "86100"
  },
  {
    "text": "and use it in AWS SCT\nto connect to BigQuery.",
    "start": "86100",
    "end": "90700"
  },
  {
    "text": "(upbeat music)",
    "start": "90700",
    "end": "93367"
  },
  {
    "text": "Check your downloads folder",
    "start": "95910",
    "end": "97320"
  },
  {
    "text": "to find a service account key file.",
    "start": "97320",
    "end": "99303"
  },
  {
    "text": "Before we launch AWS SCT,",
    "start": "100920",
    "end": "103619"
  },
  {
    "text": "let us check our source\nand target databases.",
    "start": "103620",
    "end": "106500"
  },
  {
    "text": "To demonstrate data migration,",
    "start": "106500",
    "end": "108450"
  },
  {
    "text": "we can use the line order table.",
    "start": "108450",
    "end": "110670"
  },
  {
    "text": "This table includes some\ndata in the source database",
    "start": "110670",
    "end": "113619"
  },
  {
    "text": "and it doesn't exist\nin the target database.",
    "start": "114780",
    "end": "117153"
  },
  {
    "text": "We can use AWS SCT to create it.",
    "start": "118770",
    "end": "121979"
  },
  {
    "text": "Now we start at AWS SCT\nand create a new project.",
    "start": "121980",
    "end": "126093"
  },
  {
    "text": "To connect to BigQuery,",
    "start": "128760",
    "end": "130259"
  },
  {
    "text": "we use the service account\nkey file that we created",
    "start": "130260",
    "end": "132750"
  },
  {
    "text": "in the Google Cloud management console.",
    "start": "132750",
    "end": "134973"
  },
  {
    "text": "(upbeat music)",
    "start": "135985",
    "end": "138568"
  },
  {
    "text": "Then we use database credentials",
    "start": "142230",
    "end": "144000"
  },
  {
    "text": "from AWS Secrets Manager to\nconnect to Amazon Redshift.",
    "start": "144000",
    "end": "148011"
  },
  {
    "text": "(upbeat music)",
    "start": "148011",
    "end": "150678"
  },
  {
    "text": "You can add multiple source",
    "start": "153690",
    "end": "155100"
  },
  {
    "text": "and target databases in a single project.",
    "start": "155100",
    "end": "157653"
  },
  {
    "text": "We create mapping rules to\ndescribe the source target pair",
    "start": "159990",
    "end": "163080"
  },
  {
    "text": "for our BigQuery to\nAmazon Redshift migration.",
    "start": "163080",
    "end": "166113"
  },
  {
    "start": "166000",
    "end": "358000"
  },
  {
    "text": "We switch to the main view and\nstart the schema conversion.",
    "start": "169711",
    "end": "173283"
  },
  {
    "text": "Our source data set includes\n33 tables, 278 views,",
    "start": "179040",
    "end": "183659"
  },
  {
    "text": "82 procedures, and 14 functions.",
    "start": "183660",
    "end": "186630"
  },
  {
    "text": "To assess the migration complexity,",
    "start": "186630",
    "end": "188820"
  },
  {
    "text": "we create an assessment\nreport for this data set.",
    "start": "188820",
    "end": "191493"
  },
  {
    "text": "You can see that AWS SCT converted most",
    "start": "197160",
    "end": "200160"
  },
  {
    "text": "of the database storage objects.",
    "start": "200160",
    "end": "202668"
  },
  {
    "text": "These objects include data sets,",
    "start": "202668",
    "end": "205350"
  },
  {
    "text": "tables, and materialized views.",
    "start": "205350",
    "end": "208200"
  },
  {
    "text": "Also, AWS SCT converted\nsuch database code objects",
    "start": "208200",
    "end": "212370"
  },
  {
    "text": "as views, SQL functions,\nJavaScript functions,",
    "start": "212370",
    "end": "215790"
  },
  {
    "text": "table functions, and stored procedures.",
    "start": "215790",
    "end": "218700"
  },
  {
    "text": "However, some of the database objects",
    "start": "218700",
    "end": "220950"
  },
  {
    "text": "require manual conversion.",
    "start": "220950",
    "end": "223020"
  },
  {
    "text": "AWS SCT highlights these objects",
    "start": "223020",
    "end": "225586"
  },
  {
    "text": "in blue in the conversion\nstatistics diagram.",
    "start": "225586",
    "end": "229080"
  },
  {
    "text": "In the action items tab, you\ncan see the recommended actions",
    "start": "229080",
    "end": "232320"
  },
  {
    "text": "for each conversion issue.",
    "start": "232320",
    "end": "234480"
  },
  {
    "text": "For example,",
    "start": "234480",
    "end": "235409"
  },
  {
    "text": "AWS SCT converts the\nBigQuery stream data type",
    "start": "235410",
    "end": "239130"
  },
  {
    "text": "to the character varying\ndata type in Amazon Redshift.",
    "start": "239130",
    "end": "242670"
  },
  {
    "text": "Because this data type\nis a shorter length,",
    "start": "242670",
    "end": "245069"
  },
  {
    "text": "you need to make sure\nthat you avoid data loss",
    "start": "245070",
    "end": "247110"
  },
  {
    "text": "during migration.",
    "start": "247110",
    "end": "248700"
  },
  {
    "text": "AWS SCT has several limitations",
    "start": "248700",
    "end": "251400"
  },
  {
    "text": "when using BigQuery as\na source, for example,",
    "start": "251400",
    "end": "254730"
  },
  {
    "text": "AWS SCT can't convert sub\nqueries and analytic functions",
    "start": "254730",
    "end": "259229"
  },
  {
    "text": "as well as geography,\nstatistical aggregate,",
    "start": "259230",
    "end": "262109"
  },
  {
    "text": "or some of the string functions.",
    "start": "262110",
    "end": "264300"
  },
  {
    "text": "Find the full list",
    "start": "264300",
    "end": "265169"
  },
  {
    "text": "of limitations in the AWS SCT user guide.",
    "start": "265170",
    "end": "268950"
  },
  {
    "text": "We plan to address these limitations",
    "start": "268950",
    "end": "270960"
  },
  {
    "text": "in the future releases of AWS SCT.",
    "start": "270960",
    "end": "274139"
  },
  {
    "text": "Despite these limitations,",
    "start": "274140",
    "end": "275730"
  },
  {
    "text": "you can use AWS SCT to\nautomatically convert more",
    "start": "275730",
    "end": "279240"
  },
  {
    "text": "than 80% of objects in\nyour BigQuery data sets.",
    "start": "279240",
    "end": "282750"
  },
  {
    "text": "Now let us convert the\ndata set that includes data",
    "start": "282750",
    "end": "285540"
  },
  {
    "text": "for migration.",
    "start": "285540",
    "end": "286830"
  },
  {
    "text": "Here's the table that we plan to migrate.",
    "start": "286830",
    "end": "289598"
  },
  {
    "text": "(upbeat music)",
    "start": "289598",
    "end": "292264"
  },
  {
    "text": "We convert the data set.",
    "start": "293430",
    "end": "294843"
  },
  {
    "text": "During conversion,",
    "start": "297360",
    "end": "298229"
  },
  {
    "text": "AWS SCT automatically generates\nthe assessment report.",
    "start": "298230",
    "end": "302583"
  },
  {
    "text": "You can see the converted\ncode of the line order table.",
    "start": "304320",
    "end": "307233"
  },
  {
    "text": "AWS SCT converted string columns",
    "start": "308105",
    "end": "310740"
  },
  {
    "text": "to character varying columns\nand added an action item.",
    "start": "310740",
    "end": "314490"
  },
  {
    "text": "Now we apply the converted\ncode to the target database.",
    "start": "314490",
    "end": "317973"
  },
  {
    "text": "To convert SQL statements and queries,",
    "start": "319980",
    "end": "322320"
  },
  {
    "text": "make sure that you include\nthem in the stored procedures",
    "start": "322320",
    "end": "324720"
  },
  {
    "text": "in your BigQuery project.",
    "start": "324720",
    "end": "326520"
  },
  {
    "text": "AWS SCT converts all\nBigQuery metadata objects.",
    "start": "326520",
    "end": "331169"
  },
  {
    "text": "For migrations from BigQuery,",
    "start": "331170",
    "end": "333180"
  },
  {
    "text": "you can't use AWS SCT to\nconvert external SQL scripts",
    "start": "333180",
    "end": "337289"
  },
  {
    "text": "or additional files.",
    "start": "337290",
    "end": "338820"
  },
  {
    "text": "Let us check our target database.",
    "start": "338820",
    "end": "340863"
  },
  {
    "text": "After we refresh the database,\nyou can see the new schema",
    "start": "342930",
    "end": "345930"
  },
  {
    "text": "that includes the line order table.",
    "start": "345930",
    "end": "347883"
  },
  {
    "text": "You can see that the line\norder table now exists",
    "start": "350970",
    "end": "353430"
  },
  {
    "text": "in the Amazon Redshift database.",
    "start": "353430",
    "end": "355949"
  },
  {
    "text": "It doesn't include any data.",
    "start": "355950",
    "end": "357603"
  },
  {
    "start": "358000",
    "end": "447000"
  },
  {
    "text": "AWS SCT uses data extraction\nagents to migrate data",
    "start": "362088",
    "end": "366840"
  },
  {
    "text": "from data warehouses to Amazon Redshift.",
    "start": "366840",
    "end": "369870"
  },
  {
    "text": "The zip file that you\ndownloaded to install AWS SCT",
    "start": "369870",
    "end": "373530"
  },
  {
    "text": "includes the extraction\nagent installer file.",
    "start": "373530",
    "end": "376680"
  },
  {
    "text": "Run the installer file",
    "start": "376680",
    "end": "378180"
  },
  {
    "text": "and then configure the\ndata extraction agent.",
    "start": "378180",
    "end": "380643"
  },
  {
    "text": "(upbeat music)",
    "start": "381685",
    "end": "384352"
  },
  {
    "text": "For a source vendor enter no,",
    "start": "391410",
    "end": "393300"
  },
  {
    "text": "because BigQuery doesn't\nrequire a JDBC driver.",
    "start": "393300",
    "end": "396840"
  },
  {
    "text": "For target vendor,",
    "start": "396840",
    "end": "398190"
  },
  {
    "text": "enter the path to your\nAmazon Redshift JDBC driver.",
    "start": "398190",
    "end": "401880"
  },
  {
    "text": "The data extraction agent\nstores temporary data",
    "start": "401880",
    "end": "404640"
  },
  {
    "text": "in the working folder.",
    "start": "404640",
    "end": "406320"
  },
  {
    "text": "You can use the project folder",
    "start": "406320",
    "end": "407850"
  },
  {
    "text": "as a working folder for\nthe data extraction agent.",
    "start": "407850",
    "end": "411450"
  },
  {
    "text": "After you configure the\ndata extraction agent,",
    "start": "411450",
    "end": "414030"
  },
  {
    "text": "you can review the file\nthat includes all settings.",
    "start": "414030",
    "end": "416703"
  },
  {
    "text": "Then switch back to AWS SCT,\nchoose the data migration view,",
    "start": "417690",
    "end": "422220"
  },
  {
    "text": "and register your agent.",
    "start": "422220",
    "end": "423723"
  },
  {
    "text": "Because our data\nextraction agent is running",
    "start": "424830",
    "end": "427050"
  },
  {
    "text": "on the same computer as AWS SCT,",
    "start": "427050",
    "end": "429930"
  },
  {
    "text": "we enter the local host IP\naddress as the host name.",
    "start": "429930",
    "end": "433113"
  },
  {
    "text": "(upbeat music)",
    "start": "434352",
    "end": "437789"
  },
  {
    "text": "You can see that AWS SCT can connect",
    "start": "437790",
    "end": "440460"
  },
  {
    "text": "to the data extraction agent.",
    "start": "440460",
    "end": "442852"
  },
  {
    "text": "This completes the\ninstallation and registration",
    "start": "442852",
    "end": "445830"
  },
  {
    "text": "of the data extraction agent.",
    "start": "445830",
    "end": "447573"
  },
  {
    "start": "447000",
    "end": "593000"
  },
  {
    "text": "To manage the migration of large tables,",
    "start": "450897",
    "end": "453870"
  },
  {
    "text": "you can create virtual\npartitions in AWS SCT.",
    "start": "453870",
    "end": "458220"
  },
  {
    "text": "Our source table is relatively small",
    "start": "458220",
    "end": "460260"
  },
  {
    "text": "and doesn't require partitions.",
    "start": "460260",
    "end": "462390"
  },
  {
    "text": "However, let us show you how\nto create virtual partitions.",
    "start": "462390",
    "end": "466170"
  },
  {
    "text": "We split the source table",
    "start": "466170",
    "end": "467370"
  },
  {
    "text": "depending on the values\nof the third column.",
    "start": "467370",
    "end": "470310"
  },
  {
    "text": "We use range partitioning",
    "start": "470310",
    "end": "472110"
  },
  {
    "text": "and create a separate\npartition for null values.",
    "start": "472110",
    "end": "474813"
  },
  {
    "text": "Now we create a local migration task.",
    "start": "479880",
    "end": "482850"
  },
  {
    "text": "This task uses the data\nextraction agent to migrate data.",
    "start": "482850",
    "end": "486453"
  },
  {
    "text": "For each task, you choose\nthe migration mode.",
    "start": "488370",
    "end": "491580"
  },
  {
    "text": "The available options\ninclude extract source data",
    "start": "491580",
    "end": "495060"
  },
  {
    "text": "and store it on a local\nPC where the agent runs,",
    "start": "495060",
    "end": "498360"
  },
  {
    "text": "extract data and uploaded\non an Amazon S3 bucket,",
    "start": "498360",
    "end": "502199"
  },
  {
    "text": "extract, upload, and copy\ndata from your S3 bucket",
    "start": "502200",
    "end": "505560"
  },
  {
    "text": "to Amazon Redshift.",
    "start": "505560",
    "end": "507389"
  },
  {
    "text": "We choose this option.",
    "start": "507390",
    "end": "508893"
  },
  {
    "text": "Also, we enter the Google\nCloud storage bucket",
    "start": "510600",
    "end": "513570"
  },
  {
    "text": "where the data extraction agent\ncan store the source data.",
    "start": "513570",
    "end": "517110"
  },
  {
    "text": "You can create this bucket",
    "start": "517110",
    "end": "518370"
  },
  {
    "text": "in the Google Cloud management console.",
    "start": "518370",
    "end": "520919"
  },
  {
    "text": "Finally, we set up the folder\nin our Amazon S3 bucket",
    "start": "520920",
    "end": "524790"
  },
  {
    "text": "where the data extraction\nagent can upload the data.",
    "start": "524790",
    "end": "527673"
  },
  {
    "text": "AWS SCT creates a data migration task",
    "start": "532092",
    "end": "535410"
  },
  {
    "text": "and a subtask for each virtual partition.",
    "start": "535410",
    "end": "538350"
  },
  {
    "text": "You can manage data\nmigration for each partition.",
    "start": "538350",
    "end": "541293"
  },
  {
    "text": "Now we start the data migration task.",
    "start": "542490",
    "end": "545313"
  },
  {
    "text": "The data extraction agent\nextracts data from BigQuery,",
    "start": "550350",
    "end": "553980"
  },
  {
    "text": "then the agent uploads data to Amazon S3",
    "start": "553980",
    "end": "556740"
  },
  {
    "text": "and launches the copy command\nto move it to Amazon Redshift.",
    "start": "556740",
    "end": "560102"
  },
  {
    "text": "AWS SCT migrated data",
    "start": "562670",
    "end": "564589"
  },
  {
    "text": "from the source BigQuery table\nto Amazon Redshift table.",
    "start": "564590",
    "end": "569010"
  },
  {
    "text": "Let us check this table\nin the target database.",
    "start": "569010",
    "end": "571683"
  },
  {
    "text": "You can see",
    "start": "573090",
    "end": "573923"
  },
  {
    "text": "that the data migration\ntask completed successfully",
    "start": "573923",
    "end": "576540"
  },
  {
    "text": "and the line order table includes data.",
    "start": "576540",
    "end": "578643"
  },
  {
    "text": "Now, you know how to use AWS SCT",
    "start": "579480",
    "end": "582510"
  },
  {
    "text": "to convert BigQuery\nprojects to Amazon Redshift",
    "start": "582510",
    "end": "585420"
  },
  {
    "text": "and migrate data.",
    "start": "585420",
    "end": "586980"
  },
  {
    "text": "Thanks for watching this video.",
    "start": "586980",
    "end": "588602"
  }
]