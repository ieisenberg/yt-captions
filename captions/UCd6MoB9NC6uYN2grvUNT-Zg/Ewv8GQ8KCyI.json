[
  {
    "text": "good evening and thank you for being here like you guys are warriors I",
    "start": "60",
    "end": "6859"
  },
  {
    "text": "wouldn't come even if I have to listen to myself but 7:00 p.m. so thank you for coming we really appreciate it and we",
    "start": "6859",
    "end": "13650"
  },
  {
    "text": "have something very exciting for you so this session is about how two of our customers have migrated their workloads",
    "start": "13650",
    "end": "20689"
  },
  {
    "text": "from running on Prem on to Amazon EMR",
    "start": "20689",
    "end": "26550"
  },
  {
    "text": "and the other one from running Hadoop on ec2 to EMR there are two very different use cases so we'll talk a little bit",
    "start": "26550",
    "end": "33809"
  },
  {
    "text": "about migration from on Prem or on ec2 to Amazon EMR the focus of the session",
    "start": "33809",
    "end": "40770"
  },
  {
    "text": "is going to be a lot more on the why instead of the how we will we will cover",
    "start": "40770",
    "end": "46050"
  },
  {
    "text": "the hub as well but these two customers are Airbnb and Guardian you kind of",
    "start": "46050",
    "end": "53610"
  },
  {
    "text": "heard about both of them in today's keynote as well and I and I kind of",
    "start": "53610",
    "end": "59129"
  },
  {
    "text": "looked at those two customers and they're very interesting use cases one of them Airbnb has migrated there",
    "start": "59129",
    "end": "65059"
  },
  {
    "text": "Margaret to EMR and they were already on the cloud they were using Hadoop on the cloud on ec2 machines so they have a",
    "start": "65060",
    "end": "72240"
  },
  {
    "text": "different story to tell and the other one which is Guardian they were actually",
    "start": "72240",
    "end": "78210"
  },
  {
    "text": "running Hadoop on Prem and they migrated to EMR as a part of their migration they're all in story on to the cloud so",
    "start": "78210",
    "end": "84990"
  },
  {
    "text": "the very very different stories to tell and we believe you'll take something back from both of these stories and what",
    "start": "84990",
    "end": "90390"
  },
  {
    "text": "are the experiences so we'll talk a little bit about more I'll set the context around when we look at customers",
    "start": "90390",
    "end": "97110"
  },
  {
    "text": "and when when they migrate up from on Prem or an ec2 to EMR what are the main",
    "start": "97110",
    "end": "102509"
  },
  {
    "text": "reasons customer migrate to such customers choose this migration and then we will look deeper into these stories",
    "start": "102509",
    "end": "109220"
  },
  {
    "text": "so the primary reason I think is how Hadoop deployments are hadoo / spark deployments today are very tightly",
    "start": "109220",
    "end": "116130"
  },
  {
    "text": "coupled the storage and compute for these deployments are very tightly coupled so when for example in an",
    "start": "116130",
    "end": "122430"
  },
  {
    "text": "on-premise world when you buy your server you also buy storage along with it and when your storage goes exponentially your service to your",
    "start": "122430",
    "end": "129030"
  },
  {
    "text": "servers also grow exponentially right so it's because your storage",
    "start": "129030",
    "end": "134340"
  },
  {
    "text": "servers are very tightly coupled together so as you get more data and",
    "start": "134340",
    "end": "139980"
  },
  {
    "text": "maybe with HDFS you have 3x replication your storage the earth starts to keep growing so we often hear or 100 petabyte",
    "start": "139980",
    "end": "147769"
  },
  {
    "text": "Hadoop clusters or 10 better byte Hadoop clusters but when you go and talk to people who are running those clusters",
    "start": "147769",
    "end": "154049"
  },
  {
    "text": "and you ask them what kind of jobs are you really running they would say oh we the hundred petabyte is there because",
    "start": "154049",
    "end": "160260"
  },
  {
    "text": "we're using HDFS all the data is co-located in one cluster but really the",
    "start": "160260",
    "end": "165900"
  },
  {
    "text": "jobs that run process one two or months of month of data the size of the cluster",
    "start": "165900",
    "end": "171540"
  },
  {
    "text": "is really because of the amount of storage and you have to keep buying compute because you know the storage",
    "start": "171540",
    "end": "178140"
  },
  {
    "text": "keeps growing compute requirements generally vary right so for example what",
    "start": "178140",
    "end": "183480"
  },
  {
    "text": "we see mostly in on from Hadoop deployment system is that you see a peak at the time when all environments are",
    "start": "183480",
    "end": "190620"
  },
  {
    "text": "all lines of businesses are running their jobs or you know maybe 12:00 in the night all jobs start their ETL",
    "start": "190620",
    "end": "197069"
  },
  {
    "text": "process and 9:00 in the morning there's a lot of ad hocs job and at that point of time you run out of you run out of",
    "start": "197069",
    "end": "202829"
  },
  {
    "text": "capacity running at peak but on the other times you're basically running a",
    "start": "202829",
    "end": "208260"
  },
  {
    "text": "low on capacity so under utilization or scarce resources so you see a very",
    "start": "208260",
    "end": "213840"
  },
  {
    "text": "common pattern when you look at utilization graphs from on-prem clusters",
    "start": "213840",
    "end": "218940"
  },
  {
    "text": "so you would see something like this which is a weekly peak you see a massive peak when you have to reprocess the data",
    "start": "218940",
    "end": "225180"
  },
  {
    "text": "and you see a steady state most of the time and this creates a lot of opportunities you compute essentially is",
    "start": "225180",
    "end": "232200"
  },
  {
    "text": "completely underutilized you also see contention for the same kind of",
    "start": "232200",
    "end": "237209"
  },
  {
    "text": "resources and especially because you know this Hadoop and spark and presto and there's lots of different kinds of",
    "start": "237209",
    "end": "243269"
  },
  {
    "text": "applications customers are running on a central Hadoop cluster the reason you are multi-tenant in all of these on a",
    "start": "243269",
    "end": "249569"
  },
  {
    "text": "single cluster is because the data is on that particular cluster so you have jobs",
    "start": "249569",
    "end": "254940"
  },
  {
    "text": "that are compute bound you have jobs that our memory Brown you have jobs from different departments and if you do also",
    "start": "254940",
    "end": "261000"
  },
  {
    "text": "have requirements where you have different departments wanting to use different versions of the same software",
    "start": "261000",
    "end": "266190"
  },
  {
    "text": "and that could cause a lot of contention if you do separate them out it tends to create data silos",
    "start": "266190",
    "end": "272510"
  },
  {
    "text": "so you now have to create either separate out the data or you create or recreate the data between each of these",
    "start": "272510",
    "end": "279650"
  },
  {
    "text": "clusters tends to create data silos which you really don't don't want in a longer period of time",
    "start": "279650",
    "end": "285310"
  },
  {
    "text": "replication also adds cost to the two HDFS replication is also cost and this",
    "start": "285310",
    "end": "291290"
  },
  {
    "text": "cost still doesn't give you high availability because all of these are actually sitting in a single data center",
    "start": "291290",
    "end": "297580"
  },
  {
    "text": "now let's look at a little bit from an application point of view when you look at an application for a view we see some",
    "start": "297580",
    "end": "304100"
  },
  {
    "text": "common patterns emerge for example there is large-scale transformation going on and that can be based on MapReduce hi",
    "start": "304100",
    "end": "311180"
  },
  {
    "text": "big spark so on so forth this interactive grade is going on specifically Impala Park sequel or",
    "start": "311180",
    "end": "318080"
  },
  {
    "text": "presto there's some machine learning going on there's some ad hoc usage of interactive notebooks for data science",
    "start": "318080",
    "end": "324169"
  },
  {
    "text": "workloads and there may be some no sequel based workloads if you look at all of these workflows they have very",
    "start": "324169",
    "end": "329750"
  },
  {
    "text": "different characteristics but they are all multi-talented on the same resource because your data is in the same",
    "start": "329750",
    "end": "335180"
  },
  {
    "text": "resource and your clusters are so big because your storage and compute are",
    "start": "335180",
    "end": "340340"
  },
  {
    "text": "essentially coupled together here's here's what we when we look at this this",
    "start": "340340",
    "end": "345860"
  },
  {
    "text": "is a real customer example where I asked them to build a swimlane of all the jobs",
    "start": "345860",
    "end": "351140"
  },
  {
    "text": "that they run daily and you know these these are essentially some jobs you'll see that there are some all the jobs",
    "start": "351140",
    "end": "357110"
  },
  {
    "text": "there are some eat large ETL jobs that star let's start at the midnight there are some jobs that keep running every",
    "start": "357110",
    "end": "362810"
  },
  {
    "text": "are there are some jobs that run occasionally and their jump jobs that are at hard so if you look at the first",
    "start": "362810",
    "end": "370010"
  },
  {
    "text": "circle that's the place where you your cluster is completely over utilized and if you look at the second circle you pay",
    "start": "370010",
    "end": "376250"
  },
  {
    "text": "you would see that the cluster is completely underutilized so what EMR tends to do it allows you to decouple",
    "start": "376250",
    "end": "383300"
  },
  {
    "text": "storage and compute and where we use our storages s3 because you want different",
    "start": "383300",
    "end": "388780"
  },
  {
    "text": "requirements from each one of them what we want to do is D couple your storage",
    "start": "388780",
    "end": "393979"
  },
  {
    "text": "and compute so that you can independently scale them and there are several reasons now why customers would",
    "start": "393979",
    "end": "400630"
  },
  {
    "text": "to do something like that so when you decoupled storage and compute the storage for a choice that we use is am",
    "start": "400630",
    "end": "406960"
  },
  {
    "text": "is s3 you get 11 lines of durability low-cost life cycle policies and now the",
    "start": "406960",
    "end": "414240"
  },
  {
    "text": "announcement that you saw machine learning based lifecycle call policies versioning and you have a layer on top",
    "start": "414240",
    "end": "421390"
  },
  {
    "text": "of EMR that allows you to read and write directly to s3 which we call EMR FS so",
    "start": "421390",
    "end": "427930"
  },
  {
    "text": "what are the benefits of decoupling storage and compute now if you think about it a 10 node cluster running for",
    "start": "427930",
    "end": "433960"
  },
  {
    "text": "10 hours costs you exactly the same as a hundred node cluster running for one hour so the benefit is that for you with",
    "start": "433960",
    "end": "441610"
  },
  {
    "text": "EMR if your data is sitting on s3 you can spin up a spark cluster you can process your data and then you can",
    "start": "441610",
    "end": "447340"
  },
  {
    "text": "switch off your cluster so if you look at this diagram that we started with a",
    "start": "447340",
    "end": "452710"
  },
  {
    "text": "lot of jobs in here start and stop at the same at the same time if I had data",
    "start": "452710",
    "end": "458350"
  },
  {
    "text": "that is sitting in s3 essentially the blue and the red lines can be individual clusters job scope clusters that means",
    "start": "458350",
    "end": "465580"
  },
  {
    "text": "those jha those clusters only run those jobs they can be spun up they can process the data and they you can just",
    "start": "465580",
    "end": "471730"
  },
  {
    "text": "spin down the cluster so what we see customers do a lot is as they take their",
    "start": "471730",
    "end": "478230"
  },
  {
    "text": "architecture they work from on the architecture from a job point of view and then D couple all of these are",
    "start": "478230",
    "end": "484990"
  },
  {
    "text": "deconstruct all of these into individual purpose coped clusters some of the jobs",
    "start": "484990",
    "end": "490720"
  },
  {
    "text": "which run for four to six do every hour that basically becomes your transient",
    "start": "490720",
    "end": "495820"
  },
  {
    "text": "clusters that means you can spin up a cluster and spin them down the jobs that run on a persistent basis become your",
    "start": "495820",
    "end": "501700"
  },
  {
    "text": "persistent cluster that means you spin up a cluster and you don't have to shut him down also because you are your data",
    "start": "501700",
    "end": "509620"
  },
  {
    "text": "is sitting on s3 you can also auto scale your clusters and save cost that means you can scale up your cluster for",
    "start": "509620",
    "end": "516280"
  },
  {
    "text": "example a very good use case of a persistent cluster is when you have lots of users using maybe a Jupiter notebook",
    "start": "516280",
    "end": "522760"
  },
  {
    "text": "with it so EMR provides like a native Jupiter interface so you can have multi tenant 'add a single cluster multi",
    "start": "522760",
    "end": "529240"
  },
  {
    "text": "tented where multiple users all running Jupiter interfaces or Jupiter notebook and when they all submit jobs the",
    "start": "529240",
    "end": "535600"
  },
  {
    "text": "cluster can scale up and scale down based upon the capacity requirements that each of these jobs might have the",
    "start": "535600",
    "end": "543730"
  },
  {
    "text": "benefit number three is you can logically separate these jobs for example if you remember the swim lane",
    "start": "543730",
    "end": "549340"
  },
  {
    "text": "that was there I can essentially upgrade each of these jobs to a different version because I'm running them all on",
    "start": "549340",
    "end": "555820"
  },
  {
    "text": "different clusters also there is a low blast radius of failure for example if",
    "start": "555820",
    "end": "561160"
  },
  {
    "text": "one of these jobs fail it is not causing all of my jobs to fail in to fail on top",
    "start": "561160",
    "end": "566860"
  },
  {
    "text": "of three also you don't have to manage complex queues in this segment because",
    "start": "566860",
    "end": "571930"
  },
  {
    "text": "you're building purpose cope clusters and you have disaster recovery built in",
    "start": "571930",
    "end": "577240"
  },
  {
    "text": "because you know s3 allows you to s3 replicated data across multiple availability zones so if in fact you",
    "start": "577240",
    "end": "584350"
  },
  {
    "text": "have of one availability zone go down you can essentially spin up the cluster and a completely different availability",
    "start": "584350",
    "end": "589990"
  },
  {
    "text": "zone you don't need disaster recovery you can also build regional disaster recovery by cross replicating cross",
    "start": "589990",
    "end": "598240"
  },
  {
    "text": "replicating your data into another storage so these are some of the advantages of decoupling your storage",
    "start": "598240",
    "end": "604180"
  },
  {
    "text": "and compute and I think that is the most important reason that customers migrate",
    "start": "604180",
    "end": "609730"
  },
  {
    "text": "from their ec2 environments or on EMR environments asari on-prem environments",
    "start": "609730",
    "end": "615250"
  },
  {
    "text": "to Yemen so that so to tell you the story let me first invite Jian Chen who's a software engineer at Airbnb to",
    "start": "615250",
    "end": "623080"
  },
  {
    "text": "talk about how Airbnb is migrating from running Hadoop on ec2 to Amazon EMR",
    "start": "623080",
    "end": "628600"
  },
  {
    "text": "industry",
    "start": "628600",
    "end": "631050"
  },
  {
    "text": "is the autumn hello nice yeah hi everyone i'm jin i'm",
    "start": "645379",
    "end": "655700"
  },
  {
    "text": "a software Chindia at a B&B unfortunately Guang couldn't make it today because of a scheduling conflict",
    "start": "655700",
    "end": "661400"
  },
  {
    "text": "and we're gonna share you our story of migrating from ec2 to EMR and s3 I'll",
    "start": "661400",
    "end": "670880"
  },
  {
    "text": "briefly talk about our existing existing data infrastructure Airbnb and the kind",
    "start": "670880",
    "end": "675980"
  },
  {
    "text": "of challenge we are facing this day and why choosing EMR and s3 is going to be",
    "start": "675980",
    "end": "682190"
  },
  {
    "text": "good for us down the road and we I conclude with a migration lesson we have learned so far Airbnb is a data driven",
    "start": "682190",
    "end": "692240"
  },
  {
    "text": "company we provide personalized recommendations and better search results for our guests and as a host you",
    "start": "692240",
    "end": "699980"
  },
  {
    "text": "can use a smart pricing feature to increase your booking rate and your",
    "start": "699980",
    "end": "705980"
  },
  {
    "text": "total revenue internally the data platform team powers tons of company",
    "start": "705980",
    "end": "711140"
  },
  {
    "text": "makes and supports a generation of important financial reports and there",
    "start": "711140",
    "end": "716900"
  },
  {
    "text": "are a lot of more use cases that can talk about this is a snapshot of what",
    "start": "716900",
    "end": "722990"
  },
  {
    "text": "data infrastructure looks like in 2014 we have two major incoming data sources",
    "start": "722990",
    "end": "729140"
  },
  {
    "text": "event logs and Mexico dumps event logs get generally from web servers and",
    "start": "729140",
    "end": "735530"
  },
  {
    "text": "different kind of online services these events get published to Kafka and bash",
    "start": "735530",
    "end": "741470"
  },
  {
    "text": "ingest it into our data warehouse at the bottom you see the Mexico dumps there",
    "start": "741470",
    "end": "747080"
  },
  {
    "text": "was a basically daily snapshot of bicycles databases and those get ingested into the data warehouse as well",
    "start": "747080",
    "end": "753770"
  },
  {
    "text": "and we have two main data warehouse clusters go in and silver they're",
    "start": "753770",
    "end": "758960"
  },
  {
    "text": "sitting in different Easy's we use re-air to visit another in-house project",
    "start": "758960",
    "end": "764240"
  },
  {
    "text": "to copy the critical data from gold to silver to provide isolation and also",
    "start": "764240",
    "end": "769580"
  },
  {
    "text": "disaster recovery users they can schedule a hive or spark job using air",
    "start": "769580",
    "end": "775610"
  },
  {
    "text": "flow on top of platform by just simply pointing the configuration to either go",
    "start": "775610",
    "end": "780650"
  },
  {
    "text": "or so if the jobs higher survey the kids running on go if it's lower priority to",
    "start": "780650",
    "end": "786379"
  },
  {
    "text": "get choosers silver this model serve us pretty well for one year and then we started to",
    "start": "786379",
    "end": "794870"
  },
  {
    "text": "support more use cases adding HBase and presto into the picture",
    "start": "794870",
    "end": "800410"
  },
  {
    "text": "and we also create a ball compute only cluster to serve streaming use cases we",
    "start": "800410",
    "end": "807529"
  },
  {
    "text": "are seeing a trend within the company that more teams are migrating their hive hive jobs into spark jobs simply for",
    "start": "807529",
    "end": "815959"
  },
  {
    "text": "better performance and easier testability and they also have more streaming use",
    "start": "815959",
    "end": "822980"
  },
  {
    "text": "cases I think all the streaming use case",
    "start": "822980",
    "end": "828139"
  },
  {
    "text": "I really Envy are one using fling or via air flow air stream a stream which is",
    "start": "828139",
    "end": "833990"
  },
  {
    "text": "another in-house streaming framework on top of spark streaming and around this",
    "start": "833990",
    "end": "840589"
  },
  {
    "text": "time I think we started seeing scalability issue with HDFS that's why we began archiving co data onto s3",
    "start": "840589",
    "end": "849130"
  },
  {
    "text": "periodically and that's the trending",
    "start": "849130",
    "end": "855589"
  },
  {
    "text": "we're seeing what spark were low and streaming workloads as we mentioned earlier yeah so Airbnb is growing very",
    "start": "855589",
    "end": "863209"
  },
  {
    "text": "fast our business growing our employees double year-over-year so there's a",
    "start": "863209",
    "end": "868790"
  },
  {
    "text": "number of jobs running on clusters and going fast it's great but some issues",
    "start": "868790",
    "end": "875000"
  },
  {
    "text": "started to buy us our computing storage",
    "start": "875000",
    "end": "880160"
  },
  {
    "text": "are tightly coupled as you can see from the graph the top one is the compute usage throughout the day time you can",
    "start": "880160",
    "end": "888920"
  },
  {
    "text": "see if fluctuates versus the bottom one the storage is pre flat around 40% at a",
    "start": "888920",
    "end": "895130"
  },
  {
    "text": "peak different teams are basically competing resource and people come to us",
    "start": "895130",
    "end": "901189"
  },
  {
    "text": "and complaining my jobs are getting run is the impending stage of my job getting preempted for some higher jobs but a lot",
    "start": "901189",
    "end": "909319"
  },
  {
    "text": "of time all the computers was a wasted is not being used",
    "start": "909319",
    "end": "914709"
  },
  {
    "text": "silver is a bigger cluster right now it has more than 1000 T to a X large",
    "start": "917120",
    "end": "924240"
  },
  {
    "text": "instances and we have seen a couple times where yarn does not schedule jobs",
    "start": "924240",
    "end": "929910"
  },
  {
    "text": "even though there are still available resources in a cluster we still haven't figure out exactly what",
    "start": "929910",
    "end": "935460"
  },
  {
    "text": "the root cause is but failing over yarn resource managers we saw the problem basically but on the other hand HDFS we",
    "start": "935460",
    "end": "943500"
  },
  {
    "text": "reach 150 million plus data blocks and that gives us serious problem is the",
    "start": "943500",
    "end": "950790"
  },
  {
    "text": "long GC basically killed an angel and started having frequent fell over and the fell over and also when we try Roman",
    "start": "950790",
    "end": "958589"
  },
  {
    "text": "we started classes it took a long time we consider all these as major outages - on our path Data Platform",
    "start": "958589",
    "end": "964500"
  },
  {
    "text": "that's not good and this and a picture showing you the lack of elasticity",
    "start": "964500",
    "end": "971210"
  },
  {
    "text": "wasting resources versus a contention at peak also we also running on some pretty",
    "start": "971210",
    "end": "978690"
  },
  {
    "text": "old Hadoop software 2.5 and also hive 0.13 there are a lot of legacy jobs",
    "start": "978690",
    "end": "985830"
  },
  {
    "text": "relying on these set of software settings that's why as part of us - just",
    "start": "985830",
    "end": "991380"
  },
  {
    "text": "to us simply upgrade a whole cluster with thousands no on it since we're",
    "start": "991380",
    "end": "999570"
  },
  {
    "text": "provision the closet for peak load the capital expense is pretty high and if",
    "start": "999570",
    "end": "1006260"
  },
  {
    "text": "you have maintained a cluster on ec2 with - before you know it actually the",
    "start": "1006260",
    "end": "1011600"
  },
  {
    "text": "operational overhead is quite high as well to add and removing instance to do it gracefully at ease is very hard and",
    "start": "1011600",
    "end": "1019959"
  },
  {
    "text": "it's very hard for us to allocate a cost across the entire organization in this",
    "start": "1019959",
    "end": "1025610"
  },
  {
    "text": "multi kinetic cluster setting so why",
    "start": "1025610",
    "end": "1030860"
  },
  {
    "text": "choosing Amazon s3 and EMR as a - I keep",
    "start": "1030860",
    "end": "1036110"
  },
  {
    "text": "mentioning earlier is to decouple computing storage we like the idea of having s3 as the data lake and launching",
    "start": "1036110",
    "end": "1045920"
  },
  {
    "text": "still has computer infrastructure using yeah more so that we believe will reduce",
    "start": "1045920",
    "end": "1051299"
  },
  {
    "text": "our operational overhead quite a bit it's gonna be easier for us rotate scale in SQL out the cluster for different",
    "start": "1051299",
    "end": "1057960"
  },
  {
    "text": "teams and if we create different EMR classify each business you'd it is",
    "start": "1057960",
    "end": "1063870"
  },
  {
    "text": "easier for us to attribute a cause and isolate compute resources for different",
    "start": "1063870",
    "end": "1069780"
  },
  {
    "text": "team and they can even run their own customized software if they want to they don't just start with Hadoop to15 or",
    "start": "1069780",
    "end": "1077130"
  },
  {
    "text": "hide 0.13 so that sounds great how do we",
    "start": "1077130",
    "end": "1083130"
  },
  {
    "text": "get there so today we basically restrict the user to only write to HDFS and as",
    "start": "1083130",
    "end": "1089070"
  },
  {
    "text": "soon as the right is finished we we basically upload those data to s3 we",
    "start": "1089070",
    "end": "1095070"
  },
  {
    "text": "right now the goals will be to cut off the repass or HDFS and then we selected",
    "start": "1095070",
    "end": "1102809"
  },
  {
    "text": "some non-critical jobs as a pal use case on EMR and these jobs running on some TV",
    "start": "1102809",
    "end": "1111419"
  },
  {
    "text": "static long-running cluster on EMR we try to mimic the same kind of setting on",
    "start": "1111419",
    "end": "1117000"
  },
  {
    "text": "our existing structure where you have a gateway launching a job to a group of",
    "start": "1117000",
    "end": "1122850"
  },
  {
    "text": "machines in a cluster setting which are the behavior using the same software",
    "start": "1122850",
    "end": "1128160"
  },
  {
    "text": "version and anything try to reduce the problem we face and then tune the system",
    "start": "1128160",
    "end": "1133320"
  },
  {
    "text": "from that point on so we have for long-running production cluster running and they they all have a auto scaling",
    "start": "1133320",
    "end": "1141390"
  },
  {
    "text": "enable and those job seems to run three times faster than before so that's",
    "start": "1141390",
    "end": "1146610"
  },
  {
    "text": "pretty good news our next step is to migrate all the hive application without",
    "start": "1146610",
    "end": "1154169"
  },
  {
    "text": "impacting the user too much the goal is the ideal situation has users of those",
    "start": "1154169",
    "end": "1160350"
  },
  {
    "text": "owner of those jobs shouldn't need to worry about where the job is running whereas EMR or within existing cluster",
    "start": "1160350",
    "end": "1168000"
  },
  {
    "text": "should have a zero effort on there and which means there'll be a huge effort on our side and I will start introducing",
    "start": "1168000",
    "end": "1176610"
  },
  {
    "text": "the EMR jobs over to manage long running and transient cluster in the",
    "start": "1176610",
    "end": "1183350"
  },
  {
    "text": "this is what 2019 looks like or hopefully this is what 2019 will look like you have a earmark job server in",
    "start": "1183710",
    "end": "1190800"
  },
  {
    "text": "the front to manage all the staging or streaming and and different kind of",
    "start": "1190800",
    "end": "1199080"
  },
  {
    "text": "cluster launched by a different team and storage is all backed by s3 at this",
    "start": "1199080",
    "end": "1204690"
  },
  {
    "text": "point so we have a clear separation of a compute and storage what have we learned",
    "start": "1204690",
    "end": "1213720"
  },
  {
    "text": "so far well number one get it working first our first EMR cluster had the",
    "start": "1213720",
    "end": "1221940"
  },
  {
    "text": "exact same setting as our existing infrastructure by exact setting I mean",
    "start": "1221940",
    "end": "1229130"
  },
  {
    "text": "there's a gateway there's a set of gateway summoning job to certain machine",
    "start": "1229130",
    "end": "1234750"
  },
  {
    "text": "within their clusters in this case the compute the yarn no managers running",
    "start": "1234750",
    "end": "1239850"
  },
  {
    "text": "inside mark the users still writing to existing HDFS but they don't need to",
    "start": "1239850",
    "end": "1246930"
  },
  {
    "text": "worry about what's behind a scene at this point we still pointing the same high meta story and then we started the",
    "start": "1246930",
    "end": "1254850"
  },
  {
    "text": "EMR cluster with simply just one time machine we don't want to introduce too many variables at one at this point we",
    "start": "1254850",
    "end": "1261690"
  },
  {
    "text": "can tune a machine for the application later but just for trying out purpose and migration purpose it's just one type",
    "start": "1261690",
    "end": "1268260"
  },
  {
    "text": "of r4p else ax is going up for us and picking the representative use cases as",
    "start": "1268260",
    "end": "1275610"
  },
  {
    "text": "I mentioned earlier even though we'll pick non-critical jobs those jobs the pattern of those child represents",
    "start": "1275610",
    "end": "1281430"
  },
  {
    "text": "pretty much 80% of what a main usage on our cluster looks like so the learning",
    "start": "1281430",
    "end": "1287070"
  },
  {
    "text": "from migrating those job can apply to other teams hopefully easy easily and",
    "start": "1287070",
    "end": "1293730"
  },
  {
    "text": "the rest of 20% we will Margaret them at a later time of the day default",
    "start": "1293730",
    "end": "1301770"
  },
  {
    "text": "configuration may not work well so we have a tune some config on yarn for just",
    "start": "1301770",
    "end": "1310320"
  },
  {
    "text": "to get the first version EMR cluster running and what one of the example is we had to",
    "start": "1310320",
    "end": "1318029"
  },
  {
    "text": "increase the memory for no managers on EMR just because it crashed",
    "start": "1318029",
    "end": "1323370"
  },
  {
    "text": "quite frequently and there many more and",
    "start": "1323370",
    "end": "1329760"
  },
  {
    "text": "then lastly finales although Spelling's awesome will like",
    "start": "1329760",
    "end": "1335549"
  },
  {
    "text": "the idea of having the cluster scale by itself without human intervention I think that'd save us a lot of",
    "start": "1335549",
    "end": "1341789"
  },
  {
    "text": "operational overhead I think that's it",
    "start": "1341789",
    "end": "1347059"
  },
  {
    "text": "for me today and we'll bang thank you",
    "start": "1347059",
    "end": "1359429"
  },
  {
    "text": "Jane hello everyone thanks for being here tonight on Wang Chung my name is easy",
    "start": "1359429",
    "end": "1367679"
  },
  {
    "text": "remember because that same name as that cheesy 80 song everybody Wang Chung tonight I'm sorry I just had to get that",
    "start": "1367679",
    "end": "1373860"
  },
  {
    "text": "out there every time I might present just in case you were thinking about that now back to the program I lead the",
    "start": "1373860",
    "end": "1381090"
  },
  {
    "text": "data platform architecture team at Guardian the team drives the data management strategy for the company and",
    "start": "1381090",
    "end": "1388260"
  },
  {
    "text": "also managed the Guardian Galilei which serves as a source for reporting and analytics today I'll be talking about a",
    "start": "1388260",
    "end": "1395100"
  },
  {
    "text": "topic that some of you are probably thinking about or in the process of going through a very similar journey which is migrating your Hadoop data Lake",
    "start": "1395100",
    "end": "1403620"
  },
  {
    "text": "and workload to AWS EMR first a little",
    "start": "1403620",
    "end": "1409590"
  },
  {
    "text": "bit about Guardian who we are early this morning at the keynote our CIO Adina",
    "start": "1409590",
    "end": "1416190"
  },
  {
    "text": "Becca on stage we're talking about the many exciting things that was happening at Guardian I'll be touching on some of",
    "start": "1416190",
    "end": "1422640"
  },
  {
    "text": "those transformation changes in a way he really stole my thunder but I think it's",
    "start": "1422640",
    "end": "1429090"
  },
  {
    "text": "okay the company is over 150 years old so we've been around for quite some time",
    "start": "1429090",
    "end": "1434490"
  },
  {
    "text": "we have employed employees over 9,000 or 2,700 financial representative and more",
    "start": "1434490",
    "end": "1441240"
  },
  {
    "text": "than 55 agency some of you in the audience may be a guardian policyholder and I want to thank you",
    "start": "1441240",
    "end": "1447720"
  },
  {
    "text": "being a loyal customer because we're in the business of providing products in annuities investment life insurance",
    "start": "1447720",
    "end": "1456029"
  },
  {
    "text": "dental employee benefits disability and",
    "start": "1456029",
    "end": "1461330"
  },
  {
    "text": "just a history lesson a mere three to four years ago there was no concept of",
    "start": "1462679",
    "end": "1468629"
  },
  {
    "text": "enterprise data in the organization each of the business operated in various silo",
    "start": "1468629",
    "end": "1474059"
  },
  {
    "text": "fashions they had their own databases data marts in some cases and even data",
    "start": "1474059",
    "end": "1479639"
  },
  {
    "text": "warehouses there was no cross-pollination of data across the enterprise makes you wonder how we",
    "start": "1479639",
    "end": "1486210"
  },
  {
    "text": "survived for over 150 years but to our credit were very good at what we do",
    "start": "1486210",
    "end": "1491639"
  },
  {
    "text": "but our executive leadership team recognized that we cannot continue operation this way especially if we want",
    "start": "1491639",
    "end": "1497909"
  },
  {
    "text": "to be around for the next 150 years so",
    "start": "1497909",
    "end": "1503789"
  },
  {
    "text": "in 2015 is when they made an investment not only in a big data program but also",
    "start": "1503789",
    "end": "1509009"
  },
  {
    "text": "in modernizing our technology stack introducing concepts such as DevOps",
    "start": "1509009",
    "end": "1515210"
  },
  {
    "text": "agile moving us to the cloud and most importantly fostering a spirit of",
    "start": "1515210",
    "end": "1520919"
  },
  {
    "text": "innovation we bring our vendors in conduct shark tank exercise voting",
    "start": "1520919",
    "end": "1527159"
  },
  {
    "text": "thumbs up thumbs down on new ideas to enhance the business we have employee",
    "start": "1527159",
    "end": "1532559"
  },
  {
    "text": "engagement on innovation challenges I've been very lucky enough to be part of the journey and and from the very beginning",
    "start": "1532559",
    "end": "1539940"
  },
  {
    "text": "and that transformation has been awesome really positioned us to be not only one",
    "start": "1539940",
    "end": "1545519"
  },
  {
    "text": "of the leading insurance company but a leading digital insurer so in 2015 is",
    "start": "1545519",
    "end": "1550950"
  },
  {
    "text": "when we stood up on Hadoop environment and start building our data Lake the",
    "start": "1550950",
    "end": "1556259"
  },
  {
    "text": "year after that we initiate a AWS migration program to move over 200 plus",
    "start": "1556259",
    "end": "1562620"
  },
  {
    "text": "application to AWS we were all in no longer thing we wanted to manage the",
    "start": "1562620",
    "end": "1567750"
  },
  {
    "text": "data link or data center next shared",
    "start": "1567750",
    "end": "1573450"
  },
  {
    "text": "which is last year we migrated off of Hadoop on trend and all our data to",
    "start": "1573450",
    "end": "1580970"
  },
  {
    "text": "three and all a data processing into EMR this year would continue to follow the coattails of the EMR product team by",
    "start": "1580970",
    "end": "1588110"
  },
  {
    "text": "releasing new capabilities enhancement based on each GA releases next year is",
    "start": "1588110",
    "end": "1594230"
  },
  {
    "text": "really when we really want to move the needle on advanced analytics which we're gonna be focusing on our PA AI and ml so",
    "start": "1594230",
    "end": "1604460"
  },
  {
    "text": "why did we migrate to AWS EMR first you have to understand our own Prem Duke",
    "start": "1604460",
    "end": "1610960"
  },
  {
    "text": "architecture at the time where we're using pivotal which was a Hadoop distribution and the sequel on Hadoop",
    "start": "1610960",
    "end": "1617690"
  },
  {
    "text": "was hawke we had three cluster one was the ETL operation cluster which content",
    "start": "1617690",
    "end": "1623450"
  },
  {
    "text": "connected to major data sources and start bringing the data into the cluster and doing transformation and data",
    "start": "1623450",
    "end": "1629419"
  },
  {
    "text": "processing we also have a data science cluster we're serve as a sandbox environment for our data science team to",
    "start": "1629419",
    "end": "1636169"
  },
  {
    "text": "run their predictive models and lastly disaster recovery cluster as you can see",
    "start": "1636169",
    "end": "1642980"
  },
  {
    "text": "here this architecture poses several challenges one does challenges is that",
    "start": "1642980",
    "end": "1649450"
  },
  {
    "text": "stores and compute are tightly coupled which means that source is fixed and",
    "start": "1649450",
    "end": "1655539"
  },
  {
    "text": "making us very difficult to scale up and at a very good speed because these are",
    "start": "1655539",
    "end": "1661820"
  },
  {
    "text": "physical servers that it was prone to dis failures which we had to triage and",
    "start": "1661820",
    "end": "1667190"
  },
  {
    "text": "troubleshoot I mentioned about the inability to scale for example when we stood up our data science cluster it",
    "start": "1667190",
    "end": "1673760"
  },
  {
    "text": "took six to nine months to from provisioning procuring a hardware to",
    "start": "1673760",
    "end": "1678770"
  },
  {
    "text": "setting up in our data center in today's environment that is just unacceptable as you can imagine managing multiple",
    "start": "1678770",
    "end": "1685610"
  },
  {
    "text": "cluster is very costly there's also additional d'art costs because we",
    "start": "1685610",
    "end": "1691750"
  },
  {
    "text": "leverage a third party product to do data replication across the cluster",
    "start": "1691750",
    "end": "1698080"
  },
  {
    "text": "because these were physical servers there was long-running we weren't taking",
    "start": "1698080",
    "end": "1704030"
  },
  {
    "text": "advantage of the unused capacity during off-peak periods we were very inefficient you know utilizing these",
    "start": "1704030",
    "end": "1710000"
  },
  {
    "text": "servers because of the nature of managing your data center",
    "start": "1710000",
    "end": "1716990"
  },
  {
    "text": "it requires a team of operators with boots on the ground at the data center to manage the physical hardware and for",
    "start": "1716990",
    "end": "1724010"
  },
  {
    "text": "all this reason we were slow to adapt to changing business needs just to give you",
    "start": "1724010",
    "end": "1730100"
  },
  {
    "text": "some background about on migration strategy not only did we want to move to",
    "start": "1730100",
    "end": "1736940"
  },
  {
    "text": "AWS because it was a company why initiative but also there was a financial incentive we didn't wanted to",
    "start": "1736940",
    "end": "1744140"
  },
  {
    "text": "go into 2015 2018 excuse me of paying our Hadoop subscription",
    "start": "1744140",
    "end": "1749540"
  },
  {
    "text": "license in fact the subscription ended at December 31st 2017 so that was a",
    "start": "1749540",
    "end": "1755090"
  },
  {
    "text": "drop-dead day to move all our data all our workload to s3 and Newmar part of",
    "start": "1755090",
    "end": "1762530"
  },
  {
    "text": "the migration strategy we wanted to achieve a major theme in each of the quarters they are cloud assessment and",
    "start": "1762530",
    "end": "1768800"
  },
  {
    "text": "POC secure certification environment readiness application development and",
    "start": "1768800",
    "end": "1773929"
  },
  {
    "text": "regression testing data migration and cutover the migration was a one-year effort it",
    "start": "1773929",
    "end": "1781160"
  },
  {
    "text": "was a marathon but really felt like a sprint in q1 we conducted a cloud",
    "start": "1781160",
    "end": "1789230"
  },
  {
    "text": "assessment in POC on EMR these are the categories the acceptance criteria which",
    "start": "1789230",
    "end": "1794870"
  },
  {
    "text": "we judge EMR on and graded we graded from a scale of low medium high",
    "start": "1794870",
    "end": "1801050"
  },
  {
    "text": "confidence high being the best for functional capabilities EMR needed to meet our data processing and analytical",
    "start": "1801050",
    "end": "1808340"
  },
  {
    "text": "requirements infrastructure and software cost was medium but hindsight being",
    "start": "1808340",
    "end": "1813350"
  },
  {
    "text": "20/20 I were rated as high because it was substantial cost saving as opposed",
    "start": "1813350",
    "end": "1819230"
  },
  {
    "text": "to us managing at our own data center integrated Active Directory and Kerberos",
    "start": "1819230",
    "end": "1826280"
  },
  {
    "text": "was not out of the box it will require a custom solution this was our first time",
    "start": "1826280",
    "end": "1831380"
  },
  {
    "text": "also using Amazon Linux so it requires us to establish a new minimum baseline",
    "start": "1831380",
    "end": "1837050"
  },
  {
    "text": "security image EMR somewhat very sensitive to domain names because of our",
    "start": "1837050",
    "end": "1844160"
  },
  {
    "text": "domain architecture it requires a custom DNS the team was already familiar with",
    "start": "1844160",
    "end": "1850039"
  },
  {
    "text": "managing Hadoop our daily at the our own print but this was our first foray into using EMR in history for product",
    "start": "1850039",
    "end": "1857750"
  },
  {
    "text": "installation deployment automation and dr said it was high and we felt very confident in managing on no risk factor",
    "start": "1857750",
    "end": "1864080"
  },
  {
    "text": "doing the migration period for security certification and those of you who are",
    "start": "1864080",
    "end": "1870620"
  },
  {
    "text": "thinking about moving to AWS I highly recommend that you partner with your IT",
    "start": "1870620",
    "end": "1875870"
  },
  {
    "text": "security team contrary to popular belief they are your friend and they are",
    "start": "1875870",
    "end": "1881750"
  },
  {
    "text": "critical to your success we partner with them from the very beginning they sat",
    "start": "1881750",
    "end": "1887539"
  },
  {
    "text": "down we sat down with them with our AWS partner and even the EMR product team to go over their security requirements and",
    "start": "1887539",
    "end": "1894289"
  },
  {
    "text": "make sure that the security controls were put in place I mentioned the first time for us using Amazon Linux there are",
    "start": "1894289",
    "end": "1901490"
  },
  {
    "text": "cases where we had to seek a security exception the EMR GA version at the time",
    "start": "1901490",
    "end": "1907460"
  },
  {
    "text": "of migration did not have Kerberos integrated so we needed to get a security exception to not use Kerberos",
    "start": "1907460",
    "end": "1913820"
  },
  {
    "text": "also EMR did not addressed some the CIA s security benchmarks the center for",
    "start": "1913820",
    "end": "1919880"
  },
  {
    "text": "internet security standards there's cases that you mean have to for product",
    "start": "1919880",
    "end": "1926870"
  },
  {
    "text": "software products that you're using today you and me really have to pee let's make sure they're deeply integrated with AWS for example the ETL",
    "start": "1926870",
    "end": "1935299"
  },
  {
    "text": "yeah ELT tool we're using a sink sort we're probably one the first customer of",
    "start": "1935299",
    "end": "1941240"
  },
  {
    "text": "theirs to use s3 encryption part of the migration they needed to make a product enhancements as part in support of our",
    "start": "1941240",
    "end": "1948649"
  },
  {
    "text": "migration to their credit we're probably more of exception than a norm and then provide a quick turnaround we use the",
    "start": "1948649",
    "end": "1956539"
  },
  {
    "text": "edge node to serve as the gateway between s3 and EMR no one can access our",
    "start": "1956539",
    "end": "1963019"
  },
  {
    "text": "cluster now even our admins so we wall all access and shut off any SSH",
    "start": "1963019",
    "end": "1969139"
  },
  {
    "text": "I mentioned about cost using the use of custom DNS for data protection and controls we use s3 encryption SSL HTTPS",
    "start": "1969139",
    "end": "1978700"
  },
  {
    "text": "for multi region D our requirements really love the fact that s3 support cross region data replication from east",
    "start": "1978700",
    "end": "1986120"
  },
  {
    "text": "to west and in case of a dr scenario we use automation to quickly spin up the",
    "start": "1986120",
    "end": "1991639"
  },
  {
    "text": "original and our cluster in the west region in q3 terraform and puppet was",
    "start": "1991639",
    "end": "2000220"
  },
  {
    "text": "used for automation in spinning up the edge militant cluster bitbucket jenkins",
    "start": "2000220",
    "end": "2005500"
  },
  {
    "text": "is part of our continuous integration continuous deployment pipeline we had to",
    "start": "2005500",
    "end": "2010870"
  },
  {
    "text": "refactor and test over 300 plus workloads places where we touch code",
    "start": "2010870",
    "end": "2016750"
  },
  {
    "text": "were sinks or pic script Python our shell script if you have to migrate I",
    "start": "2016750",
    "end": "2025240"
  },
  {
    "text": "highly recommend developing a very detailed code migration plan because we had to accommodate projects that were in",
    "start": "2025240",
    "end": "2031990"
  },
  {
    "text": "flight activities that were occurring in debt on the on Prem dead and UAT environment what we did was we drew a",
    "start": "2031990",
    "end": "2039250"
  },
  {
    "text": "line of sand and stab said that by X date there would be a code freeze and the code base line from those",
    "start": "2039250",
    "end": "2045580"
  },
  {
    "text": "environment will be migrated to AWS dev environment and we would propagate upwards after the code migration we",
    "start": "2045580",
    "end": "2055450"
  },
  {
    "text": "conducted a parallel production testing between on print and AWS by comparing",
    "start": "2055450",
    "end": "2060490"
  },
  {
    "text": "the record count and making sure that each of the workloads was meeting SLA x'",
    "start": "2060490",
    "end": "2066780"
  },
  {
    "text": "determine we determined upfront the data sets of snapshot for parallel production",
    "start": "2066780",
    "end": "2071888"
  },
  {
    "text": "runs this meant that for some of our reporting requirements for those data sets it may need up to 6 months 12",
    "start": "2071889",
    "end": "2079658"
  },
  {
    "text": "months and even further out so o Boyd is was once the Co is migrating AWS pointing to back to the source system we",
    "start": "2079659",
    "end": "2086980"
  },
  {
    "text": "start pulling data starting for example January 1st and then any historical data prior to that we will use snowball to",
    "start": "2086980",
    "end": "2094329"
  },
  {
    "text": "bring that data into s3 in fact we use multiple snowball edges and migrate over",
    "start": "2094329",
    "end": "2099670"
  },
  {
    "text": "350 terabytes of data after the data was landed in s3 we established a life cycle",
    "start": "2099670",
    "end": "2107109"
  },
  {
    "text": "policy to move and archive data that was infrequently used to lower tier storage",
    "start": "2107109",
    "end": "2112619"
  },
  {
    "text": "such as s3 ia englais sure after the date as migrated s3 was",
    "start": "2112619",
    "end": "2119440"
  },
  {
    "text": "shut down the on-prem were closed and repurpose hardware for other projects",
    "start": "2119440",
    "end": "2124710"
  },
  {
    "text": "from a technology perspective mentioned we were moving off a pivotal and Hawk",
    "start": "2124800",
    "end": "2130000"
  },
  {
    "text": "will use snowball edge to move the data in s3 besides EMR we use presto for",
    "start": "2130000",
    "end": "2136180"
  },
  {
    "text": "interactive queries spark is used by our data science team to run their data",
    "start": "2136180",
    "end": "2142720"
  },
  {
    "text": "science use models hive is used for the batch processing and ec2 instances is",
    "start": "2142720",
    "end": "2149230"
  },
  {
    "text": "used as our edge node the hive meta store is our user as a my sequel",
    "start": "2149230",
    "end": "2154690"
  },
  {
    "text": "database so what's our architecture design pattern there are three its",
    "start": "2154690",
    "end": "2163690"
  },
  {
    "text": "interactive and batch real-time today we",
    "start": "2163690",
    "end": "2169060"
  },
  {
    "text": "don't have real-time use cases but at least we want to make sure there's a framework in place should that day come",
    "start": "2169060",
    "end": "2174490"
  },
  {
    "text": "and data outputs and data consumer who and what application are connecting to",
    "start": "2174490",
    "end": "2181180"
  },
  {
    "text": "you your data 100% we're doing this today we're",
    "start": "2181180",
    "end": "2187030"
  },
  {
    "text": "connecting to major source system in fact we're actually also bringing connecting to legacy mainframe system",
    "start": "2187030",
    "end": "2194110"
  },
  {
    "text": "we're bringing V Sam data into s3 based on the how long our company has been",
    "start": "2194110",
    "end": "2200560"
  },
  {
    "text": "around snow there's no surprise that we actually have a very large mainframe footprint so besides mainframe we",
    "start": "2200560",
    "end": "2207670"
  },
  {
    "text": "connect to various data sources across the board the relational world those",
    "start": "2207670",
    "end": "2212800"
  },
  {
    "text": "data sets range from transactional in nature and reporting in some cases where",
    "start": "2212800",
    "end": "2218800"
  },
  {
    "text": "data sources requires large volumes we use date change data captured and we use sync so as that mechanism to bring that",
    "start": "2218800",
    "end": "2226300"
  },
  {
    "text": "data into s3 is there are cases where we have to breed at a-- from third party",
    "start": "2226300",
    "end": "2232900"
  },
  {
    "text": "such as third party vendor from external sources I mentioned about using the edge",
    "start": "2232900",
    "end": "2239170"
  },
  {
    "text": "node to serve as a bridge between s3 and EMR for beta consumer who are",
    "start": "2239170",
    "end": "2244960"
  },
  {
    "text": "connecting to our cluster there are data Sciences they business analyst and there are business",
    "start": "2244960",
    "end": "2251559"
  },
  {
    "text": "intelligent application such as business object and tableau and as well as other downstream application there's some use",
    "start": "2251559",
    "end": "2260109"
  },
  {
    "text": "cases that require us to move data from s3 and into a data Mart those options",
    "start": "2260109",
    "end": "2266950"
  },
  {
    "text": "are my sequel sequel server are yes and no sequel gradually rethinking this",
    "start": "2266950",
    "end": "2273549"
  },
  {
    "text": "design pattern and going to be look taking a little hard look into using redshift to support our data Mart's and data",
    "start": "2273549",
    "end": "2279730"
  },
  {
    "text": "warehouse requirements so how would we handle if we have real time use cases in",
    "start": "2279730",
    "end": "2288579"
  },
  {
    "text": "how we handle data stream there are a couple of options we could use Kinesis",
    "start": "2288579",
    "end": "2293829"
  },
  {
    "text": "firehose will bring the data stream straight into s3 or you can go open source and use Kafka also Kinesis",
    "start": "2293829",
    "end": "2302890"
  },
  {
    "text": "streams handle candled the data stream and move the data into any couple options for you to process those screams",
    "start": "2302890",
    "end": "2309369"
  },
  {
    "text": "which is sparse streaming storm Flynn there's also cases where you need to",
    "start": "2309369",
    "end": "2315520"
  },
  {
    "text": "expose your data through API so Lando is a great option and also send",
    "start": "2315520",
    "end": "2320920"
  },
  {
    "text": "notifications and alerts to your data consumers here are the benefits and we",
    "start": "2320920",
    "end": "2329319"
  },
  {
    "text": "saw in utilizing EMR in s3 is highly cost effective as opposed to us managing",
    "start": "2329319",
    "end": "2336460"
  },
  {
    "text": "ourselves at our own data center decoupling stores from compute is",
    "start": "2336460",
    "end": "2342329"
  },
  {
    "text": "incredibly powerful it allows us to scale at ease and at anytime because",
    "start": "2342329",
    "end": "2349059"
  },
  {
    "text": "insurance is in a highly regulated industry we need to make sure that s3",
    "start": "2349059",
    "end": "2354910"
  },
  {
    "text": "and EMR satisfy all our security requirements and it does for the most part Automation has provided a great",
    "start": "2354910",
    "end": "2363819"
  },
  {
    "text": "benefit for us allow us to spin up our edge nodes or additional cluster at ease",
    "start": "2363819",
    "end": "2370119"
  },
  {
    "text": "and on demand and also Auto scale based on increase in workload we embrace open",
    "start": "2370119",
    "end": "2378760"
  },
  {
    "text": "source technology and we like to see the fact that Ian supports open source within their own",
    "start": "2378760",
    "end": "2384009"
  },
  {
    "text": "technology stack and lastly having a highly resilient highly scalable",
    "start": "2384009",
    "end": "2389769"
  },
  {
    "text": "architecture is immensely powerful and for this reason we feel that we're in",
    "start": "2389769",
    "end": "2394839"
  },
  {
    "text": "position to grow our analytical capabilities and anime enable the",
    "start": "2394839",
    "end": "2400539"
  },
  {
    "text": "business to drive actual insights thank you very much we'll take any questions",
    "start": "2400539",
    "end": "2413199"
  },
  {
    "text": "if you have yeah we'll go here",
    "start": "2413199",
    "end": "2419069"
  },
  {
    "text": "question is what kind of challenges did you face when you migrated from high running on on-prem to high one-year mark",
    "start": "2439199",
    "end": "2446049"
  },
  {
    "text": "yeah industry so so are a lot a lot of jobs that are on trend we were using sync sort as our ETL so in some cases we",
    "start": "2446049",
    "end": "2453609"
  },
  {
    "text": "didn't want to do a little we had to refactor some of them and a lot of time was the development effort",
    "start": "2453609",
    "end": "2459849"
  },
  {
    "text": "that took to refactor those jobs as well as a significant amount of time to regression test those workloads yep",
    "start": "2459849",
    "end": "2473729"
  },
  {
    "text": "yeah so we don't have real-time use cases today but those are options that we're actually thinking about leveraging",
    "start": "2489390",
    "end": "2496640"
  },
  {
    "text": "most likely Kinesis firehose is probably the best option but Kopke is also",
    "start": "2496640",
    "end": "2502410"
  },
  {
    "text": "another option that we wanted to look into if we want to maintain and keep using open source",
    "start": "2502410",
    "end": "2510980"
  },
  {
    "text": "when you're moving from HDFS rs3 are you using this CP to move data in and out we",
    "start": "2520590",
    "end": "2527310"
  },
  {
    "text": "are we were the s3 end point is they say",
    "start": "2527310",
    "end": "2536460"
  },
  {
    "text": "there's three end point is public you solve that problem or do you use VPC end points we use BBC endpoint yeah yeah so",
    "start": "2536460",
    "end": "2548570"
  },
  {
    "text": "so we use the edge node which really reacts acts that bridge and through I am",
    "start": "2548570",
    "end": "2554250"
  },
  {
    "text": "policy that's the service calls are going through the agile and then to s3",
    "start": "2554250",
    "end": "2561170"
  },
  {
    "text": "yeah so I mean within our team it was more easy because is more of a decision",
    "start": "2582120",
    "end": "2589410"
  },
  {
    "text": "as those more above me to make a company-wide decision to move into AWS I",
    "start": "2589410",
    "end": "2594960"
  },
  {
    "text": "think they did assessment on other cloud providers it's just saw the the massive",
    "start": "2594960",
    "end": "2600510"
  },
  {
    "text": "benefits that everyone else if you look",
    "start": "2600510",
    "end": "2607050"
  },
  {
    "text": "at the keynote that happened today so Guardians CIO actually spend a lot of",
    "start": "2607050",
    "end": "2613710"
  },
  {
    "text": "time talking about that and the gist was they actually did a gap analysis and it took them almost a year to do the gap",
    "start": "2613710",
    "end": "2620190"
  },
  {
    "text": "analysis between x and y well yeah i",
    "start": "2620190",
    "end": "2630660"
  },
  {
    "text": "think you you tend to choose ones that quickly get your work done I think",
    "start": "2630660",
    "end": "2637110"
  },
  {
    "text": "that's what we have seen with a lot of people that if you can get something done really quickly that's where things",
    "start": "2637110",
    "end": "2642150"
  },
  {
    "text": "will go and it doesn't really have to be an org ID decision just in case of them it was an awkward decision",
    "start": "2642150",
    "end": "2650030"
  },
  {
    "text": "it's still early doesn't cloud yeah",
    "start": "2656430",
    "end": "2662029"
  },
  {
    "text": "we haven't yet actually we are looking into doing charge back model into other",
    "start": "2676770",
    "end": "2682380"
  },
  {
    "text": "Department but we haven't got time to know we're still in discussion yeah",
    "start": "2682380",
    "end": "2687900"
  },
  {
    "text": "because you got to look at different charge call centers the utilization so",
    "start": "2687900",
    "end": "2693360"
  },
  {
    "text": "it's a pretty complicated model to do implement",
    "start": "2693360",
    "end": "2697850"
  },
  {
    "text": "yeah so we have a couple of business users some are the data scientist to the",
    "start": "2712109",
    "end": "2719729"
  },
  {
    "text": "edge you know and so we have to cluster the ETL cluster which no one had access from the business perspective but also",
    "start": "2719729",
    "end": "2726930"
  },
  {
    "text": "we have an analytics cluster so our data science team can access the cluster and",
    "start": "2726930",
    "end": "2732209"
  },
  {
    "text": "s3 our business using more of the business analysts they access using",
    "start": "2732209",
    "end": "2737309"
  },
  {
    "text": "different sequel tools and bi tools such as tableau to connect straight into the",
    "start": "2737309",
    "end": "2743579"
  },
  {
    "text": "high tables going through the Presto engine right today we have just one",
    "start": "2743579",
    "end": "2755339"
  },
  {
    "text": "bucket yeah and then everything's Sur gated and organized through directory in",
    "start": "2755339",
    "end": "2763799"
  },
  {
    "text": "well in the beginning we were using transient cluster but because of we're",
    "start": "2763799",
    "end": "2769440"
  },
  {
    "text": "actually shifting and looking and building up our Guardian Indian practice so required us to look into running",
    "start": "2769440",
    "end": "2776190"
  },
  {
    "text": "making the cluster long running so we're actually using reserved instances to kind of manage the cause yeah",
    "start": "2776190",
    "end": "2787339"
  },
  {
    "text": "nope not yet",
    "start": "2795170",
    "end": "2799278"
  },
  {
    "text": "so let me also clarify what EMR F is consistent view does so Mr F is",
    "start": "2812070",
    "end": "2817230"
  },
  {
    "text": "consistent view is useful when you have two parallel jobs are reading and writing data or updates to the data at",
    "start": "2817230",
    "end": "2824940"
  },
  {
    "text": "the same time s3 has read up the right consistency for new objects it doesn't",
    "start": "2824940",
    "end": "2830460"
  },
  {
    "text": "have read after right consistency for existing objects so if you have like a job that writes into s3 and at the same",
    "start": "2830460",
    "end": "2837480"
  },
  {
    "text": "time there is another job that is picking up that data that's where you start to use consistent for you for",
    "start": "2837480",
    "end": "2843780"
  },
  {
    "text": "normal jobs that you are just reading and writing data to s3 you don't need consistent you have to be enabled",
    "start": "2843780",
    "end": "2850730"
  },
  {
    "text": "for us we're losing MRFs not positive",
    "start": "2881330",
    "end": "2891140"
  },
  {
    "text": "which one",
    "start": "2891140",
    "end": "2893500"
  },
  {
    "text": "uh it was a question if we still have hi yes we still have high and we also using",
    "start": "2900620",
    "end": "2906200"
  },
  {
    "text": "spark as well reason both same here you",
    "start": "2906200",
    "end": "2911540"
  },
  {
    "text": "had a question",
    "start": "2911540",
    "end": "2913900"
  },
  {
    "text": "so if I understand your question correctly you're asking what kind of job",
    "start": "2933920",
    "end": "2939240"
  },
  {
    "text": "we run using air flow",
    "start": "2939240",
    "end": "2942590"
  },
  {
    "text": "we're not actually that's actually not true maybe the picture is misleading or",
    "start": "2954170",
    "end": "2960260"
  },
  {
    "text": "confusing somehow so as we only use airflow mainly as the scheduling service so for example one",
    "start": "2960260",
    "end": "2967819"
  },
  {
    "text": "team they have a one-year mark Hasse two we basically so one yamakasi has a",
    "start": "2967819",
    "end": "2974420"
  },
  {
    "text": "master ID right so in fo job we have operated basically specified provide our",
    "start": "2974420",
    "end": "2979970"
  },
  {
    "text": "mass ID and job will basically send it over to that Yama cluster sorry what's",
    "start": "2979970",
    "end": "2991730"
  },
  {
    "text": "that yes",
    "start": "2991730",
    "end": "2997359"
  },
  {
    "text": "you mean places in castle with EMR",
    "start": "3001040",
    "end": "3005080"
  },
  {
    "text": "because the workload is ad-hoc so they can anybody can submit jobs at any point of time and we're suing up in the",
    "start": "3007990",
    "end": "3014570"
  },
  {
    "text": "process of evaluating and migrating the hive type art the other work no over the",
    "start": "3014570",
    "end": "3019609"
  },
  {
    "text": "ad-hoc piece",
    "start": "3019609",
    "end": "3022240"
  },
  {
    "text": "right now I think when we spin up a long-running cluster with about 20 note",
    "start": "3028560",
    "end": "3034869"
  },
  {
    "text": "this was small team to use is to take about 15 minutes yeah we have some chef integration that probably take some time",
    "start": "3034869",
    "end": "3041320"
  },
  {
    "text": "in there as well",
    "start": "3041320",
    "end": "3043950"
  },
  {
    "text": "at the time when using on Prem we were using Ranger but after when we moved to",
    "start": "3053490",
    "end": "3059340"
  },
  {
    "text": "UM are we didn't just have we didn't really have the time to implement Ranger but I think they're coming out part of",
    "start": "3059340",
    "end": "3066420"
  },
  {
    "text": "the roadmap is to implement some very features similar to that so the announcement that came out today around",
    "start": "3066420",
    "end": "3072600"
  },
  {
    "text": "lake formation one of the things that lake formation does is allows you to do fine-grained security on data that is",
    "start": "3072600",
    "end": "3078900"
  },
  {
    "text": "sitting on s3 Ranger doesn't do s3 Ranger does only HDFS you can still use",
    "start": "3078900",
    "end": "3086040"
  },
  {
    "text": "Ranger with EMR if your data is only resident in HDFS but the gut the",
    "start": "3086040",
    "end": "3092160"
  },
  {
    "text": "announcement that came out this today morning about lake formation which is",
    "start": "3092160",
    "end": "3097200"
  },
  {
    "text": "our data leak story is going to have integration for fine-grained resource based control and we're going to do it",
    "start": "3097200",
    "end": "3103290"
  },
  {
    "text": "across Emaar attina redshift and glue so you'll be able to go to lake formation be able to say",
    "start": "3103290",
    "end": "3112130"
  },
  {
    "text": "accesses give X access to table Y and that's how we will be able to implement",
    "start": "3112130",
    "end": "3117990"
  },
  {
    "text": "it it's not about it's not at all available right now it's been previa",
    "start": "3117990",
    "end": "3126260"
  },
  {
    "text": "yeah question here",
    "start": "3126470",
    "end": "3130099"
  },
  {
    "text": "so sorry Syd I can you can be a little",
    "start": "3149450",
    "end": "3154690"
  },
  {
    "text": "louder if you don't mind yes correct",
    "start": "3154690",
    "end": "3165660"
  },
  {
    "text": "I don't know which talk you attended but Athena doesn't so I managed both Athena an EMR as a product I lead those two",
    "start": "3178960",
    "end": "3185890"
  },
  {
    "text": "products Athena doesn't integrate with s3 select today so I think the reason",
    "start": "3185890",
    "end": "3193150"
  },
  {
    "text": "for s3 select is what happens is when you have let's say you're running SPARC",
    "start": "3193150",
    "end": "3198280"
  },
  {
    "text": "on an ec2 instance where the data is sitting in s3 when you are running a query the ec2 instance talk to s3 and",
    "start": "3198280",
    "end": "3206710"
  },
  {
    "text": "pulls all the data into its memory because Park is in memory and runs the",
    "start": "3206710",
    "end": "3212470"
  },
  {
    "text": "filters whatever you want to filter let's say if you have a where clause in the memory what s3 select allows you to",
    "start": "3212470",
    "end": "3219340"
  },
  {
    "text": "do it it allows you to take the filter and pass it down all the way into storage so I don't have to take the",
    "start": "3219340",
    "end": "3226180"
  },
  {
    "text": "entire data set and filter in memory I only take the results set in memory so",
    "start": "3226180",
    "end": "3231970"
  },
  {
    "text": "they're both using s3 as a data store but in one case I'm able to push the",
    "start": "3231970",
    "end": "3237310"
  },
  {
    "text": "filter down to s3 and the other case I'm processing the entire dataset so both",
    "start": "3237310",
    "end": "3242680"
  },
  {
    "text": "are decoupling storage and compute in Bay one is being able to push down the filter which s3 provides with rest is",
    "start": "3242680",
    "end": "3249190"
  },
  {
    "text": "light and does that help ok you don't",
    "start": "3249190",
    "end": "3254650"
  },
  {
    "text": "look convinced but I guess alright oh sorry one more",
    "start": "3254650",
    "end": "3261960"
  },
  {
    "text": "they do share the same meta story so we have a goal and civil right even if we",
    "start": "3278620",
    "end": "3284930"
  },
  {
    "text": "migrated to split them into different small ones we still at the past basically end point of either silver",
    "start": "3284930",
    "end": "3292750"
  },
  {
    "text": "metal store or go open a store to the BMR class or when a job running so they",
    "start": "3292750",
    "end": "3298100"
  },
  {
    "text": "can fetch the job the partition correctly",
    "start": "3298100",
    "end": "3302800"
  },
  {
    "text": "not that we have seen so far",
    "start": "3312910",
    "end": "3317280"
  },
  {
    "text": "basically we haven't seen an issue with that so far",
    "start": "3329800",
    "end": "3334080"
  },
  {
    "text": "all right thank you very much thank you for braving with us little hpn appreciate it thank you thank you thank",
    "start": "3337839",
    "end": "3344349"
  },
  {
    "text": "you guys [Applause]",
    "start": "3344349",
    "end": "3348480"
  }
]