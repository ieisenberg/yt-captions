[
  {
    "start": "0",
    "end": "69000"
  },
  {
    "text": "all right welcome everyone to the AWS loft and hello to the folks on Twitch",
    "start": "1610",
    "end": "7770"
  },
  {
    "text": "I'm Dylan Tong my AWS solutions architect so a little bit about myself at the moment AWS for almost two and a",
    "start": "7770",
    "end": "14070"
  },
  {
    "text": "half years now working up enterprise customers at the fortune with helping them build data",
    "start": "14070",
    "end": "20369"
  },
  {
    "text": "lakes and I'm excited today to share my experiences and inform you about the services that can help you on your data",
    "start": "20369",
    "end": "27720"
  },
  {
    "text": "Lake journey so before we get started can have a quick show of hands who here is already running a data Lake in AWS",
    "start": "27720",
    "end": "36110"
  },
  {
    "text": "okay that's great so everyone's new to what will aw screw has anyone used AWS",
    "start": "36110",
    "end": "41550"
  },
  {
    "text": "gloop for a few folks okay so this is perfect because I'm not gonna assume prior experience with data lakes or AWS",
    "start": "41550",
    "end": "48000"
  },
  {
    "text": "glue but if there are experts in a room I'm here all day I'll make myself available after this session we also",
    "start": "48000",
    "end": "55320"
  },
  {
    "text": "have a hands-on workshop where you're gonna get a chance to dive deep into glue and deeper into the concepts that",
    "start": "55320",
    "end": "61800"
  },
  {
    "text": "we're gonna discuss in the session we're gonna keep the agenda light so I want to",
    "start": "61800",
    "end": "66869"
  },
  {
    "text": "start off by sharing with you some customer examples of how they built data",
    "start": "66869",
    "end": "72060"
  },
  {
    "start": "69000",
    "end": "69000"
  },
  {
    "text": "lakes on AWS and then we're gonna go through a data link building exercise",
    "start": "72060",
    "end": "77729"
  },
  {
    "text": "and learn about how AWS glue can help build your data Lake and wrap things up",
    "start": "77729",
    "end": "84180"
  },
  {
    "text": "by giving a preview of what's next in terms of data lakes on AWS by sharing",
    "start": "84180",
    "end": "89909"
  },
  {
    "text": "with you information about lake formation a service that we intend to release later this year alright so first off let's level set",
    "start": "89909",
    "end": "99299"
  },
  {
    "start": "97000",
    "end": "97000"
  },
  {
    "text": "what is a data link and why do we build a data Lake simplistically a data Lake",
    "start": "99299",
    "end": "104579"
  },
  {
    "text": "is a central repository meant to serve data of all forms right whether it's",
    "start": "104579",
    "end": "110070"
  },
  {
    "text": "structured transactional records from a relational database or unstructured like",
    "start": "110070",
    "end": "116130"
  },
  {
    "text": "audio recordings videos or images and the data link architecture should be scalable right to exabytes it should be",
    "start": "116130",
    "end": "123899"
  },
  {
    "text": "able to handle batch and streaming workloads alike and it should be",
    "start": "123899",
    "end": "128910"
  },
  {
    "text": "flexible to support data and analytics tools for today as well as tomorrow should be",
    "start": "128910",
    "end": "135030"
  },
  {
    "text": "future-proof and the ultimate goal right is to democratize your data sets so the",
    "start": "135030",
    "end": "141110"
  },
  {
    "text": "organization's your organization can derive value from the data and it could be to support use cases such as data",
    "start": "141110",
    "end": "148320"
  },
  {
    "text": "warehousing and business intelligence to solving machine learning problems in the realm of computer vision or natural",
    "start": "148320",
    "end": "155730"
  },
  {
    "text": "language processing okay so let's give some more concrete examples of exactly what a data Lake is",
    "start": "155730",
    "end": "162300"
  },
  {
    "text": "by talking about some customer examples so the first one I want to share is the",
    "start": "162300",
    "end": "167670"
  },
  {
    "text": "FINRA use case okay so for folks that don't I'm kind of curious who here is aware of familiar FINRA yeah so some",
    "start": "167670",
    "end": "176160"
  },
  {
    "text": "folks so FINRA is the financial services regulatory authority in the u.s. in",
    "start": "176160",
    "end": "181860"
  },
  {
    "text": "essence they are responsible for ensuring that Wall Street operates with integrity and in reinvent 2017 they",
    "start": "181860",
    "end": "190110"
  },
  {
    "text": "shared with us what it took to run their most mission-critical workload this",
    "start": "190110",
    "end": "195870"
  },
  {
    "text": "market surveillance platform on AWS and their CIO shared with us some",
    "start": "195870",
    "end": "200970"
  },
  {
    "text": "interesting data points in terms of the scale that they had to operate in so for example at peak they had to support 75",
    "start": "200970",
    "end": "210240"
  },
  {
    "text": "billion events per day and back in 2017 they Ori had over 20 petabytes of data",
    "start": "210240",
    "end": "216200"
  },
  {
    "text": "so some of the things that needs to do was analyze data down to the day level all the way up to in a month and we are",
    "start": "216200",
    "end": "223620"
  },
  {
    "text": "at the month level they're offering on trillions of Records and ultimately what they need to do is they can reconstruct",
    "start": "223620",
    "end": "229440"
  },
  {
    "text": "a market event to determine if there's any specific activity okay",
    "start": "229440",
    "end": "236340"
  },
  {
    "text": "I took the slide right from there reinvent 2017 presentation and this is",
    "start": "236340",
    "end": "241920"
  },
  {
    "text": "where they talked about their journey or migrating their workloads from on-prem over to AWS and how they made",
    "start": "241920",
    "end": "247410"
  },
  {
    "text": "architectural decisions eventually landing on this data Lake architecture that you see here so some of the core",
    "start": "247410",
    "end": "255120"
  },
  {
    "text": "decisions that they make is they decide to use s3 as the storage layer for their",
    "start": "255120",
    "end": "261630"
  },
  {
    "text": "data like right and they made the decision for all the things that people usually use s34 rights with high",
    "start": "261630",
    "end": "268630"
  },
  {
    "text": "availability it's a high durability it's because they need to operate at scale",
    "start": "268630",
    "end": "273930"
  },
  {
    "text": "efficiently they didn't need to manage the infrastructure so they can scale",
    "start": "273930",
    "end": "278950"
  },
  {
    "text": "this data Lake another core component that you can see here on the right-hand",
    "start": "278950",
    "end": "283990"
  },
  {
    "text": "side of that diagram is a catalog component so when you're something like",
    "start": "283990",
    "end": "289120"
  },
  {
    "text": "when you're like FINRA and you have a lot of data sets and a lot of data you need a mechanism to catalog your data so",
    "start": "289120",
    "end": "295690"
  },
  {
    "text": "that your data sets are discoverable right and you need to have ways to govern those data sets and determine who",
    "start": "295690",
    "end": "302200"
  },
  {
    "text": "has access to those data so data back when FINRA went on their data lake",
    "start": "302200",
    "end": "307750"
  },
  {
    "text": "journey we didn't have any data cataloging services so they built their own which they call herd and since then",
    "start": "307750",
    "end": "314830"
  },
  {
    "text": "they have open source you can find it on the web and once they created this",
    "start": "314830",
    "end": "320500"
  },
  {
    "text": "foundation the storage and the catalog D now had architecture where it has",
    "start": "320500",
    "end": "325900"
  },
  {
    "text": "storage decoupled from compute and they had the flexibility to then bring the",
    "start": "325900",
    "end": "331000"
  },
  {
    "text": "right tool for the job and a big component of that is leveraging",
    "start": "331000",
    "end": "336220"
  },
  {
    "text": "something called Amazon Amazon EMR which is AWS as Hadoop as a service so what",
    "start": "336220",
    "end": "341860"
  },
  {
    "text": "they would do is let's say is a data processing use case they would spin up an EMR cluster where SPARC and do data",
    "start": "341860",
    "end": "348070"
  },
  {
    "text": "processing they need something more transactional you can spin up a cluster with HBase and perhaps presto if they",
    "start": "348070",
    "end": "354250"
  },
  {
    "text": "needed some you know distributed queries across different data stores in some cases maybe they needed service compute",
    "start": "354250",
    "end": "360700"
  },
  {
    "text": "so they can use lambda so in essence you can see it's a simplistic architecture",
    "start": "360700",
    "end": "366220"
  },
  {
    "text": "but was highly effective flexible in the sense that they had storage and compute",
    "start": "366220",
    "end": "371860"
  },
  {
    "text": "decoupled and highly economical so that's the perspective of a data Lake",
    "start": "371860",
    "end": "378010"
  },
  {
    "text": "from a large organization like FINRA let's take a look at the data like from the lens of a start-up has anyone here",
    "start": "378010",
    "end": "385390"
  },
  {
    "text": "used Robin Hood before a few folks ok so Robin Hood is a commission-free trading",
    "start": "385390",
    "end": "393070"
  },
  {
    "text": "platform ok and in reven 2018 they shared with us the challenges that day",
    "start": "393070",
    "end": "400009"
  },
  {
    "text": "face - led them to build a data Lake on AWS so some of the key challenges they",
    "start": "400009",
    "end": "407659"
  },
  {
    "text": "face included age-old challenges right we all if you guys are experienced in",
    "start": "407659",
    "end": "413300"
  },
  {
    "text": "the data lake realm or data engineering realm you know that this this has always been a data silo issue right which they",
    "start": "413300",
    "end": "418639"
  },
  {
    "text": "called data ponds so we had data sets spread across our yes redshift",
    "start": "418639",
    "end": "423699"
  },
  {
    "text": "third-party third-party data stores caf-co elasticsearch and s3 and what",
    "start": "423699",
    "end": "429770"
  },
  {
    "text": "they needed to do is provide a unified interface to their end users the second",
    "start": "429770",
    "end": "436999"
  },
  {
    "text": "challenge was really about economics but you have a rapidly growing organization",
    "start": "436999",
    "end": "442550"
  },
  {
    "start": "437000",
    "end": "437000"
  },
  {
    "text": "a rapidly growing user base there indeed their trading platform you can imagine",
    "start": "442550",
    "end": "448249"
  },
  {
    "text": "how quickly their data was growing and unfortunately what coincided with that",
    "start": "448249",
    "end": "454279"
  },
  {
    "text": "growth was a widening chasm right between the value that their data was",
    "start": "454279",
    "end": "461809"
  },
  {
    "text": "delivering through the business and the cost of enabling that analytics and the",
    "start": "461809",
    "end": "467870"
  },
  {
    "text": "root cause of the issue was partly due to architecture and technology so they had tightly their compute and storage",
    "start": "467870",
    "end": "473199"
  },
  {
    "text": "was tightly coupled so as a consequence as they scaled their platform it was",
    "start": "473199",
    "end": "481519"
  },
  {
    "text": "expensive they weren't running analytics every day so they had a lot of idle",
    "start": "481519",
    "end": "486740"
  },
  {
    "text": "compute so what they wanted was the architecture to better decouple compute and storage and ultimately achieve",
    "start": "486740",
    "end": "493159"
  },
  {
    "text": "better cost efficiency and the third challenge was around data governance so",
    "start": "493159",
    "end": "498349"
  },
  {
    "start": "495000",
    "end": "495000"
  },
  {
    "text": "they had a growing user base and he had users that wanted to access the data in",
    "start": "498349",
    "end": "504709"
  },
  {
    "text": "different ways right and it became an increasing challenge to control who can access what datasets and how and at the",
    "start": "504709",
    "end": "512419"
  },
  {
    "text": "same time with more data and more users there was challenges around maintaining data quality so eventually they arrived",
    "start": "512419",
    "end": "520339"
  },
  {
    "start": "519000",
    "end": "519000"
  },
  {
    "text": "at this data Lake architecture that uses that you see here again I took the slide",
    "start": "520339",
    "end": "525380"
  },
  {
    "text": "deck right from there 2018 presentation and you'll notice that there's a lot of",
    "start": "525380",
    "end": "531350"
  },
  {
    "text": "similarities between their architecture and FINRA's and that's not a coincidence",
    "start": "531350",
    "end": "536960"
  },
  {
    "text": "if you do some more research and take a look at data like architectures on AWS take a look at what Netflix has done",
    "start": "536960",
    "end": "543020"
  },
  {
    "text": "you'll see a lot of commonalities right this is a common pattern typically you'll have like an ingestion layer to",
    "start": "543020",
    "end": "549620"
  },
  {
    "text": "handle Street streaming and batch the data eventually lands in the storage layer which is s3 and then you have the",
    "start": "549620",
    "end": "556940"
  },
  {
    "text": "flexibility to bring the right tool for the job so for instance for data processing they",
    "start": "556940",
    "end": "562670"
  },
  {
    "text": "used EMR like FINRA for data warehousing to use redshift some of the notable",
    "start": "562670",
    "end": "568940"
  },
  {
    "text": "differences that you see here is they also use a couple of services Athena and glue okay these were services that",
    "start": "568940",
    "end": "578260"
  },
  {
    "text": "weren't available to FINRA when they started on her journey but these two services were played a critical role in",
    "start": "578260",
    "end": "584660"
  },
  {
    "text": "Robyn's hood data Lake to help them solve some of their challenges like decoupling better decoupling stores from",
    "start": "584660",
    "end": "591380"
  },
  {
    "text": "compute and achieving better cost efficiency so when take a look at ena",
    "start": "591380",
    "end": "597310"
  },
  {
    "text": "Athena helped offload some of the redshift queries so they had redshift serving they're hot data and redshift",
    "start": "597310",
    "end": "603860"
  },
  {
    "text": "was great in terms of providing the lowest latency possible on complex",
    "start": "603860",
    "end": "608960"
  },
  {
    "text": "analytical queries other queries like the warmer data they were able to",
    "start": "608960",
    "end": "615080"
  },
  {
    "text": "offload to Athena and Athena allowed them to run sequel queries directly",
    "start": "615080",
    "end": "620480"
  },
  {
    "text": "against s3 without managing a database cluster and it provided them with cost",
    "start": "620480",
    "end": "625490"
  },
  {
    "text": "efficiency because you only pay for the queries that you actually run similarily",
    "start": "625490",
    "end": "630920"
  },
  {
    "text": "on the data processing side they use glue and glue is also service so they",
    "start": "630920",
    "end": "636560"
  },
  {
    "text": "use glue to run their batch ETL processes and because a service they",
    "start": "636560",
    "end": "641690"
  },
  {
    "text": "again only paid for the resources that they use during the ETL batch processing",
    "start": "641690",
    "end": "646940"
  },
  {
    "text": "so it's through these services that they were able to gain you know high cost",
    "start": "646940",
    "end": "652010"
  },
  {
    "text": "efficient seam embedder decouple storage and compute you may look at this",
    "start": "652010",
    "end": "657770"
  },
  {
    "text": "architecture and say hey what about the data catalog that was a big component of FINRA well the data catalog is it's",
    "start": "657770",
    "end": "664640"
  },
  {
    "text": "provided by glue so glue provides ETL as well as the data catalog in this case",
    "start": "664640",
    "end": "669920"
  },
  {
    "text": "for Robin Hood you also notice that there's a lot of other complementary technologies out",
    "start": "669920",
    "end": "675260"
  },
  {
    "text": "there like for example they decided to use Apache airflow to orchestrate the workflow between EMR and glue they have",
    "start": "675260",
    "end": "683960"
  },
  {
    "text": "their own validation layer running their own custom business logic on database compute and for the front end they have",
    "start": "683960",
    "end": "691460"
  },
  {
    "text": "Jupiter serving their data scientists and looker for business intelligence and the take away again is you have a",
    "start": "691460",
    "end": "698750"
  },
  {
    "text": "flexible architecture here you have the choice to write use the right AWS services that you feel is right for the",
    "start": "698750",
    "end": "705770"
  },
  {
    "text": "job but you also have the flexibility to use open source technologies as well as",
    "start": "705770",
    "end": "711410"
  },
  {
    "text": "partner technologies as you see fit ok yes there was a question yeah good",
    "start": "711410",
    "end": "721970"
  },
  {
    "text": "question so your question is are there any other schedulers available in addition to Apache airflow right so",
    "start": "721970",
    "end": "729250"
  },
  {
    "text": "we'll get into this a little deeper but there's many options some people choose",
    "start": "729250",
    "end": "734839"
  },
  {
    "text": "to use like AWS up functions if you're aware of that so that's another option if you want to go down to AWS route",
    "start": "734839",
    "end": "740740"
  },
  {
    "text": "we're gonna share it we're gonna talk about glue a little more later and glue Ashley has its own job workflow",
    "start": "740740",
    "end": "747709"
  },
  {
    "text": "capabilities as well in this case Apache work airflow help because they want to",
    "start": "747709",
    "end": "754220"
  },
  {
    "text": "run workflows across EMR and glue so you know they landed on airflow which is a",
    "start": "754220",
    "end": "760640"
  },
  {
    "text": "common open source choice but there's certainly other options like step functions",
    "start": "760640",
    "end": "766810"
  },
  {
    "text": "it's correct okay so so the question was",
    "start": "789569",
    "end": "799209"
  },
  {
    "text": "you know they could have used redshift spectrum as the observation and that's absolutely correct right you know I",
    "start": "799209",
    "end": "805719"
  },
  {
    "text": "don't know the details and why they didn't use redshift spectrum they may be using register spectrum I'm not aware of",
    "start": "805719",
    "end": "811029"
  },
  {
    "text": "it but that's absolutely an option in a useful complimentary service for their",
    "start": "811029",
    "end": "816759"
  },
  {
    "text": "architecture so we saw in robin hood's",
    "start": "816759",
    "end": "824349"
  },
  {
    "text": "data leak architecture that glue was a major proponent to provide the data catalog as well as ETL capabilities so I",
    "start": "824349",
    "end": "831039"
  },
  {
    "text": "want to dive deeper now to help you understand how glue can help you build your data Lake so again glue is a serve",
    "start": "831039",
    "end": "837159"
  },
  {
    "start": "834000",
    "end": "834000"
  },
  {
    "text": "the state of college and ETL service so from a data cataloguing side high level what it does is it you can run these",
    "start": "837159",
    "end": "843579"
  },
  {
    "text": "crawlers they'll scan your data like and I'll discover data sets for you and does this by inferring the schema and then it",
    "start": "843579",
    "end": "850869"
  },
  {
    "text": "can then register these data sets into a data catalog and make those assets",
    "start": "850869",
    "end": "856719"
  },
  {
    "text": "searchable by users and systems on the ETL side it provides an environment",
    "start": "856719",
    "end": "863139"
  },
  {
    "text": "where you can develop your ETL scripts and as well then deploy those scripts",
    "start": "863139",
    "end": "869679"
  },
  {
    "text": "into and run job workflows and execute ETL in a repeatable and reliable manner",
    "start": "869679",
    "end": "876519"
  },
  {
    "text": "and this is all in a service context you don't need to you don't need to run any infrastructure any servers and it's",
    "start": "876519",
    "end": "883779"
  },
  {
    "text": "built on open standards like Apache spark and native support for open data",
    "start": "883779",
    "end": "889269"
  },
  {
    "text": "formats as well and you know and we're we're delighted to have rapid and broad",
    "start": "889269",
    "end": "896409"
  },
  {
    "start": "892000",
    "end": "892000"
  },
  {
    "text": "adoption from enterprise customers to startups across industries as well as",
    "start": "896409",
    "end": "901839"
  },
  {
    "text": "across the globe so I think that he's the easiest way to understand how glue",
    "start": "901839",
    "end": "907329"
  },
  {
    "text": "can help you build your data Lake is go through a data Lake building exercise so",
    "start": "907329",
    "end": "912669"
  },
  {
    "text": "let's do that so these are the most basic steps in terms of building a data Lake right first we need to set up some",
    "start": "912669",
    "end": "918909"
  },
  {
    "start": "913000",
    "end": "913000"
  },
  {
    "text": "storage right then we need to hydrate the data lake and then we",
    "start": "918909",
    "end": "924010"
  },
  {
    "text": "wanted then implement a repeatable process to cleanse prep and catalog our",
    "start": "924010",
    "end": "930280"
  },
  {
    "text": "data and then next we need to secure our data Lake and finally we can then make",
    "start": "930280",
    "end": "937900"
  },
  {
    "text": "the data available for our end users for things like analytics so we're going to",
    "start": "937900",
    "end": "943060"
  },
  {
    "text": "go through this process together okay so the first step we want to do is want to",
    "start": "943060",
    "end": "948880"
  },
  {
    "text": "set up storage for the data like and we're gonna use s3 so has anyone is",
    "start": "948880",
    "end": "955990"
  },
  {
    "start": "952000",
    "end": "952000"
  },
  {
    "text": "everyone here used s3 before and most people okay so you know how",
    "start": "955990",
    "end": "961090"
  },
  {
    "text": "easy it is gonna be to set up storage we're just gonna create some s3 buckets but let's pause for a moment and",
    "start": "961090",
    "end": "967800"
  },
  {
    "text": "validate that s3 is the right technology so if we're gonna build a data Lake it's",
    "start": "967800",
    "end": "973780"
  },
  {
    "text": "gonna serve analytics I think it's reasonable to assume that we will have some high value data that we need to",
    "start": "973780",
    "end": "979840"
  },
  {
    "text": "ensure high availability and high durability on right so we know that s3 replicates the data for us across",
    "start": "979840",
    "end": "986140"
  },
  {
    "text": "multiple availability zones to provide that availability and durability",
    "start": "986140",
    "end": "991210"
  },
  {
    "text": "we're also likely going to have some sensitive and confidential data right so we're gonna have to lock down the system",
    "start": "991210",
    "end": "997750"
  },
  {
    "text": "you got s3 mature cloud storage so we're gonna have some mature we have the comfort of knowing that we have mature",
    "start": "997750",
    "end": "1007010"
  },
  {
    "text": "security controls in place as well as industry best practices and security",
    "start": "1007010",
    "end": "1012990"
  },
  {
    "text": "accredited accreditations available to us the data Lake also needs to serve",
    "start": "1012990",
    "end": "1019500"
  },
  {
    "text": "data of all types right we talked about structured and unstructured and that's what you know s3 is designed for as well",
    "start": "1019500",
    "end": "1027270"
  },
  {
    "text": "and finally scale and I'm not talking about just having a storage layer that can scale to exabytes but at the same",
    "start": "1027270",
    "end": "1034530"
  },
  {
    "text": "time it needs to be manageable right as it scales to know petabytes and exabytes it needs to be manageable and that's as",
    "start": "1034530",
    "end": "1041339"
  },
  {
    "text": "we know where s3 shines because we don't need to manage those servers right as",
    "start": "1041339",
    "end": "1046770"
  },
  {
    "text": "well as our data like scales we need to ensure it's economical so your s3 also",
    "start": "1046770",
    "end": "1054780"
  },
  {
    "text": "has mechanisms like intelligent storing as well as lifecycle management so that",
    "start": "1054780",
    "end": "1060480"
  },
  {
    "text": "it can automatically move our data to colder storage for cost optimization so",
    "start": "1060480",
    "end": "1066570"
  },
  {
    "text": "it's got all these capabilities to ensure that you know we can scale that's economical that's secure",
    "start": "1066570",
    "end": "1073910"
  },
  {
    "text": "so one sweeps we set up our s3 buckets we need to hydrate our data like so",
    "start": "1074030",
    "end": "1083670"
  },
  {
    "start": "1081000",
    "end": "1081000"
  },
  {
    "text": "chances are we're gonna have a lot of data sources our on premise that we're gonna need to bring into our data Lake",
    "start": "1083670",
    "end": "1088740"
  },
  {
    "text": "so one option we have is let's say we got tens of hundreds of terabytes of data we can use something like snowball",
    "start": "1088740",
    "end": "1095370"
  },
  {
    "text": "so snowball is a portable storage device we can pack up data into snowballs we'll",
    "start": "1095370",
    "end": "1100680"
  },
  {
    "text": "ship them over to AWS and we can hydrate our data like that way if you're one of",
    "start": "1100680",
    "end": "1106170"
  },
  {
    "text": "those organizations that have hundreds of petabytes I'm kind of curious does anyone have more than like 20",
    "start": "1106170",
    "end": "1112680"
  },
  {
    "text": "petabytes of data here ok relatively rare but if anyone on twitch has you know a lot of data there's some",
    "start": "1112680",
    "end": "1119730"
  },
  {
    "text": "organizations that do have hundreds of pet advice or exabytes of data some of",
    "start": "1119730",
    "end": "1124800"
  },
  {
    "text": "you may recall back in dream in 2017 when we drove that semi-truck onstage was anyone there",
    "start": "1124800",
    "end": "1130710"
  },
  {
    "text": "ok curiosity ok so you guys check it out keynote reinvent 2017 we drilled a big",
    "start": "1130710",
    "end": "1136530"
  },
  {
    "text": "semi-truck on stage that's AWS snowball right so if you want exascale transfer",
    "start": "1136530",
    "end": "1144390"
  },
  {
    "text": "we can have a fleet of snowball trucks right transfer your data over to AWS in",
    "start": "1144390",
    "end": "1151800"
  },
  {
    "text": "addition to migration in some cases you want to operate in a hybrid mode so you",
    "start": "1151800",
    "end": "1156990"
  },
  {
    "text": "can use Direct Connect which gives you a dedicated private connection between on-premise and the cloud and you can",
    "start": "1156990",
    "end": "1164730"
  },
  {
    "text": "send data over a 10 gigabit pipe which you can expand on a need by basis alternatively you may want to build like",
    "start": "1164730",
    "end": "1171570"
  },
  {
    "text": "a hybrid storage architecture some data in on-premise storage some data in an",
    "start": "1171570",
    "end": "1177120"
  },
  {
    "text": "AWS like s3 as something like AWS Storage Gateway can help you build that type of architecture so those are other",
    "start": "1177120",
    "end": "1183540"
  },
  {
    "text": "ways you can hydrate your data like in addition to migration and and batch right you're probably gonna",
    "start": "1183540",
    "end": "1190470"
  },
  {
    "text": "have some streaming workloads does anyone here have like streaming workloads and on things like Kafka and",
    "start": "1190470",
    "end": "1196130"
  },
  {
    "text": "maybe IOT a few folks right so if you",
    "start": "1196130",
    "end": "1201210"
  },
  {
    "text": "have streaming data sources you have many options including using magic Africa if it's IOT there's IOT platform",
    "start": "1201210",
    "end": "1208260"
  },
  {
    "text": "and then there's Kinesis data streams and Canisius video streams and Kinesis firehose as well for stream ingestion",
    "start": "1208260",
    "end": "1216350"
  },
  {
    "text": "so once we've hydrated our data lake we",
    "start": "1216350",
    "end": "1221850"
  },
  {
    "text": "want to cleanse and print cleanse and prep our catalog in a repeatable manner and this is a critical part in our data",
    "start": "1221850",
    "end": "1229890"
  },
  {
    "text": "Laker exercise so one of the things that analysts like Gartner is observing is a lot of organizations are suffering from",
    "start": "1229890",
    "end": "1238710"
  },
  {
    "text": "a problem called dark data essentially they're collecting a lot of data but they're becoming less effective in",
    "start": "1238710",
    "end": "1246000"
  },
  {
    "text": "deriving value from that data so one of the things that you don't want to fall victim to is dark data as you build your",
    "start": "1246000",
    "end": "1252330"
  },
  {
    "start": "1250000",
    "end": "1250000"
  },
  {
    "text": "data lake so glues gonna play a critical role in terms of ensuring your data is",
    "start": "1252330",
    "end": "1257850"
  },
  {
    "text": "discoverable right through cataloging and it's also going to provide cleansing",
    "start": "1257850",
    "end": "1263670"
  },
  {
    "text": "and prep to ensure your data remains at high quality so this slide deck shows",
    "start": "1263670",
    "end": "1269700"
  },
  {
    "text": "you the diagram here can gives you a sense of all the major components in glue and how they work together so at",
    "start": "1269700",
    "end": "1277050"
  },
  {
    "text": "the very bottom right you can you see like a simplistic ETL pipeline so you",
    "start": "1277050",
    "end": "1283679"
  },
  {
    "text": "have data that's flowing into your data like in s3 in raw form and you can then",
    "start": "1283679",
    "end": "1289380"
  },
  {
    "text": "trigger like a glue job that glue job may then run some cleansing routines",
    "start": "1289380",
    "end": "1295080"
  },
  {
    "text": "some basic transformation and it'll stage the data and back into s3 in your",
    "start": "1295080",
    "end": "1300240"
  },
  {
    "text": "data Lake right at that point you may then trigger off some additional glue jobs to transform the data into",
    "start": "1300240",
    "end": "1307800"
  },
  {
    "text": "optimized format for instance you may have a glue job that will partition the",
    "start": "1307800",
    "end": "1313380"
  },
  {
    "text": "data and convert the data to or or park' so that it's optimized for Athena or",
    "start": "1313380",
    "end": "1320760"
  },
  {
    "text": "rich spectrum and as you create those data sets and as those data sets come into",
    "start": "1320760",
    "end": "1326640"
  },
  {
    "text": "your data Lake you can run these crawlers right and these crawlers are gonna scan your data and discover these",
    "start": "1326640",
    "end": "1333570"
  },
  {
    "text": "data sets and catalogued them into the glued data catalog so that it's now",
    "start": "1333570",
    "end": "1339060"
  },
  {
    "text": "discoverable by users as well as downstream systems so as we build this",
    "start": "1339060",
    "end": "1346530"
  },
  {
    "start": "1345000",
    "end": "1345000"
  },
  {
    "text": "data Lake you can see that the crawlers are gonna play a critical role in helping us make our data discoverable",
    "start": "1346530",
    "end": "1353910"
  },
  {
    "text": "and in fact it's gonna help us automatically build our data catalog and keep the data catalog in sync so some of",
    "start": "1353910",
    "end": "1362220"
  },
  {
    "text": "the things that's gonna do is like we'll go in we're gonna configure to glue assert to glue crawlers we may set up a",
    "start": "1362220",
    "end": "1369540"
  },
  {
    "text": "schedule so they run on on a spot on a regular basis they'll scan our data lake",
    "start": "1369540",
    "end": "1375990"
  },
  {
    "text": "and when he discovered a data set they can infer the schema for us automatically if it's an existing data",
    "start": "1375990",
    "end": "1383310"
  },
  {
    "text": "set it can detect the changes and track schema version changes on our behalf",
    "start": "1383310",
    "end": "1390270"
  },
  {
    "text": "it might detect partitions for us automatically it can also classify the",
    "start": "1390270",
    "end": "1395490"
  },
  {
    "text": "file format for us you know is it JSON is it parque is a torque and as well",
    "start": "1395490",
    "end": "1401070"
  },
  {
    "text": "automatically derive things like data statistics on our behalf and it all",
    "start": "1401070",
    "end": "1407580"
  },
  {
    "text": "operates in a spirit of service so you only pay for the crawlers when they actually run so let's give you a",
    "start": "1407580",
    "end": "1415320"
  },
  {
    "start": "1415000",
    "end": "1415000"
  },
  {
    "text": "concrete example so let's say when we build our data Lake we start collecting",
    "start": "1415320",
    "end": "1421110"
  },
  {
    "text": "customer reviews from our website and we go configure a crawler and it's cancer",
    "start": "1421110",
    "end": "1426960"
  },
  {
    "text": "data Lake and it finds this data set so one of the things I'll do is as you can see on you know your right you can see",
    "start": "1426960",
    "end": "1433770"
  },
  {
    "text": "that there's a schema so it can infer the schema for you automatically and then you can see that it will then build",
    "start": "1433770",
    "end": "1443310"
  },
  {
    "text": "various you know statistics for you on your behalf so for example it detects",
    "start": "1443310",
    "end": "1448950"
  },
  {
    "text": "the location of the data set the format in this case is parque data statistics are things like the",
    "start": "1448950",
    "end": "1455290"
  },
  {
    "text": "number of Records the average record size and in the option to also add custom properties and in this case at",
    "start": "1455290",
    "end": "1461200"
  },
  {
    "text": "the very bottom you can see that it's also detected some partitions for us",
    "start": "1461200",
    "end": "1467220"
  },
  {
    "text": "right so now we have our data catalog set up we have these crawlers or keeping",
    "start": "1468930",
    "end": "1475900"
  },
  {
    "start": "1469000",
    "end": "1469000"
  },
  {
    "text": "it in sync now let's talk about the ETL component so the glue ETL component it's",
    "start": "1475900",
    "end": "1481930"
  },
  {
    "text": "built on top of apache spark who here has experience with using",
    "start": "1481930",
    "end": "1487270"
  },
  {
    "text": "apache spark ok many of you that's great so glue runs on top of that pachi spark",
    "start": "1487270",
    "end": "1493000"
  },
  {
    "text": "for those who are new to spark it's essentially an open source distributed data processing engine so what this",
    "start": "1493000",
    "end": "1501370"
  },
  {
    "text": "allows us to do is we can run our apache spark scripts in glue if we wanted to",
    "start": "1501370",
    "end": "1507730"
  },
  {
    "text": "and we can write two scripts in pi spark or spark scala in the end run dimin a",
    "start": "1507730",
    "end": "1513070"
  },
  {
    "text": "context of batch ETL and we can use the core constructs for example rdd's",
    "start": "1513070",
    "end": "1520600"
  },
  {
    "text": "resilient the rdd's as well as data frames and as well as run Apache spark",
    "start": "1520600",
    "end": "1528400"
  },
  {
    "text": "sequel as needed but you also have the option of leveraging the glue library",
    "start": "1528400",
    "end": "1535450"
  },
  {
    "text": "extensions for example there's dynamic frames that are available that are",
    "start": "1535450",
    "end": "1542220"
  },
  {
    "text": "included are better optimized for batch ETL and they include some helpful",
    "start": "1542220",
    "end": "1549010"
  },
  {
    "text": "utilities so for example let's say we needed to flatten some nested JSON data",
    "start": "1549010",
    "end": "1554290"
  },
  {
    "text": "so that we can load the data into a relational database we can use a dynamic data frame and run a relational eyes",
    "start": "1554290",
    "end": "1561130"
  },
  {
    "text": "transform to flatten that data for us and just simplify our scripting logics",
    "start": "1561130",
    "end": "1568830"
  },
  {
    "text": "there may be some cases where you may want to leverage best of both worlds so for instance you know again maybe ask",
    "start": "1568980",
    "end": "1576400"
  },
  {
    "start": "1569000",
    "end": "1569000"
  },
  {
    "text": "some JSON data and I want to leverage these dynamic frames to flatten my JSON data but I also want to do some complex",
    "start": "1576400",
    "end": "1583600"
  },
  {
    "text": "transformations where I want to leverage sparks equal so there's utilities here where you",
    "start": "1583600",
    "end": "1588999"
  },
  {
    "text": "can transform between the patchy data frames and dynamic frames in glue so",
    "start": "1588999",
    "end": "1594399"
  },
  {
    "text": "that you can leverage you know best of both worlds now if that doesn't provide",
    "start": "1594399",
    "end": "1599919"
  },
  {
    "text": "you all the capability that you need there's also the option for you to import custom libraries so once we",
    "start": "1599919",
    "end": "1610479"
  },
  {
    "start": "1610000",
    "end": "1610000"
  },
  {
    "text": "created our scripts we can then run them in glue and there's gonna be scenarios where you want to run incremental jobs",
    "start": "1610479",
    "end": "1618459"
  },
  {
    "text": "right incremental processing and load and this is where we want to configure job bookmarks for those use cases so job",
    "start": "1618459",
    "end": "1626319"
  },
  {
    "text": "bookmarks essentially allow us to keep track of previously processed data the",
    "start": "1626319",
    "end": "1632379"
  },
  {
    "text": "second thing we want to do is avoid strip proliferation right some of you",
    "start": "1632379",
    "end": "1639699"
  },
  {
    "text": "may have very complex data ecosystems you know I've met customers who have hundreds of databases it's possible that",
    "start": "1639699",
    "end": "1646059"
  },
  {
    "text": "you can have a database that has over a thousand tables out of curiosity does anyone is anyone in that situation ok",
    "start": "1646059",
    "end": "1653829"
  },
  {
    "text": "but in in that situation on all those tables could be data sources and the",
    "start": "1653829",
    "end": "1660189"
  },
  {
    "text": "last thing you want to do is write a script for every slight variant so it's",
    "start": "1660189",
    "end": "1666219"
  },
  {
    "text": "important to support reuse of our ETL scripts so there's some things that you can do you can build job workflows where",
    "start": "1666219",
    "end": "1673989"
  },
  {
    "text": "each of the job represents a logical unit of work and you can have like a ETL",
    "start": "1673989",
    "end": "1680349"
  },
  {
    "text": "script triggered downstream in a workflow for reuse as well we support",
    "start": "1680349",
    "end": "1687039"
  },
  {
    "text": "the ability to parameterize your scripts so you can create more generic scripts",
    "start": "1687039",
    "end": "1692169"
  },
  {
    "text": "and launch these jobs with parameters as well to again facilitate reuse so let's",
    "start": "1692169",
    "end": "1701409"
  },
  {
    "start": "1701000",
    "end": "1701000"
  },
  {
    "text": "talk a little bit more about the job workflow so we mentioned earlier in the Robinhood use case that did use Apache",
    "start": "1701409",
    "end": "1708099"
  },
  {
    "text": "airflow and there was a question right that yes there's you could do that you",
    "start": "1708099",
    "end": "1714069"
  },
  {
    "text": "can also use a DW step functions but if you're only operating in a scope of glue",
    "start": "1714069",
    "end": "1720039"
  },
  {
    "text": "it might be easier for you to just use the box job workflow capabilities in",
    "start": "1720039",
    "end": "1725540"
  },
  {
    "text": "glue so for instance you see in the diagram here imagine we want to create a",
    "start": "1725540",
    "end": "1731180"
  },
  {
    "text": "job workflow where we have new data that's being added into our data lake at",
    "start": "1731180",
    "end": "1739820"
  },
  {
    "text": "night and then we want to launch a data cleansing job in the morning so we can",
    "start": "1739820",
    "end": "1746690"
  },
  {
    "text": "we can create a schedule trigger for that job to run click cleanse the data",
    "start": "1746690",
    "end": "1751820"
  },
  {
    "text": "and create a data set for us once that job ends we can have a trigger on that",
    "start": "1751820",
    "end": "1758540"
  },
  {
    "text": "job the job succeeds it can launch let's say a handful of glue jobs some of the",
    "start": "1758540",
    "end": "1765410"
  },
  {
    "text": "glue jobs may partition data convert it into parquet so it's optimized for Athena you may have another glue job",
    "start": "1765410",
    "end": "1772730"
  },
  {
    "text": "which transforms the data into like RDF or gremlin so that you can load it into a graph database like Neptune for graph",
    "start": "1772730",
    "end": "1779960"
  },
  {
    "text": "traversal and analysis right so that's an example of some of the job workflows",
    "start": "1779960",
    "end": "1785510"
  },
  {
    "text": "that you can create with the uh total box capabilities in glue so essence you",
    "start": "1785510",
    "end": "1791000"
  },
  {
    "text": "can create triggers based on schedules on demand it can be triggered by job events whether the previous jobs",
    "start": "1791000",
    "end": "1798410"
  },
  {
    "text": "two-seated or failed and you can configure things like retries in time out ok so now we've created this data",
    "start": "1798410",
    "end": "1808370"
  },
  {
    "start": "1806000",
    "end": "1806000"
  },
  {
    "text": "lake and it's like a well-tuned machine right where we have these crawlers that are keeping our data catalog up to date",
    "start": "1808370",
    "end": "1813980"
  },
  {
    "text": "we have these ETL processes that are not automated through job workflows how do",
    "start": "1813980",
    "end": "1819830"
  },
  {
    "text": "we know that everything is working well and how do we know when there is a problem well we can do this by going",
    "start": "1819830",
    "end": "1826520"
  },
  {
    "text": "into the glue console where we have cloud watch dashboards and metrics",
    "start": "1826520",
    "end": "1834140"
  },
  {
    "text": "available for us so for example glue provides metrics like memory usage bytes",
    "start": "1834140",
    "end": "1840440"
  },
  {
    "text": "in and out bytes shuffled the number of executors that are needed to execute your glue scripts and this allows us to",
    "start": "1840440",
    "end": "1848570"
  },
  {
    "text": "do things like debug how-to memory issues detect data SKU problems as well",
    "start": "1848570",
    "end": "1854360"
  },
  {
    "text": "as the tech Trinity's for optimizing your scripts and your job configurations right and",
    "start": "1854360",
    "end": "1861529"
  },
  {
    "text": "just like basic cloud watch right you can set up alarms on these metrics so",
    "start": "1861529",
    "end": "1867320"
  },
  {
    "text": "for example we may want to have a notification when a job fails all right",
    "start": "1867320",
    "end": "1877600"
  },
  {
    "text": "so the next step that we want to take in building our data lake is to secure our",
    "start": "1877600",
    "end": "1884720"
  },
  {
    "text": "data like so imagine a scenario maybe we want to run your customer analytics a",
    "start": "1884720",
    "end": "1892129"
  },
  {
    "start": "1886000",
    "end": "1886000"
  },
  {
    "text": "customer analytics workload on our data Lake quite possibly we may have PII data",
    "start": "1892129",
    "end": "1900529"
  },
  {
    "text": "right in that we want to analyze so we want to make sure we lock down that data",
    "start": "1900529",
    "end": "1906740"
  },
  {
    "text": "leak so the first thing we want to do is lock down the data storage which is s3",
    "start": "1906740",
    "end": "1914629"
  },
  {
    "text": "and I think many of you may have already done this before essentially what we want to do is we want to create IM",
    "start": "1914629",
    "end": "1920330"
  },
  {
    "text": "policies right where we limit users and roles in terms of the minimal access",
    "start": "1920330",
    "end": "1927799"
  },
  {
    "text": "that they should have right just you only have access to certain buckets and objects and we won't complement that",
    "start": "1927799",
    "end": "1933169"
  },
  {
    "text": "with bucket policies so those are resource level permissions that we want",
    "start": "1933169",
    "end": "1940250"
  },
  {
    "text": "to use to limit access within our data like we also want to employ the proper",
    "start": "1940250",
    "end": "1946820"
  },
  {
    "text": "encryption strategy as well sometimes it's convenient to use you know s3",
    "start": "1946820",
    "end": "1953659"
  },
  {
    "text": "managed server-side encryption but as you know s3 has also many other encryption strategies sometimes you may",
    "start": "1953659",
    "end": "1960019"
  },
  {
    "text": "want to use your KMS with s3 because for compliance reasons you want a separation",
    "start": "1960019",
    "end": "1966470"
  },
  {
    "text": "of concerns where you want to have your security team manager encryption keys but your data leak and s3 administrators",
    "start": "1966470",
    "end": "1973100"
  },
  {
    "text": "manage access on the data like so you need to think about what would be the right encryption strategy based on your",
    "start": "1973100",
    "end": "1980779"
  },
  {
    "text": "needs and lastly we want to employ best practices",
    "start": "1980779",
    "end": "1985910"
  },
  {
    "text": "around tagging so we might want to use like for example we want to tag our",
    "start": "1985910",
    "end": "1992990"
  },
  {
    "text": "objects with our classification level so if we FBI they want to maybe tag it as",
    "start": "1992990",
    "end": "1999320"
  },
  {
    "text": "PII and what that lets us do is scope down our I am and resource policies so",
    "start": "1999320",
    "end": "2006850"
  },
  {
    "text": "that we can for example in our rules create like a deny on a deny rule for",
    "start": "2006850",
    "end": "2015490"
  },
  {
    "text": "access to any objects that have been tagged as PII okay so in addition to",
    "start": "2015490",
    "end": "2023980"
  },
  {
    "start": "2022000",
    "end": "2022000"
  },
  {
    "text": "securing our data we also need to secure the data catalog and this is where glue",
    "start": "2023980",
    "end": "2030460"
  },
  {
    "text": "provides you similar to s3 right I am policies to control what users and roles",
    "start": "2030460",
    "end": "2036420"
  },
  {
    "text": "can can modify and view the catalog as well as resource based policies where we",
    "start": "2036420",
    "end": "2044080"
  },
  {
    "text": "can apply you know the same permissions on the catalog objects so for example",
    "start": "2044080",
    "end": "2049960"
  },
  {
    "start": "2049000",
    "end": "2049000"
  },
  {
    "text": "here this diagram shows the different objects in the catalog we have databases we have connections and within databases",
    "start": "2049960",
    "end": "2057250"
  },
  {
    "text": "we have tables and user-defined functions so glue allows you to define",
    "start": "2057250",
    "end": "2062800"
  },
  {
    "text": "fine-grain access controls where you can specify you know who has the rights to access things at the catalog level the",
    "start": "2062800",
    "end": "2069908"
  },
  {
    "text": "database level as well as the table level okay alright so now that our data",
    "start": "2069909",
    "end": "2078908"
  },
  {
    "text": "link is secure we are now ready to share the data with the rest of the organization right for example for",
    "start": "2078909",
    "end": "2086408"
  },
  {
    "text": "analytics and glue makes this relatively easy through its integration between the",
    "start": "2086409",
    "end": "2094360"
  },
  {
    "start": "2087000",
    "end": "2087000"
  },
  {
    "text": "group data catalog and services like Athena and redshift spectrum so through",
    "start": "2094360",
    "end": "2100810"
  },
  {
    "text": "Athena resh of spectrum you can expose the data like assets in a form of",
    "start": "2100810",
    "end": "2106990"
  },
  {
    "text": "databases and tables and then downstream systems like business intelligence tools you now have visibility into those data",
    "start": "2106990",
    "end": "2115060"
  },
  {
    "text": "sets and use Athena and redshift query dot data right likewise in EMR you can use the",
    "start": "2115060",
    "end": "2125730"
  },
  {
    "text": "glue catalog in a place of the hive meta store so imagine like a machine-learning use case you can create a",
    "start": "2125730",
    "end": "2132930"
  },
  {
    "text": "machine-learning pipeline where you extract data from the data lake through the catalog and then you can create a",
    "start": "2132930",
    "end": "2140190"
  },
  {
    "text": "pipeline which processes data adduced us and feature engineering and maybe integrate with our machine learning",
    "start": "2140190",
    "end": "2146460"
  },
  {
    "text": "platform Sage Maker to build and train a machine learning model alright and",
    "start": "2146460",
    "end": "2155250"
  },
  {
    "start": "2154000",
    "end": "2154000"
  },
  {
    "text": "that's all it takes with help of glue to put together a strong foundation for a data Lake so let's kind of recap what we",
    "start": "2155250",
    "end": "2162690"
  },
  {
    "text": "built so we have a bunch of different data sources data is coming in both batch streaming we have migration",
    "start": "2162690",
    "end": "2169500"
  },
  {
    "text": "scenarios and we have mechanisms to ingest data into AWS and into the data",
    "start": "2169500",
    "end": "2174930"
  },
  {
    "text": "Lake we then use glue to cleanse the",
    "start": "2174930",
    "end": "2180089"
  },
  {
    "text": "data and prep the data in a reliable and repeatable way and we have these crawlers that can run automatic vo as",
    "start": "2180089",
    "end": "2187980"
  },
  {
    "text": "well to detect these data sets and catalog the data for us and once the data is catalog then you can through the",
    "start": "2187980",
    "end": "2196170"
  },
  {
    "text": "integration with Athena a redshift spectrum we make it easy for our end",
    "start": "2196170",
    "end": "2201630"
  },
  {
    "text": "users to access the data like assets through business intelligence tools this",
    "start": "2201630",
    "end": "2206700"
  },
  {
    "text": "was machine learning platforms",
    "start": "2206700",
    "end": "2210050"
  },
  {
    "text": "so the final topic I want to touch on is lake formation so out of curiosity has",
    "start": "2216309",
    "end": "2224119"
  },
  {
    "text": "anyone heard of lake formation okay just a few folks so in Rio in 2018 we",
    "start": "2224119",
    "end": "2230839"
  },
  {
    "text": "announced that we were going to release a new service called lake formation sometime this year and the intention was",
    "start": "2230839",
    "end": "2236809"
  },
  {
    "text": "to make it easier for customers to build and manage data lakes on AWS think of",
    "start": "2236809",
    "end": "2246229"
  },
  {
    "text": "lake formation as a service that's going to complement glue in fact it's gonna leverage glue heavily and it's going to",
    "start": "2246229",
    "end": "2253849"
  },
  {
    "text": "augment and build on the concepts that we describe today so some of the",
    "start": "2253849",
    "end": "2259009"
  },
  {
    "start": "2258000",
    "end": "2258000"
  },
  {
    "text": "capabilities is going to provide is the concept of blueprints so essentially the",
    "start": "2259009",
    "end": "2264170"
  },
  {
    "text": "idea is there's a lot of out-of-the-box ETL capabilities that we can provide for our customers right we don't want all of",
    "start": "2264170",
    "end": "2270289"
  },
  {
    "text": "you guys reinventing the wheel on the first release of lake formation we're going to provide blueprints to simplify",
    "start": "2270289",
    "end": "2277279"
  },
  {
    "text": "the import of data into your data Lake so we're gonna provide the ability to",
    "start": "2277279",
    "end": "2283400"
  },
  {
    "text": "extract data from a relational database sources like from RDS or raw for example and load data into your data Lake in the",
    "start": "2283400",
    "end": "2291259"
  },
  {
    "text": "form of one shot or incremental we're",
    "start": "2291259",
    "end": "2297739"
  },
  {
    "start": "2296000",
    "end": "2296000"
  },
  {
    "text": "also going to release a capability called ml transforms in Glu along with",
    "start": "2297739",
    "end": "2303529"
  },
  {
    "text": "lake formation and the first use case that ml transfer is going to support is",
    "start": "2303529",
    "end": "2308599"
  },
  {
    "text": "around deduplication so how it's going to work is you can train glue to learn",
    "start": "2308599",
    "end": "2315170"
  },
  {
    "text": "what the you know how to detect duplicates in your data sources so it",
    "start": "2315170",
    "end": "2321199"
  },
  {
    "text": "could then do you know fuzzy matching on your behalf detect duplicates for you automatically",
    "start": "2321199",
    "end": "2327049"
  },
  {
    "text": "and again ultimately make it easier for you to main high data quality within",
    "start": "2327049",
    "end": "2332089"
  },
  {
    "text": "your data Lake",
    "start": "2332089",
    "end": "2334660"
  },
  {
    "start": "2336000",
    "end": "2336000"
  },
  {
    "text": "we're also going to improve data governance by providing a central",
    "start": "2337539",
    "end": "2342589"
  },
  {
    "text": "administrative interface for lake formation so if you recall when we're",
    "start": "2342589",
    "end": "2348410"
  },
  {
    "text": "going through the steps of building a data lake we talked about hey we need to",
    "start": "2348410",
    "end": "2353689"
  },
  {
    "text": "secure s3 right we need to then secure the data catalog but it's actually not",
    "start": "2353689",
    "end": "2359299"
  },
  {
    "text": "that simple as your data Lake gets more sophisticated because you may use redshift you may use Athena you may use",
    "start": "2359299",
    "end": "2366349"
  },
  {
    "text": "EMR as you know you know they all have their own i''m controls that you need to",
    "start": "2366349",
    "end": "2371900"
  },
  {
    "text": "configure so as your data lake grows as your users diversify and they use",
    "start": "2371900",
    "end": "2377239"
  },
  {
    "text": "different tools it's a lot administrative overhead and that's that's a problem that we want to fix so",
    "start": "2377239",
    "end": "2385069"
  },
  {
    "text": "lake formation is going to simplify that issue by allowing data Lake",
    "start": "2385069",
    "end": "2390469"
  },
  {
    "text": "administrators to control user access centrally and those user access controls",
    "start": "2390469",
    "end": "2397400"
  },
  {
    "text": "will then take into effect across the AWS services so if a user Dan uses",
    "start": "2397400",
    "end": "2404089"
  },
  {
    "text": "redshift or EMR or sage maker to access the data sets then we're going to have",
    "start": "2404089",
    "end": "2410559"
  },
  {
    "text": "lake formations user access controls take into effect so you can secure once",
    "start": "2410559",
    "end": "2416209"
  },
  {
    "text": "in an enforce access in multiple ways",
    "start": "2416209",
    "end": "2421599"
  },
  {
    "text": "and another big enhancement is around searchability and collaboration on a",
    "start": "2425510",
    "end": "2432420"
  },
  {
    "text": "data catalog so some of the things we like to do is allow users to add custom",
    "start": "2432420",
    "end": "2438690"
  },
  {
    "text": "business metadata where we talk about how crawlers can automatically add technical metadata for you but our",
    "start": "2438690",
    "end": "2445079"
  },
  {
    "text": "customers also want to add you know business metadata to catalog they want to make the catalog more searchable",
    "start": "2445079",
    "end": "2452430"
  },
  {
    "text": "right as well as searchable through the custom metadata that they create so we're gonna probably better full text",
    "start": "2452430",
    "end": "2458640"
  },
  {
    "text": "search capabilities faceted search as well to make again your data assets more",
    "start": "2458640",
    "end": "2465630"
  },
  {
    "text": "discoverable so I wanted to wrap things",
    "start": "2465630",
    "end": "2474359"
  },
  {
    "text": "up but just giving folks a couple of helpful tips that my customers found useful as they were going through the",
    "start": "2474359",
    "end": "2480540"
  },
  {
    "text": "data Lake implementation process so first off you know we talked about some",
    "start": "2480540",
    "end": "2487380"
  },
  {
    "start": "2483000",
    "end": "2483000"
  },
  {
    "text": "encryption options right we're aware that s3 provides a number of encryption strategies but be aware that the data",
    "start": "2487380",
    "end": "2495990"
  },
  {
    "text": "catalog and metadata and some of the glue components are not encrypted by default so there are additional",
    "start": "2495990",
    "end": "2501869"
  },
  {
    "text": "encryption options that you may want to enable so such as the metadata in the",
    "start": "2501869",
    "end": "2507690"
  },
  {
    "text": "glue catalog as well as the connection information that you store in the catalog down to the cloud watch logs",
    "start": "2507690",
    "end": "2514530"
  },
  {
    "text": "that are generated by glue those can be all encrypted but you need to make sure you enable them you know secondly you",
    "start": "2514530",
    "end": "2525059"
  },
  {
    "text": "take advantage of server lists right not just think about optimizing on cost but think about optimizing both time and",
    "start": "2525059",
    "end": "2532460"
  },
  {
    "text": "costs so here's a screenshot of one of the one of the charts that are provided",
    "start": "2532460",
    "end": "2540930"
  },
  {
    "text": "by Glu what does what this is showing us is this is a run of a Glu job and in this",
    "start": "2540930",
    "end": "2547799"
  },
  {
    "text": "case it took six minutes to run and we've allocated 17 executors this is",
    "start": "2547799",
    "end": "2556340"
  },
  {
    "text": "the resources that was allocated to this job and you can see that from this chart",
    "start": "2556340",
    "end": "2563660"
  },
  {
    "text": "what it shows us is that the script actually needed a lot more executors so these executors for those aren't",
    "start": "2563660",
    "end": "2570110"
  },
  {
    "text": "familiar with Apache spark essentially they're the workers right allow us to run tasks in parallel so because the",
    "start": "2570110",
    "end": "2577010"
  },
  {
    "text": "allocated executors is less than the needed executors there's a lot of your",
    "start": "2577010",
    "end": "2583820"
  },
  {
    "text": "pending tasks and if we added more resources there's a potential of this",
    "start": "2583820",
    "end": "2589820"
  },
  {
    "text": "job I'm completing faster if we take the extreme case where we allocated enough",
    "start": "2589820",
    "end": "2596270"
  },
  {
    "text": "resources for a hundred and seven executors right the you know if you look at the chart here at peak we needed a",
    "start": "2596270",
    "end": "2602900"
  },
  {
    "text": "hundred and seven executors if we did that you can see that the glue job would run in half the time in three minutes",
    "start": "2602900",
    "end": "2610960"
  },
  {
    "text": "okay so that saves us on time but it actually ended up costing more because",
    "start": "2610960",
    "end": "2616370"
  },
  {
    "text": "we allocated around six times more resources even though we cut down time but there's probably a seat sweet spot",
    "start": "2616370",
    "end": "2622880"
  },
  {
    "text": "somewhere right maybe if we allocated enough resources for 20 or 30 executors maybe this is a sweet spot somewhere",
    "start": "2622880",
    "end": "2629660"
  },
  {
    "text": "where we can optimize both time and cost so make sure you leverage these cloud",
    "start": "2629660",
    "end": "2635690"
  },
  {
    "text": "watch metrics to help you optimize your glue jobs and this is where you can optimize on both time and costs the",
    "start": "2635690",
    "end": "2642890"
  },
  {
    "start": "2641000",
    "end": "2641000"
  },
  {
    "text": "final tip and this is a new capability that it's just released a couple of weeks ago is glue now supports resource",
    "start": "2642890",
    "end": "2649340"
  },
  {
    "text": "tagging so you can tag your dev endpoints you can tag your glue jobs source or crawlers like in this example",
    "start": "2649340",
    "end": "2656120"
  },
  {
    "text": "here I have a tag that indicates these are resources owned by the development",
    "start": "2656120",
    "end": "2661430"
  },
  {
    "text": "team and this is gonna pay dividends in terms of cost governance so I can create cost allocation tags and it can now see",
    "start": "2661430",
    "end": "2668510"
  },
  {
    "text": "which teams are owns and spends what resources like what's terraspin as well",
    "start": "2668510",
    "end": "2675290"
  },
  {
    "text": "as again provide better security so with these tags I can create conditions in my i.m policies and",
    "start": "2675290",
    "end": "2682250"
  },
  {
    "text": "resource policies to you know limit for example the dev team should only be able to run jobs on on",
    "start": "2682250",
    "end": "2692450"
  },
  {
    "text": "resources are a tagged by the dev team right and ultimately if any of these",
    "start": "2692450",
    "end": "2699690"
  },
  {
    "text": "resources or any of the entities are compromised again we can limit blast radius so that's the end of the",
    "start": "2699690",
    "end": "2710130"
  },
  {
    "text": "presentation and you know if we have any we have some time for some questions what's next is a glue workshop and if",
    "start": "2710130",
    "end": "2719700"
  },
  {
    "text": "you guys saw any questions feel free to take a break and thanks everyone on twitch for attending this presentation",
    "start": "2719700",
    "end": "2727490"
  },
  {
    "text": "[Applause]",
    "start": "2728950",
    "end": "2733589"
  },
  {
    "text": "[Music]",
    "start": "2738410",
    "end": "2741500"
  }
]