[
  {
    "start": "0",
    "end": "37000"
  },
  {
    "text": "cool welcome everybody we're gonna get started right on time so hopefully",
    "start": "350",
    "end": "7350"
  },
  {
    "text": "you're here to hear about how Twilio scaled it's data-driven culture if you're not now is your moment to leave",
    "start": "7350",
    "end": "12840"
  },
  {
    "text": "we won't judge you it's okay my name is Daniel Mintz I'm the chief data evangelist for looker I'm gonna do",
    "start": "12840",
    "end": "19380"
  },
  {
    "text": "a quick introduction and then I'm gonna hand it over to Darin Rahul who or who you really are here to hear from and",
    "start": "19380",
    "end": "25320"
  },
  {
    "text": "they're gonna tell you all about what they've done in deep detail that I know AWS reinvent folks love so oh that's me",
    "start": "25320",
    "end": "35910"
  },
  {
    "text": "let's go past that so how many people here have been doing like data stuff for",
    "start": "35910",
    "end": "42059"
  },
  {
    "start": "37000",
    "end": "142000"
  },
  {
    "text": "more than three years more than five",
    "start": "42059",
    "end": "47430"
  },
  {
    "text": "years more than 10 years all right I want embarrass people by",
    "start": "47430",
    "end": "52620"
  },
  {
    "text": "asking more than 10 years because that can get rough but for those of you who",
    "start": "52620",
    "end": "57629"
  },
  {
    "text": "haven't been doing this for a really long time one thing that you may not realize is that the world that you live in of like crazy fast crazy cheap cloud",
    "start": "57629",
    "end": "65489"
  },
  {
    "text": "databases is not a world that has existed for a very long time it's a pretty new world until pretty recently",
    "start": "65489",
    "end": "74780"
  },
  {
    "text": "databases were very slow they were very expensive they're very heavy they had to",
    "start": "74780",
    "end": "80549"
  },
  {
    "text": "like come in on an 18-wheeler into your data center and really everything that you decided to do with data and your",
    "start": "80549",
    "end": "86220"
  },
  {
    "text": "company was mostly driven by your desire to not spend another million dollars to buy another server to put in your data",
    "start": "86220",
    "end": "92610"
  },
  {
    "text": "center that is not the world we live in anymore right",
    "start": "92610",
    "end": "97710"
  },
  {
    "text": "the there's been just an amazing proliferation of incredibly fast databases everything starting from this",
    "start": "97710",
    "end": "104549"
  },
  {
    "text": "sort of Hadoop ecosystem 10 plus years ago through redshift which came out in",
    "start": "104549",
    "end": "110250"
  },
  {
    "text": "2012 now with a Cena and Aurora and you know I mean just like everything and you",
    "start": "110250",
    "end": "116280"
  },
  {
    "text": "know lots of big players in the space are building these incredible databases and that's a huge change and it really",
    "start": "116280",
    "end": "122369"
  },
  {
    "text": "changes the way that data can be used in the enterprise because it means that all these constraints that used to exist",
    "start": "122369",
    "end": "129379"
  },
  {
    "text": "have kind of fallen away now that's great it's really neat and gives us lots of",
    "start": "129379",
    "end": "136320"
  },
  {
    "text": "new opportunities but there is a problem which is that while the database technology has changed tremendously for",
    "start": "136320",
    "end": "142710"
  },
  {
    "start": "142000",
    "end": "188000"
  },
  {
    "text": "the most part the analytic tools that we're using and sitting on top of those databases mostly haven't changed and",
    "start": "142710",
    "end": "150120"
  },
  {
    "text": "much as putting saving your you know kind of worn out kind of old tires and",
    "start": "150120",
    "end": "156000"
  },
  {
    "text": "putting them on your new Tesla to save a little money you know you're not probably gonna get quite as much value",
    "start": "156000",
    "end": "163650"
  },
  {
    "text": "out of your investment as you might otherwise if you don't upgrade your",
    "start": "163650",
    "end": "169080"
  },
  {
    "text": "whole stack right if you're using an old analytic tool on these incredibly powerful new databases those tools we're",
    "start": "169080",
    "end": "175290"
  },
  {
    "text": "kind of built in a world of slow expensive databases and made a bunch of really smart compromises that maybe",
    "start": "175290",
    "end": "182490"
  },
  {
    "text": "aren't so smart anymore because of those changes so you know I work for looker",
    "start": "182490",
    "end": "189600"
  },
  {
    "start": "188000",
    "end": "227000"
  },
  {
    "text": "but before I worked for looker I was a customer of lookers for three and a half years when I ran data and analytics at",
    "start": "189600",
    "end": "196470"
  },
  {
    "text": "upworthy and you know looker is a tool that was built in this new world brought",
    "start": "196470",
    "end": "204150"
  },
  {
    "text": "public in 2013 and so really is native to this world and is constantly thinking",
    "start": "204150",
    "end": "209670"
  },
  {
    "text": "about and is architected for this world and and thinking about how do we get as much value as we can out of these",
    "start": "209670",
    "end": "215130"
  },
  {
    "text": "incredibly powerful cheap cloud databases so you know just to give you a sense of sort of what that looks like",
    "start": "215130",
    "end": "221400"
  },
  {
    "text": "and how maybe that differs from what kinds of analytic tools you might be used to using I'm gonna give you a quick",
    "start": "221400",
    "end": "227910"
  },
  {
    "start": "227000",
    "end": "246000"
  },
  {
    "text": "architecture of sort of how most companies data looks today so you know",
    "start": "227910",
    "end": "234270"
  },
  {
    "text": "most companies who we work with have a ton of different tools that are feeding",
    "start": "234270",
    "end": "240959"
  },
  {
    "text": "them data it's not one tool it's not a POS system and nothing else it's not you know just a transactional database it's",
    "start": "240959",
    "end": "247980"
  },
  {
    "start": "246000",
    "end": "280000"
  },
  {
    "text": "they have a bunch of SAS apps they have their transactional database maybe they have an ERP web analytics like there's",
    "start": "247980",
    "end": "253950"
  },
  {
    "text": "just a ton of tools and it used to be that that process of pulling all that data out of those systems transforming",
    "start": "253950",
    "end": "259950"
  },
  {
    "text": "it and loading it into your warehouse or your database was really hard and that was because the databases were slow and",
    "start": "259950",
    "end": "265830"
  },
  {
    "text": "so you had to sort of reshape it and get it in the right format before you can put in those databases it's not true",
    "start": "265830",
    "end": "272819"
  },
  {
    "text": "anymore the databases are fast and so we can kind of just dump stuff in them and then handle that with the databases and so",
    "start": "272819",
    "end": "279120"
  },
  {
    "text": "that's what we you know encourage folks to do and so the way that lucre works is it it you put your data in any sequel",
    "start": "279120",
    "end": "286469"
  },
  {
    "text": "database so that might be a transactional our DBMS like you know my sequel Postgres arora it might be",
    "start": "286469",
    "end": "292169"
  },
  {
    "text": "something like Amazon Athena where you're not actually even putting it in a database anymore you're just putting it in s3 and you know deploying Athena on",
    "start": "292169",
    "end": "300300"
  },
  {
    "text": "top of that it might be redshift spectrum where you're getting the best of both worlds you know enterprise grade",
    "start": "300300",
    "end": "305669"
  },
  {
    "text": "warehouse and s3 or it might be you know EMR using the Hadoop ecosystem and once",
    "start": "305669",
    "end": "312990"
  },
  {
    "text": "you've done that once you've put that data in one place what looker can do is liquor then sits on top and accesses",
    "start": "312990",
    "end": "319409"
  },
  {
    "text": "that data but it does so in a way that makes sure that everybody who's accessing the data whether the most",
    "start": "319409",
    "end": "325020"
  },
  {
    "text": "technical sophisticated DBA or the least technical business user is always",
    "start": "325020",
    "end": "330509"
  },
  {
    "text": "getting the same meaning from the data when they ask what is the lifetime customer value they're gonna get the",
    "start": "330509",
    "end": "335610"
  },
  {
    "text": "same answer because looker is in between them and the data and your analysts have gone in and said here is our definition",
    "start": "335610",
    "end": "341639"
  },
  {
    "text": "company-wide for a lifetime customer value and if anybody asks for that we're gonna make sure they get the same answer",
    "start": "341639",
    "end": "346710"
  },
  {
    "text": "so looker has this agile modeling layer which sits on top it also handles things like version control which is a great",
    "start": "346710",
    "end": "353099"
  },
  {
    "text": "thing that developers love but for some reason analytics have mostly not benefited from but looker gives you that so you converge and control your models",
    "start": "353099",
    "end": "360529"
  },
  {
    "text": "connections to these data bases user management security all that and once you've got looker there that's this big",
    "start": "360529",
    "end": "366449"
  },
  {
    "text": "broad platform what you can then do is access the data through any of options",
    "start": "366449",
    "end": "372029"
  },
  {
    "start": "367000",
    "end": "378000"
  },
  {
    "text": "so most people use our web interface you know using self-service exploration and our d3 visualizations you can export",
    "start": "372029",
    "end": "378960"
  },
  {
    "start": "378000",
    "end": "402000"
  },
  {
    "text": "that data to other tools if you need you can schedule delivery because lookers a platform it's a server it runs in the",
    "start": "378960",
    "end": "385169"
  },
  {
    "text": "cloud and so if you say hey rather than having to go get this I just want it in my inbox every Monday morning great no",
    "start": "385169",
    "end": "391379"
  },
  {
    "text": "problem you can set that up but you can also deliver things by a web hook or FTP or push it to s3 so you can access that",
    "start": "391379",
    "end": "398009"
  },
  {
    "text": "data wherever and however you need with look are pushing it to you you can also embed looker you know in",
    "start": "398009",
    "end": "404540"
  },
  {
    "text": "Salesforce or via iframe or JavaScript and looker because it's modern software built in you know 2012-2013 through now",
    "start": "404540",
    "end": "412560"
  },
  {
    "text": "has a full set of restful api so you can anything that you can do manually through the web interface you could also",
    "start": "412560",
    "end": "417600"
  },
  {
    "text": "do programmatically through an api and in terms of how looker works with with",
    "start": "417600",
    "end": "423660"
  },
  {
    "start": "420000",
    "end": "467000"
  },
  {
    "text": "AWS you know we have more than 600 joint customers more than 500 of them are using redshift redshift is an amazing",
    "start": "423660",
    "end": "431040"
  },
  {
    "text": "tool I started using it right when it came out and using looker right on top of it and it just they are two great tastes that taste great together it we",
    "start": "431040",
    "end": "439650"
  },
  {
    "text": "we support a whole host of other Amazon products like the RDS and an EMR and you",
    "start": "439650",
    "end": "448890"
  },
  {
    "text": "know where we have deep integrations and partnerships with Amazon you know lots",
    "start": "448890",
    "end": "454080"
  },
  {
    "text": "of customers are getting a ton of value from looker on their AWS products if you already are using AWS databases or data",
    "start": "454080",
    "end": "460980"
  },
  {
    "text": "engines I strongly advise you to give it a look I think you'll get even more value from the investment that you're",
    "start": "460980",
    "end": "466260"
  },
  {
    "text": "making and you know just to highlight two of those cases so one is Casper the",
    "start": "466260",
    "end": "472830"
  },
  {
    "start": "467000",
    "end": "495000"
  },
  {
    "text": "mattress company and you know they're using looker-on redshift to do everything from supply chain management",
    "start": "472830",
    "end": "478740"
  },
  {
    "text": "to understand you know where are the mattresses in the supply chain you know are they arriving as scheduled are",
    "start": "478740",
    "end": "484169"
  },
  {
    "text": "people getting them you know under their SL A's and then also all the e-commerce",
    "start": "484169",
    "end": "489330"
  },
  {
    "text": "analytics that you would expect a web native company like Casper to do so marketing analytics web analytics",
    "start": "489330",
    "end": "495560"
  },
  {
    "text": "another is Rovio the maker of Angry Birds you know they're allowing analysts",
    "start": "495560",
    "end": "501000"
  },
  {
    "text": "and game developers to collaborate in a way that they hadn't been able to before where where developers can build custom",
    "start": "501000",
    "end": "507750"
  },
  {
    "text": "metrics for their game while still leveraging the sort of core game metrics ma use da use that every game you know",
    "start": "507750",
    "end": "515250"
  },
  {
    "text": "at their game studio needs to have and they're allowing business users to self-serve rather than having to wait in",
    "start": "515250",
    "end": "522120"
  },
  {
    "text": "line for an analyst to have time to write a sequel query to give them the data that's just not an efficient way to",
    "start": "522120",
    "end": "528390"
  },
  {
    "text": "leverage these databases so in terms of Twilio and I'm very excited for you guys",
    "start": "528390",
    "end": "534930"
  },
  {
    "start": "530000",
    "end": "569000"
  },
  {
    "text": "to hear from them because brilliant and doing amazing stuff you know they have been using looker on",
    "start": "534930",
    "end": "540870"
  },
  {
    "text": "Amazon for three plus years and in that time have grown tremendously and how many people are actually using it and",
    "start": "540870",
    "end": "546510"
  },
  {
    "text": "that's you know a real challenge it's a new thing for you know hundreds of people thousands people across an",
    "start": "546510",
    "end": "552720"
  },
  {
    "text": "organization to have real deep access to data not just dashboards and reports but",
    "start": "552720",
    "end": "558600"
  },
  {
    "text": "the ability to ask at how questions to find the answers and to know that they're getting trustworthy answers so",
    "start": "558600",
    "end": "564750"
  },
  {
    "text": "you know they're running hundreds of thousands of queries each month to drive all across their business so without",
    "start": "564750",
    "end": "570540"
  },
  {
    "start": "569000",
    "end": "593000"
  },
  {
    "text": "further ado I'm gonna give you a Dhara Patel and Rahul chandell who are going to talk all about how Twilio is using looker and Amazon yeah so my name is",
    "start": "570540",
    "end": "580230"
  },
  {
    "text": "Terra and I'm leading the business intelligence division which consists of the data engineering and analytics",
    "start": "580230",
    "end": "586079"
  },
  {
    "text": "functions at to you and I have Rahul with me who is a tech lead for our data",
    "start": "586079",
    "end": "591540"
  },
  {
    "text": "engineering group so in today's session we are going to talk about how we scaled",
    "start": "591540",
    "end": "597720"
  },
  {
    "start": "593000",
    "end": "647000"
  },
  {
    "text": "our analytics platform and services to fuel the data driven culture at Twilio",
    "start": "597720",
    "end": "602820"
  },
  {
    "text": "so I'll start with the introduction of Twilio and we'll talk about who we are",
    "start": "602820",
    "end": "608190"
  },
  {
    "text": "what we do then I'll talk about the analytics at video so we'll talk about what the culture looks like what are the",
    "start": "608190",
    "end": "615690"
  },
  {
    "text": "processes that we use for analytics after that I'll hand it over to Rahul to",
    "start": "615690",
    "end": "620760"
  },
  {
    "text": "talk about the evolution of our analytics architecture and in the second half I'll talk about how we use Luca and",
    "start": "620760",
    "end": "628860"
  },
  {
    "text": "Twilio and then I'll give you some tips and tricks to handle the performance at",
    "start": "628860",
    "end": "634320"
  },
  {
    "text": "scale specifically for two of the main systems that we use for analytics and",
    "start": "634320",
    "end": "639959"
  },
  {
    "text": "then I will conclude the talk by giving some business insights that we are able to provide using our platform",
    "start": "639959",
    "end": "646279"
  },
  {
    "text": "all right so for those of you who are not familiar with to you we provide",
    "start": "646279",
    "end": "652440"
  },
  {
    "start": "647000",
    "end": "729000"
  },
  {
    "text": "cloud communications platform so there are four parts of the Twilio platform the first is the super Network we are we",
    "start": "652440",
    "end": "660870"
  },
  {
    "text": "handle the interconnectivity with all the carriers and this is how you get instant access to phone numbers short",
    "start": "660870",
    "end": "667740"
  },
  {
    "text": "codes voice and wireless connectivity across the world the second is the programmable",
    "start": "667740",
    "end": "673180"
  },
  {
    "text": "communications Club which consists of the communications API is like SMS wise",
    "start": "673180",
    "end": "678970"
  },
  {
    "text": "video chat etc it also includes intelligent services like scaling your",
    "start": "678970",
    "end": "685240"
  },
  {
    "text": "application globally or extracting intelligence from text and calls together performance insights third is",
    "start": "685240",
    "end": "693880"
  },
  {
    "text": "the engagement cloud which is which consists of the higher level software api's that lets you build multi-channel",
    "start": "693880",
    "end": "701680"
  },
  {
    "text": "communications between your systems departments and customers and some of",
    "start": "701680",
    "end": "707080"
  },
  {
    "text": "the examples in this cloud are notify or task router and finally there is a",
    "start": "707080",
    "end": "712660"
  },
  {
    "text": "runtime developer experience program which saves your team time and effort to",
    "start": "712660",
    "end": "718330"
  },
  {
    "text": "build and prototype your Twilio power application one such example is functions where you can build and deploy",
    "start": "718330",
    "end": "725800"
  },
  {
    "text": "your application without worrying about servers so now we know a little bit about video let me give you some stats",
    "start": "725800",
    "end": "732630"
  },
  {
    "start": "729000",
    "end": "761000"
  },
  {
    "text": "so we have around 1.6 million developers on our platform who uses more than 50",
    "start": "732630",
    "end": "739000"
  },
  {
    "text": "api's across all our services we believe in continuous delivery to our customers",
    "start": "739000",
    "end": "745360"
  },
  {
    "text": "and release new feature every three and a half days today Twilio dot org",
    "start": "745360",
    "end": "751210"
  },
  {
    "text": "supports more than thousand nonprofit organizations and in last one year we",
    "start": "751210",
    "end": "756880"
  },
  {
    "text": "had around twenty eight billion customer interactions on our platform so let's",
    "start": "756880",
    "end": "762040"
  },
  {
    "text": "talk about analytic set to you so analytic set video falls into two",
    "start": "762040",
    "end": "767890"
  },
  {
    "text": "main buckets data warehousing and data exploration the focus of data",
    "start": "767890",
    "end": "773950"
  },
  {
    "text": "warehousing is to create the single source of truth for data so it allows us",
    "start": "773950",
    "end": "779950"
  },
  {
    "text": "to create consistent and and correct use of data across video when it comes to",
    "start": "779950",
    "end": "787690"
  },
  {
    "text": "kpi's the cross-functional alignment is required across various business units",
    "start": "787690",
    "end": "792790"
  },
  {
    "text": "and stakeholders so it will you it is supported by the data governance initiative so they helped us standardize",
    "start": "792790",
    "end": "800830"
  },
  {
    "text": "the definitions create common key metrics one important",
    "start": "800830",
    "end": "806750"
  },
  {
    "text": "aspect of data warehousing is the integration of various data points so that we can drive deeper insights on the",
    "start": "806750",
    "end": "815810"
  },
  {
    "text": "other hand data exploration is all about finding the unknown it allows users to",
    "start": "815810",
    "end": "823070"
  },
  {
    "text": "explore who what when and why about our customers and products profiling and",
    "start": "823070",
    "end": "830779"
  },
  {
    "text": "discovery helps in establishing the correlations across disparate data sources and allows to answer any",
    "start": "830779",
    "end": "839200"
  },
  {
    "text": "open-ended questions it also allows users to create trends and behavioural",
    "start": "839200",
    "end": "846230"
  },
  {
    "text": "patterns using historical data as I mentioned earlier we release new feature",
    "start": "846230",
    "end": "852830"
  },
  {
    "text": "every three and a half days so rapid data set prototyping is key for us to",
    "start": "852830",
    "end": "859190"
  },
  {
    "text": "quickly build and measure our hypothesis self-service is a key aspect of our",
    "start": "859190",
    "end": "866720"
  },
  {
    "text": "analytics culture we truly believe in empowering our users to ask and answer",
    "start": "866720",
    "end": "872900"
  },
  {
    "text": "their questions with data at the same time we enables them to share their",
    "start": "872900",
    "end": "878870"
  },
  {
    "text": "insights with a team Department or even with an organization using amazing tools",
    "start": "878870",
    "end": "885380"
  },
  {
    "text": "like looker so so next let's talk about how do we prepare the data that enables",
    "start": "885380",
    "end": "893180"
  },
  {
    "start": "888000",
    "end": "1035000"
  },
  {
    "text": "to leon's make data-driven decisions so at a very high level there are five main",
    "start": "893180",
    "end": "899660"
  },
  {
    "text": "steps involved in any analytics process so the first step for us is to gather",
    "start": "899660",
    "end": "907160"
  },
  {
    "text": "the data act will you we have many internal and third-party data sources",
    "start": "907160",
    "end": "912529"
  },
  {
    "text": "from where we extract the data lot of this internal data comes in as the banks",
    "start": "912529",
    "end": "918770"
  },
  {
    "text": "and it arrives in real time on the other hand lot of the third-party data or data",
    "start": "918770",
    "end": "925370"
  },
  {
    "text": "arrives in few hourly or daily batches all of this data is disparate in nature",
    "start": "925370",
    "end": "932990"
  },
  {
    "text": "and some of them are also hi in volume so once we gather the data",
    "start": "932990",
    "end": "938370"
  },
  {
    "text": "the next step is to integrate and transform since we are using so many",
    "start": "938370",
    "end": "945460"
  },
  {
    "text": "different data sources it is important for us to integrate the data from one",
    "start": "945460",
    "end": "950500"
  },
  {
    "text": "system to the other to make it usable and also drive deeper insights",
    "start": "950500",
    "end": "956310"
  },
  {
    "text": "transformations are also required to generate aggregations or derive calculated values as per the business",
    "start": "956310",
    "end": "963490"
  },
  {
    "text": "need so once the data is transformed the next step is to make it available in the",
    "start": "963490",
    "end": "970510"
  },
  {
    "text": "queryable systems now a Tulio we host data in two different environments one",
    "start": "970510",
    "end": "977620"
  },
  {
    "text": "is our data Lake environment which suppose the data exploration type activities and second is our analytical",
    "start": "977620",
    "end": "985450"
  },
  {
    "text": "databases which supports the data warehousing type initiatives once the",
    "start": "985450",
    "end": "991270"
  },
  {
    "text": "data is loaded the appropriate access controls are applied so that users can",
    "start": "991270",
    "end": "996700"
  },
  {
    "text": "self-serve data needs and the final step in the process is to analyze and explore",
    "start": "996700",
    "end": "1004910"
  },
  {
    "text": "so at will you Luca is a one-stop shop for both of these activities so users",
    "start": "1004910",
    "end": "1012420"
  },
  {
    "text": "can build their models or run one-time sequel queries to to perform their",
    "start": "1012420",
    "end": "1018450"
  },
  {
    "text": "exploration activities or they can use the pre-built reports and dashboards to",
    "start": "1018450",
    "end": "1024510"
  },
  {
    "text": "drive insights so next I would like to invite Rahul to talk about the evolution",
    "start": "1024510",
    "end": "1029610"
  },
  {
    "text": "of our analytics architecture Thank You Tara as we all know rome wasn't built in",
    "start": "1029610",
    "end": "1038280"
  },
  {
    "start": "1035000",
    "end": "1056000"
  },
  {
    "text": "a day same story goes for us we have also evolved our architecture over a period of time and have gone through several",
    "start": "1038280",
    "end": "1044760"
  },
  {
    "text": "phases I will primarily focus on three main phases of our architecture and walk you",
    "start": "1044760",
    "end": "1050250"
  },
  {
    "text": "through the details of how we went from phase 1 to phase 3 so first component of",
    "start": "1050250",
    "end": "1059100"
  },
  {
    "start": "1056000",
    "end": "1168000"
  },
  {
    "text": "our architecture is the extraction of data from internal and external data sources I tell you most of our",
    "start": "1059100",
    "end": "1065730"
  },
  {
    "text": "operational data is stored in my sequel database in addition there are some third-party",
    "start": "1065730",
    "end": "1070830"
  },
  {
    "text": "data sources that we use like gainsight ilaqua and salesforce we have built processes",
    "start": "1070830",
    "end": "1076800"
  },
  {
    "text": "to extract data from these systems and load them into our data Lake environment which is s3 once the data is available",
    "start": "1076800",
    "end": "1085380"
  },
  {
    "text": "in s3 it is then loaded into redshift most of these are in-house programs that",
    "start": "1085380",
    "end": "1091290"
  },
  {
    "text": "use or CPI's let's talk about Salesforce as it's one of the most critical source",
    "start": "1091290",
    "end": "1096420"
  },
  {
    "text": "and consumer systems of data in addition to making salesforce data available in",
    "start": "1096420",
    "end": "1101460"
  },
  {
    "text": "red ship there was also a requirement from business to push our customer usage",
    "start": "1101460",
    "end": "1106590"
  },
  {
    "text": "and revenue related data into Salesforce so we had to establish a two-way",
    "start": "1106590",
    "end": "1112170"
  },
  {
    "text": "integration between these two systems we use bulk api's the pull and push data",
    "start": "1112170",
    "end": "1117480"
  },
  {
    "text": "between these two systems each API request returned a json formatted data",
    "start": "1117480",
    "end": "1123390"
  },
  {
    "text": "for which we identified a predefined data type which was mapped to a schema and then load it to a table in richard",
    "start": "1123390",
    "end": "1130020"
  },
  {
    "text": "as the volume was high we created files in s3 and then use copy commands to load",
    "start": "1130020",
    "end": "1136050"
  },
  {
    "text": "these datasets interaction when we were designing our systems we had only two rules in mind first that we must be able",
    "start": "1136050",
    "end": "1143610"
  },
  {
    "text": "to incrementally load these data datasets into red ship so we can do smaller updates and then second that we",
    "start": "1143610",
    "end": "1150240"
  },
  {
    "text": "must be able to modify these as admin needed and finally lucre is connected to",
    "start": "1150240",
    "end": "1155640"
  },
  {
    "text": "redshift where queries from reports and dashboards are executed as this was the",
    "start": "1155640",
    "end": "1161160"
  },
  {
    "text": "first iteration of our architecture we ran into some challenges and at the same time had new business demands so let me",
    "start": "1161160",
    "end": "1168840"
  },
  {
    "start": "1168000",
    "end": "1640000"
  },
  {
    "text": "talk about some of the challenges first as we were extracting everything from",
    "start": "1168840",
    "end": "1173850"
  },
  {
    "text": "our my sequel databases there was heavy load on our transactional systems we were using shared environment for",
    "start": "1173850",
    "end": "1180660"
  },
  {
    "text": "reporting and transformations which was read saved actually as customer interactions on our",
    "start": "1180660",
    "end": "1186540"
  },
  {
    "text": "platform brief we ran into issues handling massive data sets and at the same time we were not able to handle",
    "start": "1186540",
    "end": "1192900"
  },
  {
    "text": "disparate data formats and were only working with CSV and JSON file formats",
    "start": "1192900",
    "end": "1198500"
  },
  {
    "text": "scalability here was clearly an issue we were doing everything in one single platform while we were dealing with our",
    "start": "1198500",
    "end": "1205500"
  },
  {
    "text": "challenges there were new demands from business as we were loading anything and",
    "start": "1205500",
    "end": "1210660"
  },
  {
    "text": "everything in redshift there was no clear single source of truth as same metrics could be derived from multiple",
    "start": "1210660",
    "end": "1217170"
  },
  {
    "text": "data sets business intelligence was a problem as we could not we did not have",
    "start": "1217170",
    "end": "1223080"
  },
  {
    "text": "govern data sets and standardized reports and dashboards which which could be used by different business units Sox",
    "start": "1223080",
    "end": "1230910"
  },
  {
    "text": "compliance came in as a requirement as we were dealing with financial data sets there was also demand to add more and",
    "start": "1230910",
    "end": "1237390"
  },
  {
    "text": "more third-party data sources in our system so with all these we came up with",
    "start": "1237390",
    "end": "1242430"
  },
  {
    "text": "our phase 2 architecture to solve the problem of heavy load on our",
    "start": "1242430",
    "end": "1247470"
  },
  {
    "text": "transactional system we decided to introduce kafka data pipeline in our architecture for those of you are not",
    "start": "1247470",
    "end": "1253980"
  },
  {
    "text": "familiar with Kafka it's a distributed streaming platform that allows you to publish subscribe streams of records in",
    "start": "1253980",
    "end": "1260910"
  },
  {
    "text": "addition it allows you to store and process them as they occur we work with our engineering teams to capture these",
    "start": "1260910",
    "end": "1267720"
  },
  {
    "text": "data as events and then instead of extracting them from our my sequel databases we were directly getting them",
    "start": "1267720",
    "end": "1273810"
  },
  {
    "text": "from Kafka once the data is available in Kafka it is then loaded into s3 and then",
    "start": "1273810",
    "end": "1280890"
  },
  {
    "text": "it goes to redshift environment as of now we have more than 60 to 70% of our",
    "start": "1280890",
    "end": "1288750"
  },
  {
    "text": "events coming from Kafka in addition to aggregations we were writing complex",
    "start": "1288750",
    "end": "1294030"
  },
  {
    "text": "transformation logics in in redshift which would trigger an expensive query and would compete for cluster resources",
    "start": "1294030",
    "end": "1300270"
  },
  {
    "text": "against regular day-to-day queries and then we realized that we needed a scalable environment which could do our",
    "start": "1300270",
    "end": "1307230"
  },
  {
    "text": "transformations in a much better manner and so for this reason we chose Apache spark it's an open source cluster",
    "start": "1307230",
    "end": "1314070"
  },
  {
    "text": "computing framework that can process huge amount of datasets so once the data",
    "start": "1314070",
    "end": "1321240"
  },
  {
    "text": "is available in spa it's then loaded into redshift as of now if we have more",
    "start": "1321240",
    "end": "1326760"
  },
  {
    "text": "than 80% of our transformations running in spark it also provided us with pre-built libraries that we use to",
    "start": "1326760",
    "end": "1332580"
  },
  {
    "text": "connect to disparate data formats and that resolved our problem of handling only CSV and Jason's to meet",
    "start": "1332580",
    "end": "1339790"
  },
  {
    "text": "the business demand of Sox compliance and single source of truth we decided to segregate a red ship clusters into two",
    "start": "1339790",
    "end": "1346540"
  },
  {
    "text": "one withdraw and what will govern datasets which could be used for data exploration and business intelligence",
    "start": "1346540",
    "end": "1352240"
  },
  {
    "text": "practices one major enhancement that we did to our architecture was the",
    "start": "1352240",
    "end": "1358390"
  },
  {
    "text": "evolution of peer-to-peer third-party integration process into a configurable service that allowed us to onboard any",
    "start": "1358390",
    "end": "1365800"
  },
  {
    "text": "new third party integration quickly as we no longer had to worry about monitoring and failure management and",
    "start": "1365800",
    "end": "1372430"
  },
  {
    "text": "some other common functionalities that were provided by the service its in-house service written in Python it's",
    "start": "1372430",
    "end": "1378730"
  },
  {
    "text": "an abstraction on top of the api's well this worked for some time but as the demands from business grew we again",
    "start": "1378730",
    "end": "1385300"
  },
  {
    "text": "started running into some challenges so let me talk about those challenges first",
    "start": "1385300",
    "end": "1390930"
  },
  {
    "text": "as a platform evolved we started creating different services for specific",
    "start": "1391800",
    "end": "1398050"
  },
  {
    "text": "objectives which also involved running jobs inside these services monitoring",
    "start": "1398050",
    "end": "1403780"
  },
  {
    "text": "these jobs running across different services and handling dependencies between them was becoming a problem we",
    "start": "1403780",
    "end": "1409540"
  },
  {
    "text": "were using cron schedules and were heavily relying on time-based dependencies so it meant if a job took",
    "start": "1409540",
    "end": "1416080"
  },
  {
    "text": "longer than expected running on one service then it meant other jobs running on different services were executed",
    "start": "1416080",
    "end": "1422380"
  },
  {
    "text": "without enough information so we had to identify the bottleneck and read an impacted jobs which caused delays in",
    "start": "1422380",
    "end": "1429340"
  },
  {
    "text": "meeting our essays there were also some new demands from business like they were",
    "start": "1429340",
    "end": "1434590"
  },
  {
    "text": "asking for real-time operational insights and since we were running our SPARC loads in daily and hourly batch",
    "start": "1434590",
    "end": "1440950"
  },
  {
    "text": "fashion we are not able to provide these insights businesses at the same time started moving from reactive to",
    "start": "1440950",
    "end": "1447010"
  },
  {
    "text": "proactive analysis and we had to respond to events as they occurred there were",
    "start": "1447010",
    "end": "1452050"
  },
  {
    "text": "also requirements to support data explanation initiatives for data science so as the demand for real-time and",
    "start": "1452050",
    "end": "1459070"
  },
  {
    "text": "historical data increase we realize the need for a scalable approach to load these datasets interaction so with all",
    "start": "1459070",
    "end": "1466240"
  },
  {
    "text": "these we came up with our phase 3 architect first component of our architecture",
    "start": "1466240",
    "end": "1472780"
  },
  {
    "text": "remains same here where we use Kafka to swim all our Twilio internal data sets",
    "start": "1472780",
    "end": "1477910"
  },
  {
    "text": "as events and then extract them into our Amazon s3 data link one new component",
    "start": "1477910",
    "end": "1484120"
  },
  {
    "text": "that we added here was an a layer of spark energy for data science activities we also created a new loader service",
    "start": "1484120",
    "end": "1490840"
  },
  {
    "text": "which was configurable in a way that it could load any s3 datasets to any of these redshift environments it it's",
    "start": "1490840",
    "end": "1498190"
  },
  {
    "text": "in-house service written in Python it looks for conditional looks for new datasets in s3 and you can also",
    "start": "1498190",
    "end": "1505150"
  },
  {
    "text": "configure the service such as like table definitions or refresh cycles different",
    "start": "1505150",
    "end": "1512080"
  },
  {
    "text": "file formats which ultimately loads s3 datasets into appropriate redshift environments or third-party loader",
    "start": "1512080",
    "end": "1519610"
  },
  {
    "text": "service was also enhanced with new functionalities and it remains a critical component of our architecture",
    "start": "1519610",
    "end": "1526050"
  },
  {
    "text": "next major enhancement that we do to our architecture was the introduction of spark streaming sparks unified",
    "start": "1526050",
    "end": "1533230"
  },
  {
    "text": "programming model and a single programming model for batch and spark",
    "start": "1533230",
    "end": "1538570"
  },
  {
    "text": "letters to choose Park streaming it's an extension of the core spark API that lets us process huge volume of data sets",
    "start": "1538570",
    "end": "1544980"
  },
  {
    "text": "it allowed us to process events from Kafka in real time and then load them",
    "start": "1544980",
    "end": "1550420"
  },
  {
    "text": "into our s3 environment we now load medium sized data sets into redshift",
    "start": "1550420",
    "end": "1556600"
  },
  {
    "text": "instead of huge one that take hours to load the reason for this is that when we wanted to stream data into redshift in",
    "start": "1556600",
    "end": "1563020"
  },
  {
    "text": "real time we wanted to load them increment ly to solve our problem of",
    "start": "1563020",
    "end": "1569140"
  },
  {
    "text": "different jobs running across multiple services we decided to introduce a workflow management platform we chose",
    "start": "1569140",
    "end": "1576370"
  },
  {
    "text": "Apache airflow where you can define your workflow definitions programmatically",
    "start": "1576370",
    "end": "1581460"
  },
  {
    "text": "since we have multiple services running in our platform we chose a distributed architecture where each service was",
    "start": "1581460",
    "end": "1589000"
  },
  {
    "text": "augmented with an airflow worker and we keep our different services that work",
    "start": "1589000",
    "end": "1594970"
  },
  {
    "text": "below definitions sync using get ensure we use",
    "start": "1594970",
    "end": "1601060"
  },
  {
    "text": "with Redis broker for queue management and execute our task in distributed fashion we also use Postgres for",
    "start": "1601060",
    "end": "1608080"
  },
  {
    "text": "execution metadata and workflow definitions airflow also provides us with a web UI which is very helpful for",
    "start": "1608080",
    "end": "1615040"
  },
  {
    "text": "visualizing pipelines in production and at the same time is helpful for monitoring and failure management in the",
    "start": "1615040",
    "end": "1622540"
  },
  {
    "text": "end I would like to say that we are still evolving our architecture as far as as per our business demands but this",
    "start": "1622540",
    "end": "1628810"
  },
  {
    "text": "I would like to hand it back to Terra where she'll tell us more about Luca and some interesting business insights that",
    "start": "1628810",
    "end": "1634420"
  },
  {
    "text": "we are providing is it this platform thank you all right so let's talk about",
    "start": "1634420",
    "end": "1640690"
  },
  {
    "start": "1640000",
    "end": "1918000"
  },
  {
    "text": "how we use Luca at Willo we have more than 700 active users on our luca",
    "start": "1640690",
    "end": "1647290"
  },
  {
    "text": "platform which is pretty much 80% of the company so we quickly realize that in",
    "start": "1647290",
    "end": "1653560"
  },
  {
    "text": "order for us to support this scale we have to distribute the ownership of lucre outside of our core team of",
    "start": "1653560",
    "end": "1660610"
  },
  {
    "text": "analysts so what it means is that we have empowered our business units and",
    "start": "1660610",
    "end": "1666880"
  },
  {
    "text": "product teams to build their own models reports and dashboards and also",
    "start": "1666880",
    "end": "1672580"
  },
  {
    "text": "contribute back to the Twilio lucre community as power comes with",
    "start": "1672580",
    "end": "1678550"
  },
  {
    "text": "responsibility our core team of analysts do provide guidance and ensures the",
    "start": "1678550",
    "end": "1683590"
  },
  {
    "text": "stable production and moment so there are two types of users we support on our",
    "start": "1683590",
    "end": "1689290"
  },
  {
    "text": "looker the developers and non developers so developers are the people who build",
    "start": "1689290",
    "end": "1695200"
  },
  {
    "text": "lucre models using luke ml so look ml is the data modeling language for Luca and",
    "start": "1695200",
    "end": "1701020"
  },
  {
    "text": "non developers are primarily reports and dashboards users we have connected our",
    "start": "1701020",
    "end": "1708250"
  },
  {
    "text": "local to github which allows our core team to review all the model changes",
    "start": "1708250",
    "end": "1714610"
  },
  {
    "text": "before they are released to production it also allows us to do the version control on all of our models so today we",
    "start": "1714610",
    "end": "1723820"
  },
  {
    "text": "have over 50 luke ml developers in Twilio who makes more than hundred contributions per week so with so many",
    "start": "1723820",
    "end": "1732040"
  },
  {
    "text": "users in Luca we surely can get away without proper access control",
    "start": "1732040",
    "end": "1737640"
  },
  {
    "text": "so we have connected our looker to Active Directory so that we can leverage the role based user groups because",
    "start": "1737640",
    "end": "1745300"
  },
  {
    "text": "surely people in finance needs different access privileges then people in messaging or voice teams we have also",
    "start": "1745300",
    "end": "1753550"
  },
  {
    "text": "designed our permission sets based on the functional roles like developers versus non developers furthermore the",
    "start": "1753550",
    "end": "1761920"
  },
  {
    "text": "permission settings are defined for customer PII data or the sensitive",
    "start": "1761920",
    "end": "1767830"
  },
  {
    "text": "financial data and all these are designed keeping in mind the Sox and GDP",
    "start": "1767830",
    "end": "1773050"
  },
  {
    "text": "our compliance requirements all the reports and the dashboards in looker are",
    "start": "1773050",
    "end": "1779470"
  },
  {
    "text": "organized into spaces based on subject areas like sales or marketing or finance",
    "start": "1779470",
    "end": "1785350"
  },
  {
    "text": "and all the access controls are applied at the space level so this way we can do",
    "start": "1785350",
    "end": "1791890"
  },
  {
    "text": "a better content management within looker so continuing with the looker",
    "start": "1791890",
    "end": "1797320"
  },
  {
    "text": "usage we get more than 8000 queries per week and as I spoke about the space",
    "start": "1797320",
    "end": "1803380"
  },
  {
    "text": "based organization we have over 300 public spaces in looker and in just last",
    "start": "1803380",
    "end": "1809050"
  },
  {
    "text": "90 days more than 2,000 reports were executed so with so much going on in",
    "start": "1809050",
    "end": "1814780"
  },
  {
    "text": "looker the question is how do we keep everything under control so that's when",
    "start": "1814780",
    "end": "1820210"
  },
  {
    "text": "the data education becomes really important and especially when we have",
    "start": "1820210",
    "end": "1825330"
  },
  {
    "text": "distributed ownership of looker so we have established a looker help channel",
    "start": "1825330",
    "end": "1831040"
  },
  {
    "text": "on slack where people can ask questions and get help so duty it has evolved into a",
    "start": "1831040",
    "end": "1837760"
  },
  {
    "text": "self-managed 4'o with more than 150 people actively contributing we also",
    "start": "1837760",
    "end": "1843850"
  },
  {
    "text": "host user training sessions as and when there is a new feature or a model or our",
    "start": "1843850",
    "end": "1849760"
  },
  {
    "text": "reports are released we host regular office hours where you",
    "start": "1849760",
    "end": "1855760"
  },
  {
    "text": "can just walk in and get your questions answered by our cool team of analysts",
    "start": "1855760",
    "end": "1861240"
  },
  {
    "text": "documentation also comes very handy when dealing with so many users so we have",
    "start": "1861240",
    "end": "1866470"
  },
  {
    "text": "developed several how-to article that helps users to navigate within Luca",
    "start": "1866470",
    "end": "1872390"
  },
  {
    "text": "and also find what they are looking for we keep up to date documentation on our",
    "start": "1872390",
    "end": "1877730"
  },
  {
    "text": "models and some of the key reports data governance also plays an important role",
    "start": "1877730",
    "end": "1883850"
  },
  {
    "text": "in keeping your reports and dashboards under control because they provide",
    "start": "1883850",
    "end": "1889460"
  },
  {
    "text": "cross-functional alignment and allows us to standardize and create common metrics",
    "start": "1889460",
    "end": "1894560"
  },
  {
    "text": "so that way we can reduce the redundancy of reports and datasets on our platform",
    "start": "1894560",
    "end": "1901540"
  },
  {
    "text": "so next let's talk about the performance so since we have such a big scale there",
    "start": "1901540",
    "end": "1908930"
  },
  {
    "text": "are two major systems for analytics which is looker and redshift so let's talk about how do we deal with the",
    "start": "1908930",
    "end": "1915200"
  },
  {
    "text": "performance for these two systems so for looker we would like our looker to be",
    "start": "1915200",
    "end": "1921410"
  },
  {
    "text": "lightning-fast and we would want all the queries and reports returned really fast well it doesn't happen on its own and",
    "start": "1921410",
    "end": "1929060"
  },
  {
    "text": "you do have to take some actions or take they take some actions to basically get",
    "start": "1929060",
    "end": "1935450"
  },
  {
    "text": "the desired performance so here are few things that we do in local so the first",
    "start": "1935450",
    "end": "1942350"
  },
  {
    "text": "is we use persistent derived tables so the derived tables are essentially the",
    "start": "1942350",
    "end": "1947930"
  },
  {
    "text": "physical copies of the pre calculated data sets in redshift now since the results are stored in",
    "start": "1947930",
    "end": "1955040"
  },
  {
    "text": "redshift the query is no longer have to calculate the results on the fly essentially improving the performance",
    "start": "1955040",
    "end": "1962230"
  },
  {
    "text": "now cherry on top is that you can also add indexes to this derived tables and",
    "start": "1962230",
    "end": "1968120"
  },
  {
    "text": "if you are using redshift like us you can add sort keys and define distribution styles for these tables so",
    "start": "1968120",
    "end": "1976130"
  },
  {
    "text": "next is the model design so we take specific measures in defining our",
    "start": "1976130",
    "end": "1981470"
  },
  {
    "text": "attributes as dimensions or measures we also pay close attention to join",
    "start": "1981470",
    "end": "1987410"
  },
  {
    "text": "cardinalities or the table relations because all this information gives look",
    "start": "1987410",
    "end": "1993350"
  },
  {
    "text": "at a good context to generate optimized queries the next thing that we use is",
    "start": "1993350",
    "end": "2000220"
  },
  {
    "text": "templated filters so templated filters are applied on the derive tables and which allows us to",
    "start": "2000220",
    "end": "2007710"
  },
  {
    "text": "restrict the datasets by certain criterias essentially generating a",
    "start": "2007710",
    "end": "2012970"
  },
  {
    "text": "subset of the data and forced filters on the other hand are applied directly to",
    "start": "2012970",
    "end": "2018670"
  },
  {
    "text": "the reports where one such example is we restrict the reports with very large",
    "start": "2018670",
    "end": "2024640"
  },
  {
    "text": "data sets to be run only at the seven day window at a time lucre also provides",
    "start": "2024640",
    "end": "2031210"
  },
  {
    "text": "default caching mechanism but you can also customize it to get the desired refresh rate and last but not the least",
    "start": "2031210",
    "end": "2039490"
  },
  {
    "text": "is explained feature in the sequel runner since we have lots of exploration",
    "start": "2039490",
    "end": "2045130"
  },
  {
    "text": "activity happens through the sequel runner explained feature really gives you hints to fine-tune your one-time",
    "start": "2045130",
    "end": "2053379"
  },
  {
    "text": "sequel queries so lucre is not the only system where we have to pay attention",
    "start": "2053380",
    "end": "2059530"
  },
  {
    "text": "but redshift also plays an important role when it comes to the performance so",
    "start": "2059530",
    "end": "2066310"
  },
  {
    "start": "2065000",
    "end": "2212000"
  },
  {
    "text": "here are few things that we do for redshift the very first thing is we run",
    "start": "2066310",
    "end": "2071320"
  },
  {
    "text": "a vacuum and analyze operations on nightly basis so vacuum operation allows",
    "start": "2071320",
    "end": "2077889"
  },
  {
    "text": "us to reclaim any unused space in our cluster and the analyze operation",
    "start": "2077890",
    "end": "2083679"
  },
  {
    "text": "collects the metadata about most of our large tables and keeps it up to date for",
    "start": "2083680",
    "end": "2089350"
  },
  {
    "text": "the query optimizer we carefully define distribution style for all our tables to",
    "start": "2089350",
    "end": "2096550"
  },
  {
    "text": "make sure the data is evenly distributed across the cluster sort keys are very",
    "start": "2096550",
    "end": "2103570"
  },
  {
    "text": "similar to the indexes in relational databases and it helps in in improving",
    "start": "2103570",
    "end": "2109780"
  },
  {
    "text": "your query performance compression and coatings are applied at the table column",
    "start": "2109780",
    "end": "2116170"
  },
  {
    "text": "level which allows you to optimize the space on your cluster the workload",
    "start": "2116170",
    "end": "2123160"
  },
  {
    "text": "management is a very important feature of redshift that really helps when it comes to performance so whenever when",
    "start": "2123160",
    "end": "2131800"
  },
  {
    "text": "user executes the query it is assigned to specific you interaction and there are",
    "start": "2131800",
    "end": "2137940"
  },
  {
    "text": "certain parameters that you can use to to customize the performance for your 4q",
    "start": "2137940",
    "end": "2144029"
  },
  {
    "text": "for example we use concurrency level so concurrency level is essentially the",
    "start": "2144029",
    "end": "2150509"
  },
  {
    "text": "number of user queries that can be executed against a queue in parallel the",
    "start": "2150509",
    "end": "2155940"
  },
  {
    "text": "second thing is you can define how much percentage of memory that should be used by a given Q user groups is also another",
    "start": "2155940",
    "end": "2164400"
  },
  {
    "text": "way to optimize the performance where you can define you can create a group of",
    "start": "2164400",
    "end": "2170849"
  },
  {
    "text": "users and then assign that group to a specific u so that if the query is",
    "start": "2170849",
    "end": "2176099"
  },
  {
    "text": "executed from one of one of those users it's alway it always goes to a specific",
    "start": "2176099",
    "end": "2181470"
  },
  {
    "text": "u auto timeout is another important feature where you can kill the you can",
    "start": "2181470",
    "end": "2187920"
  },
  {
    "text": "kill any long-running queries on your cluster automatically typically we try to set that at 10 to 15 minutes for most",
    "start": "2187920",
    "end": "2195900"
  },
  {
    "text": "of our redshift clusters so all this is fine but you must be wondering how",
    "start": "2195900",
    "end": "2201539"
  },
  {
    "text": "exactly the business is being benefited with all this hard work so let's talk",
    "start": "2201539",
    "end": "2207150"
  },
  {
    "text": "about some of the insights that we are able to provide using our platform so",
    "start": "2207150",
    "end": "2212299"
  },
  {
    "start": "2212000",
    "end": "2396000"
  },
  {
    "text": "when it comes to insights it's it's only useful if you can actually act upon it",
    "start": "2212299",
    "end": "2218400"
  },
  {
    "text": "so it's all about the action with our analytics platform we are able to",
    "start": "2218400",
    "end": "2224160"
  },
  {
    "text": "provide some of the actionable insights and let's walk through some of those so",
    "start": "2224160",
    "end": "2229890"
  },
  {
    "text": "the first is the customer segmentation so our sales and marketing teams segment",
    "start": "2229890",
    "end": "2236190"
  },
  {
    "text": "our customers using various attributes throughout their customer lifecycle we",
    "start": "2236190",
    "end": "2242880"
  },
  {
    "text": "collect all these attributes on our analytics platform we are various",
    "start": "2242880",
    "end": "2248069"
  },
  {
    "text": "processes involved in customers journey on our platform like sign up or product usage or or product spend and provide",
    "start": "2248069",
    "end": "2256950"
  },
  {
    "text": "insights that can help them segment our customers the next is the net new",
    "start": "2256950",
    "end": "2262710"
  },
  {
    "text": "business we collect lot of customer spanned data on our platform and provide",
    "start": "2262710",
    "end": "2267839"
  },
  {
    "text": "insight into net new business for each product category or new geo locations we also",
    "start": "2267839",
    "end": "2276390"
  },
  {
    "text": "hold all this revenue data so that our finance teams can do historical trends",
    "start": "2276390",
    "end": "2282210"
  },
  {
    "text": "on our on our revenue based data sets customer acquisition is probably every",
    "start": "2282210",
    "end": "2288810"
  },
  {
    "text": "marketing team's favorite topic and we provide insights through how our",
    "start": "2288810",
    "end": "2294240"
  },
  {
    "text": "customers have come to Twilio like did you come from a Google search or did you came through a Facebook field and all",
    "start": "2294240",
    "end": "2301560"
  },
  {
    "text": "these insights help our marketing teams channel their acquisition and campaign efforts sales and marketing funnels is",
    "start": "2301560",
    "end": "2310920"
  },
  {
    "text": "an interesting story for us initially our sales and marketing teams were",
    "start": "2310920",
    "end": "2316890"
  },
  {
    "text": "tracking two different funnels based on how customers arrive to Twitter and also",
    "start": "2316890",
    "end": "2322650"
  },
  {
    "text": "how they are engaging with with our sales with our sales teams now both the",
    "start": "2322650",
    "end": "2328590"
  },
  {
    "text": "goal of both of these funnels was to track the conversion rate and velocity",
    "start": "2328590",
    "end": "2333740"
  },
  {
    "text": "what it means is that basically how many people are moving from one stage to the",
    "start": "2333740",
    "end": "2339030"
  },
  {
    "text": "other and also how long does it take now one of the challenge was both of these",
    "start": "2339030",
    "end": "2345060"
  },
  {
    "text": "funnels lived in a complete isolation and there was no visibility across customers moving from the one funnel to",
    "start": "2345060",
    "end": "2352530"
  },
  {
    "text": "the other so that's where our analytics platform solve the problem by identifying the customers across these",
    "start": "2352530",
    "end": "2359370"
  },
  {
    "text": "funnels and essentially generating the unified funnel that can be used by both",
    "start": "2359370",
    "end": "2364620"
  },
  {
    "text": "the teams customer lifetime value is another important metric driven by",
    "start": "2364620",
    "end": "2370260"
  },
  {
    "text": "insights which helps us determine how much we can afford to acquire or retain",
    "start": "2370260",
    "end": "2377070"
  },
  {
    "text": "our customer and any analytic system cannot live without its financial",
    "start": "2377070",
    "end": "2383130"
  },
  {
    "text": "metrics so as ours we also provide variety of revenue metrics on our",
    "start": "2383130",
    "end": "2388230"
  },
  {
    "text": "platform and provides insights into a financial health of our company so next",
    "start": "2388230",
    "end": "2394890"
  },
  {
    "text": "let's just talk about some customer centric metrics at William one of our",
    "start": "2394890",
    "end": "2400770"
  },
  {
    "start": "2396000",
    "end": "2556000"
  },
  {
    "text": "core values is to wear customers shoes now we can only do that if we know our customers so",
    "start": "2400770",
    "end": "2408749"
  },
  {
    "text": "as an analytics team we provide insights into various customer touch points that",
    "start": "2408749",
    "end": "2414390"
  },
  {
    "text": "can help us better understand our customers so let's walk through some of these touch points the first is the sign",
    "start": "2414390",
    "end": "2422130"
  },
  {
    "text": "up so this is the first time when we become aware of our customer and we",
    "start": "2422130",
    "end": "2428130"
  },
  {
    "text": "provide insights into their website behavior like first visited pH or last",
    "start": "2428130",
    "end": "2433559"
  },
  {
    "text": "visited page which can help our team's our better understand our customers so",
    "start": "2433559",
    "end": "2440789"
  },
  {
    "text": "that we can gear you towards right products once customer starts generating",
    "start": "2440789",
    "end": "2447539"
  },
  {
    "text": "revenue on our platform we track various pain thresholds now movements across",
    "start": "2447539",
    "end": "2453479"
  },
  {
    "text": "this pain threshold allows us to understand our customer spending behavior similar to the revenue we track",
    "start": "2453479",
    "end": "2462719"
  },
  {
    "text": "various product interactions as as a variety of usage milestones on our",
    "start": "2462719",
    "end": "2469259"
  },
  {
    "text": "platform same thing here that movements across these milestones helps us take",
    "start": "2469259",
    "end": "2475859"
  },
  {
    "text": "proactive actions for our customers for instance if there is a sudden increase",
    "start": "2475859",
    "end": "2481739"
  },
  {
    "text": "in a product usage that might indicate that our customers are launching the new",
    "start": "2481739",
    "end": "2486779"
  },
  {
    "text": "application or they are running the seasonal campaign and where we can help you scale your application and on the",
    "start": "2486779",
    "end": "2494849"
  },
  {
    "text": "other hand if there is a sudden drop in customer in product usage that might",
    "start": "2494849",
    "end": "2500160"
  },
  {
    "text": "mean that you you are facing the product issue or some quality issue where we can",
    "start": "2500160",
    "end": "2505410"
  },
  {
    "text": "help you troubleshoot and fix that so next is engagement as earlier I spoke",
    "start": "2505410",
    "end": "2511739"
  },
  {
    "text": "about the runtime developer experience program it focuses on providing the",
    "start": "2511739",
    "end": "2517109"
  },
  {
    "text": "seamless experience on our platform from sign up to launch the insights that we",
    "start": "2517109",
    "end": "2522660"
  },
  {
    "text": "provide that helps Twilio provide proactive support and training for our customers and last is the charm I think John is",
    "start": "2522660",
    "end": "2531539"
  },
  {
    "text": "one area where insights can really help retain your customers we track John",
    "start": "2531539",
    "end": "2537550"
  },
  {
    "text": "using lot of span data we also use that data in conjunction with the support",
    "start": "2537550",
    "end": "2542920"
  },
  {
    "text": "tickets and the usage data to drive further insights into charm so these are",
    "start": "2542920",
    "end": "2548380"
  },
  {
    "text": "all their customer touch points where we provide insights and that that help us understand our customers better so in",
    "start": "2548380",
    "end": "2556570"
  },
  {
    "text": "the end I would like to say that analytics is truly a journey we have traveled miles but we still have a long",
    "start": "2556570",
    "end": "2563590"
  },
  {
    "text": "way to go and we are continuously evolving our architecture and our services to fuel",
    "start": "2563590",
    "end": "2569170"
  },
  {
    "text": "the data driven culture at will you so with that I'll hand it back to Daniel thank you guys that I love that",
    "start": "2569170",
    "end": "2578140"
  },
  {
    "start": "2576000",
    "end": "2684000"
  },
  {
    "text": "presentation as somebody who's gone through what they have gone through myself they talk about it as if it's no",
    "start": "2578140",
    "end": "2585040"
  },
  {
    "text": "big deal but it's really hard to do what they're doing and particularly at the scale that they're doing it and you know",
    "start": "2585040",
    "end": "2591370"
  },
  {
    "text": "I think the one of the key things to note is and one of the great things about the joint presentation is that",
    "start": "2591370",
    "end": "2596920"
  },
  {
    "text": "there are huge technical challenges and there are also huge person challenges right business challenges and and your",
    "start": "2596920",
    "end": "2604180"
  },
  {
    "text": "training and documentation and all of those things and without both of those pieces there's no way that you can scale",
    "start": "2604180",
    "end": "2609310"
  },
  {
    "text": "an analytics culture the way that they are because the technology has to be responsive enough that people when they",
    "start": "2609310",
    "end": "2614530"
  },
  {
    "text": "go to ask a question they can get an answer in a reasonable amount of time but you also need to empower people to make sure that they're asking the right",
    "start": "2614530",
    "end": "2620650"
  },
  {
    "text": "questions that they know where to ask the questions that people who are you know deeply knowledgeable about a",
    "start": "2620650",
    "end": "2626530"
  },
  {
    "text": "particular analytics area can contribute that expertise back to the sort of from",
    "start": "2626530",
    "end": "2632830"
  },
  {
    "text": "that spoke back to the core hub because if they don't do that you're really missing out right you know the the",
    "start": "2632830",
    "end": "2638950"
  },
  {
    "text": "knowledge that a finance person is gonna have about the numbers that drive their everyday business experience are very",
    "start": "2638950",
    "end": "2644470"
  },
  {
    "text": "different than the the level of expertise that you know the analytics team is going to have right those finance folks have a gut feel a tactile",
    "start": "2644470",
    "end": "2651910"
  },
  {
    "text": "feel for what those numbers should look like and when they're wrong that an analytics team that's responsible for data across the organization is never",
    "start": "2651910",
    "end": "2658540"
  },
  {
    "text": "going to have and so involving those folks in the in the process and a real",
    "start": "2658540",
    "end": "2663580"
  },
  {
    "text": "substantive way makes all the difference in the business value that you're actually arriving from the data that you're",
    "start": "2663580",
    "end": "2669430"
  },
  {
    "text": "collecting so I just want to hit on a couple more things that you might not",
    "start": "2669430",
    "end": "2675910"
  },
  {
    "text": "know about and then we'll open it up for any questions which I'm sure will be for",
    "start": "2675910",
    "end": "2681609"
  },
  {
    "text": "Rahul and Darin which is fine by me so one cool thing about AWS is just the",
    "start": "2681609",
    "end": "2688630"
  },
  {
    "start": "2684000",
    "end": "2837000"
  },
  {
    "text": "sort of openness of the platform and the fact that they let you do everything and will give you data about what is",
    "start": "2688630",
    "end": "2694270"
  },
  {
    "text": "happening one of the things that can be tricky is actually making sense of that data I speak from experience and so you",
    "start": "2694270",
    "end": "2702309"
  },
  {
    "text": "know one of the things that we've we've thought about a lot at looker because we're huge users of AWS is where we have",
    "start": "2702309",
    "end": "2708640"
  },
  {
    "text": "access to all this data how can we actually get more business value from it and one of the ways that we've done that",
    "start": "2708640",
    "end": "2714790"
  },
  {
    "text": "is by building what are called looker blocks for this AWS data and what looker",
    "start": "2714790",
    "end": "2720250"
  },
  {
    "text": "blocks are is they're pre-built analytic solutions that you can deploy on your looker instance to get value much more",
    "start": "2720250",
    "end": "2728049"
  },
  {
    "text": "quickly from the data that you're collecting and you know because look ml the modeling language that sort of sits",
    "start": "2728049",
    "end": "2734260"
  },
  {
    "text": "at the heart of looker is just code we can give you that code for free it's not a black box you can see exactly what",
    "start": "2734260",
    "end": "2740440"
  },
  {
    "text": "it's doing and because everybody's needs are slightly different you can then go in and customize it so you know you",
    "start": "2740440",
    "end": "2745750"
  },
  {
    "text": "don't have to start from scratch you don't have to start from zero you can start from 90% of the way there but then",
    "start": "2745750",
    "end": "2750970"
  },
  {
    "text": "you can customize that last 10% to meet your exact needs so the what we've done",
    "start": "2750970",
    "end": "2756010"
  },
  {
    "text": "the sort of key ones that we've we've identified as our own internal needs and so have have put out on in the looker",
    "start": "2756010",
    "end": "2763240"
  },
  {
    "text": "block's directory one is for redshift optimist optimization you know Dara talked about the explained statements",
    "start": "2763240",
    "end": "2768970"
  },
  {
    "text": "how many people have ever looked at an explained statement from redshift yes/no",
    "start": "2768970",
    "end": "2774660"
  },
  {
    "text": "Wow only a couple of people have had this very terrifying experience the rest",
    "start": "2774660",
    "end": "2779859"
  },
  {
    "text": "of you are very lucky not to have had to do that but the so redshift gives you enormous amounts of information about",
    "start": "2779859",
    "end": "2786579"
  },
  {
    "text": "where data lives and which which nodes it's being shuttled from and where the the you know fanning out is happening",
    "start": "2786579",
    "end": "2792700"
  },
  {
    "text": "and all of that but it is not necessarily the easiest to make sense of so one of our great analysts said well I",
    "start": "2792700",
    "end": "2799599"
  },
  {
    "text": "can write look ml that knows how to make sense of that right and so that's what our",
    "start": "2799599",
    "end": "2804750"
  },
  {
    "text": "redshift optimization block is we worked with AWS to validate and make sure",
    "start": "2804750",
    "end": "2810390"
  },
  {
    "text": "everything was right but what that allows you to do is look at the query level and look at a particular query",
    "start": "2810390",
    "end": "2815430"
  },
  {
    "text": "that's not running as fast as you want and rather than being presented with you know dozens of lines of explained",
    "start": "2815430",
    "end": "2822420"
  },
  {
    "text": "statement you can get a nice dashboard that explains to you exactly what's happening where the bottleneck may be",
    "start": "2822420",
    "end": "2828270"
  },
  {
    "text": "what kinds of tactics you might want to use to get better performance out of",
    "start": "2828270",
    "end": "2833280"
  },
  {
    "text": "that query and so that's available on the looker blocks directory another one",
    "start": "2833280",
    "end": "2838710"
  },
  {
    "start": "2837000",
    "end": "2911000"
  },
  {
    "text": "is security and monitoring you know cloud watch gives you an enormous amount of data about what's happening with you",
    "start": "2838710",
    "end": "2844859"
  },
  {
    "text": "know all of the AWS services that you're using but again it's pretty raw when you get your hands on it right there CSVs",
    "start": "2844859",
    "end": "2850980"
  },
  {
    "text": "and so we built a block that takes that data and presents it in a much",
    "start": "2850980",
    "end": "2857400"
  },
  {
    "text": "friendlier format so that you can see exactly what's happening with which parts of your Amazon infrastructure I'd",
    "start": "2857400",
    "end": "2864720"
  },
  {
    "text": "be remiss to mention on the the redshift one actually there's query level optimization and also table level",
    "start": "2864720",
    "end": "2870119"
  },
  {
    "text": "optimization because that actually ends up driving a huge amount of of your performance is not the actual query but",
    "start": "2870119",
    "end": "2876690"
  },
  {
    "text": "how is the table structured you know what sort keys are declared what distribution keys are declared is there",
    "start": "2876690",
    "end": "2882810"
  },
  {
    "text": "skew is there sort key skew which was a new thing I only discovered existed when the red shift engineers told me a couple",
    "start": "2882810",
    "end": "2889080"
  },
  {
    "text": "years ago never heard of that one so we give you the tools to actually understand exactly how your tables are",
    "start": "2889080",
    "end": "2894570"
  },
  {
    "text": "structured if they're structured well you know flag tables that maybe are very skewed that you might want to",
    "start": "2894570",
    "end": "2899849"
  },
  {
    "text": "redistribute or declare a new sort key on so we've got the two red shift blocks",
    "start": "2899849",
    "end": "2904859"
  },
  {
    "text": "the security and monitoring by AWS block and then finally for those of you who you know are spending a lot of money on",
    "start": "2904859",
    "end": "2912030"
  },
  {
    "start": "2911000",
    "end": "2998000"
  },
  {
    "text": "AWS there might be ways to save yourself money and so the cost and usage analysis",
    "start": "2912030",
    "end": "2917730"
  },
  {
    "text": "is another block that we provide where you can take that raw data which comes",
    "start": "2917730",
    "end": "2922920"
  },
  {
    "text": "in again very wide CSV is not exactly the easiest thing to parse load it into",
    "start": "2922920",
    "end": "2928290"
  },
  {
    "text": "your into s3 where it goes or loaded into redshift and all of a sudden you can see you know how much",
    "start": "2928290",
    "end": "2934650"
  },
  {
    "text": "usages reserved instances versus on-demand instances you know which services are using the most money how",
    "start": "2934650",
    "end": "2942600"
  },
  {
    "text": "can we save money we actually use this and are saving I don't know half a million dollars a year or something",
    "start": "2942600",
    "end": "2948170"
  },
  {
    "text": "based on the insights that we gleaned internally from this so if you're a big",
    "start": "2948170",
    "end": "2953580"
  },
  {
    "text": "user of AWS services this might be a great way to to take a look at what",
    "start": "2953580",
    "end": "2958650"
  },
  {
    "text": "you're doing and and think about whether or not you could be saving your company some money which is always a good way to",
    "start": "2958650",
    "end": "2964020"
  },
  {
    "text": "make friends across your company if you're a technical person show them how you will save them money and they will",
    "start": "2964020",
    "end": "2969690"
  },
  {
    "text": "give you more resources to do what you do so with that I'm gonna open it up to",
    "start": "2969690",
    "end": "2975510"
  },
  {
    "text": "questions we have a microphone but it's I feel like if people have questions just yell em out and we'll repeat them",
    "start": "2975510",
    "end": "2981240"
  },
  {
    "text": "so anybody who's watching on video can hear them in and then we'll do our best to answer them yeah yeah okay so the",
    "start": "2981240",
    "end": "2998640"
  },
  {
    "start": "2998000",
    "end": "3122000"
  },
  {
    "text": "question is we want to get started oh and I should say for before we answer that for anybody who's interested if you",
    "start": "2998640",
    "end": "3005810"
  },
  {
    "text": "want to see more about look or hear more about looker we're here at the Aria booth 210 and I believe I will be the",
    "start": "3005810",
    "end": "3012710"
  },
  {
    "text": "only one offering you the opportunity to experience data in virtual reality if you are over at the Venetian we have",
    "start": "3012710",
    "end": "3017810"
  },
  {
    "text": "look VR which is built on our API and you can draw on a headset and explore your data and virtual reality which is",
    "start": "3017810",
    "end": "3023830"
  },
  {
    "text": "crazy cool as it turns out I was a skeptic but it's pretty fun so so the",
    "start": "3023830",
    "end": "3030080"
  },
  {
    "text": "question is how do I get started like I want to I want to dip our toes in the water what should we do guys what do you",
    "start": "3030080",
    "end": "3035300"
  },
  {
    "text": "think you want to take that should I take that okay I mean I would say part",
    "start": "3035300",
    "end": "3040820"
  },
  {
    "text": "of the the real change that's come about by the sort of cloud databases and and all that is you can just get started by",
    "start": "3040820",
    "end": "3048190"
  },
  {
    "text": "by just starting right you don't have to there's no huge capital expenditure that predates doing stuff you can just go",
    "start": "3048190",
    "end": "3055130"
  },
  {
    "text": "spin up a red shift instance and start playing to see if you can get value and you know that Amazon will give you free",
    "start": "3055130",
    "end": "3062660"
  },
  {
    "text": "credits so you can even do it for free my experience with red shift compared to a relational databases that it will blow",
    "start": "3062660",
    "end": "3068239"
  },
  {
    "text": "your hair back it just is so fast and so powerful you will be stunned but yeah I",
    "start": "3068239",
    "end": "3073759"
  },
  {
    "text": "mean I think just start doing stuff rather than coming up with a grand master strategy and you know just start",
    "start": "3073759",
    "end": "3081919"
  },
  {
    "text": "doing stuff focus on bottom-line metrics that will drive business value that's how you get more resources to do the",
    "start": "3081919",
    "end": "3087289"
  },
  {
    "text": "next thing and don't you know as they said rome wasn't built in a day it's a journey just start doing and",
    "start": "3087289",
    "end": "3093859"
  },
  {
    "text": "you'll learn far more than you could learn you know through six months of intense planning cool yep yep",
    "start": "3093859",
    "end": "3112029"
  },
  {
    "text": "yeah so the question is about redshift and duplicate data redshift like a lot of columnar databases does an enforce",
    "start": "3121849",
    "end": "3127489"
  },
  {
    "start": "3122000",
    "end": "3143000"
  },
  {
    "text": "primary key uniqueness and so that means you can accidentally end up with two copies of the same piece of data",
    "start": "3127489",
    "end": "3133130"
  },
  {
    "text": "particularly if you're hooking it up to something that doesn't enforce once only delivery do you guys want to talk about",
    "start": "3133130",
    "end": "3139099"
  },
  {
    "text": "that in in the context of your architecture by doing lot of",
    "start": "3139099",
    "end": "3144920"
  },
  {
    "start": "3143000",
    "end": "3194000"
  },
  {
    "text": "transformations outside of red shift we have augmented our processes to do the deduplication and also put in an",
    "start": "3144920",
    "end": "3151819"
  },
  {
    "text": "automatic checks to make sure that before we load the data in redshift it's basically clean there are no",
    "start": "3151819",
    "end": "3157819"
  },
  {
    "text": "duplication and we have established the uniqueness one other thing is that just",
    "start": "3157819",
    "end": "3162949"
  },
  {
    "text": "for the in redshift you can define an identity column that will give you unique ID for each record but I would",
    "start": "3162949",
    "end": "3170690"
  },
  {
    "text": "highly suggest that do this outside of redshift before you load the data in redshirt so the lot of our processes are",
    "start": "3170690",
    "end": "3180499"
  },
  {
    "text": "all in-house so we have we are using spark heavily we have a lot of Python based processes as well so it's",
    "start": "3180499",
    "end": "3186920"
  },
  {
    "text": "basically since it's all custom built it's all in-house we have just augmented the function functionalities within",
    "start": "3186920",
    "end": "3193219"
  },
  {
    "text": "those yeah I mean I'd also add just from my own experience with redshift you know you can do deduplication in redshift",
    "start": "3193219",
    "end": "3198769"
  },
  {
    "start": "3194000",
    "end": "3297000"
  },
  {
    "text": "it's it's not ideal but you can particularly for doing windowed deduplication so for example if you're",
    "start": "3198769",
    "end": "3204920"
  },
  {
    "text": "looking you know if you're looking at data from the last day you only need to compare it to data from the last day to make sure that there's no duplicates",
    "start": "3204920",
    "end": "3211160"
  },
  {
    "text": "right and so I think a lot of people make the mistake of comparing last day's data to the last year's data which is",
    "start": "3211160",
    "end": "3216859"
  },
  {
    "text": "unnecessary so as long as you're particularly when you're dealing with time series data which is usually where this comes up you know you can",
    "start": "3216859",
    "end": "3223069"
  },
  {
    "text": "absolutely do deduplication and in redshift and particularly then vacuuming and analyzing which which Twilio does",
    "start": "3223069",
    "end": "3230299"
  },
  {
    "text": "every night becomes particularly important because if you do catch those those accidental duplicates what you're",
    "start": "3230299",
    "end": "3235339"
  },
  {
    "text": "gonna do is you're gonna delete them and so then you've got some extra space in the middle of your giant column that",
    "start": "3235339",
    "end": "3241549"
  },
  {
    "text": "you're gonna want to reclaim and and maybe reefs or you know redshift also has some great there's some great stuff",
    "start": "3241549",
    "end": "3247789"
  },
  {
    "text": "in the redshift Doc's about doing sort of partitions tables basically and then",
    "start": "3247789",
    "end": "3253369"
  },
  {
    "text": "use materialized view so that none of your individual tables become so huge you can vacuum each of those you know a month's",
    "start": "3253369",
    "end": "3259789"
  },
  {
    "text": "worth of data a week's worth of data individually but then when you address them you address them through the view",
    "start": "3259789",
    "end": "3264859"
  },
  {
    "text": "which knows about all of the tables going back forever so it looks to your",
    "start": "3264859",
    "end": "3270230"
  },
  {
    "text": "users like it's one giant table but for maintenance purposes it makes it a lot smoother",
    "start": "3270230",
    "end": "3276220"
  },
  {
    "text": "wait let's let it let's let Rahul answer that okay so the question is are using Kafka as part of your sort of ETL pipeline sort of extracting data from my",
    "start": "3296930",
    "end": "3307400"
  },
  {
    "start": "3297000",
    "end": "3401000"
  },
  {
    "text": "sequel we are using Kafka so you're not putting too much pressure on our my sequel transactional databases so we",
    "start": "3307400",
    "end": "3313490"
  },
  {
    "text": "asked our engineering teams okay why don't you send this data through Kafka so we can so there is Kafka stream",
    "start": "3313490",
    "end": "3319250"
  },
  {
    "text": "processor right you can do transformations map and reduce functions there itself and then we push it to a spot streaming pipeline where you can do",
    "start": "3319250",
    "end": "3326990"
  },
  {
    "text": "transformation and then you can create data frames and data sets and do do those transformations and directly",
    "start": "3326990",
    "end": "3332720"
  },
  {
    "text": "loaded directions second question yeah it's the data we",
    "start": "3332720",
    "end": "3340460"
  },
  {
    "text": "are using it as a deetle ik yeah Esther is infinite scalability makes it a great",
    "start": "3340460",
    "end": "3346010"
  },
  {
    "text": "data Lake and particularly I would recommend if folks haven't played with us you know or I'm a redshift spectrum",
    "start": "3346010",
    "end": "3351410"
  },
  {
    "text": "here it all of a sudden the data in s3 and your data Lake becomes much more accessible because you don't have to",
    "start": "3351410",
    "end": "3357110"
  },
  {
    "text": "you're not forced to load it into redshift to start just exploring if you want to go find some anomalous event you",
    "start": "3357110",
    "end": "3363050"
  },
  {
    "text": "can do that through spectrum or through Athena without ever pulling data out of Athena we're out of espera excuse me and",
    "start": "3363050",
    "end": "3369620"
  },
  {
    "text": "so that just makes it easy to find that piece that you want to load into redshift and do more exploration less yeah cool yeah",
    "start": "3369620",
    "end": "3378040"
  },
  {
    "text": "yeah so the question is how do you store the data in the in the data warehouse is it time series data yeah yeah I think so",
    "start": "3394130",
    "end": "3402480"
  },
  {
    "start": "3401000",
    "end": "3494000"
  },
  {
    "text": "for most of our large dimension does have a time series data and I think one of the reason is that we do the time",
    "start": "3402480",
    "end": "3408809"
  },
  {
    "text": "based partitions on s3 before it's getting loaded so part of that is that I think we do have some of the acid you",
    "start": "3408809",
    "end": "3415980"
  },
  {
    "text": "sed type too we are not using that like heavily form for the map for most of our",
    "start": "3415980",
    "end": "3422339"
  },
  {
    "text": "dimensions but there are few dimensions which does have that and again here also we prepare that before we load it into",
    "start": "3422339",
    "end": "3429540"
  },
  {
    "text": "the red ship so all the attaching all those current record attributes and",
    "start": "3429540",
    "end": "3435180"
  },
  {
    "text": "time-based attributes no we do that",
    "start": "3435180",
    "end": "3440339"
  },
  {
    "text": "before we load it into red ship so we perform that transformations within spar using the s3 files",
    "start": "3440339",
    "end": "3446069"
  },
  {
    "text": "yeah so more yeah so exactly the lookup happens to spot since everything is in",
    "start": "3446069",
    "end": "3452190"
  },
  {
    "text": "history we can directly do it terrific",
    "start": "3452190",
    "end": "3455930"
  },
  {
    "text": "absolutely yeah in fact we are considering to move towards Athena and presto or something",
    "start": "3463220",
    "end": "3470460"
  },
  {
    "text": "that will support sequel on file system because right now our architecture has three big flagship clusters and there is",
    "start": "3470460",
    "end": "3478170"
  },
  {
    "text": "certainly a room to replace one with either wretched spectrum or athina yeah and I think I'd you say yeah yeah I mean",
    "start": "3478170",
    "end": "3495089"
  },
  {
    "start": "3494000",
    "end": "3600000"
  },
  {
    "text": "so the question is Kenneth ns3 replaced redshift or other data warehouses and I'd say I think Athena and presto and",
    "start": "3495089",
    "end": "3502530"
  },
  {
    "text": "all of those are great for a lot of things I don't think that the performance or cost the cost per query",
    "start": "3502530",
    "end": "3510240"
  },
  {
    "text": "model can be a little tricky in terms of containing costs and you know the with redshift you know your costs ahead of",
    "start": "3510240",
    "end": "3516480"
  },
  {
    "text": "time but you know I think as I said at the very beginning the databases only",
    "start": "3516480",
    "end": "3521910"
  },
  {
    "text": "keep getting faster and more powerful and so we do not have infinitely fast infinitely cheap databases yet but you",
    "start": "3521910",
    "end": "3528119"
  },
  {
    "text": "know when you look at those cost curves and those speed curves we're getting there the the asymptote is coming so I",
    "start": "3528119",
    "end": "3534030"
  },
  {
    "text": "think you know build to what's available now and assume that what's available next year will be even faster or even",
    "start": "3534030",
    "end": "3539880"
  },
  {
    "text": "cheaper and and it's okay you know I think part of the embracing the whole",
    "start": "3539880",
    "end": "3545040"
  },
  {
    "text": "ecosystem as Twilio has done and as lookers really architected to do is is knowing that you build for what's the",
    "start": "3545040",
    "end": "3552299"
  },
  {
    "text": "best meat you know fit for your needs today but also know that you are gonna be swapping components out later and",
    "start": "3552299",
    "end": "3559140"
  },
  {
    "text": "replacing them as better things become available in that that's okay",
    "start": "3559140",
    "end": "3564170"
  },
  {
    "text": "yeah yeah so questions about Arif yeah the experience has been I would",
    "start": "3578650",
    "end": "3585140"
  },
  {
    "text": "certainly say that it was really helpful because we had like four to five different services and like I mentioned",
    "start": "3585140",
    "end": "3590900"
  },
  {
    "text": "we were using conscripts to schedule our jobs and like all are eating all util",
    "start": "3590900",
    "end": "3597200"
  },
  {
    "text": "processes speed and everything at night right and if something fails or if something doesn't run out takes more time all your jobs run and then what",
    "start": "3597200",
    "end": "3605600"
  },
  {
    "text": "happens is you resulted in complete data sets and you just rerun all the jobs and it used to take a lot of time with",
    "start": "3605600",
    "end": "3610970"
  },
  {
    "text": "airflow you have a single unified view where you can schedule or your jobs you can write everything in DAGs and then",
    "start": "3610970",
    "end": "3617030"
  },
  {
    "text": "you can define dependencies there are a lot of other options as well like you can spin up more workers and then you",
    "start": "3617030",
    "end": "3625250"
  },
  {
    "text": "can retry emails on failure so there are a lot of functionalities that come with air fluid cell that you'll find very",
    "start": "3625250",
    "end": "3630980"
  },
  {
    "text": "useful like we found it and then it also helps you to visualize everything right if you see anything if fact if it has a",
    "start": "3630980",
    "end": "3638600"
  },
  {
    "text": "visual view it you can easily click on a job and Don it so it it was very helpful",
    "start": "3638600",
    "end": "3644119"
  },
  {
    "text": "for our architecture it helped us tremendous aware were beginning being given the wrap-up signal",
    "start": "3644119",
    "end": "3650830"
  },
  {
    "text": "thank you everybody if folks have more questions we'll hang out and you're happy you're welcome to ask him but thanks everybody",
    "start": "3650830",
    "end": "3658029"
  }
]