[
  {
    "start": "0",
    "end": "170000"
  },
  {
    "text": "today today we're gonna be talking again about machine learning and deep learning",
    "start": "0",
    "end": "5430"
  },
  {
    "text": "and with a focus on a framework called Sakai which allows us to do kind of",
    "start": "5430",
    "end": "11340"
  },
  {
    "text": "neural machine translation and so far we've done a whole kind of series of",
    "start": "11340",
    "end": "19740"
  },
  {
    "text": "different projects on the channel this project is going to be pretty short and sweet today but because Sunil has to get",
    "start": "19740",
    "end": "27599"
  },
  {
    "text": "out of here to go speak in San Francisco if you're in San Francisco you should go listen to him speak and then after Sunil",
    "start": "27599",
    "end": "38670"
  },
  {
    "text": "leaves I'm gonna take some time and try and port the Sakai demo that we build",
    "start": "38670",
    "end": "44160"
  },
  {
    "text": "over to lambda so snail it's it's your",
    "start": "44160",
    "end": "49890"
  },
  {
    "text": "show let's let's learn how do you suck at all right hey guys so thought it'd be",
    "start": "49890",
    "end": "55350"
  },
  {
    "text": "cool to kind of build and show you guys how we can translate over from a",
    "start": "55350",
    "end": "61770"
  },
  {
    "text": "language language so that's a English to French or French to gentlemen things",
    "start": "61770",
    "end": "67560"
  },
  {
    "text": "like that but also like the whole content of your sequence sequence model",
    "start": "67560",
    "end": "75049"
  },
  {
    "text": "it is the pattern that you know the architecture that we'll be using which has a lot more applications than just",
    "start": "75049",
    "end": "82020"
  },
  {
    "text": "you're a machine translation so one of the we we did in episode 2 we did a",
    "start": "82020",
    "end": "89610"
  },
  {
    "text": "basic sequence model really simplistic one which learned how to do math over",
    "start": "89610",
    "end": "95759"
  },
  {
    "text": "time rather we give it a bunch of sequences and it trained and it",
    "start": "95759",
    "end": "101340"
  },
  {
    "text": "recognized what needs to be done you know while doing you know addition and",
    "start": "101340",
    "end": "108119"
  },
  {
    "text": "multiplication now this is a more sophisticated framework that does a lot",
    "start": "108119",
    "end": "114390"
  },
  {
    "text": "of the heavy lifting that you know you we went through the other day so it's",
    "start": "114390",
    "end": "121530"
  },
  {
    "text": "meant to be just uses like a script where you can give it a training corpus and you know it will learn on its own",
    "start": "121530",
    "end": "129179"
  },
  {
    "text": "it'll actually find the best model you can also find ensemble's of model so that you can",
    "start": "129179",
    "end": "135990"
  },
  {
    "text": "actually get a really good you know prediction so they'll go through what",
    "start": "135990",
    "end": "143000"
  },
  {
    "text": "Sakai is now the name actually again big",
    "start": "143000",
    "end": "149340"
  },
  {
    "text": "you know people wanted a name and they you know it's named after the sockeye",
    "start": "149340",
    "end": "155340"
  },
  {
    "text": "salmon that's found in not the Pacific and you know given that a lot of people",
    "start": "155340",
    "end": "162870"
  },
  {
    "text": "are based in Seattle pres Amazon's concerned they decided to see in this",
    "start": "162870",
    "end": "168330"
  },
  {
    "text": "Hawkeye so what else what additionally",
    "start": "168330",
    "end": "173370"
  },
  {
    "text": "it has you know I think here what I wanted to go through the applications right so we can use it for named entity",
    "start": "173370",
    "end": "180840"
  },
  {
    "text": "recognition we can do semantic parsing but let's let's refresh and what's",
    "start": "180840",
    "end": "186690"
  },
  {
    "text": "happening in a sequence a sequence model so you have the encoder part and then",
    "start": "186690",
    "end": "194040"
  },
  {
    "text": "the decoder part the encoder part takes the input what it does is it actually",
    "start": "194040",
    "end": "200520"
  },
  {
    "text": "learns a representation and you know keeps it keeps it in a hidden State now",
    "start": "200520",
    "end": "207600"
  },
  {
    "text": "what the decoder does is when you give it hey this is what I want you to translate or convert it looks at the",
    "start": "207600",
    "end": "214770"
  },
  {
    "text": "converted it projects and onto that space and uses that hidden state and",
    "start": "214770",
    "end": "220530"
  },
  {
    "text": "then decodes it into what we think you know it should be so so that's a very",
    "start": "220530",
    "end": "228330"
  },
  {
    "text": "high-level idea of how an encoder decoder framework works they're also",
    "start": "228330",
    "end": "234180"
  },
  {
    "text": "called sequence to sequence models just add the sequence of sequence name kind",
    "start": "234180",
    "end": "241110"
  },
  {
    "text": "of stock so that other out of curiosity there when you say encoder to decoder",
    "start": "241110",
    "end": "250170"
  },
  {
    "text": "you know the sequence the sequence are I remember last time we were dealing with",
    "start": "250170",
    "end": "256910"
  },
  {
    "text": "characters and assigning new values to each character correct in this case how",
    "start": "256910",
    "end": "264180"
  },
  {
    "text": "do we we did cover this last time and I just don't remember how do we build into",
    "start": "264180",
    "end": "270319"
  },
  {
    "text": "words and then from Renz to the sequence",
    "start": "270319",
    "end": "276590"
  },
  {
    "text": "yeah no good good question so so if you see in this case we're doing a word two",
    "start": "276590",
    "end": "283560"
  },
  {
    "text": "word translation right so we're converting the first example we'll take is converting say French to English",
    "start": "283560",
    "end": "293719"
  },
  {
    "text": "right so so what we'll do is we'll break everything into words so we'll have a",
    "start": "293719",
    "end": "299580"
  },
  {
    "text": "French essentially we'll build a French dictionary and we'll build the English dictionary and we'll see will encode I",
    "start": "299580",
    "end": "307620"
  },
  {
    "text": "will first do the encoding in terms of like we'll just replace all the words with actual numbers or you know number",
    "start": "307620",
    "end": "315509"
  },
  {
    "text": "representation because we don't need the real words we'll see how things happen we'll do the same with English words as",
    "start": "315509",
    "end": "321779"
  },
  {
    "text": "well I will see how the mapping works and all this is actually done by the framework so we don't need to do any of",
    "start": "321779",
    "end": "328409"
  },
  {
    "text": "this we just need to prepare the data in the format that the framework expects and",
    "start": "328409",
    "end": "333750"
  },
  {
    "text": "that's all and should you'll see how easy and how the heavy lifting is being",
    "start": "333750",
    "end": "340110"
  },
  {
    "text": "done the way the framework so as you can see here right so let's say I don't",
    "start": "340110",
    "end": "345900"
  },
  {
    "text": "speak French right shot ah no ah that's my bad that's my best friend and then it",
    "start": "345900",
    "end": "357449"
  },
  {
    "text": "encodes it learns these sentences over time or rather the words and how they",
    "start": "357449",
    "end": "362699"
  },
  {
    "text": "you know interact I would also we're giving it what the output is right so in this case the cat",
    "start": "362699",
    "end": "368699"
  },
  {
    "text": "is flat so what it tries to do is hey how do I go from these French word",
    "start": "368699",
    "end": "374789"
  },
  {
    "text": "mappings to English mappings and that's what the marble learns and it represents",
    "start": "374789",
    "end": "381659"
  },
  {
    "text": "so the good thing the one thing here is it it stores the hidden state what that",
    "start": "381659",
    "end": "387659"
  },
  {
    "text": "means is we can actually you know if we wanted to do something further there we",
    "start": "387659",
    "end": "394319"
  },
  {
    "text": "could actually you know join train models sometime and I'll actually pick up let me let me",
    "start": "394319",
    "end": "402750"
  },
  {
    "text": "pick up you know a slide on like word embeddings to show illustrate how to do",
    "start": "402750",
    "end": "409200"
  },
  {
    "text": "joint training but the idea is we have the words will have an embedding",
    "start": "409200",
    "end": "416630"
  },
  {
    "text": "remember we did embedding last time and then we will will actually you know",
    "start": "416630",
    "end": "425150"
  },
  {
    "text": "connect so embedding is essentially what we drink do is so the word corpus is",
    "start": "425150",
    "end": "431040"
  },
  {
    "text": "pretty huge right so we have a lot of different words we want to reduce the dimensionality and we want to get to a",
    "start": "431040",
    "end": "437760"
  },
  {
    "text": "point where the words actually are",
    "start": "437760",
    "end": "444420"
  },
  {
    "text": "projecting to a space where we can compare between words so if we take like an English dictionary what what will",
    "start": "444420",
    "end": "452250"
  },
  {
    "text": "happen is words let me pull it up I",
    "start": "452250",
    "end": "457500"
  },
  {
    "text": "think that will be helpful hold on guys to remember where I where",
    "start": "457500",
    "end": "477210"
  },
  {
    "text": "we talked about their teasing the the it's their learning embedding layers if you go back to where you were just word",
    "start": "477210",
    "end": "483169"
  },
  {
    "text": "yeah so once I can define there you go",
    "start": "483169",
    "end": "488720"
  },
  {
    "text": "let's see well so things that are",
    "start": "488720",
    "end": "496740"
  },
  {
    "text": "similar in distance kind of like end you can let's let's pick country countries",
    "start": "496740",
    "end": "504290"
  },
  {
    "text": "Europe European America German Germany kind of like all end up together right",
    "start": "504290",
    "end": "510270"
  },
  {
    "text": "so they're all similar so so what's cool is the distance between you know words",
    "start": "510270",
    "end": "519120"
  },
  {
    "text": "becomes you know is actually similar so if we go from a distance like man",
    "start": "519120",
    "end": "526820"
  },
  {
    "text": "subtract man from women and King from Queen the",
    "start": "526820",
    "end": "531900"
  },
  {
    "text": "distance is roughly the same right so so once we have that representation it",
    "start": "531900",
    "end": "537900"
  },
  {
    "text": "becomes easier for us to can put word to word because the model has a you know",
    "start": "537900",
    "end": "545700"
  },
  {
    "text": "because it's operating in a we've kind of reduce the dimensionality and like",
    "start": "545700",
    "end": "551490"
  },
  {
    "text": "given it some kind of a distance function it's helpful and it learns faster gotcha",
    "start": "551490",
    "end": "558090"
  },
  {
    "text": "so rather than being just random numbers distributed it's like it's adjusting the",
    "start": "558090",
    "end": "563810"
  },
  {
    "text": "the token I guess that we chose over time as we trained it to be closer or",
    "start": "563810",
    "end": "570840"
  },
  {
    "text": "farther away to the other tokens that make sense to it yeah yeah so so you can",
    "start": "570840",
    "end": "579660"
  },
  {
    "text": "like you know sort of see here him and he are you know and her and she are",
    "start": "579660",
    "end": "586440"
  },
  {
    "text": "together very close by the distance is almost almost there",
    "start": "586440",
    "end": "593010"
  },
  {
    "text": "so yeah that's so that's that's essentially at a very high level what's",
    "start": "593010",
    "end": "598320"
  },
  {
    "text": "it's how what's happening now going back now the problem with some of the I mean",
    "start": "598320",
    "end": "605970"
  },
  {
    "text": "languages can be strange right like where some of the like English is very",
    "start": "605970",
    "end": "613650"
  },
  {
    "text": "particular in terms of its structure and some of the languages where you can change the object in the subject and it",
    "start": "613650",
    "end": "620220"
  },
  {
    "text": "can still like the very strongly structured some language like Sanskrit",
    "start": "620220",
    "end": "626610"
  },
  {
    "text": "for example it's very structured so you can actually change all the parts and it'll still mean the same you can't do",
    "start": "626610",
    "end": "633870"
  },
  {
    "text": "that with English it's like subject verb object versus subject object verb and it's like that in English you may be",
    "start": "633870",
    "end": "640710"
  },
  {
    "text": "able to convey the meaning but you'll sound like Yoda correct so III haven't",
    "start": "640710",
    "end": "647730"
  },
  {
    "text": "taken Latin lessons I assume probably lap and given it it's an indo-european language has a similar structure so I'm",
    "start": "647730",
    "end": "658140"
  },
  {
    "text": "Tigger I'm taking a wild guess there well what happens is when you have",
    "start": "658140",
    "end": "666060"
  },
  {
    "text": "translating longer sentences it becomes very difficult because you don't have I can't remember a lot of that context so",
    "start": "666060",
    "end": "673740"
  },
  {
    "text": "it's like you know if I if I keep talking for the next ten minutes you're",
    "start": "673740",
    "end": "679320"
  },
  {
    "text": "not probably going to remember everything right you know your attention span your window of your attention span",
    "start": "679320",
    "end": "684510"
  },
  {
    "text": "is probably you know small I mean but you might mean everybody so millennial",
    "start": "684510",
    "end": "691620"
  },
  {
    "text": "so it's even smaller so so what what we",
    "start": "691620",
    "end": "698130"
  },
  {
    "text": "do is we add a mechanism called as attention which is kind of a neural",
    "start": "698130",
    "end": "703350"
  },
  {
    "text": "network that learns things that are important and unimportant to aid in the",
    "start": "703350",
    "end": "709830"
  },
  {
    "text": "translation so I think the best explanation I have found for this is it's kind of like let's say you go to a",
    "start": "709830",
    "end": "716850"
  },
  {
    "text": "buffet and you're smelling each of the individual dishes now by the end you you",
    "start": "716850",
    "end": "724050"
  },
  {
    "text": "kind of don't really remember how something you know the fourth item actually smelt like so you know we would",
    "start": "724050",
    "end": "733830"
  },
  {
    "text": "like to you know capture that and remember that and that's that can be",
    "start": "733830",
    "end": "742410"
  },
  {
    "text": "done by using a simple neural network as like an attention computer and you I",
    "start": "742410",
    "end": "750480"
  },
  {
    "text": "mean there are different mechanisms it could be as simple as using a multi-layer perceptron and if you",
    "start": "750480",
    "end": "756180"
  },
  {
    "text": "remember that from our first lesson the most simplest of network the multi-layer",
    "start": "756180",
    "end": "763950"
  },
  {
    "text": "perceptron the most Catholics name the most simple networks you can just call",
    "start": "763950",
    "end": "772890"
  },
  {
    "text": "it MLP makes it sound less intimidating no still intimidating especially after",
    "start": "772890",
    "end": "780210"
  },
  {
    "text": "you show me the math behind it little stick to this so simple again and poorer",
    "start": "780210",
    "end": "789510"
  },
  {
    "text": "encoder input sequence in this case we will give it like French or German it'll encode it'll create like in credit",
    "start": "789510",
    "end": "797370"
  },
  {
    "text": "states which will then go into an attend computer which is keeping track of what's important and then we give it a",
    "start": "797370",
    "end": "803710"
  },
  {
    "text": "decoder sequence so another input and then it'll give us",
    "start": "803710",
    "end": "808840"
  },
  {
    "text": "the decoded output okay so let's let's",
    "start": "808840",
    "end": "815410"
  },
  {
    "text": "really quickly look at what it's doing and then yeah if if folks are joining in",
    "start": "815410",
    "end": "821770"
  },
  {
    "text": "like hold on we'll do a we'll go install the code base and we'll see how simple",
    "start": "821770",
    "end": "828520"
  },
  {
    "text": "it is to actually do that in which translation me maybe I think yeah quick",
    "start": "828520",
    "end": "836320"
  },
  {
    "text": "quick whines you know the model the framework will checkpoint it will do",
    "start": "836320",
    "end": "841870"
  },
  {
    "text": "early stopping which means that if it sees that I can't improve any kind of stuff I'll to stop or if I think I've",
    "start": "841870",
    "end": "849250"
  },
  {
    "text": "achieved a good you know progress it'll stop right so all of that by we don't",
    "start": "849250",
    "end": "855910"
  },
  {
    "text": "have to code earlier we just had to I kind of guessed the number of epochs we you know change the learning rate the",
    "start": "855910",
    "end": "863320"
  },
  {
    "text": "model the framework does all that for us somewhere in doing yeah so we'll do",
    "start": "863320",
    "end": "870550"
  },
  {
    "start": "870000",
    "end": "918000"
  },
  {
    "text": "we'll we'll take a look at like you know data processing so what what is what is",
    "start": "870550",
    "end": "875560"
  },
  {
    "text": "that we want so kind of like we can think about interest so I'm gonna do a",
    "start": "875560",
    "end": "889270"
  },
  {
    "text": "little bit of formatting so that we can read better okay so let's say the",
    "start": "889270",
    "end": "896800"
  },
  {
    "text": "sheriff's closed that unchanged at you know 1825 but what we need to do first",
    "start": "896800",
    "end": "905350"
  },
  {
    "text": "is actually tokenized so that is you know take the individual words CC and",
    "start": "905350",
    "end": "913480"
  },
  {
    "text": "chop it and then create that word dictionary right so in case of Sakai all",
    "start": "913480",
    "end": "920920"
  },
  {
    "start": "918000",
    "end": "965000"
  },
  {
    "text": "we need to do is give it like a training corpus and it'll figure out so it will",
    "start": "920920",
    "end": "926080"
  },
  {
    "text": "run through all the words it'll make all the word dictionary and assign those",
    "start": "926080",
    "end": "931510"
  },
  {
    "text": "integer encoding which will be then converted to a one hot encoding we don't need to do that so",
    "start": "931510",
    "end": "938940"
  },
  {
    "text": "what I'm gonna do now is we'll start coding all right so where the the",
    "start": "938940",
    "end": "953410"
  },
  {
    "text": "project is available on github AWS labs Sakai it has instructions it's",
    "start": "953410",
    "end": "962830"
  },
  {
    "text": "it's under the Apache License and the underlying framework you know this is",
    "start": "962830",
    "end": "969280"
  },
  {
    "start": "965000",
    "end": "1023000"
  },
  {
    "text": "built on is an X net so your dependencies are quite minimal it's a",
    "start": "969280",
    "end": "975880"
  },
  {
    "text": "Python 3 project I know a friend who's gonna be happy with that numpy and",
    "start": "975880",
    "end": "982650"
  },
  {
    "text": "that's about it so let's actually go and install so I have an instance here and",
    "start": "982650",
    "end": "995190"
  },
  {
    "text": "that's it like I was easy",
    "start": "995190",
    "end": "999990"
  },
  {
    "text": "oops I do that there's a the",
    "start": "1005839",
    "end": "1014880"
  },
  {
    "text": "instructions on the GPU ones as well so I'll just do the simpler one for now",
    "start": "1014880",
    "end": "1024390"
  },
  {
    "text": "so to train right so it actually has",
    "start": "1024390",
    "end": "1029660"
  },
  {
    "text": "German to English translation and funny",
    "start": "1029660",
    "end": "1034709"
  },
  {
    "text": "now I mean that's it's kind of put a lot of the folks who do benchmarks on you know machine translation use this corpus",
    "start": "1034709",
    "end": "1040589"
  },
  {
    "text": "also the team that built this is in Berlin so so what we can do is let's",
    "start": "1040589",
    "end": "1052640"
  },
  {
    "text": "let's get the data so I I'm just kind of",
    "start": "1052640",
    "end": "1065669"
  },
  {
    "start": "1063000",
    "end": "1115000"
  },
  {
    "text": "following along here and I think Sakai is a there's an issue because it's",
    "start": "1065669",
    "end": "1071760"
  },
  {
    "text": "telling me that I need exactly M X net version 0.1 but that region doesn't seem",
    "start": "1071760",
    "end": "1078240"
  },
  {
    "text": "to be in pip anymore oh you might need to well why don't you like fix the",
    "start": "1078240",
    "end": "1087900"
  },
  {
    "text": "version like pip and I said equal to equal to one point zero point one oh",
    "start": "1087900",
    "end": "1098090"
  },
  {
    "text": "yeah no I mean I'm anyway I'll try and figure",
    "start": "1100160",
    "end": "1106310"
  },
  {
    "text": "I think it shouldn't be a problem I mean I'm on 11 so while I was doing hey maybe",
    "start": "1106310",
    "end": "1120980"
  },
  {
    "start": "1115000",
    "end": "1200000"
  },
  {
    "text": "you can post the link on the twitch channel for folks to if they want to",
    "start": "1120980",
    "end": "1126380"
  },
  {
    "text": "follow along and download the carpets you can actually do this on your local",
    "start": "1126380",
    "end": "1134300"
  },
  {
    "text": "machine as well it's just going to take a while but for the next example I'll",
    "start": "1134300",
    "end": "1141080"
  },
  {
    "text": "take a smaller data set and show how to do this so we're just downloading these",
    "start": "1141080",
    "end": "1164900"
  },
  {
    "text": "different corpuses so we're going to print we're gonna do German instead of French yeah German yeah I speak German",
    "start": "1164900",
    "end": "1176420"
  },
  {
    "text": "ah that's good each beans meal that's what I remember",
    "start": "1176420",
    "end": "1185650"
  },
  {
    "text": "sincere and OH I mean I didn't tell me something to say",
    "start": "1187450",
    "end": "1193040"
  },
  {
    "text": "and I will compare my version of translation with with the German translation we have another option",
    "start": "1193040",
    "end": "1201710"
  },
  {
    "start": "1200000",
    "end": "1270000"
  },
  {
    "text": "rather like if people want to get simpler we could just go to the movie dialogue Karpis there's probably going",
    "start": "1201710",
    "end": "1207860"
  },
  {
    "text": "to be more fun should we just do that instead sure",
    "start": "1207860",
    "end": "1214060"
  },
  {
    "text": "again it's are we worried about time for",
    "start": "1214060",
    "end": "1221890"
  },
  {
    "text": "yeah downloading the WMT yeah it has a",
    "start": "1221890",
    "end": "1227930"
  },
  {
    "text": "lot more steps yeah a lot more steps so",
    "start": "1227930",
    "end": "1233300"
  },
  {
    "text": "I just wonder like it'll be just easier and you know this is more of a fun thing",
    "start": "1233300",
    "end": "1241000"
  },
  {
    "text": "okay so what I'm gonna do is if people",
    "start": "1241000",
    "end": "1250340"
  },
  {
    "text": "want to do have you share that link",
    "start": "1250340",
    "end": "1261950"
  },
  {
    "text": "Randall so it's a b0 NOL slash dialogue",
    "start": "1261950",
    "end": "1267320"
  },
  {
    "text": "okay great so so okay let's talk about the data corpus actually here so it's a",
    "start": "1267320",
    "end": "1275990"
  },
  {
    "start": "1270000",
    "end": "1417000"
  },
  {
    "text": "it's like it's from CMU I'm sorry Carnell it's called a movie dialogue",
    "start": "1275990",
    "end": "1281300"
  },
  {
    "text": "carpets so what these folks have done is taken like dialogue between two folks",
    "start": "1281300",
    "end": "1287530"
  },
  {
    "text": "and like actually you know document to",
    "start": "1287530",
    "end": "1293960"
  },
  {
    "text": "that so I don't know remember a famous movie line and what's your favorite",
    "start": "1293960",
    "end": "1303140"
  },
  {
    "text": "movie and Star Wars sure yeah it's like",
    "start": "1303140",
    "end": "1311410"
  },
  {
    "text": "I don't know man I'm trying to remember yeah may the force be with you and yeah",
    "start": "1311410",
    "end": "1316940"
  },
  {
    "text": "I don't know I don't know the plea I don't know the pre dialogue that was said I can't remember now but it's it's",
    "start": "1316940",
    "end": "1323240"
  },
  {
    "text": "kind of those interaction dialogues that have so we have like essentially our",
    "start": "1323240",
    "end": "1331640"
  },
  {
    "text": "inputs are like the dialogue that was said dialogue said by person one and the",
    "start": "1331640",
    "end": "1337520"
  },
  {
    "text": "corresponding response to that dialogue so what we do is we learn kind of like",
    "start": "1337520",
    "end": "1344840"
  },
  {
    "text": "this is how questions get asked like are things having a movie and then here's",
    "start": "1344840",
    "end": "1350180"
  },
  {
    "text": "what the model will the model will learn to actually talk like folks have done in",
    "start": "1350180",
    "end": "1356840"
  },
  {
    "text": "the movie so that's what you're building okay from an application point of view",
    "start": "1356840",
    "end": "1364580"
  },
  {
    "text": "what are you can think is let's say you have a lot of customer I mean you're",
    "start": "1364580",
    "end": "1369590"
  },
  {
    "text": "recording a lot of customer calls and logs and so on right says a certain way people respond so you",
    "start": "1369590",
    "end": "1376280"
  },
  {
    "text": "can potentially if you have a clean data set you can potentially get the model to train where it's auto responding to",
    "start": "1376280",
    "end": "1383060"
  },
  {
    "text": "questions that are being asked by people",
    "start": "1383060",
    "end": "1386320"
  },
  {
    "text": "right I mean it's it's it's hard to do that but you know potentially we can",
    "start": "1388720",
    "end": "1395450"
  },
  {
    "text": "build that but again it's it's it's there are a few factors and one of the",
    "start": "1395450",
    "end": "1401420"
  },
  {
    "text": "factors is being having like a clean data and making sure that you know the",
    "start": "1401420",
    "end": "1407750"
  },
  {
    "text": "space that we're working in terms of the questions and things that can be asked isn't that isn't too much right so so",
    "start": "1407750",
    "end": "1416690"
  },
  {
    "text": "this is still an active area of research yeah it's interesting but let's get coding because we we have a limited",
    "start": "1416690",
    "end": "1422660"
  },
  {
    "start": "1417000",
    "end": "1477000"
  },
  {
    "text": "amount of time and I want to see what we can get done today okay okay okay",
    "start": "1422660",
    "end": "1437150"
  },
  {
    "text": "so let's let's do this ahead time and",
    "start": "1437150",
    "end": "1445730"
  },
  {
    "text": "then we'll see what it looks like - I'll",
    "start": "1445730",
    "end": "1458090"
  },
  {
    "text": "just do this dot a we're trying to find",
    "start": "1458090",
    "end": "1468380"
  },
  {
    "text": "like wait until he'll name that one now what there you go",
    "start": "1468380",
    "end": "1476590"
  },
  {
    "text": "that's trying to find like what's the best way to show both rim diff rim for the win so it's like I'm so sorry I'm",
    "start": "1476590",
    "end": "1488950"
  },
  {
    "start": "1477000",
    "end": "1525000"
  },
  {
    "text": "sorry to hear that yeah thanks did your father teach you humility he tried take it I mean things",
    "start": "1488950",
    "end": "1497570"
  },
  {
    "text": "like that right so it's a little cheesy and and so on",
    "start": "1497570",
    "end": "1505590"
  },
  {
    "text": "I think you you should see closer I mean for safety it's like I'd be safer",
    "start": "1505590",
    "end": "1511140"
  },
  {
    "text": "sleeping with that snake just as fun things and so to Train it's it's really",
    "start": "1511140",
    "end": "1526650"
  },
  {
    "start": "1525000",
    "end": "1695000"
  },
  {
    "text": "simple like we we need to go back to",
    "start": "1526650",
    "end": "1532080"
  },
  {
    "text": "Sakai right so we'll we'll have",
    "start": "1532080",
    "end": "1543660"
  },
  {
    "text": "something like I'm gonna do is just make",
    "start": "1543660",
    "end": "1551130"
  },
  {
    "text": "a copy again I already have the model",
    "start": "1551130",
    "end": "1561480"
  },
  {
    "text": "train since my show folks how to yeah train this that's a lot of files are",
    "start": "1561480",
    "end": "1574830"
  },
  {
    "text": "there any questions in the twitch channel I knew okay so so this is how we",
    "start": "1574830",
    "end": "1585630"
  },
  {
    "text": "can train we say Python actually should be Python 3 I think I do have suck yeah",
    "start": "1585630",
    "end": "1595740"
  },
  {
    "text": "so Sakai train so the next step is we give it the source right so so in this",
    "start": "1595740",
    "end": "1602760"
  },
  {
    "text": "case the source is gonna be like train a",
    "start": "1602760",
    "end": "1607580"
  },
  {
    "text": "and the target is gonna be train B and",
    "start": "1611420",
    "end": "1616910"
  },
  {
    "text": "similarly we need to have like the validation so yep so validation source",
    "start": "1616910",
    "end": "1623970"
  },
  {
    "text": "is train uh sorry just a validation target it was gonna be",
    "start": "1623970",
    "end": "1635510"
  },
  {
    "text": "now I'm actually running this on cheapy oh yeah I'm just a cpu so I'm",
    "start": "1635510",
    "end": "1643380"
  },
  {
    "text": "gonna still use a CPU to train I'm gonna",
    "start": "1643380",
    "end": "1649320"
  },
  {
    "text": "call it and that's it and it's learning",
    "start": "1649320",
    "end": "1659690"
  },
  {
    "text": "so let's see what it does right so look at this like it's got a lot of different",
    "start": "1659690",
    "end": "1667850"
  },
  {
    "text": "different parameters so it's picked some",
    "start": "1667850",
    "end": "1673049"
  },
  {
    "text": "of the default things and it just has you know taken that but we can tune that",
    "start": "1673049",
    "end": "1680820"
  },
  {
    "text": "as well so let's look at while the models training like as you can see here",
    "start": "1680820",
    "end": "1685980"
  },
  {
    "text": "right so it's it's set everything up and",
    "start": "1685980",
    "end": "1691289"
  },
  {
    "text": "now we're gonna see what are the options we have so as far as the model is",
    "start": "1691289",
    "end": "1698610"
  },
  {
    "start": "1695000",
    "end": "1775000"
  },
  {
    "text": "concerned we can say hey let's limit our vocabulary size to a certain thing right",
    "start": "1698610",
    "end": "1703799"
  },
  {
    "text": "so that's if Zanzibar only appears once we really don't know I mean we don't need to I consider things like that",
    "start": "1703799",
    "end": "1710309"
  },
  {
    "text": "right and or like min count right like if we can just filter everything that",
    "start": "1710309",
    "end": "1717210"
  },
  {
    "text": "appears only once or twice in our dictionary you can give the embedding",
    "start": "1717210",
    "end": "1723240"
  },
  {
    "text": "sighs so omitting size is like how long how large is the embedding vector so you",
    "start": "1723240",
    "end": "1730470"
  },
  {
    "text": "know if you have a larger corpus you might want to have a larger embedding space there's a smaller corpus you",
    "start": "1730470",
    "end": "1736320"
  },
  {
    "text": "really wanna have a smaller embedding space left there closer then the number",
    "start": "1736320",
    "end": "1742350"
  },
  {
    "text": "of hidden layers so you can you know tip you can specify how deep is your network",
    "start": "1742350",
    "end": "1749029"
  },
  {
    "text": "and then other stuff like attention that we talked about so you have I want to",
    "start": "1749029",
    "end": "1756450"
  },
  {
    "text": "stop for a second on attention so MLP is the multi-layer perceptron and then",
    "start": "1756450",
    "end": "1762330"
  },
  {
    "text": "there's coverage and I assume that means like how many nodes are being hit I mean",
    "start": "1762330",
    "end": "1770850"
  },
  {
    "text": "what are other dot what Ridge III haven't weighed haven't used",
    "start": "1770850",
    "end": "1777960"
  },
  {
    "start": "1775000",
    "end": "1844000"
  },
  {
    "text": "those I also yeah that's just different methods to do I thought you're talking",
    "start": "1777960",
    "end": "1786179"
  },
  {
    "text": "about so let's pull up something trying",
    "start": "1786179",
    "end": "1802380"
  },
  {
    "text": "to find like a good let's come back to",
    "start": "1802380",
    "end": "1809279"
  },
  {
    "text": "that later but okay sure it's just a different car each of them are a different kind of like network so I'm",
    "start": "1809279",
    "end": "1815700"
  },
  {
    "text": "trying to find like the definitions of yeah that's fine we can we can keep",
    "start": "1815700",
    "end": "1822570"
  },
  {
    "text": "going seek to seek translation okay",
    "start": "1822570",
    "end": "1831648"
  },
  {
    "text": "I just can't yeah sorry okay and then the rest of the stuff is",
    "start": "1836900",
    "end": "1845390"
  },
  {
    "start": "1844000",
    "end": "1972000"
  },
  {
    "text": "like you know stuff that we worked on which is the bad size right so we know",
    "start": "1845390",
    "end": "1851000"
  },
  {
    "text": "as we train the learning rate we need to adjust the batch size we can have dropouts",
    "start": "1851000",
    "end": "1856940"
  },
  {
    "text": "so that we don't over fit right so remember like we will turn off a few",
    "start": "1856940",
    "end": "1862010"
  },
  {
    "text": "neurons so that the model doesn't like all fit and learn too much and is biased",
    "start": "1862010",
    "end": "1867950"
  },
  {
    "text": "towards training data yeah we can use like a couple of different you know lost",
    "start": "1867950",
    "end": "1874490"
  },
  {
    "text": "functions it is a cross entropy or smooth cross entropy so things like that",
    "start": "1874490",
    "end": "1880690"
  },
  {
    "text": "so we have metrics so the attendee use different metrics for different tasks",
    "start": "1880690",
    "end": "1888020"
  },
  {
    "text": "when we're doing a machine translation we'll use a metric called as blue score",
    "start": "1888020",
    "end": "1894140"
  },
  {
    "text": "it stands for by lying bilingual it's",
    "start": "1894140",
    "end": "1899179"
  },
  {
    "text": "bilingual score I forget to preview at that but essentially what it's doing is it's it's saying comparing how close it",
    "start": "1899179",
    "end": "1906950"
  },
  {
    "text": "is how close the worlds are and depending upon that you know a score is",
    "start": "1906950",
    "end": "1912230"
  },
  {
    "text": "outputted which is between 0 and 1 and then we you know kind of calculate we'll",
    "start": "1912230",
    "end": "1918110"
  },
  {
    "text": "make an aggregate score perplexity is interesting it's it's kind of a metric",
    "start": "1918110",
    "end": "1927799"
  },
  {
    "text": "where we we take like our output and see how likely was it to appear in in our",
    "start": "1927799",
    "end": "1939460"
  },
  {
    "text": "original distribution so so it's kind of like when we can't do exact comparison",
    "start": "1939460",
    "end": "1948320"
  },
  {
    "text": "between words because they can sound similar so that's that's what a back city tries to capture so we can have a",
    "start": "1948320",
    "end": "1958340"
  },
  {
    "text": "couple of words that almost mean the same thing right like now if we do a direct comparison it doesn't hold up",
    "start": "1958340",
    "end": "1966049"
  },
  {
    "text": "well right so it's always going to be a 0 so we need something like that",
    "start": "1966049",
    "end": "1971820"
  },
  {
    "text": "so as you can see some of the metrics we",
    "start": "1971820",
    "end": "1976919"
  },
  {
    "start": "1972000",
    "end": "2014000"
  },
  {
    "text": "will be as you can see I think we yeah so we're optimizing for perplexity okay",
    "start": "1976919",
    "end": "1986629"
  },
  {
    "text": "now you can see our initial vocabulary was about 75,000 we we just use a min",
    "start": "1987019",
    "end": "1995460"
  },
  {
    "text": "frequency of 1 so we you know it's all started yeah and then we can see like",
    "start": "1995460",
    "end": "2005929"
  },
  {
    "text": "both of the words so after pruning we had 50,000 and then yeah gotcha",
    "start": "2005929",
    "end": "2013539"
  },
  {
    "text": "so also what these are called our sentence pairs so what we're doing is",
    "start": "2013539",
    "end": "2018710"
  },
  {
    "start": "2014000",
    "end": "2237000"
  },
  {
    "text": "doing a sentence pad conversion right so we're saying hey when somebody said I'm sorry like most of the times it just",
    "start": "2018710",
    "end": "2026690"
  },
  {
    "text": "meant that that's okay and so on so that's kind of what the model learns to convert between right so source words",
    "start": "2026690",
    "end": "2038049"
  },
  {
    "text": "yeah so whatever appeared in our sentence pair so is where a and beard like the",
    "start": "2038049",
    "end": "2045049"
  },
  {
    "text": "same pants pairs right so they're each each line corresponds to the corresponds",
    "start": "2045049",
    "end": "2051378"
  },
  {
    "text": "to the translation that we expect does that make sense",
    "start": "2051379",
    "end": "2057099"
  },
  {
    "text": "no now okay I I don't understood the",
    "start": "2057099",
    "end": "2062720"
  },
  {
    "text": "word sentence pair many times but you did not explain like why each line corresponds I guess right so it's a",
    "start": "2062720",
    "end": "2069470"
  },
  {
    "text": "sequence to a sequence model right so we're saying here is hello my name is",
    "start": "2069470",
    "end": "2075470"
  },
  {
    "text": "Reno and then right now sure thank you ms yeah so in case in case of",
    "start": "2075470",
    "end": "2081648"
  },
  {
    "text": "the translation so that that's that's called a sentence pair right so it's a",
    "start": "2081649",
    "end": "2089960"
  },
  {
    "text": "sentence pair because it's a one-to-one mapping of the to correct the exact translation in this case right so uh so",
    "start": "2089960",
    "end": "2097880"
  },
  {
    "text": "what we'll do is we will put if the framework expects that to be in two",
    "start": "2097880",
    "end": "2103190"
  },
  {
    "text": "different files where each line corresponds to its",
    "start": "2103190",
    "end": "2108270"
  },
  {
    "text": "sentence its other pair right actually do they have to be lined up then line by",
    "start": "2108270",
    "end": "2115590"
  },
  {
    "text": "line yeah so when you do write when you did it earlier I mean I noticed maybe",
    "start": "2115590",
    "end": "2121890"
  },
  {
    "text": "this was an effect of the wrapping but there were some things that have read the movie dialogue set that were way",
    "start": "2121890",
    "end": "2127410"
  },
  {
    "text": "longer than the things that were in the right because we're not doing a",
    "start": "2127410",
    "end": "2133050"
  },
  {
    "text": "translation right so in case of also they needn't be lined up or have the",
    "start": "2133050",
    "end": "2140340"
  },
  {
    "text": "same sequence then right like certain languages maybe a little more say verbose than other languages so you",
    "start": "2140340",
    "end": "2149370"
  },
  {
    "text": "might have like for example if we take Mandarin like you know you probably",
    "start": "2149370",
    "end": "2156330"
  },
  {
    "text": "write like a sentence in English which is comprised of 20 words and in Mandarin",
    "start": "2156330",
    "end": "2162180"
  },
  {
    "text": "you can probably write that in five or six words right now they needn't be of",
    "start": "2162180",
    "end": "2167670"
  },
  {
    "text": "the same length but all we are saying is",
    "start": "2167670",
    "end": "2172680"
  },
  {
    "text": "this given sentence this is the this is what we think the translation yeah okay",
    "start": "2172680",
    "end": "2178200"
  },
  {
    "text": "yeah okay so in this case the movie",
    "start": "2178200",
    "end": "2184830"
  },
  {
    "text": "dialogues it's like well you know people rambled on for to say like 20 seconds",
    "start": "2184830",
    "end": "2190380"
  },
  {
    "text": "and then well my reply was okay and then I noticed it's doing this replicating 29",
    "start": "2190380",
    "end": "2197160"
  },
  {
    "text": "random examples from the bucket and is that just trying to pad the different layers yeah so yeah so good good",
    "start": "2197160",
    "end": "2206340"
  },
  {
    "text": "extension good good pickup right like the are our samples have different",
    "start": "2206340",
    "end": "2213540"
  },
  {
    "text": "sequence length right so our dialogue as you notice we have some are like you",
    "start": "2213540",
    "end": "2218550"
  },
  {
    "text": "know 10 words on some are 20 25 and so on so to train them better what we do is",
    "start": "2218550",
    "end": "2224040"
  },
  {
    "text": "with a bucket then I will just put in two buckets and train each of them so that's that's bucketing gotcha yeah",
    "start": "2224040",
    "end": "2232410"
  },
  {
    "text": "and again that's done like it'll find the right you know",
    "start": "2232410",
    "end": "2237870"
  },
  {
    "text": "kind of that's using numpy or something like you know where exactly but yeah",
    "start": "2237870",
    "end": "2243630"
  },
  {
    "text": "like the distribution and it'll figure out the bucketing so what I'll do is this takes about like a couple of hours",
    "start": "2243630",
    "end": "2250170"
  },
  {
    "text": "to Train so now that I've shown you how you can chain let's go and do the",
    "start": "2250170",
    "end": "2256470"
  },
  {
    "text": "prediction okay all righty",
    "start": "2256470",
    "end": "2264320"
  },
  {
    "text": "it doesn't like can you control see it yeah that's right",
    "start": "2264410",
    "end": "2269670"
  },
  {
    "text": "and then just kill it yeah jobs so let",
    "start": "2269670",
    "end": "2286080"
  },
  {
    "text": "me go to the train model that I've had",
    "start": "2286080",
    "end": "2291320"
  },
  {
    "text": "so you can see here it actually like",
    "start": "2292310",
    "end": "2297600"
  },
  {
    "text": "ends up having a lot of different models which I deleted but then we'll pick a",
    "start": "2297600",
    "end": "2305700"
  },
  {
    "text": "model that says you know parameter dot best so you can see like our training",
    "start": "2305700",
    "end": "2312720"
  },
  {
    "text": "looks like something like this it looks at the perplexity score it's trying to optimize and the model says hey I",
    "start": "2312720",
    "end": "2320370"
  },
  {
    "text": "improved flexibly I'm keeping I'm improving and then it's like hey",
    "start": "2320370",
    "end": "2326210"
  },
  {
    "text": "validation perplexity is not you know improved in the last whatever I tration so I'm gonna make a smart decision say",
    "start": "2326210",
    "end": "2332280"
  },
  {
    "text": "hey this is all I can do given this data set and these parameters so I'm just going to stop here so the model the",
    "start": "2332280",
    "end": "2339570"
  },
  {
    "text": "framework will do all that for you gotcha okay I love that it's like you",
    "start": "2339570",
    "end": "2349590"
  },
  {
    "text": "know also the strike like a three leek semaphores I'm shut down as a score so",
    "start": "2349590",
    "end": "2361560"
  },
  {
    "text": "is this 1.0 3/5 is that pretty good so",
    "start": "2361560",
    "end": "2366990"
  },
  {
    "text": "let's see where we started with perplexity is if you want to minimize",
    "start": "2366990",
    "end": "2372420"
  },
  {
    "text": "right so it starts off saying hey things",
    "start": "2372420",
    "end": "2377640"
  },
  {
    "text": "we're predicting is like reading off its super off so so that's that's what it is",
    "start": "2377640",
    "end": "2383220"
  },
  {
    "text": "so we can see like immediate sander I'd like we started like as long as the",
    "start": "2383220",
    "end": "2388560"
  },
  {
    "text": "batches that's a good sign like you want to see like an exponential decay in your last function like that's when you're",
    "start": "2388560",
    "end": "2395430"
  },
  {
    "text": "like aha like it's it's it's working it's sort of working gotcha",
    "start": "2395430",
    "end": "2402230"
  },
  {
    "text": "okay that's prediction time translate so",
    "start": "2402230",
    "end": "2414390"
  },
  {
    "text": "this is all we need to do so give it the translate module set point it to the mod",
    "start": "2414390",
    "end": "2421350"
  },
  {
    "text": "model directory and say we're going to use CPU and it will load everything okay",
    "start": "2421350",
    "end": "2431460"
  },
  {
    "text": "and then we get a prompt and hopefully",
    "start": "2431460",
    "end": "2438150"
  },
  {
    "text": "that doesn't affect it's probably how",
    "start": "2438150",
    "end": "2444450"
  },
  {
    "text": "bigger those models depends on like the",
    "start": "2444450",
    "end": "2451590"
  },
  {
    "text": "day I've said this is probably like yeah there you go 500 Meg's other Meg's like",
    "start": "2451590",
    "end": "2462710"
  },
  {
    "text": "are there ways to shrink these models down yeah there are some advanced",
    "start": "2463100",
    "end": "2470970"
  },
  {
    "text": "techniques like where we don't need to keep all the precision so we work with like 32-bit floats we might not mean we",
    "start": "2470970",
    "end": "2479010"
  },
  {
    "text": "can just slice and you just ate just all the parameters there are other model compression techniques but these models",
    "start": "2479010",
    "end": "2488540"
  },
  {
    "text": "these sequence models of these called LST MS tend to be larger because you can",
    "start": "2488540",
    "end": "2497610"
  },
  {
    "text": "imagine you know remember like I talked about like the hidden state in the store so there's a state that gets stored",
    "start": "2497610",
    "end": "2504210"
  },
  {
    "text": "which is why the please get blown up yeah yeah",
    "start": "2504210",
    "end": "2511730"
  },
  {
    "text": "because you can see right like our training corpus was less like six Meg's",
    "start": "2511730",
    "end": "2518250"
  },
  {
    "text": "is like a total of like around six Maggie's okay so all right I'm gonna do",
    "start": "2518250",
    "end": "2533210"
  },
  {
    "text": "the translation hi it's it's not a fun",
    "start": "2533210",
    "end": "2551369"
  },
  {
    "text": "but Hager hi really maybe you can not",
    "start": "2551369",
    "end": "2569970"
  },
  {
    "text": "calm Uncas like unknown so ready to know it's a and it's like you don't know what",
    "start": "2569970",
    "end": "2575970"
  },
  {
    "text": "you're asking you're a killer alright so it's just like so you can see it does make some",
    "start": "2575970",
    "end": "2594660"
  },
  {
    "text": "sort of sense right like it understands it's trying to you know mimic what we",
    "start": "2594660",
    "end": "2600180"
  },
  {
    "text": "see in a movie right we dialogue it's a little glorified lets",
    "start": "2600180",
    "end": "2609680"
  },
  {
    "text": "it's very hemingway-esque I wanna now I",
    "start": "2615850",
    "end": "2623480"
  },
  {
    "text": "want to just take all of Hemingway's books all of this dialogue yeah my line by line it was cold and it was dark",
    "start": "2623480",
    "end": "2630430"
  },
  {
    "text": "yeah yeah - you're a half of - car in",
    "start": "2630430",
    "end": "2643010"
  },
  {
    "text": "the water I mean at least you can see like it associates cold and water right",
    "start": "2643010",
    "end": "2648370"
  },
  {
    "text": "you kind of see that and I don't know you have a dialogue before we before I",
    "start": "2648370",
    "end": "2655430"
  },
  {
    "text": "end this let's say may the force be with",
    "start": "2655430",
    "end": "2660920"
  },
  {
    "text": "you you haven't been able to keep the",
    "start": "2660920",
    "end": "2682119"
  },
  {
    "text": "Wow what are these models because I'm",
    "start": "2682900",
    "end": "2690770"
  },
  {
    "start": "2687000",
    "end": "3065000"
  },
  {
    "text": "gonna try and put it on lambda and try and make a little API that right I know",
    "start": "2690770",
    "end": "2696770"
  },
  {
    "text": "you have to get going but if you through this model up on s3 right correct I",
    "start": "2696770",
    "end": "2704000"
  },
  {
    "text": "shout it with you let me know if you can access it yeah let me try that right now",
    "start": "2704000",
    "end": "2711040"
  },
  {
    "text": "I'll stop my",
    "start": "2711430",
    "end": "2715420"
  },
  {
    "text": "I mean CD into a new directory here I guess I'll just switch over here so",
    "start": "2724820",
    "end": "2732980"
  },
  {
    "text": "people can see let's see",
    "start": "2732980",
    "end": "2744470"
  },
  {
    "text": "twitch Amazon",
    "start": "2744470",
    "end": "2754310"
  },
  {
    "text": "let's do twitch there let's do Sakai I",
    "start": "2754310",
    "end": "2760180"
  },
  {
    "text": "built something last night I'm just gonna call it mkdir lambda Sakai okay",
    "start": "2760180",
    "end": "2771020"
  },
  {
    "text": "yeah love you need to view guys history",
    "start": "2771020",
    "end": "2775960"
  },
  {
    "text": "yep I still don't have access by the way okay okay I think you look you'll need",
    "start": "2787300",
    "end": "2792890"
  },
  {
    "text": "to change that ECL to be public reader or something like that yeah I'm just to make it easier I'm just",
    "start": "2792890",
    "end": "2800440"
  },
  {
    "text": "building a tarball and let me send that yeah you should",
    "start": "2800440",
    "end": "2812150"
  },
  {
    "text": "just need the",
    "start": "2812150",
    "end": "2814809"
  },
  {
    "text": "and then Russia kabhi asks is there a notebook that can be shared yeah we'll definitely share the notebook yeah we",
    "start": "2825580",
    "end": "2834070"
  },
  {
    "text": "usually have our notebooks and everything on there to github so Samia",
    "start": "2834070",
    "end": "2842080"
  },
  {
    "text": "Mallya / DL - twitch - series so all the",
    "start": "2842080",
    "end": "2848110"
  },
  {
    "text": "notebooks like all the previous episodes I'll get uploaded here so I'll post the",
    "start": "2848110",
    "end": "2853990"
  },
  {
    "text": "link so in the meantime I'm gonna try",
    "start": "2853990",
    "end": "2862960"
  },
  {
    "text": "and build a lambda container that can",
    "start": "2862960",
    "end": "2871390"
  },
  {
    "text": "[Music] take all this stuff so I want to grab",
    "start": "2871390",
    "end": "2879340"
  },
  {
    "text": "okay so the first thing that I want to do is I want to install Python 3.6 on this Amazon Linux am i ll rifts freedo -",
    "start": "2879340",
    "end": "2893830"
  },
  {
    "text": "like Rita says I have an awesome prompt and you are great I do",
    "start": "2893830",
    "end": "2901380"
  },
  {
    "text": "alright Brandel I sent you I sent you the model let me know XS let me see here",
    "start": "2902490",
    "end": "2912450"
  },
  {
    "text": "[Music]",
    "start": "2913490",
    "end": "2916730"
  },
  {
    "text": "there's a very very large directory death",
    "start": "2930690",
    "end": "2937050"
  },
  {
    "text": "so I guess that'll download do you know why it's going so slow shouldn't it isn't there like an accelerate in point",
    "start": "2939550",
    "end": "2945070"
  },
  {
    "text": "or something that'll make this run faster give me an S to me yes well I",
    "start": "2945070",
    "end": "2951700"
  },
  {
    "text": "didn't put it on extra late so this regular okay well I'm gonna throw this",
    "start": "2951700",
    "end": "2958300"
  },
  {
    "text": "in the background first I'll make it",
    "start": "2958300",
    "end": "2964180"
  },
  {
    "text": "quiet I guess actually yeah I think",
    "start": "2964180",
    "end": "2974880"
  },
  {
    "text": "you're all set go yeah I think I'll get going given that you are all set yeah",
    "start": "2974880",
    "end": "2981820"
  },
  {
    "text": "we'll be back next week we'll we'll do time series modeling okay that sounds",
    "start": "2981820",
    "end": "2989170"
  },
  {
    "text": "terrifying but everything does it first yeah oh cool you scared I don't know if",
    "start": "2989170",
    "end": "2995590"
  },
  {
    "text": "people have other use cases let me know we're gonna do is we're gonna try and predict spot prices on AWS oh cool I",
    "start": "2995590",
    "end": "3002850"
  },
  {
    "text": "like that okay cool let me get going with this",
    "start": "3002850",
    "end": "3008760"
  },
  {
    "text": "thanks again for coming over I'm gonna play around with all of the translation",
    "start": "3008760",
    "end": "3014460"
  },
  {
    "text": "and stuff and see if I can build a little API for it I'll seal it alright",
    "start": "3014460",
    "end": "3022500"
  },
  {
    "text": "thank you thank you guys see ya okay",
    "start": "3022500",
    "end": "3030600"
  },
  {
    "text": "cool so give me one second here guys I'm just gonna throw together my my window where",
    "start": "3030600",
    "end": "3044820"
  },
  {
    "text": "there is this shindig it's video capture device and this gonna be",
    "start": "3044820",
    "end": "3055110"
  },
  {
    "text": "okie-dokie that's way too big all right",
    "start": "3055110",
    "end": "3066940"
  },
  {
    "text": "cool so we are all set so what we're doing now like where we are is we've built",
    "start": "3066940",
    "end": "3074860"
  },
  {
    "text": "this model and what I want to do is I want to figure out a way of putting it into lambda and I don't think I'm gonna",
    "start": "3074860",
    "end": "3080170"
  },
  {
    "text": "be successful today by the way so you guys should just take that up front I",
    "start": "3080170",
    "end": "3086280"
  },
  {
    "text": "think this is going to be fairly difficult to get working just because it",
    "start": "3086280",
    "end": "3091840"
  },
  {
    "text": "looks like this model with several gigs and lambda has a limit of about 4 500",
    "start": "3091840",
    "end": "3098050"
  },
  {
    "text": "Meg's that you can put into the container and 2/10 so I'm not entirely",
    "start": "3098050",
    "end": "3104890"
  },
  {
    "text": "sure how how I'll be able to to get that model over but I did notice it's like",
    "start": "3104890",
    "end": "3110710"
  },
  {
    "text": "the params best section was fairly small so if we look at how big all of this is",
    "start": "3110710",
    "end": "3120820"
  },
  {
    "text": "I mean I can get rid of Python 2 7 in numpy and I can get rid of I mean I can",
    "start": "3120820",
    "end": "3130060"
  },
  {
    "text": "get rid of just about everything here oh man I'm not even in the right directory",
    "start": "3130060",
    "end": "3138630"
  },
  {
    "text": "so I'm gonna go into this lambda Sakai directory and I'm about 70 megabytes",
    "start": "3138630",
    "end": "3146860"
  },
  {
    "text": "total right now and that includes MX net and Sakai and I keep thinking like maybe",
    "start": "3146860",
    "end": "3159960"
  },
  {
    "text": "let me let me try to do something here",
    "start": "3160680",
    "end": "3166260"
  },
  {
    "text": "pi - t dot",
    "start": "3170370",
    "end": "3175140"
  },
  {
    "text": "we're just gonna build yeah oh darn",
    "start": "3178749",
    "end": "3184180"
  },
  {
    "text": "maybe I should just provision a much larger like more powerful instance let me do that so let's just go get any",
    "start": "3184359",
    "end": "3193069"
  },
  {
    "text": "super powerful instance is this better",
    "start": "3193069",
    "end": "3210920"
  },
  {
    "text": "can you guys see better now where do you where do you want my face I can also",
    "start": "3210920",
    "end": "3220400"
  },
  {
    "text": "just get rid of my face so let's see what's just a crazy",
    "start": "3220400",
    "end": "3230559"
  },
  {
    "text": "ridiculous instant sized see-through",
    "start": "3230559",
    "end": "3239119"
  },
  {
    "text": "unikz large let's go for like a chi 3x large and how many BCPs does it have",
    "start": "3239119",
    "end": "3250660"
  },
  {
    "text": "their d2 GPUs 244 MX of ram I think that",
    "start": "3250660",
    "end": "3256700"
  },
  {
    "text": "should be just fine and I'll request a spot instance will say you know keep it",
    "start": "3256700",
    "end": "3265219"
  },
  {
    "text": "at like $1 per hour and then Queen",
    "start": "3265219",
    "end": "3275239"
  },
  {
    "text": "little bedbug asks can you just grab any resources you need for this demo and I",
    "start": "3275239",
    "end": "3280339"
  },
  {
    "text": "mean yeah that's one of the advantages of working at AWS now keep in mind I'm",
    "start": "3280339",
    "end": "3286849"
  },
  {
    "text": "using the same public crowd cloud that everybody else is using so it's it's",
    "start": "3286849",
    "end": "3294349"
  },
  {
    "text": "important that I'm kind of like frugal with these resources so I spin them up",
    "start": "3294349",
    "end": "3300140"
  },
  {
    "text": "and then I take them down when I'm done",
    "start": "3300140",
    "end": "3303609"
  },
  {
    "start": "3305000",
    "end": "3600000"
  },
  {
    "text": "I don't really keep it all running but",
    "start": "3305710",
    "end": "3311170"
  },
  {
    "text": "yeah come work at AWS and you can spin up whatever resources you want and then",
    "start": "3311170",
    "end": "3316809"
  },
  {
    "text": "eventually you get an email from your boss it's like hey I noticed that you've been spending a lot of time on this g3",
    "start": "3316809",
    "end": "3325780"
  },
  {
    "text": "instance and I can't help but notice that you've installed civ 5 on that g3 instance or Kerbal space program I'm",
    "start": "3325780",
    "end": "3335170"
  },
  {
    "text": "teasing that's never happened ok",
    "start": "3335170",
    "end": "3340270"
  },
  {
    "text": "so we'll wait a little bit for this instance to come around in the meantime I don't want to sit here and wait for an",
    "start": "3340270",
    "end": "3346329"
  },
  {
    "text": "umpire to install and all this other stuff to go down you know what I really should have done is I probably should",
    "start": "3346329",
    "end": "3352420"
  },
  {
    "text": "have used the deep learning ami because I would have had all this stuff pre-configured alrighty so I'll go",
    "start": "3352420",
    "end": "3370569"
  },
  {
    "text": "ahead and add this H into this thing",
    "start": "3370569",
    "end": "3374220"
  },
  {
    "text": "Rainman west and I'll say ec2 user at",
    "start": "3377250",
    "end": "3382780"
  },
  {
    "text": "this thing yes permission denied I'm guessing it's",
    "start": "3382780",
    "end": "3391359"
  },
  {
    "text": "still not done initializing so yeah this",
    "start": "3391359",
    "end": "3400240"
  },
  {
    "text": "is this is my plan what we're gonna do is we're going to take we're gonna do",
    "start": "3400240",
    "end": "3405390"
  },
  {
    "text": "we're gonna compile them pi we're going to compile Python 3 6 unless python 3 6",
    "start": "3405390",
    "end": "3413799"
  },
  {
    "text": "is already on here so I'll go ahead and python 3.6 amazon linux",
    "start": "3413799",
    "end": "3422010"
  },
  {
    "text": "[Music] that's for you oh it also helps if you",
    "start": "3430700",
    "end": "3455530"
  },
  {
    "text": "specify your key correctly pseudo yeah",
    "start": "3455530",
    "end": "3462610"
  },
  {
    "text": "I'm update - why and then we'll just say sudo yum install - why I think there's a",
    "start": "3462610",
    "end": "3469060"
  },
  {
    "text": "group install so let me I should not",
    "start": "3469060",
    "end": "3479110"
  },
  {
    "text": "have control seed bit sudo yum complete",
    "start": "3479110",
    "end": "3490240"
  },
  {
    "text": "transaction I love how fast this machine",
    "start": "3490240",
    "end": "3497110"
  },
  {
    "text": "goes if you run H top on this it's just beautiful",
    "start": "3497110",
    "end": "3502140"
  },
  {
    "text": "sudo yum update - line come on no why",
    "start": "3521690",
    "end": "3532829"
  },
  {
    "text": "did I do this - the uninstall Sheila C common package how did I get into the",
    "start": "3532829",
    "end": "3550740"
  },
  {
    "text": "state don't ever press ctrl C while updating everything look here we go",
    "start": "3550740",
    "end": "3559670"
  },
  {
    "text": "sudo package you know seriously",
    "start": "3559670",
    "end": "3567588"
  },
  {
    "text": "anybody know what the solution is here -",
    "start": "3570830",
    "end": "3579920"
  },
  {
    "text": "there we go I get that it's a duplicate",
    "start": "3581720",
    "end": "3598550"
  },
  {
    "text": "just overwrite it how did this happen",
    "start": "3598550",
    "end": "3610190"
  },
  {
    "text": "okay is it duplicate oh gee let's see I",
    "start": "3624110",
    "end": "3629780"
  },
  {
    "text": "just really want to destroy this this I'm just going to",
    "start": "3631430",
    "end": "3639920"
  },
  {
    "text": "processing dependency any clean dupes",
    "start": "3660190",
    "end": "3667750"
  },
  {
    "text": "okay let me try that can just delete",
    "start": "3667750",
    "end": "3711110"
  },
  {
    "text": "this with I just I don't know how I ruin an",
    "start": "3711110",
    "end": "3731630"
  },
  {
    "text": "instance within seconds of starting I'm",
    "start": "3731630",
    "end": "3751130"
  },
  {
    "text": "sorry guys one more time with feeling g3",
    "start": "3751130",
    "end": "3767779"
  },
  {
    "text": "and x-large that'll do it and I want one",
    "start": "3767779",
    "end": "3779059"
  },
  {
    "text": "terabyte since Sunil's models are huge I mean I was I I was in a state that I",
    "start": "3779059",
    "end": "3786680"
  },
  {
    "text": "probably could have fixed if I was gonna sit there in Google for an hour but I don't want to so I'm not going to",
    "start": "3786680",
    "end": "3793880"
  },
  {
    "text": "and I'm gonna make sure this shuts down tomorrow",
    "start": "3793880",
    "end": "3797859"
  },
  {
    "text": "I hate this little widget this is still",
    "start": "3803369",
    "end": "3834930"
  },
  {
    "text": "terminating but obviously yeah the real",
    "start": "3834930",
    "end": "3861150"
  },
  {
    "text": "issue is I press control-c in the middle of like some transaction check and then you have to go in and you have to like",
    "start": "3861150",
    "end": "3866880"
  },
  {
    "text": "find the exact package and manually remove it and there is a like magical",
    "start": "3866880",
    "end": "3875819"
  },
  {
    "text": "incantation that you can do to to get all that done but I didn't do that but I",
    "start": "3875819",
    "end": "3890749"
  },
  {
    "text": "think it's weird that you have to do all these steps to for Python 36",
    "start": "3892489",
    "end": "3898220"
  },
  {
    "text": "so we'll connect to this instance",
    "start": "3914560",
    "end": "3918730"
  },
  {
    "text": "there's some weird wrapping issue I",
    "start": "3931630",
    "end": "3937359"
  },
  {
    "text": "haven't so primo bedbug says that they've had squirrelly things happen even when you do everything right I",
    "start": "3965020",
    "end": "3971440"
  },
  {
    "text": "haven't really had that in years with the Amazon Linux am i I've definitely still had that occasionally with apt-get",
    "start": "3971440",
    "end": "3978460"
  },
  {
    "text": "on a bun - but CentOS tends to be better",
    "start": "3978460",
    "end": "3984620"
  },
  {
    "text": "at these sorts of things",
    "start": "3984620",
    "end": "3987940"
  },
  {
    "text": "okay so now we'll make some progress I won't control C at this time [Music]",
    "start": "3990709",
    "end": "3999020"
  },
  {
    "text": "doo doo doo everything should just work",
    "start": "3999439",
    "end": "4009400"
  },
  {
    "text": "there are a large number of things you get to do kind of I want to do sudo yum search Python 3 so yeah I mean it's like",
    "start": "4015400",
    "end": "4029150"
  },
  {
    "text": "Python 3 5 is the the biggest thing in the repo and enable repo it I wish that",
    "start": "4029150",
    "end": "4041329"
  },
  {
    "text": "there were just a package to install 3 6",
    "start": "4041329",
    "end": "4046119"
  },
  {
    "text": "before and I guess there tonight so this",
    "start": "4046479",
    "end": "4068749"
  },
  {
    "text": "should go pretty quick it's a lot faster and using like 82 micro yeah app getting",
    "start": "4068749",
    "end": "4077449"
  },
  {
    "text": "on a bun too has had bugs for the past several years it still has like some",
    "start": "4077449",
    "end": "4083179"
  },
  {
    "text": "rough edges there portions of it that are better than young but I mean",
    "start": "4083179",
    "end": "4090349"
  },
  {
    "text": "basically everything is better than emerge again - gin - however you want to",
    "start": "4090349",
    "end": "4096859"
  },
  {
    "text": "say it that's that's just actually thor's and this is saying that I have to",
    "start": "4096859",
    "end": "4107630"
  },
  {
    "text": "download and make my own Python SS open",
    "start": "4107630",
    "end": "4115008"
  },
  {
    "text": "SSL from source I don't really want to do that so let me just do Python 3.6 and I'll",
    "start": "4115009",
    "end": "4121600"
  },
  {
    "text": "follow their install instructions from here so I guess I can just go like this",
    "start": "4121600",
    "end": "4145739"
  },
  {
    "text": "so if you get tar",
    "start": "4147540",
    "end": "4152850"
  },
  {
    "text": "XE f Python Python that's fish and then",
    "start": "4152850",
    "end": "4166150"
  },
  {
    "text": "what are the filled instructions let's see a slash configure I'm not slacking",
    "start": "4166150",
    "end": "4186730"
  },
  {
    "text": "off my codes compiling so it needs to",
    "start": "4186730",
    "end": "4195460"
  },
  {
    "text": "boil some is Asia I don't need any of that so then I'll just go make actually",
    "start": "4195460",
    "end": "4201370"
  },
  {
    "text": "I'll just go make desu GA 32 you don't",
    "start": "4201370",
    "end": "4207520"
  },
  {
    "text": "really need that - Jade like 90% of the time anymore",
    "start": "4207520",
    "end": "4211920"
  },
  {
    "text": "and we can watch this make output scroll by very exciting stuff for working on",
    "start": "4215340",
    "end": "4220450"
  },
  {
    "text": "today and the reason that I'm doing this is that when I was working with 3/5 I",
    "start": "4220450",
    "end": "4226300"
  },
  {
    "text": "kept hitting these random errors in numpy with regard to the Python",
    "start": "4226300",
    "end": "4233170"
  },
  {
    "text": "threading struct and I'm thinking it's gonna be fixed in three six but if it's",
    "start": "4233170",
    "end": "4244420"
  },
  {
    "text": "not I'm going to be really frustrated",
    "start": "4244420",
    "end": "4247890"
  },
  {
    "text": "great so python 3.6 great so then we",
    "start": "4259350",
    "end": "4272950"
  },
  {
    "text": "should also have pit three six all right let's see what pip 3 version does yeah",
    "start": "4272950",
    "end": "4280570"
  },
  {
    "text": "so I'll just do pit three install okay so we built all of that so now let's go",
    "start": "4280570",
    "end": "4288490"
  },
  {
    "text": "look at how Sakai gets built Sakai the",
    "start": "4288490",
    "end": "4296050"
  },
  {
    "text": "first thing that I want to do is I want to use the latest version of it makes it I don't want to use the the 10 version that they're suggesting so lambda Sakai",
    "start": "4296050",
    "end": "4304690"
  },
  {
    "text": "so I'm gonna go into this directory I'm gonna say pip 3 install oops install - t",
    "start": "4304690",
    "end": "4314500"
  },
  {
    "text": "dot and that's gonna say install into this current local directory MX net",
    "start": "4314500",
    "end": "4320730"
  },
  {
    "text": "does it really need the ten point out version I guess it does",
    "start": "4326460",
    "end": "4334420"
  },
  {
    "text": "okay well we'll just do it this way then",
    "start": "4334420",
    "end": "4340680"
  },
  {
    "text": "install Sakai and then again we'll say -",
    "start": "4340680",
    "end": "4348790"
  },
  {
    "text": "t dot and that should put everything in the current directory and maybe you want",
    "start": "4348790",
    "end": "4360160"
  },
  {
    "text": "to turn on some more verbose output pip more verbose okay because it's not just",
    "start": "4360160",
    "end": "4374740"
  },
  {
    "text": "- V I think it's like past something Anna's it's like a build instruction",
    "start": "4374740",
    "end": "4388349"
  },
  {
    "text": "yeah so yeah basically now we're just doing a",
    "start": "4414460",
    "end": "4420619"
  },
  {
    "text": "whole lot of compiling I don't know how long this is gonna take the end Pied",
    "start": "4420619",
    "end": "4432290"
  },
  {
    "text": "sort there should be a wheel there",
    "start": "4432290",
    "end": "4441290"
  },
  {
    "text": "should be a Linux wheel for four no hi I",
    "start": "4441290",
    "end": "4446650"
  },
  {
    "text": "know there's a wheel on OS X and",
    "start": "4446650",
    "end": "4455079"
  },
  {
    "text": "basically what I'm trying to do is get all of these into one directory so that I can tar it up and build it into a",
    "start": "4455079",
    "end": "4461210"
  },
  {
    "text": "little package and then I can se P that over to my machine and I can just coat locally with the appropriate Linux",
    "start": "4461210",
    "end": "4469489"
  },
  {
    "text": "libraries and they're definitely things that we could do on the land beside to",
    "start": "4469489",
    "end": "4474829"
  },
  {
    "text": "make all of this easier but when you're dealing with machine learning and stuff",
    "start": "4474829",
    "end": "4480670"
  },
  {
    "text": "you're mostly dealing with code written by academics there's a terrible joke I should not go down this road somebody's",
    "start": "4480670",
    "end": "4486739"
  },
  {
    "text": "gonna shoot me and they just don't have the same things move slower they're not",
    "start": "4486739",
    "end": "4496699"
  },
  {
    "text": "as interested in getting things done",
    "start": "4496699",
    "end": "4504969"
  },
  {
    "text": "big businesses move slowly too this is fascinating",
    "start": "4513050",
    "end": "4519870"
  },
  {
    "text": "this is like really really fascinating stuff everybody compile well I can",
    "start": "4519870",
    "end": "4535590"
  },
  {
    "text": "always go check Facebook or something does even you have any questions",
    "start": "4535590",
    "end": "4540750"
  },
  {
    "text": "does anybody care I mean like this is the most boring thing I've ever screamed",
    "start": "4540750",
    "end": "4547550"
  },
  {
    "text": "I'm so sorry there's no pie bill linear",
    "start": "4547550",
    "end": "4557700"
  },
  {
    "text": "algebra libraries there are a couple of different things that it can include there's this thing called blasts which",
    "start": "4557700",
    "end": "4563520"
  },
  {
    "text": "is one of the linear algebra libraries but there's also one called catalyst and if you have those packages installed you",
    "start": "4563520",
    "end": "4570450"
  },
  {
    "text": "can do you can do like a couple of",
    "start": "4570450",
    "end": "4576270"
  },
  {
    "text": "different optimizations with your number pi sub and it'll include those shared optic libraries here we go and then",
    "start": "4576270",
    "end": "4584100"
  },
  {
    "text": "we're going okay cool so let's see how big this was so 72 Meg's total that's pretty reasonable",
    "start": "4584100",
    "end": "4591470"
  },
  {
    "text": "of course numpy is the majority of that and then we have not really anything",
    "start": "4591470",
    "end": "4601440"
  },
  {
    "text": "else but what I'm gonna do now is gonna try from Sakai and port",
    "start": "4601440",
    "end": "4610220"
  },
  {
    "text": "inference and it worked okay so the good news is everything in piled everything",
    "start": "4610220",
    "end": "4616260"
  },
  {
    "text": "in the right place we can now create our kind of test thing I really hope this",
    "start": "4616260",
    "end": "4627030"
  },
  {
    "text": "doesn't open on the you know oops",
    "start": "4627030",
    "end": "4632599"
  },
  {
    "text": "there we go that way you guys can't see my super secret email so now what I want to do is",
    "start": "4634460",
    "end": "4645480"
  },
  {
    "text": "I want to download that model that's Anil gave me so let's just do that very",
    "start": "4645480",
    "end": "4650489"
  },
  {
    "text": "quickly WB right now no W get and we're 20 megabits per",
    "start": "4650489",
    "end": "4670950"
  },
  {
    "text": "second that won't take too long and it's",
    "start": "4670950",
    "end": "4680760"
  },
  {
    "text": "1.2 gigs so what I'm gonna have to do is within the lambda function itself I'm",
    "start": "4680760",
    "end": "4686160"
  },
  {
    "text": "gonna have to download this via s3 so I",
    "start": "4686160",
    "end": "4693930"
  },
  {
    "text": "hope I hope this leg isn't I hope I can",
    "start": "4693930",
    "end": "4700830"
  },
  {
    "text": "get this in another way too I'm gonna try in just a second to get it a different way or the commands too low so",
    "start": "4700830",
    "end": "4713840"
  },
  {
    "text": "I one second here let me let me try and fix this I don't know why it's not the",
    "start": "4713840",
    "end": "4720780"
  },
  {
    "text": "right sides",
    "start": "4720780",
    "end": "4724610"
  },
  {
    "text": "that should be better so I got that",
    "start": "4740850",
    "end": "4755650"
  },
  {
    "text": "Sakai model I want to just try something really fast I'm gonna rewrite this command AWS s3 CP test tgz so let me run",
    "start": "4755650",
    "end": "4782500"
  },
  {
    "text": "this command locally really fast and see if I can get it working there great yeah",
    "start": "4782500",
    "end": "4796630"
  },
  {
    "text": "that way cool cool so that's basically what we're gonna do in the lambda function Queen mo bedbug",
    "start": "4796630",
    "end": "4805870"
  },
  {
    "text": "asks are there any restrictions on what resources your code can access from within a lambda call yeah there are a",
    "start": "4805870",
    "end": "4812020"
  },
  {
    "text": "couple of lambda limits I'll show them to you AWS lamda limits so you allocate memory",
    "start": "4812020",
    "end": "4827739"
  },
  {
    "text": "and 64 mega mega byte increments you have a ephemeral disk space of 512",
    "start": "4827739",
    "end": "4833080"
  },
  {
    "text": "megabytes you can only have a thousand 24,000 scripters which that that's like",
    "start": "4833080",
    "end": "4840580"
  },
  {
    "text": "here you book your you limit setting and Linux if you're familiar with that so you I think the default limit is",
    "start": "4840580",
    "end": "4848739"
  },
  {
    "text": "actually higher than 1000 now anyway the",
    "start": "4848739",
    "end": "4853870"
  },
  {
    "text": "number of processes and threads is 1,024 and your payload size",
    "start": "4853870",
    "end": "4862960"
  },
  {
    "text": "can't be more than six Meg's which is huge interesting cool okay cool let's",
    "start": "4862960",
    "end": "4886350"
  },
  {
    "text": "since everything seems to be working ish ish let's take Sunil's model here",
    "start": "4886350",
    "end": "4895740"
  },
  {
    "text": "tarek see their ex yeah XV f sakai I",
    "start": "4895740",
    "end": "4903990"
  },
  {
    "text": "probably should have thrown this on for both so we could have seen how it was going along I'm hoping it doesn't take",
    "start": "4906300",
    "end": "4912250"
  },
  {
    "text": "too long though I also might use a different compression algorithm yeah so",
    "start": "4912250",
    "end": "4927330"
  },
  {
    "text": "471 megabytes for pram top best which is the only thing we really need sudo yum",
    "start": "4927330",
    "end": "4935140"
  },
  {
    "text": "install that looks really um and still early let's do hip 3 install ipython pseudo",
    "start": "4935140",
    "end": "4949330"
  },
  {
    "text": "bang bang which pip3 pseudos that she",
    "start": "4949330",
    "end": "4962650"
  },
  {
    "text": "uses in 3 install ipython okay so we're",
    "start": "4962650",
    "end": "4975670"
  },
  {
    "text": "gonna have ipython your history will not",
    "start": "4975670",
    "end": "4981820"
  },
  {
    "text": "be saved well oh well and we're gonna do",
    "start": "4981820",
    "end": "4987400"
  },
  {
    "text": "import Sakai dot inference",
    "start": "4987400",
    "end": "4993929"
  },
  {
    "text": "from Sakai import inference and this",
    "start": "4999090",
    "end": "5005850"
  },
  {
    "text": "doesn't have the right directory Python 3 oh no it does it does so let me let me",
    "start": "5005850",
    "end": "5014490"
  },
  {
    "text": "copy / am stop best into Williams uh",
    "start": "5014490",
    "end": "5019740"
  },
  {
    "text": "Sakai and we're gonna say import Sakai",
    "start": "5019740",
    "end": "5030710"
  },
  {
    "text": "inference we're gonna say import and maximin as MX and context is going to be",
    "start": "5030710",
    "end": "5039000"
  },
  {
    "text": "in X that CPU and then we need to create",
    "start": "5039000",
    "end": "5047190"
  },
  {
    "text": "the translator object and I don't really know how to do that yet that is",
    "start": "5047190",
    "end": "5055980"
  },
  {
    "text": "something payloads her talks asks hey",
    "start": "5055980",
    "end": "5061470"
  },
  {
    "text": "what are you working on and what we're doing is we're trying to build a lambda",
    "start": "5061470",
    "end": "5067620"
  },
  {
    "text": "machine learning model so it's pretty easy to put most models into lambda with",
    "start": "5067620",
    "end": "5072870"
  },
  {
    "text": "like lambda MX net but getting all of",
    "start": "5072870",
    "end": "5077970"
  },
  {
    "text": "this Sakai and other stuff and numpy and all this other things into lambda",
    "start": "5077970",
    "end": "5083910"
  },
  {
    "text": "requires some finagling and getting it down to the right size so then cream Oh bed bug asked payload",
    "start": "5083910",
    "end": "5093510"
  },
  {
    "text": "size as in what your pass to lambda which is something that i was covering a little bit ago when I was going over the",
    "start": "5093510",
    "end": "5099270"
  },
  {
    "text": "lambda limits so with the async invitation the event type invocation you have 128 K as your limit when you submit",
    "start": "5099270",
    "end": "5106920"
  },
  {
    "text": "something to lambda with the request response model you have six megabytes so",
    "start": "5106920",
    "end": "5112260"
  },
  {
    "text": "that would allow you to pass in an image is a base64 encoded string for instance I would never recommend that but",
    "start": "5112260",
    "end": "5119420"
  },
  {
    "text": "I mean you can and in fact I do that in a couple places for like the cloud ninja",
    "start": "5119420",
    "end": "5125600"
  },
  {
    "text": "bot okay so we're gonna create a chance later and I'm gonna get this wrong by",
    "start": "5125600",
    "end": "5131000"
  },
  {
    "text": "the way at least the first few times translator context linear and the this",
    "start": "5131000",
    "end": "5149810"
  },
  {
    "text": "is just the type of model that it is and then I'm gonna unpack the arguments of",
    "start": "5149810",
    "end": "5157750"
  },
  {
    "text": "yeah so I want to do Sakai let's say model stuff equals Sakai dot",
    "start": "5157750",
    "end": "5170810"
  },
  {
    "text": "inference load models context 100 is",
    "start": "5170810",
    "end": "5178940"
  },
  {
    "text": "gonna be the input length 5 is gonna be the beam size and then we'll just throw",
    "start": "5178940",
    "end": "5184160"
  },
  {
    "text": "everything in actually the directory is",
    "start": "5184160",
    "end": "5195800"
  },
  {
    "text": "just gonna be like here for now it's just gonna be here but on lambda will change this dot to be slashed him but",
    "start": "5195800",
    "end": "5202430"
  },
  {
    "text": "for now I just will keep it as done oh my god I'm gonna get all of this wrong",
    "start": "5202430",
    "end": "5211390"
  },
  {
    "text": "okay so I need more than what I had so",
    "start": "5212020",
    "end": "5218300"
  },
  {
    "text": "let me just copy all of CD or CP the",
    "start": "5218300",
    "end": "5225520"
  },
  {
    "text": "lambda the model vocab",
    "start": "5225520",
    "end": "5233230"
  },
  {
    "text": "I'll just copy everything into here so",
    "start": "5234679",
    "end": "5247640"
  },
  {
    "text": "how big are we now I've been up 46 bags so yeah if we don't keep the training state that actually",
    "start": "5247640",
    "end": "5254480"
  },
  {
    "text": "isn't too bad but we're still too big so we're gonna have to do some some crazy",
    "start": "5254480",
    "end": "5261380"
  },
  {
    "text": "stuff here Roby it's like rip out libraries we're not using so I'm gonna",
    "start": "5261380",
    "end": "5267560"
  },
  {
    "text": "resume and this model was trained with",
    "start": "5267560",
    "end": "5279710"
  },
  {
    "text": "the wrong version or an older version of Sakai and that is where I am done no",
    "start": "5279710",
    "end": "5298850"
  },
  {
    "text": "version five found",
    "start": "5298850",
    "end": "5301900"
  },
  {
    "text": "so there's no version file found okay wait wait wait I can I can figure this out so no",
    "start": "5307679",
    "end": "5321909"
  },
  {
    "text": "version file found in the next minute",
    "start": "5321909",
    "end": "5328199"
  },
  {
    "text": "Sakai so can i specify a version is the",
    "start": "5330150",
    "end": "5342579"
  },
  {
    "text": "real question here let's see let's see",
    "start": "5342579",
    "end": "5362699"
  },
  {
    "text": "and so I think I might need this config",
    "start": "5362699",
    "end": "5368770"
  },
  {
    "text": "okay okay I can do that so let me say CP",
    "start": "5368770",
    "end": "5376469"
  },
  {
    "text": "model config oh no the King pig is there",
    "start": "5376590",
    "end": "5382829"
  },
  {
    "text": "so what's in this configuration",
    "start": "5382829",
    "end": "5386550"
  },
  {
    "text": "if anybody's machine learning expert and they know how to specify this version",
    "start": "5410940",
    "end": "5419260"
  },
  {
    "text": "size off the top of their head it would save us some time Sakai no it's not like a it's a model",
    "start": "5419260",
    "end": "5429388"
  },
  {
    "text": "model so variation isn't specified no",
    "start": "5430320",
    "end": "5453039"
  },
  {
    "text": "versions file",
    "start": "5453039",
    "end": "5455639"
  },
  {
    "text": "let me just see if there's a way of hard-coding this we can look at the",
    "start": "5468599",
    "end": "5477309"
  },
  {
    "text": "source",
    "start": "5477309",
    "end": "5479579"
  },
  {
    "text": "checks given duration against code versions appendage Ehrman compatibility",
    "start": "5490199",
    "end": "5496409"
  },
  {
    "text": "so where is it getting I guess we have to find out all the places that load version is called from version I mean",
    "start": "5503849",
    "end": "5531249"
  },
  {
    "text": "he'll I could just change that I could just say in the interest of getting this working let's just completely make",
    "start": "5531249",
    "end": "5540670"
  },
  {
    "text": "terrible decisions version and I'll",
    "start": "5540670",
    "end": "5556239"
  },
  {
    "text": "change this to 1/7",
    "start": "5556239",
    "end": "5559860"
  },
  {
    "text": "huh that's weird normally reload works but it's not",
    "start": "5581710",
    "end": "5588320"
  },
  {
    "text": "working this time be fresh",
    "start": "5588320",
    "end": "5593929"
  },
  {
    "text": "I swear reload almost always works",
    "start": "5593929",
    "end": "5599559"
  },
  {
    "text": "reload Sakai huh huh I am confused and",
    "start": "5600789",
    "end": "5623380"
  },
  {
    "text": "inference",
    "start": "5623380",
    "end": "5626380"
  },
  {
    "text": "okay well at this point and of course I",
    "start": "5640909",
    "end": "5648719"
  },
  {
    "text": "have no history so we'll just we'll",
    "start": "5648719",
    "end": "5654560"
  },
  {
    "text": "we'll start keeping all this over here",
    "start": "5654560",
    "end": "5665550"
  },
  {
    "text": "we'll say import Sakai dot inference and we'll say import MX net as MX and",
    "start": "5665550",
    "end": "5675500"
  },
  {
    "text": "contacts equals MX dot yeah that will choose GPU and CPU pronounced and",
    "start": "5675500",
    "end": "5682580"
  },
  {
    "text": "translator equals Sakai I mean this we don't know how to do yet model stuff",
    "start": "5682580",
    "end": "5689310"
  },
  {
    "text": "equals Sakai inference no it's gonna be",
    "start": "5689310",
    "end": "5700409"
  },
  {
    "text": "that context object that we were trying to load in inference dot load models",
    "start": "5700409",
    "end": "5710150"
  },
  {
    "text": "context Python and",
    "start": "5710150",
    "end": "5718400"
  },
  {
    "text": "context linear and for now I'll just go",
    "start": "5727540",
    "end": "5736880"
  },
  {
    "text": "here and let's just see how far along that gets us oh I didn't have the like",
    "start": "5736880",
    "end": "5753140"
  },
  {
    "text": "max input length and beam size and all that other stuff oops and I also have tabs so after",
    "start": "5753140",
    "end": "5775130"
  },
  {
    "text": "context after linear it's not the right thing we want 100 and we want 5 yeah",
    "start": "5775130",
    "end": "5796880"
  },
  {
    "text": "this is basically unusable load config",
    "start": "5796880",
    "end": "5802000"
  },
  {
    "text": "object you had frozen dictionary objects",
    "start": "5802000",
    "end": "5807860"
  },
  {
    "text": "I mean that's just that it doesn't they have pickle I mean that's like a pie",
    "start": "5807860",
    "end": "5817970"
  },
  {
    "text": "yeah mole",
    "start": "5817970",
    "end": "5820480"
  },
  {
    "text": "okay I'm calling it I will try and figure this out later too many different",
    "start": "5836120",
    "end": "5845489"
  },
  {
    "text": "random broken things I mean why is config that PI is calling object",
    "start": "5845489",
    "end": "5851910"
  },
  {
    "text": "underscore at frozen I mean I like I just want to know what they think they're supposed to be doing they're",
    "start": "5851910",
    "end": "5858270"
  },
  {
    "text": "like if we go to Sakai when we go to config",
    "start": "5858270",
    "end": "5863870"
  },
  {
    "text": "I'm pretty sure this doesn't work as I like I think the straight-up doesn't work oh well I'll figure it out okay this is",
    "start": "5893150",
    "end": "5906300"
  },
  {
    "text": "disappointing bye everybody",
    "start": "5906300",
    "end": "5909829"
  }
]