[
  {
    "start": "0",
    "end": "88000"
  },
  {
    "text": "thank you everybody for joining we're going to talk about building a data Lake",
    "start": "1160",
    "end": "6560"
  },
  {
    "text": "on AWS I'm Ian Myers I'm a solution architect based in the UK and I focus on",
    "start": "6560",
    "end": "13320"
  },
  {
    "text": "big data and analytics problems that are largest and uh most you know complicated",
    "start": "13320",
    "end": "19000"
  },
  {
    "text": "customer use cases that we can come up with and I'm really glad to be able to talk about data leg today because it's",
    "start": "19000",
    "end": "25480"
  },
  {
    "text": "one of the things that's just really popping up quite a lot and uh I I think we need to take some steps to Define it",
    "start": "25480",
    "end": "33440"
  },
  {
    "text": "and help make it real for our customers data lake is one of these sort of terms that's being uh confused and confounded",
    "start": "33440",
    "end": "40879"
  },
  {
    "text": "in ways that you know cloud and big data have historically and it's really important for us to deal with something",
    "start": "40879",
    "end": "47120"
  },
  {
    "text": "concrete so our objective today will be to give you an idea of what we see as",
    "start": "47120",
    "end": "52199"
  },
  {
    "text": "the data Lake Challenge and give you some patterns that you can use to try and solve that",
    "start": "52199",
    "end": "58000"
  },
  {
    "text": "problem for your customers internal or",
    "start": "58000",
    "end": "63760"
  },
  {
    "text": "external so once upon a time and perhaps some of you are still there",
    "start": "65520",
    "end": "70920"
  },
  {
    "text": "today the Enterprise data warehouse was the center of the universe it gave us a lot of",
    "start": "70920",
    "end": "79960"
  },
  {
    "text": "benefits that we have taken advantage of for 20",
    "start": "79960",
    "end": "85520"
  },
  {
    "text": "30 years those benefits included things",
    "start": "85520",
    "end": "91000"
  },
  {
    "start": "88000",
    "end": "88000"
  },
  {
    "text": "like a self-documenting schema for our data very concrete and enforced data",
    "start": "91000",
    "end": "98439"
  },
  {
    "text": "types so that people couldn't make mistakes and interpret data in the wrong",
    "start": "98439",
    "end": "103560"
  },
  {
    "text": "way normalization rules that helped us have consistency with how we expressed",
    "start": "103560",
    "end": "110360"
  },
  {
    "text": "business relationships it gave us a very common ubiquitous security",
    "start": "110360",
    "end": "116840"
  },
  {
    "text": "model and then of course it was very simple access with a tool like",
    "start": "116840",
    "end": "123119"
  },
  {
    "text": "SQL and it gave us some interesting capabilities around the transactionality that we worked with our data so that we",
    "start": "123119",
    "end": "129360"
  },
  {
    "text": "could entrust it with storing the financial Systems records from our",
    "start": "129360",
    "end": "135000"
  },
  {
    "text": "transaction processing systems these are all really important business",
    "start": "135000",
    "end": "141120"
  },
  {
    "text": "attributes that we have to consider when we think about building a data Lake and",
    "start": "141120",
    "end": "147200"
  },
  {
    "text": "defining a data Lake and setting priorities for what a data Lake even needs to",
    "start": "147200",
    "end": "153280"
  },
  {
    "text": "do one of the things that an Enterprise data warehouse does is it very tightly couples compute",
    "start": "153319",
    "end": "160760"
  },
  {
    "text": "and storage the idea is that you push your business logic into that data warehouse",
    "start": "160760",
    "end": "168599"
  },
  {
    "text": "that you build stored procedures and Views that sit right alongside your data",
    "start": "168599",
    "end": "174440"
  },
  {
    "text": "and you scale that environment to meet your business objectives by adding cores and RAM and you have",
    "start": "174440",
    "end": "180360"
  },
  {
    "text": "the storage associated with that being highly scalable highly",
    "start": "180360",
    "end": "186840"
  },
  {
    "text": "available and that's gone a really long way to solving very complex business",
    "start": "188000",
    "end": "193200"
  },
  {
    "text": "problems but our customers have other",
    "start": "193200",
    "end": "198760"
  },
  {
    "text": "requirements they want to be more agile they want to be able to work with data that doesn't necessarily fit",
    "start": "199440",
    "end": "206000"
  },
  {
    "text": "into a pre-specified schema",
    "start": "206000",
    "end": "211120"
  },
  {
    "text": "they want to use pure in memory tools they want to do natural language",
    "start": "211120",
    "end": "216360"
  },
  {
    "text": "processing they have a particular requirement to solve a graph problem and they struggle to do that",
    "start": "216360",
    "end": "223640"
  },
  {
    "text": "with a tool like SQL and so we need to consider how we've",
    "start": "223640",
    "end": "229760"
  },
  {
    "text": "moved forward and we see that there was the rise of the hup",
    "start": "229760",
    "end": "235159"
  },
  {
    "text": "ecosystem and on AWS elastic map produce provides us",
    "start": "235159",
    "end": "240599"
  },
  {
    "text": "a runtime for that ecosystem that leverages the storage environment on S3",
    "start": "240599",
    "end": "245640"
  },
  {
    "text": "and lets us build highly robust scalable and",
    "start": "245640",
    "end": "250959"
  },
  {
    "text": "diverse engines that can solve lots of different problems now one of the things",
    "start": "250959",
    "end": "258280"
  },
  {
    "text": "that the Hadoop ecosystem and the Big Data solution to a problem helped us",
    "start": "258280",
    "end": "263919"
  },
  {
    "text": "do was to take that tightly coupled compute and storage and break it apart",
    "start": "263919",
    "end": "271280"
  },
  {
    "text": "so that we could run lots and lots of compute on a system that was independent",
    "start": "271280",
    "end": "278680"
  },
  {
    "text": "of the job that you were trying to accomplish and so in some",
    "start": "278680",
    "end": "285639"
  },
  {
    "text": "ways Big Data technologies have gone a long way towards helping us implement",
    "start": "285639",
    "end": "291199"
  },
  {
    "text": "this concept of a data Lake but is it enough to say that our Hadoop environment is our data",
    "start": "291199",
    "end": "298320"
  },
  {
    "text": "Lake well no it isn't we need to do",
    "start": "298320",
    "end": "303960"
  },
  {
    "text": "more and when we think about the benefits of separating out compute and",
    "start": "303960",
    "end": "310720"
  },
  {
    "start": "305000",
    "end": "305000"
  },
  {
    "text": "storage we're able to get a lot more cost efficiency because we're focusing on",
    "start": "310720",
    "end": "318319"
  },
  {
    "text": "providing the necessary compute just to fulfill the storage requirements not",
    "start": "318319",
    "end": "323360"
  },
  {
    "text": "having everything glommed in together we can do really interesting",
    "start": "323360",
    "end": "328520"
  },
  {
    "text": "things like independent cost attribution for different pieces of data within the",
    "start": "328520",
    "end": "334360"
  },
  {
    "text": "storage environment rather than looking at an Enterprise data warehouse and saying well there's an overall cost for that",
    "start": "334360",
    "end": "340120"
  },
  {
    "text": "Enterprise data warehouse we can look at what it costs us to store an individual business units data and service their",
    "start": "340120",
    "end": "346479"
  },
  {
    "text": "compute requirements this sort of architecture gives us the ability to choose the right",
    "start": "346479",
    "end": "353039"
  },
  {
    "text": "tool to solve a problem at the right time running elastic map reduce clusters",
    "start": "353039",
    "end": "359960"
  },
  {
    "text": "with spark Presto adup streaming hbas whatever you require is available on",
    "start": "359960",
    "end": "366319"
  },
  {
    "text": "demand and you choose that tool because it's the right way of expressing a solution to a",
    "start": "366319",
    "end": "372599"
  },
  {
    "text": "problem we also have this really nice ability to increase the durability of our data we use that term a lot",
    "start": "372599",
    "end": "379759"
  },
  {
    "text": "durability what does that mean it means how safe is our",
    "start": "379759",
    "end": "384800"
  },
  {
    "text": "data and how do we provide extremely high durability to our data",
    "start": "384800",
    "end": "391479"
  },
  {
    "text": "without doing a lot of operations and managing that on a Hadoop",
    "start": "391479",
    "end": "397080"
  },
  {
    "text": "cluster and we can also move towards this idea that I think data Lake really tries to achieve which is a common",
    "start": "397080",
    "end": "404440"
  },
  {
    "text": "model for data storage without enforcing any type of access",
    "start": "404440",
    "end": "410400"
  },
  {
    "text": "method so the Big Data Technologies are really a start towards building a data",
    "start": "410400",
    "end": "417440"
  },
  {
    "text": "Lake but we need to talk about what are the other things that give us the same kind of benefits that an Enterprise data",
    "start": "417440",
    "end": "423840"
  },
  {
    "text": "warehouse has delivered for years and go much further than just what we can get from a big data",
    "start": "423840",
    "end": "431680"
  },
  {
    "text": "solution if we compare Big Data versus an Enterprise",
    "start": "431960",
    "end": "437000"
  },
  {
    "text": "data warehouse apologies you know they're very",
    "start": "437000",
    "end": "443160"
  },
  {
    "text": "complimentary and you can expect that your big data environment may be",
    "start": "443160",
    "end": "448199"
  },
  {
    "text": "producing data for your Enterprise dat a warehouse and separating compute and",
    "start": "448199",
    "end": "454039"
  },
  {
    "text": "storage gives us some really great benefits like providing a model on read semantic",
    "start": "454039",
    "end": "462000"
  },
  {
    "text": "versus a model on write that we have to use when we want to injust data into our relational",
    "start": "462000",
    "end": "469280"
  },
  {
    "text": "database Big Data Solutions give us much better agility we should be able to ingest data very very quickly because we",
    "start": "469759",
    "end": "476159"
  },
  {
    "text": "don't have to go through an analysis process to pull that data to in and make it available to our end",
    "start": "476159",
    "end": "484199"
  },
  {
    "text": "users and if we look at what an Enterprise data warehouse was good for it's really really good at",
    "start": "484560",
    "end": "490479"
  },
  {
    "text": "reporting it's really good at business intelligence but as soon as we want to do things like",
    "start": "490479",
    "end": "496479"
  },
  {
    "text": "predictions data science plus those reporting use cases we start to get into some very challenging",
    "start": "496479",
    "end": "504639"
  },
  {
    "text": "Solutions also as many of you I'm sure will have encountered we typically have to make a",
    "start": "505039",
    "end": "512399"
  },
  {
    "text": "concession on the amount of data that we're able to push into our Enterprise data warehouse and with services like",
    "start": "512399",
    "end": "519320"
  },
  {
    "text": "Amazon redshift with such a disruptive price point we're able to really start",
    "start": "519320",
    "end": "524760"
  },
  {
    "text": "ingesting everything into our Enterprise data warehouse but how far do you",
    "start": "524760",
    "end": "530000"
  },
  {
    "text": "go would you put all of the network exhaust from your platform as a telecoms",
    "start": "530000",
    "end": "538200"
  },
  {
    "text": "operator into red shift you might but there can be other",
    "start": "538200",
    "end": "543560"
  },
  {
    "text": "ways of solving problems against gigabit per second sort of data",
    "start": "543560",
    "end": "549279"
  },
  {
    "text": "sets that a more decoupled architecture will",
    "start": "549279",
    "end": "555360"
  },
  {
    "text": "allow and we see some other attributes you know tend to have very very tight",
    "start": "555360",
    "end": "560880"
  },
  {
    "text": "slas on our Enterprise data warehouse and in many cases that is",
    "start": "560880",
    "end": "568279"
  },
  {
    "text": "conceded on a Big Data platform because it's an experiment within your company",
    "start": "568279",
    "end": "573560"
  },
  {
    "text": "or it's not handling the crown jewels",
    "start": "573560",
    "end": "579120"
  },
  {
    "text": "so we have two systems that are really kind of at odds and they're both serving great",
    "start": "579120",
    "end": "585959"
  },
  {
    "text": "requirements within our businesses but we've often created another",
    "start": "585959",
    "end": "592760"
  },
  {
    "text": "problem which is that the two really can't overlap very",
    "start": "592760",
    "end": "598320"
  },
  {
    "text": "well so do we duplicate our data between an",
    "start": "598320",
    "end": "607360"
  },
  {
    "text": "unstructured system and a structured system is our Enterprise data warehouse populated from our big data",
    "start": "607360",
    "end": "616279"
  },
  {
    "text": "environment and how do we enable these other tools to work against data sets that are in the Enterprise data",
    "start": "616279",
    "end": "623560"
  },
  {
    "text": "warehouse and ultimately we need to synthesize these two domains together we",
    "start": "624000",
    "end": "629959"
  },
  {
    "text": "need to give us the ability to harmonize these two sources and this is an",
    "start": "629959",
    "end": "636240"
  },
  {
    "start": "631000",
    "end": "631000"
  },
  {
    "text": "objective that we'll set for our idea of what a data Lake can",
    "start": "636240",
    "end": "641720"
  },
  {
    "text": "be so what does this system do well the Big Data component the unstructured data",
    "start": "641880",
    "end": "647240"
  },
  {
    "text": "management systems are doing things like ingesting any type of data at scale and",
    "start": "647240",
    "end": "653440"
  },
  {
    "text": "very very agile and doing things like massively parallel data cleansing",
    "start": "653440",
    "end": "661040"
  },
  {
    "text": "and we also can move towards this idea of having a data catalog that sits",
    "start": "661040",
    "end": "667519"
  },
  {
    "text": "outside of our Enterprise data warehouse when somebody in an oracle",
    "start": "667519",
    "end": "674120"
  },
  {
    "text": "environment does a describe table they get the table comments that's an extremely useful",
    "start": "674120",
    "end": "679480"
  },
  {
    "text": "feature how do we extend that same experience to all of our",
    "start": "679480",
    "end": "686000"
  },
  {
    "text": "data and that process that we employ should be then used to",
    "start": "687079",
    "end": "694399"
  },
  {
    "text": "push data into our more structured environment for things like structured analysis giving access to",
    "start": "694399",
    "end": "702800"
  },
  {
    "text": "analysts potentially doing more efficient aggregation although a very common pattern is to",
    "start": "702800",
    "end": "710399"
  },
  {
    "text": "pre-aggregate at the store everything forever level and we still get benefit from",
    "start": "710399",
    "end": "717920"
  },
  {
    "text": "implementing structured business rules ruls within our Enterprise data warehouse that's not the end of the",
    "start": "717920",
    "end": "723839"
  },
  {
    "text": "story because there's a huge amount of value that we've captured in our Enterprise data warehouse that we may need to use to",
    "start": "723839",
    "end": "731680"
  },
  {
    "text": "solve other problems so exporting out computed",
    "start": "731680",
    "end": "740959"
  },
  {
    "text": "Aggregates business rules and so on so that we can do things like Trend",
    "start": "740959",
    "end": "747519"
  },
  {
    "text": "analysis implment machine learning and so on it's quite a nice pattern and you",
    "start": "747519",
    "end": "753120"
  },
  {
    "text": "can see how they can work very well together and so a data Lake really needs",
    "start": "753120",
    "end": "760240"
  },
  {
    "text": "to provide us this idea that an Enterprise data warehouse is just another projection of our",
    "start": "760240",
    "end": "767000"
  },
  {
    "text": "company's core data but it isn't the only thing that we use to solve a problem and so the",
    "start": "767000",
    "end": "774880"
  },
  {
    "text": "benefits that we've been getting from an Enterprise data warehouse have to live in other places as well",
    "start": "774880",
    "end": "781120"
  },
  {
    "start": "783000",
    "end": "783000"
  },
  {
    "text": "well so the components that we're going to walk through",
    "start": "783199",
    "end": "789079"
  },
  {
    "text": "include base storage capability for data but crucially also our ability to",
    "start": "789079",
    "end": "795480"
  },
  {
    "text": "work with streams a data Lake can't just be a",
    "start": "795480",
    "end": "801399"
  },
  {
    "text": "massive pile of data at rest it also needs to be able to support the idea that things are changing all the",
    "start": "801399",
    "end": "807240"
  },
  {
    "text": "time and our storage streaming layer needs to give us extremely high",
    "start": "807240",
    "end": "813800"
  },
  {
    "text": "durability and a key design pattern that we recommend to our customers is that that storage tier should store raw",
    "start": "813920",
    "end": "822199"
  },
  {
    "text": "data as you captured it in the first place the moment that you reduce the",
    "start": "822199",
    "end": "828000"
  },
  {
    "text": "granularity of the data that you store you are losing business",
    "start": "828000",
    "end": "833759"
  },
  {
    "text": "information okay some data you don't need to store but",
    "start": "833759",
    "end": "840759"
  },
  {
    "text": "as soon as you decide that in fact it would have been quite nice to have that and you've deleted it you're out of",
    "start": "840759",
    "end": "848720"
  },
  {
    "text": "luck the storage API needs to be able to store any type of data at all and allow us to answer the",
    "start": "850720",
    "end": "859000"
  },
  {
    "text": "question when a business user comes to us can't we just thought we might introduce this piece of data say yes",
    "start": "859000",
    "end": "864800"
  },
  {
    "text": "absolutely we can store that and in order to have that sort of full take architecture it needs to",
    "start": "864800",
    "end": "871519"
  },
  {
    "text": "deliver low cost and not just low cost but potentially low variable cost where",
    "start": "871519",
    "end": "878839"
  },
  {
    "text": "we're able to put data in the right sort of system that gives us the cost profile",
    "start": "878839",
    "end": "883959"
  },
  {
    "text": "that that data justifies on the streaming side streaming isn't just about pushing data",
    "start": "883959",
    "end": "890959"
  },
  {
    "text": "in which is extremely important and looking at ways for us to",
    "start": "890959",
    "end": "897040"
  },
  {
    "text": "move away from a file-based inest architecture to a stream based one we'll",
    "start": "897040",
    "end": "903079"
  },
  {
    "text": "talk about why that might matter in a little while but also the ability to turn",
    "start": "903079",
    "end": "908560"
  },
  {
    "text": "around and say well what if I wanted to consume a piece of data from my data Lake as a stream what would that look",
    "start": "908560",
    "end": "914320"
  },
  {
    "text": "like because that's quite neat if every time a piece of data",
    "start": "914320",
    "end": "920040"
  },
  {
    "text": "changed I was able just to consume that and have extremely low latency that can offer some really interesting Business",
    "start": "920040",
    "end": "927800"
  },
  {
    "text": "Solutions and really enable new ideas to be",
    "start": "927800",
    "end": "933800"
  },
  {
    "text": "fulfilled the next component of a data Lake that we would ask you to",
    "start": "933800",
    "end": "938839"
  },
  {
    "text": "consider is that a data lake is more than just storage it is not just a pile of",
    "start": "938839",
    "end": "944480"
  },
  {
    "text": "data but it also has this ability to document itself that it can enable data",
    "start": "944480",
    "end": "950720"
  },
  {
    "text": "Discovery and that it can facilitate our end users being more productive with the",
    "start": "950720",
    "end": "956560"
  },
  {
    "text": "data so we need a data catalog the metadata Lake if you",
    "start": "956560",
    "end": "963600"
  },
  {
    "text": "will and what this is going to do is it's going to give us summary statistics about our data it'll do things like",
    "start": "963600",
    "end": "972800"
  },
  {
    "text": "classification a really simple problem that we need to solve with our Big Data Systems show me all my customer",
    "start": "973639",
    "end": "980199"
  },
  {
    "text": "data right how do you do that it's actually",
    "start": "980199",
    "end": "985399"
  },
  {
    "text": "really hard and then on the other side search is increasingly important to our",
    "start": "985399",
    "end": "991079"
  },
  {
    "text": "customers who need to not just be able to say I've got a very prescribed way that I ingest data I move it into this",
    "start": "991079",
    "end": "997120"
  },
  {
    "text": "system and then I do something with it and then it comes back out again but I need to be able to just",
    "start": "997120",
    "end": "1004120"
  },
  {
    "text": "discover patterns in my data see where data is related and expand that picture that",
    "start": "1004120",
    "end": "1011319"
  },
  {
    "text": "says actually I didn't think that this data source actually had any customer data in it but it does and search can",
    "start": "1011319",
    "end": "1017920"
  },
  {
    "text": "enable us to find that it's a really important component of our data Lake",
    "start": "1017920",
    "end": "1025319"
  },
  {
    "text": "strategy the next thing we need to consider is what I'm going to call entitlements entitlements isn't just",
    "start": "1026600",
    "end": "1035438"
  },
  {
    "text": "security But it includes security as well as things like authorization regionalization chargeback",
    "start": "1035439",
    "end": "1043240"
  },
  {
    "text": "quotas masking all of the things that you may need to implement to be client with your",
    "start": "1043240",
    "end": "1050240"
  },
  {
    "text": "business the contracts that you've signed and so",
    "start": "1050240",
    "end": "1054880"
  },
  {
    "text": "on customers that use S3 can store any type of data but in",
    "start": "1056120",
    "end": "1061960"
  },
  {
    "text": "many cases they're going to end up with ubiquitous access or no access at",
    "start": "1061960",
    "end": "1068679"
  },
  {
    "text": "all and so we need to consider that problem fully and we're going to give you some patterns that you can consider",
    "start": "1068679",
    "end": "1074160"
  },
  {
    "text": "implementing depending on what your requirements are and then",
    "start": "1074160",
    "end": "1080679"
  },
  {
    "text": "lastly we need an API and a user interface that exposes this environment to the outside",
    "start": "1080679",
    "end": "1087880"
  },
  {
    "text": "world now data lake is as you might have guessed composed of AWS services and of",
    "start": "1088799",
    "end": "1095000"
  },
  {
    "text": "course they all have apis but gaining access to this environment isn't necessarily as simple",
    "start": "1095000",
    "end": "1101280"
  },
  {
    "text": "as just exposing those AWS apis to your customers because you have very complex",
    "start": "1101280",
    "end": "1107360"
  },
  {
    "text": "business rules that you've implemented in your entitlement system and those absolutely",
    "start": "1107360",
    "end": "1114120"
  },
  {
    "text": "have to be respected you know that API is really",
    "start": "1114120",
    "end": "1120120"
  },
  {
    "text": "the way that you implement governance of this system because as we all know human",
    "start": "1120120",
    "end": "1128000"
  },
  {
    "text": "policy based governance is extremely hard to get right so let's dive into this storage",
    "start": "1128000",
    "end": "1135760"
  },
  {
    "text": "and streams what's it look like well no great",
    "start": "1135760",
    "end": "1141559"
  },
  {
    "text": "surprise that Amazon S3 the simple storage service provides a foundation",
    "start": "1141559",
    "end": "1147120"
  },
  {
    "text": "for any data Lake we build and it is absolutely not the case that we would contradict the idea that",
    "start": "1147120",
    "end": "1153240"
  },
  {
    "text": "S3 can become your system of record for all the data that you ingest why is",
    "start": "1153240",
    "end": "1160000"
  },
  {
    "text": "that well it's a system designed to allow you to store a virtually unlimited amount of",
    "start": "1160000",
    "end": "1166080"
  },
  {
    "text": "data and do so with a dur durability design of 119 of",
    "start": "1166080",
    "end": "1173640"
  },
  {
    "text": "durability when we talk about keeping data safe in the durability measure you can think about 119 of",
    "start": "1173640",
    "end": "1180799"
  },
  {
    "text": "durability is if you stored uh 10,000",
    "start": "1180799",
    "end": "1186360"
  },
  {
    "text": "objects in S3 you could theoretically lose one of those in 10 million",
    "start": "1186360",
    "end": "1192919"
  },
  {
    "text": "years that's what 119 means and",
    "start": "1192919",
    "end": "1199240"
  },
  {
    "text": "even that 11 9's durability we go to Great Lengths to CRC check and provide",
    "start": "1199240",
    "end": "1207400"
  },
  {
    "text": "check sums of your data to prevent bit rot and allow us to store data on the",
    "start": "1207400",
    "end": "1213080"
  },
  {
    "text": "magnitude that S3 is built for when we talk about what data can be",
    "start": "1213080",
    "end": "1219000"
  },
  {
    "text": "ingested into our data L S3 is completely open to storing whatever data you",
    "start": "1219000",
    "end": "1224559"
  },
  {
    "text": "want from a tiny one by data",
    "start": "1224559",
    "end": "1230000"
  },
  {
    "text": "marker including the functionality of S3 that says I used to have a piece of data",
    "start": "1230000",
    "end": "1235240"
  },
  {
    "text": "here but it's actually over there which we'll use for some interesting lineage",
    "start": "1235240",
    "end": "1241039"
  },
  {
    "text": "problems we can go up to 5 terabytes of storage and it's happy to store",
    "start": "1241039",
    "end": "1246120"
  },
  {
    "text": "absolutely anything S3 isn't a file system it's object storage which means",
    "start": "1246120",
    "end": "1253080"
  },
  {
    "text": "that we can access it over HTTP we can put any type of data in and it only",
    "start": "1253080",
    "end": "1258919"
  },
  {
    "text": "looks like a file because it has a file name attribute and a m m type attribute but the AWS sdks and client",
    "start": "1258919",
    "end": "1267200"
  },
  {
    "text": "tools allow you to work with file data as though S3 was a very simple file",
    "start": "1267200",
    "end": "1273039"
  },
  {
    "text": "system and S3 is a regional service so when you create a data",
    "start": "1273039",
    "end": "1279360"
  },
  {
    "text": "storage environment for your data Lake and you choose a region that's appropriate for you S3 is implemented",
    "start": "1279360",
    "end": "1285960"
  },
  {
    "text": "across all the availability zones in that region and crucially the data won't be moved",
    "start": "1285960",
    "end": "1291799"
  },
  {
    "text": "outside that region and with services like AWS cloud trail you can ensure the data is",
    "start": "1291799",
    "end": "1298559"
  },
  {
    "text": "journaled and audited for how it's being used S3 supports Concepts such as access",
    "start": "1298559",
    "end": "1305880"
  },
  {
    "text": "logging and we'll talk about the identity and access management capabilities to secure this data to make",
    "start": "1305880",
    "end": "1312240"
  },
  {
    "text": "it extremely secure and then we can take advantage of things like server side encryption as well",
    "start": "1312240",
    "end": "1320200"
  },
  {
    "text": "as client side encryption to make this data Ultra secure and ensure that we only use it for the task at",
    "start": "1320200",
    "end": "1328440"
  },
  {
    "text": "hand S3 is really a great system for ingesting",
    "start": "1328760",
    "end": "1335960"
  },
  {
    "text": "anything and then helping us manage cost as",
    "start": "1335960",
    "end": "1340640"
  },
  {
    "start": "1337000",
    "end": "1337000"
  },
  {
    "text": "well and we've long had the concept of life cycle",
    "start": "1341080",
    "end": "1346400"
  },
  {
    "text": "integration with Amazon glacier a service designed for Cold",
    "start": "1346400",
    "end": "1352720"
  },
  {
    "text": "Storage also offering us 119 of durability but with a retrieval time for",
    "start": "1352720",
    "end": "1359679"
  },
  {
    "text": "our objects of between 3 and 5 hours and now we can also take advantage",
    "start": "1359679",
    "end": "1365720"
  },
  {
    "text": "of a great new feature of S3 called S3 in frequent access S3",
    "start": "1365720",
    "end": "1372440"
  },
  {
    "text": "IIA IIA is a much cheaper option that's still giving us online access to data at",
    "start": "1372440",
    "end": "1380080"
  },
  {
    "text": "a significantly reduced cost an S3 is a perfect",
    "start": "1380080",
    "end": "1387039"
  },
  {
    "text": "environment to use for data that you almost never access but if you needed to",
    "start": "1387039",
    "end": "1393559"
  },
  {
    "text": "you have to know it's available and one of the great benefits",
    "start": "1393559",
    "end": "1398679"
  },
  {
    "text": "we can take advantage of of this storage system is that S3 will manage this tiering for",
    "start": "1398679",
    "end": "1403799"
  },
  {
    "text": "you S3 life cycle integration allows you to write a policy that says after a certain period of time move my data to",
    "start": "1403799",
    "end": "1411640"
  },
  {
    "text": "S3 after a further period of time move my data to Glacier and after yet another period of",
    "start": "1411640",
    "end": "1419240"
  },
  {
    "text": "time delete it entirely if that's your requirement so we can really look at our",
    "start": "1419240",
    "end": "1426840"
  },
  {
    "text": "data that comes into our environment and say well what is the right cost profile for our data and make sure you target",
    "start": "1426840",
    "end": "1432919"
  },
  {
    "text": "that environment so what is this data and how",
    "start": "1432919",
    "end": "1438799"
  },
  {
    "text": "do we store it well consider when you use S3 as a",
    "start": "1438799",
    "end": "1444000"
  },
  {
    "start": "1439000",
    "end": "1439000"
  },
  {
    "text": "foundation for your data Lake that not all file formats are created equally you're going to have totally",
    "start": "1444000",
    "end": "1450720"
  },
  {
    "text": "unstructured data you're going to have some semi-structured data that has variability over time and you are",
    "start": "1450720",
    "end": "1457720"
  },
  {
    "text": "absolutely going to have very structured data but across all of those different",
    "start": "1457720",
    "end": "1463640"
  },
  {
    "text": "types of data we really do want to store a raw first copy of the data so that we",
    "start": "1463640",
    "end": "1469919"
  },
  {
    "text": "can recover our business and almost replay the business if you",
    "start": "1469919",
    "end": "1477039"
  },
  {
    "text": "will you would Implement a downstream workflow to do things like standardization of your data Implement",
    "start": "1478520",
    "end": "1485080"
  },
  {
    "text": "data cleansing and so on but it is absolutely vital that you consider some structure that allows you to store",
    "start": "1485080",
    "end": "1490919"
  },
  {
    "text": "everything as you received it in the first place and then build a picture of",
    "start": "1490919",
    "end": "1496080"
  },
  {
    "text": "what happened to it as you go and as you store that data you should",
    "start": "1496080",
    "end": "1501159"
  },
  {
    "text": "use a data format that is sympathetic to your data if you will rather than",
    "start": "1501159",
    "end": "1507600"
  },
  {
    "text": "turning around and forcing everything into a data format that you kind of decided",
    "start": "1507600",
    "end": "1513679"
  },
  {
    "text": "because it seemed like a good idea or it was trendy or whatever the case may",
    "start": "1513679",
    "end": "1520320"
  },
  {
    "text": "be but you will want to do things like having a common approach to",
    "start": "1520320",
    "end": "1527039"
  },
  {
    "text": "compression and you you want to pick your compression capabilities so that they allow you to work with the data and",
    "start": "1527039",
    "end": "1535200"
  },
  {
    "text": "do things like transparent decompression streaming and so on and a crucial part of how you store",
    "start": "1535200",
    "end": "1542960"
  },
  {
    "text": "data and what data format you choose is to consider how it will change over",
    "start": "1542960",
    "end": "1548399"
  },
  {
    "text": "time you know when you choose to store all your data as orc or parquet you have",
    "start": "1548399",
    "end": "1555360"
  },
  {
    "text": "to consider that that's great for the schema that it implements today what happens when it changes in the",
    "start": "1555360",
    "end": "1561159"
  },
  {
    "text": "future and is the format going to be able to deal with that",
    "start": "1561159",
    "end": "1567278"
  },
  {
    "start": "1567000",
    "end": "1567000"
  },
  {
    "text": "change so for unstructured data it's actually very simple store the",
    "start": "1567960",
    "end": "1573880"
  },
  {
    "text": "native file format logs Dum files whatever we really do mean store",
    "start": "1573880",
    "end": "1579360"
  },
  {
    "text": "everything but use a codec that allows you to stream that data and work with it",
    "start": "1579360",
    "end": "1586399"
  },
  {
    "text": "on systems like Hadoop very very easily I personally Advocate LZ",
    "start": "1586399",
    "end": "1592360"
  },
  {
    "text": "compression just about everywhere because it gives us good compression it's about 20% bigger than gzip but it's",
    "start": "1592360",
    "end": "1599600"
  },
  {
    "text": "a streaming codec and it has client tools on Linux and it's supported by almost every sort of Hadoop tool that",
    "start": "1599600",
    "end": "1606120"
  },
  {
    "text": "you can imagine semi-structured data this is",
    "start": "1606120",
    "end": "1611480"
  },
  {
    "text": "going to be Json XML whatever the case may be this is an area where you really need",
    "start": "1611480",
    "end": "1617559"
  },
  {
    "text": "to think about evolvability so a data format like Avo can be very",
    "start": "1617559",
    "end": "1625000"
  },
  {
    "text": "useful for allowing you to assimilate changes to the schema of the data over",
    "start": "1625000",
    "end": "1632080"
  },
  {
    "text": "time and an interesting ability is to store within S3 an attribute on a given",
    "start": "1632360",
    "end": "1640440"
  },
  {
    "text": "file that says well where is the schema for this piece of data where can I go look that up if I",
    "start": "1640440",
    "end": "1645600"
  },
  {
    "text": "needed to and on the structured",
    "start": "1645600",
    "end": "1651799"
  },
  {
    "text": "side with the best will in the world lots and lots and lots of data is",
    "start": "1651799",
    "end": "1658159"
  },
  {
    "text": "CSV you know these Legacy systems mainframes uh you application servers",
    "start": "1658159",
    "end": "1665600"
  },
  {
    "text": "that have been built for years are going to be able to generate CSV it's a very common inter Chase",
    "start": "1665600",
    "end": "1671720"
  },
  {
    "text": "format and this is an area where you may say actually using par or orc does make sense because it has a lot of the same",
    "start": "1671720",
    "end": "1677640"
  },
  {
    "text": "attributes of CSV but it gives us massive massive",
    "start": "1677640",
    "end": "1683480"
  },
  {
    "text": "compression but be very careful on selecting one of those data",
    "start": "1684960",
    "end": "1690480"
  },
  {
    "text": "formats because you may find that you need to work with that CSV",
    "start": "1690480",
    "end": "1696440"
  },
  {
    "text": "data with a tool that doesn't support this year's most hot data",
    "start": "1696440",
    "end": "1704720"
  },
  {
    "text": "format and you know we have great experience with par we have great experience with",
    "start": "1705080",
    "end": "1710960"
  },
  {
    "text": "orc but just make sure that it's going to stand the test of time for the sort of tools that you",
    "start": "1710960",
    "end": "1718120"
  },
  {
    "text": "require the other thing we need to consider for a data lake is where should the data be actually",
    "start": "1719880",
    "end": "1725159"
  },
  {
    "text": "stored okay in S3 we're all on the same page there but",
    "start": "1725159",
    "end": "1730760"
  },
  {
    "text": "S3 has a very flat key space and if you ingest 100 terab of data into S3 and",
    "start": "1730760",
    "end": "1736440"
  },
  {
    "text": "then decide that you don't really like how it's organized you have a very big job on your hands to change",
    "start": "1736440",
    "end": "1743679"
  },
  {
    "text": "that so we absolutely recommend that you spend a lot of time upfront thinking",
    "start": "1743679",
    "end": "1749120"
  },
  {
    "text": "about how you lay your data out on S3 to give yourself the most benefits possible",
    "start": "1749120",
    "end": "1755519"
  },
  {
    "text": "doing things like naturally taking advantage of the partitioning that your",
    "start": "1755519",
    "end": "1760840"
  },
  {
    "text": "data gives you and using a prefix structure that lends itself to a more",
    "start": "1760840",
    "end": "1768880"
  },
  {
    "text": "intuitive interaction with the data so what does that mean well you",
    "start": "1768880",
    "end": "1774279"
  },
  {
    "text": "might decide that you have a common way of storing data on S3 that takes the business unit the application name the",
    "start": "1774279",
    "end": "1781399"
  },
  {
    "text": "type of data that you're storing and maybe time as a key component of how you",
    "start": "1781399",
    "end": "1786559"
  },
  {
    "text": "organize data on S3 and once you've made that",
    "start": "1786559",
    "end": "1793360"
  },
  {
    "text": "decision stick with it and make decisions about how you change it very carefully",
    "start": "1793360",
    "end": "1799360"
  },
  {
    "text": "because we're going to use this almost as a basis for a metadata catalog and I'd like to introduce the idea of",
    "start": "1799360",
    "end": "1807200"
  },
  {
    "start": "1805000",
    "end": "1805000"
  },
  {
    "text": "resource oriented architecture something you can read",
    "start": "1807200",
    "end": "1812760"
  },
  {
    "text": "about on uh Wikipedia it's an interesting idea that comes out of the way that we would",
    "start": "1812760",
    "end": "1819080"
  },
  {
    "text": "structure a web API that we expose resources and then",
    "start": "1819080",
    "end": "1824840"
  },
  {
    "text": "the operations that we can perform against those so this has some really interesting",
    "start": "1824840",
    "end": "1830519"
  },
  {
    "text": "ideas for how we might lay our data out on S3 organized by System application the",
    "start": "1830519",
    "end": "1838559"
  },
  {
    "text": "year the type of data and then also considering that we might have",
    "start": "1838559",
    "end": "1843880"
  },
  {
    "text": "tagging and resource markers within our data that facilitate building really",
    "start": "1843880",
    "end": "1851240"
  },
  {
    "text": "interesting integration patterns using S3 as a store system and we send",
    "start": "1851240",
    "end": "1859039"
  },
  {
    "text": "around the prefix as a key for working with very complex data in our integration",
    "start": "1859039",
    "end": "1867840"
  },
  {
    "text": "system on the streaming side it's a real complement to our storage",
    "start": "1867840",
    "end": "1873480"
  },
  {
    "text": "architecture we need to have data come in and we need to have data go out and we need to be agile and consume that not",
    "start": "1874320",
    "end": "1880559"
  },
  {
    "text": "at the granularity of files only but of lots of other types of",
    "start": "1880559",
    "end": "1885679"
  },
  {
    "text": "data streams matter because there is more and more customer focus on",
    "start": "1885679",
    "end": "1891480"
  },
  {
    "text": "latency between an event occurring and being able to take an action and an enterpris and Warehouse",
    "start": "1891480",
    "end": "1898639"
  },
  {
    "text": "sort of architecture and many uh even Big Data Systems would Target maybe sort",
    "start": "1898639",
    "end": "1904120"
  },
  {
    "text": "of hourly as an a latency that that's an objective but you know streaming is",
    "start": "1904120",
    "end": "1909200"
  },
  {
    "text": "going to be in the 1 to two second range and building an architecture around streams can help us facilitate these use",
    "start": "1909200",
    "end": "1916519"
  },
  {
    "text": "cases but bu bware that can be a little bit more complicated to",
    "start": "1916519",
    "end": "1921559"
  },
  {
    "text": "operate and so when we consider how do we integrate streams into our data Lake we need to consider the business value",
    "start": "1921559",
    "end": "1928320"
  },
  {
    "text": "that we receive from having a streaming approach so kesis gives us this great",
    "start": "1928320",
    "end": "1935279"
  },
  {
    "text": "ability to handle streams both inbound and outbound and with tools like Kinesis",
    "start": "1935279",
    "end": "1942720"
  },
  {
    "text": "fire hose we can have fully managed delivery of data inbound to 3",
    "start": "1942720",
    "end": "1950440"
  },
  {
    "text": "automatically Kinesis gives us that streaming capability to also build realtime analytics",
    "start": "1950440",
    "end": "1956960"
  },
  {
    "text": "systems and the storage is extremely durable in a similar way to",
    "start": "1956960",
    "end": "1962000"
  },
  {
    "text": "S3 the shards within Kinesis are implemented across multiple availability zones they give us very high durability",
    "start": "1962000",
    "end": "1968880"
  },
  {
    "text": "and we can have as many consumers of the data as we require so the archive and ingestion",
    "start": "1968880",
    "end": "1976320"
  },
  {
    "text": "sort of use case that Kinesis fire hose enables can be complemented with things like sliding window analysis using spark",
    "start": "1976320",
    "end": "1982639"
  },
  {
    "text": "streaming automatically loading data into our Enterprise Data Warehouse in the form of redshift and also building",
    "start": "1982639",
    "end": "1990120"
  },
  {
    "text": "sophisticated event processing",
    "start": "1990120",
    "end": "1993760"
  },
  {
    "start": "1994000",
    "end": "1994000"
  },
  {
    "text": "systems this interacts with our Object Store in a slightly subtle",
    "start": "1995360",
    "end": "2001679"
  },
  {
    "text": "way our analytics applications of course are going to be reading and writing file data but we also have the ability to",
    "start": "2001679",
    "end": "2007000"
  },
  {
    "text": "move away from files at source take a file and just stream that onto",
    "start": "2007000",
    "end": "2013559"
  },
  {
    "text": "Kinesis that gives us a great benefit on our application tier we can throw the file away we don't run out of dis",
    "start": "2013559",
    "end": "2018960"
  },
  {
    "text": "anymore because we've made every single event durable on that stream those analytics applications are",
    "start": "2018960",
    "end": "2025559"
  },
  {
    "text": "going to be reading and writing from streams but we can also implement the idea that we want to replay history of",
    "start": "2025559",
    "end": "2032639"
  },
  {
    "text": "our data Lake by having a component that takes a specification for a piece of data on",
    "start": "2032639",
    "end": "2038679"
  },
  {
    "text": "S3 and squirts it up into Kinesis so that our applications can Rec consume",
    "start": "2038679",
    "end": "2044039"
  },
  {
    "text": "that so when you build these type of systems you need to consider what does what does a streaming replay look",
    "start": "2044039",
    "end": "2050040"
  },
  {
    "text": "like how would I detect that an event was being replayed and what would the business rules be in my application that",
    "start": "2050040",
    "end": "2056520"
  },
  {
    "text": "I'd need to change and then certainly where you're using streaming for inbound data",
    "start": "2056520",
    "end": "2063760"
  },
  {
    "text": "storage we push that on to S3 for POS for long-term",
    "start": "2063760",
    "end": "2070839"
  },
  {
    "text": "archival moving on in the components of a data Lake let's talk about catalog and search so the catalog is really about",
    "start": "2070839",
    "end": "2078800"
  },
  {
    "start": "2075000",
    "end": "2075000"
  },
  {
    "text": "giving us a aggregate picture of the data we have on",
    "start": "2078800",
    "end": "2083878"
  },
  {
    "text": "S3 it gives us the ability to Define new metadata characteristics define things like",
    "start": "2083879",
    "end": "2090280"
  },
  {
    "text": "ownership of data not in terms of the owner of an object on S3 but a much more",
    "start": "2090280",
    "end": "2095480"
  },
  {
    "text": "abstract sense of ownership it gives us the ability to think about things like lineage I ingested a piece of data then",
    "start": "2095480",
    "end": "2101880"
  },
  {
    "text": "it moved there then it went somewhere else we need a component that will do that and so the data catalog is this",
    "start": "2101880",
    "end": "2107119"
  },
  {
    "text": "abstraction layer that lets us start to consider the idea that we should be able",
    "start": "2107119",
    "end": "2112960"
  },
  {
    "text": "to identify customer data that's more than just a prefix on S3 it's a collection of prefixes it's multiple",
    "start": "2112960",
    "end": "2120839"
  },
  {
    "text": "regions of S3 even and this will help us to use our",
    "start": "2120839",
    "end": "2126800"
  },
  {
    "text": "resource oriented approach to expose this data back to our customers and it gives us a really nice API for our",
    "start": "2126800",
    "end": "2133680"
  },
  {
    "text": "entitlements system one of the components of our data catalog is a metadata",
    "start": "2133680",
    "end": "2140320"
  },
  {
    "text": "index it needs to store information about S3 so this is going to be things like the total size of the objects on S3",
    "start": "2140320",
    "end": "2146599"
  },
  {
    "text": "the number of objects per prefix a data classification this is Red Data you have",
    "start": "2146599",
    "end": "2152880"
  },
  {
    "text": "to have a certain entitlement to be able to touch it when was it last refreshed",
    "start": "2152880",
    "end": "2159119"
  },
  {
    "text": "is it versioned these kind of questions and a pattern that we would",
    "start": "2159119",
    "end": "2164280"
  },
  {
    "text": "recommend you consider is using Lambda to process S3",
    "start": "2164280",
    "end": "2170440"
  },
  {
    "text": "events to build this metadata index very easily and store those in Dynamo DB and",
    "start": "2170440",
    "end": "2175960"
  },
  {
    "text": "in fact we like this pattern so much that one of my colleagues wrote a blog post about it and I would highly",
    "start": "2175960",
    "end": "2182359"
  },
  {
    "text": "encourage you to read it and use the principles that it implements",
    "start": "2182359",
    "end": "2189560"
  },
  {
    "text": "you know this is a a totally serverless architecture we just let it sit there and it maintains all these",
    "start": "2189720",
    "end": "2195680"
  },
  {
    "text": "beautiful metrics about our S3 environment that are a massive Improvement on just basic S3",
    "start": "2195680",
    "end": "2204599"
  },
  {
    "text": "storage we choose to implement our metadata index in Dynamo DB because it",
    "start": "2208240",
    "end": "2213520"
  },
  {
    "text": "has some really great characteristics it can be a document store it can be a key value store it gives us multi-",
    "start": "2213520",
    "end": "2220240"
  },
  {
    "text": "availability Zone durability and it can be scaled on demand and AWS",
    "start": "2220240",
    "end": "2227079"
  },
  {
    "text": "Lambda is an environment that allows us to implement simple logic without having",
    "start": "2227079",
    "end": "2232319"
  },
  {
    "text": "to run any servers and in fact building a metadata index is a fairly trivial problem to solve I received a new file",
    "start": "2232319",
    "end": "2238800"
  },
  {
    "text": "in S3 I need to make a record of the fact that that object arrived and then I store its metadata as",
    "start": "2238800",
    "end": "2246359"
  },
  {
    "text": "a summary I increment ment counters for size and object count in a prefix and so",
    "start": "2246359",
    "end": "2251640"
  },
  {
    "text": "on an AWS Lambda is able to scale automatically with the volume of data that's coming into our data",
    "start": "2251640",
    "end": "2260040"
  },
  {
    "text": "L on the search side we want to be able to take all of",
    "start": "2260480",
    "end": "2266359"
  },
  {
    "text": "these different systems that are pushing their data into S3 and give us some",
    "start": "2266359",
    "end": "2271839"
  },
  {
    "text": "standardization about how we interact with that data and search systems are going to tokenize the contents of our",
    "start": "2271839",
    "end": "2277200"
  },
  {
    "text": "file files it's going to deal with cases and different languages and so on and",
    "start": "2277200",
    "end": "2282760"
  },
  {
    "text": "also index and match data together and",
    "start": "2282760",
    "end": "2288200"
  },
  {
    "start": "2288000",
    "end": "2288000"
  },
  {
    "text": "so we can use services like Cloud search and our new elastic search service that",
    "start": "2288200",
    "end": "2294520"
  },
  {
    "text": "are extremely simple to use high durability high performance and",
    "start": "2294520",
    "end": "2299839"
  },
  {
    "text": "don't require any operations to scale so we build on our data",
    "start": "2299839",
    "end": "2309119"
  },
  {
    "text": "index by capitalizing on Dynam DB update streams pardon me update streams give us",
    "start": "2309119",
    "end": "2316240"
  },
  {
    "text": "the ability to consume the changes that are being made to Dynamo DB using a streaming interface that we",
    "start": "2316240",
    "end": "2322640"
  },
  {
    "text": "can process with you guessed it AWS Lambda so every time we add a new",
    "start": "2322640",
    "end": "2328560"
  },
  {
    "text": "component to our data Lake it gets registered with our metadata index that metadata index emits an event",
    "start": "2328560",
    "end": "2334640"
  },
  {
    "text": "that says a file has been added and then we're able to go and read that file and index it into our search",
    "start": "2334640",
    "end": "2343599"
  },
  {
    "text": "system and so we end up with a very sophisticated metadata",
    "start": "2343839",
    "end": "2349760"
  },
  {
    "text": "catalog where object created object deleted is being run through our Dynamo",
    "start": "2349760",
    "end": "2355520"
  },
  {
    "text": "DB based metadata index and Lambda sits at the heart of maintaining the search index and the",
    "start": "2355520",
    "end": "2362440"
  },
  {
    "text": "metadata index for us really nice simple pattern that allows you then to build",
    "start": "2362440",
    "end": "2367960"
  },
  {
    "text": "whatever attributes you require into Dynamo DB into that metadata index and",
    "start": "2367960",
    "end": "2373160"
  },
  {
    "text": "take advantage of cloud search and elastic search for your search",
    "start": "2373160",
    "end": "2378319"
  },
  {
    "text": "interface so let's talk about entitlements entitlements give us the",
    "start": "2380040",
    "end": "2385560"
  },
  {
    "text": "ability to not just treat this data store as a mass of miscellaneous data",
    "start": "2385560",
    "end": "2392040"
  },
  {
    "text": "but instead allow us to very finely secure our system and one thing that I think is an important assertion that you take away",
    "start": "2392040",
    "end": "2399440"
  },
  {
    "text": "is that data Lake no longer means completely open access to",
    "start": "2399440",
    "end": "2404880"
  },
  {
    "text": "data there may be some kinds of data that you're very happy just to store in effectively a pseudo public context for",
    "start": "2404880",
    "end": "2411160"
  },
  {
    "text": "your business that's great that's fine but in the same way that an Enterprise data warehouse gave us a",
    "start": "2411160",
    "end": "2418680"
  },
  {
    "text": "common security model we need to consider that the data going into our data lake is going to include the crown",
    "start": "2418680",
    "end": "2424920"
  },
  {
    "text": "jewels and so we can't just leave it open for anybody to access so identity and access",
    "start": "2424920",
    "end": "2430920"
  },
  {
    "start": "2429000",
    "end": "2429000"
  },
  {
    "text": "management is a system within AWS that allows us to secure data on S3 and",
    "start": "2430920",
    "end": "2437760"
  },
  {
    "text": "manage users and groups and roles in a very sophisticated fashion it's based on",
    "start": "2437760",
    "end": "2443720"
  },
  {
    "text": "a policy language and we're going to look at some of the ways you can use that against your your S3 environment to",
    "start": "2443720",
    "end": "2450040"
  },
  {
    "text": "secure S3 the policy language is not just a static point in time definition but it",
    "start": "2450040",
    "end": "2456200"
  },
  {
    "text": "also has this concept of dynamic variable resolution here are some of the examples",
    "start": "2456200",
    "end": "2462680"
  },
  {
    "text": "of what you can refer to within an AM context they include things like the",
    "start": "2462680",
    "end": "2468000"
  },
  {
    "text": "user ID the username how they've authenticated and so on so you might say that you use IM",
    "start": "2468000",
    "end": "2476520"
  },
  {
    "text": "am and some of your public Open Access Data where public means people within",
    "start": "2476520",
    "end": "2481640"
  },
  {
    "text": "your business they just have to be using a secured transport",
    "start": "2481640",
    "end": "2488079"
  },
  {
    "text": "other data is going to be much more subtly secured and an example of this is what",
    "start": "2488079",
    "end": "2494040"
  },
  {
    "text": "if I just wanted to let people write data into my data Lake but it had to be qualified as that they own it you can",
    "start": "2494040",
    "end": "2501040"
  },
  {
    "text": "write a policy that includes the list and the put uses their AWS",
    "start": "2501040",
    "end": "2508160"
  },
  {
    "text": "username and that's all they can get at that's a nice example of a statically defined and yet Dynamic policy for",
    "start": "2508160",
    "end": "2514720"
  },
  {
    "text": "securing our data Lake an I am Federation allows us to extend that",
    "start": "2514720",
    "end": "2520560"
  },
  {
    "text": "definition of who someone is all the way out into our active directory environment so directory Services allows",
    "start": "2520560",
    "end": "2528480"
  },
  {
    "text": "you to run an ad connector and then all of this policy language and all of the security that",
    "start": "2528480",
    "end": "2534680"
  },
  {
    "text": "you're going to build within IAM can be based upon those existing roles in your",
    "start": "2534680",
    "end": "2540200"
  },
  {
    "text": "business but we do see cases where we need to extend that and build more userdefined security taking advantage of",
    "start": "2540640",
    "end": "2547720"
  },
  {
    "text": "things like our data classification policy and this is where the idea of a token vending machine comes in the token",
    "start": "2547720",
    "end": "2553920"
  },
  {
    "text": "vending machine and there's a link there for an implementation of a TVM uh that",
    "start": "2553920",
    "end": "2559800"
  },
  {
    "text": "runs in beanock that would allow you to implement new business rules for",
    "start": "2559800",
    "end": "2564920"
  },
  {
    "text": "temporary access to your data Lake the token vending machine uses I am",
    "start": "2564920",
    "end": "2570800"
  },
  {
    "text": "security token Service Plus other business rules to decide if you have access to something",
    "start": "2570800",
    "end": "2578200"
  },
  {
    "start": "2577000",
    "end": "2577000"
  },
  {
    "text": "and the TVM is going to be combined with encryption whether that's Cloud HSM the AWS Key Management Service or just",
    "start": "2578200",
    "end": "2584680"
  },
  {
    "text": "simple server side encryption to do things like securing",
    "start": "2584680",
    "end": "2590520"
  },
  {
    "text": "access to our encryption Keys we've encrypted our data but we",
    "start": "2590520",
    "end": "2595559"
  },
  {
    "text": "need to also control whether or not people even have rights to decrypt data so we build a customer master key and",
    "start": "2595559",
    "end": "2602680"
  },
  {
    "text": "that generates customer data keys and we encrypt our data with that",
    "start": "2602680",
    "end": "2608839"
  },
  {
    "text": "and then we push it into S3 and we only do that because we had a temporary token",
    "start": "2608839",
    "end": "2614359"
  },
  {
    "text": "that let us do encryption now you might have a policy that says anybody can generate encryption keys and that's fine but",
    "start": "2614359",
    "end": "2621520"
  },
  {
    "text": "there could be very sensitive encryption keys that you do need to control access to on the basis of some other",
    "start": "2621520",
    "end": "2628040"
  },
  {
    "text": "metric so an example of how you would use an entitlement system with a token",
    "start": "2628040",
    "end": "2633319"
  },
  {
    "text": "vending machine to build a very secure data flow as shown here your users don't",
    "start": "2633319",
    "end": "2638640"
  },
  {
    "text": "just push data into S3 or consume data for S from S3 but instead they talk to an",
    "start": "2638640",
    "end": "2644839"
  },
  {
    "text": "API that talks to your token vending machine it checks your metadata index to determine if the customer is operating",
    "start": "2644839",
    "end": "2651920"
  },
  {
    "text": "in a context where they should be able to access data they get an STS token that comes back they encrypt the data",
    "start": "2651920",
    "end": "2658559"
  },
  {
    "text": "with that the encryption keys that that are allowed and then they're able to push data into S3 because the STS token",
    "start": "2658559",
    "end": "2665800"
  },
  {
    "text": "gave them access to S3 just for that put which is a really really nice",
    "start": "2665800",
    "end": "2673200"
  },
  {
    "text": "pattern and then if we wanted to unwind that for gets it would work exactly the same way only that last mile would be a",
    "start": "2673200",
    "end": "2680240"
  },
  {
    "text": "get where our access to an individual object has been secured with an STS",
    "start": "2680240",
    "end": "2686520"
  },
  {
    "text": "token and then lastly we need an API and a user interface that exposes this",
    "start": "2687720",
    "end": "2693359"
  },
  {
    "text": "because while S3 is the foundation for our data Lake customers can no longer just go and",
    "start": "2693359",
    "end": "2699880"
  },
  {
    "text": "browse all the data in S3 they have to work through our entitlement",
    "start": "2699880",
    "end": "2706558"
  },
  {
    "start": "2706000",
    "end": "2706000"
  },
  {
    "text": "system so you may find that when you build your data Lake that I is adequate",
    "start": "2707160",
    "end": "2714720"
  },
  {
    "text": "for your security model but as you move further and further towards building things like data classification into",
    "start": "2714720",
    "end": "2721200"
  },
  {
    "text": "your metadata Index this concept will become more and more important and then we have as the",
    "start": "2721200",
    "end": "2727280"
  },
  {
    "text": "question well should we architect our data Lake in a similar way that we've built services within",
    "start": "2727280",
    "end": "2734480"
  },
  {
    "text": "AWS and say that anything that we expose from a user interface perspective has to go through the API that we've",
    "start": "2734480",
    "end": "2742800"
  },
  {
    "text": "built in any case the API is going to be extremely important to expose our token",
    "start": "2743960",
    "end": "2750359"
  },
  {
    "text": "vending machine an API Gateway is a fantastic service to enable you to present API",
    "start": "2750359",
    "end": "2757640"
  },
  {
    "text": "that do things like have versioning you can distribute API keys and build an SDK for that automatically",
    "start": "2757640",
    "end": "2764319"
  },
  {
    "text": "imagine if you could distribute a data Lake SDK to your internal developers awesome",
    "start": "2764319",
    "end": "2770040"
  },
  {
    "text": "idea and you can do things like exposing Services through whatever language you're happy implementing and whatever",
    "start": "2770040",
    "end": "2776079"
  },
  {
    "text": "runtime whether that's Docker or beant stock but you can also use Lambda functions to implement this this is a",
    "start": "2776079",
    "end": "2782559"
  },
  {
    "text": "great idea how do we build a token vending machine in Lambda it's actually very uh straightforward",
    "start": "2782559",
    "end": "2789720"
  },
  {
    "text": "thing to achieve it also has some really nice features it does some cashing it does",
    "start": "2789720",
    "end": "2795520"
  },
  {
    "text": "throttling it does some really useful things that our data Lake API will",
    "start": "2795520",
    "end": "2800920"
  },
  {
    "text": "require we would highly encourage you to consider building that into your",
    "start": "2800920",
    "end": "2807559"
  },
  {
    "start": "2804000",
    "end": "2804000"
  },
  {
    "text": "environment so whether you're going to be syndicating the data Lake internally or",
    "start": "2808160",
    "end": "2813839"
  },
  {
    "text": "externally all your API calls are going to go via Cloud front and then hit the API Gateway which",
    "start": "2813839",
    "end": "2821240"
  },
  {
    "text": "gives us caching monitoring and then implements our endpoints on ec2 whatever",
    "start": "2821240",
    "end": "2826760"
  },
  {
    "text": "the case may be so ultimately our API in our UI",
    "start": "2826760",
    "end": "2833760"
  },
  {
    "text": "architecture sees that our users fundamentally get access to the API Gateway through I",
    "start": "2833760",
    "end": "2840079"
  },
  {
    "text": "am or through something like our active directory policy we may Implement our user",
    "start": "2840079",
    "end": "2846160"
  },
  {
    "text": "interface in a plastic beant stock and drive that with API Gateway But ultimately the token vending machine and",
    "start": "2846160",
    "end": "2852760"
  },
  {
    "text": "Lambda are really working against our metadata index most of the",
    "start": "2852760",
    "end": "2858800"
  },
  {
    "text": "time to give us a token that allows us to access our data",
    "start": "2858800",
    "end": "2864319"
  },
  {
    "text": "lake so then overall where have we ended",
    "start": "2864760",
    "end": "2870440"
  },
  {
    "text": "up a data Lake needs to be a foundation of extremely durable storage the ability",
    "start": "2870440",
    "end": "2876040"
  },
  {
    "text": "to consume that data and archive it through a streaming interface and crucially includes a",
    "start": "2876040",
    "end": "2882160"
  },
  {
    "text": "metadata index and a workflow which helps us categorize and govern that data",
    "start": "2882160",
    "end": "2887640"
  },
  {
    "text": "that's stored in the data L we add to that metadata index with a",
    "start": "2887640",
    "end": "2893760"
  },
  {
    "start": "2890000",
    "end": "2890000"
  },
  {
    "text": "search index and workflows which enable us to discover new",
    "start": "2893760",
    "end": "2899480"
  },
  {
    "text": "data and even consider indexing from a search perspective the metadata",
    "start": "2899480",
    "end": "2907400"
  },
  {
    "text": "index so that we can discover what aspects of the metadata index are",
    "start": "2907400",
    "end": "2913520"
  },
  {
    "text": "available we also need to absolutely focus on a robust set of security controls governance through technology",
    "start": "2913520",
    "end": "2920400"
  },
  {
    "text": "not through policy for many of us who've done Enterprise data warehousing projects you",
    "start": "2920400",
    "end": "2926880"
  },
  {
    "text": "get a great enforcement of policy because of the model on right",
    "start": "2926880",
    "end": "2933359"
  },
  {
    "text": "semantics but still going around and fighting with people about when they create copies of data and so on is",
    "start": "2933359",
    "end": "2939559"
  },
  {
    "text": "extremely cost ineffective and so if we can Implement controls within our data catalog that",
    "start": "2939559",
    "end": "2945760"
  },
  {
    "text": "dictate when can people create copies of data and how are those copies secured we get all of the data",
    "start": "2945760",
    "end": "2952319"
  },
  {
    "text": "governance rules built in and then the API gives us that last",
    "start": "2952319",
    "end": "2957839"
  },
  {
    "text": "mile to ensure that our entitlement system is respected this isn't something you can leave up to",
    "start": "2957839",
    "end": "2964079"
  },
  {
    "text": "chance you cannot afford the possibility of data leakage from a system like a data Lake because it is everything",
    "start": "2964079",
    "end": "2971559"
  },
  {
    "text": "within your business what does that look",
    "start": "2971559",
    "end": "2978359"
  },
  {
    "text": "like at a very high level we have a foundation of S3 Kinesis",
    "start": "2978359",
    "end": "2986240"
  },
  {
    "text": "and Glacier for our storage and streaming we have a data catalog",
    "start": "2986240",
    "end": "2993000"
  },
  {
    "text": "implemented on Dynamo DB and a cloud search or elastic search",
    "start": "2993000",
    "end": "2998839"
  },
  {
    "text": "index providing a search capability all backed up with AWS Lambda",
    "start": "2998839",
    "end": "3004079"
  },
  {
    "text": "as our data orchestrator for this flow of data",
    "start": "3004079",
    "end": "3009280"
  },
  {
    "text": "through your data Lake and then our entitlement system has at its heart a token vending machine",
    "start": "3009280",
    "end": "3016240"
  },
  {
    "text": "that's built on something like elastic beant stock or the ec2 container service",
    "start": "3016240",
    "end": "3022000"
  },
  {
    "text": "that encapsulates temporary access credentials to data that that are",
    "start": "3022000",
    "end": "3027200"
  },
  {
    "text": "fundamentally based upon the information in our metadata index which gives us the flexibility to",
    "start": "3027200",
    "end": "3033160"
  },
  {
    "text": "Define any type of rule we require sitting at the top the API and",
    "start": "3033160",
    "end": "3038680"
  },
  {
    "text": "the UI expose this to your internal users sits on your internet potentially you expose this as",
    "start": "3038680",
    "end": "3045160"
  },
  {
    "text": "a service to your customers and then the only way that our customers are able to access data on",
    "start": "3045160",
    "end": "3052280"
  },
  {
    "text": "this data lake is through temporary credentials that are V Ed To Us by that",
    "start": "3052280",
    "end": "3057400"
  },
  {
    "text": "token vending",
    "start": "3057400",
    "end": "3059960"
  },
  {
    "text": "machine so thank you very much for your time and attention I'd like to request if you",
    "start": "3064400",
    "end": "3071760"
  },
  {
    "text": "could we would love to hear what you thought about our idea about what a data Lake really ought to",
    "start": "3071760",
    "end": "3077799"
  },
  {
    "text": "be and I would say thank you very much and I would open the floor for any questions that people have uh and thank",
    "start": "3077799",
    "end": "3084200"
  },
  {
    "text": "you very much",
    "start": "3084200",
    "end": "3088559"
  }
]