[
  {
    "text": "hello my name is Laura Cardona and I'm",
    "start": "240",
    "end": "3120"
  },
  {
    "text": "an analytics Solutions architect at AWS",
    "start": "3120",
    "end": "5819"
  },
  {
    "text": "this is the fourth video in a series for",
    "start": "5819",
    "end": "8580"
  },
  {
    "text": "startups called analytics bytes in this",
    "start": "8580",
    "end": "11219"
  },
  {
    "text": "series we Empower you to think big start",
    "start": "11219",
    "end": "13860"
  },
  {
    "text": "small and scale fast on your data",
    "start": "13860",
    "end": "16379"
  },
  {
    "text": "Journey leveraging the modern data",
    "start": "16379",
    "end": "17880"
  },
  {
    "text": "architecture on AWS",
    "start": "17880",
    "end": "19560"
  },
  {
    "text": "in the last video we talked about the",
    "start": "19560",
    "end": "21660"
  },
  {
    "text": "benefits of getting insights from high",
    "start": "21660",
    "end": "23520"
  },
  {
    "text": "volume High Velocity streaming data with",
    "start": "23520",
    "end": "26640"
  },
  {
    "text": "common use cases such as log monitoring",
    "start": "26640",
    "end": "28859"
  },
  {
    "text": "and real-time device management today",
    "start": "28859",
    "end": "31500"
  },
  {
    "text": "I'm going to introduce the topic of",
    "start": "31500",
    "end": "33000"
  },
  {
    "text": "building transactional data Lakes on AWS",
    "start": "33000",
    "end": "37040"
  },
  {
    "text": "to get started we'll look at General",
    "start": "37739",
    "end": "39600"
  },
  {
    "text": "challenges that bring the need for",
    "start": "39600",
    "end": "41219"
  },
  {
    "text": "transactional data Lakes we'll also",
    "start": "41219",
    "end": "43320"
  },
  {
    "text": "explore open source Frameworks",
    "start": "43320",
    "end": "45180"
  },
  {
    "text": "integrated with AWS services that make",
    "start": "45180",
    "end": "47520"
  },
  {
    "text": "them possible and finally we'll look at",
    "start": "47520",
    "end": "50039"
  },
  {
    "text": "a few reference architectures examining",
    "start": "50039",
    "end": "52200"
  },
  {
    "text": "the data flow and see how successful",
    "start": "52200",
    "end": "54059"
  },
  {
    "text": "transactional data Lakes are being",
    "start": "54059",
    "end": "55440"
  },
  {
    "text": "implemented",
    "start": "55440",
    "end": "58038"
  },
  {
    "text": "in the previous three sessions we",
    "start": "58500",
    "end": "60300"
  },
  {
    "text": "introduced the concept and reference",
    "start": "60300",
    "end": "61860"
  },
  {
    "text": "architecture of a serverless data Lake",
    "start": "61860",
    "end": "64198"
  },
  {
    "text": "in stream processing these are two",
    "start": "64199",
    "end": "66780"
  },
  {
    "text": "common starting steps for startup",
    "start": "66780",
    "end": "68340"
  },
  {
    "text": "customers on their data Journey with the",
    "start": "68340",
    "end": "70740"
  },
  {
    "text": "growth of their data and the number of",
    "start": "70740",
    "end": "72540"
  },
  {
    "text": "streaming data sources many startup",
    "start": "72540",
    "end": "74880"
  },
  {
    "text": "customers have higher requirements on",
    "start": "74880",
    "end": "76740"
  },
  {
    "text": "their data Lake they want lower ETL time",
    "start": "76740",
    "end": "80159"
  },
  {
    "text": "of their data pipeline so that the end",
    "start": "80159",
    "end": "82320"
  },
  {
    "text": "consumer can access the data within 5 to",
    "start": "82320",
    "end": "84600"
  },
  {
    "text": "15 minutes latency they need consistent",
    "start": "84600",
    "end": "87840"
  },
  {
    "text": "reads and writes across multiple",
    "start": "87840",
    "end": "89520"
  },
  {
    "text": "concurrent users and they need support",
    "start": "89520",
    "end": "91680"
  },
  {
    "text": "of streaming upserts and more",
    "start": "91680",
    "end": "93720"
  },
  {
    "text": "these requirements are hard to achieve",
    "start": "93720",
    "end": "96060"
  },
  {
    "text": "on a serverless data lake with plain",
    "start": "96060",
    "end": "97860"
  },
  {
    "text": "spark Hive presto",
    "start": "97860",
    "end": "100200"
  },
  {
    "text": "over the past five years there are a few",
    "start": "100200",
    "end": "103140"
  },
  {
    "text": "different open source projects that have",
    "start": "103140",
    "end": "105420"
  },
  {
    "text": "come out targeting these requirements",
    "start": "105420",
    "end": "107340"
  },
  {
    "text": "with different solutions they are Apache",
    "start": "107340",
    "end": "109920"
  },
  {
    "text": "hoodie Apache Iceberg and Delta Lake",
    "start": "109920",
    "end": "113040"
  },
  {
    "text": "these three Frameworks make it possible",
    "start": "113040",
    "end": "115140"
  },
  {
    "text": "to implement transactional data Lakes on",
    "start": "115140",
    "end": "117060"
  },
  {
    "text": "AWS",
    "start": "117060",
    "end": "119600"
  },
  {
    "text": "some challenges that a traditional data",
    "start": "120240",
    "end": "122220"
  },
  {
    "text": "lake has with streaming include having",
    "start": "122220",
    "end": "124140"
  },
  {
    "text": "to deal with duplicate and late arriving",
    "start": "124140",
    "end": "126240"
  },
  {
    "text": "data duplicated events cause data",
    "start": "126240",
    "end": "129000"
  },
  {
    "text": "accuracy problems with analytics and",
    "start": "129000",
    "end": "131160"
  },
  {
    "text": "late arriving data adds processing",
    "start": "131160",
    "end": "132959"
  },
  {
    "text": "complexity for Downstream consumers",
    "start": "132959",
    "end": "135360"
  },
  {
    "text": "streaming data ingestion also generates",
    "start": "135360",
    "end": "137580"
  },
  {
    "text": "small files which is good for right",
    "start": "137580",
    "end": "139200"
  },
  {
    "text": "performance but not for read performance",
    "start": "139200",
    "end": "141680"
  },
  {
    "text": "therefore optimal file sizes have to be",
    "start": "141680",
    "end": "144420"
  },
  {
    "text": "maintained and it requires a certain",
    "start": "144420",
    "end": "146520"
  },
  {
    "text": "platform to manage file sizes",
    "start": "146520",
    "end": "150180"
  },
  {
    "text": "more challenges come from dealing with",
    "start": "150180",
    "end": "152640"
  },
  {
    "text": "change data capture",
    "start": "152640",
    "end": "154680"
  },
  {
    "text": "challenge with consistency and",
    "start": "154680",
    "end": "156420"
  },
  {
    "text": "concurrency",
    "start": "156420",
    "end": "157800"
  },
  {
    "text": "when you're writing a parquet file to an",
    "start": "157800",
    "end": "160200"
  },
  {
    "text": "S3 bucket if there is another job that",
    "start": "160200",
    "end": "162540"
  },
  {
    "text": "reads data in the same prefix while",
    "start": "162540",
    "end": "164280"
  },
  {
    "text": "you're processing it it can produce",
    "start": "164280",
    "end": "166400"
  },
  {
    "text": "inconsistencies due to lack of snapshot",
    "start": "166400",
    "end": "169019"
  },
  {
    "text": "isolation",
    "start": "169019",
    "end": "170340"
  },
  {
    "text": "when you want to update the data in the",
    "start": "170340",
    "end": "172200"
  },
  {
    "text": "data lake with CDC you get a collection",
    "start": "172200",
    "end": "174540"
  },
  {
    "text": "of rows that tell you that a particular",
    "start": "174540",
    "end": "176280"
  },
  {
    "text": "row was updated",
    "start": "176280",
    "end": "177780"
  },
  {
    "text": "the problem is that the metastore does",
    "start": "177780",
    "end": "179760"
  },
  {
    "text": "not actually have any file or record",
    "start": "179760",
    "end": "181860"
  },
  {
    "text": "level indices it lacks an index on the",
    "start": "181860",
    "end": "185099"
  },
  {
    "text": "data storage",
    "start": "185099",
    "end": "186360"
  },
  {
    "text": "therefore if you want a file it must go",
    "start": "186360",
    "end": "189180"
  },
  {
    "text": "to that partition and go to that prefix",
    "start": "189180",
    "end": "191400"
  },
  {
    "text": "on S3 and the job has to do a list maybe",
    "start": "191400",
    "end": "194640"
  },
  {
    "text": "Multiple List operations to get the list",
    "start": "194640",
    "end": "196739"
  },
  {
    "text": "of files we cannot just run an update",
    "start": "196739",
    "end": "199440"
  },
  {
    "text": "command or a SQL statement because",
    "start": "199440",
    "end": "201720"
  },
  {
    "text": "objects are immutable in order to take a",
    "start": "201720",
    "end": "204540"
  },
  {
    "text": "parquet file and update a record from it",
    "start": "204540",
    "end": "206340"
  },
  {
    "text": "we need to read the whole parquet file",
    "start": "206340",
    "end": "208800"
  },
  {
    "text": "process it in memory and then write a",
    "start": "208800",
    "end": "211860"
  },
  {
    "text": "new parquet file to S3 storage no upsert",
    "start": "211860",
    "end": "215040"
  },
  {
    "text": "support results in write amplification",
    "start": "215040",
    "end": "217280"
  },
  {
    "text": "this can be slow and expensive",
    "start": "217280",
    "end": "221900"
  },
  {
    "text": "similarly gdpr gives consumers the right",
    "start": "222720",
    "end": "225840"
  },
  {
    "text": "to be forgotten where consumers can ask",
    "start": "225840",
    "end": "228180"
  },
  {
    "text": "the company to delete their data this",
    "start": "228180",
    "end": "230700"
  },
  {
    "text": "demands record level delete and because",
    "start": "230700",
    "end": "233159"
  },
  {
    "text": "there's no Global index you still must",
    "start": "233159",
    "end": "235620"
  },
  {
    "text": "read the entire data set remove the",
    "start": "235620",
    "end": "238200"
  },
  {
    "text": "specific record and then rewrite the",
    "start": "238200",
    "end": "240120"
  },
  {
    "text": "data this can be very i o intensive",
    "start": "240120",
    "end": "242640"
  },
  {
    "text": "let's look at some open source",
    "start": "242640",
    "end": "244620"
  },
  {
    "text": "Frameworks that solve these challenges",
    "start": "244620",
    "end": "248239"
  },
  {
    "text": "Apache hoodie is an open source",
    "start": "248239",
    "end": "250680"
  },
  {
    "text": "transactional data Lake framework that",
    "start": "250680",
    "end": "252840"
  },
  {
    "text": "greatly simplifies incremental data",
    "start": "252840",
    "end": "255120"
  },
  {
    "text": "processing and data pipeline development",
    "start": "255120",
    "end": "257160"
  },
  {
    "text": "by providing record level insert update",
    "start": "257160",
    "end": "260160"
  },
  {
    "text": "upsert and delete capabilities",
    "start": "260160",
    "end": "262860"
  },
  {
    "text": "you can use it to comply with data",
    "start": "262860",
    "end": "264960"
  },
  {
    "text": "privacy regulations and simplify data",
    "start": "264960",
    "end": "267720"
  },
  {
    "text": "ingestion pipelines that deal with late",
    "start": "267720",
    "end": "269400"
  },
  {
    "text": "arriving or updated records from",
    "start": "269400",
    "end": "271740"
  },
  {
    "text": "streaming data sources or to ingest data",
    "start": "271740",
    "end": "274500"
  },
  {
    "text": "using change data capture from",
    "start": "274500",
    "end": "276360"
  },
  {
    "text": "transactional systems",
    "start": "276360",
    "end": "278160"
  },
  {
    "text": "and finally hoodie is a platform it has",
    "start": "278160",
    "end": "281580"
  },
  {
    "text": "data and table services that are tightly",
    "start": "281580",
    "end": "284100"
  },
  {
    "text": "integrated with the hoodie kernel and it",
    "start": "284100",
    "end": "286199"
  },
  {
    "text": "gives us the ability to deliver",
    "start": "286199",
    "end": "287340"
  },
  {
    "text": "cross-layer optimization reliability and",
    "start": "287340",
    "end": "290520"
  },
  {
    "text": "ease of use",
    "start": "290520",
    "end": "292860"
  },
  {
    "text": "Apache iceberg is a high performance",
    "start": "292860",
    "end": "295020"
  },
  {
    "text": "table format typically used for huge",
    "start": "295020",
    "end": "297540"
  },
  {
    "text": "analytic tables what makes Apache",
    "start": "297540",
    "end": "300000"
  },
  {
    "text": "Iceberg effective is how it stores",
    "start": "300000",
    "end": "302220"
  },
  {
    "text": "records in object storage",
    "start": "302220",
    "end": "304560"
  },
  {
    "text": "Iceberg manages a large collection of",
    "start": "304560",
    "end": "307020"
  },
  {
    "text": "files and tables and it supports modern",
    "start": "307020",
    "end": "308940"
  },
  {
    "text": "analytical data Lake operations Apache",
    "start": "308940",
    "end": "311580"
  },
  {
    "text": "Iceberg contains three layers at the top",
    "start": "311580",
    "end": "314520"
  },
  {
    "text": "we have a catalog which stores the",
    "start": "314520",
    "end": "316320"
  },
  {
    "text": "metadata pointers which keep track of",
    "start": "316320",
    "end": "318419"
  },
  {
    "text": "the current state of the table",
    "start": "318419",
    "end": "320220"
  },
  {
    "text": "then there's the metadata layer which",
    "start": "320220",
    "end": "322199"
  },
  {
    "text": "stores the metadata information of a",
    "start": "322199",
    "end": "324300"
  },
  {
    "text": "table at a certain point in time think",
    "start": "324300",
    "end": "326580"
  },
  {
    "text": "of a snapshot of a particular table",
    "start": "326580",
    "end": "329160"
  },
  {
    "text": "the Manifest list focuses on the table",
    "start": "329160",
    "end": "331919"
  },
  {
    "text": "at a particular point in time and has a",
    "start": "331919",
    "end": "334199"
  },
  {
    "text": "list of manifest files and each manifest",
    "start": "334199",
    "end": "337620"
  },
  {
    "text": "file is a list of the data files and has",
    "start": "337620",
    "end": "340139"
  },
  {
    "text": "a metadata about partitioning columns",
    "start": "340139",
    "end": "342660"
  },
  {
    "text": "Etc",
    "start": "342660",
    "end": "343500"
  },
  {
    "text": "this structure makes purring fast and",
    "start": "343500",
    "end": "345720"
  },
  {
    "text": "the layers allow pruning and trimming",
    "start": "345720",
    "end": "347759"
  },
  {
    "text": "down of which files need to be scanned",
    "start": "347759",
    "end": "351680"
  },
  {
    "text": "Delta lake is a unified data management",
    "start": "351900",
    "end": "354300"
  },
  {
    "text": "system which brings reliability and fast",
    "start": "354300",
    "end": "356820"
  },
  {
    "text": "analytics to cloud data Lakes Delta lake",
    "start": "356820",
    "end": "359520"
  },
  {
    "text": "is a transaction layer that runs on top",
    "start": "359520",
    "end": "361500"
  },
  {
    "text": "of existing cloud data lakes and it's",
    "start": "361500",
    "end": "363780"
  },
  {
    "text": "compatible with the Apache spark API it",
    "start": "363780",
    "end": "366600"
  },
  {
    "text": "has scalable metadata handling and it",
    "start": "366600",
    "end": "368820"
  },
  {
    "text": "unifies streaming and batch processing",
    "start": "368820",
    "end": "372380"
  },
  {
    "text": "although each of these open source",
    "start": "372380",
    "end": "374520"
  },
  {
    "text": "Frameworks integrate with our AWS",
    "start": "374520",
    "end": "376080"
  },
  {
    "text": "Services we'll focus on just Apache",
    "start": "376080",
    "end": "378600"
  },
  {
    "text": "hoodie as an example",
    "start": "378600",
    "end": "380460"
  },
  {
    "text": "Apache hoodie integrates well with our",
    "start": "380460",
    "end": "382979"
  },
  {
    "text": "existing AWS analytics services",
    "start": "382979",
    "end": "385620"
  },
  {
    "text": "we have an AWS blue connector for Apache",
    "start": "385620",
    "end": "388199"
  },
  {
    "text": "hoodie that writes data to a hoodie",
    "start": "388199",
    "end": "390479"
  },
  {
    "text": "table from AWS Blue Jobs",
    "start": "390479",
    "end": "393479"
  },
  {
    "text": "with Amazon EMR release version",
    "start": "393479",
    "end": "396560"
  },
  {
    "text": "5.28.0 and later EMR installs hoodie",
    "start": "396560",
    "end": "400319"
  },
  {
    "text": "components by default when spark Hive",
    "start": "400319",
    "end": "402900"
  },
  {
    "text": "Presto or Flink are installed",
    "start": "402900",
    "end": "405539"
  },
  {
    "text": "hoodie supports sinking hoodie tables to",
    "start": "405539",
    "end": "407880"
  },
  {
    "text": "a catalog on AWS you can either use AWS",
    "start": "407880",
    "end": "411660"
  },
  {
    "text": "blue data catalog or Hive metastore as a",
    "start": "411660",
    "end": "415020"
  },
  {
    "text": "metadata store for your hoodie tables",
    "start": "415020",
    "end": "417960"
  },
  {
    "text": "currently Athena supports snapshot",
    "start": "417960",
    "end": "420300"
  },
  {
    "text": "queries and read optimized queries for",
    "start": "420300",
    "end": "422639"
  },
  {
    "text": "hoodie query types",
    "start": "422639",
    "end": "424979"
  },
  {
    "text": "you can also use Amazon redshift",
    "start": "424979",
    "end": "427259"
  },
  {
    "text": "Spectrum external tables to query data",
    "start": "427259",
    "end": "429780"
  },
  {
    "text": "in Apache hoodie copy on write format",
    "start": "429780",
    "end": "432419"
  },
  {
    "text": "when using the AWS gluted catalog as",
    "start": "432419",
    "end": "435060"
  },
  {
    "text": "your metadata store",
    "start": "435060",
    "end": "437900"
  },
  {
    "text": "point I want to share with you a few",
    "start": "438600",
    "end": "440400"
  },
  {
    "text": "reference architectures that demonstrate",
    "start": "440400",
    "end": "442560"
  },
  {
    "text": "successful transactional data Lakes on",
    "start": "442560",
    "end": "444240"
  },
  {
    "text": "AWS",
    "start": "444240",
    "end": "446220"
  },
  {
    "text": "our first reference architecture is the",
    "start": "446220",
    "end": "448319"
  },
  {
    "text": "batch processing architecture AWS",
    "start": "448319",
    "end": "451139"
  },
  {
    "text": "database migration service is used to",
    "start": "451139",
    "end": "453539"
  },
  {
    "text": "connect Source SQL Server endpoints and",
    "start": "453539",
    "end": "456539"
  },
  {
    "text": "a Target S3 endpoint DMS tasks extract",
    "start": "456539",
    "end": "460319"
  },
  {
    "text": "ongoing changes from The Source SQL",
    "start": "460319",
    "end": "462960"
  },
  {
    "text": "server and loads data into the S3 data",
    "start": "462960",
    "end": "465300"
  },
  {
    "text": "Lake raw Zone a raw zone is a landing S3",
    "start": "465300",
    "end": "468660"
  },
  {
    "text": "bucket for all incoming data stored in",
    "start": "468660",
    "end": "470759"
  },
  {
    "text": "its raw format",
    "start": "470759",
    "end": "472139"
  },
  {
    "text": "and focusing on the transactionality of",
    "start": "472139",
    "end": "474060"
  },
  {
    "text": "the state of Lake Apache hoodie enables",
    "start": "474060",
    "end": "476280"
  },
  {
    "text": "the efficient upserts and deletes which",
    "start": "476280",
    "end": "478560"
  },
  {
    "text": "lead to better data freshness and lower",
    "start": "478560",
    "end": "480660"
  },
  {
    "text": "ETL costs",
    "start": "480660",
    "end": "481919"
  },
  {
    "text": "hoodie also provides the ability to",
    "start": "481919",
    "end": "484020"
  },
  {
    "text": "perform data deletion based on record",
    "start": "484020",
    "end": "485819"
  },
  {
    "text": "key which is crucial for gdpr",
    "start": "485819",
    "end": "489120"
  },
  {
    "text": "AWS glue is used to trigger an Apache",
    "start": "489120",
    "end": "491759"
  },
  {
    "text": "hoodie job accomplishing copy on right",
    "start": "491759",
    "end": "493680"
  },
  {
    "text": "and this reference architecture uses the",
    "start": "493680",
    "end": "496020"
  },
  {
    "text": "hoodie connection and ETL jobs to create",
    "start": "496020",
    "end": "498539"
  },
  {
    "text": "hoodie tables which maintain acid",
    "start": "498539",
    "end": "500639"
  },
  {
    "text": "transactions on the data Lake",
    "start": "500639",
    "end": "502860"
  },
  {
    "text": "this provides data Quality Maintenance",
    "start": "502860",
    "end": "504979"
  },
  {
    "text": "leveraging the AWS glue connector for",
    "start": "504979",
    "end": "507720"
  },
  {
    "text": "Apache hoodie simplifies the process to",
    "start": "507720",
    "end": "510479"
  },
  {
    "text": "create and update Apache hoodie tables",
    "start": "510479",
    "end": "512580"
  },
  {
    "text": "from AWS glue this enables the",
    "start": "512580",
    "end": "515159"
  },
  {
    "text": "consumption layer to have a ready-to-use",
    "start": "515159",
    "end": "517140"
  },
  {
    "text": "version of the data to be queried by",
    "start": "517140",
    "end": "519599"
  },
  {
    "text": "Amazon Athena and then have the data",
    "start": "519599",
    "end": "521760"
  },
  {
    "text": "visualized by Amazon quicksite",
    "start": "521760",
    "end": "525200"
  },
  {
    "text": "for our event stream processing",
    "start": "525200",
    "end": "527580"
  },
  {
    "text": "reference architecture we have data",
    "start": "527580",
    "end": "529800"
  },
  {
    "text": "coming in from iot sensors you may",
    "start": "529800",
    "end": "532620"
  },
  {
    "text": "remember from our last video series",
    "start": "532620",
    "end": "534240"
  },
  {
    "text": "discussing streaming data and see how",
    "start": "534240",
    "end": "536820"
  },
  {
    "text": "this architecture fits the use case of",
    "start": "536820",
    "end": "538800"
  },
  {
    "text": "real-time device management",
    "start": "538800",
    "end": "540839"
  },
  {
    "text": "for ingest we're sending the data to",
    "start": "540839",
    "end": "542760"
  },
  {
    "text": "Amazon Kinesis data streams with Lambda",
    "start": "542760",
    "end": "545700"
  },
  {
    "text": "schema validation happens at this point",
    "start": "545700",
    "end": "547800"
  },
  {
    "text": "and Lambda takes these events and sends",
    "start": "547800",
    "end": "550500"
  },
  {
    "text": "detected anomalies to SNS to trigger",
    "start": "550500",
    "end": "553019"
  },
  {
    "text": "Downstream remediation",
    "start": "553019",
    "end": "555420"
  },
  {
    "text": "the Apache hoodie layer greatly",
    "start": "555420",
    "end": "557700"
  },
  {
    "text": "simplifies incremental data processing",
    "start": "557700",
    "end": "559800"
  },
  {
    "text": "providing support for transactions",
    "start": "559800",
    "end": "561959"
  },
  {
    "text": "record level updates and deletes on the",
    "start": "561959",
    "end": "564060"
  },
  {
    "text": "data and this reference architecture",
    "start": "564060",
    "end": "566399"
  },
  {
    "text": "hoodie is integrating with AWS glue the",
    "start": "566399",
    "end": "569100"
  },
  {
    "text": "Apache hoodie file format is used to",
    "start": "569100",
    "end": "571200"
  },
  {
    "text": "store the data and perform streaming ETL",
    "start": "571200",
    "end": "573540"
  },
  {
    "text": "using AWS glue streaming jobs while",
    "start": "573540",
    "end": "577019"
  },
  {
    "text": "Apache hoodie solves the record level",
    "start": "577019",
    "end": "579240"
  },
  {
    "text": "update and delete challenges AWS blue",
    "start": "579240",
    "end": "582180"
  },
  {
    "text": "streaming jobs convert the long-running",
    "start": "582180",
    "end": "584519"
  },
  {
    "text": "batch Transformations into low latency",
    "start": "584519",
    "end": "587220"
  },
  {
    "text": "micro batch Transformations which allows",
    "start": "587220",
    "end": "590220"
  },
  {
    "text": "writing transform data to Amazon S3",
    "start": "590220",
    "end": "593519"
  },
  {
    "text": "continuously",
    "start": "593519",
    "end": "595560"
  },
  {
    "text": "for the consumption layer data science",
    "start": "595560",
    "end": "597839"
  },
  {
    "text": "teams can access the data sets and",
    "start": "597839",
    "end": "600360"
  },
  {
    "text": "perform ml model training using Amazon's",
    "start": "600360",
    "end": "602880"
  },
  {
    "text": "sagemaker Amazon Athena and Amazon",
    "start": "602880",
    "end": "605339"
  },
  {
    "text": "quicksite are used for ad hoc querying",
    "start": "605339",
    "end": "607620"
  },
  {
    "text": "data visualizations and reports",
    "start": "607620",
    "end": "611660"
  },
  {
    "text": "aspects of these reference architectures",
    "start": "612540",
    "end": "614640"
  },
  {
    "text": "are utilized and bring success to many",
    "start": "614640",
    "end": "617040"
  },
  {
    "text": "companies including NerdWallet",
    "start": "617040",
    "end": "619399"
  },
  {
    "text": "NerdWallet is the company that provides",
    "start": "619399",
    "end": "621660"
  },
  {
    "text": "the tools information and insight people",
    "start": "621660",
    "end": "624180"
  },
  {
    "text": "need to navigate all of life's financial",
    "start": "624180",
    "end": "626519"
  },
  {
    "text": "decisions",
    "start": "626519",
    "end": "627800"
  },
  {
    "text": "NerdWallet had an ever-growing data",
    "start": "627800",
    "end": "630120"
  },
  {
    "text": "volume they soon experienced issues with",
    "start": "630120",
    "end": "632700"
  },
  {
    "text": "scalability and long batch processing",
    "start": "632700",
    "end": "634860"
  },
  {
    "text": "time and complex transformation logic",
    "start": "634860",
    "end": "637740"
  },
  {
    "text": "their workload contained millions of",
    "start": "637740",
    "end": "640140"
  },
  {
    "text": "clients and they had limited",
    "start": "640140",
    "end": "641519"
  },
  {
    "text": "extendability for real-time use cases as",
    "start": "641519",
    "end": "644399"
  },
  {
    "text": "they needed to scale to handle hourly",
    "start": "644399",
    "end": "646860"
  },
  {
    "text": "updates of thousands of record upserts",
    "start": "646860",
    "end": "649320"
  },
  {
    "text": "per second",
    "start": "649320",
    "end": "650640"
  },
  {
    "text": "their solution was to build a data lake",
    "start": "650640",
    "end": "652740"
  },
  {
    "text": "with a serverless stream processing",
    "start": "652740",
    "end": "654480"
  },
  {
    "text": "architecture that can scale to thousands",
    "start": "654480",
    "end": "656880"
  },
  {
    "text": "of writes per second within minutes of",
    "start": "656880",
    "end": "658920"
  },
  {
    "text": "freshness on their data lakes with",
    "start": "658920",
    "end": "661740"
  },
  {
    "text": "hoodie on Amazon S3 nerd wallet built a",
    "start": "661740",
    "end": "665519"
  },
  {
    "text": "high leverage Foundation to personalize",
    "start": "665519",
    "end": "667920"
  },
  {
    "text": "user experience their consumers can now",
    "start": "667920",
    "end": "670620"
  },
  {
    "text": "build more sophisticated signals to",
    "start": "670620",
    "end": "672899"
  },
  {
    "text": "provide Clarity of all of life's",
    "start": "672899",
    "end": "674579"
  },
  {
    "text": "financial decisions",
    "start": "674579",
    "end": "677420"
  },
  {
    "text": "thank you for joining us today as we",
    "start": "677459",
    "end": "679740"
  },
  {
    "text": "explored how to run successful",
    "start": "679740",
    "end": "681120"
  },
  {
    "text": "transactional data Lakeland AWS please",
    "start": "681120",
    "end": "683760"
  },
  {
    "text": "join us again for our next video",
    "start": "683760",
    "end": "685500"
  },
  {
    "text": "discussing Amazon redshift we will look",
    "start": "685500",
    "end": "688440"
  },
  {
    "text": "at common business use cases of data",
    "start": "688440",
    "end": "690480"
  },
  {
    "text": "warehouses and discuss the reference",
    "start": "690480",
    "end": "692279"
  },
  {
    "text": "architectures overall you'll get to see",
    "start": "692279",
    "end": "695160"
  },
  {
    "text": "how the data warehouse plays a vital",
    "start": "695160",
    "end": "697260"
  },
  {
    "text": "role in the modern data architecture",
    "start": "697260",
    "end": "700380"
  },
  {
    "text": "see you next time thank you",
    "start": "700380",
    "end": "704000"
  }
]