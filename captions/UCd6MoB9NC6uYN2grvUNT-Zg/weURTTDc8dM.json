[
  {
    "text": "hi my name is clarissa kosrak and today i'll be walking through an example use case demonstration",
    "start": "5839",
    "end": "11440"
  },
  {
    "text": "for automobile fleet predictive maintenance using amazon sagemaker amazon sagemaker helps data scientists",
    "start": "11440",
    "end": "18160"
  },
  {
    "text": "and developers to quickly prepare build train and deploy high quality machine learning models by",
    "start": "18160",
    "end": "25119"
  },
  {
    "text": "bringing together a broad set of capabilities purpose built for ml we'll be working in",
    "start": "25119",
    "end": "31279"
  },
  {
    "text": "sagemaker studio for this demo sagemaker studio is a fully integrated development environment for machine",
    "start": "31279",
    "end": "37200"
  },
  {
    "text": "learning making it a great tool for rapidly iterating through the building training and deploying of machine",
    "start": "37200",
    "end": "43360"
  },
  {
    "text": "learning models for our predictive maintenance demo the example notebook focuses on binary",
    "start": "43360",
    "end": "50079"
  },
  {
    "text": "classification using a sagemaker built-in algorithm called linear learner in order to predict vehicle failures the",
    "start": "50079",
    "end": "58000"
  },
  {
    "text": "repository containing the example notebook data set and additional documentation",
    "start": "58000",
    "end": "63120"
  },
  {
    "text": "can be found in the link in the description box in order to follow along you will need to clone the example repo",
    "start": "63120",
    "end": "69040"
  },
  {
    "text": "and download the data sets the industry use case we'll be covering is predictive",
    "start": "69040",
    "end": "74159"
  },
  {
    "text": "maintenance now what is predictive maintenance and why is it important for businesses",
    "start": "74159",
    "end": "79200"
  },
  {
    "text": "predictive maintenance is a proactive strategy for minimizing machinery maintenance downtime",
    "start": "79200",
    "end": "84400"
  },
  {
    "text": "by predicting machinery failures some of the main goals for predictive maintenance are",
    "start": "84400",
    "end": "89520"
  },
  {
    "text": "one to reduce operational costs by optimizing the time and effort needed to repair machinery",
    "start": "89520",
    "end": "95360"
  },
  {
    "text": "and two to minimize unexpected downtime due to machinery failures",
    "start": "95360",
    "end": "100560"
  },
  {
    "text": "predictive maintenance allows for businesses to proactively address equipment issues thus reducing unnecessary expenses",
    "start": "100560",
    "end": "107439"
  },
  {
    "text": "boosting scalability and productivity and increasing transparency into production",
    "start": "107439",
    "end": "113600"
  },
  {
    "text": "when machinery breaks down it negatively affects a business's efficiency productivity and growth with the",
    "start": "113600",
    "end": "120000"
  },
  {
    "text": "adoption of machine learning you can create actionable insights and extend the remaining useful life of your",
    "start": "120000",
    "end": "125200"
  },
  {
    "text": "machinery at the lowest cost and improve operational efficiency",
    "start": "125200",
    "end": "130479"
  },
  {
    "text": "for a use case demo today we'll start off by briefly exploring sagemaker studio then transition to preparing the fleet",
    "start": "130479",
    "end": "137040"
  },
  {
    "text": "data set using sagemaker datawrangler and finally walk through an example notebook for",
    "start": "137040",
    "end": "142319"
  },
  {
    "text": "predictive maintenance this will be a great opportunity for data scientists and developers new to",
    "start": "142319",
    "end": "148000"
  },
  {
    "text": "aws and sagemaker studio to explore some of the sagemaker features in action and be able to quickly create",
    "start": "148000",
    "end": "154480"
  },
  {
    "text": "their own predictive maintenance solution to fit their business needs note that for this demo you will first need to",
    "start": "154480",
    "end": "160959"
  },
  {
    "text": "create an aws account and you will also need to create an s3 bucket and with that let's get started great so",
    "start": "160959",
    "end": "168879"
  },
  {
    "text": "once you open up sagemaker studio and the launcher tab is open you can see that the launcher has a variety of",
    "start": "168879",
    "end": "175040"
  },
  {
    "text": "different features and quick start methods that you can use to get up and running with sagemaker studio",
    "start": "175040",
    "end": "181040"
  },
  {
    "text": "you could try using the sagemaker jumpstart or use autumn pilot which will quickly create a",
    "start": "181040",
    "end": "186720"
  },
  {
    "text": "model automatically and under ml tasks and components you have the ability to leverage some of the",
    "start": "186720",
    "end": "192480"
  },
  {
    "text": "new features from sagemaker so first off we have new feature group and new feature group",
    "start": "192480",
    "end": "197599"
  },
  {
    "text": "is a new amazon feature called amazon sagemaker feature store",
    "start": "197599",
    "end": "202879"
  },
  {
    "text": "and that will connect with datawrangler and here we have a new dataflow and for the new dataflow this will",
    "start": "202879",
    "end": "208879"
  },
  {
    "text": "leverage the sagemaker data wrangler feature which we will actually walk through in a bit you also have the ability to jump",
    "start": "208879",
    "end": "216480"
  },
  {
    "text": "right into using a python 3 notebook and you can select a new notebook",
    "start": "216480",
    "end": "221599"
  },
  {
    "text": "and you can select which sagemaker image you want to use by selecting this drop down and picking from this list to start off",
    "start": "221599",
    "end": "229040"
  },
  {
    "text": "we will go to our file browser by selecting the icon in the top left hand corner and here we",
    "start": "229040",
    "end": "234959"
  },
  {
    "text": "can see any folders or notebooks that are available to us here you can see i have the aws",
    "start": "234959",
    "end": "241040"
  },
  {
    "text": "predictive maintenance repo already cloned right here and just as a note you will need to clone the aws predictive maintenance",
    "start": "241040",
    "end": "247360"
  },
  {
    "text": "repo and upload the data sets to s3 in order to follow along with this tutorial there",
    "start": "247360",
    "end": "254000"
  },
  {
    "text": "are also a few other additional icons on the left hand side that i'm going to walk you through so under the file browser we have git",
    "start": "254000",
    "end": "260799"
  },
  {
    "text": "and this is actually where i cloned the git repository needed for this tutorial and you can initialize the repository",
    "start": "260799",
    "end": "267520"
  },
  {
    "text": "here as well under git we have the running terminals and the kernels",
    "start": "267520",
    "end": "272639"
  },
  {
    "text": "so if you select this tab you can see here that we have our running instances our apps and our kernel sessions",
    "start": "272639",
    "end": "278720"
  },
  {
    "text": "and this will tell us exactly what is running the size of the instances that are running and then also",
    "start": "278720",
    "end": "284400"
  },
  {
    "text": "we have the ability here to kill any of the instances sessions or running apps if needed",
    "start": "284400",
    "end": "290960"
  },
  {
    "text": "so the last tab i'm going to walk you through is the sagemaker components and registries tab and that's this bottom",
    "start": "290960",
    "end": "296400"
  },
  {
    "text": "one down here and here you're able to select the different components or the registries that you want to view",
    "start": "296400",
    "end": "302320"
  },
  {
    "text": "so if you select the drop down you can see that there are projects data wrangler feature store pipeline etc and this is a really",
    "start": "302320",
    "end": "309600"
  },
  {
    "text": "great tool to be able to navigate and toggle between the different components and registries",
    "start": "309600",
    "end": "315360"
  },
  {
    "text": "within the sagemaker ecosystem so for our purposes if we select data",
    "start": "315360",
    "end": "320400"
  },
  {
    "text": "wrangler we can see it gets populated with the data wrangler flows that have already been created i have three here that are other flows",
    "start": "320400",
    "end": "328479"
  },
  {
    "text": "not related to our project and then we have this prm predictive maintenance dot flow file",
    "start": "328479",
    "end": "334240"
  },
  {
    "text": "so if we go ahead and select the flow file we open up and get this data flow for our data",
    "start": "334240",
    "end": "340880"
  },
  {
    "text": "wrangler job and i've mentioned datawrangler before but what exactly is sagemaker",
    "start": "340880",
    "end": "346479"
  },
  {
    "text": "datawrangler well sagemaker datawrangler is a new feature that simplifies the process of data preparation and feature",
    "start": "346479",
    "end": "353120"
  },
  {
    "text": "engineering it allows you to complete each step of the data preparation workflow including data selection",
    "start": "353120",
    "end": "359680"
  },
  {
    "text": "cleansing exploration and visualization all from a single interface making data preparation quicker and",
    "start": "359680",
    "end": "366319"
  },
  {
    "text": "easier i'll start by walking us through the data flow for the predictive maintenance use case",
    "start": "366319",
    "end": "371680"
  },
  {
    "text": "demo and as you can see here i have already imported the data from our two",
    "start": "371680",
    "end": "376800"
  },
  {
    "text": "sources so we have our source here which is from s3 and this is the example fleet",
    "start": "376800",
    "end": "381880"
  },
  {
    "text": "info.csv as well as the other object in s3 which is examplefleetsensorlogs.csv",
    "start": "381880",
    "end": "388800"
  },
  {
    "text": "when you're importing data into datawrangler you have the option of selecting between a variety of different",
    "start": "388800",
    "end": "394160"
  },
  {
    "text": "sources so for our purposes we used s3 but you could also select through amazon athena",
    "start": "394160",
    "end": "399680"
  },
  {
    "text": "or other amazon sources both of these data sets are coming from my s3 bucket",
    "start": "399680",
    "end": "405199"
  },
  {
    "text": "called sagemaker fleet predictive maintenance solution and you will need to create your own s3 bucket and load the data sets into it in",
    "start": "405199",
    "end": "412319"
  },
  {
    "text": "order to use the data wrangler flow a little bit more detail about these two data sets",
    "start": "412319",
    "end": "418080"
  },
  {
    "text": "so the first one is example fleet info.csv",
    "start": "418080",
    "end": "423240"
  },
  {
    "text": "examplefleetinfo.csv is fleet data information about the vehicles such as make and model",
    "start": "423240",
    "end": "429360"
  },
  {
    "text": "and their engine types and has not and it has other information about the vehicle fleets as",
    "start": "429360",
    "end": "435120"
  },
  {
    "text": "well the second data set is the example fleet sensors log.csv",
    "start": "435120",
    "end": "440560"
  },
  {
    "text": "and this data set includes all of the sensor log information for each of the vehicles engines",
    "start": "440560",
    "end": "446400"
  },
  {
    "text": "within datawrangler you have the ability of using 300 built-in data transformations on",
    "start": "446400",
    "end": "452240"
  },
  {
    "text": "your data and you can use all of them from this convenient visual user interface",
    "start": "452240",
    "end": "458080"
  },
  {
    "text": "within datawrangler you also have the option of creating custom transformations using pi spark",
    "start": "458080",
    "end": "463759"
  },
  {
    "text": "sql or pandas we can do things like encode categorical variables or deal with",
    "start": "463759",
    "end": "469440"
  },
  {
    "text": "missing data or outliers we can add or remove columns and the list of possible transformations goes on",
    "start": "469440",
    "end": "475440"
  },
  {
    "text": "and on now that i've given you a brief introduction to sagemaker data wrangler",
    "start": "475440",
    "end": "481039"
  },
  {
    "text": "i'm going to walk you through this date wrangler flow for the use case and go into a little bit of detail of",
    "start": "481039",
    "end": "486560"
  },
  {
    "text": "all of the different steps for the flow and show you how i exported the file some of these steps are going to include",
    "start": "486560",
    "end": "492639"
  },
  {
    "text": "things like encoding categorical columns renaming columns reordering the data set",
    "start": "492639",
    "end": "498160"
  },
  {
    "text": "and joining both the data sets together this is less of a step-by-step process and more of an overview of the flow",
    "start": "498160",
    "end": "505680"
  },
  {
    "text": "to start off we can look at the data types for example fleet info.csv and inspect the data set a little bit so",
    "start": "505680",
    "end": "513599"
  },
  {
    "text": "we select data types and here you can see again that the example fleet info csv",
    "start": "513599",
    "end": "519360"
  },
  {
    "text": "is mostly attributes of each of the vehicles so we have the make the model the year vehicle class and engine type",
    "start": "519360",
    "end": "527279"
  },
  {
    "text": "and if we go back to the data flow and select the data types for the other data",
    "start": "527279",
    "end": "532839"
  },
  {
    "text": "set here we can see that we have the vehicle id again which will be",
    "start": "532839",
    "end": "538000"
  },
  {
    "text": "needed for our join later on we have the target and then we have time stamp which our time stamp is every two hours and",
    "start": "538000",
    "end": "545760"
  },
  {
    "text": "then we have voltage current and resistance and voltage current or resistance these readings are taken every two hours",
    "start": "545760",
    "end": "552880"
  },
  {
    "text": "if we hop back to the data flow then in steps parentheses four",
    "start": "552880",
    "end": "558480"
  },
  {
    "text": "we can select the bottom encode categorical and from here we can see that we have",
    "start": "558480",
    "end": "564880"
  },
  {
    "text": "previous steps six and the six steps are to",
    "start": "564880",
    "end": "570000"
  },
  {
    "text": "import the data sets look at the data types and change them if needed like we just did and then we have these four encode",
    "start": "570000",
    "end": "577040"
  },
  {
    "text": "categorical steps so as we saw in the previous step we have the make the model",
    "start": "577040",
    "end": "582800"
  },
  {
    "text": "year the vehicle class and the engine type as our fleet vehicle info and all of these except for",
    "start": "582800",
    "end": "589760"
  },
  {
    "text": "year are considered non-ordinal so we can create these in code categorical steps",
    "start": "589760",
    "end": "595920"
  },
  {
    "text": "and using the one hot encode transform as opposed to ordinal we select our input column so i did this",
    "start": "595920",
    "end": "602640"
  },
  {
    "text": "for make model vehicle class and engine type and then i decided to keep the",
    "start": "602640",
    "end": "609680"
  },
  {
    "text": "invalid handling strategy as keep instead of skip or air",
    "start": "609680",
    "end": "615120"
  },
  {
    "text": "and finally i changed all of the names to be the original name in this case it's engine underscore type",
    "start": "615120",
    "end": "621040"
  },
  {
    "text": "and then underscore code from there if we go back to the data flow and look at the transform",
    "start": "621040",
    "end": "628240"
  },
  {
    "text": "step for manage columns",
    "start": "628240",
    "end": "633839"
  },
  {
    "text": "from here we can see that we had three previous steps which were the importing of the data the looking",
    "start": "636320",
    "end": "642320"
  },
  {
    "text": "and the changing of the data types and the managing columns and here for manage columns i just went",
    "start": "642320",
    "end": "647519"
  },
  {
    "text": "ahead and changed the name from timestamp to datetime and i did this because the datetime column will",
    "start": "647519",
    "end": "653680"
  },
  {
    "text": "will be needed for the notebook later on so going back to the data flow the next",
    "start": "653680",
    "end": "659200"
  },
  {
    "text": "step is to join both of the data sets and i will go ahead and and i'll go",
    "start": "659200",
    "end": "664320"
  },
  {
    "text": "ahead and skip over to the steps parentheses five and hit the",
    "start": "664320",
    "end": "669360"
  },
  {
    "text": "bottom manage columns and if we go to previous step six the",
    "start": "669360",
    "end": "676959"
  },
  {
    "text": "first step is this join so i'll select the join and we can inspect it a little bit further",
    "start": "676959",
    "end": "682000"
  },
  {
    "text": "so you can see i did a right outer join i joined both the vehicle id and vehicle id from both of the tables",
    "start": "682000",
    "end": "689600"
  },
  {
    "text": "and from there we get a merge table that has both of the vehicle id columns still",
    "start": "689600",
    "end": "694720"
  },
  {
    "text": "in it we have vehicle id 0 and vehicle id1 and that is because those are the join",
    "start": "694720",
    "end": "700640"
  },
  {
    "text": "keys and they keep both of the columns so i went ahead and dropped the vehicle id 0",
    "start": "700640",
    "end": "707519"
  },
  {
    "text": "change vehicle id 1 to vehicle id and then i moved the three columns",
    "start": "707519",
    "end": "714839"
  },
  {
    "text": "target date time and vehicle id to the front of the data set",
    "start": "714839",
    "end": "721839"
  },
  {
    "text": "one thing to note here is that these data sets are generally pretty clean because they're generated",
    "start": "721839",
    "end": "726880"
  },
  {
    "text": "data so there are not a lot of transformation steps that are done for this use case tutorial but i encourage you to",
    "start": "726880",
    "end": "734000"
  },
  {
    "text": "explore some of the additional transforms that you can do under the ad header you can add custom transforms",
    "start": "734000",
    "end": "741120"
  },
  {
    "text": "such as using pi spark or pandas or sql and you can also add in custom formulas",
    "start": "741120",
    "end": "747839"
  },
  {
    "text": "or encode variables differently you can featurize text and format string",
    "start": "747839",
    "end": "753360"
  },
  {
    "text": "so i encourage you to explore more of the transforms that are available and even use some of the transforms for",
    "start": "753360",
    "end": "759360"
  },
  {
    "text": "analysis so once we're done with these transformations we can go back to the",
    "start": "759360",
    "end": "764399"
  },
  {
    "text": "data flow select analyze",
    "start": "764399",
    "end": "769040"
  },
  {
    "text": "and under analyze and the steps parentheses five we go down to manage columns where the",
    "start": "771440",
    "end": "776639"
  },
  {
    "text": "chart icon is select manage columns and this will take us to our analysis pane so here we have",
    "start": "776639",
    "end": "784560"
  },
  {
    "text": "all of the different analyze charts that we've created thus far so we select the vehicle age",
    "start": "784560",
    "end": "789680"
  },
  {
    "text": "distribution you can see here that the analysis populates and from here we can see",
    "start": "789680",
    "end": "796720"
  },
  {
    "text": "a snippet of the table we can also see the selections that i've made so here i've",
    "start": "796720",
    "end": "802000"
  },
  {
    "text": "created a histogram called vehicle age distribution and we are looking at the",
    "start": "802000",
    "end": "807200"
  },
  {
    "text": "age of the vehicles and it's going to be colored by the target so from here we can go",
    "start": "807200",
    "end": "813279"
  },
  {
    "text": "back to analysis and by selecting create new analysis we are opened up to a new analysis pane",
    "start": "813279",
    "end": "819680"
  },
  {
    "text": "where we can create a new analysis chart so here we have the option of either using the",
    "start": "819680",
    "end": "825440"
  },
  {
    "text": "configure options such as any of these drop-down charts or we can go into code and create",
    "start": "825440",
    "end": "832000"
  },
  {
    "text": "our own chart based on pandas data frames or we can come here to create our own chart",
    "start": "832000",
    "end": "838480"
  },
  {
    "text": "using a pandas data frame so for purposes here we'll go to configure",
    "start": "838480",
    "end": "844880"
  },
  {
    "text": "we will select table summary and rename the analysis",
    "start": "844880",
    "end": "852320"
  },
  {
    "text": "summary stats and here we have a populated",
    "start": "854839",
    "end": "863440"
  },
  {
    "text": "table summary of all of the different summary statistics for our data set so you can take",
    "start": "863440",
    "end": "869360"
  },
  {
    "text": "advantage of this and look into some of the different summary statistics and explore your data set earlier on and",
    "start": "869360",
    "end": "877040"
  },
  {
    "text": "then once you are done looking at here you can select create and you will have created a new table",
    "start": "877040",
    "end": "882079"
  },
  {
    "text": "summary and here you can see all of your analyses once you're all done with your analysis",
    "start": "882079",
    "end": "887360"
  },
  {
    "text": "you can select export and it will take you back to the data flow from here you select the final step",
    "start": "887360",
    "end": "894959"
  },
  {
    "text": "so for us it's going to be step 5. manage columns and then you'll see that",
    "start": "894959",
    "end": "900800"
  },
  {
    "text": "on the right hand side we have this export step pop-up so we'll select export step and",
    "start": "900800",
    "end": "906800"
  },
  {
    "text": "here we have the option of exporting our data flow as a data wrangler job a pipeline python code or feature store and just to",
    "start": "906800",
    "end": "915040"
  },
  {
    "text": "remind you feature store is a new sagemaker feature so we have the ability here to export",
    "start": "915040",
    "end": "920160"
  },
  {
    "text": "our data flow as a feature store feature group and that will add the features to either an",
    "start": "920160",
    "end": "925360"
  },
  {
    "text": "offline or an online feature store for our purposes we are going to use the datawrangler job",
    "start": "925360",
    "end": "931759"
  },
  {
    "text": "as our export and i've already gone ahead and done this so once you select this you will be taken to a new notebook that",
    "start": "931759",
    "end": "940000"
  },
  {
    "text": "will be your data wrangler export and here's mine and the only things to",
    "start": "940000",
    "end": "948560"
  },
  {
    "text": "note here that i changed and that you would need to do as well if you're going to follow along with this tutorial step by step",
    "start": "948560",
    "end": "954720"
  },
  {
    "text": "is that under parameters i've gone ahead and added in the bucket as my bucket name i've",
    "start": "954720",
    "end": "960800"
  },
  {
    "text": "also used the magic store for bucket and i've also changed the name to dw",
    "start": "960800",
    "end": "966320"
  },
  {
    "text": "underscore output underscore prefix and again stored the dw underscore output underscore prefix so",
    "start": "966320",
    "end": "973279"
  },
  {
    "text": "both of these stored variables will be used later on in the actual notebook so you'll need to change those on your end",
    "start": "973279",
    "end": "979279"
  },
  {
    "text": "to make sure that you have all of the right steps for this tutorial and that's it so we made it through all",
    "start": "979279",
    "end": "985040"
  },
  {
    "text": "of the sagemaker data wrangler steps you've heard a little bit about what datawrangler is and how you can leverage",
    "start": "985040",
    "end": "991199"
  },
  {
    "text": "it for this demo and also how you can start playing around with datawrangler for your own data sets",
    "start": "991199",
    "end": "996800"
  },
  {
    "text": "so now that we're done here we're going to flip over to our actual notebook and get started with our demo",
    "start": "996800",
    "end": "1003360"
  },
  {
    "text": "now that we have finished our walkthrough of the sagemaker data wrangler flow we can go into the aws predictive",
    "start": "1003440",
    "end": "1010160"
  },
  {
    "text": "maintenance repo and open the linear learner fleet predictive maintenance notebook",
    "start": "1010160",
    "end": "1015920"
  },
  {
    "text": "once you're in the example notebook you can see that there is a mix of documentation and code in python this python code can",
    "start": "1015920",
    "end": "1023519"
  },
  {
    "text": "be run in entirety or you can run it cell by cell we will actually walk through a pre-run",
    "start": "1023519",
    "end": "1029199"
  },
  {
    "text": "version of this notebook so that we can see all of the outputs at once although this notebook has commentary",
    "start": "1029199",
    "end": "1034798"
  },
  {
    "text": "throughout i highly recommend going back to the repo and reading the readme markdown file for more information and",
    "start": "1034799",
    "end": "1041600"
  },
  {
    "text": "additional background about predictive maintenance there you will also find an architecture diagram so the example notebook that",
    "start": "1041600",
    "end": "1048640"
  },
  {
    "text": "we're going to walk through today is using a sagemaker built-in algorithm called linear learner",
    "start": "1048640",
    "end": "1053840"
  },
  {
    "text": "in order to predict for vehicle fleet failures predictive maintenance is a useful use",
    "start": "1053840",
    "end": "1058880"
  },
  {
    "text": "case for a wide reach of customers there are many types of predictive maintenance solutions such as anomaly",
    "start": "1058880",
    "end": "1064559"
  },
  {
    "text": "detection remaining useful life when you predict how much time is left before the next failure",
    "start": "1064559",
    "end": "1070000"
  },
  {
    "text": "or fault classification when you are predicting whether or not a machine will fail within a predicted time frame",
    "start": "1070000",
    "end": "1076400"
  },
  {
    "text": "for the purposes of this use case we will approach this problem as a binary classification problem for fault",
    "start": "1076400",
    "end": "1082080"
  },
  {
    "text": "classification for our first attempt at solving our fault classification problem we will leverage the sagemaker built-in",
    "start": "1082080",
    "end": "1089120"
  },
  {
    "text": "linear learner algorithm which can be used for either classification or regression",
    "start": "1089120",
    "end": "1094640"
  },
  {
    "text": "with this algorithm you can simultaneously explore different training objectives and choose the best solution from a",
    "start": "1094640",
    "end": "1100400"
  },
  {
    "text": "validation set you can also explore a large number of models and seamlessly select the best model to deploy",
    "start": "1100400",
    "end": "1108000"
  },
  {
    "text": "another advantage of linear learner is that it's very fast and allows you to iterate through modeling in an efficient",
    "start": "1108000",
    "end": "1113840"
  },
  {
    "text": "manner as with many classification problems one of the biggest challenges for predictive",
    "start": "1113840",
    "end": "1119120"
  },
  {
    "text": "maintenance is data issues and that there is generally a class imbalance issue meaning that there are far fewer",
    "start": "1119120",
    "end": "1125200"
  },
  {
    "text": "examples of failures compared to non-failures in most cases there are fewer failures",
    "start": "1125200",
    "end": "1130799"
  },
  {
    "text": "than non-failures because businesses don't generally want to allow for more failures just for the sake of having more failure",
    "start": "1130799",
    "end": "1137120"
  },
  {
    "text": "data another issue is that failures could be inaccurately labeled which could also cause there to be fewer",
    "start": "1137120",
    "end": "1143440"
  },
  {
    "text": "failures in the data set class imbalances can make fault classification problems more difficult",
    "start": "1143440",
    "end": "1148880"
  },
  {
    "text": "so that's something to be aware of and think through as you're going through this use case the overall flow in this notebook will",
    "start": "1148880",
    "end": "1155280"
  },
  {
    "text": "be to first set up our notebook with sagemaker download our clean data from sagemaker",
    "start": "1155280",
    "end": "1160400"
  },
  {
    "text": "data wrangler then do some additional data cleaning and feature engineering then we'll start the modeling section by",
    "start": "1160400",
    "end": "1166880"
  },
  {
    "text": "first creating a sagemaker experiment we will then build our linear learner model train the linear learner algorithm",
    "start": "1166880",
    "end": "1173840"
  },
  {
    "text": "do hyperparameter tuning with sagemaker automatic tuning job and finally use batch transform to",
    "start": "1173840",
    "end": "1179840"
  },
  {
    "text": "deploy our best model and get inferences on our test data set now that we've gone through the",
    "start": "1179840",
    "end": "1185200"
  },
  {
    "text": "background of the fleet predictive maintenance notebook we can start with the setup",
    "start": "1185200",
    "end": "1190400"
  },
  {
    "text": "if we scroll down we can see with many sagemaker studio example notebooks the",
    "start": "1190400",
    "end": "1195600"
  },
  {
    "text": "setup is pretty consistent as i mentioned before you'll need an s3 bucket and you'll need to set up an iam",
    "start": "1195600",
    "end": "1201520"
  },
  {
    "text": "role for sagemaker and for data permissions in this cell here i have added in a magic store and you can",
    "start": "1201520",
    "end": "1209120"
  },
  {
    "text": "retrieve the stored variables from previous sessions if you want to pick up the notebook and run part of it and then",
    "start": "1209120",
    "end": "1215520"
  },
  {
    "text": "return to it later and down here i've also imported the necessary libraries",
    "start": "1215520",
    "end": "1222320"
  },
  {
    "text": "we'll also be using the high level sagemaker python sdk to allow us to easily work with amazon",
    "start": "1222320",
    "end": "1227840"
  },
  {
    "text": "sagemaker and call its apis in a data science friendly way here i've added in the bucket which is",
    "start": "1227840",
    "end": "1234480"
  },
  {
    "text": "sagemaker fleet predictive maintenance solution you will want to replace this with your own s3 bucket name",
    "start": "1234480",
    "end": "1241120"
  },
  {
    "text": "during this setup we will also install and import the newest version of sagemaker experiments as well as boto3",
    "start": "1241120",
    "end": "1248880"
  },
  {
    "text": "after we have completed our setup we'll move on to the data preparation of the notebook",
    "start": "1248880",
    "end": "1254880"
  },
  {
    "text": "so we saw before with sagemaker datawrangler that we were able to load in our two datasets",
    "start": "1254880",
    "end": "1260000"
  },
  {
    "text": "change our column data types and our headers we're able to join the data sets together do some preliminary analysis and create",
    "start": "1260000",
    "end": "1267280"
  },
  {
    "text": "some transforms for our data set and finally we were able to export our data flow as a",
    "start": "1267280",
    "end": "1272960"
  },
  {
    "text": "sagemaker data wrangler job now we will be able to pick up where we left off by downloading the processed",
    "start": "1272960",
    "end": "1279120"
  },
  {
    "text": "data from sagemaker datawrangler and reading the data into python using pandas",
    "start": "1279120",
    "end": "1284720"
  },
  {
    "text": "so in this section of the notebook we will be downloading the dataset from sagemaker datawrangler",
    "start": "1284720",
    "end": "1290320"
  },
  {
    "text": "and reading all the csv files into a pandas data frame",
    "start": "1290320",
    "end": "1295679"
  },
  {
    "text": "once the data set is loaded into a pandas data frame we'll add in some additional features and change some data",
    "start": "1295919",
    "end": "1301440"
  },
  {
    "text": "types before reordering the columns so that the target column is first as that is necessary for the linear",
    "start": "1301440",
    "end": "1307360"
  },
  {
    "text": "learner algorithm so here we add in our additional features we've changed our data types",
    "start": "1307360",
    "end": "1313760"
  },
  {
    "text": "added in the vehicle class engine type and engine age features as categories",
    "start": "1313760",
    "end": "1320880"
  },
  {
    "text": "and finally reordered again it's important to note here that the data set we're using is generated",
    "start": "1320880",
    "end": "1327360"
  },
  {
    "text": "vehicle fleet data based off another aws labs repository thus the data sets are pretty clean and",
    "start": "1327360",
    "end": "1333440"
  },
  {
    "text": "require minimal cleaning and a more normal predictive maintenance solution and pre-processing necessary in",
    "start": "1333440",
    "end": "1340240"
  },
  {
    "text": "order to get the data sets and the state ready for modeling so once we've made all of those changes",
    "start": "1340240",
    "end": "1345600"
  },
  {
    "text": "and take a closer look at our data set we can see that the data contains the target the vehicle id",
    "start": "1345600",
    "end": "1350880"
  },
  {
    "text": "and date time of each of the sensor readings as well as the make model year vehicle class and engine type",
    "start": "1350880",
    "end": "1357840"
  },
  {
    "text": "we also have the sensor readings which are voltage current and resistance and all the features that are related to",
    "start": "1357840",
    "end": "1363280"
  },
  {
    "text": "the engine's performance we have made a few initial features and after some exploration we will create",
    "start": "1363280",
    "end": "1368880"
  },
  {
    "text": "some additional time series features some of the observations are that we can see that the data set has 9",
    "start": "1368880",
    "end": "1375440"
  },
  {
    "text": "000 observations and 43 columns and we can also note that the vehicles can be",
    "start": "1375440",
    "end": "1380559"
  },
  {
    "text": "identified using the vehicle underscore id column and each of the vehicles has 100",
    "start": "1380559",
    "end": "1387200"
  },
  {
    "text": "rows associated with them each row represents one time stamp",
    "start": "1387200",
    "end": "1392400"
  },
  {
    "text": "with three sensor readings we also know that the label column called target is the indicator of failure",
    "start": "1392400",
    "end": "1398480"
  },
  {
    "text": "where zero is no failure and one is a failure next we can plot some visuals and do",
    "start": "1398480",
    "end": "1404000"
  },
  {
    "text": "some initial analysis as i mentioned before this is a class and balance problem so we can see here",
    "start": "1404000",
    "end": "1410320"
  },
  {
    "text": "that there are about 19.6 failures compared to about 80-ish",
    "start": "1410320",
    "end": "1416799"
  },
  {
    "text": "non-failures so this is definitely more of a class imbalance problem in the next cell we can evaluate the",
    "start": "1416799",
    "end": "1423039"
  },
  {
    "text": "percentage of failures by the vehicle id we can see that there are no missing values and none of the sensor readings",
    "start": "1423039",
    "end": "1429679"
  },
  {
    "text": "are giving us zero values which would be an abnormality for this data set once we've finished up our data cleaning",
    "start": "1429679",
    "end": "1435919"
  },
  {
    "text": "and our initial analysis we can move on to our feature engineering so if we scroll down to the next section",
    "start": "1435919",
    "end": "1441679"
  },
  {
    "text": "we get into the feature engineering and for our purposes we will create some",
    "start": "1441679",
    "end": "1447520"
  },
  {
    "text": "time series features and use categorical labels that we created before we're just going to focus on getting the",
    "start": "1447520",
    "end": "1454080"
  },
  {
    "text": "lag for voltage current and resistance and we will focus on lags because with time series data",
    "start": "1454080",
    "end": "1459600"
  },
  {
    "text": "we know that the sensor readings at time t are probably related to the sensor readings at time t minus 1",
    "start": "1459600",
    "end": "1465600"
  },
  {
    "text": "and t minus 2 etc we will also want to create rolling statistics for voltage current",
    "start": "1465600",
    "end": "1471279"
  },
  {
    "text": "and resistance and group them by vehicle id the statistics we will focus on are mean and",
    "start": "1471279",
    "end": "1476640"
  },
  {
    "text": "standard deviation once we've created the rolling mean and rolling standard deviation for our",
    "start": "1476640",
    "end": "1481679"
  },
  {
    "text": "window then we'll join the new columns with the fleet data set and that's what we get here we can also",
    "start": "1481679",
    "end": "1487520"
  },
  {
    "text": "take one more look at the summary statistics as seen here",
    "start": "1487520",
    "end": "1493200"
  },
  {
    "text": "and also look at some of the plots of the histograms for the different features",
    "start": "1494799",
    "end": "1501760"
  },
  {
    "text": "once we've finalized our feature data set and saved it to csv now we can move on to training i",
    "start": "1501760",
    "end": "1507760"
  },
  {
    "text": "mentioned before that we are treating our predictive maintenance sensor readings as time series data so want to split our train test and",
    "start": "1507760",
    "end": "1514799"
  },
  {
    "text": "validation sets based on time and also take special care to make sure there is no data leakage",
    "start": "1514799",
    "end": "1520880"
  },
  {
    "text": "here we will devote 80 to training and we'll save 10 for 10 and about 10 percent for",
    "start": "1520880",
    "end": "1526240"
  },
  {
    "text": "validation keeping in mind that the validation data set will have slightly less records to avoid the leakage from overlapping",
    "start": "1526240",
    "end": "1533520"
  },
  {
    "text": "dates when we are happy with our train tests and validation sets",
    "start": "1533520",
    "end": "1539520"
  },
  {
    "text": "down here we will upload our turning tests and validation sets to s3 and begin our modeling",
    "start": "1539520",
    "end": "1546320"
  },
  {
    "text": "from here we'll use a couple of additional sagemaker features we will use sagewaker experiments as",
    "start": "1546320",
    "end": "1552080"
  },
  {
    "text": "well as sagemaker automatic model tuning and batch transform so moving on to train",
    "start": "1552080",
    "end": "1558880"
  },
  {
    "text": "we will start with sagemaker experiments by creating a sagemaker experiment called",
    "start": "1558880",
    "end": "1563919"
  },
  {
    "text": "ll for linear learner dash failure dash classification dash create date we'll store the",
    "start": "1563919",
    "end": "1569919"
  },
  {
    "text": "experiment's name for future use and throughout the rest of the notebook we'll create trial components for each",
    "start": "1569919",
    "end": "1575760"
  },
  {
    "text": "of the models and we will evaluate and add them to the experiment so here we have our try accept to see if",
    "start": "1575760",
    "end": "1583520"
  },
  {
    "text": "there is an experiment by that name and if there is go ahead and use that experiment otherwise we'll create a brand new",
    "start": "1583520",
    "end": "1589760"
  },
  {
    "text": "experiment scrolling down by similar logic here we will create the trials if there is a trial that goes by that",
    "start": "1589760",
    "end": "1596799"
  },
  {
    "text": "name then we will use that one otherwise we will create a brand new trial throughout this notebook we will",
    "start": "1596799",
    "end": "1601840"
  },
  {
    "text": "be training multiple models and to start out we will first initialize an estimator for logistic",
    "start": "1601840",
    "end": "1607440"
  },
  {
    "text": "regression there are a number of components that we will need to set up such as the image",
    "start": "1607440",
    "end": "1613600"
  },
  {
    "text": "uri the iam role and the number of instances and training type that we will want to",
    "start": "1613600",
    "end": "1618799"
  },
  {
    "text": "use as well as the output path for stage maker session you can change the configuration or keep the configuration",
    "start": "1618799",
    "end": "1625600"
  },
  {
    "text": "and the hyper parameters the same as they are in this cell currently here there are a",
    "start": "1625600",
    "end": "1630640"
  },
  {
    "text": "number of parameters that we can set as well we will set the predictor type to binary classifier",
    "start": "1630640",
    "end": "1637760"
  },
  {
    "text": "and type of loss to logistic finally when we are fitting the model to",
    "start": "1637760",
    "end": "1643120"
  },
  {
    "text": "the data we will add an experiment configuration that loads the algorithm container loads",
    "start": "1643120",
    "end": "1648559"
  },
  {
    "text": "the data from s3 trains the model outputs the model artifact to the s3 path that we passed in earlier",
    "start": "1648559",
    "end": "1656240"
  },
  {
    "text": "and tears down the training cluster once it's complete one thing to note here is that our inputs are actually going to be the s3",
    "start": "1656240",
    "end": "1663279"
  },
  {
    "text": "paths we created above we will pass in the path to our training data sets as well as the path to our",
    "start": "1663279",
    "end": "1669120"
  },
  {
    "text": "validation data set we do this because linear learner requires the path to s3 as opposed to",
    "start": "1669120",
    "end": "1675039"
  },
  {
    "text": "passing in an array as you would when you are building a model with something like sklearn",
    "start": "1675039",
    "end": "1680399"
  },
  {
    "text": "now i already ran the cell and i trained the model but i received the output logs here in blue",
    "start": "1680399",
    "end": "1686159"
  },
  {
    "text": "you can view these logs and monitor amazon sagemaker using amazon cloudwatch",
    "start": "1686159",
    "end": "1691520"
  },
  {
    "text": "amazon cloudwatch collects raw data and processes it into readable near real-time metrics",
    "start": "1691520",
    "end": "1698000"
  },
  {
    "text": "these statistics are then kept for 15 months so you can access historical information and gain a better",
    "start": "1698000",
    "end": "1703360"
  },
  {
    "text": "perspective on how your web application or service is performing so since i've created multiple models in this notebook",
    "start": "1703360",
    "end": "1710399"
  },
  {
    "text": "i'm now going to skip ahead to the next section so once we've created the several",
    "start": "1710399",
    "end": "1715520"
  },
  {
    "text": "different trials and trained multiple models we can compare and evaluate these models against each other and move on to the",
    "start": "1715520",
    "end": "1722080"
  },
  {
    "text": "hyperparameter tuning for one of the models in addition to the logistic regression model that we walked",
    "start": "1722080",
    "end": "1728480"
  },
  {
    "text": "through this notebook also trains a support vector machines model",
    "start": "1728480",
    "end": "1733520"
  },
  {
    "text": "and we will use that model for our hyperparameter tuning job",
    "start": "1733520",
    "end": "1738799"
  },
  {
    "text": "we will first set up a hyper parameter tuning job and with sagemaker hyper parameter tuning we can set this hyperparameter",
    "start": "1738799",
    "end": "1745600"
  },
  {
    "text": "tuning job for a number of different hyper parameters sagemaker hyperparameter tuning will",
    "start": "1745600",
    "end": "1751039"
  },
  {
    "text": "automatically launch multiple training jobs with different hyper parameter settings evaluate the results of those training",
    "start": "1751039",
    "end": "1757200"
  },
  {
    "text": "jobs based off pre-divine objective metrics and select the hyper parameter settings",
    "start": "1757200",
    "end": "1762399"
  },
  {
    "text": "for future attempts based off the results we will define the maximum number of",
    "start": "1762399",
    "end": "1767760"
  },
  {
    "text": "training jobs as 20 down here and we will also set the number of parallel jobs as",
    "start": "1767760",
    "end": "1773440"
  },
  {
    "text": "two up here we'll create the tuning job name and we'll store that",
    "start": "1773440",
    "end": "1779360"
  },
  {
    "text": "name we will again set the output path and the container",
    "start": "1779360",
    "end": "1785440"
  },
  {
    "text": "and create the tuning job now the hyper parameter tuning job does take quite a",
    "start": "1785440",
    "end": "1790799"
  },
  {
    "text": "bit of time so i'm going to pause here and i will meet you when it's done great so once the hyper parameter tuning",
    "start": "1790799",
    "end": "1797679"
  },
  {
    "text": "job is done we're going to go ahead and associate the hyper parameter tuning job with the sagemaker experiment",
    "start": "1797679",
    "end": "1804399"
  },
  {
    "text": "this is an optional section of the notebook so i won't go into too much detail here but if you're interested in doing",
    "start": "1804399",
    "end": "1809760"
  },
  {
    "text": "this you can go to the notebook and step through each cell from here we'll move on to the prediction portion",
    "start": "1809760",
    "end": "1817360"
  },
  {
    "text": "we're going to get inferences for an entire data set using batch transform with batch transform you",
    "start": "1821200",
    "end": "1826880"
  },
  {
    "text": "create a batch transport job using a trained model from our hyper parameter tuning job above",
    "start": "1826880",
    "end": "1832399"
  },
  {
    "text": "we will use our test data set which is stored in s3 to get predictions on",
    "start": "1832399",
    "end": "1838640"
  },
  {
    "text": "amazon sagemaker saves the inferences in an s3 bucket that you specify when you create the batch transform job",
    "start": "1838640",
    "end": "1845520"
  },
  {
    "text": "batch transform manages all of the compute resources required to get inferences meaning that batch transform handles",
    "start": "1845520",
    "end": "1852399"
  },
  {
    "text": "launching instances and deleting them after the batch transform job has completed without any extra steps from us so",
    "start": "1852399",
    "end": "1859519"
  },
  {
    "text": "starting here first we'll get the best model from our tuner above as well as the batch input",
    "start": "1859519",
    "end": "1864640"
  },
  {
    "text": "which is the path to our test data once we have created our transformer we'll actually get the batch inferences",
    "start": "1864640",
    "end": "1871360"
  },
  {
    "text": "by calling transform and passing in the batch input the content type so here it will be a csv and the split",
    "start": "1871360",
    "end": "1878559"
  },
  {
    "text": "type which will be line so that it's splitting each row of the test data set if we run this cell again we will get",
    "start": "1878559",
    "end": "1885120"
  },
  {
    "text": "logs and we can check those logs in cloudwatch as i explained above and once the batch inference is done it",
    "start": "1885120",
    "end": "1891760"
  },
  {
    "text": "will automatically upload the output s3 next we will evaluate our results by downloading the results from s3",
    "start": "1891760",
    "end": "1899840"
  },
  {
    "text": "the results will come in as a json file and i've created an evaluate model helper function here",
    "start": "1899840",
    "end": "1906159"
  },
  {
    "text": "that will take in the batch file path the tests labels as well as the model name we will parse",
    "start": "1906159",
    "end": "1912480"
  },
  {
    "text": "the json file to get the predicted label outputs and from there we calculate the evaluation metrics such as",
    "start": "1912480",
    "end": "1919279"
  },
  {
    "text": "accuracy recall and precision and f1",
    "start": "1919279",
    "end": "1924480"
  },
  {
    "text": "finally we had all of the different metrics as well as the confusion matrix",
    "start": "1924480",
    "end": "1929919"
  },
  {
    "text": "finally we can go ahead and move on to the clean up section",
    "start": "1932640",
    "end": "1937919"
  },
  {
    "text": "the cleanup is optional but we can go ahead and delete all of our experiments or we can just clean up",
    "start": "1937919",
    "end": "1943279"
  },
  {
    "text": "individual experiments you can go ahead and delete the unused or unneeded",
    "start": "1943279",
    "end": "1948399"
  },
  {
    "text": "experiments or you can keep all of the components up and running just know that there will be a cost",
    "start": "1948399",
    "end": "1954000"
  },
  {
    "text": "associated with keeping the components of this tutorial running there are some additional resources and",
    "start": "1954000",
    "end": "1959600"
  },
  {
    "text": "extensions at the end of this tutorial you could continue to evaluate the hyper",
    "start": "1959600",
    "end": "1966559"
  },
  {
    "text": "parameters and tune them to improve the model you could try more of a tree based model",
    "start": "1966559",
    "end": "1972799"
  },
  {
    "text": "to deal with the class imbalance piece such as sagemaker's built-in algorithm xg boosts",
    "start": "1972799",
    "end": "1978000"
  },
  {
    "text": "which is commonly used for predictive maintenance and finally once you've created a model and are happy with the results",
    "start": "1978000",
    "end": "1984240"
  },
  {
    "text": "you can actually extend this tutorial by deploying the model at the edge using amazon sagemaker neo",
    "start": "1984240",
    "end": "1990159"
  },
  {
    "text": "and aws iot green grass there is an additional example of deploying a",
    "start": "1990159",
    "end": "1995200"
  },
  {
    "text": "predictive maintenance solution at the edge linked in the end of this notebook and there you have it we've explored sagemaker and its",
    "start": "1995200",
    "end": "2001679"
  },
  {
    "text": "features and walked through a demo of vehicle fleet fault classification until next",
    "start": "2001679",
    "end": "2006960"
  },
  {
    "text": "time happy developing",
    "start": "2006960",
    "end": "2014398"
  }
]