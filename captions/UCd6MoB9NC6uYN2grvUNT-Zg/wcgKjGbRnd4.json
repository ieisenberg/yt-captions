[
  {
    "text": "Due to their massive size,",
    "start": "180",
    "end": "1829"
  },
  {
    "text": "foundation models are trained on hardware accelerators,",
    "start": "1830",
    "end": "4980"
  },
  {
    "text": "so you must split workloads across thousands of them",
    "start": "4980",
    "end": "7890"
  },
  {
    "text": "to train foundation models efficiently,",
    "start": "7890",
    "end": "10410"
  },
  {
    "text": "which requires deep data science expertise.",
    "start": "10410",
    "end": "13618"
  },
  {
    "text": "It's also cumbersome and time consuming",
    "start": "13619",
    "end": "16139"
  },
  {
    "text": "to identify, isolate, debug, repair, and recover",
    "start": "16140",
    "end": "20730"
  },
  {
    "text": "from the issues in the training environment.",
    "start": "20730",
    "end": "23910"
  },
  {
    "text": "Amazon's SageMaker Hyper Pod offers infrastructure",
    "start": "23910",
    "end": "27300"
  },
  {
    "text": "optimized for large scale distributed training,",
    "start": "27300",
    "end": "30539"
  },
  {
    "text": "so you can accelerate foundation model training",
    "start": "30540",
    "end": "33360"
  },
  {
    "text": "and enhance resource utilization.",
    "start": "33360",
    "end": "36040"
  },
  {
    "text": "You can train foundation models for months at a time",
    "start": "36040",
    "end": "39570"
  },
  {
    "text": "without disruption",
    "start": "39570",
    "end": "40870"
  },
  {
    "text": "by automatically detecting, diagnosing and recovering",
    "start": "41716",
    "end": "44850"
  },
  {
    "text": "from faulty nodes.",
    "start": "44850",
    "end": "46658"
  },
  {
    "text": "And because it is pre-configured",
    "start": "46659",
    "end": "48750"
  },
  {
    "text": "with Amazon SageMaker's distributed training libraries,",
    "start": "48750",
    "end": "52018"
  },
  {
    "text": "you can automatically split foundation models",
    "start": "52018",
    "end": "54780"
  },
  {
    "text": "and training data sets across GPU instances,",
    "start": "54780",
    "end": "58289"
  },
  {
    "text": "allowing you to improve training performance",
    "start": "58290",
    "end": "60690"
  },
  {
    "text": "while efficiently utilizing",
    "start": "60690",
    "end": "62250"
  },
  {
    "text": "the clusters' compute, memory, and network.",
    "start": "62250",
    "end": "66210"
  },
  {
    "text": "Train Foundation models",
    "start": "66210",
    "end": "67560"
  },
  {
    "text": "with thousands of accelerators faster",
    "start": "67560",
    "end": "70170"
  },
  {
    "text": "with Amazon's SageMaker Hyper Pod.",
    "start": "70170",
    "end": "72842"
  }
]