[
  {
    "text": "so my name is Yamaha jaganathan I'm a specialized Solutions Architect at it",
    "start": "30",
    "end": "5759"
  },
  {
    "text": "AWS on Microsoft platform and along with me I have my name's Robert Sega I'm a",
    "start": "5759",
    "end": "11670"
  },
  {
    "text": "senior software engineer at Thomson Reuters alright so in this session we're going to talk about how Thomson Reuters",
    "start": "11670",
    "end": "17640"
  },
  {
    "text": "used windows contain Amazon ECS to host our windows containers before we get too",
    "start": "17640",
    "end": "24449"
  },
  {
    "text": "far I just want to know how many of you have heard of containers and what are container hosting services we have so",
    "start": "24449",
    "end": "31650"
  },
  {
    "text": "based on that I'll skip a few slides if need be just level 300 okay it's kind of",
    "start": "31650",
    "end": "38520"
  },
  {
    "text": "30 70 ok no ok I'll make it quick because we have a lot to cover you're",
    "start": "38520",
    "end": "43829"
  },
  {
    "text": "getting into the details and stuff so we have two major services which you can",
    "start": "43829",
    "end": "50550"
  },
  {
    "text": "use to host your containers Amazon ECS and Amazon eks easy as this elastic",
    "start": "50550",
    "end": "55920"
  },
  {
    "text": "container service and Amazon eks is elastic container service for kubernetes",
    "start": "55920",
    "end": "62460"
  },
  {
    "text": "so you see as I saw orchestration service that you can use your darker applications and eks is our kubernetes",
    "start": "62460",
    "end": "70610"
  },
  {
    "text": "environment so with ECS you can also host your applications or you can launch your",
    "start": "70610",
    "end": "76040"
  },
  {
    "text": "applications in AWS for gate mode so far gate is basically like serverless for",
    "start": "76040",
    "end": "82470"
  },
  {
    "text": "containers right and with ECS you can",
    "start": "82470",
    "end": "89250"
  },
  {
    "text": "and if use Fargate you can actually deploy your application so that you having to manage and maintain your VMs",
    "start": "89250",
    "end": "96240"
  },
  {
    "text": "which you will use as hosts to how to to run your container applications you",
    "start": "96240",
    "end": "102150"
  },
  {
    "text": "don't have to configure the instance types or network or anything like that and far gate will take care of it for",
    "start": "102150",
    "end": "108420"
  },
  {
    "text": "you and ECS allows you to easily host your applications any kind of",
    "start": "108420",
    "end": "114750"
  },
  {
    "text": "conventional applications long running applications micro services or or it",
    "start": "114750",
    "end": "120000"
  },
  {
    "text": "could be something like machine learning or anything like that so any kind of workload as possible with Fargate you",
    "start": "120000",
    "end": "125610"
  },
  {
    "text": "just said your overhead of managing your underlying infrastructure is taken care of by by AWS and ECS also launches your",
    "start": "125610",
    "end": "134010"
  },
  {
    "text": "dinner's in your own VPC if you want to and you can also secure it with a security group with that your compute is",
    "start": "134010",
    "end": "139950"
  },
  {
    "text": "not being shared with any other customers so you have your complete compute for you you're not sharing your",
    "start": "139950",
    "end": "146069"
  },
  {
    "text": "house to it with another customer you know if it gives you more isolation for",
    "start": "146069",
    "end": "152430"
  },
  {
    "text": "your workloads and also easiest is deeply integrated with other services like I am cloud formation cloud watch",
    "start": "152430",
    "end": "158819"
  },
  {
    "text": "and all other services so that it can that makes it very powerful for you to use and leverage the underlying error",
    "start": "158819",
    "end": "164730"
  },
  {
    "text": "less bad form as well when you use ECS then you have ECS or EK s which is",
    "start": "164730",
    "end": "171329"
  },
  {
    "text": "container service for kubernetes so you don't have a control plane that you need to manage when you used it so kubernetes",
    "start": "171329",
    "end": "177569"
  },
  {
    "text": "has as very it's very tweakable there's a lot of configurations that you can change and modify so we make it easier",
    "start": "177569",
    "end": "184230"
  },
  {
    "text": "for you to host your kubernetes orchestration engine on on on AWS and",
    "start": "184230",
    "end": "190560"
  },
  {
    "text": "also it's secure by default we establish a secure communication channel between",
    "start": "190560",
    "end": "196680"
  },
  {
    "text": "the worker nodes and the control plane nodes by default so you have a highly",
    "start": "196680",
    "end": "201780"
  },
  {
    "text": "secure environment by default and you don't have to work towards creating a second alarm by default if secure given",
    "start": "201780",
    "end": "207870"
  },
  {
    "text": "for you just like any other IW services and also we work with the kubernetes",
    "start": "207870",
    "end": "213480"
  },
  {
    "text": "community closely so with that we are able to leverage what's going on up to date and we also contribute back to the",
    "start": "213480",
    "end": "219209"
  },
  {
    "text": "communities community which helps us and the kubernetes community at the same time it's a win-win scenario for for all",
    "start": "219209",
    "end": "224970"
  },
  {
    "text": "of us and also we are up strand kubernetes upstream so basically means",
    "start": "224970",
    "end": "230609"
  },
  {
    "text": "carbonate is conformant any kubernetes workload will be you'll be able to successfully run on eks then when it",
    "start": "230609",
    "end": "238799"
  },
  {
    "text": "comes to Linux windows containers and how we support on what service and this what it looks like as of today right or",
    "start": "238799",
    "end": "246840"
  },
  {
    "text": "if it's Linux container so you can run them on ECS pretty easy s within the",
    "start": "246840",
    "end": "252030"
  },
  {
    "text": "forget mode or are in eks but when it comes to windows containers or we only",
    "start": "252030",
    "end": "257909"
  },
  {
    "text": "support on it's only support on Amazon ECS we don't windows containers don't have the Fargate launch type support or",
    "start": "257909",
    "end": "264020"
  },
  {
    "text": "you cannot run windows containers on eks that's because version 1.5 kubernetes",
    "start": "264020",
    "end": "271320"
  },
  {
    "text": "started the alpha support for Windows and right now we are version 1.1 to",
    "start": "271320",
    "end": "276560"
  },
  {
    "text": "kubernetes it still is not GA or supported fully it's it's in beta so",
    "start": "276560",
    "end": "283080"
  },
  {
    "text": "none of the cloud providers are supporting yet and we are closely watching and as soon as things get",
    "start": "283080",
    "end": "289020"
  },
  {
    "text": "stable then you will see updates from vice as well right and that's the scenario as of now when it comes to",
    "start": "289020",
    "end": "296490"
  },
  {
    "text": "dotnet applications if it's dartnet core if it's Donette core is definitely the future much faster it's modular it's",
    "start": "296490",
    "end": "303090"
  },
  {
    "text": "more it's it's it's secured in several manner in many ways so it definitely",
    "start": "303090",
    "end": "310590"
  },
  {
    "text": "looks like the future so if you have any new projects you have probably already starting on Dartmouth core which means you have many options you have you can",
    "start": "310590",
    "end": "317190"
  },
  {
    "text": "either run and run on the next containers if you still want for whatever reason to run you're on Windows Server containers then you can still use",
    "start": "317190",
    "end": "323580"
  },
  {
    "text": "that as well also you can turn on down a server containers containers - but if it's",
    "start": "323580",
    "end": "329130"
  },
  {
    "text": "dotnet framework obviously it cannot run on Linux and dot the nano server does",
    "start": "329130",
    "end": "334530"
  },
  {
    "text": "not support data framework as well because it is highly optimized only for a dotnet core and it cannot need only on",
    "start": "334530",
    "end": "340560"
  },
  {
    "text": "64-bit applications so you are left with Windows Server container if you're if you have a dotnet",
    "start": "340560",
    "end": "347760"
  },
  {
    "text": "framework application this is great for lift and shift scenario there are many customers who have legacy lob",
    "start": "347760",
    "end": "352770"
  },
  {
    "text": "applications in our case Thomson Reuters as well so that they have to use Windows",
    "start": "352770",
    "end": "359820"
  },
  {
    "text": "Server that they have to stick with Donnie framework for them they have an option and they can still run their containerize their environment in the",
    "start": "359820",
    "end": "365520"
  },
  {
    "text": "host Ron DCs this shows how much Windows Server container also has",
    "start": "365520",
    "end": "371639"
  },
  {
    "text": "improved since what past three years if you see in 2016 it was about eleven gig",
    "start": "371639",
    "end": "377220"
  },
  {
    "text": "and now it's like less than half of it which is good but it's still not the best it's still five gig that's a lot in",
    "start": "377220",
    "end": "384000"
  },
  {
    "text": "the container world so if you and some of the containers are even like few megabytes like 10 megabytes or even less",
    "start": "384000",
    "end": "390409"
  },
  {
    "text": "so because the concept of container has",
    "start": "390409",
    "end": "395940"
  },
  {
    "text": "been there in the linux ecosystem for a very long time so it's a very natural and it was able to pick up and dark",
    "start": "395940",
    "end": "402270"
  },
  {
    "text": "was designed keeping all that in mind so there's a lot of advantages when it comes to in line Linux containers but",
    "start": "402270",
    "end": "408000"
  },
  {
    "text": "Windows is trying to fit into this mode and they're changing a lot of things",
    "start": "408000",
    "end": "413010"
  },
  {
    "text": "Microsoft changing a lot of things so it will get there at some point but as of now it's around five gig still not as",
    "start": "413010",
    "end": "420450"
  },
  {
    "text": "bad as artists as it used to be three years ago but still windows containers still need more memory and CPU for to",
    "start": "420450",
    "end": "427770"
  },
  {
    "text": "run an identical workload compared to a Linux container right so as of now we",
    "start": "427770",
    "end": "434010"
  },
  {
    "text": "have Windows Server 2016 1803 as host on ECS you can use that and you when you're",
    "start": "434010",
    "end": "442200"
  },
  {
    "text": "running Windows containers to make sure that you can get much you can get some issues if you are running a higher",
    "start": "442200",
    "end": "448050"
  },
  {
    "text": "version of Windows container on a lower version of hosts so one of the best practices to like upgrade your hosts",
    "start": "448050",
    "end": "455010"
  },
  {
    "text": "first before you have greater containers because of the way how Windows works and there's no separation between Windows",
    "start": "455010",
    "end": "461430"
  },
  {
    "text": "and and the container that's running in there so it's still not as modular excellent access and also because",
    "start": "461430",
    "end": "468780"
  },
  {
    "text": "windows containers can only run on Windows so you actually need a Windows host to run Windows containers because",
    "start": "468780",
    "end": "474450"
  },
  {
    "text": "of which you cannot have mixed container environment like Linux and windows containers the same cluster so because",
    "start": "474450",
    "end": "481230"
  },
  {
    "text": "of this you have to have separate clusters on EECS so you will have two clusters let's say in a micro service",
    "start": "481230",
    "end": "486600"
  },
  {
    "text": "environment your all your new applications are dotnet core you'll have that as a separate cluster and your",
    "start": "486600",
    "end": "492660"
  },
  {
    "text": "legacy application that is calling the api's will be hosted in a separate cluster right so you can make sure that",
    "start": "492660",
    "end": "498900"
  },
  {
    "text": "your container is only getting launched on the windows cluster by the by the",
    "start": "498900",
    "end": "504360"
  },
  {
    "text": "placement constraint in the in the container itself and also the easiest",
    "start": "504360",
    "end": "510690"
  },
  {
    "text": "agent unlike Linux it runs on the host and you can actually configure it using",
    "start": "510690",
    "end": "515940"
  },
  {
    "text": "PowerShell and make sure that the host is actually joining the cluster that you want to one the host to join right and",
    "start": "515940",
    "end": "523490"
  },
  {
    "text": "next do not run Windows Update in your container so that's anti-pattern so if",
    "start": "523490",
    "end": "528840"
  },
  {
    "text": "you want to upgrade your container so get a new image from Microsoft Microsoft keeps publishing new images with patches",
    "start": "528840",
    "end": "535320"
  },
  {
    "text": "sometime they're just layers on top of the container image sometimes they replace the base layer itself but that doesn't",
    "start": "535320",
    "end": "542040"
  },
  {
    "text": "really matter so you can all you should always get the newer image and deploy a",
    "start": "542040",
    "end": "547740"
  },
  {
    "text": "layer your application layer on top and publish to ECR and start using it the idea is to treat them as cattle and",
    "start": "547740",
    "end": "554880"
  },
  {
    "text": "outlet pets right and there because of I can mention before because of how the",
    "start": "554880",
    "end": "563040"
  },
  {
    "text": "linux ecosystem has evolved or a couple of decades the network stack is not at",
    "start": "563040",
    "end": "570300"
  },
  {
    "text": "as as varied or strong as a cell in Windows is not as as varied as strong as",
    "start": "570300",
    "end": "577110"
  },
  {
    "text": "Linux ecosystem is because of which we only support Wynette and networking mode now and so you can containers get like",
    "start": "577110",
    "end": "584970"
  },
  {
    "text": "private IP on the host and you can access the container by through the",
    "start": "584970",
    "end": "590490"
  },
  {
    "text": "private private IP but you can do localhost holistic you couldn't do until before 1803 but I think now you can but",
    "start": "590490",
    "end": "598740"
  },
  {
    "text": "there's one like this there are many there are a few constraints that you will hit when you start using Windows containers because of the of the of the",
    "start": "598740",
    "end": "606660"
  },
  {
    "text": "maturity level of the windows container ecosystem and also ECS container is is",
    "start": "606660",
    "end": "612269"
  },
  {
    "text": "open source it's available on github and we encourage PRS if anyone is interested",
    "start": "612269",
    "end": "617970"
  },
  {
    "text": "to contribute right ok so having said that I will hand over my control to",
    "start": "617970",
    "end": "624149"
  },
  {
    "text": "Robert it's going to talk about how they used CCS and other things to host their",
    "start": "624149",
    "end": "629820"
  },
  {
    "text": "legacy Windows applications on a Windows container on Amazon ECS",
    "start": "629820",
    "end": "635839"
  },
  {
    "text": "hi everyone my name is Robert Sigler and I'm a cloud software engineer at Thomson Reuters if you haven't heard of Thomson",
    "start": "646899",
    "end": "653980"
  },
  {
    "text": "Reuters it's a company which provides news information and tools for professionals in the legal Tax and",
    "start": "653980",
    "end": "659860"
  },
  {
    "text": "Accounting compliance government and media markets if you've ever heard of Reuters news that's the media division",
    "start": "659860",
    "end": "666459"
  },
  {
    "text": "of Thomson Reuters at Thomson Reuters I work on the cloud center of excellence our",
    "start": "666459",
    "end": "671670"
  },
  {
    "text": "responsibilities at the company include working with the business units of Thomson Reuters to help them deploy and",
    "start": "671670",
    "end": "677259"
  },
  {
    "text": "run their applications on Amazon Web Services and to drive best practices and enforce policies in the public cloud I'm",
    "start": "677259",
    "end": "686649"
  },
  {
    "text": "here to share my story about working with Amazon Web Services with you all the story of how we wanted to deploy our",
    "start": "686649",
    "end": "693160"
  },
  {
    "text": "dotnet services to Amazon ECS on Windows how we struggled with limitations around",
    "start": "693160",
    "end": "698529"
  },
  {
    "text": "building Windows containers and how we overcame those limitations and our experience is running Windows containers",
    "start": "698529",
    "end": "703959"
  },
  {
    "text": "on Amazon ECS our story begins with a thomson reuters product called on view on view is a Thomson Reuters",
    "start": "703959",
    "end": "715329"
  },
  {
    "text": "unveils Thomson Reuters next generation of tax and accounting software it's a suite of cloud-based products for all",
    "start": "715329",
    "end": "721689"
  },
  {
    "text": "aspects of accounting firm operations behind the scenes on vo consists of many",
    "start": "721689",
    "end": "727329"
  },
  {
    "text": "micro services which are running production workloads in five different regions on Amazon Web Services some of",
    "start": "727329",
    "end": "733449"
  },
  {
    "text": "these micro services run on Linux but others were written to be run on Windows and previously these micro services were",
    "start": "733449",
    "end": "739360"
  },
  {
    "text": "being run on ec2 but we decided we would like to migrate them to Amazon SES here",
    "start": "739360",
    "end": "746589"
  },
  {
    "text": "is a simplified diagram which shows how on BL would be set up after the ECS transformation which I'm about to",
    "start": "746589",
    "end": "752139"
  },
  {
    "text": "discuss I don't expect this diagram to be surprising to anybody who has deployed micro services on Amazon Web",
    "start": "752139",
    "end": "757899"
  },
  {
    "text": "Services before picture here you can see an application load balancer which has been deployed to the public subnets of",
    "start": "757899",
    "end": "764199"
  },
  {
    "text": "our VP see there are many micro services which are running in the private subnets and they are being routed to by routing",
    "start": "764199",
    "end": "770679"
  },
  {
    "text": "rules on the application load balancer the containers are running on Linux and Windows Amazon ECS clusters which are",
    "start": "770679",
    "end": "777009"
  },
  {
    "text": "backed by ec2 instances in Auto scale groups before we go further we should",
    "start": "777009",
    "end": "783819"
  },
  {
    "text": "discuss why we wanted to change anything in the first place one motivating factor for switching was",
    "start": "783819",
    "end": "789039"
  },
  {
    "text": "that we wanted to see if we could speed up our build and deploy process so that we could deliver new features to our",
    "start": "789039",
    "end": "794799"
  },
  {
    "text": "customers faster another reason was that we wanted to simplify our CI CD pipeline",
    "start": "794799",
    "end": "800019"
  },
  {
    "text": "to make it easier to run and maintain so as long as I was working with on vo on this transformation we decided to",
    "start": "800019",
    "end": "806049"
  },
  {
    "text": "revisit CI CD as well like I mentioned a",
    "start": "806049",
    "end": "812619"
  },
  {
    "text": "moment ago one of our goals was to increase our build speed so we looked at switching to containers when running on",
    "start": "812619",
    "end": "818199"
  },
  {
    "text": "ec2 we were building our code and then baking it into an Amazon machine image using the Amazon EBS builder of hosh",
    "start": "818199",
    "end": "824079"
  },
  {
    "text": "core Packer if you are unfamiliar of Packer it's a tool by haché Corp which lets you provision images the Amazon EBS",
    "start": "824079",
    "end": "831039"
  },
  {
    "text": "builder provisions and Amazon machine image by spinning up a new ec2 instance connecting to it via SSH once that",
    "start": "831039",
    "end": "837489"
  },
  {
    "text": "instance becomes available running the steps you define to provision the image stopping the instance taking a snapshot of your EBS",
    "start": "837489",
    "end": "844269"
  },
  {
    "text": "volume and bridge during an image backed by the snapshot bacon images is very useful but sometimes can be a little",
    "start": "844269",
    "end": "850089"
  },
  {
    "text": "slow we theorized that our image building could be faster if we use docker container builds instead because",
    "start": "850089",
    "end": "855939"
  },
  {
    "text": "that would eliminate the need for spinning up new infrastructure and during the build process we also guessed",
    "start": "855939",
    "end": "862389"
  },
  {
    "text": "that our deploys would be faster if we switch to containers when deploying a new container are using the ECC to",
    "start": "862389",
    "end": "867819"
  },
  {
    "text": "launch type it starts on an already provisioned ec2 instance which may be faster than spinning up a new ec2",
    "start": "867819",
    "end": "873309"
  },
  {
    "text": "instance a one feature of on vio is previous deploy strategy was that we",
    "start": "873309",
    "end": "878829"
  },
  {
    "text": "leveraged immutable deployments we baked the micro service code into a new Amazon machine image and deploy that this",
    "start": "878829",
    "end": "885339"
  },
  {
    "text": "ensures that our application can auto scale freely because new instances can be added to the auto scaling group without any manual intervention and",
    "start": "885339",
    "end": "891699"
  },
  {
    "text": "similarly unhealthy instances can be replaced when they fail health checks without any sort of manual intervention",
    "start": "891699",
    "end": "896979"
  },
  {
    "text": "we wanted to continue this practice which using containers would allow us to do naturally because you can build",
    "start": "896979",
    "end": "902439"
  },
  {
    "text": "container images now that we've discussed the reasons why",
    "start": "902439",
    "end": "909510"
  },
  {
    "text": "we chose to use containers let's find out why Amazon ECS was chosen specifically elastic kubernetes service",
    "start": "909510",
    "end": "916200"
  },
  {
    "text": "was not generally available when we started working on this transformation and even if it had been windows support",
    "start": "916200",
    "end": "921600"
  },
  {
    "text": "for kubernetes does not yet generally available so technically the only choice was Amazon ECS however even if there",
    "start": "921600",
    "end": "927930"
  },
  {
    "text": "were other options there are lots of reasons to choose Amazon ECS we were",
    "start": "927930",
    "end": "933180"
  },
  {
    "text": "already using cloth mission as our infrastructure as code solution and cloud mission can be used to manage ECS",
    "start": "933180",
    "end": "938580"
  },
  {
    "text": "resources that was one reason the base Amazon machine image that we baked Oliver code onto for Linux images is",
    "start": "938580",
    "end": "945600"
  },
  {
    "text": "Amazon Linux and Amazon Web Services already provides an Amazon ECS optimized Amazon Linux ami so it was easy for us",
    "start": "945600",
    "end": "952830"
  },
  {
    "text": "to begin consuming that image because it matched the rest of our environments and we were already familiar with Amazon Linux micro-services nan vo were already",
    "start": "952830",
    "end": "961290"
  },
  {
    "text": "using AWS I am instance profiles to grant applications permissions without having to manage credentials and Amazon",
    "start": "961290",
    "end": "968550"
  },
  {
    "text": "ECS integrates well with AWS I am which meant that if we switch to ECS we could continue to use it for managing our",
    "start": "968550",
    "end": "974370"
  },
  {
    "text": "permissions which was great once we decided to use Amazon ECS we had to pick",
    "start": "974370",
    "end": "980880"
  },
  {
    "text": "whether or not we would run it with ec2 launched type or with faregates however Fargate has not yet deployed to all the",
    "start": "980880",
    "end": "987150"
  },
  {
    "text": "regions we need and there's no Fargate support for windows containers yet so we went with ec2",
    "start": "987150",
    "end": "993200"
  },
  {
    "text": "one of the focuses of our transformation was to rework and simplify our CI CD",
    "start": "995810",
    "end": "1001220"
  },
  {
    "text": "process so we decided to use Amazon Web Services code pipeline to deploy these micro services if you've not used it",
    "start": "1001220",
    "end": "1009470"
  },
  {
    "text": "before AWS code pipeline is a TBS has managed CI CD service so users can define",
    "start": "1009470",
    "end": "1014510"
  },
  {
    "text": "pipelines as a series of steps and code pipeline execute each step in sequence someone might define a pipeline which",
    "start": "1014510",
    "end": "1021139"
  },
  {
    "text": "says first appoint a CI then run a test Suites and then deploy to production and a pipeline will stop if any one of these",
    "start": "1021139",
    "end": "1027530"
  },
  {
    "text": "particular steps fails which is meant to prevent ban changes from reaching production so if your tests fail you don't move on next environment a",
    "start": "1027530",
    "end": "1035410"
  },
  {
    "text": "pipeline is triggered by a source action which is used to grab code from a version control repository or in Amazon",
    "start": "1035410",
    "end": "1041449"
  },
  {
    "text": "s3 bucket triggers - of your pipeline typically occur when you push changes to your master branch but that can be",
    "start": "1041449",
    "end": "1046730"
  },
  {
    "text": "configured and supported repository locations include github calm github enterprise bit bucket and code commit",
    "start": "1046730",
    "end": "1053270"
  },
  {
    "text": "and if you don't use one of these options zip archives containing your source code can be dropped into Amazon",
    "start": "1053270",
    "end": "1058760"
  },
  {
    "text": "s3 in order to trigger your pipeline before we go further I'd like to discuss",
    "start": "1058760",
    "end": "1064610"
  },
  {
    "text": "the way we set up our 8 abused accounts at Thomson Reuters 4c ICD first for every project we create a CIC or a",
    "start": "1064610",
    "end": "1071809"
  },
  {
    "text": "pipeline account and that's where CI CD resources like a code pipeline pipeline are created and then we have application",
    "start": "1071809",
    "end": "1079280"
  },
  {
    "text": "accounts and that's where the actual application infrastructure lives so an IM role will be created in the pipeline",
    "start": "1079280",
    "end": "1084530"
  },
  {
    "text": "account for use in pipelines which has a trust relationship to a deployment role in the application account and therefore",
    "start": "1084530",
    "end": "1091690"
  },
  {
    "text": "the pipeline the pipeline could actually assume the deployment role in the application account and from that point",
    "start": "1091690",
    "end": "1097669"
  },
  {
    "text": "on any API calls that are made are made to the application account and new resources are created in that",
    "start": "1097669",
    "end": "1102799"
  },
  {
    "text": "application account so depending on the project at thomson reuters developers may not even be granted write access to",
    "start": "1102799",
    "end": "1108980"
  },
  {
    "text": "the application account at all in this way we can enforce CI CD so the only way for developers to even interact with",
    "start": "1108980",
    "end": "1114860"
  },
  {
    "text": "accounts is through the pipeline so we guarantee eivol code that reaches the application account has to go through certain checks",
    "start": "1114860",
    "end": "1120679"
  },
  {
    "text": "and processes four on vo Amazon ECR repositories were",
    "start": "1120679",
    "end": "1126250"
  },
  {
    "text": "created in both our pipeline and our application accounts our intention was to reduce the blast radius of deleting",
    "start": "1126250",
    "end": "1132220"
  },
  {
    "text": "images so even if an image is deleted from the repository in the pipeline accounts there would be no outage",
    "start": "1132220",
    "end": "1137679"
  },
  {
    "text": "because a copy of that image was persisted and being used in the application accounts however what this",
    "start": "1137679",
    "end": "1142780"
  },
  {
    "text": "meant for us is that our pipeline has to include an action for pushing the image from the pipeline account to the",
    "start": "1142780",
    "end": "1147790"
  },
  {
    "text": "application account before beginning the app deploy so this is a simplified view of what the CIC pipeline we built looks",
    "start": "1147790",
    "end": "1154630"
  },
  {
    "text": "like the pipeline has a source action followed by a container build followed by pushing the image to an amazon ECR",
    "start": "1154630",
    "end": "1160720"
  },
  {
    "text": "repo in the application accounts and then updating the Amazon ECS service to use the new image details that have been",
    "start": "1160720",
    "end": "1166960"
  },
  {
    "text": "left out include code compilation because we didn't make changes to our CI process for this or include that in the",
    "start": "1166960",
    "end": "1172179"
  },
  {
    "text": "pipeline and deploying to multiple environments because as you can see this pipeline only show is deploying the first environment I didn't want to have",
    "start": "1172179",
    "end": "1178690"
  },
  {
    "text": "like another several stages in there we",
    "start": "1178690",
    "end": "1184990"
  },
  {
    "text": "ran into an obstacle though on our path to CI CD for Amazon yes which made it a little more interesting recall that some",
    "start": "1184990",
    "end": "1191169"
  },
  {
    "text": "of the onmyo micro services run on Windows they actually used dotnet framework 4.6 and the services make call",
    "start": "1191169",
    "end": "1197290"
  },
  {
    "text": "to Windows dynamic link libraries which would have to so these micro services would have to be rewritten in order to",
    "start": "1197290",
    "end": "1202960"
  },
  {
    "text": "run on dotnet core so before we went and asked all the application developers to rewrite the apps we asked ourselves how",
    "start": "1202960",
    "end": "1209320"
  },
  {
    "text": "could we maybe build Windows containers instead we were already using a base",
    "start": "1209320",
    "end": "1215110"
  },
  {
    "text": "code built as our build solution for Linux containers so we asked ourselves that we could use code build to build",
    "start": "1215110",
    "end": "1220299"
  },
  {
    "text": "Windows containers as well and Amazon added a to base code build support for Windows shortly after our Amazon ECS",
    "start": "1220299",
    "end": "1227020"
  },
  {
    "text": "migration work began so this sounded promising and we decided to investigate",
    "start": "1227020",
    "end": "1232410"
  },
  {
    "text": "however switching our database code build build type to Windows and running docker build turned out to be",
    "start": "1232410",
    "end": "1238360"
  },
  {
    "text": "unsuccessful docker was not installed on the image already and all attempts to install it and run it from within our",
    "start": "1238360",
    "end": "1244690"
  },
  {
    "text": "build project running Windows proved to be unsuccessful so why didn't it work we wanted to find out behind the scenes",
    "start": "1244690",
    "end": "1252160"
  },
  {
    "text": "code build creates new docker containers for your builds when you run a build we can tell that this is a case because Amazon publishes the dockerfile used for",
    "start": "1252160",
    "end": "1259440"
  },
  {
    "text": "code builds default images and because we are able to push our own docker images to Amazon ECR and use them from",
    "start": "1259440",
    "end": "1265650"
  },
  {
    "text": "code build code build projects running on Windows are no different than their Linux counterparts in that they also run",
    "start": "1265650",
    "end": "1271440"
  },
  {
    "text": "on a container in this case Windows server core containers and the state of the art in 2017 was that there is no",
    "start": "1271440",
    "end": "1277890"
  },
  {
    "text": "support for running docker builds from within a Windows container supporting list would apparently require changes",
    "start": "1277890",
    "end": "1282990"
  },
  {
    "text": "both docker and to the Windows kernel and it didn't look like he was on anyone's roadmap so it seemed hopeless",
    "start": "1282990",
    "end": "1289400"
  },
  {
    "text": "in 2018 there was a brief glimmer of hope as users found out that later versions of Windows may actually allow",
    "start": "1289400",
    "end": "1295770"
  },
  {
    "text": "container to access the hosts docker engine so if you mount the docker engine volume onto your cane container",
    "start": "1295770",
    "end": "1301740"
  },
  {
    "text": "allegedly it can be used from within the container however it doesn't look like this workaround will work for us in this",
    "start": "1301740",
    "end": "1307800"
  },
  {
    "text": "scenario we don't know what code build projects running in Windows we don't know they're running on a Windows 1709",
    "start": "1307800",
    "end": "1313230"
  },
  {
    "text": "or later host and code build windows projects did not allow you to enable the privileged build flag which is actually",
    "start": "1313230",
    "end": "1319800"
  },
  {
    "text": "required to access the hosts docker engine so all of these discoveries led",
    "start": "1319800",
    "end": "1325470"
  },
  {
    "text": "us to believe that code build cannot yet be used to build windows containers some",
    "start": "1325470",
    "end": "1330870"
  },
  {
    "text": "of you might be asking why we didn't run Windows containers builds on Linux and is amaya said earlier the reason is",
    "start": "1330870",
    "end": "1335910"
  },
  {
    "text": "because you can't that you need the container operating system needs to match the hosts Canadian use",
    "start": "1335910",
    "end": "1341730"
  },
  {
    "text": "virtualization and system calls made by a container go directly to the kernel so you can't build a Windows container on a",
    "start": "1341730",
    "end": "1347130"
  },
  {
    "text": "linux host because there's no Windows kernel for you to interact with so for this reason you need to have a Windows",
    "start": "1347130",
    "end": "1352560"
  },
  {
    "text": "host to build all of our discoveries led us to conclude that code build cannot be",
    "start": "1352560",
    "end": "1357720"
  },
  {
    "text": "used to build Windows docker containers yet so we'll just have to build them on an Amazon ec2 instance and said the only",
    "start": "1357720",
    "end": "1363300"
  },
  {
    "text": "question now is how to integrate this container build with the rest of our ADA base code pipeline pipeline now one",
    "start": "1363300",
    "end": "1370020"
  },
  {
    "text": "solution we considered was to use Jenkins Jenkins can run on an ec2 instance running Windows Server 2016 or",
    "start": "1370020",
    "end": "1376350"
  },
  {
    "text": "we could use an ec2 instance we could use ec2 instances running Windows Server 2016 as build notes for a Jenkins master",
    "start": "1376350",
    "end": "1383160"
  },
  {
    "text": "and there exists a a Tobias code pipeline plugin for Jenkins which allows you to add any",
    "start": "1383160",
    "end": "1388700"
  },
  {
    "text": "Jenkins job - your code pipeline so it'll run the job when you're on your pipeline therefore we could use Jenkins",
    "start": "1388700",
    "end": "1394730"
  },
  {
    "text": "as a build executor which already supports code pipeline immigration so that's great however we decided against",
    "start": "1394730",
    "end": "1400820"
  },
  {
    "text": "using Jenkins for this case one of my goals at Thomson Reuters is to drive reusable cloud solutions at Thomson",
    "start": "1400820",
    "end": "1407150"
  },
  {
    "text": "Reuters and we know that other teams are also going to be looking to for this capability building Windows docker",
    "start": "1407150",
    "end": "1412430"
  },
  {
    "text": "containers but they're also looking to get out of the business of maintaining their own Jenkins if they can so one",
    "start": "1412430",
    "end": "1418100"
  },
  {
    "text": "criteria I was looking for in a solution that it would either be software as a service hopefully managed by somebody else or at very least beat cloud native",
    "start": "1418100",
    "end": "1425830"
  },
  {
    "text": "so another solution we considered was running Amazon ec2 instances on demand whenever we need to build a container we",
    "start": "1425830",
    "end": "1432320"
  },
  {
    "text": "could provide a user data script to your run instances API call which would contain all of the steps required for",
    "start": "1432320",
    "end": "1437810"
  },
  {
    "text": "building a container and pushing it Amazon ECR that run instances API call could be made from either a code build",
    "start": "1437810",
    "end": "1443720"
  },
  {
    "text": "project or an ATS lambda function invoked by the pipeline so integrating with code pipeline would actually be",
    "start": "1443720",
    "end": "1449210"
  },
  {
    "text": "pretty easy however we decided against this strategy for speed reasons each build would require starting a new",
    "start": "1449210",
    "end": "1454520"
  },
  {
    "text": "Windows Server 2016 ec2 instance so a speed would probably be comparable to our ami baek's since we're still waiting",
    "start": "1454520",
    "end": "1460880"
  },
  {
    "text": "for infrastructure to be provisioned so",
    "start": "1460880",
    "end": "1466790"
  },
  {
    "text": "the solution that we finally settled on was to build a custom action for a base code pipeline which would run on Amazon",
    "start": "1466790",
    "end": "1472190"
  },
  {
    "text": "ec2 the solution allowed us to run Windows container builds faster than it would take to run new ec2 instances for",
    "start": "1472190",
    "end": "1478190"
  },
  {
    "text": "each build and with less management required than running a jenkins so a",
    "start": "1478190",
    "end": "1485270"
  },
  {
    "text": "custom action is a code pipeline resource which lets you define a custom task that needs to be done in your",
    "start": "1485270",
    "end": "1490430"
  },
  {
    "text": "pipeline custom actions allow you to write code which can run on Amazon ec2 or technically anywhere that can access the",
    "start": "1490430",
    "end": "1497570"
  },
  {
    "text": "code pipeline API to implement a build or deployment task which code pipeline does not yet support the way a custom",
    "start": "1497570",
    "end": "1505730"
  },
  {
    "text": "action works is you write a program which calls the pull for jobs code pipeline API with request parameters",
    "start": "1505730",
    "end": "1511310"
  },
  {
    "text": "which say which custom action you're implementing at a regular interval most of the time the response is going",
    "start": "1511310",
    "end": "1516470"
  },
  {
    "text": "to be an empty array of jobs however if a pipeline is running and the custom action what you care about is currently",
    "start": "1516470",
    "end": "1521750"
  },
  {
    "text": "in progress in that pipeline then the job will appear in the list of jobs that have returned to your tier code your",
    "start": "1521750",
    "end": "1528520"
  },
  {
    "text": "program will then call the acknowledged job API to tell code by plan that you've received a job and that you intends to complete it from there your pipeline",
    "start": "1528520",
    "end": "1536050"
  },
  {
    "text": "will download the input artifacts from s3 that were passed into your custom action if there are any and do whatever",
    "start": "1536050",
    "end": "1541630"
  },
  {
    "text": "work is appropriate when you complete the action you upload your output our fax to s3 if you have any and you call",
    "start": "1541630",
    "end": "1547150"
  },
  {
    "text": "the put job successful results API to complete the job now before you can deploy and use your custom action you",
    "start": "1547150",
    "end": "1553660"
  },
  {
    "text": "first you need to create a custom action type a custom action type is a code pipeline resource which you can create",
    "start": "1553660",
    "end": "1559240"
  },
  {
    "text": "via the API or via a device cloud formation a custom action type is a version interface so it lets you define",
    "start": "1559240",
    "end": "1565720"
  },
  {
    "text": "the configuration properties input artifacts and output artifacts that go into and come out of your custom action",
    "start": "1565720",
    "end": "1571480"
  },
  {
    "text": "so you can think of these as being the parameters and the return type I said",
    "start": "1571480",
    "end": "1577090"
  },
  {
    "text": "that a custom action defines a versioned interface so what I mean by that is once you've defined a custom action type you",
    "start": "1577090",
    "end": "1582820"
  },
  {
    "text": "can't change the configuration properties or the artifacts that it expects instead you need to create a new",
    "start": "1582820",
    "end": "1587920"
  },
  {
    "text": "version of your custom action type so this allows consumers to use a version of your custom action in their pipeline",
    "start": "1587920",
    "end": "1593170"
  },
  {
    "text": "without worrying about the contract breaking so once you've created a provider name and version for your",
    "start": "1593170",
    "end": "1598570"
  },
  {
    "text": "custom action type that provider name and version can never again be used for a new custom action type than that accountant region so this is true even",
    "start": "1598570",
    "end": "1605380"
  },
  {
    "text": "if you delete the custom action type and attempt to recreate it so therefore if you're like me and often find yourself",
    "start": "1605380",
    "end": "1611860"
  },
  {
    "text": "like actively developing something by tearing it down redeploying it you might prefer to use a version which is",
    "start": "1611860",
    "end": "1617230"
  },
  {
    "text": "randomly generated or derived from your git commit sha or something so that you don't accidentally consume a version",
    "start": "1617230",
    "end": "1622750"
  },
  {
    "text": "number that you actually wanted to keep and released like 1.0 point out or something now here's a sample of what it",
    "start": "1622750",
    "end": "1629650"
  },
  {
    "text": "looks like to declare a custom action type and at CloudFormation template so as you can see you can provide configuration properties as well as the",
    "start": "1629650",
    "end": "1636220"
  },
  {
    "text": "number or the range of allowed number of input artifacts and output artifacts as well as a provider name and a version",
    "start": "1636220",
    "end": "1643710"
  },
  {
    "text": "once your custom action is created it can be referred to in a pipeline definition and selected in the code",
    "start": "1643710",
    "end": "1649150"
  },
  {
    "text": "pipeline console when you're editing your pipeline",
    "start": "1649150",
    "end": "1653040"
  },
  {
    "text": "so when you were implementing the code that backs a custom action you need to write it to pull for new jobs agree to",
    "start": "1654600",
    "end": "1660640"
  },
  {
    "text": "run a job execute the required steps and then send the results of your job to code pipeline the the pole for jobs API",
    "start": "1660640",
    "end": "1667690"
  },
  {
    "text": "is the first one they'll call that what you'll call it asking for all the jobs which are currently needed to be run and the API will return a list of jobs that",
    "start": "1667690",
    "end": "1675310"
  },
  {
    "text": "I may choose to acknowledge and execute the pole for jobs API can return zero one or many jobs to acknowledge and it",
    "start": "1675310",
    "end": "1682000"
  },
  {
    "text": "is up to your custom action to decide which jobs to take on so if you know that your particular instance can handle",
    "start": "1682000",
    "end": "1687160"
  },
  {
    "text": "up to three jobs and there are five jobs in the pole for jobs response then you can choose to acknowledge say three of",
    "start": "1687160",
    "end": "1693070"
  },
  {
    "text": "them the other two will continue to be returned by the pole for jobs API until some other custom job acknowledges them",
    "start": "1693070",
    "end": "1699810"
  },
  {
    "text": "so the acknowledged job API takes as input a job ID that your workers claiming and a nonce the job ID and",
    "start": "1699810",
    "end": "1706690"
  },
  {
    "text": "nonce are values which come from pole for jobs and basically this is your way of saying this job is mine and I'm about to execute it if multiple separate",
    "start": "1706690",
    "end": "1713680"
  },
  {
    "text": "workers receive the same job from pole for jobs and all of them attempted to acknowledge it about the same time then",
    "start": "1713680",
    "end": "1719320"
  },
  {
    "text": "ATS will return an error for all but one of the workers so the single worker who did not receive an error knows that",
    "start": "1719320",
    "end": "1725890"
  },
  {
    "text": "their that their the worker that the job truly belongs to and they can execute without fear of race conditions or",
    "start": "1725890",
    "end": "1731650"
  },
  {
    "text": "without duplicating any work so eighty base code pipelines pole I can acknowledge handshake actually",
    "start": "1731650",
    "end": "1737260"
  },
  {
    "text": "guarantees that only one worker has a given job at a time so due to the way",
    "start": "1737260",
    "end": "1742480"
  },
  {
    "text": "these api's are designed and used code pipeline custom action worker is actually really easily made into a cloud",
    "start": "1742480",
    "end": "1747880"
  },
  {
    "text": "native micro service so for example custom workers poling for jobs obviate the need for service discovery think of",
    "start": "1747880",
    "end": "1754420"
  },
  {
    "text": "it this way there's no need for code pipeline to be told where to find your custom workers since the custom workers",
    "start": "1754420",
    "end": "1759700"
  },
  {
    "text": "are the ones that are sending the request code pipeline additionally there's no need to worry about D registering terminated instances and",
    "start": "1759700",
    "end": "1765520"
  },
  {
    "text": "service discovery because they stopped receiving jobs as soon as they stop asking for them so in this way things",
    "start": "1765520",
    "end": "1770620"
  },
  {
    "text": "like Eureka and console they're just they're not needed in a similar vein the job acknowledgement handshake that",
    "start": "1770620",
    "end": "1776650"
  },
  {
    "text": "custom actions use is an alternate strategy for load bouncing so you can scale your custom action worker",
    "start": "1776650",
    "end": "1782140"
  },
  {
    "text": "horizontally across multiple threads or even multiple instances without worrying about to work attempting to execute the same job",
    "start": "1782140",
    "end": "1788260"
  },
  {
    "text": "however you have to be careful that doesn't that doesn't scale forever you can get throttled if you hit the pole",
    "start": "1788260",
    "end": "1793390"
  },
  {
    "text": "for jobs API too frequently so let's say your demand for your custom action is really high and you have a hundred",
    "start": "1793390",
    "end": "1799210"
  },
  {
    "text": "different instances that are all implementing the custom action worker code then you need to probably develop some sort of job queue system to make",
    "start": "1799210",
    "end": "1805870"
  },
  {
    "text": "sure that you're not hitting the API too much the put job tailor result API is",
    "start": "1805870",
    "end": "1811720"
  },
  {
    "text": "used to tell code pipeline that the custom action has failed and is really really important to prevent your custom action from dying without first sending",
    "start": "1811720",
    "end": "1818320"
  },
  {
    "text": "the results to code pipeline so if you were to say throw an exception and your app dies before sending the failure",
    "start": "1818320",
    "end": "1824440"
  },
  {
    "text": "results then your pipeline would be stuck in the in progress state until it times out and a timeout for a custom",
    "start": "1824440",
    "end": "1829990"
  },
  {
    "text": "action is one hour so your pipeline would be inoperable until then so but if your pipeline does get stuck there is a",
    "start": "1829990",
    "end": "1835510"
  },
  {
    "text": "trick you can use to recover it you can either rename the stage in your pipeline that is currently stuck or you can",
    "start": "1835510",
    "end": "1840760"
  },
  {
    "text": "delete the pipeline and recreate it and then you know the that execution will be forgotten and you can continue using",
    "start": "1840760",
    "end": "1846520"
  },
  {
    "text": "your pipeline BPUT job success result API is used to",
    "start": "1846520",
    "end": "1852400"
  },
  {
    "text": "tell code pipeline that the custom action has succeeded so that's what you call when you say I'm done I want the",
    "start": "1852400",
    "end": "1858250"
  },
  {
    "text": "pipeline to move on but the pipeline or the API can also be used to indicate that the execution does not yet complete",
    "start": "1858250",
    "end": "1863440"
  },
  {
    "text": "but our particular worker is done with it for now so when you use the API that way you have to generate a continuation",
    "start": "1863440",
    "end": "1869290"
  },
  {
    "text": "token which contains enough information for your for the next worker to pick up where your worker left off and then you",
    "start": "1869290",
    "end": "1876340"
  },
  {
    "text": "have to provide that to the API if you do that code pipeline will later offer that job to a different worker so we'll",
    "start": "1876340",
    "end": "1885610"
  },
  {
    "text": "we call the custom action that we built to handle our Windows container builds the UN Tanner Iser it's a custom action",
    "start": "1885610",
    "end": "1891070"
  },
  {
    "text": "which runs on a Windows Server 2016 ami that has docker for Windows installed so it can build and run Windows docker",
    "start": "1891070",
    "end": "1897070"
  },
  {
    "text": "containers the custom action worker code itself is implemented in Java and it uses the spring boot framework here's a",
    "start": "1897070",
    "end": "1904660"
  },
  {
    "text": "diagram which shows how the container as it works it is running on Amazon ec2 instances in an auto scaling group and",
    "start": "1904660",
    "end": "1910180"
  },
  {
    "text": "it's frequently looking for new him just to build and what it finds a new job that's going to build that image and push it to Amazon ECR the when Tanner",
    "start": "1910180",
    "end": "1918880"
  },
  {
    "text": "Iser implements to to custom action types the first is the when Tanner eyes are build so that is",
    "start": "1918880",
    "end": "1924559"
  },
  {
    "text": "going to call the poll for jobs API at a regular info in a scheduled task and there's several different threads and a",
    "start": "1924559",
    "end": "1929599"
  },
  {
    "text": "thread pool which are all configured to look for tasks so each instance running the maintainer Iser will find and run several builds concurrently if again",
    "start": "1929599",
    "end": "1936099"
  },
  {
    "text": "when a new job is found we will download the input artifact zip from Amazon s3 and extract that to a temporary",
    "start": "1936099",
    "end": "1942529"
  },
  {
    "text": "directory and that zip is going to contain compiled code binaries and a docker file in order to build we then",
    "start": "1942529",
    "end": "1948889"
  },
  {
    "text": "run docker build a sub process and push the new image that's created to an Amazon ECR repository in our pipeline",
    "start": "1948889",
    "end": "1954919"
  },
  {
    "text": "account and the build output itself is going to it's going to be pushed to Amazon s3 where we can view it later",
    "start": "1954919",
    "end": "1961399"
  },
  {
    "text": "then we're going to remove the temporary directory and the docker image we're going to lose out on some caching by",
    "start": "1961399",
    "end": "1966919"
  },
  {
    "text": "doing that but if we allow for accumulation of all these docker images and layers then eventually we have to be",
    "start": "1966919",
    "end": "1972080"
  },
  {
    "text": "ready for our disk to fill up and our instance to fall over and we would like to avoid that the container eyes are",
    "start": "1972080",
    "end": "1979879"
  },
  {
    "text": "also implements a custom action type call 20 neurites or push and we need that to run image pushes on Amazon ec2",
    "start": "1979879",
    "end": "1986269"
  },
  {
    "text": "as well because the docker push command requires access to the docker engine and therefore it can't be called from code",
    "start": "1986269",
    "end": "1992119"
  },
  {
    "text": "build or lambda for Windows containers we briefly thought about implementing",
    "start": "1992119",
    "end": "1997549"
  },
  {
    "text": "the push action with ad based lambda instead our idea was to maybe re-implement docker push without using",
    "start": "1997549",
    "end": "2003219"
  },
  {
    "text": "the docker engine at all by using the a dubious API instead there are several ECR api's they've been used for",
    "start": "2003219",
    "end": "2009369"
  },
  {
    "text": "uploading layers and registering images to send a docker image to Amazon ECR so we thought about doing that but",
    "start": "2009369",
    "end": "2015099"
  },
  {
    "text": "ultimately we decided against it because those layer managing api's are meant to be used by an Amazon ECR proxy and not",
    "start": "2015099",
    "end": "2021849"
  },
  {
    "text": "necessarily by end-users and besides these lambda function executions are short-lived and therefore you wouldn't",
    "start": "2021849",
    "end": "2027669"
  },
  {
    "text": "be caching layers which are shared between your docker images and caching is really important when you're handling Windows container images which might",
    "start": "2027669",
    "end": "2034389"
  },
  {
    "text": "have somewhere between five and eleven gigabytes of layers which are shared between every single image so now I have",
    "start": "2034389",
    "end": "2042849"
  },
  {
    "text": "a demo prepared for you all it's going to show me updating a micro service which is returning static content and",
    "start": "2042849",
    "end": "2049599"
  },
  {
    "text": "you'll be able to see you get redeployed and let the belt output looks like I'm going to begin it now",
    "start": "2049599",
    "end": "2055169"
  },
  {
    "text": "so right now it says hello reinvented static content being served from a nodejs Express container that's running",
    "start": "2055169",
    "end": "2062530"
  },
  {
    "text": "on Windows it says V zero point six point zero and I'm going to change it to V zero point seven point zero this is",
    "start": "2062530",
    "end": "2068200"
  },
  {
    "text": "what the the project looks like I'm going to modify the or I'm going to show the docker file I'm calling the",
    "start": "2068200",
    "end": "2074800"
  },
  {
    "text": "powershell like OS version command to show you that it really is running on a Windows container and I'm going to copy",
    "start": "2074800",
    "end": "2080260"
  },
  {
    "text": "my index.html file to the location where static content will be served from so",
    "start": "2080260",
    "end": "2086230"
  },
  {
    "text": "I'm going to change the file to say zero point seven point zero I'll save that update the version file and the project",
    "start": "2086230",
    "end": "2094450"
  },
  {
    "text": "as well I'm going to commit these and I'm going to push the commits to get and",
    "start": "2094450",
    "end": "2100060"
  },
  {
    "text": "behind the scenes I actually have a web hook set up which is going to zip all the contents of what I just pushed but",
    "start": "2100060",
    "end": "2105910"
  },
  {
    "text": "it's going to upload it to Amazon s3 so then when I go to code pipeline we're going to see that the the pipeline has",
    "start": "2105910",
    "end": "2111790"
  },
  {
    "text": "been triggered it's currently in the source stage so it's actually copying the artifact to the pipeline bucket",
    "start": "2111790",
    "end": "2117100"
  },
  {
    "text": "right now where it can be read by later stages and after that it's going to run the retainer as a build custom action so",
    "start": "2117100",
    "end": "2124240"
  },
  {
    "text": "we can see it's running right now it'll be done in a little bit and after that I'll show you what the what the build",
    "start": "2124240",
    "end": "2131200"
  },
  {
    "text": "output looks like because that's going to get sent to to Amazon s3 it's",
    "start": "2131200",
    "end": "2136720"
  },
  {
    "text": "actually going to be sent to a static bucket so we can see what it looks like in our browser we're in the push action",
    "start": "2136720",
    "end": "2144100"
  },
  {
    "text": "right now I think after the push is done I show with the what the build output looks like",
    "start": "2144100",
    "end": "2149070"
  },
  {
    "text": "there we go Sneed to be refreshed and there's the built-up hood we can see the Windows server core like version which",
    "start": "2151200",
    "end": "2158339"
  },
  {
    "text": "is the docker build is running on we can see the copy happen and then we can see that it was pushed to an ECR repository",
    "start": "2158339",
    "end": "2164400"
  },
  {
    "text": "with the version that we specified so",
    "start": "2164400",
    "end": "2170579"
  },
  {
    "text": "after that we will go back to the pipeline and we'll see that the the push will probably be done shortly and now the deploy is happening so once",
    "start": "2170579",
    "end": "2179339"
  },
  {
    "text": "once we think the deploy has has gotten to the stage where the app might be ready we're going to go to the URL and",
    "start": "2179339",
    "end": "2185010"
  },
  {
    "text": "we're gonna see that it's actually zero point seven point zero now so we've just",
    "start": "2185010",
    "end": "2190349"
  },
  {
    "text": "redeployed our windows known jazz Express on application using code",
    "start": "2190349",
    "end": "2195390"
  },
  {
    "text": "pipeline I think the last thing which is gonna happen in a minute here it I'm going to go to the pipeline execution",
    "start": "2195390",
    "end": "2201089"
  },
  {
    "text": "history and show you how long it took normally this would take a long time but this took I think three minutes and 50 seconds there was some some editing in",
    "start": "2201089",
    "end": "2208349"
  },
  {
    "text": "the videos because I didn't want to after you watch and spin for for a minute yeah so that's the the wind",
    "start": "2208349",
    "end": "2214740"
  },
  {
    "text": "Tanner Iser demo so now that I've described how we were able to deploy to Amazon SES with a CI CD I wanted to",
    "start": "2214740",
    "end": "2221970"
  },
  {
    "text": "describe how migrating actually had benefited us so one benefit was that we",
    "start": "2221970",
    "end": "2227130"
  },
  {
    "text": "saw the speed of our image built increase when baking Amazon machine images we saw typical ami big steak in",
    "start": "2227130",
    "end": "2232380"
  },
  {
    "text": "at least three minutes with some edge cases taking over 20 minutes and obviously that's dependent on what",
    "start": "2232380",
    "end": "2237720"
  },
  {
    "text": "you're doing in your build but we we saw a room for improvement even the fastest of our bakes so when building a Windows",
    "start": "2237720",
    "end": "2243450"
  },
  {
    "text": "container with the container hazard we saw typical builds completing in about one minute another benefit of moving the",
    "start": "2243450",
    "end": "2250440"
  },
  {
    "text": "Amazon ECS was that our deploys were faster when deploying to Amazon ec2 with an ami our instances typically take a",
    "start": "2250440",
    "end": "2256920"
  },
  {
    "text": "minute or so to start up but when deploying a container east yes with the ec2 launch type our container startup in",
    "start": "2256920",
    "end": "2262500"
  },
  {
    "text": "seconds ultimately the majority of the time spent when deploying is waiting for the application to start and become",
    "start": "2262500",
    "end": "2267839"
  },
  {
    "text": "healthy and that's great because that's something that we can optimize that's something we have control over next I",
    "start": "2267839",
    "end": "2274740"
  },
  {
    "text": "wanted to talk about some of the lessons we learned when working with Windows on Amazon ECS",
    "start": "2274740",
    "end": "2280260"
  },
  {
    "text": "I have a quote here from rich Adams who wrote a very good blog post called a TBS tips I wish before I started you're probably",
    "start": "2280260",
    "end": "2286800"
  },
  {
    "text": "wondering what that has to do with Amazon ECS but I promise I'll explain the connection in a second and the quote says if you have to ssh into your",
    "start": "2286800",
    "end": "2293340"
  },
  {
    "text": "servers then your automation has failed what is meant here is that if you need",
    "start": "2293340",
    "end": "2299430"
  },
  {
    "text": "SSH and tr4 say Amazon ec2 instance in order to fix some problem then that task",
    "start": "2299430",
    "end": "2304650"
  },
  {
    "text": "probably should have been automated and you're not really leveraging all the power that a tigress has to offer and the same is even more true for AM Sony's",
    "start": "2304650",
    "end": "2311160"
  },
  {
    "text": "yes there's no persistent storage for your container unless you mount a volume to it so I need tweaks that you're doing",
    "start": "2311160",
    "end": "2316680"
  },
  {
    "text": "it through a container it would be lost in a redeploy and use treat it like like cattle and not pets and you should automate everything you need to do",
    "start": "2316680",
    "end": "2324260"
  },
  {
    "text": "another idea from that blog post is that association to an instance in order to view logs or metrics doesn't scale it's",
    "start": "2324260",
    "end": "2330810"
  },
  {
    "text": "slow and the ec2 instances that if you're running the cloud a micro service there are ephemeral anyways so how can",
    "start": "2330810",
    "end": "2336630"
  },
  {
    "text": "you read the logs on ec2 instance which was actually terminated 15 minutes go by auto scaling so the solution here to",
    "start": "2336630",
    "end": "2342900"
  },
  {
    "text": "that problem is to send those logs and metrics to a central location instead and if log in MA during are important",
    "start": "2342900",
    "end": "2350130"
  },
  {
    "text": "when you run an application on Amazon ec2 then the same is especially true when running on Amazon ECS after all",
    "start": "2350130",
    "end": "2356010"
  },
  {
    "text": "your containers are now an extra layer away in order to get inside a shell in the container you'd have to SSH and then",
    "start": "2356010",
    "end": "2361860"
  },
  {
    "text": "run docker exec so luckily there are handy integrations with Amazon CloudWatch and Amazon ECS which let you easily see",
    "start": "2361860",
    "end": "2368520"
  },
  {
    "text": "your metrics and logs so Amazon ECS supports Amazon CloudWatch metrics out of the box which lets you see or CPU and",
    "start": "2368520",
    "end": "2375660"
  },
  {
    "text": "memory utilization on your container which is great for troubleshooting your application I've used this before to",
    "start": "2375660",
    "end": "2380850"
  },
  {
    "text": "identify situations when an app was under high load and also to identify situations where I had I had under",
    "start": "2380850",
    "end": "2386220"
  },
  {
    "text": "provisioned my container in the first place and needed to give it more memory for the app to run Amazon ECS allsports",
    "start": "2386220",
    "end": "2394980"
  },
  {
    "text": "Amazon CloudWatch logs which is it's easy to configure automatic Amazon CloudWatch login by defining it in your",
    "start": "2394980",
    "end": "2400770"
  },
  {
    "text": "task definition here's what like a sample of what the clock mason template might look like if you define a task definition which has",
    "start": "2400770",
    "end": "2407580"
  },
  {
    "text": "cloud wach wach login setup all you have to do is you say in-law configuration log driver any bus logs and as long as",
    "start": "2407580",
    "end": "2413460"
  },
  {
    "text": "you've defined your log group as well it more or less just works ma'am son cloud watch logs make it easy",
    "start": "2413460",
    "end": "2420430"
  },
  {
    "text": "to see the logs for containers so these logs are actually gonna be pulled from the output of the docket logs command so",
    "start": "2420430",
    "end": "2426069"
  },
  {
    "text": "they're coming from the standard output of the entry point or CMD command in your docker file instead of from a file",
    "start": "2426069",
    "end": "2431859"
  },
  {
    "text": "in the container or something so because of this there's no need to set up a log a log of shipping infrastructure or add",
    "start": "2431859",
    "end": "2437589"
  },
  {
    "text": "log agents or sidecars to your containers because the ec2 ECS is doing all the work",
    "start": "2437589",
    "end": "2443849"
  },
  {
    "text": "infrastructure as code is also an important practice for managing resources in Amazon yes for example we",
    "start": "2443849",
    "end": "2450430"
  },
  {
    "text": "used a device CloudFormation to manage our Amazon ECS clusters and this allowed us to guarantee that our clusters look",
    "start": "2450430",
    "end": "2455710"
  },
  {
    "text": "the same in each environments and allowed us to reliably and repeatedly roll out changes to the auto-scaling groups backing our Amazon STS clusters",
    "start": "2455710",
    "end": "2463829"
  },
  {
    "text": "additionally we used cloud nation to manage our ECR repositories like I",
    "start": "2463829",
    "end": "2468940"
  },
  {
    "text": "described earlier we created separate ECR repositories for each app in the pipeline account and the application",
    "start": "2468940",
    "end": "2474069"
  },
  {
    "text": "account in order to reduce our blast radius however what we found was that ECR repositories cannot be deleted until",
    "start": "2474069",
    "end": "2480339"
  },
  {
    "text": "all the images inside them are related which made the early stages of designing our cloud recent templates difficult",
    "start": "2480339",
    "end": "2485440"
  },
  {
    "text": "because once we began to use an ECR repository the staff that created it could actually not be deleted until we",
    "start": "2485440",
    "end": "2491049"
  },
  {
    "text": "emptied the repository first and we worked around this by making our ECR repository resources depend on a custom",
    "start": "2491049",
    "end": "2496599"
  },
  {
    "text": "resource the custom resource invokes a lambda function which deletes all the images from that repository and",
    "start": "2496599",
    "end": "2501940"
  },
  {
    "text": "therefore when someone attempts to delete the confirmation stack that the repository has automatically emptied and",
    "start": "2501940",
    "end": "2507430"
  },
  {
    "text": "is allowed to be deleted this was really helpful at the beginning of development stages when I would often refactor in my",
    "start": "2507430",
    "end": "2513849"
  },
  {
    "text": "cloud mansion sax but I would encourage you to disable this or remove it from the template entirely in production I",
    "start": "2513849",
    "end": "2519250"
  },
  {
    "text": "don't want to wipe out all your images so that brings us to the end of our",
    "start": "2519250",
    "end": "2525400"
  },
  {
    "text": "session today we talked about how we were able to deploy our dotnet services to Amazon ACS with Windows how we",
    "start": "2525400",
    "end": "2530710"
  },
  {
    "text": "overcame limitations around windows container builds and how we benefited from running Windows containers on Amazon ECS thanks for listening this",
    "start": "2530710",
    "end": "2539200"
  },
  {
    "text": "slide deck is actually posted online at a later time so don't worry if you missed anything I can take any questions you all may have and we have that a",
    "start": "2539200",
    "end": "2547000"
  },
  {
    "text": "little extra time so hopefully have questions otherwise we'll get some time back what it's even signing up this uh the",
    "start": "2547000",
    "end": "2556930"
  },
  {
    "text": "CIC infrastructure it was it was a while it was at least at least two or three months I'd say and after that it still",
    "start": "2556930",
    "end": "2563050"
  },
  {
    "text": "takes a while sort of like get everybody on board with like this whole DevOps thing and get and get deploying to",
    "start": "2563050",
    "end": "2569560"
  },
  {
    "text": "production for example and not always fast we haven't had anybody had come to",
    "start": "2569560",
    "end": "2578320"
  },
  {
    "text": "us yet and asked for Windows ECS deploys because it is kind of a niche thing you know there's not there's not that many people in this room for example but but",
    "start": "2578320",
    "end": "2587020"
  },
  {
    "text": "we do plan to share that and I even wanted to open source this but I haven't gotten around to doing that yet because",
    "start": "2587020",
    "end": "2592360"
  },
  {
    "text": "there's some there's some bureaucracy to wade through before you can open source something yes",
    "start": "2592360",
    "end": "2600510"
  },
  {
    "text": "right right we did do that and the only reason I chose stuck with nodejs and",
    "start": "2613380",
    "end": "2618479"
  },
  {
    "text": "Express was because it was really easy to set up my I didn't want to have to I'm running on a Mac here and I didn't",
    "start": "2618479",
    "end": "2624180"
  },
  {
    "text": "actually know how to how to compile like certain like dotnet framework like 4.6 for example or asp.net or anything like",
    "start": "2624180",
    "end": "2630390"
  },
  {
    "text": "that so that's why one would be the simplest example possible but there are like real like dotnet framework applications being deployed with this",
    "start": "2630390",
    "end": "2636749"
  },
  {
    "text": "maintainer Iser as well I don't think so",
    "start": "2636749",
    "end": "2645319"
  },
  {
    "text": "so the thing is the actual the actual under your application is dotnet 4.6 so",
    "start": "2645319",
    "end": "2652529"
  },
  {
    "text": "just the demo that Robert showed I was based on node.js Express but the actual",
    "start": "2652529",
    "end": "2659039"
  },
  {
    "text": "application itself is a speed under 4.6 so it's in production",
    "start": "2659039",
    "end": "2664998"
  },
  {
    "text": "now so it's a it's a darker iced container so obviously it is using",
    "start": "2670729",
    "end": "2676049"
  },
  {
    "text": "darker engine",
    "start": "2676049",
    "end": "2678769"
  },
  {
    "text": "yeah so I mean darker it's there's a lot of a lot of parts of darker it's",
    "start": "2692240",
    "end": "2698810"
  },
  {
    "text": "open-source and some part is not like for example darker themselves they have an orchestration platform pulse warm",
    "start": "2698810",
    "end": "2704089"
  },
  {
    "text": "they have another one called darker ee so but the darker core engine itself is",
    "start": "2704089",
    "end": "2709940"
  },
  {
    "text": "open source so amazon ECS make sure that there is backward compatibility for existing customers and also there's no",
    "start": "2709940",
    "end": "2716690"
  },
  {
    "text": "breaking changes so we constantly keep updating the engine that has already put inside the ECS environment yes yeah",
    "start": "2716690",
    "end": "2730540"
  },
  {
    "text": "yes yes that's right this it wasn't like a huge change I guess one of the one of the benefits here was that on view was",
    "start": "2736320",
    "end": "2743910"
  },
  {
    "text": "already doing some of these cloud native 12 factor app like best practices like these apps we're not they didn't have",
    "start": "2743910",
    "end": "2749430"
  },
  {
    "text": "States so they could be Auto scaled and scaled out horizontally etc so it it was",
    "start": "2749430",
    "end": "2755400"
  },
  {
    "text": "kind of uh there was a little bit of a learning curve but it wasn't like it wasn't like wow we had a monolith than",
    "start": "2755400",
    "end": "2760980"
  },
  {
    "text": "we containerized it was we were starting with a cloud native ec2 application and",
    "start": "2760980",
    "end": "2766020"
  },
  {
    "text": "we put it in containers I'm still undone",
    "start": "2766020",
    "end": "2771990"
  },
  {
    "text": "at framework I think there's so there are some dll's which contained some some code which they didn't want to rewrite I",
    "start": "2771990",
    "end": "2777870"
  },
  {
    "text": "think it's that the decision was made because the code is being shared between actually like some client so I'm client",
    "start": "2777870",
    "end": "2783270"
  },
  {
    "text": "software just running on Windows and then on the servers as well so it still",
    "start": "2783270",
    "end": "2788490"
  },
  {
    "text": "taught network",
    "start": "2788490",
    "end": "2791119"
  },
  {
    "text": "looks like we might have run into questions which is okay there are more windows container sessions check out",
    "start": "2795620",
    "end": "2802430"
  },
  {
    "text": "yeah there are some tomorrow there's the one when three one four is actually a builder session you get hands-on",
    "start": "2802430",
    "end": "2809240"
  },
  {
    "text": "experience you can do that or the other one I think one of the top that's a breakout I believe very cool all right",
    "start": "2809240",
    "end": "2816740"
  },
  {
    "text": "thanks guys really appreciate it [Applause]",
    "start": "2816740",
    "end": "2821750"
  }
]