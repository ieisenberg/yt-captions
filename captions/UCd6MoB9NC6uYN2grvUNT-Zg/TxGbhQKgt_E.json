[
  {
    "start": "0",
    "end": "45000"
  },
  {
    "text": "hi my name is Aparna Ellen Govan I'm a Solutions Architect here at Amazon Web",
    "start": "5450",
    "end": "11400"
  },
  {
    "text": "Services specializing in machine learning and we are so glad to have you joining us today for this series of",
    "start": "11400",
    "end": "18330"
  },
  {
    "text": "webinars on ml services offered by AWS this particular session open source ml",
    "start": "18330",
    "end": "25050"
  },
  {
    "text": "frameworks on Amazon sage maker featuring anser flow PI torch MX nipt",
    "start": "25050",
    "end": "31109"
  },
  {
    "text": "Kerris and gluon will show you how to use these frameworks on Amazon site",
    "start": "31109",
    "end": "36270"
  },
  {
    "text": "maker this is a technical session and assumes that you worked with at least",
    "start": "36270",
    "end": "41760"
  },
  {
    "text": "one deep learning framework before amazon says maker provides every",
    "start": "41760",
    "end": "47700"
  },
  {
    "start": "45000",
    "end": "100000"
  },
  {
    "text": "developer and data scientist with the ability to build train and deploy machine learning models quickly Amazon",
    "start": "47700",
    "end": "55980"
  },
  {
    "text": "sage maker is a fully managed service that covers the entire machine learning workflow the label and prepare your data",
    "start": "55980",
    "end": "64110"
  },
  {
    "text": "choose an algorithm train the algorithm tune and optimize it for deployment and",
    "start": "64110",
    "end": "70080"
  },
  {
    "text": "make predictions sage maker has built-in support for machine learning frameworks tensorflow",
    "start": "70080",
    "end": "77610"
  },
  {
    "text": "pi torch chain er MX net and scikit-learn tensorflow is a leading",
    "start": "77610",
    "end": "85619"
  },
  {
    "text": "choice among many frameworks for deep learning workloads and a recent study",
    "start": "85619",
    "end": "90810"
  },
  {
    "text": "found that 88% Afghans slow production workloads running in the cloud are",
    "start": "90810",
    "end": "96960"
  },
  {
    "text": "actually running on AWS when you choose to use Amazon sage maker your machine",
    "start": "96960",
    "end": "103829"
  },
  {
    "text": "learning experiment is automatically tracked making it easier to repeat your experiment as an example attempts to",
    "start": "103829",
    "end": "111840"
  },
  {
    "text": "improve your model can take weeks and sometimes your first model might be the",
    "start": "111840",
    "end": "116909"
  },
  {
    "text": "best one so if you want to replicate a run as of a certain point in time you",
    "start": "116909",
    "end": "123750"
  },
  {
    "text": "would have to figure out exactly which settings were used with sage maker all",
    "start": "123750",
    "end": "129030"
  },
  {
    "text": "the hyper parameters code and data are ethically tracked for you making it much",
    "start": "129030",
    "end": "136770"
  },
  {
    "text": "easier to replicate your experiment sage maker also makes it very easy as",
    "start": "136770",
    "end": "143370"
  },
  {
    "text": "simple as one click to productionize your model into a api available over",
    "start": "143370",
    "end": "149489"
  },
  {
    "text": "HTTP now let's look at how all this happens they can look at sage maker as",
    "start": "149489",
    "end": "157739"
  },
  {
    "start": "155000",
    "end": "322000"
  },
  {
    "text": "having two different faces like the two machine-learning faces the training face",
    "start": "157739",
    "end": "162780"
  },
  {
    "text": "and the inference face during the training phase you provide your source",
    "start": "162780",
    "end": "168959"
  },
  {
    "text": "code your data in Amazon's simple storage service also called s3 a docker",
    "start": "168959",
    "end": "175799"
  },
  {
    "text": "image either built in or you can bring your own and finally the type of ec2",
    "start": "175799",
    "end": "182010"
  },
  {
    "text": "instance you want to use for your training such as a GPU or a CPU based",
    "start": "182010",
    "end": "187200"
  },
  {
    "text": "instance generally speaking it is better to have your training data in s3 as",
    "start": "187200",
    "end": "193860"
  },
  {
    "text": "opposed to the training job directly connecting to a database this is because",
    "start": "193860",
    "end": "199470"
  },
  {
    "text": "if you want your machine learning training job to be repeatable your training data should not change with s3",
    "start": "199470",
    "end": "208140"
  },
  {
    "text": "you can put I am rules and policies in place so that the data is immutable",
    "start": "208140",
    "end": "215420"
  },
  {
    "text": "when you launch a Amazon sage maker training job sage maker automatically provisions the",
    "start": "215420",
    "end": "222750"
  },
  {
    "text": "ec2 instances for you and shuts them down when the training completes so you",
    "start": "222750",
    "end": "228900"
  },
  {
    "text": "only pay for when your training job runs optimizing cost it then pulls the docker",
    "start": "228900",
    "end": "237630"
  },
  {
    "text": "image that you specify downloads your code and data from s3 automatically it",
    "start": "237630",
    "end": "243989"
  },
  {
    "text": "then executes the training job and copies and results and models back from",
    "start": "243989",
    "end": "250410"
  },
  {
    "text": "the container host to s3 one cool thing here is halfway through say you cancel",
    "start": "250410",
    "end": "257370"
  },
  {
    "text": "your job sage maker will automatically copy all the artifacts created so far into s3",
    "start": "257370",
    "end": "264349"
  },
  {
    "text": "this is a useful functionality special as part of training if you've been",
    "start": "264349",
    "end": "269700"
  },
  {
    "text": "snapshotting the best model so far you don't have to worry about losing the mass model if you choose to stop the",
    "start": "269700",
    "end": "276180"
  },
  {
    "text": "training job halfway through it also captures all the log and print",
    "start": "276180",
    "end": "281820"
  },
  {
    "text": "statements that you write to the console in Amazon CloudWatch this allows you to",
    "start": "281820",
    "end": "289680"
  },
  {
    "text": "go back in time and look at past history of how the job ran it also captures the",
    "start": "289680",
    "end": "296160"
  },
  {
    "text": "infrastructure metrics such as CPU GPU memory and disk utilization which become",
    "start": "296160",
    "end": "303990"
  },
  {
    "text": "helpful when you want to optimize the time it takes for your training it can",
    "start": "303990",
    "end": "309510"
  },
  {
    "text": "also track the model performance metric such as mean squared error or any custom",
    "start": "309510",
    "end": "315540"
  },
  {
    "text": "metric you choose next let's look at the inference phase during this phase you",
    "start": "315540",
    "end": "323910"
  },
  {
    "start": "322000",
    "end": "429000"
  },
  {
    "text": "provide the s3 location of where the modeled related artifacts are stored a",
    "start": "323910",
    "end": "330050"
  },
  {
    "text": "darker image either built-in or your urn and the type of instance you want to run",
    "start": "330050",
    "end": "336570"
  },
  {
    "text": "the inference on Sage Maker automatically launches the instance and",
    "start": "336570",
    "end": "341850"
  },
  {
    "text": "runs the docker container it then downloads the model from s3 and with a",
    "start": "341850",
    "end": "348030"
  },
  {
    "text": "single line of code our single click you have your inference endpoint ready this",
    "start": "348030",
    "end": "354780"
  },
  {
    "text": "endpoint can automatically scale based on a auto scaling policy that you",
    "start": "354780",
    "end": "360900"
  },
  {
    "text": "specify for example say you start off with a single instance for inference and",
    "start": "360900",
    "end": "367140"
  },
  {
    "text": "then you apply a policy that says if the number of requests per second is more",
    "start": "367140",
    "end": "372960"
  },
  {
    "text": "than a threshold then increase the number of saving instances to two the",
    "start": "372960",
    "end": "378900"
  },
  {
    "text": "same applies when the number of requests go down and the scaling policy to scale",
    "start": "378900",
    "end": "384000"
  },
  {
    "text": "down kicks in and the number of instances can be reduced to one this",
    "start": "384000",
    "end": "390060"
  },
  {
    "text": "sort of auto scaling policy optimizes cost performance reliability of your",
    "start": "390060",
    "end": "396750"
  },
  {
    "text": "machine learning application just like the training face all the logs and metrics are",
    "start": "396750",
    "end": "403110"
  },
  {
    "text": "automatically captured in cloud watch this allows you to look at information",
    "start": "403110",
    "end": "408750"
  },
  {
    "text": "such as how your model was used and how frequently it was used to produce the",
    "start": "408750",
    "end": "414090"
  },
  {
    "text": "results and model latency now let's look",
    "start": "414090",
    "end": "419250"
  },
  {
    "text": "at a more concrete example on how to use tensor flow and caris with Python 3 on",
    "start": "419250",
    "end": "426270"
  },
  {
    "text": "Saints Maker this example here is a",
    "start": "426270",
    "end": "432330"
  },
  {
    "start": "429000",
    "end": "474000"
  },
  {
    "text": "typical ml code base with a couple of Python files and the dependencies",
    "start": "432330",
    "end": "437910"
  },
  {
    "text": "specified in a requirements file now let's look at the entry file in this",
    "start": "437910",
    "end": "443700"
  },
  {
    "text": "example called the main underscore train dot py the code snippet shows you a",
    "start": "443700",
    "end": "451440"
  },
  {
    "text": "typical parser in Python where you require some parameters such as the",
    "start": "451440",
    "end": "457530"
  },
  {
    "text": "training data location the number of epochs to run your training job now in",
    "start": "457530",
    "end": "464070"
  },
  {
    "text": "order to submit this code as part of sage maker here's a sample of what the",
    "start": "464070",
    "end": "469980"
  },
  {
    "text": "jobs submit code would look like first",
    "start": "469980",
    "end": "474990"
  },
  {
    "text": "when you use tensorflow on sage maker you tell it where the source code is located and what the main",
    "start": "474990",
    "end": "481800"
  },
  {
    "text": "entry file is your source code can be located either in s3 or the local file",
    "start": "481800",
    "end": "489360"
  },
  {
    "text": "system the entry point file is located relative to the source directory now",
    "start": "489360",
    "end": "496800"
  },
  {
    "text": "sage maker knows where to find your code and what the entry is next you have to",
    "start": "496800",
    "end": "502650"
  },
  {
    "text": "tell sage maker where the training and validation data sets are located these",
    "start": "502650",
    "end": "508800"
  },
  {
    "start": "508000",
    "end": "613000"
  },
  {
    "text": "are called channels in sage maker and this is how you would specify that when",
    "start": "508800",
    "end": "515280"
  },
  {
    "text": "you submit your training job you pass in a dictionary that map's keys",
    "start": "515280",
    "end": "520289"
  },
  {
    "text": "to s3 locations the Tamiya code you reference the data downloaded using the",
    "start": "520290",
    "end": "526950"
  },
  {
    "text": "environment variable sm underscore channel underscore key name you used in",
    "start": "526950",
    "end": "534300"
  },
  {
    "text": "this example here your Kurd would find the training data directory from the environment",
    "start": "534300",
    "end": "540570"
  },
  {
    "text": "variable pointed to by SM underscore channel underscore train and the suffix",
    "start": "540570",
    "end": "546720"
  },
  {
    "text": "train matches the key name in the submit job you can call this by any name say",
    "start": "546720",
    "end": "553200"
  },
  {
    "text": "ABC then the dictionary key name would be ABC and you would refer to it by",
    "start": "553200",
    "end": "559530"
  },
  {
    "text": "using the environment variable s sum underscore channel underscore ABC one",
    "start": "559530",
    "end": "566790"
  },
  {
    "text": "thing to note here is that the data is automatically downloaded for you from s3",
    "start": "566790",
    "end": "572220"
  },
  {
    "text": "location that you specify and is placed in the local file system in the pot",
    "start": "572220",
    "end": "578250"
  },
  {
    "text": "pointed to by the environment variable prefixed with FM underscore channel",
    "start": "578250",
    "end": "584150"
  },
  {
    "text": "underscore this means you can use your usual file operations to read and write",
    "start": "584150",
    "end": "591120"
  },
  {
    "text": "data without having s3 specific code and this gives you the flexibility to run",
    "start": "591120",
    "end": "598200"
  },
  {
    "text": "your training on either your laptop or on AWS next you want to specify any",
    "start": "598200",
    "end": "606210"
  },
  {
    "text": "hyper parameters for your training job such as the bat size or the number of",
    "start": "606210",
    "end": "611730"
  },
  {
    "text": "epochs and this is how you do it you pass them as hyper parameters and the",
    "start": "611730",
    "end": "619470"
  },
  {
    "start": "613000",
    "end": "645000"
  },
  {
    "text": "names of the arguments you have in your code match the name of the hyper",
    "start": "619470",
    "end": "624540"
  },
  {
    "text": "parameter dictionary key for example the name epochs here match the name epochs",
    "start": "624540",
    "end": "633000"
  },
  {
    "text": "here next you save your model artifacts in your local file system so that when",
    "start": "633000",
    "end": "640350"
  },
  {
    "text": "the training job completes sage maker automatically uploads them to s3 and",
    "start": "640350",
    "end": "645440"
  },
  {
    "start": "645000",
    "end": "714000"
  },
  {
    "text": "this is how you do it you save your model artifacts in the file system",
    "start": "645440",
    "end": "651230"
  },
  {
    "text": "pointed to by the environment variable sm underscore model underscore dir and",
    "start": "651230",
    "end": "658680"
  },
  {
    "text": "any occurred to submit your job you specify the s3 part to upload your",
    "start": "658680",
    "end": "664620"
  },
  {
    "text": "artifacts to finally you tell saij maker what type of instance to use",
    "start": "664620",
    "end": "671280"
  },
  {
    "text": "for a training job in this example here we are using a mlc for extra-large",
    "start": "671280",
    "end": "678240"
  },
  {
    "text": "instance type and this is it sage maker will now launch an mlc for",
    "start": "678240",
    "end": "684540"
  },
  {
    "text": "extra-large instance pull a hands-off load docker container download the code",
    "start": "684540",
    "end": "690180"
  },
  {
    "text": "and data pointed by the s3 locations you specify execute your code capture all",
    "start": "690180",
    "end": "696480"
  },
  {
    "text": "the logs and metrics and then upload all the output artifacts to s3 and then",
    "start": "696480",
    "end": "702000"
  },
  {
    "text": "eventually shut down the training instance so you're only charged for the duration of your job next let's look at",
    "start": "702000",
    "end": "710430"
  },
  {
    "text": "how to track your model specific metric when you run your training jumps you",
    "start": "710430",
    "end": "716700"
  },
  {
    "start": "714000",
    "end": "746000"
  },
  {
    "text": "want to know how your model performs on a validation set let's say to maker you",
    "start": "716700",
    "end": "723360"
  },
  {
    "text": "can automatically track a model metric such as mean squared error and visualize",
    "start": "723360",
    "end": "729240"
  },
  {
    "text": "it on a dashboard here's how you do it this is a typical Kara skirt that",
    "start": "729240",
    "end": "735060"
  },
  {
    "text": "constructs a model and evaluates it so for saint maker to track the performance",
    "start": "735060",
    "end": "740460"
  },
  {
    "text": "of your model you simply print the results in a specific format in this",
    "start": "740460",
    "end": "746850"
  },
  {
    "start": "746000",
    "end": "816000"
  },
  {
    "text": "example here I am printing the mean squared error the mean absolute error and mean absolute percentage error using",
    "start": "746850",
    "end": "755130"
  },
  {
    "text": "the convention hash hash space followed by validation underscore metric under",
    "start": "755130",
    "end": "762630"
  },
  {
    "text": "scope followed by the metric name and then the score as part of my training",
    "start": "762630",
    "end": "767970"
  },
  {
    "text": "job submission script I specified the regular expression that will extract",
    "start": "767970",
    "end": "773460"
  },
  {
    "text": "this expression from the log files and finally we specify the tensorflow",
    "start": "773460",
    "end": "780270"
  },
  {
    "text": "versions and the Python version you want to use as shown here in this example we",
    "start": "780270",
    "end": "787110"
  },
  {
    "text": "are using Python 3 with Tenzer flow 1.11 and these versions are used to launch",
    "start": "787110",
    "end": "793530"
  },
  {
    "text": "the right sage maker containers to execute your training job now let's",
    "start": "793530",
    "end": "798990"
  },
  {
    "text": "switch to a demo and see how this all fits together this demo is to predict the age of",
    "start": "798990",
    "end": "806340"
  },
  {
    "text": "abalone a type of shellfish based on its physical characteristics such as height",
    "start": "806340",
    "end": "812280"
  },
  {
    "text": "weight and diameter I'm going to walk",
    "start": "812280",
    "end": "818220"
  },
  {
    "start": "816000",
    "end": "956000"
  },
  {
    "text": "through the components of a typical ml code base you have a main entry file",
    "start": "818220",
    "end": "824340"
  },
  {
    "text": "that parses the arguments and starts the training this card shows a general",
    "start": "824340",
    "end": "830640"
  },
  {
    "text": "training process where you define the deep Learning Network the optimization",
    "start": "830640",
    "end": "837390"
  },
  {
    "text": "criteria the code to load the training data fit the model against the training",
    "start": "837390",
    "end": "844890"
  },
  {
    "text": "set evaluated against a validation set and print the results the metrics",
    "start": "844890",
    "end": "855870"
  },
  {
    "text": "printed a mean absolute error mean absolute percentage error and mean",
    "start": "855870",
    "end": "861540"
  },
  {
    "text": "square error the format in which the metrics are printed must be uniquely",
    "start": "861540",
    "end": "866820"
  },
  {
    "text": "extractable by a regular expression you then specify this regular expression",
    "start": "866820",
    "end": "872940"
  },
  {
    "text": "when you submit the job as shown on the previous lights and finally you save the",
    "start": "872940",
    "end": "880260"
  },
  {
    "text": "model you save the model na tends a flow format as opposed to the native Kerr's",
    "start": "880260",
    "end": "885480"
  },
  {
    "text": "format because you can take advantage of the built-in Tanza flow model to serve",
    "start": "885480",
    "end": "891210"
  },
  {
    "text": "your model another thing to note here is where the model is saved the variable",
    "start": "891210",
    "end": "897600"
  },
  {
    "text": "model underscore snapshot dir is an argument passed to the train function if",
    "start": "897600",
    "end": "904530"
  },
  {
    "text": "you look at where that's coming from it comes from the input parser and by",
    "start": "904530",
    "end": "909870"
  },
  {
    "text": "default points to the directory SM underscore model under school d AR the",
    "start": "909870",
    "end": "916920"
  },
  {
    "text": "rest of the current block shows a typical argument pasa in Python to parse",
    "start": "916920",
    "end": "922080"
  },
  {
    "text": "the input argument you would pass through a command line and start the training process next you have the model",
    "start": "922080",
    "end": "933330"
  },
  {
    "text": "exporter that exports a Keres model to attends a flow model",
    "start": "933330",
    "end": "939780"
  },
  {
    "text": "this is pretty much boilerplate code and you would find more details on either Karis or tensorflow",
    "start": "940399",
    "end": "946879"
  },
  {
    "text": "documentation or on Stack Overflow you",
    "start": "946879",
    "end": "952430"
  },
  {
    "text": "also have a requirements file used to specify the dependencies so that your",
    "start": "952430",
    "end": "957620"
  },
  {
    "start": "956000",
    "end": "1002000"
  },
  {
    "text": "code can be consistent no matter where you run and finally you have a setup py",
    "start": "957620",
    "end": "965990"
  },
  {
    "text": "that is required so that your code is smart as a package let's look at the",
    "start": "965990",
    "end": "972439"
  },
  {
    "text": "argument parsing step in detail the code block here parses the input argument and",
    "start": "972439",
    "end": "978740"
  },
  {
    "text": "uses default values to load the data from directories pointed by say to maker",
    "start": "978740",
    "end": "984860"
  },
  {
    "text": "environment variables this is a neat way of having the option to provide alternative inputs and making it sage",
    "start": "984860",
    "end": "992329"
  },
  {
    "text": "maker compatible now let's look at how to deploy the skirt as part of sage",
    "start": "992329",
    "end": "997759"
  },
  {
    "text": "maker you can deploy occurred on Sage Maker using sage maker SDK in python or",
    "start": "997759",
    "end": "1005649"
  },
  {
    "start": "1002000",
    "end": "1054000"
  },
  {
    "text": "as part of a CI CD pipeline for the purpose of a demo I'm going to use a",
    "start": "1005649",
    "end": "1011949"
  },
  {
    "text": "duper notebook to submit training jobs and then deploying an inference endpoint",
    "start": "1011949",
    "end": "1017350"
  },
  {
    "text": "it is a fairly straightforward process to create a notebook instance on sage",
    "start": "1017350",
    "end": "1023709"
  },
  {
    "text": "maker as shown here it does take a few minutes to create a notebook and I'm",
    "start": "1023709",
    "end": "1030880"
  },
  {
    "text": "going to use Jupiter Labs for this demo",
    "start": "1030880",
    "end": "1035819"
  },
  {
    "text": "Jupiter Labs provide a nice convenient way to browse your files and code as a",
    "start": "1039929",
    "end": "1045819"
  },
  {
    "text": "post to a notebook on its own I'm going to use a terminal to pull my code from",
    "start": "1045819",
    "end": "1052210"
  },
  {
    "text": "gifts if you want any files to be persisted between notebook restarts you",
    "start": "1052210",
    "end": "1057850"
  },
  {
    "start": "1054000",
    "end": "1200000"
  },
  {
    "text": "want to make sure their place in the home slash says make a directory I have",
    "start": "1057850",
    "end": "1063010"
  },
  {
    "text": "now cloned my code onto a notebook instance let's have a look at the notebook when you launch a notebook for",
    "start": "1063010",
    "end": "1073510"
  },
  {
    "text": "the first time you would have to select a kernel because I'm demonstrating code that",
    "start": "1073510",
    "end": "1078820"
  },
  {
    "text": "requires standard flow and Python 3 I select the tender flow Python 3 kernel",
    "start": "1078820",
    "end": "1084100"
  },
  {
    "text": "in Jupiter notebooks kernel is a process that interprets and execute your code",
    "start": "1084100",
    "end": "1089610"
  },
  {
    "text": "the kondeh underscore tends the flow under school p36 kernel comes with Python 3 tensorflow",
    "start": "1089610",
    "end": "1097330"
  },
  {
    "text": "and sage maker libraries already installed you can create your own kernel",
    "start": "1097330",
    "end": "1104049"
  },
  {
    "text": "in which case you would have to reinstall the right Python version and any other dependencies your code needs",
    "start": "1104049",
    "end": "1110380"
  },
  {
    "text": "to run first you set up the environment the role here",
    "start": "1110380",
    "end": "1117910"
  },
  {
    "text": "is an important one this is the one that gives sage maker job permission to access resources such as your s3 bucket",
    "start": "1117910",
    "end": "1126220"
  },
  {
    "text": "where your data is located let's quickly explore the data sets the features used",
    "start": "1126220",
    "end": "1136059"
  },
  {
    "text": "are the age of abalone the diameter",
    "start": "1136059",
    "end": "1141669"
  },
  {
    "text": "height and weight etc the label is the age which is what we want to predict",
    "start": "1141669",
    "end": "1148620"
  },
  {
    "text": "next I'm going to upload my code next",
    "start": "1148620",
    "end": "1154540"
  },
  {
    "text": "I'm going to upload my data use for training and validation to s3 this is",
    "start": "1154540",
    "end": "1160030"
  },
  {
    "text": "for a demo ideally in a production scenario you would have a data leak with all the training data set up for ML in",
    "start": "1160030",
    "end": "1167500"
  },
  {
    "text": "s3 the data set has three files for train validation and test I'm listing this",
    "start": "1167500",
    "end": "1174910"
  },
  {
    "text": "contents of source directory just to show you the code files that I showed you on the Python IDE",
    "start": "1174910",
    "end": "1182610"
  },
  {
    "text": "this is how you submit the training job and the curd sample is exactly the same as the one we saw on the slides before",
    "start": "1183320",
    "end": "1190210"
  },
  {
    "text": "these are Python statements and function calls so you can integrate them into",
    "start": "1190210",
    "end": "1195770"
  },
  {
    "text": "your CI CD pipeline without having to use a notebook let's start training now",
    "start": "1195770",
    "end": "1201710"
  },
  {
    "start": "1200000",
    "end": "1262000"
  },
  {
    "text": "if you go back to the stage maker console you can see that the job is starting you can see that it has",
    "start": "1201710",
    "end": "1209720"
  },
  {
    "text": "captured the job status the container or",
    "start": "1209720",
    "end": "1215480"
  },
  {
    "text": "the algorithm in this case which is standard flow the input channel and it's",
    "start": "1215480",
    "end": "1222380"
  },
  {
    "text": "corresponding s3 location the validation",
    "start": "1222380",
    "end": "1230060"
  },
  {
    "text": "channels",
    "start": "1230060",
    "end": "1232480"
  },
  {
    "text": "the metric to capture model performance hyper parameters required to start the",
    "start": "1237720",
    "end": "1245639"
  },
  {
    "text": "job such as the bad size let's see how",
    "start": "1245639",
    "end": "1258419"
  },
  {
    "text": "the job is looking through the jupiter notebook and looks like it's completed",
    "start": "1258419",
    "end": "1263580"
  },
  {
    "text": "successfully the training job has quite a lot of useful logging information for",
    "start": "1263580",
    "end": "1272519"
  },
  {
    "text": "instance it has installed the dependencies in the requirements file it",
    "start": "1272519",
    "end": "1281090"
  },
  {
    "text": "shows how the inputs were used to start your main entry file",
    "start": "1281090",
    "end": "1288169"
  },
  {
    "text": "you can also see the print statements of the validation scores have been logged",
    "start": "1295440",
    "end": "1302510"
  },
  {
    "text": "let's go back to the sage maker console and see what has been tracked",
    "start": "1304430",
    "end": "1312200"
  },
  {
    "text": "the job status is recorded as completed and most other attributes of the",
    "start": "1323000",
    "end": "1329430"
  },
  {
    "text": "training job remain the same",
    "start": "1329430",
    "end": "1332990"
  },
  {
    "text": "let's look at how some of the performance metrics are tracked it shows",
    "start": "1335670",
    "end": "1341970"
  },
  {
    "text": "the infrastructure performance visualizing your instance performance such as CPU and memory utilization you",
    "start": "1341970",
    "end": "1352380"
  },
  {
    "text": "also get to see your model performance and metrics displayed you can see that",
    "start": "1352380",
    "end": "1358500"
  },
  {
    "text": "your model metric you printed with your code such as the mean absolute error mean squared error and mean absolute",
    "start": "1358500",
    "end": "1366000"
  },
  {
    "text": "percentage error is displayed in a graph that's pretty cool if you look at the",
    "start": "1366000",
    "end": "1371490"
  },
  {
    "text": "model output sage makers automatically gzipped the artifacts",
    "start": "1371490",
    "end": "1376590"
  },
  {
    "text": "you saved in the location pointed to by the environment variable as sum underscore model underscore dar to s3",
    "start": "1376590",
    "end": "1384080"
  },
  {
    "start": "1379000",
    "end": "1449000"
  },
  {
    "text": "now let's look at how to turn this model into an API this is just one line here",
    "start": "1384080",
    "end": "1396900"
  },
  {
    "text": "that says deploy and passes the instance count and the type as an argument I've",
    "start": "1396900",
    "end": "1402630"
  },
  {
    "text": "used a single T to medium instance to host my endpoint that's how easy it is",
    "start": "1402630",
    "end": "1410460"
  },
  {
    "text": "to set up your model and make it available over a Hesse TP endpoint as an",
    "start": "1410460",
    "end": "1416580"
  },
  {
    "text": "API now let's invoke it with some sample",
    "start": "1416580",
    "end": "1422280"
  },
  {
    "text": "data the code here simply loads the test CSV into a tensorflow dataset let's",
    "start": "1422280",
    "end": "1429000"
  },
  {
    "text": "print the features of abalone we want to predict the age for in this demo I also",
    "start": "1429000",
    "end": "1436530"
  },
  {
    "text": "have the actual ages so let's print those as well to compare them with the predicted ages the code here invoke the",
    "start": "1436530",
    "end": "1444870"
  },
  {
    "text": "API over HTTP and here are the predictions let's go back to the console",
    "start": "1444870",
    "end": "1451860"
  },
  {
    "start": "1449000",
    "end": "1516000"
  },
  {
    "text": "and see the endpoint the HTTP URL here",
    "start": "1451860",
    "end": "1457140"
  },
  {
    "text": "is a URL your clients will use to invoke the API this endpoint is by default",
    "start": "1457140",
    "end": "1464840"
  },
  {
    "text": "secured using iam policies so you have to give your clients explicit I am",
    "start": "1464840",
    "end": "1471140"
  },
  {
    "text": "permissions to invoke it you can see that it has captured the infrastructure",
    "start": "1471140",
    "end": "1476780"
  },
  {
    "text": "related metrics the endpoint runtime",
    "start": "1476780",
    "end": "1481910"
  },
  {
    "text": "settings here are related to auto scaling policies because this is a sample code I have not applied auto",
    "start": "1481910",
    "end": "1489050"
  },
  {
    "text": "scaling so the initial count of 1 is the same as the desired count the change",
    "start": "1489050",
    "end": "1494510"
  },
  {
    "text": "maker endpoint you can do a/b testing where you route some percentage of the",
    "start": "1494510",
    "end": "1500270"
  },
  {
    "text": "traffic to one model and rest to the other in this demo here this is just one",
    "start": "1500270",
    "end": "1506180"
  },
  {
    "text": "version of the model and hence all traffic is routed to the same model this",
    "start": "1506180",
    "end": "1511820"
  },
  {
    "text": "is the end of the demo and let's switch back to the slides in this section let's",
    "start": "1511820",
    "end": "1518480"
  },
  {
    "text": "look at how to use PI tours with sage maker the method to submit training job",
    "start": "1518480",
    "end": "1526130"
  },
  {
    "text": "is very similar to tensorflow with one minor change instead of specifying",
    "start": "1526130",
    "end": "1531740"
  },
  {
    "text": "tensorflow you specify PI torch to submit the job",
    "start": "1531740",
    "end": "1537310"
  },
  {
    "text": "here you've specified PI torch and similar to tensorflow you specify the",
    "start": "1537310",
    "end": "1542720"
  },
  {
    "text": "source code location and the entry point file just like in tensorflow you specify",
    "start": "1542720",
    "end": "1549710"
  },
  {
    "text": "the training data location s3 and use the key name as a suffix to the environment variable sm underscore",
    "start": "1549710",
    "end": "1557180"
  },
  {
    "text": "channel underscore the load your training data next you specify the hyper",
    "start": "1557180",
    "end": "1563090"
  },
  {
    "text": "parameters next you specify the model part and finally specify the instance",
    "start": "1563090",
    "end": "1570110"
  },
  {
    "text": "type you can see this is very similar to the tensorflow job that you previously saw before I get",
    "start": "1570110",
    "end": "1579440"
  },
  {
    "text": "into the details of how to implement inference on PI torch I want to go through a general inference face further",
    "start": "1579440",
    "end": "1587570"
  },
  {
    "text": "tensorflow demo we implicitly use the tensor flow model server when we call",
    "start": "1587570",
    "end": "1592760"
  },
  {
    "text": "the deploy command for ML frameworks that do not have a model server this is",
    "start": "1592760",
    "end": "1598250"
  },
  {
    "text": "a general sage maker inference flow that shows you how to customize process first a client submits a HTTP",
    "start": "1598250",
    "end": "1608440"
  },
  {
    "text": "request containing the data you want to run the predictions for along with the",
    "start": "1608440",
    "end": "1613570"
  },
  {
    "text": "authorization header token required to invoke the endpoint the HTTP request",
    "start": "1613570",
    "end": "1619539"
  },
  {
    "text": "also contains the content type to specify what the format of the payload",
    "start": "1619539",
    "end": "1624820"
  },
  {
    "text": "is the payload can be a JSON file as shown here it can be a CSV format at one",
    "start": "1624820",
    "end": "1631630"
  },
  {
    "text": "or any other format you choose then the request reaches the sage maker",
    "start": "1631630",
    "end": "1638679"
  },
  {
    "text": "endpoint you can write custom code to parse the request based on the request",
    "start": "1638679",
    "end": "1644500"
  },
  {
    "text": "content type to produce a Python object and the customer input function is",
    "start": "1644500",
    "end": "1650500"
  },
  {
    "text": "passed the request body and the content type to produce a results depending on",
    "start": "1650500",
    "end": "1656740"
  },
  {
    "text": "the framework the model object can also be one of the inputs to this function",
    "start": "1656740",
    "end": "1662789"
  },
  {
    "text": "the model underscore FN is executed once at the start to load the persisted model",
    "start": "1662789",
    "end": "1669370"
  },
  {
    "text": "from disk and deserialize it to produce a Python object required to run",
    "start": "1669370",
    "end": "1674890"
  },
  {
    "text": "predictions once the input is parsed into a Python object the input object is",
    "start": "1674890",
    "end": "1681370"
  },
  {
    "text": "passed to a predict function to run inference the resulting prediction",
    "start": "1681370",
    "end": "1686950"
  },
  {
    "text": "object is then passed to an output function that C realises the Python",
    "start": "1686950",
    "end": "1693130"
  },
  {
    "text": "object depending on the response content based on the accept header in the",
    "start": "1693130",
    "end": "1698710"
  },
  {
    "text": "request please note that the exact details depend on the framework and the",
    "start": "1698710",
    "end": "1705640"
  },
  {
    "text": "version so do check the sage maker documentation you looked at the training",
    "start": "1705640",
    "end": "1713020"
  },
  {
    "text": "face for pie torch a light Tenzer flow model server there are no built-in model",
    "start": "1713020",
    "end": "1719530"
  },
  {
    "text": "servers for pi torch so you have to define a model function model underscore",
    "start": "1719530",
    "end": "1725380"
  },
  {
    "text": "FN that loads and DC realizes the model from the file system the model load",
    "start": "1725380",
    "end": "1732309"
  },
  {
    "text": "function is going to be boilerplate code in mirskiy here's the example make sure the",
    "start": "1732309",
    "end": "1740440"
  },
  {
    "text": "signature of the function matches the one specified here including the return",
    "start": "1740440",
    "end": "1745540"
  },
  {
    "text": "type I also want to show you another nice little trick you can deploy",
    "start": "1745540",
    "end": "1753340"
  },
  {
    "text": "occurred in either an instance that only has a CPU or an instance that has GPUs",
    "start": "1753340",
    "end": "1760270"
  },
  {
    "text": "so you want your code to make use of either the CPU or GPU if that is",
    "start": "1760270",
    "end": "1766420"
  },
  {
    "text": "available this code here shows a PI torch way to check if GPUs available",
    "start": "1766420",
    "end": "1774280"
  },
  {
    "text": "then use the GPU else use the CPU next",
    "start": "1774280",
    "end": "1779410"
  },
  {
    "text": "let's look at some more extensions that sage maker offers in this section here",
    "start": "1779410",
    "end": "1787650"
  },
  {
    "start": "1785000",
    "end": "2029000"
  },
  {
    "text": "we are going to look at MX net on sage maker by now you should be very familiar",
    "start": "1787650",
    "end": "1795910"
  },
  {
    "text": "with the way that you can submit sage maker training jobs MX net on sage maker",
    "start": "1795910",
    "end": "1802570"
  },
  {
    "text": "is no different you just change the name on to MX net to launch MX net on sage",
    "start": "1802570",
    "end": "1810220"
  },
  {
    "text": "maker and everything else follows the same pattern we went through for",
    "start": "1810220",
    "end": "1815860"
  },
  {
    "text": "tensorflow and PI torch MX net comes with a model",
    "start": "1815860",
    "end": "1821890"
  },
  {
    "text": "server so you don't have to write code to load the model if you're only",
    "start": "1821890",
    "end": "1826960"
  },
  {
    "text": "artifact required for inference it's an object of type MX net and then dot",
    "start": "1826960",
    "end": "1833950"
  },
  {
    "text": "module but if you're working in natural language processing you usually have additional artifacts",
    "start": "1833950",
    "end": "1841480"
  },
  {
    "text": "such as the vocabulary that needs to be loaded along with that model in that",
    "start": "1841480",
    "end": "1847420"
  },
  {
    "text": "case you would have to write custom code to load the model this example here",
    "start": "1847420",
    "end": "1853720"
  },
  {
    "text": "shows you how to load a gluon model and as always the function signature needs",
    "start": "1853720",
    "end": "1860980"
  },
  {
    "text": "to match so that your custom code is executed when sage maker calls the",
    "start": "1860980",
    "end": "1866320"
  },
  {
    "text": "function model underscore FN this is a slightly advanced",
    "start": "1866320",
    "end": "1872590"
  },
  {
    "text": "functionality to show code examples for the general say to make an influence",
    "start": "1872590",
    "end": "1877600"
  },
  {
    "text": "frog floor I went through a couple of slides ago if you're use standard",
    "start": "1877600",
    "end": "1883030"
  },
  {
    "text": "formats like Jason in your head sheet appeal request you would normally not have to customize your input requests or",
    "start": "1883030",
    "end": "1890470"
  },
  {
    "text": "output responses but in this example here I'm using a binary like format a",
    "start": "1890470",
    "end": "1896980"
  },
  {
    "text": "pickled version of ndra as my HTTP request content so this",
    "start": "1896980",
    "end": "1902740"
  },
  {
    "text": "custom input function transforms the binary format into a Python MX net and",
    "start": "1902740",
    "end": "1908950"
  },
  {
    "text": "the array object you can do the same for the output formats as well the function",
    "start": "1908950",
    "end": "1916120"
  },
  {
    "text": "signature should match the ones required for your specific version of MX net on sage maker in this session we looked at",
    "start": "1916120",
    "end": "1926470"
  },
  {
    "text": "how to submit training jobs on Sage Maker using Karis tensorflow MX net and",
    "start": "1926470",
    "end": "1932980"
  },
  {
    "text": "PI torch we also looked at how to deploy inference and points on those frameworks",
    "start": "1932980",
    "end": "1938380"
  },
  {
    "text": "in summary amazon says maker offers an alternative to building your own machine",
    "start": "1938380",
    "end": "1944890"
  },
  {
    "text": "learning pipeline and removes the complexity for both developers and data",
    "start": "1944890",
    "end": "1950200"
  },
  {
    "text": "scientists it allows your code to be portable in yet take full advantage of",
    "start": "1950200",
    "end": "1956230"
  },
  {
    "text": "Amazon sage maker functionality such as experiment tracking hyper parameter",
    "start": "1956230",
    "end": "1962410"
  },
  {
    "text": "tuning and allows you to very quickly productionize your model by deploying an",
    "start": "1962410",
    "end": "1967660"
  },
  {
    "text": "endpoint API and make it optimized for cost performance reliability and",
    "start": "1967660",
    "end": "1974140"
  },
  {
    "text": "security if you have any questions please ask the experts and you can learn",
    "start": "1974140",
    "end": "1981730"
  },
  {
    "text": "more about machine learning on aws.amazon.com slush machine - learning",
    "start": "1981730",
    "end": "1989400"
  },
  {
    "text": "you can also access training resources on w WAW s training for machine learning",
    "start": "1989400",
    "end": "1999000"
  },
  {
    "text": "thank you again for attending and we really appreciate your feedback so that we can better",
    "start": "1999000",
    "end": "2004710"
  },
  {
    "text": "understand the topics and services you would like to know more about so please do take the time to fill out our survey",
    "start": "2004710",
    "end": "2011610"
  },
  {
    "text": "and let us know what you think hope you enjoyed this session and it helps you",
    "start": "2011610",
    "end": "2017190"
  },
  {
    "text": "migrate your ml solutions running on laptops to a production quality ml",
    "start": "2017190",
    "end": "2023160"
  },
  {
    "text": "pipeline on Sage Maker happy coding and let's go build",
    "start": "2023160",
    "end": "2029809"
  }
]