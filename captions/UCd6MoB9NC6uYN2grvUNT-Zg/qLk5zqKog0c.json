[
  {
    "start": "0",
    "end": "193000"
  },
  {
    "text": "welcome to session arc 346 scaling to 25 billion daily requests within three months my name is Jack Chavla and I'm",
    "start": "960",
    "end": "8800"
  },
  {
    "text": "chief architect with the weather channel i also have Rahul Frius with me who solution architect for",
    "start": "8800",
    "end": "16160"
  },
  {
    "text": "AWS so what do we expect from this session we are going to be talking about our experience weather channels",
    "start": "17400",
    "end": "23519"
  },
  {
    "text": "experience building a big data distribution platform we'll start out with the goals what the goals were for",
    "start": "23519",
    "end": "28800"
  },
  {
    "text": "those plat that platform then we'll take a deep dive into architecture both leg logical as well as",
    "start": "28800",
    "end": "34520"
  },
  {
    "text": "physical we'll also talk about data supply chain from ingest all the way to the distribution then we'll talk about",
    "start": "34520",
    "end": "41440"
  },
  {
    "text": "our journey how did we scale to those uh 25 billion requests uh per day then I'll",
    "start": "41440",
    "end": "47440"
  },
  {
    "text": "hand it over to Rahul to provide some AWS insights and finally we'll wrap it",
    "start": "47440",
    "end": "52559"
  },
  {
    "text": "up with the evolution of the architecture so before we actually jump into the",
    "start": "52559",
    "end": "59520"
  },
  {
    "text": "platform itself let me give you a background of the weather channel or the weather company itself um the company here uh",
    "start": "59520",
    "end": "67680"
  },
  {
    "text": "the technical team has done an awesome job building a state-of-the-art big data distribution platform",
    "start": "67680",
    "end": "74040"
  },
  {
    "text": "and let's start with the problem statement itself and for that I would like to show you a",
    "start": "74040",
    "end": "80540"
  },
  {
    "text": "[Music]",
    "start": "80540",
    "end": "88600"
  },
  {
    "text": "video stress man you're talking to someone",
    "start": "88600",
    "end": "98360"
  },
  {
    "text": "you got the message you got the message",
    "start": "98960",
    "end": "104118"
  },
  {
    "text": "[Music]",
    "start": "105510",
    "end": "122260"
  },
  {
    "text": "nice video this is awesome video put it",
    "start": "122479",
    "end": "128240"
  },
  {
    "text": "down heat heat [Music]",
    "start": "129239",
    "end": "140239"
  },
  {
    "text": "[Music]",
    "start": "142290",
    "end": "148680"
  },
  {
    "text": "[Music]",
    "start": "150750",
    "end": "161120"
  },
  {
    "text": "heat heat [Music]",
    "start": "161120",
    "end": "173400"
  },
  {
    "text": "wow [Music]",
    "start": "173400",
    "end": "179520"
  },
  {
    "text": "[Music]",
    "start": "181970",
    "end": "192939"
  },
  {
    "start": "193000",
    "end": "250000"
  },
  {
    "text": "wow that's so awesome isn't it imagine the problem statement for a minute what",
    "start": "193879",
    "end": "199760"
  },
  {
    "text": "we are trying to achieve here we are trying to map the atmosphere of the earth not just in two dimensions of",
    "start": "199760",
    "end": "206239"
  },
  {
    "text": "latitude longitude but also third dimension of those 62 miles of elevation",
    "start": "206239",
    "end": "213040"
  },
  {
    "text": "because that happens to be very important for weather and then there is a fourth dimension itself time because",
    "start": "213040",
    "end": "219599"
  },
  {
    "text": "the data that we are dealing with is extremely volatile and it changes very",
    "start": "219599",
    "end": "224760"
  },
  {
    "text": "rapidly imagine the amount of data involved in this effort and then on top of that we",
    "start": "224760",
    "end": "232720"
  },
  {
    "text": "have to apply all the cool cool weather science to it you know forecast models and all those things and finally we have",
    "start": "232720",
    "end": "239920"
  },
  {
    "text": "to take the information out of it and we have to distribute it out via billions and billions of API requests that's the",
    "start": "239920",
    "end": "247360"
  },
  {
    "text": "platform platform we are going to talk about so to summarize we are the weather",
    "start": "247360",
    "end": "252720"
  },
  {
    "start": "250000",
    "end": "318000"
  },
  {
    "text": "company we that's where word comes to gets its weather information and we have",
    "start": "252720",
    "end": "258400"
  },
  {
    "text": "various channels of distribution of course we have the weather channel itself which is the number one most",
    "start": "258400",
    "end": "263600"
  },
  {
    "text": "distributed cable network for weather but then we have the digital properties",
    "start": "263600",
    "end": "269360"
  },
  {
    "text": "weather.com wonderdown wonderground.com 125 million monthly",
    "start": "269360",
    "end": "274720"
  },
  {
    "text": "uniques and counting and then we have some of the top mobile apps on uh iOS",
    "start": "274720",
    "end": "280960"
  },
  {
    "text": "and Android 170 million app downloads and counting all in all we reach 158",
    "start": "280960",
    "end": "288160"
  },
  {
    "text": "million unduplicated consumers but even if you don't use uh these platforms these digital",
    "start": "288160",
    "end": "295840"
  },
  {
    "text": "properties or the apps chances are that you consume our data because we power",
    "start": "295840",
    "end": "301199"
  },
  {
    "text": "weather for Apple Facebook Google Microsoft and quite a few other companies and then we have a B2B",
    "start": "301199",
    "end": "308560"
  },
  {
    "text": "division WSI that has 4,600 plus customers across",
    "start": "308560",
    "end": "314240"
  },
  {
    "text": "60 countries in various industry verticles so how do we do it actually at",
    "start": "314240",
    "end": "321120"
  },
  {
    "start": "318000",
    "end": "410000"
  },
  {
    "text": "our heart we are simply a data company that simply happen to specialize in one",
    "start": "321120",
    "end": "326240"
  },
  {
    "text": "type of data which is weather we are a data company so we start out with",
    "start": "326240",
    "end": "331720"
  },
  {
    "text": "ingesting terabytes and terabytes of data per day from various different sources 800 sources of ingest 20 uh",
    "start": "331720",
    "end": "338639"
  },
  {
    "text": "terabytes data ingested daily data from for example network of weather sensors",
    "start": "338639",
    "end": "344080"
  },
  {
    "text": "100k weather sensors or more global lightning detection network radar location satellites planes and then we",
    "start": "344080",
    "end": "353520"
  },
  {
    "text": "have some of the largest collection of weather data itself observations and forecasts from various agencies",
    "start": "353520",
    "end": "359600"
  },
  {
    "text": "government and non-government we take all this data and then we actually apply",
    "start": "359600",
    "end": "366800"
  },
  {
    "text": "state-of-the- forecast technologies forecast models some of the best industry forecast models that we have we",
    "start": "366800",
    "end": "373759"
  },
  {
    "text": "apply proprietary radar algorithms for visualization we do weather analytics on that all of this backed by more than 200",
    "start": "373759",
    "end": "382720"
  },
  {
    "text": "full-time meteorologists on staff we do human over the loop as",
    "start": "382720",
    "end": "387960"
  },
  {
    "text": "well we also produce a lot of weather related content video images articles",
    "start": "387960",
    "end": "394319"
  },
  {
    "text": "all those in house and then we take all this information that we have produced and then we expose it out to the world",
    "start": "394319",
    "end": "401759"
  },
  {
    "text": "via our weather and content APIs and that's the platform that distributes those 40 billion API requests",
    "start": "401759",
    "end": "410560"
  },
  {
    "start": "410000",
    "end": "447000"
  },
  {
    "text": "daily so what about the data itself all right we have weather data observations",
    "start": "411080",
    "end": "416800"
  },
  {
    "text": "forecast radar alerts notices emergency bulletins we have all that we also have",
    "start": "416800",
    "end": "422400"
  },
  {
    "text": "health and lifestyle data data like pollen uh ski resorts information and all that information and then we talked",
    "start": "422400",
    "end": "429039"
  },
  {
    "text": "about content articles images slideshows videos maps all of that information we",
    "start": "429039",
    "end": "435520"
  },
  {
    "text": "also ingest a lot of domain specific data from industry verticals like aviation energy and insurance so we get",
    "start": "435520",
    "end": "442479"
  },
  {
    "text": "data from those as well so this is a cliche slide uh three",
    "start": "442479",
    "end": "451039"
  },
  {
    "start": "447000",
    "end": "539000"
  },
  {
    "text": "V's but every big data presentation needs one so here it is so we talked about the volume 800 plus",
    "start": "451039",
    "end": "458240"
  },
  {
    "text": "partners 50 GB of raw compressed data per hour that's flowing through our ingest pipeline and then for",
    "start": "458240",
    "end": "464080"
  },
  {
    "text": "distribution several billion requests per day so we got the volume we have that locked but all the data that we saw",
    "start": "464080",
    "end": "471360"
  },
  {
    "text": "on the last slide we get in a variety of different formats we get those in",
    "start": "471360",
    "end": "476479"
  },
  {
    "text": "proprietary open textual structured unstructured binary pictures 2 if you can think a format",
    "start": "476479",
    "end": "484879"
  },
  {
    "text": "chances are that we have it so we got the variety as well and then how does the data arrive",
    "start": "484879",
    "end": "490960"
  },
  {
    "text": "to us we do both push and pull data is either pushed to us or we go and pull it from the sources at varying frequencies",
    "start": "490960",
    "end": "498400"
  },
  {
    "text": "and sometimes as short as every 5 minutes but a lot of our data if you",
    "start": "498400",
    "end": "503919"
  },
  {
    "text": "think about the weather it's very event- driven it's alerts and notifications so as a event happen a severe weather a",
    "start": "503919",
    "end": "511599"
  },
  {
    "text": "flash flood a lightning we get notified we get alerts for those things because",
    "start": "511599",
    "end": "517120"
  },
  {
    "text": "we are dealing with some of the world's most volatile atmospheric data we have",
    "start": "517120",
    "end": "523360"
  },
  {
    "text": "due to the severity of certain events very short time to prepare it sometimes",
    "start": "523360",
    "end": "528560"
  },
  {
    "text": "as brief as 50 to 20 seconds to prepare and sell so we got the velocities velocity as well so we have the three",
    "start": "528560",
    "end": "535279"
  },
  {
    "text": "V's cover and hence big data okay so what about the distribution",
    "start": "535279",
    "end": "542080"
  },
  {
    "start": "539000",
    "end": "577000"
  },
  {
    "text": "the other side of it once the information is prepared and ready to be served all these channels that we talked",
    "start": "542080",
    "end": "548480"
  },
  {
    "text": "about earlier digital properties mobile apps um Apple Google of the world all",
    "start": "548480",
    "end": "554000"
  },
  {
    "text": "these our partnerships and B2B we serve them all via our API platform that's",
    "start": "554000",
    "end": "560800"
  },
  {
    "text": "what we uh initial initially talked about when we actually started building it we were targeting 25 billion API",
    "start": "560800",
    "end": "567600"
  },
  {
    "text": "requests per day now we are more than 40 billion API requests per day and we expected to go to 60 billion per day by",
    "start": "567600",
    "end": "574080"
  },
  {
    "text": "end of year 2015 so how are we doing it this is",
    "start": "574080",
    "end": "581360"
  },
  {
    "start": "577000",
    "end": "637000"
  },
  {
    "text": "before the cloud in the dark ages about 3 years back this is how we were doing it we didn't even have an ondemand",
    "start": "581360",
    "end": "587440"
  },
  {
    "text": "forecast system we just couldn't do it we had a batchbased forecast system that will essentially compute the forecast uh",
    "start": "587440",
    "end": "593920"
  },
  {
    "text": "write it to a database uh and from there it is going to be deplicated out that's how we were doing it we had a Java based",
    "start": "593920",
    "end": "602000"
  },
  {
    "text": "monolithic applications or a set of monolithic applications targeted for things like big web mobile web and data",
    "start": "602000",
    "end": "609519"
  },
  {
    "text": "services as well we were calling those clean factories all backed by homegrown CMS",
    "start": "609519",
    "end": "616480"
  },
  {
    "text": "and run from TWWC data center the net result was the next two",
    "start": "616480",
    "end": "621839"
  },
  {
    "text": "things that you see one we were slow in time to market for any new product and content uh the time was measured in",
    "start": "621839",
    "end": "630279"
  },
  {
    "text": "months and we were also limited in terms of distributed scaling due to limitation of our data centers",
    "start": "630279",
    "end": "636959"
  },
  {
    "text": "so we set out to fix all that and reboot and reimagine our entire technical",
    "start": "636959",
    "end": "642519"
  },
  {
    "start": "637000",
    "end": "785000"
  },
  {
    "text": "stack and what were our goals there from a business perspective business wanted",
    "start": "642519",
    "end": "648480"
  },
  {
    "text": "an extremely low latency ondemand forecast system no more batch",
    "start": "648480",
    "end": "653839"
  },
  {
    "text": "forecasting so that was the key an ondemand forecasting system then we wanted to take all that",
    "start": "653839",
    "end": "660160"
  },
  {
    "text": "data and we wanted to distribute it out to billions and billions of requests that will come our way so highly",
    "start": "660160",
    "end": "666320"
  },
  {
    "text": "scalable distribution platform then we wanted to reboot our entire digital stack our digital",
    "start": "666320",
    "end": "672959"
  },
  {
    "text": "properties weather.com mobile apps and CMS and all those to actually leverage",
    "start": "672959",
    "end": "678720"
  },
  {
    "text": "this entire stack that we were going to build main objective there reduce time",
    "start": "678720",
    "end": "684079"
  },
  {
    "text": "to deploy the new data sets from months to weeks and we also started to see that",
    "start": "684079",
    "end": "691279"
  },
  {
    "text": "data distribution platform itself as product so we wanted to expose the exact",
    "start": "691279",
    "end": "696720"
  },
  {
    "text": "same APIs that we were going to use to our partners for getting the weather",
    "start": "696720",
    "end": "702800"
  },
  {
    "text": "weather data which mean we needed a secured and metered access to those",
    "start": "702800",
    "end": "707959"
  },
  {
    "text": "APIs and while doing all this the requirement was also to consol consolidate all the data",
    "start": "707959",
    "end": "714440"
  },
  {
    "text": "centers so mapping those business goals to technical strategy this is what we came up with we wanted a 100%",
    "start": "714440",
    "end": "720959"
  },
  {
    "text": "cloud-based solution that's capable of in ingesting proc and processing those",
    "start": "720959",
    "end": "726399"
  },
  {
    "text": "terabytes of data but also handling billions of requests per day so both",
    "start": "726399",
    "end": "731839"
  },
  {
    "text": "ingest as well as distribution we were targeting extremely low latency APIs 25",
    "start": "731839",
    "end": "738079"
  },
  {
    "text": "to 100 millisecond early in the game somebody threw up somebody actually came",
    "start": "738079",
    "end": "743680"
  },
  {
    "text": "up with this number 25 millisecond and that got stuck um we were also of course looking for a",
    "start": "743680",
    "end": "750240"
  },
  {
    "text": "highly scalable and a highly available system 49s that's what we were targeting",
    "start": "750240",
    "end": "755279"
  },
  {
    "text": "in order for us to reduce time to deploy new data sets we needed a very sophisticated generic data processing",
    "start": "755279",
    "end": "761920"
  },
  {
    "text": "engine and you will see that that's the foundation of our stack so we were targeting generic data processing engine",
    "start": "761920",
    "end": "768680"
  },
  {
    "text": "DPE of course for APIs we were targeting developer friendly APIs and all back",
    "start": "768680",
    "end": "775200"
  },
  {
    "text": "with authentication metering and throttling so how did we do it what was our our",
    "start": "775200",
    "end": "781120"
  },
  {
    "text": "architectural blueprint what was the plan looks something like this like I",
    "start": "781120",
    "end": "787839"
  },
  {
    "start": "785000",
    "end": "960000"
  },
  {
    "text": "mentioned earlier at the bottom the foundation of this entire stack is this data processing",
    "start": "787839",
    "end": "794040"
  },
  {
    "text": "engine then the next layer on top of this is the storage layer and also",
    "start": "794040",
    "end": "799279"
  },
  {
    "text": "sitting on right systems of record the reason being sometimes you cannot just",
    "start": "799279",
    "end": "805200"
  },
  {
    "text": "store that there are always going to be systems within your technical stack that are really good at doing its job and",
    "start": "805200",
    "end": "812639"
  },
  {
    "text": "doing its job very well example in our case is forecast on demand system it has a lot and lot of",
    "start": "812639",
    "end": "821120"
  },
  {
    "text": "meteorological science weather weather science in it and that's what it does it",
    "start": "821120",
    "end": "826320"
  },
  {
    "text": "does its job really well it's a C++based in-house custom system that takes a lot",
    "start": "826320",
    "end": "832079"
  },
  {
    "text": "of data and actually it produces a forecast out of it so that would be an example of system of record so sometimes",
    "start": "832079",
    "end": "838720"
  },
  {
    "text": "data processing engine needs to take all that data and gives it to those systems of record and then they will produce",
    "start": "838720",
    "end": "845040"
  },
  {
    "text": "their end result from there layered on top of it is data",
    "start": "845040",
    "end": "850160"
  },
  {
    "text": "services so every API request comes through that data services never hits",
    "start": "850160",
    "end": "855760"
  },
  {
    "text": "those systems of record directly that's going to be responsible for distribution",
    "start": "855760",
    "end": "861920"
  },
  {
    "text": "it's going to be responsible for scaling it's going to be responsible for caching it's going to be responsible for everything everything from a",
    "start": "861920",
    "end": "868480"
  },
  {
    "text": "distribution perspective gateway or API gateway is responsible for routing metering",
    "start": "868480",
    "end": "875839"
  },
  {
    "text": "throttling those kind of things and of course we need CDN from an edge perspective caching as well as",
    "start": "875839",
    "end": "882279"
  },
  {
    "text": "compute so why did we design it this way we knew it was an extremely large",
    "start": "882279",
    "end": "887959"
  },
  {
    "text": "undertaking so we have to do a divide and conquer approach we designed our layers decoupled or",
    "start": "887959",
    "end": "895360"
  },
  {
    "text": "loosely coupled so that different teams who are focusing on developing these",
    "start": "895360",
    "end": "901040"
  },
  {
    "text": "layers can do their job and do do their job really well not only",
    "start": "901040",
    "end": "906519"
  },
  {
    "text": "that they can work on an independent delivery timeline but they can also pick",
    "start": "906519",
    "end": "912399"
  },
  {
    "text": "best tools and technologies for the job so I gave an example of forecast on demand system it's a C++ system but",
    "start": "912399",
    "end": "919600"
  },
  {
    "text": "another system of record for example is Drupal and that's a PHP based system uh",
    "start": "919600",
    "end": "925040"
  },
  {
    "text": "data processing engine is written in Java data services are written in Java and Scala both so we wanted to give",
    "start": "925040",
    "end": "931440"
  },
  {
    "text": "these teams flexibility to pick their the best technology that is suitable for",
    "start": "931440",
    "end": "936480"
  },
  {
    "text": "what they were doing the only thing that we did between these layers is we standardized on restful",
    "start": "936480",
    "end": "944760"
  },
  {
    "text": "APIs over HTTP as our interface essentially we were building",
    "start": "944760",
    "end": "951920"
  },
  {
    "text": "this data data platform which just happens to serve weather data as",
    "start": "951920",
    "end": "957839"
  },
  {
    "start": "960000",
    "end": "1018000"
  },
  {
    "text": "service okay so let's jump into this starting with the foundation itself DPE",
    "start": "960360",
    "end": "966160"
  },
  {
    "text": "we were targeting a generic DPE that's APIdriven and data agnostic and that's",
    "start": "966160",
    "end": "971360"
  },
  {
    "text": "very important because we have variety of data different types of data coming in from all these various sources of",
    "start": "971360",
    "end": "978000"
  },
  {
    "text": "ingest and we want to reduce the time to market for those the new new data sets",
    "start": "978000",
    "end": "984160"
  },
  {
    "text": "so these two points are important however different data sets do require",
    "start": "984160",
    "end": "989279"
  },
  {
    "text": "custom processing and may require different way of storing and archiving so this s this DPE needs to be",
    "start": "989279",
    "end": "995279"
  },
  {
    "text": "extensible you need to be able to provide that custom code for those things it needs to be always on always",
    "start": "995279",
    "end": "1002480"
  },
  {
    "text": "flowing asynchronous and non-blocking mostly it's eventdriven and an extremely",
    "start": "1002480",
    "end": "1007759"
  },
  {
    "text": "volatile data very fastmoving data so those are the key requirements and then we were targeting high availability low",
    "start": "1007759",
    "end": "1015759"
  },
  {
    "text": "latency and horizontal scalability so how does the architecture",
    "start": "1015759",
    "end": "1021120"
  },
  {
    "start": "1018000",
    "end": "1160000"
  },
  {
    "text": "itself looks like looks something like this so we standardize for data coming",
    "start": "1021120",
    "end": "1027120"
  },
  {
    "text": "in on an injection API or I APIs as a restful web service that web service is",
    "start": "1027120",
    "end": "1033918"
  },
  {
    "text": "a very simple web service where providers are going to post the data or we go and pull the data and then post it",
    "start": "1033919",
    "end": "1039120"
  },
  {
    "text": "there however we do it it's going to be very simple it takes the data and it writes it to a rabbit",
    "start": "1039120",
    "end": "1046160"
  },
  {
    "text": "MQ2 cluster and then returns an HTT HTTP 200",
    "start": "1046160",
    "end": "1052080"
  },
  {
    "text": "okay back to the providers so its job is done data processing engines or",
    "start": "1052080",
    "end": "1059039"
  },
  {
    "text": "different DPE based on the configuration are the workers that are reading those",
    "start": "1059039",
    "end": "1064960"
  },
  {
    "text": "cues they are popping the data from the cube and then they are doing their job they are processing the data storing the",
    "start": "1064960",
    "end": "1071360"
  },
  {
    "text": "data archiving the data they could be actually posting it to another Rabbit MQ cluster a partner exchange or they could",
    "start": "1071360",
    "end": "1079120"
  },
  {
    "text": "hand it over to system of record for processing of that data what about the architecture of DPE",
    "start": "1079120",
    "end": "1087400"
  },
  {
    "text": "itself there are two major components to that dp core and these plugins dp core",
    "start": "1087400",
    "end": "1094919"
  },
  {
    "text": "is the only dependency when you are writing a when you are actually deploying a particular DPE and it's a",
    "start": "1094919",
    "end": "1101640"
  },
  {
    "text": "configurationdriven think of it like a job orchestrator it's responsible is to",
    "start": "1101640",
    "end": "1106799"
  },
  {
    "text": "invoking the plugins these plugins plug-in 1 2 3 in sequence based on the",
    "start": "1106799",
    "end": "1112760"
  },
  {
    "text": "configuration plugins are essentially POJO where you provide your custom code",
    "start": "1112760",
    "end": "1118559"
  },
  {
    "text": "for process download store and archive so essentially DP comes in it reads the",
    "start": "1118559",
    "end": "1124400"
  },
  {
    "text": "data from the queue invokes plug plug-in one which might go and download additional information data coming in",
    "start": "1124400",
    "end": "1131200"
  },
  {
    "text": "could be just metadata itself for example then plug-in two it invokes that and that plug-in two actually processes",
    "start": "1131200",
    "end": "1137760"
  },
  {
    "text": "that data plug-in three could be writing it to radius plug-in four which is not",
    "start": "1137760",
    "end": "1144720"
  },
  {
    "text": "here could be actually archiving it to S3 and we wrote it in Java",
    "start": "1144720",
    "end": "1150200"
  },
  {
    "text": "1.7 and this is actually running on Amazon Linux we are going to talk about",
    "start": "1150200",
    "end": "1155840"
  },
  {
    "text": "storage and archiving and uh those things in a minute okay so from a AWS perspective",
    "start": "1155840",
    "end": "1163039"
  },
  {
    "start": "1160000",
    "end": "1214000"
  },
  {
    "text": "how does that look like essentially this is a single region view availability zones of",
    "start": "1163039",
    "end": "1169160"
  },
  {
    "text": "course data comes in from publisher it ends up in on an API endpoint which is",
    "start": "1169160",
    "end": "1176000"
  },
  {
    "text": "essentially a bunch of servers running API code uh behind any and one of those",
    "start": "1176000",
    "end": "1182240"
  },
  {
    "text": "picks up and writes it to a rabbit MQ cluster from rabbit MQ cluster there is binding",
    "start": "1182240",
    "end": "1188880"
  },
  {
    "text": "in the cluster itself which based on the binding puts it in appropriate queue and then you have different DPEs working",
    "start": "1188880",
    "end": "1196960"
  },
  {
    "text": "acting as workers for that queue which are actually taking the data reading the",
    "start": "1196960",
    "end": "1202000"
  },
  {
    "text": "data and processing the data based on their injection pipeline and they could be writing it to",
    "start": "1202000",
    "end": "1208480"
  },
  {
    "text": "these stores underneath",
    "start": "1208480",
    "end": "1212120"
  },
  {
    "start": "1214000",
    "end": "1259000"
  },
  {
    "text": "okay so what about the storage itself so we got the data and now we need to write it somewhere we went with an architecture",
    "start": "1214960",
    "end": "1222720"
  },
  {
    "text": "we knew that the days of standardizing on a database are long over so we went",
    "start": "1222720",
    "end": "1227760"
  },
  {
    "text": "with a polyglot architecture we wanted to bring technology team we wanted to",
    "start": "1227760",
    "end": "1233120"
  },
  {
    "text": "give them flexibility to bring the best store for the job and not only that most cost effective store for the job not all",
    "start": "1233120",
    "end": "1240240"
  },
  {
    "text": "data is created equal so we went with bring your own store kind of environment",
    "start": "1240240",
    "end": "1245679"
  },
  {
    "text": "and then if you remember we are targeting extremely high scalability distribution as well as very low latency",
    "start": "1245679",
    "end": "1252960"
  },
  {
    "text": "so we knew we needed a lot and lot of cache so we made this environment cache",
    "start": "1252960",
    "end": "1258919"
  },
  {
    "text": "rich so how does the storage polyglot itself looks like something like this for realtime",
    "start": "1258919",
    "end": "1266400"
  },
  {
    "start": "1259000",
    "end": "1392000"
  },
  {
    "text": "data and caching we are relying heavily on radius we love radius a lot",
    "start": "1266400",
    "end": "1272640"
  },
  {
    "text": "and we have been able to extract sub millisecond of response time out of let out of",
    "start": "1272640",
    "end": "1278280"
  },
  {
    "text": "radius if you think about the forecast data it naturally falls in a timeline",
    "start": "1278280",
    "end": "1284240"
  },
  {
    "text": "kind of a timeline series or time series kind of environment so radius allows us to move",
    "start": "1284240",
    "end": "1290000"
  },
  {
    "text": "back and forth in time for that data sometimes it's so awesome it's so freaky",
    "start": "1290000",
    "end": "1297679"
  },
  {
    "text": "that it looks like response game from the future so we internally call it time machine so that's radius we are relying",
    "start": "1297679",
    "end": "1304480"
  },
  {
    "text": "for real-time data and caching on radius for historical weather",
    "start": "1304480",
    "end": "1310760"
  },
  {
    "text": "archive we are using riarch as well as for data",
    "start": "1310760",
    "end": "1316399"
  },
  {
    "text": "migration we are also very heavy users of Cassandra we use it for things like",
    "start": "1316679",
    "end": "1322400"
  },
  {
    "text": "gateway data analytics heavy users of analytics on Cassandra and also for",
    "start": "1322400",
    "end": "1330159"
  },
  {
    "text": "things like uh profile data also for things like crowdsource weather data",
    "start": "1330159",
    "end": "1336400"
  },
  {
    "text": "those kind of things and then we have a lot of data that we archive pretty much all",
    "start": "1336400",
    "end": "1344480"
  },
  {
    "text": "the data except historical weather we put it in S3 we are very heavy users of S3 and we like it a lot so we archive it",
    "start": "1344480",
    "end": "1351919"
  },
  {
    "text": "there not only that for static assets for binary assets like images videos we",
    "start": "1351919",
    "end": "1358720"
  },
  {
    "text": "are heavily heavily relying on S3 we put those in S3 and we serve it out from there of course CDN front ended then",
    "start": "1358720",
    "end": "1366080"
  },
  {
    "text": "from an from an analytics perspective lately we have been migrating to this",
    "start": "1366080",
    "end": "1372320"
  },
  {
    "text": "data lake concept which is Spark A plus whatever storage based mostly in our case it's Spark A plus S3 so we are",
    "start": "1372320",
    "end": "1379760"
  },
  {
    "text": "building our data lake on that and then of course we have relational databases MySQL and SQL",
    "start": "1379760",
    "end": "1387120"
  },
  {
    "text": "server for things like Informatica and Drupal",
    "start": "1387120",
    "end": "1391919"
  },
  {
    "start": "1392000",
    "end": "1467000"
  },
  {
    "text": "repositories so what about caching itself cache is your friend but it needs to be used very",
    "start": "1392360",
    "end": "1399640"
  },
  {
    "text": "carefully so we went with a caching architecture which is three tier of course from the edge your CDN is going",
    "start": "1399640",
    "end": "1406720"
  },
  {
    "text": "going to provide edge caching as well as edge compute for things like ESIs but for origin we are relying",
    "start": "1406720",
    "end": "1414159"
  },
  {
    "text": "heavily on varnish that's our origin cache in fact all of our architecture",
    "start": "1414159",
    "end": "1419760"
  },
  {
    "text": "when I'm going to say app servers behind ELB looks something like this elb with a",
    "start": "1419760",
    "end": "1425039"
  },
  {
    "text": "bunch of varnish instances going to ELB and bunch of app instances",
    "start": "1425039",
    "end": "1432080"
  },
  {
    "text": "and we talked about radius as our app caching some of the lessons that we",
    "start": "1432080",
    "end": "1437679"
  },
  {
    "text": "learned in caching is you need to keep this very very",
    "start": "1437679",
    "end": "1445000"
  },
  {
    "text": "simple don't try to do things like dynamically expiring caches or trying to synchronize the caching or trying to",
    "start": "1445000",
    "end": "1452080"
  },
  {
    "text": "dynamically expire this object or that object but make sure all the data elements are TTLdriven from the ingest",
    "start": "1452080",
    "end": "1459279"
  },
  {
    "text": "itself from the beginning and that every step in your chain respect that",
    "start": "1459279",
    "end": "1466158"
  },
  {
    "start": "1467000",
    "end": "1515000"
  },
  {
    "text": "TTL okay so what about systems of record architecture tenants there again",
    "start": "1467400",
    "end": "1473600"
  },
  {
    "text": "were let's let system designers focus on the job that they are trying to do and let them do that job let them pick the",
    "start": "1473600",
    "end": "1480400"
  },
  {
    "text": "best tools and technology for the job and just make sure that you interface with standard protocols both for input",
    "start": "1480400",
    "end": "1486480"
  },
  {
    "text": "and output use restful APIs over HTTP and then let the layer below and above",
    "start": "1486480",
    "end": "1494240"
  },
  {
    "text": "do its job so let BP do the hard work of ingesting the data and normalizing and",
    "start": "1494240",
    "end": "1499760"
  },
  {
    "text": "all those things and then let the data services do the hard work of distributing it out",
    "start": "1499760",
    "end": "1506640"
  },
  {
    "text": "don't put that burden on the system of records itself and of course it needs to support both push and pull model for",
    "start": "1506640",
    "end": "1512960"
  },
  {
    "text": "publication so what does that mean so first two examples are a pull model so",
    "start": "1512960",
    "end": "1518960"
  },
  {
    "start": "1515000",
    "end": "1579000"
  },
  {
    "text": "that's the example of forecast on demand system and currents on demand system which is observations that exposes that",
    "start": "1518960",
    "end": "1526240"
  },
  {
    "text": "data once it has produced forecast and obsesses that data over restful",
    "start": "1526240",
    "end": "1532000"
  },
  {
    "text": "interface so our data services is simply sitting on top of this and all the requests are",
    "start": "1532000",
    "end": "1539600"
  },
  {
    "text": "coming through that proxying through that and data services is going to try to attempt to actually service that out",
    "start": "1539600",
    "end": "1546799"
  },
  {
    "text": "utilizing its own cache and in case of cache miss it's going to forward that request to the system of record",
    "start": "1546799",
    "end": "1551840"
  },
  {
    "text": "underneath so that's one but what about things like CMS content management",
    "start": "1551840",
    "end": "1558400"
  },
  {
    "text": "system when it's actually there's a publication mechanism involved so when CMS is ready to publish the data it",
    "start": "1558400",
    "end": "1565600"
  },
  {
    "text": "actually posts to a endpoint in data services and that data services then",
    "start": "1565600",
    "end": "1572159"
  },
  {
    "text": "stores it internally in its own store and in future it's going to serve it out from",
    "start": "1572159",
    "end": "1578880"
  },
  {
    "text": "there okay what about the design of data services itself again restful API design",
    "start": "1579320",
    "end": "1586640"
  },
  {
    "text": "and it needs to be stateless decoupled but one of the key things here is this concept of atomic and aggregation",
    "start": "1586640",
    "end": "1593200"
  },
  {
    "text": "services so what is that atomic services are the services",
    "start": "1593200",
    "end": "1598640"
  },
  {
    "text": "that are either provided by system of record or if we write it in data services at",
    "start": "1598640",
    "end": "1606480"
  },
  {
    "text": "layer itself it's going to go and read it from the storage layer and return that data so example is system of record",
    "start": "1606480",
    "end": "1613799"
  },
  {
    "text": "provides 5-day forecast that's an atomic service uh we also have pollen data in our",
    "start": "1613799",
    "end": "1621360"
  },
  {
    "text": "storage and data services goes and reads it and returns the pollen data that's another atomic service but we have a",
    "start": "1621360",
    "end": "1628240"
  },
  {
    "text": "concept of aggregation which means I can make a single API request asking for",
    "start": "1628240",
    "end": "1633600"
  },
  {
    "text": "multiple data points I can ask for both pollen as well as forecast data in a single request and aggregation service",
    "start": "1633600",
    "end": "1640400"
  },
  {
    "text": "is going to go and it's going to get the data from both these atomic service services aggregate it and return it out",
    "start": "1640400",
    "end": "1646320"
  },
  {
    "text": "in a single response This needs to be all keydriven a me for",
    "start": "1646320",
    "end": "1652559"
  },
  {
    "text": "a and metering horizontally scalable capable of serving billions of requests per day and one of the key observation",
    "start": "1652559",
    "end": "1658320"
  },
  {
    "text": "is our data lends very well to caching so let's use it let's use let's use the edge let's use varnish",
    "start": "1658320",
    "end": "1666480"
  },
  {
    "text": "let's use radius so how does that look like api",
    "start": "1666480",
    "end": "1672159"
  },
  {
    "start": "1668000",
    "end": "1740000"
  },
  {
    "text": "user comes in makes a request and if CDN can sub it out from the edge it's going",
    "start": "1672159",
    "end": "1677919"
  },
  {
    "text": "to just sub it out from the edge if not now API gateway needs to make a",
    "start": "1677919",
    "end": "1684760"
  },
  {
    "text": "determination whether it's a aggregate request or atomic",
    "start": "1684760",
    "end": "1689960"
  },
  {
    "text": "request and if it is an aggregate request it actually decomposes it into atomic request and we use ESIs for",
    "start": "1689960",
    "end": "1697559"
  },
  {
    "text": "those those atomic requests are the one that's that are going to origin so that's O API and that's what it serves",
    "start": "1697559",
    "end": "1704399"
  },
  {
    "text": "those atomic requests and that's an intelligent mechanism where first it checks for the",
    "start": "1704399",
    "end": "1711679"
  },
  {
    "text": "cache of course DDL driven and if it can serve it out from there it's going to serve it out from there if not and the",
    "start": "1711679",
    "end": "1718640"
  },
  {
    "text": "data is stored internally in its storage layer radius or riac or whatever in case of a push model it's going to serve it",
    "start": "1718640",
    "end": "1725440"
  },
  {
    "text": "out from there but if the data is served from system of records it's dispatched to",
    "start": "1725440",
    "end": "1732960"
  },
  {
    "text": "those systems and the request comes from there and of course then we cach it",
    "start": "1732960",
    "end": "1738799"
  },
  {
    "start": "1740000",
    "end": "1781000"
  },
  {
    "text": "again okay so how does the request flow looks like in AWS again sticking with",
    "start": "1740279",
    "end": "1745360"
  },
  {
    "text": "the same model single region two availability zones request comes in goes",
    "start": "1745360",
    "end": "1751200"
  },
  {
    "text": "to an ELB from there it's going to go to OAPI instances and actually they are uh",
    "start": "1751200",
    "end": "1758080"
  },
  {
    "text": "varnish elbish ELB O API and request comes from there and there if it needs",
    "start": "1758080",
    "end": "1765039"
  },
  {
    "text": "to go to caching it's going to go to caching it's need to go to stores data stores it's going to read it from the stores and in case it needs to actually",
    "start": "1765039",
    "end": "1772240"
  },
  {
    "text": "go to the underlying system of records it's dispatched to those via their ELBs",
    "start": "1772240",
    "end": "1777679"
  },
  {
    "text": "because those systems are also running in Amazon AWS so what about the push model or the",
    "start": "1777679",
    "end": "1785559"
  },
  {
    "start": "1781000",
    "end": "1828000"
  },
  {
    "text": "content Drupal CMS for example when it's ready to post the content",
    "start": "1785559",
    "end": "1790799"
  },
  {
    "text": "In case of binary content like images and videos",
    "start": "1790799",
    "end": "1796399"
  },
  {
    "text": "it's going to take the asset itself and store it in our static asset pools which are S3 based so they are all in S3 and",
    "start": "1796399",
    "end": "1803520"
  },
  {
    "text": "the metadata is actually written to a metadata store in case of textual data like",
    "start": "1803520",
    "end": "1809880"
  },
  {
    "text": "articles everything goes there in the metadata store itself and then our services are essentially reading it from",
    "start": "1809880",
    "end": "1816720"
  },
  {
    "text": "there so things like image cut service things like video distribution services if an MRSS",
    "start": "1816720",
    "end": "1821880"
  },
  {
    "text": "feeds they are just reading it from there and serving it out so that's the data",
    "start": "1821880",
    "end": "1828799"
  },
  {
    "start": "1828000",
    "end": "1865000"
  },
  {
    "text": "services okay so what about authentication routing metering and throttling all these kind of things this",
    "start": "1828919",
    "end": "1835039"
  },
  {
    "text": "is all the job of gateway some of the key requirements for us for gateway were this gateway needs",
    "start": "1835039",
    "end": "1842320"
  },
  {
    "text": "not needs to be not just CDN aware but CDN driven and we are going to talk about what that means in a",
    "start": "1842320",
    "end": "1849159"
  },
  {
    "text": "minute also remember that Tesky 25 millisecond latency target it's still with us so we cannot add a lot of",
    "start": "1849159",
    "end": "1855760"
  },
  {
    "text": "latency to this so we looked at quite a few solutions out there and we could not",
    "start": "1855760",
    "end": "1861440"
  },
  {
    "text": "find something that satisfies so we rolled our own and this is how it looks like api",
    "start": "1861440",
    "end": "1868240"
  },
  {
    "start": "1865000",
    "end": "1922000"
  },
  {
    "text": "user comes in makes a request cdn itself checks for authorization by doing a look",
    "start": "1868240",
    "end": "1873840"
  },
  {
    "text": "aside so what does that mean we wrote a quick service a quick",
    "start": "1873840",
    "end": "1879159"
  },
  {
    "text": "authentication service quick response service which is essentially a very simple service that reads from Reddis",
    "start": "1879159",
    "end": "1886080"
  },
  {
    "text": "and in submillisecond response times returns a yes or no authorized or not and then CDN once if it is yes it",
    "start": "1886080",
    "end": "1893600"
  },
  {
    "text": "proceeds with its normal caching and origin routing and all those kind of things but the source of authentication",
    "start": "1893600",
    "end": "1899679"
  },
  {
    "text": "truth is actually sitting in Cassandra database that's where the keys are that's where metering and all those",
    "start": "1899679",
    "end": "1905919"
  },
  {
    "text": "things happen and they all happen in the background so from there they are simply synchronized to radius so we actually",
    "start": "1905919",
    "end": "1913480"
  },
  {
    "text": "sacrifice some real time throttling and metering",
    "start": "1913480",
    "end": "1919000"
  },
  {
    "text": "for low latency okay so so far we have been",
    "start": "1919000",
    "end": "1925440"
  },
  {
    "text": "predominantly talking about a single way data flow from the weather channel itself to our users but our users are",
    "start": "1925440",
    "end": "1933840"
  },
  {
    "text": "actually doing a lot of things on the platforms itself so and and hence they",
    "start": "1933840",
    "end": "1939039"
  },
  {
    "text": "are generating events how do we handle that we use Amazon SQS for that initially we thought of rolling our own",
    "start": "1939039",
    "end": "1946640"
  },
  {
    "text": "by using maybe some Q and web service in front of it but then we realized SQS",
    "start": "1946640",
    "end": "1951679"
  },
  {
    "text": "actually does it so let's just use that that's what we are using so events are coming in they go in SQS and then Spark",
    "start": "1951679",
    "end": "1958640"
  },
  {
    "text": "streaming actually can read it and do stream processing from there but we have our own custom injection pipeline which",
    "start": "1958640",
    "end": "1963679"
  },
  {
    "text": "takes it from there and puts it in our data lakeink for batch files for example",
    "start": "1963679",
    "end": "1969760"
  },
  {
    "text": "Google DFP all those kind of files or third party files may come from S3 or",
    "start": "1969760",
    "end": "1975120"
  },
  {
    "text": "from whatever batch source we use Informatica ETL for that so we read it from there and that also goes in our",
    "start": "1975120",
    "end": "1983120"
  },
  {
    "text": "data lake our data lake is essentially",
    "start": "1983120",
    "end": "1989320"
  },
  {
    "text": "S3based and it's it has long-term raw storage so raw files also go there and",
    "start": "1989320",
    "end": "1995679"
  },
  {
    "text": "then we use EMR and all those kind of abilities to create part files and put that in",
    "start": "1995679",
    "end": "2001320"
  },
  {
    "text": "S3 uh we are still heavy users of Cassandra and you know Spark Cassandra",
    "start": "2001320",
    "end": "2006720"
  },
  {
    "text": "combination for short-term storage and big data processing but we actually like the concept of data lake a lot",
    "start": "2006720",
    "end": "2014720"
  },
  {
    "text": "then we have SparkSQL for data access and then for visualization and dashboard we use things like click view and click",
    "start": "2014720",
    "end": "2022880"
  },
  {
    "text": "clicksense and click view and all those things sitting in front of it so that's how our events and analytics look",
    "start": "2022880",
    "end": "2029799"
  },
  {
    "text": "like so let's put it all together and see how the entire stack looks",
    "start": "2029799",
    "end": "2035159"
  },
  {
    "text": "like looks something like this we are across all four global regions well we",
    "start": "2035159",
    "end": "2040960"
  },
  {
    "text": "are across four global regions and we have the entire data stack running in",
    "start": "2040960",
    "end": "2046399"
  },
  {
    "text": "these four regions from ingestion all the way to distribution so we ingest data in all",
    "start": "2046399",
    "end": "2053040"
  },
  {
    "text": "the regions in all these four regions we process it there we actually cache it and store it and all those things and",
    "start": "2053040",
    "end": "2059358"
  },
  {
    "text": "distribute it out from there of course due to the limitation of databases and limitation of certain other",
    "start": "2059359",
    "end": "2065158"
  },
  {
    "text": "things cost included you can't really do those things all the time so we have",
    "start": "2065159",
    "end": "2071200"
  },
  {
    "text": "some remote injection and replication going on as well but by and large we like to stick with this environment",
    "start": "2071200",
    "end": "2076960"
  },
  {
    "text": "where we actually run the entire thing in each region we have another region dedicated for configuration management",
    "start": "2076960",
    "end": "2083358"
  },
  {
    "text": "monitoring and automation for things like uh Puppet and Chef and Zabix and all those kind of things",
    "start": "2083359",
    "end": "2091398"
  },
  {
    "start": "2090000",
    "end": "2104000"
  },
  {
    "text": "okay so that was the plan so that's the architecture and we had a road map based on that and we were building different",
    "start": "2091760",
    "end": "2097480"
  },
  {
    "text": "components project was moving along and then this",
    "start": "2097480",
    "end": "2104160"
  },
  {
    "start": "2104000",
    "end": "2148000"
  },
  {
    "text": "happened we were served a carball the moment our forecast on",
    "start": "2104280",
    "end": "2109760"
  },
  {
    "text": "demand system was ready we used that to actually strike a deal with a major mobile phone company",
    "start": "2109760",
    "end": "2117440"
  },
  {
    "text": "to ship the new APIs for them for that weather data and due to the scale of",
    "start": "2117440",
    "end": "2123000"
  },
  {
    "text": "that we were targeting 25 billion requests per day and time to market for the for this",
    "start": "2123000",
    "end": "2129839"
  },
  {
    "text": "was 3 months we had to deliver it in three months so after some celebration and heavy",
    "start": "2129839",
    "end": "2136640"
  },
  {
    "text": "dinking heavy drinking and also actually throwing up though not necessarily sure",
    "start": "2136640",
    "end": "2141839"
  },
  {
    "text": "in which order for all of us we sat down to do",
    "start": "2141839",
    "end": "2147960"
  },
  {
    "text": "it and this is what we found that's where our decisions for",
    "start": "2147960",
    "end": "2154880"
  },
  {
    "text": "loosely coupled architecture atomic aggregation services and the scale of",
    "start": "2154880",
    "end": "2160200"
  },
  {
    "text": "AWS really shined because we have atomic services ready coming from forecast on demand system we",
    "start": "2160200",
    "end": "2167599"
  },
  {
    "text": "have to write a custom aggregate for that vendor for that partner so we wrote",
    "start": "2167599",
    "end": "2173200"
  },
  {
    "text": "that and then we have to scale that to handle those 25 billion requests all",
    "start": "2173200",
    "end": "2178320"
  },
  {
    "text": "this in a matter of 3 months so we wrote that and then we launched ourself into this cycle load test tune destructive",
    "start": "2178320",
    "end": "2186560"
  },
  {
    "text": "test fix and we caused a lot of things we cause VPC net saturation DNS server DNS",
    "start": "2186560",
    "end": "2194640"
  },
  {
    "text": "server saturation so we had to figure out the correct sizing we had to figure out the correct instant types and sizes",
    "start": "2194640",
    "end": "2199920"
  },
  {
    "text": "for every layer or every component in that layer for radius for I APIs for O",
    "start": "2199920",
    "end": "2206720"
  },
  {
    "text": "API servers for varnishes for all of those and then we ran into OS kernel",
    "start": "2206720",
    "end": "2212160"
  },
  {
    "text": "limits as well so we had to do kernel parameter tuning we ran into issues with compression and decompression so we had",
    "start": "2212160",
    "end": "2219280"
  },
  {
    "text": "to actually fine-tune that as well and then we also did for make to",
    "start": "2219280",
    "end": "2225440"
  },
  {
    "text": "make our environment very resilient we did this chaos monkey philosophy destructive test and fix we brought down",
    "start": "2225440",
    "end": "2232880"
  },
  {
    "text": "instances availability zones regions we brought down we corrupted caches we",
    "start": "2232880",
    "end": "2238960"
  },
  {
    "text": "brought down databases or the shards for the databases and caches and we fixed",
    "start": "2238960",
    "end": "2244240"
  },
  {
    "text": "those things we figured out ways around it",
    "start": "2244240",
    "end": "2249680"
  },
  {
    "start": "2248000",
    "end": "2269000"
  },
  {
    "text": "so one of the key tenants key takeaway here is this step is very very important",
    "start": "2249839",
    "end": "2256800"
  },
  {
    "text": "it's extremely important as important as the development itself but it takes time",
    "start": "2256800",
    "end": "2262320"
  },
  {
    "text": "to figure this all out so make sure that you budget time and resources for both load and destructive",
    "start": "2262320",
    "end": "2269560"
  },
  {
    "text": "testing with that I'll hand it over to Rahul to provide some AWS insights",
    "start": "2269560",
    "end": "2275119"
  },
  {
    "text": "thank you Yakme good morning everyone for",
    "start": "2275119",
    "end": "2281359"
  },
  {
    "text": "you uh good morning well my name is Rahul Frias i'm a solutions architect for Amazon Web Services i've been in AWS",
    "start": "2281800",
    "end": "2288720"
  },
  {
    "text": "for two years and actually this is my second reinvent and really excited about it and um so building this type of",
    "start": "2288720",
    "end": "2295040"
  },
  {
    "text": "architecture so just like Jack mentioned takes a lot of time and effort to build them out and it's something that you",
    "start": "2295040",
    "end": "2301040"
  },
  {
    "text": "need to budget for and one of the things that the weather company had already had built was was a decoupled architecture",
    "start": "2301040",
    "end": "2307040"
  },
  {
    "start": "2307000",
    "end": "2407000"
  },
  {
    "text": "and u that was they just needed to focus like on scaling the different layers of that architecture and um when some of",
    "start": "2307040",
    "end": "2315119"
  },
  {
    "text": "you may find yourselves with similar challenges just as a weather campaign meet where you have a very small amount",
    "start": "2315119",
    "end": "2321599"
  },
  {
    "text": "of time and limited set of resources and people and tools uh one of the things",
    "start": "2321599",
    "end": "2326720"
  },
  {
    "text": "that you could build out and streamline building this kind of this kind of applications architectures is kind of",
    "start": "2326720",
    "end": "2332640"
  },
  {
    "text": "leverage the managed services and uh so I'm not here to tell you about all these",
    "start": "2332640",
    "end": "2339599"
  },
  {
    "text": "managed services uh I'm actually here to talk to you like when might be a good fit or may not be a good fit for your",
    "start": "2339599",
    "end": "2346240"
  },
  {
    "text": "different application and u one of the things that we see with this managed services is customers using them in one",
    "start": "2346240",
    "end": "2353200"
  },
  {
    "text": "of two different paths one path customers have start building brand new architectures brand new applications and",
    "start": "2353200",
    "end": "2359119"
  },
  {
    "text": "they're leveraging this managed services from day one and they love them they hook them they're hooked to them and um",
    "start": "2359119",
    "end": "2365680"
  },
  {
    "text": "sometimes they even grow out of them and um the other path that we usually see is",
    "start": "2365680",
    "end": "2370880"
  },
  {
    "text": "customers uh managing the AWS basically as an infrastructure platform and",
    "start": "2370880",
    "end": "2376560"
  },
  {
    "text": "managing their IT resources basically the same way they have done it before like in the old ways like uh the dark",
    "start": "2376560",
    "end": "2382560"
  },
  {
    "text": "ages before the cloud And uh after some adoption we usually see them now starting to leverage these managed",
    "start": "2382560",
    "end": "2388480"
  },
  {
    "text": "services and do this undifferiated heavy lifting that you probably heard about and you're going to hear about it a lot",
    "start": "2388480",
    "end": "2394800"
  },
  {
    "text": "in AWS today so um I'm not going to cover all of this managed services there's going to be sessions that",
    "start": "2394800",
    "end": "2400960"
  },
  {
    "text": "specialize in all of them um I'm just going to cover some of this and how we match them to some of the services that",
    "start": "2400960",
    "end": "2407440"
  },
  {
    "start": "2407000",
    "end": "2524000"
  },
  {
    "text": "the what the company has done and how you can streamline streamline building some architectures like this um so first",
    "start": "2407440",
    "end": "2414640"
  },
  {
    "text": "of all recalling if you recall the storage layer from the weather company where they're basically using a lot of",
    "start": "2414640",
    "end": "2420240"
  },
  {
    "text": "stoages and a lot of databases out there uh they're actually they did mention from for relational database stand uh",
    "start": "2420240",
    "end": "2427119"
  },
  {
    "text": "point of view they're using MySQL and Microsoft SQL server um so one of the",
    "start": "2427119",
    "end": "2432560"
  },
  {
    "text": "options that you have here is uh we have a managed service for database service uh AWS RDS relational database service",
    "start": "2432560",
    "end": "2440480"
  },
  {
    "text": "um and uh when one when when you would like to",
    "start": "2440480",
    "end": "2445839"
  },
  {
    "text": "use an RDS solution versus an easy to manage relational database right so um",
    "start": "2445839",
    "end": "2452480"
  },
  {
    "text": "two main factors here performance and size usually are some of the factors that will your decision will be made",
    "start": "2452480",
    "end": "2459440"
  },
  {
    "text": "from you can see from the table on this on the deck the performance and terabyte",
    "start": "2459440",
    "end": "2465200"
  },
  {
    "text": "sizes depending on the engines we already just saw this is already outdated right we already have Maria DB",
    "start": "2465200",
    "end": "2471680"
  },
  {
    "text": "now also as an RDS choice um but when you start reaching this limits this",
    "start": "2471680",
    "end": "2477599"
  },
  {
    "text": "maximum limits it's probably a good sign that you're starting to grow out of the services and you should go out into an",
    "start": "2477599",
    "end": "2483839"
  },
  {
    "text": "EC2 based relational database also licensing bring your own license or if",
    "start": "2483839",
    "end": "2489359"
  },
  {
    "text": "you're interested in non-demand licensing RDS offers you know the both options and there's a couple of options",
    "start": "2489359",
    "end": "2495359"
  },
  {
    "text": "for SQL server also uh regarding uh on demand licensing replication uh RDS",
    "start": "2495359",
    "end": "2502480"
  },
  {
    "text": "manages bas basic replication in multiac's master slave replication if if",
    "start": "2502480",
    "end": "2508400"
  },
  {
    "text": "you're interested in having more advanced or complex replication schemes then you should definitely uh build your",
    "start": "2508400",
    "end": "2514800"
  },
  {
    "text": "own uh the EC2based solution and of course the differentiator heavy lifting",
    "start": "2514800",
    "end": "2520160"
  },
  {
    "text": "right the backups the updates installing and everything else also u the weather company inject",
    "start": "2520160",
    "end": "2527040"
  },
  {
    "start": "2524000",
    "end": "2766000"
  },
  {
    "text": "mentioned a lot of options out there from NoSQL right and u so many customers",
    "start": "2527040",
    "end": "2532560"
  },
  {
    "text": "are you know we're seeing a lot of traction with the NoSQL database space and this is not something that's going to go away anytime soon there's a lot of",
    "start": "2532560",
    "end": "2539200"
  },
  {
    "text": "options out there and the questions with uh our customers usually go around like which which would be the best No SQL",
    "start": "2539200",
    "end": "2545760"
  },
  {
    "text": "option for me out there so I won't cover all the uh options that are out there i'm just gonna cover this two and a",
    "start": "2545760",
    "end": "2552480"
  },
  {
    "text": "manage version like from Amazon instead of uh and uh just going through to the",
    "start": "2552480",
    "end": "2558079"
  },
  {
    "text": "ones that the weather companies are already using right so first of all all these NoSQL databases share some common",
    "start": "2558079",
    "end": "2563920"
  },
  {
    "text": "patterns you know that's schemaless they don't have any kind of relational the foreign keys for to relate different",
    "start": "2563920",
    "end": "2569760"
  },
  {
    "text": "tables and uh indexing is a very important thing that you uh there's no",
    "start": "2569760",
    "end": "2575680"
  },
  {
    "text": "uh that you should consider from the beginning right there's no afterthought in the indexing and so if you care about",
    "start": "2575680",
    "end": "2581760"
  },
  {
    "text": "the performance of your database you really need to um analyze your query patterns for your performance in NoSQL",
    "start": "2581760",
    "end": "2589680"
  },
  {
    "text": "so when does uh these ones apply and some some of the challenges first of or Cassandra um if right performance is a",
    "start": "2589680",
    "end": "2598480"
  },
  {
    "text": "concern for you this is one where this NoSQL database really shines out also they have a very good multi-native uh",
    "start": "2598480",
    "end": "2605760"
  },
  {
    "text": "native multi-reion multi-AC replication with their own nomenclature um also",
    "start": "2605760",
    "end": "2611119"
  },
  {
    "text": "there's uh commercial options out there with uh data stacks offers solutions uh services and um support for commercial u",
    "start": "2611119",
    "end": "2619920"
  },
  {
    "text": "options out there with a huge ecosystem and um some of the challenges that customers face when building this kind",
    "start": "2619920",
    "end": "2626480"
  },
  {
    "text": "of non-managed solutions like Cassandra and this is also shared with Riyak and this deck repetitioning right so when",
    "start": "2626480",
    "end": "2633200"
  },
  {
    "text": "you want to handle scaling scale out your own uh um your NoSQL database this",
    "start": "2633200",
    "end": "2639280"
  },
  {
    "text": "is not an automatic process this is a manual process that you need to take some stuff into consideration and this",
    "start": "2639280",
    "end": "2645760"
  },
  {
    "text": "consideration sometimes will lead you know into overprovisioning EBS and EC2",
    "start": "2645760",
    "end": "2651760"
  },
  {
    "text": "uh resources the all the operational burden installing configuring u setting",
    "start": "2651760",
    "end": "2657760"
  },
  {
    "text": "setting it up it's still a manual process that you need to do yourself and also if you're going to do some kind of",
    "start": "2657760",
    "end": "2662960"
  },
  {
    "text": "multi- region or multi-AC replication and this applies to both Ryak and Cassandra too you're going to incur in",
    "start": "2662960",
    "end": "2668640"
  },
  {
    "text": "the data transfer costs Right so um the second option Ryek also used a lot by the weather company uh",
    "start": "2668640",
    "end": "2675760"
  },
  {
    "text": "this one really shines a lot in all the the availability space right based on",
    "start": "2675760",
    "end": "2680960"
  },
  {
    "text": "the Amazon paper um there's zero downtime design for that um they're also",
    "start": "2680960",
    "end": "2687680"
  },
  {
    "text": "shine really good in the cross region replication with their own nomenclature they face similar challenges in this",
    "start": "2687680",
    "end": "2693839"
  },
  {
    "text": "space very similar challenges to what Cassandra deployments are repartitioning scaling in uh it's not automatic you",
    "start": "2693839",
    "end": "2701440"
  },
  {
    "text": "need to consider uh you need to have some considerations regarding that and the same operational and data transfer",
    "start": "2701440",
    "end": "2706800"
  },
  {
    "text": "cost and u dynamob just to put it out there i'm not I'm not going to sell you Dynamob many of you already know it's",
    "start": "2706800",
    "end": "2713119"
  },
  {
    "text": "effortless scaling you just need to put the write and three write and read throughputs uh effortless scaling and um",
    "start": "2713119",
    "end": "2719680"
  },
  {
    "text": "the one thing is about this is also constantly evolving right Dynamob so like you just come you just came from",
    "start": "2719680",
    "end": "2725920"
  },
  {
    "text": "the keynote and you probably see that we like to make announcements a lot in this company so uh this is constantly",
    "start": "2725920",
    "end": "2732079"
  },
  {
    "text": "changing but some of you may be interested in when this may not be the best fit for you right um so it size",
    "start": "2732079",
    "end": "2739280"
  },
  {
    "text": "right now from Dynamob right now it sits at 400 kilobytes per uh item if you if",
    "start": "2739280",
    "end": "2744720"
  },
  {
    "text": "your items in no SQL are higher than that this may not be the best fit for your application and also cross region",
    "start": "2744720",
    "end": "2750800"
  },
  {
    "text": "replication with Dynamo DB uh last year we launched the DynamoB streams but there's still some manual procedures",
    "start": "2750800",
    "end": "2756880"
  },
  {
    "text": "that you need to do on the work that you need to do and uh apply so we're still constantly evolving and I would just",
    "start": "2756880",
    "end": "2762079"
  },
  {
    "text": "tell you to stay tuned this probably be something that we we're working",
    "start": "2762079",
    "end": "2767160"
  },
  {
    "start": "2766000",
    "end": "2814000"
  },
  {
    "text": "on on the other one um recalling the the data distribution engine or the the data",
    "start": "2767160",
    "end": "2772800"
  },
  {
    "text": "processing engine from the weather company as you may recall from the from JCMstack uh this layer was basically in",
    "start": "2772800",
    "end": "2779920"
  },
  {
    "text": "charge of for the whole scaling ingesting the data and processing this data and um there's a way there's some",
    "start": "2779920",
    "end": "2786079"
  },
  {
    "text": "services that you could use to streamline uh this building of a very similar data processing engine uh with",
    "start": "2786079",
    "end": "2793040"
  },
  {
    "text": "two different technologies right so first of all uh streaming storage right the whole the whole point of streaming",
    "start": "2793040",
    "end": "2798720"
  },
  {
    "text": "storage is we could decouple producers from the consumer so that we can scale them independently you have a temporary",
    "start": "2798720",
    "end": "2804800"
  },
  {
    "text": "buffer of your data and uh you can serialize all your information that you're putting in the stream storage and",
    "start": "2804800",
    "end": "2810400"
  },
  {
    "text": "you can kind of do like a streaming map reduce from your client side and this is something that we usually see with two",
    "start": "2810400",
    "end": "2817599"
  },
  {
    "text": "tools right the manage option from AWS Amazon Kinesis and there's Apache CFKA also used by by a lot of our customers",
    "start": "2817599",
    "end": "2825359"
  },
  {
    "text": "and um they both have a lot of similarities you know multiple consumers the ordering of records low latency",
    "start": "2825359",
    "end": "2831680"
  },
  {
    "text": "highly durable scalable and available So the interesting thing the differences right um record lifetime and record size",
    "start": "2831680",
    "end": "2839800"
  },
  {
    "text": "most most of the differences rely on that um record lifetime in Amazon kinesis is set in 24 hours it's not",
    "start": "2839800",
    "end": "2847200"
  },
  {
    "text": "configurable uh whereas in cfka you can configure that also the record size",
    "start": "2847200",
    "end": "2854880"
  },
  {
    "text": "right now in kinesis it sits at maximum one megabyte per record at GA that was launched it was in a kilobytes of size",
    "start": "2854880",
    "end": "2861040"
  },
  {
    "text": "it now sits at one megabyte CFKA this is completely configurable right um of course Kinesis are fully",
    "start": "2861040",
    "end": "2867920"
  },
  {
    "text": "managed there's no EC2 instances that you need to launch and configure and and handle so this is the first component",
    "start": "2867920",
    "end": "2873760"
  },
  {
    "start": "2872000",
    "end": "2930000"
  },
  {
    "text": "let's the streaming storage and um and here like giving like an AWS style to",
    "start": "2873760",
    "end": "2879520"
  },
  {
    "text": "this data procession uh data processing engine from the weather company and this is something that we really we're really",
    "start": "2879520",
    "end": "2884880"
  },
  {
    "text": "excited to show it to the weather company and to other customers that's how about imagining a serverless",
    "start": "2884880",
    "end": "2890640"
  },
  {
    "text": "approach to building this type of a ingestion and processing layer right so",
    "start": "2890640",
    "end": "2896160"
  },
  {
    "text": "here comes Lambda right you our code as a service where we you basically give us",
    "start": "2896160",
    "end": "2901440"
  },
  {
    "text": "your code your application and we run it for you uh there's no EC2 instances there's no servers to manage out there",
    "start": "2901440",
    "end": "2907040"
  },
  {
    "text": "and u this is how we would imagine this like having Amazon Kinesis having all",
    "start": "2907040",
    "end": "2912400"
  },
  {
    "text": "managing all your streaming storage from your different data inputs and having lambda take action on them and process",
    "start": "2912400",
    "end": "2918079"
  },
  {
    "text": "it and actually putting it afterwards in a persistent storage Amazon RDS S3 red",
    "start": "2918079",
    "end": "2923760"
  },
  {
    "text": "shift your own EC2based relational or non-reational databases right so",
    "start": "2923760",
    "end": "2930040"
  },
  {
    "text": "um with that um I hope this quick AWS insights give you some practical ways to",
    "start": "2930040",
    "end": "2937599"
  },
  {
    "text": "build architectures like this faster and reduce your burden and I'm going to leave back to Jack me so that he can",
    "start": "2937599",
    "end": "2942960"
  },
  {
    "text": "talk you about the future thank you thank you R",
    "start": "2942960",
    "end": "2948119"
  },
  {
    "text": "[Applause] okay so how are we evolving the architecture the architecture that you",
    "start": "2949140",
    "end": "2954160"
  },
  {
    "text": "saw earlier of course we want to stick with the things that work for us and we",
    "start": "2954160",
    "end": "2959520"
  },
  {
    "text": "want to try to make an attempt to fix our pain points so let's take that approach and see how we are evolving it",
    "start": "2959520",
    "end": "2967359"
  },
  {
    "text": "to begin with we want to stick with the layered architecture and loose coupling",
    "start": "2967359",
    "end": "2973680"
  },
  {
    "text": "interface via restful APIs we are going to stick with that concept that concept has worked well for us and we are going",
    "start": "2973680",
    "end": "2980079"
  },
  {
    "text": "to stick with that so we are still going to have GDM CDN and all those kind of things we are still going to have edge",
    "start": "2980079",
    "end": "2988079"
  },
  {
    "text": "caching and edge compute and edge side includes we are still going to have common services layer we are going to",
    "start": "2988079",
    "end": "2993599"
  },
  {
    "text": "extend that actually to do for multiple more things router and controller and we are going to still stick with our and",
    "start": "2993599",
    "end": "3000040"
  },
  {
    "text": "metering we are still going to stick with our storage poly cloth however the problems that we are",
    "start": "3000040",
    "end": "3007040"
  },
  {
    "text": "suffering are in these two layers DP below and data services above and that's",
    "start": "3007040",
    "end": "3014079"
  },
  {
    "text": "actually the pain of monolithic architecture so enter microservices so we are planning to",
    "start": "3014079",
    "end": "3020800"
  },
  {
    "text": "evolve this architecture with a microservices approach for data services we are planning to actually break it",
    "start": "3020800",
    "end": "3026640"
  },
  {
    "text": "down into microservices that's already in progress",
    "start": "3026640",
    "end": "3032240"
  },
  {
    "text": "so microservices are going to be centered around data points or different data types or cluster of those services",
    "start": "3032240",
    "end": "3038000"
  },
  {
    "text": "for example for location for aggregation for forecast and lifestyle and so on and",
    "start": "3038000",
    "end": "3043680"
  },
  {
    "text": "these microservices are also going to bring in another important part to this layer and that's the language polyglot",
    "start": "3043680",
    "end": "3049760"
  },
  {
    "text": "so we are going to give technology team flexibility to write even the services in different language or language of",
    "start": "3049760",
    "end": "3055599"
  },
  {
    "text": "their choice so they can write it in Scala Closure Java NodeJS whatever they want",
    "start": "3055599",
    "end": "3062680"
  },
  {
    "text": "we are going to take the same approach or similar approach I should say to micro DP try to break it down DPE that",
    "start": "3062800",
    "end": "3069119"
  },
  {
    "text": "monolithic DP into micro DPS try to bring a services approach microservices",
    "start": "3069119",
    "end": "3075040"
  },
  {
    "text": "approach to that and we are going to do that by doing things like",
    "start": "3075040",
    "end": "3081160"
  },
  {
    "text": "avo planning to do microservices type of model for ingest with a thrift and protobuff so we are going to attempt to",
    "start": "3081160",
    "end": "3087440"
  },
  {
    "text": "break it down for queuing we have already we are already using SQS for events and we are",
    "start": "3087440",
    "end": "3093520"
  },
  {
    "text": "going to extend that usage so the plan is to actually use it more and more for",
    "start": "3093520",
    "end": "3098800"
  },
  {
    "text": "streaming we are looking at Apache Afka that's what we are looking at and then for distribution microservices and",
    "start": "3098800",
    "end": "3106079"
  },
  {
    "text": "language polyglot I already talked about that and then we are going to actually build a service discovery engine around",
    "start": "3106079",
    "end": "3112760"
  },
  {
    "text": "that for storage our plan is to look into Aurora Amazon Aurora and RDS and",
    "start": "3112760",
    "end": "3119839"
  },
  {
    "text": "all those things for relational databases that we have for Drupal and",
    "start": "3119839",
    "end": "3125839"
  },
  {
    "text": "Informatica and all those things so we're going to stick with bring your own storecon supplier as well and for",
    "start": "3125839",
    "end": "3132839"
  },
  {
    "text": "analytics we are going to keep on building on our data lake with parket",
    "start": "3132839",
    "end": "3138160"
  },
  {
    "text": "and S3 combination we are going to rely on spark but we are also looking at Amazon EMR to actually do produce those",
    "start": "3138160",
    "end": "3146720"
  },
  {
    "text": "park files for us so wrapping it up what are our key what",
    "start": "3146720",
    "end": "3154880"
  },
  {
    "start": "3150000",
    "end": "3225000"
  },
  {
    "text": "are the key takeaways here to begin with have an architectural blueprint think through your",
    "start": "3154880",
    "end": "3162440"
  },
  {
    "text": "layers and then keep those layers loosely coupled or",
    "start": "3162440",
    "end": "3167720"
  },
  {
    "text": "decoupled communicate communication for these layers standardize those with a standard protocol like restful interface",
    "start": "3167720",
    "end": "3174000"
  },
  {
    "text": "over HTTP and then try to keep your architectural plan technology",
    "start": "3174000",
    "end": "3179480"
  },
  {
    "text": "agnostic give your technology team flexibility to pick the tools or stores",
    "start": "3179480",
    "end": "3184640"
  },
  {
    "text": "of their choice or language of their choice storage polyglot and language polyglot then if there was one thing and",
    "start": "3184640",
    "end": "3191200"
  },
  {
    "text": "one thing I could tell myself three years back when we actually got into this effort that's this be aware of the",
    "start": "3191200",
    "end": "3197480"
  },
  {
    "text": "monoliths and that's what we are trying to do with microservices another key learning keep",
    "start": "3197480",
    "end": "3204160"
  },
  {
    "text": "caching architecture simple make it DTL driven and of course we touched on always always always budget for load",
    "start": "3204160",
    "end": "3211280"
  },
  {
    "text": "testing and destructive testing with that thank you very",
    "start": "3211280",
    "end": "3217880"
  },
  {
    "text": "much and that's it [Applause]",
    "start": "3217880",
    "end": "3227010"
  }
]