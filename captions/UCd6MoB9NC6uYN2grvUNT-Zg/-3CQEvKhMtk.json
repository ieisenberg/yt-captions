[
  {
    "text": "hi my name is Ashish Mukunda I'm a principal solutions architect and global financial services and with me is John",
    "start": "0",
    "end": "7859"
  },
  {
    "text": "thank you so much for coming this afternoon oh we trust this will be a beneficial time for you all thanks Jesus",
    "start": "7859",
    "end": "14880"
  },
  {
    "text": "hi I'm John Kane I'm business development manager in our financial services organization and I focused",
    "start": "14880",
    "end": "20609"
  },
  {
    "text": "particularly on the capital markets segment and in today's session if we can",
    "start": "20609",
    "end": "25859"
  },
  {
    "text": "kind of leave three things with you it will be first regulatory reporting is a",
    "start": "25859",
    "end": "31710"
  },
  {
    "text": "challenging application in the financial services industry and it's one that's becoming more complex as regulatory",
    "start": "31710",
    "end": "39079"
  },
  {
    "text": "mandates continue to change and evolve just making that more challenging and it's not just a challenge from a",
    "start": "39079",
    "end": "45239"
  },
  {
    "text": "technical perspective in delivering reports or data to the regulator but it's very much creating a process that's",
    "start": "45239",
    "end": "50640"
  },
  {
    "text": "audible repeatable and can be sustained through operational issues no matter how",
    "start": "50640",
    "end": "55710"
  },
  {
    "text": "infrequently occur and so today's session will walk through an architecture that can be generically",
    "start": "55710",
    "end": "61440"
  },
  {
    "text": "used for regulatory challenges in general the ability to take various forms of semi formatted data ingest that",
    "start": "61440",
    "end": "69150"
  },
  {
    "text": "normalize it and then use that for reporting in analytics but for today's session we have a demonstration that's",
    "start": "69150",
    "end": "75540"
  },
  {
    "text": "specifically focused on upcoming regulatory changes that are related to the consolidated audit trail or SCC's",
    "start": "75540",
    "end": "82470"
  },
  {
    "text": "new mandate for broker dealers in the United States to report their transactions to the regulators for",
    "start": "82470",
    "end": "88740"
  },
  {
    "text": "market surveillance and so we'll actually do an implementation of a shishas architecture specifically",
    "start": "88740",
    "end": "94710"
  },
  {
    "text": "related to the challenges that come along with the implementation of cat now",
    "start": "94710",
    "end": "100049"
  },
  {
    "text": "so today's agenda I'll provide a little bit of an overview on the Reg reporting space in general some of the challenges",
    "start": "100049",
    "end": "106920"
  },
  {
    "text": "from an application perspective but also some of the regulatory changes that have occurred particularly in the transaction",
    "start": "106920",
    "end": "113100"
  },
  {
    "text": "reporting space we'll give an overview of the consolidated audit trail it's the basis of today's demonstration and",
    "start": "113100",
    "end": "119340"
  },
  {
    "text": "therefore will give you a sense of not just what it does but sort of the data that's involved and sort of the timing",
    "start": "119340",
    "end": "124979"
  },
  {
    "text": "and the reporting to the regulators as she shall walk you through the architecture involved hitting on the key",
    "start": "124979",
    "end": "131069"
  },
  {
    "text": "design elements how we've implemented the specifically to address these issues and give a demonstration of its actual",
    "start": "131069",
    "end": "137640"
  },
  {
    "text": "implementation in AWS environment the end of the session I'll do a quick recap and then we'll open up the floor",
    "start": "137640",
    "end": "143010"
  },
  {
    "text": "to questions so red reporting covers a",
    "start": "143010",
    "end": "153210"
  },
  {
    "text": "lot of topics in the industry because certainly not only capital market firms have to report to their regulators it",
    "start": "153210",
    "end": "158850"
  },
  {
    "text": "certainly occurs across banking and insurance and by reg reporting we really mean any time that you have to share",
    "start": "158850",
    "end": "164760"
  },
  {
    "text": "data with your regulators now that is a large and complex challenge for a global",
    "start": "164760",
    "end": "171900"
  },
  {
    "text": "financial firm they face off against a universe of regulators all we want data",
    "start": "171900",
    "end": "178050"
  },
  {
    "text": "in different formats at different frequencies using some but not all of the same data elements as there are kind",
    "start": "178050",
    "end": "184470"
  },
  {
    "text": "of fellow regulators and those mandates are constantly changing so there's a cottage industry inside of financial",
    "start": "184470",
    "end": "190620"
  },
  {
    "text": "services just to build solutions that respond to regulatory demands now in our",
    "start": "190620",
    "end": "196410"
  },
  {
    "text": "case we're really looking at the changes that are occurring on the transaction side now we're financial services highly",
    "start": "196410",
    "end": "203220"
  },
  {
    "text": "regulated industry and I don't think sometimes we appreciate how much data we actually share with regulators and now",
    "start": "203220",
    "end": "208590"
  },
  {
    "text": "it's everything from financial statements you know shareholdings your",
    "start": "208590",
    "end": "213630"
  },
  {
    "text": "loan portfolio how it's divided up among consumers even your capital stress tests and those things occur infrequently",
    "start": "213630",
    "end": "219950"
  },
  {
    "text": "require a lot of effort to report and generate and report to the regulators but we're really focused on today is all",
    "start": "219950",
    "end": "227160"
  },
  {
    "text": "the changes that are going on in the transaction reporting space over the last few years regulations have changed dramatically in",
    "start": "227160",
    "end": "233610"
  },
  {
    "text": "the amount of data that we have to share with radiators particularly when comes to trading activity during the day now",
    "start": "233610",
    "end": "238950"
  },
  {
    "text": "we started a dodd-frank and Asthma that we have to report derivatives trades in real time up to the regulators with some",
    "start": "238950",
    "end": "246030"
  },
  {
    "text": "extra information at the end of the day and that was a step and then we method to dramatically increase the amount of",
    "start": "246030",
    "end": "252720"
  },
  {
    "text": "information provided to regulators in amia for not just listed transactions",
    "start": "252720",
    "end": "258030"
  },
  {
    "text": "but also attempt to deal voice conversations as well as related",
    "start": "258030",
    "end": "263220"
  },
  {
    "text": "derivatives transactions and that was you know a 10x increase in the amount of information",
    "start": "263220",
    "end": "268300"
  },
  {
    "text": "shared we now have Finn requiring the reporting of US Treasury transactions to",
    "start": "268300",
    "end": "273970"
  },
  {
    "text": "trace in a real-time basis and next summer for broker-dealers the consolidated audit trail hits which is a",
    "start": "273970",
    "end": "279940"
  },
  {
    "text": "large expansion of how we surveil the equity markets and how much data is actually being reported to regulators",
    "start": "279940",
    "end": "285509"
  },
  {
    "text": "but as these regulatory mandates come out the volume of data that has to be",
    "start": "285509",
    "end": "291310"
  },
  {
    "text": "reported and the number of fields that were reporting are ever increasing and the challenges we have legacy",
    "start": "291310",
    "end": "297069"
  },
  {
    "text": "infrastructure within the industry that's not really designed to handle these types of changes now when we look",
    "start": "297069",
    "end": "303639"
  },
  {
    "text": "at how the industry is grown up it's not surprising that we have things that are highly siloed it used to be that",
    "start": "303639",
    "end": "309400"
  },
  {
    "text": "equities was a completely separate business from fixed income which is separate from your derivatives trading that even if you're in the same",
    "start": "309400",
    "end": "315520"
  },
  {
    "text": "organization you might get that better but so many financial firms are actually you know made out of a consolidation of",
    "start": "315520",
    "end": "322060"
  },
  {
    "text": "multiple financial firms into one and these silos don't have a common format",
    "start": "322060",
    "end": "327120"
  },
  {
    "text": "across infrastructure to represent the data the same way these different ETL",
    "start": "327120",
    "end": "332169"
  },
  {
    "text": "processes and different data dictionaries they don't represent transactions in the same manner from",
    "start": "332169",
    "end": "337300"
  },
  {
    "text": "silo to silo and so when you have to build out an infrastructure report to the regulator you end up having to build",
    "start": "337300",
    "end": "342940"
  },
  {
    "text": "another layer of data stores underneath that to pull in the data that you want transform it and then provide it to the",
    "start": "342940",
    "end": "349719"
  },
  {
    "text": "regulator in the format that they want each of those silos has to scale to the worst-case so your existing data store",
    "start": "349719",
    "end": "356469"
  },
  {
    "text": "and your equities universe has to be as big as the worst equity day but then anything that's going to report both",
    "start": "356469",
    "end": "361810"
  },
  {
    "text": "equities and options has to be big as both of them combined so it's a large amount of transaction infrastructure",
    "start": "361810",
    "end": "367360"
  },
  {
    "text": "that you have to build out for infrequent and occasional use and then because it's not one organization using",
    "start": "367360",
    "end": "373300"
  },
  {
    "text": "that data keeping track of changes upstream that affect downstream reporting to regulators and then having",
    "start": "373300",
    "end": "379389"
  },
  {
    "text": "to fix it later on is an incredible challenge and so while this infrastructure works today and there's a",
    "start": "379389",
    "end": "384729"
  },
  {
    "text": "large amount of the industry focused on doing this correctly on a day to day basis it's terribly complex in somewhat",
    "start": "384729",
    "end": "390130"
  },
  {
    "text": "fragile and as these new regulations come into place it gives us the opportunity to relook at the",
    "start": "390130",
    "end": "395949"
  },
  {
    "text": "infrastructure that we have to report against this and how we might be able to do it better but before we can do it better we have",
    "start": "395949",
    "end": "402970"
  },
  {
    "text": "to appreciate sort of what makes reg reporting hard and what are the key things and an infrastructure necessary to support it so first like everything",
    "start": "402970",
    "end": "411640"
  },
  {
    "text": "in the financial security industry it has to be secure and so it's almost redundant to say anything in financial",
    "start": "411640",
    "end": "417550"
  },
  {
    "text": "services has to be secure because I think we already know that but don't forget this is transaction data and for the firms that actually",
    "start": "417550",
    "end": "423550"
  },
  {
    "text": "trade on the exchanges this is the lifeblood of what they do and it's probably some of the most sensitive information that they have right and",
    "start": "423550",
    "end": "430240"
  },
  {
    "text": "sort of the potential for this to be exposed horrific so any architecture has",
    "start": "430240",
    "end": "435520"
  },
  {
    "text": "to deal with security in a very fundamental layer and it has to be the basis of any good design now the other",
    "start": "435520",
    "end": "441730"
  },
  {
    "text": "thing that's challenging my reg reporting is lineage and when I say lineage I mean the ability to actually",
    "start": "441730",
    "end": "447220"
  },
  {
    "text": "look at a transaction flow from beginning to end and understand how each component ended up getting to the end",
    "start": "447220",
    "end": "453520"
  },
  {
    "text": "result so I need to be able to know what data I got in how I transformed it what",
    "start": "453520",
    "end": "458890"
  },
  {
    "text": "the format and version of it was where it went through the process and what actually got delivered to the regulator and that's not just from an operational",
    "start": "458890",
    "end": "466240"
  },
  {
    "text": "perspective right because you have to have confidence in what you're providing your regulator is accurate so from an audit operations perspective it's",
    "start": "466240",
    "end": "472510"
  },
  {
    "text": "helpful but also when you have that issue with late or missing data or code",
    "start": "472510",
    "end": "477520"
  },
  {
    "text": "change and you don't have all the characteristics you thought you had in the data you need to go able to go back in a point of time and replay that",
    "start": "477520",
    "end": "484540"
  },
  {
    "text": "information to make sure it's accurate as part of the regulatory reporting even if you have to report at a later date",
    "start": "484540",
    "end": "489850"
  },
  {
    "text": "and I've been in situations where I've had to report data two or three years after the fact and you may like well",
    "start": "489850",
    "end": "496060"
  },
  {
    "text": "does anybody care they do regulators ask for information that has been historic",
    "start": "496060",
    "end": "501340"
  },
  {
    "text": "in the past and when you make a commitment to a regulator it has to be correct you have to make sure you can redo it from the other challenges there",
    "start": "501340",
    "end": "510520"
  },
  {
    "text": "are so many data sources inside the financial services industries that represents the same thing but all",
    "start": "510520",
    "end": "516099"
  },
  {
    "text": "slightly different I think recently looked globally there's over 160 different formats for market data feeds",
    "start": "516099",
    "end": "522099"
  },
  {
    "text": "which are just basically bits potent paths but we figure out how to do it at every exchange in every market even when",
    "start": "522099",
    "end": "527860"
  },
  {
    "text": "we have things like fixed an industry standard for transaction information each exchange has its own version",
    "start": "527860",
    "end": "533240"
  },
  {
    "text": "it's on custom tags its own way of representing transactions it's a little bit different from everybody else",
    "start": "533240",
    "end": "539059"
  },
  {
    "text": "so that ETL process has to be well controlled and has to deal with a variety of formats and then the data",
    "start": "539059",
    "end": "545839"
  },
  {
    "text": "we're talking about it's not insignificant we're talking tens if not hundreds of millions of transactions that occur just in equities just under",
    "start": "545839",
    "end": "553519"
  },
  {
    "text": "the current system not including any expansion that's envisioned FINRA talks about having to transact 35 billion",
    "start": "553519",
    "end": "560059"
  },
  {
    "text": "records a day so the sheer volume of data we're talking abouts pretty amazing but what's even more challenging is the",
    "start": "560059",
    "end": "566959"
  },
  {
    "text": "volatility of that data because that reporting has to work on a day when you have typical trading volume we're in a",
    "start": "566959",
    "end": "573439"
  },
  {
    "text": "day like brexit we have 10 times a typical trading volume your systems have to scale and be able to deliver those",
    "start": "573439",
    "end": "579470"
  },
  {
    "text": "reports on time otherwise you have to deal with fine it's one of the few areas",
    "start": "579470",
    "end": "585800"
  },
  {
    "text": "where you get fine for doing your job poorly even if you didn't do anything wrong",
    "start": "585800",
    "end": "591110"
  },
  {
    "text": "so you report data late you miss reporting data regulators do find you",
    "start": "591110",
    "end": "596720"
  },
  {
    "text": "for that there's millions of dollars every year in fines that various regulatory agencies hit people for",
    "start": "596720",
    "end": "601730"
  },
  {
    "text": "reporting transactions late or in completely even if in the transaction data doesn't say anything was wrong the",
    "start": "601730",
    "end": "606980"
  },
  {
    "text": "fact that it's late is a problem and so as we go and implement an architecture",
    "start": "606980",
    "end": "612980"
  },
  {
    "text": "to deal with these things it unlocks other opportunities so we can use reg reporting as an opportunity to unlock",
    "start": "612980",
    "end": "618949"
  },
  {
    "text": "data within the enterprise once we solve these kind of core issues so once you can transform a variety of unstructured",
    "start": "618949",
    "end": "625279"
  },
  {
    "text": "data in multiple formats at scale into a data warehouse or warehouses you can",
    "start": "625279",
    "end": "630860"
  },
  {
    "text": "then open that up to various analytics and whether it's just the support middle office trading P&L portfolio exposures",
    "start": "630860",
    "end": "637579"
  },
  {
    "text": "or you can open it up for machine learning and statistical analytics it allows you from a data officer",
    "start": "637579",
    "end": "643610"
  },
  {
    "text": "perspective not only satisfy your needs for good governance and operations of data but your desire to unlock that for",
    "start": "643610",
    "end": "650779"
  },
  {
    "text": "financial benefit so today's conversation though very much focused on",
    "start": "650779",
    "end": "656689"
  },
  {
    "text": "the consolidated audit trail now existing today there is a system in",
    "start": "656689",
    "end": "662120"
  },
  {
    "text": "place called oats where all firms all broker-dealers to report to FINRA all their",
    "start": "662120",
    "end": "668150"
  },
  {
    "text": "transactions at the end of the day FINRA then looks at them and surveilled them to see if there's any market misconduct",
    "start": "668150",
    "end": "673730"
  },
  {
    "text": "anything that goes on that may influence prices in an untoward manner its violates for law and that's a huge",
    "start": "673730",
    "end": "680420"
  },
  {
    "text": "undertaking it as is its 35 million records that you know FINRA has to process but despite that after the flash",
    "start": "680420",
    "end": "688520"
  },
  {
    "text": "crash and the financial crisis the SEC didn't think that was enough information they didn't think there was enough",
    "start": "688520",
    "end": "693620"
  },
  {
    "text": "information the existing 35 billion roads being processed by the industry today to have comfort that they could",
    "start": "693620",
    "end": "699590"
  },
  {
    "text": "understand what was going on in the industry and so what they've actually done is expanded the scope of what oats",
    "start": "699590",
    "end": "705470"
  },
  {
    "text": "covers so no longer will you just report your equity trades you have to report your equity option trades and how",
    "start": "705470",
    "end": "712250"
  },
  {
    "text": "they're linked together no longer does a broker dealer say this is my trades they have to expose the customers that are",
    "start": "712250",
    "end": "718850"
  },
  {
    "text": "behind those trades and give visibility to the regulator's of who's driving the trades and then if you're a firm that",
    "start": "718850",
    "end": "724670"
  },
  {
    "text": "actually trades on behalf of multiple customers you now have to report how those trades are allocated out to you",
    "start": "724670",
    "end": "729770"
  },
  {
    "text": "individual customers and now all those things have to be linked together so we're talking about now having to tie",
    "start": "729770",
    "end": "735530"
  },
  {
    "text": "what are traditionally two different sets of data equity options data together pulling in customer resource",
    "start": "735530",
    "end": "741590"
  },
  {
    "text": "information into that data set and then make sure it can link up to any allocation data our customers have it's",
    "start": "741590",
    "end": "747980"
  },
  {
    "text": "an overall revamp of what's being done today and of course it's a completely new standard and a completely new format",
    "start": "747980",
    "end": "754940"
  },
  {
    "text": "for the data that has to be reported so it's basically a redo of the entire",
    "start": "754940",
    "end": "761060"
  },
  {
    "text": "reporting infrastructure for oats today and so just to give you a sense of sort of a very simplified overview of what",
    "start": "761060",
    "end": "767690"
  },
  {
    "text": "we're talking about so customers provide orders to their broker dealers either directly or to exchanges the broker",
    "start": "767690",
    "end": "774110"
  },
  {
    "text": "dealers are then responsible for routing those orders the appropriate exchange or to another broker dealer and if using",
    "start": "774110",
    "end": "780350"
  },
  {
    "text": "one of the costs the broker dealers algorithm how that order is actually split up across multiple markets the",
    "start": "780350",
    "end": "785750"
  },
  {
    "text": "state of how that order is maintained and then how it's finally either filled or cancelled for the customer and it has",
    "start": "785750",
    "end": "792050"
  },
  {
    "text": "to maintain that record during the entire day and so as trading activity occurs across multiple customers the",
    "start": "792050",
    "end": "797930"
  },
  {
    "text": "broker dealers collecting up a sense records that indicate the lifecycle each and every single order come the end of",
    "start": "797930",
    "end": "804230"
  },
  {
    "text": "the day the broker-dealer needs to report all that information to the cat",
    "start": "804230",
    "end": "809660"
  },
  {
    "text": "consolidator who's the industry facility is responsible for kind of collecting all the industry across the all the",
    "start": "809660",
    "end": "815810"
  },
  {
    "text": "information across the industry and providing it for kind of regulatory review now the part of the demonstration",
    "start": "815810",
    "end": "821630"
  },
  {
    "text": "we're gonna focus on today is this end of day process where a broker dealer takes their large red set of fixed",
    "start": "821630",
    "end": "828860"
  },
  {
    "text": "transactions and we're going to assume for the sake of this demonstration virtually all our post trade informations and fix this industry",
    "start": "828860",
    "end": "834860"
  },
  {
    "text": "standard format broadly used to report this type of information that we're",
    "start": "834860",
    "end": "840019"
  },
  {
    "text": "going to ingest it into our infrastructure normalize it and then report out in a new format that meets",
    "start": "840019",
    "end": "848329"
  },
  {
    "text": "the new consolidated audit trail specification as part of that we're going to do that transformation from",
    "start": "848329",
    "end": "853550"
  },
  {
    "text": "what's fixed to the new standard which is JSON but we're also going to have to link those message states together and",
    "start": "853550",
    "end": "859310"
  },
  {
    "text": "see how they tie together in an end-to-end solution and just to kind of give you a sense for sort of the type of",
    "start": "859310",
    "end": "864529"
  },
  {
    "text": "data that we're working with if you're not familiar fixed somewhat industry standard supposed to be kind of human",
    "start": "864529",
    "end": "870380"
  },
  {
    "text": "readable and if you do fix for a while you can kind of get it but essentially it's a tag value format where each of",
    "start": "870380",
    "end": "875959"
  },
  {
    "text": "the initial tags is actually you look up in the data dictionary that tells you a little bit about the transaction each",
    "start": "875959",
    "end": "881240"
  },
  {
    "text": "transaction type has its own messages and tags and then we have to take that for a series of exchanges all of which",
    "start": "881240",
    "end": "887690"
  },
  {
    "text": "use slightly different versions of fix tie those together into an audit trail and then convert it into JSON which is",
    "start": "887690",
    "end": "893569"
  },
  {
    "text": "actually the expected format for the consolidated audit trail reporting and so in the demonstration as she's about",
    "start": "893569",
    "end": "900529"
  },
  {
    "text": "to walk you through we're going to build an infrastructure that takes tens if not hundreds of millions of fixed messages",
    "start": "900529",
    "end": "906199"
  },
  {
    "text": "links them together converts them into an optimized format and then allows us",
    "start": "906199",
    "end": "912110"
  },
  {
    "text": "to generate reports that we could send to the regulators and that's all gonna be done in a secure manner it's going to",
    "start": "912110",
    "end": "917899"
  },
  {
    "text": "scale to meet demand and it's going to be cost effective so sheesh if you could talk us through",
    "start": "917899",
    "end": "924199"
  },
  {
    "text": "the magic I'd appreciate it a cat reporting is definitely a",
    "start": "924199",
    "end": "931189"
  },
  {
    "text": "challenge but with the fit for purpose architecture and proper design we can",
    "start": "931189",
    "end": "937129"
  },
  {
    "text": "not only meet the challenges of the cat reporting but also used the data the",
    "start": "937129",
    "end": "943309"
  },
  {
    "text": "processing for additional benefits like doing analytics on top of that so cat",
    "start": "943309",
    "end": "950600"
  },
  {
    "text": "reporting is basically three stages you've got ingestion you've got transformation and cat",
    "start": "950600",
    "end": "958579"
  },
  {
    "text": "reporting the reporting and analytics to the right so you see the on the left you have the orange ingestion blue you have",
    "start": "958579",
    "end": "964399"
  },
  {
    "text": "the transformation which takes the data and puts into the optimized format and then the dual use cases that you see",
    "start": "964399",
    "end": "971870"
  },
  {
    "text": "there using the same kind of architecture cat the architecture for",
    "start": "971870",
    "end": "979670"
  },
  {
    "text": "cat reporting is going to be more than likely the difference between success",
    "start": "979670",
    "end": "986480"
  },
  {
    "text": "and failure designing it properly is essential if you have an architecture",
    "start": "986480",
    "end": "992779"
  },
  {
    "text": "that cannot scale or even architecture that is not secure you will fail at this added this effort",
    "start": "992779",
    "end": "998529"
  },
  {
    "text": "broadly speaking you've got an architecture that processes over a",
    "start": "998529",
    "end": "1003939"
  },
  {
    "text": "billion messages that is over three times the volume of the traditional",
    "start": "1003939",
    "end": "1009240"
  },
  {
    "text": "broker-dealer firm high volume Broker Dealer firm has to deal with this",
    "start": "1009240",
    "end": "1014889"
  },
  {
    "text": "architecture is designed for security it's designed to handle lineage and",
    "start": "1014889",
    "end": "1021569"
  },
  {
    "text": "we're gonna look at these components in a little bit more detail the core",
    "start": "1021569",
    "end": "1027699"
  },
  {
    "text": "services that are being used here you see two sets of icons one on the left and one on the right we'll talk about",
    "start": "1027699",
    "end": "1034720"
  },
  {
    "text": "the icons on the Left first this is not the universe of services that are used",
    "start": "1034720",
    "end": "1041409"
  },
  {
    "text": "in this architecture for instance we don't have here sqs or lambda you might",
    "start": "1041409",
    "end": "1046720"
  },
  {
    "text": "use that for automation you might use an RDS or relational database service for",
    "start": "1046720",
    "end": "1052840"
  },
  {
    "text": "persisting things you might use dynamodb those are peripheral not essential but",
    "start": "1052840",
    "end": "1058690"
  },
  {
    "text": "these core components are what you'll need to construct this architecture for the reporting so let's",
    "start": "1058690",
    "end": "1065260"
  },
  {
    "text": "start start off on the left you have Direct Connect that's your private connectivity from your on-premises",
    "start": "1065260",
    "end": "1071380"
  },
  {
    "text": "environment to the AWS cloud to the right you have s3 and glacier that forms",
    "start": "1071380",
    "end": "1079299"
  },
  {
    "text": "our core storage and the cloud it provides a highly available secure",
    "start": "1079299",
    "end": "1085840"
  },
  {
    "text": "durable storage s3 is highly redundant",
    "start": "1085840",
    "end": "1091660"
  },
  {
    "text": "glacier can be used for worm storage write once read money and then we use",
    "start": "1091660",
    "end": "1097120"
  },
  {
    "text": "cloud watch and cloud trail for monitoring the environment cloud watch",
    "start": "1097120",
    "end": "1102660"
  },
  {
    "text": "you can have the monitoring of operational and performance metrics and then cloud trail gives you that API",
    "start": "1102660",
    "end": "1110080"
  },
  {
    "text": "level detail on your entire infrastructure you have AWS kms helps",
    "start": "1110080",
    "end": "1117190"
  },
  {
    "text": "keep track of our keys encryption keys and then EMR the manage to Hadoop",
    "start": "1117190",
    "end": "1122350"
  },
  {
    "text": "platform for doing a bunch of the heavy lifting and batch processing amazon",
    "start": "1122350",
    "end": "1129130"
  },
  {
    "text": "athena and quick site are then used in combination to present data visually so",
    "start": "1129130",
    "end": "1135040"
  },
  {
    "text": "athena is the interactive query service service environment and then that in",
    "start": "1135040",
    "end": "1140650"
  },
  {
    "text": "combination with quick site lets you visualize the data beyond the needs of reporting to the right of this you have",
    "start": "1140650",
    "end": "1148059"
  },
  {
    "text": "a blue icon heard you've probably not heard of it it's no pun intended but",
    "start": "1148059",
    "end": "1153970"
  },
  {
    "text": "it's an icon that represents the open source vendor project it keeps track of",
    "start": "1153970",
    "end": "1162070"
  },
  {
    "text": "data in a unified format it's a data cataloguing and management platform now",
    "start": "1162070",
    "end": "1169210"
  },
  {
    "text": "in this architecture security as John mentioned is absolutely essential this",
    "start": "1169210",
    "end": "1175390"
  },
  {
    "text": "architecture is designed to handle highly sensitive data securely we",
    "start": "1175390",
    "end": "1182980"
  },
  {
    "text": "realize the importance of the security right from the very beginning of the",
    "start": "1182980",
    "end": "1189309"
  },
  {
    "text": "data flow all the way to the end in addition to security of the data this architecture will also enable you",
    "start": "1189309",
    "end": "1196270"
  },
  {
    "text": "to have complete control of the data by bringing your own keys from on-premises",
    "start": "1196270",
    "end": "1202150"
  },
  {
    "text": "and bringing them to the aid of this environment you can use your on-premises hardware security module to create the",
    "start": "1202150",
    "end": "1208660"
  },
  {
    "text": "keys and then you use those master keys to create data encryption keys that are",
    "start": "1208660",
    "end": "1214870"
  },
  {
    "text": "then used for encrypting the data and then using standard AWS services like I",
    "start": "1214870",
    "end": "1219970"
  },
  {
    "text": "am you can grant granular access control to the to the components that both use",
    "start": "1219970",
    "end": "1227080"
  },
  {
    "text": "and transform this data throughout the entire process not only do you have the",
    "start": "1227080",
    "end": "1232660"
  },
  {
    "text": "controls for each of those components but then you can track the auditing of those controls throughout the process so",
    "start": "1232660",
    "end": "1241440"
  },
  {
    "text": "security if you look at security for of the cat reporting pipeline it's essentially three main components you",
    "start": "1241530",
    "end": "1248290"
  },
  {
    "text": "have network isolation you have encryption and auditing in terms of",
    "start": "1248290",
    "end": "1253720"
  },
  {
    "text": "network isolation you're quite familiar with the standard constructs of the direct connect as I talked about a",
    "start": "1253720",
    "end": "1259870"
  },
  {
    "text": "little bit earlier you can also use an IPSec VPN tunnel over to reconnect and in terms of encryption you have both the",
    "start": "1259870",
    "end": "1268230"
  },
  {
    "text": "data and transit and data at rest now for data in transit we use the transport",
    "start": "1268230",
    "end": "1273820"
  },
  {
    "text": "layer security cryptographic protocol for securing all communications for data",
    "start": "1273820",
    "end": "1280480"
  },
  {
    "text": "at rest you have a couple of options you can either encrypt the data on the client side or you could encrypt the",
    "start": "1280480",
    "end": "1287230"
  },
  {
    "text": "data on the server side for client-side encryption you would be responsible for creating the 256 bit data key and then",
    "start": "1287230",
    "end": "1295000"
  },
  {
    "text": "supplying the key encryption key for all the services that you'll use in AWS for server-side encryption AWS will encrypt",
    "start": "1295000",
    "end": "1303970"
  },
  {
    "text": "the data for you but there's multiple ways in which you can manage the encryption of that data so you can have",
    "start": "1303970",
    "end": "1311560"
  },
  {
    "text": "server-side encryption that is managed by s3 for instance in that case s3 will",
    "start": "1311560",
    "end": "1317350"
  },
  {
    "text": "manage both your master key as well as your data key you can have server-side encryption with kms where you provide",
    "start": "1317350",
    "end": "1324130"
  },
  {
    "text": "the master key and then all the rotation and management of the keys was handled by kms it creates the data key based off of",
    "start": "1324130",
    "end": "1332590"
  },
  {
    "text": "your master key and the third option is you can manage all the keys yourself so",
    "start": "1332590",
    "end": "1338740"
  },
  {
    "text": "customer managed key but server-side encryption and all three options should be very clear AWS is actually encrypting",
    "start": "1338740",
    "end": "1346240"
  },
  {
    "text": "the data for you so you don't actually have to write any encryption or decryption programs auditing is",
    "start": "1346240",
    "end": "1354039"
  },
  {
    "text": "essential here so Cloud trail gives you that API level calls throughout your",
    "start": "1354039",
    "end": "1359620"
  },
  {
    "text": "infrastructure you can track who did what when based on the auditing of the",
    "start": "1359620",
    "end": "1366840"
  },
  {
    "text": "kms which keys were used when they were use at what process and cloud watch you",
    "start": "1366840",
    "end": "1373330"
  },
  {
    "text": "can set up alarms that trigger off certain actions and then you can create lambda functions to do further",
    "start": "1373330",
    "end": "1379210"
  },
  {
    "text": "processing so this is in essence this is the security that you need to build to",
    "start": "1379210",
    "end": "1385270"
  },
  {
    "text": "provide that platform for reporting now we're going to look at a little demo of",
    "start": "1385270",
    "end": "1390970"
  },
  {
    "text": "how to ensure that your data at rest is encrypted it's one thing to just check",
    "start": "1390970",
    "end": "1397690"
  },
  {
    "text": "off a box and say I want encryption on the server side it's another thing to ensure that every datum that goes into",
    "start": "1397690",
    "end": "1405070"
  },
  {
    "text": "that platform is actually encrypted so what this is going to do is we're gonna create a bucket we're gonna set up",
    "start": "1405070",
    "end": "1413130"
  },
  {
    "text": "encryption for it and then we're going to try and break that system I'm gonna",
    "start": "1413130",
    "end": "1418750"
  },
  {
    "text": "try and upload a file that is not encrypted and see what happens",
    "start": "1418750",
    "end": "1423750"
  },
  {
    "text": "we're gonna create a bucket here",
    "start": "1427160",
    "end": "1430690"
  },
  {
    "text": "turn on versioning and by default we're gonna turn on encryption which is AES",
    "start": "1443220",
    "end": "1449680"
  },
  {
    "text": "256-bit and then we're gonna create a",
    "start": "1449680",
    "end": "1458920"
  },
  {
    "text": "bucket policy I had something in memory copied and pasted that here you're gonna",
    "start": "1458920",
    "end": "1464620"
  },
  {
    "text": "see it's just a few lines of code and",
    "start": "1464620",
    "end": "1471640"
  },
  {
    "text": "what you're seeing here is that this policy is gonna deny any put object",
    "start": "1471640",
    "end": "1476880"
  },
  {
    "text": "unless it has that header which says it's encrypted all right so the idea is",
    "start": "1476880",
    "end": "1482290"
  },
  {
    "text": "you shouldn't be able to put anything into a three into that bucket unless you have encrypted that data so I'm gonna",
    "start": "1482290",
    "end": "1488410"
  },
  {
    "text": "try now an unencrypted file let's see what happens you can see at the bottom",
    "start": "1488410",
    "end": "1498810"
  },
  {
    "text": "encryption none but we want encryption on the bucket so what happens you get an",
    "start": "1498810",
    "end": "1504520"
  },
  {
    "text": "error yes we will not allow that based on that bucket policy but what if you tried that from a command line remember",
    "start": "1504520",
    "end": "1510550"
  },
  {
    "text": "you could go from a SDK or a command line what happens does that change so",
    "start": "1510550",
    "end": "1516460"
  },
  {
    "text": "I'm gonna try and copy something from my drive same thing access denied because you did not encrypt it you do not",
    "start": "1516460",
    "end": "1523270"
  },
  {
    "text": "provide that header now we're going to encrypt the file",
    "start": "1523270",
    "end": "1527970"
  },
  {
    "text": "there's your Amazon s3 master key encryption so as three is gonna manage",
    "start": "1532379",
    "end": "1537869"
  },
  {
    "text": "the encrypting of that data file as you upload it and there it is so with a few",
    "start": "1537869",
    "end": "1546809"
  },
  {
    "text": "point and clicks you're able to set up an environment that guarantees that",
    "start": "1546809",
    "end": "1552879"
  },
  {
    "text": "every data in that bucket is encrypted that's very important now a secure",
    "start": "1552879",
    "end": "1562539"
  },
  {
    "text": "framework is solidified with proper lineage what is data lineage john",
    "start": "1562539",
    "end": "1568659"
  },
  {
    "text": "mentioned it briefly broadly data lineage is also known as data provenance",
    "start": "1568659",
    "end": "1574200"
  },
  {
    "text": "it is tracking of the data from the origin from the source all the way to",
    "start": "1574200",
    "end": "1579609"
  },
  {
    "text": "the end where it is consumed it answers questions like where was this data",
    "start": "1579609",
    "end": "1586379"
  },
  {
    "text": "originated who touched the data when was it touched what changes were made if any",
    "start": "1586379",
    "end": "1593349"
  },
  {
    "text": "of you've worked with the word document with review changes you know exactly what i'm talking about you have a whole",
    "start": "1593349",
    "end": "1598599"
  },
  {
    "text": "bunch of folks looking at that document and you review the changes you know exactly what was changed who changed it",
    "start": "1598599",
    "end": "1605019"
  },
  {
    "text": "what time what comments they made what they added what they deleted well we're just talking about the exact same thing",
    "start": "1605019",
    "end": "1610179"
  },
  {
    "text": "in terms of data lineage except it's talking about multiple data sets across",
    "start": "1610179",
    "end": "1615519"
  },
  {
    "text": "an entire business so that can be challenging but capturing lineage has",
    "start": "1615519",
    "end": "1622119"
  },
  {
    "text": "many benefits you could ascertain data quality because of the lineage you could",
    "start": "1622119",
    "end": "1627489"
  },
  {
    "text": "track the source of errors you can do root cause analysis you can also replace",
    "start": "1627489",
    "end": "1633339"
  },
  {
    "text": "in Aereo's as John talked about the regulators might want you to go back to a certain version of data and replay",
    "start": "1633339",
    "end": "1638859"
  },
  {
    "text": "that and then it allows you to do backward tracing and forward tracing",
    "start": "1638859",
    "end": "1644229"
  },
  {
    "text": "backward tracing is looking at an operation and looking at all the inputs that went into it forward tracing is the",
    "start": "1644229",
    "end": "1651820"
  },
  {
    "text": "converse of that looking at an operation and looking at the outputs that come out of that particular operation now lineage",
    "start": "1651820",
    "end": "1659109"
  },
  {
    "text": "when you talk about cat reporting needs to be addressed at two levels first of all the data itself the data",
    "start": "1659109",
    "end": "1666130"
  },
  {
    "text": "and the jobs themselves so the versions of the data the schemas what jobs ran",
    "start": "1666130",
    "end": "1672220"
  },
  {
    "text": "where that's important that's one level of tracking lineage and the second level",
    "start": "1672220",
    "end": "1677890"
  },
  {
    "text": "from a regulatory perspective and an audit perspective is the tracking of an order with all this lifecycle events",
    "start": "1677890",
    "end": "1685740"
  },
  {
    "text": "what we're using here is as I mentioned heard the FINRA open source project for",
    "start": "1685740",
    "end": "1692020"
  },
  {
    "text": "doing the lineage you could also use glue which automatically can't catalogs your s3 data herd is open source it's in",
    "start": "1692020",
    "end": "1702220"
  },
  {
    "text": "github it provides a unified data catalog it captures both audit and data",
    "start": "1702220",
    "end": "1709780"
  },
  {
    "text": "lineage information to fulfill the requirements of a highly regulated business now the unified data catalog",
    "start": "1709780",
    "end": "1718870"
  },
  {
    "text": "provides restful api s so that you can have publishers and consumers create new",
    "start": "1718870",
    "end": "1727900"
  },
  {
    "text": "schemas new versions and register with heard it decouples the storage from",
    "start": "1727900",
    "end": "1734140"
  },
  {
    "text": "processing and what that means is that in a large environment you could have a variety of tools you can have a",
    "start": "1734140",
    "end": "1740230"
  },
  {
    "text": "heterogenous environment of different storage platforms different processing tools you don't need to tweak each one",
    "start": "1740230",
    "end": "1746860"
  },
  {
    "text": "you could have a centralized way of managing all that data across all those platforms by using herd it helps you",
    "start": "1746860",
    "end": "1755980"
  },
  {
    "text": "keep track of not only the most current version of the data but also as of a certain date herd also enables you to",
    "start": "1755980",
    "end": "1763990"
  },
  {
    "text": "have centralized storage policy so for instance you might create a policy that",
    "start": "1763990",
    "end": "1770050"
  },
  {
    "text": "says this particular bucket will have data for such a certain you know such and such a time and after that time it",
    "start": "1770050",
    "end": "1776830"
  },
  {
    "text": "needs to move to infrequent access or to cold storage you could put it off into glacier now think of the challenge of",
    "start": "1776830",
    "end": "1785250"
  },
  {
    "text": "maintaining this kind of lineage in a large distributed this diverse",
    "start": "1785250",
    "end": "1791770"
  },
  {
    "text": "environment who has used your data when did they use it the traditional way of doing that is to",
    "start": "1791770",
    "end": "1798720"
  },
  {
    "text": "create multiple databases and then provide specific access custom",
    "start": "1798720",
    "end": "1805710"
  },
  {
    "text": "permissions and access permissions for restricting or granting access to certain databases certain tables by line",
    "start": "1805710",
    "end": "1812640"
  },
  {
    "text": "of business and business unit that gets very cumbersome with her you can keep",
    "start": "1812640",
    "end": "1817860"
  },
  {
    "text": "track not only of your data and jobs but also your publishers and consumers who's using that data and when you start",
    "start": "1817860",
    "end": "1825360"
  },
  {
    "text": "expanding your utilization of all the data that you're processing you're going",
    "start": "1825360",
    "end": "1830670"
  },
  {
    "text": "to start getting into more and more de ribes datasets and Hurd can help you keep track of that as well another",
    "start": "1830670",
    "end": "1839880"
  },
  {
    "text": "important feature of Hurd is it's shared meta store when EMR clusters start up",
    "start": "1839880",
    "end": "1846600"
  },
  {
    "text": "and they initialize sometimes it could take a few minutes especially if you have a large number of partitions and",
    "start": "1846600",
    "end": "1852900"
  },
  {
    "text": "you it's not unheard of to talk about millions of partitions with the shared",
    "start": "1852900",
    "end": "1859020"
  },
  {
    "text": "meta store in hive EMR clusters can point to them and come up rapidly and",
    "start": "1859020",
    "end": "1866630"
  },
  {
    "text": "when new versions of data are registered new schemas are registered the meta",
    "start": "1866630",
    "end": "1872940"
  },
  {
    "text": "store is automatically notified by sqs notification so EMR clusters pointing to",
    "start": "1872940",
    "end": "1878940"
  },
  {
    "text": "the shared meta store can start up rapidly the other advantage of using",
    "start": "1878940",
    "end": "1884400"
  },
  {
    "text": "this meta store is that I'm going to show you now in a minute the demo you actually define your domain objects you",
    "start": "1884400",
    "end": "1892020"
  },
  {
    "text": "define your data jobs in a simple XML format Hurd can then generate the DDL",
    "start": "1892020",
    "end": "1897960"
  },
  {
    "text": "for you now the DDL can be used in a variety of ways across your business let's take a look at herd in action what",
    "start": "1897960",
    "end": "1905880"
  },
  {
    "text": "you're going to see is a bunch of XML files that define the data and you can",
    "start": "1905880",
    "end": "1913020"
  },
  {
    "text": "imagine across a large business several business units creating tons of data",
    "start": "1913020",
    "end": "1919920"
  },
  {
    "text": "jobs they are responsible for defining that data job that domain what the data",
    "start": "1919920",
    "end": "1925200"
  },
  {
    "text": "is how much data is there was the size the role count and it's a flexible and",
    "start": "1925200",
    "end": "1931280"
  },
  {
    "text": "customizable API you can add your own attributes to it we're gonna send this using the restful api call over to heard",
    "start": "1931280",
    "end": "1939530"
  },
  {
    "text": "and heard will then persist that data and then what we're gonna see is we will",
    "start": "1939530",
    "end": "1944630"
  },
  {
    "text": "be able to query haven't done the query in the demo but you can just picture and you can query that database a simple",
    "start": "1944630",
    "end": "1950210"
  },
  {
    "text": "Postgres database and run queries across your entire lineage of a certain job or",
    "start": "1950210",
    "end": "1958100"
  },
  {
    "text": "a certain counterparty or a certain symbol or a certain date you can run all",
    "start": "1958100",
    "end": "1963530"
  },
  {
    "text": "kinds of queries against that so let's take a quick look at this demo here",
    "start": "1963530",
    "end": "1969230"
  },
  {
    "text": "we're defining a data job data definition of the in a certain namespace",
    "start": "1969230",
    "end": "1977510"
  },
  {
    "text": "this could be your line of business and who is providing that data in this case",
    "start": "1977510",
    "end": "1983210"
  },
  {
    "text": "exchange then you can add some further attributes to that job definition here",
    "start": "1983210",
    "end": "1992540"
  },
  {
    "text": "we're partitioning on market code just some random name you could partition on",
    "start": "1992540",
    "end": "1997940"
  },
  {
    "text": "something else it shows you the path of where the data is it's a gzipped file what the size is a little over 40 gig",
    "start": "1997940",
    "end": "2005200"
  },
  {
    "text": "and over 18 million rows that's the job we're looking at this is you receive",
    "start": "2005200",
    "end": "2010720"
  },
  {
    "text": "this fixed file and now you're gonna process it here we're sending it through a restful api call and we're piping it",
    "start": "2010720",
    "end": "2018370"
  },
  {
    "text": "through xml so we can look at the output that's generated when the data is inserted into heard the response comes",
    "start": "2018370",
    "end": "2027010"
  },
  {
    "text": "back you have all the details of the entire job and as i mentioned earlier",
    "start": "2027010",
    "end": "2032440"
  },
  {
    "text": "this is customizable you can add further attributes that your line of business might be interested in moving over from",
    "start": "2032440",
    "end": "2040810"
  },
  {
    "text": "this request that sent over to heard in",
    "start": "2040810",
    "end": "2045850"
  },
  {
    "text": "restful api we're gonna look on the server side this is the master node and Hurd has a post-grad sequel database so",
    "start": "2045850",
    "end": "2054460"
  },
  {
    "text": "we're just going to look at the data we sent over via this Russell API and there",
    "start": "2054460",
    "end": "2059919"
  },
  {
    "text": "you have it it tracks it along with the file size the attributes",
    "start": "2059920",
    "end": "2065770"
  },
  {
    "text": "that you mentioned the line count and heard has a plethora of tables for job",
    "start": "2065770",
    "end": "2074520"
  },
  {
    "text": "for data for the business domain and you can certainly add as much detail as you",
    "start": "2074520",
    "end": "2081310"
  },
  {
    "text": "like so the purpose of this demonstration what I want you to take from this demo is that it is very easy",
    "start": "2081310",
    "end": "2087158"
  },
  {
    "text": "for your entire business regardless of what the platform is in terms of storage of data regardless of the tools that",
    "start": "2087159",
    "end": "2093849"
  },
  {
    "text": "you're using for processing the data to keep track and and have the lineage across all your data now that we've",
    "start": "2093849",
    "end": "2102730"
  },
  {
    "text": "talked about the foundational components of security and lineage let's go through the data flow from left to right which",
    "start": "2102730",
    "end": "2109720"
  },
  {
    "text": "is ingestion transformation and reporting in analytics this part of the",
    "start": "2109720",
    "end": "2118720"
  },
  {
    "text": "architecture is responsible for ingesting raw fix messages raw messages",
    "start": "2118720",
    "end": "2124329"
  },
  {
    "text": "that come from a bunch of counterparties perhaps or it comes from the exchanges",
    "start": "2124329",
    "end": "2130470"
  },
  {
    "text": "company data specific data sets and it deals with getting this data from on-premises in to the AWS environment s3",
    "start": "2130470",
    "end": "2141730"
  },
  {
    "text": "here serves as the data lake it is the source of truth you can automate end-of-day uploads via a cron job and do",
    "start": "2141730",
    "end": "2149829"
  },
  {
    "text": "an s3 multi-part upload for best performance glaciar can be used for",
    "start": "2149829",
    "end": "2155530"
  },
  {
    "text": "vault locks creating vault locks to be in compliance with SEC rules 17a for if",
    "start": "2155530",
    "end": "2161920"
  },
  {
    "text": "you're not familiar with that that rule basically outlines the necessity of retention and indexing and accessing the",
    "start": "2161920",
    "end": "2168940"
  },
  {
    "text": "data for immediate access you have to maintain the data for two years for non",
    "start": "2168940",
    "end": "2174640"
  },
  {
    "text": "immediate access you need to maintain it for a minimum of six years and then duplicates have to maintain the same",
    "start": "2174640",
    "end": "2181270"
  },
  {
    "text": "time frame now you can have a schedule job that takes data from s3 buckets and",
    "start": "2181270",
    "end": "2187839"
  },
  {
    "text": "moves into glacier to be in compliance with SEC rule 17 a four",
    "start": "2187839",
    "end": "2194780"
  },
  {
    "text": "so here's I'm gonna give you a demo on ingestion how that's made easy if you use the AWS SDK you know it's a matter",
    "start": "2194780",
    "end": "2202200"
  },
  {
    "text": "of creating a simple Java program matter of minutes and you can set up your",
    "start": "2202200",
    "end": "2208099"
  },
  {
    "text": "ingestion there's simple process you",
    "start": "2208099",
    "end": "2213480"
  },
  {
    "text": "initialize it you create the multi parts and then you complete it that code was all that was required for the multi part",
    "start": "2213480",
    "end": "2219990"
  },
  {
    "text": "here you're seeing the files coming in we're going to put it into the bucket of reinvent 27 FS v 3 or 2 and we're gonna",
    "start": "2219990",
    "end": "2228359"
  },
  {
    "text": "take that fixed data and put it in there so now we've got a pom.xml using maven",
    "start": "2228359",
    "end": "2233970"
  },
  {
    "text": "to execute a job a job for s3 upload and",
    "start": "2233970",
    "end": "2242540"
  },
  {
    "text": "this is an asynchronous process each of those upload parts are going asynchronously with it via thread and",
    "start": "2242540",
    "end": "2250760"
  },
  {
    "text": "there you have it so what this is showing is that this particular",
    "start": "2250760",
    "end": "2257150"
  },
  {
    "text": "ingestion process can be made very simple a simple Java program or a pro",
    "start": "2257150",
    "end": "2262170"
  },
  {
    "text": "language that you prefer and you you can automate this end of day job putting",
    "start": "2262170",
    "end": "2267839"
  },
  {
    "text": "into a cron job and your data it gets gets into AWS very easily but once the",
    "start": "2267839",
    "end": "2274859"
  },
  {
    "text": "data is in the s3 data lake we've got to transform it so this portion is going to",
    "start": "2274859",
    "end": "2280770"
  },
  {
    "text": "focus on taking that data that's sitting in s3 as a source of truth and then",
    "start": "2280770",
    "end": "2286640"
  },
  {
    "text": "converting it into the format that is optimized for further processing here",
    "start": "2286640",
    "end": "2292710"
  },
  {
    "text": "we're going to use EMR to go directly against s3 the goal here is to optimize",
    "start": "2292710",
    "end": "2300180"
  },
  {
    "text": "the data not only for reporting but also for analytics we're going to convert the data from fixed to park' using EMR",
    "start": "2300180",
    "end": "2307349"
  },
  {
    "text": "clusters this is the heart of what makes this architecture highly efficient and",
    "start": "2307349",
    "end": "2313020"
  },
  {
    "text": "able to scale to billions of messages s3 serves as the data lake and because it",
    "start": "2313020",
    "end": "2318960"
  },
  {
    "text": "is highly redundant it's infinitely scalable and available for massive parallel access this is a difference",
    "start": "2318960",
    "end": "2326400"
  },
  {
    "text": "between having to copy data in HDFS and creating replicas everywhere you're able to access all the",
    "start": "2326400",
    "end": "2333230"
  },
  {
    "text": "EMR data that is sitting on s3 with multiple clusters at the same time you",
    "start": "2333230",
    "end": "2341119"
  },
  {
    "text": "have to create multiple perp of a fit for purpose EMR clusters to perform this",
    "start": "2341119",
    "end": "2346520"
  },
  {
    "text": "job EMR clusters are used for this transformation and either a transient",
    "start": "2346520",
    "end": "2353090"
  },
  {
    "text": "way or in a permanent way so you could have transient EMR clusters at the end of day when your job is finished and the",
    "start": "2353090",
    "end": "2359150"
  },
  {
    "text": "day billing if you can shut down the EMR cluster or you could have an ongoing permanent EMR cluster that is awaiting",
    "start": "2359150",
    "end": "2366380"
  },
  {
    "text": "new jobs you can also have custom security configurations for your individual EMR clusters this could be",
    "start": "2366380",
    "end": "2374420"
  },
  {
    "text": "based on your business unit requirements and then at the heart of an EMR cluster",
    "start": "2374420",
    "end": "2380000"
  },
  {
    "text": "you've got core nodes and task nodes core nodes and task nodes can also be",
    "start": "2380000",
    "end": "2385070"
  },
  {
    "text": "customized based on the workload so you might have a compute intensive job and",
    "start": "2385070",
    "end": "2391520"
  },
  {
    "text": "you might use a C family of ec2 instances for that or you might have a spark job spark jobs usually take a lot",
    "start": "2391520",
    "end": "2398420"
  },
  {
    "text": "of memory so with the SPARC job you might customize your EMR cluster to use some of those nodes as an our family of",
    "start": "2398420",
    "end": "2406580"
  },
  {
    "text": "ec2 instance in addition to that you can auto scale the EMR cluster John talked",
    "start": "2406580",
    "end": "2412880"
  },
  {
    "text": "about the the fact that you have to be able to scale perhaps in a market event brexit and you have 10x volume or a",
    "start": "2412880",
    "end": "2420230"
  },
  {
    "text": "hundred X volume you don't anticipate that you don't want to have that infrastructure upfront EMR clusters can",
    "start": "2420230",
    "end": "2425630"
  },
  {
    "text": "auto scale scale out and then the jobs apache hives spark presto they can",
    "start": "2425630",
    "end": "2431510"
  },
  {
    "text": "automatically take advantage of the additional capacity not only do you",
    "start": "2431510",
    "end": "2436940"
  },
  {
    "text": "scale out but then you have to scale back in because you don't want to leave that cluster running so the same kind of",
    "start": "2436940",
    "end": "2442790"
  },
  {
    "text": "policy that you use for scaling out you can create policies to scale back in and",
    "start": "2442790",
    "end": "2448400"
  },
  {
    "text": "also you can use spot instances to lower your cost so we've got this data now by",
    "start": "2448400",
    "end": "2454490"
  },
  {
    "text": "using Amazon EMR converted fixed to park' and that's what it looks like",
    "start": "2454490",
    "end": "2461630"
  },
  {
    "text": "the performance of Parque 488 thousands records scanned per second compared to",
    "start": "2461630",
    "end": "2468980"
  },
  {
    "text": "the others now you don't have to use Parque you can use any other compressed format mileage may vary and often varies",
    "start": "2468980",
    "end": "2476630"
  },
  {
    "text": "between reads and writes so you have to choose the appropriate format but the point is you take this cluster and",
    "start": "2476630",
    "end": "2482290"
  },
  {
    "text": "convert the data into a format that can be used later now we're going to look at",
    "start": "2482290",
    "end": "2487820"
  },
  {
    "text": "a demo that transforms fixed messages to JSON and to parque you can see that this",
    "start": "2487820",
    "end": "2494330"
  },
  {
    "text": "is relatively simple here have the same pom.xml with a",
    "start": "2494330",
    "end": "2500510"
  },
  {
    "text": "different execution goal in this case the input is a sample fixed message",
    "start": "2500510",
    "end": "2505880"
  },
  {
    "text": "multiple fixed messages and then the output is a JSON format so we're going",
    "start": "2505880",
    "end": "2516080"
  },
  {
    "text": "to look at this file and you can see that it is binary see all those control",
    "start": "2516080",
    "end": "2523580"
  },
  {
    "text": "characters in there I'm going to take this and convert that to JSON",
    "start": "2523580",
    "end": "2529060"
  },
  {
    "text": "so we execute this and you can picture doing this in an email cluster it could be a spark job in this case it's a",
    "start": "2534710",
    "end": "2541950"
  },
  {
    "text": "simple standalone Java program and there you have it it converts it to the required format",
    "start": "2541950",
    "end": "2548509"
  },
  {
    "text": "that the regulators require I take a",
    "start": "2548509",
    "end": "2554880"
  },
  {
    "text": "look at that look at the output and there's your output in JSON how do we do",
    "start": "2554880",
    "end": "2561749"
  },
  {
    "text": "this simple Java program there's the code it's I don't know 20 25 lines of",
    "start": "2561749",
    "end": "2567479"
  },
  {
    "text": "code to convert this is using quick fix J the open source software to convert",
    "start": "2567479",
    "end": "2574039"
  },
  {
    "text": "the fix from binary to JSON now once you",
    "start": "2574039",
    "end": "2579719"
  },
  {
    "text": "have it in JSON format you can take it and convert it to parque here you just",
    "start": "2579719",
    "end": "2585719"
  },
  {
    "text": "need two lines of code which spark you have the data frames that have been so",
    "start": "2585719",
    "end": "2590729"
  },
  {
    "text": "beneficial to create great jobs in like two lines of code you've taken JSON and",
    "start": "2590729",
    "end": "2598489"
  },
  {
    "text": "with the simple data frame write to park a converted that to parking now once we",
    "start": "2598489",
    "end": "2605969"
  },
  {
    "text": "have this data in the optimized format we are ready for reporting and analytics",
    "start": "2605969",
    "end": "2611359"
  },
  {
    "text": "so this section is going to talk about the data that's already in s3 but now in",
    "start": "2611359",
    "end": "2618539"
  },
  {
    "text": "a format that can be read by different tools we can use both EMR as well as",
    "start": "2618539",
    "end": "2627029"
  },
  {
    "text": "athina to report off of the data the EMR",
    "start": "2627029",
    "end": "2632130"
  },
  {
    "text": "clusters running spark and hive and presto can be used for more complex logic and Athena can be used to query",
    "start": "2632130",
    "end": "2639479"
  },
  {
    "text": "the data and write it back in another format now even if the cat spec were to change this does not impact us immensely",
    "start": "2639479",
    "end": "2647700"
  },
  {
    "text": "because as you see is a few lines of code and you can change that it's the architecture that enables you to be",
    "start": "2647700",
    "end": "2653430"
  },
  {
    "text": "nimble here's an Athena table you create",
    "start": "2653430",
    "end": "2660630"
  },
  {
    "text": "an external table all you do is represent the data that's sitting on s3 athena is",
    "start": "2660630",
    "end": "2667020"
  },
  {
    "text": "schema on read is based on a schema on read which means that the data residing",
    "start": "2667020",
    "end": "2672180"
  },
  {
    "text": "on s3 is not impacted even if you were to delete the table that defines it so",
    "start": "2672180",
    "end": "2678630"
  },
  {
    "text": "they're independent it's sort of you think of it in terms of a view of how to look at the data if this cats back",
    "start": "2678630",
    "end": "2684240"
  },
  {
    "text": "changes delete the Athena table create a new one and you're ready to go as you",
    "start": "2684240",
    "end": "2689550"
  },
  {
    "text": "can see here we're storing it in a parquet format using Athena instead of EMR so now that we have this data in a",
    "start": "2689550",
    "end": "2699480"
  },
  {
    "text": "parquet format we can actually do analytics on it Amazon quick site can be pointed to",
    "start": "2699480",
    "end": "2707370"
  },
  {
    "text": "Athena and you can retrieve the data from Athena and visualize it in a",
    "start": "2707370",
    "end": "2712650"
  },
  {
    "text": "point-and-click fashion I think about this what if you had a regulator that",
    "start": "2712650",
    "end": "2718080"
  },
  {
    "text": "says we want all the trades for the past five years that you have in your record",
    "start": "2718080",
    "end": "2725730"
  },
  {
    "text": "for a particular farm you might go back and look at your history table and that's over nine terabytes of data what",
    "start": "2725730",
    "end": "2732930"
  },
  {
    "text": "what would be your options well first of all you could have an option of",
    "start": "2732930",
    "end": "2738170"
  },
  {
    "text": "maintaining everything online you could say we anticipate such requests coming",
    "start": "2738170",
    "end": "2744510"
  },
  {
    "text": "from the regulators and we're gonna keep everything and a huge massive database that would cost you if you're using RDS",
    "start": "2744510",
    "end": "2751380"
  },
  {
    "text": "for a year that will cost you roughly $27,000 that's expensive",
    "start": "2751380",
    "end": "2758240"
  },
  {
    "text": "well option two you could archive the data and upon request bring that data in",
    "start": "2758240",
    "end": "2765300"
  },
  {
    "text": "to a database and then use some queries to run the data for a particular firm",
    "start": "2765300",
    "end": "2771960"
  },
  {
    "text": "and send it off to the regulators that's a lot of effort you know the the tedium",
    "start": "2771960",
    "end": "2780510"
  },
  {
    "text": "of going to tape archives and getting data on premises and then reporting off",
    "start": "2780510",
    "end": "2786660"
  },
  {
    "text": "of that well there is a third option you could use data at rest on s3 and use",
    "start": "2786660",
    "end": "2792990"
  },
  {
    "text": "Amazon Athena or Amazon edge of spectrum to query the data the difference between",
    "start": "2792990",
    "end": "2799170"
  },
  {
    "text": "this as you could do the third option for around $45 scanning",
    "start": "2799170",
    "end": "2804660"
  },
  {
    "text": "nine terabytes of data and soon I'm going to show you a demo of how it long that would take to process that kind of",
    "start": "2804660",
    "end": "2811050"
  },
  {
    "text": "data so quick sight then allows you to import data from multiple formats and",
    "start": "2811050",
    "end": "2817950"
  },
  {
    "text": "from multiple sources s3 athina RDS redshift presto Salesforce tera data",
    "start": "2817950",
    "end": "2826890"
  },
  {
    "text": "from a file there's a lot of sources and I'm sure they're gonna add more",
    "start": "2826890",
    "end": "2831980"
  },
  {
    "text": "here's a simple output from a quick site point-and-click you could ask questions",
    "start": "2832070",
    "end": "2838530"
  },
  {
    "text": "of the data which of my customers are trading above or below the trend you",
    "start": "2838530",
    "end": "2845310"
  },
  {
    "text": "could ask which ones are having challenges in their P&L or you could ask which product",
    "start": "2845310",
    "end": "2851910"
  },
  {
    "text": "is being traded the most by which firm so let's put a thena and quick site into",
    "start": "2851910",
    "end": "2857940"
  },
  {
    "text": "action I'm gonna have a quick demo we're",
    "start": "2857940",
    "end": "2864150"
  },
  {
    "text": "processing over a billion messages and this is data pointing to s3 so I count",
    "start": "2864150",
    "end": "2875190"
  },
  {
    "text": "star from the table",
    "start": "2875190",
    "end": "2879050"
  },
  {
    "text": "and there you have it 1.15 7 billion rows and it took 2 minutes and 20",
    "start": "2884800",
    "end": "2891220"
  },
  {
    "text": "seconds to do that now let's scan 740",
    "start": "2891220",
    "end": "2898930"
  },
  {
    "text": "gig using SQL here's a simple select query you select",
    "start": "2898930",
    "end": "2905830"
  },
  {
    "text": "a few things from the table and we're grouping by you notice there's no where clause so this is actually gonna scan",
    "start": "2905830",
    "end": "2913150"
  },
  {
    "text": "the entire data set and this process",
    "start": "2913150",
    "end": "2918630"
  },
  {
    "text": "took 2 minutes and 25 seconds and it scans 740 gig of data now if you try and",
    "start": "2918630",
    "end": "2925900"
  },
  {
    "text": "do that in a traditional database environment that you know that that's expensive that's difficult sometimes",
    "start": "2925900",
    "end": "2931300"
  },
  {
    "text": "impossible once you have the data you're ready for point-and-click analytics you",
    "start": "2931300",
    "end": "2938590"
  },
  {
    "text": "point Athena quick site to Athena it create a new analysis there are some of",
    "start": "2938590",
    "end": "2946000"
  },
  {
    "text": "the options I've got a parque format and by the way John mentioned that you could have multiple formats here I'm using a",
    "start": "2946000",
    "end": "2952810"
  },
  {
    "text": "fix ml format instead of the binary fix",
    "start": "2952810",
    "end": "2957210"
  },
  {
    "text": "add the attributes that you interested in quick site will automatically",
    "start": "2957930",
    "end": "2963430"
  },
  {
    "text": "visualize the data in the appropriate format as you keep adding new attributes",
    "start": "2963430",
    "end": "2969250"
  },
  {
    "text": "it changes that automatically and there you have within seconds a point click",
    "start": "2969250",
    "end": "2975460"
  },
  {
    "text": "analysis of all your data which kind of parties which products which symbols the dates volumes whatever you have put in",
    "start": "2975460",
    "end": "2983650"
  },
  {
    "text": "your query you're able to visualize that another feature here is that you could take this data and then create filters",
    "start": "2983650",
    "end": "2990490"
  },
  {
    "text": "on it and by creating filters you can hone in on certain counterparties you",
    "start": "2990490",
    "end": "2995590"
  },
  {
    "text": "could hone in on certain products or symbols and these this is all fictitious",
    "start": "2995590",
    "end": "3002940"
  },
  {
    "text": "data it's not meant to represent any firm it's all generated",
    "start": "3002940",
    "end": "3009109"
  },
  {
    "text": "so there you have it you can compare now at the top multiple counterparties",
    "start": "3022339",
    "end": "3027829"
  },
  {
    "text": "across various attributes well quick site is both powerful and convenient",
    "start": "3027829",
    "end": "3032989"
  },
  {
    "text": "because it's a managed service but you don't necessarily have to use quick site you can use any of your existing tools",
    "start": "3032989",
    "end": "3039529"
  },
  {
    "text": "to perform this and we have a variety of",
    "start": "3039529",
    "end": "3044719"
  },
  {
    "text": "partners that you can use you probably have your own visualization tools",
    "start": "3044719",
    "end": "3049969"
  },
  {
    "text": "on-premises the point is that we create this architecture in such a way that is reusable not just for reporting but also",
    "start": "3049969",
    "end": "3057170"
  },
  {
    "text": "for analytics it's in the format you're already doing the work so you may as well leverage that for additional",
    "start": "3057170",
    "end": "3063529"
  },
  {
    "text": "benefits John so when we talked at the",
    "start": "3063529",
    "end": "3072529"
  },
  {
    "text": "beginning of the presentation we were the key aspects we needed from an architecture first one we talked about security and I think she's shown you",
    "start": "3072529",
    "end": "3079130"
  },
  {
    "text": "know a framework that covers the core elements right the dresses encryption of",
    "start": "3079130",
    "end": "3084499"
  },
  {
    "text": "data both in rest in transit restricts access to that data from that network isolation and then applies both bucket",
    "start": "3084499",
    "end": "3091999"
  },
  {
    "text": "policies and iam rules to restrict access to the data and sets up an audit trail about data access so it kicks the",
    "start": "3091999",
    "end": "3099049"
  },
  {
    "text": "Box there next one we talked about was lineage and by using us three and landing all the",
    "start": "3099049",
    "end": "3104359"
  },
  {
    "text": "data in s3 is using it as a data link it gives us that original data that we can",
    "start": "3104359",
    "end": "3110029"
  },
  {
    "text": "always work against and begin that trail of lineage from that original ingestion into s3 out to reports of the regulator",
    "start": "3110029",
    "end": "3116989"
  },
  {
    "text": "by architecting around heard he was able to create metadata that showed when that data was being transformed each step of",
    "start": "3116989",
    "end": "3124039"
  },
  {
    "text": "the process and kept track of which version of software end of data was being used in that process we addressed",
    "start": "3124039",
    "end": "3131689"
  },
  {
    "text": "the volume concerns through using EMR to ingest that data the Ammar gives us the",
    "start": "3131689",
    "end": "3137390"
  },
  {
    "text": "ability to automatically scale up to whatever data challenges we needed so we could kick off jobs at the end of the",
    "start": "3137390",
    "end": "3144199"
  },
  {
    "text": "day when that data was available in EMR would scale up to address the demands that we had transformed that data each",
    "start": "3144199",
    "end": "3150499"
  },
  {
    "text": "EMR cluster could have a different parser and so what we did certainly with fixed it's not just related to fix you could",
    "start": "3150499",
    "end": "3157010"
  },
  {
    "text": "have different parses parts different types of data so even outside of the cat demonstration overall that ingestion",
    "start": "3157010",
    "end": "3164120"
  },
  {
    "text": "architecture works for a variety of red reporting uses and then certainly once that data's been normalized it unlocks a",
    "start": "3164120",
    "end": "3170750"
  },
  {
    "text": "large number of reporting and analytic solutions that can be done on top of it so addresses are core need of actually",
    "start": "3170750",
    "end": "3177470"
  },
  {
    "text": "having to report out to the regulator right sort of the core basis but then that data is now available to data",
    "start": "3177470",
    "end": "3182750"
  },
  {
    "text": "scientists and business intelligence folks inside the firm to leverage that data for a greater variety of uses and",
    "start": "3182750",
    "end": "3189670"
  },
  {
    "text": "because both from an EMR perspective there were only scaling when we need to and scaling down when we don't it's cost",
    "start": "3189670",
    "end": "3197390"
  },
  {
    "text": "efficient and then by using s3 is our source of historic data when we have infrequent access like regulatory",
    "start": "3197390",
    "end": "3203930"
  },
  {
    "text": "requests or even when we want to do ad hoc analysis on historic data we can do that without keeping it in a database",
    "start": "3203930",
    "end": "3210170"
  },
  {
    "text": "and by using Athena or redshift we get a very cost-effective way to actually manage data that's in that environment",
    "start": "3210170",
    "end": "3216800"
  },
  {
    "text": "and so hopefully today we've covered sort of all those key characteristics that we said we were going to address in",
    "start": "3216800",
    "end": "3222680"
  },
  {
    "text": "a regulatory reporting architecture we still have a few minutes left in the session we have two mics on both sides",
    "start": "3222680",
    "end": "3229820"
  },
  {
    "text": "of the stand if there's anyone who asked questions we're kind of happy to take them now",
    "start": "3229820",
    "end": "3235210"
  }
]