[
  {
    "start": "0",
    "end": "240000"
  },
  {
    "text": "all right we'll get started here first thank you guys very much for coming I",
    "start": "1460",
    "end": "6690"
  },
  {
    "text": "know it's the last session it's been a long week here at reinvent I'm gonna take you guys through best practices",
    "start": "6690",
    "end": "12929"
  },
  {
    "text": "with Amazon redshift my name is Tony Gibbs I'm a data warehousing solution architect here with",
    "start": "12929",
    "end": "18180"
  },
  {
    "text": "Amazon Web Services I always like to kind of just get a feel for where you",
    "start": "18180",
    "end": "23580"
  },
  {
    "text": "guys are at how many of you people in the room here aren't using redshift today maybe you're just here you're",
    "start": "23580",
    "end": "29460"
  },
  {
    "text": "evaluating it that sort of thing okay so quite a few of you how many of",
    "start": "29460",
    "end": "35969"
  },
  {
    "text": "you use redshift today day in and day out okay so much less I actually thought",
    "start": "35969",
    "end": "41969"
  },
  {
    "text": "that might happen today this just to level set this presentation is primarily",
    "start": "41969",
    "end": "47070"
  },
  {
    "text": "meant for existing redshift users it is a technical 400 level deep dive that",
    "start": "47070",
    "end": "52920"
  },
  {
    "text": "being said I will try to I'll slow down a little bit in the architectures and",
    "start": "52920",
    "end": "57989"
  },
  {
    "text": "concepts and try and fill in a little bit more for the rest of you that haven't used redshift yet this section",
    "start": "57989",
    "end": "64710"
  },
  {
    "text": "really is kind of will be review for the experience redshift users then I'm gonna jump into data ingestion basically how",
    "start": "64710",
    "end": "71250"
  },
  {
    "text": "you get data into redshift according to our best practices how to do alt and redshift then I'm gonna go into workload",
    "start": "71250",
    "end": "77790"
  },
  {
    "text": "management which is one of our more advanced concepts so cover that hopefully you'll walk out of here with",
    "start": "77790",
    "end": "84119"
  },
  {
    "text": "enough information or if you are going to be going back and configuring redshift you'll be able to set up basic",
    "start": "84119",
    "end": "90450"
  },
  {
    "text": "WLAN configuration then I'm going to cluster sizing and resizing this is",
    "start": "90450",
    "end": "96150"
  },
  {
    "text": "really kind of information that will be for everyone it's actually one of the most frequently asked questions that I",
    "start": "96150",
    "end": "102030"
  },
  {
    "text": "get in how do i size a redshift cluster I'm gonna wrap up by giving you guys some additional links resources kind of",
    "start": "102030",
    "end": "109320"
  },
  {
    "text": "blogs and that sort of thing that we find useful or that we find out customers find useful I'll give you that",
    "start": "109320",
    "end": "115079"
  },
  {
    "text": "to go if there is time left over I'm happy to do open Q&A I'm not sure if",
    "start": "115079",
    "end": "120689"
  },
  {
    "text": "there's microphones out or not but if there are I'll do open Q&A I will also answer questions out in the hallway",
    "start": "120689",
    "end": "127439"
  },
  {
    "text": "Lobby area out there as well so if you know you guys have a lot of questions and we're getting kicked out of the room",
    "start": "127439",
    "end": "132720"
  },
  {
    "text": "we can go stand out there and I'll answer questions for as long as you guys have them so let's get started here",
    "start": "132720",
    "end": "140900"
  },
  {
    "text": "redshift really kind of starts out from being a fork off of Postgres as we took",
    "start": "141230",
    "end": "146850"
  },
  {
    "text": "it we forked it off of Postgres we rewrote the entire storage engine to be",
    "start": "146850",
    "end": "152490"
  },
  {
    "text": "column er so it's Postgres equal column our data storage we also made it MPP",
    "start": "152490",
    "end": "159120"
  },
  {
    "text": "which is massively parallel processing this allows us to scale redshift out up to a hundred and twenty eight nodes or",
    "start": "159120",
    "end": "165360"
  },
  {
    "text": "two petabytes of raw storage we added a lot of analytics functions windowing",
    "start": "165360",
    "end": "171030"
  },
  {
    "text": "functions and that sort of thing approximate functions basically the types of stuff that you want two",
    "start": "171030",
    "end": "176660"
  },
  {
    "text": "functions you want to have in an analytics database we wrapped it all up in the AWS ecosystem so this is tight",
    "start": "176660",
    "end": "183930"
  },
  {
    "text": "integration with s3 where you typically load and unload data from integration when things like kms so you can do full",
    "start": "183930",
    "end": "189930"
  },
  {
    "text": "disk encryption I am for authentication so you can authenticate to redshift",
    "start": "189930",
    "end": "195390"
  },
  {
    "text": "using I am the combination of all of this really is what makes redshift what",
    "start": "195390",
    "end": "200670"
  },
  {
    "text": "it is we launched redshift on Valentine's Day 2013",
    "start": "200670",
    "end": "206340"
  },
  {
    "text": "so almost six years ago since that time we've continued to innovate and add more",
    "start": "206340",
    "end": "211470"
  },
  {
    "text": "features to the service we roll out patches typically on a two-week cadence",
    "start": "211470",
    "end": "217350"
  },
  {
    "text": "so every two weeks typically get a new patch this is entirely automated you set",
    "start": "217350",
    "end": "222540"
  },
  {
    "text": "a 30-minute maintenance window 30 minutes when you want that patch and we will take care of all of the patching",
    "start": "222540",
    "end": "227580"
  },
  {
    "text": "both to redshift the operating system and any type of security updates that",
    "start": "227580",
    "end": "232620"
  },
  {
    "text": "need to happen all that kind of happens automatically because redshift is a fully managed service looking at the",
    "start": "232620",
    "end": "241680"
  },
  {
    "start": "240000",
    "end": "240000"
  },
  {
    "text": "redshift architecture here you can kind of I'm going to start out with this top green box here which is sequel client",
    "start": "241680",
    "end": "246720"
  },
  {
    "text": "tools that's what you connect from so we supply both JDBC and ODBC drivers",
    "start": "246720",
    "end": "253790"
  },
  {
    "text": "because redshift was forked off of Postgres we've maintained that compatibility so that you can actually",
    "start": "253790",
    "end": "260790"
  },
  {
    "text": "also use the open source Postgres drivers so say for example you were connecting from Python or",
    "start": "260790",
    "end": "266880"
  },
  {
    "text": "or Ruby or some other language like that you can get just use the open-source Postgres drivers and you can use those",
    "start": "266880",
    "end": "273360"
  },
  {
    "text": "to connect to redshift your connection goes to what we call the leader node that's that blue box there up at the top",
    "start": "273360",
    "end": "279620"
  },
  {
    "text": "the leader node acts as the query coordinator also holds all of the metadata about the table so if you're",
    "start": "279620",
    "end": "285870"
  },
  {
    "text": "familiar with Postgres the PG catalog that resides on the leader node behind",
    "start": "285870",
    "end": "291270"
  },
  {
    "text": "the leader node we have between 2 and 128 compute nodes in this example I just have three that's where all of the data",
    "start": "291270",
    "end": "298800"
  },
  {
    "text": "resides so all the data hopefully is spread evenly across the cluster I'll",
    "start": "298800",
    "end": "303990"
  },
  {
    "text": "talk about how you spread data out across the cluster later when you execute a query unread shift every one",
    "start": "303990",
    "end": "311160"
  },
  {
    "text": "of those compute nodes executes it entirely in parallel which is a really important concept it's kind of why we",
    "start": "311160",
    "end": "317910"
  },
  {
    "text": "call this massively parallel share nothing architecture underneath the compute nodes is s3 that's typically how",
    "start": "317910",
    "end": "325560"
  },
  {
    "text": "we ingest data we either load it or if you want to remove data out of your redshift cluster you can unload it the",
    "start": "325560",
    "end": "332280"
  },
  {
    "text": "backups in redshift are entirely taken care of for you redshift just box up data asynchronously in the background",
    "start": "332280",
    "end": "338520"
  },
  {
    "text": "and that also synchronizes to s3 if you were to restore a cluster that also",
    "start": "338520",
    "end": "344880"
  },
  {
    "text": "comes off of s3 all of those operations should happen in parallel a little over",
    "start": "344880",
    "end": "352170"
  },
  {
    "text": "a year and a half ago we launched what we call Amazon redshift spectrum was it which was a really an extension to",
    "start": "352170",
    "end": "357720"
  },
  {
    "text": "redshift it's this elastic layer of compute that sits between your compute and s3 and allows you to query s3 so if",
    "start": "357720",
    "end": "367440"
  },
  {
    "text": "you say for example have a day delay can you have all these park' files sitting on s3 you can expose those park' files",
    "start": "367440",
    "end": "373500"
  },
  {
    "text": "in your redshift cluster as an external table and query them that query gets",
    "start": "373500",
    "end": "378600"
  },
  {
    "text": "pushed down in this compute layer that compute layer pulls the data up off of s3 returns results up to redshift you",
    "start": "378600",
    "end": "385560"
  },
  {
    "text": "can even join tables that reside on s3 with local redshift tables and redshift",
    "start": "385560",
    "end": "391380"
  },
  {
    "text": "knows how to push or pull data in whichever direction it needs to to limit the amount of data movement",
    "start": "391380",
    "end": "399530"
  },
  {
    "text": "so the first piece of terminology I'm gonna introduce you to is column or redshift is a column or data warehouse",
    "start": "399660",
    "end": "405830"
  },
  {
    "start": "400000",
    "end": "400000"
  },
  {
    "text": "what this means is that we store data on disk column by column rather than row by",
    "start": "405830",
    "end": "411660"
  },
  {
    "text": "row like a traditional database like Postgres the reason we do this is that the types of queries that you typically",
    "start": "411660",
    "end": "418320"
  },
  {
    "text": "execute in an analytics database really usually only query a subset of the",
    "start": "418320",
    "end": "423960"
  },
  {
    "text": "columns so we're able to reduce the amount of i/o that's needed to be done just to illustrate this so if I have",
    "start": "423960",
    "end": "431550"
  },
  {
    "text": "this deep dive table up here you can see the DDL for it and I've just have a handful of rows there and I want to",
    "start": "431550",
    "end": "438780"
  },
  {
    "text": "execute this select statement it's just a very simple query I'm just selecting the minimum data out of this table in a",
    "start": "438780",
    "end": "445080"
  },
  {
    "text": "row based database assuming there's no index or anything like that to help you you would end up having to read all of",
    "start": "445080",
    "end": "451290"
  },
  {
    "text": "the data in this entire table to find that minimum date in redshift because",
    "start": "451290",
    "end": "456710"
  },
  {
    "text": "it's a column or data warehouse all we need to do is look at the data in that",
    "start": "456710",
    "end": "461730"
  },
  {
    "text": "date column to find the minimum date we don't need to touch the other columns and that's the advantage of a column or",
    "start": "461730",
    "end": "468480"
  },
  {
    "text": "data warehouse for these types of queries so the next piece of terminology",
    "start": "468480",
    "end": "474120"
  },
  {
    "start": "472000",
    "end": "472000"
  },
  {
    "text": "is compression sometimes we'll use this term interchangeably compression and encoding so if you hear me say encoding",
    "start": "474120",
    "end": "480590"
  },
  {
    "text": "compression I'm really kind of talking about the same thing this really does",
    "start": "480590",
    "end": "486000"
  },
  {
    "text": "two things for you the first is is compression allows you to store significantly more data in your red ship",
    "start": "486000",
    "end": "491370"
  },
  {
    "text": "cluster typically we see somewhere between three and four times compression these days with the new compression",
    "start": "491370",
    "end": "498000"
  },
  {
    "text": "types that we've added to redshift it's closer to four times compression but I still kind of always assume three times",
    "start": "498000",
    "end": "503880"
  },
  {
    "text": "as a safe number that you'll get the second thing is compression also",
    "start": "503880",
    "end": "510680"
  },
  {
    "text": "improves performance so the reason why is is it reduces the amount of i/o that",
    "start": "510680",
    "end": "516659"
  },
  {
    "text": "we need to do off disk and therefore still it actually does help with performance the first time you load data",
    "start": "516660",
    "end": "524400"
  },
  {
    "text": "into a redshift table with a copy command we will automatically apply",
    "start": "524400",
    "end": "530520"
  },
  {
    "text": "compression to that table so if you haven't set compression on existing table or a new table the first",
    "start": "530520",
    "end": "535680"
  },
  {
    "text": "time there's a load into it we will automatically set it we also have this command called the analyze compression",
    "start": "535680",
    "end": "542010"
  },
  {
    "text": "command the analyze compression command you can just type out analyze compression the name of a table and if",
    "start": "542010",
    "end": "547620"
  },
  {
    "text": "there's data in the table we sample the data out of that table and we figure out the optimal compression for that table",
    "start": "547620",
    "end": "555920"
  },
  {
    "start": "556000",
    "end": "556000"
  },
  {
    "text": "just as simple example same table same rows we have 12 different encoding types",
    "start": "556130",
    "end": "562550"
  },
  {
    "text": "basically we can apply those encoding types by modifying the DDL this is the",
    "start": "562550",
    "end": "568050"
  },
  {
    "text": "manual way to do it so you can see the encode Z STD which is Z standard by dick",
    "start": "568050",
    "end": "573420"
  },
  {
    "text": "run length for example and one important thing to note is is that each of these columns can grow independent grow or",
    "start": "573420",
    "end": "580980"
  },
  {
    "text": "shrink independently so some columns might be longer than others and that's because some data may",
    "start": "580980",
    "end": "586710"
  },
  {
    "text": "compress better than others as well because all the data types in each of these columns are all stored together",
    "start": "586710",
    "end": "592680"
  },
  {
    "text": "each of these would be if you know maybe all timestamps are in this case all integers for example or all chars they",
    "start": "592680",
    "end": "599130"
  },
  {
    "text": "compress very well so that's one of the other advantages of column ER and compression together our best practices",
    "start": "599130",
    "end": "606840"
  },
  {
    "start": "605000",
    "end": "605000"
  },
  {
    "text": "are to apply compression pretty much to every table one thing you might notice though is our analyze compression may at",
    "start": "606840",
    "end": "613710"
  },
  {
    "text": "times return back an encoding type called raw which basically means there's no compression at all the reason it does",
    "start": "613710",
    "end": "620880"
  },
  {
    "text": "this is if you have a really small table like maybe I don't 10,000 rows for example it may there might not be any",
    "start": "620880",
    "end": "628170"
  },
  {
    "text": "benefit to applying compression so redshift will say well it's better just to leave this uncompressed same thing if",
    "start": "628170",
    "end": "633960"
  },
  {
    "text": "you have sparse columns if you have columns filled with a lot of nulls because of the way redshift does nulls",
    "start": "633960",
    "end": "639060"
  },
  {
    "text": "you might also get raw back on those columns as well I did a little snippet",
    "start": "639060",
    "end": "644550"
  },
  {
    "text": "of sequel here that's just where you can find the encoding type on an existing table so the next piece to introduce you",
    "start": "644550",
    "end": "651570"
  },
  {
    "text": "to is blocks blocks really are just that's what they're blocks they're what we store our data in the important thing",
    "start": "651570",
    "end": "658620"
  },
  {
    "text": "to know here is is that they're immutable and very large one Meg we use one Meg immutable blocks that will be",
    "start": "658620",
    "end": "664920"
  },
  {
    "text": "encoded with one of 12 encoding because the when you apply compression",
    "start": "664920",
    "end": "672390"
  },
  {
    "text": "and because they're so large they can actually in certain cases soar millions of values so you think about that that's",
    "start": "672390",
    "end": "678510"
  },
  {
    "text": "millions of rows stuffed into a single block so they are very big the next",
    "start": "678510",
    "end": "684720"
  },
  {
    "text": "piece to introduce you to is something we call zone maps zone maps are they're basically an in-memory data structure",
    "start": "684720",
    "end": "691380"
  },
  {
    "text": "that stores the minimum and maximum values for every single block that we have on disk that way when a sequel",
    "start": "691380",
    "end": "698279"
  },
  {
    "text": "query executes say with a predicate we can check that in-memory data structure and know if we need to read that block",
    "start": "698279",
    "end": "704700"
  },
  {
    "text": "off disk or not the next piece of terminology introduce you as data",
    "start": "704700",
    "end": "710010"
  },
  {
    "text": "sorting this is really just the physical sorting of data on disk so for example",
    "start": "710010",
    "end": "715080"
  },
  {
    "text": "you can say like I want to sort by this column and this column and the data will",
    "start": "715080",
    "end": "720240"
  },
  {
    "text": "be physically stored on disk sorted by those two columns the reason or the",
    "start": "720240",
    "end": "725460"
  },
  {
    "text": "primary purpose of sorting or why you want to use it is to make the zone Maps more effective so just as a example same",
    "start": "725460",
    "end": "734850"
  },
  {
    "text": "table same data as before I'm gonna apply a sort key to this so you can see the modified DDL up there I'm first",
    "start": "734850",
    "end": "742140"
  },
  {
    "text": "gonna sort by the date and then by the location in this example the table is",
    "start": "742140",
    "end": "748560"
  },
  {
    "text": "gonna be first sorted by this date and then when there's a tie we're then gonna look at the location and then sort by",
    "start": "748560",
    "end": "754410"
  },
  {
    "text": "the location so that's basically how data sorting works in Amazon redshift so",
    "start": "754410",
    "end": "761160"
  },
  {
    "start": "761000",
    "end": "761000"
  },
  {
    "text": "tying the two concepts of zone maps and sort keys together because like I said sort keys the primary purpose is to make",
    "start": "761160",
    "end": "767430"
  },
  {
    "text": "the zone Maps more effective so we have this table with these four blocks you can see the zone maps kind of written",
    "start": "767430",
    "end": "773520"
  },
  {
    "text": "out there and we came along with a sequel query and we're just going to count the records on a particular date",
    "start": "773520",
    "end": "779300"
  },
  {
    "text": "redshift is gonna first check the zone maps and look to see if it's possible for data to be in each block in this",
    "start": "779300",
    "end": "787620"
  },
  {
    "text": "example we know that there could be data in those three blocks and in this case",
    "start": "787620",
    "end": "792720"
  },
  {
    "text": "we've reduced IO if however we came along and we sorted this table by this date column we would end up with",
    "start": "792720",
    "end": "800579"
  },
  {
    "text": "zone maps that are gonna be in perfectly in sequential order like that and now we've reduced io further so the primary",
    "start": "800579",
    "end": "807480"
  },
  {
    "text": "purpose of soar keys is to make these zone Maps more effective they typically go on the columns that you're using to",
    "start": "807480",
    "end": "815129"
  },
  {
    "text": "filter on basically a predicate columns a few best practices around them",
    "start": "815129",
    "end": "821730"
  },
  {
    "start": "818000",
    "end": "818000"
  },
  {
    "text": "one is is you'll usually want to apply it to the columns that you typically",
    "start": "821730",
    "end": "827939"
  },
  {
    "text": "filter on in a data warehouse that's usually a timestamp or a date column or",
    "start": "827939",
    "end": "832980"
  },
  {
    "text": "something like that because we're almost always query and data between various dates so that is one of the primary",
    "start": "832980",
    "end": "840119"
  },
  {
    "text": "columns that you will want to sort by if you do have a column and you want to",
    "start": "840119",
    "end": "845399"
  },
  {
    "text": "sort by multiple columns it doesn't make sense to sort on an extra column if you have something that's really high",
    "start": "845399",
    "end": "851249"
  },
  {
    "text": "cardinality so imagine you had a time stamp that went all the way down to seconds and you had all of these values",
    "start": "851249",
    "end": "856889"
  },
  {
    "text": "all the way down to seconds it wouldn't make sense to apply a second column after that so just kind of a little bit",
    "start": "856889",
    "end": "862829"
  },
  {
    "text": "of a catch there one thing is is we also have a couple of scripts up here on github those scripts that we have up",
    "start": "862829",
    "end": "871319"
  },
  {
    "text": "there if you have an established workload that runs on redshift they can help to help you determine if you have",
    "start": "871319",
    "end": "877829"
  },
  {
    "text": "the correct sort keys or what should be the correct sort keys or not when they also small note if you have really small",
    "start": "877829",
    "end": "884429"
  },
  {
    "text": "tables it usually doesn't make sense to sort them if there's like 10,000 rows it's all gonna be in a single block anyways so though you're not helping out",
    "start": "884429",
    "end": "890910"
  },
  {
    "text": "with his own maps this next piece is materialising columns again this is also",
    "start": "890910",
    "end": "898139"
  },
  {
    "start": "893000",
    "end": "893000"
  },
  {
    "text": "to make redshift able to leverage his own Maps better sometimes I'll see there's a I'll just step back there's",
    "start": "898139",
    "end": "904619"
  },
  {
    "text": "two things on this slide one is with dimension tables and the other is with",
    "start": "904619",
    "end": "910019"
  },
  {
    "text": "functions and such so I'm gonna talk about the dimension tables first because this is actually one of the most common",
    "start": "910019",
    "end": "915179"
  },
  {
    "text": "ones a lot of data warehouses have these time dimension tables basically they're unchanging dimension tables for the most",
    "start": "915179",
    "end": "922230"
  },
  {
    "text": "part and you typically filtering on them well what's gonna happen if you look at",
    "start": "922230",
    "end": "927329"
  },
  {
    "text": "this first example of these two lines of sequel here is we're going to end up writing this predicate on this demand",
    "start": "927329",
    "end": "934350"
  },
  {
    "text": "which has a very small number of rows and then we're gonna join it back to our fact table and that's gonna result in",
    "start": "934350",
    "end": "940410"
  },
  {
    "text": "essentially a full column scan on that fact table with a hash join this second",
    "start": "940410",
    "end": "945810"
  },
  {
    "text": "example where I've materialized and taken the values from that dimension table and written them out into my fact",
    "start": "945810",
    "end": "952320"
  },
  {
    "text": "table that will allow redshift to use the zone maps and reduce i/o on the fact",
    "start": "952320",
    "end": "958380"
  },
  {
    "text": "table this will run significantly faster if you do that the second is is around",
    "start": "958380",
    "end": "964620"
  },
  {
    "text": "calculated columns so if you are doing calculations on columns typically in the",
    "start": "964620",
    "end": "970530"
  },
  {
    "text": "where Clause like as I have in this example where I'm extracting an epoch out of a time stamp that won't be able",
    "start": "970530",
    "end": "976920"
  },
  {
    "text": "to use the zone maps in fact what's going to happen is is you're gonna go through every value in that column extracting that value just so you can do",
    "start": "976920",
    "end": "984120"
  },
  {
    "text": "the comparison if instead you actually materialized or rewrote out that value",
    "start": "984120",
    "end": "990330"
  },
  {
    "text": "what would happen is we could use those own maps and dramatically reduce i/o so just a couple best practices there on",
    "start": "990330",
    "end": "996830"
  },
  {
    "text": "materializing out or writing out extra columns because redshift is column ER writing out a few extra columns to a",
    "start": "996830",
    "end": "1002720"
  },
  {
    "text": "table or having very wide tables there's not a lot of penalty to that you pay a little bit extra for storage but it has",
    "start": "1002720",
    "end": "1008990"
  },
  {
    "text": "a dramatic impact on performance slices are a really important concept in",
    "start": "1008990",
    "end": "1015320"
  },
  {
    "text": "redshift they are basically like you can almost think I don't like virtual compute notes they're essentially how we",
    "start": "1015320",
    "end": "1021230"
  },
  {
    "text": "get parallelism within each one of our compute notes so as our eight Excel nodes for example",
    "start": "1021230",
    "end": "1026660"
  },
  {
    "text": "which I'll talk about later they are split up into 16 slices so it's almost",
    "start": "1026660",
    "end": "1032150"
  },
  {
    "text": "like there's 16 virtual compute nodes inside each slice we also store data per",
    "start": "1032150",
    "end": "1038240"
  },
  {
    "text": "slice and every slice only works on the data that belongs to it kind of leads to",
    "start": "1038240",
    "end": "1043790"
  },
  {
    "text": "how do we distribute data and redshift there's four ways of doing this now the",
    "start": "1043790",
    "end": "1050270"
  },
  {
    "text": "first is distributing by what we call key this is essentially where you pick",
    "start": "1050270",
    "end": "1055340"
  },
  {
    "text": "one of the columns in your table to be the distribution key and what we do is",
    "start": "1055340",
    "end": "1060950"
  },
  {
    "text": "is for each one of the rows we take the value for that column we hash it that hash corresponds to one of the slices",
    "start": "1060950",
    "end": "1067070"
  },
  {
    "text": "and that's where the entire row resides the",
    "start": "1067070",
    "end": "1072080"
  },
  {
    "text": "next is is what we call distribution style even-even is where you just say redshift please round-robin this data",
    "start": "1072080",
    "end": "1078290"
  },
  {
    "text": "for me and redshift just evenly spreads the data across the cluster then we have distribution style all which is kind of",
    "start": "1078290",
    "end": "1084470"
  },
  {
    "text": "a special case one where we make a complete copy of the table on each node in the cluster this is kind of meant",
    "start": "1084470",
    "end": "1091490"
  },
  {
    "text": "more for small dimension tables that's the primary purpose for dis style all we just release distribution style Auto you",
    "start": "1091490",
    "end": "1099710"
  },
  {
    "text": "might not have it yet it's working its way through the various regions as we roll out patches it's out in some",
    "start": "1099710",
    "end": "1105530"
  },
  {
    "text": "regions but not in all yet and distribution style Auto is the new default in redshift so all new tables",
    "start": "1105530",
    "end": "1112310"
  },
  {
    "text": "going forward are going to be created with this if you don't specify something what it does is it combines even in all",
    "start": "1112310",
    "end": "1119480"
  },
  {
    "text": "together so that a table small tables start out as a dis style all and when",
    "start": "1119480",
    "end": "1125180"
  },
  {
    "text": "they reach a certain size they switch to distribution style even automatically and are rewritten so I'm just going to",
    "start": "1125180",
    "end": "1134230"
  },
  {
    "text": "illustrate these let's say we have to compute notes here these are our little nodes and they each have two slices and",
    "start": "1134230",
    "end": "1140570"
  },
  {
    "text": "I have the same table I've been working with all four since the beginning and",
    "start": "1140570",
    "end": "1146300"
  },
  {
    "text": "since we started and I have these four rows which I'm going to insert into here the current distribution style which you",
    "start": "1146300",
    "end": "1152570"
  },
  {
    "text": "can see dist I'll even there up on the table is even I'm gonna distribute these four rows in here with even the data",
    "start": "1152570",
    "end": "1161000"
  },
  {
    "text": "simply round robins through the cluster just like that and redshift evenly spreads the data if we went within",
    "start": "1161000",
    "end": "1169550"
  },
  {
    "text": "picked a distribution key so in this example I am gonna pick the location column so you can see disk key key disk",
    "start": "1169550",
    "end": "1175700"
  },
  {
    "text": "key location there or dis style and the values for our SFO JFK SFO JFK so SFO",
    "start": "1175700",
    "end": "1183980"
  },
  {
    "text": "might hash here JFK will say hash is there SFO is going to go back to that",
    "start": "1183980",
    "end": "1190490"
  },
  {
    "text": "slice 0 and then JFK over here this is an example of a bad distribution key",
    "start": "1190490",
    "end": "1196790"
  },
  {
    "text": "being picked and redshift the reason why is if you executed a sequel query like",
    "start": "1196790",
    "end": "1201799"
  },
  {
    "text": "this cluster you can see that the second compute node doesn't have any data on it and it doesn't know none of the work so",
    "start": "1201799",
    "end": "1209350"
  },
  {
    "text": "this is what we don't want to have happen so what I'm gonna do is I'm gonna pick a better distribution keep so I'm",
    "start": "1209350",
    "end": "1214909"
  },
  {
    "start": "1211000",
    "end": "1211000"
  },
  {
    "text": "gonna pick this a ID you can see it looks kind of like a primary key maybe it's one two three four high cardinality",
    "start": "1214909",
    "end": "1220610"
  },
  {
    "text": "and in this case maybe one goes there to there three back over there and four",
    "start": "1220610",
    "end": "1226669"
  },
  {
    "text": "there I made the example work perfect so that is an example of a good",
    "start": "1226669",
    "end": "1232100"
  },
  {
    "text": "distribution key if you have a significant amount of data millions of rows for example and you have a nice key",
    "start": "1232100",
    "end": "1237679"
  },
  {
    "text": "like this it will statistically give you a nice even spread of data then what we",
    "start": "1237679",
    "end": "1243619"
  },
  {
    "text": "have is distribution style all like I said this is kind of a special case for small tables dimension tables and such",
    "start": "1243619",
    "end": "1248749"
  },
  {
    "text": "what we do here is we write to the first",
    "start": "1248749",
    "end": "1254179"
  },
  {
    "text": "slice on every note so in this case all four rows are represented on both nodes",
    "start": "1254179",
    "end": "1260389"
  },
  {
    "text": "in the cluster so just to summarize",
    "start": "1260389",
    "end": "1266509"
  },
  {
    "start": "1264000",
    "end": "1264000"
  },
  {
    "text": "where you use each of these distribution style bikie is typically used to co-locate data and redshift this is",
    "start": "1266509",
    "end": "1273830"
  },
  {
    "text": "usually meant for large joints so if you have to say too large fact tables or a fact table and a really large dimension",
    "start": "1273830",
    "end": "1280549"
  },
  {
    "text": "table if you're able to distribute them by the column that's in the on clause in",
    "start": "1280549",
    "end": "1287059"
  },
  {
    "text": "your sequel statement or on your join that will result in the both rows being",
    "start": "1287059",
    "end": "1292669"
  },
  {
    "text": "on the same slice and the join will be significantly faster there is one catch",
    "start": "1292669",
    "end": "1297919"
  },
  {
    "text": "you want to make sure that you don't cause that skew that I showed in that one example when I picked the location",
    "start": "1297919",
    "end": "1303220"
  },
  {
    "text": "to see if there's skew I have this sequel query up here it's one of the system tables it's SBB underscore table",
    "start": "1303220",
    "end": "1310519"
  },
  {
    "text": "underscore info and in there there's a column called skew rows this skew rows",
    "start": "1310519",
    "end": "1315799"
  },
  {
    "text": "has a ratio a value in it that has the ratios between the slice with the least",
    "start": "1315799",
    "end": "1321230"
  },
  {
    "text": "amount of data and the slice with the most amount of data ideally that value should be close to one so in this case I",
    "start": "1321230",
    "end": "1327739"
  },
  {
    "text": "have 1.07 that's that's a great skew I'd say rule of thumb anything under 1.3 is",
    "start": "1327739",
    "end": "1333230"
  },
  {
    "text": "totally fine the other reason to use dis tile key is if you're moving data from one table to",
    "start": "1333230",
    "end": "1339660"
  },
  {
    "text": "another with an insert into with a select statement that if both tables",
    "start": "1339660",
    "end": "1345270"
  },
  {
    "text": "have the same distribution key that operation will be also be significantly faster dis style all a rule of thumb",
    "start": "1345270",
    "end": "1352590"
  },
  {
    "text": "here is that the table should be under three million rows it also causes a type",
    "start": "1352590",
    "end": "1357660"
  },
  {
    "text": "of collocation so the joins are also really fast with these small to smaller dimension tables and they also actually",
    "start": "1357660",
    "end": "1364620"
  },
  {
    "text": "they store small table is actually more optimally as well I used to always say",
    "start": "1364620",
    "end": "1370950"
  },
  {
    "text": "if you don't know or if you're not sure stick with this style even I think going forward I'm gonna have to say if you're",
    "start": "1370950",
    "end": "1376020"
  },
  {
    "text": "not sure what to do here stick with dis style auto dis tile auto like I said is brand new it's not in all",
    "start": "1376020",
    "end": "1381510"
  },
  {
    "text": "region so you guys should have it hopefully within the next several weeks it should be everywhere but yeah dis",
    "start": "1381510",
    "end": "1387750"
  },
  {
    "text": "style auto like I said combines all and even together so just to summarize the",
    "start": "1387750",
    "end": "1396480"
  },
  {
    "start": "1394000",
    "end": "1394000"
  },
  {
    "text": "best practices on table design here we want to add compression and pretty much all of our tables as long as we can we",
    "start": "1396480",
    "end": "1403170"
  },
  {
    "text": "want to make sure we have sort keys on the columns that we primarily filter on so it's like I said usually time stamps we want to materialize those columns",
    "start": "1403170",
    "end": "1410580"
  },
  {
    "text": "into our fact table that you know we're running those predicates on we want to pre calculate certain columns where it",
    "start": "1410580",
    "end": "1417150"
  },
  {
    "text": "makes sense Cole okay columns for our joints I didn't really mention this but",
    "start": "1417150",
    "end": "1423240"
  },
  {
    "text": "the temporal columns usually make a poor choice of distribution so I think based",
    "start": "1423240",
    "end": "1428430"
  },
  {
    "text": "on time usually doesn't just distribute very well worst one I've seen I think is",
    "start": "1428430",
    "end": "1433830"
  },
  {
    "text": "when someone picks like a month to be the distribution key it does not work lastly keep if you have data types keep",
    "start": "1433830",
    "end": "1441990"
  },
  {
    "text": "them as narrow as they kind of need to be or as big as they need to be I sometimes see like these migrations from Oracle and everything is a bar chart 255",
    "start": "1441990",
    "end": "1449070"
  },
  {
    "text": "I'm not sure why but that's just what people from Oracle pick so make them shorter it doesn't change storage",
    "start": "1449070",
    "end": "1455070"
  },
  {
    "text": "footprint in any way but it can impact query performance it uses a little bit more memory and redshift so if you can",
    "start": "1455070",
    "end": "1461010"
  },
  {
    "text": "keep those as narrow as they need to be so let's get into data ingestion so one",
    "start": "1461010",
    "end": "1467790"
  },
  {
    "start": "1466000",
    "end": "1466000"
  },
  {
    "text": "of the things redshift the nodes are significantly bigger than we advertise I don't think this is anywhere in our documentation or",
    "start": "1467790",
    "end": "1474000"
  },
  {
    "text": "anything so I think like it for example if you look at our two terabyte instance",
    "start": "1474000",
    "end": "1481020"
  },
  {
    "text": "type it actually has six terabytes the reason it has so much space is that we",
    "start": "1481020",
    "end": "1486540"
  },
  {
    "text": "mirror all the data on every node to another node so your data is safely",
    "start": "1486540",
    "end": "1492120"
  },
  {
    "text": "written to two nodes the moment a commit happens in redshift we obviously also have space set aside for temporary temp",
    "start": "1492120",
    "end": "1499940"
  },
  {
    "text": "the operating system all of that stuff so the space that we advertise is the usable amount of space that you as a",
    "start": "1499940",
    "end": "1507120"
  },
  {
    "text": "customer have for your data that being said like I said the commits there they",
    "start": "1507120",
    "end": "1513780"
  },
  {
    "text": "when a commits occurs data is on both tables or two nodes sorry the other",
    "start": "1513780",
    "end": "1519570"
  },
  {
    "text": "thing is is we also are asynchronously synchronous asynchronous seen asynchronously backing up all of our",
    "start": "1519570",
    "end": "1526080"
  },
  {
    "text": "data to s3 and redshift as well so that kind of happens in the back background our temporary tables though they're kind",
    "start": "1526080",
    "end": "1533490"
  },
  {
    "text": "of special they don't back they're not backed up to s3 and we also don't write out that mirrored copy so because of",
    "start": "1533490",
    "end": "1541350"
  },
  {
    "text": "that they actually write twice as fast",
    "start": "1541350",
    "end": "1545630"
  },
  {
    "text": "redshift is fully acid compliant fully transactional it uses isolation level",
    "start": "1547670",
    "end": "1553440"
  },
  {
    "text": "serializable there's a two-phase commit there's a local commit that happens on each node and then a global commit that",
    "start": "1553440",
    "end": "1558960"
  },
  {
    "text": "happens across all the nodes every time a commit happens we've dramatically improved commit performance that was",
    "start": "1558960",
    "end": "1566550"
  },
  {
    "text": "about last July August we put out some patches that had a large impact on that even still it makes sense to try and",
    "start": "1566550",
    "end": "1574230"
  },
  {
    "text": "group workflows together into a transaction to reduce the number of",
    "start": "1574230",
    "end": "1580650"
  },
  {
    "text": "commits that happen one thing that customers will sometimes do is they'll have a bunch of DDL and what they don't",
    "start": "1580650",
    "end": "1586350"
  },
  {
    "text": "realize is is that each one of those DDL statements is implicitly creating and",
    "start": "1586350",
    "end": "1592110"
  },
  {
    "text": "committing a transaction trend DDL is transactional in redshift",
    "start": "1592110",
    "end": "1599270"
  },
  {
    "start": "1599000",
    "end": "1599000"
  },
  {
    "text": "so I'm gonna talk a little bit about the copy statement the copy statement is how we typically get data into redshift in",
    "start": "1599360",
    "end": "1606080"
  },
  {
    "text": "this example I have this dc28 XL node type it has 16 slices and I'm gonna use",
    "start": "1606080",
    "end": "1611840"
  },
  {
    "text": "the copy statement to ingest this one large file what would happen is is the",
    "start": "1611840",
    "end": "1618680"
  },
  {
    "text": "leader notes gonna check s3 it's gonna notice that there's this file on s3 and it's gonna have in this particular case",
    "start": "1618680",
    "end": "1623890"
  },
  {
    "text": "the first slice in the cluster load that file up it's going to parse it distribute it write it out to disk",
    "start": "1623890",
    "end": "1629960"
  },
  {
    "text": "across all the slices this isn't exactly very efficient if I took this exact same",
    "start": "1629960",
    "end": "1635090"
  },
  {
    "text": "file and I split it into 16 chunks for example because this cluster has 16",
    "start": "1635090",
    "end": "1640400"
  },
  {
    "text": "slices what's gonna happen is is every one of those slices is going to be able to reach out to s3 and ingest that file",
    "start": "1640400",
    "end": "1647000"
  },
  {
    "text": "in parallel so our rule of thumb is to have as many files as you have slices or",
    "start": "1647000",
    "end": "1653450"
  },
  {
    "text": "a multiple of that so for example you had 32 files that would work just as well ideally these files should be",
    "start": "1653450",
    "end": "1661010"
  },
  {
    "text": "somewhere between 1 Meg and 1 gig after gzip compression just some best",
    "start": "1661010",
    "end": "1669620"
  },
  {
    "text": "practices these are kind of from what I've seen with customers I always suggest just picking using delimited",
    "start": "1669620",
    "end": "1675290"
  },
  {
    "text": "files picking a simple delimiter like pipe or comma tab whatever your preference is don't pick something crazy",
    "start": "1675290",
    "end": "1680900"
  },
  {
    "text": "like a utf-8 unprintable character pick a simple null character make sure things",
    "start": "1680900",
    "end": "1687560"
  },
  {
    "text": "are wrapped in double quotes for your varchars used by you know a scape character for double quotes and things",
    "start": "1687560",
    "end": "1693980"
  },
  {
    "text": "like that and then to get the number of slices in the cluster that's a sequel query that",
    "start": "1693980",
    "end": "1699800"
  },
  {
    "text": "you can use to get that so I know I kind of mentioned you want to have as many files you have slices that's where you can find that one pattern that customers",
    "start": "1699800",
    "end": "1711350"
  },
  {
    "text": "have started using spectrum for so like I said we released spectrum which is essentially external tables is to ingest",
    "start": "1711350",
    "end": "1718190"
  },
  {
    "text": "data through it there's a couple advantages for using spectrum for this one you have full sequel so you can",
    "start": "1718190",
    "end": "1725210"
  },
  {
    "text": "aggregate incoming data with spectrum you can select a subset of columns with",
    "start": "1725210",
    "end": "1730280"
  },
  {
    "text": "spectrum or you could also filter out rows that you don't with spectrum so you can do a lot of data cleansing on the incoming data or",
    "start": "1730280",
    "end": "1737330"
  },
  {
    "text": "the best three and that's not something you can do with a copy statement so one of our kind of if you would like to or",
    "start": "1737330",
    "end": "1744230"
  },
  {
    "text": "if you think that it would be beneficial to you definitely check out using spectrum to ingest your data if there's",
    "start": "1744230",
    "end": "1750830"
  },
  {
    "text": "some benefit for you redshift is really designed around Big",
    "start": "1750830",
    "end": "1757400"
  },
  {
    "start": "1753000",
    "end": "1753000"
  },
  {
    "text": "Data it's not meant for small ingestion because the blocks are so large and redshift being one Meg blocks the amount",
    "start": "1757400",
    "end": "1765470"
  },
  {
    "text": "of overhead to ingest just a handful of rows is roughly the same as ingesting hundreds of thousands of rows so ideally",
    "start": "1765470",
    "end": "1773390"
  },
  {
    "text": "you want to be ingesting fairly large batches of data word about update and",
    "start": "1773390",
    "end": "1779630"
  },
  {
    "text": "delete deletes simply mark rows for deletion they don't actually remove the rows",
    "start": "1779630",
    "end": "1785500"
  },
  {
    "text": "updates mark a row for deletion and reinsert it to the end of the table so",
    "start": "1785500",
    "end": "1795410"
  },
  {
    "text": "this is also one of the frequently asked questions that I get is how do i do d dupe logic up certs and redshift say for",
    "start": "1795410",
    "end": "1801500"
  },
  {
    "text": "example we have that table we've been working with this deep dive table here and I had a CSV file that I wanted to",
    "start": "1801500",
    "end": "1807830"
  },
  {
    "text": "ingest and this CSV file has data in it that's gonna do an update to two of the",
    "start": "1807830",
    "end": "1813110"
  },
  {
    "text": "rows here and it's gonna add two additional rows to the to that table how do we do this what we end up doing is is",
    "start": "1813110",
    "end": "1821930"
  },
  {
    "text": "we want to load that CSV file into a staging table then what we're gonna do is we're gonna delete the duplicate rows",
    "start": "1821930",
    "end": "1828950"
  },
  {
    "text": "or the rows that are gonna match out of the production table and then we're gonna insert every row from that staging",
    "start": "1828950",
    "end": "1835160"
  },
  {
    "text": "table into our production table the sequel for this we're gonna start with a",
    "start": "1835160",
    "end": "1841850"
  },
  {
    "text": "transaction remember when I was talking about the global commits and the their bit expensive still we're gonna wrap all",
    "start": "1841850",
    "end": "1847820"
  },
  {
    "text": "of this workflow in a transaction it's also kind of good because if something were to fail halfway through it either",
    "start": "1847820",
    "end": "1853220"
  },
  {
    "text": "all happen or rollback we're gonna create a temporary table because",
    "start": "1853220",
    "end": "1858560"
  },
  {
    "text": "temporary tables write twice as fast I'm gonna use this like keyword here which",
    "start": "1858560",
    "end": "1864380"
  },
  {
    "text": "is gonna copy over the distribution style our deep dive table and it's also going",
    "start": "1864380",
    "end": "1869900"
  },
  {
    "text": "to copy over compression settings from that deep dive table as well then I'm going to copy into that staging table",
    "start": "1869900",
    "end": "1878150"
  },
  {
    "text": "that I created that CSV file notice I kind of redundant Li put in comp update off that means I don't want red ship to",
    "start": "1878150",
    "end": "1884510"
  },
  {
    "text": "try to figure out the compression for this because it already has it from the deep dive table then what we're going to do is we're going to delete all of the",
    "start": "1884510",
    "end": "1891170"
  },
  {
    "text": "rows and the deep dive table that match in that staging table and I'm gonna insert every row over into the deep dive",
    "start": "1891170",
    "end": "1897560"
  },
  {
    "text": "table I'm gonna drop the staging table and I'm gonna commit the transaction and all of this will happen or it all will",
    "start": "1897560",
    "end": "1904790"
  },
  {
    "text": "roll back so this is the best way the fastest way to do this type of logic",
    "start": "1904790",
    "end": "1911030"
  },
  {
    "text": "I've seen it done with update statements it technically works but this is faster",
    "start": "1911030",
    "end": "1917080"
  },
  {
    "start": "1917000",
    "end": "1917000"
  },
  {
    "text": "so our best practice is just a summary wrap all the workflows in explicit transactions consider if you can using",
    "start": "1917230",
    "end": "1924890"
  },
  {
    "text": "drop table truncate instead of using delete statements if the if it makes sense use temporary tables if you can in",
    "start": "1924890",
    "end": "1932810"
  },
  {
    "text": "here for your staging tables if you do need a permanent table we have a backup no option consider using that as well",
    "start": "1932810",
    "end": "1939140"
  },
  {
    "text": "I'm so that disables the backup to s3 for that staging table make sure that if",
    "start": "1939140",
    "end": "1944810"
  },
  {
    "text": "you do have these staging tables and prod tables if you can if they can share the same distribution key that will make",
    "start": "1944810",
    "end": "1950810"
  },
  {
    "text": "that insert into select much faster turn off automatic compression for these staging tables when you're loading data",
    "start": "1950810",
    "end": "1957590"
  },
  {
    "text": "and/or apply compression manually like with using the like statement the reason",
    "start": "1957590",
    "end": "1962840"
  },
  {
    "text": "why sometimes I'll see these workflows that are creating a staging table and then red ship figures out the compression for it and then they drop",
    "start": "1962840",
    "end": "1968840"
  },
  {
    "text": "the staging table then they create the staging table again red ship figured out the compression again and that happens over and over again every 15 minutes",
    "start": "1968840",
    "end": "1975050"
  },
  {
    "text": "very wasteful so apply compression to those tables word about vacuum and",
    "start": "1975050",
    "end": "1982730"
  },
  {
    "start": "1981000",
    "end": "1981000"
  },
  {
    "text": "analyze I was talking about those ghost rows or what happens when we tag rows is delete for deletion we have a vacuum",
    "start": "1982730",
    "end": "1990770"
  },
  {
    "text": "command and red shift vacuum for deletes is automatic it runs in the background",
    "start": "1990770",
    "end": "1996200"
  },
  {
    "text": "for you all automatically we release that recently so we have been working making vacuum fully-automatic today it",
    "start": "1996200",
    "end": "2004210"
  },
  {
    "text": "only does the deletes so you do still need to do the secondary tasks or scheduled that at some point which is",
    "start": "2004210",
    "end": "2010780"
  },
  {
    "text": "where we globally sort the table we've analyzed which also collects",
    "start": "2010780",
    "end": "2016360"
  },
  {
    "text": "statistics that is also now automatic as well so we have released both vacuum",
    "start": "2016360",
    "end": "2022000"
  },
  {
    "text": "delete only automatic and analyze so those are those happen in the background",
    "start": "2022000",
    "end": "2027730"
  },
  {
    "text": "now our best practices then would be run vacuum still at this moment until we get",
    "start": "2027730",
    "end": "2033730"
  },
  {
    "text": "auto vacuum sort out most customers run it once a day maybe on weekends so",
    "start": "2033730",
    "end": "2039040"
  },
  {
    "text": "that's really all you have to do is maybe run it at the end of your ETL just do a global sort it's gonna jump in a",
    "start": "2039040",
    "end": "2046660"
  },
  {
    "text": "workload management here workload management is how we separate various",
    "start": "2046660",
    "end": "2052030"
  },
  {
    "text": "workloads in redshift so if say you had two types of users maybe your dashboard users your data science and analysts",
    "start": "2052030",
    "end": "2058780"
  },
  {
    "text": "users and you wanted to kind of separate those workloads throttle one versus the other give priority to one type of",
    "start": "2058780",
    "end": "2064480"
  },
  {
    "text": "workload that's what W will AB is meant for so you can go through some of the",
    "start": "2064480",
    "end": "2069898"
  },
  {
    "text": "terminology about what makes up w LM queues every sequel query that executes",
    "start": "2069899",
    "end": "2076120"
  },
  {
    "text": "in redshift will execute in one queue so basically what will happen is based on",
    "start": "2076120",
    "end": "2081220"
  },
  {
    "text": "your user or based on a session variable that will direct which cue that that",
    "start": "2081220",
    "end": "2086320"
  },
  {
    "text": "query will be executed in we also have a concept that we call short cui acceleration it's just quite literally a",
    "start": "2086320",
    "end": "2092408"
  },
  {
    "text": "checkbox in the console and you can either disable it or leave it on I recommend leaving it on and what this",
    "start": "2092409",
    "end": "2098710"
  },
  {
    "text": "feature does is if a queue is full and we detect that the incoming query is a",
    "start": "2098710",
    "end": "2105730"
  },
  {
    "text": "short running query maybe it's only going to run in a few seconds we bounce that into a special queue that we call",
    "start": "2105730",
    "end": "2111370"
  },
  {
    "text": "the short query queue and it runs immediately so the idea here is to try to make short running queries continue",
    "start": "2111370",
    "end": "2118630"
  },
  {
    "text": "to run even if the system is fully loaded up the next concept that we have",
    "start": "2118630",
    "end": "2124300"
  },
  {
    "text": "is query slots this is the query slots are how we divide up each one of the",
    "start": "2124300",
    "end": "2130000"
  },
  {
    "text": "queues so we have a queue and it's divided into slots in the console",
    "start": "2130000",
    "end": "2135130"
  },
  {
    "text": "it's called concurrency in redshift is called slots a little bit of confusion there sometimes but they really kind of",
    "start": "2135130",
    "end": "2140920"
  },
  {
    "text": "mean the exact same thing then I usually don't talk about upcoming preview",
    "start": "2140920",
    "end": "2147040"
  },
  {
    "text": "features and a best practice talk because I try to keep with the features that are available but this one's such a",
    "start": "2147040",
    "end": "2152740"
  },
  {
    "text": "big feature I won't really wanted to put it in here we just released or announced what we call concurrency scaling it was",
    "start": "2152740",
    "end": "2160030"
  },
  {
    "text": "announced on Wednesday actually was announced last week actually so concurrency scaling is it's an",
    "start": "2160030",
    "end": "2167320"
  },
  {
    "text": "attribute of the queue and you can enable it so that when the queue is full the basically what will happen is is the",
    "start": "2167320",
    "end": "2174940"
  },
  {
    "text": "queries in it will be routed to transient redshift clusters so we will manage that fully and it is quite",
    "start": "2174940",
    "end": "2181089"
  },
  {
    "text": "literally just a drop-down whether or not you want queries in that queue to route to a secondary clusters fully",
    "start": "2181089",
    "end": "2188980"
  },
  {
    "text": "managed for you you don't do anything other than select that option and basically that's it so I'm gonna come up",
    "start": "2188980",
    "end": "2196240"
  },
  {
    "text": "with a use case this is a very typical use case that I see with a lot of customers light ingestion kind of",
    "start": "2196240",
    "end": "2201670"
  },
  {
    "text": "continually happening throughout the day a little bit of just light loading then during the business hours there's a lot",
    "start": "2201670",
    "end": "2208089"
  },
  {
    "text": "of reporting going on business users that sort of thing and then in the evening is when the typical large",
    "start": "2208089",
    "end": "2213250"
  },
  {
    "text": "ingestion of data happens obviously various users business reports analysts state DBA is that sort of thing so how I",
    "start": "2213250",
    "end": "2222130"
  },
  {
    "text": "would recommend setting up wlm for such a cynic scenario is I would create one",
    "start": "2222130",
    "end": "2227920"
  },
  {
    "text": "queue here which I'd call the ingestion queue and then keep the concurrency or the slots on that quite low probably to",
    "start": "2227920",
    "end": "2234550"
  },
  {
    "text": "but I would give each slot quite a bit of memory as well so in this case there's 20% memory assigned that means",
    "start": "2234550",
    "end": "2241210"
  },
  {
    "text": "that each slot or concurrency every query is gonna get about 10% memory I am",
    "start": "2241210",
    "end": "2248470"
  },
  {
    "text": "NOT going to enable in this case concurrency scaling for this queue I figure it's a fairly predictable workload I know it's when it's gonna run",
    "start": "2248470",
    "end": "2254440"
  },
  {
    "text": "and I'm fairly confident and that will work then I have my dashboard queue which is will say maybe tableau or",
    "start": "2254440",
    "end": "2259660"
  },
  {
    "text": "something like that and I'm gonna set the concurrency for this actually quite a bit higher up to 10 however each query",
    "start": "2259660",
    "end": "2265540"
  },
  {
    "text": "in this case is only going to get 5% of the cluster memory when it executes I'm also",
    "start": "2265540",
    "end": "2270550"
  },
  {
    "text": "in this case just as an example gonna set a timeout of two minutes I'm also",
    "start": "2270550",
    "end": "2276190"
  },
  {
    "text": "going to enable concurrency scaling for this queue so what that means is is if there were a large number of queries",
    "start": "2276190",
    "end": "2283060"
  },
  {
    "text": "that were running against this maybe you know you hit fifteen queries well what happened is is once you went over ten",
    "start": "2283060",
    "end": "2288790"
  },
  {
    "text": "the queries would begin to queue with concurrency scaling enabled we will in",
    "start": "2288790",
    "end": "2294460"
  },
  {
    "text": "the background create these transient clusters and begin routing those queries to those clusters clear the backlog and",
    "start": "2294460",
    "end": "2301690"
  },
  {
    "text": "then we can get rid of those transient clusters then in the end there I have",
    "start": "2301690",
    "end": "2307720"
  },
  {
    "text": "the default queue which is kind of the if you don't if you're not in doing ingestion you're not a dashboard end up",
    "start": "2307720",
    "end": "2313420"
  },
  {
    "text": "running in here I've just set the concurrency fairly low three 25% memory and it would kind of be business",
    "start": "2313420",
    "end": "2319600"
  },
  {
    "text": "requirements whether or not you wanted to enable concurrency scaling I just kind of left it as either it's just to",
    "start": "2319600",
    "end": "2325000"
  },
  {
    "text": "make it as an example one thing that some of you may have noticed is if you",
    "start": "2325000",
    "end": "2330640"
  },
  {
    "text": "add up that memory it only adds up to 95 percent I did that on purpose if you leave unassigned memory it goes",
    "start": "2330640",
    "end": "2336580"
  },
  {
    "text": "into a general pool and it'll be used on a first-come first-served basis by any queue any query so that's basically what",
    "start": "2336580",
    "end": "2342850"
  },
  {
    "text": "would happen to that memory and my best practices for this would be to enable at short query acceleration the next thing",
    "start": "2342850",
    "end": "2350440"
  },
  {
    "text": "is is we also have what we call a super user queue I wouldn't say it's like a totally hidden queue it's just it's not",
    "start": "2350440",
    "end": "2355750"
  },
  {
    "text": "visible in the console so a lot of people don't realize it's there it is in the documentation it is a queue that",
    "start": "2355750",
    "end": "2362440"
  },
  {
    "text": "anyone that who is a super user can change to but you do have to manually set that variable and you can be used",
    "start": "2362440",
    "end": "2370090"
  },
  {
    "text": "this special super user queue is typically used for DBA type operations",
    "start": "2370090",
    "end": "2375670"
  },
  {
    "text": "maybe you want to find out who's running what query and maybe cancel a query something like that this next piece here",
    "start": "2375670",
    "end": "2384640"
  },
  {
    "start": "2382000",
    "end": "2382000"
  },
  {
    "text": "qmr it's really an extension to wlm we built it primarily for those of you who",
    "start": "2384640",
    "end": "2392740"
  },
  {
    "text": "have users that connect your cluster that you wish didn't connect to your cluster it it really is meant for",
    "start": "2392740",
    "end": "2400420"
  },
  {
    "text": "runaway queries so you can programmatically set up a set of rules to detect say like a Cartesian product",
    "start": "2400420",
    "end": "2407440"
  },
  {
    "text": "or to detect if someone decides is a good idea to return a hundred million records and so you can have redshift",
    "start": "2407440",
    "end": "2413680"
  },
  {
    "text": "kill that query rather than it consuming a ton of resources you getting paged or",
    "start": "2413680",
    "end": "2420010"
  },
  {
    "text": "whatever and having to log in and find out who's doing what and kill someone's query so it was really meant to",
    "start": "2420010",
    "end": "2425349"
  },
  {
    "text": "programmatically take care of that the other use case that we found very useful",
    "start": "2425349",
    "end": "2430660"
  },
  {
    "text": "it wasn't what this was primarily built for was logging queries we when we built it we didn't want just to obviously",
    "start": "2430660",
    "end": "2436839"
  },
  {
    "text": "abort maybe in certain cases you might just want to log one best practice that I give for a lot of customers that are",
    "start": "2436839",
    "end": "2442420"
  },
  {
    "text": "setting up qmr is just set up rules to log these log long-running queries or",
    "start": "2442420",
    "end": "2448059"
  },
  {
    "text": "these expensive queries even if you don't want to just programmatically kill them and that way you can go back",
    "start": "2448059",
    "end": "2453339"
  },
  {
    "text": "through and look at the sequel and what's running and maybe adjust things just some examples of what you can kind",
    "start": "2453339",
    "end": "2460390"
  },
  {
    "text": "of do with qmr and how its kind of structured and how it's like this if then and you know you pick these various metrics like I said one of my favorite",
    "start": "2460390",
    "end": "2467410"
  },
  {
    "text": "ones is the return row count the reason I bring that one up is typically if you're returning a large number of rows",
    "start": "2467410",
    "end": "2473920"
  },
  {
    "text": "it actually is better to just do use the unload command and unload it to s3 into",
    "start": "2473920",
    "end": "2480130"
  },
  {
    "text": "say CSV format or something like that it's significantly faster than dragging the data through the leader node so just",
    "start": "2480130",
    "end": "2489400"
  },
  {
    "start": "2488000",
    "end": "2488000"
  },
  {
    "text": "general best practices on wlm in qumar I always say keep the number of queues to a minimum sometimes I'll get on site",
    "start": "2489400",
    "end": "2495849"
  },
  {
    "text": "with a customer they'll have like six cues defined and then this sequel query or this script I have on github here",
    "start": "2495849",
    "end": "2502329"
  },
  {
    "text": "this is a script that you can execute on an existing cluster and it'll look at like the last three days of what's",
    "start": "2502329",
    "end": "2509200"
  },
  {
    "text": "happened and it'll tell you the peak number or a queue depth on every one of",
    "start": "2509200",
    "end": "2514420"
  },
  {
    "text": "your cues what I usually see is customers that define like six queues they have like three of them that are",
    "start": "2514420",
    "end": "2520270"
  },
  {
    "text": "unused and they have all these resources assigned to them so usually I would say keep things to a minimum on the number",
    "start": "2520270",
    "end": "2526450"
  },
  {
    "text": "of wlm cues three or four it's probably a good limit use wlm to ingest your in",
    "start": "2526450",
    "end": "2535480"
  },
  {
    "text": "PLT especially during the day I usually say during the day maybe keep it to like 2 maybe 3 or else it will have an impact",
    "start": "2535480",
    "end": "2542500"
  },
  {
    "text": "on your your queries your reports and such um the global number of concurrency or",
    "start": "2542500",
    "end": "2549300"
  },
  {
    "text": "number of slots and the clusters we usually recommend about 15 that's about where your that's a sweet spot where",
    "start": "2549300",
    "end": "2555220"
  },
  {
    "text": "you're gonna get peak performance that's why we would give that recommendation you technically can tune it all the way",
    "start": "2555220",
    "end": "2560320"
  },
  {
    "text": "up to 50 however if you tune it all the way up to 50 that means the most any",
    "start": "2560320",
    "end": "2565330"
  },
  {
    "text": "slot could have would be 2% memory basically so if you just want eyes gonna have 50 slots every query we get 2% and",
    "start": "2565330",
    "end": "2572500"
  },
  {
    "text": "you might end up with a whole bunch of queries that starts spilling a disk so keep that to about 15 or less I usually",
    "start": "2572500",
    "end": "2578680"
  },
  {
    "text": "find it somewhere between 12 and 18 is the sweet spot for most customers it's about 15 is bang-on last thing I",
    "start": "2578680",
    "end": "2586990"
  },
  {
    "text": "mentioned log log log your long log your long-running queries with qmr and save",
    "start": "2586990",
    "end": "2593350"
  },
  {
    "text": "that Super User queue for you know admin tasks so cluster sizing one of the most",
    "start": "2593350",
    "end": "2601180"
  },
  {
    "text": "frequently asked things I get when customers especially are starting at POC we have two different flavors of nodes",
    "start": "2601180",
    "end": "2607840"
  },
  {
    "text": "in redshift one is our DC 2 which is our dense compute we call it which is SSD",
    "start": "2607840",
    "end": "2613600"
  },
  {
    "text": "backed and our dense storage node type or ds2 that's magnetic disk factor you",
    "start": "2613600",
    "end": "2618760"
  },
  {
    "text": "know just HDD then each of these node types come in two different sizes we",
    "start": "2618760",
    "end": "2624700"
  },
  {
    "text": "have a smaller size and an 8x cell size basically you can see one thing there's",
    "start": "2624700",
    "end": "2630730"
  },
  {
    "text": "call out you can see the slices there two or sixteen depending on the small instance type or the large one so the",
    "start": "2630730",
    "end": "2640840"
  },
  {
    "start": "2638000",
    "end": "2638000"
  },
  {
    "text": "first thing I'll always have a customer do is calculate the amount of uncompressed data that they believe",
    "start": "2640840",
    "end": "2647650"
  },
  {
    "text": "they're gonna have in their redshift cluster this just needs to be a ballpark estimate then when I say is we're gonna",
    "start": "2647650",
    "end": "2653200"
  },
  {
    "text": "assume that we're gonna have three times compression ratio on this data that's a very conservative number chances are",
    "start": "2653200",
    "end": "2659260"
  },
  {
    "text": "there's a really good chance in fact it'll be closer to four times and then we want to usually target somewhere I",
    "start": "2659260",
    "end": "2664840"
  },
  {
    "text": "think the sweet spot is around thirty to forty percent free space ideally I'd say you should be using at",
    "start": "2664840",
    "end": "2671880"
  },
  {
    "text": "least 15% of the cluster that'd probably be like the you should at least use that and I would say no more than 80% so it's",
    "start": "2671880",
    "end": "2679049"
  },
  {
    "text": "kind of the range where you want to be but I think the sweet spots between 30 and 40 and that's what I try and target then based on your performance",
    "start": "2679049",
    "end": "2686160"
  },
  {
    "text": "requirements you're going to either pick the magnetic discs or the SSDs so if you're like I really need",
    "start": "2686160",
    "end": "2691349"
  },
  {
    "text": "high-performance pick SSDs magnetic disks and we always size redshift by the",
    "start": "2691349",
    "end": "2696509"
  },
  {
    "text": "amount of data that you have and pick the appropriate platform so just as an",
    "start": "2696509",
    "end": "2701789"
  },
  {
    "text": "example say I have 20 terabytes of uncompressed data what's going to end up happening is we'll just assume that's",
    "start": "2701789",
    "end": "2707279"
  },
  {
    "text": "gonna be at 6.7 terabytes after factoring in compression then depending on your performance requirements we're",
    "start": "2707279",
    "end": "2713579"
  },
  {
    "text": "either gonna pick four of those dc28 XLS or five of our ds2 extra-large notes",
    "start": "2713579",
    "end": "2719789"
  },
  {
    "text": "both of those give approximately ten terabytes of capacity land us in that",
    "start": "2719789",
    "end": "2725130"
  },
  {
    "text": "estimated sweet spot if you end up getting better compression than you thought you can resize if you the",
    "start": "2725130",
    "end": "2732209"
  },
  {
    "text": "magnetic disks you started with weren't giving you the performance you needed you can also resize into a new into the",
    "start": "2732209",
    "end": "2738869"
  },
  {
    "text": "other node type and I'll talk about that right now so we have two types of",
    "start": "2738869",
    "end": "2743880"
  },
  {
    "start": "2741000",
    "end": "2741000"
  },
  {
    "text": "resizes and redshift we recently released what we call a stick resize so if some of you haven't heard of that that is actually quite new the classic",
    "start": "2743880",
    "end": "2752369"
  },
  {
    "text": "resize which was the resize our old resize basically what it does is it",
    "start": "2752369",
    "end": "2758099"
  },
  {
    "text": "transfers data from the your original cluster into a new one that we provision",
    "start": "2758099",
    "end": "2763680"
  },
  {
    "text": "and that's why we can change the node type one other interesting thing you can do with it is you can also enable or",
    "start": "2763680",
    "end": "2769979"
  },
  {
    "text": "disable encryption on your redshift cluster using the reason that resize the",
    "start": "2769979",
    "end": "2776789"
  },
  {
    "text": "new elastic resize what it does is it you we add and remove nodes from an",
    "start": "2776789",
    "end": "2782400"
  },
  {
    "text": "existing cluster so we're not doing the whole data transfer thing I'll illustrate both of these so let's just",
    "start": "2782400",
    "end": "2789089"
  },
  {
    "text": "say we have a three node cluster here these little white lines those are there",
    "start": "2789089",
    "end": "2794670"
  },
  {
    "text": "16 of those I counted exactly so you have 16 little slices there on this",
    "start": "2794670",
    "end": "2801509"
  },
  {
    "text": "c28 Excel node type let's say we wanted to resize this up to four notes what we",
    "start": "2801509",
    "end": "2808889"
  },
  {
    "text": "would do or what the service is gonna do is it's gonna provision in the background a whole new cluster with the four nodes we're gonna drop your source",
    "start": "2808889",
    "end": "2816869"
  },
  {
    "text": "cluster into read-only mode then we're gonna do a binary transfer all of this data and we're gonna redistribute it all",
    "start": "2816869",
    "end": "2822599"
  },
  {
    "text": "across these slices once this operation is complete we're then going to repoint",
    "start": "2822599",
    "end": "2828239"
  },
  {
    "text": "the dns and we're going to bounce the connections to the original cluster when everyone reconnects they reconnect it to",
    "start": "2828239",
    "end": "2834779"
  },
  {
    "text": "this new cluster and that's how resize worked up until a few weeks ago",
    "start": "2834779",
    "end": "2840239"
  },
  {
    "text": "and still does work if you select classic resize so now I'm going to talk",
    "start": "2840239",
    "end": "2847199"
  },
  {
    "text": "about elastic resize I'm gonna go with the same example so I have the three node cluster here each node with 16",
    "start": "2847199",
    "end": "2853889"
  },
  {
    "text": "slices and when I select classic resize to go up to or elastic resize to go up",
    "start": "2853889",
    "end": "2860279"
  },
  {
    "text": "to four nodes what we're gonna do is we're gonna immediately provision this new node we're gonna attach it to your",
    "start": "2860279",
    "end": "2865829"
  },
  {
    "text": "cluster we're also going to begin backing up all the data in the cluster it's just an incremental backup it's actually really fast this operation",
    "start": "2865829",
    "end": "2873149"
  },
  {
    "text": "takes roughly about 15 ish minutes I know I have a bit of a time range on there there is some certain cases where",
    "start": "2873149",
    "end": "2878849"
  },
  {
    "text": "it does take longer especially if you're ingesting data and such well this is happening so this when this part ends",
    "start": "2878849",
    "end": "2886439"
  },
  {
    "text": "what I'm gonna do is I'm going to redistribute some of these slices so if you see these others those 16 little",
    "start": "2886439",
    "end": "2893130"
  },
  {
    "text": "white lines some of them just moved over so what we do is is we redistribute some",
    "start": "2893130",
    "end": "2898829"
  },
  {
    "text": "of those slices from those three nodes to that fourth node in this example now",
    "start": "2898829",
    "end": "2904109"
  },
  {
    "text": "every one of the nodes has 12 slices so that's essentially how we do the resize",
    "start": "2904109",
    "end": "2909269"
  },
  {
    "text": "this operation to shuffle these slices around takes about four minutes during",
    "start": "2909269",
    "end": "2914909"
  },
  {
    "text": "this time all of the connections all of the in-flight queries are parked and held will resubmit them once this is",
    "start": "2914909",
    "end": "2922619"
  },
  {
    "text": "done there with the exception of some write rights that occur in a transaction",
    "start": "2922619",
    "end": "2929099"
  },
  {
    "text": "unfortunately those ones will be rolled back once that operation of shuffling is done",
    "start": "2929099",
    "end": "2935950"
  },
  {
    "text": "we then begin to stream the data out of s3 back into this new compute node that's been added to your cluster and",
    "start": "2935950",
    "end": "2942010"
  },
  {
    "text": "the cluster is fully readable and writeable during this time if you have in-flight queries we'll just fetch that",
    "start": "2942010",
    "end": "2947830"
  },
  {
    "text": "data immediately off s3 to fulfill that query so with elastic resize way of a",
    "start": "2947830",
    "end": "2956920"
  },
  {
    "text": "depending on your instance type that you start with and your original configuration there are limits and",
    "start": "2956920",
    "end": "2962890"
  },
  {
    "text": "ranges to where you can grow so for example I'm gonna go through an example here with four nodes so if we started",
    "start": "2962890",
    "end": "2969700"
  },
  {
    "text": "with four nodes as our original configuration and we were on the smaller instance types because they have so few",
    "start": "2969700",
    "end": "2975130"
  },
  {
    "text": "slices the only valid configurations to go on this one would be from four down",
    "start": "2975130",
    "end": "2980500"
  },
  {
    "text": "to two you'd go back up to four if you wanted or all the way up to eight so those are the valid configurations for",
    "start": "2980500",
    "end": "2986440"
  },
  {
    "text": "that if however you're on the eight excel node types you've a lot more choice so in this case I have the",
    "start": "2986440",
    "end": "2992560"
  },
  {
    "text": "started with four I can either incrementally go up all the way up to eight I can pick or I could just jump",
    "start": "2992560",
    "end": "2998500"
  },
  {
    "text": "all the way up to eight if I wanted to or I could go all the way down to two or three and you can pick and move anywhere",
    "start": "2998500",
    "end": "3004440"
  },
  {
    "text": "within that based on what your clusters original slice count was so when would",
    "start": "3004440",
    "end": "3013050"
  },
  {
    "text": "you use these well one of the cases this wasn't primarily what it was meant for",
    "start": "3013050",
    "end": "3018450"
  },
  {
    "text": "but one that we had a lot of customers using it for when we had this in beta was using it for when they knew they had",
    "start": "3018450",
    "end": "3025170"
  },
  {
    "text": "a spiky workload so we had customers using it especially for a nightly batch ingestion to speed that up so for",
    "start": "3025170",
    "end": "3032550"
  },
  {
    "text": "example they would kick off an elastic resize double like the size of their cluster for example so they get significantly more compute do their",
    "start": "3032550",
    "end": "3039120"
  },
  {
    "text": "ingestion and then elastically resize back down once that was completed you",
    "start": "3039120",
    "end": "3044220"
  },
  {
    "text": "can use it to incrementally add storage especially on the 8x cells so it's really useful for that so if you say for example in the example I had with three",
    "start": "3044220",
    "end": "3050880"
  },
  {
    "text": "nodes needed extra storage and you want to go up to four nodes I would recommend using elastic resize if that's an option",
    "start": "3050880",
    "end": "3057090"
  },
  {
    "text": "you'll use classic resize when you move between SSDs hdds or if you wanted to",
    "start": "3057090",
    "end": "3062580"
  },
  {
    "text": "change between a small node type and a larger node type the biggest difference between them is",
    "start": "3062580",
    "end": "3069180"
  },
  {
    "text": "the amount of time so with the elastic resize you get four ish minutes of",
    "start": "3069180",
    "end": "3074430"
  },
  {
    "text": "parked connections with the classic resize it's somewhere between you know",
    "start": "3074430",
    "end": "3080430"
  },
  {
    "text": "usually I say it's an overnight operation it can range depend on how much data we have to move anywhere between 1 and 24 hours we always try and",
    "start": "3080430",
    "end": "3087240"
  },
  {
    "text": "have these completed within 24 hours so best practices on cluster sizing I",
    "start": "3087240",
    "end": "3095280"
  },
  {
    "start": "3091000",
    "end": "3091000"
  },
  {
    "text": "always recommend 2 nodes we have an option to run what we call single node clusters and redshift single node",
    "start": "3095280",
    "end": "3101790"
  },
  {
    "text": "clusters are really meant for dev tests QA just some experimentation that sort",
    "start": "3101790",
    "end": "3106829"
  },
  {
    "text": "of thing I always recommend using a multi node cluster in production so you get that redundancy the safety of the",
    "start": "3106829",
    "end": "3113190"
  },
  {
    "text": "second copy always maintain at least 20 percent free space in your red ship",
    "start": "3113190",
    "end": "3118410"
  },
  {
    "text": "cluster if you can you can usually maybe might be able to get away with a little bit more than that but what will end up",
    "start": "3118410",
    "end": "3124140"
  },
  {
    "text": "happening is if there are temporary tables that are needed intermediate tables that sort of thing those consume",
    "start": "3124140",
    "end": "3129569"
  },
  {
    "text": "disk space so for a lot of cases it just simply makes sense to you know add an",
    "start": "3129569",
    "end": "3135180"
  },
  {
    "text": "extra node add some capacity so you don't get it out of disk errors if you",
    "start": "3135180",
    "end": "3140460"
  },
  {
    "text": "happen to be one of these people who are still on a DC one I know there are some redshift users in here if you're using DC one instance",
    "start": "3140460",
    "end": "3147720"
  },
  {
    "text": "types please upgrade to DC 2 instance types you can resize from DC 1 to DC -",
    "start": "3147720",
    "end": "3154940"
  },
  {
    "text": "it was about a year ago that we launched this new instance type and it is significantly faster in a lot of cases",
    "start": "3154940",
    "end": "3161520"
  },
  {
    "text": "they can be twice as fast we have made it very easy to upgrade you can just do the resize if you have reserved",
    "start": "3161520",
    "end": "3168150"
  },
  {
    "text": "instances on DC one we are letting you migrate your reserved instance at no",
    "start": "3168150",
    "end": "3173730"
  },
  {
    "text": "cost to a DC - reserved instance so you can just move the reserved instance and you can do that now in the console we",
    "start": "3173730",
    "end": "3180720"
  },
  {
    "text": "want you to be using the latest hardware with redshift wherever possible we want you to have the best experience with redshift so we don't ever want you",
    "start": "3180720",
    "end": "3187260"
  },
  {
    "text": "lingering around on an old hardware some additional resources I had these",
    "start": "3187260",
    "end": "3194910"
  },
  {
    "text": "github links littered throughout the presentation we have what we call Amazon",
    "start": "3194910",
    "end": "3200460"
  },
  {
    "text": "redshift utilities on github this is a collection of scripts and views and that",
    "start": "3200460",
    "end": "3205500"
  },
  {
    "text": "sort of thing that we internally have built when we work with customers and",
    "start": "3205500",
    "end": "3210780"
  },
  {
    "text": "we've exposed that and put that out there so there are a lot of this things that we find very useful for diagnosing",
    "start": "3210780",
    "end": "3217290"
  },
  {
    "text": "and debugging things I particularly love the admin screen in men views a couple",
    "start": "3217290",
    "end": "3225119"
  },
  {
    "text": "of blogs that are very useful the design playbook is a very in-depth blog it goes",
    "start": "3225119",
    "end": "3231810"
  },
  {
    "text": "through a ton of different concepts it talks about some of the more advanced things the top ten tuning techniques a",
    "start": "3231810",
    "end": "3238619"
  },
  {
    "text": "lot of it I've covered in here but there are a couple of things I'm not able to fit into a one-hour session that are on",
    "start": "3238619",
    "end": "3244650"
  },
  {
    "text": "there as well and then we have if you're using Amazon redshift spectrum a handful",
    "start": "3244650",
    "end": "3249960"
  },
  {
    "text": "of best practices for that as well thank you guys very much I hope you guys",
    "start": "3249960",
    "end": "3255869"
  },
  {
    "text": "enjoyed this and [Applause]",
    "start": "3255869",
    "end": "3262329"
  }
]