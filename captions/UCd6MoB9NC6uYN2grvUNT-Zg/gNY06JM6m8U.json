[
  {
    "text": "welcome once again to the multi-art",
    "start": "880",
    "end": "3320"
  },
  {
    "text": "series on best practices for building",
    "start": "3320",
    "end": "5920"
  },
  {
    "text": "geni applications on",
    "start": "5920",
    "end": "8519"
  },
  {
    "text": "AWS in this video I will show you how I",
    "start": "8519",
    "end": "11960"
  },
  {
    "text": "improved my rack system with a metric",
    "start": "11960",
    "end": "14480"
  },
  {
    "text": "driven development approach and how you",
    "start": "14480",
    "end": "16800"
  },
  {
    "text": "can do it too my name is Felix sumaher",
    "start": "16800",
    "end": "20400"
  },
  {
    "text": "and I'm a AWS Solutions architect",
    "start": "20400",
    "end": "23560"
  },
  {
    "text": "covering geni we all know that retrieval",
    "start": "23560",
    "end": "27640"
  },
  {
    "text": "augumented generation or short rack is",
    "start": "27640",
    "end": "30560"
  },
  {
    "text": "an extremely useful Paradigm for",
    "start": "30560",
    "end": "32880"
  },
  {
    "text": "augmenting llms with custom data it",
    "start": "32880",
    "end": "36440"
  },
  {
    "text": "enables the llm to present accurate",
    "start": "36440",
    "end": "39640"
  },
  {
    "text": "relevant information with Source",
    "start": "39640",
    "end": "42039"
  },
  {
    "text": "attribution which not only increases",
    "start": "42039",
    "end": "44520"
  },
  {
    "text": "trust but also unlocks a variety of use",
    "start": "44520",
    "end": "47800"
  },
  {
    "text": "cases for example here in our notebook",
    "start": "47800",
    "end": "50960"
  },
  {
    "text": "it enables businesses to automate",
    "start": "50960",
    "end": "53680"
  },
  {
    "text": "information extraction from 10K",
    "start": "53680",
    "end": "55960"
  },
  {
    "text": "documents which then can be used",
    "start": "55960",
    "end": "58719"
  },
  {
    "text": "Downstream for fur",
    "start": "58719",
    "end": "61680"
  },
  {
    "text": "analysis unfortunately unlike for llms",
    "start": "61680",
    "end": "65239"
  },
  {
    "text": "there are no benchmarks or leaderboards",
    "start": "65239",
    "end": "67280"
  },
  {
    "text": "for an entire rack system or its",
    "start": "67280",
    "end": "70600"
  },
  {
    "text": "components and this can make it",
    "start": "70600",
    "end": "73119"
  },
  {
    "text": "difficult to assess with which",
    "start": "73119",
    "end": "75720"
  },
  {
    "text": "components and configuration to start",
    "start": "75720",
    "end": "78159"
  },
  {
    "text": "with or what to optimize when working",
    "start": "78159",
    "end": "80960"
  },
  {
    "text": "with an existing rag",
    "start": "80960",
    "end": "83079"
  },
  {
    "text": "system and to make matters worse there",
    "start": "83079",
    "end": "85680"
  },
  {
    "text": "are many factors that can impact the",
    "start": "85680",
    "end": "88119"
  },
  {
    "text": "performance of the rack system and that",
    "start": "88119",
    "end": "91159"
  },
  {
    "text": "is why it's crucial to have a systematic",
    "start": "91159",
    "end": "94119"
  },
  {
    "text": "evaluation approach without it a change",
    "start": "94119",
    "end": "97560"
  },
  {
    "text": "in one part of the system for example V",
    "start": "97560",
    "end": "99920"
  },
  {
    "text": "chunk size that determines how a Source",
    "start": "99920",
    "end": "103520"
  },
  {
    "text": "Tax is stored in the knowledge base",
    "start": "103520",
    "end": "106200"
  },
  {
    "text": "could have an unintended impact on other",
    "start": "106200",
    "end": "109159"
  },
  {
    "text": "parts of the system that could go",
    "start": "109159",
    "end": "112040"
  },
  {
    "text": "unnoticed unfortunately we don't have to",
    "start": "112040",
    "end": "115520"
  },
  {
    "text": "create a new evaluation framework from",
    "start": "115520",
    "end": "118159"
  },
  {
    "text": "scratch rather the can use an existing",
    "start": "118159",
    "end": "121320"
  },
  {
    "text": "open- Source framework such as retrieval",
    "start": "121320",
    "end": "124240"
  },
  {
    "text": "augumented generation assessment or",
    "start": "124240",
    "end": "127039"
  },
  {
    "text": "ragas as it is more commonly known ragas",
    "start": "127039",
    "end": "130879"
  },
  {
    "text": "is an evaluation framework to quantify",
    "start": "130879",
    "end": "134239"
  },
  {
    "text": "the performance of the rack system",
    "start": "134239",
    "end": "136519"
  },
  {
    "text": "across different metrics it includes",
    "start": "136519",
    "end": "139760"
  },
  {
    "text": "metrics covering wide variety of use",
    "start": "139760",
    "end": "142800"
  },
  {
    "text": "cases and enables a metric driven",
    "start": "142800",
    "end": "146040"
  },
  {
    "text": "development approach if those metrics",
    "start": "146040",
    "end": "148720"
  },
  {
    "text": "are tracked in experiments over",
    "start": "148720",
    "end": "152200"
  },
  {
    "text": "time this notebook outlines how to",
    "start": "152200",
    "end": "154920"
  },
  {
    "text": "improve retrieval performance",
    "start": "154920",
    "end": "157080"
  },
  {
    "text": "systematically by leveraging the metrics",
    "start": "157080",
    "end": "160959"
  },
  {
    "text": "that Dan covered in the previous video",
    "start": "160959",
    "end": "164760"
  },
  {
    "text": "which was context Precision context",
    "start": "164760",
    "end": "167519"
  },
  {
    "text": "recall faithfulness answer relevancy and",
    "start": "167519",
    "end": "171239"
  },
  {
    "text": "answer",
    "start": "171239",
    "end": "172480"
  },
  {
    "text": "correctness if you're not familiar with",
    "start": "172480",
    "end": "174760"
  },
  {
    "text": "these met rats then I definitely",
    "start": "174760",
    "end": "176879"
  },
  {
    "text": "recommend to watch that video first but",
    "start": "176879",
    "end": "179680"
  },
  {
    "text": "I I've also included some brief",
    "start": "179680",
    "end": "181680"
  },
  {
    "text": "descriptions here in the notebook",
    "start": "181680",
    "end": "183879"
  },
  {
    "text": "itself for now let me just say that if",
    "start": "183879",
    "end": "187400"
  },
  {
    "text": "you've been in it for a while then",
    "start": "187400",
    "end": "189840"
  },
  {
    "text": "you're familiar with the saying garbage",
    "start": "189840",
    "end": "192040"
  },
  {
    "text": "in garbage out the same is true for rack",
    "start": "192040",
    "end": "195120"
  },
  {
    "text": "system then the retrieval part is not",
    "start": "195120",
    "end": "198519"
  },
  {
    "text": "working well there is no relevant",
    "start": "198519",
    "end": "200480"
  },
  {
    "text": "context to give to its llm component and",
    "start": "200480",
    "end": "204400"
  },
  {
    "text": "as a result you will be less likely to",
    "start": "204400",
    "end": "207480"
  },
  {
    "text": "get a meaningful response",
    "start": "207480",
    "end": "210519"
  },
  {
    "text": "and this is why for our information",
    "start": "210519",
    "end": "212720"
  },
  {
    "text": "extraction use case we evaluate and",
    "start": "212720",
    "end": "215799"
  },
  {
    "text": "prioritize the metrics in the following",
    "start": "215799",
    "end": "217879"
  },
  {
    "text": "order context recall context Precision",
    "start": "217879",
    "end": "221640"
  },
  {
    "text": "faithfulness answer relevance and then",
    "start": "221640",
    "end": "224400"
  },
  {
    "text": "answer",
    "start": "224400",
    "end": "226760"
  },
  {
    "text": "correctness the solution architecture is",
    "start": "228080",
    "end": "231080"
  },
  {
    "text": "still largely the same as in the",
    "start": "231080",
    "end": "233360"
  },
  {
    "text": "previous videos we primarily use a",
    "start": "233360",
    "end": "237079"
  },
  {
    "text": "notebook environment here for",
    "start": "237079",
    "end": "238439"
  },
  {
    "text": "illustration we use code editor hosted",
    "start": "238439",
    "end": "242040"
  },
  {
    "text": "in sagemaker studio to interact with",
    "start": "242040",
    "end": "245200"
  },
  {
    "text": "Amazon petrock which gives us access to",
    "start": "245200",
    "end": "248360"
  },
  {
    "text": "dozens of llms as a service and in",
    "start": "248360",
    "end": "252200"
  },
  {
    "text": "production you would typically embed",
    "start": "252200",
    "end": "254159"
  },
  {
    "text": "this evaluation in a pipeline to ensure",
    "start": "254159",
    "end": "257400"
  },
  {
    "text": "continuous evaluations for every change",
    "start": "257400",
    "end": "260720"
  },
  {
    "text": "we will link this repo in the",
    "start": "260720",
    "end": "262880"
  },
  {
    "text": "description below so you can get started",
    "start": "262880",
    "end": "265160"
  },
  {
    "text": "quickly and not only does this repo",
    "start": "265160",
    "end": "268160"
  },
  {
    "text": "include a stepbystep instructions on how",
    "start": "268160",
    "end": "270720"
  },
  {
    "text": "to get started but it also includes",
    "start": "270720",
    "end": "274039"
  },
  {
    "text": "infrastructure as a code with cloud",
    "start": "274039",
    "end": "275880"
  },
  {
    "text": "formation so if you're logged in into",
    "start": "275880",
    "end": "279479"
  },
  {
    "text": "your AWS Management console and you have",
    "start": "279479",
    "end": "282160"
  },
  {
    "text": "sufficient access then you can just",
    "start": "282160",
    "end": "284240"
  },
  {
    "text": "click on this button here and it will",
    "start": "284240",
    "end": "287320"
  },
  {
    "text": "deploy everything for you once",
    "start": "287320",
    "end": "290240"
  },
  {
    "text": "everything is deployed you go into code",
    "start": "290240",
    "end": "292800"
  },
  {
    "text": "editor in sagemaker studio and do a git",
    "start": "292800",
    "end": "295759"
  },
  {
    "text": "clone office",
    "start": "295759",
    "end": "298120"
  },
  {
    "text": "repository and if you we have a",
    "start": "298120",
    "end": "300520"
  },
  {
    "text": "pre-existing Vector store that you want",
    "start": "300520",
    "end": "302360"
  },
  {
    "text": "to leverage you can update the",
    "start": "302360",
    "end": "304880"
  },
  {
    "text": "environment file",
    "start": "304880",
    "end": "306600"
  },
  {
    "text": "here and if you don't you can just leave",
    "start": "306600",
    "end": "310560"
  },
  {
    "text": "everything as is and jump right into",
    "start": "310560",
    "end": "312960"
  },
  {
    "text": "this notebook here in this notebook we",
    "start": "312960",
    "end": "316039"
  },
  {
    "text": "first create a cond environment and",
    "start": "316039",
    "end": "318639"
  },
  {
    "text": "install all of the",
    "start": "318639",
    "end": "320479"
  },
  {
    "text": "dependencies we load all of our",
    "start": "320479",
    "end": "323120"
  },
  {
    "text": "environment variables",
    "start": "323120",
    "end": "325440"
  },
  {
    "text": "here you can see we have one environment",
    "start": "325440",
    "end": "328360"
  },
  {
    "text": "variable here for MLF flow to track our",
    "start": "328360",
    "end": "331360"
  },
  {
    "text": "experiments over time but to keep this",
    "start": "331360",
    "end": "334199"
  },
  {
    "text": "simple it's deactivated by a default so",
    "start": "334199",
    "end": "336919"
  },
  {
    "text": "you can run this notebook without mlflow",
    "start": "336919",
    "end": "339400"
  },
  {
    "text": "as well next we download our Crown trof",
    "start": "339400",
    "end": "343520"
  },
  {
    "text": "data",
    "start": "343520",
    "end": "345840"
  },
  {
    "text": "set which is still the same as in our uh",
    "start": "346120",
    "end": "349840"
  },
  {
    "text": "previous video it is a set of about 20",
    "start": "349840",
    "end": "352680"
  },
  {
    "text": "question answer pairs with relevant",
    "start": "352680",
    "end": "355080"
  },
  {
    "text": "context and I would definitely recommend",
    "start": "355080",
    "end": "357759"
  },
  {
    "text": "to use a bigger CR proof dat data set",
    "start": "357759",
    "end": "360160"
  },
  {
    "text": "for an actual production scenario but",
    "start": "360160",
    "end": "363199"
  },
  {
    "text": "more important than the number of",
    "start": "363199",
    "end": "365160"
  },
  {
    "text": "questions is that it is a representative",
    "start": "365160",
    "end": "368520"
  },
  {
    "text": "sample and that you get this framework",
    "start": "368520",
    "end": "370720"
  },
  {
    "text": "in place and then we download our data",
    "start": "370720",
    "end": "373599"
  },
  {
    "text": "set an Amazon 10K report and read it",
    "start": "373599",
    "end": "377199"
  },
  {
    "text": "into a list of strings which is a sample",
    "start": "377199",
    "end": "380199"
  },
  {
    "text": "document from which we want to extract",
    "start": "380199",
    "end": "383080"
  },
  {
    "text": "the information from next we create a",
    "start": "383080",
    "end": "385800"
  },
  {
    "text": "vector database in this case open search",
    "start": "385800",
    "end": "388000"
  },
  {
    "text": "serverless and uh a Bedrock knowledge",
    "start": "388000",
    "end": "390720"
  },
  {
    "text": "base which we will use in our",
    "start": "390720",
    "end": "393560"
  },
  {
    "text": "evaluation once that is",
    "start": "393560",
    "end": "395919"
  },
  {
    "text": "done we Define our prompt templates that",
    "start": "395919",
    "end": "399160"
  },
  {
    "text": "we want to use for",
    "start": "399160",
    "end": "402720"
  },
  {
    "text": "evaluation here you can see we have two",
    "start": "403160",
    "end": "405560"
  },
  {
    "text": "prompt templates one with step-by-step",
    "start": "405560",
    "end": "408520"
  },
  {
    "text": "instructions and another one with an",
    "start": "408520",
    "end": "411240"
  },
  {
    "text": "example of a human assistant",
    "start": "411240",
    "end": "413800"
  },
  {
    "text": "workflow and now to simplify the",
    "start": "413800",
    "end": "417680"
  },
  {
    "text": "evaluation we created a few helper",
    "start": "417680",
    "end": "420280"
  },
  {
    "text": "methods we don't have time to go into",
    "start": "420280",
    "end": "422720"
  },
  {
    "text": "the details of each of them but the",
    "start": "422720",
    "end": "426000"
  },
  {
    "text": "first one is to create a rag system on",
    "start": "426000",
    "end": "428840"
  },
  {
    "text": "the Fly for a given set of parameters",
    "start": "428840",
    "end": "432080"
  },
  {
    "text": "and the second one is actually a repper",
    "start": "432080",
    "end": "434280"
  },
  {
    "text": "to work with ragas and the latest",
    "start": "434280",
    "end": "437000"
  },
  {
    "text": "badrock models you could use length",
    "start": "437000",
    "end": "440039"
  },
  {
    "text": "chain here as well but if you want to",
    "start": "440039",
    "end": "441960"
  },
  {
    "text": "run ragas evaluations with the latest",
    "start": "441960",
    "end": "444759"
  },
  {
    "text": "and greatest llms then it is easier to",
    "start": "444759",
    "end": "448080"
  },
  {
    "text": "just create your own rapper class CL and",
    "start": "448080",
    "end": "450560"
  },
  {
    "text": "it also gives you a bit more control",
    "start": "450560",
    "end": "452639"
  },
  {
    "text": "here for example we capture the number",
    "start": "452639",
    "end": "455000"
  },
  {
    "text": "of tokens used and the use that",
    "start": "455000",
    "end": "457960"
  },
  {
    "text": "information to calculate",
    "start": "457960",
    "end": "460599"
  },
  {
    "text": "cost and then the last one is a helper",
    "start": "460599",
    "end": "463440"
  },
  {
    "text": "class to simplify the experiment with",
    "start": "463440",
    "end": "466120"
  },
  {
    "text": "ragas for the given rag",
    "start": "466120",
    "end": "469000"
  },
  {
    "text": "system so now that we've covered the",
    "start": "469000",
    "end": "472720"
  },
  {
    "text": "setup and our Benchmark data set let's",
    "start": "472720",
    "end": "475400"
  },
  {
    "text": "talk about the experiment",
    "start": "475400",
    "end": "477879"
  },
  {
    "text": "parameters all the experiment parameters",
    "start": "477879",
    "end": "480919"
  },
  {
    "text": "that we are trying to optimize are",
    "start": "480919",
    "end": "483280"
  },
  {
    "text": "either based on how the source documents",
    "start": "483280",
    "end": "486120"
  },
  {
    "text": "are represented in the vector database",
    "start": "486120",
    "end": "489720"
  },
  {
    "text": "or how we search for them once they are",
    "start": "489720",
    "end": "492400"
  },
  {
    "text": "stored in the vector",
    "start": "492400",
    "end": "494240"
  },
  {
    "text": "database and we perform the evaluation",
    "start": "494240",
    "end": "498199"
  },
  {
    "text": "in stages so first we do tests to find",
    "start": "498199",
    "end": "501879"
  },
  {
    "text": "the best chunk size then in the second",
    "start": "501879",
    "end": "505000"
  },
  {
    "text": "series of tests we identify the best",
    "start": "505000",
    "end": "507879"
  },
  {
    "text": "performing text splitter for our data",
    "start": "507879",
    "end": "510599"
  },
  {
    "text": "set and then after we have identified",
    "start": "510599",
    "end": "513518"
  },
  {
    "text": "the best performing Tex splitter we use",
    "start": "513519",
    "end": "516120"
  },
  {
    "text": "the best chunk size and tex splitter to",
    "start": "516120",
    "end": "518719"
  },
  {
    "text": "identify the best performing embedding",
    "start": "518719",
    "end": "521200"
  },
  {
    "text": "model and then we use the best",
    "start": "521200",
    "end": "523560"
  },
  {
    "text": "performing embedding model to identify",
    "start": "523560",
    "end": "526279"
  },
  {
    "text": "the best retriever",
    "start": "526279",
    "end": "527839"
  },
  {
    "text": "method and last but not least we then",
    "start": "527839",
    "end": "530800"
  },
  {
    "text": "use with the best retriever method we",
    "start": "530800",
    "end": "534240"
  },
  {
    "text": "evaluate the best prompt template for",
    "start": "534240",
    "end": "537480"
  },
  {
    "text": "rec system",
    "start": "537480",
    "end": "540839"
  },
  {
    "text": "so here uh you can see how we are doing",
    "start": "542160",
    "end": "545120"
  },
  {
    "text": "that so in our first",
    "start": "545120",
    "end": "549079"
  },
  {
    "text": "experiment we use a token Tex",
    "start": "549079",
    "end": "552440"
  },
  {
    "text": "splitter with a chunk size of 128 and",
    "start": "552440",
    "end": "556279"
  },
  {
    "text": "the chunk overlap of 64 we use our rack",
    "start": "556279",
    "end": "560360"
  },
  {
    "text": "system helper class to on create on",
    "start": "560360",
    "end": "563040"
  },
  {
    "text": "thefly Rec",
    "start": "563040",
    "end": "565200"
  },
  {
    "text": "system and then we run our evaluation",
    "start": "565200",
    "end": "568920"
  },
  {
    "text": "with the ragus helper class that we",
    "start": "568920",
    "end": "572920"
  },
  {
    "text": "created earlier and once this ran we can",
    "start": "572920",
    "end": "577079"
  },
  {
    "text": "not only see the average llm query time",
    "start": "577079",
    "end": "581320"
  },
  {
    "text": "and the rack system cost for the llm and",
    "start": "581320",
    "end": "585000"
  },
  {
    "text": "embedding tokens but we can also see all",
    "start": "585000",
    "end": "587839"
  },
  {
    "text": "of the RAS metrics that we want to use",
    "start": "587839",
    "end": "590959"
  },
  {
    "text": "in our",
    "start": "590959",
    "end": "593440"
  },
  {
    "text": "comparison now we go ahead and actually",
    "start": "594920",
    "end": "597760"
  },
  {
    "text": "run the same thing with chunk sizes like",
    "start": "597760",
    "end": "602440"
  },
  {
    "text": "250 6",
    "start": "602440",
    "end": "605240"
  },
  {
    "text": "512 and",
    "start": "605240",
    "end": "607839"
  },
  {
    "text": "1,24 and when we compare and we can see",
    "start": "607839",
    "end": "613079"
  },
  {
    "text": "that the 512 token chunk size performed",
    "start": "613079",
    "end": "616600"
  },
  {
    "text": "best across our prioritized set of",
    "start": "616600",
    "end": "619800"
  },
  {
    "text": "metrics and then we essentially do the",
    "start": "619800",
    "end": "622760"
  },
  {
    "text": "same with a recursive character text",
    "start": "622760",
    "end": "625680"
  },
  {
    "text": "splitter and here we look at chunk sizes",
    "start": "625680",
    "end": "628399"
  },
  {
    "text": "of ,000 like here and then chunk size of",
    "start": "628399",
    "end": "634240"
  },
  {
    "text": "1,500 2,000 and",
    "start": "634240",
    "end": "637519"
  },
  {
    "text": "2,500 and once again once you run all of",
    "start": "637519",
    "end": "640440"
  },
  {
    "text": "these we can compare the",
    "start": "640440",
    "end": "643079"
  },
  {
    "text": "results and we can see that the",
    "start": "643079",
    "end": "646800"
  },
  {
    "text": "recursive character text splitter with a",
    "start": "646800",
    "end": "649240"
  },
  {
    "text": "chunk size of 2,500 performed best and",
    "start": "649240",
    "end": "653519"
  },
  {
    "text": "then we use the best recursive character",
    "start": "653519",
    "end": "657680"
  },
  {
    "text": "text splitter and best uh token text",
    "start": "657680",
    "end": "661639"
  },
  {
    "text": "splitter and compare those two against",
    "start": "661639",
    "end": "664800"
  },
  {
    "text": "each",
    "start": "664800",
    "end": "666920"
  },
  {
    "text": "other and we can see that the token text",
    "start": "666920",
    "end": "670160"
  },
  {
    "text": "splitter outperformed the character text",
    "start": "670160",
    "end": "672800"
  },
  {
    "text": "splitter in context recall which is in",
    "start": "672800",
    "end": "676079"
  },
  {
    "text": "blue here now we want to look at",
    "start": "676079",
    "end": "679680"
  },
  {
    "text": "different embedding models next here we",
    "start": "679680",
    "end": "682639"
  },
  {
    "text": "compare Titan V1 and V2 against cohere",
    "start": "682639",
    "end": "686920"
  },
  {
    "text": "multilingual and English and again we",
    "start": "686920",
    "end": "690440"
  },
  {
    "text": "run the same",
    "start": "690440",
    "end": "692440"
  },
  {
    "text": "set of rack systems and then we can",
    "start": "692440",
    "end": "697000"
  },
  {
    "text": "compare the",
    "start": "697000",
    "end": "699279"
  },
  {
    "text": "results and we can observe that the",
    "start": "699279",
    "end": "702560"
  },
  {
    "text": "coher",
    "start": "702560",
    "end": "703959"
  },
  {
    "text": "multilingual model performs very well",
    "start": "703959",
    "end": "707399"
  },
  {
    "text": "even at a significantly smaller token",
    "start": "707399",
    "end": "709720"
  },
  {
    "text": "trunk size such as 350 and we can also",
    "start": "709720",
    "end": "713279"
  },
  {
    "text": "see that the Titan V2 model is not only",
    "start": "713279",
    "end": "716839"
  },
  {
    "text": "more cost effective but also performs V1",
    "start": "716839",
    "end": "720720"
  },
  {
    "text": "in context precision and answer",
    "start": "720720",
    "end": "724240"
  },
  {
    "text": "relevancy next we look at different",
    "start": "724240",
    "end": "728240"
  },
  {
    "text": "retriever types here we",
    "start": "728240",
    "end": "732199"
  },
  {
    "text": "compare the approximate nearest neighbor",
    "start": "732199",
    "end": "736480"
  },
  {
    "text": "of uh open search with the MMR",
    "start": "736480",
    "end": "741240"
  },
  {
    "text": "implementation of open search against a",
    "start": "741240",
    "end": "743519"
  },
  {
    "text": "Bedrock knowledge based",
    "start": "743519",
    "end": "745240"
  },
  {
    "text": "Retriever and if I scroll to the result",
    "start": "745240",
    "end": "750079"
  },
  {
    "text": "you can observe that the Bedrock",
    "start": "750079",
    "end": "752639"
  },
  {
    "text": "knowledge based retriever which is a",
    "start": "752639",
    "end": "754639"
  },
  {
    "text": "green bar here uh performs very well",
    "start": "754639",
    "end": "758800"
  },
  {
    "text": "with the default uh",
    "start": "758800",
    "end": "762120"
  },
  {
    "text": "settings and even outperforms the other",
    "start": "762120",
    "end": "765560"
  },
  {
    "text": "two retriever options in context recall",
    "start": "765560",
    "end": "769279"
  },
  {
    "text": "and",
    "start": "769279",
    "end": "771480"
  },
  {
    "text": "finally we look at different prompt",
    "start": "771560",
    "end": "774839"
  },
  {
    "text": "templates we can see that our prompt uh",
    "start": "774839",
    "end": "778240"
  },
  {
    "text": "template 2 here performs slightly better",
    "start": "778240",
    "end": "782000"
  },
  {
    "text": "than prompt template one and promt",
    "start": "782000",
    "end": "784480"
  },
  {
    "text": "template 2 was the one that used",
    "start": "784480",
    "end": "786760"
  },
  {
    "text": "basically the interactions between the",
    "start": "786760",
    "end": "790079"
  },
  {
    "text": "assistant and human to guide the llm in",
    "start": "790079",
    "end": "794920"
  },
  {
    "text": "our Benchmark here a token teex splitter",
    "start": "794920",
    "end": "798519"
  },
  {
    "text": "with a chunk size of 350 worked well in",
    "start": "798519",
    "end": "801920"
  },
  {
    "text": "combination with coher embedding model",
    "start": "801920",
    "end": "804959"
  },
  {
    "text": "and also Amazon Bedrock knowledge based",
    "start": "804959",
    "end": "808279"
  },
  {
    "text": "Retriever with coher embedding model and",
    "start": "808279",
    "end": "810800"
  },
  {
    "text": "default settings performed even slightly",
    "start": "810800",
    "end": "813240"
  },
  {
    "text": "better across our prioritized dragas",
    "start": "813240",
    "end": "815440"
  },
  {
    "text": "metrics compared to our own open search",
    "start": "815440",
    "end": "818880"
  },
  {
    "text": "retriever implementation with various",
    "start": "818880",
    "end": "820959"
  },
  {
    "text": "chunking strategies and search types on",
    "start": "820959",
    "end": "823920"
  },
  {
    "text": "this data",
    "start": "823920",
    "end": "825519"
  },
  {
    "text": "set but actually more important than the",
    "start": "825519",
    "end": "828360"
  },
  {
    "text": "specific benchmark results here we",
    "start": "828360",
    "end": "830639"
  },
  {
    "text": "established that there is no one size",
    "start": "830639",
    "end": "832440"
  },
  {
    "text": "fits all approach and instead",
    "start": "832440",
    "end": "834440"
  },
  {
    "text": "demonstrated a metric driven development",
    "start": "834440",
    "end": "836920"
  },
  {
    "text": "approach to optimize the retrieval",
    "start": "836920",
    "end": "838720"
  },
  {
    "text": "performance performance of the rack",
    "start": "838720",
    "end": "840480"
  },
  {
    "text": "system which you can apply to your own",
    "start": "840480",
    "end": "843240"
  },
  {
    "text": "data rack system and use case and once",
    "start": "843240",
    "end": "846759"
  },
  {
    "text": "again we will link this notebook in the",
    "start": "846759",
    "end": "848800"
  },
  {
    "text": "description below to get you started and",
    "start": "848800",
    "end": "851040"
  },
  {
    "text": "of course there's a lot more that can be",
    "start": "851040",
    "end": "852759"
  },
  {
    "text": "done in terms of the evaluation and I",
    "start": "852759",
    "end": "856040"
  },
  {
    "text": "included some thoughts on that here as",
    "start": "856040",
    "end": "858000"
  },
  {
    "text": "well thank you for watching",
    "start": "858000",
    "end": "862560"
  }
]