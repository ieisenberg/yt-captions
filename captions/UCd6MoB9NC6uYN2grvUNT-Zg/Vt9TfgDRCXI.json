[
  {
    "start": "0",
    "end": "38000"
  },
  {
    "text": "we'll get started um good afternoon everybody and uh welcome my name is John",
    "start": "440",
    "end": "7439"
  },
  {
    "text": "Handler I'm a Solutions architect with AWS uh and I will be talking to you",
    "start": "7439",
    "end": "12440"
  },
  {
    "text": "about uh log analytics on Amazon elastic search service and also I'll be uh",
    "start": "12440",
    "end": "18000"
  },
  {
    "text": "joined by Kip chaan who is uh at Expedia and runs a number of different uh log",
    "start": "18000",
    "end": "25279"
  },
  {
    "text": "analytics Solutions at Expedia so we'll have a little bit of a an overview and",
    "start": "25279",
    "end": "30320"
  },
  {
    "text": "then uh Co deep will take us into a deep dive talk about log analytics at Expedia",
    "start": "30320",
    "end": "35800"
  },
  {
    "text": "and how that works oh did my slide switch um so I",
    "start": "35800",
    "end": "41320"
  },
  {
    "text": "want to start the conversation first of all like who completely new to elastic",
    "start": "41320",
    "end": "46520"
  },
  {
    "text": "search few folks okay uh elastic search obviously very popular technology uh",
    "start": "46520",
    "end": "52600"
  },
  {
    "text": "even for folks that are familiar with elastic surch you may or may not know elastic search itself is classified as a",
    "start": "52600",
    "end": "58719"
  },
  {
    "text": "database um and I wanted to start the conversation by thinking a little bit about databases and uh zooming in on",
    "start": "58719",
    "end": "66159"
  },
  {
    "text": "elastic search from that angle so you know within AWS we have a number of different database Services uh we have",
    "start": "66159",
    "end": "72479"
  },
  {
    "text": "relational databases we have nosql databases or non-relational databases and then we have this odd bucket which",
    "start": "72479",
    "end": "79119"
  },
  {
    "text": "is analytics uh where elastic search service and elastic search sits in that analytics",
    "start": "79119",
    "end": "85479"
  },
  {
    "text": "bucket and so we when we think about it and",
    "start": "85479",
    "end": "91000"
  },
  {
    "text": "when you when you think about a database solution right every database is designed for a particular thing if it's",
    "start": "91000",
    "end": "97960"
  },
  {
    "text": "a relational database uh it is my slides are not following me the way I want them",
    "start": "97960",
    "end": "103720"
  },
  {
    "text": "to there we go uh they're all built with a different purpose right so a relational database built with a",
    "start": "103720",
    "end": "109600"
  },
  {
    "text": "particular thing in mind elastic search built with a particular thing in mind as you think about it though you want to",
    "start": "109600",
    "end": "115280"
  },
  {
    "text": "pick the right tool for what you're building and for what you're doing right so uh if I'm am going down to the market",
    "start": "115280",
    "end": "121039"
  },
  {
    "text": "to pick up eggs I'm not going to use my big earth mover right that's not an appropriate tool for me to do that uh if",
    "start": "121039",
    "end": "128200"
  },
  {
    "text": "I want to take a ton of uh Goods across the country I'm not going to use my pickup truck probably I'll use an",
    "start": "128200",
    "end": "134400"
  },
  {
    "text": "18wheeler so not to stress the point too much but just we build things and we",
    "start": "134400",
    "end": "139680"
  },
  {
    "text": "create tools that have a purpose for what for which they're",
    "start": "139680",
    "end": "144920"
  },
  {
    "text": "intended somehow it's going backwards there we go um and when we look at the",
    "start": "147879",
    "end": "153280"
  },
  {
    "text": "databases world you know relational databases built to have referential integrity built to be acid compliant uh",
    "start": "153280",
    "end": "160239"
  },
  {
    "text": "all of these things a key Value Store focus on rapid and high volume in input",
    "start": "160239",
    "end": "167319"
  },
  {
    "text": "ingest uh and getting that data in we have these uh document and time series",
    "start": "167319",
    "end": "172519"
  },
  {
    "text": "databases these again have a focus on the real- time aspect or the the",
    "start": "172519",
    "end": "177840"
  },
  {
    "text": "throughput aspect but they addition Ally provide you the ability to do projections and analytics and and other",
    "start": "177840",
    "end": "185519"
  },
  {
    "text": "uh munging of the data that you have and graph database is sort of a special",
    "start": "185519",
    "end": "190720"
  },
  {
    "text": "purpose so with all of that and by way of introduction I didn't do",
    "start": "190720",
    "end": "196239"
  },
  {
    "text": "that uh I'm fighting with my is somebody else",
    "start": "196239",
    "end": "202560"
  },
  {
    "text": "clicking oh gosh that's going to be fun",
    "start": "202560",
    "end": "208280"
  },
  {
    "text": "yes please I can deactivate the clicker oh yeah I can just I can just use my uh",
    "start": "216040",
    "end": "221439"
  },
  {
    "text": "gosh this this uh this the oldfashioned technology right yes I got it thank you very much",
    "start": "221439",
    "end": "229799"
  },
  {
    "text": "okay hopefully that's going to stay all right so the here the the point of the",
    "start": "229799",
    "end": "235560"
  },
  {
    "start": "230000",
    "end": "321000"
  },
  {
    "text": "conversation to this point is that uh let's think about what is elastic search what is its purpose it lives in the",
    "start": "235560",
    "end": "241879"
  },
  {
    "text": "database World which means it's going to hold data and it's going to afford you certain capabilities in terms of what",
    "start": "241879",
    "end": "247920"
  },
  {
    "text": "you can do right so uh there are a number of different purposes that elastic search supports first of those",
    "start": "247920",
    "end": "254720"
  },
  {
    "text": "is Tech search uh elastic search is a search engine uh much like what you",
    "start": "254720",
    "end": "259840"
  },
  {
    "text": "interact with every day things like uh Google for web or Amazon for products uh",
    "start": "259840",
    "end": "267000"
  },
  {
    "text": "a tech search engine is good at handling natural Lang language it's good at scoring and retrieving results that are",
    "start": "267000",
    "end": "273759"
  },
  {
    "text": "relevant to a particular purpose right and it does that in a way that that",
    "start": "273759",
    "end": "279120"
  },
  {
    "text": "supports getting the information you need so that's text search and elastic search is good at that uh Second thing",
    "start": "279120",
    "end": "285520"
  },
  {
    "text": "is streaming data so much like the nosql or document uh databases elastic search",
    "start": "285520",
    "end": "292919"
  },
  {
    "text": "functions to allow you to stream data in quickly and then analyze and visualize",
    "start": "292919",
    "end": "299160"
  },
  {
    "text": "the data uh that is coming into elastic search and then finally there's that analysis",
    "start": "299160",
    "end": "304600"
  },
  {
    "text": "piece where if I have a set of data I can actually project graphs from it I",
    "start": "304600",
    "end": "310560"
  },
  {
    "text": "can see what's going on uh with underlying uh Hardware or other logs",
    "start": "310560",
    "end": "316919"
  },
  {
    "text": "that I'm pulling in nope that's not going to work okay uh with AWS with our Amazon",
    "start": "316919",
    "end": "325520"
  },
  {
    "start": "321000",
    "end": "352000"
  },
  {
    "text": "elastic search Service uh what we're providing you is a a native elastic search and we make it easy to own and",
    "start": "325520",
    "end": "332039"
  },
  {
    "text": "operate that elastic search cluster in the AWS cloud and we'll get into more detail about this but um you know we",
    "start": "332039",
    "end": "338680"
  },
  {
    "text": "came out with this in 2015 uh elastic search hugely popular technology uh we wanted to bring a",
    "start": "338680",
    "end": "344919"
  },
  {
    "text": "service that would bring the AWS kind of goodness of management and uh monitoring",
    "start": "344919",
    "end": "350960"
  },
  {
    "text": "and all of that stuff so again just staying sort of in the database uh scenario or way of",
    "start": "350960",
    "end": "358479"
  },
  {
    "start": "352000",
    "end": "387000"
  },
  {
    "text": "thinking we can look here at an Apache web log and a log line itself each log",
    "start": "358479",
    "end": "365240"
  },
  {
    "text": "line constitutes some kind of event that happened in Apache web log uh you know",
    "start": "365240",
    "end": "370800"
  },
  {
    "text": "somebody came to your server they did a get request against a particular URL uh and then that constitutes an event uh",
    "start": "370800",
    "end": "378720"
  },
  {
    "text": "with a search engine every entity is a document so each log line then is a",
    "start": "378720",
    "end": "384199"
  },
  {
    "text": "document for elastic search and the key kind of idea is that",
    "start": "384199",
    "end": "391000"
  },
  {
    "text": "that log line itself contains some structured information it's not actually structured as a string but it is",
    "start": "391000",
    "end": "397639"
  },
  {
    "text": "information that you can put structure to so in this case uh you know we have a number of different fields a field here",
    "start": "397639",
    "end": "404160"
  },
  {
    "text": "is something like the IP address it's the httv verb it's the URL that you went",
    "start": "404160",
    "end": "409360"
  },
  {
    "text": "to uh it's the Response Code all of those things constitute pieces of information that are present in all of",
    "start": "409360",
    "end": "415919"
  },
  {
    "text": "those log lines and as you ingest them they serve as the basis for or what you're going to do with them whether",
    "start": "415919",
    "end": "421800"
  },
  {
    "text": "it's search or analyze or visualize elastic search works with Jason so as I start with that Apache web",
    "start": "421800",
    "end": "429840"
  },
  {
    "start": "424000",
    "end": "445000"
  },
  {
    "text": "log line I have to go and structure that as Json I make it with I make field",
    "start": "429840",
    "end": "435199"
  },
  {
    "text": "names things like the verb and the request URL and the host and all of that take a piece of Json and push that to",
    "start": "435199",
    "end": "441680"
  },
  {
    "text": "elastic search as a document underneath the covers elastic",
    "start": "441680",
    "end": "448280"
  },
  {
    "text": "search works with a technology ology called Apache Lucine Lucine is a Java library that provides the ability to",
    "start": "448280",
    "end": "454479"
  },
  {
    "text": "read and write indexes okay so the the core capability of elastic search is to",
    "start": "454479",
    "end": "460759"
  },
  {
    "text": "index data and then provide you the ability to retrieve it and provide you the ability to analyze and visualize",
    "start": "460759",
    "end": "466879"
  },
  {
    "text": "it under the covers Lucine is creating indexes on each of those fields so we've",
    "start": "466879",
    "end": "473840"
  },
  {
    "text": "created fields for the verb we've created fields for the request URL um we pass those off to Lucine elastic search",
    "start": "473840",
    "end": "480199"
  },
  {
    "text": "passes those to Lucine Lucine takes the values in all of the fields and creates",
    "start": "480199",
    "end": "486240"
  },
  {
    "text": "what's called a lexicon for each value there's an entry in the Lexicon that",
    "start": "486240",
    "end": "491520"
  },
  {
    "text": "points back to the document that contained that value right so we take",
    "start": "491520",
    "end": "497039"
  },
  {
    "text": "all those values and we say okay the the verb get is present in",
    "start": "497039",
    "end": "502840"
  },
  {
    "text": "documents 4 8 12 Etc this is called an inverted index and how that supports squaring",
    "start": "502840",
    "end": "509879"
  },
  {
    "text": "we'll get to in a second so all of those field indices are in Lucine and they're organized into a",
    "start": "509879",
    "end": "517719"
  },
  {
    "start": "512000",
    "end": "566000"
  },
  {
    "text": "higher level index which is the name you use to refer to that Corpus of data that",
    "start": "517719",
    "end": "523279"
  },
  {
    "text": "collection of data you take the each of that entire",
    "start": "523279",
    "end": "529880"
  },
  {
    "text": "Corpus and you split it into what are called charts right so this is for for your horizontal replication you have",
    "start": "529880",
    "end": "536959"
  },
  {
    "text": "primary shards that contain the entire copy of the data right so if I",
    "start": "536959",
    "end": "542200"
  },
  {
    "text": "have five shards then each Shard contains one of all of the documents",
    "start": "542200",
    "end": "547320"
  },
  {
    "text": "that I've sent and then I can have replica shards um that allow me to scale",
    "start": "547320",
    "end": "554279"
  },
  {
    "text": "and provide additional redundancy again all of those are organized into a highle index uh which",
    "start": "554279",
    "end": "561040"
  },
  {
    "text": "when I send queries I query at the high level",
    "start": "561040",
    "end": "565720"
  },
  {
    "start": "566000",
    "end": "608000"
  },
  {
    "text": "index elastic search is called elastic because it's able to take those shards and distribute them onto a cluster of",
    "start": "566760",
    "end": "575600"
  },
  {
    "text": "instances those shards again represent the data they allow for",
    "start": "575600",
    "end": "581079"
  },
  {
    "text": "querying and they they're the workhorses of elastic search and what elastic",
    "start": "581079",
    "end": "586560"
  },
  {
    "text": "search does is you tell it okay I want five shards elastic search says okay I'll take those shards I'll put",
    "start": "586560",
    "end": "592000"
  },
  {
    "text": "primaries and replicas onto instances and then I'll do that in such a way that primary and replica sit on different",
    "start": "592000",
    "end": "599040"
  },
  {
    "text": "inst es to provide redundancy um and that's how you get the",
    "start": "599040",
    "end": "604519"
  },
  {
    "text": "compute and search from elastic search so if we think about the how to",
    "start": "604519",
    "end": "611720"
  },
  {
    "start": "608000",
    "end": "694000"
  },
  {
    "text": "scale elastic search and how elastic search scales the first layer the easiest thing to think about is elastic",
    "start": "611720",
    "end": "618880"
  },
  {
    "text": "search is a storage medium I'm sending my documents in it's creating indexes and those indexes are written to disk on",
    "start": "618880",
    "end": "626279"
  },
  {
    "text": "disk an index is not the same size as the original Source data right because",
    "start": "626279",
    "end": "632560"
  },
  {
    "text": "I'm cutting it up and putting it into indexes that's a different data structure but actually it's pretty close",
    "start": "632560",
    "end": "638839"
  },
  {
    "text": "usually we see about 10% inflation source to index size uh there are",
    "start": "638839",
    "end": "644360"
  },
  {
    "text": "various parameters that you can tweak to change that ratio but 10% larger that's a good place to",
    "start": "644360",
    "end": "651160"
  },
  {
    "text": "guess every time you add a replica then you increase your storage need by 1X so",
    "start": "651160",
    "end": "656560"
  },
  {
    "text": "if I have a primary and a replica I need two times the storage for that index so when I'm thinking about initial",
    "start": "656560",
    "end": "663399"
  },
  {
    "text": "scale and initial deployment what I do is I pick a number of instances that",
    "start": "663399",
    "end": "668440"
  },
  {
    "text": "supports the amount of storage that I need for my for my indexes for example",
    "start": "668440",
    "end": "673519"
  },
  {
    "text": "if I have a one terabyte uh Corpus right with inflation I'll have 1.1 terabytes",
    "start": "673519",
    "end": "679600"
  },
  {
    "text": "of index I'll want a replica so that means I need 2.2 terabyt of storage I",
    "start": "679600",
    "end": "685680"
  },
  {
    "text": "can deploy two instances each with 1 and half terabyt of EBS to get a total of three and that is where I can",
    "start": "685680",
    "end": "694079"
  },
  {
    "start": "694000",
    "end": "807000"
  },
  {
    "text": "start so shards themselves are actually both units of storage and units of",
    "start": "694600",
    "end": "700880"
  },
  {
    "text": "compute again this is how elastic search distributes the data it's how it responds to your queries so um some",
    "start": "700880",
    "end": "709320"
  },
  {
    "text": "people don't get the monkeys the monkeys are the shards uh so we can look at now you set the Shard",
    "start": "709320",
    "end": "715680"
  },
  {
    "text": "count right when you create an index so you tell elastic search how how many primary shards you want to have for your",
    "start": "715680",
    "end": "721440"
  },
  {
    "text": "index and everybody should do that there's a default of five which is frequently not",
    "start": "721440",
    "end": "727480"
  },
  {
    "text": "right um what we like to say is that you want to Target 40 gigabytes per primary",
    "start": "727480",
    "end": "733279"
  },
  {
    "text": "Shard okay so going to my example if I have a 2 terabyte Corpus I would think",
    "start": "733279",
    "end": "739519"
  },
  {
    "text": "50 primary shards would be the right number of shards okay um for I3",
    "start": "739519",
    "end": "745440"
  },
  {
    "text": "instances with nvme ssds uh High disband wi High Network bandwidth in fact we say",
    "start": "745440",
    "end": "751399"
  },
  {
    "text": "you could go as high as 100 for those um targeting 90 to give yourself some",
    "start": "751399",
    "end": "757440"
  },
  {
    "text": "overhead right you always want to use at least one replica in production so your",
    "start": "757440",
    "end": "763839"
  },
  {
    "text": "replica your first replica again provides that data redundancy so you set it up you say yeah I want one replica",
    "start": "763839",
    "end": "770160"
  },
  {
    "text": "additional replicas you use to increase your query capacity but that first replica is very",
    "start": "770160",
    "end": "776079"
  },
  {
    "text": "important if you're running multiple different indexes multiple different tenants in a cluster it's important to",
    "start": "776079",
    "end": "782360"
  },
  {
    "text": "try to keep The Shard sizes roughly the same and that's because elastic search actually",
    "start": "782360",
    "end": "787760"
  },
  {
    "text": "distributes the shards to instances in the cluster based on a count of how many",
    "start": "787760",
    "end": "792800"
  },
  {
    "text": "shards are on each instance so if they're radically different sizes you can actually get uh hotpots for",
    "start": "792800",
    "end": "799839"
  },
  {
    "text": "storage so you set up your sharding you set up your uh your instances and then",
    "start": "799839",
    "end": "807680"
  },
  {
    "start": "807000",
    "end": "925000"
  },
  {
    "text": "you have to get data into it so this is an area that uh cep will be",
    "start": "807680",
    "end": "812880"
  },
  {
    "text": "spending quite a bit of time on so I'm just going to give you a little bit of a conceptual framework to understand",
    "start": "812880",
    "end": "818440"
  },
  {
    "text": "what's happening in terms of inest we'll take questions or come see me afterwards",
    "start": "818440",
    "end": "823920"
  },
  {
    "text": "thank you so your injest pipeline needs to accomplish several tasks the first",
    "start": "823920",
    "end": "830160"
  },
  {
    "text": "thing it needs you have some data sources right so this is your servers this is your applications this is maybe",
    "start": "830160",
    "end": "837040"
  },
  {
    "text": "S3 this is maybe your containers um there are number of data sources you",
    "start": "837040",
    "end": "843000"
  },
  {
    "text": "need something on each of those data sources or that connects to each of those data sources that can ship logs",
    "start": "843000",
    "end": "848959"
  },
  {
    "text": "off them uh there are a number of different Technologies here some of them open source some of them AWS uh that",
    "start": "848959",
    "end": "855560"
  },
  {
    "text": "make it easy to to collect those logs after you collect the logs then",
    "start": "855560",
    "end": "861639"
  },
  {
    "text": "they're going to be strings you need to transform them into Json again there's a number of Technologies here one open",
    "start": "861639",
    "end": "867600"
  },
  {
    "text": "source technology is called log Dash if you're using Kinesis fire hose uh it's a great solution Kinesis fire hose can do",
    "start": "867600",
    "end": "874320"
  },
  {
    "text": "this transformation in line as you're sending data across uh cloudwatch logs can also do this transformation with an",
    "start": "874320",
    "end": "881519"
  },
  {
    "text": "AWS you also want to buffer uh these these Json documents you can connect and",
    "start": "881519",
    "end": "887480"
  },
  {
    "text": "send one document at a time but the overhead of all of those connections will quickly swamp your cluster so you",
    "start": "887480",
    "end": "894240"
  },
  {
    "text": "need to create bulk batches that you're going to send off to elastic search again again number of different",
    "start": "894240",
    "end": "899759"
  },
  {
    "text": "Technologies each of them can some of them can do this uh in various ways and",
    "start": "899759",
    "end": "905120"
  },
  {
    "text": "then finally you need something to deliver to elastic search something has to actually make that request to the",
    "start": "905120",
    "end": "910360"
  },
  {
    "text": "elastic search cluster and send the data in uh we'll see three really good examples from cep uh and there's many",
    "start": "910360",
    "end": "917600"
  },
  {
    "text": "ways to do this but that's the framework that you use no that's still not",
    "start": "917600",
    "end": "924360"
  },
  {
    "text": "working okay uh when you send your data into elastic search you know as we said",
    "start": "924360",
    "end": "929759"
  },
  {
    "start": "925000",
    "end": "1020000"
  },
  {
    "text": "there's the individual field level indices but actually there's a larger logical collection of those uh which are",
    "start": "929759",
    "end": "937199"
  },
  {
    "text": "the the big top level indices for streaming data and log analytics the pattern you use is index per time period",
    "start": "937199",
    "end": "945000"
  },
  {
    "text": "this is supported by most of the injest Technologies uh directly or there are",
    "start": "945000",
    "end": "950160"
  },
  {
    "text": "apis within elastic search that would let you accomplish this rolling index structure thank",
    "start": "950160",
    "end": "957000"
  },
  {
    "text": "you um now there's like a bunch of buttons and I don't know which one is what I'll figure it out uh so normal",
    "start": "957000",
    "end": "965079"
  },
  {
    "text": "structure is to create an index per time period and the usual time period is to",
    "start": "965079",
    "end": "970160"
  },
  {
    "text": "create an index per day again this is supported by a number of the different injest Technologies right um we see at",
    "start": "970160",
    "end": "978880"
  },
  {
    "text": "about one or more terabytes a day maybe two terabytes a day of inest uh you",
    "start": "978880",
    "end": "984000"
  },
  {
    "text": "start to get into need for an hourly index structure at you know less than 250 gabes maybe you go to weekly index",
    "start": "984000",
    "end": "991480"
  },
  {
    "text": "structuring but most of the time it's a daily uh and the reason that you do this",
    "start": "991480",
    "end": "997120"
  },
  {
    "text": "is it actually makes it easier to manage the data in your cluster with elastic search it's very easy you send an HTTP",
    "start": "997120",
    "end": "1003319"
  },
  {
    "text": "delete to delete out uh an old index and that's the cleanest easiest way to",
    "start": "1003319",
    "end": "1008720"
  },
  {
    "text": "manage that data often you'll take a snapshot or archive that that index somewhere before you delete it but there",
    "start": "1008720",
    "end": "1014880"
  },
  {
    "text": "is a daily uh rotation where you just delete off the oldest data",
    "start": "1014880",
    "end": "1020440"
  },
  {
    "start": "1020000",
    "end": "1086000"
  },
  {
    "text": "that one is not you no I'm I'm still fighting",
    "start": "1020440",
    "end": "1026839"
  },
  {
    "text": "it well let's just jump here so we'll talk about the query engine um So within",
    "start": "1026839",
    "end": "1034038"
  },
  {
    "text": "elastic search uh you send a query something like tell me uh status 500 on",
    "start": "1034039",
    "end": "1040480"
  },
  {
    "text": "on my HTTP requests right that goes to some index or it can go to multiple",
    "start": "1040480",
    "end": "1045720"
  },
  {
    "text": "indexes and that is distributed across the cluster uh to some node some",
    "start": "1045720",
    "end": "1052520"
  },
  {
    "text": "instance in the cluster gets that request that's going to act as a coordinator that coordinator then",
    "start": "1052520",
    "end": "1058480"
  },
  {
    "text": "distributes the query to each of the shards in in the indexes that it touches",
    "start": "1058480",
    "end": "1063760"
  },
  {
    "text": "could be a primary could be a replica but the uh the coordinator sends to one",
    "start": "1063760",
    "end": "1068960"
  },
  {
    "text": "of each so that the whole Corpus responds to that query each Shard then",
    "start": "1068960",
    "end": "1074760"
  },
  {
    "text": "figures out a response to that query sends it back to the coordinator coordinator reaggregates sends it back",
    "start": "1074760",
    "end": "1081600"
  },
  {
    "text": "as a",
    "start": "1081600",
    "end": "1083960"
  },
  {
    "start": "1086000",
    "end": "1162000"
  },
  {
    "text": "result uh So within when we process a query in elastic search we come back to",
    "start": "1087559",
    "end": "1093919"
  },
  {
    "text": "Lucine and the inverted Index right so the we take a certain set of fields and",
    "start": "1093919",
    "end": "1099320"
  },
  {
    "text": "we say tell me this field has this value you know status uh 500 size greater than",
    "start": "1099320",
    "end": "1104600"
  },
  {
    "text": "50,000 whatever it is uh we go to the inverted IND index and we look up the",
    "start": "1104600",
    "end": "1110480"
  },
  {
    "text": "values right so if I'm looking for status is uh 500 I'll go to the status",
    "start": "1110480",
    "end": "1115520"
  },
  {
    "text": "index and I'll get the posting list which is all the documents that have status 500 I repeat that for all of the",
    "start": "1115520",
    "end": "1121919"
  },
  {
    "text": "portions of the query and then I apply the logic of the query uh to either",
    "start": "1121919",
    "end": "1127480"
  },
  {
    "text": "intersect or Union the sets of posting lists to come out with the total posting",
    "start": "1127480",
    "end": "1132799"
  },
  {
    "text": "list right so if I said status 500 and uh request includes Star Wars uh so I",
    "start": "1132799",
    "end": "1139240"
  },
  {
    "text": "get status 500 posting list request Star Wars posting list it's an and I do a set",
    "start": "1139240",
    "end": "1144760"
  },
  {
    "text": "intersection and I get the matches for that query that's what I return well I",
    "start": "1144760",
    "end": "1150120"
  },
  {
    "text": "get the matches for that query I then go and score each of those matches mostly",
    "start": "1150120",
    "end": "1155360"
  },
  {
    "text": "usually and sort them by how good a match they are that's what I return as",
    "start": "1155360",
    "end": "1161159"
  },
  {
    "text": "my result with elastic search you also have",
    "start": "1161159",
    "end": "1166480"
  },
  {
    "start": "1162000",
    "end": "1214000"
  },
  {
    "text": "the ability to do more complex statistics on the values that are in your field so this is Fields this is how",
    "start": "1166480",
    "end": "1173400"
  },
  {
    "text": "elastic search supports analysis right I have the same kind of thing I do I have",
    "start": "1173400",
    "end": "1178840"
  },
  {
    "text": "I take my results set right I then look at the field data let's say I want a histogram of the verbs that came from a",
    "start": "1178840",
    "end": "1186640"
  },
  {
    "text": "particular IP address so I go I look up the IP address I find all the documents",
    "start": "1186640",
    "end": "1192120"
  },
  {
    "text": "I go and look at the verb for each of those documents and I can easily create",
    "start": "1192120",
    "end": "1197200"
  },
  {
    "text": "a histogram out of that the the verb itself the values provide me Buckets things like get put post head",
    "start": "1197200",
    "end": "1204640"
  },
  {
    "text": "what have you and then I can just count how many times that happened underlying all of the sort of analysis that elastic",
    "start": "1204640",
    "end": "1211400"
  },
  {
    "text": "search does is this mechanism I can use that then to build",
    "start": "1211400",
    "end": "1218080"
  },
  {
    "start": "1214000",
    "end": "1278000"
  },
  {
    "text": "visualizations on what's in my log lines right so in this case what I did is I",
    "start": "1218080",
    "end": "1224400"
  },
  {
    "text": "graphed a breakdown of my traffic based on the URL I graphed a p pie chart with uh all of",
    "start": "1224400",
    "end": "1232559"
  },
  {
    "text": "the requests that are coming in so I can see differentially what are my users interacting with uh I have a keyword",
    "start": "1232559",
    "end": "1238280"
  },
  {
    "text": "Cloud because it's the 90s still uh but keywords are cool so I have a keyword",
    "start": "1238280",
    "end": "1243320"
  },
  {
    "text": "Cloud that shows me what are my what are people searching for and I also have a response code graph that lets me know uh",
    "start": "1243320",
    "end": "1250000"
  },
  {
    "text": "these are the 200s and then the gold bars there are uh errors now it's a search engine like all of this stuff is",
    "start": "1250000",
    "end": "1256120"
  },
  {
    "text": "in a search engine so if I want to find the 500s I can just search for them if I'm trying to diagnose some problem in",
    "start": "1256120",
    "end": "1263039"
  },
  {
    "text": "in production I can dig in and I can find the individual log lines across all of my servers that generated 500 errors",
    "start": "1263039",
    "end": "1271840"
  },
  {
    "text": "that's kind of the power of elastic search that's where we see most people adopting and using",
    "start": "1271840",
    "end": "1278200"
  },
  {
    "start": "1278000",
    "end": "1356000"
  },
  {
    "text": "it I'm still going to struggle with this a little bit so I'll tell you about two",
    "start": "1278320",
    "end": "1283480"
  },
  {
    "text": "uh two different customers and in some little detail and then cep is going to tell you a lot about speedia uh so the",
    "start": "1283480",
    "end": "1290760"
  },
  {
    "text": "first case study is mirror web I love these guys because not so much for elastic search but for the way that they",
    "start": "1290760",
    "end": "1297320"
  },
  {
    "text": "got data into elastic search so they had 120 terabytes of the United Kingdom's",
    "start": "1297320",
    "end": "1303840"
  },
  {
    "text": "web and parliamentary archives that they had to chew through and and send into",
    "start": "1303840",
    "end": "1309200"
  },
  {
    "text": "elastic search the data was not clean there were duplicates there was all kinds of stuff so they deployed about a",
    "start": "1309200",
    "end": "1316880"
  },
  {
    "text": "thousand spot instances broke the data sources into 100 megabyte chunks and",
    "start": "1316880",
    "end": "1322240"
  },
  {
    "text": "then chewed through it all in about 10 hours to load all of that data they ended up with an index in the like eight",
    "start": "1322240",
    "end": "1328480"
  },
  {
    "text": "or n terabyte range uh but this was just a fantastic use of spot and ec2 and data",
    "start": "1328480",
    "end": "1335919"
  },
  {
    "text": "ingest at a really extremely high volume they they were 10x uh the speed of the",
    "start": "1335919",
    "end": "1341840"
  },
  {
    "text": "prior record which was with Hadoop so they did a great job with that",
    "start": "1341840",
    "end": "1348840"
  },
  {
    "start": "1356000",
    "end": "1398000"
  },
  {
    "text": "okay uh so another case study Financial Times uh Financial Times Obviously",
    "start": "1356320",
    "end": "1362039"
  },
  {
    "text": "online media also paper media uh they track their user interaction against",
    "start": "1362039",
    "end": "1368720"
  },
  {
    "text": "their website they take their web logs uh they ingest them internally and they",
    "start": "1368720",
    "end": "1373840"
  },
  {
    "text": "have a system that they call lantern that they expose uh the interaction with",
    "start": "1373840",
    "end": "1379320"
  },
  {
    "text": "the site to the journalists and editors so that journalists can get excited about like hey I published this article",
    "start": "1379320",
    "end": "1385880"
  },
  {
    "text": "and everybody's loving it and uh the editors can say hey this is a great topic area we should go and uh publish",
    "start": "1385880",
    "end": "1392919"
  },
  {
    "text": "more about it um so they they did a a really nice tool there all right I'm",
    "start": "1392919",
    "end": "1400320"
  },
  {
    "start": "1398000",
    "end": "1528000"
  },
  {
    "text": "going to talk just a tiny bit about elastic search service and let you know uh what the service provides and and",
    "start": "1400320",
    "end": "1406400"
  },
  {
    "text": "sort of how we provide it so uh the number one thing is with the service we wanted to provide you the ability to",
    "start": "1406400",
    "end": "1413400"
  },
  {
    "text": "run an elastic surge cluster in the AWS Cloud where you would get an equivalent",
    "start": "1413400",
    "end": "1418679"
  },
  {
    "text": "experience to what you would get with open source so we wanted you to be able to use elastic search like you're already using elastic search um we make",
    "start": "1418679",
    "end": "1426279"
  },
  {
    "text": "it easy to use so elastic search clustered technology there's a lot of deployment there's a lot of configuration we do all that stuff for",
    "start": "1426279",
    "end": "1433240"
  },
  {
    "text": "you you just send an API call we deploy a cluster we make it scalable so if you're you're running on a particular",
    "start": "1433240",
    "end": "1439880"
  },
  {
    "text": "set of instances and you want to change that set of instances to a different one you want to change your storage anything",
    "start": "1439880",
    "end": "1445480"
  },
  {
    "text": "about add nodes uh it's a single API call we just make it easy to scale it up",
    "start": "1445480",
    "end": "1450559"
  },
  {
    "text": "we have security built into the service uh that allows you to secure your cluster from uh outside Intruders we",
    "start": "1450559",
    "end": "1458480"
  },
  {
    "text": "make it we have a feature to make it more highly available uh with a check box you can just deploy to two different",
    "start": "1458480",
    "end": "1464240"
  },
  {
    "text": "availability zones um and we have tight Integrations with other AWS Services I",
    "start": "1464240",
    "end": "1469919"
  },
  {
    "text": "mentioned Kinesis fire hose cloudwatch logs we have nice tight Integrations iot",
    "start": "1469919",
    "end": "1475000"
  },
  {
    "text": "as well when you use the service again you",
    "start": "1475000",
    "end": "1480960"
  },
  {
    "text": "use the console SDK CLI you send off that API call and you tell us how many",
    "start": "1480960",
    "end": "1487679"
  },
  {
    "text": "data nodes you want and how many Master nodes you want uh we go off we deploy that we build it into a cluster and we",
    "start": "1487679",
    "end": "1494480"
  },
  {
    "text": "provide that to you over an HTTP endpoint right so it's a DNS entry restful endpoint um we put load",
    "start": "1494480",
    "end": "1501640"
  },
  {
    "text": "balancing in front of the cluster and then we also use uh you use IM am to uh",
    "start": "1501640",
    "end": "1507200"
  },
  {
    "text": "block that cluster or provide Access Control in addition we send monitoring",
    "start": "1507200",
    "end": "1512480"
  },
  {
    "text": "data to cloudwatch so you can monitor your cluster figure out how it's performing do you need to scale it and",
    "start": "1512480",
    "end": "1518679"
  },
  {
    "text": "we also send uh administrative uh usage to cloud trail so if you want to know",
    "start": "1518679",
    "end": "1523880"
  },
  {
    "text": "who's building clusters who's taking them down Etc so we'll talk in a little teeny bit of depth about some of these",
    "start": "1523880",
    "end": "1529960"
  },
  {
    "start": "1528000",
    "end": "1581000"
  },
  {
    "text": "so uh in terms of security again we provide you a DNS entry which is an endpoint that uh DNS entry can be",
    "start": "1529960",
    "end": "1536919"
  },
  {
    "text": "publicly available so it's a public endpoint uh or it can be within your VPC",
    "start": "1536919",
    "end": "1542520"
  },
  {
    "text": "if it's a public endpoint um we recommend that you set an I am policy that limits what Anonymous users can do",
    "start": "1542520",
    "end": "1549399"
  },
  {
    "text": "there because people will find it um and so you use I am to secure that and then",
    "start": "1549399",
    "end": "1556000"
  },
  {
    "text": "uh if it's within your VPC that DNS resolves to the address space within your VPC so it's not available outside",
    "start": "1556000",
    "end": "1562640"
  },
  {
    "text": "of the VPC in that case you can also use I am policies to provide granular access",
    "start": "1562640",
    "end": "1568600"
  },
  {
    "text": "control for different actors in your organization and then finally we support",
    "start": "1568600",
    "end": "1574200"
  },
  {
    "text": "uh encryption based on your KMS keys this is encryption at",
    "start": "1574200",
    "end": "1580760"
  },
  {
    "start": "1581000",
    "end": "1653000"
  },
  {
    "text": "rest I briefly touched on dedicated Masters uh So within a search clusters",
    "start": "1582520",
    "end": "1589200"
  },
  {
    "text": "there is a master instance that Master instance is responsible for orchestrating the cluster it knows all",
    "start": "1589200",
    "end": "1595760"
  },
  {
    "text": "of the schema it knows all of the instances it knows all of the indexes you need that Master function to keep",
    "start": "1595760",
    "end": "1602120"
  },
  {
    "text": "the cluster being a cluster now the problem is if you put that on the data node then and you're running like hot",
    "start": "1602120",
    "end": "1608200"
  },
  {
    "text": "with queries and and updates that query node can become loaded and you can't respond for master functions so what is",
    "start": "1608200",
    "end": "1616240"
  },
  {
    "text": "recommended and especially for production is to employ to deploy dedicated instances that do nothing but",
    "start": "1616240",
    "end": "1622840"
  },
  {
    "text": "the master function and you should use three dedicated Masters quickly because",
    "start": "1622840",
    "end": "1629480"
  },
  {
    "text": "uh the master is elected based on the eligible nodes and three doesn't divide evenly uh you know if you have a network",
    "start": "1629480",
    "end": "1636120"
  },
  {
    "text": "split so you should use three they can be much smaller than the data nodes they don't do a ton they just",
    "start": "1636120",
    "end": "1642320"
  },
  {
    "text": "orchestrate um but they do make your cluster a lot more stable so in production those are always recommended",
    "start": "1642320",
    "end": "1648200"
  },
  {
    "text": "Dev test you know if you can rebuild the cluster in an hour it doesn't matter so",
    "start": "1648200",
    "end": "1653279"
  },
  {
    "start": "1653000",
    "end": "1680000"
  },
  {
    "text": "much I mentioned Zone awareness uh Zone awareness again just a checkbox with",
    "start": "1653279",
    "end": "1658600"
  },
  {
    "text": "this we put your instances in your cluster into two different availability zones uh we split primary and replica",
    "start": "1658600",
    "end": "1665399"
  },
  {
    "text": "shards you have to have a replica but we split primary and replica uh into those two zones that gives you 100% of data in",
    "start": "1665399",
    "end": "1671880"
  },
  {
    "text": "both zones so should you lose a Zone uh you you retain all that data and elastic",
    "start": "1671880",
    "end": "1678200"
  },
  {
    "text": "search continues to function now I mentioned we send uh metrics to cloudwatch uh these are some",
    "start": "1678200",
    "end": "1685799"
  },
  {
    "start": "1680000",
    "end": "1749000"
  },
  {
    "text": "of the important ones that you want to set alarms uh first of those cluster status red if your cluster is red it",
    "start": "1685799",
    "end": "1693080"
  },
  {
    "text": "means that you've lost a primary uh Shard somewhere uh if we're blocking rights definitely you want to know that",
    "start": "1693080",
    "end": "1701120"
  },
  {
    "text": "uh for the CPU utilization uh we like to see the average CPU utilization across",
    "start": "1701120",
    "end": "1706519"
  },
  {
    "text": "the cluster 80% % or below we don't like that to go above 80% uh jvm memory",
    "start": "1706519",
    "end": "1712159"
  },
  {
    "text": "pressure tells you how much data is getting stuck in the old pool in your jvm uh if we see that go above 80%",
    "start": "1712159",
    "end": "1719039"
  },
  {
    "text": "that's a problem and if it goes linear up into to the right that's also a",
    "start": "1719039",
    "end": "1724399"
  },
  {
    "text": "problem so putting a putting a alarm there uh helps you catch that free",
    "start": "1724399",
    "end": "1730960"
  },
  {
    "text": "storage space uh we'd like to see 25% of the total storage remaining free",
    "start": "1730960",
    "end": "1736919"
  },
  {
    "text": "so you set a an alarm at 25% uh and then you scale it up and then",
    "start": "1736919",
    "end": "1743240"
  },
  {
    "text": "we take automated snapshots in the service if one of those fails you probably want to know",
    "start": "1743240",
    "end": "1748960"
  },
  {
    "text": "that okay so uh we also with an elastic search uh you can set a threshold for",
    "start": "1748960",
    "end": "1755640"
  },
  {
    "start": "1749000",
    "end": "1784000"
  },
  {
    "text": "elastic search to write a log line if an index takes longer than that threshold",
    "start": "1755640",
    "end": "1761440"
  },
  {
    "text": "or if a query takes longer than that threshold within the service we make it uh easy for you to set that up and then",
    "start": "1761440",
    "end": "1768159"
  },
  {
    "text": "we push those slow logs out to cloudwatch logs this is really useful if you want to figure out you know what are",
    "start": "1768159",
    "end": "1773919"
  },
  {
    "text": "my slow queries what are the things that are not performing in my cluster you can use slow logs to to get a report about",
    "start": "1773919",
    "end": "1782320"
  },
  {
    "text": "that uh within the service of the cost model uh we have three components you",
    "start": "1783919",
    "end": "1789640"
  },
  {
    "start": "1784000",
    "end": "1809000"
  },
  {
    "text": "pay for instance hours for the Masters and data instances uh you pay for EBS",
    "start": "1789640",
    "end": "1795840"
  },
  {
    "text": "within the service you can choose EBS or the ephemeral storage on the instance itself if you choose EBS there's an EBS",
    "start": "1795840",
    "end": "1802080"
  },
  {
    "text": "charge and then data transfer uh standard AWS data transfer",
    "start": "1802080",
    "end": "1808880"
  },
  {
    "text": "applies all right so that uh is a quick overview of the service and I'll bring",
    "start": "1808880",
    "end": "1814799"
  },
  {
    "text": "up Co deep now thank you very",
    "start": "1814799",
    "end": "1818158"
  },
  {
    "text": "much um thanks John for the overview of elastic search talking about how it is",
    "start": "1820919",
    "end": "1826919"
  },
  {
    "text": "helping folks uh analyze the machine data out there um so I'm Kip Chan I'm",
    "start": "1826919",
    "end": "1832720"
  },
  {
    "text": "from xedia group uh today I'm going to talk about three different things talk about the usage of elastic search why",
    "start": "1832720",
    "end": "1840000"
  },
  {
    "text": "did we choose elastic search service that Amazon is offering over the Open Source One and I'm going to talk about",
    "start": "1840000",
    "end": "1846519"
  },
  {
    "text": "three different architectures of how folks are using elastic search surveys within Expedia um for those of you who",
    "start": "1846519",
    "end": "1853679"
  },
  {
    "text": "don't know Expedia group is one of the leading travel uh like online travel",
    "start": "1853679",
    "end": "1858799"
  },
  {
    "text": "industry uh leaders um we have a lot of brands that make up xedia group like xedia homeway",
    "start": "1858799",
    "end": "1865320"
  },
  {
    "text": "orbits cheap tickets what if I think pretty much apart from Priceline and booking.com are under xedia group",
    "start": "1865320",
    "end": "1874760"
  },
  {
    "text": "um I I've been with Expedia group for the last five years um very recently I",
    "start": "1874760",
    "end": "1880559"
  },
  {
    "start": "1875000",
    "end": "1893000"
  },
  {
    "text": "transitioned from Brand Expedia to homeway brand and I moved to Austin um",
    "start": "1880559",
    "end": "1886120"
  },
  {
    "text": "I'm a new B to Austin now uh I used to live in Seattle um with that I'll get started on",
    "start": "1886120",
    "end": "1892600"
  },
  {
    "text": "the usage um around how many elastic search uh clusters are we running um so",
    "start": "1892600",
    "end": "1898840"
  },
  {
    "start": "1893000",
    "end": "1931000"
  },
  {
    "text": "we have about 175 across different uh accounts um when I say accounts I'm not",
    "start": "1898840",
    "end": "1903880"
  },
  {
    "text": "talking about hundreds of accounts probably like three or four accounts that we have um and the number of",
    "start": "1903880",
    "end": "1909480"
  },
  {
    "text": "instances that are powering these instance uh these clusters are about 500 um of all these clusters really um like",
    "start": "1909480",
    "end": "1918200"
  },
  {
    "text": "about there are about five to 10 clusters that are the heavy heers everything else folks either they want",
    "start": "1918200",
    "end": "1924279"
  },
  {
    "text": "to try out elastic search or it's like low amount of data that we pump into these systems",
    "start": "1924279",
    "end": "1931519"
  },
  {
    "start": "1931000",
    "end": "1973000"
  },
  {
    "text": "um and um the amount of data across all these big clusters is about 35 terabytes",
    "start": "1931519",
    "end": "1937080"
  },
  {
    "text": "and I'm only talking about production here not test or stage uh and that to this is not that we keep this data",
    "start": "1937080",
    "end": "1943880"
  },
  {
    "text": "forever right like as John mentioned we do roll over indexes so one of our",
    "start": "1943880",
    "end": "1949799"
  },
  {
    "text": "biggest cluster has about 10 terabyte but we only keep three days worth of data in it we don't keep more than three",
    "start": "1949799",
    "end": "1955440"
  },
  {
    "text": "days worth of data um so we we pump in a lot of data but again this is more used for analysis um not something to use for",
    "start": "1955440",
    "end": "1963240"
  },
  {
    "text": "our historical data or whatnot lot of the historical data goes into S3 and from there if we need to pull the data",
    "start": "1963240",
    "end": "1968880"
  },
  {
    "text": "back in we pull back pull that back into elastic search um so I think the bigger question",
    "start": "1968880",
    "end": "1975679"
  },
  {
    "start": "1973000",
    "end": "2026000"
  },
  {
    "text": "right like as John mentioned there is an open source solution you can you can spin up your own ec2 instance go andun",
    "start": "1975679",
    "end": "1982320"
  },
  {
    "text": "install elastic surge lock stash whatever right that whole stack that you",
    "start": "1982320",
    "end": "1988279"
  },
  {
    "text": "you can install yourself and we used to do that as well right like before elastic search service came along uh we",
    "start": "1988279",
    "end": "1994279"
  },
  {
    "text": "used Chef uh to to install elastic search onto it create an image out of it",
    "start": "1994279",
    "end": "2000039"
  },
  {
    "text": "and then when we needed a cluster um we we were able to fully automate it but",
    "start": "2000039",
    "end": "2006399"
  },
  {
    "text": "the bigger problem was how do we scale it right like when when we run out of the data on the nodes how do we increase",
    "start": "2006399",
    "end": "2013000"
  },
  {
    "text": "the the size of the cluster though those were some of the challenges with that we didn't spend time on automating um and",
    "start": "2013000",
    "end": "2020399"
  },
  {
    "text": "it was painful as well to be honest with you um and then elastic search service",
    "start": "2020399",
    "end": "2025519"
  },
  {
    "text": "came along um as John mentioned it is very easy to set it up right like you go to your console you can use a CLI what",
    "start": "2025519",
    "end": "2032519"
  },
  {
    "start": "2026000",
    "end": "2087000"
  },
  {
    "text": "we do is um we use the AWS SDK and then we wrap it with uh",
    "start": "2032519",
    "end": "2038240"
  },
  {
    "text": "some automation so that we have governance to make sure that there are enough taxs um and then also apply some",
    "start": "2038240",
    "end": "2045039"
  },
  {
    "text": "security policies on it so that it's not wide open to everyone um a lot of folks who have used elastic search might know",
    "start": "2045039",
    "end": "2051760"
  },
  {
    "text": "there were it was wide open for a long time lot of clusters and people were able to access those clusters from um",
    "start": "2051760",
    "end": "2058679"
  },
  {
    "text": "any location they can so that's what we did was just WRA the SDK um using and",
    "start": "2058679",
    "end": "2066118"
  },
  {
    "text": "then put some governance on top of off it but creating the cluster was within minutes we had the cluster people will",
    "start": "2066119",
    "end": "2072280"
  },
  {
    "text": "go to a Jenkins job and say I want I do2 extra large I do four extra large I want",
    "start": "2072280",
    "end": "2077358"
  },
  {
    "text": "this many number of Master nodes um and then 10 to 15 minutes we have a cluster up and",
    "start": "2077359",
    "end": "2082599"
  },
  {
    "text": "running um that was the biggest selling point to be honest um and then the second one was the high availability",
    "start": "2082599",
    "end": "2089839"
  },
  {
    "start": "2087000",
    "end": "2121000"
  },
  {
    "text": "right where uh and a lot of the times no one knows what is the amount of data that they are going to pump into the",
    "start": "2089839",
    "end": "2095800"
  },
  {
    "text": "systems right like you'll start with maybe one terab that you think but for whatever reason someone changes a small",
    "start": "2095800",
    "end": "2101760"
  },
  {
    "text": "field in the log statement or there is some additional data one terab becomes 10 terabytes um so the high availability",
    "start": "2101760",
    "end": "2109640"
  },
  {
    "text": "feature of elastic search service gives you that option to scale when you want to scale right like if you want to add",
    "start": "2109640",
    "end": "2116440"
  },
  {
    "text": "more number of nodes or if you want to add um more number of um like data nodes",
    "start": "2116440",
    "end": "2124320"
  },
  {
    "start": "2121000",
    "end": "2155000"
  },
  {
    "text": "versus more number of Master nodes what type of instance you want you can all just do that through a",
    "start": "2124320",
    "end": "2129680"
  },
  {
    "text": "console R SDK and then within like again as I said based on typically takes",
    "start": "2129680",
    "end": "2136119"
  },
  {
    "text": "minutes but it depends on you how much data you have in the cluster as well if you have terabytes worth of data it may",
    "start": "2136119",
    "end": "2141680"
  },
  {
    "text": "take more time for it to be uh rescaled um versus but at least you don't have to",
    "start": "2141680",
    "end": "2146839"
  },
  {
    "text": "worry about it someone else is doing it for you um um and the secur is one I think as I",
    "start": "2146839",
    "end": "2153480"
  },
  {
    "text": "mentioned earlier um the the VPC support that John showed was a re recent feature",
    "start": "2153480",
    "end": "2159000"
  },
  {
    "start": "2155000",
    "end": "2210000"
  },
  {
    "text": "sometime end of last year was released before that it was not there right like and when you created an elastic search",
    "start": "2159000",
    "end": "2165280"
  },
  {
    "text": "cluster it was wide open again this is a simplified policy this is not the real policy that we applied uh the point is",
    "start": "2165280",
    "end": "2172319"
  },
  {
    "text": "you see the bottom there is an IP address that we used to scope um so that we only give access to someone who is",
    "start": "2172319",
    "end": "2178359"
  },
  {
    "text": "running within expedia's Network or if they are running inside the um inside",
    "start": "2178359",
    "end": "2184680"
  },
  {
    "text": "our own vpcs basically this address would be an N IP are your office public",
    "start": "2184680",
    "end": "2190359"
  },
  {
    "text": "IPS um and if you are running in the vpcb you you typically used to give full access uh but again we scope it to the",
    "start": "2190359",
    "end": "2197240"
  },
  {
    "text": "IM roles that need access full access um so that's how we were able to implement",
    "start": "2197240",
    "end": "2202319"
  },
  {
    "text": "the VPC scoping the r clusters to VPC before the VPC support came",
    "start": "2202319",
    "end": "2208480"
  },
  {
    "text": "along um and the the third the last one monitoring and backups right uh",
    "start": "2208480",
    "end": "2213720"
  },
  {
    "start": "2210000",
    "end": "2231000"
  },
  {
    "text": "thankfully we never had to do a restore of a backup yet uh but you know that there is a backup feature that you can",
    "start": "2213720",
    "end": "2219359"
  },
  {
    "text": "just enable and then you at least have that peace of mind that someone is taking the backup and we haven't done",
    "start": "2219359",
    "end": "2224960"
  },
  {
    "text": "that yet but we know that it's it's available if you need to",
    "start": "2224960",
    "end": "2230680"
  },
  {
    "text": "um with with that I'll talk about three different architectures today of how we are using elastic surch and um my aim",
    "start": "2230680",
    "end": "2239560"
  },
  {
    "start": "2231000",
    "end": "2251000"
  },
  {
    "text": "was to do more breadth than depth right and if you need any details I'll be happy to talk to you after this session",
    "start": "2239560",
    "end": "2246359"
  },
  {
    "text": "um but again my Focus here is more uh bread versus uh depth um so these are three different",
    "start": "2246359",
    "end": "2253760"
  },
  {
    "start": "2251000",
    "end": "2284000"
  },
  {
    "text": "architectures that I pick to speak today uh one was how do you stream the docker",
    "start": "2253760",
    "end": "2259599"
  },
  {
    "text": "startup blogs into elastic search um then um how do you analyze the data",
    "start": "2259599",
    "end": "2266040"
  },
  {
    "text": "that's there in cloud trail uh to to solve a business case right I'm I'm going to talk about the business case",
    "start": "2266040",
    "end": "2271280"
  },
  {
    "text": "there why we uh Parts like analyze the data that's available in cloud trail and",
    "start": "2271280",
    "end": "2276599"
  },
  {
    "text": "then um how we are using elastic search within our distributed TR tracing system",
    "start": "2276599",
    "end": "2281640"
  },
  {
    "text": "so these are the three things that I'm going to talk today um Docker Journey um at Expedia",
    "start": "2281640",
    "end": "2288040"
  },
  {
    "start": "2284000",
    "end": "2386000"
  },
  {
    "text": "started like three years back um we are heavy users of ECS the E to container",
    "start": "2288040",
    "end": "2293960"
  },
  {
    "text": "service um we we P chos it again for the reasons of it was managed by AWS we were",
    "start": "2293960",
    "end": "2301079"
  },
  {
    "text": "we can focus on writing the apps and then pushing our stuff into AWS then trying to um run clusters ourselves um",
    "start": "2301079",
    "end": "2309480"
  },
  {
    "text": "with that being said by that time and even today um our log Management systems",
    "start": "2309480",
    "end": "2315480"
  },
  {
    "text": "for like the application logs if you will or to a file and then we use plunk",
    "start": "2315480",
    "end": "2320800"
  },
  {
    "text": "uh to to visualize all that uh application locks on a on a continuous",
    "start": "2320800",
    "end": "2326000"
  },
  {
    "text": "basis right like once the app is running then it will write to the disk and then the data will push to uh Splunk um with",
    "start": "2326000",
    "end": "2333079"
  },
  {
    "text": "Docker what we did was we use something called a sidecar approach so we always run a Splunk container as well uh the",
    "start": "2333079",
    "end": "2340000"
  },
  {
    "text": "challenge what we ran into was um if if the app was running then we could stream",
    "start": "2340000",
    "end": "2345720"
  },
  {
    "text": "the data into Splunk but but some of sometimes the app would wouldn't even",
    "start": "2345720",
    "end": "2351040"
  },
  {
    "text": "start then how do you capture those logs to understand why a particular Docker uh",
    "start": "2351040",
    "end": "2356200"
  },
  {
    "text": "app didn't start up um so that's where um we use elastic search Serv uh spun",
    "start": "2356200",
    "end": "2362599"
  },
  {
    "text": "did not provide a lock driver um on Docker at that point um like docker 1.7",
    "start": "2362599",
    "end": "2368640"
  },
  {
    "text": "is when they started enabling this feature called log drivers that you can use to pump data to different sources of",
    "start": "2368640",
    "end": "2375960"
  },
  {
    "text": "course it didn't support everything that was out there so that was the high level problem that we were trying to solve",
    "start": "2375960",
    "end": "2381960"
  },
  {
    "text": "here to make sure that the folks understood why their app didn't start um",
    "start": "2381960",
    "end": "2387119"
  },
  {
    "start": "2386000",
    "end": "2511000"
  },
  {
    "text": "so as I mentioned we used ECS ecsr cluster management solution uh so what we did was on both",
    "start": "2387119",
    "end": "2394359"
  },
  {
    "text": "ECS um log driver and the docker itself uh log driver like the actual the task",
    "start": "2394359",
    "end": "2401000"
  },
  {
    "text": "or the container um we pumped both the logs for those to ECS agent and then",
    "start": "2401000",
    "end": "2407480"
  },
  {
    "text": "Docker to fluentd uh for those of you who don't know fluentd fluent is an open source",
    "start": "2407480",
    "end": "2413599"
  },
  {
    "text": "solution which unifies data collection and consumption so it decouples the data",
    "start": "2413599",
    "end": "2419160"
  },
  {
    "text": "sources from the consumers um by just providing an interface where you could",
    "start": "2419160",
    "end": "2424200"
  },
  {
    "text": "have multiple data sources pump the data into fluent and then you could have multiple subscribers of that data as",
    "start": "2424200",
    "end": "2430400"
  },
  {
    "text": "well and what we did here was um there was no elastic search log driver um so",
    "start": "2430400",
    "end": "2435440"
  },
  {
    "text": "that's the reason we set up fluent D clusters ourselves uh in AWS and then we",
    "start": "2435440",
    "end": "2441040"
  },
  {
    "text": "created um the Amazon um elastic search service clusters and then configured",
    "start": "2441040",
    "end": "2446680"
  },
  {
    "text": "fluent D to pump the data there um so very quickly right like this is it was very easy for us to set it up once we",
    "start": "2446680",
    "end": "2453720"
  },
  {
    "text": "had the this we figured out how do we do the fluent part um the spinning of the E",
    "start": "2453720",
    "end": "2459880"
  },
  {
    "text": "uh elastic search clusters was pretty easy for us then we spent some time figuring out how we do that with the",
    "start": "2459880",
    "end": "2465760"
  },
  {
    "text": "fluent part um once that was available then people can would go to kuana to do their",
    "start": "2465760",
    "end": "2472920"
  },
  {
    "text": "searches um what we used to do with this is when a particular app didn't start or",
    "start": "2472920",
    "end": "2479119"
  },
  {
    "text": "particular deployment failed we would send a uh an email sorry and in the",
    "start": "2479119",
    "end": "2484280"
  },
  {
    "text": "jenin job itself you would see a hyperlink that talks about hey this is your app name in kibana that you can go",
    "start": "2484280",
    "end": "2491000"
  },
  {
    "text": "and search for so as people would just click on it and then they would be able to see why their particular app didn't",
    "start": "2491000",
    "end": "2497040"
  },
  {
    "text": "even start um so that way people were able to visualize the errors um of",
    "start": "2497040",
    "end": "2502480"
  },
  {
    "text": "course once the app was fully running the the locks used to go to Splunk and then people can go and uh take a look at",
    "start": "2502480",
    "end": "2508160"
  },
  {
    "text": "it if there is an exception or or whatnot um very quickly this is the",
    "start": "2508160",
    "end": "2514240"
  },
  {
    "start": "2511000",
    "end": "2537000"
  },
  {
    "text": "setup that we did it was pretty easy to do um we just said anything that comes to flu and D um we would uh sorry this",
    "start": "2514240",
    "end": "2522000"
  },
  {
    "text": "is on the docker anything that we pump to flu and D we would tag with the docker version itself like whatever was",
    "start": "2522000",
    "end": "2527839"
  },
  {
    "text": "the docker image tag we would just tag that particular log that we pumped to uh",
    "start": "2527839",
    "end": "2534280"
  },
  {
    "text": "elastic search as well um and then on um within fluentd we were just blindly",
    "start": "2534280",
    "end": "2540440"
  },
  {
    "start": "2537000",
    "end": "2581000"
  },
  {
    "text": "whatever was pumped to fluent D cluster because we set fluent d as just as a bridge here to just push the data to",
    "start": "2540440",
    "end": "2547680"
  },
  {
    "text": "elastic search so anything that came into fluent D would go to elastic search",
    "start": "2547680",
    "end": "2553000"
  },
  {
    "text": "and these were the like different elastic search domains that we set up in different regions in different environments uh so each fluent cluster",
    "start": "2553000",
    "end": "2560119"
  },
  {
    "text": "had its own configuration if you will uh to send it to the appropriate elastic",
    "start": "2560119",
    "end": "2565960"
  },
  {
    "text": "search um and again that that's how we were able to solve that um again we",
    "start": "2565960",
    "end": "2571000"
  },
  {
    "text": "continue to iterate on it um but again it has been super helpful for our folks to actually visualize some of the stuff",
    "start": "2571000",
    "end": "2578599"
  },
  {
    "text": "why their apps were not starting up um for those of you who don't know clot",
    "start": "2578599",
    "end": "2585160"
  },
  {
    "start": "2581000",
    "end": "2655000"
  },
  {
    "text": "Cloud trial um Cloud trial records all the apis that you make in your account and then pushes the data into",
    "start": "2585160",
    "end": "2592079"
  },
  {
    "text": "S3 um we had a issue with um that we were trying to solve here we are as I",
    "start": "2592079",
    "end": "2599720"
  },
  {
    "text": "said we had like three or four accounts um and most of you who are heavy users",
    "start": "2599720",
    "end": "2605040"
  },
  {
    "text": "of AWS would have run into AP rate limiting um if not by now maybe later on",
    "start": "2605040",
    "end": "2611520"
  },
  {
    "text": "um so we were constantly hit by that API rate limiting issue um that we needed",
    "start": "2611520",
    "end": "2617160"
  },
  {
    "text": "some level of data to understand who was that a im am user or I am Ro who was",
    "start": "2617160",
    "end": "2624520"
  },
  {
    "text": "making that many number of calls and we also wanted to know like which service",
    "start": "2624520",
    "end": "2629640"
  },
  {
    "text": "was that making like whether it was load balancers or whether it was ECS or ec2",
    "start": "2629640",
    "end": "2634760"
  },
  {
    "text": "in general and we wanted to know it very quickly right like we we didn't have a lot of time to analyze because either we",
    "start": "2634760",
    "end": "2641599"
  },
  {
    "text": "were not able to push our changes to AWS because we were rate limited um so elastic search uh came in",
    "start": "2641599",
    "end": "2649800"
  },
  {
    "text": "very handy there where we where we were able to just spin up the cluster um what",
    "start": "2649800",
    "end": "2656079"
  },
  {
    "start": "2655000",
    "end": "2777000"
  },
  {
    "text": "we did was um on cloud tril we created this uh cloud trail configuration if you will um and and what we did was create a",
    "start": "2656079",
    "end": "2664400"
  },
  {
    "text": "cloud trail for all the regions um earlier on on like three or four years back you would have to do that for each",
    "start": "2664400",
    "end": "2670200"
  },
  {
    "text": "region at a time uh now Cloud trial provides you an option to just say hey I just want to um recall all the things",
    "start": "2670200",
    "end": "2677000"
  },
  {
    "text": "that are happening across all the accounts and then send that to an S3 bucket so on S3 once the data gets into",
    "start": "2677000",
    "end": "2684960"
  },
  {
    "text": "S3 um we we had an SNS notification as well configured so that whenever the",
    "start": "2684960",
    "end": "2690559"
  },
  {
    "text": "file got deposited into S3 we would we would get an SNS notification um there",
    "start": "2690559",
    "end": "2696480"
  },
  {
    "text": "are a couple of ways how you could invoke a Lambda from here right um you could you could invoke a Lambda directly",
    "start": "2696480",
    "end": "2703119"
  },
  {
    "text": "from SNS or you could just create a uh subscription on the bucket itself to say",
    "start": "2703119",
    "end": "2708880"
  },
  {
    "text": "whenever a file gets deposited you could just call a Lambda uh the reason why we chose this one was this uh gives us",
    "start": "2708880",
    "end": "2716280"
  },
  {
    "text": "options to have multiple subscribers on the same SNS um on S3 you Scope it to a",
    "start": "2716280",
    "end": "2722000"
  },
  {
    "text": "particular prefix then you have to deal with those prefix Wars if someone had a BL get prefix and someone wanted a",
    "start": "2722000",
    "end": "2728760"
  },
  {
    "text": "subset of that prefix there were some issues with how you can invoke a Lambda from there so that's the reason we chose",
    "start": "2728760",
    "end": "2735040"
  },
  {
    "text": "SNS here um and then once the data was available uh that notification kicked in",
    "start": "2735040",
    "end": "2740960"
  },
  {
    "text": "our Lambda uh we just read the data from S3 and then pumped into uh elastic",
    "start": "2740960",
    "end": "2747359"
  },
  {
    "text": "search service it was uh like to be honest it took a day for us to do all this uh not more than a",
    "start": "2747359",
    "end": "2754480"
  },
  {
    "text": "day um it even it would have been even less than a day um and once data was",
    "start": "2754480",
    "end": "2760480"
  },
  {
    "text": "available there right like we we were able to create some dashboards to understand the top API callers or",
    "start": "2760480",
    "end": "2766559"
  },
  {
    "text": "whatnot from our account I'll show a dashboard in a minute um so that was was",
    "start": "2766559",
    "end": "2771839"
  },
  {
    "text": "very quickly that we were able to solve the problem at hand uh with the help of elastic",
    "start": "2771839",
    "end": "2776960"
  },
  {
    "text": "search and again this is some uh configurations that we did on the cloud trail which said anything that comes in",
    "start": "2776960",
    "end": "2783640"
  },
  {
    "text": "for all the regions just send it to an S3 bucket and then we had an SNS to",
    "start": "2783640",
    "end": "2788800"
  },
  {
    "text": "Lambda trigger um and this is all the code that we wrote maybe I little bit simplified here for the sake of",
    "start": "2788800",
    "end": "2795119"
  },
  {
    "text": "presentation but that's all we had to deal do is just get get the data from S3",
    "start": "2795119",
    "end": "2800400"
  },
  {
    "text": "and then push it to elastic search and all that elastic search domains and whatnot was all like environment",
    "start": "2800400",
    "end": "2805960"
  },
  {
    "text": "variables on Lambda um so once the data was there in elastic search this was one",
    "start": "2805960",
    "end": "2811960"
  },
  {
    "start": "2808000",
    "end": "2830000"
  },
  {
    "text": "of the dashboards that I just talked to understand that the APA calls who are",
    "start": "2811960",
    "end": "2817640"
  },
  {
    "text": "making the top APA calls again for the last 10 minutes once the data is available in elastic search you can",
    "start": "2817640",
    "end": "2823760"
  },
  {
    "text": "slice and dice however you want um this this was one of the dashboards that we",
    "start": "2823760",
    "end": "2829119"
  },
  {
    "text": "buil and um I have also open sourced this uh solution as a serverless",
    "start": "2829119",
    "end": "2834480"
  },
  {
    "start": "2830000",
    "end": "2858000"
  },
  {
    "text": "application so you can use Sam to run this application full fully end to endend with the cloud trail",
    "start": "2834480",
    "end": "2840760"
  },
  {
    "text": "configuration it does uh pushing create an S3 bucket create a elastic search",
    "start": "2840760",
    "end": "2846000"
  },
  {
    "text": "domain as well for you and then uh does that wiring which will send the data to um elastic search and",
    "start": "2846000",
    "end": "2852640"
  },
  {
    "text": "then you can create your own dashboards however you want to whatever solutions that you want to",
    "start": "2852640",
    "end": "2858319"
  },
  {
    "start": "2858000",
    "end": "2985000"
  },
  {
    "text": "create and the last one um that I wanted to talk today was um how we are using",
    "start": "2858319",
    "end": "2864920"
  },
  {
    "text": "elastic surch within our distributed tracing system um again for those of you",
    "start": "2864920",
    "end": "2870520"
  },
  {
    "text": "who are transitioning from Big monoliths into a lot of microservices uh someday",
    "start": "2870520",
    "end": "2876319"
  },
  {
    "text": "you would need this um I'm I'm sure some people might have already been using it",
    "start": "2876319",
    "end": "2881480"
  },
  {
    "text": "um again this is to understand when a request comes to your site um how is it being traversed through across all these",
    "start": "2881480",
    "end": "2887880"
  },
  {
    "text": "microservices that you have um the one that I'm showing here is from Twitter uh it's called as Zipkin Uber did their",
    "start": "2887880",
    "end": "2895000"
  },
  {
    "text": "open source called as AER we are working on open sourcing this we call it as haac internally uh we use open",
    "start": "2895000",
    "end": "2902480"
  },
  {
    "text": "tracing uh to collect all this data and then customize it to expedience environment if you will and then um then",
    "start": "2902480",
    "end": "2909720"
  },
  {
    "text": "our teams would use it to do their traces uh very quickly right like um again I don't have something to point",
    "start": "2909720",
    "end": "2916079"
  },
  {
    "text": "the one in the top it's typically called as a trace uh Trace is a single operation performed by an app it could",
    "start": "2916079",
    "end": "2922480"
  },
  {
    "text": "be just calling one service or it could be calling thousand more services uh behind the scenes and the ones that you",
    "start": "2922480",
    "end": "2928640"
  },
  {
    "text": "see later on are called as spans the span typically says how much time it took when did it start um and typically",
    "start": "2928640",
    "end": "2936040"
  },
  {
    "text": "a trace could have multiple spans uh than uh and they could be also nested",
    "start": "2936040",
    "end": "2942079"
  },
  {
    "text": "once uh the reason why I'm bringing this up right like as we were collecting all this Trace information we were pushing",
    "start": "2942079",
    "end": "2948799"
  },
  {
    "text": "all this data into um Cassandra as a key value pair to all to store all these",
    "start": "2948799",
    "end": "2953880"
  },
  {
    "text": "traces for every app that we were running um but we also needed a way to do um easy uh easy way to do some kind",
    "start": "2953880",
    "end": "2962119"
  },
  {
    "text": "of query on top of it and um doing queries on top of Cassandra costly for",
    "start": "2962119",
    "end": "2967760"
  },
  {
    "text": "us when I say costly not the money aspect but the time it used to take then trying to put the data around was also",
    "start": "2967760",
    "end": "2974160"
  },
  {
    "text": "something that was timec consuming um I'll talk about few of the use cases that we soled in through elastic search",
    "start": "2974160",
    "end": "2980599"
  },
  {
    "text": "once I talk about the architecture um that was what we were trying to solve here um with all of our microservices",
    "start": "2980599",
    "end": "2988240"
  },
  {
    "text": "that were running in AWS um we created a Java Library most of our stock is Java uh so we had a",
    "start": "2988240",
    "end": "2994839"
  },
  {
    "text": "platform library that we that every app would just add in um and then what what",
    "start": "2994839",
    "end": "3000280"
  },
  {
    "text": "that does is it generates a tra uh for each of that uh particular request that",
    "start": "3000280",
    "end": "3005720"
  },
  {
    "text": "app is serving um and then pushes that data into Kinesis so we use Kinesis as a",
    "start": "3005720",
    "end": "3013000"
  },
  {
    "text": "collector if you will to collect all the data um that was coming from all these",
    "start": "3013000",
    "end": "3018760"
  },
  {
    "text": "different microservices so we had only one Kinesis stream per environment and",
    "start": "3018760",
    "end": "3024040"
  },
  {
    "text": "each of that Kinesis stream would pick pull that data in and then push it to um",
    "start": "3024040",
    "end": "3029799"
  },
  {
    "text": "and and what we did was we have a we had a Java app that would read from Kinesis and then send it to Kafka um for folks",
    "start": "3029799",
    "end": "3037799"
  },
  {
    "text": "who could know who who knows Kafka and kineses most of both of them are similar right like the way that they are trying",
    "start": "3037799",
    "end": "3043520"
  },
  {
    "text": "to solve is is almost the same um the reason why we have both here is one",
    "start": "3043520",
    "end": "3048839"
  },
  {
    "text": "Kinesis is highly managed highly available fully scalable but again it's also expensive um so what we do there is",
    "start": "3048839",
    "end": "3056880"
  },
  {
    "text": "push all the data to Kinesis and from them from Kinesis we again push it to Kafka and we have multiple subscribers",
    "start": "3056880",
    "end": "3063319"
  },
  {
    "text": "of that data from Kafka uh then having those subscribers on top of Kinesis itself um so once the data is in",
    "start": "3063319",
    "end": "3071799"
  },
  {
    "text": "Kafka um then we have another app that will actually get the data from Kafka uh do the same thing as what John mentioned",
    "start": "3071799",
    "end": "3078200"
  },
  {
    "text": "we do batch it we don't send every record that comes into Kafka into elastic search so we batch it and then",
    "start": "3078200",
    "end": "3085200"
  },
  {
    "text": "once the batch is based on some predefined values that we have if it reaches a certain size then we we push",
    "start": "3085200",
    "end": "3092359"
  },
  {
    "text": "it to elastic search um and also we also do a time frame as well it's not just we",
    "start": "3092359",
    "end": "3097640"
  },
  {
    "text": "wait for the the size to be particular size and then once the data reaches that",
    "start": "3097640",
    "end": "3103000"
  },
  {
    "text": "particular size we push that into elastic search service um and then we had a node API",
    "start": "3103000",
    "end": "3110319"
  },
  {
    "text": "node UI that would uh interact with this elastic search Service uh to get that",
    "start": "3110319",
    "end": "3115680"
  },
  {
    "text": "data um the way that we actually used uh this data was when we push the data into",
    "start": "3115680",
    "end": "3122280"
  },
  {
    "start": "3118000",
    "end": "3151000"
  },
  {
    "text": "elastic search um we always annotated the data saying that this is the source uh the trace ID um as you see here like",
    "start": "3122280",
    "end": "3129400"
  },
  {
    "text": "the transaction ID is what we call it as Trace ID and it could have different multiple metadatas right um a particular",
    "start": "3129400",
    "end": "3137920"
  },
  {
    "text": "Trace could have is it expedia.com is it orbits.com like we call it as tiits or",
    "start": "3137920",
    "end": "3143559"
  },
  {
    "text": "whatnot um there's a lot of metadata that we store in our systems um the reason why we do that with this elastic",
    "start": "3143559",
    "end": "3150480"
  },
  {
    "text": "search solution um was to be able to do a queries on top of the data that we",
    "start": "3150480",
    "end": "3156200"
  },
  {
    "text": "have so example someone would come to our dashboard and say give me all the traces for the last one hour right uh",
    "start": "3156200",
    "end": "3162920"
  },
  {
    "text": "what we would do is we actually go and first query elastic search for that data once we get that information from",
    "start": "3162920",
    "end": "3168559"
  },
  {
    "text": "elastic search then we use that Trace IDs that I just showed you and then go and make that call to Cassandra uh to",
    "start": "3168559",
    "end": "3175400"
  },
  {
    "text": "get the trace actual data to show so that's how we are able to use elastic search and then the second aspect was",
    "start": "3175400",
    "end": "3182960"
  },
  {
    "text": "example just what I said right like the filtering the traces um if someone wanted to know hey give me all the",
    "start": "3182960",
    "end": "3188839"
  },
  {
    "text": "traces but I want to know only trace for this particular app um then um we also",
    "start": "3188839",
    "end": "3194680"
  },
  {
    "text": "do that using elastic search to to get all the traces for a particular service and then we send that",
    "start": "3194680",
    "end": "3201280"
  },
  {
    "text": "to um then we pull that from Cassandra and then display it um and again I'll",
    "start": "3201280",
    "end": "3206680"
  },
  {
    "text": "try to wrap it up quickly um things to keep in mind right like I've talked so many things about elastic search how it",
    "start": "3206680",
    "end": "3213400"
  },
  {
    "text": "has benefited uh first thing always when you talk about scaling scaling actually",
    "start": "3213400",
    "end": "3218440"
  },
  {
    "text": "results in a new cluster it is not resizing your existing cluster um the",
    "start": "3218440",
    "end": "3223480"
  },
  {
    "text": "reason why I bring it up here is when your system is already getting hit by lot of request",
    "start": "3223480",
    "end": "3230480"
  },
  {
    "text": "for whatever reason that elastic search is already unhealthy um you typically don't want to do a resize again at that",
    "start": "3230480",
    "end": "3236280"
  },
  {
    "text": "point point because one the jvm pressure is already high and then you're asking it to again do a resize which means data",
    "start": "3236280",
    "end": "3243480"
  },
  {
    "text": "is being copied from your cluster again to a different cluster so you are going to really hurt your cluster sometimes",
    "start": "3243480",
    "end": "3249680"
  },
  {
    "text": "it's maybe better off to not do the resize until your traffic load reduces",
    "start": "3249680",
    "end": "3254880"
  },
  {
    "text": "um and then do the resize uh just keep that in mind um um even though Amazon does sorry",
    "start": "3254880",
    "end": "3261680"
  },
  {
    "text": "elastic service does a lot of the heavy lifting you earn the monitoring you are",
    "start": "3261680",
    "end": "3266880"
  },
  {
    "text": "responsible for your clusters like there is no magic W which will automatically scale or resize um so you have to keep",
    "start": "3266880",
    "end": "3274599"
  },
  {
    "text": "like you have all the tools right like I'm not saying there are not the tools are not there the tools are there but you actually own that responsibility of",
    "start": "3274599",
    "end": "3282799"
  },
  {
    "text": "uh monitoring and scaling whatnot that you have to do um again these two are wish list uh which I think I brought it",
    "start": "3282799",
    "end": "3289799"
  },
  {
    "text": "up in reent as well um with elastic surges also has been evolving very",
    "start": "3289799",
    "end": "3295160"
  },
  {
    "text": "rapidly right like now I think it's six is something is the version 6.x whatever",
    "start": "3295160",
    "end": "3300280"
  },
  {
    "text": "um if you want to upgrade from one class one version to a different version there is no magic button unfortunately um you",
    "start": "3300280",
    "end": "3306839"
  },
  {
    "text": "have to create a new cluster take a dump off your existing cluster and then push",
    "start": "3306839",
    "end": "3312599"
  },
  {
    "text": "it to the new cluster for most part it does work I haven't seen any uh backward",
    "start": "3312599",
    "end": "3317720"
  },
  {
    "text": "compatibility issues in general um not backward like maybe the upgrade issues but again I wish there was upgrade",
    "start": "3317720",
    "end": "3324280"
  },
  {
    "text": "button um um and then U this is again just a small",
    "start": "3324280",
    "end": "3329400"
  },
  {
    "text": "detail U this was more for me when I was trying to collect the starts around all the Clusters um was nowh to tell me how",
    "start": "3329400",
    "end": "3337119"
  },
  {
    "text": "much we are actually using it does tell you how much is left on the cluster but it doesn't tell you how much is actually",
    "start": "3337119",
    "end": "3343000"
  },
  {
    "text": "being in use um and then the last one um just to wrap up the the real benefit for",
    "start": "3343000",
    "end": "3350400"
  },
  {
    "text": "for us uh was the the ease of setting it up and the high availability the rest of",
    "start": "3350400",
    "end": "3355799"
  },
  {
    "text": "the two are essential right like you can't escape from security you need to make sure that your clusters are",
    "start": "3355799",
    "end": "3361200"
  },
  {
    "text": "protected um so that and then the the last obvious one the monitoring and",
    "start": "3361200",
    "end": "3367119"
  },
  {
    "start": "3367000",
    "end": "3393000"
  },
  {
    "text": "backup um with that uh that concludes my presentation if you have any questions I'll be around to answer and please make",
    "start": "3367119",
    "end": "3373880"
  },
  {
    "text": "sure to review our uh talk thank you",
    "start": "3373880",
    "end": "3380318"
  },
  {
    "text": "[Music]",
    "start": "3384430",
    "end": "3392079"
  }
]