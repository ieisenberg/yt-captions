[
  {
    "text": "thank you for coming on a Friday morning really appreciate it",
    "start": "30",
    "end": "5069"
  },
  {
    "text": "Simon all I've been at Netflix for several years now and for the past two",
    "start": "5069",
    "end": "10290"
  },
  {
    "text": "and a half three years I've been working on streaming and stream processing initiatives and I've helped define the",
    "start": "10290",
    "end": "17250"
  },
  {
    "text": "space and the platform that we currently have at Netflix and I'm going to be talking about that today so I'm gonna be",
    "start": "17250",
    "end": "27869"
  },
  {
    "text": "talking about the amazing work my team and I've been doing in the stream",
    "start": "27869",
    "end": "33149"
  },
  {
    "text": "processing phase and the platform that we've built I've organized this talk basically based on three different roles",
    "start": "33149",
    "end": "40680"
  },
  {
    "text": "and perspectives that I have seen and building out this platform so if you're",
    "start": "40680",
    "end": "46800"
  },
  {
    "text": "a data engineer you'll get a glimpse of you know why stream processing and what does such a platform offer and if your",
    "start": "46800",
    "end": "54030"
  },
  {
    "text": "data leader then you get an idea of what such a product or a vision for such a product looks like and I'll also cover how we build it and",
    "start": "54030",
    "end": "61050"
  },
  {
    "text": "how we operate the whole platform so if you're a back an engineer developing it you'll get some glimpse of that as well",
    "start": "61050",
    "end": "68090"
  },
  {
    "text": "so focus on the business insights platform that we are building predominantly on flink and I won't be",
    "start": "68090",
    "end": "74729"
  },
  {
    "text": "talking about the operational insights we have a separate system call at listen mantas if we use for that and I won't be",
    "start": "74729",
    "end": "80520"
  },
  {
    "text": "comparing the different stream processing engines or cover streaming concepts I have a separate talk I gave",
    "start": "80520",
    "end": "86310"
  },
  {
    "text": "last year at strange loop that covers that aspect of it so if you wanted you can hit it up later",
    "start": "86310",
    "end": "92180"
  },
  {
    "text": "so why stream processing right at Netflix you know we have over 100",
    "start": "92310",
    "end": "97680"
  },
  {
    "text": "million customers and we want to understand how they're interacting with their service what they're playing and we want to give them a really good",
    "start": "97680",
    "end": "103770"
  },
  {
    "text": "personalized experience and these interactions that the users perform on our website and also the systems that",
    "start": "103770",
    "end": "111000"
  },
  {
    "text": "are backing it a generate a ton of events and we want to understand all this ecosystem and give a really good",
    "start": "111000",
    "end": "116909"
  },
  {
    "text": "experience to the users and so for us getting low latency business insights",
    "start": "116909",
    "end": "122340"
  },
  {
    "text": "and other analytics enables interesting personalizations that we could do it",
    "start": "122340",
    "end": "128520"
  },
  {
    "text": "also helps us do some innovation in marketing of new shows and it enables a turn-off",
    "start": "128520",
    "end": "133800"
  },
  {
    "text": "other things that we can do research wise and processing data as it arrives",
    "start": "133800",
    "end": "139110"
  },
  {
    "text": "also has its benefit because now you can spread the workload of processing over time and if you're processing things",
    "start": "139110",
    "end": "146640"
  },
  {
    "text": "like you know data quality or you doing some aggregations you're doing some enrichment upfront then you don't have",
    "start": "146640",
    "end": "152100"
  },
  {
    "text": "to repeat those again when you're processing those events again so you're also a wide redundancy and as we all",
    "start": "152100",
    "end": "160680"
  },
  {
    "text": "know the sources that we see are producing a lot of events and they'll",
    "start": "160680",
    "end": "165780"
  },
  {
    "text": "continue to do so as there are more and more customers and these are unbounded data sets right and so we need a way to",
    "start": "165780",
    "end": "173160"
  },
  {
    "text": "kind of efficiently process this in a way of processing this in real time and",
    "start": "173160",
    "end": "179430"
  },
  {
    "text": "so building such a platform gives us both business wins and technical wins when we build it correctly so why it all",
    "start": "179430",
    "end": "187260"
  },
  {
    "text": "build a platform you know why not just let people write their own stream processing jobs because building a good",
    "start": "187260",
    "end": "193230"
  },
  {
    "text": "stream processing jobs that are accurate and latencies sensitive and or productionize it's not a trivial task",
    "start": "193230",
    "end": "198830"
  },
  {
    "text": "and if you let everybody build you build a lot of boilerplate and a lot of redundancy and a lot of wasted",
    "start": "198830",
    "end": "204900"
  },
  {
    "text": "development resources so I think we want our users to be able to focus on",
    "start": "204900",
    "end": "210470"
  },
  {
    "text": "actually working on the business insights and not have to worry a lot about building complex infrastructure",
    "start": "210470",
    "end": "215550"
  },
  {
    "text": "and the tooling that goes along with it so you may ask what does a stream",
    "start": "215550",
    "end": "221640"
  },
  {
    "text": "processing platform offer so a stream processing platform needs to be able to",
    "start": "221640",
    "end": "228150"
  },
  {
    "text": "offer users the ability to trade off between ease of use flexibility and",
    "start": "228150",
    "end": "234120"
  },
  {
    "text": "capability of the platform so that they can pick the right combination for the solution at hand they may have a very",
    "start": "234120",
    "end": "240420"
  },
  {
    "text": "simple thing to do like a filtering or a transformation or they may have real complex been doing the session ization needs and based on what they need to do",
    "start": "240420",
    "end": "247200"
  },
  {
    "text": "the platform needs to offer ways of doing it and make it as simple as possible and make complex use cases",
    "start": "247200",
    "end": "253050"
  },
  {
    "text": "really possible on the platform",
    "start": "253050",
    "end": "256340"
  },
  {
    "text": "so we kind of take a four pronged approach towards this one is the users",
    "start": "258570",
    "end": "264810"
  },
  {
    "text": "can set up stream processing based on just a point-and-click UI and they can set up rich filtering projections and",
    "start": "264810",
    "end": "272190"
  },
  {
    "text": "they can route it to different sinks that we support out of the box and the second one is if that's not sufficient then they can create their own streaming",
    "start": "272190",
    "end": "279030"
  },
  {
    "text": "jobs and we're also looking towards supporting sequel like DSL so it makes",
    "start": "279030",
    "end": "284970"
  },
  {
    "text": "it even easier for them to create those jobs and down the line create interactive capabilities so that they",
    "start": "284970",
    "end": "291540"
  },
  {
    "text": "can explore the streams and do quick prototyping before they decide to go one way or the other so we'll first look at",
    "start": "291540",
    "end": "299610"
  },
  {
    "text": "the point-and-click approach of doing this and what the product actually looks like so the core of it we have a ingest",
    "start": "299610",
    "end": "307620"
  },
  {
    "text": "pipeline called the Keystone pipeline that offers this point and click functionality and it forms the backbone",
    "start": "307620",
    "end": "314070"
  },
  {
    "text": "of our infrastructure because if you want to have events and real-time events",
    "start": "314070",
    "end": "319770"
  },
  {
    "text": "you need a way to kind of funnel them through your system very reliably and then you can do some stream processing",
    "start": "319770",
    "end": "325200"
  },
  {
    "text": "with it so this pipeline enables reliable event publishing collection and",
    "start": "325200",
    "end": "330930"
  },
  {
    "text": "then routing them to different sinks for batch and stream processing so this is",
    "start": "330930",
    "end": "336360"
  },
  {
    "text": "completely serverless to the user it's a out-of-the-box turnkey solution it's run hundred percent in AWS so have",
    "start": "336360",
    "end": "345150"
  },
  {
    "text": "a quick demo here that shows what we do when we when a user wants to create a brand-new stream on this in ingest",
    "start": "345150",
    "end": "351120"
  },
  {
    "text": "pipeline that means they want to start producing events and they're absolutely zero infrastructure at this point in",
    "start": "351120",
    "end": "356670"
  },
  {
    "text": "time so user comes in and starts",
    "start": "356670",
    "end": "363180"
  },
  {
    "text": "creating a new stream they give it a name and they provide their email address so we can contact them for",
    "start": "363180",
    "end": "369060"
  },
  {
    "text": "interesting things and they provide a brief description so that it helps their other team members to know why they",
    "start": "369060",
    "end": "374760"
  },
  {
    "text": "treated that stream for and other people who are involved they can pick a region and an environment they want to deploy",
    "start": "374760",
    "end": "380010"
  },
  {
    "text": "and plug it in so here we're creating it in test and they're saying their",
    "start": "380010",
    "end": "385050"
  },
  {
    "text": "estimate is 12 megabytes per second for the stream and once you do that they say you know create a stream and then at",
    "start": "385050",
    "end": "391470"
  },
  {
    "text": "this point in time the data is not going to go anywhere yet because they still have to configure where the data needs landing so they're",
    "start": "391470",
    "end": "398100"
  },
  {
    "text": "gonna add different outputs and here we're gonna add a sink for Apache hive",
    "start": "398100",
    "end": "405660"
  },
  {
    "text": "and add the table name that it needs to go to and we have a couple of deployment options to address the low latency lunar",
    "start": "405660",
    "end": "413310"
  },
  {
    "text": "low duplicates a scenario and once they set that up then the pipelines kind of",
    "start": "413310",
    "end": "421230"
  },
  {
    "text": "set now they're adding one more filter because they want one unfiltered stream",
    "start": "421230",
    "end": "427650"
  },
  {
    "text": "going to hive and they want a filtered stream going to hive so now they are setting up another filtered stream and we have an expert based DSL we have our",
    "start": "427650",
    "end": "435210"
  },
  {
    "text": "own custom parser kind of that supports this and the data flowing through usually is JSON so this allows them to",
    "start": "435210",
    "end": "441900"
  },
  {
    "text": "quickly write a filter based on the expert language and it has logical and",
    "start": "441900",
    "end": "447720"
  },
  {
    "text": "arithmetic operators that are available for them to group and create this expression and once they create the",
    "start": "447720",
    "end": "454470"
  },
  {
    "text": "expression as we can see they are says that you need to send it somewhere so let's create an output for it so we're",
    "start": "454470",
    "end": "460320"
  },
  {
    "text": "creating another hive table because we want the filtered events to go to another and we want to retain the raw stream so we can do additional analytics",
    "start": "460320",
    "end": "468150"
  },
  {
    "text": "downstream so that's what the user chose to do and once that's done the user",
    "start": "468150",
    "end": "476960"
  },
  {
    "text": "specifies again another deployment model the same thing as before and they create",
    "start": "476960",
    "end": "482760"
  },
  {
    "text": "the pipeline so that's all the user had to do once they do this we provision the",
    "start": "482760",
    "end": "489180"
  },
  {
    "text": "stream for them and then the user can start publishing so in a few minutes after they do that sorry so few minutes",
    "start": "489180",
    "end": "498150"
  },
  {
    "text": "after they do that they get the pipeline provision so what happened here is",
    "start": "498150",
    "end": "504800"
  },
  {
    "text": "decreed at one pipeline to produce the events into and they created a fan out",
    "start": "504800",
    "end": "509910"
  },
  {
    "text": "of two so that you can have the events routed to do different things from the same originating stream so here we look",
    "start": "509910",
    "end": "520710"
  },
  {
    "text": "at the different message formats we support such as keys our own internal custom binary wrappers",
    "start": "520710",
    "end": "526270"
  },
  {
    "text": "we can support multiple different formats and we did this so that internally we can evolve the system without having to do a lot of changes so",
    "start": "526270",
    "end": "532899"
  },
  {
    "text": "that today we support JSON and what you see here simple JSON has nothing but a flattened version of JSON and that's",
    "start": "532899",
    "end": "539140"
  },
  {
    "text": "there because of legacy reasons of how the pipeline evolved at Netflix and you're also looking at adding error in",
    "start": "539140",
    "end": "544779"
  },
  {
    "text": "the future and that we could easily put it in our wrapper and send it across the",
    "start": "544779",
    "end": "550180"
  },
  {
    "text": "wire I said this point in time we don't have any explicit schemas those schemas",
    "start": "550180",
    "end": "555910"
  },
  {
    "text": "are implicit based on the JSON structure we have that flows through the system",
    "start": "555910",
    "end": "561600"
  },
  {
    "text": "and the other thing they could do is projections or is equip them on how they could set up projection for the screen",
    "start": "561600",
    "end": "569860"
  },
  {
    "text": "so let's say they want to add one more fan-out stream from this source stream",
    "start": "569860",
    "end": "575980"
  },
  {
    "text": "and they want to include only the fields that match these criteria so if you have",
    "start": "575980",
    "end": "581260"
  },
  {
    "text": "a JSON blob and if you have fields in there called user ID and clicks it takes a projection of that and only that",
    "start": "581260",
    "end": "586959"
  },
  {
    "text": "information gets eventually seen ascent to the sink so if you're sending this to",
    "start": "586959",
    "end": "592240"
  },
  {
    "text": "a hive sink as well and you're only sending a subset of the data that's flowing in so you have a producer that's",
    "start": "592240",
    "end": "598660"
  },
  {
    "text": "producing at one time into a stream and then you have all these interesting consumers who may want to consume just a",
    "start": "598660",
    "end": "604600"
  },
  {
    "text": "portion of it or a projection of the data set of the payload or just process",
    "start": "604600",
    "end": "609610"
  },
  {
    "text": "the whole raw stream so all this is just achieved by user coming into your system and with a few clicks to have it all",
    "start": "609610",
    "end": "615459"
  },
  {
    "text": "provision for them so here's a example",
    "start": "615459",
    "end": "622089"
  },
  {
    "text": "of setting up a elasticsearch sink so here we let the user specify an index how we roll the index whether it's",
    "start": "622089",
    "end": "629200"
  },
  {
    "text": "monthly daily or weekly and they can also specify which field is the one they",
    "start": "629200",
    "end": "635110"
  },
  {
    "text": "want to use for indexing and what's the timestamp field so it gives them flexibility to write to elasticsearch as",
    "start": "635110",
    "end": "641410"
  },
  {
    "text": "one of the sinks we also allow them to write to you a Kafka cluster which is a",
    "start": "641410",
    "end": "647110"
  },
  {
    "text": "supported sink out of the box and here we allow them to specify a partition key so that you can actually write partition",
    "start": "647110",
    "end": "653800"
  },
  {
    "text": "messages into the pipeline and it follows the same syntax of picking the",
    "start": "653800",
    "end": "659650"
  },
  {
    "text": "key name from the JSON blob so this is an example of another pipeline that was set up here",
    "start": "659650",
    "end": "666850"
  },
  {
    "text": "the incoming stream is being sent to three different things elasticsearch",
    "start": "666850",
    "end": "672069"
  },
  {
    "text": "hive and Kafka so what just happened is",
    "start": "672069",
    "end": "678940"
  },
  {
    "text": "we created a topic on Kafka we created three stream processing jobs and then we created a topic on the consuming Kafka",
    "start": "678940",
    "end": "687459"
  },
  {
    "text": "cluster and hive in elasticsearch are more dynamic the hive automatically creates tables first time it sees an",
    "start": "687459",
    "end": "694060"
  },
  {
    "text": "event coming from the topic so all this got provisioned for the users and they",
    "start": "694060",
    "end": "699279"
  },
  {
    "text": "didn't have to do anything for it we manage the whole infrastructure and so",
    "start": "699279",
    "end": "704319"
  },
  {
    "text": "we'll quickly go through the event flow they'll keep seeing this image again and again as we dig through the different layers of the system so the left-hand",
    "start": "704319",
    "end": "712180"
  },
  {
    "text": "side what you see right now is the producer the user uses a producer",
    "start": "712180",
    "end": "717519"
  },
  {
    "text": "library or a proxy to produce the events into our fronting Kafka cluster once it's there if these are we have quite a",
    "start": "717519",
    "end": "724870"
  },
  {
    "text": "few of these clusters and it gets routed to the right cluster based on capacity that we set up and we do so user doesn't",
    "start": "724870",
    "end": "732100"
  },
  {
    "text": "have to do that either and then we launch the routing job that we had",
    "start": "732100",
    "end": "738399"
  },
  {
    "text": "created so there's one for each sync if you remember and the data for each one gets routed to those and if you see the",
    "start": "738399",
    "end": "746230"
  },
  {
    "text": "first job routed data drive the second job right routes the data to elasticsearch and the third one goes to",
    "start": "746230",
    "end": "752560"
  },
  {
    "text": "the Kafka and all these are happening in parallel they're not waiting for each other and there's a nice level of",
    "start": "752560",
    "end": "758230"
  },
  {
    "text": "isolation here because let's say your last exert is down it doesn't prevent your hive job or consuming Kafka job to",
    "start": "758230",
    "end": "764680"
  },
  {
    "text": "be delayed or events being delivered to be too delayed because each one is running off of a separate job so there's",
    "start": "764680",
    "end": "770380"
  },
  {
    "text": "built-in isolation so a user creating a filter and if the filter is bad they",
    "start": "770380",
    "end": "775630"
  },
  {
    "text": "don't impact another user who may have a different stream coming off of the same source Kafka topic we don't allow keyed",
    "start": "775630",
    "end": "785050"
  },
  {
    "text": "messages into our fronting cluster and I'll go into the reasons why that's a really highly available part of our",
    "start": "785050",
    "end": "790930"
  },
  {
    "text": "infrastructure and we want to make sure we get every single event that's thrown at us and we don't lose them and with key topics and",
    "start": "790930",
    "end": "800020"
  },
  {
    "text": "with the scale we run at it becomes very difficult to guarantee that availability so we do offer that functionality",
    "start": "800020",
    "end": "805630"
  },
  {
    "text": "downstream and in addition to you creating this pipeline the framework",
    "start": "805630",
    "end": "812470"
  },
  {
    "text": "automatically generates dashboards for each stream so in this scenario it would create three different sets of",
    "start": "812470",
    "end": "818550"
  },
  {
    "text": "dashboards one for each stream and this is just a snippet of what we captured there are tens of metrics that we",
    "start": "818550",
    "end": "826000"
  },
  {
    "text": "capture here it's just showing what the producer is producing and what the router is reading from and what the",
    "start": "826000",
    "end": "832330"
  },
  {
    "text": "message lag is how many messages were filtered what were not filtered are there any failures so just these two",
    "start": "832330",
    "end": "839260"
  },
  {
    "text": "graphs gives us everything that we need to know quickly and this is expose to our users as well so they can self-serve",
    "start": "839260",
    "end": "844720"
  },
  {
    "text": "themselves to see therein issues and they can always escalate it back to us if there's a problem in the",
    "start": "844720",
    "end": "849820"
  },
  {
    "text": "infrastructure because they don't manage any of those it's travellish this is more for an admin use for us behind the",
    "start": "849820",
    "end": "856600"
  },
  {
    "text": "scenes for our own calls we create searchable index logs from these jobs automatically so that if something goes",
    "start": "856600",
    "end": "863110"
  },
  {
    "text": "wrong we can quickly take a look at it now behaviorally and rely on our instrumentation and metrics and",
    "start": "863110",
    "end": "868660"
  },
  {
    "text": "dashboards but there are always cases where when you're using a lot of third-party libraries there may be some",
    "start": "868660",
    "end": "873940"
  },
  {
    "text": "logs that you want to take a look at then something goes wrong so this is where we use it for it's more of a complementary then do",
    "start": "873940",
    "end": "881350"
  },
  {
    "text": "everything through logs kind of approach in addition to that we also expose a",
    "start": "881350",
    "end": "889780"
  },
  {
    "text": "link directly to the stream processing jobs UI so flink exposes a web UI for",
    "start": "889780",
    "end": "895480"
  },
  {
    "text": "the job that's running so we can actually dig in and look further what's happening into the system so this is the",
    "start": "895480",
    "end": "903400"
  },
  {
    "text": "part of the UI that gives access to all these links so the dashboards the UI we",
    "start": "903400",
    "end": "909250"
  },
  {
    "text": "can look at the laws we can look at the job history deployment and see what happened with different deployments and",
    "start": "909250",
    "end": "914290"
  },
  {
    "text": "so it gives a one place access for our calls or for administration purposes and",
    "start": "914290",
    "end": "919660"
  },
  {
    "text": "operations purposes so it's fully managed we managed the",
    "start": "919660",
    "end": "927380"
  },
  {
    "text": "capacity planning the scaling of the 24/7 operations for it we even do garbage collection we look at streams",
    "start": "927380",
    "end": "934580"
  },
  {
    "text": "that have not been used for a month or two and then V inform the users and then we just clean them up automatically so",
    "start": "934580",
    "end": "940280"
  },
  {
    "text": "it's truly managed end to end and so for the Keystone pipeline what we're trying",
    "start": "940280",
    "end": "946280"
  },
  {
    "text": "to do ahead is add more support for UDF so you could go into the UI and you know plug in your JavaScript code or point to",
    "start": "946280",
    "end": "952640"
  },
  {
    "text": "a jar and we'll run it for you as part of the processing do some data hygiene modules do some data alerting so that",
    "start": "952640",
    "end": "959480"
  },
  {
    "text": "you can have custom rules in there to fire and then also an ability to chain",
    "start": "959480",
    "end": "965900"
  },
  {
    "text": "all these components in the UI itself so you can have complex chaining or filters projections UDF's data hygiene and then",
    "start": "965900",
    "end": "972710"
  },
  {
    "text": "you've built a pipeline based on those components schema support and data lineage and we also do cost attribution",
    "start": "972710",
    "end": "979880"
  },
  {
    "text": "so that we can expose this to our users and they're mindful of how much cost they're incurring by by provisioning the",
    "start": "979880",
    "end": "986030"
  },
  {
    "text": "data stream so that they can either cut back or they can continue provisioning it and so this is our approach to scale",
    "start": "986030",
    "end": "993800"
  },
  {
    "text": "him scaling up by scaling down because we've had you know customers or users",
    "start": "993800",
    "end": "998810"
  },
  {
    "text": "actually realized that they're spending a lot and they've looked at the streams and they've trimmed it down because they",
    "start": "998810",
    "end": "1004180"
  },
  {
    "text": "had excess data going through it which was never used so now we'll jump into you looking at",
    "start": "1004180",
    "end": "1010810"
  },
  {
    "text": "the product for the streaming jobs so when would you need to write one is when",
    "start": "1010810",
    "end": "1016270"
  },
  {
    "text": "you functionality provided by the pipeline's not sufficient you may want to do something more complex than just the basic projection and filtering so",
    "start": "1016270",
    "end": "1024160"
  },
  {
    "text": "out of the box we have support for generating a job out of a template this",
    "start": "1024160",
    "end": "1030640"
  },
  {
    "text": "is very similar to you may have an archetype approach but this is our custom-built tooling for greater because",
    "start": "1030640",
    "end": "1036579"
  },
  {
    "text": "it does a lot more than just that and so we used this tooling to create a quick job it automatically commits the code",
    "start": "1036579",
    "end": "1044530"
  },
  {
    "text": "for you in a git repo that you specify it even creates a Jenkins build it",
    "start": "1044530",
    "end": "1050560"
  },
  {
    "text": "creates a build pipeline and it also creates the configuration so you can learn the layup locally you can run the",
    "start": "1050560",
    "end": "1056500"
  },
  {
    "text": "job so the Jenkins job had created and then you can also run it locally in the IDE right",
    "start": "1056500",
    "end": "1061840"
  },
  {
    "text": "now we have a support for one ID the one that's used most widely and the users",
    "start": "1061840",
    "end": "1068050"
  },
  {
    "text": "can just load it up and then just debug the job locally in their IDE once they are ready to deploy it into",
    "start": "1068050",
    "end": "1074500"
  },
  {
    "text": "one of the supported environments then they come in and create a configuration",
    "start": "1074500",
    "end": "1080980"
  },
  {
    "text": "so here they specify what their app name is again who the owner is and then we have support for some of the built in",
    "start": "1080980",
    "end": "1088150"
  },
  {
    "text": "sources and sinks so if it's one of the built in source like Kafka then we",
    "start": "1088150",
    "end": "1093670"
  },
  {
    "text": "automatically find the cluster for them we give the WIPP name to them we automatically configure job the job for",
    "start": "1093670",
    "end": "1099550"
  },
  {
    "text": "them so we put these overrides when they deploy it so for example if they give us an image and these are docker images",
    "start": "1099550",
    "end": "1105790"
  },
  {
    "text": "that they give us in test environment it might be connecting to a different cluster in prod it might be connecting",
    "start": "1105790",
    "end": "1111520"
  },
  {
    "text": "to a different one so they come in here and configure those overrides and then we apply those overrides this",
    "start": "1111520",
    "end": "1116860"
  },
  {
    "text": "configuration when we launch these jobs on our container runtime so they get",
    "start": "1116860",
    "end": "1122590"
  },
  {
    "text": "some out-of-the-box support for the sources so that they don't fat-finger the name of the cluster or they don't",
    "start": "1122590",
    "end": "1127870"
  },
  {
    "text": "have to go and discover it themselves and we keep the list internally and we can even search it based on the topics",
    "start": "1127870",
    "end": "1134680"
  },
  {
    "text": "as well so in addition to that they can",
    "start": "1134680",
    "end": "1139900"
  },
  {
    "text": "even specify the container containers they need what the sizing of that each of the container is and then there's an",
    "start": "1139900",
    "end": "1147010"
  },
  {
    "text": "option for adding additional properties that's just a free-flow name really pairs they can use to override anything",
    "start": "1147010",
    "end": "1153430"
  },
  {
    "text": "they specified in their configuration files that are packaged in the bundle or in the jar that the providers packaged",
    "start": "1153430",
    "end": "1162310"
  },
  {
    "text": "in a docker container so it's easy to deploy the same job in a different",
    "start": "1162310",
    "end": "1167740"
  },
  {
    "text": "environment and we kind of working on the promotion to make it even easier so",
    "start": "1167740",
    "end": "1173050"
  },
  {
    "text": "that you could just say promote the stress shop to prod and the configuration that needs to be changed they can change that configuration we",
    "start": "1173050",
    "end": "1179800"
  },
  {
    "text": "can look at a list of steps that were taken to actually deploy a job because at times there could be some failures",
    "start": "1179800",
    "end": "1185290"
  },
  {
    "text": "and this gives them an easy way to look at what happened in this case the last step of the job is a border",
    "start": "1185290",
    "end": "1191559"
  },
  {
    "text": "so they could look and see actually what went wrong and they could try redeploying it and this is useful tooling for us as well to look at any",
    "start": "1191559",
    "end": "1198249"
  },
  {
    "text": "infrastructure failures that we may have had so again similar to you the routing",
    "start": "1198249",
    "end": "1204129"
  },
  {
    "text": "jobs the user has access to all the dashboards and the search locks that we",
    "start": "1204129",
    "end": "1210490"
  },
  {
    "text": "provide for them out of the box so we create a bunch of metrics for them for",
    "start": "1210490",
    "end": "1215980"
  },
  {
    "text": "example what consumer offsets they're reading from are they lagging behind if",
    "start": "1215980",
    "end": "1221440"
  },
  {
    "text": "they're reading from Kafka what are the JV MDC metrics what are the Container metrics what a network metrics so",
    "start": "1221440",
    "end": "1226659"
  },
  {
    "text": "there's a slew of metrics we automatically generate for each job when they get to deployed and on top of this",
    "start": "1226659",
    "end": "1233320"
  },
  {
    "text": "the user can customize this dashboard and add their own metrics application metrics that they define in their code",
    "start": "1233320",
    "end": "1239679"
  },
  {
    "text": "and we also very make it very easy for them to define metrics in their code we",
    "start": "1239679",
    "end": "1245320"
  },
  {
    "text": "leverage Netflix's Atlas and Stovall library so they can just start creating counters and gauges and everything will",
    "start": "1245320",
    "end": "1251049"
  },
  {
    "text": "show up here and not only that but when we have new updates to our dashboard they can actually select it updated",
    "start": "1251049",
    "end": "1258159"
  },
  {
    "text": "automatically so they won't lose their customizations but will actually update all their dashboard with our new metrics",
    "start": "1258159",
    "end": "1264789"
  },
  {
    "text": "that said we added a new metric and we want it to show up so there are omissions they're out of the box and the",
    "start": "1264789",
    "end": "1270639"
  },
  {
    "text": "job logs also show up just like the router jobs we index them in elasticsearch so they can take a look at",
    "start": "1270639",
    "end": "1276610"
  },
  {
    "text": "it in addition to you providing the tooling for building developing and",
    "start": "1276610",
    "end": "1283269"
  },
  {
    "text": "deploying and monitoring it also becomes important to provide you know good consulting and documentation especially",
    "start": "1283269",
    "end": "1289389"
  },
  {
    "text": "even you're building out a platform that's in a green space like stream processing you have to you you know tell",
    "start": "1289389",
    "end": "1296379"
  },
  {
    "text": "users and teach them how to think tree streaming first how to approach the problem from streaming perspective and",
    "start": "1296379",
    "end": "1302200"
  },
  {
    "text": "so there's some amount of you know guidance that's necessary as part of",
    "start": "1302200",
    "end": "1307509"
  },
  {
    "text": "this platform and also to provide some recipes so it makes it easier for them to build what they're building so we'll",
    "start": "1307509",
    "end": "1315009"
  },
  {
    "text": "quickly look at what kind of screen processing jobs are these kind of custom jobs that we look at the two broad",
    "start": "1315009",
    "end": "1320649"
  },
  {
    "text": "categories of these jobs one is stateless what I mean by stateless is there's no",
    "start": "1320649",
    "end": "1326890"
  },
  {
    "text": "state maintained between the events they just process an event they do something with it they move on to the next one",
    "start": "1326890",
    "end": "1332320"
  },
  {
    "text": "they don't maintain anything across those events the second class of events are sorry the streaming jobs are",
    "start": "1332320",
    "end": "1338200"
  },
  {
    "text": "stateful that means the adt in some kind of state across these events and these",
    "start": "1338200",
    "end": "1345130"
  },
  {
    "text": "could be like you know windowing or aggregations or joins and stateless could be you're just doing some map or",
    "start": "1345130",
    "end": "1351250"
  },
  {
    "text": "filtering or just doing a quick transformation and sending the data back on Toa differencing so it's this part of",
    "start": "1351250",
    "end": "1359530"
  },
  {
    "text": "the architecture that we're going to be talking about we talked about the",
    "start": "1359530",
    "end": "1364570"
  },
  {
    "text": "Keystone pipeline which feeds into the consumer cough or cluster and the streaming jobs pick up from there so for",
    "start": "1364570",
    "end": "1371830"
  },
  {
    "text": "a stateless stream processor this kind of it looks like they're each events coming flowing through n of you doing",
    "start": "1371830",
    "end": "1378310"
  },
  {
    "text": "the processing and then it goes its merry way out for the stateful sorry",
    "start": "1378310",
    "end": "1386650"
  },
  {
    "text": "before I go to that the so stateless has two variations unlike the traditional",
    "start": "1386650",
    "end": "1393130"
  },
  {
    "text": "way of thinking about it in stream processing when we talk about stateless we are talking about what's happening in the process that's processing the events",
    "start": "1393130",
    "end": "1399430"
  },
  {
    "text": "now you could have external database that you're maintaining to write some data to and look up some data so for",
    "start": "1399430",
    "end": "1405460"
  },
  {
    "text": "example you can have some lookup data that you want to look up right so that's not really considered the state that's",
    "start": "1405460",
    "end": "1410620"
  },
  {
    "text": "being built into the platform itself when you're processing so it's important to you kind of draw that distinction so",
    "start": "1410620",
    "end": "1417090"
  },
  {
    "text": "now let's take a look at an example of a stateless job so we have again users",
    "start": "1417090",
    "end": "1422620"
  },
  {
    "text": "interacting on our website we want to understand that behavior and feed this into our personalization system and also",
    "start": "1422620",
    "end": "1429670"
  },
  {
    "text": "how we can effectively discover these shows so this job kind of takes that information and feeds it downstream for",
    "start": "1429670",
    "end": "1435910"
  },
  {
    "text": "additional processing said the high level it kind of looks like this where the play logs are getting into a",
    "start": "1435910",
    "end": "1441490"
  },
  {
    "text": "Keystone pipeline which kind of feeds into the stream processing job this job calls actually a live service does an",
    "start": "1441490",
    "end": "1448150"
  },
  {
    "text": "enrichment and also looks up some live lookup data and then it creates this new",
    "start": "1448150",
    "end": "1454030"
  },
  {
    "text": "payload and then sends it to Keystone which sends it to high velocity shirts and other things so what we did here is you're using the",
    "start": "1454030",
    "end": "1461500"
  },
  {
    "text": "Keystone pipeline as a glue for additional processing that can be done so this job generated a feed that's",
    "start": "1461500",
    "end": "1468280"
  },
  {
    "text": "interesting to different users and they can take those feeds and then route it to where they need to and for stateful",
    "start": "1468280",
    "end": "1478960"
  },
  {
    "text": "stream processing for example and you're doing VIN during you need to hold the state across events for the amount of",
    "start": "1478960",
    "end": "1484270"
  },
  {
    "text": "duration of the window or if you're doing aggregations across those windows then you need to do the same thing so in",
    "start": "1484270",
    "end": "1490120"
  },
  {
    "text": "this scenario you're actually building up state in in the process that's processing these events so an example",
    "start": "1490120",
    "end": "1496270"
  },
  {
    "text": "for that would be let's say the user is searching for movies and we want to create a session out of the short",
    "start": "1496270",
    "end": "1501970"
  },
  {
    "text": "sessions and figure out how many such search sessions happen and what could we do better to make the user experience",
    "start": "1501970",
    "end": "1507880"
  },
  {
    "text": "better so in this scenario we're trying",
    "start": "1507880",
    "end": "1514179"
  },
  {
    "text": "to create custom windows out of out of order events because you could press a key and each key becomes an event and",
    "start": "1514179",
    "end": "1521110"
  },
  {
    "text": "they could be delivered out of order because these are happening on a device it's happening on a television at your house and they get routed back to our",
    "start": "1521110",
    "end": "1528010"
  },
  {
    "text": "back-end and it could be delivered out of order right there's no guarantee if it being delivered in exact sequence as",
    "start": "1528010",
    "end": "1533559"
  },
  {
    "text": "it happened so you could have as s start event and use the end event and this",
    "start": "1533559",
    "end": "1539950"
  },
  {
    "text": "could define our session so this is the best case where you get them in order there's another scenario where you could",
    "start": "1539950",
    "end": "1546790"
  },
  {
    "text": "get the start of end and you may not get the end event for awhile and your session may span for ours and then you",
    "start": "1546790",
    "end": "1552910"
  },
  {
    "text": "get an end event right and then you get another start of it so now we pick the start of and that's the earliest and",
    "start": "1552910",
    "end": "1559420"
  },
  {
    "text": "then the end event and then now we have formed a session so to do something like this you have to retain those events in",
    "start": "1559420",
    "end": "1566559"
  },
  {
    "text": "the state of the processor and then you can do any aggregations or you can omit results based on even though once you",
    "start": "1566559",
    "end": "1573340"
  },
  {
    "text": "identify the session window or you could decide that the end event didn't arrive in a certain time out and you could just",
    "start": "1573340",
    "end": "1578860"
  },
  {
    "text": "drop the whole session or Park it somewhere else in a deadlier queue and come back to it later so with stateful",
    "start": "1578860",
    "end": "1588130"
  },
  {
    "text": "streaming application there's local state now then there's local state how guarantee accuracy vendors fault and",
    "start": "1588130",
    "end": "1596230"
  },
  {
    "text": "fault domains in your system what if some system some parts of your job goes down some containers go down how do you",
    "start": "1596230",
    "end": "1603400"
  },
  {
    "text": "recover right so this is where we could rely on checkpoints and save points so",
    "start": "1603400",
    "end": "1608679"
  },
  {
    "text": "there's a quick animation here that'll draw the difference between the two so as the systems flowing through you can",
    "start": "1608679",
    "end": "1615490"
  },
  {
    "text": "configure regular checkpoints through the system so what it does is based on",
    "start": "1615490",
    "end": "1620860"
  },
  {
    "text": "the duration you've set up it takes regular snapshots of the data that are being accumulated the states that's",
    "start": "1620860",
    "end": "1626679"
  },
  {
    "text": "being accumulated and it does Delta's of it so it only does incremental checkpoints and then as a",
    "start": "1626679",
    "end": "1634390"
  },
  {
    "text": "user you can actually trigger explicit same points so the difference between checkpoints and save points is the save",
    "start": "1634390",
    "end": "1641410"
  },
  {
    "text": "point takes a complete global snapshot of that point in time and it does it in",
    "start": "1641410",
    "end": "1646570"
  },
  {
    "text": "a very safe way that means it makes sure that nothing else is being processed while it took that save point and if you",
    "start": "1646570",
    "end": "1652660"
  },
  {
    "text": "recover from that same point it'll start off where it left off so let's say you're processing off set hundred from",
    "start": "1652660",
    "end": "1658929"
  },
  {
    "text": "Kafka and then you process the first hundred messages and then your job went down and you take in the same point",
    "start": "1658929",
    "end": "1665080"
  },
  {
    "text": "before it when you come back and start from that same point it'll actually start from message 101 and then it'll",
    "start": "1665080",
    "end": "1670510"
  },
  {
    "text": "start processing accurately from there and the checkpoints you're only taking",
    "start": "1670510",
    "end": "1675580"
  },
  {
    "text": "incrementally and those are useful when you have a partial failure of your stream processing job because it could",
    "start": "1675580",
    "end": "1682600"
  },
  {
    "text": "be running on multiple instances and two of the instances could fail the rather of ten are running fine and this helps",
    "start": "1682600",
    "end": "1688090"
  },
  {
    "text": "you do partial recovery and we leverage the functionality in flame that supports",
    "start": "1688090",
    "end": "1693429"
  },
  {
    "text": "distributed checkpoints and safe pointing so we have added some additional tooling in our pipeline we",
    "start": "1693429",
    "end": "1701020"
  },
  {
    "text": "don't have the uia to be working on it so I don't have a demo for you on that but what we do is before we do a",
    "start": "1701020",
    "end": "1708549"
  },
  {
    "text": "redeploy or deployment we automatically take a save point on Amazon s3 and we",
    "start": "1708549",
    "end": "1715750"
  },
  {
    "text": "have the ability to automatically resume from the same point when they say I want to redeploy it you can pick a safe one and it will resume from that and in",
    "start": "1715750",
    "end": "1722530"
  },
  {
    "text": "addition to that the check points are automatically configured out of the bar so it'll automatically tech checkpoints",
    "start": "1722530",
    "end": "1727829"
  },
  {
    "text": "so if there's partial failure it'll partially recover so the user does not have to do anything special for checkpointing and they don't have to do",
    "start": "1727829",
    "end": "1734129"
  },
  {
    "text": "anything special for save pointing except to choose which one they want to resume from if they don't choose the",
    "start": "1734129",
    "end": "1739649"
  },
  {
    "text": "most recent one will automatically get selected for them so some of the",
    "start": "1739649",
    "end": "1746489"
  },
  {
    "text": "high-level features just to summarize you know we could work with stateless jobs stateful jobs we can do event and",
    "start": "1746489",
    "end": "1753179"
  },
  {
    "text": "Richmond we can use reusable blocks so what we saw earlier about the different",
    "start": "1753179",
    "end": "1758609"
  },
  {
    "text": "sources and sync connectors and the Keystone pipeline can be leveraged in here so that you can just say I want to write to you lassic search with this web",
    "start": "1758609",
    "end": "1765719"
  },
  {
    "text": "and we'll figure out the configuration and we look at the server's they'll do the batching in appropriate retries and",
    "start": "1765719",
    "end": "1771209"
  },
  {
    "text": "we'll send it - it will generate the metrics we'll do all of them for them so they can just use this component and attach it to their job and just say dot",
    "start": "1771209",
    "end": "1778169"
  },
  {
    "text": "sync and they can write the events there in the code that they're writing so",
    "start": "1778169",
    "end": "1784739"
  },
  {
    "text": "right now our stateful jobs we have that hundreds of gigabytes of state and any point in time its life and you're",
    "start": "1784739",
    "end": "1790469"
  },
  {
    "text": "working to support much larger state for some of the use cases that we have so",
    "start": "1790469",
    "end": "1796859"
  },
  {
    "text": "we're just getting started on the streaming jobs part of it we want to do easy resource provisioning so that we",
    "start": "1796859",
    "end": "1804779"
  },
  {
    "text": "can provide them with the right bootstrap set of machines to start off with and you're trying to continually",
    "start": "1804779",
    "end": "1811379"
  },
  {
    "text": "evolve this tooling as well as we go along our tooling is kind of generic enough that we're also looking at adding",
    "start": "1811379",
    "end": "1817799"
  },
  {
    "text": "support for sprach streaming we have a smaller user base of using that",
    "start": "1817799",
    "end": "1822929"
  },
  {
    "text": "technology but we can easily leverage the same tooling for spark streaming as well so what scale is all this running",
    "start": "1822929",
    "end": "1831209"
  },
  {
    "text": "at we've seen a significant increase since the last three years in the amount",
    "start": "1831209",
    "end": "1836369"
  },
  {
    "text": "of events that are being produced into our system so we process over 1.3 trillion events every day 600 million",
    "start": "1836369",
    "end": "1843989"
  },
  {
    "text": "uniques - 1 trillion earnings depending on the season so if you are hitting the peak season slowly so we'll see a lot",
    "start": "1843989",
    "end": "1850229"
  },
  {
    "text": "more events flowing through our system now and about 2 petabytes comes in and if you recall we have a fan-out",
    "start": "1850229",
    "end": "1857009"
  },
  {
    "text": "depending on how many things the data is going to and so we produce about 4.5 petabytes",
    "start": "1857009",
    "end": "1863129"
  },
  {
    "text": "every single day we peak to about 12 million events and 36 gigabytes a second",
    "start": "1863129",
    "end": "1869899"
  },
  {
    "text": "and all this is running on when you saw the different paths we looked at the",
    "start": "1869899",
    "end": "1875490"
  },
  {
    "text": "streaming job that had three we have about close to you 1,900 of those jobs actually running today and runs on over",
    "start": "1875490",
    "end": "1882419"
  },
  {
    "text": "9000 containers and on 3004 for excels so this whole infrastructure is only the",
    "start": "1882419",
    "end": "1888929"
  },
  {
    "text": "routing infrastructure this is not the streaming jobs one and this thing is fully managed I was saying is throttle",
    "start": "1888929",
    "end": "1894149"
  },
  {
    "text": "less for the users so the question is how do we do it so we have four core",
    "start": "1894149",
    "end": "1901379"
  },
  {
    "text": "pieces in our platform one is the core",
    "start": "1901379",
    "end": "1907970"
  },
  {
    "text": "messaging system and we use leveraged Kafka for and we have our stream",
    "start": "1907970",
    "end": "1913049"
  },
  {
    "text": "processing layer that we looked at and we have something called a keystone management service that does the",
    "start": "1913049",
    "end": "1918509"
  },
  {
    "text": "orchestration of this it's the thing that's backing the UI and it has some autumn cool automations around it and",
    "start": "1918509",
    "end": "1924139"
  },
  {
    "text": "Keystone pipeline kind of leverages all those three blocks and it's built on top and then the stream processing jobs gets",
    "start": "1924139",
    "end": "1930779"
  },
  {
    "text": "used to it again and we also have a development environment a test environment and a prod environment and",
    "start": "1930779",
    "end": "1936059"
  },
  {
    "text": "we have functionality do granule shadowing what I mean by that is you may",
    "start": "1936059",
    "end": "1941100"
  },
  {
    "text": "have a small job or a small stream and you have a new version of it you want to try it before you deploy it so we have",
    "start": "1941100",
    "end": "1947240"
  },
  {
    "text": "functionality to kind of try that in production with a different stream and you can route it to a different sink and",
    "start": "1947240",
    "end": "1953730"
  },
  {
    "text": "if you see everything's fine then you can say ok now let me enable this for actual production so back to the graphic",
    "start": "1953730",
    "end": "1963059"
  },
  {
    "text": "as promised so the stuff that's not grayed out are the components of the",
    "start": "1963059",
    "end": "1970139"
  },
  {
    "text": "streaming pipeline the Keystone pipeline and the streaming jobs and we'll peel the layers one at a time of each of",
    "start": "1970139",
    "end": "1976529"
  },
  {
    "text": "these components so let's look at a producer library so this is a Java",
    "start": "1976529",
    "end": "1981779"
  },
  {
    "text": "client wrapper on the Kafka library and vias OTN version of Kafka what we",
    "start": "1981779",
    "end": "1987450"
  },
  {
    "text": "automatically do when a user uses this libraries we inject certain amount of metadata into it unique",
    "start": "1987450",
    "end": "1993180"
  },
  {
    "text": "I'm sorry unique ID a timestamp host it's coming from an app and this is really useful for us downstream to do",
    "start": "1993180",
    "end": "1999720"
  },
  {
    "text": "some neat application and identify where the directly came from and we also do",
    "start": "1999720",
    "end": "2005290"
  },
  {
    "text": "transparent routing of producers what I mean by that is if you add a new cluster",
    "start": "2005290",
    "end": "2011960"
  },
  {
    "text": "tomorrow and we want that producer to route to it the user does not have to make any changes they don't have to",
    "start": "2011960",
    "end": "2017000"
  },
  {
    "text": "redeploy they don't have to change their app we change a dynamic property and we",
    "start": "2017000",
    "end": "2022400"
  },
  {
    "text": "reset the connection to you the Kafka cluster and the producer starts",
    "start": "2022400",
    "end": "2027410"
  },
  {
    "text": "producing to a different cluster so it automatically happens and the user is not involved in it we can we can easily",
    "start": "2027410",
    "end": "2034340"
  },
  {
    "text": "manage that and I talked about our custom binary data wrapper that allows",
    "start": "2034340",
    "end": "2041600"
  },
  {
    "text": "us to support multiple serialization formats and also it allows us to add additional metadata that we can use",
    "start": "2041600",
    "end": "2048290"
  },
  {
    "text": "within the system and it has really tight integration into our Netflix ecosystem of metrics and discovery",
    "start": "2048290",
    "end": "2054379"
  },
  {
    "text": "service and properly sort of dynamic property services so the boundary of the",
    "start": "2054380",
    "end": "2063290"
  },
  {
    "text": "custom data wrapper is just within the pipeline we don't expose it downstream",
    "start": "2063290",
    "end": "2068840"
  },
  {
    "text": "unless the user explicitly selects to get it in the chaski format as we saw in the UI otherwise it's bounded within our within",
    "start": "2068840",
    "end": "2075560"
  },
  {
    "text": "our system boundaries and when it lands at a certain sink like elastic search or",
    "start": "2075560",
    "end": "2080720"
  },
  {
    "text": "hive it lands in a JSON format or on the format that they specified so our goal",
    "start": "2080720",
    "end": "2087560"
  },
  {
    "text": "is to keep the uptime really high because these libraries are being used in API servers and other end servers",
    "start": "2087560",
    "end": "2094970"
  },
  {
    "text": "that are serving actually user traffic so we don't want to impact those so our approach is to drop a few events rather",
    "start": "2094970",
    "end": "2102050"
  },
  {
    "text": "than actually impacting the application that we're running on so that's why we run with act equals one and CAFTA",
    "start": "2102050",
    "end": "2108500"
  },
  {
    "text": "we run with unclean unclean leader election we don't allow keyed messages into our fronting kafka cluster and we",
    "start": "2108500",
    "end": "2114800"
  },
  {
    "text": "also have several retries me retry mechanisms built in and we do a round robin right of events into Kafka which",
    "start": "2114800",
    "end": "2122480"
  },
  {
    "text": "allows us to make sure there's at least one partition available even if there's an unclean leader election and despite",
    "start": "2122480",
    "end": "2128180"
  },
  {
    "text": "these discounts that you've done in the system this is a little conservative but",
    "start": "2128180",
    "end": "2134180"
  },
  {
    "text": "we do upward to four nines of reliability on affronting Kafka clusters so we have a self not losing more than",
    "start": "2134180",
    "end": "2141860"
  },
  {
    "text": "one person of data per stream every day so it's a pretty high 'silly and this format has worked for us so far so the",
    "start": "2141860",
    "end": "2150650"
  },
  {
    "text": "next is the gate view proxy this is just a rest or gr pc-based proxy that non java clients can use to send events into",
    "start": "2150650",
    "end": "2156590"
  },
  {
    "text": "our system then it's the fronting Kafka clusters and the consumer Kafka clusters",
    "start": "2156590",
    "end": "2163630"
  },
  {
    "text": "and we have message sizes anywhere from a few kilobytes to up to 10 megabytes",
    "start": "2163630",
    "end": "2170300"
  },
  {
    "text": "and so that's one reason why we have to look at a custom solution rather than",
    "start": "2170300",
    "end": "2176330"
  },
  {
    "text": "use something out of the box our Kafka classes also get used for Eric",
    "start": "2176330",
    "end": "2181850"
  },
  {
    "text": "messaging not just for the analytics use case and he also makes it easier for us to do",
    "start": "2181850",
    "end": "2188270"
  },
  {
    "text": "enhancements like for example we had a track of their deployment partition",
    "start": "2188270",
    "end": "2193370"
  },
  {
    "text": "assignments to it as well and we can make some other enhancements to it for",
    "start": "2193370",
    "end": "2198470"
  },
  {
    "text": "integration into our ecosystem better make it easier to deploy and make changes to it but most of the time all",
    "start": "2198470",
    "end": "2206240"
  },
  {
    "text": "the stuff we do we try to contribute it back to open source Kafka the only",
    "start": "2206240",
    "end": "2211700"
  },
  {
    "text": "confidence is not managed for us we have to build and manage this service so",
    "start": "2211700",
    "end": "2218120"
  },
  {
    "text": "coming back to why we have the segregation between a fronting Kafka cluster and a consumer Kafka cluster",
    "start": "2218120",
    "end": "2224240"
  },
  {
    "text": "apart from the high availability is also a way to scale because we have a very",
    "start": "2224240",
    "end": "2229250"
  },
  {
    "text": "large fan-out compared to the messages coming in and to support that large",
    "start": "2229250",
    "end": "2235100"
  },
  {
    "text": "fan-out we take a couple approaches one is the hierarchical arrangement of",
    "start": "2235100",
    "end": "2241490"
  },
  {
    "text": "clusters so that we have one cluster that's producing we take that and put it into another cluster and then we can fan",
    "start": "2241490",
    "end": "2248270"
  },
  {
    "text": "out from there and there's a I'll get to there's a limitation of the number of",
    "start": "2248270",
    "end": "2253880"
  },
  {
    "text": "brokers we have in a cluster so that's why we need this hierarchical nature because our clusters cannot grow",
    "start": "2253880",
    "end": "2259239"
  },
  {
    "text": "indefinitely in the number of brokers we have so the alternative solution is just",
    "start": "2259239",
    "end": "2266079"
  },
  {
    "text": "logically shard the topic across two clusters and then read from them this is something we are actually working on",
    "start": "2266079",
    "end": "2272299"
  },
  {
    "text": "right now as well and there are some challenges around when it comes to key topics and being able to do fail overs",
    "start": "2272299",
    "end": "2278150"
  },
  {
    "text": "but this really offers us functionality to do quick fail overs if you wanted to and the producers if they use this",
    "start": "2278150",
    "end": "2284809"
  },
  {
    "text": "library the multi consumer library that people think then it's transparent to them so it allows us to do failover and",
    "start": "2284809",
    "end": "2292160"
  },
  {
    "text": "fall tones really easily for our consumers we do that today for our producers and we'll I'll go over that in",
    "start": "2292160",
    "end": "2300200"
  },
  {
    "text": "a second how we do that but consumers we don't have that functionality so this would provide us that functionality so",
    "start": "2300200",
    "end": "2309259"
  },
  {
    "text": "I'll quickly run through these deployment strategies and if you want to know a lot more we have a presentation",
    "start": "2309259",
    "end": "2315849"
  },
  {
    "text": "from my colleague at Kafka summit this year and you can look at all the details but at the high level we try to keep our",
    "start": "2315849",
    "end": "2322249"
  },
  {
    "text": "clusters small in a less than 200 brokers and less than 10,000 partitions",
    "start": "2322249",
    "end": "2327259"
  },
  {
    "text": "we have a dedicated zookeeper cluster for each cluster so that any outages in",
    "start": "2327259",
    "end": "2332869"
  },
  {
    "text": "one zookeeper cluster does not take out a whole farm we do recover up Lucas admin so that it's zone aware and",
    "start": "2332869",
    "end": "2339519"
  },
  {
    "text": "outages in one zone we don't lose all the data we keep two copies and we do",
    "start": "2339519",
    "end": "2345140"
  },
  {
    "text": "only unclean leader election on the fronting clusters when it comes to the consumer Kafka clusters we may relax",
    "start": "2345140",
    "end": "2350960"
  },
  {
    "text": "that and have more copies but as of this time we're on and we don't use any",
    "start": "2350960",
    "end": "2355969"
  },
  {
    "text": "transactions yet we use o to end version of the cluster so so we have over thirty",
    "start": "2355969",
    "end": "2361219"
  },
  {
    "text": "six clusters running in different regions and we have over 4,000 brokers",
    "start": "2361219",
    "end": "2366410"
  },
  {
    "text": "and this runs on ec2 this does not run on containers and we have over 700",
    "start": "2366410",
    "end": "2372589"
  },
  {
    "text": "different topics that we support on this and it's highly available and we use",
    "start": "2372589",
    "end": "2377660"
  },
  {
    "text": "different retention time periods depending on which cluster and topic it is and they range from two hours to 24",
    "start": "2377660",
    "end": "2384170"
  },
  {
    "text": "hours so now looking at the streaming jobs and the router and the stream",
    "start": "2384170",
    "end": "2390739"
  },
  {
    "text": "consumers and let's see how that's getting built excuse me so at the high level the user",
    "start": "2390739",
    "end": "2398420"
  },
  {
    "text": "creates the configuration and point-and-click we launched the job using our deployment tooling called",
    "start": "2398420",
    "end": "2404510"
  },
  {
    "text": "spinnaker it's the continuous delivery platform that we have and once the user",
    "start": "2404510",
    "end": "2410329"
  },
  {
    "text": "specifies declaratively the filters and projections we package that configuration and then we launch the job",
    "start": "2410329",
    "end": "2416000"
  },
  {
    "text": "on our container runtime and image they provide us as immutable we don't change them and we just change the",
    "start": "2416000",
    "end": "2421970"
  },
  {
    "text": "configuration that it gets deployed with which overrides the configuration that's baked in and it's this platform is built",
    "start": "2421970",
    "end": "2432020"
  },
  {
    "text": "on flink one three two and each flink",
    "start": "2432020",
    "end": "2437750"
  },
  {
    "text": "router is as mentioning becomes its own stream processing job and the provisioning of these routers are based",
    "start": "2437750",
    "end": "2444109"
  },
  {
    "text": "on incoming traffic so if we look at the incoming traffic based on the user estimate initially or if it's a live",
    "start": "2444109",
    "end": "2450710"
  },
  {
    "text": "stream then we look at the past weeks traffic every so often and then based on",
    "start": "2450710",
    "end": "2457280"
  },
  {
    "text": "the traffic either scale it up or scale it down so the user does not have to worry about it and we also have override",
    "start": "2457280",
    "end": "2464210"
  },
  {
    "text": "on it for cases where we know that somebody's going to produce large streams and it's planned we can scale it",
    "start": "2464210",
    "end": "2469579"
  },
  {
    "text": "up ahead of time so that they don't see any latency and all the streaming jobs",
    "start": "2469579",
    "end": "2476480"
  },
  {
    "text": "run on our container runtime environment and we don't have any cross region",
    "start": "2476480",
    "end": "2481670"
  },
  {
    "text": "streaming jobs at this time all of them are contained in each ad aqueous region so when it comes to the",
    "start": "2481670",
    "end": "2488390"
  },
  {
    "text": "custom streaming jobs we looked at the stateless and stateful examples it's a similar flow we leverage the similar",
    "start": "2488390",
    "end": "2495200"
  },
  {
    "text": "tooling that we have the differences the user provides us the code rather than V",
    "start": "2495200",
    "end": "2501109"
  },
  {
    "text": "providing the code and the management of it as split so they manage the business application side of issues and problems",
    "start": "2501109",
    "end": "2507829"
  },
  {
    "text": "that they happen when it runs we manage the infrastructure part of it so if you",
    "start": "2507829",
    "end": "2513680"
  },
  {
    "text": "peel the layers and see what actually is being used at the bottom you have ec2",
    "start": "2513680",
    "end": "2519920"
  },
  {
    "text": "where runs on then we have our container runtime system and then the core of the",
    "start": "2519920",
    "end": "2525470"
  },
  {
    "text": "framework code that we have is the stream processing platform that's based on",
    "start": "2525470",
    "end": "2530510"
  },
  {
    "text": "fling that has integrations into the Netflix ecosystem and build systems and then on top of that we have reusable",
    "start": "2530510",
    "end": "2537470"
  },
  {
    "text": "components that they can include into their projects and then on top of those",
    "start": "2537470",
    "end": "2542780"
  },
  {
    "text": "two we build the routers and the streaming jobs and then we have the orthogonal functionality about",
    "start": "2542780",
    "end": "2548720"
  },
  {
    "text": "monitoring dashboards development and tooling and that's leveraged across the stack so digging in one more layer",
    "start": "2548720",
    "end": "2557450"
  },
  {
    "text": "deeper how does this actually get deployed and what's the architecture of link link has a very similar architecture to spark if you're familiar",
    "start": "2557450",
    "end": "2565070"
  },
  {
    "text": "with it has a controller within a job called the job manager and then it has a bunch of task managers which are like",
    "start": "2565070",
    "end": "2570920"
  },
  {
    "text": "the executors and there's a leader you can run it in an on a che mode then you only need one job manager but we run it",
    "start": "2570920",
    "end": "2577160"
  },
  {
    "text": "in Nha mode so that the remaining job manager and can pick up if one fails so",
    "start": "2577160",
    "end": "2583400"
  },
  {
    "text": "the jobs always up and running and such a job banner is responsible for creating",
    "start": "2583400",
    "end": "2588440"
  },
  {
    "text": "the job graph taking the job graph and getting it executed and coordinating the",
    "start": "2588440",
    "end": "2594140"
  },
  {
    "text": "checkpoints triggering the same point so it helps kind of coordinate those activities but it's pretty light on",
    "start": "2594140",
    "end": "2600620"
  },
  {
    "text": "resource usage if you run it on just a two CPU container and it runs fine and",
    "start": "2600620",
    "end": "2609530"
  },
  {
    "text": "if you look at what's happening inside the task managers flink likes to split it up into what they call slots and each",
    "start": "2609530",
    "end": "2615860"
  },
  {
    "text": "slot can have operators change so for example if you have a filter and a map",
    "start": "2615860",
    "end": "2621130"
  },
  {
    "text": "those don't get scheduled on different machines they get collapsed and run on one box in one process and one thread so",
    "start": "2621130",
    "end": "2627980"
  },
  {
    "text": "that way you get a lot of efficiencies out of the box there so talking about",
    "start": "2627980",
    "end": "2635120"
  },
  {
    "text": "say plans and checkpoints as I was talking about at the job manager actually triggers it but the checkpoints",
    "start": "2635120",
    "end": "2641510"
  },
  {
    "text": "are actually taken directly by the task manager so they're distributed in a",
    "start": "2641510",
    "end": "2646550"
  },
  {
    "text": "fashion so the snapshots are distributed they're not all funneling back to the job manager and saving it somewhere it's",
    "start": "2646550",
    "end": "2652460"
  },
  {
    "text": "just a trigger and each task manager will actually save it independently on to the backend store",
    "start": "2652460",
    "end": "2660400"
  },
  {
    "text": "so this is from the fling Docs and it talks about how the check pointing",
    "start": "2660400",
    "end": "2665960"
  },
  {
    "text": "mechanism works it's very much similar to like our two phase commit where it waits for all the acts for the",
    "start": "2665960",
    "end": "2671630"
  },
  {
    "text": "checkpoints and it waits for everybody to having flushed out those checkpoints and then it says okay now the checkpoint",
    "start": "2671630",
    "end": "2677150"
  },
  {
    "text": "is complete so the checkpoints are taken",
    "start": "2677150",
    "end": "2682790"
  },
  {
    "text": "and often we have jobs that are set up to 15 seconds every 15 seconds we take a checkpoint so we only have to do 15",
    "start": "2682790",
    "end": "2689120"
  },
  {
    "text": "seconds worth of work is there's a failure and we recover from it and some",
    "start": "2689120",
    "end": "2694910"
  },
  {
    "text": "of the things that get saved in state is like cough coughs that is one of them in addition to the state that give me my",
    "start": "2694910",
    "end": "2700880"
  },
  {
    "text": "building so let's say a cluster in your job one of the containers filled right",
    "start": "2700880",
    "end": "2706370"
  },
  {
    "text": "and when the container fields we had already been saving State on a regular basis and now our container runtime",
    "start": "2706370",
    "end": "2714110"
  },
  {
    "text": "automatically replaces that container the container joins the job quorum reports to the job manager so it's back",
    "start": "2714110",
    "end": "2720770"
  },
  {
    "text": "in the job and now whatever failed whatever operations failed on it gets rescheduled on it and before it gets",
    "start": "2720770",
    "end": "2727520"
  },
  {
    "text": "rescheduled the state got restored so it can resume from where it left off now you might reprocess a few events again",
    "start": "2727520",
    "end": "2734630"
  },
  {
    "text": "and if your sink is not at important and you'll end up creating duplicates but this is an at least one system so if you",
    "start": "2734630",
    "end": "2742460"
  },
  {
    "text": "want an exactly one system then you'll have to have end-to-end control over your source and sink either it has to be",
    "start": "2742460",
    "end": "2747890"
  },
  {
    "text": "idempotent or it needs to support transactions so let's stick a little bit",
    "start": "2747890",
    "end": "2753860"
  },
  {
    "text": "into the management service and how it's being built so this is the current",
    "start": "2753860",
    "end": "2760520"
  },
  {
    "text": "architecture and I'll talk about what we are moving to so right now what happens is when you look at the provisioning of",
    "start": "2760520",
    "end": "2767330"
  },
  {
    "text": "the streams in the UI or you're launching a new streaming job we need to",
    "start": "2767330",
    "end": "2773150"
  },
  {
    "text": "actually take this action somewhere so this infrastructure creates what we call",
    "start": "2773150",
    "end": "2778340"
  },
  {
    "text": "as job LEDs to create a topic to create appropriate metrics to launch a",
    "start": "2778340",
    "end": "2783890"
  },
  {
    "text": "streaming job on a container system and these job 'lets are chainable and these",
    "start": "2783890",
    "end": "2789020"
  },
  {
    "text": "job lights are usable because each one does one thing accurate and once that graph is kind of figured",
    "start": "2789020",
    "end": "2796490"
  },
  {
    "text": "out it gets scheduled on the workers and it gets through Amazon s qsq so it's fault tolerant and so currently that's",
    "start": "2796490",
    "end": "2805190"
  },
  {
    "text": "how it works we are moving to you a little more declarative approach there what we want to do is we want to take a",
    "start": "2805190",
    "end": "2811400"
  },
  {
    "text": "look at the current state of the system and we want to take a look at what our goal state is where and where we want",
    "start": "2811400",
    "end": "2816710"
  },
  {
    "text": "the system to move to so for example you could have a Kafka topic that's ten partitions and you know doing ten",
    "start": "2816710",
    "end": "2822829"
  },
  {
    "text": "megabytes a second and user says I actually want my stream to be 30 megabytes a second and for that stream",
    "start": "2822829",
    "end": "2830109"
  },
  {
    "text": "the user does not know how many partitions they just say this is my bandwidth so from that we figure out",
    "start": "2830109",
    "end": "2836270"
  },
  {
    "text": "okay this actually needs another 20 partitions and then we reconcile that",
    "start": "2836270",
    "end": "2841369"
  },
  {
    "text": "difference saying the current state is 10 we actually need 20 how do we actually drive the system towards that",
    "start": "2841369",
    "end": "2847579"
  },
  {
    "text": "end state and so we look at the cache of the goal state and the current state we",
    "start": "2847579",
    "end": "2854270"
  },
  {
    "text": "compare it and the reconciler kind of runs its magic on it and then figures out what needs to be done and then it",
    "start": "2854270",
    "end": "2860420"
  },
  {
    "text": "schedules the job on the job runner and the job runner does the same thing like we saw in the previous picture where it",
    "start": "2860420",
    "end": "2866329"
  },
  {
    "text": "sends these jobs to a run or framework and executes suits on our behalf so the",
    "start": "2866329",
    "end": "2871940"
  },
  {
    "text": "benefit of this is even if there are failures across the system will discover the current state and that it's not at",
    "start": "2871940",
    "end": "2878630"
  },
  {
    "text": "the state that we want it to be and we can know this reconcile easily because in a distributed system you always have",
    "start": "2878630",
    "end": "2884540"
  },
  {
    "text": "failures when you're trying to access multiple systems and this is actually managing across several distance",
    "start": "2884540",
    "end": "2890030"
  },
  {
    "text": "different systems but in Netflix you know Kafka the stream processing engine and a metric system the continuous",
    "start": "2890030",
    "end": "2897349"
  },
  {
    "text": "delivery system the container one time system so when it's working across all this you have to make sure you have a",
    "start": "2897349",
    "end": "2902869"
  },
  {
    "text": "kind of a good reconciliation architecture to deal with this so this is how the new kind of architecture",
    "start": "2902869",
    "end": "2908990"
  },
  {
    "text": "looks like every major component one that's managing Kafka one that's managing stream processing gets its own",
    "start": "2908990",
    "end": "2915829"
  },
  {
    "text": "reconciliation framework and on top of that we have another reconciliation",
    "start": "2915829",
    "end": "2922040"
  },
  {
    "text": "framework that kind of ties all this together sits up it's like more like a composite pattern where we're taking the blocks and using the",
    "start": "2922040",
    "end": "2927800"
  },
  {
    "text": "same thing at different levels but making it really more declarative so this way when we start building new",
    "start": "2927800",
    "end": "2935990"
  },
  {
    "text": "flows if you don't have to hand code everything we just specify the current in goal State and then we use we convert",
    "start": "2935990",
    "end": "2943610"
  },
  {
    "text": "this into like assert space problem and use a-star algorithm to kind of search through the space and figure out what we",
    "start": "2943610",
    "end": "2948680"
  },
  {
    "text": "need to run and how we need to run so it gives us this dynamic composability of stuff that we want to do management wise",
    "start": "2948680",
    "end": "2954260"
  },
  {
    "text": "and it allows a lot more flexibility in what we want to do and less time in",
    "start": "2954260",
    "end": "2959450"
  },
  {
    "text": "coding every single pathway through the system so the unique features are this",
    "start": "2959450",
    "end": "2968510"
  },
  {
    "text": "system allows us to add locks and semaphores and critical resources that we have so for example we can only",
    "start": "2968510",
    "end": "2974180"
  },
  {
    "text": "launch containers that are certain rate because of certain limitations and this allows us to set those limits and so",
    "start": "2974180",
    "end": "2981970"
  },
  {
    "text": "these are some of the reasons why you're not able to use a deeply step functions",
    "start": "2981970",
    "end": "2987860"
  },
  {
    "text": "in SWF because we needed these functionality for our framework to provide us the functionality that we",
    "start": "2987860",
    "end": "2994640"
  },
  {
    "text": "need so towards the last section how do we actually operate you know such a",
    "start": "2994640",
    "end": "2999800"
  },
  {
    "text": "large system so as icing the scale operations using systems and not humans",
    "start": "2999800",
    "end": "3006010"
  },
  {
    "text": "that's our motto so we try to do as much automation which we can so everything that I showed you today we actually",
    "start": "3006010",
    "end": "3013120"
  },
  {
    "text": "build it and we run it so we don't have a separate ops team you don't have a separate queue team there's no separate",
    "start": "3013120",
    "end": "3019030"
  },
  {
    "text": "of team it's just all one team there about ten developers and one UI engineer in total that's shared and so we are the",
    "start": "3019030",
    "end": "3027250"
  },
  {
    "text": "folks who actually build this functionality and we rely heavily on like collecting metrics doing the",
    "start": "3027250",
    "end": "3033310"
  },
  {
    "text": "appropriate monitoring automated paging alerting and remediation to at times where we can terminate certain Kafka",
    "start": "3033310",
    "end": "3040660"
  },
  {
    "text": "brokers or restart certain jobs automatically based on alerts that we have and we heavily rely on other",
    "start": "3040660",
    "end": "3048790"
  },
  {
    "text": "Netflix system as well we have a separate metric system call Atlas that managers and dashboards all our metrics",
    "start": "3048790",
    "end": "3056640"
  },
  {
    "text": "we have a separate dynamic configuration system and that's if we leverage heavily and we",
    "start": "3056640",
    "end": "3066339"
  },
  {
    "text": "also have built-in automation systems that allow us to write Python script to",
    "start": "3066339",
    "end": "3071799"
  },
  {
    "text": "automate some aspects of it and that's closely tied in to our alert generation system there's also a lot of automation",
    "start": "3071799",
    "end": "3078849"
  },
  {
    "text": "within the Keystone management functionality that I showed you as well",
    "start": "3078849",
    "end": "3084990"
  },
  {
    "text": "we have a very easy way to look at the alerts that are fired or configured and",
    "start": "3084990",
    "end": "3090430"
  },
  {
    "text": "here's another view of the alerts that are currently fired so we can quickly take a look at what's happening to our systems and operating our gateway is",
    "start": "3090430",
    "end": "3099369"
  },
  {
    "text": "pretty simple it's a stateless service we use an elastic load balancing in auto scaling to scale up and scale down that",
    "start": "3099369",
    "end": "3105670"
  },
  {
    "text": "service the event producer related metrics and monitoring so here is a",
    "start": "3105670",
    "end": "3111790"
  },
  {
    "text": "dashboard that shows what we track across the system the common events are produced what's the drop rate and are",
    "start": "3111790",
    "end": "3120970"
  },
  {
    "text": "there any applications that are falling below the SLA that you specified are there any topics that are falling below the SLA and a high level so when it",
    "start": "3120970",
    "end": "3129160"
  },
  {
    "text": "comes to kafka clusters we have developed tooling to do automatic kafka",
    "start": "3129160",
    "end": "3134200"
  },
  {
    "text": "failover so it's a one-button failover and reset",
    "start": "3134200",
    "end": "3139270"
  },
  {
    "text": "when we are done with the failover and this is how it works let's say we have a happy path kafka cluster of your",
    "start": "3139270",
    "end": "3145960"
  },
  {
    "text": "routing events and something bad happens to that cluster we leave the flink router running in case we can still",
    "start": "3145960",
    "end": "3151839"
  },
  {
    "text": "extract events and route it back to the event we actually create a brand-new kafka cluster of a smaller capacity same",
    "start": "3151839",
    "end": "3160329"
  },
  {
    "text": "number of nodes but each node is a smaller node for cost reasons we copy all the metadata we recreate all the",
    "start": "3160329",
    "end": "3165700"
  },
  {
    "text": "topics and then we change the producer through dynamic properties to automatically start routing to the new",
    "start": "3165700",
    "end": "3171309"
  },
  {
    "text": "kafka cluster we launch another router job so it starts sprouting so this gives",
    "start": "3171309",
    "end": "3177970"
  },
  {
    "text": "us time to recover and fix the old cluster without having to drop any data for our customers and once the cluster",
    "start": "3177970",
    "end": "3185680"
  },
  {
    "text": "is fixed we point the producer back we finish draining all the events from the failed back cluster and then",
    "start": "3185680",
    "end": "3192990"
  },
  {
    "text": "we we decommission the they will failed",
    "start": "3192990",
    "end": "3199500"
  },
  {
    "text": "over cluster and the routing job and then with the click of a button we are",
    "start": "3199500",
    "end": "3207390"
  },
  {
    "text": "back to a steady-state again so for the Kafka consumer clusters currently you",
    "start": "3207390",
    "end": "3212819"
  },
  {
    "text": "don't have this kind of automated failover because we need the consumers to move over to a multi consumer library",
    "start": "3212819",
    "end": "3221460"
  },
  {
    "text": "approach and we use this automation also",
    "start": "3221460",
    "end": "3227790"
  },
  {
    "text": "as our Kafka Kong we do this routinely we actually intentionally fail over on a",
    "start": "3227790",
    "end": "3233160"
  },
  {
    "text": "lot a real-life traffic to make sure actually this stuff works when you really need it so we do it very often and it's bland so we monitor a broker",
    "start": "3233160",
    "end": "3242490"
  },
  {
    "text": "health in terms of networking broker",
    "start": "3242490",
    "end": "3248400"
  },
  {
    "text": "fetch latency use replication latencies and when we identify an outlier we have some automation that'll automatically",
    "start": "3248400",
    "end": "3253500"
  },
  {
    "text": "terminate a broker and we only terminate one in a certain period of time so we don't have the risk of losing data a fee",
    "start": "3253500",
    "end": "3259589"
  },
  {
    "text": "accidentally terminated both the copies we scale up by adding partitions and we",
    "start": "3259589",
    "end": "3266819"
  },
  {
    "text": "add partitions by spreading them evenly across brokers and that's how we do our partition assignment we scale down or up",
    "start": "3266819",
    "end": "3274440"
  },
  {
    "text": "by using the same failover functionality that I showed you we just start up a brand new cluster we just drop the events to it and we just tear down the",
    "start": "3274440",
    "end": "3280920"
  },
  {
    "text": "old one and that's how we do our updates as well so we have zero downtime upgrades and scallops and care scale",
    "start": "3280920",
    "end": "3287099"
  },
  {
    "text": "downs and you leverage the same functionality the failover functionality for so for the stream processing jobs as",
    "start": "3287099",
    "end": "3295650"
  },
  {
    "text": "we talked about you have checkpoints save points container replacements we have built-in retries into the framework",
    "start": "3295650",
    "end": "3301650"
  },
  {
    "text": "and also isolation between the different streams we have router deployment",
    "start": "3301650",
    "end": "3307770"
  },
  {
    "text": "options where we can the automatic partial recovery so there's a feature in fling called final in recovery so it",
    "start": "3307770",
    "end": "3315270"
  },
  {
    "text": "analyzes your job graph and if you have parallel streams that are not dependent on each other think about it as a as a",
    "start": "3315270",
    "end": "3322349"
  },
  {
    "text": "digraph and if you don't have any dependencies and if there's a path to it continue running that path and the ones",
    "start": "3322349",
    "end": "3328010"
  },
  {
    "text": "that it failed it will recover them so that way you're not lagging behind in not processing anything at all you're",
    "start": "3328010",
    "end": "3334460"
  },
  {
    "text": "actually processing that stuff so this is something we found based on our testing it is something we helped add",
    "start": "3334460",
    "end": "3340160"
  },
  {
    "text": "into the into the flink runtime because it's really useful for a routing jobs and of course not everything can be",
    "start": "3340160",
    "end": "3347300"
  },
  {
    "text": "automated there are always some cases where we need manual intervention and we have Gudrun books for them and this is",
    "start": "3347300",
    "end": "3353780"
  },
  {
    "text": "one scenario where the UI pops have been saying it needs manual intervention and the user can or somebody's on call can",
    "start": "3353780",
    "end": "3361130"
  },
  {
    "text": "follow up on it so we do a lot of capacity planning and we do this ahead",
    "start": "3361130",
    "end": "3367910"
  },
  {
    "text": "of time looking at the traffic that's flowing through the system and sometimes",
    "start": "3367910",
    "end": "3373130"
  },
  {
    "text": "we manually increase it as well in anticipation of increased traffic like for example right now we're getting into",
    "start": "3373130",
    "end": "3378140"
  },
  {
    "text": "the holiday season so this is a place where we can actually do an override and",
    "start": "3378140",
    "end": "3384880"
  },
  {
    "text": "increase the capacity in addition to what's automated automatically",
    "start": "3384880",
    "end": "3390980"
  },
  {
    "text": "determined based on input traffic because we could say this traffic is going to increase 2x but that traffic is",
    "start": "3390980",
    "end": "3396410"
  },
  {
    "text": "not actually flowing in so there's no way to determine that and so this is where we can override it and we can do",
    "start": "3396410",
    "end": "3401660"
  },
  {
    "text": "bulk updates or upgrades of routers across our whole ecosystem so that deployment framework makes it very easy",
    "start": "3401660",
    "end": "3407410"
  },
  {
    "text": "so this is the again the message flow we look at application metrics heavily for",
    "start": "3407410",
    "end": "3413090"
  },
  {
    "text": "operations and this is the filtering metrics which tells us what's getting filtered and if there are any drop",
    "start": "3413090",
    "end": "3418580"
  },
  {
    "text": "messages because of filtering and errors and these are the cuff offset metrics",
    "start": "3418580",
    "end": "3423890"
  },
  {
    "text": "like how many records are coming in what cuff cops is for partition we are doing so this allows us to look at clustering",
    "start": "3423890",
    "end": "3431510"
  },
  {
    "text": "if something's you weigh off from the clustering we know it's an outlier and makes it easy for us to identify them",
    "start": "3431510",
    "end": "3437710"
  },
  {
    "text": "and then we have the same mechanics for jvm metrics as well where we try to see",
    "start": "3437710",
    "end": "3444350"
  },
  {
    "text": "the clustering and see if there any outliers and so this direct buffer and the usual JVM metrics that we measure",
    "start": "3444350",
    "end": "3452050"
  },
  {
    "text": "there are lot more metrics that we measure but it's hard to kind of list all of them here",
    "start": "3452050",
    "end": "3458540"
  },
  {
    "text": "so this is a view of the alerting you eye again so the streaming jobs that the",
    "start": "3458540",
    "end": "3464630"
  },
  {
    "text": "user develop this is an interesting Minka's mix because there are gray areas where it may be an application issue it",
    "start": "3464630",
    "end": "3470300"
  },
  {
    "text": "may be a platform issue so that's something we are kind of working on with our customers to see how much of it they",
    "start": "3470300",
    "end": "3476060"
  },
  {
    "text": "can address the pages versus us but it's kind of a split right now and we have",
    "start": "3476060",
    "end": "3482620"
  },
  {
    "text": "automated paging systems at pages to do it and our platform customers also",
    "start": "3482620",
    "end": "3488780"
  },
  {
    "text": "follow the same model if they build a job they're responsible for running and operating it unless it's a problem with",
    "start": "3488780",
    "end": "3494960"
  },
  {
    "text": "the infrastructure that we've built and then they can reach out to us and escalate it to us so these are some of",
    "start": "3494960",
    "end": "3501800"
  },
  {
    "text": "the metrics like how many and this is an example application that we wrote to",
    "start": "3501800",
    "end": "3507710"
  },
  {
    "text": "quickly get these data points but it talks about how many enrichments were happening how many counts they did so",
    "start": "3507710",
    "end": "3515270"
  },
  {
    "text": "this is more of an application level metrics that they added there's some",
    "start": "3515270",
    "end": "3520310"
  },
  {
    "text": "system level metrics so the road ahead for us is to get to a more true auto",
    "start": "3520310",
    "end": "3526430"
  },
  {
    "text": "scaling we don't have true auto scaling right now and to have better provisioning for streaming jobs because",
    "start": "3526430",
    "end": "3532160"
  },
  {
    "text": "it's difficult to know what code user certain and what the performance profile is so we want to build some tooling to",
    "start": "3532160",
    "end": "3537680"
  },
  {
    "text": "gauge the performance profile and then work based on that so if you want to",
    "start": "3537680",
    "end": "3543740"
  },
  {
    "text": "learn more there are a bunch of related presentations that we've had in the past",
    "start": "3543740",
    "end": "3549260"
  },
  {
    "text": "and then you can hit them up and take a look at it thank you for",
    "start": "3549260",
    "end": "3556210"
  },
  {
    "text": "[Applause]",
    "start": "3556240",
    "end": "3563520"
  }
]