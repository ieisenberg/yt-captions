[
  {
    "text": "So there's a lot of public interest in this\nrecently and it feels like hype.",
    "start": "170",
    "end": "8170"
  },
  {
    "text": "Is this the same, or is this something where\nwe can see that this is a real foundation",
    "start": "8170",
    "end": "14480"
  },
  {
    "text": "for future application development?",
    "start": "14480",
    "end": "16570"
  },
  {
    "text": "We are living in very exciting times with\nmachine learning.",
    "start": "16570",
    "end": "20350"
  },
  {
    "text": "The speed of ML model development will really\nactually increase.",
    "start": "20350",
    "end": "26150"
  },
  {
    "text": "But you won't get to that end state that we\nwant in the next coming years",
    "start": "26150",
    "end": "31878"
  },
  {
    "text": "unless we actually make these models more\n accessible to everybody.",
    "start": "31878",
    "end": "37000"
  },
  {
    "text": "Swami Sivasubramanian oversees database, analytics\nand machine learning at AWS.",
    "start": "51550",
    "end": "58210"
  },
  {
    "text": "For the past 15 years, he has helped lead\nthe way on AI and ML in the industry.",
    "start": "58210",
    "end": "63370"
  },
  {
    "text": "Swami’s teams have a strong track record\nof taking new technologies and turning these",
    "start": "63370",
    "end": "68860"
  },
  {
    "text": "into viable tools.",
    "start": "68860",
    "end": "71010"
  },
  {
    "text": "Today, Generative AI is dominating news feeds\nand conversations.",
    "start": "71010",
    "end": "75430"
  },
  {
    "text": "Consumers are interacting with it and brands\nare trying to understand how to best harness",
    "start": "75430",
    "end": "80570"
  },
  {
    "text": "its potential for their customers.",
    "start": "80570",
    "end": "83130"
  },
  {
    "text": "So, I sat down with Swami to better understand\nthe broad landscape of this technology.",
    "start": "83130",
    "end": "89960"
  },
  {
    "text": "Swami, we go back a long time.",
    "start": "89960",
    "end": "96216"
  },
  {
    "text": "Tell me a bit.\nDo you remember your first day at Amazon?",
    "start": "96217",
    "end": "99850"
  },
  {
    "text": "I still remember because it's not very common\nfor PhD students to join Amazon at that time",
    "start": "99850",
    "end": "108450"
  },
  {
    "text": "because you were known as retailer or ecommerce.",
    "start": "108450",
    "end": "113670"
  },
  {
    "text": "We were building things.",
    "start": "113670",
    "end": "115460"
  },
  {
    "text": "And so that's also quite a departure from\na foreign academic.",
    "start": "115460",
    "end": "119118"
  },
  {
    "text": "Definitely, for a PhD student to go from\nthinking to, actually, how do I build this?",
    "start": "119119",
    "end": "124479"
  },
  {
    "text": "So you brought actually DynamoDB to the world\nand quite a few other databases since then,",
    "start": "124479",
    "end": "131950"
  },
  {
    "text": "but under your purview now is also AI and\nmachine learning.",
    "start": "131950",
    "end": "138370"
  },
  {
    "text": "So tell me a bit about how does your world\nof AI look like?",
    "start": "138370",
    "end": "141470"
  },
  {
    "text": "After building a bunch of these databases\nand analytics services,",
    "start": "141470",
    "end": "147040"
  },
  {
    "text": "I got fascinated by AI because that literally\nAI and machine learning because that puts",
    "start": "147040",
    "end": "152180"
  },
  {
    "text": "data to work.",
    "start": "152180",
    "end": "153519"
  },
  {
    "text": "And if you look at machine learning technology\nitself broadly, it's not necessarily new.",
    "start": "153519",
    "end": "158650"
  },
  {
    "text": "In fact, some of the first papers of deep\nlearning was written even like 30 years ago.",
    "start": "158650",
    "end": "163170"
  },
  {
    "text": "But even in those papers, they explicitly\ncalled out for it to get large scale adoption.",
    "start": "163170",
    "end": "169390"
  },
  {
    "text": "It required massive amount of compute and\nmassive amount of data to actually succeed.",
    "start": "169390",
    "end": "174550"
  },
  {
    "text": "And that's what cloud got us to actually unlock\nthe power of deep learning technologies.",
    "start": "174550",
    "end": "181560"
  },
  {
    "text": "So which led me to early on this is like six,\nseven years ago to start the machine Learning",
    "start": "181560",
    "end": "187150"
  },
  {
    "text": "organization because we wanted to take machine\nlearning, especially deep learning style technologies,",
    "start": "187150",
    "end": "193180"
  },
  {
    "text": "from not just in the hands of scientists to\neveryday developers.",
    "start": "193180",
    "end": "197010"
  },
  {
    "text": "If you think about the early days of Amazon,\nthe retailer with similarities and recommendations",
    "start": "197010",
    "end": "203569"
  },
  {
    "text": "and things like that, were they the same algorithms\nthat we're seeing being used today or is that,",
    "start": "203569",
    "end": "210819"
  },
  {
    "text": "I mean that's a long time ago, 30 years.",
    "start": "210819",
    "end": "215019"
  },
  {
    "text": "Machine learning has really gone through huge\ngrowth in actually the complexity of the algorithms",
    "start": "215019",
    "end": "221520"
  },
  {
    "text": "and applicability of the use cases.",
    "start": "221520",
    "end": "224400"
  },
  {
    "text": "Early on the algorithms were a lot more simple,\na lot more like linear algorithms based or",
    "start": "224400",
    "end": "229610"
  },
  {
    "text": "gradient boosting.",
    "start": "229610",
    "end": "231540"
  },
  {
    "text": "If you see last decade, it was all around\nlike deep learning early part of last decade,",
    "start": "231540",
    "end": "237189"
  },
  {
    "text": "which was essentially a step up in the ability\nfor neural nets to actually understand and",
    "start": "237190",
    "end": "243120"
  },
  {
    "text": "learn from the patterns, which is effectively\nwhat all the image based image processing",
    "start": "243120",
    "end": "248480"
  },
  {
    "text": "algorithms come from.",
    "start": "248480",
    "end": "250340"
  },
  {
    "text": "And then also personalization with different\ntypes of neural nets and so forth.",
    "start": "250340",
    "end": "255129"
  },
  {
    "text": "And that's what led to the invention like\nAlexa,",
    "start": "255130",
    "end": "258000"
  },
  {
    "text": "which has a remarkable accuracy compared to others.",
    "start": "258000",
    "end": "261100"
  },
  {
    "text": "So the neural nets and deep learning has really\nbeen a step up.",
    "start": "261100",
    "end": "265040"
  },
  {
    "text": "And the next big step up is what is happening\ntoday in machine learning.",
    "start": "265040",
    "end": "270090"
  },
  {
    "text": "So a lot of the talk these days is around\ngenerative AI,",
    "start": "270090",
    "end": "274423"
  },
  {
    "text": "large language models, foundation models.",
    "start": "274423",
    "end": "277422"
  },
  {
    "text": "Tell me a bit why is that different from,\nlet's say the more task based like vision",
    "start": "277422",
    "end": "283229"
  },
  {
    "text": "algorithms and things like that?",
    "start": "283229",
    "end": "285479"
  },
  {
    "text": "I mean, if you take a step back and look at what's -",
    "start": "285479",
    "end": "289357"
  },
  {
    "text": "How this foundation models -\nlarge language models -  is all about",
    "start": "289357",
    "end": "294134"
  },
  {
    "text": "These are big models which are trained with",
    "start": "294134",
    "end": "297631"
  },
  {
    "text": "hundreds of millions of parameters if not billion",
    "start": "297631",
    "end": "300830"
  },
  {
    "text": "A parameter, just to give context, is like an \ninternal variable",
    "start": "300830",
    "end": "305587"
  },
  {
    "text": "where the ML algorithm has learned from its data set.",
    "start": "305588",
    "end": "308947"
  },
  {
    "text": "Now, to give a sense, what is this\nbig thing suddenly that has happened?",
    "start": "308947",
    "end": "314699"
  },
  {
    "text": "Few things -",
    "start": "314699",
    "end": "315980"
  },
  {
    "text": "One, if you take a look at Transformers, has\nbeen a big change.",
    "start": "315980",
    "end": "322270"
  },
  {
    "text": "Transformer is a kind of neural net technology\nthat is remarkably scalable than the previous",
    "start": "322270",
    "end": "330349"
  },
  {
    "text": "versions like RNNS or various others.",
    "start": "330350",
    "end": "333250"
  },
  {
    "text": "So what does this mean?",
    "start": "333250",
    "end": "334620"
  },
  {
    "text": "Why did this suddenly lead to this transformation?",
    "start": "334620",
    "end": "338690"
  },
  {
    "text": "Because it is actually scalable and you can\ntrain them a lot faster now you can throw",
    "start": "338690",
    "end": "342660"
  },
  {
    "text": "a lot of hardware and lot of data.",
    "start": "342660",
    "end": "345199"
  },
  {
    "text": "Now that means now I can actually crawl the\nentire World Wide Web and actually feed it",
    "start": "345199",
    "end": "353460"
  },
  {
    "text": "into these kind of algorithms and start actually\nbuilding models that can actually understand",
    "start": "353460",
    "end": "360740"
  },
  {
    "text": "human knowledge.",
    "start": "360740",
    "end": "362030"
  },
  {
    "text": "At a high level, a generative AI text model\nis good at using natural language processing",
    "start": "362030",
    "end": "369530"
  },
  {
    "text": "to analyze text and predict the next word\nthat comes in a sequence of words.",
    "start": "369530",
    "end": "374590"
  },
  {
    "text": "By paying attention to certain words or phrases\nin the input, these models can infer context.",
    "start": "374590",
    "end": "381220"
  },
  {
    "text": "And they can use that context to find the\nwords that have the highest probability of",
    "start": "381220",
    "end": "386539"
  },
  {
    "text": "following the words that came before it.",
    "start": "386539",
    "end": "389150"
  },
  {
    "text": "Structuring inputs as instructions with relevant\ncontext can prompt a model to generate answers",
    "start": "389150",
    "end": "395406"
  },
  {
    "text": "for language understanding, knowledge,\nand composition",
    "start": "395407",
    "end": "399370"
  },
  {
    "text": "Foundation Models are also capable of what\nis called “in-context learning,” which",
    "start": "399370",
    "end": "404720"
  },
  {
    "text": "is what happens when you include a handful\nof demonstration examples",
    "start": "404720",
    "end": "408740"
  },
  {
    "text": "as part of a prompt to improve\n the model’s output on the fly.",
    "start": "408740",
    "end": "413147"
  },
  {
    "text": "We supply examples to further explain \n the instruction",
    "start": "413148",
    "end": "416687"
  },
  {
    "text": "And this helps the model adjust the output based \non the pattern and style in the examples.",
    "start": "416687",
    "end": "423569"
  },
  {
    "text": "When the models use billions of parameters\nand their training corpus is the entire internet,",
    "start": "423569",
    "end": "430320"
  },
  {
    "text": "the results can be remarkable.",
    "start": "430320",
    "end": "432479"
  },
  {
    "text": "The training is unsupervised and task agnostic.",
    "start": "432479",
    "end": "436349"
  },
  {
    "text": "And the mountains of web data used for training\nlet it respond to natural language instructions",
    "start": "436349",
    "end": "441879"
  },
  {
    "text": "for many different tasks.",
    "start": "441879",
    "end": "444660"
  },
  {
    "text": "So the task based models that we had before\nand that we were already really good at, could",
    "start": "444660",
    "end": "449440"
  },
  {
    "text": "you build them based on these foundation\nmodels?",
    "start": "449440",
    "end": "453180"
  },
  {
    "text": "You no longer need these task specific models\nor do we still need them?",
    "start": "453180",
    "end": "458410"
  },
  {
    "text": "The way to think about it is the need for\ntask based specific models are not going away.",
    "start": "458410",
    "end": "464180"
  },
  {
    "text": "But what essentially is how we go about building\nthem.",
    "start": "464180",
    "end": "467729"
  },
  {
    "text": "You still need a model to translate from one\nlanguage to another",
    "start": "467729",
    "end": "472499"
  },
  {
    "text": "or to generate code and so forth.",
    "start": "472499",
    "end": "475159"
  },
  {
    "text": "But how easy now you can build them is essentially\na big change because with foundation models,",
    "start": "475159",
    "end": "481569"
  },
  {
    "text": "which are the entire corpus of knowledge of,\nlet's say huge amount of data, now it is simply",
    "start": "481569",
    "end": "488490"
  },
  {
    "text": "a matter of actually building on top of this\nwith fine tuning, with specific examples.",
    "start": "488490",
    "end": "493780"
  },
  {
    "text": "Think about if you're running like a recruiting\nfirm as an example and you want to ingest",
    "start": "493780",
    "end": "501099"
  },
  {
    "text": "all your resumes and store it in a format that\nis standard for you to search and index on,",
    "start": "501099",
    "end": "506602"
  },
  {
    "text": "instead of building a custom NLP model\nto do all that.",
    "start": "506602",
    "end": "510400"
  },
  {
    "text": "Now using foundation models and give a few\nexamples of here is an input resume in this",
    "start": "510400",
    "end": "515140"
  },
  {
    "text": "format and here is the output resume.",
    "start": "515140",
    "end": "517550"
  },
  {
    "text": "Now you can even fine tune these models\nby just giving few specific examples and then",
    "start": "517550",
    "end": "525300"
  },
  {
    "text": "you essentially are good to go.",
    "start": "525300",
    "end": "527560"
  },
  {
    "text": "So in the past, most of the work went into\nprobably labeling the data and that was also",
    "start": "527560",
    "end": "534640"
  },
  {
    "text": "the hardest part because that drives the accuracy.",
    "start": "534640",
    "end": "537529"
  },
  {
    "text": "Exactly.",
    "start": "537529",
    "end": "538529"
  },
  {
    "text": "So in this particular case, with these foundation\nmodels, no longer labeling is needed?",
    "start": "538529",
    "end": "545010"
  },
  {
    "text": "Essentially, I mean, yes and no.",
    "start": "545010",
    "end": "547860"
  },
  {
    "text": "As always with these things, there is a nuance.",
    "start": "547860",
    "end": "550529"
  },
  {
    "text": "But majority of what makes these large scale\nmodels remarkable is they actually can be",
    "start": "550530",
    "end": "556860"
  },
  {
    "text": "trained on a lot of unlabeled data.",
    "start": "556860",
    "end": "560140"
  },
  {
    "text": "You actually go through what I call as a pretraining\nphase, which is essentially you collect data",
    "start": "560140",
    "end": "565670"
  },
  {
    "text": "sets from, let's say, the World Wide Web,\nlike common crawl data, or code data and various",
    "start": "565670",
    "end": "570380"
  },
  {
    "text": "other data sites, Wikipedia, whatnot.",
    "start": "570380",
    "end": "572690"
  },
  {
    "text": "And then you don't even label them, you kind\nof feed them as it is.",
    "start": "572690",
    "end": "577149"
  },
  {
    "text": "But you have to of course go through Sanitization\nstep in terms of making sure you cleanse data",
    "start": "577149",
    "end": "582930"
  },
  {
    "text": "from PII or actually all other stuff like\nnegative things or HP and whatnot.",
    "start": "582930",
    "end": "590351"
  },
  {
    "text": "But then you actually start training on large\nnumber of hardware clusters because these",
    "start": "590351",
    "end": "597950"
  },
  {
    "text": "models to train them can take tens of millions\nof dollars to actually go through that training.",
    "start": "597950",
    "end": "603950"
  },
  {
    "text": "And then you actually finally you get a notion\nof a model and then you go through the next",
    "start": "603950",
    "end": "609959"
  },
  {
    "text": "step of what is called inference.",
    "start": "609959",
    "end": "612360"
  },
  {
    "text": "When it comes to building these LLMs, the\neasy part is the training.",
    "start": "612360",
    "end": "619451"
  },
  {
    "text": "The hardest part is the data.",
    "start": "619452",
    "end": "622710"
  },
  {
    "text": "Training models with poor data quality will\nlead to poor results.",
    "start": "622710",
    "end": "626490"
  },
  {
    "text": "You’ll need to filter out bias, hate speech,\nand toxicity.",
    "start": "626490",
    "end": "630470"
  },
  {
    "text": "You’ll need to make sure that the data is\nfree of PII or sensitive data.",
    "start": "630470",
    "end": "636829"
  },
  {
    "text": "You’ll need to make sure your data is deduplicated,\nbalanced, and doesn’t lead to oversampling.",
    "start": "636829",
    "end": "643709"
  },
  {
    "text": "Because the whole process can be so expensive\nand requires access large amounts of compute",
    "start": "643710",
    "end": "648870"
  },
  {
    "text": "and storage, many companies feel lost on where\nto even start.",
    "start": "648870",
    "end": "654250"
  },
  {
    "text": "Let's speak object detection in video that\nwould be as a smaller model than what we see",
    "start": "654250",
    "end": "663010"
  },
  {
    "text": "now with the foundation models.",
    "start": "663010",
    "end": "666190"
  },
  {
    "text": "What's the cost of running a model like that?",
    "start": "666190",
    "end": "669380"
  },
  {
    "text": "Because now these models with these hundreds\nof billions of parameters are probably very",
    "start": "669380",
    "end": "675889"
  },
  {
    "text": "large pieces of data.",
    "start": "675889",
    "end": "678889"
  },
  {
    "text": "That's a great question because there is so\nmuch talk only happening around training these",
    "start": "678889",
    "end": "683420"
  },
  {
    "text": "models, but very little talk on the cost of\nrunning these models to make predictions,",
    "start": "683420",
    "end": "689529"
  },
  {
    "text": "which is inference, which is a signal that\nvery few people are actually deploying it",
    "start": "689530",
    "end": "694530"
  },
  {
    "text": "and runtime for actual production.",
    "start": "694530",
    "end": "696750"
  },
  {
    "text": "Or once they actually deploy in production,\nthey will realize oh no, these models are",
    "start": "696750",
    "end": "701110"
  },
  {
    "text": "very expensive to run and that is where few\nimportant techniques actually really come",
    "start": "701110",
    "end": "707640"
  },
  {
    "text": "into play.",
    "start": "707640",
    "end": "708640"
  },
  {
    "text": "So one, once you build these large models\nto run them in production, you need to do",
    "start": "708640",
    "end": "714200"
  },
  {
    "text": "a few things to make them affordable to run\nat cost, run at scale and run actually very",
    "start": "714200",
    "end": "721140"
  },
  {
    "text": "in an economical fashion.",
    "start": "721140",
    "end": "723115"
  },
  {
    "text": "One is what we call as quantization.",
    "start": "723115",
    "end": "725980"
  },
  {
    "text": "The other one is what I call as distillation,\nwhich is that you have these large teacher",
    "start": "725980",
    "end": "731449"
  },
  {
    "text": "models and even though they are trained hundreds\nof billions of models, they kind of are distilled",
    "start": "731450",
    "end": "737020"
  },
  {
    "text": "to a smaller fine grained model and speaking\nin a super abstract term, but that is the",
    "start": "737020",
    "end": "742710"
  },
  {
    "text": "essence of these models.",
    "start": "742710",
    "end": "746699"
  },
  {
    "text": "Of course, there’s a lot that goes into\ntraining the model, but what about inference?",
    "start": "746699",
    "end": "752319"
  },
  {
    "text": "It turns out that the sheer size of these\nmodels can make inference expensive to run.",
    "start": "752320",
    "end": "758370"
  },
  {
    "text": "To reduce model size, we can do “quantization,”\nwhich is approximating a neural network by",
    "start": "758370",
    "end": "764589"
  },
  {
    "text": "using smaller, 8-bit integers instead of 32-\nor 16-bit floating point numbers.",
    "start": "764589",
    "end": "770769"
  },
  {
    "text": "We can also use “distillation”, which\nis effectively a transferring of knowledge",
    "start": "770769",
    "end": "775829"
  },
  {
    "text": "from a larger “teacher” model to a smaller\nand faster “student” model.",
    "start": "775830",
    "end": "781199"
  },
  {
    "text": "These techniques have reduced the model size\nsignificantly for us, while providing similar",
    "start": "781199",
    "end": "786170"
  },
  {
    "text": "accuracy and improved latency.",
    "start": "786170",
    "end": "789190"
  },
  {
    "text": "So we do have this custom hardware to help\nout with this that happens at I mean, normally",
    "start": "789190",
    "end": "797130"
  },
  {
    "text": "this is all GPU based, which are expensive\nenergy hungry beasts.",
    "start": "797130",
    "end": "803480"
  },
  {
    "text": "Tell us what we can do with custom silicon\nthat  makes it so much cheaper",
    "start": "803480",
    "end": "809649"
  },
  {
    "text": "both in terms of cost as well as in,\nlet's say, your carbon footprint of the energy.",
    "start": "809649",
    "end": "817190"
  },
  {
    "text": "When it comes to custom silicon, as mentioned,\nthe cost is becoming a big issue in these",
    "start": "817190",
    "end": "823209"
  },
  {
    "text": "foundation models because they are very\nexpensive to train",
    "start": "823209",
    "end": "826834"
  },
  {
    "text": "and very expensive also to run at scale.",
    "start": "826834",
    "end": "829674"
  },
  {
    "text": "You can actually run like build a playground\nand test your chatbot and at low scale and",
    "start": "829674",
    "end": "834889"
  },
  {
    "text": "it may not be that big a deal, but once you\nstart deploying at scale",
    "start": "834889",
    "end": "839013"
  },
  {
    "text": "as part of your core business operation,\nthen these things add up.",
    "start": "839013",
    "end": "843800"
  },
  {
    "text": "So since in AWS we did invest in our custom\nsilicones for training with Trainium and with",
    "start": "843800",
    "end": "851920"
  },
  {
    "text": "Inferentia with inference.",
    "start": "851920",
    "end": "853540"
  },
  {
    "text": "And all these things are like ways for us\nto actually understand the essence of which",
    "start": "853540",
    "end": "859920"
  },
  {
    "text": "operators are making are involved in making\nthese prediction decisions and optimizing",
    "start": "859920",
    "end": "865380"
  },
  {
    "text": "them at the core silicon level and software\nstack level.",
    "start": "865380",
    "end": "868870"
  },
  {
    "text": "I mean, if cost is also a reflection of energy\nused because in essence, that's what you're",
    "start": "868870",
    "end": "875170"
  },
  {
    "text": "paying for, you can also see that they are,\nfrom a sustainability point of view, much",
    "start": "875170",
    "end": "880509"
  },
  {
    "text": "more important than running it on general\npurpose GPUs.",
    "start": "880509",
    "end": "884410"
  },
  {
    "text": "So there's a lot of public interest in this\nrecently and it feels like hype.",
    "start": "884410",
    "end": "892329"
  },
  {
    "text": "Is this the same or is this something where\nwe can see that this is a real foundation",
    "start": "892329",
    "end": "898649"
  },
  {
    "text": "for future application development?",
    "start": "898649",
    "end": "900529"
  },
  {
    "text": "First of all, we are living in very exciting\ntimes with machine learning.",
    "start": "900529",
    "end": "905079"
  },
  {
    "text": "I have probably said this now every year.",
    "start": "905079",
    "end": "907550"
  },
  {
    "text": " But this year is even more special because",
    "start": "907550",
    "end": "911220"
  },
  {
    "text": "these large language models and foundation\nmodels truly can actually enable so many use",
    "start": "911220",
    "end": "918420"
  },
  {
    "text": "cases where people don't have to staff as\nseparable teams to go build task specific",
    "start": "918420",
    "end": "924820"
  },
  {
    "text": "models.",
    "start": "924820",
    "end": "925820"
  },
  {
    "text": "The speed of ML model development will really\nactually increase.",
    "start": "925820",
    "end": "931709"
  },
  {
    "text": "But you won't get to that end state that we\nwant in the next coming years unless we actually",
    "start": "931709",
    "end": "938940"
  },
  {
    "text": "make these models more accessible to everybody.",
    "start": "938940",
    "end": "943080"
  },
  {
    "text": "And this is what we did with SageMaker early\non with machine learning and that's what we",
    "start": "943080",
    "end": "948220"
  },
  {
    "text": "need to do with Bedrock and all its applications\nas well.",
    "start": "948220",
    "end": "952769"
  },
  {
    "text": "But we do think while the Hype cycle will\nsubside like with any technology, but these",
    "start": "952770",
    "end": "958959"
  },
  {
    "text": "are going to become a core part of every application\nin the coming years.",
    "start": "958959",
    "end": "964860"
  },
  {
    "text": "And they will be done in a grounded way, but\nin a responsible fashion too, because there",
    "start": "964860",
    "end": "970800"
  },
  {
    "text": "is a lot more stuff that people need to think\nthrough in a generative AI context.",
    "start": "970800",
    "end": "976220"
  },
  {
    "text": "Because what kind of data did it learn from\nto actually what response does it generate?",
    "start": "976220",
    "end": "981509"
  },
  {
    "text": "How truthful it is as well?",
    "start": "981509",
    "end": "983120"
  },
  {
    "text": "These are stuff we are excited to actually\nhelp our customers.",
    "start": "983120",
    "end": "987490"
  },
  {
    "text": "So when you say that this is the most exciting\ntime in machine learning,",
    "start": "987490",
    "end": "993907"
  },
  {
    "text": "what are you going to say next year?",
    "start": "993907",
    "end": "996839"
  },
  {
    "text": "Well, Swami, thank you for talking to me.",
    "start": "996839",
    "end": "1000620"
  },
  {
    "text": "I mean, you educated me quite a bit on what\nthe current state of the field is.",
    "start": "1000620",
    "end": "1005269"
  },
  {
    "text": "So I'm very grateful for that.",
    "start": "1005269",
    "end": "1006990"
  },
  {
    "text": "My pleasure. Thanks again for having me, sir.",
    "start": "1006990",
    "end": "1011055"
  },
  {
    "text": "I'm excited to see how  builders use this technology",
    "start": "1011055",
    "end": "1014710"
  },
  {
    "text": "and continue to push the possibilities forward.",
    "start": "1014710",
    "end": "1017661"
  },
  {
    "text": "I want to say thanks to Swami.\nHis insights and understanding of the space",
    "start": "1017661",
    "end": "1022853"
  },
  {
    "text": "are a great way to begin this conversation.",
    "start": "1022853",
    "end": "1025649"
  },
  {
    "text": "I'm looking forward to diving even deeper\nand exploring the architectures",
    "start": "1025649",
    "end": "1029897"
  },
  {
    "text": "behind some of this.",
    "start": "1029898",
    "end": "1031475"
  },
  {
    "text": "And how large models can be used by engineers\nand developers",
    "start": "1031476",
    "end": "1034977"
  },
  {
    "text": "To create meaningful experiences.",
    "start": "1034978",
    "end": "1037658"
  }
]