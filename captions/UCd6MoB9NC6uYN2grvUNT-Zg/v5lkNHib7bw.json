[
  {
    "start": "0",
    "end": "30000"
  },
  {
    "text": "good afternoon and welcome to effective data lates on AWS this talk is a",
    "start": "60",
    "end": "6359"
  },
  {
    "text": "distillation of the lessons we learned in the field from helping various customers build their data leak on AWS",
    "start": "6359",
    "end": "13219"
  },
  {
    "text": "my name is Radhika Ravi Allah I am an EMR Solutions Architect at AWS and joining me on stage",
    "start": "13219",
    "end": "20369"
  },
  {
    "text": "today is a nanny motus what has a nanny I'm sorry who is also a Solutions",
    "start": "20369",
    "end": "25590"
  },
  {
    "text": "Architect with our strategic team so before we kick off this talk a quick",
    "start": "25590",
    "end": "32488"
  },
  {
    "start": "30000",
    "end": "47000"
  },
  {
    "text": "poll how many of you are or have currently built a data leak on AWS are",
    "start": "32489",
    "end": "38850"
  },
  {
    "text": "considering building a data leak on AWS fantastic this is what I expected to see",
    "start": "38850",
    "end": "44899"
  },
  {
    "text": "so let's get started so here is a quick preview of what we will be covering",
    "start": "44899",
    "end": "51390"
  },
  {
    "start": "47000",
    "end": "88000"
  },
  {
    "text": "today we'll look some motivation behind why organizations are looking at data leaks some concepts and principles",
    "start": "51390",
    "end": "58500"
  },
  {
    "text": "around designing a data Lake common cautions and challenges we get asked",
    "start": "58500",
    "end": "64049"
  },
  {
    "text": "when we meet customers and field as they embark on this journey some design patterns to counter these challenges and",
    "start": "64049",
    "end": "71460"
  },
  {
    "text": "finally we'll look at some security constructs to secure and govern your data Lake in an effective manner",
    "start": "71460",
    "end": "78000"
  },
  {
    "text": "hopefully we'll have time for Q&A if not both motors and I will be available",
    "start": "78000",
    "end": "83310"
  },
  {
    "text": "outside the room to take your questions so let's get started so as you can",
    "start": "83310",
    "end": "90150"
  },
  {
    "start": "88000",
    "end": "121000"
  },
  {
    "text": "imagine businesses today are generating enormous amount of data to the extent of",
    "start": "90150",
    "end": "95430"
  },
  {
    "text": "even petabytes a day a data Lake is a design pattern that has the potential to",
    "start": "95430",
    "end": "103829"
  },
  {
    "text": "transform businesses we're not only enabling you to capture manage store",
    "start": "103829",
    "end": "109049"
  },
  {
    "text": "these petabytes of data but also empower you to build complex data processing as",
    "start": "109049",
    "end": "116399"
  },
  {
    "text": "well as advanced analytics to drive some insights into your own businesses so with that we look at what we have as a",
    "start": "116399",
    "end": "124710"
  },
  {
    "text": "data Lake on AWS as you can see in this picture at the at the core of a data",
    "start": "124710",
    "end": "130110"
  },
  {
    "text": "Lake we have our Amazon s3 service now we chose s3 as the data lake because",
    "start": "130110",
    "end": "137010"
  },
  {
    "text": "it's a versatile data store that was designed for scale has many security",
    "start": "137010",
    "end": "143910"
  },
  {
    "text": "features built-in and is very very inexpensive now to get the data into s3",
    "start": "143910",
    "end": "150120"
  },
  {
    "text": "there are a number of mechanisms you can bulk load your data using snow ball or",
    "start": "150120",
    "end": "156600"
  },
  {
    "text": "snowmobile you can stream your data using streaming services such as kinases or kafka you can simply do an s3 copy",
    "start": "156600",
    "end": "163890"
  },
  {
    "text": "using CLI or SDK over VPN or Direct Connect if you have oil TP databases you",
    "start": "163890",
    "end": "169920"
  },
  {
    "text": "can migrate them using DMS database migration service into s3 and finally",
    "start": "169920",
    "end": "175709"
  },
  {
    "text": "you can also leverage hybrid storage solutions such as storage bay gateway to",
    "start": "175709",
    "end": "181320"
  },
  {
    "text": "seamlessly migrate your data into s3 and once you have the data in s3 it is",
    "start": "181320",
    "end": "187680"
  },
  {
    "text": "important to catalog and search it now traditionally customers have been using",
    "start": "187680",
    "end": "192780"
  },
  {
    "text": "a combination of DynamoDB and elasticsearch to accomplish this however",
    "start": "192780",
    "end": "198900"
  },
  {
    "text": "with the launch of glue you can now discover in four schema have the create",
    "start": "198900",
    "end": "204450"
  },
  {
    "text": "tables created automatically for you to start working on your analytics you can",
    "start": "204450",
    "end": "210269"
  },
  {
    "text": "also leverage tools like app sync and API gateway to provide access and a UI",
    "start": "210269",
    "end": "215750"
  },
  {
    "text": "to your data Lake using Cognito for sign up and sign end and of course security",
    "start": "215750",
    "end": "223019"
  },
  {
    "text": "is pivotal to everything we do and it is the same for a data Lake so you can manage your data Lake using a number of",
    "start": "223019",
    "end": "229970"
  },
  {
    "text": "security controls we have on the platform such as AWS kms the key",
    "start": "229970",
    "end": "235200"
  },
  {
    "text": "management service I am Identity and Access Management and of course cloud watch and cloud trail to monitor and log",
    "start": "235200",
    "end": "242519"
  },
  {
    "text": "your activities so all this is being done for one purpose right there are a",
    "start": "242519",
    "end": "248190"
  },
  {
    "text": "myriad of tools or in the analytics space that you can leverage to process",
    "start": "248190",
    "end": "254220"
  },
  {
    "text": "and consume your data in a data Lake these include Amazon Athena EMR redshift",
    "start": "254220",
    "end": "259560"
  },
  {
    "text": "as well as ruptured spectrum and a plethora of others so now that we have",
    "start": "259560",
    "end": "265770"
  },
  {
    "start": "264000",
    "end": "337000"
  },
  {
    "text": "an overview of what a data Lake looks like in AWS we have come to define some",
    "start": "265770",
    "end": "274050"
  },
  {
    "text": "key attributes of what a data Lake should have and so if you think about it",
    "start": "274050",
    "end": "279930"
  },
  {
    "text": "a data Lake is where all data is in one place and serves as a single source of",
    "start": "279930",
    "end": "286320"
  },
  {
    "text": "truth for all your analytic needs it has the ability to handle structured",
    "start": "286320",
    "end": "291770"
  },
  {
    "text": "semi-structured unstructured or even raw data it H has to be inexpensive has to",
    "start": "291770",
    "end": "299280"
  },
  {
    "text": "support schema and read also support decoupling of storage and compute so",
    "start": "299280",
    "end": "305910"
  },
  {
    "text": "that you have freedom from from these two resources and of course security is",
    "start": "305910",
    "end": "311250"
  },
  {
    "text": "as I said is critical to everything we do on AWS now you want your data assets and the resource accessing that data",
    "start": "311250",
    "end": "318449"
  },
  {
    "text": "Lake to be as secure as possible so all these are critical elements of that form",
    "start": "318449",
    "end": "326729"
  },
  {
    "text": "a data Lake and most of the field engagements that we have stress upon",
    "start": "326729",
    "end": "333479"
  },
  {
    "text": "these individual attributes at a greater length so with that let's see how",
    "start": "333479",
    "end": "338880"
  },
  {
    "start": "337000",
    "end": "383000"
  },
  {
    "text": "customers are building and designing their data Lakes today so in our experience customers build a data Lake",
    "start": "338880",
    "end": "346500"
  },
  {
    "text": "in a tiered fashion so we start with tier 1 tier 1 is simply an s3 bucket or",
    "start": "346500",
    "end": "353970"
  },
  {
    "text": "a prefix which serves as a single source of truth for your raw data you want to",
    "start": "353970",
    "end": "359610"
  },
  {
    "text": "preserve this raw data as much as possible and so this is where you want",
    "start": "359610",
    "end": "365759"
  },
  {
    "text": "to apply least amount of transformations this is also a place where you want to",
    "start": "365759",
    "end": "372180"
  },
  {
    "text": "apply life cycle policies to transition them to a lower tiered storage such as",
    "start": "372180",
    "end": "377550"
  },
  {
    "text": "s3 ia or even glacier when you are done using the data once you have the data in",
    "start": "377550",
    "end": "385770"
  },
  {
    "start": "383000",
    "end": "430000"
  },
  {
    "text": "tier 3 you take the data raw data and build what is called a tier tier 2 in",
    "start": "385770",
    "end": "392280"
  },
  {
    "text": "tier 2 your primary goal is to organize your data into partitions",
    "start": "392280",
    "end": "397810"
  },
  {
    "text": "convert them into query optimized columnar formats like parquet RoR C and",
    "start": "397810",
    "end": "403900"
  },
  {
    "text": "you have to do some maintenance on this tier as well for example over a period",
    "start": "403900",
    "end": "409210"
  },
  {
    "text": "of years you might accumulate a lot of partitions and some of the customers",
    "start": "409210",
    "end": "414910"
  },
  {
    "text": "have seen have millions of partitions it is important that you coil is these",
    "start": "414910",
    "end": "419980"
  },
  {
    "text": "smaller partitions into larger partitions for better performance of your queries using these analytic tools",
    "start": "419980",
    "end": "425530"
  },
  {
    "text": "so in essence this tier is optimized for analytics and then we move into tier 3",
    "start": "425530",
    "end": "433590"
  },
  {
    "start": "430000",
    "end": "469000"
  },
  {
    "text": "tier 3 is where you build domain level data marts this is where you are taking",
    "start": "433590",
    "end": "440320"
  },
  {
    "text": "the data that has been processed and ready for analytics in tier 2 and",
    "start": "440320",
    "end": "447210"
  },
  {
    "text": "optimizing it for individual use cases for example you might have a data science team you might have a financial",
    "start": "447419",
    "end": "453820"
  },
  {
    "text": "team or a marketing team each of them have different kinds of data they need and the process data can be pushed into",
    "start": "453820",
    "end": "460979"
  },
  {
    "text": "different folders different prefixes in tier 3 and there's essentially optimized",
    "start": "460979",
    "end": "466900"
  },
  {
    "text": "for specialized analytics most customers that we have been working with who use",
    "start": "466900",
    "end": "473380"
  },
  {
    "start": "469000",
    "end": "524000"
  },
  {
    "text": "data like on s3 or build data like Ana's 3 also use a data warehouse like",
    "start": "473380",
    "end": "478600"
  },
  {
    "text": "redshift to complement their data leak and a redshift data warehouse is really",
    "start": "478600",
    "end": "486280"
  },
  {
    "text": "optimized for faster responses to your structured schemas it it helps serve",
    "start": "486280",
    "end": "492760"
  },
  {
    "text": "your dashboards and reports for your BI tools it has fine-grained access control",
    "start": "492760",
    "end": "498190"
  },
  {
    "text": "and most importantly it has the ability to support joints between the native",
    "start": "498190",
    "end": "504070"
  },
  {
    "text": "tables residing in redshift with the external tables that you define and s3 and so these elements make it an ideal",
    "start": "504070",
    "end": "512349"
  },
  {
    "text": "choice to be a companion to your data leak and of course when you're done with using your wretched cluster you can",
    "start": "512349",
    "end": "519400"
  },
  {
    "text": "lifecycle the data back to s3 data leg to save on costs so with that let's get",
    "start": "519400",
    "end": "526630"
  },
  {
    "start": "524000",
    "end": "594000"
  },
  {
    "text": "into some common asks and challenges that we encountered in the feel and some of them may resonate with",
    "start": "526630",
    "end": "532510"
  },
  {
    "text": "your situations as well so here here are some of the sample questions that we get",
    "start": "532510",
    "end": "537670"
  },
  {
    "text": "can I do streaming ingest into a data Lake can a data Lake replace our",
    "start": "537670",
    "end": "543220"
  },
  {
    "text": "database replicas we maintain for analytics how do I organize data within",
    "start": "543220",
    "end": "548800"
  },
  {
    "text": "this data Lake how do I handle late events coming into older partitions how",
    "start": "548800",
    "end": "553930"
  },
  {
    "text": "to perform updates and deletes to the data inside a data Lake how can I run",
    "start": "553930",
    "end": "558940"
  },
  {
    "text": "machine learning training on data in a data Lake how can I open the data with",
    "start": "558940",
    "end": "564310"
  },
  {
    "text": "real-time predictions during ETL or even during ingestion how do we enforce data",
    "start": "564310",
    "end": "570280"
  },
  {
    "text": "protection rules in a data Lake and what are some authentication and authorization mechanisms available for",
    "start": "570280",
    "end": "576670"
  },
  {
    "text": "your data assets in ns3 right so these are some of the questions that we get and this is just a subset of",
    "start": "576670",
    "end": "582970"
  },
  {
    "text": "the questions that we get and let's see how we can address these concerns and",
    "start": "582970",
    "end": "589660"
  },
  {
    "text": "questions using some examples from the design patterns that we have seen so you",
    "start": "589660",
    "end": "595660"
  },
  {
    "start": "594000",
    "end": "698000"
  },
  {
    "text": "might have a different use case in your own environment for the purpose of this",
    "start": "595660",
    "end": "600820"
  },
  {
    "text": "presentation we will consider a log or a clickstream analytics or even processing",
    "start": "600820",
    "end": "608110"
  },
  {
    "text": "IOT sensor data as a use case so let's say you have an application that is",
    "start": "608110",
    "end": "613470"
  },
  {
    "text": "streaming data through Kinesis firewalls and that data is destined for s3 because",
    "start": "613470",
    "end": "620230"
  },
  {
    "text": "you have configured firehose to deliver it to s3 and then from s3 you are serving it the data is being used by",
    "start": "620230",
    "end": "628210"
  },
  {
    "text": "analytic tools like Athena presto and Android chef spectrum or even redshift",
    "start": "628210",
    "end": "633400"
  },
  {
    "text": "copying into redshift cluster however there is a problem you might end up with",
    "start": "633400",
    "end": "639760"
  },
  {
    "text": "too many small files which is not necessarily optimized for analytics and this could happen because IOT sensor",
    "start": "639760",
    "end": "647290"
  },
  {
    "text": "data is tiny and maybe you have configured the buffer interval and the Kinesis firehose to be smaller like 1",
    "start": "647290",
    "end": "653470"
  },
  {
    "text": "minute or 3 minutes and so you're not gathering enough data for it to become a",
    "start": "653470",
    "end": "660340"
  },
  {
    "text": "reasonably sized file and so how do we address this so you can follow the same approach of",
    "start": "660340",
    "end": "668079"
  },
  {
    "text": "streaming your data through Kinesis firehose however you send the data into",
    "start": "668079",
    "end": "673779"
  },
  {
    "text": "Tier one which is which is the place where we house the raw data and use a",
    "start": "673779",
    "end": "680290"
  },
  {
    "text": "glue ETL job to perform hourly compactions and convert them to O RC and parquet and put them into tier two and",
    "start": "680290",
    "end": "688870"
  },
  {
    "text": "then serve your data from tier two for",
    "start": "688870",
    "end": "694029"
  },
  {
    "text": "all these analytic and other application needs what about a use case where you",
    "start": "694029",
    "end": "701019"
  },
  {
    "start": "698000",
    "end": "731000"
  },
  {
    "text": "need fast ingestion in this case you can still configure Kinesis firehose to",
    "start": "701019",
    "end": "707139"
  },
  {
    "text": "deliver to a redshift cluster instead of s3 directly and on a frequent basis on a",
    "start": "707139",
    "end": "713019"
  },
  {
    "text": "periodic basis you can have a glue job connect your redshift cluster perform",
    "start": "713019",
    "end": "719139"
  },
  {
    "text": "the ETL and put the process data in s3 and you can have Amazon redshift",
    "start": "719139",
    "end": "725769"
  },
  {
    "text": "spectrum act as a single serving layer for all your queries what if you want a",
    "start": "725769",
    "end": "732639"
  },
  {
    "start": "731000",
    "end": "806000"
  },
  {
    "text": "faster ingestion in this case you replace your Kinesis fire hose with",
    "start": "732639",
    "end": "737769"
  },
  {
    "text": "Kinesis data streams and process your data in micro batches using spark",
    "start": "737769",
    "end": "743920"
  },
  {
    "text": "streaming running on EMR push the data into an HBase cluster running on EMR",
    "start": "743920",
    "end": "749110"
  },
  {
    "text": "performed regular ETL jobs using glue and push the data into a data Lake and",
    "start": "749110",
    "end": "754899"
  },
  {
    "text": "have presto running on EMR federate your queries and serve as a single serving",
    "start": "754899",
    "end": "761589"
  },
  {
    "text": "layer for your queries what you want but",
    "start": "761589",
    "end": "766899"
  },
  {
    "text": "if you want the fattest fastest ingestion we can still stream the data through Kinesis data streams but in this case",
    "start": "766899",
    "end": "773709"
  },
  {
    "text": "instead of using Spock streaming which micro batches you can leverage apache flink",
    "start": "773709",
    "end": "779139"
  },
  {
    "text": "which is a first-class citizen on EMR to process your data push the data into a",
    "start": "779139",
    "end": "785110"
  },
  {
    "text": "rich HBase cluster running on EMR and repeat the same process of performing",
    "start": "785110",
    "end": "790959"
  },
  {
    "text": "the ETL and then pushing your processed data into s3",
    "start": "790959",
    "end": "796890"
  },
  {
    "text": "and and this process is the same for serving you use presto an EMR which does",
    "start": "796890",
    "end": "802260"
  },
  {
    "text": "the federated query and acts as a federated query and single serving layer now moving on to the next pressing",
    "start": "802260",
    "end": "810510"
  },
  {
    "start": "806000",
    "end": "931000"
  },
  {
    "text": "question which is about replacing your database replicas with metadata later",
    "start": "810510",
    "end": "816210"
  },
  {
    "text": "right in this case we have many customers who want to take their data in",
    "start": "816210",
    "end": "821940"
  },
  {
    "text": "their oil TP databases push them into s3 using a service like database migration",
    "start": "821940",
    "end": "827840"
  },
  {
    "text": "service and then have that act as the",
    "start": "827840",
    "end": "833460"
  },
  {
    "text": "datasets for your analytic tools but there is a problem updates and deletes",
    "start": "833460",
    "end": "840270"
  },
  {
    "text": "creates newer versions of records so those who have used database migration might have noticed each row in the that",
    "start": "840270",
    "end": "849990"
  },
  {
    "text": "is being caught migrated to s3 has an additional column to indicate whether",
    "start": "849990",
    "end": "855300"
  },
  {
    "text": "it's an update or a dilly so this is a problem because you'll have multiple versions of it and so here again you",
    "start": "855300",
    "end": "863610"
  },
  {
    "text": "deploy the same technique you push the DMS data into Tier one at a periodic",
    "start": "863610",
    "end": "870080"
  },
  {
    "text": "frequency you could have a glue ETL job do the merge of your versions put them",
    "start": "870080",
    "end": "877440"
  },
  {
    "text": "in tier two and serve it to your analytic tools like before so you can",
    "start": "877440",
    "end": "882900"
  },
  {
    "text": "also create views to preserve the database view of records now again in",
    "start": "882900",
    "end": "888720"
  },
  {
    "text": "this case you might have a problem and that is grouping records in views can be",
    "start": "888720",
    "end": "895110"
  },
  {
    "text": "expensive operation over time and this is this is something that we have seen",
    "start": "895110",
    "end": "900480"
  },
  {
    "text": "in the field so how do we address that so in this case you want to go through the same ETL process with your Tier one",
    "start": "900480",
    "end": "908220"
  },
  {
    "text": "and put them in tier two and you in in addition to doing that you also take a",
    "start": "908220",
    "end": "914610"
  },
  {
    "text": "daily snapshots to preserve the database view of records this allows all the",
    "start": "914610",
    "end": "921030"
  },
  {
    "text": "analytic tools to get a proper view of the records at a certain time of the day",
    "start": "921030",
    "end": "929690"
  },
  {
    "start": "931000",
    "end": "1003000"
  },
  {
    "text": "so we'll switch gears here and and move to a different kind of use case one of",
    "start": "932060",
    "end": "938970"
  },
  {
    "text": "the common questions that we get from most data centers is how do I run my",
    "start": "938970",
    "end": "945210"
  },
  {
    "text": "batch training pipeline with my data in a data Lake like s3 now again the",
    "start": "945210",
    "end": "951300"
  },
  {
    "text": "process is fairly straightforward here you have your data sitting in a raw data",
    "start": "951300",
    "end": "957030"
  },
  {
    "text": "arriving in tier one like before and you use either spark running on EMR or",
    "start": "957030",
    "end": "962730"
  },
  {
    "text": "leverage a glue job to do your data preparation and this data preparation could include feature extraction or",
    "start": "962730",
    "end": "970380"
  },
  {
    "text": "vectorization of your data and you can put the process data into tier two like",
    "start": "970380",
    "end": "975930"
  },
  {
    "text": "before and you can invoke another glue job to provide the data to a batch",
    "start": "975930",
    "end": "982530"
  },
  {
    "text": "training job on Sage Maker and one stage maker is done building a model it can",
    "start": "982530",
    "end": "989400"
  },
  {
    "text": "push the model artifacts to s3 and another Python job or a glue job can",
    "start": "989400",
    "end": "995450"
  },
  {
    "text": "invoke as the sage maker API to do the model deployment using the sage makers",
    "start": "995450",
    "end": "1001580"
  },
  {
    "text": "hosting services now what about predictions on a streaming data and this",
    "start": "1001580",
    "end": "1007430"
  },
  {
    "start": "1003000",
    "end": "1087000"
  },
  {
    "text": "is a real-life scenario that we have seen with some of our customers where they have real events coming in from a",
    "start": "1007430",
    "end": "1015560"
  },
  {
    "text": "web portal and they want to augment this data with some predictions for coming in",
    "start": "1015560",
    "end": "1021620"
  },
  {
    "text": "from a sage maker endpoint so in this case the data can be streamed through Kinesis firehose which is integrated",
    "start": "1021620",
    "end": "1028579"
  },
  {
    "text": "with a lambda function and this lambda function essentially augments your",
    "start": "1028579",
    "end": "1033860"
  },
  {
    "text": "streaming data with predictions from a sage maker endpoint and then delivers it",
    "start": "1033860",
    "end": "1039560"
  },
  {
    "text": "to the s3 and s3 data lake now this is a problem because it's not a major problem",
    "start": "1039560",
    "end": "1048319"
  },
  {
    "text": "but it is a problem because it violates our first principle which which talks",
    "start": "1048319",
    "end": "1053630"
  },
  {
    "text": "about least transformations on the raw data right so we don't want we want the",
    "start": "1053630",
    "end": "1058850"
  },
  {
    "text": "raw data but to be as pristine as possible and so this disposes a problem breaking",
    "start": "1058850",
    "end": "1064310"
  },
  {
    "text": "that rule and so in order to address that you can still stream your data through Kinesis firehose but instead of",
    "start": "1064310",
    "end": "1071000"
  },
  {
    "text": "integrating with lambda you can push it into Tier one and then invoke a glue job",
    "start": "1071000",
    "end": "1076520"
  },
  {
    "text": "that can make an API call to a size maker endpoint augment your data put it",
    "start": "1076520",
    "end": "1083120"
  },
  {
    "text": "in tier two and serve it to your analytic tools so all these design",
    "start": "1083120",
    "end": "1089540"
  },
  {
    "start": "1087000",
    "end": "1201000"
  },
  {
    "text": "patterns kind of give us some foundational principles required to build a data Lake right and the the",
    "start": "1089540",
    "end": "1097130"
  },
  {
    "text": "first principle is called principle of minimal ingestion contract what this means is that you need to decide as a",
    "start": "1097130",
    "end": "1103940"
  },
  {
    "text": "data engineer or an owner of a data Lake you need to decide on a location for ingestion and select a frequency and",
    "start": "1103940",
    "end": "1110210"
  },
  {
    "text": "ingestion mechanism to meet your SLA and the producer of the data is given only",
    "start": "1110210",
    "end": "1116390"
  },
  {
    "text": "the location of that s3 bucket or a prefix where they can deposit their files and once the data arrives you want",
    "start": "1116390",
    "end": "1124190"
  },
  {
    "text": "to partition the data but and you want to partition with keys that align with your common query predicates this",
    "start": "1124190",
    "end": "1131450"
  },
  {
    "text": "partitioning is essential to especially your query needs simply because",
    "start": "1131450",
    "end": "1138440"
  },
  {
    "text": "partitioning helps prune unnecessary folders that are not needed for your",
    "start": "1138440",
    "end": "1143990"
  },
  {
    "text": "query thereby reducing your query performance significantly because many",
    "start": "1143990",
    "end": "1149420"
  },
  {
    "text": "of the analytic tools that you use are hadoop based for example a teen hours pressed or running on spar most of them",
    "start": "1149420",
    "end": "1156770"
  },
  {
    "text": "use the underlying hadoop infrastructure you want to keep the file sizes to be",
    "start": "1156770",
    "end": "1164920"
  },
  {
    "text": "around 256 megabytes to 1 1 GB in general and",
    "start": "1164920",
    "end": "1170020"
  },
  {
    "text": "preferably in columnar format for partition this allows you to get the",
    "start": "1170020",
    "end": "1175850"
  },
  {
    "text": "maximum benefit from the clusters that are running your query of course you",
    "start": "1175850",
    "end": "1181250"
  },
  {
    "text": "want to compact your data on a scheduled basis if your data that is arriving is",
    "start": "1181250",
    "end": "1187130"
  },
  {
    "text": "not to this optimal size you you can perform compactions using glue",
    "start": "1187130",
    "end": "1194780"
  },
  {
    "text": "to bring them to at least 256 megabytes and then start processing them so then",
    "start": "1194780",
    "end": "1202400"
  },
  {
    "start": "1201000",
    "end": "1260000"
  },
  {
    "text": "comes the questions so how do we partition the columns so the key to",
    "start": "1202400",
    "end": "1207460"
  },
  {
    "text": "partitioning your columns is to know your query filters and your group by",
    "start": "1207460",
    "end": "1212810"
  },
  {
    "text": "columns and these should align with your partition columns that you will be",
    "start": "1212810",
    "end": "1218150"
  },
  {
    "text": "creating in s3 and if you're using tools like EMR or high running on EMR or",
    "start": "1218150",
    "end": "1224710"
  },
  {
    "text": "athina then it is better for you to partition this using hive compatible",
    "start": "1224710",
    "end": "1232430"
  },
  {
    "text": "format to get the optimal performance as as mentioned before you want to aim to",
    "start": "1232430",
    "end": "1237470"
  },
  {
    "text": "have optimal file sizes around 256 megabytes or up to 1gb and you want to",
    "start": "1237470",
    "end": "1244820"
  },
  {
    "text": "identify the typical queries can range as well because partitioning often",
    "start": "1244820",
    "end": "1250490"
  },
  {
    "text": "happens on on dates it is important to know if your queries leverage and and",
    "start": "1250490",
    "end": "1256370"
  },
  {
    "text": "you want to partition around the date range as well so here is an example of",
    "start": "1256370",
    "end": "1262280"
  },
  {
    "start": "1260000",
    "end": "1305000"
  },
  {
    "text": "how you may want to think about partitioning let's say you have a use case where you want to aggregate a large",
    "start": "1262280",
    "end": "1269090"
  },
  {
    "text": "time series data and you have 100 devices IOT sensor sending data and an",
    "start": "1269090",
    "end": "1276620"
  },
  {
    "text": "hourly basis and so you decided on a partition which has device year-month-day our and you have a data",
    "start": "1276620",
    "end": "1283940"
  },
  {
    "text": "retention or a query scan range of five years and you have one file per partition with each file size being ten",
    "start": "1283940",
    "end": "1290780"
  },
  {
    "text": "megabytes now if you do the math it comes to about four point three partitions now this is a lot and this",
    "start": "1290780",
    "end": "1298970"
  },
  {
    "text": "can pose a problem especially when you have large volume of data and so to fix",
    "start": "1298970",
    "end": "1305120"
  },
  {
    "start": "1305000",
    "end": "1350000"
  },
  {
    "text": "this you can do this instead of partitioning by device you can partition",
    "start": "1305120",
    "end": "1310430"
  },
  {
    "text": "on year-month-day remove the hour and device and you bucket your data by the",
    "start": "1310430",
    "end": "1318380"
  },
  {
    "text": "device let's say you define 50 buckets and so your hundred devices are properly",
    "start": "1318380",
    "end": "1324620"
  },
  {
    "text": "bucketed into these 50 buckets and with that partitioning scheme you're",
    "start": "1324620",
    "end": "1330980"
  },
  {
    "text": "effectively reducing the number of partitions to 1825 which is far less",
    "start": "1330980",
    "end": "1337129"
  },
  {
    "text": "than the 4.3 million that we saw earlier and this is more manageable and [Music]",
    "start": "1337129",
    "end": "1342759"
  },
  {
    "text": "operationally efficient way of working with your partitions because this keeps",
    "start": "1342759",
    "end": "1348919"
  },
  {
    "text": "growing another compelling question that",
    "start": "1348919",
    "end": "1353929"
  },
  {
    "start": "1350000",
    "end": "1409000"
  },
  {
    "text": "we often get asked is what about mutable data customers often have updates and",
    "start": "1353929",
    "end": "1360230"
  },
  {
    "text": "deletes on existing data and because as three is an append-only system how do we handle that in if you have a lot of",
    "start": "1360230",
    "end": "1368210"
  },
  {
    "text": "updates and merges to perform we highly recommend that you use a database like",
    "start": "1368210",
    "end": "1374049"
  },
  {
    "text": "Amazon redshift or HBase running on EMR for the time the data can mutate and",
    "start": "1374049",
    "end": "1380419"
  },
  {
    "text": "once the data becomes static we can offload the data to s3 that's that's an",
    "start": "1380419",
    "end": "1387379"
  },
  {
    "text": "ideal way of handling this scenario there's another way which in which you can append to the Delta files port",
    "start": "1387379",
    "end": "1395210"
  },
  {
    "text": "partition and compact on scheduled basis using either a spark job or using a glue",
    "start": "1395210",
    "end": "1401720"
  },
  {
    "text": "job so both these approaches have been used effectively in in production by our",
    "start": "1401720",
    "end": "1408919"
  },
  {
    "text": "customers so here is an example of how to serve immutable data let's say you",
    "start": "1408919",
    "end": "1414139"
  },
  {
    "start": "1409000",
    "end": "1452000"
  },
  {
    "text": "have an record coming from your OLTP data base and that database you have",
    "start": "1414139",
    "end": "1420799"
  },
  {
    "text": "used DMR's to push that and you can version you can add an additional column that says or that specifies the version",
    "start": "1420799",
    "end": "1428119"
  },
  {
    "text": "for that record and as the tracker changes you get a newer version of that",
    "start": "1428119",
    "end": "1434119"
  },
  {
    "text": "file it gets appended and you invoke a glue job on a periodic basis to keep the",
    "start": "1434119",
    "end": "1442249"
  },
  {
    "text": "most recent version and delete the rest this way your data set remains lean and",
    "start": "1442249",
    "end": "1447440"
  },
  {
    "text": "you still get the benefit of having the latest version in the data leak there",
    "start": "1447440",
    "end": "1454009"
  },
  {
    "start": "1452000",
    "end": "1500000"
  },
  {
    "text": "are other data like optimizations that you can perform bucketed data we saw an example earlier so for additional",
    "start": "1454009",
    "end": "1460790"
  },
  {
    "text": "performance you can bucket data in each partition on a high cardinality key this",
    "start": "1460790",
    "end": "1465860"
  },
  {
    "text": "this setting is honored by presto Athena hive and improves your Curie performance",
    "start": "1465860",
    "end": "1472820"
  },
  {
    "text": "significantly so that's an example of how you could write you can you you",
    "start": "1472820",
    "end": "1478910"
  },
  {
    "text": "could write a data a data frame by using the bucket by API call you can also",
    "start": "1478910",
    "end": "1485540"
  },
  {
    "text": "order the data for additional performance you can sort the data in",
    "start": "1485540",
    "end": "1490580"
  },
  {
    "text": "each partitions by the secondary key and as the example shows there are api's to",
    "start": "1490580",
    "end": "1495800"
  },
  {
    "text": "do that very easily and to get your data or requested data faster there are more",
    "start": "1495800",
    "end": "1502610"
  },
  {
    "start": "1500000",
    "end": "1527000"
  },
  {
    "text": "advanced techniques like bloom Flint or bloom filters these bloom filters are a",
    "start": "1502610",
    "end": "1507680"
  },
  {
    "text": "space efficient probabilistic data structures that is used to test whether an element is a member of a set and this",
    "start": "1507680",
    "end": "1514640"
  },
  {
    "text": "also def reduces your response time",
    "start": "1514640",
    "end": "1519710"
  },
  {
    "text": "significantly when when you are querying against a database sitting in an s3 all",
    "start": "1519710",
    "end": "1528920"
  },
  {
    "text": "right with that I turn it over to my colleague to talk about some security and governance patterns we have observed",
    "start": "1528920",
    "end": "1535160"
  },
  {
    "text": "in the field and will take your questions after that thank you ready can finally I get to get",
    "start": "1535160",
    "end": "1541130"
  },
  {
    "text": "out of my seat so hi everyone so let me",
    "start": "1541130",
    "end": "1548810"
  },
  {
    "start": "1546000",
    "end": "1604000"
  },
  {
    "text": "start by section at that section by you know mentioning six timeless security",
    "start": "1548810",
    "end": "1553820"
  },
  {
    "text": "and governance concerns they apply not really only to your data link but also across your you know any IT solution",
    "start": "1553820",
    "end": "1560510"
  },
  {
    "text": "really authentication making sure people are who they say they are authorization",
    "start": "1560510",
    "end": "1565930"
  },
  {
    "text": "making sure people are can only do what they are allowed to protection of data",
    "start": "1565930",
    "end": "1572210"
  },
  {
    "text": "through encryption at rest and in transit the presence of other trails to",
    "start": "1572210",
    "end": "1577310"
  },
  {
    "text": "ensure that we can trace activity back to unique individuals or principles",
    "start": "1577310",
    "end": "1583720"
  },
  {
    "text": "centralized governments to ensure that whichever security measures that you want to implement are applied uniformly",
    "start": "1583720",
    "end": "1590480"
  },
  {
    "text": "and consistently across your enterprise data Lake and finally the ability to achieve and",
    "start": "1590480",
    "end": "1598410"
  },
  {
    "text": "demonstrate compliance to regulators without major fire drills AWS provides",
    "start": "1598410",
    "end": "1606720"
  },
  {
    "text": "you with the richest the most diverse portfolio of security compliance and governance focused services and features",
    "start": "1606720",
    "end": "1614390"
  },
  {
    "text": "so 203 significant services and features focused on that from services that help",
    "start": "1614390",
    "end": "1621630"
  },
  {
    "text": "you establish managed and federated entities such as AWS identity access management and our suite of cloud",
    "start": "1621630",
    "end": "1628980"
  },
  {
    "text": "directory services and Federation capabilities to services that help you",
    "start": "1628980",
    "end": "1634850"
  },
  {
    "text": "achieve and demonstrate compliance such as AWS artifact which provides you with",
    "start": "1634850",
    "end": "1641159"
  },
  {
    "text": "detailed AWS compliance reports AWS cloud rain which helps you trace API",
    "start": "1641159",
    "end": "1648780"
  },
  {
    "text": "level activity in your AWS accounts an AWS config which can give you an accurate picture of your the footprint",
    "start": "1648780",
    "end": "1656520"
  },
  {
    "text": "in your AWS accounts and how that footprint changes over time and finally",
    "start": "1656520",
    "end": "1663240"
  },
  {
    "text": "to services that help you encrypt your data at rest and in transit and be able",
    "start": "1663240",
    "end": "1669510"
  },
  {
    "text": "to manage your encryption keys in whichever way you choose and beyond",
    "start": "1669510",
    "end": "1675809"
  },
  {
    "text": "traditional security controls we are constantly innovating in order to help our customers secure their data leaks at",
    "start": "1675809",
    "end": "1681390"
  },
  {
    "text": "scale and it is no surprise that machine learning in specific can be applied very effectively in order to automate some of",
    "start": "1681390",
    "end": "1688679"
  },
  {
    "text": "the critical security tasks for your data link and there is why late last",
    "start": "1688679",
    "end": "1695669"
  },
  {
    "start": "1693000",
    "end": "1737000"
  },
  {
    "text": "year we introduced Amazon Mesa Amazon macey can recognize sensitive information or",
    "start": "1695669",
    "end": "1701940"
  },
  {
    "text": "sensitive data in your data Lake such as personally identifiable information and",
    "start": "1701940",
    "end": "1707510"
  },
  {
    "text": "intellectual property it uses machine learning models for that",
    "start": "1707510",
    "end": "1712580"
  },
  {
    "text": "Macie can also detect monic continuously monitors and detects monitors data",
    "start": "1712580",
    "end": "1719250"
  },
  {
    "text": "access to your data lake and is able to generate and provide alerts if it",
    "start": "1719250",
    "end": "1725549"
  },
  {
    "text": "detects a risk of unauthorized access or and in like a",
    "start": "1725549",
    "end": "1730879"
  },
  {
    "text": "data leak but a data Lake data leak sorry by mistake but with that quick",
    "start": "1730879",
    "end": "1739549"
  },
  {
    "start": "1737000",
    "end": "1755000"
  },
  {
    "text": "overview behind us let's focus our discussion a little bit about two key aspects of the italics security data",
    "start": "1739549",
    "end": "1746119"
  },
  {
    "text": "storage security and metadata security let's start by data storage security so",
    "start": "1746119",
    "end": "1755929"
  },
  {
    "start": "1755000",
    "end": "1828000"
  },
  {
    "text": "there are three key topics that we should consider when putting together a data Lake storage security strategy",
    "start": "1755929",
    "end": "1764200"
  },
  {
    "text": "first is that it's common for our customers to have multiple teams want to",
    "start": "1764200",
    "end": "1769610"
  },
  {
    "text": "access a single enterprise data Lake so the question is how do you implement",
    "start": "1769610",
    "end": "1775340"
  },
  {
    "text": "access control for those teams we will see that this is largely dependent on",
    "start": "1775340",
    "end": "1780980"
  },
  {
    "text": "how you implement data and resource ownership and here we differentiate between two kinds of data and resource",
    "start": "1780980",
    "end": "1787490"
  },
  {
    "text": "ownership coarse-grained datin resource ownership and fine-grained a-10s or ownership the second topic is about how",
    "start": "1787490",
    "end": "1795889"
  },
  {
    "text": "to implement fine-grained access control to Amazon s3 and here we talk also about how to control access to SP from EMR and",
    "start": "1795889",
    "end": "1803330"
  },
  {
    "text": "redshift and you also discuss a new exciting feature about Amazon s3 that helps you completely block public access",
    "start": "1803330",
    "end": "1811070"
  },
  {
    "text": "to any Amazon s3 buckets in your AWS accounts and finally I'll briefly",
    "start": "1811070",
    "end": "1816710"
  },
  {
    "text": "overview I gave you a quick overview of encryption and the encryption options available to you",
    "start": "1816710",
    "end": "1823720"
  },
  {
    "start": "1828000",
    "end": "1900000"
  },
  {
    "text": "so let's start by talking about fine-grained access control of fine grade data and resource ownership in",
    "start": "1829680",
    "end": "1835860"
  },
  {
    "text": "this with this strategy what happens is that teams share fractions of resources",
    "start": "1835860",
    "end": "1841320"
  },
  {
    "text": "if you will so if you if we take a look at the dash boundaries that we have in front of us we see that team X owns",
    "start": "1841320",
    "end": "1847340"
  },
  {
    "text": "certain databases and schemas in the redshift cluster and so does team y and",
    "start": "1847340",
    "end": "1853050"
  },
  {
    "text": "both effectively share the same redshift cluster we can have a similar situation",
    "start": "1853050",
    "end": "1859950"
  },
  {
    "text": "with an Amazon EMR cluster where both teams also share the same cluster obviously this approach has advantages",
    "start": "1859950",
    "end": "1866850"
  },
  {
    "text": "and the biggest advantage is efficiency shared resources are better utilized but",
    "start": "1866850",
    "end": "1873300"
  },
  {
    "text": "at the same time the trade-off that you are going to make here is that access control would be now relatively more",
    "start": "1873300",
    "end": "1879150"
  },
  {
    "text": "complex to set up and maintain and the reason for that is that EMR and redshift",
    "start": "1879150",
    "end": "1885270"
  },
  {
    "text": "both have their native identity management mechanisms so you will have",
    "start": "1885270",
    "end": "1890670"
  },
  {
    "text": "to build your user and identity structures in both of those clusters now",
    "start": "1890670",
    "end": "1900000"
  },
  {
    "start": "1900000",
    "end": "1950000"
  },
  {
    "text": "we can contrast that to coarse-grained data and resource ownership and with",
    "start": "1900000",
    "end": "1905940"
  },
  {
    "text": "this strategy your team's own entire AWS resources such as the such as Team X",
    "start": "1905940",
    "end": "1911850"
  },
  {
    "text": "here and in this example they own their own redshift clusters in their own EMR clusters their own Amazon has three",
    "start": "1911850",
    "end": "1918240"
  },
  {
    "text": "buckets and even their own accounts this simplifies much setting up access",
    "start": "1918240",
    "end": "1925440"
  },
  {
    "text": "controls and moreover it lends itself naturally to controlling to setting up",
    "start": "1925440",
    "end": "1931710"
  },
  {
    "text": "access controls through AWS organizations and the reason access",
    "start": "1931710",
    "end": "1938460"
  },
  {
    "text": "control setup is simple here is because the primary mechanism that uses AWS Identity and Access Management but you",
    "start": "1938460",
    "end": "1944910"
  },
  {
    "text": "can also feather 8 access to your own identity management infrastructure as well moving on let's discuss access",
    "start": "1944910",
    "end": "1954870"
  },
  {
    "start": "1950000",
    "end": "2107000"
  },
  {
    "text": "control to s3 so Amazon asleep provides you with three different mechanisms to control access",
    "start": "1954870",
    "end": "1961330"
  },
  {
    "text": "to data provides you with user policies which can be attached these around",
    "start": "1961330",
    "end": "1966970"
  },
  {
    "text": "policies that can be attached to users groups or roles the bucket policies which can be attached to Amazon s3",
    "start": "1966970",
    "end": "1973090"
  },
  {
    "text": "buckets and bucket and access bucket and object access control lists",
    "start": "1973090",
    "end": "1979590"
  },
  {
    "text": "so usually customers combine all of these three mechanisms in order to",
    "start": "1979590",
    "end": "1984789"
  },
  {
    "text": "achieve very fine-grained and flexible access control this is especially useful",
    "start": "1984789",
    "end": "1991539"
  },
  {
    "text": "if you decide to lean more towards the fine-grained data and resource ownership strategy and reality are going to end up",
    "start": "1991539",
    "end": "2000000"
  },
  {
    "text": "with a blend of both you're going to end up with a blend even if you decide to implement coarse-grained data resource",
    "start": "2000000",
    "end": "2005340"
  },
  {
    "text": "ownership within a single team you will need to set up access controls for individual team members but it becomes",
    "start": "2005340",
    "end": "2012149"
  },
  {
    "text": "much easier than having multiple teams sharing as a single resource moving on",
    "start": "2012149",
    "end": "2021390"
  },
  {
    "text": "to Amazon EMR and Amazon redshift both services when you when you set up",
    "start": "2021390",
    "end": "2028409"
  },
  {
    "text": "clusters they can access Amazon s3 through Identity and Access Management roles which are called service roles so",
    "start": "2028409",
    "end": "2035279"
  },
  {
    "text": "you can attach I am roles to your Amazon redshift cluster and when you do that",
    "start": "2035279",
    "end": "2041909"
  },
  {
    "text": "all of your Amazon redshift cluster users they have X the same level of access to your Amazon s3 buckets and",
    "start": "2041909",
    "end": "2049108"
  },
  {
    "text": "this is important to keep in mind when you are set in defining the permissions for those service roles Amazon Emaar has",
    "start": "2049109",
    "end": "2057868"
  },
  {
    "text": "the same thing basically so you can also use a cluster level role to access",
    "start": "2057869",
    "end": "2063270"
  },
  {
    "text": "Amazon s3 but it also provides you with an additional feature amazonia our file",
    "start": "2063270",
    "end": "2068429"
  },
  {
    "text": "system gives you the ability to establish a mapping between your EMR users and groups to certain AWS Identity",
    "start": "2068429",
    "end": "2076320"
  },
  {
    "text": "and Access Management roles and based on that mapping EMR will decide how to",
    "start": "2076320",
    "end": "2082618"
  },
  {
    "text": "access which role it it should assume when a certain user tries tries to access s3 it will decide which Identity",
    "start": "2082619",
    "end": "2089820"
  },
  {
    "text": "and Access Management to assume in order to access Amazon s3 so this gives you more control and more",
    "start": "2089820",
    "end": "2095080"
  },
  {
    "text": "fine-grained control over the permissions that you provide to Amazon EMR users and groups moving on to this",
    "start": "2095080",
    "end": "2109510"
  },
  {
    "start": "2107000",
    "end": "2318000"
  },
  {
    "text": "new feature so we would like you to be able to use Amazon s3 for for public",
    "start": "2109510",
    "end": "2117310"
  },
  {
    "text": "access because this is a feature that is helpful for some specific use cases particularly website hosting but the",
    "start": "2117310",
    "end": "2125380"
  },
  {
    "text": "same time we want to give you tools for you to be sure that you not give",
    "start": "2125380",
    "end": "2130480"
  },
  {
    "text": "accidental public access to your data and that is why we released Amazon s3",
    "start": "2130480",
    "end": "2136180"
  },
  {
    "text": "block public access Amazon s3 public",
    "start": "2136180",
    "end": "2141430"
  },
  {
    "text": "access allows you to block public access for existing and future Amazon s3",
    "start": "2141430",
    "end": "2147010"
  },
  {
    "text": "buckets that you create in your account it provides you settings that you can",
    "start": "2147010",
    "end": "2152650"
  },
  {
    "text": "apply at the account level and at the public level at the sorry at the bucket level so on here let's go quickly",
    "start": "2152650",
    "end": "2160180"
  },
  {
    "text": "through those those settings so the first setting is blocked public ACLs this setting enables you to block new",
    "start": "2160180",
    "end": "2168760"
  },
  {
    "text": "put requests or new editions of public Asya public object on bucket ACLs and",
    "start": "2168760",
    "end": "2173850"
  },
  {
    "text": "this is especially useful if you provide cross account access to Amazon s3",
    "start": "2173850",
    "end": "2179980"
  },
  {
    "text": "buckets because if you do that the the other accounts would own their objects",
    "start": "2179980",
    "end": "2185590"
  },
  {
    "text": "and they have the ability to make their objects public so with this wood block",
    "start": "2185590",
    "end": "2191380"
  },
  {
    "text": "public ecl's any new publications are rejected the second setting is ignore public ACLs",
    "start": "2191380",
    "end": "2198010"
  },
  {
    "text": "this is like a kill switch on all public access so when you enable that setting",
    "start": "2198010",
    "end": "2204070"
  },
  {
    "text": "what happens is that all public if there are public objects and bucket ACLs these",
    "start": "2204070",
    "end": "2209770"
  },
  {
    "text": "get ignored so you only have private access to the objects the third setting",
    "start": "2209770",
    "end": "2215980"
  },
  {
    "text": "is block public policy so this is similar to the first setting but it rejects public Amazon s3 bucket policies",
    "start": "2215980",
    "end": "2224320"
  },
  {
    "text": "rather than ACLs and restrict public buckets enables you if you have again if",
    "start": "2224320",
    "end": "2232180"
  },
  {
    "text": "you have cross account access enables you to restrict any access to a specific Amazon s3 bucket to only authorized",
    "start": "2232180",
    "end": "2240250"
  },
  {
    "text": "users in the owning account so other accounts will cross account access no",
    "start": "2240250",
    "end": "2246400"
  },
  {
    "text": "longer are not able to access the data in in Amazon s3 and remember this is your right as an account as a bucket",
    "start": "2246400",
    "end": "2252850"
  },
  {
    "text": "owner you can deny access you can delete objects but you cannot read the contents",
    "start": "2252850",
    "end": "2258730"
  },
  {
    "text": "of the objects but let's stop here and define what is what exactly is public so",
    "start": "2258730",
    "end": "2264850"
  },
  {
    "text": "when we say public ACL or Public Policy what do we mean a public ACL grants permissions to members of the predefined",
    "start": "2264850",
    "end": "2271960"
  },
  {
    "text": "all users or authenticated users groups so if a public if an ACL has grants",
    "start": "2271960",
    "end": "2278890"
  },
  {
    "text": "permission stood any of those groups it's a publication so it gets treated accordingly by these settings that we",
    "start": "2278890",
    "end": "2285610"
  },
  {
    "text": "talk through public bucket policies it's",
    "start": "2285610",
    "end": "2291880"
  },
  {
    "text": "a little bit convoluted here doesn't grant permissions to only fix varies but the the sense of that is that if you",
    "start": "2291880",
    "end": "2297490"
  },
  {
    "text": "have wildcards if you have permissions if you grant permissions to any two",
    "start": "2297490",
    "end": "2304680"
  },
  {
    "text": "principles or condition elements that have wildcards in them that is considered public by Amazon s3 and",
    "start": "2304680",
    "end": "2310780"
  },
  {
    "text": "therefore it is treated accordingly according to the to the settings that you configured finally for Amazon s3",
    "start": "2310780",
    "end": "2322380"
  },
  {
    "start": "2318000",
    "end": "2407000"
  },
  {
    "text": "there are multiple options to encrypt your data at rest and in transit so with",
    "start": "2322380",
    "end": "2327790"
  },
  {
    "text": "Amazon s3 you have the ability to encrypt your data client-side if you want to or server-side it's really up to",
    "start": "2327790",
    "end": "2335170"
  },
  {
    "text": "you you know where you want to encrypt your data some customers do to compliance reasons would have to encrypt",
    "start": "2335170",
    "end": "2341410"
  },
  {
    "text": "the data before it even leaves their data centers let's say so they use",
    "start": "2341410",
    "end": "2348130"
  },
  {
    "text": "client-side encryption or you can use server-side encryption but more importantly than where you encrypt your",
    "start": "2348130",
    "end": "2353710"
  },
  {
    "text": "data is how you manage your keys and whether you want to manage them and here and here Amazon s3 provides you",
    "start": "2353710",
    "end": "2359650"
  },
  {
    "text": "with multiple options you can manage your own encryption keys in your own identity",
    "start": "2359650",
    "end": "2364960"
  },
  {
    "text": "sorry key management infrastructure KMI and use and so manage your master keys",
    "start": "2364960",
    "end": "2371230"
  },
  {
    "text": "master encryption keys or you can use AWS key management service which by the",
    "start": "2371230",
    "end": "2377079"
  },
  {
    "text": "way integrates with 52 different AWS services so it's quite integrated with",
    "start": "2377079",
    "end": "2382660"
  },
  {
    "text": "the AWS ecosystem of services including services that are relevant to data Lake",
    "start": "2382660",
    "end": "2388480"
  },
  {
    "text": "such as Amazon athena AWS blue and red shift spectrum so all of those support",
    "start": "2388480",
    "end": "2394059"
  },
  {
    "text": "arabesque ems and you still have full control of your keys you can import keys delete keys rotate those keys so i have",
    "start": "2394059",
    "end": "2401890"
  },
  {
    "text": "full control over your keys now let's",
    "start": "2401890",
    "end": "2408970"
  },
  {
    "start": "2407000",
    "end": "2413000"
  },
  {
    "text": "move on to metadata security which is the other aspect of the atelic security and metadata is really data about your",
    "start": "2408970",
    "end": "2416799"
  },
  {
    "start": "2413000",
    "end": "2473000"
  },
  {
    "text": "datasets it's its it includes things",
    "start": "2416799",
    "end": "2421930"
  },
  {
    "text": "such as it's mainly the schema that you overlay on top of your unstructured data and Amazon s3 if you have CSV files JSON",
    "start": "2421930",
    "end": "2429640"
  },
  {
    "text": "files so this includes things such as database definitions and table definitions including column names",
    "start": "2429640",
    "end": "2436240"
  },
  {
    "text": "column data types and so on an AWS Glu provides you with a fully managed and",
    "start": "2436240",
    "end": "2442569"
  },
  {
    "text": "persistent metadata store that you can",
    "start": "2442569",
    "end": "2448089"
  },
  {
    "text": "use for your data like this meta store is this metadata store is hive with a",
    "start": "2448089",
    "end": "2454299"
  },
  {
    "text": "store compatible so you can use all of the Hadoop ecosystem tools that you that you are ready familiar with so sparks",
    "start": "2454299",
    "end": "2460599"
  },
  {
    "text": "equal presto hive and so on also Amazon it AWS services such as Athena glue and",
    "start": "2460599",
    "end": "2469029"
  },
  {
    "text": "redshift spectrum integrate with the AWS glue data catalog now here are some of",
    "start": "2469029",
    "end": "2475839"
  },
  {
    "start": "2473000",
    "end": "2579000"
  },
  {
    "text": "the key things that we see our customers do when it comes to securing and",
    "start": "2475839",
    "end": "2480940"
  },
  {
    "text": "governing their metadata stores so that the first one is very popular we often",
    "start": "2480940",
    "end": "2486700"
  },
  {
    "text": "here our customers say that they want to build a central meter store or a centralized data catalog and",
    "start": "2486700",
    "end": "2494310"
  },
  {
    "text": "this is if you think about it it makes sense because not only does it provide",
    "start": "2494310",
    "end": "2499630"
  },
  {
    "text": "like a single source of truth for your metadata and all of your applications it",
    "start": "2499630",
    "end": "2504640"
  },
  {
    "text": "is also more it also simplifies governance and security for your data catalog and think about it if you",
    "start": "2504640",
    "end": "2513970"
  },
  {
    "text": "centralize it with a catalog you are effectively having a shared resource across all of your teams and therefore",
    "start": "2513970",
    "end": "2519700"
  },
  {
    "text": "you need fine-grained access control so AWS glue provides you with these fine-grained access controls you can",
    "start": "2519700",
    "end": "2526330"
  },
  {
    "text": "control access to individual elements of your data schema databases tables and",
    "start": "2526330",
    "end": "2534040"
  },
  {
    "text": "even more kinds of objects such as connections user-defined functions and",
    "start": "2534040",
    "end": "2539320"
  },
  {
    "text": "so on so AWS glue supports this kind of fine-grained access control finally the",
    "start": "2539320",
    "end": "2546250"
  },
  {
    "text": "AWS data glue data catalog supports encryption encryption of metadata and this is also one of the asks that we",
    "start": "2546250",
    "end": "2552370"
  },
  {
    "text": "hear from our customers who have certain compliance requirements where they need to encrypt such metadata and AWS glue",
    "start": "2552370",
    "end": "2559240"
  },
  {
    "text": "data catalog supports supports that not only does it support encryption of the",
    "start": "2559240",
    "end": "2564910"
  },
  {
    "text": "your metadata and the data catalog it also supports encryption of data that it",
    "start": "2564910",
    "end": "2570490"
  },
  {
    "text": "reads and writes to Amazon s3 in addition to any cloud watch logs that are generated by your AWS glue jobs so",
    "start": "2570490",
    "end": "2581410"
  },
  {
    "start": "2579000",
    "end": "2643000"
  },
  {
    "text": "here are two examples just to show you that controlling access for the AWS glue",
    "start": "2581410",
    "end": "2587560"
  },
  {
    "text": "data catalog is users really familiar I am policy language that you already",
    "start": "2587560",
    "end": "2592960"
  },
  {
    "text": "know so there is nothing new there this is an example of providing cross account access where you have a catalog in an",
    "start": "2592960",
    "end": "2600400"
  },
  {
    "text": "account a and you have a certain user in account B so here you are combining resaw a resource based policy so on the",
    "start": "2600400",
    "end": "2608860"
  },
  {
    "text": "photo for the account a this is a resource based policy that you attach to your AWS glue data catalog and then we",
    "start": "2608860",
    "end": "2616300"
  },
  {
    "text": "provide permissions to the principal which identifies the user Bob and account B and we also can I can against",
    "start": "2616300",
    "end": "2623950"
  },
  {
    "text": "which resources the Bob has access to on the other hand side I mean for cross",
    "start": "2623950",
    "end": "2629980"
  },
  {
    "text": "account access of course we need permissions on both ends in a county and account B so an account B we use regular",
    "start": "2629980",
    "end": "2636430"
  },
  {
    "text": "user policies to establish those permissions this is another example",
    "start": "2636430",
    "end": "2646930"
  },
  {
    "text": "where you have let's for this specific example we have development team and we",
    "start": "2646930",
    "end": "2654010"
  },
  {
    "text": "need to provide fine-grained access to different elements of a single catalog",
    "start": "2654010",
    "end": "2659550"
  },
  {
    "text": "so for instance the the example on the left here we are providing only read",
    "start": "2659550",
    "end": "2666550"
  },
  {
    "text": "access to tables that begin with the prefix prod underscore so production",
    "start": "2666550",
    "end": "2672550"
  },
  {
    "text": "tables the dev team only has the ability to read those tables and query those things but not modify those tables on",
    "start": "2672550",
    "end": "2680890"
  },
  {
    "text": "the right-hand side we have a set of permissions which give the developer team full access to development tables",
    "start": "2680890",
    "end": "2688900"
  },
  {
    "text": "so again just a simple example to show you how simple it is to have fine-grained access for AWS glue we",
    "start": "2688900",
    "end": "2699700"
  },
  {
    "start": "2696000",
    "end": "2770000"
  },
  {
    "text": "thought of include including this just because it represents a nice summary of you know the capabilities of the",
    "start": "2699700",
    "end": "2705910"
  },
  {
    "text": "different services just to get a big picture view of that and specifically I",
    "start": "2705910",
    "end": "2712510"
  },
  {
    "text": "want to call out the compliance for all of these services so all of these",
    "start": "2712510",
    "end": "2718210"
  },
  {
    "text": "services are HIPAA eligible and me ephedrine FedRAMP compliance for EMR but also I mean of course we cannot include",
    "start": "2718210",
    "end": "2724570"
  },
  {
    "text": "all of the compliance compliance is for those services here so you know please",
    "start": "2724570",
    "end": "2730210"
  },
  {
    "text": "visit the AWS compliance website and make sure to check you know for your particular compliance regime that you're",
    "start": "2730210",
    "end": "2738370"
  },
  {
    "text": "interested in and it's worth concluding",
    "start": "2738370",
    "end": "2744190"
  },
  {
    "text": "this section by saying that AWS has the most the broadest and most comprehensive regulatory compliance program across all",
    "start": "2744190",
    "end": "2751660"
  },
  {
    "text": "cloud providers so things such as ISO PCI compliance GDP",
    "start": "2751660",
    "end": "2758080"
  },
  {
    "text": "compliance also when it comes to HIPAA compliance and public sector compliance so again please check out our website",
    "start": "2758080",
    "end": "2764620"
  },
  {
    "text": "for any of those programs if you have certain interest now that concludes this",
    "start": "2764620",
    "end": "2772420"
  },
  {
    "text": "part thank you I want to wrap up the",
    "start": "2772420",
    "end": "2777970"
  },
  {
    "text": "session with a little more information about a service that you might have heard at the keynote yesterday in Andy's",
    "start": "2777970",
    "end": "2785590"
  },
  {
    "text": "keynote so data lakes are the most popular architectures that come up many",
    "start": "2785590",
    "end": "2791770"
  },
  {
    "text": "organizations are building today and you might have heard about the many of the",
    "start": "2791770",
    "end": "2798030"
  },
  {
    "text": "cautions and challenges that we discussed today that customers are",
    "start": "2798030",
    "end": "2803110"
  },
  {
    "text": "facing as they embark on this journey and so it appears Lake formation is a",
    "start": "2803110",
    "end": "2808900"
  },
  {
    "text": "service that will help with addressing some of these topics that we discussed",
    "start": "2808900",
    "end": "2815080"
  },
  {
    "text": "today for example did the lake formation is a service that lets you quickly build",
    "start": "2815080",
    "end": "2821680"
  },
  {
    "text": "data lakes you can move store catalog and clean your data faster it is very easy to get",
    "start": "2821680",
    "end": "2829030"
  },
  {
    "text": "started you can add the connection information from your data data stores you want to move the data from or point",
    "start": "2829030",
    "end": "2835360"
  },
  {
    "text": "lake formation to the data you already ingested through Kinesis or you can",
    "start": "2835360",
    "end": "2840730"
  },
  {
    "text": "identify a data from an AWS data database and then lake formation will automatically crawl and identify the",
    "start": "2840730",
    "end": "2847660"
  },
  {
    "text": "layout of the data that you have in the data stores you can then train lake",
    "start": "2847660",
    "end": "2854050"
  },
  {
    "text": "formation using machine learning to clean and prepare this data to start",
    "start": "2854050",
    "end": "2860410"
  },
  {
    "text": "training you could provide examples of what you would like your data to look like after it's been clean for example",
    "start": "2860410",
    "end": "2868150"
  },
  {
    "text": "you can train lake formation the locations in a commercial insurance",
    "start": "2868150",
    "end": "2874210"
  },
  {
    "text": "database this is just an example this training process can be asked with us 15",
    "start": "2874210",
    "end": "2879460"
  },
  {
    "text": "minutes and inside Amazon the same technology is used to D dupe and match",
    "start": "2879460",
    "end": "2884740"
  },
  {
    "text": "records for your database once you have the you can also easily secure access using",
    "start": "2884740",
    "end": "2892630"
  },
  {
    "text": "a centralized dashboard now one of the problems that we have seen today is as",
    "start": "2892630",
    "end": "2898089"
  },
  {
    "text": "as motels alluded to earlier fine-grained access control is a complex setup especially if you have different",
    "start": "2898089",
    "end": "2906220"
  },
  {
    "text": "analytic tools that you are using for example if you are using EMR you can enforce MRFs authorization for your",
    "start": "2906220",
    "end": "2913660"
  },
  {
    "text": "datasets and s3 but you also need a tool like Ranger to impose fine-grained",
    "start": "2913660",
    "end": "2919720"
  },
  {
    "text": "access control with redshift you have to do something else and it's athina something else so in order to address",
    "start": "2919720",
    "end": "2926470"
  },
  {
    "text": "all these issues we lake formation attempts to centrally define your table",
    "start": "2926470",
    "end": "2932859"
  },
  {
    "text": "and column level data access and enforce it across all the tools all you need is",
    "start": "2932859",
    "end": "2938650"
  },
  {
    "text": "a user that can authenticate to Lake formation and based on that user's",
    "start": "2938650",
    "end": "2945809"
  },
  {
    "text": "authorization level he or she should be able to access the data as assets they",
    "start": "2945809",
    "end": "2951640"
  },
  {
    "text": "are allowed to and then you can also use the data catalog in lake formation to search and find relevant data sets and",
    "start": "2951640",
    "end": "2958660"
  },
  {
    "text": "share them across multiple users and accounts so these are the primary features that are built into this new",
    "start": "2958660",
    "end": "2965710"
  },
  {
    "text": "service and here's a pictorial representation of how it looks like and",
    "start": "2965710",
    "end": "2970839"
  },
  {
    "start": "2967000",
    "end": "3149000"
  },
  {
    "text": "on the AWS console the first step or the stage is creating a data Lake and this",
    "start": "2970839",
    "end": "2976299"
  },
  {
    "text": "can be accomplished either by launching a crawler and crawling the data or importing data and in stage 2 you define",
    "start": "2976299",
    "end": "2984099"
  },
  {
    "text": "table level permissions or user level permissions for your glue catalog and in",
    "start": "2984099",
    "end": "2991210"
  },
  {
    "text": "stage 3 you can search the data catalog add any additional metadata that you",
    "start": "2991210",
    "end": "2997029"
  },
  {
    "text": "want for your data sets and finally you can also monitor and audit your data",
    "start": "2997029",
    "end": "3004109"
  },
  {
    "text": "assets using a central unified location so this is so stay tuned for more",
    "start": "3004109",
    "end": "3011220"
  },
  {
    "text": "updates on daedalic formation through our blogs and web site and certainly",
    "start": "3011220",
    "end": "3017730"
  },
  {
    "text": "provide us your feedback on how the session was and we are happy to take some questions have",
    "start": "3017730",
    "end": "3023930"
  },
  {
    "text": "any [Applause]",
    "start": "3023930",
    "end": "3035030"
  },
  {
    "text": "sure the question is how did we choose or why did we choose HBase and one of",
    "start": "3052830",
    "end": "3058500"
  },
  {
    "text": "the first architectural patterns that have shown why not DynamoDB so dynamodb",
    "start": "3058500",
    "end": "3063540"
  },
  {
    "text": "is a great service it's a key value store but DynamoDB also has payload size",
    "start": "3063540",
    "end": "3069180"
  },
  {
    "text": "limitation so you can't have more than 250 KB of payload and it's essentially",
    "start": "3069180",
    "end": "3075600"
  },
  {
    "text": "for smaller data items we are dealing with big data here and for big data and",
    "start": "3075600",
    "end": "3082110"
  },
  {
    "text": "for faster for data on which you want to perform periodic updates you would want",
    "start": "3082110",
    "end": "3089550"
  },
  {
    "text": "to use a tool like HBase or red shell",
    "start": "3089550",
    "end": "3093290"
  },
  {
    "text": "yeah thank you",
    "start": "3099880",
    "end": "3103690"
  },
  {
    "text": "let me know where to stop",
    "start": "3106799",
    "end": "3110520"
  },
  {
    "text": "it takes time to rewind all of this this is the right way yeah I think so",
    "start": "3144510",
    "end": "3151440"
  },
  {
    "start": "3149000",
    "end": "3569000"
  },
  {
    "text": "so the question is so if I have the setup and I write files you know in",
    "start": "3151440",
    "end": "3156510"
  },
  {
    "text": "almost real time maybe seconds minutes and then I have a compaction job to have like a second tier of data and maybe I",
    "start": "3156510",
    "end": "3163380"
  },
  {
    "text": "run it hourly right in this case if I want to access data for example it's presto what is the best practice what is",
    "start": "3163380",
    "end": "3169920"
  },
  {
    "text": "the recommendation to not just get you know the tier 2 data because I am obviously interested in real-time right",
    "start": "3169920",
    "end": "3176369"
  },
  {
    "text": "I don't want to wait for an hour to get the results back right so how do I query sort of both tiers at the same time",
    "start": "3176369",
    "end": "3183450"
  },
  {
    "text": "assuming the schema is the same yeah I mean you can't definitely define tables",
    "start": "3183450",
    "end": "3188579"
  },
  {
    "text": "over raw data as well it's just a matter of crawling through blue catalog or defining a DDL and Athena or even",
    "start": "3188579",
    "end": "3195210"
  },
  {
    "text": "through presto running on iam are you could still define I mean I've seen",
    "start": "3195210",
    "end": "3200609"
  },
  {
    "text": "customers defined raw tables as well as processed tables and which is tier 1 and tier 2 and that is also an essential",
    "start": "3200609",
    "end": "3208319"
  },
  {
    "text": "step for example you want to explore data and in terms of what it has for",
    "start": "3208319",
    "end": "3214529"
  },
  {
    "text": "machine learning or what you want extract essentially you can still create a table some tier 1 and query just write",
    "start": "3214529",
    "end": "3223440"
  },
  {
    "text": "what Radhika said you don't even need to crawl the data if it's all about new files you just define the schema once then you can query that it as it comes",
    "start": "3223440",
    "end": "3230309"
  },
  {
    "text": "in into a stream so yes so my use case is more about what if the data format is",
    "start": "3230309",
    "end": "3235349"
  },
  {
    "text": "the same let's say it's already processed so I really just want to compact it and I don't want to submit to",
    "start": "3235349",
    "end": "3240420"
  },
  {
    "text": "queries all the time so is it is it something like a view that I should use or some way to join it somewhere not",
    "start": "3240420",
    "end": "3247559"
  },
  {
    "text": "necessary if you already are already getting processed data then essentially",
    "start": "3247559",
    "end": "3253079"
  },
  {
    "text": "you're eliminating Tier one I mean your tier 1 and tier 2 are the same right",
    "start": "3253079",
    "end": "3258299"
  },
  {
    "text": "you can have one table created over your single tier and have queries against",
    "start": "3258299",
    "end": "3264839"
  },
  {
    "text": "that okay I will probably have some",
    "start": "3264839",
    "end": "3271260"
  },
  {
    "text": "potentially issues on a3 with the consistency when I override files but",
    "start": "3271260",
    "end": "3277360"
  },
  {
    "text": "that's a separate discussion yes that's the reason I mean you want to keep the raw data as ease but if you don't want",
    "start": "3277360",
    "end": "3284110"
  },
  {
    "text": "to do our lick of passion you can do every 15 minutes every 5 minutes as well it's not necessary that was for a",
    "start": "3284110",
    "end": "3291730"
  },
  {
    "text": "general bache scenario that I described but you could do it on a periodic basis",
    "start": "3291730",
    "end": "3297310"
  },
  {
    "text": "using Spock streaming as well and have it produce your process data at a more",
    "start": "3297310",
    "end": "3303250"
  },
  {
    "text": "faster pace we have seen faster ingestion right you can do faster processing yes Val using flink on EMR",
    "start": "3303250",
    "end": "3309190"
  },
  {
    "text": "and have that data available for queried",
    "start": "3309190",
    "end": "3313560"
  },
  {
    "text": "so a quick question concerning firehose right now when you're when firehose",
    "start": "3318660",
    "end": "3324940"
  },
  {
    "text": "streams in s3 it partitions by light yes I think they you know what I'm talking",
    "start": "3324940",
    "end": "3333130"
  },
  {
    "text": "about is there something in the pipeline to allow us to specify how we want the partitions to be stay tuned for that",
    "start": "3333130",
    "end": "3341280"
  },
  {
    "text": "stay tuned that's that's a very common request because it's not in hive compatible format you can hide yeah and",
    "start": "3341520",
    "end": "3348820"
  },
  {
    "text": "that's a request that we got from many many customers and it's been actively worked on it was for a because right now",
    "start": "3348820",
    "end": "3356320"
  },
  {
    "text": "we have to run a job right for now you have to run a job a glue job that can",
    "start": "3356320",
    "end": "3361810"
  },
  {
    "text": "simply or even a patent script that can simply take it and change the folder or",
    "start": "3361810",
    "end": "3367990"
  },
  {
    "text": "the prefix name yeah alright thank you",
    "start": "3367990",
    "end": "3372420"
  },
  {
    "text": "hi in one of the previous slide before this if you can go back I just want to",
    "start": "3374130",
    "end": "3380680"
  },
  {
    "text": "ask a question let me know where to stop",
    "start": "3380680",
    "end": "3389920"
  },
  {
    "text": "yeah I think it was one of the first one two slides where you talked about",
    "start": "3389920",
    "end": "3396990"
  },
  {
    "text": "federating the Presto layer a query layer",
    "start": "3396990",
    "end": "3403110"
  },
  {
    "text": "can you talk a little bit about that I was not sure what you meant by that",
    "start": "3419280",
    "end": "3424650"
  },
  {
    "text": "when you say single-serving lair yes so",
    "start": "3424650",
    "end": "3430560"
  },
  {
    "text": "for data sitting in as when you have data that is arriving in through Kinesis",
    "start": "3430560",
    "end": "3436680"
  },
  {
    "text": "claims processed by spark streaming you could query the data using presto like",
    "start": "3436680",
    "end": "3444960"
  },
  {
    "text": "you could push it back to s3 and have it being queried by athena as well but in",
    "start": "3444960",
    "end": "3451260"
  },
  {
    "text": "this situation you could use presto as well running on anymore this is just an option yes because the reason I'm asking",
    "start": "3451260",
    "end": "3457859"
  },
  {
    "text": "is we have exactly the same setup where we have presto near mark but my question",
    "start": "3457859",
    "end": "3463050"
  },
  {
    "text": "is more of towards the security aspect the authorization part where and how do",
    "start": "3463050",
    "end": "3468200"
  },
  {
    "text": "you suggest we do the authorization part in this scenario authorization for",
    "start": "3468200",
    "end": "3475310"
  },
  {
    "text": "grading right at the table levels column level or at a row level I mean which one",
    "start": "3475310",
    "end": "3481800"
  },
  {
    "text": "are you looking at side table table 11 yeah table level okay so one of the",
    "start": "3481800",
    "end": "3487589"
  },
  {
    "text": "things like I could tell you is because glue today supports fine-grained access",
    "start": "3487589",
    "end": "3493589"
  },
  {
    "text": "control at the table level if your EMR cluster has glue catalog defined when",
    "start": "3493589",
    "end": "3501150"
  },
  {
    "text": "you're launching the cluster you can specify glue as a data catalog and when you specify that hive spark and presto",
    "start": "3501150",
    "end": "3508619"
  },
  {
    "text": "can leverage blue catalog and so a user who is using your presto interface goes",
    "start": "3508619",
    "end": "3517520"
  },
  {
    "text": "the authorization that are available to him the security policies apply to that",
    "start": "3517520",
    "end": "3523950"
  },
  {
    "text": "user when he is accessing the glue data catalog it doesn't apply to hive meta store it applies to blue kit data can I",
    "start": "3523950",
    "end": "3530190"
  },
  {
    "text": "have we have hive meta stone right now okay so for hive unfortunately there is",
    "start": "3530190",
    "end": "3536030"
  },
  {
    "text": "no good solution I would say because there is Ranger that can do fine-grained",
    "start": "3536030",
    "end": "3542099"
  },
  {
    "text": "access control but only only for hive and HBase but is not for pests or",
    "start": "3542099",
    "end": "3547890"
  },
  {
    "text": "injured they have a trust to plugin coming but I don't know when it will be",
    "start": "3547890",
    "end": "3553600"
  },
  {
    "text": "available thank you so we are happy to take your questions",
    "start": "3553600",
    "end": "3559720"
  },
  {
    "text": "of the stage we have we're gonna get it we have time we can take yeah we can",
    "start": "3559720",
    "end": "3565420"
  },
  {
    "text": "take the questions here thank you very much for you thank you",
    "start": "3565420",
    "end": "3570600"
  }
]