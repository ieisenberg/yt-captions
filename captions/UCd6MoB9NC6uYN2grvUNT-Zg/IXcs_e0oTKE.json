[
  {
    "start": "0",
    "end": "38000"
  },
  {
    "text": "okay great so I usually like to start out these presentations just with a simple question who has used the",
    "start": "860",
    "end": "7470"
  },
  {
    "text": "streaming data product before just to show enhance wonderful who has used",
    "start": "7470",
    "end": "12570"
  },
  {
    "text": "Amazon Kinesis any of the three services before wonderful what brings a smile to",
    "start": "12570",
    "end": "19320"
  },
  {
    "text": "my face so much is that every year we give these presentations and the percentage of the crowd that we see you",
    "start": "19320",
    "end": "25980"
  },
  {
    "text": "that I've used the streaming data product anything between Apache Kafka Amazon Kinesis spark streaming",
    "start": "25980",
    "end": "32450"
  },
  {
    "text": "continually goes up and the number of customers using Amazon Kinesis also continues to grow up so it's great to see so for those unfamiliar with",
    "start": "32450",
    "end": "40800"
  },
  {
    "start": "38000",
    "end": "136000"
  },
  {
    "text": "streaming data it's there's one primary difference between batch data processing",
    "start": "40800",
    "end": "46379"
  },
  {
    "text": "and stream data processing and what I like to think of it is stream data processing is continuous and you usually",
    "start": "46379",
    "end": "53550"
  },
  {
    "text": "do it in real time so that's what makes upstream data processing so whereas with",
    "start": "53550",
    "end": "58710"
  },
  {
    "text": "batch data processing you might be collecting logs on your application server and then you run a cron job once",
    "start": "58710",
    "end": "64588"
  },
  {
    "text": "an hour to spin up and send those servers to durable storage like Amazon s3 with stream data processing you're",
    "start": "64589",
    "end": "71760"
  },
  {
    "text": "typically continuously writing data to a streaming service like Amazon Kinesis so",
    "start": "71760",
    "end": "77939"
  },
  {
    "text": "it's one of the primary differences that the continuous nature versus the periodic nature of batch the other is",
    "start": "77939",
    "end": "83880"
  },
  {
    "text": "the frequency so when you're doing things continuously you're doing typically acting upon a very large",
    "start": "83880",
    "end": "89579"
  },
  {
    "text": "volume of small events so in the in the example that I just provided your server",
    "start": "89579",
    "end": "96030"
  },
  {
    "text": "wakes up every hour you've capably grabbing a much larger file say a megabyte 10 megabytes a hundred",
    "start": "96030",
    "end": "102180"
  },
  {
    "text": "megabytes we're in the Giga bytes to persist that data Tammuz on s3 whereas with Kinesis you're acting along",
    "start": "102180",
    "end": "108329"
  },
  {
    "text": "these typically smaller events so a 1 KB event that you're continuously writing",
    "start": "108329",
    "end": "113369"
  },
  {
    "text": "to a stream you're aggregating that data and then writing that data say to a persistent store like Amazon s3 perhaps",
    "start": "113369",
    "end": "120149"
  },
  {
    "text": "performing continuous processing like computing the average number of API",
    "start": "120149",
    "end": "126000"
  },
  {
    "text": "calls for one of your public facing services but that's the frequency really really is a differentiator between the",
    "start": "126000",
    "end": "135110"
  },
  {
    "start": "136000",
    "end": "212000"
  },
  {
    "text": "so why do people use streaming dinner there are several reasons but one of the examples that we like to point out for",
    "start": "136140",
    "end": "142210"
  },
  {
    "text": "as an Amazon Kinesis team is that data loses value over time and the simplest example of this is say fraud detection",
    "start": "142210",
    "end": "149950"
  },
  {
    "text": "so detecting fraud typically when you have a credit you say you visit go",
    "start": "149950",
    "end": "156520"
  },
  {
    "text": "overseas from your home country you charge go to a restaurant charge your",
    "start": "156520",
    "end": "161980"
  },
  {
    "text": "card and you get an instant notification from many of your banks telling you hey we've detected that your your recent",
    "start": "161980",
    "end": "169720"
  },
  {
    "text": "charge was not typical that's a real-time alert and that matters a lot but if someone did take",
    "start": "169720",
    "end": "175330"
  },
  {
    "text": "your card and is using it in some overseas country from your own country you want that information right away",
    "start": "175330",
    "end": "181230"
  },
  {
    "text": "being told an hour later a week later or a month later perhaps in a monthly",
    "start": "181230",
    "end": "186640"
  },
  {
    "text": "report that the bank has detected some fraud is not after many many charges of incurs not something that is not nearly",
    "start": "186640",
    "end": "193810"
  },
  {
    "text": "as valuable as doing so in real-time so the a lot of customers move to streaming",
    "start": "193810",
    "end": "199810"
  },
  {
    "text": "data so that they can get data into their data warehouses in minutes instead of hours to react to that data in real",
    "start": "199810",
    "end": "206350"
  },
  {
    "text": "time and Amazon Kinesis provides the mechanism to help do that so we're going",
    "start": "206350",
    "end": "214360"
  },
  {
    "start": "212000",
    "end": "277000"
  },
  {
    "text": "to go through a lot of different use cases in this presentation but they all follow a very typical pattern and that",
    "start": "214360",
    "end": "221110"
  },
  {
    "text": "is you've got a data producer some sort of streaming service some buffer and",
    "start": "221110",
    "end": "226180"
  },
  {
    "text": "then in the middle that's receiving data from that data producer and then a data",
    "start": "226180",
    "end": "232000"
  },
  {
    "text": "consumer that's consuming data in real-time or near real-time off of that streaming service so you'll",
    "start": "232000",
    "end": "238510"
  },
  {
    "text": "see this pattern over and over again but this is when we talk about I say mention the word continuous think about the data",
    "start": "238510",
    "end": "244840"
  },
  {
    "text": "producer continuously writing to a streaming service a streaming server is continuously ingested and durably",
    "start": "244840",
    "end": "250480"
  },
  {
    "text": "storing that data that's received from the data producer and then a data consumer reacting to that data",
    "start": "250480",
    "end": "257290"
  },
  {
    "text": "processing that data in real-time and doing something with something with it and that's something could be as simple as I'm buffering it learning Sybil ETL",
    "start": "257290",
    "end": "265300"
  },
  {
    "text": "and durably during it to Amazon s3 and he can get as complicated or sophisticated as I'm",
    "start": "265300",
    "end": "270620"
  },
  {
    "text": "running real time machine learning algorithms to detect anomalies so Amazon",
    "start": "270620",
    "end": "278960"
  },
  {
    "start": "277000",
    "end": "419000"
  },
  {
    "text": "Kinesis is made up of three services Amazon Kinesis data streams data analytics and data firehose data streams",
    "start": "278960",
    "end": "286669"
  },
  {
    "text": "was the first service that we lost we launched it several years ago at AWS reinvent and it provides sort of the",
    "start": "286669",
    "end": "292070"
  },
  {
    "text": "core infrastructure that other services build upon or attach to you'll build a",
    "start": "292070",
    "end": "297620"
  },
  {
    "text": "custom data producer that writes data to this stream continuously we have provide a number of libraries to do so you can",
    "start": "297620",
    "end": "303560"
  },
  {
    "text": "use everything from the AWS SDKs to call put to write data to a stream and we offer a number of libraries both",
    "start": "303560",
    "end": "310250"
  },
  {
    "text": "open-source as well as services like AWS lambda and Kinesis data analytics to consume data off the stream follows that",
    "start": "310250",
    "end": "316940"
  },
  {
    "text": "same pattern that I described in the last slide on the far side Amazon can",
    "start": "316940",
    "end": "323479"
  },
  {
    "text": "uses data fires was the second service we launched and we launched it right around two years ago and it takes one of",
    "start": "323479",
    "end": "328669"
  },
  {
    "text": "the most common use cases for streaming get it makes it dead simple and that is I'm just trying to get a high volume",
    "start": "328669",
    "end": "334610"
  },
  {
    "text": "stream of events stored durably to one of my persistent stores like Amazon s3",
    "start": "334610",
    "end": "339740"
  },
  {
    "text": "Amazon redshift or Amazon Elastic search service with as little effort as possible so with firehose you create a delivery",
    "start": "339740",
    "end": "346970"
  },
  {
    "text": "stream you set up a number of configurations and I'll go through some of them later in the deck and then after",
    "start": "346970",
    "end": "353870"
  },
  {
    "text": "that your data producers just rights to the stream and will deliver that data to you with basically no operations and",
    "start": "353870",
    "end": "359120"
  },
  {
    "text": "completely service so a very common use case many many customers use Kinesis data streams in conjunction with Kinesis",
    "start": "359120",
    "end": "366500"
  },
  {
    "text": "data firehose so very a few customers use one or the other but we typically see customers using both finally the",
    "start": "366500",
    "end": "374389"
  },
  {
    "text": "service we most recently launched was a little less than two years ago Amazon Kinesis data analytics so about a year",
    "start": "374389",
    "end": "380389"
  },
  {
    "text": "and a half ago at the New York City summit last year cases data analytics is a managed stream consumer so we think",
    "start": "380389",
    "end": "386630"
  },
  {
    "text": "about that pattern that I brought up on the far end it continuously reads data from either an Amazon can uses data",
    "start": "386630",
    "end": "392780"
  },
  {
    "text": "stream or an amazon can uses data firehose and processes that data using sequel code so you set up a",
    "start": "392780",
    "end": "400160"
  },
  {
    "text": "streaming source specify your sequel code to do things like compute the average number of viewers on your",
    "start": "400160",
    "end": "407060"
  },
  {
    "text": "website or the leaderboard for your mobile application and it makes it very very simple to do that in a managed",
    "start": "407060",
    "end": "413450"
  },
  {
    "text": "service and elastic application so let's",
    "start": "413450",
    "end": "420710"
  },
  {
    "start": "419000",
    "end": "501000"
  },
  {
    "text": "go through data streams a little bit more detail thisis data streams one of",
    "start": "420710",
    "end": "426200"
  },
  {
    "text": "the primary value props of it is that it's you can build a real-time application and the framework of your choice again the pattern that I showed",
    "start": "426200",
    "end": "433760"
  },
  {
    "text": "them the first couple slides prior imagine you're capturing data from either clickstream data from your mobile website or your mobile app IOT data from",
    "start": "433760",
    "end": "442070"
  },
  {
    "text": "your iot sensors into a konista stream and then you get to choose one or more",
    "start": "442070",
    "end": "447290"
  },
  {
    "text": "of the consumers of your choice so the three most the four most popular is",
    "start": "447290",
    "end": "452450"
  },
  {
    "text": "shown on this slide here in order of popularity we provide an open source",
    "start": "452450",
    "end": "457910"
  },
  {
    "text": "library called the Kinesis client library which allows you to run custom code on ec2 AWS lambda and AW lambda",
    "start": "457910",
    "end": "466010"
  },
  {
    "text": "function allows you to execute a piece of Python Java or a number of other languages code in a service framework",
    "start": "466010",
    "end": "472070"
  },
  {
    "text": "reading directly off of the stream very useful for stateless type of processing Canisius analytics which I just",
    "start": "472070",
    "end": "479690"
  },
  {
    "text": "mentioned allows you to write sequel over the stream very useful for stateful processing so doing long-running window",
    "start": "479690",
    "end": "485030"
  },
  {
    "text": "types of operators and then spark streaming on EMR which is an open source framework that allows you to do a it's a",
    "start": "485030",
    "end": "491660"
  },
  {
    "text": "plethora of things such as simple ETL to machine learning to even more",
    "start": "491660",
    "end": "497660"
  },
  {
    "text": "complicated analytics so Kinesis data",
    "start": "497660",
    "end": "503180"
  },
  {
    "start": "501000",
    "end": "656000"
  },
  {
    "text": "analytics R is the service that I mentioned allows you write sequel over streaming data the primary value prop of",
    "start": "503180",
    "end": "511670"
  },
  {
    "text": "this is that it's really really easy to get started to build fairly powerful real-time applications so you can set up",
    "start": "511670",
    "end": "517940"
  },
  {
    "text": "a POC to read data off of a stream compute some useful information we talked about that time to value the",
    "start": "517940",
    "end": "524330"
  },
  {
    "text": "value of the time value of data over time and it allows you to get that",
    "start": "524330",
    "end": "529610"
  },
  {
    "text": "information very very quickly so there's two very common or use cases that we see with data analytics the first is aggregation in",
    "start": "529610",
    "end": "536540"
  },
  {
    "text": "front of a database and the other is responsive analytics and we'll go over then later on the exactly a little bit",
    "start": "536540",
    "end": "541880"
  },
  {
    "text": "more in detail the aggregation for a database is one of the simplest ones to understand and it's simple I either have",
    "start": "541880",
    "end": "547820"
  },
  {
    "text": "like an operational database that it could be a no sequel database and my sequel database of Redis cluster and I",
    "start": "547820",
    "end": "553279"
  },
  {
    "text": "have a stream of events that's coming in at a thousand ten thousand a hundred thousand a million events per second and",
    "start": "553279",
    "end": "559040"
  },
  {
    "text": "I want to get turned that raw stream of events into useful information to feed",
    "start": "559040",
    "end": "564079"
  },
  {
    "text": "my lab applications so you'll turn that hundred thousand events per second into",
    "start": "564079",
    "end": "569390"
  },
  {
    "text": "ten key data per second it could be the top ten users it could be things like",
    "start": "569390",
    "end": "575440"
  },
  {
    "text": "what's the last known state per user and then I'm writing to my operational data",
    "start": "575440",
    "end": "580490"
  },
  {
    "text": "store that's feeding my live applications at the rate of 10 per second versus a hundred thousand per second the same sort of scenario is done",
    "start": "580490",
    "end": "587300"
  },
  {
    "text": "with reporting databases and data warehouses like Amazon redshift or maybe a my sequel database you're using for",
    "start": "587300",
    "end": "592700"
  },
  {
    "text": "reporting where you're doing a lot of the aggregation up front so you have far fewer if any ETL steps to do before you",
    "start": "592700",
    "end": "599779"
  },
  {
    "text": "present your users with the Coria built able so imagine instead of data hitting as three doing a series of batch ETL",
    "start": "599779",
    "end": "606769"
  },
  {
    "text": "jobs and then finally you get to a Coria before your users and that takes maybe",
    "start": "606769",
    "end": "612920"
  },
  {
    "text": "an hour maybe sometimes even more maybe only doing it once a day the going to the going to a model where you update",
    "start": "612920",
    "end": "619250"
  },
  {
    "text": "that table every 30 seconds or every 10 seconds so they're getting that information there as fast as possible",
    "start": "619250",
    "end": "625630"
  },
  {
    "text": "the other use case is responsive data analytics and this is really reacting to the data in real time and it's less to",
    "start": "625630",
    "end": "631790"
  },
  {
    "text": "go to a durable store but possibly detecting sort of a needle in the haystack finding a particular message of",
    "start": "631790",
    "end": "638269"
  },
  {
    "text": "interest and then sending out several notifications maybe your computing I mentioned API error rates previously",
    "start": "638269",
    "end": "644209"
  },
  {
    "text": "maybe a computing aerates and once they reach a certain threshold you're sending out a notification or a page so this is",
    "start": "644209",
    "end": "650839"
  },
  {
    "text": "reacting the data as quickly as possible to feed your live applications so",
    "start": "650839",
    "end": "657199"
  },
  {
    "start": "656000",
    "end": "757000"
  },
  {
    "text": "Kinesis data fires I mentioned that when you use kunis data fires you create a delivery stream and then after you",
    "start": "657199",
    "end": "663020"
  },
  {
    "text": "create that delivery team all you really worry about is writing the delivery stream and then the data is",
    "start": "663020",
    "end": "668269"
  },
  {
    "text": "delivered without servers without any operations to the destination of your choice so when you configure a firehose",
    "start": "668269",
    "end": "675500"
  },
  {
    "text": "you configure a number of different things the first and perhaps the most important what we call buffering hints",
    "start": "675500",
    "end": "680810"
  },
  {
    "text": "and they're both a time period and a size of how large of data or how often do you want to deliver data to your",
    "start": "680810",
    "end": "687230"
  },
  {
    "text": "destination so Amazon s3 being the simplest example let's just say you",
    "start": "687230",
    "end": "692300"
  },
  {
    "text": "would like data delivered in you know 100 megabyte files every end or every 10",
    "start": "692300",
    "end": "697850"
  },
  {
    "text": "minutes to Amazon s3 and the value prop this provides is that again that hundred",
    "start": "697850",
    "end": "703790"
  },
  {
    "text": "thousand stream of events hundred thousand per second per stream of events your attorney instead of ending up with",
    "start": "703790",
    "end": "709940"
  },
  {
    "text": "a lot of small files that are hard to deal with on Amazon s3 you deal with very nice large chunky files so it",
    "start": "709940",
    "end": "716600"
  },
  {
    "text": "optimized your batch processing Amazon cases data fires is really good for it",
    "start": "716600",
    "end": "722329"
  },
  {
    "text": "if you want to use your existing batch data processing tools you don't really want to do the real-time processing as",
    "start": "722329",
    "end": "728089"
  },
  {
    "text": "much but you want to make it easier and get that data into those tools faster Kinesis data fires is a great choice for",
    "start": "728089",
    "end": "733820"
  },
  {
    "text": "that so aside from buffering hints and other things that you can configure our encryption compression you can even run",
    "start": "733820",
    "end": "741709"
  },
  {
    "text": "inline transformations via AWS lambda so server like a service ETL pipeline",
    "start": "741709",
    "end": "747380"
  },
  {
    "text": "getting data into s3 offloading a bunch of the work cleaning up that data as it",
    "start": "747380",
    "end": "753079"
  },
  {
    "text": "hits your data like so back to Kinesis",
    "start": "753079",
    "end": "760010"
  },
  {
    "start": "757000",
    "end": "805000"
  },
  {
    "text": "data analytics when you create a Kinesis data analytics application and in general most streaming applications",
    "start": "760010",
    "end": "766250"
  },
  {
    "text": "follow this sort of pattern where you connect to a source you write a piece of code in this case it's sequel code and",
    "start": "766250",
    "end": "772640"
  },
  {
    "text": "then you just configure where you continuously deliver their sequel results and the two examples that I",
    "start": "772640",
    "end": "778819"
  },
  {
    "text": "provided previously in the aggregating data before a database or either an",
    "start": "778819",
    "end": "783949"
  },
  {
    "text": "operational or a reporting database you would deliver those results to say on my sequel database at DynamoDB table that",
    "start": "783949",
    "end": "791029"
  },
  {
    "text": "type of thing in the responsive data analytics you would send it to say Amazon SNS maybe even another Kinesis",
    "start": "791029",
    "end": "796370"
  },
  {
    "text": "stream to reactivate it or do further processing before you complete",
    "start": "796370",
    "end": "801930"
  },
  {
    "text": "your processing on that data so that's",
    "start": "801930",
    "end": "807149"
  },
  {
    "start": "805000",
    "end": "831000"
  },
  {
    "text": "enough about just an overview of the services the rest of the presentation I'm basically going to go over some common use cases and we're going to dive",
    "start": "807149",
    "end": "813089"
  },
  {
    "text": "fairly deep into one use case and then what we'll do is we'll end the presentation I'll give you guys the link to download a cloud formation template",
    "start": "813089",
    "end": "819839"
  },
  {
    "text": "that works through one of those use cases that I walk through in very good",
    "start": "819839",
    "end": "825120"
  },
  {
    "text": "detail sets up an end-to-end application that you can run yourself get a real-time dashboard in a couple minutes",
    "start": "825120",
    "end": "831589"
  },
  {
    "start": "831000",
    "end": "950000"
  },
  {
    "text": "so across all streaming data use cases let's look at three common scenarios so",
    "start": "838850",
    "end": "845699"
  },
  {
    "text": "streaming ingest transforming load or streaming ETL this is very similar to",
    "start": "845699",
    "end": "851879"
  },
  {
    "text": "what cases data fire was the primary purpose of it the people also use Kinesis data streams to do it as well",
    "start": "851879",
    "end": "857040"
  },
  {
    "text": "but the primary purpose is you're doing ETL as data is continuously doing ETL",
    "start": "857040",
    "end": "862709"
  },
  {
    "text": "and then doing that prior to the data wherever you're storing the data be it Amazon s3 Amazon redshift it's the most",
    "start": "862709",
    "end": "869279"
  },
  {
    "text": "common use case we see customers implement and it's usually where customers start it probably represents",
    "start": "869279",
    "end": "875970"
  },
  {
    "text": "I'd say 50 percent of use cases of what we see for streaming days just straight streaming ETL the next is continuous",
    "start": "875970",
    "end": "884220"
  },
  {
    "text": "metric generation it probably represents about 30 40 percent and this is getting some insight on the raw stream prior to",
    "start": "884220",
    "end": "891839"
  },
  {
    "text": "either delivery or reacting to it so very common use cases are leaderboards",
    "start": "891839",
    "end": "897089"
  },
  {
    "text": "so I have a you know calculate the number of points per user through a",
    "start": "897089",
    "end": "902220"
  },
  {
    "text": "group rank and then present that back to the users another very common use case",
    "start": "902220",
    "end": "907649"
  },
  {
    "text": "is to do computer metric in the new data enrichment on that metric provide some",
    "start": "907649",
    "end": "912990"
  },
  {
    "text": "context and then pass that event back to users but the point is that you're continuously doing your metric",
    "start": "912990",
    "end": "918899"
  },
  {
    "text": "generation versus doing in batch previously and the last one actionable insights is really reacting to it so",
    "start": "918899",
    "end": "925740"
  },
  {
    "text": "instead of just immediately storing it and then processing later react doing something with that data later your",
    "start": "925740",
    "end": "931379"
  },
  {
    "text": "or reacting to in real-time which is the example I keep on bringing you up about sending notifications pages that type of",
    "start": "931379",
    "end": "936869"
  },
  {
    "text": "thing so we're going to focus on the bottom two I'll covers from screaming",
    "start": "936869",
    "end": "943049"
  },
  {
    "text": "ETL best practices as well but for the use cases that when I walk through we're going to focus on the bottom two so web",
    "start": "943049",
    "end": "951569"
  },
  {
    "start": "950000",
    "end": "1147000"
  },
  {
    "text": "analytics and user boards so here's a common architecture that we see from our customers implement so that they'll have",
    "start": "951569",
    "end": "958169"
  },
  {
    "text": "either a lightweight piece of JavaScript client code running on their browser or mobile app or a web server on Amazon ec2",
    "start": "958169",
    "end": "965419"
  },
  {
    "text": "using Amazon kognito to provide secure temporary credentials and this is what",
    "start": "965419",
    "end": "971309"
  },
  {
    "text": "we in the pattern that we brought up in the very beginning of the presentation this is your data producer this is the",
    "start": "971309",
    "end": "976559"
  },
  {
    "text": "data where you're ingesting the data at the main point where you're writing data to the Kinesis screen",
    "start": "976559",
    "end": "981649"
  },
  {
    "text": "you'll ingest that data to a Kinesis stream the most common approach so when",
    "start": "981649",
    "end": "987449"
  },
  {
    "text": "you write data to be nice' stream there's two primary three primary parameters the stream name so where",
    "start": "987449",
    "end": "993989"
  },
  {
    "text": "you're going to store their events in that temporary buffer that can uses data streams provides the piece of data that",
    "start": "993989",
    "end": "999659"
  },
  {
    "text": "you're sending so this could be an individual event or maybe five or ten of different events for doing micro",
    "start": "999659",
    "end": "1004819"
  },
  {
    "text": "batching writing through that stream and the last thing is the partition key the partition key is a key associated with",
    "start": "1004819",
    "end": "1011059"
  },
  {
    "text": "the data that you're used to word where we you where we used that piece of data",
    "start": "1011059",
    "end": "1016519"
  },
  {
    "text": "to logically determine where it's stored in the Canisius data stream so after",
    "start": "1016519",
    "end": "1022159"
  },
  {
    "text": "ingest the consumer that we have for this use case is an Amazon Kinesis data analytics application this is computing",
    "start": "1022159",
    "end": "1029509"
  },
  {
    "text": "the top ten users so this will be when you look at the code for this will be very very similar to select my number of",
    "start": "1029509",
    "end": "1036409"
  },
  {
    "text": "my key columns my aggregation columns group by user and then there will be a",
    "start": "1036409",
    "end": "1041629"
  },
  {
    "text": "time period associated with it so when you're doing continuous processing one",
    "start": "1041629",
    "end": "1046759"
  },
  {
    "text": "of the primary difference between continuous processing and batch processing is when you do batch processing you are naturally bounding",
    "start": "1046759",
    "end": "1054470"
  },
  {
    "text": "the data on the data set you're operating on so you might have look at a specific s3 bucket or set of s3 buckets",
    "start": "1054470",
    "end": "1061279"
  },
  {
    "text": "and you've done processing when you've finished the end of that data because of the continuous aspect of",
    "start": "1061279",
    "end": "1068990"
  },
  {
    "text": "streaming data you need some way to bound your data processing so computing top 10 users does that mean for all time",
    "start": "1068990",
    "end": "1076040"
  },
  {
    "text": "for the last hour for the last 10 minutes it's usually a time semantic that will bound the data",
    "start": "1076040",
    "end": "1083030"
  },
  {
    "text": "processing you can also use different keys like maybe I've seen n number of",
    "start": "1083030",
    "end": "1088160"
  },
  {
    "text": "events and then I'll stop processing so give me the top 10 users for the last thousand events you could also say give me the 10 top 10",
    "start": "1088160",
    "end": "1095300"
  },
  {
    "text": "users for this specific segment and do this continuously over time but the",
    "start": "1095300",
    "end": "1100940"
  },
  {
    "text": "whole point is you need to bound the data processing in some way to determine when you're going to emit results where",
    "start": "1100940",
    "end": "1109070"
  },
  {
    "text": "do we send that data so in this particular case we're going to use an AWS lambda function to write it to a DynamoDB table and this allows us to",
    "start": "1109070",
    "end": "1116330"
  },
  {
    "text": "write data in an optimized format for DynamoDB perhaps we'll take a number of",
    "start": "1116330",
    "end": "1121970"
  },
  {
    "text": "results combine them into a single document and then upload to DynamoDB maybe we'll do some post-processing like",
    "start": "1121970",
    "end": "1128270"
  },
  {
    "text": "we'll read from past results from a dynamo TV table combine them in some interesting way and then upload the",
    "start": "1128270",
    "end": "1133820"
  },
  {
    "text": "results you'll see aw slammed in a lot of these use cases because it really provides some additional flexibility or",
    "start": "1133820",
    "end": "1141170"
  },
  {
    "text": "some customization if you need it in your particular use case",
    "start": "1141170",
    "end": "1145990"
  },
  {
    "start": "1147000",
    "end": "1269000"
  },
  {
    "text": "so monitoring IOT devices this is a another common use case we see in fact",
    "start": "1154440",
    "end": "1160870"
  },
  {
    "text": "this use case is growing very very fast Kinesis is used by out IOT customers primarily because it solves the key",
    "start": "1160870",
    "end": "1168520"
  },
  {
    "text": "problem that I keep on emphasizing and the fact that you have a large number of events that are generated by hundreds",
    "start": "1168520",
    "end": "1174640"
  },
  {
    "text": "thousands millions of sensors and to ingest that data into your data processing tools and services is very",
    "start": "1174640",
    "end": "1181750"
  },
  {
    "text": "very expensive if you do by hand but Kinesis streams and Kinesis data firehose both solve that problem very",
    "start": "1181750",
    "end": "1187630"
  },
  {
    "text": "well by providing that temporary buffer ahead of your data processing tools so",
    "start": "1187630",
    "end": "1193809"
  },
  {
    "text": "amazon Kinesis data streams has a number of built-in integrations with data services across AWS they include AWS",
    "start": "1193809",
    "end": "1201549"
  },
  {
    "text": "cloud trail VPC flow log file watch logs the one I highlight here is AWS IOT",
    "start": "1201549",
    "end": "1207340"
  },
  {
    "text": "service when you set this up you can configure a very simple rule in the AWS",
    "start": "1207340",
    "end": "1213700"
  },
  {
    "text": "IOT service - for data - a caduceus stream and you can for an all data or a filtered set of data you can forward to",
    "start": "1213700",
    "end": "1220480"
  },
  {
    "text": "n different number of cases streams here I'm just forwarding to one this is a very common use case for both AWS IOT as",
    "start": "1220480",
    "end": "1227980"
  },
  {
    "text": "well as Amazon Kinesis because the integration is sort of it's again it's serverless it's very easy to set up it's",
    "start": "1227980",
    "end": "1235419"
  },
  {
    "text": "very easy to debug when there are issues so getting that data again into that",
    "start": "1235419",
    "end": "1240520"
  },
  {
    "text": "stream and here we're computing the average temp every 10 seconds so I've added the time parameter in which we're",
    "start": "1240520",
    "end": "1245860"
  },
  {
    "text": "computing it so and then triggering if",
    "start": "1245860",
    "end": "1251799"
  },
  {
    "text": "you if the temperature reaches a certain point you're triggering a lambda function to perhaps to react to the data",
    "start": "1251799",
    "end": "1256809"
  },
  {
    "text": "or persist that data into a time series analytics into a database so again you",
    "start": "1256809",
    "end": "1262179"
  },
  {
    "text": "see very common patterns around these common use cases because it's what we see customers typically do so this is",
    "start": "1262179",
    "end": "1270640"
  },
  {
    "start": "1269000",
    "end": "1397000"
  },
  {
    "text": "the use case we're going to be spending the majority of the time for the remainder of the presentation on I",
    "start": "1270640",
    "end": "1275830"
  },
  {
    "text": "mentioned that Kinesis data streams as well as Kinesis data fires have a number",
    "start": "1275830",
    "end": "1281620"
  },
  {
    "text": "of built-in integrations with other AWS services cloud trail is one of them so with AWS",
    "start": "1281620",
    "end": "1287860"
  },
  {
    "text": "cloud sure you can set up an Amazon Cloud watch event trigger and this is basically a simple trigger to take data",
    "start": "1287860",
    "end": "1293080"
  },
  {
    "text": "that's generated by a leaders cloud trail and send it to Amazon Kinesis AWS",
    "start": "1293080",
    "end": "1298210"
  },
  {
    "text": "cloud so for those who are not familiar with it it logs activity associated with your AWS account so the API calls that",
    "start": "1298210",
    "end": "1304870"
  },
  {
    "text": "you make when you work with the SDKs when you work with the console that gets logged as well as metadata about the",
    "start": "1304870",
    "end": "1310840"
  },
  {
    "text": "call gets called logged so you're the I am users making the call get logged whether the result of the request get",
    "start": "1310840",
    "end": "1317740"
  },
  {
    "text": "called a lot of very useful information so people typically analyze these to perform things like audit checks",
    "start": "1317740",
    "end": "1324250"
  },
  {
    "text": "compliance checks to track usage and even for operational use cases so you",
    "start": "1324250",
    "end": "1330820"
  },
  {
    "text": "want to know if one of your users spun up and number of ec2 instance or something like that provides very used a",
    "start": "1330820",
    "end": "1336310"
  },
  {
    "text": "to be ascribed to provides very useful mechanism of track that information the combination of AWS Kinesis services",
    "start": "1336310",
    "end": "1342310"
  },
  {
    "text": "I show here provide a useful metric to develop very valuable operational",
    "start": "1342310",
    "end": "1347530"
  },
  {
    "text": "metrics so things like what are my top 10 I am users for any given period of time what is who what user is calling",
    "start": "1347530",
    "end": "1355270"
  },
  {
    "text": "ec2 the most often are there any anomalies associated with how my users are calling ec2 these are all key",
    "start": "1355270",
    "end": "1362650"
  },
  {
    "text": "metrics that can be computed in Kinesis data analytics and we'll walk through some of them later today again very very",
    "start": "1362650",
    "end": "1369340"
  },
  {
    "text": "common scenario that I show on the back end of this is using Kinesis a combination of Kinesis services and a",
    "start": "1369340",
    "end": "1375640"
  },
  {
    "text": "dubious lambda to persist date it to a durable storage like Amazon DynamoDB and then archiving the raw events throughout",
    "start": "1375640",
    "end": "1382720"
  },
  {
    "text": "the pipeline so Kinesis data fire stores the raw events if you want to perform batch analysis later as well as has a",
    "start": "1382720",
    "end": "1389890"
  },
  {
    "text": "bucket for processed data so you can reevaluate any past results as well",
    "start": "1389890",
    "end": "1395700"
  },
  {
    "start": "1397000",
    "end": "1431000"
  },
  {
    "text": "so I'm gonna go through the use case I just went through sort of in pieces",
    "start": "1403100",
    "end": "1408480"
  },
  {
    "text": "right so I'm gonna go back real quick so we're gonna go from left to right like this is the ingestion here are some common scenarios associated with this",
    "start": "1408480",
    "end": "1414840"
  },
  {
    "text": "use case as well as the broader use cases and then the the middle part of actual processing we'll walk through how",
    "start": "1414840",
    "end": "1420690"
  },
  {
    "text": "to do windowed analytics using Kinesis data analytics and then the final part will show like some best practices",
    "start": "1420690",
    "end": "1427109"
  },
  {
    "text": "associated with persisting data to dinamo so as I mentioned cloud show",
    "start": "1427109",
    "end": "1434309"
  },
  {
    "start": "1431000",
    "end": "1558000"
  },
  {
    "text": "provides a basically continuous logging of account activity the what the cloud",
    "start": "1434309",
    "end": "1441239"
  },
  {
    "text": "watch events does and it does this this is not the only integration that cloud much events does but it allows you to trigger an event from point A to point B",
    "start": "1441239",
    "end": "1448679"
  },
  {
    "text": "so as an example we're sending data to Kinesis data firehose here but you could",
    "start": "1448679",
    "end": "1454080"
  },
  {
    "text": "also send it to a Kinesis stream you could invoke a lambda function you could send it to cloud watch logs the it's a",
    "start": "1454080",
    "end": "1461129"
  },
  {
    "text": "very useful integration that not a lot of customers I feel not enough customers know about because it's basically set it",
    "start": "1461129",
    "end": "1467820"
  },
  {
    "text": "and forget it so you set up the cloud watch event trigger and the data start just showing up into the service that you pick and",
    "start": "1467820",
    "end": "1473369"
  },
  {
    "text": "there's a lot of different combinations between point A and point P outside of even streaming data I highly encourage",
    "start": "1473369",
    "end": "1479309"
  },
  {
    "text": "you to look at it so each event I said it includes a bunch of useful information and one of the things that",
    "start": "1479309",
    "end": "1485159"
  },
  {
    "text": "includes is a timestamp and we're gonna go a little bit more detail with this later but it's important to note is this",
    "start": "1485159",
    "end": "1490859"
  },
  {
    "text": "is the timestamp that the event was created so when the call was received by the AWS service that the metadata",
    "start": "1490859",
    "end": "1498119"
  },
  {
    "text": "associated that call is sent to the cloud trail and then through cloud watch events sent to a Canisius data firehose we typically call this the event time or",
    "start": "1498119",
    "end": "1505919"
  },
  {
    "text": "the client-side time and it's when you're computing metrics in real time the concept of time is very very",
    "start": "1505919",
    "end": "1512369"
  },
  {
    "text": "important in this case this is the event time Kinesis data fires in this case",
    "start": "1512369",
    "end": "1517859"
  },
  {
    "text": "archives all data Amazon s3 using the buffering hints that you have since this use case is a real time processing use",
    "start": "1517859",
    "end": "1525359"
  },
  {
    "text": "case the files that will probably deliver to Amazon s3 aren't really latency sensitive we're doing much",
    "start": "1525359",
    "end": "1531210"
  },
  {
    "text": "faster processing in another pipeline so we might set our buffering hints for this to be maybe one hour or",
    "start": "1531210",
    "end": "1536470"
  },
  {
    "text": "very very large files so that we can more efficiently work through them if",
    "start": "1536470",
    "end": "1543130"
  },
  {
    "text": "you didn't have the other half of the pipeline you might set the buffering him to be a lot faster so maybe once a",
    "start": "1543130",
    "end": "1549220"
  },
  {
    "text": "minute so you get that data to land in s3 and start your ETL processes as fast as possible so I mentioned there's a lot",
    "start": "1549220",
    "end": "1561160"
  },
  {
    "start": "1558000",
    "end": "1746000"
  },
  {
    "text": "of different ways to get data into Amazon Kinesis and what Amazon kinis is here means Amazon Kinesis data streams",
    "start": "1561160",
    "end": "1566890"
  },
  {
    "text": "and Amazon Kinesis data firehose so there's several services that we have",
    "start": "1566890",
    "end": "1572050"
  },
  {
    "text": "that are enabled for automatic ingestion including BBC flow logs cloud trail cloud watch logs AWS IOT and Amazon",
    "start": "1572050",
    "end": "1579910"
  },
  {
    "text": "pinpoint and when I say automatic ingestion I mean you go to the UI or download the CLI and you make one or two API calls",
    "start": "1579910",
    "end": "1587110"
  },
  {
    "text": "and it's setup and data is just automatically writing to the firehose of",
    "start": "1587110",
    "end": "1592120"
  },
  {
    "text": "the stream don't not even I wouldn't even call it easy",
    "start": "1592120",
    "end": "1597130"
  },
  {
    "text": "it's like dead simple setup the next is sort of easy set up some very common patterns that we see so since Amazon API",
    "start": "1597130",
    "end": "1604570"
  },
  {
    "text": "gateway has come out a very common pattern that we have seen is just using API gateway 2 for data to a Canisius",
    "start": "1604570",
    "end": "1611440"
  },
  {
    "text": "stream ELB is also used in this scenario and sometimes they're backed by lambda",
    "start": "1611440",
    "end": "1616660"
  },
  {
    "text": "functions or ec2 instances sometimes they're not but meaning that you'll have API gateway",
    "start": "1616660",
    "end": "1621670"
  },
  {
    "text": "which will send data to a lambda function which in turn will send it to a commuter stream some customers skip the lambda function and some do not",
    "start": "1621670",
    "end": "1628920"
  },
  {
    "text": "the other free easy setup that's a relatively common use case is change",
    "start": "1628920",
    "end": "1635440"
  },
  {
    "text": "data capture so example on Amazon edia RDS you can set up an agent to read from the bin log associated with that and",
    "start": "1635440",
    "end": "1642100"
  },
  {
    "text": "send change data capture events directly to a kanesha stream we have an agent which is on the right hand side called",
    "start": "1642100",
    "end": "1648400"
  },
  {
    "text": "the Amazon Kinesis agent which just tails a log file and does things like file rotation things like that",
    "start": "1648400",
    "end": "1654430"
  },
  {
    "text": "the durably sends data to Canisius firehose working stream very very easily",
    "start": "1654430",
    "end": "1659770"
  },
  {
    "text": "setting up those 4 RDS very quick to set up so drugging minutes or hours",
    "start": "1659770",
    "end": "1667860"
  },
  {
    "text": "or you can write your own we offer a number of open-source libraries the one",
    "start": "1667860",
    "end": "1673540"
  },
  {
    "text": "I just mentioned Amazon cases agent really optimized for like the log ingestion use case we also have log4j",
    "start": "1673540",
    "end": "1681190"
  },
  {
    "text": "fluent II connectors that allow your write data to a can usage data stream we",
    "start": "1681190",
    "end": "1687070"
  },
  {
    "text": "have an Amazon cases producer library this library is specifically meant for large log servers so if you're",
    "start": "1687070",
    "end": "1692320"
  },
  {
    "text": "generating you know tens of megabytes of data per second is very very efficient",
    "start": "1692320",
    "end": "1697870"
  },
  {
    "text": "in writing data to a keynesian stream it bet it'll buffer and batch the records per configuration it'll compress them it",
    "start": "1697870",
    "end": "1704950"
  },
  {
    "text": "will use protobuf if configured so it's all about basically it's really about cost optimizations and throughput",
    "start": "1704950",
    "end": "1710770"
  },
  {
    "text": "optimizations and finally and I would be remiss for not saying this the AWS SDKs",
    "start": "1710770",
    "end": "1716260"
  },
  {
    "text": "which are very very very easy to write with a you know probably 10 15 lines of",
    "start": "1716260",
    "end": "1721960"
  },
  {
    "text": "code write a data producer or using an AWS SDK on the Amazon Kinesis data",
    "start": "1721960",
    "end": "1729610"
  },
  {
    "text": "streams website there's a developer resources page that provides a little bit more of a comprehensive list than I",
    "start": "1729610",
    "end": "1734710"
  },
  {
    "text": "have here as a bunch of lick list links to github where we have some of these open source libraries so have a look",
    "start": "1734710",
    "end": "1741190"
  },
  {
    "text": "this is just a sample of some of the more popular ones there's a lot more",
    "start": "1741190",
    "end": "1745890"
  },
  {
    "start": "1746000",
    "end": "1838000"
  },
  {
    "text": "so the we're now on the middle part right we ingest the data into a Kinesis",
    "start": "1753160",
    "end": "1758420"
  },
  {
    "text": "stream now we're going to compute some operational metrics on that real-time data so if you remember the cloud trail",
    "start": "1758420",
    "end": "1765140"
  },
  {
    "text": "log includes the call the service that you called the I am user that called it",
    "start": "1765140",
    "end": "1770480"
  },
  {
    "text": "the server the actual API that you called the time stamp associated with it",
    "start": "1770480",
    "end": "1775550"
  },
  {
    "text": "this is very useful information about what what you're doing within your AWS",
    "start": "1775550",
    "end": "1781010"
  },
  {
    "text": "account there's a lot of different patterns for having to analyze there's this data the pattern that we're going",
    "start": "1781010",
    "end": "1787580"
  },
  {
    "text": "to talk about today is how to process it using Kinesis data analytics using sequel code some of the metrics that are",
    "start": "1787580",
    "end": "1793970"
  },
  {
    "text": "useful are things like the ec2 API failures so are you continuously making",
    "start": "1793970",
    "end": "1799940"
  },
  {
    "text": "an I am error when you make an ec2 call detecting that type of thing a percentage error rate on a specific API",
    "start": "1799940",
    "end": "1807320"
  },
  {
    "text": "call from a specific I am user so like maybe there's you have a cron job that",
    "start": "1807320",
    "end": "1812870"
  },
  {
    "text": "starts up every five minutes and you really want to make sure as fast as possible that that cron job was",
    "start": "1812870",
    "end": "1818000"
  },
  {
    "text": "successfully run and when you know the API signature is associated with that call this is one mechanism of do it the",
    "start": "1818000",
    "end": "1825800"
  },
  {
    "text": "purpose here of me bringing up these examples is that you really really can get fairly specific with the",
    "start": "1825800",
    "end": "1831410"
  },
  {
    "text": "customization of what metrics you compute and you can do this in real time",
    "start": "1831410",
    "end": "1836470"
  },
  {
    "text": "so how do I write streaming sequel code what does it look like so there's two",
    "start": "1837850",
    "end": "1843560"
  },
  {
    "start": "1838000",
    "end": "2212000"
  },
  {
    "text": "primary concepts when you build a Kinesis data analytics application so I'm sorry there's three primary",
    "start": "1843560",
    "end": "1850340"
  },
  {
    "text": "components the source the code and the destination we will continuously read",
    "start": "1850340",
    "end": "1856370"
  },
  {
    "text": "data from the source continuously process that data with your code and continuously emit the results to the",
    "start": "1856370",
    "end": "1861620"
  },
  {
    "text": "destination that you've configured the code for the middle part the code is",
    "start": "1861620",
    "end": "1867230"
  },
  {
    "text": "there's two primary components of the code there are these in memory objects that we have they're called in",
    "start": "1867230",
    "end": "1872810"
  },
  {
    "text": "application streams you can think of them as almost like a materialized view on the stream alright so we ingest your",
    "start": "1872810",
    "end": "1879560"
  },
  {
    "text": "JSON data will auto-detect most scenarios for JSON data we have some",
    "start": "1879560",
    "end": "1884900"
  },
  {
    "text": "mechanisms to around those issues if we don't auto detect it and then we map them to a stream with a column types and their",
    "start": "1884900",
    "end": "1893840"
  },
  {
    "text": "data value types so like the event timestamp timestamp computation type bar charts with five six this is similar to",
    "start": "1893840",
    "end": "1902480"
  },
  {
    "text": "a table you create this in your application code nothing happens it's just an object right in order to get it",
    "start": "1902480",
    "end": "1908929"
  },
  {
    "text": "to actually process data we have this second concept called a pump which is basically just as continuous select",
    "start": "1908929",
    "end": "1914990"
  },
  {
    "text": "statement that we're inserting into this stream the source stream that we create",
    "start": "1914990",
    "end": "1923090"
  },
  {
    "text": "that is the materialized view on your Kinesis data stream your external Kinesis data stream where your external",
    "start": "1923090",
    "end": "1928190"
  },
  {
    "text": "can use of data firehose is always represented by source equal stream 0 0 1 something like that in higher scale use",
    "start": "1928190",
    "end": "1936380"
  },
  {
    "text": "cases we will enumerate it will be 0 0 1 0 0 2 so on and so forth for most",
    "start": "1936380",
    "end": "1942080"
  },
  {
    "text": "customers it just looks like this so you're gonna select from one stream and insert into another stream this allows",
    "start": "1942080",
    "end": "1948679"
  },
  {
    "text": "you to develop basically serial step pipelines in your in your application",
    "start": "1948679",
    "end": "1954650"
  },
  {
    "text": "code so one of the most common scenarios you see is the first step dee doop so deduplication over a window get rid of",
    "start": "1954650",
    "end": "1960799"
  },
  {
    "text": "any data duplicates that might have been caused from the data producer based off of a set of keys or maybe you're only",
    "start": "1960799",
    "end": "1966669"
  },
  {
    "text": "you only want one event per user coming in something like that but deduplicating",
    "start": "1966669",
    "end": "1972260"
  },
  {
    "text": "event or perhaps a filter the next step is usually the analytic step so you'll do one insert into a stream read from",
    "start": "1972260",
    "end": "1979220"
  },
  {
    "text": "that stream and then perform your analytics so you here I was selecting",
    "start": "1979220",
    "end": "1987140"
  },
  {
    "text": "the event time stamp which is an important concept that I brought up this is the time stamp when the event was",
    "start": "1987140",
    "end": "1992780"
  },
  {
    "text": "actually created that time stamp that I brought up earlier in the presentation you make the API call to the service it",
    "start": "1992780",
    "end": "1998480"
  },
  {
    "text": "gives a time stamp that's we're calling that the event time stamp when approximately when the event was created",
    "start": "1998480",
    "end": "2003549"
  },
  {
    "text": "you could use your own time stamp as its associated with the API call as well the",
    "start": "2003549",
    "end": "2010030"
  },
  {
    "text": "next concept is the processing time stamp and the processing time stamp is",
    "start": "2010030",
    "end": "2015340"
  },
  {
    "text": "the server time of the application so we don't really the only concepts we expose from a",
    "start": "2015340",
    "end": "2021580"
  },
  {
    "text": "server is this the clock on the server and it basically is whatever nodes are",
    "start": "2021580",
    "end": "2028150"
  },
  {
    "text": "running your application code it's the clock time of those nodes in the group",
    "start": "2028150",
    "end": "2033430"
  },
  {
    "text": "by we've got a couple things here the group by a key so in this case the source IP address and then two step",
    "start": "2033430",
    "end": "2040000"
  },
  {
    "text": "functions the first aggregates data by the processing time and this serves as",
    "start": "2040000",
    "end": "2045550"
  },
  {
    "text": "basically a trigger it says we want results emitted every minute continuously the second step function is",
    "start": "2045550",
    "end": "2053700"
  },
  {
    "text": "essentially a key based grouping so I want them every minute every minute based off of the clock time of the",
    "start": "2053700",
    "end": "2060820"
  },
  {
    "text": "application no this processing data but I want to group them by their event time why do we do this I'll go into a little",
    "start": "2060820",
    "end": "2067870"
  },
  {
    "text": "bit more detail but we do this to handle late data when you're processing data in the batch context all right your data",
    "start": "2067870",
    "end": "2075190"
  },
  {
    "text": "has those bounds right there you do one job runs up processes all the data",
    "start": "2075190",
    "end": "2080320"
  },
  {
    "text": "that's the context of what the data processing was whatever it's confined by just the data that you specified in your",
    "start": "2080320",
    "end": "2086889"
  },
  {
    "text": "job with streaming data we're running this continuously and we're typically computing at a much higher frequency so",
    "start": "2086890",
    "end": "2093250"
  },
  {
    "text": "in this case it's one minute so over the course of that minute you might have events that arrive on time meeting that",
    "start": "2093250",
    "end": "2100170"
  },
  {
    "text": "milliseconds or seconds after the data was generated by cloud trail it's processed by your Kinesis data analytics",
    "start": "2100170",
    "end": "2106030"
  },
  {
    "text": "application so we call that on time when the difference between the application",
    "start": "2106030",
    "end": "2111130"
  },
  {
    "text": "node or the process time is very little between the event time stamp but you",
    "start": "2111130",
    "end": "2117490"
  },
  {
    "text": "also will have guaranteed to happen late events and these are events that arrive",
    "start": "2117490",
    "end": "2122920"
  },
  {
    "text": "late due to servers crashing you know network lag it could be a number of",
    "start": "2122920",
    "end": "2128740"
  },
  {
    "text": "different things maybe you had a process we start on one of your ec2 servers and",
    "start": "2128740",
    "end": "2134020"
  },
  {
    "text": "then it arrives a minute or two late so how do you deal with that data this is",
    "start": "2134020",
    "end": "2139990"
  },
  {
    "text": "the mechanism in cases and data analytics deals with it will emit that data but we'll handle the late data by",
    "start": "2139990",
    "end": "2145000"
  },
  {
    "text": "aggregating it by its own key for yum some use cases this is very very relevant so if you want to use",
    "start": "2145000",
    "end": "2152510"
  },
  {
    "text": "accurate results and use them for a long period of time after the fact you wanted you this is the mechanism you want to",
    "start": "2152510",
    "end": "2158330"
  },
  {
    "text": "use you want to group by your event timestamp for super real-time use cases we have customers just filter out",
    "start": "2158330",
    "end": "2164360"
  },
  {
    "text": "real-time data and a super exactly easy example of that is you're detecting",
    "start": "2164360",
    "end": "2170690"
  },
  {
    "text": "number of live sessions a user has and you say that you know the user may only",
    "start": "2170690",
    "end": "2175820"
  },
  {
    "text": "have five sessions at once at this at any given time and if they go above five sessions then you're going to block any",
    "start": "2175820",
    "end": "2182630"
  },
  {
    "text": "other sessions like imagine you're logging into Netflix and you're detecting the number of live sessions",
    "start": "2182630",
    "end": "2187910"
  },
  {
    "text": "they have they only allow you to have five if you have six they block you out in the context of this late data means",
    "start": "2187910",
    "end": "2193490"
  },
  {
    "text": "nothing so the people just discard it so this concept of time is extremely important we'll go through it a little",
    "start": "2193490",
    "end": "2198710"
  },
  {
    "text": "bit more detail as we go through the presentation but think about your use case and whether this data is going to",
    "start": "2198710",
    "end": "2204770"
  },
  {
    "text": "be useful after the fact if it is you want to group by your event time stamp if it isn't perhaps you just want to filter and throw away all the late data",
    "start": "2204770",
    "end": "2213130"
  },
  {
    "start": "2212000",
    "end": "2435000"
  },
  {
    "text": "these step functions I talked about are called windows and when you do a some",
    "start": "2220360",
    "end": "2226430"
  },
  {
    "text": "account of min or max these are called windowed operators and there's different types of windows what I just showed you",
    "start": "2226430",
    "end": "2232730"
  },
  {
    "text": "is a tumbling window but there's also sliding windows session windows hopping windows lots of other different custom",
    "start": "2232730",
    "end": "2238610"
  },
  {
    "text": "windows there's lots of different types of windows and the windows to find the bounds I brought that up several times",
    "start": "2238610",
    "end": "2244190"
  },
  {
    "text": "now while you're bounding a result set by time or by key they define where to",
    "start": "2244190",
    "end": "2249200"
  },
  {
    "text": "start your computation and where to end your computation on a continuous data stream so again we're in the batch",
    "start": "2249200",
    "end": "2255500"
  },
  {
    "text": "context you're just bound by the data you're doing the job on in the streaming context you've got to specify the",
    "start": "2255500",
    "end": "2260780"
  },
  {
    "text": "balance when you do things like aggregations so when do I want to stop aggregating results and windows provide",
    "start": "2260780",
    "end": "2266690"
  },
  {
    "text": "that mechanism so I mentioned there are several different window types the",
    "start": "2266690",
    "end": "2271910"
  },
  {
    "text": "example we have up here are is a tumbling window what makes a tumbling window unique is that it's a fixed size",
    "start": "2271910",
    "end": "2279110"
  },
  {
    "text": "and group keys do not overlap so meaning",
    "start": "2279110",
    "end": "2284660"
  },
  {
    "text": "that on the from left the right the results if I'm just doing a count would be if I can read it",
    "start": "2284660",
    "end": "2291349"
  },
  {
    "text": "correctly two for three they're not overlapping they're the same size the",
    "start": "2291349",
    "end": "2297470"
  },
  {
    "text": "size in this case is defined by time so T is equal to 0 to 5 5 to 10 10 to 15 so",
    "start": "2297470",
    "end": "2306579"
  },
  {
    "text": "sliding windows and session windows and other other types of windows have different mechanisms some of them might",
    "start": "2306579",
    "end": "2312290"
  },
  {
    "text": "be fixed size some of them might not be fixed size someone that might be overlapping excuse me some of might be",
    "start": "2312290",
    "end": "2319220"
  },
  {
    "text": "overlapping so an example of a sliding window is overlapping all right so a sliding window computes a rolling window",
    "start": "2319220",
    "end": "2325040"
  },
  {
    "text": "so instead of the time being from T 0 to t 5 you'll see that window continuously",
    "start": "2325040",
    "end": "2330710"
  },
  {
    "text": "compute so at any given point it'll say doing a count again one two and then we",
    "start": "2330710",
    "end": "2337369"
  },
  {
    "text": "might include the third event in that sliding window 3 so on and so forth and as events enter the window and exit the",
    "start": "2337369",
    "end": "2344480"
  },
  {
    "text": "window the result changes tumbling windows are great for reporting and leaderboards and things like that",
    "start": "2344480",
    "end": "2350359"
  },
  {
    "text": "sliding windows are very very good for operational analytic use cases",
    "start": "2350359",
    "end": "2355640"
  },
  {
    "text": "the reason being sliding window you get a result whenever the result changes essentially tumbling windows you get a",
    "start": "2355640",
    "end": "2361700"
  },
  {
    "text": "result at the end of the window so you have that group by that I showed on the last slide",
    "start": "2361700",
    "end": "2366890"
  },
  {
    "text": "we'll get that count of two at T is equal to five all right versus a sliding",
    "start": "2366890",
    "end": "2372440"
  },
  {
    "text": "window you'll get a count of one at T is equal to 1 2 at t is equal to 2 and then",
    "start": "2372440",
    "end": "2377599"
  },
  {
    "text": "if your window is longer 3 is equal to t is equal to 6 so you get the result immediately that's why it's great for",
    "start": "2377599",
    "end": "2384230"
  },
  {
    "text": "operational use cases a lot of times are used in conjunction with sliding and tumbling windows the other windows get a",
    "start": "2384230",
    "end": "2390950"
  },
  {
    "text": "little more complex another example of a window is a session window a session window is overlapping and not fixed size",
    "start": "2390950",
    "end": "2399470"
  },
  {
    "text": "and you can think of it as a user session on a website so user logs on your website they might be browsing for",
    "start": "2399470",
    "end": "2405650"
  },
  {
    "text": "2 minutes or 5 minutes or 10 minutes you don't really know at a time you're closing the window based off of some",
    "start": "2405650",
    "end": "2412069"
  },
  {
    "text": "condition associated with it such as I haven't seen any events from this",
    "start": "2412069",
    "end": "2417230"
  },
  {
    "text": "particular user for one minute I'm gonna assume the session is closed and now stop my aggregation admit the",
    "start": "2417230",
    "end": "2423609"
  },
  {
    "text": "result so there's different types of Windows but the two primary components you want to think about is what size are",
    "start": "2423609",
    "end": "2430239"
  },
  {
    "text": "they are they fixed or not fixed and whether they overlap or not so invent",
    "start": "2430239",
    "end": "2438430"
  },
  {
    "start": "2435000",
    "end": "2535000"
  },
  {
    "text": "event ingest the processing time and I covered this a little bit on the earlier slide and this is a copy/paste from the",
    "start": "2438430",
    "end": "2444400"
  },
  {
    "text": "previous sequel code so in this particular case we're we're triggering",
    "start": "2444400",
    "end": "2450369"
  },
  {
    "text": "the result by a processing time that clock we monitor those on the applications and if you use any other",
    "start": "2450369",
    "end": "2455710"
  },
  {
    "text": "streaming system they do the same thing we're monitoring it and making sure that the clock is accurate and but every",
    "start": "2455710",
    "end": "2462190"
  },
  {
    "text": "minute you're going to get a result no matter what the aggregation key is the",
    "start": "2462190",
    "end": "2468579"
  },
  {
    "text": "on the event timestamp is really the timestamp grouping key this is what the result is you're gonna have that the",
    "start": "2468579",
    "end": "2474670"
  },
  {
    "text": "result you're going to have that means something to you it doesn't matter what the server time is to you the event",
    "start": "2474670",
    "end": "2480549"
  },
  {
    "text": "timestamp is what you want to do your aggregation on for use cases that require it so what happens with late",
    "start": "2480549",
    "end": "2487690"
  },
  {
    "text": "data so some late it is very easy to handle we perhaps you have got 10 events",
    "start": "2487690",
    "end": "2493089"
  },
  {
    "text": "9 arrive on time 1 arrives late and you've got a keys associated with that",
    "start": "2493089",
    "end": "2498099"
  },
  {
    "text": "well we'll do with this code that's in front of use when the minute is over we'll omit that 9 and say that event",
    "start": "2498099",
    "end": "2505779"
  },
  {
    "text": "comes very late let's say it comes an hour late will then event a 1 and you'll",
    "start": "2505779",
    "end": "2511809"
  },
  {
    "text": "post process the data can do combine them to 9 plus 1 is equal to 10 this is",
    "start": "2511809",
    "end": "2517029"
  },
  {
    "text": "why databases are so popular as a destination for streaming analytics because they provide a very easy",
    "start": "2517029",
    "end": "2522519"
  },
  {
    "text": "mechanism to do things like insert on duplicate key so I'm inserting for this set of keys for this timestamp if I",
    "start": "2522519",
    "end": "2529479"
  },
  {
    "text": "already have a value add them together or define the max",
    "start": "2529479",
    "end": "2534298"
  },
  {
    "start": "2535000",
    "end": "2637000"
  },
  {
    "text": "so we went through the middle piece and we went through the ingest piece and the last piece is persisting data and in for",
    "start": "2536990",
    "end": "2543920"
  },
  {
    "text": "this case is for a real-time dashboard so walking you through the architecture",
    "start": "2543920",
    "end": "2549560"
  },
  {
    "text": "that we chose for this particular solution we're taking the real-time metrics the that we produced off the",
    "start": "2549560",
    "end": "2555410"
  },
  {
    "text": "cloud real data like top 10 I am users ec2 failure rate so on and so forth we're writing them to a Kinesis data",
    "start": "2555410",
    "end": "2562250"
  },
  {
    "text": "fire hose which archives all the data in Amazon s3 so if you need any of the process result we will doing long-term",
    "start": "2562250",
    "end": "2568369"
  },
  {
    "text": "are called Carville and s3 we're then going to use the lambda function to",
    "start": "2568369",
    "end": "2573410"
  },
  {
    "text": "update the DynamoDB table and it's going to have some of that logic that I mentioned about previously that insert",
    "start": "2573410",
    "end": "2578660"
  },
  {
    "text": "on duplicate key logic to combine late and current combined current results",
    "start": "2578660",
    "end": "2584960"
  },
  {
    "text": "with previously computed results in this",
    "start": "2584960",
    "end": "2590000"
  },
  {
    "text": "solution we also have a real-time dashboard which is in the example I'm going to provide at the end of the slide",
    "start": "2590000",
    "end": "2595070"
  },
  {
    "text": "what we use is just a popular open source library for drawing graphs called charge a s and it's very simple way to",
    "start": "2595070",
    "end": "2601550"
  },
  {
    "text": "get real-time dashboards via static website and Amazon s3 another common use",
    "start": "2601550",
    "end": "2609109"
  },
  {
    "text": "case is to write this data to elasticsearch of a lot of our customers",
    "start": "2609109",
    "end": "2615320"
  },
  {
    "text": "Amazon cases all up across all three services a lot of them use the elastic search and Cabana for visualization so",
    "start": "2615320",
    "end": "2621859"
  },
  {
    "text": "they might send the data there you might semi-date at Amazon s3 and visualize with quick site we use this solution",
    "start": "2621859",
    "end": "2629000"
  },
  {
    "text": "because it's very very simple and very easy to assemble to set up very easy to customize and it's very cheap so with",
    "start": "2629000",
    "end": "2637550"
  },
  {
    "start": "2637000",
    "end": "2744000"
  },
  {
    "text": "that so the late events I mentioned writing data to a database it has",
    "start": "2637550",
    "end": "2645200"
  },
  {
    "text": "advantages in that when we produce these amendments you can combine the two in",
    "start": "2645200",
    "end": "2651920"
  },
  {
    "text": "certain cases this is easy to do so in the count example it's dead simple min",
    "start": "2651920",
    "end": "2657980"
  },
  {
    "text": "max very very easy to do so what do you do though if you're doing like a count unique or if you're doing the result of",
    "start": "2657980",
    "end": "2667160"
  },
  {
    "text": "machine learning algorithm yourself like that the difference between the metrics that I just talked about and the",
    "start": "2667160",
    "end": "2673069"
  },
  {
    "text": "two examples that I just brought up is that one the results are cumulative and it's",
    "start": "2673069",
    "end": "2678799"
  },
  {
    "text": "super easy to get a real-time result and then update the results for late data the other two you need to do recoup",
    "start": "2678799",
    "end": "2685269"
  },
  {
    "text": "recompute the you need all of the data in the time series to recompute it so if",
    "start": "2685269",
    "end": "2690769"
  },
  {
    "text": "you're doing account count unique you produce 100 a late event comes in the",
    "start": "2690769",
    "end": "2696349"
  },
  {
    "text": "account unique will be one so what we typically recommend for use cases for no",
    "start": "2696349",
    "end": "2703009"
  },
  {
    "text": "scenarios is to produce either longer windows and produce multiple results so you'll have one set of windows that",
    "start": "2703009",
    "end": "2709130"
  },
  {
    "text": "produce the results immediately and another set of Windows that compute it over longer wonder maybe an hour to make",
    "start": "2709130",
    "end": "2714950"
  },
  {
    "text": "sure that you all the late data has arrived so you get both an early result and a complete result this allows you to",
    "start": "2714950",
    "end": "2723319"
  },
  {
    "text": "do all the processing in the real time application without another thread did you clean up after the fact again this",
    "start": "2723319",
    "end": "2729499"
  },
  {
    "text": "would be handled by the lambda function and post-processing when you insert on duplicate key you mark whether it's a",
    "start": "2729499",
    "end": "2736309"
  },
  {
    "text": "complete result or not a complete result and you update your the row accordingly",
    "start": "2736309",
    "end": "2741910"
  },
  {
    "start": "2744000",
    "end": "2772000"
  },
  {
    "text": "so what does all this cost so the solution that I using data available to us the average cloud shell customer",
    "start": "2744309",
    "end": "2751160"
  },
  {
    "text": "average number of API calls the solution I presented to you today the with all the details is about $100 a month so the",
    "start": "2751160",
    "end": "2759230"
  },
  {
    "text": "it's about $60 a month in Kinesis analytics costs and the remaining of the services are about 40 bucks and you get",
    "start": "2759230",
    "end": "2766009"
  },
  {
    "text": "a live dashboard that feeds your real-time results so where do you go",
    "start": "2766009",
    "end": "2773150"
  },
  {
    "start": "2772000",
    "end": "2848000"
  },
  {
    "text": "next so if you go to this tinyurl.com buys real time dashboard I have",
    "start": "2773150",
    "end": "2780319"
  },
  {
    "text": "basically this three-page document very simple solution that shows you how to fire up a CloudFormation template and",
    "start": "2780319",
    "end": "2786829"
  },
  {
    "text": "for those of you taking pictures we post these slides as well after the fact so you won't be lost with it you basically",
    "start": "2786829",
    "end": "2793940"
  },
  {
    "text": "can set up with a couple of clicks this you'll get a live dashboard on your either boost account",
    "start": "2793940",
    "end": "2799190"
  },
  {
    "text": "activity and it we we chose some very very common things that we've seen other",
    "start": "2799190",
    "end": "2804560"
  },
  {
    "text": "people do but you can customize it to meet your needs which is part of what I brought up previously the fact that the",
    "start": "2804560",
    "end": "2809930"
  },
  {
    "text": "part of the power of the solution is first it handles a lot of scale it's very easily customizable you can write",
    "start": "2809930",
    "end": "2817609"
  },
  {
    "text": "your own sequel code modify to make your own metrics add your own graphs and it's",
    "start": "2817609",
    "end": "2823040"
  },
  {
    "text": "cheap it's very inexpensive and you get a lot of value from it with a very little setup the other thing I'd",
    "start": "2823040",
    "end": "2830030"
  },
  {
    "text": "encourage you guys to do is go out to one of the blogs that my favorite blog on the AWS is the AWS big data blog",
    "start": "2830030",
    "end": "2835910"
  },
  {
    "text": "we've have a lot of great customer examples not just for this example but a lot of other solutions will have some",
    "start": "2835910",
    "end": "2841579"
  },
  {
    "text": "spark streaming an example of some AWS lamda examples that follow similar use cases to this and other use cases on the",
    "start": "2841579",
    "end": "2850819"
  },
  {
    "start": "2848000",
    "end": "2947000"
  },
  {
    "text": "Amazon case website there are a lot of customer examples that provide good references to how people have solved",
    "start": "2850819",
    "end": "2856579"
  },
  {
    "text": "these problems before there's a lot of customer diversity here we have",
    "start": "2856579",
    "end": "2862839"
  },
  {
    "text": "enterprises advertising technology companies mobile apps online retail so",
    "start": "2862839",
    "end": "2871880"
  },
  {
    "text": "you should be able to find an example either in the blog post or on one of our customer references that helps cater to",
    "start": "2871880",
    "end": "2877400"
  },
  {
    "text": "sort of what you guys need to do since I just went to one use case there are many many different use cases that customers",
    "start": "2877400",
    "end": "2882950"
  },
  {
    "text": "solve for and these will help you provide a starting point we also have a",
    "start": "2882950",
    "end": "2889790"
  },
  {
    "text": "lot of integration so we showed one example but we have a lot of customers that use one of our other solutions so",
    "start": "2889790",
    "end": "2896000"
  },
  {
    "text": "they might not use Kinesis data analytics they might use Splunk Kinesis data fires fire hose actually recently",
    "start": "2896000",
    "end": "2903230"
  },
  {
    "text": "announced a beta for a Splunk integration so if you just want to deliver your streaming data to your spun",
    "start": "2903230",
    "end": "2908569"
  },
  {
    "text": "cluster we've got that we have a number of connectors with other solutions such",
    "start": "2908569",
    "end": "2914150"
  },
  {
    "text": "as data breaks apache flink this is one way to solve a problem so there's many",
    "start": "2914150",
    "end": "2921079"
  },
  {
    "text": "many different ways to solve the problem they're all provide very very good integrations if you need some help we've",
    "start": "2921079",
    "end": "2927589"
  },
  {
    "text": "got some partner systems integrators that have a lot of experience helping customers along building their first or their second or",
    "start": "2927589",
    "end": "2934859"
  },
  {
    "text": "third streaming data solution so with",
    "start": "2934859",
    "end": "2941609"
  },
  {
    "text": "that thank you guys very much for attending the call excuse me attending the presentation",
    "start": "2941609",
    "end": "2948109"
  }
]