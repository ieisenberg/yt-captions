[
  {
    "start": "0",
    "end": "81000"
  },
  {
    "text": "hello everyone welcome thank you for joining us today this",
    "start": "920",
    "end": "6350"
  },
  {
    "text": "is event handling at scale designing an auditable ingestion and persistence",
    "start": "6350",
    "end": "12509"
  },
  {
    "text": "architecture for 10,000 plus events per second possibly the longest session title that",
    "start": "12509",
    "end": "19470"
  },
  {
    "text": "you hear this year my name is Benjamin Feldon I'm a Solutions Architect with Amazon Web Services and with me here",
    "start": "19470",
    "end": "26820"
  },
  {
    "text": "today is Terry sage vice president of engineering at mcgraw-hill education",
    "start": "26820",
    "end": "32630"
  },
  {
    "text": "so I'd like to start today's talk by kind of framing the conversation a",
    "start": "32630",
    "end": "37649"
  },
  {
    "text": "little bit as a Solutions Architect one of the customers that I've had a privilege of working with is mcgraw-hill",
    "start": "37649",
    "end": "44280"
  },
  {
    "text": "education and I began working with the different teams about two years ago and",
    "start": "44280",
    "end": "50070"
  },
  {
    "text": "it was back then when we first began discussing their learning analytics platform and throughout this period of",
    "start": "50070",
    "end": "58050"
  },
  {
    "text": "time that platform has evolved through a number of iterations and has grown to be",
    "start": "58050",
    "end": "63390"
  },
  {
    "text": "ultimately a true believer in the service computing approach and so we",
    "start": "63390",
    "end": "68700"
  },
  {
    "text": "wanted to invite Terry to come and speak to us today about how this platform has evolved and how they've managed to scale",
    "start": "68700",
    "end": "75570"
  },
  {
    "text": "it and some of the things that they have learnt along the way so with that I won't hand it over to Terry",
    "start": "75570",
    "end": "80930"
  },
  {
    "text": "Thank You Benjamin so what to expect from this session I'm gonna go over the",
    "start": "80930",
    "end": "86729"
  },
  {
    "start": "81000",
    "end": "81000"
  },
  {
    "text": "business background in terms of why we built this platform how it fits into the overall learning ecosystem talk about",
    "start": "86729",
    "end": "94140"
  },
  {
    "text": "learning events and what they mean and how we're using JSON for the learning events talk about the overall",
    "start": "94140",
    "end": "100229"
  },
  {
    "text": "architecture and the different Amazon services that we use to build it out talk about to some of the trade-offs so",
    "start": "100229",
    "end": "107189"
  },
  {
    "text": "for example why we selected lambda versus many ec2 instances and then go",
    "start": "107189",
    "end": "113610"
  },
  {
    "text": "over the challenges that we encountered such as we lost events how we built confidence despite losing those events",
    "start": "113610",
    "end": "120270"
  },
  {
    "text": "and then some of the lessons learned so mcgraw-hill education is a hundred",
    "start": "120270",
    "end": "126990"
  },
  {
    "start": "124000",
    "end": "124000"
  },
  {
    "text": "and twenty eight year old publishing company oh we've changed we've actually",
    "start": "126990",
    "end": "133730"
  },
  {
    "text": "transformed quite a bit into a from this traditional publishing company into a",
    "start": "133730",
    "end": "139870"
  },
  {
    "text": "digital learning company so what does that mean the basis for this is learning",
    "start": "139870",
    "end": "145310"
  },
  {
    "text": "analytics so what we do is we have what's called a learning management system that learning management system",
    "start": "145310",
    "end": "152480"
  },
  {
    "text": "is basically where the students log in they look at their homework assignments they may read a book online and then",
    "start": "152480",
    "end": "159110"
  },
  {
    "text": "they submit the homework assignment that causes an event to be sent then from that system to the analytics platform",
    "start": "159110",
    "end": "168160"
  },
  {
    "text": "so well basically the very first incarnation of this needs to support",
    "start": "168160",
    "end": "173540"
  },
  {
    "text": "about two million students and then to scale to 14 million students with an",
    "start": "173540",
    "end": "178790"
  },
  {
    "text": "initial load of 10,000 events per second and in the future we need to handle about 15 million events per second and",
    "start": "178790",
    "end": "186410"
  },
  {
    "text": "the reason for this is we're adding on obviously more applications we're things such as mobile devices as well as video",
    "start": "186410",
    "end": "194739"
  },
  {
    "text": "in terms of the cyclical nature of our business and what's really super",
    "start": "194739",
    "end": "199940"
  },
  {
    "text": "important for us is the ability to scale up and scale down you probably are very familiar what",
    "start": "199940",
    "end": "207320"
  },
  {
    "text": "happens from about the August to December timeframe and then from January",
    "start": "207320",
    "end": "212630"
  },
  {
    "text": "to May we have to be able to sustain the loads particularly like for a college student",
    "start": "212630",
    "end": "219590"
  },
  {
    "text": "where we see a lot of traffic Sunday through Monday this is because generally",
    "start": "219590",
    "end": "225760"
  },
  {
    "text": "college students like to submit their homework assignments late I did",
    "start": "225760",
    "end": "230890"
  },
  {
    "text": "and then in terms of the service level agreements and also to what we had to",
    "start": "230890",
    "end": "238280"
  },
  {
    "text": "think about not only just the ingestion of the events but what would it take to provide the",
    "start": "238280",
    "end": "245470"
  },
  {
    "text": "visualizations and the reports so it's about sustaining a load of output of about 300 events output per second and",
    "start": "245470",
    "end": "254360"
  },
  {
    "text": "then also to to handle what we call the batch load of learning events into the system another sort of point here to make is",
    "start": "254360",
    "end": "263450"
  },
  {
    "text": "that our business is very cost conscientious so we have to be able to create systems with a very low price",
    "start": "263450",
    "end": "270340"
  },
  {
    "text": "point per student and then we have what are called just-in-time insights it's",
    "start": "270340",
    "end": "276370"
  },
  {
    "text": "about the learning moment so it's about providing almost in real time",
    "start": "276370",
    "end": "281640"
  },
  {
    "text": "either reports or some sort of an analysis or even like an adaptive engine",
    "start": "281640",
    "end": "288130"
  },
  {
    "text": "based on these learning events I'm going to provide some examples of connect insights the visualizations that are",
    "start": "288130",
    "end": "295180"
  },
  {
    "text": "actually powered by these learning events",
    "start": "295180",
    "end": "299220"
  },
  {
    "text": "so then [Music] this is the very first report this is",
    "start": "300480",
    "end": "306310"
  },
  {
    "text": "that what we call now risk report which provides instructors with one-click access to a dashboard did identify",
    "start": "306310",
    "end": "313600"
  },
  {
    "text": "students who are at risk of dropping out due to low engagement levels so we look",
    "start": "313600",
    "end": "318760"
  },
  {
    "text": "for patterns basically of online activity to determine the engagement level of the student including events",
    "start": "318760",
    "end": "325510"
  },
  {
    "text": "such as the frequency of logins and assignment submissions this is the",
    "start": "325510",
    "end": "330820"
  },
  {
    "text": "second report this is about student performance so it allows instructors to",
    "start": "330820",
    "end": "335830"
  },
  {
    "text": "search for a specific student in the class in this example Megan and focus on Megan's progress across assignments so",
    "start": "335830",
    "end": "344290"
  },
  {
    "text": "they view the assignment that has been submitted any any other assignment this student has in progress they have they",
    "start": "344290",
    "end": "350440"
  },
  {
    "text": "may have like an attempt to on attempt to or in an attempt three and then also to to be able to provide guidance or",
    "start": "350440",
    "end": "356410"
  },
  {
    "text": "feedback during the process of those submissions this is what we call a student",
    "start": "356410",
    "end": "363390"
  },
  {
    "text": "distribution report it basically gives information at a glance about this",
    "start": "363390",
    "end": "368740"
  },
  {
    "text": "section so this chart lets the instructors review the performance of",
    "start": "368740",
    "end": "373810"
  },
  {
    "text": "the entire section and then to drill down so for example if you look at the seven on the lower right hand corner",
    "start": "373810",
    "end": "380260"
  },
  {
    "text": "you'll see that these students have low scores but they've spent a lot of time working on the assignments so you're",
    "start": "380260",
    "end": "386620"
  },
  {
    "text": "gonna have an instructor ask why has this happen",
    "start": "386620",
    "end": "391080"
  },
  {
    "text": "so you drill down and you see the section performance for these seven of students which is depicted here and",
    "start": "392820",
    "end": "399490"
  },
  {
    "text": "their scores on the eight assignments there's obviously more questions that the instructor will continue to ask them",
    "start": "399490",
    "end": "405639"
  },
  {
    "text": "to drill down this is what we call a connect quick review report that allows instructors to",
    "start": "405639",
    "end": "412719"
  },
  {
    "text": "run reports that span multiple sections and students and then drill down on one student so for example they've select",
    "start": "412719",
    "end": "420249"
  },
  {
    "text": "one of the students here this is Nikki Adam this is the drill down and the instructor can quickly review",
    "start": "420249",
    "end": "426099"
  },
  {
    "text": "performance of this student so we talked briefly about learning",
    "start": "426099",
    "end": "432159"
  },
  {
    "start": "429000",
    "end": "429000"
  },
  {
    "text": "events again these are the things if you will that we send from one system to",
    "start": "432159",
    "end": "437229"
  },
  {
    "text": "another system and we use what's called a caliper standard so this caliper standard is",
    "start": "437229",
    "end": "445119"
  },
  {
    "text": "from IMS global consortium and was created in 2013 we do support the 47 different events",
    "start": "445119",
    "end": "452789"
  },
  {
    "text": "and these are what we call Jason link directory json-ld and then so basically",
    "start": "452789",
    "end": "460779"
  },
  {
    "text": "what the platform does is that it then ingests those caliper events and then",
    "start": "460779",
    "end": "466629"
  },
  {
    "text": "there may be mini caliper events something the one message which is what that apart parse it and we have various",
    "start": "466629",
    "end": "473409"
  },
  {
    "text": "analytics that are run against that data set the image depicts the IMS Analytics",
    "start": "473409",
    "end": "479110"
  },
  {
    "text": "implementation guide which may be found at the IMS consortium website",
    "start": "479110",
    "end": "485550"
  },
  {
    "text": "so this is an example of a caliper events its json-ld it's a test quiz file",
    "start": "485550",
    "end": "492519"
  },
  {
    "start": "486000",
    "end": "486000"
  },
  {
    "text": "and shown is a scaffolding for one question the title of the item is given a line",
    "start": "492519",
    "end": "499689"
  },
  {
    "text": "seven that a chopper is a type of question mark the response variables defined in lines",
    "start": "499689",
    "end": "507699"
  },
  {
    "text": "12 through 19 the associated correct answers defined",
    "start": "507699",
    "end": "513130"
  },
  {
    "text": "as a motorcycle which appears in mind 16 through 17 and then for the outcome",
    "start": "513130",
    "end": "519068"
  },
  {
    "text": "variables defined for the item in lines 20 through 50 one of which we only see lines 20 through about 37 here in this",
    "start": "519069",
    "end": "527199"
  },
  {
    "text": "image so they're two variable score and max or these are used to store the user",
    "start": "527199",
    "end": "533170"
  },
  {
    "text": "score and the maximum possible scores for this item",
    "start": "533170",
    "end": "538440"
  },
  {
    "start": "537000",
    "end": "537000"
  },
  {
    "text": "so we actually evolved this platform this Benjamin has mentioned over several",
    "start": "538440",
    "end": "543730"
  },
  {
    "text": "years this is the very first iteration of the learning analytics platform and basically it's lap 100 and it's a",
    "start": "543730",
    "end": "552040"
  },
  {
    "text": "cluster of nodejs servers we were able to ingest a fairly large volume of",
    "start": "552040",
    "end": "557769"
  },
  {
    "text": "learner data and then process aggregations and then write these",
    "start": "557769",
    "end": "563019"
  },
  {
    "text": "aggregations to MongoDB and then to provide the connect inside reports",
    "start": "563019",
    "end": "568410"
  },
  {
    "text": "however after performance testing we realized this architecture would not scaled and subsequently we have multiple",
    "start": "568410",
    "end": "576430"
  },
  {
    "text": "iterations on this architecture of their span of about two years resulting in",
    "start": "576430",
    "end": "581529"
  },
  {
    "text": "iterations of lap 1.1 through lap 1.5 for lap 1.1 we applied a classic",
    "start": "581529",
    "end": "589930"
  },
  {
    "text": "software engineering solution which is basically to put a queue between the nodejs servers in the database Amazon",
    "start": "589930",
    "end": "597430"
  },
  {
    "text": "sqs but unfortunately again we weren't really able to hit that peak load that we committed to which was the 10,000",
    "start": "597430",
    "end": "604029"
  },
  {
    "text": "events per second so when we looked at this we realized we probably needed to decouple the",
    "start": "604029",
    "end": "610540"
  },
  {
    "text": "ingestion further from their processing and storage",
    "start": "610540",
    "end": "615240"
  },
  {
    "text": "so for lap 1.2 we had some difficulties with MongoDB",
    "start": "619079",
    "end": "625240"
  },
  {
    "text": "tuning for conditional inserts and different types of aggregations the crux",
    "start": "625240",
    "end": "630910"
  },
  {
    "text": "of the matter was we really didn't have the institutional knowledge with DB so processing was modified to",
    "start": "630910",
    "end": "638259"
  },
  {
    "text": "basically pre a get with aggregate events in f3 so that's when a full set",
    "start": "638259",
    "end": "643660"
  },
  {
    "text": "of aggregations were compiled and they were loaded into MongoDB this solve",
    "start": "643660",
    "end": "649930"
  },
  {
    "text": "basically the MongoDB performance issues that introduced some subtle data loss",
    "start": "649930",
    "end": "655089"
  },
  {
    "text": "problems this is lap 1.3 basically we have event",
    "start": "655089",
    "end": "661000"
  },
  {
    "text": "grouping and pre aggregation which we migrated over to Amazon DynamoDB",
    "start": "661000",
    "end": "666720"
  },
  {
    "text": "the data consistency problem went away and this resolved really our data loss",
    "start": "666720",
    "end": "673170"
  },
  {
    "text": "issues that we were encountering when we still had some minor bugs in the system",
    "start": "673170",
    "end": "678390"
  },
  {
    "text": "so laughs board that four and 1.5 were about stability and fixing the bugs that",
    "start": "678390",
    "end": "683890"
  },
  {
    "text": "we had so um when we started to talk that this architecture though we realized that we",
    "start": "683890",
    "end": "690460"
  },
  {
    "text": "had customized it to just be able to deliver those connect insight reports we",
    "start": "690460",
    "end": "696190"
  },
  {
    "text": "needed really a more general solution to be able to accept all of the events as",
    "start": "696190",
    "end": "702670"
  },
  {
    "text": "well as even external applications that would provide events to us and we also",
    "start": "702670",
    "end": "708430"
  },
  {
    "text": "did an estimate of what it's going to cost for 1 billion events that was about",
    "start": "708430",
    "end": "715240"
  },
  {
    "text": "430 K so we realized that what's the many different types of applications",
    "start": "715240",
    "end": "720430"
  },
  {
    "text": "that we have within our portfolio in concert with the overall sort of that we",
    "start": "720430",
    "end": "728170"
  },
  {
    "text": "wanted to support third-party applications that this really would quickly exceed the national",
    "start": "728170",
    "end": "734710"
  },
  {
    "text": "debt that the rate we were going so hence the reason that we really needs",
    "start": "734710",
    "end": "741130"
  },
  {
    "text": "to think about the architecture to re-architect it as a cost-effective highly scalable and reliable",
    "start": "741130",
    "end": "747540"
  },
  {
    "text": "architecture so if he fast Ford this is learning",
    "start": "747540",
    "end": "752710"
  },
  {
    "start": "750000",
    "end": "750000"
  },
  {
    "text": "analytics platform 27 2016 we've completely rerp tected and implemented a",
    "start": "752710",
    "end": "759670"
  },
  {
    "text": "robust solution which has integrated many of the Amazon components including",
    "start": "759670",
    "end": "764830"
  },
  {
    "text": "our AWS API gateway lambda Kinesis streams simple storage",
    "start": "764830",
    "end": "771490"
  },
  {
    "text": "services and elastic search services dynamodb and RDS Postgres Siebel the",
    "start": "771490",
    "end": "779290"
  },
  {
    "text": "architecture is composed of following four main elements input output",
    "start": "779290",
    "end": "785400"
  },
  {
    "text": "reconciled and stream processing layer the infant API is basically the",
    "start": "785400",
    "end": "792010"
  },
  {
    "text": "interface for the events and other domain events for the learning analytics",
    "start": "792010",
    "end": "797500"
  },
  {
    "text": "platform the main events will be basically a superset of calliper events",
    "start": "797500",
    "end": "803130"
  },
  {
    "text": "they are intended to capture information that may not be contained within the caliper specification",
    "start": "803130",
    "end": "810330"
  },
  {
    "text": "the reconcile API basically maintains the audit trail and it's used to",
    "start": "810330",
    "end": "816130"
  },
  {
    "text": "playback events then the output API is used for supporting queries to build a reports",
    "start": "816130",
    "end": "822400"
  },
  {
    "text": "like to connect inside reports then we have what's this stream processing layer this is actually the",
    "start": "822400",
    "end": "829600"
  },
  {
    "text": "most important element because this is where we aggregate process and store events to storage mechanism as well as",
    "start": "829600",
    "end": "836080"
  },
  {
    "text": "provide additional analytics the HSU is where we run a lot of our research algorithms on the event data so for",
    "start": "836080",
    "end": "844480"
  },
  {
    "text": "example we may have like a cheating detection algorithm so we before we dive",
    "start": "844480",
    "end": "849550"
  },
  {
    "text": "further into each layer of the architecture benjamin is going to give us an overview of the Amazon services",
    "start": "849550",
    "end": "854920"
  },
  {
    "text": "used Thank You Terry so I wanted to provide an overview of",
    "start": "854920",
    "end": "862090"
  },
  {
    "start": "858000",
    "end": "858000"
  },
  {
    "text": "some of the services that feature in the learning analytics platform but the idea here is not to do a deep dive into any",
    "start": "862090",
    "end": "868420"
  },
  {
    "text": "of them this is more other an intro for the benefit of those of us who perhaps haven't had a chance yet to to work with",
    "start": "868420",
    "end": "874990"
  },
  {
    "text": "them some of them like s3 and RDS and dynamodb have been around for a while",
    "start": "874990",
    "end": "880120"
  },
  {
    "text": "longer and so I'll move through them at a quicker pace I'd like to start with Amazon API",
    "start": "880120",
    "end": "885820"
  },
  {
    "text": "gateway API gateway is a fully managed service that is makes it easy for",
    "start": "885820",
    "end": "891010"
  },
  {
    "text": "developers to create publish maintain monitor and secure API at any scale",
    "start": "891010",
    "end": "898200"
  },
  {
    "start": "898000",
    "end": "898000"
  },
  {
    "text": "API gateway lets you switch between or combine multiple backends",
    "start": "898200",
    "end": "903750"
  },
  {
    "text": "and it similarly it allows you to you work using multiple versions of the same",
    "start": "903750",
    "end": "910240"
  },
  {
    "text": "API so that lets you test and release and iterate through multiple versions",
    "start": "910240",
    "end": "918330"
  },
  {
    "text": "API gateway provides network protection and that's something that we do very",
    "start": "918330",
    "end": "924010"
  },
  {
    "text": "well because it requires hyperscale so while you may not be able to auto scale",
    "start": "924010",
    "end": "929080"
  },
  {
    "text": "your application to mitigate an attack API Gateway can and it will provide that layer of protection",
    "start": "929080",
    "end": "936540"
  },
  {
    "text": "another functionality is that it allows you to authenticate access into your api's using some identity tools that AWS",
    "start": "936870",
    "end": "944710"
  },
  {
    "text": "has in its platform like I am identity access management and Amazon Cognito so",
    "start": "944710",
    "end": "950050"
  },
  {
    "text": "API gateway will authorize access to your API using those tools if you've ever worked against any AWS service API",
    "start": "950050",
    "end": "958060"
  },
  {
    "text": "you may be familiar with cig v4 SiC v4 is an authentication algorithm that lets",
    "start": "958060",
    "end": "963760"
  },
  {
    "text": "you sign API calls with authentication information so with API gateway we've",
    "start": "963760",
    "end": "970000"
  },
  {
    "text": "essentially extended our identity service so that you now can authorize access to your API using sig v4 another",
    "start": "970000",
    "end": "977050"
  },
  {
    "text": "common use case is making your API into a business where you can provide third-party developers",
    "start": "977050",
    "end": "983920"
  },
  {
    "text": "with authentication information and then throttle meter or capped their usage on",
    "start": "983920",
    "end": "990520"
  },
  {
    "text": "a daily basis this depicts how API gateway fits into the ecosystem what happens is a",
    "start": "990520",
    "end": "997810"
  },
  {
    "start": "992000",
    "end": "992000"
  },
  {
    "text": "CloudFront distribution is created for every API you won't see that cloud fund distribution in your account and you",
    "start": "997810",
    "end": "1003780"
  },
  {
    "text": "won't be charged for it and then clients will hit your API through the cloud front distribution through its network",
    "start": "1003780",
    "end": "1010230"
  },
  {
    "text": "of edge locations API gate we can be set to cache responses but when it does need to hit",
    "start": "1010230",
    "end": "1017520"
  },
  {
    "text": "the backend API that can be a lambda function it can be an HTTP endpoint running on ec2 or anything else that is",
    "start": "1017520",
    "end": "1024390"
  },
  {
    "text": "publicly accessible with cloud watch you can get metrics about your API usage including down to",
    "start": "1024390",
    "end": "1030870"
  },
  {
    "text": "the individual method level the next service I'd like to talk about is lambda AWS lambda is a service that",
    "start": "1030870",
    "end": "1038400"
  },
  {
    "text": "allows you to run code without provisioning or managing service at all with lambda you only pay for the compute",
    "start": "1038400",
    "end": "1045300"
  },
  {
    "text": "cycles that you consume and you consume them in a hundred millisecond increments there is no charge for when your code is",
    "start": "1045300",
    "end": "1051180"
  },
  {
    "text": "not running lambda can support virtually any type of application or back-end service all with",
    "start": "1051180",
    "end": "1056880"
  },
  {
    "text": "zero administration and lambda is truly at the heart of service computing it will completely abstract away all of the",
    "start": "1056880",
    "end": "1064200"
  },
  {
    "start": "1058000",
    "end": "1058000"
  },
  {
    "text": "underlying infrastructure lambda will automatically scale your application by running code in response to each trigger",
    "start": "1064200",
    "end": "1071100"
  },
  {
    "text": "so from your perspective it is the same level of effort that is associated with",
    "start": "1071100",
    "end": "1076110"
  },
  {
    "text": "running your function once or once every hour or thousands of times per second",
    "start": "1076110",
    "end": "1082340"
  },
  {
    "text": "lambda currently supports code written in nodejs Python and Java you simply",
    "start": "1082340",
    "end": "1088140"
  },
  {
    "text": "upload your code specify how you wish to invoke it and your code can include existing libraries and even native ones",
    "start": "1088140",
    "end": "1095900"
  },
  {
    "start": "1095000",
    "end": "1095000"
  },
  {
    "text": "so this concept of being event-driven that event can be an object landing in",
    "start": "1095900",
    "end": "1101790"
  },
  {
    "text": "s3 or a dynamodb update an api gateway call or even a scheduled job similar to",
    "start": "1101790",
    "end": "1107910"
  },
  {
    "text": "a cron job lambda will then execute your code and provide you insight into your metrics",
    "start": "1107910",
    "end": "1114360"
  },
  {
    "text": "and logs through cloud watch when that code is executing it actually",
    "start": "1114360",
    "end": "1120690"
  },
  {
    "text": "access other AWS services whether inside or outside of your V PC so one example",
    "start": "1120690",
    "end": "1127110"
  },
  {
    "text": "that we we sometimes use to kind of demonstrate how all of this ties together is to imagine a workflow where",
    "start": "1127110",
    "end": "1133260"
  },
  {
    "text": "an image lands into s3 and that triggers a lambda function that creates a",
    "start": "1133260",
    "end": "1138480"
  },
  {
    "text": "thumbnail of that image and that gets stored back into s3 and if you wanted to",
    "start": "1138480",
    "end": "1144390"
  },
  {
    "text": "the thumbnail being stored and s3 could be a trigger for a separate lambda function and so it's easy to imagine how",
    "start": "1144390",
    "end": "1150480"
  },
  {
    "text": "with these isolated functions and these events you can create either very simple",
    "start": "1150480",
    "end": "1155790"
  },
  {
    "text": "or very complex workflows the next service I'll talk about is Kinesis streams Kaneesha streams allows",
    "start": "1155790",
    "end": "1163470"
  },
  {
    "text": "you to build custom applications that process or analyze streaming data in near real-time",
    "start": "1163470",
    "end": "1169940"
  },
  {
    "text": "Kaneesha streams will continuously capture and store up to terabytes of data per hour from hundreds of thousands",
    "start": "1169940",
    "end": "1176430"
  },
  {
    "text": "of sources so things like website click streams financial transactions social",
    "start": "1176430",
    "end": "1181980"
  },
  {
    "text": "media feeds or in the example that we're talking about today learning events some customers think about Kinesis",
    "start": "1181980",
    "end": "1189090"
  },
  {
    "text": "streams as Amazon's fully managed alternative to running Apache Kafka on ec2 you may have also heard of services",
    "start": "1189090",
    "end": "1196800"
  },
  {
    "text": "called Kinesis fire hose and Kinesis analytic those are beyond the scope of what we'll talk about today but the",
    "start": "1196800",
    "end": "1202890"
  },
  {
    "text": "takeaway is that Kinesis has evolved into being a true platform for handling real-time",
    "start": "1202890",
    "end": "1208800"
  },
  {
    "text": "streaming data if we look at how Kinesis streams will make large-scale data ingest easy on the",
    "start": "1208800",
    "end": "1217170"
  },
  {
    "text": "one hand we have data producers and you will configure your producers to push data into the kanesha stream and then",
    "start": "1217170",
    "end": "1225000"
  },
  {
    "text": "you can have multiple consumers of those data that will read and process you have full",
    "start": "1225000",
    "end": "1231990"
  },
  {
    "text": "control over how the stream is organized how you partition your data how you",
    "start": "1231990",
    "end": "1237150"
  },
  {
    "text": "scale your stream and how you process the data so if you look at the technologies by",
    "start": "1237150",
    "end": "1243330"
  },
  {
    "start": "1241000",
    "end": "1241000"
  },
  {
    "text": "which we can send data into the stream they consist of a variety of technologies all the way from the AWS",
    "start": "1243330",
    "end": "1250110"
  },
  {
    "text": "SDKs the Amazon Kinesis producer library and a variety of open source",
    "start": "1250110",
    "end": "1256710"
  },
  {
    "text": "technologies like log4j and flume and fluent D and on the other hand we have a",
    "start": "1256710",
    "end": "1262410"
  },
  {
    "text": "variety of technologies that will consume the data so all the way up from raw API calls to high-level methods like",
    "start": "1262410",
    "end": "1269220"
  },
  {
    "text": "the Kinesis client library or Kinesis analytics or lambda the Kinesis client",
    "start": "1269220",
    "end": "1275670"
  },
  {
    "text": "library is an open source one that will make the task of writing your stream",
    "start": "1275670",
    "end": "1281550"
  },
  {
    "text": "processing application much easier and then finally we have open source",
    "start": "1281550",
    "end": "1286970"
  },
  {
    "text": "technologies like apache storm or spark that have the native capacity to consume",
    "start": "1286970",
    "end": "1292650"
  },
  {
    "text": "the stream I'd like to talk about elasticsearch but before we get into the elasticsearch",
    "start": "1292650",
    "end": "1298980"
  },
  {
    "text": "service that we operate let's talk about it in a little bit of a broader context elasticsearch is an open source search",
    "start": "1298980",
    "end": "1305820"
  },
  {
    "text": "engine it's around since 2009 it's been very successful with over 120 million",
    "start": "1305820",
    "end": "1311820"
  },
  {
    "text": "downloads to date a traditional search engine will take in documents and then find relevant matches based on textual",
    "start": "1311820",
    "end": "1319320"
  },
  {
    "text": "queries from very early on elastic search really specialized to",
    "start": "1319320",
    "end": "1324600"
  },
  {
    "text": "provide in gist and analysis of log files of log lines so things like Apache",
    "start": "1324600",
    "end": "1330200"
  },
  {
    "text": "web logs or system logs or applique in logs and you may have heard of what is",
    "start": "1330200",
    "end": "1336160"
  },
  {
    "text": "commonly referred to as an elk stack which combines elasticsearch and open-source log stash and Cabana and",
    "start": "1336160",
    "end": "1343030"
  },
  {
    "text": "this trio has become the de facto standard in real-time monitoring and analysis so one thing that we kept",
    "start": "1343030",
    "end": "1350770"
  },
  {
    "text": "hearing from customers is we love elasticsearch but the task of creating and managing these clusters is we see it",
    "start": "1350770",
    "end": "1358090"
  },
  {
    "text": "as consuming time consuming and undifferentiated and so enter Amazon",
    "start": "1358090",
    "end": "1364150"
  },
  {
    "start": "1362000",
    "end": "1362000"
  },
  {
    "text": "Elastic search service so the elastic search service allows you to set up and configure your elastic search cluster in",
    "start": "1364150",
    "end": "1371260"
  },
  {
    "text": "minutes the service will provision all of the resources that are needed and will launch them the service can help",
    "start": "1371260",
    "end": "1377830"
  },
  {
    "text": "you scale by making either an API call or using the console",
    "start": "1377830",
    "end": "1382950"
  },
  {
    "text": "with the elastic search service you get access to the elastic search open source",
    "start": "1382950",
    "end": "1389140"
  },
  {
    "text": "API so that any code or existing applications that you have that already using elastic search will work out of",
    "start": "1389140",
    "end": "1395860"
  },
  {
    "text": "the box or in other words it is a is designed as a drop-in replacement for your existing elastic search cluster the",
    "start": "1395860",
    "end": "1403630"
  },
  {
    "start": "1397000",
    "end": "1397000"
  },
  {
    "text": "service will detect and replace failed elastic search nodes and that reduces the overhead that is associated with",
    "start": "1403630",
    "end": "1409990"
  },
  {
    "text": "managing them if you turn on zone awareness it will provide high availability by spreading node members",
    "start": "1409990",
    "end": "1417250"
  },
  {
    "text": "across two availability zones and with built-in features like taking snapshots",
    "start": "1417250",
    "end": "1422370"
  },
  {
    "text": "your data is highly durable as well and also we provide security for the cluster",
    "start": "1422370",
    "end": "1429040"
  },
  {
    "text": "down to the index level and it will integrate with im4 policy monitoring",
    "start": "1429040",
    "end": "1434410"
  },
  {
    "text": "with cloud watch and auditing with cloud trail so one of the greatest benefits of the",
    "start": "1434410",
    "end": "1441100"
  },
  {
    "start": "1438000",
    "end": "1438000"
  },
  {
    "text": "elastic search service is it's in native integrations for ingest with AWS",
    "start": "1441100",
    "end": "1446350"
  },
  {
    "text": "services so things like cloud watch logs or Kinesis firehose or cloud formation",
    "start": "1446350",
    "end": "1451650"
  },
  {
    "text": "you can also ingest data using lambda and that will connect you to things like s3 and dynamodb and you can also connect",
    "start": "1451650",
    "end": "1460240"
  },
  {
    "text": "logstash to an elastic search cluster so here you can see a data flow diagram",
    "start": "1460240",
    "end": "1465910"
  },
  {
    "text": "that the some of the ways that elasticsearch service can integrate with other services",
    "start": "1465910",
    "end": "1473490"
  },
  {
    "start": "1473000",
    "end": "1473000"
  },
  {
    "text": "so I imagined that at this point stre requires little introduction but just as",
    "start": "1473490",
    "end": "1478510"
  },
  {
    "text": "a very quick refresher s3 simple storage service provides developers and IT teams with secure",
    "start": "1478510",
    "end": "1486400"
  },
  {
    "text": "durable highly scalable cloud storage s3 is very easy to use object storage with",
    "start": "1486400",
    "end": "1492820"
  },
  {
    "text": "a simple web interface that can allows you to store and retrieve any amount of",
    "start": "1492820",
    "end": "1498760"
  },
  {
    "text": "data from anywhere on the web with s3 you only pay for the stories that you",
    "start": "1498760",
    "end": "1503950"
  },
  {
    "text": "actually use and so there's no planning associated with any spikes in usage or",
    "start": "1503950",
    "end": "1509530"
  },
  {
    "text": "storage growth so s3 is designed really for",
    "start": "1509530",
    "end": "1515860"
  },
  {
    "start": "1512000",
    "end": "1512000"
  },
  {
    "text": "applications to access the data directly not through an operating system or a file system and that makes it uniquely",
    "start": "1515860",
    "end": "1522370"
  },
  {
    "text": "suited for big data applications so unlike things like HDFS there's no need",
    "start": "1522370",
    "end": "1528040"
  },
  {
    "text": "to run a computer just for the storage if you have multiple consumers of the",
    "start": "1528040",
    "end": "1533560"
  },
  {
    "text": "same data sets things like spark or hive or presto that's a perfect use case for",
    "start": "1533560",
    "end": "1539110"
  },
  {
    "text": "s3 because they can all access the same shared data sets securely and one of the",
    "start": "1539110",
    "end": "1544870"
  },
  {
    "text": "biggest advantages of s3 in our big data environment is you can store literally",
    "start": "1544870",
    "end": "1550060"
  },
  {
    "text": "any amount of data and very high bandwidth with really no aggregate throughput limits",
    "start": "1550060",
    "end": "1557070"
  },
  {
    "start": "1558000",
    "end": "1558000"
  },
  {
    "text": "so like s3 I imagine that RDS requires no introduction at this point as well so",
    "start": "1558450",
    "end": "1563770"
  },
  {
    "text": "as a quick reminder RDS will make it easy to set up operate and scale a",
    "start": "1563770",
    "end": "1569140"
  },
  {
    "text": "relational database in the cloud so it provides very cost efficient and resizable capacity and it frees you up",
    "start": "1569140",
    "end": "1576130"
  },
  {
    "text": "from some of the database administration administrator to tasks like so it frees",
    "start": "1576130",
    "end": "1582760"
  },
  {
    "text": "you up to focus on your applications and your data and your business RDS supports a variety of different",
    "start": "1582760",
    "end": "1589690"
  },
  {
    "text": "engines all the way from open source options to commercial ones in today's",
    "start": "1589690",
    "end": "1595120"
  },
  {
    "text": "talk we will be focusing on RDS Postgres because that is the engine that's being used the learning analytics platform but",
    "start": "1595120",
    "end": "1602410"
  },
  {
    "text": "with all the different engines the value proposition is similar so with RDS you can deploy scaleable",
    "start": "1602410",
    "end": "1609790"
  },
  {
    "text": "Postgres in minutes with cost efficient and resizable hardware capacity",
    "start": "1609790",
    "end": "1616110"
  },
  {
    "text": "RDS will manage the complex and time-consuming tasks like the Postgres",
    "start": "1616110",
    "end": "1621580"
  },
  {
    "text": "software install or upgrades or Storage Management or replication for high",
    "start": "1621580",
    "end": "1626800"
  },
  {
    "text": "availability or backups RDS Postgres can scale from 5 gigabytes",
    "start": "1626800",
    "end": "1632470"
  },
  {
    "text": "to 6 terabytes and from a thousand iOS to 30,000 it also supports read replicas",
    "start": "1632470",
    "end": "1640510"
  },
  {
    "text": "in both local and remote regions the last service that our cover is",
    "start": "1640510",
    "end": "1646780"
  },
  {
    "text": "DynamoDB DynamoDB is a fast and flexible no sequel database it's designed for any",
    "start": "1646780",
    "end": "1654460"
  },
  {
    "text": "type of application including Tier one applications that require very consistent single-digit latency sorry",
    "start": "1654460",
    "end": "1661600"
  },
  {
    "text": "single-digit millisecond latency at virtually any scale it's a fully managed service which means there's no",
    "start": "1661600",
    "end": "1667870"
  },
  {
    "text": "administration and zero infrastructure for you to to manage",
    "start": "1667870",
    "end": "1673140"
  },
  {
    "text": "DynamoDB supports both a document and a key value model store sorry key value store model and",
    "start": "1673140",
    "end": "1679690"
  },
  {
    "text": "durability is achieved by automatically replicating data across three different facilities",
    "start": "1679690",
    "end": "1686130"
  },
  {
    "text": "Dynamo is often a natural choice for big data application architectures and the",
    "start": "1686130",
    "end": "1692590"
  },
  {
    "text": "reasons are that the no sequel nature and the durability and availability that",
    "start": "1692590",
    "end": "1699580"
  },
  {
    "text": "is provided as part of the service are extremely supportive of very large datasets from multiple sources",
    "start": "1699580",
    "end": "1706950"
  },
  {
    "text": "when you create a table you need to specify how much request capacity you",
    "start": "1706950",
    "end": "1712090"
  },
  {
    "start": "1707000",
    "end": "1707000"
  },
  {
    "text": "need and so if that ever changes you can always change the read or the right capacity that you need by making an API",
    "start": "1712090",
    "end": "1719770"
  },
  {
    "text": "call or through the console we have customers that run over a million requests per second on a single table",
    "start": "1719770",
    "end": "1727390"
  },
  {
    "text": "and more than a hundred terabytes of worth of data so the takeaway is that",
    "start": "1727390",
    "end": "1734470"
  },
  {
    "text": "you're no longer preoccupied with horizontally scaling a no sequel database you will simply continue the",
    "start": "1734470",
    "end": "1740680"
  },
  {
    "text": "right to dynamo DB and with that I'll hand it back to Tara Thank You Benjamin",
    "start": "1740680",
    "end": "1747570"
  },
  {
    "text": "so let's take a look at the four elements of the architecture input output and reconcile API and then the",
    "start": "1747870",
    "end": "1754630"
  },
  {
    "text": "stream processing layer this is the input API as mentioned previously it's",
    "start": "1754630",
    "end": "1760360"
  },
  {
    "start": "1756000",
    "end": "1756000"
  },
  {
    "text": "really the front door for our system it's implemented using a simple rest",
    "start": "1760360",
    "end": "1765550"
  },
  {
    "text": "endpoint to receive one or more up to 300 caliper events in a single request",
    "start": "1765550",
    "end": "1771180"
  },
  {
    "text": "these events that are validated against an optional user supplied schema and if",
    "start": "1771180",
    "end": "1777010"
  },
  {
    "text": "it's successful then the record is published to Amazon Kinesis stream a primary rut audit record is written and",
    "start": "1777010",
    "end": "1784150"
  },
  {
    "text": "finally a receipt is generated and returned back to the event producer let's walk through some of the steps of",
    "start": "1784150",
    "end": "1790540"
  },
  {
    "text": "this micro service and note that a similar pattern is used for the output reconcile API and stream processing",
    "start": "1790540",
    "end": "1796690"
  },
  {
    "text": "layer step one this is the event producer this is the element that actually generates",
    "start": "1796690",
    "end": "1803230"
  },
  {
    "text": "the calliper events and in the future we'll be generating the domain events and then the input API",
    "start": "1803230",
    "end": "1809730"
  },
  {
    "text": "so the event producer generates thousands of calliper events per second Dustin for the input API and then we",
    "start": "1809730",
    "end": "1816670"
  },
  {
    "text": "receive these events through the public endpoint the event producers signed a set of AWS",
    "start": "1816670",
    "end": "1823690"
  },
  {
    "text": "credentials and we use these values to apply signe for against the request",
    "start": "1823690",
    "end": "1829890"
  },
  {
    "text": "step two the API gateway authentication and producer credential enable Sigma for",
    "start": "1829890",
    "end": "1836440"
  },
  {
    "text": "processing in the API gateway by setting the authorization type equal to AWS I am",
    "start": "1836440",
    "end": "1842830"
  },
  {
    "text": "from that point all calls made against the employee are required to supply a",
    "start": "1842830",
    "end": "1847900"
  },
  {
    "text": "valid authorization header in the request step three is lambda invocation the API",
    "start": "1847900",
    "end": "1855580"
  },
  {
    "text": "gateway service then executes the input lambda function when an inbound request is processed",
    "start": "1855580",
    "end": "1861900"
  },
  {
    "text": "step forth the authorized user metadata one of the educated user invokes a",
    "start": "1861900",
    "end": "1867580"
  },
  {
    "text": "lambda function there's metadata about this particular request so for example",
    "start": "1867580",
    "end": "1873250"
  },
  {
    "text": "there may be source property that is not carried within the body of the message itself and we store this information in",
    "start": "1873250",
    "end": "1879340"
  },
  {
    "text": "a separate mhe system step 5 it's a schema validation where",
    "start": "1879340",
    "end": "1885460"
  },
  {
    "text": "the architecture validates the calliper event against the schema then step 6 is about the Kinesis",
    "start": "1885460",
    "end": "1893800"
  },
  {
    "text": "publishing the calliper event has been published to the amazon Kinesis stream the service requires that we set",
    "start": "1893800",
    "end": "1900250"
  },
  {
    "text": "explicit permissions which let's execute them to put object requests the present",
    "start": "1900250",
    "end": "1907030"
  },
  {
    "text": "presence of this record in Kinesis means that the input api has accepted the event and successfully written it to",
    "start": "1907030",
    "end": "1913720"
  },
  {
    "text": "Kinesis for the further processing step 7 Todd watch metrics and logging",
    "start": "1913720",
    "end": "1921370"
  },
  {
    "text": "each service publishes mess metrics and output logging to cloud watch",
    "start": "1921370",
    "end": "1926430"
  },
  {
    "text": "step 8 is about continuous integration we use Jenkins as a simple continuous",
    "start": "1926430",
    "end": "1932200"
  },
  {
    "text": "integration service for building and deploying or code step 9 is about DevOps access which is",
    "start": "1932200",
    "end": "1939580"
  },
  {
    "text": "really providing access to the set of micro services that we have as well as",
    "start": "1939580",
    "end": "1944710"
  },
  {
    "text": "any of the Amazon services that are used the output API is basically the service",
    "start": "1944710",
    "end": "1950770"
  },
  {
    "text": "that's responsible again for enabling data consumers to query the data in a variety of ways so the query results",
    "start": "1950770",
    "end": "1958630"
  },
  {
    "text": "that are used in the visualizations or the reports it has to be lightweight enough which of their has to be just",
    "start": "1958630",
    "end": "1964930"
  },
  {
    "text": "enough infrastructure around it so that the queried process is able to give the users full control over the chords that",
    "start": "1964930",
    "end": "1970540"
  },
  {
    "text": "they have written so each endpoint actually takes the form of a caliper event type and a caliper",
    "start": "1970540",
    "end": "1976960"
  },
  {
    "text": "action value in this way it's possible to query all different caliper events using a very simple URL structure",
    "start": "1976960",
    "end": "1985650"
  },
  {
    "text": "so from step one the event consumer or user or the application interested in",
    "start": "1985650",
    "end": "1991390"
  },
  {
    "text": "extracting the information stored in elasticsearch we receive these query requests through the public endpoint",
    "start": "1991390",
    "end": "1998179"
  },
  {
    "text": "then step three is the invocation of the output Lamba and then step six is the",
    "start": "1998179",
    "end": "2003669"
  },
  {
    "text": "output api lamda that reads the data stores including an elastic search cluster dynamodb s3 and Postgres RDS",
    "start": "2003669",
    "end": "2013350"
  },
  {
    "text": "the reconcile api and this is actually super critical for us this is where we",
    "start": "2013350",
    "end": "2018549"
  },
  {
    "start": "2014000",
    "end": "2014000"
  },
  {
    "text": "maintain the audit of Laurel all the events that go throughout our system and",
    "start": "2018549",
    "end": "2024309"
  },
  {
    "text": "it's basically sorted an elastic search cluster what's really critical for us is to be",
    "start": "2024309",
    "end": "2031749"
  },
  {
    "text": "able to identify any sort of processing failures that may be occur to look locate also to this archive the event in",
    "start": "2031749",
    "end": "2038889"
  },
  {
    "text": "the if we have to play it back and also to distort as a record for any sort of",
    "start": "2038889",
    "end": "2046840"
  },
  {
    "text": "additional audit trail information we may have to provide to a school for stream processing this is really the",
    "start": "2046840",
    "end": "2054490"
  },
  {
    "start": "2051000",
    "end": "2051000"
  },
  {
    "text": "secret sauce so this is the consumer of the events from Kinesis and the output",
    "start": "2054490",
    "end": "2060040"
  },
  {
    "text": "of events to some sort of a storage mechanism the services perform the filtering and",
    "start": "2060040",
    "end": "2065919"
  },
  {
    "text": "transformation of the messages and this is also to where I mentioned we performed we run various algorithms",
    "start": "2065919",
    "end": "2072190"
  },
  {
    "text": "against it so we may have like an adaptive tutoring system gaming or some",
    "start": "2072190",
    "end": "2077648"
  },
  {
    "text": "additional homework support system we run the algorithms then in the",
    "start": "2077649",
    "end": "2083049"
  },
  {
    "text": "processing lambda so I want to go over some of the architecture trade-offs that we made",
    "start": "2083049",
    "end": "2089108"
  },
  {
    "start": "2085000",
    "end": "2085000"
  },
  {
    "text": "while building the lab system we've looked at Amazon API gateway and",
    "start": "2089109",
    "end": "2095138"
  },
  {
    "text": "we actually we did compare it to writing all the code that we would have needed to receive HTTP requests authenticated",
    "start": "2095139",
    "end": "2103809"
  },
  {
    "text": "process it started to think about what it would take to auto scale the instances based",
    "start": "2103809",
    "end": "2110020"
  },
  {
    "text": "on CPU utilization and came to the rapid conclusion that was just crazy so we",
    "start": "2110020",
    "end": "2118240"
  },
  {
    "text": "decided to go ahead and move forward with the input API the pro it's really is highly integrated it's integrated",
    "start": "2118240",
    "end": "2125260"
  },
  {
    "text": "with all the other downstream services that we use it automatically scales for us",
    "start": "2125260",
    "end": "2131550"
  },
  {
    "text": "it handles the thousands of concurrent calls that we need and also to provides all the other elements traffic",
    "start": "2131550",
    "end": "2137340"
  },
  {
    "text": "management authorization access control the con that we encounter was that it",
    "start": "2137340",
    "end": "2143280"
  },
  {
    "text": "does not support gzip compression so if you think that where we may have a event",
    "start": "2143280",
    "end": "2149840"
  },
  {
    "text": "requests with many different events in it with over 300 events in it potentially scaling up to 500 that's you",
    "start": "2149840",
    "end": "2158490"
  },
  {
    "text": "know those messages actually get quite large and so without gzip compression",
    "start": "2158490",
    "end": "2163910"
  },
  {
    "text": "it's a little bit problematic for us one workaround this is set up cloud front",
    "start": "2163910",
    "end": "2169940"
  },
  {
    "text": "and to enable compression to that another architecture trade-off is with",
    "start": "2169940",
    "end": "2177720"
  },
  {
    "start": "2174000",
    "end": "2174000"
  },
  {
    "text": "lambis so we looked at what it would take to actually build out all the ec2 instances and then compared that that",
    "start": "2177720",
    "end": "2185940"
  },
  {
    "text": "with AWS lambda and what we really liked about it was that lambda is a low cost",
    "start": "2185940",
    "end": "2191700"
  },
  {
    "text": "approach we wouldn't have to provision the thousands and thousands of ec2 instances for this type of architecture",
    "start": "2191700",
    "end": "2198619"
  },
  {
    "text": "also - we only paid for the compute time so there's really isn't any charge when",
    "start": "2198619",
    "end": "2204930"
  },
  {
    "text": "the code is not running and this is again super critical for us to maintain that low price point so the other thing",
    "start": "2204930",
    "end": "2212190"
  },
  {
    "text": "is that we really run the code without it a with zero administration those servers no instances so we don't have to",
    "start": "2212190",
    "end": "2218280"
  },
  {
    "text": "turn around toward a Bob's team and say hey guess what we need a bunch of ec2 instances so it really it's been great for the",
    "start": "2218280",
    "end": "2226410"
  },
  {
    "text": "developers it continuously scales it had it automatically scales based on the",
    "start": "2226410",
    "end": "2232080"
  },
  {
    "text": "needs of our application particularly to when we have the great volume of students who hop on the system let's say",
    "start": "2232080",
    "end": "2237570"
  },
  {
    "text": "Monday morning to submit their homework assignments also - we really like that it's easier",
    "start": "2237570",
    "end": "2245369"
  },
  {
    "text": "for the developers to set it up and to use lambda so we selected lambda and the",
    "start": "2245369",
    "end": "2251490"
  },
  {
    "text": "comm though that you know and we plan for it though I don't think we plan enough time we have we had very limited",
    "start": "2251490",
    "end": "2259260"
  },
  {
    "text": "experience with lambda so there is a little bit of a learning curve doll so to our limited to cloud",
    "start": "2259260",
    "end": "2265780"
  },
  {
    "text": "watch for monitoring and the lambda can only handle about six megabytes of data",
    "start": "2265780",
    "end": "2271420"
  },
  {
    "text": "in the request or response body and so we've had issues with performance testing with large data sets then lastly",
    "start": "2271420",
    "end": "2280380"
  },
  {
    "text": "debugging is time consuming as there really aren't any good debug tools so you have to go through the various logs",
    "start": "2280380",
    "end": "2288270"
  },
  {
    "start": "2288000",
    "end": "2288000"
  },
  {
    "text": "Amazon Kinesis streams we compared this to setting up Afghan zookeeper we have",
    "start": "2289740",
    "end": "2295300"
  },
  {
    "text": "we have actually a lot of experience with Kafka and zookeeper we know we",
    "start": "2295300",
    "end": "2300490"
  },
  {
    "text": "needed a streaming that mechanism for processing the high volumes that we would receive again the 10000 events per",
    "start": "2300490",
    "end": "2307599"
  },
  {
    "text": "second and we had we had to think of how we were going to scale this to grow to eventually 15 million events we've got a",
    "start": "2307599",
    "end": "2315760"
  },
  {
    "text": "lot of experience again that I mentioned and that we thought well we're gonna have to be able to predict the storage",
    "start": "2315760",
    "end": "2321369"
  },
  {
    "text": "and volume needs so we also said we had a very serious issue that occurred over",
    "start": "2321369",
    "end": "2327250"
  },
  {
    "text": "the summer time were I call it that Cathy lost its brains we lost all the",
    "start": "2327250",
    "end": "2333070"
  },
  {
    "text": "configuration files and so it took us a couple of hours to restore those",
    "start": "2333070",
    "end": "2338490"
  },
  {
    "text": "so Kinesis it's a managed service it has built-in Crossville availability zone",
    "start": "2338490",
    "end": "2346260"
  },
  {
    "text": "replication and failover and with we'd gone with Kafka we'd actually have to",
    "start": "2346260",
    "end": "2351880"
  },
  {
    "text": "have had set this up and so again Kinesis has all the integrations and all the other downstream services that we",
    "start": "2351880",
    "end": "2358750"
  },
  {
    "text": "need so Kinesis for us acts as a type of queue it allows us to smooth out the",
    "start": "2358750",
    "end": "2364390"
  },
  {
    "text": "sudden burst of datas that we receive it's also to to recognize some of the",
    "start": "2364390",
    "end": "2371440"
  },
  {
    "text": "resiliency functions that you have that are built in there now this is sort of",
    "start": "2371440",
    "end": "2377520"
  },
  {
    "text": "more an oddity if you will that applies to a use case and what may apply to some",
    "start": "2377520",
    "end": "2384730"
  },
  {
    "text": "other unit I use cases but one thing we encountered was that",
    "start": "2384730",
    "end": "2390930"
  },
  {
    "text": "Kinesis may lose records if you don't have setting set correctly would sort of",
    "start": "2390930",
    "end": "2397900"
  },
  {
    "text": "discovered this a little bit after the fact so what happens is that you have a shard",
    "start": "2397900",
    "end": "2403990"
  },
  {
    "text": "in the Kinesis stream that is processed by an instance of the lambda function",
    "start": "2403990",
    "end": "2410770"
  },
  {
    "text": "which consumes the batches of records from the shard and if the lambda function processing if the shard throws",
    "start": "2410770",
    "end": "2418480"
  },
  {
    "text": "an error it will be invoked again with that very same batch of information of records so if the errors continue though",
    "start": "2418480",
    "end": "2425980"
  },
  {
    "text": "it there is an automatic back off and retry but it will walk processing until",
    "start": "2425980",
    "end": "2431710"
  },
  {
    "text": "the batches process successfully so what we encountered was really a data backup",
    "start": "2431710",
    "end": "2437050"
  },
  {
    "text": "in Kinesis and so we had to adjust this up to seven days and it's working quite",
    "start": "2437050",
    "end": "2442450"
  },
  {
    "text": "well for us now for us three you know again this is",
    "start": "2442450",
    "end": "2449380"
  },
  {
    "text": "really kind of a no-brainer decision I mean we just compared it to Rackspace and to OpenStack Swift",
    "start": "2449380",
    "end": "2455970"
  },
  {
    "text": "but really it's very secure it's a managed service very easy to use and",
    "start": "2455970",
    "end": "2461560"
  },
  {
    "text": "expensive the only get started con that we encountered was you do receive SSL",
    "start": "2461560",
    "end": "2466810"
  },
  {
    "text": "mismatch errors if you want to use your own domain name in terms of elasticsearch we compared",
    "start": "2466810",
    "end": "2474490"
  },
  {
    "text": "this with setting up our own leucine and elasticsearch instances",
    "start": "2474490",
    "end": "2479580"
  },
  {
    "text": "but again elasticsearch was really integrated with all the other services that we needed as well as some other",
    "start": "2479580",
    "end": "2486430"
  },
  {
    "text": "services that were contemplating for the future and so elasticsearch has become really our data Lake for all learning",
    "start": "2486430",
    "end": "2493030"
  },
  {
    "text": "events that we receive and we basically we use it then and",
    "start": "2493030",
    "end": "2499830"
  },
  {
    "text": "we have systems in place that monitor the index on elasticsearch and",
    "start": "2499830",
    "end": "2505030"
  },
  {
    "text": "automatically rollover indices when they exceed certain thresholds the con which applies to our use case",
    "start": "2505030",
    "end": "2512770"
  },
  {
    "text": "again is that the Amazon version is typically about a release behind the elasticsearch company it may not be",
    "start": "2512770",
    "end": "2520540"
  },
  {
    "text": "critical to others it's not really critical to us thankfully",
    "start": "2520540",
    "end": "2525900"
  },
  {
    "start": "2525000",
    "end": "2525000"
  },
  {
    "text": "Amazon dynamo dB we compared this again with MongoDB but we had really serious performance",
    "start": "2525900",
    "end": "2533650"
  },
  {
    "text": "issues even after we had tuned it fairly significantly we encountered date of",
    "start": "2533650",
    "end": "2538750"
  },
  {
    "text": "loss what DynamoDB we actually had a lot of teams who had experience with dynamo",
    "start": "2538750",
    "end": "2546000"
  },
  {
    "text": "it's very low cost it's fast and flexible in the sequel datastore it is",
    "start": "2546000",
    "end": "2551080"
  },
  {
    "text": "fully managed we use dynamo DB to maintain metadata about the events to",
    "start": "2551080",
    "end": "2557650"
  },
  {
    "text": "manage our user preferences and to provide what's called an aloft data API",
    "start": "2557650",
    "end": "2564180"
  },
  {
    "text": "we have service endpoints that are called by an application we need to",
    "start": "2564180",
    "end": "2570310"
  },
  {
    "text": "ensure that that launch is not reused the con",
    "start": "2570310",
    "end": "2575400"
  },
  {
    "text": "you need to be aware of is some of the limitations on the row size",
    "start": "2575400",
    "end": "2581680"
  },
  {
    "text": "and that it's limited to one megabyte query however if you use the last evaluated",
    "start": "2581680",
    "end": "2589000"
  },
  {
    "text": "key for the query response you can retrieve more results for an item size dynamo DB allocates",
    "start": "2589000",
    "end": "2595990"
  },
  {
    "text": "resources for your table according to the number of reads or writes and the kept write capacity units that you've",
    "start": "2595990",
    "end": "2602950"
  },
  {
    "text": "specified this is really super important here because I want to talk about an issue that we encountered",
    "start": "2602950",
    "end": "2608610"
  },
  {
    "text": "so if your examples I'm sorry if your items are greater than four kilobytes",
    "start": "2608610",
    "end": "2616570"
  },
  {
    "text": "and you may need some additional capacity in this",
    "start": "2616570",
    "end": "2622410"
  },
  {
    "start": "2622000",
    "end": "2622000"
  },
  {
    "text": "so the last architecture trade-off that we like that was Postgres sequel there",
    "start": "2622560",
    "end": "2629530"
  },
  {
    "text": "are actually three different sort of facets to this that we compared the first was you know can it handle handle",
    "start": "2629530",
    "end": "2636850"
  },
  {
    "text": "the number of concurrent connections second was can it actually",
    "start": "2636850",
    "end": "2642610"
  },
  {
    "text": "compile and will will be able to use some advanced analytical queries against it and then third was does it actually",
    "start": "2642610",
    "end": "2651220"
  },
  {
    "text": "will be able to actually scale the storage to meet our needs so definitely check its scales to the six terabytes so",
    "start": "2651220",
    "end": "2657880"
  },
  {
    "text": "it's definitely it's with them our immediate need it supports the concurrent connections that we",
    "start": "2657880",
    "end": "2663760"
  },
  {
    "text": "needed and it has that full analytical engine capability and this is a cond",
    "start": "2663760",
    "end": "2670450"
  },
  {
    "text": "it's something that it's not really immediate to us but something we're thinking about for the future we know",
    "start": "2670450",
    "end": "2677170"
  },
  {
    "text": "that our data volumes are going to grow considerably in the future so we've thought about some of the archiving and",
    "start": "2677170",
    "end": "2682960"
  },
  {
    "text": "other strategies to reduce the volume that we have eventually we may need to",
    "start": "2682960",
    "end": "2688559"
  },
  {
    "text": "switch this out for redshift also to internally we're using Aurora",
    "start": "2688559",
    "end": "2695319"
  },
  {
    "text": "for many of our systems including one called in grade Pro",
    "start": "2695319",
    "end": "2700530"
  },
  {
    "start": "2700000",
    "end": "2700000"
  },
  {
    "text": "in terms of estimated cost savings and talked about this little bit earlier",
    "start": "2700530",
    "end": "2707589"
  },
  {
    "text": "overall we estimated it was about four hundred and thirty K to handle the 1 billion events but with this new",
    "start": "2707589",
    "end": "2715270"
  },
  {
    "text": "architecture it's about a hundred and thirty K to handle the 1 billion events and obviously we want to drive those",
    "start": "2715270",
    "end": "2720490"
  },
  {
    "text": "costs down further and as you can see really landing using",
    "start": "2720490",
    "end": "2725829"
  },
  {
    "text": "lambda has been the biggest cost saver in this overall architecture we also - we didn't include an estimate",
    "start": "2725829",
    "end": "2732430"
  },
  {
    "text": "of the additional cost savings such as really we require fewer DevOps resources",
    "start": "2732430",
    "end": "2738540"
  },
  {
    "text": "for the building operations of this new architecture and we estimate it would actually take about seven DevOps",
    "start": "2738540",
    "end": "2745599"
  },
  {
    "text": "resources if we had gone forward with that with the many ec2 instances that we have we've also to experience gains in",
    "start": "2745599",
    "end": "2753280"
  },
  {
    "text": "adil 'ti the developers are able to very quickly code something invoke it from lambda and",
    "start": "2753280",
    "end": "2760780"
  },
  {
    "text": "see what it does these are two graphics the first graphic",
    "start": "2760780",
    "end": "2767440"
  },
  {
    "start": "2764000",
    "end": "2764000"
  },
  {
    "text": "depicts really are it's sort of our monthly growth terms of what we've seen for connect sites and as you can see we",
    "start": "2767440",
    "end": "2773109"
  },
  {
    "text": "have the peak between January and May and then between September and December",
    "start": "2773109",
    "end": "2779079"
  },
  {
    "text": "then the wall in between so we want to basically scale our systems up and scale them down last graphic on the bottom is",
    "start": "2779079",
    "end": "2786670"
  },
  {
    "text": "the hours by which reports are accessed and you can see clearly that students and instructors are night owls",
    "start": "2786670",
    "end": "2793240"
  },
  {
    "start": "2793000",
    "end": "2793000"
  },
  {
    "text": "so some of the challenges that we've encountered and how we work through them I mentioned early that we lost events we",
    "start": "2793240",
    "end": "2800869"
  },
  {
    "text": "are looking at some of the Ryoka texture for the processing and aggravation layer",
    "start": "2800869",
    "end": "2806030"
  },
  {
    "text": "not to depend as much an elastic search what we encountered was that we used elastic search incorrectly we",
    "start": "2806030",
    "end": "2814190"
  },
  {
    "text": "used it for sort of acid properties it does not maintain a mutable state",
    "start": "2814190",
    "end": "2819400"
  },
  {
    "text": "so it lost basically rights when you can currently try to create documents within",
    "start": "2819400",
    "end": "2824660"
  },
  {
    "text": "it so this has been really the root cause of our aggregation processing so we realized that we're probably gonna",
    "start": "2824660",
    "end": "2830690"
  },
  {
    "text": "have to perform some additional pre aggregations up front elastic search performance",
    "start": "2830690",
    "end": "2838330"
  },
  {
    "text": "we've encountered basically some issues with report queries particularly for",
    "start": "2838330",
    "end": "2844760"
  },
  {
    "text": "another product that we have called reading wonders it's not many in the service level agreements that we",
    "start": "2844760",
    "end": "2850040"
  },
  {
    "text": "establish with the business and so we've been able dock value optimization",
    "start": "2850040",
    "end": "2855470"
  },
  {
    "text": "elasticsearch that certainly we've tuned the performance up there but also to",
    "start": "2855470",
    "end": "2860599"
  },
  {
    "text": "we're going to probably have to move some of the processing out of elasticsearch into the application here",
    "start": "2860599",
    "end": "2865820"
  },
  {
    "text": "we also to mean I'm have to increase the number of database nodes to get that last bit of performance",
    "start": "2865820",
    "end": "2872950"
  },
  {
    "text": "and let's see be aware to if the elastic",
    "start": "2872950",
    "end": "2878480"
  },
  {
    "text": "search limits I spoke a little about these",
    "start": "2878480",
    "end": "2883930"
  },
  {
    "text": "some of the elastic search limits are that it handles at 32 terabytes and so",
    "start": "2883930",
    "end": "2890960"
  },
  {
    "text": "obviously you may need to think about some additional mechanisms for handling the data there and",
    "start": "2890960",
    "end": "2897250"
  },
  {
    "text": "you know as especailly about the Kinesis stream retention that's very particular",
    "start": "2897250",
    "end": "2902630"
  },
  {
    "text": "to our use case and we had to adjust that from 24 hours to 7 days and then we encountered this really kind",
    "start": "2902630",
    "end": "2910670"
  },
  {
    "text": "of weird problem with dynamodb over time we had a write throttling",
    "start": "2910670",
    "end": "2917240"
  },
  {
    "text": "error we applied a bandaid by increasing the write provision level of the global",
    "start": "2917240",
    "end": "2922970"
  },
  {
    "text": "index it worked for a while until the table of size exceeded the",
    "start": "2922970",
    "end": "2929600"
  },
  {
    "text": "AWS defined boundaries and so what happened was it basically there was",
    "start": "2929600",
    "end": "2935100"
  },
  {
    "text": "repartition at the table and this lowered the per node right capacity so",
    "start": "2935100",
    "end": "2940619"
  },
  {
    "text": "again we had to increase the provisioning level and at some point we were over ten thousand units per second",
    "start": "2940619",
    "end": "2947010"
  },
  {
    "text": "and we were still on camel a bottle air it was like oh my gosh what the heck have we done so we contacted our",
    "start": "2947010",
    "end": "2954980"
  },
  {
    "text": "enterprise support team and they sent us this heat map",
    "start": "2954980",
    "end": "2960650"
  },
  {
    "start": "2958000",
    "end": "2958000"
  },
  {
    "text": "guess what we were only using two of the Keith spaces - two of the hundred Keith",
    "start": "2960650",
    "end": "2968820"
  },
  {
    "text": "spaces for the partition so they really helped us diagnose the problem and determine what was happening we had to",
    "start": "2968820",
    "end": "2976770"
  },
  {
    "text": "redesign the table basically so that rights are evenly spaced out across partitions so",
    "start": "2976770",
    "end": "2982250"
  },
  {
    "text": "how do we build confidence you know we have lost events we had problems - died of a doobie it's like wow didn't we",
    "start": "2982250",
    "end": "2989220"
  },
  {
    "text": "weren't meeting the SLA of the business we went through extensive testing so we built the confidence through every",
    "start": "2989220",
    "end": "2996869"
  },
  {
    "text": "possible imaginable test performance failover functional and business",
    "start": "2996869",
    "end": "3002480"
  },
  {
    "text": "acceptance testing - you can see two images here on basically the top images",
    "start": "3002480",
    "end": "3008300"
  },
  {
    "text": "of the input where we demonstrate we can scale and handle the 10,000 events per",
    "start": "3008300",
    "end": "3014840"
  },
  {
    "text": "second really no errors and then the output we were able to receive events as",
    "start": "3014840",
    "end": "3021470"
  },
  {
    "text": "fast as we were pushing them through the platform",
    "start": "3021470",
    "end": "3026200"
  },
  {
    "text": "so caliper events that are sent to the input API and we validate to the persistence to elasticsearch and s3",
    "start": "3028270",
    "end": "3035810"
  },
  {
    "text": "that's really the essence of the test the tools that we use for testing HP loadrunner and sistah hosted cloud test",
    "start": "3035810",
    "end": "3043180"
  },
  {
    "text": "cloud tests for the reconcile api and again this is really critical for us we",
    "start": "3043180",
    "end": "3048770"
  },
  {
    "text": "can't lose events so we basically use the reconcile API",
    "start": "3048770",
    "end": "3055079"
  },
  {
    "text": "to demonstrate and to confirm that we've successfully processed the events after the fact or even if we have to replay",
    "start": "3055079",
    "end": "3062759"
  },
  {
    "text": "events and in terms of monitoring of the",
    "start": "3062759",
    "end": "3067799"
  },
  {
    "text": "components we monitor the input Kinesis streams s3 in classic search clusters",
    "start": "3067799",
    "end": "3074999"
  },
  {
    "text": "and lambda functions and query API and all this was at the end of the day",
    "start": "3074999",
    "end": "3082349"
  },
  {
    "text": "we were able to meet her service level agreements the 10000 events per second",
    "start": "3082349",
    "end": "3088289"
  },
  {
    "text": "output of 300 events for the queries to feed the reports and visualizations and",
    "start": "3088289",
    "end": "3093930"
  },
  {
    "text": "to support batches of 300 events so some of the lessons learned",
    "start": "3093930",
    "end": "3099319"
  },
  {
    "text": "we used the service architecture framework for managing the API gateway",
    "start": "3099319",
    "end": "3104400"
  },
  {
    "text": "and lambda configurations and code deployments greatly streamlining streamlining our overall development and",
    "start": "3104400",
    "end": "3111359"
  },
  {
    "text": "deployments we've relied heavily on cloud watch for monitoring all services and sewer logic",
    "start": "3111359",
    "end": "3117569"
  },
  {
    "text": "for close to real-time log searches and analysis and if you - we may look at replacing this without teams develop",
    "start": "3117569",
    "end": "3125489"
  },
  {
    "text": "automated tests and this is - I think one thing that is enabled us to be successful so",
    "start": "3125489",
    "end": "3131449"
  },
  {
    "text": "develop your into M test early across all your various integration points",
    "start": "3131449",
    "end": "3137089"
  },
  {
    "text": "we have also to develop custom dashboard mixing business metrics with cloud watch",
    "start": "3137089",
    "end": "3142650"
  },
  {
    "text": "data and Kinesis lambda in elasticsearch we actually provide some of these custom",
    "start": "3142650",
    "end": "3148140"
  },
  {
    "text": "access to these custom dashboards to our customers so that they can see what's going on for Amazon API gateway we",
    "start": "3148140",
    "end": "3155489"
  },
  {
    "text": "experienced some Sigma V 4 issues for now we're manually managing the iam keys",
    "start": "3155489",
    "end": "3162199"
  },
  {
    "start": "3161000",
    "end": "3161000"
  },
  {
    "text": "for lambda this was actually another hard lesson particularly when you have",
    "start": "3162319",
    "end": "3167729"
  },
  {
    "text": "great number of students are hopping on the system at 8 a.m. in the morning cold",
    "start": "3167729",
    "end": "3172979"
  },
  {
    "text": "start so you may need to use a warmer which basically you pump a whole bunch",
    "start": "3172979",
    "end": "3179099"
  },
  {
    "text": "of performance data through the system and then you flush it up we've also - we've had great",
    "start": "3179099",
    "end": "3185489"
  },
  {
    "text": "integrations with Kinesis s3 and others services for this event based triggering that was actually you know the least",
    "start": "3185489",
    "end": "3192030"
  },
  {
    "text": "painful part of it we were anticipating that that we would have the most problem here and we didn't one thing to be aware",
    "start": "3192030",
    "end": "3198690"
  },
  {
    "text": "of is that it's very substantive lambdas very sensitive to easy to auto scaling",
    "start": "3198690",
    "end": "3204510"
  },
  {
    "text": "outages and we've experienced a couple of outages and slowness well so - you",
    "start": "3204510",
    "end": "3211440"
  },
  {
    "text": "need some better debug tools to understand what's going on inside of lambda so distributed tracing a debug",
    "start": "3211440",
    "end": "3217800"
  },
  {
    "text": "support for Kinesis it scales up and down the",
    "start": "3217800",
    "end": "3224970"
  },
  {
    "text": "number of shards but this is a bit of a challenge and we've used some third-party tools for this",
    "start": "3224970",
    "end": "3230060"
  },
  {
    "text": "so you really need to go ahead and provision it's safer to provision up front for your expected peak this is so",
    "start": "3230060",
    "end": "3236790"
  },
  {
    "text": "that you don't get slammed when you've got the 14 million students who hop on the system at 8 a.m. in the morning",
    "start": "3236790",
    "end": "3242390"
  },
  {
    "text": "it's much more involved to delete and recreate lambda Kinesis event sources",
    "start": "3242390",
    "end": "3249150"
  },
  {
    "text": "and so we'd like the ability to be able to purge like for example if we receive",
    "start": "3249150",
    "end": "3254670"
  },
  {
    "text": "poison pill messages we want to be able to quickly purge this is a functionality that exists within sqs",
    "start": "3254670",
    "end": "3262250"
  },
  {
    "text": "for elasticsearch service we've developed scripts to automate the creation of indices aliases and mappings",
    "start": "3262250",
    "end": "3269280"
  },
  {
    "text": "and let's say there's quite a bit of time tools we have to do any sort of restoration",
    "start": "3269280",
    "end": "3274670"
  },
  {
    "text": "terms of queue capacity limits this has to do with the number of Kinesis charge",
    "start": "3274670",
    "end": "3279900"
  },
  {
    "text": "to handle data throughput 25 concurrent lambdas cannot execute",
    "start": "3279900",
    "end": "3285510"
  },
  {
    "text": "large bulk index requests without exceeding the elasticsearch queue capacity limits",
    "start": "3285510",
    "end": "3293750"
  },
  {
    "text": "unfortunately any sort of configuration change requires a full cluster rebuild and data migration which can take about",
    "start": "3293750",
    "end": "3301109"
  },
  {
    "text": "a half an hour to an hour and we can't we can't have downtime in their system it's extremely disruptive",
    "start": "3301109",
    "end": "3307880"
  },
  {
    "text": "also to in terms of we need some performance tuning and monitoring it's a",
    "start": "3307880",
    "end": "3313500"
  },
  {
    "text": "bit more difficult with elasticsearch if we'd use the company version they have some additional to that are available",
    "start": "3313500",
    "end": "3320180"
  },
  {
    "text": "there so we'd like some some more tools of Hilo for performance tuning and then lastly AWS enterprise support",
    "start": "3320180",
    "end": "3328540"
  },
  {
    "text": "they're wonderful reach out to them if you have issues they say this countless",
    "start": "3328540",
    "end": "3333800"
  },
  {
    "text": "hours trying to determine the root cause they provided us many insights really some",
    "start": "3333800",
    "end": "3340930"
  },
  {
    "text": "key engineering talent that we just don't have within our own shop",
    "start": "3340930",
    "end": "3346810"
  },
  {
    "start": "3346000",
    "end": "3346000"
  },
  {
    "text": "so in terms of takeaways know about what you're in the state requires in terms of",
    "start": "3346810",
    "end": "3351890"
  },
  {
    "text": "production scale we had to build a system that handles",
    "start": "3351890",
    "end": "3357080"
  },
  {
    "text": "the two to fourteen million concurrent students to handle the 10k events and in the",
    "start": "3357080",
    "end": "3363560"
  },
  {
    "text": "future we want to be able to scale it scale that up to 15 million events also to think about your service level",
    "start": "3363560",
    "end": "3370130"
  },
  {
    "text": "agreements for the business so for us we had to make sure that we met that ten thousand events per second be able to",
    "start": "3370130",
    "end": "3377510"
  },
  {
    "text": "produce three hundred queries for the output and then to be",
    "start": "3377510",
    "end": "3382940"
  },
  {
    "text": "able to handle batches of three hundred events think about the overall estimated costs",
    "start": "3382940",
    "end": "3389810"
  },
  {
    "text": "including DevOps engineers architects and some of the cost savers and for us",
    "start": "3389810",
    "end": "3395420"
  },
  {
    "text": "lambda to say this quite a bit of cost as well as with the other services that we use because they were already",
    "start": "3395420",
    "end": "3400610"
  },
  {
    "text": "integrated take the time or architecture took you",
    "start": "3400610",
    "end": "3407540"
  },
  {
    "text": "know about two years to really get the right integrations to get everything working well the multiple iterations",
    "start": "3407540",
    "end": "3414490"
  },
  {
    "text": "and lastly enterprise support well worth the cost and they provided us a lot of",
    "start": "3414490",
    "end": "3421130"
  },
  {
    "text": "information alright thank you very much Terry",
    "start": "3421130",
    "end": "3427780"
  },
  {
    "text": "I wanted to kind of wrap things up very quickly from from my perspective as well and kind of think about what what this",
    "start": "3427780",
    "end": "3435980"
  },
  {
    "text": "means and what we can learn from this and as a Solutions Architect I'm constantly learning from from my",
    "start": "3435980",
    "end": "3441860"
  },
  {
    "text": "customers and I think one of the things that are becoming very evident in the last one or two years is that we're",
    "start": "3441860",
    "end": "3447140"
  },
  {
    "text": "seeing sort of this new breed of applications and platforms that are running on AWS things like the learning",
    "start": "3447140",
    "end": "3452300"
  },
  {
    "text": "analytics form and this new breed is really embracing the the service",
    "start": "3452300",
    "end": "3458809"
  },
  {
    "text": "approach and and decoupling every element of their architecture and these are things that development teams have",
    "start": "3458809",
    "end": "3464630"
  },
  {
    "text": "been asking to do for years but now with services like lambda like API gateway like Kinesis like these fully managed",
    "start": "3464630",
    "end": "3472130"
  },
  {
    "text": "data stores it's become possible to take this to the next logical step and from a",
    "start": "3472130",
    "end": "3478579"
  },
  {
    "text": "business perspective what we're seeing is that companies like mcgraw-hill are doing this for the applications that are",
    "start": "3478579",
    "end": "3483979"
  },
  {
    "text": "mission-critical and are well within the critical path of their revenue generating products and what we're",
    "start": "3483979",
    "end": "3490039"
  },
  {
    "text": "hearing from them is that they're doing this because it drives down cost significantly but also they're now able",
    "start": "3490039",
    "end": "3495559"
  },
  {
    "text": "to consume significantly larger data sets and that makes their products richer and",
    "start": "3495559",
    "end": "3501309"
  },
  {
    "text": "that's even before we begin to mention the huge benefits of the significantly",
    "start": "3501309",
    "end": "3506449"
  },
  {
    "text": "less management that's needed the less upkeep that's needed with architectures like this compared to traditional",
    "start": "3506449",
    "end": "3512179"
  },
  {
    "text": "architectures so hopefully this has been educational and helpful for you to kind of leave here with thoughts about how",
    "start": "3512179",
    "end": "3518809"
  },
  {
    "text": "you can architect new workloads on AWS and perhaps re-architect or refactor",
    "start": "3518809",
    "end": "3524269"
  },
  {
    "text": "existing workloads in an iterative fashion I'd like to thank Terry very very much for joining us today and",
    "start": "3524269",
    "end": "3530779"
  },
  {
    "text": "walking us through that I'd like to thank you as well for joining us thank you very much and please fill out your",
    "start": "3530779",
    "end": "3536389"
  },
  {
    "text": "feedback on [Applause]",
    "start": "3536389",
    "end": "3540520"
  }
]