[
  {
    "start": "0",
    "end": "26000"
  },
  {
    "text": "good afternoon and welcome to the Amazon or a deep type session my name is Cady",
    "start": "290",
    "end": "6720"
  },
  {
    "text": "Singh I'm a Solutions Architect with AWS consulting partners team and this is my colleague Scott watt who is a solution",
    "start": "6720",
    "end": "13620"
  },
  {
    "text": "architect with AWS technical partners team we like to keep some time and",
    "start": "13620",
    "end": "19680"
  },
  {
    "text": "towards the end of this session for Q&A and we have a lot to cover so let's get started so rotor is my sequel compatible",
    "start": "19680",
    "end": "31040"
  },
  {
    "start": "26000",
    "end": "26000"
  },
  {
    "text": "Enterprise great data database service",
    "start": "31040",
    "end": "36210"
  },
  {
    "text": "designed for the cloud in many ways it's a it's a game changer when it comes to",
    "start": "36210",
    "end": "41340"
  },
  {
    "text": "traditional database systems which don't really scale that well and don't perform that well and are costly",
    "start": "41340",
    "end": "48239"
  },
  {
    "text": "there are many technical enhancements that we have done with Aurora to make it perform really well and scale very",
    "start": "48239",
    "end": "54570"
  },
  {
    "text": "really well and still the pricing is is how we typically price AWS services",
    "start": "54570",
    "end": "60590"
  },
  {
    "text": "pay-as-you-go no lock-in and no upfront commitments we'll be spending a bulk of",
    "start": "60590",
    "end": "68220"
  },
  {
    "text": "this session on how we did things so in this in this particular slide I'm going to spend some time covering some of the",
    "start": "68220",
    "end": "74729"
  },
  {
    "text": "key features that we have with the rock Amazon Aurora is designed to be very",
    "start": "74729",
    "end": "79740"
  },
  {
    "text": "durable the data volumes in Aurora are actually divided into segments each",
    "start": "79740",
    "end": "85979"
  },
  {
    "text": "segment is 10 gigabyte each and each segment is 6 we replicated across three",
    "start": "85979",
    "end": "92310"
  },
  {
    "text": "availability zones so that's a lot of durability it's designed to be fault tolerant so all these data segments",
    "start": "92310",
    "end": "102420"
  },
  {
    "text": "across availability zones make them fault fault isolated and also it can",
    "start": "102420",
    "end": "108180"
  },
  {
    "text": "withstand loss of two out of those six segments without affecting your write",
    "start": "108180",
    "end": "113340"
  },
  {
    "text": "availability and it can withstand loss of three out of those six segments for",
    "start": "113340",
    "end": "119610"
  },
  {
    "text": "without any loss for read availability of your database it's really fault tolerant it does continuous incremental",
    "start": "119610",
    "end": "125930"
  },
  {
    "text": "backup of your data in s3 in Amazon s3 s3 as most of us know is designed",
    "start": "125930",
    "end": "132990"
  },
  {
    "text": "eleven lines of durability so this continuous backup features really neat it is very high performing for similarly",
    "start": "132990",
    "end": "141480"
  },
  {
    "text": "configured hardware Aurora gives you five times the performance of standard",
    "start": "141480",
    "end": "146790"
  },
  {
    "text": "by sequel with Aurora you can run up to 15 read replicas these read replicas can",
    "start": "146790",
    "end": "154440"
  },
  {
    "text": "be put in a new any availability zone and with these read replicas you can get really highly performance for your",
    "start": "154440",
    "end": "161100"
  },
  {
    "text": "applications but with these replicas you can also use them as failover target so",
    "start": "161100",
    "end": "166470"
  },
  {
    "text": "if something were to happen to your primary Aurora instance any one of these read replicas can be a user's failover",
    "start": "166470",
    "end": "173280"
  },
  {
    "text": "target so you you really won't lose any availability of your data with them",
    "start": "173280",
    "end": "179600"
  },
  {
    "text": "it is self-healing so it automatically detects any issues with with its nodes",
    "start": "179600",
    "end": "186630"
  },
  {
    "text": "and disks that it's running on it uses SSD behind the scene it detects that and",
    "start": "186630",
    "end": "192360"
  },
  {
    "text": "if it finds some problem with any of the nodes or discs it's going to replace",
    "start": "192360",
    "end": "197910"
  },
  {
    "text": "them seamlessly without affecting availability as well and then it's it",
    "start": "197910",
    "end": "204330"
  },
  {
    "text": "uses features like survivable buffer cache so what we did with Aurora was we took the caching layer which is",
    "start": "204330",
    "end": "210240"
  },
  {
    "text": "available which is present in pretty much all commercial databases and we we",
    "start": "210240",
    "end": "215460"
  },
  {
    "text": "took it out outside the database tier itself so even if your database has to",
    "start": "215460",
    "end": "221220"
  },
  {
    "text": "restart for any reason your cache would survive that so you get instant crash recovery because of that and finally",
    "start": "221220",
    "end": "229080"
  },
  {
    "text": "it's designed to be very secure by default Aurora is going to run in Amazon what you private cloud or VPC service",
    "start": "229080",
    "end": "236160"
  },
  {
    "text": "giving you network isolation and all the features that the PC offers including subnetting and security groups etc you",
    "start": "236160",
    "end": "243900"
  },
  {
    "text": "can also encrypt your data using SSL AES 256-bit keys for data in transit and for",
    "start": "243900",
    "end": "251850"
  },
  {
    "text": "data at rest you can encrypt your data with with Aurora as well and those",
    "start": "251850",
    "end": "257040"
  },
  {
    "text": "encryption keys for ADA at rest can be managed using the Amazon key management service so it's very seamless and works",
    "start": "257040",
    "end": "263850"
  },
  {
    "text": "well with other Amazon services so we launched Arora made it",
    "start": "263850",
    "end": "270440"
  },
  {
    "text": "generally available in July of 2015 and since its launch it has it has been",
    "start": "270440",
    "end": "278740"
  },
  {
    "text": "widely it's been widely used and now it has become the fastest towing service in",
    "start": "278740",
    "end": "285170"
  },
  {
    "text": "Arabia's history absolutely you know it used to be that ship but now it's Aurora so we have seen really rapid the usage",
    "start": "285170",
    "end": "292730"
  },
  {
    "text": "and we have seen partners big and small as well as customers big and small use",
    "start": "292730",
    "end": "298250"
  },
  {
    "text": "it for a ton of different use cases whether it's for business applications and enterprise apps or whether it's for",
    "start": "298250",
    "end": "305510"
  },
  {
    "text": "web social media gaming IOT you name it you know people are using for it for all",
    "start": "305510",
    "end": "311030"
  },
  {
    "text": "kinds of use cases let's look at some of these customer examples in some more",
    "start": "311030",
    "end": "316460"
  },
  {
    "start": "314000",
    "end": "314000"
  },
  {
    "text": "detail the first one I want to talk about is Expedia which is a online travel portal they actually operate more",
    "start": "316460",
    "end": "323930"
  },
  {
    "text": "than hundred 50 sites in 70 different countries all around the world so they have a ton of data coming into their",
    "start": "323930",
    "end": "330320"
  },
  {
    "text": "travel marketplace this is data that has key business insights and they want to run data and real-time bi analytics on",
    "start": "330320",
    "end": "336890"
  },
  {
    "text": "top of that data their current sequel server based solution was deemed too",
    "start": "336890",
    "end": "343970"
  },
  {
    "text": "expensive they were not really happy with its performance as well as the data volumes were increasing the performance",
    "start": "343970",
    "end": "350030"
  },
  {
    "text": "was not scaling well for them and it was getting too costly for their for their",
    "start": "350030",
    "end": "356000"
  },
  {
    "text": "liking so they explored some solutions they they looked at leveraging Cassandra with",
    "start": "356000",
    "end": "362030"
  },
  {
    "text": "solar index but to get the amount of performance they needed the memory footprint had to be huge so the number",
    "start": "362030",
    "end": "368060"
  },
  {
    "text": "of nodes they had to deploy for such a solution made it very costly with Aurora",
    "start": "368060",
    "end": "374300"
  },
  {
    "text": "they were able to scale effortlessly to the amount of performance they wanted and and they got to do it at a at a much",
    "start": "374300",
    "end": "382400"
  },
  {
    "text": "lower cost point as well the next one is Thomas publishing it's actually more",
    "start": "382400",
    "end": "389030"
  },
  {
    "text": "than a century old business and their goal is to is to somehow",
    "start": "389030",
    "end": "395479"
  },
  {
    "text": "bridge the gap between buyers and suppliers they started with the digital publishing but now they have a lot of",
    "start": "395479",
    "end": "401599"
  },
  {
    "text": "different companies under that entity and they had been actually using Oracle for their production workloads and they",
    "start": "401599",
    "end": "409490"
  },
  {
    "text": "were facing problems in terms of you know scaling their database well and and",
    "start": "409490",
    "end": "415789"
  },
  {
    "text": "getting the performance they wanted to get out of it another issue for them was up from investment they had to make a",
    "start": "415789",
    "end": "423169"
  },
  {
    "text": "lot of capital expenditure just to get the infrastructure as well as Oracle licensing required for their needs so",
    "start": "423169",
    "end": "430539"
  },
  {
    "text": "they ended up leveraging Garuda as well so for migration of their production",
    "start": "430539",
    "end": "437539"
  },
  {
    "text": "database nobody wants their production database to go down they want minimal or no downtime so the leverage to services",
    "start": "437539",
    "end": "444740"
  },
  {
    "text": "in Amazon portfolio they they use the AWS schema migration tool that helps",
    "start": "444740",
    "end": "450649"
  },
  {
    "text": "migrate our convert things like view stored procedures and functions in",
    "start": "450649",
    "end": "456830"
  },
  {
    "text": "Oracle and Microsoft sequel space to similar attributes that can be leveraged",
    "start": "456830",
    "end": "463309"
  },
  {
    "text": "in aurora's the schema migration tool was good for them and then they use the AWS database migration service or DMS to",
    "start": "463309",
    "end": "471080"
  },
  {
    "text": "migrate the actual production data over to AWS the database migration tool it's",
    "start": "471080",
    "end": "478069"
  },
  {
    "text": "it's a standalone service you you you spin up this service you point it to",
    "start": "478069",
    "end": "483379"
  },
  {
    "text": "your source database and you point it to your destination database and it does all the orchestration required to move",
    "start": "483379",
    "end": "489529"
  },
  {
    "text": "the data for you so with using these services they were actually seamlessly able to bring their data over from",
    "start": "489529",
    "end": "495979"
  },
  {
    "text": "Oracle production database to adora in less than four weeks so they are one happy customer a couple more eyes is CS",
    "start": "495979",
    "end": "504529"
  },
  {
    "text": "is in the insurance industry space and they have been using Oracle and sequel",
    "start": "504529",
    "end": "511309"
  },
  {
    "text": "servers for the Microsoft sequel server for their database and warehousing needs",
    "start": "511309",
    "end": "517268"
  },
  {
    "text": "for them you know cost was important they you know these solutions were",
    "start": "517269",
    "end": "522289"
  },
  {
    "text": "getting to be very costly we see a pattern here we actually designed Aurora to be you know similar in",
    "start": "522289",
    "end": "531270"
  },
  {
    "text": "performance to high-end commercial databases but up to when one-tenth the cost of these high-end commercial",
    "start": "531270",
    "end": "537090"
  },
  {
    "text": "databases so no doubt these is CS people also like the cost they were getting",
    "start": "537090",
    "end": "544530"
  },
  {
    "text": "with Aurora but another problem for them was maintenance of their database",
    "start": "544530",
    "end": "549660"
  },
  {
    "text": "you know the ongoing mundane tasks that you have to do in day in day out just to keep your database up with Aurora that",
    "start": "549660",
    "end": "556050"
  },
  {
    "text": "pain point went away because Aurora database engine is actually managed by Amazon relational database service or",
    "start": "556050",
    "end": "562290"
  },
  {
    "text": "RDS so you know things like provisioning your database or patching it detecting",
    "start": "562290",
    "end": "568620"
  },
  {
    "text": "for any issues or problems and repairing those problems and doing backups all those things that we can deem to be",
    "start": "568620",
    "end": "575280"
  },
  {
    "text": "undifferentiated heavy lifting items that you just have to do to keep your database float they are done and managed",
    "start": "575280",
    "end": "581670"
  },
  {
    "text": "by Aurora for you so they were really happy with migrating to Aurora and on",
    "start": "581670",
    "end": "588270"
  },
  {
    "text": "top of it they actually realized 70% cost savings over their current sequel server deployment and the last one is",
    "start": "588270",
    "end": "596850"
  },
  {
    "text": "alfresco which is an AWS partner in the enterprise document management space and",
    "start": "596850",
    "end": "603480"
  },
  {
    "text": "for that it was all about scaling so they wanted to scale to over a billion documents and even at that big scale",
    "start": "603480",
    "end": "611250"
  },
  {
    "text": "they wanted a sub-second response time for user requests to their to these",
    "start": "611250",
    "end": "616500"
  },
  {
    "text": "documents so they they actually leveraged Aurora and they were able to scale to a billion documents with a",
    "start": "616500",
    "end": "623370"
  },
  {
    "text": "throughput of 3 million documents per hour which actually was 10 times better than the performance they were cutting",
    "start": "623370",
    "end": "629520"
  },
  {
    "text": "in their existing set of and they're they're migrating from big data centers",
    "start": "629520",
    "end": "635640"
  },
  {
    "text": "to leverage so let's get a little bit into why we designed Aurora if we if we",
    "start": "635640",
    "end": "643560"
  },
  {
    "text": "think about any relational database system at the very core of it it's some",
    "start": "643560",
    "end": "649680"
  },
  {
    "text": "store and some compute and typically this store and this compute is all running on a single box there are",
    "start": "649680",
    "end": "656340"
  },
  {
    "text": "different layers of allottee you know if you look at the picture we have sequel transaction",
    "start": "656340",
    "end": "662649"
  },
  {
    "text": "caching logging and storage different layers of functionality you can think of these as different tiers but the bottom",
    "start": "662649",
    "end": "667930"
  },
  {
    "text": "line is all of this is running on a single box it's all well and good if",
    "start": "667930",
    "end": "673480"
  },
  {
    "text": "you're not loading it that much or if your data volumes are not increasing that much but you know that's not",
    "start": "673480",
    "end": "679510"
  },
  {
    "text": "typically the case if you're a successful enterprise or a successful user volumes increase so to scale your",
    "start": "679510",
    "end": "686140"
  },
  {
    "text": "database with it the first approach you can take is vertical scaling got to",
    "start": "686140",
    "end": "691149"
  },
  {
    "text": "scale up you know you have a smaller box you just get a bigger box that bigger fork is not sufficient you just get a",
    "start": "691149",
    "end": "697779"
  },
  {
    "text": "bigger box than that but as you can imagine there are actual physical limits beyond which you can't scale vertically",
    "start": "697779",
    "end": "703839"
  },
  {
    "text": "you just can't get a bigger box so people have been using and and this database is 30 40 year-old things so",
    "start": "703839",
    "end": "710350"
  },
  {
    "text": "people have been using different techniques over the years some of them are highlighted here sharding is one",
    "start": "710350",
    "end": "716380"
  },
  {
    "text": "which means creating horizontal partitions of your data essentially and putting them in different notes and but",
    "start": "716380",
    "end": "724209"
  },
  {
    "text": "the bottom line is if you want to run sequel queries against your data you typically want to do it against the full",
    "start": "724209",
    "end": "730120"
  },
  {
    "text": "data set so in case you want you have data distributed in different nodes your application now has to deal with where",
    "start": "730120",
    "end": "737260"
  },
  {
    "text": "should I send my query you know which node what is sitting on so there is a lot of complexity related to sharding",
    "start": "737260",
    "end": "743649"
  },
  {
    "text": "that you have to manage and if one of the node goes down for some reason you have to deal with it you have to deal",
    "start": "743649",
    "end": "750040"
  },
  {
    "text": "with data loss so there are issues with scaling it shared-nothing approaches",
    "start": "750040",
    "end": "756310"
  },
  {
    "text": "were also popular in this case your application does not have to deal with how and where to send the query it's",
    "start": "756310",
    "end": "763750"
  },
  {
    "text": "handled by your sequel layer so it's a little bit simpler on the application side of things but it's still a problem",
    "start": "763750",
    "end": "770740"
  },
  {
    "text": "if the data node goes away data is lost so as you keep on increasing the number",
    "start": "770740",
    "end": "776260"
  },
  {
    "text": "of nodes in your database cluster the availability overall availability of",
    "start": "776260",
    "end": "781540"
  },
  {
    "text": "your database goes down and finally you know share disk the key idea here is",
    "start": "781540",
    "end": "788080"
  },
  {
    "text": "do you essentially want to fuse the caches across different boxes of different nodes in your system so that",
    "start": "788080",
    "end": "794529"
  },
  {
    "text": "they are reading off of the same caching layer this this is a little bit better",
    "start": "794529",
    "end": "799990"
  },
  {
    "text": "in terms of data loss etc but the problem here is when we load the system",
    "start": "799990",
    "end": "806230"
  },
  {
    "text": "a lot and a lot of transactions are happening these this caching layer and",
    "start": "806230",
    "end": "811690"
  },
  {
    "text": "the caching algorithms they can become very chatty they just need to you know move a lot of data back and forth and",
    "start": "811690",
    "end": "817450"
  },
  {
    "text": "can lead to problem like hot keys if you look at any of these pictures the main",
    "start": "817450",
    "end": "823600"
  },
  {
    "text": "thing is that you are actually replicating the same stack over and over",
    "start": "823600",
    "end": "829510"
  },
  {
    "text": "again to scale out so you fundamentally run into scaling issues with it",
    "start": "829510",
    "end": "835140"
  },
  {
    "text": "but this is how things were done in the old world now you know imagine if you",
    "start": "835140",
    "end": "840279"
  },
  {
    "start": "839000",
    "end": "839000"
  },
  {
    "text": "had to design a data base today you know what would you do you would design something that can scale out easily you",
    "start": "840279",
    "end": "847450"
  },
  {
    "text": "know if you have monolithic tiers of of your application all running in a single",
    "start": "847450",
    "end": "853420"
  },
  {
    "text": "box whereas that's not a good pattern to follow you you want to design something that can scale out you want to design",
    "start": "853420",
    "end": "859570"
  },
  {
    "text": "something that is self-healing and you want to design something that can leverage other modern services",
    "start": "859570",
    "end": "866980"
  },
  {
    "text": "available in cloud platforms will actually design something like Aurora so",
    "start": "866980",
    "end": "872380"
  },
  {
    "text": "in Aurora this is the Aurora picture I'll spend a minute or two on this what we essentially did was followed the",
    "start": "872380",
    "end": "879520"
  },
  {
    "text": "service-oriented architecture design patterns so again we have the same stack",
    "start": "879520",
    "end": "889380"
  },
  {
    "text": "sequel transaction caching logging and storage but what we have done is we have",
    "start": "889380",
    "end": "895180"
  },
  {
    "text": "broken them out logging and storage is now a separate system it's being handled",
    "start": "895180",
    "end": "901449"
  },
  {
    "text": "by a separate system now it's not part of your of the monolithic stack anymore and then what we have done is we have",
    "start": "901449",
    "end": "909670"
  },
  {
    "text": "done we have done this automatic backups to s3 if you think about traditional",
    "start": "909670",
    "end": "915970"
  },
  {
    "text": "databases these databases had to do was your your",
    "start": "915970",
    "end": "921110"
  },
  {
    "text": "DB or sequel layer would write redo logs to your storage tier but for those",
    "start": "921110",
    "end": "926630"
  },
  {
    "text": "systems the storage tier were what just dump blocks they were just blocks blobs",
    "start": "926630",
    "end": "932330"
  },
  {
    "text": "of storage you just put things into them they can't do anything about it you have to manage everything in this case what",
    "start": "932330",
    "end": "939710"
  },
  {
    "text": "we are done is this storage layer is smart enough for example to do things like take the redo logs that's equal",
    "start": "939710",
    "end": "946310"
  },
  {
    "text": "tier is giving to it and then create data pages from those little logs on its",
    "start": "946310",
    "end": "952190"
  },
  {
    "text": "own and store them to s3 so it has some smarts built in to take some of the load",
    "start": "952190",
    "end": "958280"
  },
  {
    "text": "off of your DB tier and and do storage needed items or logging related items on its own so it can obviously scale",
    "start": "958280",
    "end": "965210"
  },
  {
    "text": "independently of your DB tier and do things on its own so it's a it's a big",
    "start": "965210",
    "end": "970640"
  },
  {
    "text": "win there what we have also done is moved caching out of your DB tier which",
    "start": "970640",
    "end": "976700"
  },
  {
    "text": "gives you survivable buffer caches an instant crash recovery in this case if",
    "start": "976700",
    "end": "982070"
  },
  {
    "text": "the DB has to be restarted the cash is still populated that data is not going away anywhere so things were better and",
    "start": "982070",
    "end": "991010"
  },
  {
    "text": "finally to to orchestrate this whole system to run your database or a",
    "start": "991010",
    "end": "996170"
  },
  {
    "text": "database there are control plane services to make the whole thing happen for example we will average some of the",
    "start": "996170",
    "end": "1004180"
  },
  {
    "text": "popular AWS services like dynamo DP which is going to contain your volume",
    "start": "1004180",
    "end": "1010420"
  },
  {
    "text": "error data so the volume area ties in now in dynamo DB it's outside off of the main stack so it can scale independently",
    "start": "1010420",
    "end": "1017950"
  },
  {
    "text": "and you can get really good throughput read write performance order dynamo DB to get to your made metadata we are",
    "start": "1017950",
    "end": "1023950"
  },
  {
    "text": "using the simple workflow service or SWF to do things like starting an instance",
    "start": "1023950",
    "end": "1030370"
  },
  {
    "text": "or scaling an instance so it manages the workflow for doing it and finally we we leveraged route 53",
    "start": "1030370",
    "end": "1037860"
  },
  {
    "text": "which is the DNS service to do fail overs using route 53 if the primary",
    "start": "1037860",
    "end": "1044140"
  },
  {
    "text": "instance goes down route 53 is going to the DNS magic to fail over to your",
    "start": "1044140",
    "end": "1050450"
  },
  {
    "text": "replica or or to your failover instance seamlessly and can do it in one to two",
    "start": "1050450",
    "end": "1056299"
  },
  {
    "text": "minutes so so this is how SOA principles are actuated for now let's look at some",
    "start": "1056299",
    "end": "1065690"
  },
  {
    "text": "performance numbers because this is this is important so what we did was we used",
    "start": "1065690",
    "end": "1071390"
  },
  {
    "text": "the my sequel sis bench a benchmarking suite and we we ran our benchmarks",
    "start": "1071390",
    "end": "1080659"
  },
  {
    "text": "against it we used for client machines with thousand connections each that were",
    "start": "1080659",
    "end": "1085970"
  },
  {
    "text": "reading from this Aurora database and we were able to achieve more than hundred",
    "start": "1085970",
    "end": "1091820"
  },
  {
    "text": "thousand writes per second and five hundred thousand reads per second using this this benchmark which are really",
    "start": "1091820",
    "end": "1099830"
  },
  {
    "text": "really good numbers not long ago people were talking about a thousand beats alright so this is this",
    "start": "1099830",
    "end": "1106039"
  },
  {
    "text": "is pretty big but as as we know benchmarking numbers done in a lab",
    "start": "1106039",
    "end": "1113570"
  },
  {
    "text": "environment don't really mean much in the real world in the real world we have to read deal with real problems real",
    "start": "1113570",
    "end": "1119899"
  },
  {
    "text": "problems like there are actually let me first talk about how we got to those",
    "start": "1119899",
    "end": "1126470"
  },
  {
    "text": "numbers before I get into the real world let's let's stay in the lab for a minute if you want to do benchmarking on your",
    "start": "1126470",
    "end": "1134630"
  },
  {
    "text": "own we have provided some guidelines or a guide so that you can get to those",
    "start": "1134630",
    "end": "1139789"
  },
  {
    "text": "numbers yourself at the bottom of this slide we'll be putting these slides online shortly after the event but or",
    "start": "1139789",
    "end": "1146450"
  },
  {
    "text": "you can search for it the benchmark guideline document and you can get to",
    "start": "1146450",
    "end": "1152179"
  },
  {
    "text": "how we set up the benchmark you'll see we we we provide information like use",
    "start": "1152179",
    "end": "1158630"
  },
  {
    "text": "the same VPC for your client machines as your aurora database instance use the",
    "start": "1158630",
    "end": "1163940"
  },
  {
    "text": "same AZ where your database instance is running you use",
    "start": "1163940",
    "end": "1169860"
  },
  {
    "text": "more advanced ec2 instances for your clients networking turned on for good",
    "start": "1169860",
    "end": "1177840"
  },
  {
    "text": "network throughput you have to tune your Linux we show you how to tune your Linux to get good performance so you can get",
    "start": "1177840",
    "end": "1184409"
  },
  {
    "text": "to those numbers but the more important thing is these are some of the best practices that you can also follow to",
    "start": "1184409",
    "end": "1190649"
  },
  {
    "text": "get good performance order for your app now let's go to the real world in a real",
    "start": "1190649",
    "end": "1196019"
  },
  {
    "start": "1194000",
    "end": "1194000"
  },
  {
    "text": "world things are things are different you have to contend with things like there are multiple users each making",
    "start": "1196019",
    "end": "1203639"
  },
  {
    "text": "multiple requests it's not going to be a single user with a single request and you can't measure that you have multiple",
    "start": "1203639",
    "end": "1209249"
  },
  {
    "text": "requests going on that decreases performance you have situations where your metadata would not really fit in",
    "start": "1209249",
    "end": "1217379"
  },
  {
    "text": "the data dictionary cash flow leading to poor performance you would have to deal",
    "start": "1217379",
    "end": "1224279"
  },
  {
    "text": "with your caches won't be big enough to hold the data so a buffer caches are",
    "start": "1224279",
    "end": "1232409"
  },
  {
    "text": "going to overflow and finally in a real world you never want your database to go down you want fault tolerance you want",
    "start": "1232409",
    "end": "1239399"
  },
  {
    "text": "high availability which translates to using read replicas and failing over and things like that so since this is a",
    "start": "1239399",
    "end": "1247279"
  },
  {
    "text": "enterprise-grade database solution is we want to test its performance against",
    "start": "1247279",
    "end": "1253529"
  },
  {
    "text": "these real world scenarios so we did some experiments I'm going to go over those experiments now the first",
    "start": "1253529",
    "end": "1260309"
  },
  {
    "start": "1258000",
    "end": "1258000"
  },
  {
    "text": "experiment was you know when there are contending requests what happens so in",
    "start": "1260309",
    "end": "1266700"
  },
  {
    "text": "this case we scaled the number of user connections we are again using the Syst bench benchmark",
    "start": "1266700",
    "end": "1273480"
  },
  {
    "text": "it's OLTP workload with 250 tables and we scaled user connections going from 50",
    "start": "1273480",
    "end": "1279809"
  },
  {
    "text": "to 5,000 and we saw that rora scaled with it it's performance actually",
    "start": "1279809",
    "end": "1285899"
  },
  {
    "text": "improved and if you if you look at my sequel the second column here is about",
    "start": "1285899",
    "end": "1294239"
  },
  {
    "text": "my sequel it's performance actually degraded and Aurora was able to perform",
    "start": "1294239",
    "end": "1299940"
  },
  {
    "text": "eight times better than my sequel in this case and just for your reference the my",
    "start": "1299940",
    "end": "1305730"
  },
  {
    "text": "sequel that we competed against it's not a small instance we were using 30,000",
    "start": "1305730",
    "end": "1313950"
  },
  {
    "text": "provision die offs in a single AZ to run my sequel and RDS this is this is prolly",
    "start": "1313950",
    "end": "1319980"
  },
  {
    "text": "the biggest box for my sequel and RDS that you can get in Amazon so we compare it against the best we can get in Amazon",
    "start": "1319980",
    "end": "1326010"
  },
  {
    "text": "environment and there was still performed eight times better in this case the next one was about the data",
    "start": "1326010",
    "end": "1333180"
  },
  {
    "start": "1331000",
    "end": "1331000"
  },
  {
    "text": "dictionary cache so what we did was we scaled the table count and we went from",
    "start": "1333180",
    "end": "1339750"
  },
  {
    "text": "ten tables to up to ten thousand tables and we looked at the throughput and we",
    "start": "1339750",
    "end": "1345000"
  },
  {
    "text": "competed against three different flavors of my sequel running in Amazon the first",
    "start": "1345000",
    "end": "1350130"
  },
  {
    "text": "one was again your RDS with my sequel 30,000 provision",
    "start": "1350130",
    "end": "1356550"
  },
  {
    "text": "die ups and the second one was I two instances with which have good IO to its",
    "start": "1356550",
    "end": "1364140"
  },
  {
    "text": "local SSD instant store so we lavish that and the third flavor was using Ram",
    "start": "1364140",
    "end": "1369270"
  },
  {
    "text": "disk for for my sequel using i28 extra large instances which are really good",
    "start": "1369270",
    "end": "1374820"
  },
  {
    "text": "instance types on in ec2 and again across all these different",
    "start": "1374820",
    "end": "1380130"
  },
  {
    "text": "configurations amazon roar are performed up to 11 times faster what this shows us",
    "start": "1380130",
    "end": "1386730"
  },
  {
    "text": "is that io is not really the only problem it's not really a problem here",
    "start": "1386730",
    "end": "1391800"
  },
  {
    "text": "because these configurations for example the RAM disk configuration io is is not",
    "start": "1391800",
    "end": "1397770"
  },
  {
    "text": "what's bounding the performance for my sequel here its internal things like log",
    "start": "1397770",
    "end": "1402990"
  },
  {
    "text": "sharding and algorithms that are happening internal to the database tier which are causing the performance not to",
    "start": "1402990",
    "end": "1409830"
  },
  {
    "text": "scale the next experiment was what if",
    "start": "1409830",
    "end": "1416040"
  },
  {
    "start": "1412000",
    "end": "1412000"
  },
  {
    "text": "your data does not is bigger than the buffer cache so we started with a",
    "start": "1416040",
    "end": "1423090"
  },
  {
    "text": "gigabyte of data set which would data volume which would pretty much fit in",
    "start": "1423090",
    "end": "1430290"
  },
  {
    "text": "most commercial caches out there but then we scale it up to a terabyte and",
    "start": "1430290",
    "end": "1435590"
  },
  {
    "text": "when we saw what what what happened a rotor performance it went down it was",
    "start": "1435590",
    "end": "1443210"
  },
  {
    "text": "stable till up 200 gigabyte and then it went down about 4x but if we look at what happened with RDS its performance",
    "start": "1443210",
    "end": "1451039"
  },
  {
    "text": "dropped more and it it performed early it the performance it dropped more and",
    "start": "1451039",
    "end": "1457760"
  },
  {
    "text": "it dropped earlier than when it dropped in RDS so in inner hora so we essentially see aurora performing up to",
    "start": "1457760",
    "end": "1464270"
  },
  {
    "text": "67 times faster in this case in case of scaling idea sets and just to be sure we",
    "start": "1464270",
    "end": "1473149"
  },
  {
    "text": "ran the same experiment against cloud harmony TPCC benchmark which calculates",
    "start": "1473149",
    "end": "1478880"
  },
  {
    "text": "throughput numbers and their aurora was performing 136 times faster and the last",
    "start": "1478880",
    "end": "1486380"
  },
  {
    "start": "1486000",
    "end": "1486000"
  },
  {
    "text": "one i want to talk about is is running your databases in a fault-tolerant highly available mode which essentially",
    "start": "1486380",
    "end": "1493279"
  },
  {
    "text": "boils down to running read replicas in case of RDS when we do read replicas you",
    "start": "1493279",
    "end": "1501230"
  },
  {
    "text": "can do up to five Reed replicas with RDS my sequel if you do that we see that the",
    "start": "1501230",
    "end": "1509809"
  },
  {
    "text": "replicas lag goes from a few millisecond when we are doing thousand updates per second to over 300 seconds when we scale",
    "start": "1509809",
    "end": "1518360"
  },
  {
    "text": "your updates to 10,000 so that that's a lot of lag in replicas which means that",
    "start": "1518360",
    "end": "1523730"
  },
  {
    "text": "if you were to fail over to a replica you will lose a ton of data so and if we",
    "start": "1523730",
    "end": "1530330"
  },
  {
    "text": "if we look at Aurora as we scale from thousand updates to 10,000 the lag",
    "start": "1530330",
    "end": "1536029"
  },
  {
    "text": "replicas lag is is fairly constant it's not increasing much and it's it's five milliseconds so that's that's really",
    "start": "1536029",
    "end": "1543070"
  },
  {
    "text": "good lag number to have four fail overs so in this case Aurora was performing",
    "start": "1543070",
    "end": "1551059"
  },
  {
    "text": "five hundred times better then similarly powered my sequel instances so how do we",
    "start": "1551059",
    "end": "1560419"
  },
  {
    "text": "do that you know if you want to scale your database performance there are some",
    "start": "1560419",
    "end": "1566450"
  },
  {
    "text": "things that pretty much an commercial database would do things like",
    "start": "1566450",
    "end": "1572570"
  },
  {
    "text": "you know lock sharding or improving algorithms on the database tier but you",
    "start": "1572570",
    "end": "1579090"
  },
  {
    "text": "need to do more than that it's not really architecture work at that point you have to do things like you either",
    "start": "1579090",
    "end": "1586260"
  },
  {
    "text": "have to do less work or you have to do work more efficiently so it's to scale",
    "start": "1586260",
    "end": "1591660"
  },
  {
    "text": "your performance it's all about iOS you can't afford to do too many iOS so how you do iOS and how many iOS you do",
    "start": "1591660",
    "end": "1598470"
  },
  {
    "text": "matters a lot and then how many network packets you are driving when you are",
    "start": "1598470",
    "end": "1604560"
  },
  {
    "text": "talking about network attached storage in a distributed system manner if you",
    "start": "1604560",
    "end": "1610860"
  },
  {
    "text": "don't have monolithic black so you have network attached storage then the packets per second to your storage matter a lot you you don't want this to",
    "start": "1610860",
    "end": "1618420"
  },
  {
    "text": "be a high number now you know you can do a million to two million packets per second from high-end hardware like ec2",
    "start": "1618420",
    "end": "1626640"
  },
  {
    "text": "with enhanced networking but still it's a constrained number beyond certain point at won't scale so you really can't",
    "start": "1626640",
    "end": "1632250"
  },
  {
    "text": "afford to do too many network packets and and then you you can't really afford",
    "start": "1632250",
    "end": "1638760"
  },
  {
    "text": "to have context switches you know if you have a lot of context which is the performance is going to go down so a lot",
    "start": "1638760",
    "end": "1645360"
  },
  {
    "text": "of architecture work we did to make it over perform better was in terms of",
    "start": "1645360",
    "end": "1651140"
  },
  {
    "text": "reducing the number of iOS reducing the number of packets per second that we are",
    "start": "1651140",
    "end": "1656640"
  },
  {
    "text": "driving to storage and and moving towards more asynchronous processing now",
    "start": "1656640",
    "end": "1661980"
  },
  {
    "text": "to open the hood end and show you how does these things I'm going to have Scott tell us about that",
    "start": "1661980",
    "end": "1668120"
  },
  {
    "text": "thanks Kerry all right so let's go ahead and take a deeper dive into some of the",
    "start": "1668120",
    "end": "1675150"
  },
  {
    "text": "architectural components that go along with Aurora and even kind of help speak to how we achieve some of these numbers",
    "start": "1675150",
    "end": "1681120"
  },
  {
    "text": "that Katie just presented so to start with this is a simple representation",
    "start": "1681120",
    "end": "1687600"
  },
  {
    "text": "this we consider an Aurora cluster so it's got at least one primary database",
    "start": "1687600",
    "end": "1693690"
  },
  {
    "text": "instance and as a storage tier that covers three availability zones and",
    "start": "1693690",
    "end": "1699150"
  },
  {
    "text": "whatever AWS region you're running in and there's six storage nodes within",
    "start": "1699150",
    "end": "1704340"
  },
  {
    "text": "that storage layer across those three availability zones with it with two storage nodes at each availability zone",
    "start": "1704340",
    "end": "1709789"
  },
  {
    "text": "and that cluster backs up all of its data to Amazon s3 so getting the",
    "start": "1709789",
    "end": "1715500"
  },
  {
    "text": "durability and availability that comes with s3 this is a deeper representation",
    "start": "1715500",
    "end": "1724080"
  },
  {
    "text": "of a cluster so you've now got read replicas so we have once again our primary instance leveraging the storage",
    "start": "1724080",
    "end": "1729210"
  },
  {
    "text": "node we also have our read replicas and because that storage layer covers three",
    "start": "1729210",
    "end": "1735720"
  },
  {
    "text": "availability zones with an AWS region I can place my read replicas in any availability zone within that region",
    "start": "1735720",
    "end": "1741750"
  },
  {
    "text": "that I want to whether it be is the same as the master or it can be a different availability zones from that master and",
    "start": "1741750",
    "end": "1748169"
  },
  {
    "text": "on top of that these read replicas are actually your target from a failover perspective so when there is a failover",
    "start": "1748169",
    "end": "1753720"
  },
  {
    "text": "with the master those read replicas are gonna be proponent promoted to the masters so having the ability to place",
    "start": "1753720",
    "end": "1759419"
  },
  {
    "text": "those and availability zones that are different from the master to help guard against any network connectivity issues",
    "start": "1759419",
    "end": "1766080"
  },
  {
    "text": "with the actual availability zone or issues with the master is a benefit to be able to use those other availability",
    "start": "1766080",
    "end": "1771630"
  },
  {
    "text": "zones when it comes to an Aurora cluster so Katie was talking about how IO is",
    "start": "1771630",
    "end": "1779130"
  },
  {
    "text": "such a big deal when it comes to databases and the performance of databases so here in this slides of",
    "start": "1779130",
    "end": "1785789"
  },
  {
    "text": "representation of what a my sequel database running on Amazon RDS would",
    "start": "1785789",
    "end": "1791490"
  },
  {
    "text": "look like with a standby database you've got your primary instance and one availability zone and you've got your",
    "start": "1791490",
    "end": "1796530"
  },
  {
    "text": "standby instance in another availability zone and when you do a write operation in that primary instance what it's going",
    "start": "1796530",
    "end": "1802980"
  },
  {
    "text": "to do is it's going to take and then write the data against the attach to Amazon elastic block store the EBS",
    "start": "1802980",
    "end": "1808799"
  },
  {
    "text": "service will automatically mirror that data to another EBS volume to deal with the durability and availability that EBS",
    "start": "1808799",
    "end": "1816299"
  },
  {
    "text": "gives you and then that same write operation is going to be issued to the standby database and that's Tam my",
    "start": "1816299",
    "end": "1822299"
  },
  {
    "text": "database will once again write the data to the EBS volume that's attached to it and EBS will do the mirroring of that",
    "start": "1822299",
    "end": "1828090"
  },
  {
    "text": "data but it's not just the actual data right there's also a lot of different log files",
    "start": "1828090",
    "end": "1834119"
  },
  {
    "text": "that are involved with my sequel that need to be moved around do the different storage layers and as well to the",
    "start": "1834119",
    "end": "1840029"
  },
  {
    "text": "standby instance and down to the storage layers of the standby instance so the end of the day you have a lot of moving",
    "start": "1840029",
    "end": "1846089"
  },
  {
    "text": "parts when it comes to the movement of data and the i/o operations that have to happen to have a complete success",
    "start": "1846089",
    "end": "1852689"
  },
  {
    "text": "message and to have everything in sync between your primary and your standby database and if you start thinking about",
    "start": "1852689",
    "end": "1858719"
  },
  {
    "text": "things like network jitter or latency that may play into the equation you now have a situation where those types of",
    "start": "1858719",
    "end": "1865679"
  },
  {
    "text": "issues can impact your database and it's standby being in a state where you feel comfortable or where things are at a",
    "start": "1865679",
    "end": "1871889"
  },
  {
    "text": "state where you feel that you can recover successfully one of the things",
    "start": "1871889",
    "end": "1876929"
  },
  {
    "text": "that we did with this configuration is we actually although we did another performance test we use the Syst bench",
    "start": "1876929",
    "end": "1883529"
  },
  {
    "text": "write workload with a hundred gig data file we ran it for 30 minutes against a",
    "start": "1883529",
    "end": "1888659"
  },
  {
    "text": "my sequel database running in a single availability zone configuration on RDS we ran that for 30 minutes and at the",
    "start": "1888659",
    "end": "1896759"
  },
  {
    "text": "end of the time we found that we were able to get 780 thousand right transactions out of that test which is pretty good it works out to about 7.4",
    "start": "1896759",
    "end": "1904979"
  },
  {
    "text": "million read/write operations per million transactions this is excluding anything",
    "start": "1904979",
    "end": "1910979"
  },
  {
    "text": "having to do with the standby database in this because everything that you're seeing that how that standby database",
    "start": "1910979",
    "end": "1916649"
  },
  {
    "text": "happens is really part of the RDS service and it's not really necessarily relevant to what's actually happening on",
    "start": "1916649",
    "end": "1922799"
  },
  {
    "text": "the actual primary database instance as far as this test is concerned so on",
    "start": "1922799",
    "end": "1929699"
  },
  {
    "text": "Aurora the architecture is a little bit different we minimize and work to",
    "start": "1929699",
    "end": "1936779"
  },
  {
    "text": "minimize the amount of i/o traffic that's actually happening within the art of the Aurora database one of the",
    "start": "1936779",
    "end": "1942059"
  },
  {
    "text": "primary things that we do is that that storage layer that's spanning the multiple availability zones all the",
    "start": "1942059",
    "end": "1949079"
  },
  {
    "text": "primary instances doing is sending media log files to that storage node it's",
    "start": "1949079",
    "end": "1954269"
  },
  {
    "text": "collecting them up it's boxcar em it's it's but it's it's combining them together and sending them in regular",
    "start": "1954269",
    "end": "1959459"
  },
  {
    "text": "chunks to that storage node these redo log files are very small and less back",
    "start": "1959459",
    "end": "1964830"
  },
  {
    "text": "helps to really reduce the amount of network IO that we're actually sending across the wire to this storage layer",
    "start": "1964830",
    "end": "1973430"
  },
  {
    "text": "there's and when those long files actually get to the storage layer we end",
    "start": "1973430",
    "end": "1979290"
  },
  {
    "text": "up doing six times more rights than you would see in my sequel because we're writing it to all six nodes but it",
    "start": "1979290",
    "end": "1986550"
  },
  {
    "text": "actually ends up being about nine times less network traffic we're compared with what you would see",
    "start": "1986550",
    "end": "1992640"
  },
  {
    "text": "on the my sequel configuration and it having to write to its database storage system because all we're doing is we're",
    "start": "1992640",
    "end": "1998730"
  },
  {
    "text": "sending these small redo log files to the storage node so it allows the Aurora",
    "start": "1998730",
    "end": "2004670"
  },
  {
    "text": "cluster to be a lot more tolerant of what's going on from a networking standpoint because it's sending little",
    "start": "2004670",
    "end": "2012380"
  },
  {
    "text": "amounts of data to start with so any little hiccups are less impactful to what's happening from the Aurora cluster",
    "start": "2012380",
    "end": "2018740"
  },
  {
    "text": "standpoint so we went and ran that same suspension against an Aurora cluster",
    "start": "2018740",
    "end": "2026080"
  },
  {
    "text": "once again 30 minutes at 100 gigabyte data set and the results of that were is",
    "start": "2026080",
    "end": "2031790"
  },
  {
    "text": "that we were actually able to write 27 million transactions over that 30 minute",
    "start": "2031790",
    "end": "2037250"
  },
  {
    "text": "time frame which is 35 times more than we were able actually do on the my sequel test that worked out to about",
    "start": "2037250",
    "end": "2043730"
  },
  {
    "text": "nine hundred and fifty thousand IO operations per million transactions now that has six times amplification so it's",
    "start": "2043730",
    "end": "2051200"
  },
  {
    "text": "covering all six of the storage nodes if you broke that down it's about 150 eight thousand read/write operations per",
    "start": "2051200",
    "end": "2057500"
  },
  {
    "text": "storage notes that's seven just a little over seven and a half times less than",
    "start": "2057500",
    "end": "2063050"
  },
  {
    "text": "what it took from a my sequel standpoint so with the Aurora and the way it's configured in it and in the amount of",
    "start": "2063050",
    "end": "2068919"
  },
  {
    "text": "i/o that it's it's been able to save it's able to do more because in the",
    "start": "2068919",
    "end": "2074240"
  },
  {
    "text": "background it's doing a lot less from an i/o standpoint so let's go one step",
    "start": "2074240",
    "end": "2080929"
  },
  {
    "text": "further and let's actually take a peek inside that storage node as far as what's going on",
    "start": "2080929",
    "end": "2086000"
  },
  {
    "text": "so the first step as we already talked about is that that primary instance is gonna send its log records to that",
    "start": "2086000",
    "end": "2092480"
  },
  {
    "text": "storage node the storage node is gonna put that into an in-memory queue and then it's gonna pull off of that",
    "start": "2092480",
    "end": "2098520"
  },
  {
    "text": "queue and it's actually gonna persist that data and write it to disk at that point the data's durable on the storage",
    "start": "2098520",
    "end": "2105090"
  },
  {
    "text": "node and we acknowledge back to the primary instance and the interaction when the primary instance is done that",
    "start": "2105090",
    "end": "2110250"
  },
  {
    "text": "is the critical path from an Aurora standpoint everything else that happens after this is done asynchronously and it",
    "start": "2110250",
    "end": "2116550"
  },
  {
    "text": "can happen independent of any communication with the primary instance so once the storage node has its log",
    "start": "2116550",
    "end": "2123570"
  },
  {
    "text": "files it's actually gonna start organizing those log files and the records and those log files and you know",
    "start": "2123570",
    "end": "2128850"
  },
  {
    "text": "because things can show up out of sequence or not show up at all at times it'll find out you know do I have",
    "start": "2128850",
    "end": "2134790"
  },
  {
    "text": "everything am I missing any rice is there anything out of sequence and one of the cool things that will actually do then is",
    "start": "2134790",
    "end": "2140340"
  },
  {
    "text": "that there is it for all those six stores no there's actually a peer to peer gossip network that's in place that",
    "start": "2140340",
    "end": "2146100"
  },
  {
    "text": "allows all those storage nodes to talk to each other and help resolve conflicts where one of the nodes is maybe missing",
    "start": "2146100",
    "end": "2152160"
  },
  {
    "text": "some data so they'll communicate with each other and they'll say I'm missing this do you have this and they'll sort all that out and you'll exchange that",
    "start": "2152160",
    "end": "2158970"
  },
  {
    "text": "data so that each of the storage knows the end of the day has the same amount of data as all the other storage nodes",
    "start": "2158970",
    "end": "2164820"
  },
  {
    "text": "and that they resolve any missing data or conflicts with data that one storage node may have so after that has done and",
    "start": "2164820",
    "end": "2172920"
  },
  {
    "text": "they figured out who has what and they've got everything in order the going coalesce those log records into",
    "start": "2172920",
    "end": "2177960"
  },
  {
    "text": "really new data block versions turn those into the data blocks that are needed for the database and then",
    "start": "2177960",
    "end": "2183540"
  },
  {
    "text": "periodically once again asynchronously and very regularly the storage nodes",
    "start": "2183540",
    "end": "2188880"
  },
  {
    "text": "will actually backup that log and block data to s3 you know that being like this",
    "start": "2188880",
    "end": "2194160"
  },
  {
    "text": "is our durable storage now it's storage place for the data that we're using in our database storage nodes will also go",
    "start": "2194160",
    "end": "2202260"
  },
  {
    "text": "through and do garbage collection periodically looking for old log files and data blocks that are that are out of",
    "start": "2202260",
    "end": "2208140"
  },
  {
    "text": "date that have been replaced by other data blocks and log files that it now has and get rid of those and then",
    "start": "2208140",
    "end": "2213450"
  },
  {
    "text": "finally it will actually do some scrubbing of the data it will actually go through and read read data blocks",
    "start": "2213450",
    "end": "2221420"
  },
  {
    "text": "independent of a request from the from the storage it from the database instance and check those blocks verify",
    "start": "2221420",
    "end": "2228030"
  },
  {
    "text": "the check sums against those blocks make sure that they're actually good datablock still that they haven't been corrupted through through normal",
    "start": "2228030",
    "end": "2233820"
  },
  {
    "text": "disk usage or though that they're dirty now and if they do find a bad block they'll actually leverage that peer-to-peer",
    "start": "2233820",
    "end": "2239610"
  },
  {
    "text": "network again to talk with the other storage nodes and say hey I've got a bad block here does somebody else have a good copy of this block that they could",
    "start": "2239610",
    "end": "2246210"
  },
  {
    "text": "share with me and they'll seal themselves by sharing that data across that network again so a couple of things",
    "start": "2246210",
    "end": "2254370"
  },
  {
    "text": "about this that input queue of the data coming into the storage now that actually ends up being 46 times less",
    "start": "2254370",
    "end": "2260190"
  },
  {
    "text": "than the data that's being transferred from a my sequel standpoint because it's just redo log files it's not all the",
    "start": "2260190",
    "end": "2266460"
  },
  {
    "text": "other data and the big blocks that go along with writing from my sequel database standpoint we favor the latency",
    "start": "2266460",
    "end": "2273360"
  },
  {
    "text": "sensitive operations getting that out of the way up front and making everything else",
    "start": "2273360",
    "end": "2278550"
  },
  {
    "text": "asynchronous the storage tier is a multi-tenant storaged here so there are",
    "start": "2278550",
    "end": "2283980"
  },
  {
    "text": "going to be patterns of high usage and low usage during the day of that you",
    "start": "2283980",
    "end": "2290040"
  },
  {
    "text": "know on that storage tier so we work to take advantage of the low points in that to get a lot of these asynchronous jobs",
    "start": "2290040",
    "end": "2296130"
  },
  {
    "text": "done so that we're not putting any negative impact on the customers but still able to get all the work done in a",
    "start": "2296130",
    "end": "2302490"
  },
  {
    "text": "decent amount of time so what are the",
    "start": "2302490",
    "end": "2307980"
  },
  {
    "start": "2306000",
    "end": "2306000"
  },
  {
    "text": "other things that we've done is we've kind of worked to change how commits happen in the Aurora database so in a",
    "start": "2307980",
    "end": "2316230"
  },
  {
    "text": "traditional database approach the way that commits typically work is that somebody will do a write and those",
    "start": "2316230",
    "end": "2323190"
  },
  {
    "text": "rights are collected in a buffer and when the buffer is full or when they're",
    "start": "2323190",
    "end": "2328920"
  },
  {
    "text": "amount of time that's passed the buffer will get you know flushed and written the disk the problem with that is is",
    "start": "2328920",
    "end": "2336240"
  },
  {
    "text": "that whoever is the first writer into that new buffer will get a latency penalty if they don't if not enough",
    "start": "2336240",
    "end": "2343920"
  },
  {
    "text": "transactions come in or if that buffer doesn't fill up fast enough they're stuck waiting until the actual timeout",
    "start": "2343920",
    "end": "2349560"
  },
  {
    "text": "flush happens on that one which is you know it's not their fault they didn't do anything they just happen to be that first writer into that particular buffer",
    "start": "2349560",
    "end": "2356400"
  },
  {
    "text": "so there's a penalty on that one that's just kind of naturally exists with that approach on the Aurora side",
    "start": "2356400",
    "end": "2363870"
  },
  {
    "text": "we've kind of changed all that one we don't have to we're interacting with our code we wrote the code base we don't",
    "start": "2363870",
    "end": "2370350"
  },
  {
    "text": "necessarily have to live with the the Linux file system as far as how we need to do commits and everything so we",
    "start": "2370350",
    "end": "2375750"
  },
  {
    "text": "actually the way this works is that as soon as the first write happens we start",
    "start": "2375750",
    "end": "2380940"
  },
  {
    "text": "IO operations every right gets its own IO it's not waiting for anything else",
    "start": "2380940",
    "end": "2386070"
  },
  {
    "text": "and then those rights are collected up in a buffer and a background job collects those at some point and then",
    "start": "2386070",
    "end": "2392130"
  },
  {
    "text": "they're sent off to the storage node and they're considered durable when four out",
    "start": "2392130",
    "end": "2399510"
  },
  {
    "text": "of the six storage nodes acknowledge yes I've got the data and I've committed it to disk so at that point we consider it",
    "start": "2399510",
    "end": "2405630"
  },
  {
    "text": "very durable the next thing we're gonna do is that we're going to look at the last log record number in this case it's",
    "start": "2405630",
    "end": "2415320"
  },
  {
    "text": "number forty-seven and we're going to say who is who needs an acknowledgment who below this number needs an",
    "start": "2415320",
    "end": "2421020"
  },
  {
    "text": "acknowledgement and we're going to acknowledge back to all the numbers below that that the write was successful",
    "start": "2421020",
    "end": "2426600"
  },
  {
    "text": "and we're gonna actually at that point that's when we consider that database durable and we advanced to you know the",
    "start": "2426600",
    "end": "2433200"
  },
  {
    "text": "durability point of the database saying everything's good up to this point",
    "start": "2433200",
    "end": "2438620"
  },
  {
    "start": "2440000",
    "end": "2440000"
  },
  {
    "text": "another thing that we've done on the Aurora site is we've actually taken and kind of remodeled and re-engineered the",
    "start": "2440750",
    "end": "2447120"
  },
  {
    "text": "thread pooling that exists in my sequel today so today with my sequel every",
    "start": "2447120",
    "end": "2453780"
  },
  {
    "text": "connection gets a thread and as you grow the number of connections on your",
    "start": "2453780",
    "end": "2458940"
  },
  {
    "text": "database and your database becomes more heavily used that couldn't be a problem it can lead to performance challenges it",
    "start": "2458940",
    "end": "2465330"
  },
  {
    "text": "can really burden down your database having many number you know being able to support more and more connections as",
    "start": "2465330",
    "end": "2471090"
  },
  {
    "text": "each one of those wants a thread on its own some of that solved with Enterprise",
    "start": "2471090",
    "end": "2477180"
  },
  {
    "text": "Edition of my sequel where it's using some some thread groups where it will look and see if it sees anything that's",
    "start": "2477180",
    "end": "2482520"
  },
  {
    "text": "long-running it'll add another thread to accommodate that but but some of the work around that actually in call involves using stall threshold tuning",
    "start": "2482520",
    "end": "2489810"
  },
  {
    "text": "and you know you're using that tuning to actually say you know when something's been running so long add another",
    "start": "2489810",
    "end": "2496300"
  },
  {
    "text": "thread problem with that is that you have to really get that tuning done well because if it's not right",
    "start": "2496300",
    "end": "2502060"
  },
  {
    "text": "you may add a end up adding too many threads which will once again overburden your database or not add enough threads",
    "start": "2502060",
    "end": "2508330"
  },
  {
    "text": "which will delay getting things actually processed on the database so on the arora side what we've done from a thread",
    "start": "2508330",
    "end": "2515620"
  },
  {
    "text": "model perspective is that we've had everything connecting in through a pole all the connections happen in there and",
    "start": "2515620",
    "end": "2521080"
  },
  {
    "text": "behind there is the task queue which is actually got a bunch of threads that are looking for work when they aren't doing",
    "start": "2521080",
    "end": "2527980"
  },
  {
    "text": "work they're lookin to e pole for the work that they need to do and because these are independent we're actually",
    "start": "2527980",
    "end": "2533260"
  },
  {
    "text": "able to scale up and down the number of threads dynamically based on the number of pending transactions or connections",
    "start": "2533260",
    "end": "2539050"
  },
  {
    "text": "coming in to e pole one of the other cool things that's about this is that we",
    "start": "2539050",
    "end": "2545260"
  },
  {
    "text": "can actually it's aware of when a transaction is awaiting a commit and while we're waiting for that we can",
    "start": "2545260",
    "end": "2552100"
  },
  {
    "text": "actually repurpose that thread and let it go and do other work and keep another thread around to actually handle the",
    "start": "2552100",
    "end": "2557980"
  },
  {
    "text": "acknowledgment of the commits when they happen so it's smart enough to be able to to repurpose things so that it gets",
    "start": "2557980",
    "end": "2563050"
  },
  {
    "text": "the most out of what it's already allocated so with this approach it's actually you know comes back some of the",
    "start": "2563050",
    "end": "2568960"
  },
  {
    "text": "numbers @kt supported Katie showed Aurora is gracefully able to have a happen excuse me",
    "start": "2568960",
    "end": "2574660"
  },
  {
    "text": "graceful able to support well over 5,000 client connections on our largest instance side which is our 3/8",
    "start": "2574660",
    "end": "2580180"
  },
  {
    "text": "extra-large so another area where Aurora",
    "start": "2580180",
    "end": "2587320"
  },
  {
    "text": "has done a lot of work to kind of help improve the i/o traffic pattern is around read replicas so on the left hand",
    "start": "2587320",
    "end": "2594490"
  },
  {
    "text": "side you've got a my sequel configuration with a primary and a read replicas and the thing with read",
    "start": "2594490",
    "end": "2599680"
  },
  {
    "text": "replicas here is that whatever write work the primary database does the standby database is doing that as well",
    "start": "2599680",
    "end": "2605760"
  },
  {
    "text": "so in this example if you've got a my sequel database that is 70% right heavy",
    "start": "2605760",
    "end": "2611860"
  },
  {
    "text": "only supporting 30 percent reads your read replicas is going to be 70 percent right heavy even though it's supposed to",
    "start": "2611860",
    "end": "2617470"
  },
  {
    "text": "be a read replica and what that does is it means that if you are expecting to support more reads than 30% with your",
    "start": "2617470",
    "end": "2624220"
  },
  {
    "text": "read replicas you may need to be looking to have a read replica that's bigger than your primary or you may need to",
    "start": "2624220",
    "end": "2629380"
  },
  {
    "text": "have multiple read replicas in order to be able to support the demand from a read replicas standpoint it also means",
    "start": "2629380",
    "end": "2635110"
  },
  {
    "text": "that you're running independent layers of storage you've got two different pieces of storage that support both your",
    "start": "2635110",
    "end": "2640780"
  },
  {
    "text": "primary and your standard database which have to be monitored and cared and fed for and you can actually depending on",
    "start": "2640780",
    "end": "2646840"
  },
  {
    "text": "the volume of writes and once again the latency from an il perspective between your primary standard they can get out",
    "start": "2646840",
    "end": "2652330"
  },
  {
    "text": "of sync they can drift apart depending on how much traffic is going through so",
    "start": "2652330",
    "end": "2658000"
  },
  {
    "text": "on the Aurora side we've already talked about that the read replicas are tied in to the overall storage node and you can",
    "start": "2658000",
    "end": "2664480"
  },
  {
    "text": "put them into any availability zone but when you add a read replica what actually happens is that that master",
    "start": "2664480",
    "end": "2670450"
  },
  {
    "text": "database not only ships its redo logs to the storage node it actually shifts those redo logs to all the standby",
    "start": "2670450",
    "end": "2676360"
  },
  {
    "text": "databases or to the read replicas databases as well and what those read replica databases do is they actually",
    "start": "2676360",
    "end": "2681970"
  },
  {
    "text": "look at their cache say do I have this or not if they don't they write it to the cache it allows those read replicas",
    "start": "2681970",
    "end": "2688450"
  },
  {
    "text": "to really one have all the data that the master database has very quickly and to",
    "start": "2688450",
    "end": "2694060"
  },
  {
    "text": "be able to focus on delivering nearly a hundred percent read focused operations as a read replicas so let's talk about",
    "start": "2694060",
    "end": "2704770"
  },
  {
    "text": "availability and how important that is you know if you guys go through and you architect your applications and",
    "start": "2704770",
    "end": "2712660"
  },
  {
    "text": "everything like that and you're trying to get everything as performant as possible it really doesn't matter unless",
    "start": "2712660",
    "end": "2718300"
  },
  {
    "text": "that database is actually up and running so having the database actually available is very important so we talked",
    "start": "2718300",
    "end": "2724360"
  },
  {
    "start": "2723000",
    "end": "2723000"
  },
  {
    "text": "about some of these already you know on the on the storage side we use a quorum system for both reads and writes which",
    "start": "2724360",
    "end": "2731020"
  },
  {
    "text": "means that it's very latency tolerant so if some of the nodes are having a temporary network issue and they can't respond in",
    "start": "2731020",
    "end": "2737830"
  },
  {
    "text": "time we rely on the other nodes that can respond to us in time we use that peer to peer gossip network to help fill in",
    "start": "2737830",
    "end": "2743560"
  },
  {
    "text": "the holes with the data and any transactions that didn't arrive or any corrupt data that we might find we use",
    "start": "2743560",
    "end": "2751060"
  },
  {
    "text": "s3 as a backup to help with availability from a data perspective we continuously",
    "start": "2751060",
    "end": "2757000"
  },
  {
    "text": "scrub the data blocks that we talked about earlier and then the monitoring so because it's",
    "start": "2757000",
    "end": "2762700"
  },
  {
    "text": "a managed service you get all the benefits of the managed service provider actually monitoring those nodes and the",
    "start": "2762700",
    "end": "2768759"
  },
  {
    "text": "discs for repair the Aurora system and the Aurora team it's constantly watching those nodes to make sure that everything",
    "start": "2768759",
    "end": "2773859"
  },
  {
    "text": "is healthy and knowing where they need to go in and make make changes or fix things as kini pointed out earlier we",
    "start": "2773859",
    "end": "2780759"
  },
  {
    "text": "store the data in 10 gigabytes segments and what that allows us to do it allows us to repair bad pieces of data very",
    "start": "2780759",
    "end": "2789249"
  },
  {
    "text": "quickly because they're not two biggest segments so that we can move them around the storage tier very quickly and it",
    "start": "2789249",
    "end": "2794890"
  },
  {
    "text": "also allows us when we identify maybe a hotspot on a disk where a couple of maybe very heavy",
    "start": "2794890",
    "end": "2800979"
  },
  {
    "text": "popular customers have added some very key pieces of data and they both land on the same disk in order to make sure that",
    "start": "2800979",
    "end": "2806319"
  },
  {
    "text": "that this doesn't become too burden we can actually move those pieces of data to other disks and because they're in these 10 gigabyte segments we can move",
    "start": "2806319",
    "end": "2812680"
  },
  {
    "text": "them very quickly and frequently and then the Aurora team you know for this",
    "start": "2812680",
    "end": "2820089"
  },
  {
    "text": "storage layer they've invested a lot of time to make sure that they can add and remove members of those storage node",
    "start": "2820089",
    "end": "2827559"
  },
  {
    "text": "without impacting you so as as new nodes are coming and going from that overall",
    "start": "2827559",
    "end": "2833829"
  },
  {
    "text": "storage layer you're not impacted by them there's no freezes that happen",
    "start": "2833829",
    "end": "2838900"
  },
  {
    "text": "because something is coming or going it still will work and get the appropriate number of responses and just focus on",
    "start": "2838900",
    "end": "2845109"
  },
  {
    "text": "the other nodes that are actually there and available so things can come and go failures can happen they can self heal",
    "start": "2845109",
    "end": "2850630"
  },
  {
    "text": "and you won't be impacted from a read and write perspective so databases I've",
    "start": "2850630",
    "end": "2858999"
  },
  {
    "start": "2855000",
    "end": "2855000"
  },
  {
    "text": "seen it they crashed and they usually don't crash when you want them to so",
    "start": "2858999",
    "end": "2864039"
  },
  {
    "text": "when a database crashes you have to first get the database backup always",
    "start": "2864039",
    "end": "2870910"
  },
  {
    "text": "happens like 2:00 in the morning but you've got to get the database backup and then at that point you need to",
    "start": "2870910",
    "end": "2877539"
  },
  {
    "text": "actually checkpoint you need to kind your last good checkpoint typically those are done in about five minute",
    "start": "2877539",
    "end": "2883479"
  },
  {
    "text": "increments and then you're having to replay all the log files from that checkpoint up to the point that the",
    "start": "2883479",
    "end": "2889329"
  },
  {
    "text": "database crashed from a my sequel perspective that job is actually single-threaded so if you're",
    "start": "2889329",
    "end": "2895530"
  },
  {
    "text": "running on a very large box or Larry large ec2 instance or compute instance and it's got multiple cores it's only",
    "start": "2895530",
    "end": "2901290"
  },
  {
    "text": "gonna be using one of them because it's a single threaded process on top of that because if there's a crashes occurred",
    "start": "2901290",
    "end": "2906810"
  },
  {
    "text": "your data is more than likely not in the cache anymore so it's actually having to go back to disk to get a lot of the",
    "start": "2906810",
    "end": "2912690"
  },
  {
    "text": "information that it needs to complete this process so there's a lot of extra iOS that happen so this process can take",
    "start": "2912690",
    "end": "2918750"
  },
  {
    "text": "a period of time it's a little bit unpredictable because it really depends on what's happened since that last checkpoint and what your latency might",
    "start": "2918750",
    "end": "2925470"
  },
  {
    "text": "be from an i/o perspective to get all that data off the disk so on the Aurora side and we've already talked a lot",
    "start": "2925470",
    "end": "2932220"
  },
  {
    "text": "about that that all the redo logs are sitting there at the storage layer so you've got the ability to actually",
    "start": "2932220",
    "end": "2937320"
  },
  {
    "text": "replay all your log files directly from the storage node into your database",
    "start": "2937320",
    "end": "2943110"
  },
  {
    "text": "instances and what happens is that when you have a recovery or even if you bring",
    "start": "2943110",
    "end": "2948120"
  },
  {
    "text": "a brand-new instance into your cluster from an Aurora standpoint it's gonna reach into the storage layer and it's",
    "start": "2948120",
    "end": "2954420"
  },
  {
    "text": "gonna ask hey I'd you know when a query is executed when somebody asks for data it's gonna reach into the storage layer",
    "start": "2954420",
    "end": "2959700"
  },
  {
    "text": "say hey I'd like this block of data it's gonna pull that 10 gig block of data out it's gonna compare and say hey are there",
    "start": "2959700",
    "end": "2965790"
  },
  {
    "text": "any log entries that have actually updated this data block if it is it's gonna add them in it's gonna return it back to the database node the thing is",
    "start": "2965790",
    "end": "2972690"
  },
  {
    "text": "is that that is the same thing that happens if you were just running without a failure it's no different and it all",
    "start": "2972690",
    "end": "2978750"
  },
  {
    "text": "happens parallel distributed in asynchronous so it can happen very quickly so recovery on the aurora's",
    "start": "2978750",
    "end": "2986730"
  },
  {
    "text": "standpoint is almost you know near instantaneous because you just need to start it up and start using it you don't",
    "start": "2986730",
    "end": "2992430"
  },
  {
    "text": "need to wait for a for a checkpoint replay or anything to get caught up to a point in time you're able to start using",
    "start": "2992430",
    "end": "2997740"
  },
  {
    "text": "the storage and information in the storage immediately we talked a little",
    "start": "2997740",
    "end": "3004340"
  },
  {
    "start": "3002000",
    "end": "3002000"
  },
  {
    "text": "bit about survivable cash as we won't talk much more about it but but we have on the Aurora site have removed the caching layer from the database the",
    "start": "3004340",
    "end": "3011510"
  },
  {
    "text": "caching layer operates independent of the database so when there's a reboot of the database or over the database has a",
    "start": "3011510",
    "end": "3017090"
  },
  {
    "text": "failure you're caching layer is going to stay warm you don't have to worry about",
    "start": "3017090",
    "end": "3022640"
  },
  {
    "text": "spending time too and replay or to go back to disk to get that caching layer up and running when",
    "start": "3022640",
    "end": "3029400"
  },
  {
    "text": "you bring your database up back base back up you're able to fully start running at the level that you were",
    "start": "3029400",
    "end": "3034950"
  },
  {
    "text": "delivering to your applications prior to your failure or your reboot so when you combine this caching layer the ability",
    "start": "3034950",
    "end": "3042150"
  },
  {
    "text": "to stay warm at all times and the ability to kind of instantly recover from your crashes you you really have",
    "start": "3042150",
    "end": "3047610"
  },
  {
    "text": "like this very very easy path to be able to recover from anything that's a database failure when you're using the",
    "start": "3047610",
    "end": "3053160"
  },
  {
    "text": "Aurora clusters okay so we've got a little bit of timeline here on like kind",
    "start": "3053160",
    "end": "3059520"
  },
  {
    "start": "3056000",
    "end": "3056000"
  },
  {
    "text": "of how failures might happen and how they become a little bit more predictable from an Aurora standpoint so",
    "start": "3059520",
    "end": "3065010"
  },
  {
    "text": "the top timeline is a my sequel database running on RDS so you have their detection the RDS service is going out",
    "start": "3065010",
    "end": "3071550"
  },
  {
    "text": "and continuously monitoring the master node to make sure that it's healthy and that the availability zone is healthy",
    "start": "3071550",
    "end": "3077220"
  },
  {
    "text": "and when it detects if there's a failure it's going to start the the failover process and that field over processes is",
    "start": "3077220",
    "end": "3084870"
  },
  {
    "text": "going to be you know moving over to a standby database that you have running for your hopefully you're running a",
    "start": "3084870",
    "end": "3089910"
  },
  {
    "text": "multi availability zone and then failing over to that standby database that recovery period is going to be a little",
    "start": "3089910",
    "end": "3095820"
  },
  {
    "text": "bit variable depending on you know how much data needs to be replayed from the log files and how much data needs to be",
    "start": "3095820",
    "end": "3101430"
  },
  {
    "text": "caught up from between when the master went down and what the standby has so the recovery happens and then there's",
    "start": "3101430",
    "end": "3107670"
  },
  {
    "text": "also a DNS propagation that happens so when the RDS service fails your master over to a standby database it actually",
    "start": "3107670",
    "end": "3114120"
  },
  {
    "text": "changes the DNS entries for your database to point to the standby database which is now going to become",
    "start": "3114120",
    "end": "3119910"
  },
  {
    "text": "your master database that can take anywhere from 30 to 60 seconds so at the end of that you've got your app up and",
    "start": "3119910",
    "end": "3125700"
  },
  {
    "text": "running but there's a lot of unknowns as far as how long that time might take so",
    "start": "3125700",
    "end": "3131130"
  },
  {
    "text": "on the Aurora side you actually have your failure detection that's happening once again and once that failure is",
    "start": "3131130",
    "end": "3137460"
  },
  {
    "text": "determined the recovery is almost instantaneous because once again we",
    "start": "3137460",
    "end": "3142470"
  },
  {
    "text": "talked about once a new instance comes into play if it's a read replica it's already got all the production data or",
    "start": "3142470",
    "end": "3147720"
  },
  {
    "text": "if a brand new instance has to come into play it's able to start reading data off of the storage node immediately so your",
    "start": "3147720",
    "end": "3152850"
  },
  {
    "text": "recovery time is very very small there and your application can actually be up and running right away so I'm not actually",
    "start": "3152850",
    "end": "3158950"
  },
  {
    "text": "addressing the DNS part here because one of the cool things here is that we've been working with a Mariah DV team and",
    "start": "3158950",
    "end": "3164230"
  },
  {
    "text": "they've actually created drivers JDBC and ODBC drivers that actually work with Aurora and with those drivers they're",
    "start": "3164230",
    "end": "3170770"
  },
  {
    "text": "actually aware of all the read replicas in your Aurora cluster and what Aurora does is that it actually puts an entry",
    "start": "3170770",
    "end": "3177730"
  },
  {
    "text": "into all those read replicas of what who the master database currently is so when a failover happens the RDS service will",
    "start": "3177730",
    "end": "3184720"
  },
  {
    "text": "actually update all the read replicas to say here's the IP address of the current master and the mariah DB driver will",
    "start": "3184720",
    "end": "3190240"
  },
  {
    "text": "actually resolve that and make sure they are always connecting to the master database so it allows you to avoid having to worry about any DNS",
    "start": "3190240",
    "end": "3196420"
  },
  {
    "text": "propagation allows your application to get up and running a lot faster anytime you have any database instance issues or",
    "start": "3196420",
    "end": "3202300"
  },
  {
    "text": "failures okay and I think this is the last thing for us so high availability",
    "start": "3202300",
    "end": "3207850"
  },
  {
    "start": "3204000",
    "end": "3204000"
  },
  {
    "text": "of three replicas we all know the read replicas can be placed in any availability zone they can be tied into",
    "start": "3207850",
    "end": "3212980"
  },
  {
    "text": "the storage instance one of the cool things with the read replicas is you can actually define a priority to those you",
    "start": "3212980",
    "end": "3219490"
  },
  {
    "text": "can say which read replicas has priority from a failover perspective you can also",
    "start": "3219490",
    "end": "3225460"
  },
  {
    "text": "size your read replicas to be a different size than your primary database instance maybe your database instance your primary is very right",
    "start": "3225460",
    "end": "3232450"
  },
  {
    "text": "heavy and there's not a lot of reads for your read replicas so maybe you read replicas they're size smaller or a vice versa maybe your your primary is not",
    "start": "3232450",
    "end": "3239380"
  },
  {
    "text": "very right heavy you have a lot of reads coming on maybe you size your read replicas higher what you have the option",
    "start": "3239380",
    "end": "3245560"
  },
  {
    "text": "to is you could have one read replica that's maybe sized exactly like your master database and set that as priority",
    "start": "3245560",
    "end": "3251470"
  },
  {
    "text": "zero and that's your that's your primary point for failover and maybe your other read replicas are sized differently to",
    "start": "3251470",
    "end": "3256930"
  },
  {
    "text": "maybe handle a more appropriate workload so what happens here is that if there's",
    "start": "3256930",
    "end": "3263440"
  },
  {
    "text": "a failure on your master the RDS service will promote the highest tier read",
    "start": "3263440",
    "end": "3269050"
  },
  {
    "text": "replicas to now be your master database so in this case the one that was tier zero got promoted to be our master",
    "start": "3269050",
    "end": "3275110"
  },
  {
    "text": "database okay all these things we've",
    "start": "3275110",
    "end": "3281500"
  },
  {
    "text": "given you the option to be able to simulate those you know Katie mentioned earlier that you know this is there's real-world things that happen",
    "start": "3281500",
    "end": "3287950"
  },
  {
    "text": "out there just running tests on empty boxes is maybe not the the the true",
    "start": "3287950",
    "end": "3293220"
  },
  {
    "text": "statement of how well it performs so you actually have the option in Aurora to run sequel statements that will help",
    "start": "3293220",
    "end": "3299230"
  },
  {
    "text": "simulate failures and you can actually use that to see how the Aurora system performs and how your applications",
    "start": "3299230",
    "end": "3304900"
  },
  {
    "text": "perform whether you're simulating the failure of a database node simulate the failure of disks or even simulate some",
    "start": "3304900",
    "end": "3311140"
  },
  {
    "text": "Network issues or the failure of a network to see how everything is performing so I encourage you to go in",
    "start": "3311140",
    "end": "3316210"
  },
  {
    "text": "and test that out as kind of your final validation of how Aurora is working for you and your applications as you're",
    "start": "3316210",
    "end": "3321700"
  },
  {
    "text": "using it so that comes to the end we're going about to wrap up I'd like to first",
    "start": "3321700",
    "end": "3328059"
  },
  {
    "text": "of all say thank you everybody for coming make sure you fill out your surveys we're really interested in your feedback we want to make this as good as possible",
    "start": "3328059",
    "end": "3334510"
  },
  {
    "text": "and we'll stick around for Q&A if you want to if you want to go to the mic we'll take Q&A or if you just want to",
    "start": "3334510",
    "end": "3339520"
  },
  {
    "text": "meet us down here in the corner if you're too shy we'll happy to take your questions thank you there was a lot of",
    "start": "3339520",
    "end": "3352980"
  },
  {
    "text": "eating your own dog food as far as AWS infrastructure but curiously absent from that list was Kinesis was that something",
    "start": "3352980",
    "end": "3361059"
  },
  {
    "text": "that you considered for like the transaction log and do you feel like it does does have a place in the Aurora",
    "start": "3361059",
    "end": "3366099"
  },
  {
    "text": "infrastructure to use something that's a little bit lower latency um I I mean",
    "start": "3366099",
    "end": "3371109"
  },
  {
    "text": "they were the aurora product was like a three years in the work and it works yeah and Kinesis is a little bit you",
    "start": "3371109",
    "end": "3377740"
  },
  {
    "text": "know it's been a couple years out and my although I wasn't part of the team for that what I mean I'm sure that they had",
    "start": "3377740",
    "end": "3383290"
  },
  {
    "text": "to rethink it maybe Kinesis would play into it but I don't also say that Kinesis wet it wasn't there when some of",
    "start": "3383290",
    "end": "3388299"
  },
  {
    "text": "the key components are built but I would have to go back to the team more specifically and say his Canisius a good",
    "start": "3388299",
    "end": "3394420"
  },
  {
    "text": "fit or not to answer that question entirely cool all right thank you yep I",
    "start": "3394420",
    "end": "3399720"
  },
  {
    "text": "know there's some some of the challenges you guys the first supporting expediate",
    "start": "3399720",
    "end": "3405190"
  },
  {
    "text": "you you're the ruler you have anything to do",
    "start": "3405190",
    "end": "3411560"
  },
  {
    "text": "you think the Expedia came to you try to resolve some of the issues with RO",
    "start": "3411560",
    "end": "3418810"
  },
  {
    "text": "alright so the Expedia as I mentioned earlier they're the issues were related",
    "start": "3418810",
    "end": "3424970"
  },
  {
    "text": "to cost and performance scaling and so",
    "start": "3424970",
    "end": "3430570"
  },
  {
    "text": "you know they explored other things on Amazon like Cassandra solar index but",
    "start": "3430570",
    "end": "3436550"
  },
  {
    "text": "they eventually zero down on for its superior cost performance as well cause",
    "start": "3436550",
    "end": "3442190"
  },
  {
    "text": "characteristics as well as the performance they were able to drive out of it so is that your question or okay",
    "start": "3442190",
    "end": "3454570"
  },
  {
    "text": "so that is not public in the public domain right now so you know can't",
    "start": "3454570",
    "end": "3463070"
  },
  {
    "text": "really share customer details unless they want us to are you interested in",
    "start": "3463070",
    "end": "3468200"
  },
  {
    "text": "talking to them as far as what they did or are you looking for okay okay was",
    "start": "3468200",
    "end": "3481400"
  },
  {
    "text": "announced in November 2014 and then it was in private beta for a few months and",
    "start": "3481400",
    "end": "3488210"
  },
  {
    "text": "general availability was in July not sure when Expedia started their journey",
    "start": "3488210",
    "end": "3494510"
  },
  {
    "text": "with Aurora maybe as beta or after it was GA but yeah the world has been around for some time it's just July last",
    "start": "3494510",
    "end": "3502070"
  },
  {
    "text": "year that we made it widely I can tell you I mean we were very open to customer feedback we live and die by customer",
    "start": "3502070",
    "end": "3507859"
  },
  {
    "text": "feedback so a company like Expedia if they were using Aurora I'm sure that there was some feedback that was given",
    "start": "3507859",
    "end": "3513740"
  },
  {
    "text": "to us we're pretty agile we'll be able to role changes into the environment we",
    "start": "3513740",
    "end": "3518900"
  },
  {
    "text": "do patching for the Aurora database so it wouldnt be surprised if there was some feedback but we sir yet we don't",
    "start": "3518900",
    "end": "3524119"
  },
  {
    "text": "have the visibility on",
    "start": "3524119",
    "end": "3527049"
  }
]