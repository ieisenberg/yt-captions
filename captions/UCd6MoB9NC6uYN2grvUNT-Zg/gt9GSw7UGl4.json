[
  {
    "start": "0",
    "end": "66000"
  },
  {
    "text": "good afternoon I'm uh guess we get started here I'm I'm Brent big injure",
    "start": "439",
    "end": "6569"
  },
  {
    "text": "and I'm a database engineer at Amazon I'm a longtime DBA who's very recently",
    "start": "6569",
    "end": "15179"
  },
  {
    "text": "kind of jumped in the Postgres pool both feet first as we have migrated several",
    "start": "15179",
    "end": "21029"
  },
  {
    "text": "hundred databases over to Aurora Postgres so gonna talk about that our",
    "start": "21029",
    "end": "26189"
  },
  {
    "text": "migration story and you know some of the lessons we've learned in order to do",
    "start": "26189",
    "end": "32700"
  },
  {
    "text": "that I work for the department I work for is Amazon Fulfillment technology you can kind of think of prime which is",
    "start": "32700",
    "end": "40520"
  },
  {
    "text": "everything that happens in the fulfillment centers all of the inventory movement in the fulfillment centers and",
    "start": "40520",
    "end": "47430"
  },
  {
    "text": "all of the different teams that work in there and so I'll tell you a little bit about that as well as some technical",
    "start": "47430",
    "end": "54539"
  },
  {
    "text": "challenges that we had to overcome along the way and in really just our migration",
    "start": "54539",
    "end": "59670"
  },
  {
    "text": "story so aft Amazon Fulfillment",
    "start": "59670",
    "end": "64680"
  },
  {
    "text": "technologies this is a picture of one of the fulfillment centers there are",
    "start": "64680",
    "end": "70530"
  },
  {
    "start": "66000",
    "end": "66000"
  },
  {
    "text": "approximately we migrated approximately 350 of them worldwide so each each one of them had a source Oracle database",
    "start": "70530",
    "end": "77070"
  },
  {
    "text": "that was it represents one fulfillment center and you can see there some of",
    "start": "77070",
    "end": "83790"
  },
  {
    "text": "them are very large they range in size from tens of thousands of feet to over a",
    "start": "83790",
    "end": "89220"
  },
  {
    "text": "million feet and they have hundreds or even thousands of FC associates in miles",
    "start": "89220",
    "end": "96450"
  },
  {
    "text": "of different conveyance is pushing totes all along so there's to give you an",
    "start": "96450",
    "end": "102299"
  },
  {
    "text": "example you know one of our larger ones in in size at least is you know about 1.2 million square feet it's about six",
    "start": "102299",
    "end": "109680"
  },
  {
    "text": "of the largest Costco's or 28 football fields so there's a lot happening there and about 2,500 employees or so and a",
    "start": "109680",
    "end": "118049"
  },
  {
    "text": "lot of systems in place to do inventory movement and packages one of the",
    "start": "118049",
    "end": "125600"
  },
  {
    "text": "interesting parts of of this system is that it we deal with physical machines",
    "start": "125600",
    "end": "131370"
  },
  {
    "text": "and processes and people some of these automated some of these synchronous some",
    "start": "131370",
    "end": "137819"
  },
  {
    "text": "of them asynchronous but everything really requires the inventory picture",
    "start": "137819",
    "end": "143459"
  },
  {
    "text": "you know to be able to see where an item is in different stations so for instance these are showing these are showing",
    "start": "143459",
    "end": "151890"
  },
  {
    "text": "inventory bins most likely this associate is is picking items he's looking at a handheld scanner getting",
    "start": "151890",
    "end": "158280"
  },
  {
    "text": "the items that need to be picked putting them in a tote and sending the long conveyance to to the next station other",
    "start": "158280",
    "end": "165269"
  },
  {
    "text": "FCS are automated and where they'll literally pick up a shelf and the FC associate is gonna be stationary and",
    "start": "165269",
    "end": "172260"
  },
  {
    "text": "it'll robot will bring the shelf to him and he's able to do his job but one of",
    "start": "172260",
    "end": "177690"
  },
  {
    "text": "the things that's important to us in that I wanted to mention is his latency because he's looking at this in",
    "start": "177690",
    "end": "182879"
  },
  {
    "text": "real-time and he has to pick things very quickly you know the you know sequel",
    "start": "182879",
    "end": "189540"
  },
  {
    "text": "queries and things like that from from the database have a very strict SLA in order to allow him in you know a hundred",
    "start": "189540",
    "end": "196349"
  },
  {
    "text": "other associates doing the same job to not be standing around another station",
    "start": "196349",
    "end": "203099"
  },
  {
    "text": "here is a pack station they're taking the items that were in this tote and software is telling them size of the Box",
    "start": "203099",
    "end": "209760"
  },
  {
    "text": "to use and and other things to pack it with and to put it on the conveyor belt to be sent along and one other picture I",
    "start": "209760",
    "end": "217410"
  },
  {
    "text": "just wanted to show you is again why latency is important you can see in the in the background here that red light is",
    "start": "217410",
    "end": "223319"
  },
  {
    "text": "a scanner it's basically scanning a label that a pack associate has put on a box and what",
    "start": "223319",
    "end": "230699"
  },
  {
    "text": "it does with that is it looks up that piece of inventory or those what's in that shipment and it's gonna make a lot",
    "start": "230699",
    "end": "236940"
  },
  {
    "text": "of business decisions based on what's in there based on the weight based on how many items it is based on where it's shipping to based on if you know for",
    "start": "236940",
    "end": "244949"
  },
  {
    "text": "instance nail polishes is in there you wouldn't think so but it's a hazardous item has to be treated a little",
    "start": "244949",
    "end": "251010"
  },
  {
    "text": "differently so in real time it's making decisions and it's ultimately going to print a label and slam it on the box so",
    "start": "251010",
    "end": "258810"
  },
  {
    "text": "all of this has to be done rather quickly accessing database queries so if there's any latency in the in the",
    "start": "258810",
    "end": "267210"
  },
  {
    "text": "API calls this automated conveyor belt as it moves along its gonna kick out this box and cause problem solve",
    "start": "267210",
    "end": "274590"
  },
  {
    "text": "associates to have to deal with this manually and and so it's obviously not",
    "start": "274590",
    "end": "281069"
  },
  {
    "text": "desirable so as I mentioned we did",
    "start": "281069",
    "end": "288240"
  },
  {
    "start": "286000",
    "end": "286000"
  },
  {
    "text": "migrate about 350 of Oracle source databases they are they all had the same",
    "start": "288240",
    "end": "295889"
  },
  {
    "text": "schema they were all set up as oracle data guard with active Data Guard in our",
    "start": "295889",
    "end": "301919"
  },
  {
    "text": "in our Oracle environment and one of the tall orders to Aurora Postgres is to to",
    "start": "301919",
    "end": "310590"
  },
  {
    "text": "have to live up to supporting 20 years of DBAs kind of wrenching and tweaking",
    "start": "310590",
    "end": "316289"
  },
  {
    "text": "and optimizing queries and migrate that over to Postgres and have that pretty",
    "start": "316289",
    "end": "321570"
  },
  {
    "text": "much just work we also had minimal downtime requirements will I'll talk a",
    "start": "321570",
    "end": "327720"
  },
  {
    "text": "lot more about what those were as we as we go along and dozens of different",
    "start": "327720",
    "end": "333300"
  },
  {
    "text": "micro services and there are a lot of having 20 years of legacy systems we",
    "start": "333300",
    "end": "340530"
  },
  {
    "text": "didn't know every single use case that was there we knew they were probably some that were missed little jobs little",
    "start": "340530",
    "end": "345690"
  },
  {
    "text": "things little systems that were in place and just running that teams may not have",
    "start": "345690",
    "end": "350789"
  },
  {
    "text": "known about and so there was a lot of dependency analysis that we needed to do and we talked about the call latency as",
    "start": "350789",
    "end": "358800"
  },
  {
    "text": "well but all in all these three hundred and fifty databases we we empowered our",
    "start": "358800",
    "end": "365820"
  },
  {
    "text": "service teams to do migrations kind of on their schedule so what really ended up happening is we did well over a",
    "start": "365820",
    "end": "370889"
  },
  {
    "text": "thousand migrations because we did them at different times after we decoupled some of these relationships but so that",
    "start": "370889",
    "end": "379020"
  },
  {
    "text": "was just some of the the migration complexity I wanted to mention at a",
    "start": "379020",
    "end": "384380"
  },
  {
    "text": "50,000 foot view this is kind of what our environment looks like we have a",
    "start": "384380",
    "end": "389400"
  },
  {
    "text": "number of different teams such as receive or stow or pick and sort pack",
    "start": "389400",
    "end": "394620"
  },
  {
    "text": "and ship all of these were accessing inventory calls to get the state of",
    "start": "394620",
    "end": "400199"
  },
  {
    "text": "inventory as they perform their functions as well as directly accessing the Oracle",
    "start": "400199",
    "end": "406650"
  },
  {
    "text": "database as well some of the challenges",
    "start": "406650",
    "end": "412590"
  },
  {
    "start": "411000",
    "end": "411000"
  },
  {
    "text": "in our legacy Oracle on premise systems were we're definitely scalability it was",
    "start": "412590",
    "end": "419850"
  },
  {
    "text": "difficult to scale in an Oracle active Data Guard you've got you know primary and a and a standby instance and in",
    "start": "419850",
    "end": "425940"
  },
  {
    "text": "order to scale up to the next instance size we need to restore a database bring",
    "start": "425940",
    "end": "432270"
  },
  {
    "text": "it into our data guard replication and then after that's in place failover to",
    "start": "432270",
    "end": "438030"
  },
  {
    "text": "it and then do the same thing on the new standby as well so scaling up to another",
    "start": "438030",
    "end": "443160"
  },
  {
    "text": "instance actually was a lot of effort and required custom hardware we were on",
    "start": "443160",
    "end": "448350"
  },
  {
    "text": "the largest hardware that was available to us at amazon.com not AWS but this",
    "start": "448350",
    "end": "456180"
  },
  {
    "text": "this Hardware had to be specially built for us to support our i/o requirements",
    "start": "456180",
    "end": "461430"
  },
  {
    "text": "and such other challenges for us were",
    "start": "461430",
    "end": "467090"
  },
  {
    "text": "availability for instance if we had a failover and and had a hardware issue",
    "start": "467090",
    "end": "473820"
  },
  {
    "text": "that had to be worked on by data center technicians and involving our team to make sure that we can we can bring that",
    "start": "473820",
    "end": "481050"
  },
  {
    "text": "back to a healthy state so we sometimes took many hours to get back to a high availability of active replication and",
    "start": "481050",
    "end": "487980"
  },
  {
    "text": "as well as failover took a little bit longer one to one to two minutes three",
    "start": "487980",
    "end": "494070"
  },
  {
    "text": "minutes approximately and the hardware management was more difficult for us",
    "start": "494070",
    "end": "500130"
  },
  {
    "text": "having to forecast and look against our forecast to ensure that our peak season",
    "start": "500130",
    "end": "505980"
  },
  {
    "text": "we're gonna have to juggle around Hardware at times if forecasts are off for a different database systems or",
    "start": "505980",
    "end": "511770"
  },
  {
    "text": "different FC's and created a lot of operational burden so one of the things",
    "start": "511770",
    "end": "518430"
  },
  {
    "start": "518000",
    "end": "518000"
  },
  {
    "text": "we did are our service teams kind of took a step back and looked at their services and really looked at what was",
    "start": "518430",
    "end": "524610"
  },
  {
    "text": "the optimal persistence layer for their services so looking at the purpose-built",
    "start": "524610",
    "end": "530970"
  },
  {
    "text": "databases relational has obviously been there for many many years but in a lot of",
    "start": "530970",
    "end": "536730"
  },
  {
    "text": "cases the service teams were able to determined it that's not optimal for them they actually could be better",
    "start": "536730",
    "end": "543600"
  },
  {
    "text": "suited in a key-value store such as dynamo or any other number of kind of purpose-built databases that better suit",
    "start": "543600",
    "end": "550529"
  },
  {
    "text": "their service so what what we ended up",
    "start": "550529",
    "end": "556019"
  },
  {
    "text": "with was something closer to this which is we have a set of purpose-built",
    "start": "556019",
    "end": "562439"
  },
  {
    "text": "databases that each team evaluated and they can access what technology makes",
    "start": "562439",
    "end": "569610"
  },
  {
    "text": "sense to them now they still all need to access the inventory picture and this is a foundational service so teams worked",
    "start": "569610",
    "end": "577860"
  },
  {
    "text": "hard to ensure that any inventory calls were done at a service layer instead of",
    "start": "577860",
    "end": "583589"
  },
  {
    "text": "going directly to the database which has just kind of been done over you know 20",
    "start": "583589",
    "end": "588629"
  },
  {
    "text": "years of legacy operation but when calling inventory those databases and",
    "start": "588629",
    "end": "593850"
  },
  {
    "text": "and several others here they access or Postgres but the other other services",
    "start": "593850",
    "end": "602759"
  },
  {
    "text": "can make decisions what's the best technology for them so I won't spend too",
    "start": "602759",
    "end": "610379"
  },
  {
    "start": "608000",
    "end": "608000"
  },
  {
    "text": "much sight on time on this slide if we've kind of talked about why we chose or a Postgres performance and",
    "start": "610379",
    "end": "618089"
  },
  {
    "text": "availability that suited our needs as well as Postgres is feature and sequel parity with Oracle was was important to",
    "start": "618089",
    "end": "625439"
  },
  {
    "text": "us scalability the ability to scale vertically or horizontally and we'll",
    "start": "625439",
    "end": "630660"
  },
  {
    "text": "talk more about some of the decisions we looked at for horizontal scaling as well and that it was a managed service and we",
    "start": "630660",
    "end": "638160"
  },
  {
    "text": "have taken a lot of we've taken advantage of the fact that we now have",
    "start": "638160",
    "end": "643709"
  },
  {
    "text": "snapshot backups and to use those in nonprofits and things like that so to",
    "start": "643709",
    "end": "650249"
  },
  {
    "text": "talk about our migration a little bit the preparation a couple of the kind of",
    "start": "650249",
    "end": "657299"
  },
  {
    "text": "overall design patterns that we looked at we're just to separate our AWS accounts into into prod and non prod as",
    "start": "657299",
    "end": "664429"
  },
  {
    "text": "a kind of a best practice as well as look at our applications and",
    "start": "664429",
    "end": "670470"
  },
  {
    "text": "what could we leverage to use a fleet of readers to be able to have the ability",
    "start": "670470",
    "end": "675630"
  },
  {
    "text": "to scale horizontally so application teams looked at their at their code and",
    "start": "675630",
    "end": "681420"
  },
  {
    "text": "we are able to make use of you know up to 15 readers at this point for for our",
    "start": "681420",
    "end": "687120"
  },
  {
    "text": "read workload that's applicable and then the utilizing snapshots and clones as I",
    "start": "687120",
    "end": "694320"
  },
  {
    "text": "mentioned as we're able to obtain a data set for a large database and then slowly",
    "start": "694320",
    "end": "702240"
  },
  {
    "text": "build our test harness around that getting better in our load testing and to be able to use that we have we had",
    "start": "702240",
    "end": "709980"
  },
  {
    "text": "set out to to make sure that we can do this with our largest database volume of",
    "start": "709980",
    "end": "719010"
  },
  {
    "text": "course security at different levels encryption in transit and at rest something that's very easy to do as as",
    "start": "719010",
    "end": "726029"
  },
  {
    "text": "we move into over or Postgres and we plan to automate using standard AWS",
    "start": "726029",
    "end": "733199"
  },
  {
    "text": "api's and CL is which is was critical to us as we're migrating doing over a",
    "start": "733199",
    "end": "738930"
  },
  {
    "text": "thousand migrations one of the other things that that we plan to do is not",
    "start": "738930",
    "end": "746550"
  },
  {
    "text": "just do one or two kind of test cut overs but a half a dozen or more and we",
    "start": "746550",
    "end": "753720"
  },
  {
    "text": "really learn things every time so if this was this was something that we",
    "start": "753720",
    "end": "758790"
  },
  {
    "text": "probably could have gotten by with a couple of them but doing multiple cut overs really allowed us to learn every",
    "start": "758790",
    "end": "764760"
  },
  {
    "text": "time and get better as our automation kind of continued so I'm gonna go",
    "start": "764760",
    "end": "771660"
  },
  {
    "text": "through some of the differences that you know going from an Oracle database these are kind of the bigger ones there's",
    "start": "771660",
    "end": "778170"
  },
  {
    "text": "certainly a lot of considerations to make but for us time stamps we had to",
    "start": "778170",
    "end": "783779"
  },
  {
    "text": "really consider that when we have an application server fleet that has a shared connection pool that's connecting",
    "start": "783779",
    "end": "791459"
  },
  {
    "text": "to a number of databases in a region like a hundred databases our applications actually expect the",
    "start": "791459",
    "end": "798390"
  },
  {
    "text": "server's timezone which is what Oracle's estate provides versus like a clock time scamp giving a",
    "start": "798390",
    "end": "804269"
  },
  {
    "text": "client time zone and I mean if you were gonna build an application today you probably would use UTC and and not have",
    "start": "804269",
    "end": "811319"
  },
  {
    "text": "some of these maybe issues but this was legacy and it's just something that we had to consider the time zone",
    "start": "811319",
    "end": "818220"
  },
  {
    "text": "differences in our in our applications",
    "start": "818220",
    "end": "822379"
  },
  {
    "start": "823000",
    "end": "823000"
  },
  {
    "text": "partitioning differences were also something to consider for instance we we",
    "start": "824480",
    "end": "829649"
  },
  {
    "text": "use partition tables to manage our data lifecycle and to just age off old data",
    "start": "829649",
    "end": "835310"
  },
  {
    "text": "date range partitions and we had to at least start conversations around not",
    "start": "835310",
    "end": "841860"
  },
  {
    "text": "being able to have global uniqueness across an entire table to be able to",
    "start": "841860",
    "end": "847230"
  },
  {
    "text": "ensure that applications aren't relying on that and if so find some ways to code around that because even even with",
    "start": "847230",
    "end": "854639"
  },
  {
    "text": "improvements you know from 9 6 to 11 and you know there obviously have been a",
    "start": "854639",
    "end": "860220"
  },
  {
    "text": "whole lot there's there still feature differences between what what Oracle provides in in Postgres another",
    "start": "860220",
    "end": "871920"
  },
  {
    "text": "consideration was the collation of character types so for instance text VAR",
    "start": "871920",
    "end": "878850"
  },
  {
    "text": "car and car data in in Oracle uses see",
    "start": "878850",
    "end": "884579"
  },
  {
    "text": "collation and when you spin up a and or Postgres database by default you get you",
    "start": "884579",
    "end": "892170"
  },
  {
    "text": "get English us so to take a look at the character types here ordered by by a",
    "start": "892170",
    "end": "898139"
  },
  {
    "text": "value in different ways they're you know they're sorted in this case three to one",
    "start": "898139",
    "end": "904880"
  },
  {
    "text": "whereas in Oracle that same that same collation difference would have been one",
    "start": "904880",
    "end": "910350"
  },
  {
    "text": "two three in that way so that can if your applications depend on that sort order for various queries that's also",
    "start": "910350",
    "end": "917399"
  },
  {
    "text": "something to consider and it was also something to consider for our migration we used the AWS database migration tool",
    "start": "917399",
    "end": "924810"
  },
  {
    "text": "that also does validations in based on collation order",
    "start": "924810",
    "end": "932120"
  },
  {
    "start": "935000",
    "end": "935000"
  },
  {
    "text": "as I mentioned the analyzing dependencies was was a big was it was a",
    "start": "935620",
    "end": "942560"
  },
  {
    "text": "big issue for us or a big challenge because we didn't know all of everything",
    "start": "942560",
    "end": "947600"
  },
  {
    "text": "that accessed a bunch of shared tables or views we had a lot of materialized views that were across database and",
    "start": "947600",
    "end": "954440"
  },
  {
    "text": "accessing different databases and ETL jobs so it was a big web of dependencies",
    "start": "954440",
    "end": "960920"
  },
  {
    "text": "that we really had to unpack to start this and spent a lot of time doing that",
    "start": "960920",
    "end": "966970"
  },
  {
    "text": "we started looking at Oracle views DBA",
    "start": "966970",
    "end": "972050"
  },
  {
    "text": "hysts active session history as well as periodic sampling of the dollar sequel",
    "start": "972050",
    "end": "978950"
  },
  {
    "text": "or V dollar sequel area to capture capture the different sequels as well as",
    "start": "978950",
    "end": "984230"
  },
  {
    "text": "logon triggers and very specific logon triggers in some cases but to be able to capture this data and provide it to our",
    "start": "984230",
    "end": "991130"
  },
  {
    "text": "service teams in order for them to programmatically go through this and and identify those dependencies and for",
    "start": "991130",
    "end": "998870"
  },
  {
    "text": "instance things that are running on a standby as well those we have a lot of",
    "start": "998870",
    "end": "1003960"
  },
  {
    "text": "data warehouse jobs where we did that were running on an active Data Guard standby all of those kind of needed to",
    "start": "1003960",
    "end": "1010330"
  },
  {
    "text": "be considered in unpacking this dependency web of you know 20 years of",
    "start": "1010330",
    "end": "1015910"
  },
  {
    "text": "development of course there are other ways too",
    "start": "1015910",
    "end": "1022000"
  },
  {
    "text": "many ways to look at and try to understand what squaring your database",
    "start": "1022000",
    "end": "1027250"
  },
  {
    "text": "there's there's auditing and things like that that was kind of a heavier approach and we were able to do it without doing",
    "start": "1027250",
    "end": "1036010"
  },
  {
    "text": "active like Oracle auditing which is more with Whittemore of greater",
    "start": "1036010",
    "end": "1043120"
  },
  {
    "text": "performance impact",
    "start": "1043120",
    "end": "1046079"
  },
  {
    "text": "so in our migration I'm gonna talk a bit about our our automation at it at least",
    "start": "1048570",
    "end": "1056610"
  },
  {
    "start": "1051000",
    "end": "1051000"
  },
  {
    "text": "a high level and so launching databases and you know integrating with a lot of",
    "start": "1056610",
    "end": "1062130"
  },
  {
    "text": "different components there we we first started with you know being able to",
    "start": "1062130",
    "end": "1068520"
  },
  {
    "text": "obviously track these databases hundreds of databases and entering it in you know",
    "start": "1068520",
    "end": "1074490"
  },
  {
    "text": "management schema of ours and then creating reserved instances for",
    "start": "1074490",
    "end": "1080550"
  },
  {
    "text": "instance you might seem a little strange that we pay AWS but you know amazon.com",
    "start": "1080550",
    "end": "1086220"
  },
  {
    "text": "as a customer might be shifting from one hand to another at some level but our",
    "start": "1086220",
    "end": "1091350"
  },
  {
    "text": "department is charged and in the same way as an external customer and our",
    "start": "1091350",
    "end": "1097380"
  },
  {
    "text": "budgets are based on that so we're very frugal to look at you know reserved",
    "start": "1097380",
    "end": "1102540"
  },
  {
    "text": "instances to cut down on on-demand pricing so spinning creating reserved",
    "start": "1102540",
    "end": "1108240"
  },
  {
    "text": "instances and then creating databases we",
    "start": "1108240",
    "end": "1113910"
  },
  {
    "text": "on boarded various different ETL ETL jobs as well as created schedules jobs",
    "start": "1113910",
    "end": "1120570"
  },
  {
    "text": "for things like our partitioning like a mentioned you know rolling awful old data integrating with cloud watch",
    "start": "1120570",
    "end": "1130400"
  },
  {
    "text": "creating monitoring on top of that and alerting and then finally applying the",
    "start": "1130400",
    "end": "1137460"
  },
  {
    "text": "DDL to our over a Postgres databases and finally kicking off DMS was our tool to",
    "start": "1137460",
    "end": "1145650"
  },
  {
    "text": "migrate the data our work we had a",
    "start": "1145650",
    "end": "1152130"
  },
  {
    "start": "1150000",
    "end": "1150000"
  },
  {
    "text": "number of migration requirements to do this one was kind of to do no harm of course in our live production systems",
    "start": "1152130",
    "end": "1159120"
  },
  {
    "text": "that would impact FC associates and a relatively short downtime window from",
    "start": "1159120",
    "end": "1166320"
  },
  {
    "text": "about an hour to two hours at the very most and the Arora Postgres performance",
    "start": "1166320",
    "end": "1173490"
  },
  {
    "text": "obviously had to be on par better than what we were moving from and so when",
    "start": "1173490",
    "end": "1179910"
  },
  {
    "text": "when migrating a data we needed to perform a full load and keep in continuous replication and",
    "start": "1179910",
    "end": "1187230"
  },
  {
    "text": "have that validation done as its streaming by you know setting up",
    "start": "1187230",
    "end": "1192330"
  },
  {
    "text": "migration a week ahead of time and being able to validate that data we didn't",
    "start": "1192330",
    "end": "1198720"
  },
  {
    "text": "want to find out on our migration day that once we cut the stream of replication that we had a data error all",
    "start": "1198720",
    "end": "1206160"
  },
  {
    "text": "of our schemas all of the schema is is the same but obviously data in different",
    "start": "1206160",
    "end": "1212490"
  },
  {
    "text": "regions can be can be different jogo question sure I'll show you in one",
    "start": "1212490",
    "end": "1222780"
  },
  {
    "text": "more slide it I'll give a big picture there's but the database migration",
    "start": "1222780",
    "end": "1228120"
  },
  {
    "text": "service is what we what we did use and it was using change data capture to do",
    "start": "1228120",
    "end": "1234090"
  },
  {
    "text": "the to do the streaming so ongoing replication and then to have alarming",
    "start": "1234090",
    "end": "1241740"
  },
  {
    "text": "and in monitoring as well on that",
    "start": "1241740",
    "end": "1246860"
  },
  {
    "text": "automation of course is is with key to to our migration efforts to have the",
    "start": "1247040",
    "end": "1252240"
  },
  {
    "text": "automated provisioning and data migration as well as the ability to",
    "start": "1252240",
    "end": "1260340"
  },
  {
    "text": "we're gonna do a thousand migrations as planned we're gonna have to be doing a lot of these per day and so in our kind",
    "start": "1260340",
    "end": "1267600"
  },
  {
    "text": "of peak migration time period we often did about 20 of these plus a day and so",
    "start": "1267600",
    "end": "1276480"
  },
  {
    "text": "here's a very high-level picture of what our what it looked like for us for using",
    "start": "1276480",
    "end": "1283470"
  },
  {
    "text": "the AWS schema conversion tool to access our Oracle source database obtained you",
    "start": "1283470",
    "end": "1290010"
  },
  {
    "text": "know the DDL and the schema convert that to Postgres DDL and then apply that to",
    "start": "1290010",
    "end": "1296520"
  },
  {
    "text": "an Aurora Postgres instance and then once we once we applied that of course",
    "start": "1296520",
    "end": "1302310"
  },
  {
    "text": "then starting the database migration service to access our source database",
    "start": "1302310",
    "end": "1308870"
  },
  {
    "text": "perform a full data load and then start that CDC ongoing replication",
    "start": "1308870",
    "end": "1317210"
  },
  {
    "start": "1320000",
    "end": "1320000"
  },
  {
    "text": "so I wanted to talk about you know a couple of things that that we did in the migration as well is that we never",
    "start": "1320450",
    "end": "1326460"
  },
  {
    "text": "planned for a zero down time migration it's probably we could have it",
    "start": "1326460",
    "end": "1332130"
  },
  {
    "text": "technically it would have been possible or very close to zero but it wasn't a requirement and it and it really saved",
    "start": "1332130",
    "end": "1337830"
  },
  {
    "text": "us a lot of effort to plan to have small down time windows as opposed to very",
    "start": "1337830",
    "end": "1343890"
  },
  {
    "text": "complicated processes that would have had to support that so our our migration",
    "start": "1343890",
    "end": "1349140"
  },
  {
    "text": "methodology to to do that was was helpful this was primarily a lift and",
    "start": "1349140",
    "end": "1355380"
  },
  {
    "text": "shift we didn't make too many code changes unless we we needed to or we picked up a piece of software where it",
    "start": "1355380",
    "end": "1362880"
  },
  {
    "text": "just made sense to make changes we started of course with a lot of my",
    "start": "1362880",
    "end": "1368460"
  },
  {
    "text": "manual migrations and a handful of pilot databases added added automation and and",
    "start": "1368460",
    "end": "1378180"
  },
  {
    "text": "then we migrated our largest database and that was done intentionally to",
    "start": "1378180",
    "end": "1383880"
  },
  {
    "text": "basically exercise that we could do it for all of the rest of our very large databases as well and then as we went we",
    "start": "1383880",
    "end": "1391920"
  },
  {
    "text": "added full migration automation so I",
    "start": "1391920",
    "end": "1398880"
  },
  {
    "text": "wanted to kind of talk about the migration automation that we that we built all of this was is a little bit",
    "start": "1398880",
    "end": "1406500"
  },
  {
    "start": "1399000",
    "end": "1399000"
  },
  {
    "text": "complicated diagram so I'll walk through pieces of it but it's based on Amazon",
    "start": "1406500",
    "end": "1411510"
  },
  {
    "text": "simple workflow and it was it's intended for you know a DevOps operator to kick",
    "start": "1411510",
    "end": "1417930"
  },
  {
    "text": "off via a CLI or a web UI in different pieces and have a this system",
    "start": "1417930",
    "end": "1426240"
  },
  {
    "text": "description tell basically the system",
    "start": "1426240",
    "end": "1433590"
  },
  {
    "text": "description is going to describe every step along the way and it uses a requires or provides and invokes type of",
    "start": "1433590",
    "end": "1442910"
  },
  {
    "text": "step through so if you're going steps one to a hundred we step through serially for one through",
    "start": "1442910",
    "end": "1449520"
  },
  {
    "text": "and then step six invokes three different parallel paths and then it on",
    "start": "1449520",
    "end": "1455730"
  },
  {
    "text": "step seven all of those have to complete and it going through this this workflow",
    "start": "1455730",
    "end": "1461130"
  },
  {
    "text": "engine so a simple workflow is able to kick off these different components here",
    "start": "1461130",
    "end": "1468590"
  },
  {
    "text": "for instance a sequel component we needed to make changes to source or",
    "start": "1468590",
    "end": "1474240"
  },
  {
    "text": "target databases throughout the migration so to ensure that did all of",
    "start": "1474240",
    "end": "1479670"
  },
  {
    "text": "our that we don't have any more data in our source database we can have our application teams turn off their",
    "start": "1479670",
    "end": "1486150"
  },
  {
    "text": "applications but to be additionally sure for instance renaming tables we needed",
    "start": "1486150",
    "end": "1491460"
  },
  {
    "text": "automation to go and execute a sequel script so this automation the sequel",
    "start": "1491460",
    "end": "1497670"
  },
  {
    "text": "component is going to execute various different the sequel script and and a dozen other ones so these components are",
    "start": "1497670",
    "end": "1504840"
  },
  {
    "text": "kind of the you know the the what to do and and you know the sequel script is how to do it as well as the Java",
    "start": "1504840",
    "end": "1512280"
  },
  {
    "text": "components as how our service teams integrated with this and and interacted",
    "start": "1512280",
    "end": "1519360"
  },
  {
    "text": "with their services or DMS components DMS component was able to interface with",
    "start": "1519360",
    "end": "1527940"
  },
  {
    "text": "the database migration service and so forth so what like I said what we ended",
    "start": "1527940",
    "end": "1536310"
  },
  {
    "text": "up with is essentially being able to have a DevOps operator start multiple",
    "start": "1536310",
    "end": "1541560"
  },
  {
    "text": "workflows and and do this and well like database teams and software teams were",
    "start": "1541560",
    "end": "1547670"
  },
  {
    "text": "available and on-call as our migrations got started it was really it was really",
    "start": "1547670",
    "end": "1553710"
  },
  {
    "text": "automated and DevOps was able to perform all of these migrations 20 plus a day as",
    "start": "1553710",
    "end": "1559770"
  },
  {
    "text": "we went along",
    "start": "1559770",
    "end": "1562790"
  },
  {
    "text": "so after after performing migrations you",
    "start": "1566090",
    "end": "1571980"
  },
  {
    "start": "1568000",
    "end": "1568000"
  },
  {
    "text": "know want to share some of the lessons that we learned along the way",
    "start": "1571980",
    "end": "1577790"
  },
  {
    "text": "having schemas that were quite old not all of our tables had primary unique",
    "start": "1577790",
    "end": "1584880"
  },
  {
    "text": "keys probably 98% of them did but but not all of them and we tried for a while",
    "start": "1584880",
    "end": "1591210"
  },
  {
    "text": "to kind of using the database migration service try to work around that in",
    "start": "1591210",
    "end": "1596610"
  },
  {
    "text": "different ways and it was really just fighting an uphill battle so we ended up going and doing kind of the right thing in adding",
    "start": "1596610",
    "end": "1602970"
  },
  {
    "text": "PK's uk's to the tables that didn't have it it's certainly required for for lobs",
    "start": "1602970",
    "end": "1608550"
  },
  {
    "text": "in as well as for data validation so you know doing this again looking at some of",
    "start": "1608550",
    "end": "1615690"
  },
  {
    "text": "the deficiencies in that we ran into with DMS we're really just kind of our poor schema design over over the years",
    "start": "1615690",
    "end": "1623960"
  },
  {
    "text": "identifying lob tables and DMS in particular treats lobs differently in",
    "start": "1624650",
    "end": "1631800"
  },
  {
    "text": "limited lob mode versus full lob mode and and now inline lob mode is in",
    "start": "1631800",
    "end": "1639180"
  },
  {
    "text": "limited lob mode you can tell the database or the the DMS agent how large",
    "start": "1639180",
    "end": "1646110"
  },
  {
    "text": "that lob is going to be and it can effectively do do this much faster than in full lob mode it has no idea how big",
    "start": "1646110",
    "end": "1652860"
  },
  {
    "text": "this lob is so it has to transfer them one by one in line lob boat is a blend",
    "start": "1652860",
    "end": "1660420"
  },
  {
    "text": "of of both of those and was recently released by the DMS team one thing that",
    "start": "1660420",
    "end": "1669360"
  },
  {
    "text": "we found very quickly was using large replication instances really helped us",
    "start": "1669360",
    "end": "1675000"
  },
  {
    "text": "out quite a bit in the in the replication so like an R for instance class series and it really was minimally",
    "start": "1675000",
    "end": "1683010"
  },
  {
    "text": "a very minimal cost yeah please",
    "start": "1683010",
    "end": "1690539"
  },
  {
    "text": "that's right yes yes",
    "start": "1690539",
    "end": "1697190"
  },
  {
    "text": "so a yes I'm sorry so she's asking about",
    "start": "1703070",
    "end": "1709219"
  },
  {
    "text": "when I previous previously mentioned that that global uniqueness on on a",
    "start": "1709219",
    "end": "1714469"
  },
  {
    "text": "partition table wasn't possible in Postgres and now I'm showing here probably kind of confusingly that a",
    "start": "1714469",
    "end": "1720539"
  },
  {
    "text": "primary key is required for some source tables here so I in the source database",
    "start": "1720539",
    "end": "1727440"
  },
  {
    "text": "we did have global uniqueness for our source table Oracle does provide that in Postgres I still have primary keys and",
    "start": "1727440",
    "end": "1735539"
  },
  {
    "text": "unique keys but they are at a at a local partition level so if you're relying on",
    "start": "1735539",
    "end": "1742799"
  },
  {
    "text": "the database to to ensure uniqueness globally if you're making an insert at",
    "start": "1742799",
    "end": "1748279"
  },
  {
    "text": "11:59 p.m. and then you have a you know daily range based partition in in Oracle",
    "start": "1748279",
    "end": "1755759"
  },
  {
    "text": "if you inserted it at 12:01 that's still a violation you know if you have a global a global view of your entire",
    "start": "1755759",
    "end": "1763369"
  },
  {
    "text": "table as in Postgres that would just be double inserted it I'm sorry good yeah",
    "start": "1763369",
    "end": "1779639"
  },
  {
    "text": "that's correct in the source database you require primary key but it doesn't have to be global it could be local as",
    "start": "1779639",
    "end": "1786749"
  },
  {
    "text": "well",
    "start": "1786749",
    "end": "1788929"
  },
  {
    "text": "yeah you can do it without it on the",
    "start": "1810960",
    "end": "1820149"
  },
  {
    "text": "source as well but one of the pieces that you miss out on is data validation",
    "start": "1820149",
    "end": "1825299"
  },
  {
    "text": "and for lobs you certainly can't you do need a primary key or unique key so",
    "start": "1825299",
    "end": "1836500"
  },
  {
    "text": "monitoring monitoring our replication instances you know DMS does make it",
    "start": "1836500",
    "end": "1842950"
  },
  {
    "text": "fairly easy to use cloud watch metrics and alarms to ensure that the",
    "start": "1842950",
    "end": "1848080"
  },
  {
    "text": "availability the instance or that the replication lag and things like that",
    "start": "1848080",
    "end": "1854429"
  },
  {
    "text": "one of the things we also did was use a heartbeat table for instance which is a very simple kind of per DMS task so if",
    "start": "1855850",
    "end": "1863140"
  },
  {
    "text": "you have a source database and have a source table you're basically inserting kind of a dummy record with a timestamp",
    "start": "1863140",
    "end": "1869200"
  },
  {
    "text": "which gets then replicated over to the target side and one thing that that",
    "start": "1869200",
    "end": "1874779"
  },
  {
    "text": "gives you is to pretty easily check how latent that that DMS task is but also",
    "start": "1874779",
    "end": "1880830"
  },
  {
    "text": "when you're doing your your cut over you can basically ensure that once you",
    "start": "1880830",
    "end": "1886330"
  },
  {
    "text": "stopped the stopped writes on your on your source side that now that timestamp",
    "start": "1886330",
    "end": "1892419"
  },
  {
    "text": "that you've stopped the writes and you have this heartbeat happening as long as",
    "start": "1892419",
    "end": "1897789"
  },
  {
    "text": "the heartbeat timestamp is greater than when you stopped your your writes on",
    "start": "1897789",
    "end": "1903909"
  },
  {
    "text": "your source you know that you have every piece of data replicated over it's just kind of a quick and dirty way to also",
    "start": "1903909",
    "end": "1909700"
  },
  {
    "text": "validate that",
    "start": "1909700",
    "end": "1912360"
  },
  {
    "text": "our applications have historically used a number of triggers to populate created",
    "start": "1916800",
    "end": "1925240"
  },
  {
    "text": "by or last updated in some cases columns and to do that just need to be ensure",
    "start": "1925240",
    "end": "1931150"
  },
  {
    "text": "that you are enabling those after your Postgres replication is stopped otherwise you won't be able to validate",
    "start": "1931150",
    "end": "1937000"
  },
  {
    "text": "the data properly other migration",
    "start": "1937000",
    "end": "1944190"
  },
  {
    "start": "1942000",
    "end": "1942000"
  },
  {
    "text": "lessons learned we took a look at our data data lifecycle retention and as I",
    "start": "1944190",
    "end": "1951130"
  },
  {
    "text": "mentioned in partition tables in one case we having these conversations of if",
    "start": "1951130",
    "end": "1958690"
  },
  {
    "text": "we had six months of data retention do we really need that and it provided a",
    "start": "1958690",
    "end": "1963790"
  },
  {
    "text": "lot of benefits because sometimes people forget why you need six months of data and re-evaluating that a lot of cases we",
    "start": "1963790",
    "end": "1970810"
  },
  {
    "text": "found that we really only needed one month of data so for the migration it's much easier to migrate a month worth of",
    "start": "1970810",
    "end": "1978310"
  },
  {
    "text": "data than six but also on the once we migrated to Aurora Postgres now you're",
    "start": "1978310",
    "end": "1984970"
  },
  {
    "text": "paying for storage costs so if you don't if you migrate the data you really have",
    "start": "1984970",
    "end": "1990580"
  },
  {
    "text": "no way once you drop the data and drop it down to to one month you also don't",
    "start": "1990580",
    "end": "1995980"
  },
  {
    "text": "have a very good way to reduce that retention and actually reclaim the space in the Postgres database so reducing the",
    "start": "1995980",
    "end": "2003450"
  },
  {
    "text": "retention and having those conversations prior to migration was was was critical",
    "start": "2003450",
    "end": "2008670"
  },
  {
    "text": "to keeping our database very small and lean what we did migrate we also kind of",
    "start": "2008670",
    "end": "2017310"
  },
  {
    "text": "took the opportunity to add instrumentation of our sequel statements so for instance adding comments as to",
    "start": "2017310",
    "end": "2023760"
  },
  {
    "text": "which API is these calls are coming from and that's been very helpful for",
    "start": "2023760",
    "end": "2029250"
  },
  {
    "text": "troubleshooting we don't have to spend a lot of time trying to figure out with our application service owners which API",
    "start": "2029250",
    "end": "2035670"
  },
  {
    "text": "is making these these sequel calls and so that's been that's been extremely helpful especially as we're moving to in",
    "start": "2035670",
    "end": "2042630"
  },
  {
    "text": "a new database engine and we're talking with our service teams quite a bit so",
    "start": "2042630",
    "end": "2047640"
  },
  {
    "text": "having that instrumentation in having that telemetry into the origin",
    "start": "2047640",
    "end": "2052830"
  },
  {
    "text": "of the sequel was very helpful and it is it's allowed us to go into code repositories and really identify where",
    "start": "2052830",
    "end": "2060060"
  },
  {
    "text": "that call is coming from and have a better conversation with our software developers sequences in Oracle and",
    "start": "2060060",
    "end": "2069179"
  },
  {
    "text": "Postgres are are not the same while they their Oracle uses a global",
    "start": "2069179",
    "end": "2075600"
  },
  {
    "text": "sequence cache where at a session level it occurs in Postgres some of our",
    "start": "2075600",
    "end": "2081388"
  },
  {
    "text": "applications it actually made a difference in while they didn't need anything like a like a monatomic Li",
    "start": "2081389",
    "end": "2087628"
  },
  {
    "text": "increasing sequence but it did impact when you have when we had various",
    "start": "2087629",
    "end": "2093840"
  },
  {
    "text": "different sessions and there were very large gaps in in the data which again",
    "start": "2093840",
    "end": "2099840"
  },
  {
    "text": "probably you wouldn't design that way but legacy applications we had that was that was the issue we did a lot of",
    "start": "2099840",
    "end": "2106410"
  },
  {
    "text": "testing and just setting the cache value to one so essentially no caching we",
    "start": "2106410",
    "end": "2112830"
  },
  {
    "text": "actually didn't see any any dip or any performance impact from that this is a",
    "start": "2112830",
    "end": "2119220"
  },
  {
    "text": "very these are very OLTP like applications and so batch batch",
    "start": "2119220",
    "end": "2125490"
  },
  {
    "text": "workloads your mileage probably will vary the use of verify full or SSL and",
    "start": "2125490",
    "end": "2135350"
  },
  {
    "text": "no again is is treated differently and empty string and null are treated the",
    "start": "2135350",
    "end": "2140609"
  },
  {
    "text": "same in Oracle whereas in Postgres it's you know an empty string is truly stored as an empty string so if you have you",
    "start": "2140609",
    "end": "2147960"
  },
  {
    "text": "know applications that that that matters that it's something to consider did you have a question as I mentioned our",
    "start": "2147960",
    "end": "2160890"
  },
  {
    "text": "applications had a number of triggers and so having exception blocks in our triggers was was consuming transaction",
    "start": "2160890",
    "end": "2168869"
  },
  {
    "text": "IDs that we found pretty quickly and even when even when the exception was",
    "start": "2168869",
    "end": "2176010"
  },
  {
    "text": "not raised so burning through transaction IDs obviously isn't something that we wanted to do and the",
    "start": "2176010",
    "end": "2181770"
  },
  {
    "text": "fix pretty easy to just change change the exception block to raise a notice or an",
    "start": "2181770",
    "end": "2187550"
  },
  {
    "text": "error but certainly it's something to to take a look at especially if you have",
    "start": "2187550",
    "end": "2192560"
  },
  {
    "text": "triggers that are called so in",
    "start": "2192560",
    "end": "2199070"
  },
  {
    "text": "difference in in engines you know Oracle if you have a uncommitted transaction we had different",
    "start": "2199070",
    "end": "2206869"
  },
  {
    "text": "sniper jobs that may look at if there are sessions waiting and kill those for",
    "start": "2206869",
    "end": "2212210"
  },
  {
    "text": "various reasons you know how long how long I've been waiting or how many sessions it may be blocking but if if in",
    "start": "2212210",
    "end": "2220340"
  },
  {
    "text": "Oracle you have a uncommitted transaction and nobody's really waiting for it it's also really not that much of",
    "start": "2220340",
    "end": "2226190"
  },
  {
    "text": "a problem in Oracle but in Postgres obviously it can it can block did tuple",
    "start": "2226190",
    "end": "2231470"
  },
  {
    "text": "cleanup and multi transaction cleanup so I would strongly consider looking at the",
    "start": "2231470",
    "end": "2237170"
  },
  {
    "text": "idle in transaction session timeout and having some conversations with",
    "start": "2237170",
    "end": "2242210"
  },
  {
    "text": "application teams we we honestly didn't realize very much that some applications",
    "start": "2242210",
    "end": "2247550"
  },
  {
    "text": "just kind of left transactions open for various reasons for a very long time that was truly unnecessary until we",
    "start": "2247550",
    "end": "2254900"
  },
  {
    "text": "started seeing this and you know moving over to post Chris where it doesn't matter quite a bit more and then not",
    "start": "2254900",
    "end": "2264560"
  },
  {
    "text": "having global temporary tables was not a showstopper but in some cases instead of",
    "start": "2264560",
    "end": "2270560"
  },
  {
    "text": "having a local temporary table we opted to make sequel changes to not have a lot",
    "start": "2270560",
    "end": "2277280"
  },
  {
    "text": "of creating drops happening all the time with a per session at a per session",
    "start": "2277280",
    "end": "2282350"
  },
  {
    "text": "level",
    "start": "2282350",
    "end": "2284740"
  },
  {
    "text": "obviously auto vacuum is a is a huge topic and I think a colleague of mine",
    "start": "2289150",
    "end": "2295910"
  },
  {
    "text": "has a very good session on that later but one of the things to to certainly",
    "start": "2295910",
    "end": "2303589"
  },
  {
    "text": "look for an alarm for is is the is on vacuum transaction ID easing and cloud watch has a has a metric and you know",
    "start": "2303589",
    "end": "2310519"
  },
  {
    "text": "set alarms on that so add a minimum really should be watching that along",
    "start": "2310519",
    "end": "2315979"
  },
  {
    "text": "with a lot of other cloud watch monitoring lots of ways to monitor the",
    "start": "2315979",
    "end": "2322219"
  },
  {
    "text": "for unn for your on vacuum transaction IDs PG stat all tables in looking at",
    "start": "2322219",
    "end": "2328910"
  },
  {
    "text": "live and dead tuples in the last auto vacuum date and then just to kind of",
    "start": "2328910",
    "end": "2339709"
  },
  {
    "text": "throw out that you know are the auto vacuum settings really are just defaulting can't accommodate all of our",
    "start": "2339709",
    "end": "2345339"
  },
  {
    "text": "situations so taking a look at each per table is is very helpful and looking at",
    "start": "2345339",
    "end": "2353569"
  },
  {
    "text": "your type of table maybe the fill factor filling filling a page all the way to a",
    "start": "2353569",
    "end": "2358969"
  },
  {
    "text": "hundred percent might work really well for an insert only table but you probably really want to consider it",
    "start": "2358969",
    "end": "2364489"
  },
  {
    "text": "reconsider you know changing that if you have a very heavy DML table that does a lot of updates deletes and then once",
    "start": "2364489",
    "end": "2372890"
  },
  {
    "text": "moving on to or Postgres performance insights is a very helpful tool to get an idea into the weight system and",
    "start": "2372890",
    "end": "2380410"
  },
  {
    "text": "another colleague has another topic on that I've seen the him that he's",
    "start": "2380410",
    "end": "2386509"
  },
  {
    "text": "delivered that's a very good very good talk that I believe it's on Thursday or",
    "start": "2386509",
    "end": "2394309"
  },
  {
    "text": "20 on Thursday so performance insights really allows you to go and and dive",
    "start": "2394309",
    "end": "2399709"
  },
  {
    "text": "into the sequel level and to be able to dive deeper into and using weight events",
    "start": "2399709",
    "end": "2407979"
  },
  {
    "text": "recently added I'm really a fan of counter metrics as well allowing you to",
    "start": "2407979",
    "end": "2414170"
  },
  {
    "start": "2408000",
    "end": "2408000"
  },
  {
    "text": "get an idea of both database in database and OS metrics here and allow you to",
    "start": "2414170",
    "end": "2421640"
  },
  {
    "text": "take a look at what's been happening but you know performance insights is sampling at at one second level so you",
    "start": "2421640",
    "end": "2427370"
  },
  {
    "text": "can get very very granular as to what's happening at what time and for instance if you wanted to see transactions",
    "start": "2427370",
    "end": "2433430"
  },
  {
    "text": "committed you know or a dozen or two other metrics that it's very helpful",
    "start": "2433430",
    "end": "2441070"
  },
  {
    "text": "utilizing cloud watch dashboards and alarms they just you know you get these",
    "start": "2441070",
    "end": "2446480"
  },
  {
    "text": "they just come along with my you know going into RDS or or or Postgres very",
    "start": "2446480",
    "end": "2453620"
  },
  {
    "text": "helpful to just use them and after we performed our migrations we were",
    "start": "2453620",
    "end": "2459050"
  },
  {
    "text": "concerned about getting those migrations done but afterwards obviously making",
    "start": "2459050",
    "end": "2464720"
  },
  {
    "text": "sure that we were being frugal about things so AWS cost Explorer is very helpful to kind of slice and dice and",
    "start": "2464720",
    "end": "2469730"
  },
  {
    "text": "filter down what you're paying for you're paying for storage costs you're paying for backup retention costs you",
    "start": "2469730",
    "end": "2475670"
  },
  {
    "text": "paying for iOS in the database you know all the things that you do get charged for so to kind of just kind of sum some",
    "start": "2475670",
    "end": "2485840"
  },
  {
    "start": "2482000",
    "end": "2482000"
  },
  {
    "text": "up a lot of the performance benefits that we were able to get in were to be",
    "start": "2485840",
    "end": "2492380"
  },
  {
    "text": "able to support our query latency and no backup penalty to to take our backups",
    "start": "2492380",
    "end": "2500120"
  },
  {
    "text": "and scale to scale to our workloads being able to scale up and down instant",
    "start": "2500120",
    "end": "2507110"
  },
  {
    "text": "sizes is a big benefit that it takes minutes now instead of you know many hours or even days and to leverage",
    "start": "2507110",
    "end": "2514870"
  },
  {
    "text": "horizontal scaling for our reads as well the availability aspect of much faster",
    "start": "2514870",
    "end": "2523850"
  },
  {
    "text": "fail overs when we see approximately 30 seconds for a failover is is very helpful and and maybe even",
    "start": "2523850",
    "end": "2530330"
  },
  {
    "text": "more significant is being able to get back to a high availability state in minutes or even seconds but even in a",
    "start": "2530330",
    "end": "2538460"
  },
  {
    "text": "the worst case of a hardware replacement you know the AWS control plane will replace that instance in you know a",
    "start": "2538460",
    "end": "2544520"
  },
  {
    "text": "matter of minutes to get us back to a fully high availability state being able",
    "start": "2544520",
    "end": "2552200"
  },
  {
    "text": "to provision our hardware and not have to attend to using working with data center",
    "start": "2552200",
    "end": "2559260"
  },
  {
    "text": "technicians to try to get healthy hardware after a problem has happened for a RAID controller failure it's",
    "start": "2559260",
    "end": "2566910"
  },
  {
    "text": "really helped with our operational burden and being able to leverage all of",
    "start": "2566910",
    "end": "2572940"
  },
  {
    "text": "the API is and CLI is that AWS offers",
    "start": "2572940",
    "end": "2577950"
  },
  {
    "text": "has really helped us to perform our migrations and then add to what we can",
    "start": "2577950",
    "end": "2585360"
  },
  {
    "text": "do to manage a large enterprise enterprise fleet as we as we go forward",
    "start": "2585360",
    "end": "2590630"
  },
  {
    "text": "and there's no more Oracle licensing costs which is just nice so thank you",
    "start": "2590630",
    "end": "2599670"
  },
  {
    "text": "guys very much please let me know if you have any questions I can that can help with yes the question was how do you",
    "start": "2599670",
    "end": "2613500"
  },
  {
    "text": "migrate stored procedures so also using the schema conversion tool we didn't",
    "start": "2613500",
    "end": "2619020"
  },
  {
    "text": "have a lot it is much more complicated because you know it is complicated to",
    "start": "2619020",
    "end": "2625260"
  },
  {
    "text": "turn that into into Postgres PG it's",
    "start": "2625260",
    "end": "2630300"
  },
  {
    "text": "equal a lot of the the DDL that will be",
    "start": "2630300",
    "end": "2635640"
  },
  {
    "text": "produced even if it doesn't migrate you will get some active hints of to why",
    "start": "2635640",
    "end": "2641100"
  },
  {
    "text": "this particular procedure didn't migrate over one to one but definitely your mileage is going to be going to vary",
    "start": "2641100",
    "end": "2647910"
  },
  {
    "text": "there yes",
    "start": "2647910",
    "end": "2653150"
  },
  {
    "text": "sure so the question was do we have any legacy systems using oracle streams i know it does exist in other parts of",
    "start": "2661830",
    "end": "2667930"
  },
  {
    "text": "amazon not in our team we we didn't so i can't tell you how they addressed",
    "start": "2667930",
    "end": "2674770"
  },
  {
    "text": "address that but yeah i'm sorry so i",
    "start": "2674770",
    "end": "2697870"
  },
  {
    "text": "think probably the the man that you would like to talk to his right behind you there's Kevin he he knows exactly",
    "start": "2697870",
    "end": "2704290"
  },
  {
    "text": "what you're talking about",
    "start": "2704290",
    "end": "2707400"
  },
  {
    "text": "yes size-wise it was approximately about",
    "start": "2717120",
    "end": "2727300"
  },
  {
    "text": "eight or ten terabytes eight to ten so not humongous but sizable there was a",
    "start": "2727300",
    "end": "2734650"
  },
  {
    "text": "lot of cleanup that we were able to do when you know that was very helpful so are the size of our Postgres databases",
    "start": "2734650",
    "end": "2741550"
  },
  {
    "text": "is approximately four terabytes or so there was a lot of cleanup that we were",
    "start": "2741550",
    "end": "2746680"
  },
  {
    "text": "able to do yes",
    "start": "2746680",
    "end": "2751050"
  },
  {
    "text": "mm-hmm so I we on our migration we",
    "start": "2760140",
    "end": "2766030"
  },
  {
    "text": "haven't now it's it's become now that we're in AWS cloud I think that it is a lot easier to to kind of build systems",
    "start": "2766030",
    "end": "2773849"
  },
  {
    "text": "in that way now we we didn't have any we don't have we didn't have any migrations",
    "start": "2773849",
    "end": "2781150"
  },
  {
    "text": "that were that were needed for Kinesis or a redshift yes",
    "start": "2781150",
    "end": "2794380"
  },
  {
    "text": "so we do have data warehouse jobs and they pull from you know a reader fleet",
    "start": "2794380",
    "end": "2800230"
  },
  {
    "text": "and we continued some of those and we were able to a lot of our data was able",
    "start": "2800230",
    "end": "2806560"
  },
  {
    "text": "to be integrated in and pull from the service calls itself to be put in our data Lake and you know in and accessed",
    "start": "2806560",
    "end": "2814119"
  },
  {
    "text": "via that you know in whatever way is is necessary not directly accessing the",
    "start": "2814119",
    "end": "2819700"
  },
  {
    "text": "database it was it's not preferred for us to have these ETL jobs you know",
    "start": "2819700",
    "end": "2825849"
  },
  {
    "text": "querying the live database sure the",
    "start": "2825849",
    "end": "2846220"
  },
  {
    "text": "hardware picture was a they are they were direct attached storage and with",
    "start": "2846220",
    "end": "2854079"
  },
  {
    "text": "with SSDs and I want to say I have to",
    "start": "2854079",
    "end": "2860319"
  },
  {
    "text": "think back a little bit 200 and 128 CPUs",
    "start": "2860319",
    "end": "2865920"
  },
  {
    "text": "512 memory so they were they were sizable systems right now are our",
    "start": "2865920",
    "end": "2872050"
  },
  {
    "text": "largest instances are on our 4 16x large and a lot of those we've descaled quite",
    "start": "2872050",
    "end": "2879910"
  },
  {
    "text": "a bit because you know after after migrating after we become a little more",
    "start": "2879910",
    "end": "2885700"
  },
  {
    "text": "comfortable that we just over scaled a little bit so 16x large though yes are you talking about a",
    "start": "2885700",
    "end": "2904800"
  },
  {
    "text": "total project are you talking about like per per day per database migration sure",
    "start": "2904800",
    "end": "2913760"
  },
  {
    "text": "we as we started getting gaining confidence you know initially this we",
    "start": "2913760",
    "end": "2918960"
  },
  {
    "text": "had started data replication data replication is hard so you know making sure that we start the data replication",
    "start": "2918960",
    "end": "2924930"
  },
  {
    "text": "long ahead oh I need to wrap up here",
    "start": "2924930",
    "end": "2929990"
  },
  {
    "text": "approximately two weeks we would start data replication get it in sync and then",
    "start": "2929990",
    "end": "2937140"
  },
  {
    "text": "just pick a date to cut it over oh thank you guys very much appreciate it",
    "start": "2937140",
    "end": "2942750"
  },
  {
    "text": "[Applause]",
    "start": "2942750",
    "end": "2949260"
  }
]