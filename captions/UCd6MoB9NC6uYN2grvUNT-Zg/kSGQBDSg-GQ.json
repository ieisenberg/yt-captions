[
  {
    "start": "0",
    "end": "333000"
  },
  {
    "text": "so welcome to this session of how Expedia leverages Amazon DynamoDB and",
    "start": "30",
    "end": "7560"
  },
  {
    "text": "the DynamoDB accelerator technology that we released early this year so i'm widg",
    "start": "7560",
    "end": "14099"
  },
  {
    "text": "a nitrogen i'm a principal product manager in the Amazon DynamoDB team and",
    "start": "14099",
    "end": "19520"
  },
  {
    "text": "this is Brandon and thanks Brandon for coming over and presenting the session",
    "start": "19520",
    "end": "26279"
  },
  {
    "text": "as to how experience experience has been so I'll just you know set the stage in",
    "start": "26279",
    "end": "32189"
  },
  {
    "text": "terms of for folks who are not familiar with DynamoDB and Doc's I'll give you a very quick brief",
    "start": "32189",
    "end": "38520"
  },
  {
    "text": "overview and I'll hand it over to Brandon to talk about how their",
    "start": "38520",
    "end": "44489"
  },
  {
    "text": "real-time travel analytics application leverages our dynamically and darks on the business",
    "start": "44489",
    "end": "53190"
  },
  {
    "text": "and the technology benefits that the team realized so how many of you are",
    "start": "53190",
    "end": "59399"
  },
  {
    "text": "familiar with what dynamodb is show of hands ok about half of you",
    "start": "59399",
    "end": "67110"
  },
  {
    "text": "how many of you have DynamoDB in production today ok ok so for those of",
    "start": "67110",
    "end": "77549"
  },
  {
    "text": "you who have not familiar DynamoDB is a fully managed or no sequel cloud",
    "start": "77549",
    "end": "83850"
  },
  {
    "text": "database service that is essentially our promises single digit millisecond",
    "start": "83850",
    "end": "91530"
  },
  {
    "text": "latency at any scale by any scale I mean whether it is you know tables with few",
    "start": "91530",
    "end": "97530"
  },
  {
    "text": "items in them or tables with hundreds of terabytes of data in them right so the",
    "start": "97530",
    "end": "102960"
  },
  {
    "text": "scalability aspect is what primarily differentiates our dynamodb with other",
    "start": "102960",
    "end": "108329"
  },
  {
    "text": "offerings out there then I'm going to be supports our two data models it's a key value store as well as a document store",
    "start": "108329",
    "end": "116280"
  },
  {
    "text": "in JSON format going back to a scalability aspect all you need to do is",
    "start": "116280",
    "end": "123149"
  },
  {
    "text": "you just come into the you know ArrayList management console or through the API you create a dynamo DB table and",
    "start": "123149",
    "end": "130649"
  },
  {
    "text": "you turn on this thing called Auto scan that we released earlier this year where",
    "start": "130649",
    "end": "136650"
  },
  {
    "text": "you set the target utilization that you want for your table and dynamodb under",
    "start": "136650",
    "end": "142440"
  },
  {
    "text": "the hood essentially monitors all the consumption throughput that is coming in from your application and scales up or",
    "start": "142440",
    "end": "149730"
  },
  {
    "text": "down the protein capacity to ensure that you are getting the throughput that you need at the same time offering it in",
    "start": "149730",
    "end": "155940"
  },
  {
    "text": "that single-digit millisecond response times so that speaks with the fully",
    "start": "155940",
    "end": "161489"
  },
  {
    "text": "managed aspect of it further and with dynamodb you are not you know",
    "start": "161489",
    "end": "167340"
  },
  {
    "text": "approaching any hardware as within a cloud value proposition you're not thinking about software you're not thinking about patching you're not",
    "start": "167340",
    "end": "173760"
  },
  {
    "text": "thinking about maintenance operations none of that all you do is just create a table and you are good to go right then",
    "start": "173760",
    "end": "181260"
  },
  {
    "text": "would you be asked with any aw service is also highly available where we",
    "start": "181260",
    "end": "187769"
  },
  {
    "text": "maintain three copies and different AZ's in a region so you do not have to worry",
    "start": "187769",
    "end": "193590"
  },
  {
    "text": "about availability as well so it is designed for supporting any business",
    "start": "193590",
    "end": "198989"
  },
  {
    "text": "critical application that you might have switching over to our DynamoDB",
    "start": "198989",
    "end": "204540"
  },
  {
    "text": "accelerator essentially think of it as a in-memory caching layer on top of the",
    "start": "204540",
    "end": "210450"
  },
  {
    "text": "anima DB that is 100% compatible with DynamoDB it has all the characteristics",
    "start": "210450",
    "end": "216690"
  },
  {
    "text": "that we talked about in terms of scalability where you can scale out as amateurs or ten nodes in the dax cluster",
    "start": "216690",
    "end": "222510"
  },
  {
    "text": "it has a it is a fault tolerant in with inbuilt fault tolerance as well as it is",
    "start": "222510",
    "end": "230069"
  },
  {
    "text": "of you know 100% API compatible with dynamodb so from an application perspective if you have an existing",
    "start": "230069",
    "end": "237299"
  },
  {
    "text": "application that you have against dynamodb you make basically three lines of code change to make it talk to Docs",
    "start": "237299",
    "end": "244349"
  },
  {
    "text": "and that's pretty much you know figures it all out the caching our technology itself is based on you know our last",
    "start": "244349",
    "end": "250590"
  },
  {
    "text": "recently used so the more you know when you access a data or an item from the",
    "start": "250590",
    "end": "256349"
  },
  {
    "text": "back-end DynamoDB danam would be the accelerator automatically caches the data when you are you know writing to in",
    "start": "256349",
    "end": "265110"
  },
  {
    "text": "darts it automatically you know welcome back to the dynamodb behind you can have a duck's cluster map-21",
    "start": "265110",
    "end": "271890"
  },
  {
    "text": "dynamodb table or you can map it to multiple dynamodb tables it's up to you",
    "start": "271890",
    "end": "277540"
  },
  {
    "text": "you would find us as with the you know dynamodb over the past several years",
    "start": "277540",
    "end": "283620"
  },
  {
    "text": "continue to expand on you know DynamoDB accelerator as well where essentially",
    "start": "283620",
    "end": "289660"
  },
  {
    "text": "from an offering standpoint they would be fairly indistinguishable in terms of you know regional availability sdk",
    "start": "289660",
    "end": "297310"
  },
  {
    "text": "availability and so forth what Expedia has done is essentially leveraged as a",
    "start": "297310",
    "end": "304530"
  },
  {
    "text": "preview participant when we before we released that's GA for the real-time analytics",
    "start": "304530",
    "end": "312790"
  },
  {
    "text": "app so what Brandon is gonna talk through is their journey in for their",
    "start": "312790",
    "end": "319180"
  },
  {
    "text": "streaming application how they went through the different requirements and netted out without making a choice of",
    "start": "319180",
    "end": "326140"
  },
  {
    "text": "DynamoDB and ducts so with that I will turn over the floor to Brandon thank you",
    "start": "326140",
    "end": "332020"
  },
  {
    "text": "alright thanks BJ everyone I'm Brandon O'Brien principal software engineer at Expedia",
    "start": "332020",
    "end": "338169"
  },
  {
    "start": "333000",
    "end": "448000"
  },
  {
    "text": "Inc and I work on streaming data systems and that means I take systems like spark",
    "start": "338169",
    "end": "345070"
  },
  {
    "text": "and storm and acha and use that to build compute clusters to process a lot of",
    "start": "345070",
    "end": "350800"
  },
  {
    "text": "data really fast and the use case I use that for is real-time travel analytics",
    "start": "350800",
    "end": "356380"
  },
  {
    "text": "and you know with that I want to talk",
    "start": "356380",
    "end": "361840"
  },
  {
    "text": "about how DynamoDB and Dax have fit into that architecture and more than just",
    "start": "361840",
    "end": "368080"
  },
  {
    "text": "talking about dynamodb and Dax I want to talk about the evolution of the approaches that we tried in our",
    "start": "368080",
    "end": "374919"
  },
  {
    "text": "architecture with different streaming systems that led us to look at dynamodb and acts as the solution for something",
    "start": "374919",
    "end": "382030"
  },
  {
    "text": "called reference data in streaming systems and reference data is simply any data that the streaming processor needs",
    "start": "382030",
    "end": "388570"
  },
  {
    "text": "to implement the business logic that it doesn't get directly from the data stream so my hope is that by sharing",
    "start": "388570",
    "end": "396729"
  },
  {
    "text": "this information everyone here will be able to take home a little bit of of information that they",
    "start": "396729",
    "end": "402630"
  },
  {
    "text": "can use for their work so before I start the presentation let me just do a quick icebreaker so raise your hand if you're",
    "start": "402630",
    "end": "409590"
  },
  {
    "text": "from the West Coast where's your hand if you're from the east coast raise your",
    "start": "409590",
    "end": "416970"
  },
  {
    "text": "hand if you're from outside the US raise your hand if you think Bitcoin at ten",
    "start": "416970",
    "end": "424560"
  },
  {
    "text": "thousand is a bubble raise your hand if you want to take bets on that okay okay",
    "start": "424560",
    "end": "429900"
  },
  {
    "text": "just just just kidding okay now seriously so who has who has",
    "start": "429900",
    "end": "435180"
  },
  {
    "text": "built a streaming system before and deploy to production okay who has not",
    "start": "435180",
    "end": "440970"
  },
  {
    "text": "built one but is looking at building one now okay great well let's get into it",
    "start": "440970",
    "end": "448490"
  },
  {
    "start": "448000",
    "end": "489000"
  },
  {
    "text": "so first I want to give some context for what we do at Expedia what travel",
    "start": "448490",
    "end": "454050"
  },
  {
    "text": "analytics look like so that you have some business context for the problem that that my team is looking at solving",
    "start": "454050",
    "end": "460470"
  },
  {
    "text": "and then we're gonna look at patterns for reference data and this is the",
    "start": "460470",
    "end": "466380"
  },
  {
    "text": "patterns to make that reference data accessible to the streaming processor and that that'll be the main part of the presentation and then we'll end with how",
    "start": "466380",
    "end": "473580"
  },
  {
    "text": "we've started to use dynamodb and Dax to provide that reference data in a way that provides scalability and",
    "start": "473580",
    "end": "479820"
  },
  {
    "text": "performance and in a way that has lower operational overhead and that's been a",
    "start": "479820",
    "end": "484830"
  },
  {
    "text": "big reducer for Hague's on my team so first off so for anyone who's not",
    "start": "484830",
    "end": "492090"
  },
  {
    "start": "489000",
    "end": "537000"
  },
  {
    "text": "familiar Expedia is a two-sided marketplace it's multiple products multiple brands and it's a global travel",
    "start": "492090",
    "end": "499410"
  },
  {
    "text": "system so on the shopping side we have multiple websites multiple brands Expedia comm hotels.com home away for",
    "start": "499410",
    "end": "508320"
  },
  {
    "text": "vacation rentals and many others on the supply side there's all kinds of supply systems that we wire into different GDS",
    "start": "508320",
    "end": "516060"
  },
  {
    "text": "--is for lodging and on the flights as well many as a huge network of supply",
    "start": "516060",
    "end": "523860"
  },
  {
    "text": "integration and so for someone who's looking at travel analytics and data what you actually have is two very",
    "start": "523860",
    "end": "531030"
  },
  {
    "text": "massive data streams from the supply side and from the shopper side to give you an idea of the",
    "start": "531030",
    "end": "539620"
  },
  {
    "start": "537000",
    "end": "570000"
  },
  {
    "text": "kind of scale that we're operating at so we have about 500,000 properties for",
    "start": "539620",
    "end": "544829"
  },
  {
    "text": "traditional lodging lodging properties for vacation rentals at home away it's",
    "start": "544829",
    "end": "550390"
  },
  {
    "text": "about 1.5 million and then you can snapshot this but for the purpose of the presentation really 600 million site",
    "start": "550390",
    "end": "557440"
  },
  {
    "text": "visits per month across all sites will give you an idea of the scale that the streaming processors that I'll be",
    "start": "557440",
    "end": "563410"
  },
  {
    "text": "talking about will be operating out because the main focus for my systems is essentially user behavior analysis so",
    "start": "563410",
    "end": "571660"
  },
  {
    "start": "570000",
    "end": "684000"
  },
  {
    "text": "for the class of travel analytics a big one is creating a live view of demand",
    "start": "571660",
    "end": "577570"
  },
  {
    "text": "patterns so that's you have shoppers coming to Expedia and they're doing Hotel searches they're doing flight",
    "start": "577570",
    "end": "582970"
  },
  {
    "text": "searches they're doing bookings and all of that data ends up in Kafka as",
    "start": "582970",
    "end": "588760"
  },
  {
    "text": "messages and so you have this massive firehose of rich user behavior data and",
    "start": "588760",
    "end": "593950"
  },
  {
    "text": "that's where my systems plug into it and a classic use case for that is just",
    "start": "593950",
    "end": "599019"
  },
  {
    "text": "looking at demand patterns and giving that to hotel suppliers in an anonymized way so that they can see okay what dates",
    "start": "599019",
    "end": "606010"
  },
  {
    "text": "are popular in my market and you know this weekend versus no next weekend and",
    "start": "606010",
    "end": "613170"
  },
  {
    "text": "providing that information in a real-time way so that the lodging supplier has a real-time view of that so",
    "start": "613170",
    "end": "619420"
  },
  {
    "text": "they can make decisions quickly on how do they want to position themselves in the market then for shoppers here's the",
    "start": "619420",
    "end": "628029"
  },
  {
    "text": "example below so we have the cross hot price they're essentially to compute",
    "start": "628029",
    "end": "633790"
  },
  {
    "text": "that price we want to look at real time price prices as they flow those through the system and then create an aggregate price in real time and the challenge",
    "start": "633790",
    "end": "640959"
  },
  {
    "text": "there is that the prices that go into that aggregate figure are always are always moving as hotels are changing",
    "start": "640959",
    "end": "647800"
  },
  {
    "text": "prices and selling out so we need a real-time system to compute that and make it available for display on our",
    "start": "647800",
    "end": "652810"
  },
  {
    "text": "brand web sites like Expedia comm and then additionally creating live views of",
    "start": "652810",
    "end": "658209"
  },
  {
    "text": "the market for internal business strategy and operations teams so those are essentially the you know the class",
    "start": "658209",
    "end": "664870"
  },
  {
    "text": "of use cases that we're looking at now to tie it back to where dynamodb and",
    "start": "664870",
    "end": "670269"
  },
  {
    "text": "Dax will fit in in in my work with these travel analytics the the key feature",
    "start": "670269",
    "end": "676509"
  },
  {
    "text": "that has really exploded the possible space of features we can implement is getting access to reference data and",
    "start": "676509",
    "end": "685529"
  },
  {
    "start": "684000",
    "end": "780000"
  },
  {
    "text": "conceptually that's that's a very simple thing it's say you have a hotel stream",
    "start": "685529",
    "end": "690550"
  },
  {
    "text": "and it has keys for the hotels and you want to look up information about the hotels that's not available on the",
    "start": "690550",
    "end": "696100"
  },
  {
    "text": "stream and that could be something like the star rating for the hotel so that you could compute a real time histogram",
    "start": "696100",
    "end": "701649"
  },
  {
    "text": "of searches by star rating to use that for whatever business case you want and while conceptually simple in reality",
    "start": "701649",
    "end": "709059"
  },
  {
    "text": "in production at scale you run into all kinds of challenges like data availability and scalability and",
    "start": "709059",
    "end": "716730"
  },
  {
    "text": "mutability of the data set where these data sets are not static over time and",
    "start": "716730",
    "end": "722259"
  },
  {
    "text": "so if you were to just load say a static lookup file into your streaming processor that will quickly go out of",
    "start": "722259",
    "end": "727990"
  },
  {
    "text": "date and as of right now for some of the streaming processors I've been working on I have one that's been running in",
    "start": "727990",
    "end": "733930"
  },
  {
    "text": "production for coming up on three years now and I can tell you that the the reference data it needs has constantly",
    "start": "733930",
    "end": "740199"
  },
  {
    "text": "mutated over time and so we need a way to be able to handle that elegantly and",
    "start": "740199",
    "end": "745269"
  },
  {
    "text": "additionally typically when you're building a streaming system you're not just building a single streaming system",
    "start": "745269",
    "end": "751779"
  },
  {
    "text": "you end up building an entire ecosystem of streaming systems of streaming processors to handle different use cases",
    "start": "751779",
    "end": "757449"
  },
  {
    "text": "and I started using this term called reference domain layer to talk about the",
    "start": "757449",
    "end": "763899"
  },
  {
    "text": "platform that provides that the data access to the reference data in a way that's reusable so when you have an",
    "start": "763899",
    "end": "770259"
  },
  {
    "text": "ecosystem of streaming processors that they can all leverage they can leverage access to that reference data and that's",
    "start": "770259",
    "end": "777730"
  },
  {
    "text": "that's where dynamodb and Dax will fit in so for the reference data itself this",
    "start": "777730",
    "end": "783009"
  },
  {
    "start": "780000",
    "end": "868000"
  },
  {
    "text": "will fit into performance wherever I look at least in the data that I look at",
    "start": "783009",
    "end": "788589"
  },
  {
    "text": "it always follows this power-law distribution sorry was that microphone not actually on before we're good okay",
    "start": "788589",
    "end": "796920"
  },
  {
    "text": "it always it always follows this purrito distribution and that's exactly what it",
    "start": "796920",
    "end": "803320"
  },
  {
    "text": "looks like in this case for example this is hotel searches by city and so you",
    "start": "803320",
    "end": "809620"
  },
  {
    "text": "have the vegas --is and you have the the Paris's and Tokyo's of the world and",
    "start": "809620",
    "end": "815050"
  },
  {
    "text": "they get orders of magnitude more searches more action than somewhere in",
    "start": "815050",
    "end": "820450"
  },
  {
    "text": "save rural Nebraska and what this lends itself to is a high cash ability so if",
    "start": "820450",
    "end": "826240"
  },
  {
    "text": "you have a very non-uniform access pattern to the data you can exploit that to keep keep recently accessed values in",
    "start": "826240",
    "end": "832630"
  },
  {
    "text": "some place in memory which will work with tax later and reference datasets",
    "start": "832630",
    "end": "839110"
  },
  {
    "text": "common ones things like travel dates hotels flight routes things like that",
    "start": "839110",
    "end": "844560"
  },
  {
    "text": "while we have a high cache hit rate the datasets can often be very high",
    "start": "844560",
    "end": "849820"
  },
  {
    "text": "cardinality very large data sets and so this sample of data that we see for",
    "start": "849820",
    "end": "855490"
  },
  {
    "text": "example it actually spends extends about two to three times out to the right and so if you were to just cache all of this",
    "start": "855490",
    "end": "862720"
  },
  {
    "text": "data it could become prohibitively expensive to do that so here's the",
    "start": "862720",
    "end": "870699"
  },
  {
    "start": "868000",
    "end": "924000"
  },
  {
    "text": "here's the grading system that I've started using in looking at access to reference data and I've convinced myself",
    "start": "870699",
    "end": "877930"
  },
  {
    "text": "that when a streaming processor gets to a sufficient amount of complexity when you're trying to handle very sophisticated use cases you will have to",
    "start": "877930",
    "end": "884709"
  },
  {
    "text": "think about all of these things for access to the reference data so you have performance things like scalability",
    "start": "884709",
    "end": "890760"
  },
  {
    "text": "throughput you have data management so is the data safe is it durable handling",
    "start": "890760",
    "end": "898000"
  },
  {
    "text": "the mutability over time and being able to scale the storage without having to change the architecture and then things",
    "start": "898000",
    "end": "904390"
  },
  {
    "text": "about the wider system so reusability of that data from multiple systems is very",
    "start": "904390",
    "end": "910779"
  },
  {
    "text": "key and then also operational overhead and simplicity and flexibility so one",
    "start": "910779",
    "end": "918519"
  },
  {
    "text": "thing I definitely want to avoid is a very high performance high scale a Rube Goldberg machine so let me put those",
    "start": "918519",
    "end": "925839"
  },
  {
    "start": "924000",
    "end": "942000"
  },
  {
    "text": "over there and so so here's the journey of different things that we've actually tried over time",
    "start": "925839",
    "end": "932590"
  },
  {
    "text": "before giving to dynamodb and acts and so we'll walk through these and I'll talk about some of the pros and cons",
    "start": "932590",
    "end": "937880"
  },
  {
    "text": "that we've had dealing with these so",
    "start": "937880",
    "end": "943250"
  },
  {
    "start": "942000",
    "end": "995000"
  },
  {
    "text": "first off keeping the streaming data keeping the reference data in memory and",
    "start": "943250",
    "end": "948620"
  },
  {
    "text": "keep and this is this is pretty much the fastest approach that that you can take",
    "start": "948620",
    "end": "954400"
  },
  {
    "text": "so this is essentially keeping the data directly in a hash map in memory in the streaming processor and this is this is",
    "start": "954400",
    "end": "961130"
  },
  {
    "text": "really kind of an ideal approach if you can fit all of your data in memory in",
    "start": "961130",
    "end": "966200"
  },
  {
    "text": "your streaming processor in the JVM that that works great but then you run into",
    "start": "966200",
    "end": "972100"
  },
  {
    "text": "some problems around durability so it only exists for the life cycle of this trimming processor you might run into",
    "start": "972100",
    "end": "978380"
  },
  {
    "text": "challenges of updating that over time and then obviously that's not very",
    "start": "978380",
    "end": "983720"
  },
  {
    "text": "reusable because it only exists in memory in the streaming processor so",
    "start": "983720",
    "end": "989480"
  },
  {
    "text": "it's a it's an approach that works very well but just for a small amounts of data set that don't mutate over time so",
    "start": "989480",
    "end": "996410"
  },
  {
    "start": "995000",
    "end": "1098000"
  },
  {
    "text": "then you might think okay well let me look at putting it on the streaming processor disk and I know this this has",
    "start": "996410",
    "end": "1002080"
  },
  {
    "text": "been a very popular approach for certain use cases and so this is essentially",
    "start": "1002080",
    "end": "1007900"
  },
  {
    "text": "creating some kind of key value store like rocks DB putting it on the streaming processor disk and then the",
    "start": "1007900",
    "end": "1016060"
  },
  {
    "text": "streaming processor can access it without having to do network i/o and for certain use cases that works well but",
    "start": "1016060",
    "end": "1021820"
  },
  {
    "text": "then you run into problems where you either have to keep a complete copy of",
    "start": "1021820",
    "end": "1026829"
  },
  {
    "text": "the reference data set on every streaming node or you'll have to do",
    "start": "1026830",
    "end": "1032199"
  },
  {
    "text": "network traffic you might be able to avoid that network traffic if your key partitioning strategy for your stream",
    "start": "1032200",
    "end": "1038020"
  },
  {
    "text": "can coincide with the key partitioning strategy for the data but then that locks you into a single use case and as",
    "start": "1038020",
    "end": "1043750"
  },
  {
    "text": "soon as you modify the partitioning strategy now you have taken that hit to",
    "start": "1043750",
    "end": "1050800"
  },
  {
    "text": "go across the network and at that point why complicate the streaming processor",
    "start": "1050800",
    "end": "1055860"
  },
  {
    "text": "and that complication of a streaming processor is really complicating the lifecycle of the streaming processor so",
    "start": "1055860",
    "end": "1061780"
  },
  {
    "text": "if you wanted to containerize your sparknotes for example now you have to load all that reference",
    "start": "1061780",
    "end": "1068650"
  },
  {
    "text": "data on to that that node let's say on ECS for example or kubernetes",
    "start": "1068650",
    "end": "1073870"
  },
  {
    "text": "and you have to load that after that node becomes available but before it's actually attached to the cluster for",
    "start": "1073870",
    "end": "1080490"
  },
  {
    "text": "taking messages and now it gets very complicated and personally one approach",
    "start": "1080490",
    "end": "1086290"
  },
  {
    "text": "that I'm interested in taking later is looking at doing server list streaming processing and obviously an approach",
    "start": "1086290",
    "end": "1092350"
  },
  {
    "text": "like this for accessing reference data it will work in that case so a direct",
    "start": "1092350",
    "end": "1099730"
  },
  {
    "start": "1098000",
    "end": "1162000"
  },
  {
    "text": "service call so this is an interesting approach so you might think okay well let me just not manage the reference",
    "start": "1099730",
    "end": "1106120"
  },
  {
    "text": "data myself I'll just go directly to that hotel service that has that data and you know presumably you have maximum",
    "start": "1106120",
    "end": "1112690"
  },
  {
    "text": "durability and scalable storage because",
    "start": "1112690",
    "end": "1117820"
  },
  {
    "text": "you're talking directly to the services responsible for managing that it's it's it's simple it's just a service call",
    "start": "1117820",
    "end": "1123540"
  },
  {
    "text": "operational overhead is low because someone else is managing it and reusability is high multiple streaming",
    "start": "1123540",
    "end": "1130750"
  },
  {
    "text": "processors can use it and it may or may not be fast you may have the availability it depends on the SLA for",
    "start": "1130750",
    "end": "1136360"
  },
  {
    "text": "that service but there's another issue and that's kind of a deal breaker for me",
    "start": "1136360",
    "end": "1142620"
  },
  {
    "text": "which is if you're adding contention to resources that were designed for the",
    "start": "1142620",
    "end": "1148450"
  },
  {
    "text": "users for your website that's really it's really a showstopper and don't tell",
    "start": "1148450",
    "end": "1155650"
  },
  {
    "text": "anyone but I actually had to have this conversation with my streaming processors long ago when I first got started so Redis Redis ElastiCache so",
    "start": "1155650",
    "end": "1167170"
  },
  {
    "start": "1162000",
    "end": "1293000"
  },
  {
    "text": "this is an approach that I love but it just doesn't quite work so this is",
    "start": "1167170",
    "end": "1173650"
  },
  {
    "text": "having a separate ElastiCache instance outside of your streaming processors so",
    "start": "1173650",
    "end": "1178780"
  },
  {
    "text": "you have multiple streaming processors each streaming processor has multiple nodes and the idea is then you have some",
    "start": "1178780",
    "end": "1184840"
  },
  {
    "text": "other application that's responsible for writing data into Redis and then all of your streaming processor nodes can just",
    "start": "1184840",
    "end": "1190840"
  },
  {
    "text": "pick up that data from Redis and they then they may or may not combine this with that approach as well of keeping it",
    "start": "1190840",
    "end": "1196630"
  },
  {
    "text": "in memory with some kind of of tactical short cash and this this",
    "start": "1196630",
    "end": "1202929"
  },
  {
    "text": "works reasonably well it's extremely fast because the data is all in memory and read us the access times are",
    "start": "1202929",
    "end": "1210000"
  },
  {
    "text": "extremely fast problems come in with durability and availability so if if so",
    "start": "1210000",
    "end": "1219929"
  },
  {
    "text": "obviously Redis is designed as as just a cache not an actual data store for",
    "start": "1219929",
    "end": "1225490"
  },
  {
    "text": "record so it this worked for me using this and it's not working well is kind",
    "start": "1225490",
    "end": "1230529"
  },
  {
    "text": "of my fault for going out of out of really the the realm of what Redis was designed for but a couple other things",
    "start": "1230529",
    "end": "1237850"
  },
  {
    "text": "happen which complicate this and this is at least as of 2.8 dot X the defaults",
    "start": "1237850",
    "end": "1243190"
  },
  {
    "text": "for Redis ElastiCache in AWS if you overload the cache with data I thought",
    "start": "1243190",
    "end": "1249220"
  },
  {
    "text": "with an LRU approach that the data would actually just drop off but it actually",
    "start": "1249220",
    "end": "1255370"
  },
  {
    "text": "will just crash Redis you will lose your data and you will not have a good time",
    "start": "1255370",
    "end": "1261279"
  },
  {
    "text": "maybe that specs does have 3.2 I haven't tested that yet and additionally we did",
    "start": "1261279",
    "end": "1267370"
  },
  {
    "text": "run into a case where Redis the Machine actually became unresponsive this is",
    "start": "1267370",
    "end": "1273100"
  },
  {
    "text": "with elastic cache and we actually had to kill that node and create a new one and so maybe if you use something like",
    "start": "1273100",
    "end": "1279659"
  },
  {
    "text": "Netflix is dynamite to create a Cassandra like ring of reticent senses you could avoid those kind of",
    "start": "1279659",
    "end": "1285250"
  },
  {
    "text": "availability problems so it's an approach worth worth exploring so at",
    "start": "1285250",
    "end": "1294909"
  },
  {
    "start": "1293000",
    "end": "1326000"
  },
  {
    "text": "this point in the journey so a quote comes to mind which is essentially",
    "start": "1294909",
    "end": "1300159"
  },
  {
    "text": "performance doesn't matter if your system isn't running so when you're",
    "start": "1300159",
    "end": "1305289"
  },
  {
    "text": "building a streaming system or when I'm building a steering system it really got to the point where we have to hand we",
    "start": "1305289",
    "end": "1312309"
  },
  {
    "text": "have to make sure the availability is there and that it's processing the data correctly get that a hundred percent and",
    "start": "1312309",
    "end": "1319299"
  },
  {
    "text": "then dialed back up performance so I started looking at more traditional databases we try to Cassandra and so",
    "start": "1319299",
    "end": "1327789"
  },
  {
    "start": "1326000",
    "end": "1405000"
  },
  {
    "text": "Cassandra meets more of the requirements obviously the performance is there the data management is there you have the",
    "start": "1327789",
    "end": "1334610"
  },
  {
    "text": "redundancy across different nodes you can specify the replication factor so",
    "start": "1334610",
    "end": "1340400"
  },
  {
    "text": "you can actually tune how safe the data is it's very mutable you just have whatever whatever system is managing",
    "start": "1340400",
    "end": "1347180"
  },
  {
    "text": "that reference data can just write the updates into Cassandra it's available from all streaming processors I think",
    "start": "1347180",
    "end": "1356030"
  },
  {
    "text": "it's then in terms of operational overhead and then actually simplicity and flexibility that that's where it",
    "start": "1356030",
    "end": "1362570"
  },
  {
    "text": "starts to break down so in my experience and this is more comment just about the",
    "start": "1362570",
    "end": "1367880"
  },
  {
    "text": "still set of my team rather than Cassandra the operational overhead became prohibitive we are small team we",
    "start": "1367880",
    "end": "1375110"
  },
  {
    "text": "had one guy who ended up spending about half of his time just keeping the whole thing running and the biggest piece of",
    "start": "1375110",
    "end": "1380630"
  },
  {
    "text": "that was Cassandra and so at that point I realized ok it gives us the performance and the availability and the",
    "start": "1380630",
    "end": "1387080"
  },
  {
    "text": "durability but now we're having to tell our product team that we really have to",
    "start": "1387080",
    "end": "1392270"
  },
  {
    "text": "slow down because we're spending so much time just keeping what we have running running and we can't build new features",
    "start": "1392270",
    "end": "1399320"
  },
  {
    "text": "so I thought ok we need something that is easier to run so I started looking at",
    "start": "1399320",
    "end": "1406940"
  },
  {
    "start": "1405000",
    "end": "1459000"
  },
  {
    "text": "traditional databases RDS my sequel and that's indeed far easier to run in my",
    "start": "1406940",
    "end": "1412730"
  },
  {
    "text": "experience it has all the durability and availability operational overhead is is",
    "start": "1412730",
    "end": "1418160"
  },
  {
    "text": "basically zero because it's a managed service so so I thinking ok this this this is working a lot better but for",
    "start": "1418160",
    "end": "1425500"
  },
  {
    "text": "higher scale high-performance streaming systems the the access to the data it",
    "start": "1425500",
    "end": "1432080"
  },
  {
    "text": "just wasn't quite fast enough so if we're looking at it's depending on the system and how high you scale you might",
    "start": "1432080",
    "end": "1437900"
  },
  {
    "text": "have like 50 milliseconds for a read time or maybe higher with the streaming",
    "start": "1437900",
    "end": "1445430"
  },
  {
    "text": "processor you really need to be like 10 milliseconds or less ideally on average",
    "start": "1445430",
    "end": "1450640"
  },
  {
    "text": "when you look at the data access time or else it's just really going to slow down the streaming process or when you're",
    "start": "1450640",
    "end": "1455750"
  },
  {
    "text": "operating a scale so I thought okay great let me just",
    "start": "1455750",
    "end": "1461300"
  },
  {
    "start": "1459000",
    "end": "1574000"
  },
  {
    "text": "combine that with Redis ElastiCache and I felt like we were really getting",
    "start": "1461300",
    "end": "1468200"
  },
  {
    "text": "something we were really getting somewhere here where we had that availability and durability that the",
    "start": "1468200",
    "end": "1475340"
  },
  {
    "text": "traditional sequel database offered and now now that we were leveraging that with a single external cache that all of",
    "start": "1475340",
    "end": "1482570"
  },
  {
    "text": "the streaming processors could leverage now we were really really getting the availability and performance average",
    "start": "1482570",
    "end": "1489350"
  },
  {
    "text": "performance over time and that the system economics of that worked out really well where you have one node of",
    "start": "1489350",
    "end": "1495560"
  },
  {
    "text": "one streaming processor it does the heavy lifting of going to the database to pick up the data when it's a cache",
    "start": "1495560",
    "end": "1501950"
  },
  {
    "text": "miss putting that in the cache and then any other node in any other streaming processor can just pick it up from Redis",
    "start": "1501950",
    "end": "1508040"
  },
  {
    "text": "and have a one millisecond access time instead of 50 millisecond access times so that worked out pretty well the",
    "start": "1508040",
    "end": "1515630"
  },
  {
    "text": "complicating factor was now we were writing all kinds of cache coherency code so now now this streaming processor",
    "start": "1515630",
    "end": "1523250"
  },
  {
    "text": "had to check the cache if it's not there it has to check the database has to pick it up and put it in a cache and maybe",
    "start": "1523250",
    "end": "1529400"
  },
  {
    "text": "you have some external system that is updating that data in my sequel and and Redis well there's a right to Redis",
    "start": "1529400",
    "end": "1536060"
  },
  {
    "text": "first does it right to my sequel first and can you miss something or then you have a stale stale instance of the data",
    "start": "1536060",
    "end": "1541970"
  },
  {
    "text": "somewhere higher up in the cache and it's it's it's a little annoying but",
    "start": "1541970",
    "end": "1547130"
  },
  {
    "text": "it's not that bad because at a certain point you've you've tested it you've written the code and it works the only",
    "start": "1547130",
    "end": "1553220"
  },
  {
    "text": "problem is if you're if you're changing that and as you're writing new streaming processors if you use say you want to",
    "start": "1553220",
    "end": "1559640"
  },
  {
    "text": "use some new streaming processor you might have to re-implement that it's just an opportunity to introduce bugs",
    "start": "1559640",
    "end": "1566090"
  },
  {
    "text": "and dating' its inconsistencies into the system so we had that all working and we",
    "start": "1566090",
    "end": "1575540"
  },
  {
    "text": "were thinking okay this is this is pretty good and then at that point",
    "start": "1575540",
    "end": "1581620"
  },
  {
    "text": "I just boil it so we we have that",
    "start": "1583680",
    "end": "1590130"
  },
  {
    "text": "working at that point I started working with AWS on a tax preview and I had",
    "start": "1590130",
    "end": "1596490"
  },
  {
    "text": "looked at dynamodb before for this use case but I thought I I'd done the napkin math on what the provision read capacity",
    "start": "1596490",
    "end": "1603150"
  },
  {
    "text": "would be to to actually handle these kind of use case and I thought okay that",
    "start": "1603150",
    "end": "1608670"
  },
  {
    "text": "you know that might not work for what I'm trying to do here and that's and then if I'm using DynamoDB it will be as",
    "start": "1608670",
    "end": "1615900"
  },
  {
    "text": "complex as what I've already implemented with RDS my sequel so really I don't really have any an excuse to try this",
    "start": "1615900",
    "end": "1621600"
  },
  {
    "text": "out but when I learned about tax it looks very familiar to what I had",
    "start": "1621600",
    "end": "1628940"
  },
  {
    "text": "implemented myself so it handled all that cache coherency for me in a way",
    "start": "1628940",
    "end": "1634500"
  },
  {
    "text": "where I didn't have to worry about it so I thought all right well let me try this out and so so far this is working very",
    "start": "1634500",
    "end": "1645510"
  },
  {
    "text": "well so it gives us that that performance that we're looking for a super availability of DynamoDB with the",
    "start": "1645510",
    "end": "1654030"
  },
  {
    "text": "fast access times tax and you can conceptually think about it like Redis",
    "start": "1654030",
    "end": "1659610"
  },
  {
    "text": "elastic hash in front of DynamoDB but it's managed and and again though that",
    "start": "1659610",
    "end": "1668220"
  },
  {
    "text": "performance though is predicated on the non-uniform data access patterns so if",
    "start": "1668220",
    "end": "1673230"
  },
  {
    "text": "we had a completely flat distribution it wouldn't really be cashable unless we wanted to store everything in a cache",
    "start": "1673230",
    "end": "1678620"
  },
  {
    "text": "then the performance would not be nearly as good but in our instance we are able",
    "start": "1678620",
    "end": "1685679"
  },
  {
    "text": "to actually observe very high cache hit rates in production and so this this is working out quite well operational",
    "start": "1685679",
    "end": "1692760"
  },
  {
    "text": "overhead is essentially zero in terms of challenges to this approach well now I",
    "start": "1692760",
    "end": "1698850"
  },
  {
    "text": "really have to spend time to figure out what what the right provisioned capacity should be and if you go above that you",
    "start": "1698850",
    "end": "1705690"
  },
  {
    "text": "get you get throttled and so you have to spend time kind of tweaking what that what those knobs should look like but",
    "start": "1705690",
    "end": "1712200"
  },
  {
    "text": "for me that's that's a much easier game to play than some of the complexity",
    "start": "1712200",
    "end": "1717240"
  },
  {
    "text": "there approaches and additionally say say for example if you're doing cold start on a streaming system then I've",
    "start": "1717240",
    "end": "1724380"
  },
  {
    "text": "actually gone and just up the read capacity to be able to hydrate the cache",
    "start": "1724380",
    "end": "1729660"
  },
  {
    "text": "appropriately and then dial it back down afterwards and then when I do that I",
    "start": "1729660",
    "end": "1736410"
  },
  {
    "text": "actually just do that honestly you don't want to crank up the read provision capacity to high because of something",
    "start": "1736410",
    "end": "1743370"
  },
  {
    "text": "called partitioning dilution which I'll explain and then in reality this is",
    "start": "1743370",
    "end": "1750810"
  },
  {
    "text": "approached with a very tactical in heap cache so something like Google guava for",
    "start": "1750810",
    "end": "1756030"
  },
  {
    "text": "an LRU approach so then in reality what happens is you have the data coming in on the stream the streaming processor",
    "start": "1756030",
    "end": "1762720"
  },
  {
    "text": "asks maybe a singleton instance of Google guava do you have that it doesn't",
    "start": "1762720",
    "end": "1768030"
  },
  {
    "text": "have it it goes to Dax if Dax doesn't have it it goes to DynamoDB and the system elegantly hydrates the levels of",
    "start": "1768030",
    "end": "1774180"
  },
  {
    "text": "the cache for you so this is what it looks like and I'm happy to say that",
    "start": "1774180",
    "end": "1781890"
  },
  {
    "start": "1777000",
    "end": "1862000"
  },
  {
    "text": "it's actually a rather simple diagram and that's that's what Jackson tiny mode",
    "start": "1781890",
    "end": "1787280"
  },
  {
    "text": "DynamoDB have allowed this to be so we have event streams over on the left they",
    "start": "1787280",
    "end": "1793500"
  },
  {
    "text": "come into Kafka and then we have our streaming processor picking that up and then it does whatever processing sends",
    "start": "1793500",
    "end": "1801090"
  },
  {
    "text": "it downstream to different data sinks meanwhile it's picking up data directly from Dax",
    "start": "1801090",
    "end": "1806220"
  },
  {
    "text": "Dax may or may not have that data it may have to go to dynamo DB but the application doesn't know or care about",
    "start": "1806220",
    "end": "1811770"
  },
  {
    "text": "that because to the application the client for Dax is exactly like dynamodb",
    "start": "1811770",
    "end": "1818130"
  },
  {
    "text": "so it doesn't actually know if it's talking to dynamodb or Dax and then the",
    "start": "1818130",
    "end": "1824790"
  },
  {
    "text": "way that we handled the reference the reference data is we have some kind of reference data change stream so if you",
    "start": "1824790",
    "end": "1830880"
  },
  {
    "text": "have a hotel Hotel data set for example whatever service is responsible for managing that's Hotel data when it has",
    "start": "1830880",
    "end": "1839250"
  },
  {
    "text": "an attribute change for the hotel it publishes that to Kafka and we have just a very small application picks up the",
    "start": "1839250",
    "end": "1845550"
  },
  {
    "text": "key value store and writes it to Dax and Dax can function as write through cache",
    "start": "1845550",
    "end": "1850860"
  },
  {
    "text": "so the key value writer writes to Dax Dax stores it in memory and then writes",
    "start": "1850860",
    "end": "1856740"
  },
  {
    "text": "through to dynamodb to persistent so for",
    "start": "1856740",
    "end": "1863910"
  },
  {
    "start": "1862000",
    "end": "1908000"
  },
  {
    "text": "some of the original performance benchmarks that gave me the confidence to move forward with this approach let",
    "start": "1863910",
    "end": "1869310"
  },
  {
    "text": "me let me talk about those so the set up is 1:1 Dax instance against the DynamoDB",
    "start": "1869310",
    "end": "1875370"
  },
  {
    "text": "table and it's the smallest instance which has about 13 gigabytes of storage default TTL 5 minutes and we're using",
    "start": "1875370",
    "end": "1883860"
  },
  {
    "text": "eventually consistent reads one thing to note is that if you're doing consistent means if you know for example that you",
    "start": "1883860",
    "end": "1890160"
  },
  {
    "text": "need to do all consistent reads against the database you wouldn't want to use Dax because when you use the consistent",
    "start": "1890160",
    "end": "1896400"
  },
  {
    "text": "read it actually bypasses Dax the data set as small cardinality a million",
    "start": "1896400",
    "end": "1904260"
  },
  {
    "text": "million items and these are small key value pairs and the rules of thumb that",
    "start": "1904260",
    "end": "1909930"
  },
  {
    "start": "1908000",
    "end": "1937000"
  },
  {
    "text": "I've used throughout testing is we have a Dax cache hit it's about a millisecond",
    "start": "1909930",
    "end": "1915390"
  },
  {
    "text": "or less when you have a miss it's 11 milliseconds and that's about 1",
    "start": "1915390",
    "end": "1920460"
  },
  {
    "text": "millisecond more than what it would take to actually do the read directly against DynamoDB and so there is a little bit of",
    "start": "1920460",
    "end": "1928410"
  },
  {
    "text": "a performance hit for using Dax in that case but to the degree that you can achieve a high cache hit ratio that",
    "start": "1928410",
    "end": "1933660"
  },
  {
    "text": "performance will look better over time that's a cure sure the results that I",
    "start": "1933660",
    "end": "1939180"
  },
  {
    "start": "1937000",
    "end": "2042000"
  },
  {
    "text": "ran so on a small test and the test setup this is a spark cluster of about",
    "start": "1939180",
    "end": "1945390"
  },
  {
    "text": "four nodes hitting a single Dax instance issuing 3,500 requests per second we're",
    "start": "1945390",
    "end": "1952260"
  },
  {
    "text": "using batching so you have about 25 items per request so I was able to look",
    "start": "1952260",
    "end": "1957660"
  },
  {
    "text": "up 80 thousand items per second with a cache hit ray of effectively 80% and the",
    "start": "1957660",
    "end": "1965750"
  },
  {
    "text": "CPU of Dass is unstressed at 5% and here's here's the kicker here's that",
    "start": "1965750",
    "end": "1971100"
  },
  {
    "text": "here's the thing that that I think is cool about this is that with that cache hit ratio the provision to read capacity",
    "start": "1971100",
    "end": "1977580"
  },
  {
    "text": "on dynamo is tiny so we have 100 read",
    "start": "1977580",
    "end": "1982830"
  },
  {
    "text": "capacity units but we were able to achieve 80,000 lookups per second and to me that's",
    "start": "1982830",
    "end": "1989130"
  },
  {
    "text": "that's very cool and but that's you you achieve that at this steady-state running so if you have to hydrate the",
    "start": "1989130",
    "end": "1995220"
  },
  {
    "text": "cache initially you may you might need more RCU or it might do some throttling but once you get to that 80% you can",
    "start": "1995220",
    "end": "2002480"
  },
  {
    "text": "achieve very very high read throughput on a low provision capacity in dynamo DB",
    "start": "2002480",
    "end": "2007789"
  },
  {
    "text": "because Dax is storing it in memory higher volume test 15,000 requests per",
    "start": "2007789",
    "end": "2013730"
  },
  {
    "text": "second returning 300,000 key value pairs per second and again these are very small items cache hit rate 99% at this point",
    "start": "2013730",
    "end": "2022730"
  },
  {
    "text": "almost everything's in memory CPU is stressed at this point 80 percent and",
    "start": "2022730",
    "end": "2028510"
  },
  {
    "text": "RCU is at 500 but still when you have a cache hit rate that is high enough you",
    "start": "2028510",
    "end": "2035510"
  },
  {
    "text": "can achieve insane read throughput on low provision capacity for the table so",
    "start": "2035510",
    "end": "2044000"
  },
  {
    "text": "for our team this has really provided a great balance for performance and",
    "start": "2044000",
    "end": "2051020"
  },
  {
    "text": "scalability availability durability everything in a way that's much lower",
    "start": "2051020",
    "end": "2057260"
  },
  {
    "text": "operational overhead and so we have it just takes less time to keep it running",
    "start": "2057260",
    "end": "2062510"
  },
  {
    "text": "and we have a higher guarantee that it's it's not going to fail and for me I like",
    "start": "2062510",
    "end": "2069200"
  },
  {
    "text": "not I don't know about you guys but if I ever if I ever get woken up by the ha phone at 2:00 a.m. I'm not like a hobo a",
    "start": "2069200",
    "end": "2076220"
  },
  {
    "text": "party time maybe maybe maybe you guys are I want something that just works",
    "start": "2076220",
    "end": "2083830"
  },
  {
    "text": "this also works well for in situ reusability across multiple streaming processors and then provided that you",
    "start": "2083830",
    "end": "2091398"
  },
  {
    "text": "have a sufficiently non-uniform data access pattern it can be extremely cost",
    "start": "2091399",
    "end": "2096408"
  },
  {
    "text": "effective and performance because it's storing everything index so for me",
    "start": "2096409",
    "end": "2103390"
  },
  {
    "text": "access to reference data should be simple conceptually just a key value lookup and a hash join with the data and",
    "start": "2103390",
    "end": "2111859"
  },
  {
    "text": "I feel like this approach with dynamo DB index helps to keep it simple",
    "start": "2111859",
    "end": "2118270"
  },
  {
    "start": "2118000",
    "end": "2276000"
  },
  {
    "text": "all right so a few tips to share with you biggest performance boost on Dax use",
    "start": "2118490",
    "end": "2127559"
  },
  {
    "text": "batching I believe the max backs match back size right now is 25 but that gives",
    "start": "2127559",
    "end": "2133680"
  },
  {
    "text": "me about a 10 to 20 X speed up with the dynamodb client itself",
    "start": "2133680",
    "end": "2138690"
  },
  {
    "text": "I have not consistently observed performance improvements using the TCP keepalive one note with the Dax client",
    "start": "2138690",
    "end": "2145619"
  },
  {
    "text": "it's it uses open TCP sockets",
    "start": "2145619",
    "end": "2150930"
  },
  {
    "text": "so it's essentially already doing that for larger objects you can save a little bit on bandwidth and deserialization",
    "start": "2150930",
    "end": "2156720"
  },
  {
    "text": "time if you use a projection so if you have many objects in the tape or many attributes in the table you can just",
    "start": "2156720",
    "end": "2163500"
  },
  {
    "text": "return the subset which you actually need and then partition dilution so this",
    "start": "2163500",
    "end": "2170720"
  },
  {
    "text": "raise your head if you understand what what partition devolution is all right",
    "start": "2170720",
    "end": "2176450"
  },
  {
    "text": "I'll go ahead and explain that because it's really important if you're running dynamo DB in production so you have the",
    "start": "2176450",
    "end": "2183240"
  },
  {
    "text": "provisioned throughput capacity both on the read side and the right side and say you have you set it at 103 capacity 100",
    "start": "2183240",
    "end": "2191720"
  },
  {
    "text": "that's that's at the whole table but what actually happens is it's divided",
    "start": "2191720",
    "end": "2197160"
  },
  {
    "text": "equally among all the partitions and so if you have 10 partitions in a table of",
    "start": "2197160",
    "end": "2202619"
  },
  {
    "text": "100 read capacity that's actually 10 read capacity per partition and so if",
    "start": "2202619",
    "end": "2210029"
  },
  {
    "text": "your access to that data is uniform then you can achieve 100 RCU in aggregate but",
    "start": "2210029",
    "end": "2218700"
  },
  {
    "text": "say for example if you have non-uniform access to that data and you have a",
    "start": "2218700",
    "end": "2224160"
  },
  {
    "text": "honkey so that all your requests go to the same partition your table will be bottlenecked effectively 10 RC you when",
    "start": "2224160",
    "end": "2232170"
  },
  {
    "text": "you had provisioned 100 RS you for the entire table and so the the area where",
    "start": "2232170",
    "end": "2241380"
  },
  {
    "text": "that can happen is one is if you have so if you choose a low cardinality key for",
    "start": "2241380",
    "end": "2248160"
  },
  {
    "text": "your table you can exacerbate that problem by having hotter keys but then also if you",
    "start": "2248160",
    "end": "2253169"
  },
  {
    "text": "scale up your your RC use massively it'll increase the number of partitions",
    "start": "2253169",
    "end": "2259679"
  },
  {
    "text": "in the dynamodb table and then if you scale back down you run the risk of getting bottlenecks not not based on the",
    "start": "2259679",
    "end": "2269130"
  },
  {
    "text": "RCO of the entire table but just for an individual hot partition so for the",
    "start": "2269130",
    "end": "2278909"
  },
  {
    "start": "2276000",
    "end": "2374000"
  },
  {
    "text": "client itself so it's thread-safe so you could reuse the client you could keep a singleton instance in your streaming",
    "start": "2278909",
    "end": "2284579"
  },
  {
    "text": "processor the instantiation time is only about 20 milliseconds so it's not saving you a whole bunch in terms of guaranteed",
    "start": "2284579",
    "end": "2293699"
  },
  {
    "text": "message processing and spark which is what I'm using this for with the provisioned throughput exceeded",
    "start": "2293699",
    "end": "2300359"
  },
  {
    "text": "exception the way I handled that is I actually paused the streaming processor and do an exponential back-off and so",
    "start": "2300359",
    "end": "2308429"
  },
  {
    "text": "when it gets that throttling exception a spark essentially just just pauses and waits and then keeps trying and in the",
    "start": "2308429",
    "end": "2315089"
  },
  {
    "text": "meantime it doesn't update the kafka offsets and I have a separate process which then monitors the kafka lag and",
    "start": "2315089",
    "end": "2323099"
  },
  {
    "text": "will alert me if the streaming processor is falling too far behind so then what you can run into is just some",
    "start": "2323099",
    "end": "2329489"
  },
  {
    "text": "intermittent throttling but you run into that and then the processor is able to",
    "start": "2329489",
    "end": "2335039"
  },
  {
    "text": "continue once it gets past that and if that is happening too frequently then you should probably just provision a",
    "start": "2335039",
    "end": "2341189"
  },
  {
    "text": "little bit more Reap acity on the table and then for spark for a singleton off",
    "start": "2341189",
    "end": "2348239"
  },
  {
    "text": "LRU cache museum guava and just hold it in a single tenant static place in the executor JVM it's just your reference",
    "start": "2348239",
    "end": "2355709"
  },
  {
    "text": "data so again you have streaming processor coming in or streaming data coming in spark asks the goop lava",
    "start": "2355709",
    "end": "2363359"
  },
  {
    "text": "instance do you have data guava goes to Dax X goes to dynamo DB and this this so",
    "start": "2363359",
    "end": "2369509"
  },
  {
    "text": "far is working reasonably well",
    "start": "2369509",
    "end": "2373069"
  },
  {
    "start": "2374000",
    "end": "2398000"
  },
  {
    "text": "so with that that's my story and like to",
    "start": "2376310",
    "end": "2381750"
  },
  {
    "text": "thank you for listening I hope that each person was able to get at least some some piece of information from this and",
    "start": "2381750",
    "end": "2387980"
  },
  {
    "text": "like to open up for Q&A if anyone has questions thank you",
    "start": "2387980",
    "end": "2394590"
  },
  {
    "text": "[Applause]",
    "start": "2394590",
    "end": "2397709"
  }
]