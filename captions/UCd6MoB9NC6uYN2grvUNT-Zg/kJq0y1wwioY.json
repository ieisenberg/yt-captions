[
  {
    "start": "0",
    "end": "76000"
  },
  {
    "text": "okay folks good morning we're gonna get started thank you for sticking around day three",
    "start": "2720",
    "end": "9360"
  },
  {
    "text": "uh welcome to media 402 building a scalable video digital asset management platform in the",
    "start": "9360",
    "end": "15920"
  },
  {
    "text": "cloud or damn i'm going to say damn a lot today",
    "start": "15920",
    "end": "22080"
  },
  {
    "text": "damn anyone ever see uh joe dirt yep damn okay so",
    "start": "22080",
    "end": "30000"
  },
  {
    "text": "with me today and by the way i'm mike lemcaco i'm a solutions architect with amazon web services and with me today is",
    "start": "30000",
    "end": "37600"
  },
  {
    "text": "jonathan rivers from pbs director of technical operations and together we'll describe",
    "start": "37600",
    "end": "43440"
  },
  {
    "text": "some general best practices for building a scalable dam in the cloud we have a lot of",
    "start": "43440",
    "end": "49200"
  },
  {
    "text": "material to cover and in fact because this is a 400 level session our goal was to dive deep into",
    "start": "49200",
    "end": "56960"
  },
  {
    "text": "some bits of code we will still do aspects of that but in the interest of time we may be",
    "start": "56960",
    "end": "62239"
  },
  {
    "text": "jumping ahead now we will try our very best just to give you a heads up to open source",
    "start": "62239",
    "end": "67760"
  },
  {
    "text": "and or publish the bits of code that we'll be talking about today",
    "start": "67760",
    "end": "73840"
  },
  {
    "start": "76000",
    "end": "165000"
  },
  {
    "text": "so the agenda before you is pretty straightforward we're going to",
    "start": "76720",
    "end": "81840"
  },
  {
    "text": "provide you with a general overview the big picture what's the context for dam in the cloud",
    "start": "81840",
    "end": "88000"
  },
  {
    "text": "what are some conceptual logical and physical architecture approaches to realize that that picture or that",
    "start": "88000",
    "end": "94880"
  },
  {
    "text": "vision and then we'll go ahead and open up the kimono and actually build out aspects of a damn platform",
    "start": "94880",
    "end": "102399"
  },
  {
    "text": "in the cloud now by the way absolutely recognize that there are existing dam solutions out there today",
    "start": "102399",
    "end": "109040"
  },
  {
    "text": "there are damn platforms there are marketplace offerings there are standard shrink wrap products",
    "start": "109040",
    "end": "117439"
  },
  {
    "text": "absolutely recognize and appreciate that the goal of today's discussion is to dig a little deeper if for",
    "start": "117439",
    "end": "124000"
  },
  {
    "text": "whatever reason you you need to engineer your own equivalent solution or build or extend an existing",
    "start": "124000",
    "end": "130479"
  },
  {
    "text": "capability that you have today using the concepts that will present we will also show a representative",
    "start": "130479",
    "end": "137680"
  },
  {
    "text": "client experience and how that might lay on top of the cloud services that we're talking about",
    "start": "137680",
    "end": "143840"
  },
  {
    "text": "and then of course we'll shift over to jonathan and team and talk about how pbs has successfully",
    "start": "143840",
    "end": "149760"
  },
  {
    "text": "continues to successfully deploy these types of concepts uh in their operations today and then",
    "start": "149760",
    "end": "156160"
  },
  {
    "text": "finally we'll wrap it up and hopefully we'll leave some room for uh for q a",
    "start": "156160",
    "end": "162000"
  },
  {
    "start": "165000",
    "end": "282000"
  },
  {
    "text": "so again just to level set everyone the focus of this particular session is comprehensive enough to cover",
    "start": "165440",
    "end": "174319"
  },
  {
    "text": "the general pattern of media ingest storage and discovery across the board",
    "start": "174319",
    "end": "180159"
  },
  {
    "text": "however we do want to pay special attention to the pro media side of the house because we",
    "start": "180159",
    "end": "186720"
  },
  {
    "text": "we tend to give a lot of love rightfully so to the user generated type of content consumer generated content and their",
    "start": "186720",
    "end": "193680"
  },
  {
    "text": "their specific workflows for the next 50 minutes or so we'd like",
    "start": "193680",
    "end": "198800"
  },
  {
    "text": "to spend a little bit of that time in addressing the pro media",
    "start": "198800",
    "end": "203840"
  },
  {
    "text": "side of the house if you're a broadcast or an organization",
    "start": "203840",
    "end": "209519"
  },
  {
    "text": "that's responsible for producing feature film content original content bash dub content you're very familiar",
    "start": "209519",
    "end": "216480"
  },
  {
    "text": "with the idea that you have to take in large amounts of source material from various sources",
    "start": "216480",
    "end": "222319"
  },
  {
    "text": "not always digital at least not originally staging that into a number of storage",
    "start": "222319",
    "end": "229200"
  },
  {
    "text": "uh architecture options and progressing that through a workflow such as transcode transmux",
    "start": "229200",
    "end": "236480"
  },
  {
    "text": "conversion down sampling and such and then pushing that further down the chain up to the point of presence where",
    "start": "236480",
    "end": "244080"
  },
  {
    "text": "creative actors or creative workers consumers editors assistants",
    "start": "244080",
    "end": "250640"
  },
  {
    "text": "everyone who's a part of this organization can find that content and make use of it",
    "start": "250640",
    "end": "256879"
  },
  {
    "text": "so that said media asset management is a big",
    "start": "262240",
    "end": "268560"
  },
  {
    "text": "it's a big space we're going to focus this discussion even further on the actual storage metadata",
    "start": "268560",
    "end": "276000"
  },
  {
    "text": "management and discovery aspect of this entire workflow",
    "start": "276000",
    "end": "282800"
  },
  {
    "start": "282000",
    "end": "398000"
  },
  {
    "text": "conceptually where does that fit in the big picture it's the items you see highlighted",
    "start": "282800",
    "end": "289680"
  },
  {
    "text": "all of these parts of course extremely important but discovery delivery and how you access",
    "start": "289680",
    "end": "295759"
  },
  {
    "text": "that content obviously very critical to your to your user experience",
    "start": "295759",
    "end": "302560"
  },
  {
    "text": "so what are some key business or functional requirements that are a part of a typical",
    "start": "303039",
    "end": "308240"
  },
  {
    "text": "damn initiative we can see here that across the board there's a number of key",
    "start": "308240",
    "end": "314240"
  },
  {
    "text": "features that any comprehensive dam solution should deliver from ingest to metadata extraction",
    "start": "314240",
    "end": "321759"
  },
  {
    "text": "pulling out the technical bits of information and user-generated annotations pulling that out saving that off to a",
    "start": "321759",
    "end": "329440"
  },
  {
    "text": "database or making that discoverable creating renditions and renditions have",
    "start": "329440",
    "end": "334479"
  },
  {
    "text": "different that's a there's a different context for renditions depending on where you're coming from this could be a low res proxy that could",
    "start": "334479",
    "end": "341520"
  },
  {
    "text": "be mezzanine files to help enable the actual post-production process",
    "start": "341520",
    "end": "347039"
  },
  {
    "text": "or it could be a browsable adaptive bit rate web web data sets that",
    "start": "347039",
    "end": "353520"
  },
  {
    "text": "i could look at on my iphone so creating renditions from original source material that's obviously an",
    "start": "353520",
    "end": "359440"
  },
  {
    "text": "important part of the dam solution building the catalog being able to discover",
    "start": "359440",
    "end": "365199"
  },
  {
    "text": "the various data sets according to different dimensions that's obviously critical enabling rich",
    "start": "365199",
    "end": "371600"
  },
  {
    "text": "search against those data data elements and finally as as a complementary set of",
    "start": "371600",
    "end": "378639"
  },
  {
    "text": "features managing the overall storage lifecycle and providing efficient delivery of those media assets",
    "start": "378639",
    "end": "385199"
  },
  {
    "text": "whether we're caching that content or providing a streaming experience to the end consumer",
    "start": "385199",
    "end": "390319"
  },
  {
    "text": "trying to make sure that that discovery process is closed in an effective way",
    "start": "390319",
    "end": "397600"
  },
  {
    "start": "398000",
    "end": "557000"
  },
  {
    "text": "and of course we want to do this at scale it's so important to realize that with",
    "start": "399039",
    "end": "405280"
  },
  {
    "text": "the increasing amount of content you know variety velocity and veracity content is just growing different",
    "start": "405280",
    "end": "412319"
  },
  {
    "text": "formats different standards different consumers we have to address the idea that this platform needs to scale many",
    "start": "412319",
    "end": "422160"
  },
  {
    "text": "of us in this industry can appreciate the fact that with increasing resolution for example",
    "start": "422160",
    "end": "428639"
  },
  {
    "text": "2k 4k and beyond we're just looking at massive amounts of content professional organizations creating",
    "start": "428639",
    "end": "435440"
  },
  {
    "text": "feature film content are easily hitting multi-petabyte range for finished content",
    "start": "435440",
    "end": "443199"
  },
  {
    "text": "it's it's mind-boggling to think about how much content is being captured by raw camera raw camera sources on set",
    "start": "443199",
    "end": "450400"
  },
  {
    "text": "if we could just pick up that raw material and make it available for the larger",
    "start": "450400",
    "end": "455759"
  },
  {
    "text": "consumer base there's there's just great potential for that but we're talking about massive",
    "start": "455759",
    "end": "460800"
  },
  {
    "text": "amounts of content petabytes easily and of course",
    "start": "460800",
    "end": "465919"
  },
  {
    "text": "between the different formats standards codecs resolutions and device types it's it's becoming an",
    "start": "465919",
    "end": "473599"
  },
  {
    "text": "increasingly challenging problem cloud as a business enabler allows us to",
    "start": "473599",
    "end": "480240"
  },
  {
    "text": "start thinking about opportunities how to in terms of how to take advantage of cloud services like",
    "start": "480240",
    "end": "485840"
  },
  {
    "text": "near infinite storage to be able to enable the capture and the persistence and the discovery of some of that traditionally unused",
    "start": "485840",
    "end": "492879"
  },
  {
    "text": "content content that is left on the cutting room floor for example what if we could actually persist that stored efficiently",
    "start": "492879",
    "end": "500800"
  },
  {
    "text": "at a cost effective rate there's a potential new business model for that",
    "start": "500800",
    "end": "507840"
  },
  {
    "text": "search specifically is an interesting challenge because we have to deal with the wide variety of",
    "start": "510240",
    "end": "515599"
  },
  {
    "text": "of structured semi-structured elements that are a part of the technical header",
    "start": "515599",
    "end": "521518"
  },
  {
    "text": "of any container the the challenge here of course is that",
    "start": "521519",
    "end": "526720"
  },
  {
    "text": "there are so many ways to call an entity a specific term",
    "start": "526720",
    "end": "532480"
  },
  {
    "text": "the concept of codec for example the term codec might appear in container x",
    "start": "532480",
    "end": "538240"
  },
  {
    "text": "as a certain string and yet in another container it might be described as a numeric",
    "start": "538240",
    "end": "543600"
  },
  {
    "text": "this is standard typical data modeling channel a standard typical data modeling",
    "start": "543600",
    "end": "549600"
  },
  {
    "text": "challenge that we can address using some of the concepts that we'll talk about today",
    "start": "549600",
    "end": "555440"
  },
  {
    "start": "557000",
    "end": "708000"
  },
  {
    "text": "all right so why don't we go ahead and dive deep let's go ahead and look in and lift the covers a little bit and",
    "start": "558000",
    "end": "563279"
  },
  {
    "text": "start building out aspects of this solution",
    "start": "563279",
    "end": "567360"
  },
  {
    "text": "so i'm going to switch gears here and show you an example of a course",
    "start": "569760",
    "end": "576240"
  },
  {
    "text": "it's my ec2 instance i'll just bring that up again in a bit but let's say",
    "start": "576240",
    "end": "583600"
  },
  {
    "text": "let's take a step back here's the use case i'm i'm a media organization i'm doing",
    "start": "583600",
    "end": "588880"
  },
  {
    "text": "collaborative work we're creating some original and maybe not so original new content mashup",
    "start": "588880",
    "end": "594720"
  },
  {
    "text": "from original material that might be available to us we're going to create some new",
    "start": "594720",
    "end": "602079"
  },
  {
    "text": "interesting web features that incorporate content from other digital providers or digital sources",
    "start": "602079",
    "end": "608640"
  },
  {
    "text": "code rush 2010 is over a hundred tapes now digitized of content from a",
    "start": "608640",
    "end": "614480"
  },
  {
    "text": "movie that was produced in late 99 i believe it was on pbs at one point",
    "start": "614480",
    "end": "619839"
  },
  {
    "text": "and it it describes the the trials and tribulations of uh the the netscape and mozilla project",
    "start": "619839",
    "end": "626800"
  },
  {
    "text": "team in the late 90s it's a really it's actually kind of a cool project you should watch it one day",
    "start": "626800",
    "end": "632560"
  },
  {
    "text": "but the idea here is that we have a collaborative work group they need to find content that's a part of this they need to scrub it they need",
    "start": "632560",
    "end": "638880"
  },
  {
    "text": "to annotate it they need to to identify interesting snippets",
    "start": "638880",
    "end": "644160"
  },
  {
    "text": "push that up to a common persistent collaborative location and make it available for other users in the",
    "start": "644160",
    "end": "649680"
  },
  {
    "text": "in the team so i could for example",
    "start": "649680",
    "end": "657839"
  },
  {
    "text": "i could for example be a user who has access to a logging tool like prelude i could have",
    "start": "658079",
    "end": "664959"
  },
  {
    "text": "the digital media available to me and i could be scrubbing through the various assets looking for interesting areas to pull",
    "start": "664959",
    "end": "672320"
  },
  {
    "text": "out for our feature film and we may identify a section where",
    "start": "672320",
    "end": "678959"
  },
  {
    "text": "throughout the clip in one of the scrum meetings at netscape we see a gentleman an engineer using",
    "start": "678959",
    "end": "684720"
  },
  {
    "text": "some interesting ancient device and we find that kind of cute so we decided to go ahead and mark that",
    "start": "684720",
    "end": "690320"
  },
  {
    "text": "as an interesting clip for further use and again just recognizing that there",
    "start": "690320",
    "end": "695360"
  },
  {
    "text": "are standard tools and techniques for this the user can go ahead and use the tools",
    "start": "695360",
    "end": "700399"
  },
  {
    "text": "that they know and love and at some point we need to bridge into the cloud and that's what we're going to do",
    "start": "700399",
    "end": "706640"
  },
  {
    "start": "708000",
    "end": "774000"
  },
  {
    "text": "so a user would go ahead through this logging workflow identify snippets that",
    "start": "709120",
    "end": "714399"
  },
  {
    "text": "were of interest across the breadth of the raw content that they had collected they could then go ahead and using the",
    "start": "714399",
    "end": "720720"
  },
  {
    "text": "tools that they're used to today whether it's prelude or avid or final cut pro they",
    "start": "720720",
    "end": "726560"
  },
  {
    "text": "can go ahead and save this to a what they think is a nas device or a local storage system and behind the",
    "start": "726560",
    "end": "732560"
  },
  {
    "text": "scenes we've actually mounted that could could be a variety of technologies we've",
    "start": "732560",
    "end": "739600"
  },
  {
    "text": "mounted this user experience so that when they save those files it's actually getting uploaded to the",
    "start": "739600",
    "end": "745040"
  },
  {
    "text": "cloud you could be using for example aspera or signiant or cloud opt",
    "start": "745040",
    "end": "752160"
  },
  {
    "text": "i actually happen to be using blackberry blackberry drive cloud drive did i say",
    "start": "752160",
    "end": "757600"
  },
  {
    "text": "blackberry sorry guys cloudberry drive",
    "start": "757600",
    "end": "763440"
  },
  {
    "text": "which allows me to basically take content from my file based workflow",
    "start": "763440",
    "end": "768560"
  },
  {
    "text": "copy it over to a shared drive and have multi-part upload kick in and thread up to the cloud behind the",
    "start": "768560",
    "end": "775440"
  },
  {
    "text": "scenes we'll pick it up in the cloud ingest it extract metadata register it",
    "start": "775440",
    "end": "780880"
  },
  {
    "text": "in the catalog and make it searchable well what is what does searchable mean from a user experience perspective",
    "start": "780880",
    "end": "786480"
  },
  {
    "text": "let's go ahead and sign into our portal this is a representative user interface",
    "start": "786480",
    "end": "794720"
  },
  {
    "text": "i actually have this built out hosted in on beanstalk",
    "start": "794720",
    "end": "801040"
  },
  {
    "text": "a user may for example be presented with a home page that shows them the different bins the different projects",
    "start": "801040",
    "end": "806880"
  },
  {
    "text": "that they've been working on behind the scenes we'd be storing these containers which represent what content",
    "start": "806880",
    "end": "812880"
  },
  {
    "text": "they've collected and annotated we'd be storing the metadata behind this in at least",
    "start": "812880",
    "end": "818880"
  },
  {
    "text": "dynamo for example if they wanted to upload through the browser interface they could use the",
    "start": "818880",
    "end": "825040"
  },
  {
    "text": "browser interface as well let's say some box art they forgot to upload some box art they want to go ahead and upload",
    "start": "825040",
    "end": "830160"
  },
  {
    "text": "that throughout this entire process they also should have visibility",
    "start": "830160",
    "end": "835519"
  },
  {
    "text": "into the activities of the system i should be able to for example understand where i am in the ingest",
    "start": "835519",
    "end": "842399"
  },
  {
    "text": "pipeline where's my rendition workflow what's happening with that well ultimately at the end of the day",
    "start": "842399",
    "end": "848880"
  },
  {
    "text": "what's important to them what's important to us for this use case is search and discovery",
    "start": "848880",
    "end": "856079"
  },
  {
    "text": "and this part of the of the system is in fact hitting dynamo and cloud search and we'll talk",
    "start": "856079",
    "end": "861600"
  },
  {
    "text": "about the details behind the scenes but right now we're going and hitting",
    "start": "861600",
    "end": "867279"
  },
  {
    "text": "we're hitting a fleet of bean stock managed servers which are then propagating back",
    "start": "867279",
    "end": "872800"
  },
  {
    "text": "to a number of search capabilities in the form of dynamodb and cloud search",
    "start": "872800",
    "end": "879360"
  },
  {
    "text": "as you would expect i'm able to preview web proxies",
    "start": "879360",
    "end": "886160"
  },
  {
    "text": "all of this of course would have been pre-processed ahead of time through the services that we have such as elastic transcoding service",
    "start": "886160",
    "end": "892880"
  },
  {
    "text": "we could also look at the technical metadata if if it was important we can go ahead",
    "start": "892880",
    "end": "898480"
  },
  {
    "text": "and click that and find all the technical metadata that was extracted as a part of the ingest",
    "start": "898480",
    "end": "904320"
  },
  {
    "text": "activity but going back to our story about the",
    "start": "904320",
    "end": "909440"
  },
  {
    "text": "the mozilla netflix use case we're interested in finding clips from that original 1998 film",
    "start": "909440",
    "end": "916959"
  },
  {
    "text": "so i'm going to go ahead i know our team in a remote location has scrubbed it they've logged it they've metadata",
    "start": "916959",
    "end": "922880"
  },
  {
    "text": "tagged it and they've ingested it into the cloud at least they've told me that that's the case so i can go ahead and start searching",
    "start": "922880",
    "end": "929759"
  },
  {
    "text": "so i can start searching by title maybe there's some aspect of the title it's called code rush",
    "start": "929759",
    "end": "936480"
  },
  {
    "text": "i know that they told me that there was specific formats and codex that they applied during the during the",
    "start": "937440",
    "end": "943440"
  },
  {
    "text": "transform process so i could go ahead and say limit my search to avc or h.264",
    "start": "943440",
    "end": "950399"
  },
  {
    "text": "and i do my search and the system will go ahead and filter those clips as appropriate",
    "start": "950399",
    "end": "956399"
  },
  {
    "text": "now i know that the logger as a part of that initial workflow identified specific scenes and specific",
    "start": "956399",
    "end": "963920"
  },
  {
    "text": "sub clips within these larger clips wouldn't it be interesting if we could go ahead and",
    "start": "963920",
    "end": "969360"
  },
  {
    "text": "introduce that concept of almost object recognition without actually",
    "start": "969360",
    "end": "974880"
  },
  {
    "text": "throwing in computer vision technology we're going to use the human we're going to use the logger the",
    "start": "974880",
    "end": "982000"
  },
  {
    "text": "assistant editor at the beginning of this workflow who tagged the content at specific points of the clip",
    "start": "982000",
    "end": "987199"
  },
  {
    "text": "to find these relevant pieces that we need to work on further so i'm going to go ahead and type in look",
    "start": "987199",
    "end": "994480"
  },
  {
    "text": "for track marker comments where the word uh unique was used or",
    "start": "994480",
    "end": "1001680"
  },
  {
    "text": "classic or i like i love this again it's a little bit of an artificial",
    "start": "1001680",
    "end": "1007440"
  },
  {
    "text": "example but you get the idea that we can come up with a mechanism where loggers use specific tags and we",
    "start": "1007440",
    "end": "1013519"
  },
  {
    "text": "can search for those tags and not just do a a dumb kind of search this is very specific it dives deep into the actual",
    "start": "1013519",
    "end": "1019680"
  },
  {
    "text": "technical metadata to pull this out and in fact yes we find that this is the",
    "start": "1019680",
    "end": "1025120"
  },
  {
    "text": "clip that we wanted",
    "start": "1025120",
    "end": "1028079"
  },
  {
    "text": "there's a comments button that's activated which will park me right into the section of the",
    "start": "1030400",
    "end": "1035918"
  },
  {
    "text": "clip pulling from s3 or from from cloudfront back to s3 and then using the",
    "start": "1035919",
    "end": "1043600"
  },
  {
    "text": "still emergence but very useful media fragment uri",
    "start": "1043600",
    "end": "1048799"
  },
  {
    "text": "rfc from the w3 org to be able to peek right into that clip to be able to",
    "start": "1048799",
    "end": "1054559"
  },
  {
    "text": "specify from a url perspective where i want that range request to park itself",
    "start": "1054559",
    "end": "1060799"
  },
  {
    "text": "so this is taking advantage of effective effective bandwidth it's also parking me",
    "start": "1060799",
    "end": "1067280"
  },
  {
    "text": "right in the section that i want to be in and we're also painting back the clips",
    "start": "1067280",
    "end": "1072320"
  },
  {
    "text": "that were tagged as a part of prelude all of this is available today with existing existing capabilities there's",
    "start": "1072320",
    "end": "1079360"
  },
  {
    "text": "no magic here it's all about integration and laying it on top of amazon web services",
    "start": "1079360",
    "end": "1086000"
  },
  {
    "text": "and finally wish i had more time to actually build this out but there's a button somewhere here that says i like",
    "start": "1086480",
    "end": "1092000"
  },
  {
    "text": "that clip i now want to i want to order it and use it as a part of my post production workflow",
    "start": "1092000",
    "end": "1098160"
  },
  {
    "text": "i don't want to have to download the full gig the full 20 minute clip i really want just that 60 seconds of",
    "start": "1098160",
    "end": "1104480"
  },
  {
    "text": "interesting content go pull that back from the system well between s3 and just and",
    "start": "1104480",
    "end": "1112559"
  },
  {
    "text": "cloud district cloudfront supporting range requests um and moreover recently very recently",
    "start": "1112559",
    "end": "1120799"
  },
  {
    "text": "last friday the ets team announced clip generation so with all the metadata that we're able",
    "start": "1120799",
    "end": "1127039"
  },
  {
    "text": "to extract out of this that's available in our dynamodb and cloud search content that's coming back we could",
    "start": "1127039",
    "end": "1133280"
  },
  {
    "text": "actually initiate an api call to ets and specify",
    "start": "1133280",
    "end": "1139760"
  },
  {
    "text": "the clip marks that i want the start time and the duration",
    "start": "1139760",
    "end": "1144960"
  },
  {
    "text": "so very cool at the very end of this jobs once i submit this job i can go ahead and submit this job and",
    "start": "1144960",
    "end": "1150559"
  },
  {
    "text": "imagine if it was integrated into the gui oh well of course it wouldn't be a demo if",
    "start": "1150559",
    "end": "1156640"
  },
  {
    "text": "i didn't have a red box but like a good cooking show i've got the",
    "start": "1156640",
    "end": "1162720"
  },
  {
    "start": "1158000",
    "end": "1190000"
  },
  {
    "text": "turkey all finished ready to go so that actually came out of elastic",
    "start": "1162720",
    "end": "1169280"
  },
  {
    "text": "transcoder service using the new clip mark feature so now i have 60 seconds of content that i was able to",
    "start": "1169280",
    "end": "1176799"
  },
  {
    "text": "discover from someone who had marked it previously as a part of their workflow uploaded it to the cloud",
    "start": "1176799",
    "end": "1181919"
  },
  {
    "text": "found it and pulled it out and now i can continue with my post-production activity",
    "start": "1181919",
    "end": "1187360"
  },
  {
    "text": "so kind of cool all right so let's talk architecture",
    "start": "1187360",
    "end": "1195679"
  },
  {
    "start": "1190000",
    "end": "1223000"
  },
  {
    "text": "so how how would we build something like this how have we built it what are some of the patterns we see in",
    "start": "1197360",
    "end": "1203360"
  },
  {
    "text": "our customer spaces before we start talking about amazon web service components and technologies i",
    "start": "1203360",
    "end": "1208960"
  },
  {
    "text": "think it's a good idea to talk about the conceptual building blocks from edge",
    "start": "1208960",
    "end": "1214960"
  },
  {
    "text": "in green from edge and customer facing components to i need my mouse here",
    "start": "1214960",
    "end": "1224320"
  },
  {
    "text": "edge middleware media verticals and storage and",
    "start": "1224480",
    "end": "1230640"
  },
  {
    "text": "discovery the system would be wired using these concept building blocks",
    "start": "1230640",
    "end": "1237120"
  },
  {
    "text": "the flow is users would integrate with this cloud dam platform using their file",
    "start": "1237120",
    "end": "1243039"
  },
  {
    "text": "based workflows remember we want to be as non-invasive as possible that content would be discovered",
    "start": "1243039",
    "end": "1250400"
  },
  {
    "text": "by a number of middleware agents they would be basically be monitoring for ingest activity and then",
    "start": "1250400",
    "end": "1256880"
  },
  {
    "text": "emitting events notifying interested parties those interested parties would then",
    "start": "1256880",
    "end": "1262000"
  },
  {
    "text": "pick up the the remaining requests from a queuing construct and in the first case perform rendition",
    "start": "1262000",
    "end": "1269760"
  },
  {
    "text": "processing creating thumbnails low res proxies mezzanine files and in the second case",
    "start": "1269760",
    "end": "1275679"
  },
  {
    "text": "running in parallel uh extracting metadata and possibly you can see yourself",
    "start": "1275679",
    "end": "1282720"
  },
  {
    "text": "looking at this saying you know there's there's way more things that can be added to that that entire workflow yes we could start adding",
    "start": "1282720",
    "end": "1289440"
  },
  {
    "text": "in a very scalable way we could start adding additional plugins down the pipe for things like computer vision scene",
    "start": "1289440",
    "end": "1296720"
  },
  {
    "text": "recognition water marking there's there's other things we can continue to add to this",
    "start": "1296720",
    "end": "1302840"
  },
  {
    "text": "pipeline at the end of the day the bits the pixels the video content gets",
    "start": "1302840",
    "end": "1309440"
  },
  {
    "text": "stored in a back-end store and it gets cataloged and synchronized to a front-end catalog",
    "start": "1309440",
    "end": "1315039"
  },
  {
    "text": "or database and from a user perspective they interact with this back-end",
    "start": "1315039",
    "end": "1320159"
  },
  {
    "text": "persistent slayer through a through a a user interface through an api they discover content",
    "start": "1320159",
    "end": "1325520"
  },
  {
    "text": "they request that content be delivered and it has the potential to be streamed out through a delivery cache",
    "start": "1325520",
    "end": "1333679"
  },
  {
    "start": "1333000",
    "end": "1431000"
  },
  {
    "text": "the tools that are available to us to build out this this this vision are listed here i think",
    "start": "1333679",
    "end": "1340559"
  },
  {
    "text": "it's all pretty obvious to us in this room you're at an aws conference we might expect to see a couple of our services in the third",
    "start": "1340559",
    "end": "1348159"
  },
  {
    "text": "column that's not to say that any objective solution needs to be limited to those",
    "start": "1348159",
    "end": "1353840"
  },
  {
    "text": "but for today's discussion we're going to focus on these ingest s3 great obviously great",
    "start": "1353840",
    "end": "1359520"
  },
  {
    "text": "candidate for near infinite amount of storage for metadata processing we're going to have",
    "start": "1359520",
    "end": "1364799"
  },
  {
    "text": "to introduce some vertical media vertical components that right on top of ec2",
    "start": "1364799",
    "end": "1370400"
  },
  {
    "text": "i find it even easier if we outsource some of that some of the some of the building blocks",
    "start": "1370400",
    "end": "1376000"
  },
  {
    "text": "that are required to make that scalable in the form of auto scaling groups and load balancers and monitoring and logging by putting all of",
    "start": "1376000",
    "end": "1382559"
  },
  {
    "text": "that in bean stock so you can put your api servers behind bean stock creating renditions again elastic",
    "start": "1382559",
    "end": "1389679"
  },
  {
    "text": "transcoder is a great option here of course we also have partner solutions out there that can",
    "start": "1389679",
    "end": "1395520"
  },
  {
    "text": "hit the breadth of raw media types and catalog processing itself",
    "start": "1395520",
    "end": "1402080"
  },
  {
    "text": "i'm going to hit this in more detail in just a minute so let me come back to that storage and delivery of course we have",
    "start": "1402080",
    "end": "1409039"
  },
  {
    "text": "amazon s3 and integrated potentially with glacier where you can set up lifecycle policies to age some of the",
    "start": "1409039",
    "end": "1414960"
  },
  {
    "text": "content out to a lower cost tier of storage that's the s3 the glacier lifecycle integration",
    "start": "1414960",
    "end": "1421039"
  },
  {
    "text": "and then of course cloudfront for for integrated streaming uh progressive",
    "start": "1421039",
    "end": "1427919"
  },
  {
    "text": "and download streaming from from your source database okay as far as the actual core of this",
    "start": "1427919",
    "end": "1434480"
  },
  {
    "start": "1431000",
    "end": "1542000"
  },
  {
    "text": "discussion the damn catalog i'm recommending at least for this discussion that we talk about a",
    "start": "1434480",
    "end": "1441760"
  },
  {
    "text": "a uniform search interface that's that's actually enabled by two parts",
    "start": "1441760",
    "end": "1448000"
  },
  {
    "text": "dynamo would in my mind provide the aspects of administrative",
    "start": "1448000",
    "end": "1453200"
  },
  {
    "text": "entities or support administrative entities persisting information about users groups projects the mapping",
    "start": "1453200",
    "end": "1460000"
  },
  {
    "text": "between projects and content administrative entities i think this is a good candidate for dynamo dynamodb of course",
    "start": "1460000",
    "end": "1467039"
  },
  {
    "text": "has all the great capabilities of a managed nosql service offering",
    "start": "1467039",
    "end": "1473840"
  },
  {
    "text": "it's multi-easy in nature you don't have to worry about it you provision the iops",
    "start": "1473919",
    "end": "1479520"
  },
  {
    "text": "but from a data modeling perspective as far as the core content itself you know i talked about",
    "start": "1479520",
    "end": "1485520"
  },
  {
    "text": "administrative entities what about the core catalog data itself dynamodb makes a great candidate for",
    "start": "1485520",
    "end": "1492960"
  },
  {
    "text": "storing the breadth of your catalog metadata because it does make allowances for a varying set of",
    "start": "1492960",
    "end": "1500320"
  },
  {
    "text": "data models and metadata elements if your container a container b and container c all have different",
    "start": "1500320",
    "end": "1506240"
  },
  {
    "text": "definitions and different layouts whether it's xmp based or dublin core",
    "start": "1506240",
    "end": "1512080"
  },
  {
    "text": "or fcxml you're going to be able to utilize",
    "start": "1512080",
    "end": "1517159"
  },
  {
    "text": "dynamodb's very flexible no model no see no schema mechanism to kind of mash",
    "start": "1517159",
    "end": "1524480"
  },
  {
    "text": "those up together to normalize that into a flat data space and because we're not doing",
    "start": "1524480",
    "end": "1530159"
  },
  {
    "text": "heavy transactional work against this catalog it's really queries you know it's not as",
    "start": "1530159",
    "end": "1535440"
  },
  {
    "text": "critical necessarily to run this in a in a relational construct although you can",
    "start": "1535440",
    "end": "1543840"
  },
  {
    "start": "1542000",
    "end": "1591000"
  },
  {
    "text": "and then the other aspect is actually searching against that that entity that",
    "start": "1544240",
    "end": "1549679"
  },
  {
    "text": "catalog that integrated catalog entity cloud search of course is our managed search service it allows",
    "start": "1549679",
    "end": "1556240"
  },
  {
    "text": "you to go ahead and define indexes or indices which will allow us to search against",
    "start": "1556240",
    "end": "1561760"
  },
  {
    "text": "these various metadata elements and there's strong support for features such as",
    "start": "1561760",
    "end": "1567039"
  },
  {
    "text": "case folding stemming stopward removal setting up synonyms how many times have you seen h.264",
    "start": "1567039",
    "end": "1573520"
  },
  {
    "text": "spelled out h.264 or avc or advanced video coding what if i",
    "start": "1573520",
    "end": "1580000"
  },
  {
    "text": "could just aggregate all those so that when i search against one term i get hits",
    "start": "1580000",
    "end": "1585279"
  },
  {
    "text": "against all the relevant companion data sets cloud search will allow us to do this",
    "start": "1585279",
    "end": "1591840"
  },
  {
    "start": "1591000",
    "end": "1679000"
  },
  {
    "text": "finally some other goodies that might be possible in a damn solution at least in the one that i presented",
    "start": "1592400",
    "end": "1598880"
  },
  {
    "text": "they include use of broad use of the aws cli",
    "start": "1598880",
    "end": "1603919"
  },
  {
    "text": "the command line interface it's such a great tool set it's been incredibly enhanced in the last couple",
    "start": "1603919",
    "end": "1610559"
  },
  {
    "text": "months in fact the demonstration i showed you and all the backend infrastructure that we hope to open source is",
    "start": "1610559",
    "end": "1616559"
  },
  {
    "text": "less than 400 lines of script code built on top of the cli fantastic stuff",
    "start": "1616559",
    "end": "1622960"
  },
  {
    "text": "some open source code open source decode utilities to include the use of media info and",
    "start": "1622960",
    "end": "1629120"
  },
  {
    "text": "exif tool um these are great for extracting and peeking at the technical metadata behind",
    "start": "1629120",
    "end": "1634720"
  },
  {
    "text": "these media assets there's others of course there's apache tikka there's there's",
    "start": "1634720",
    "end": "1641679"
  },
  {
    "text": "ffm peg even but these are these tools i have found to be very good at picking up some of the more subtle data",
    "start": "1641679",
    "end": "1650080"
  },
  {
    "text": "elements in these metadata fields we're using talend uh for some of the",
    "start": "1650080",
    "end": "1656480"
  },
  {
    "text": "complex etl pipeline work to be able to transform some of the metadata models between",
    "start": "1656480",
    "end": "1661919"
  },
  {
    "text": "the different container types that's just a representative solution and then finally from a front-end",
    "start": "1661919",
    "end": "1667840"
  },
  {
    "text": "perspective we're using node.js the aw aws node sdk",
    "start": "1667840",
    "end": "1673279"
  },
  {
    "text": "and for those of you who care angular angularjs",
    "start": "1673279",
    "end": "1678720"
  },
  {
    "start": "1679000",
    "end": "1779000"
  },
  {
    "text": "finally let's go ahead and build this out from a technical perspective this is the architecture enablers all wired",
    "start": "1680240",
    "end": "1687919"
  },
  {
    "text": "together you can see the different piece parts as you might expect s3 providing that storage backing",
    "start": "1687919",
    "end": "1695200"
  },
  {
    "text": "we have a dedicated a singleton in this case a singleton ec2",
    "start": "1695200",
    "end": "1700320"
  },
  {
    "text": "node auto scaling group one max one min one which is basically monitoring the state",
    "start": "1700320",
    "end": "1705679"
  },
  {
    "text": "of ingest activity on s3 buckets that have been designated by the organization",
    "start": "1705679",
    "end": "1710799"
  },
  {
    "text": "it then goes ahead it finds new content being ingested into the system and sends a tickler through the",
    "start": "1710799",
    "end": "1717679"
  },
  {
    "text": "middleware stream through sns sns has two sqs queues which are wired",
    "start": "1717679",
    "end": "1722960"
  },
  {
    "text": "up to the sns inbound topic every time there's a new event the",
    "start": "1722960",
    "end": "1728399"
  },
  {
    "text": "rendition jobs get kicked off and the queue excuse me the metadata processing jobs get kicked off",
    "start": "1728399",
    "end": "1735360"
  },
  {
    "text": "and then of course we have some ec2 instances with the appropriate bit of software that we talked about such as",
    "start": "1735360",
    "end": "1740559"
  },
  {
    "text": "media info exif tools and the aws cli which enable the rest of our architecture the key",
    "start": "1740559",
    "end": "1748320"
  },
  {
    "text": "piece being extracting that metadata normalizing it and then persisting a copy to dynamodb",
    "start": "1748320",
    "end": "1754880"
  },
  {
    "text": "and then using some of the new cli features synchronizing the record between dynamo and cloud search",
    "start": "1754880",
    "end": "1762000"
  },
  {
    "text": "providing a unified api for users portal experiences",
    "start": "1762000",
    "end": "1767039"
  },
  {
    "text": "so that when they do hit this box they really don't care if they're hitting dynamo or cloud search they know they're getting fast",
    "start": "1767039",
    "end": "1773960"
  },
  {
    "text": "multi-dimensional queries answered very quickly",
    "start": "1773960",
    "end": "1781840"
  },
  {
    "start": "1779000",
    "end": "1919000"
  },
  {
    "text": "there are parts of this architecture that i'd like to show you and i do apologize we are running a little",
    "start": "1784399",
    "end": "1790640"
  },
  {
    "text": "behind so i will speed things up here so we can get to jonathan's section",
    "start": "1790640",
    "end": "1796559"
  },
  {
    "text": "but i'm going to go ahead and",
    "start": "1796960",
    "end": "1802720"
  },
  {
    "text": "log into ec2",
    "start": "1804880",
    "end": "1809840"
  },
  {
    "text": "okay maybe not let me check my security",
    "start": "1811360",
    "end": "1816880"
  },
  {
    "text": "groups",
    "start": "1820840",
    "end": "1823840"
  },
  {
    "text": "i apologize folks we're just going to check our security group settings here real quick",
    "start": "1838000",
    "end": "1851840"
  },
  {
    "text": "10.32 and if we don't get this in one second",
    "start": "1858360",
    "end": "1863679"
  },
  {
    "text": "here i'm just going to go ahead and punt it",
    "start": "1863679",
    "end": "1867440"
  },
  {
    "text": "of course our address got switched",
    "start": "1869679",
    "end": "1879840"
  },
  {
    "text": "brain surgery on the fly all right apply it",
    "start": "1884559",
    "end": "1893840"
  },
  {
    "text": "all right let's try that one more time",
    "start": "1896799",
    "end": "1900480"
  },
  {
    "text": "okay let's go ahead let's just move forward i do apologize for that what",
    "start": "1904399",
    "end": "1912080"
  },
  {
    "text": "what we can do is we can show you at the end of the metadata extraction process",
    "start": "1912080",
    "end": "1918720"
  },
  {
    "text": "here here's the quick walk through we we basically set up the buckets we",
    "start": "1918720",
    "end": "1924240"
  },
  {
    "start": "1919000",
    "end": "1958000"
  },
  {
    "text": "make sure that we have a staging area for receiving content from edge locations we wire up we define an",
    "start": "1924240",
    "end": "1931279"
  },
  {
    "text": "sns object and wired it up to 2sqsqs we create a dynamodb and cloud search",
    "start": "1931279",
    "end": "1938960"
  },
  {
    "text": "index we create a dynamodb table using a defined skeleton of a schema as a starting point",
    "start": "1938960",
    "end": "1945360"
  },
  {
    "text": "you know we are talking about a no schema solution or a nosql solution but we do have to start with something and",
    "start": "1945360",
    "end": "1951039"
  },
  {
    "text": "in this regard i recommend using a combination of xmp and dublin core as our starting point",
    "start": "1951039",
    "end": "1957200"
  },
  {
    "text": "skeleton once those are all set up and wired together as we step through",
    "start": "1957200",
    "end": "1965120"
  },
  {
    "start": "1958000",
    "end": "1994000"
  },
  {
    "text": "the workflow the system will monitor the ingest of new files getting copied up to the cloud",
    "start": "1965120",
    "end": "1972000"
  },
  {
    "text": "we have an ec2 worker which is running in an auto scaling group multi-az it scans s3 for the staging content",
    "start": "1972000",
    "end": "1979760"
  },
  {
    "text": "and then it moves the content to another bucket and informs the rendition threads to start to start",
    "start": "1979760",
    "end": "1986559"
  },
  {
    "text": "their job",
    "start": "1986559",
    "end": "1991840"
  },
  {
    "start": "1994000",
    "end": "2040000"
  },
  {
    "text": "the the workers for processing the metadata receive event notification through sns",
    "start": "1995440",
    "end": "2001919"
  },
  {
    "text": "and sqs they're just constantly pulling the sqs queue looking for new work",
    "start": "2001919",
    "end": "2007600"
  },
  {
    "text": "in that queue message is a tickler along with a pointer to the actual content",
    "start": "2007600",
    "end": "2012960"
  },
  {
    "text": "an s3 uri that worker picks up that new content and broadcasts",
    "start": "2012960",
    "end": "2019360"
  },
  {
    "text": "a uh a new event so that the metadata extractors that are running down down down range",
    "start": "2019360",
    "end": "2026240"
  },
  {
    "text": "can pick up that work and run exif tool and media info",
    "start": "2026240",
    "end": "2031360"
  },
  {
    "text": "to process the header data and store that in dynamodb and cloud search",
    "start": "2031360",
    "end": "2037679"
  },
  {
    "start": "2040000",
    "end": "2093000"
  },
  {
    "text": "one part that's pretty challenging is normalizing the content that's coming from these open source utilities",
    "start": "2041600",
    "end": "2048079"
  },
  {
    "text": "as they extract all the technical metadata from these various container types we need to normalize",
    "start": "2048079",
    "end": "2054079"
  },
  {
    "text": "that and make it palatable for dynamodb to be able to ingest and this is where we utilize a tool like",
    "start": "2054079",
    "end": "2061358"
  },
  {
    "text": "talend talend or any other",
    "start": "2061359",
    "end": "2066480"
  },
  {
    "text": "comparable tool set to help us do some of the complex joining and transformation of the model",
    "start": "2066480",
    "end": "2072158"
  },
  {
    "text": "mediation process that's a lot to say what we're really saying is given various container types",
    "start": "2072159",
    "end": "2078638"
  },
  {
    "text": "given given different tools that extract that content and the layout they put these data data fields in we need to be able to",
    "start": "2078639",
    "end": "2085520"
  },
  {
    "text": "mash them up together normalize it into a manner that's friendly to our data model",
    "start": "2085520",
    "end": "2092800"
  },
  {
    "text": "so we're running media info we're running exif tool we're writing the aws",
    "start": "2094079",
    "end": "2099520"
  },
  {
    "text": "cli and we're running uh talend on ec2",
    "start": "2099520",
    "end": "2105040"
  },
  {
    "text": "once that data has been injected into dynamo it's really just a matter of hitting",
    "start": "2105040",
    "end": "2112160"
  },
  {
    "text": "hitting an extra cli call to synchronize dynamo with cloud surge and then making that search available",
    "start": "2112160",
    "end": "2118240"
  },
  {
    "text": "through an http call and my last a little bit here before i hand it over to jonathan is to show you",
    "start": "2118240",
    "end": "2124240"
  },
  {
    "text": "what that final result looks like that final record that gets stored",
    "start": "2124240",
    "end": "2130400"
  },
  {
    "text": "in cloud search is available through this through an http url",
    "start": "2130400",
    "end": "2135760"
  },
  {
    "text": "it's a simple get experience and if i go to i'm just suck here",
    "start": "2135760",
    "end": "2144480"
  },
  {
    "text": "i actually use a restful test client you could use curl but i already have",
    "start": "2144480",
    "end": "2152000"
  },
  {
    "text": "the actual query that we generate that's part of the protocol that cloud search expects already filled in",
    "start": "2152000",
    "end": "2160320"
  },
  {
    "text": "i click submit and basically what comes back are a number of json elements 28 records",
    "start": "2160320",
    "end": "2167680"
  },
  {
    "text": "and each record has a number of core elements and that's what gets processed",
    "start": "2167680",
    "end": "2172720"
  },
  {
    "text": "and presented in the user experience",
    "start": "2172720",
    "end": "2176480"
  },
  {
    "start": "2180000",
    "end": "2198000"
  },
  {
    "text": "so with that i'm going to go ahead and turn it over to jonathan for a discussion on some real world examples",
    "start": "2180960",
    "end": "2186480"
  },
  {
    "text": "of how they've applied the use of cloud search and other enablers in in making this",
    "start": "2186480",
    "end": "2192480"
  },
  {
    "text": "solution successful awesome thanks",
    "start": "2192480",
    "end": "2197680"
  },
  {
    "start": "2198000",
    "end": "2256000"
  },
  {
    "text": "so i'm jonathan rivers i'm the director of technical operations at pbs i want to talk to you about our",
    "start": "2199440",
    "end": "2206720"
  },
  {
    "text": "cms and dam system we call it merlin merlin is our primary authoritative",
    "start": "2206720",
    "end": "2213280"
  },
  {
    "text": "metadata repository so it is both the back end system as well as the",
    "start": "2213280",
    "end": "2218880"
  },
  {
    "text": "front end system that our folks get their uh their media into the system with so",
    "start": "2218880",
    "end": "2225200"
  },
  {
    "text": "it's uh structured metadata we bring in about 200 web objects daily",
    "start": "2225200",
    "end": "2231440"
  },
  {
    "text": "we've got about 30 000 web objects in the system they ingest about 150 video objects",
    "start": "2231440",
    "end": "2237599"
  },
  {
    "text": "daily we have 91 000 videos in the system now we generate around",
    "start": "2237599",
    "end": "2245200"
  },
  {
    "text": "200 hours of video every week going into the system we have users from",
    "start": "2245200",
    "end": "2251040"
  },
  {
    "text": "over 150 of our member stations and 30 national producers",
    "start": "2251040",
    "end": "2257119"
  },
  {
    "start": "2256000",
    "end": "2283000"
  },
  {
    "text": "so what does it do it's a large multi-tenant system it has about 1200 registered users",
    "start": "2257119",
    "end": "2262240"
  },
  {
    "text": "this is the system that gets all of the video into our video portal to our",
    "start": "2262240",
    "end": "2268079"
  },
  {
    "text": "over-the-top applications to our mobile clients everything does",
    "start": "2268079",
    "end": "2273280"
  },
  {
    "text": "about 250 million streams per month we service about 20 million unique viewers we deliver about eight petabyte",
    "start": "2273280",
    "end": "2280560"
  },
  {
    "text": "of video monthly so how do we get the data in",
    "start": "2280560",
    "end": "2287280"
  },
  {
    "start": "2283000",
    "end": "2328000"
  },
  {
    "text": "we're moving increasingly towards automation so for a lot of the web content the urls",
    "start": "2287280",
    "end": "2294240"
  },
  {
    "text": "instead of crawling them we use rss feeds that are programmatically ingested",
    "start": "2294240",
    "end": "2300400"
  },
  {
    "text": "and set up by the content editors we have a batch video ingest api so instead of a user having to go in",
    "start": "2300400",
    "end": "2306880"
  },
  {
    "text": "manually set up all of their metadata say i want this image this video they can just go ahead",
    "start": "2306880",
    "end": "2312720"
  },
  {
    "text": "and post that to an api and let the system go ahead and do it so that way they can have",
    "start": "2312720",
    "end": "2318000"
  },
  {
    "text": "programmatic workflows uh to to get the stuff in a little bit faster or finally they can go ahead and do",
    "start": "2318000",
    "end": "2325040"
  },
  {
    "text": "manually entered videos in the system this is a quick",
    "start": "2325040",
    "end": "2330320"
  },
  {
    "start": "2328000",
    "end": "2388000"
  },
  {
    "text": "overview of of how merlin works at a very sort of high level on the left",
    "start": "2330320",
    "end": "2336160"
  },
  {
    "text": "you have all of the the inputs so the the user input the ingest",
    "start": "2336160",
    "end": "2341599"
  },
  {
    "text": "api and the rss and that's coming into to merlin merlin is all run in ec2",
    "start": "2341599",
    "end": "2348400"
  },
  {
    "text": "in auto scaling arrays it is backed by an rds utilizing their read-only slaves",
    "start": "2348400",
    "end": "2354800"
  },
  {
    "text": "so when content comes in it's handed over to the workflow service which has",
    "start": "2354800",
    "end": "2360079"
  },
  {
    "text": "image transformation video transformation transcoding it's storing all of the",
    "start": "2360079",
    "end": "2366000"
  },
  {
    "text": "files in s3 and fronted by a cdn and then registers with our content apis there",
    "start": "2366000",
    "end": "2372480"
  },
  {
    "text": "are actually three different apis that power merlin and we'll talk",
    "start": "2372480",
    "end": "2377599"
  },
  {
    "text": "about that in a little bit and then finally there's a search utility service that goes and pulls all of those apis gets the information about",
    "start": "2377599",
    "end": "2384320"
  },
  {
    "text": "the content and throws it into cloud search so talking about the basic workflow for",
    "start": "2384320",
    "end": "2390800"
  },
  {
    "start": "2388000",
    "end": "2445000"
  },
  {
    "text": "a minute the object is is registered with merlin a user goes in they create a video they",
    "start": "2390800",
    "end": "2396079"
  },
  {
    "text": "use an api so there are three basic things that are",
    "start": "2396079",
    "end": "2401280"
  },
  {
    "text": "required for any piece of video content in pbs it is either it is a mezzanine video a",
    "start": "2401280",
    "end": "2408000"
  },
  {
    "text": "closed captioning file not",
    "start": "2408000",
    "end": "2410800"
  },
  {
    "text": "that are available into the system and then those are stored in s3 and fronted by",
    "start": "2423520",
    "end": "2429680"
  },
  {
    "text": "cloudfront or another cdn and then once all of that image transcoding and video",
    "start": "2429680",
    "end": "2436079"
  },
  {
    "text": "transcoding has been done it then registers with the the three different apis",
    "start": "2436079",
    "end": "2441359"
  },
  {
    "text": "and makes the objects sort of discoverable so making it discoverable to all of the",
    "start": "2441359",
    "end": "2449040"
  },
  {
    "text": "clients that consume it when i talk about the the cloud search clients we're talking about our video portal our over-the-top",
    "start": "2449040",
    "end": "2456240"
  },
  {
    "text": "applications like xbox uh our mobile applications rather than actually the digital asset",
    "start": "2456240",
    "end": "2463359"
  },
  {
    "text": "management system itself that's handled by simple django search everybody knows what they're looking for they know what they created",
    "start": "2463359",
    "end": "2469599"
  },
  {
    "text": "so there's not really a search problem there the search utility service runs every hour and when it runs it actually",
    "start": "2469599",
    "end": "2476960"
  },
  {
    "text": "pulls about three hours of data our apis are all pretty heavily cached",
    "start": "2476960",
    "end": "2482160"
  },
  {
    "text": "so it's possible when it runs it might not have the the the last set of updates so we always",
    "start": "2482160",
    "end": "2487280"
  },
  {
    "text": "want to make sure that we're going back looking at the modified times and getting any records that we don't have",
    "start": "2487280",
    "end": "2493520"
  },
  {
    "text": "so it hits those apis uses the modified time and then updates our amazon cloud search index we're",
    "start": "2493520",
    "end": "2500319"
  },
  {
    "text": "running two indexes primarily uh one for all of the merlin content all of the video content and the second one",
    "start": "2500319",
    "end": "2506720"
  },
  {
    "text": "for our shop objects that are actually populated by ingesting a spreadsheet so",
    "start": "2506720",
    "end": "2514720"
  },
  {
    "start": "2513000",
    "end": "2661000"
  },
  {
    "text": "a couple of things for pbs when we're talking about this the the cloud search implementation and",
    "start": "2514720",
    "end": "2520880"
  },
  {
    "text": "and making the the uh content discoverable we we needed hidden objects so we had to have the",
    "start": "2520880",
    "end": "2527599"
  },
  {
    "text": "ability to have objects in the system but not have them in the search results this is sometimes done for embargoed",
    "start": "2527599",
    "end": "2534000"
  },
  {
    "text": "videos some things for our press tour or maybe just even internal content right some of our our",
    "start": "2534000",
    "end": "2539839"
  },
  {
    "text": "internal training videos things like that are in the system and can be consumed by some clients but we don't want them showing up in search",
    "start": "2539839",
    "end": "2546960"
  },
  {
    "text": "a second is is rights management some videos are only available in certain places so they",
    "start": "2546960",
    "end": "2553440"
  },
  {
    "text": "might be available u.s only they might be restricted just to the uk due to a licensing",
    "start": "2553440",
    "end": "2560800"
  },
  {
    "text": "arrangement or in some cases available only in a single state so we needed ability to to tailor these",
    "start": "2560800",
    "end": "2568480"
  },
  {
    "text": "results so that users are only getting content that they can see and that's sort of dovetailing into the",
    "start": "2568480",
    "end": "2575599"
  },
  {
    "text": "to the partition search we run video portals for all of our stations and what we want is when a",
    "start": "2575599",
    "end": "2581839"
  },
  {
    "text": "user comes in and does a search on the video portal that they're going to get results",
    "start": "2581839",
    "end": "2587280"
  },
  {
    "text": "that are local to them so they want to see the shows that are on their member station and maybe not another member station",
    "start": "2587280",
    "end": "2593839"
  },
  {
    "text": "right so if somebody's from minnesota they might not be interested in the content on on idaho or virginia's",
    "start": "2593839",
    "end": "2600319"
  },
  {
    "text": "uh sites so the other big thing that it allowed us to do which is which",
    "start": "2600319",
    "end": "2607040"
  },
  {
    "text": "is really the the big deal is it allowed us to flatten our data model so we have three different apis uh that",
    "start": "2607040",
    "end": "2614240"
  },
  {
    "text": "are being pulled for small pieces of the data and",
    "start": "2614240",
    "end": "2619359"
  },
  {
    "text": "by flattening it in cloud search it allows us to build one record that has all of the information that we need so",
    "start": "2619359",
    "end": "2625599"
  },
  {
    "text": "that way any client that's consuming it then doesn't actually have to go make three separate calls they can just call",
    "start": "2625599",
    "end": "2631440"
  },
  {
    "text": "cloud search get one fully formed record and not have to deal with the overhead of doing that on themselves",
    "start": "2631440",
    "end": "2638480"
  },
  {
    "text": "so from there the other big search consideration was we found that when our viewers are coming to the",
    "start": "2638480",
    "end": "2645920"
  },
  {
    "text": "site they're looking for specific programs and we and and that made our life really easy it made uh",
    "start": "2645920",
    "end": "2652319"
  },
  {
    "text": "it made it really suitable for for structured data they're just looking for a program title",
    "start": "2652319",
    "end": "2657599"
  },
  {
    "text": "and then that's generally what we're giving them back so challenges of implementing cloud",
    "start": "2657599",
    "end": "2663839"
  },
  {
    "start": "2661000",
    "end": "2892000"
  },
  {
    "text": "search for us they they weren't uh particularly legion uh or or difficult there's no native time field",
    "start": "2663839",
    "end": "2670640"
  },
  {
    "text": "so we needed to convert all of the dates to integers and use epic time and then the versioning of the documents",
    "start": "2670640",
    "end": "2676960"
  },
  {
    "text": "was a little a little rough for us as well and again we use epic time for that",
    "start": "2676960",
    "end": "2683520"
  },
  {
    "text": "and then we needed to go ahead and basically have two version of most fields one that was just tech searchable and",
    "start": "2683520",
    "end": "2690560"
  },
  {
    "text": "another's faceted for ease of search implementations",
    "start": "2690560",
    "end": "2696800"
  },
  {
    "text": "so i want to talk a moment for the actual search consumers themselves so on pbs.org when somebody is is",
    "start": "2696800",
    "end": "2704319"
  },
  {
    "text": "performing a search they might search for masterpiece or or don abbey for example and what we",
    "start": "2704319",
    "end": "2709440"
  },
  {
    "text": "want to do is pull all of those results that came in from those web feeds right so all of their site updates that",
    "start": "2709440",
    "end": "2716880"
  },
  {
    "text": "are handled in rss and imported into the system we're going to get them the results",
    "start": "2716880",
    "end": "2722640"
  },
  {
    "text": "in the center section and on the right hand side we're actually making a separate cloud search call",
    "start": "2722640",
    "end": "2727680"
  },
  {
    "text": "to that second index our shop index and returning objects uh like dvds books things that",
    "start": "2727680",
    "end": "2735040"
  },
  {
    "text": "are in the pbs shop so they can go ahead and purchase those if they want the second big consumer is",
    "start": "2735040",
    "end": "2742480"
  },
  {
    "text": "our video portal and the search here is handled in two main ways the the the video search",
    "start": "2742480",
    "end": "2748079"
  },
  {
    "text": "itself so they're they're looking for don abbey or or masterpiece and it's returning that program",
    "start": "2748079",
    "end": "2753760"
  },
  {
    "text": "the second place is is really sort of a neat thing for us we use it to generate our programs a to",
    "start": "2753760",
    "end": "2760240"
  },
  {
    "text": "z list so we've got a lot of different programs and we've had a lot of different programs over the",
    "start": "2760240",
    "end": "2765359"
  },
  {
    "text": "years and what this allowed us to do instead of having to manually curate a list of our programs so anytime the content team",
    "start": "2765359",
    "end": "2772079"
  },
  {
    "text": "dreams up a new program or station has a new program we don't actually have to go into a list and",
    "start": "2772079",
    "end": "2778000"
  },
  {
    "text": "somebody put it in and what the url is we're using cloud search to go ahead and generate that list so that it's always",
    "start": "2778000",
    "end": "2784319"
  },
  {
    "text": "up to date and somebody's not in there tinkering with it all the time",
    "start": "2784319",
    "end": "2789599"
  },
  {
    "text": "and finally we're using it for our xbox and our ott applications these are fairly",
    "start": "2789599",
    "end": "2795599"
  },
  {
    "text": "straightforward program searches that are returning results it's not",
    "start": "2795599",
    "end": "2802000"
  },
  {
    "text": "super complex so that's a a pretty good overview of",
    "start": "2802000",
    "end": "2809200"
  },
  {
    "text": "how we have all of that still got a couple of minutes left like 13 or so",
    "start": "2809200",
    "end": "2816079"
  },
  {
    "text": "so we'll uh just wrap it up real quick and then we'll leave we have room for a couple questions if anyone has questions",
    "start": "2816079",
    "end": "2822480"
  },
  {
    "text": "um so again hopefully hopefully with uh today's session you've seen that",
    "start": "2822480",
    "end": "2828400"
  },
  {
    "text": "a damn scalable cloud solution is not only possible it's it's enabled",
    "start": "2828400",
    "end": "2834560"
  },
  {
    "text": "through our services and implemented very successfully in uh in the context in the context of large operations like",
    "start": "2834560",
    "end": "2841520"
  },
  {
    "text": "pbs there's many services to choose from we offer one example of a reference",
    "start": "2841520",
    "end": "2847599"
  },
  {
    "text": "architecture that can be built out with a number of managed services to enable it such as s3",
    "start": "2847599",
    "end": "2854000"
  },
  {
    "text": "dynamodb cloud search and and finally we also talked about the",
    "start": "2854000",
    "end": "2859440"
  },
  {
    "text": "importance of recognizing that there are other components that sit outside the cloud ecosystem",
    "start": "2859440",
    "end": "2865920"
  },
  {
    "text": "that complement any objective solution and we should always consider that and",
    "start": "2865920",
    "end": "2871520"
  },
  {
    "text": "finally uh everything that we've talked about today is is highly scalable always uh always",
    "start": "2871520",
    "end": "2878160"
  },
  {
    "text": "wonderful to talk about the capabilities from an enterprise perspective and with jonathan's discussion at pbs",
    "start": "2878160",
    "end": "2884240"
  },
  {
    "text": "hopefully you could see how that's all real so thank you very much we're happy to",
    "start": "2884240",
    "end": "2889839"
  },
  {
    "text": "take questions at this point if you have any questions",
    "start": "2889839",
    "end": "2894720"
  }
]