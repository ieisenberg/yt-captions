[
  {
    "start": "0",
    "end": "21000"
  },
  {
    "text": "In this video, you'll see how you can use purpose-built AWS\ndata analytics services for your modern data architecture.",
    "start": "80",
    "end": "6000"
  },
  {
    "text": "With this architecture, you can ingest and store data using\nAmazon Simple Storage Service (Amazon S3) and Kinesis Firehose,",
    "start": "6445",
    "end": "13394"
  },
  {
    "text": "catalog the data using AWS Glue and transform it using Amazon EMR,\nand query and analyze the data using Amazon Athena.",
    "start": "13394",
    "end": "20728"
  },
  {
    "start": "21000",
    "end": "48000"
  },
  {
    "text": "To get started with our data lake, we're going to upload a   dataset\ninto our previously-created S3 bucket 'reference data' folder.",
    "start": "21792",
    "end": "27632"
  },
  {
    "text": "This dataset is a JSON file \ncomposed of song artists and tracks.",
    "start": "29016",
    "end": "32464"
  },
  {
    "text": "The destination path is the reference_data folder.",
    "start": "35883",
    "end": "38283"
  },
  {
    "text": "Now that the upload is complete, let's quickly visit the\ndestination folder to see the dataset object.",
    "start": "41495",
    "end": "46055"
  },
  {
    "start": "48000",
    "end": "220000"
  },
  {
    "text": "The track_lists JSON file appears \nin the reference_data subfolder.",
    "start": "48490",
    "end": "52090"
  },
  {
    "text": "Let's move on and access Amazon Kinesis,",
    "start": "54867",
    "end": "57426"
  },
  {
    "text": "where we'll create a data delivery stream to \nour S3 bucket using Kinesis Data Firehose.",
    "start": "57426",
    "end": "62000"
  },
  {
    "text": "We'll use a Direct PUT source and \nspecify Amazon S3 as the destination.",
    "start": "62419",
    "end": "66499"
  },
  {
    "text": "Let's name the delivery stream.",
    "start": "72466",
    "end": "73940"
  },
  {
    "text": "Let's confirm that the data transformation and \nrecord format conversion options are disabled.",
    "start": "78866",
    "end": "82982"
  },
  {
    "text": "We'll choose the path for the \nS3 bucket we previously created.",
    "start": "86146",
    "end": "88946"
  },
  {
    "text": "Let's use the prefix \"raw\" so it’s clear which folder in our bucket\nreceives the raw data from the delivery stream.",
    "start": "94477",
    "end": "99840"
  },
  {
    "text": "We can also adjust the default buffer options.",
    "start": "100912",
    "end": "103232"
  },
  {
    "text": "A lower buffer size will be faster in delivery.",
    "start": "105450",
    "end": "107850"
  },
  {
    "text": "A lower interval sends data more frequently.",
    "start": "110341",
    "end": "112651"
  },
  {
    "text": "In the Advanced settings, we'll check that \nAmazon CloudWatch error logging is enabled.",
    "start": "115861",
    "end": "120261"
  },
  {
    "text": "Let's create the delivery stream.",
    "start": "123357",
    "end": "124797"
  },
  {
    "text": "While the delivery stream is being \ncreated, we can review its configuration.",
    "start": "126066",
    "end": "129586"
  },
  {
    "text": "Here we see the S3 bucket where the \ndelivery stream will dump the data.",
    "start": "134055",
    "end": "137254"
  },
  {
    "text": "Now the delivery stream has an \"Active\" status.",
    "start": "139999",
    "end": "142454"
  },
  {
    "text": "Next, we need a user pool that can access the \ndelivery stream to generate and produce data.",
    "start": "143520",
    "end": "147826"
  },
  {
    "text": "Let's create a CloudFormation stack to establish \nan Amazon Cognito user pool with a single user.",
    "start": "148591",
    "end": "153564"
  },
  {
    "text": "We'll use an Amazon S3 template.",
    "start": "156000",
    "end": "157771"
  },
  {
    "text": "Let's specify a username \nand password for the stack.  ",
    "start": "160843",
    "end": "163504"
  },
  {
    "text": "We'll use these credentials later when \nwe access the Kinesis data generator.",
    "start": "164123",
    "end": "167471"
  },
  {
    "text": "We'll retain the default stack options.",
    "start": "169837",
    "end": "171677"
  },
  {
    "text": "Let's create the stack.",
    "start": "174819",
    "end": "175859"
  },
  {
    "text": "After a few minutes, the stack is created.",
    "start": "181347",
    "end": "183380"
  },
  {
    "text": "On the Outputs tab, we can find \nthe URL for our data generator.",
    "start": "185655",
    "end": "189175"
  },
  {
    "text": "We'll log in using the username and \npassword we specified for the stack.",
    "start": "190637",
    "end": "194077"
  },
  {
    "text": "We’ll also select the Region \nwhere our S3 bucket is located.",
    "start": "196168",
    "end": "199048"
  },
  {
    "text": "Let’s adjust the number of records per second.",
    "start": "201139",
    "end": "203139"
  },
  {
    "text": "We're using a simple script that will pump \nin the records once we start sending data.",
    "start": "204935",
    "end": "208455"
  },
  {
    "text": "Let's start sending data.",
    "start": "210272",
    "end": "211537"
  },
  {
    "text": "Once the data generator completes its process,\nwe can visit our S3 bucket to confirm the data is present.",
    "start": "213221",
    "end": "218340"
  },
  {
    "text": "Here we can see all the data that was dumped \nfrom our delivery stream into our raw folder.",
    "start": "220637",
    "end": "224316"
  },
  {
    "text": "Now that we've ingested material into the data \nbucket, we’ll catalog the data using AWS Glue.",
    "start": "225370",
    "end": "230330"
  },
  {
    "text": "AWS Glue is a fully managed ETL \n(extract, transform, and load) service.",
    "start": "231059",
    "end": "235939"
  },
  {
    "text": "Here, we'll create a crawler to \nmine the data in the S3 bucket.",
    "start": "236797",
    "end": "239874"
  },
  {
    "text": "Before setting up the crawler, you'll want to ensure you've created\nan AWS Identity and Access Management (AWS IAM) role",
    "start": "240786",
    "end": "247470"
  },
  {
    "text": "with both S3FullAccess and GlueServiceRole policies enacted.",
    "start": "247470",
    "end": "251162"
  },
  {
    "text": "For the purposes of this demonstration, \nthat has already been set up.",
    "start": "251906",
    "end": "254706"
  },
  {
    "text": "First, we'll name the crawler.",
    "start": "255506",
    "end": "256706"
  },
  {
    "text": "We can retain the source type criteria.",
    "start": "260946",
    "end": "262786"
  },
  {
    "text": "Here, we’ll specify the path for the S3 bucket.",
    "start": "265711",
    "end": "268210"
  },
  {
    "text": "We won’t add another data store.",
    "start": "273723",
    "end": "275242"
  },
  {
    "text": "We'll specify the IAM role we previously created.",
    "start": "276000",
    "end": "278692"
  },
  {
    "text": "We'll have our crawler run on demand.",
    "start": "283859",
    "end": "285619"
  },
  {
    "text": "Let's add a database to store \nthe data the crawler captures.",
    "start": "286397",
    "end": "289229"
  },
  {
    "text": "Now we’ll quickly review our specifications \nand finish creating the crawler.",
    "start": "295496",
    "end": "299059"
  },
  {
    "text": "Here’s the crawler. Let’s run it.",
    "start": "300752",
    "end": "302352"
  },
  {
    "text": "The crawler has stopped running. As you can \nsee, it added two tables to our database.",
    "start": "305563",
    "end": "309705"
  },
  {
    "text": "Let’s look at this table.",
    "start": "315586",
    "end": "316626"
  },
  {
    "text": "This is the raw data captured by the crawler.",
    "start": "320203",
    "end": "322460"
  },
  {
    "text": "Now that the data has been cataloged in AWS \nGlue, we can query the data using Amazon Athena.",
    "start": "324935",
    "end": "330055"
  },
  {
    "text": "Let’s explore the query editor.",
    "start": "331003",
    "end": "332522"
  },
  {
    "text": "We’ll select the database we just created.",
    "start": "335517",
    "end": "337560"
  },
  {
    "text": "As you can see, the two tables stored in \nthe database are present for querying.",
    "start": "342432",
    "end": "346032"
  },
  {
    "text": "We’ll use this simple code to \nquery a column from the raw table.",
    "start": "348603",
    "end": "351615"
  },
  {
    "text": "Before we run this query, we’ll need \nto set up a location for the results.  ",
    "start": "353163",
    "end": "356609"
  },
  {
    "text": "Let's save this query before we proceed.",
    "start": "356935",
    "end": "358988"
  },
  {
    "text": "Now let’s go to the query editor settings \nto set up a query result location.",
    "start": "365975",
    "end": "369698"
  },
  {
    "text": "We'll choose our S3 bucket.",
    "start": "372144",
    "end": "373504"
  },
  {
    "text": "By editing the S3 path directly, we can create \na new folder to store the query results.",
    "start": "376557",
    "end": "381343"
  },
  {
    "text": "Let's return to the query editor.",
    "start": "383322",
    "end": "384841"
  },
  {
    "text": "Let's run our first query.",
    "start": "389722",
    "end": "391035"
  },
  {
    "text": "Here we can see our query results.",
    "start": "394636",
    "end": "396315"
  },
  {
    "text": "Now that we've shown we can \nquery data directly in Athena,",
    "start": "398935",
    "end": "401815"
  },
  {
    "text": "let’s see how we can transform our data \nusing Amazon Elastic MapReduce (Amazon EMR).",
    "start": "401815",
    "end": "405000"
  },
  {
    "start": "408000",
    "end": "575000"
  },
  {
    "text": "Amazon EMR is a managed cluster platform that \nsimplifies running vast amounts of data on AWS.",
    "start": "408352",
    "end": "413660"
  },
  {
    "text": "We'll be creating a cluster on S3 and using PySpark jobs to perform\nsome transformation and aggregations on our raw data.",
    "start": "414328",
    "end": "420408"
  },
  {
    "text": "For the purposes of this demonstration, we have \nalready created two additional folders in our S3 bucket",
    "start": "420968",
    "end": "425434"
  },
  {
    "text": "to store the PySpark script and EMR \nlogs and uploaded the script accordingly.",
    "start": "425434",
    "end": "429470"
  },
  {
    "text": "Let's start by creating the cluster.",
    "start": "430088",
    "end": "431688"
  },
  {
    "text": "We’ll name the cluster and select \nthe path for the log file folder.",
    "start": "432717",
    "end": "435761"
  },
  {
    "text": "We'll use a step execution launch mode.",
    "start": "444144",
    "end": "446144"
  },
  {
    "text": "Let's configure a Spark application step.",
    "start": "448659",
    "end": "450738"
  },
  {
    "text": "We’ll choose the path location \nwith the PySpark script file.",
    "start": "454705",
    "end": "457513"
  },
  {
    "text": "We'll also supply an argument that \npasses the script to the S3 bucket.",
    "start": "464533",
    "end": "468052"
  },
  {
    "text": "If the cluster fails, we’ll terminate it.",
    "start": "471000",
    "end": "472920"
  },
  {
    "text": "Let's add the step.",
    "start": "475220",
    "end": "476158"
  },
  {
    "text": "Now we’ll review the configuration.",
    "start": "477919",
    "end": "479698"
  },
  {
    "text": "As you can see, the cluster will have \nthree instances. Let's create it.",
    "start": "480912",
    "end": "484512"
  },
  {
    "text": "We can view the cluster's progress as it executes.",
    "start": "486877",
    "end": "489352"
  },
  {
    "text": "Once all steps have been completed, \nthe cluster will terminate.",
    "start": "492000",
    "end": "494786"
  },
  {
    "text": "We can see that the cluster \ncompleted the two applications.",
    "start": "497151",
    "end": "499812"
  },
  {
    "text": "Let's go to the S3 bucket to \nconfirm the dataset is present.",
    "start": "500775",
    "end": "503630"
  },
  {
    "text": "We can see that the processed \ndata was saved in Parquet format.",
    "start": "511037",
    "end": "514237"
  },
  {
    "text": "Now let's visit AWS Glue and run the \ncrawler to catalog the transformed data.",
    "start": "515200",
    "end": "519593"
  },
  {
    "text": "Now that the crawler has finished \nrunning, let's view the database tables.",
    "start": "527850",
    "end": "531056"
  },
  {
    "text": "Here we can see the mined EMR processed data.",
    "start": "534545",
    "end": "536935"
  },
  {
    "text": "Now let's return to Athena \nand query the processed data.",
    "start": "538111",
    "end": "540831"
  },
  {
    "text": "Here’s the emr_processed_data table.",
    "start": "545117",
    "end": "547316"
  },
  {
    "text": "We'll run another simple query that returns \na list of artist names ordered by count.",
    "start": "548213",
    "end": "552133"
  },
  {
    "text": "As you can see, Data Lake architecture \nmakes it possible to ingest, catalog,",
    "start": "553141",
    "end": "557061"
  },
  {
    "text": "query, and analyze large volumes of \ndata to uncover business insights.",
    "start": "557061",
    "end": "560897"
  },
  {
    "text": "You've just seen how you can use purpose-built AWS data\nanalytics services for your modern data architecture.",
    "start": "561895",
    "end": "567335"
  },
  {
    "text": "You can learn more about this topic in \nthe description and links for this video.",
    "start": "568533",
    "end": "571579"
  },
  {
    "text": "Thanks for watching. Now it’s your turn to try.",
    "start": "571837",
    "end": "573894"
  }
]