[
  {
    "text": "good morning everybody or good afternoon depending on where you are happy Thursday thanks for joining our stage",
    "start": "3980",
    "end": "9629"
  },
  {
    "text": "maker webinar today we're going to cover the XG boost algorithm which I can tell is a very interesting topic for a lot of",
    "start": "9629",
    "end": "18150"
  },
  {
    "text": "people we had a great many of registrations and we've got a lot of you on the lines today so thanks for joining us",
    "start": "18150",
    "end": "24240"
  },
  {
    "text": "I am su e wig and I'm the AWS global partner marketing manager for AI and machine learning everybody on this",
    "start": "24240",
    "end": "30390"
  },
  {
    "text": "webinar today all of you belong to one of our consulting partners who've achieved the AWS machine learning",
    "start": "30390",
    "end": "36300"
  },
  {
    "text": "competencies so these are private webinars just for you to learn more about the algorithms that are associated",
    "start": "36300",
    "end": "42989"
  },
  {
    "text": "with sage maker and to ask any questions that you have our main presenter today is Chris Greenwich that's Chris with a K",
    "start": "42989",
    "end": "49829"
  },
  {
    "text": "he is our partner solution architect for AWS and he leads the machine learning",
    "start": "49829",
    "end": "54899"
  },
  {
    "text": "segment globally joining Chris also on the line is Pratap Rama Murthy who is",
    "start": "54899",
    "end": "60120"
  },
  {
    "text": "going to be taking your questions so he's kind of monitoring the question and answer panel that's on the right",
    "start": "60120",
    "end": "66000"
  },
  {
    "text": "navigation pane of the GoToWebinar interface during the webinar just pop",
    "start": "66000",
    "end": "71399"
  },
  {
    "text": "your questions in there so we can make sure that we address all of them during the call sometimes we don't get to all of them even though we have an hour and",
    "start": "71399",
    "end": "78000"
  },
  {
    "text": "45 minutes and in that case we will send an email afterward with some with",
    "start": "78000",
    "end": "83430"
  },
  {
    "text": "answers to those additional questions speaking of after the webinar we will",
    "start": "83430",
    "end": "88439"
  },
  {
    "text": "you will get the recording for today's webinar which includes the presentation so any of the materials that Chris shows",
    "start": "88439",
    "end": "94950"
  },
  {
    "text": "you today and let's just jump right in I know Chris has pumped up this is a",
    "start": "94950",
    "end": "101520"
  },
  {
    "text": "really popular algorithm and he's excited to share it with you so Chris take it away hello world this is Chris",
    "start": "101520",
    "end": "109649"
  },
  {
    "text": "green ak so we're going to talk about the ultimate weapon in machine learning",
    "start": "109649",
    "end": "115219"
  },
  {
    "text": "it is XG boost so why is that the ultimate weapon and the choice of so",
    "start": "115219",
    "end": "121380"
  },
  {
    "text": "many data scientists XG boosts which stands for extreme gradient boosting is",
    "start": "121380",
    "end": "128759"
  },
  {
    "text": "a popular and efficient open-source implementation of the gradient boosted",
    "start": "128759",
    "end": "133830"
  },
  {
    "text": "trees algorithm gradient boosting is a machine learning algorithm that attempts to accurately predict target",
    "start": "133830",
    "end": "140999"
  },
  {
    "text": "variables by combining the estimates of a set of simpler weaker models by",
    "start": "140999",
    "end": "147359"
  },
  {
    "text": "applying a gradient boosting to decision tree models in a highly scalable manner XG boost does remarkably well in machine",
    "start": "147359",
    "end": "155939"
  },
  {
    "text": "learning competitions and in business it's also it also robustly handles a",
    "start": "155939",
    "end": "161489"
  },
  {
    "text": "variety of data types relationships and distributions it provides a large number",
    "start": "161489",
    "end": "167099"
  },
  {
    "text": "of hyper parameter variables that can be used to tune and improve model performance this flexibility makes XG",
    "start": "167099",
    "end": "174299"
  },
  {
    "text": "boost a solid choice for various machine learning problems in particular working",
    "start": "174299",
    "end": "180060"
  },
  {
    "text": "on tabular data you may know XG boost",
    "start": "180060",
    "end": "185159"
  },
  {
    "text": "because it is pretty much the king of Kaggle that is where its name was really",
    "start": "185159",
    "end": "191790"
  },
  {
    "text": "made you could see on the chart there that between well 2013 just was had was",
    "start": "191790",
    "end": "198840"
  },
  {
    "text": "a remarkable year for XG boost in particular it won this Higgs boson",
    "start": "198840",
    "end": "205109"
  },
  {
    "text": "machine learning challenge just bring up that page very quickly and of course",
    "start": "205109",
    "end": "211169"
  },
  {
    "text": "you'll be the PowerPoint will be available to everyone on the call after the call but this is this is the one",
    "start": "211169",
    "end": "218040"
  },
  {
    "text": "that sort of put XG boost on the scene and it's what you know most people refer",
    "start": "218040",
    "end": "224549"
  },
  {
    "text": "to as it's sort of a starting point but it's notable that it won a number of",
    "start": "224549",
    "end": "230250"
  },
  {
    "text": "other competitions on Kaggle including the predicting house prices playground",
    "start": "230250",
    "end": "235379"
  },
  {
    "text": "out brain click prediction the all state claims severity competition Santander",
    "start": "235379",
    "end": "241799"
  },
  {
    "text": "product recommendation talking data mobile user democratic graphics the Red",
    "start": "241799",
    "end": "247769"
  },
  {
    "text": "Hat business value prediction and you can see that this is quite a broad spectrum of vertical markets that XG",
    "start": "247769",
    "end": "257009"
  },
  {
    "text": "Boost found some lovin but I will point out that if you take a look at that",
    "start": "257009",
    "end": "262860"
  },
  {
    "text": "chart which ends in 2016 that deep neural Nets began to overtake it there",
    "start": "262860",
    "end": "268699"
  },
  {
    "text": "so once again just qualify that XG boost is is really",
    "start": "268699",
    "end": "275080"
  },
  {
    "text": "really useful for predicting a number predicting a class based off of tabular",
    "start": "275080",
    "end": "280240"
  },
  {
    "text": "data but for image recognition and media recognition deep learnings still",
    "start": "280240",
    "end": "286779"
  },
  {
    "text": "certainly is the machine learning algorithm of choice so a little bit of",
    "start": "286779",
    "end": "293199"
  },
  {
    "text": "terminology first of all XG boost is a supervised",
    "start": "293199",
    "end": "298209"
  },
  {
    "text": "learning technique that means that your data is labeled importantly you've made",
    "start": "298209",
    "end": "303490"
  },
  {
    "text": "some observations about a thing and event or collected characteristics or",
    "start": "303490",
    "end": "309429"
  },
  {
    "text": "have used those characteristics to make additional features and for each set of those features you have given a label",
    "start": "309429",
    "end": "316269"
  },
  {
    "text": "once you've identified a label for an observation you can use many techniques to create a model to create predictions",
    "start": "316269",
    "end": "325629"
  },
  {
    "text": "importantly back propagation back propagation and its recent variant developed by Geoffrey Hinton at the",
    "start": "325629",
    "end": "332080"
  },
  {
    "text": "University of Toronto using GPUs is principally responsible for the current Renaissance of AI and machine learning",
    "start": "332080",
    "end": "340079"
  },
  {
    "text": "regression is a set of techniques for predicting a number classification is a",
    "start": "340079",
    "end": "346089"
  },
  {
    "text": "set of techniques for predicting a class or identifying a discrete object such as",
    "start": "346089",
    "end": "351159"
  },
  {
    "text": "a cat or dog a decision tree is a technique that uses a tree like graph of",
    "start": "351159",
    "end": "357099"
  },
  {
    "text": "decisions and their possible consequences including chance event outcomes excuse me resource costs and",
    "start": "357099",
    "end": "365649"
  },
  {
    "text": "utility it's one way to display an algorithm that only contains contains",
    "start": "365649",
    "end": "370869"
  },
  {
    "text": "conditional control statements finally ensemble learning the use of multiple",
    "start": "370869",
    "end": "378279"
  },
  {
    "text": "machine learning techniques in a model to obtain a better predictive performance than could be obtained with",
    "start": "378279",
    "end": "386199"
  },
  {
    "text": "just the Constituent learning algorithms alone he is generally referred to an",
    "start": "386199",
    "end": "391360"
  },
  {
    "text": "ensemble learning next bias and variance",
    "start": "391360",
    "end": "396419"
  },
  {
    "text": "error so these are important concepts because in the process of refining a",
    "start": "396419",
    "end": "403479"
  },
  {
    "text": "model if you take a look at these targets here I think they pretty well illustrate if",
    "start": "403479",
    "end": "409419"
  },
  {
    "text": "you just look horizontally around low bias you'll see that you have a very",
    "start": "409419",
    "end": "415660"
  },
  {
    "text": "tight cluster on the upper left-hand corner target and then a cluster that's",
    "start": "415660",
    "end": "420669"
  },
  {
    "text": "somewhat spread out that's a high variance low bias illustration but you",
    "start": "420669",
    "end": "426430"
  },
  {
    "text": "can see it's generally centered around the target if you look at the lower two targets a high bias with low variance is",
    "start": "426430",
    "end": "434650"
  },
  {
    "text": "clustered but it's not hitting the target and then high bias and high variance in the lower right is just you",
    "start": "434650",
    "end": "441280"
  },
  {
    "text": "know awful yeah so we're we're really talking about error and when we're refining a model",
    "start": "441280",
    "end": "447850"
  },
  {
    "text": "typically what we're looking at is being able to get as good an error figure or",
    "start": "447850",
    "end": "453910"
  },
  {
    "text": "as low an error ratio as possible so bias error is useful to quantify how",
    "start": "453910",
    "end": "459910"
  },
  {
    "text": "much on average our predicted value is different from an actual value a high",
    "start": "459910",
    "end": "466270"
  },
  {
    "text": "bias error means that we have an underperforming model and we keep missing important trends so variance on",
    "start": "466270",
    "end": "473050"
  },
  {
    "text": "the other side quantifies how are the predictions made on some observation different from each other a high",
    "start": "473050",
    "end": "480010"
  },
  {
    "text": "variance model will over fit on your training population and perform badly on",
    "start": "480010",
    "end": "485110"
  },
  {
    "text": "an observation beyond training fall and then boosting and excuse me so boosting",
    "start": "485110",
    "end": "493510"
  },
  {
    "text": "is a general technique for correcting bias errors and in particular adaptive",
    "start": "493510",
    "end": "499300"
  },
  {
    "text": "boosting and gradient boosting generally boosting algorithms allow fitting many",
    "start": "499300",
    "end": "506500"
  },
  {
    "text": "weak classifiers to re-weighted versions of the training data so a weak",
    "start": "506500",
    "end": "513400"
  },
  {
    "text": "classifier now that I've used that term is an algorithm that's slightly better than random guessing so so what does",
    "start": "513400",
    "end": "522760"
  },
  {
    "text": "this look like graphically so if you take a look at box one two and three you",
    "start": "522760",
    "end": "529180"
  },
  {
    "text": "can see that using some weak predictions on those boxes we are poorly classifying",
    "start": "529180",
    "end": "536950"
  },
  {
    "text": "the positives from the it is inbox 4 we've combined those techniques to create a much better",
    "start": "536950",
    "end": "543520"
  },
  {
    "text": "classifier now what is gradient boosting with gradient boosting we are taking",
    "start": "543520",
    "end": "550600"
  },
  {
    "text": "we're using well stochastic gradient descent to create a much more refined model to get to our target values now",
    "start": "550600",
    "end": "560560"
  },
  {
    "text": "decision trees generally and this is from the the initial XG boost paper",
    "start": "560560",
    "end": "567420"
  },
  {
    "text": "decision trees are essentially binary trees this particular tree illustrates how we",
    "start": "567420",
    "end": "574780"
  },
  {
    "text": "would predict the likelihood that a an individual was a gamer so here we see if",
    "start": "574780",
    "end": "583300"
  },
  {
    "text": "you are on the tree on the Left if you're less than 15 and you're male you have a very high probability if you're a",
    "start": "583300",
    "end": "589450"
  },
  {
    "text": "female it's still high but a little bit lower if you're older than 15 the",
    "start": "589450",
    "end": "594550"
  },
  {
    "text": "probability goes negative so that's pretty clear in reality these trees get",
    "start": "594550",
    "end": "600970"
  },
  {
    "text": "a lot more complex and one of the important aspects of any tree is",
    "start": "600970",
    "end": "608260"
  },
  {
    "text": "determining how many how deep you want the tree to go so in this illustration",
    "start": "608260",
    "end": "615000"
  },
  {
    "text": "right here we see the strong green line",
    "start": "615000",
    "end": "620290"
  },
  {
    "text": "has a depth of 1 and it is you know it's",
    "start": "620290",
    "end": "626050"
  },
  {
    "text": "it's clearly under fitting the data as we add more as we add more nodes to the",
    "start": "626050",
    "end": "632470"
  },
  {
    "text": "tree depth it gets more accurate but we do run the danger on the very smallest",
    "start": "632470",
    "end": "637960"
  },
  {
    "text": "depth line of 20 of extreme overfitting so that's all about trees let's talk",
    "start": "637960",
    "end": "645460"
  },
  {
    "text": "about ensemble learning so I heard this conversationally the other day and I",
    "start": "645460",
    "end": "651910"
  },
  {
    "text": "thought it was a pretty good example of what of how to understand ensembles so",
    "start": "651910",
    "end": "657310"
  },
  {
    "text": "let's say that you were considering buying a stock and you got",
    "start": "657310",
    "end": "663190"
  },
  {
    "text": "recommendations from 6 completely independent sources one was the employee",
    "start": "663190",
    "end": "668920"
  },
  {
    "text": "of the company and the company that you wanted to invest and say he was right 70% of the time the",
    "start": "668920",
    "end": "674980"
  },
  {
    "text": "second was a financial adviser of the company he's right 75% of the time the",
    "start": "674980",
    "end": "680920"
  },
  {
    "text": "third was a stock market trader he's right 70% you met an employee of a",
    "start": "680920",
    "end": "686410"
  },
  {
    "text": "competitor let's say he was right 60% of the time a market research firm in the same segment was right 75 percent of the",
    "start": "686410",
    "end": "693759"
  },
  {
    "text": "time and then finally you meet a social media expert and he's right 65 percent",
    "start": "693759",
    "end": "699819"
  },
  {
    "text": "of the time on his stock recommendations now I mentioned how often they were",
    "start": "699819",
    "end": "705129"
  },
  {
    "text": "right but let's consider how frequently they're wrong right so obviously the",
    "start": "705129",
    "end": "711279"
  },
  {
    "text": "employee of the companies right there he's right 70 percent of the time that means his error is 30% and if we add up",
    "start": "711279",
    "end": "717459"
  },
  {
    "text": "all their error rates and these are all completely independent opinions then we",
    "start": "717459",
    "end": "723310"
  },
  {
    "text": "and and they all conclude that you should buy this stock then you have a pretty high degree of confidence that",
    "start": "723310",
    "end": "730259"
  },
  {
    "text": "this is a good decision so 99.9 - that's that's pretty high what",
    "start": "730259",
    "end": "737620"
  },
  {
    "text": "if these people all know each other well you really have to throw that out the window these have to be complete and",
    "start": "737620",
    "end": "744660"
  },
  {
    "text": "independent opinions or else you you really can't create a good predictive",
    "start": "744660",
    "end": "750699"
  },
  {
    "text": "model so a little bit history on XG boost before we dig into the code it was",
    "start": "750699",
    "end": "757810"
  },
  {
    "text": "started by a graduate student T Anki Chen I looked at his web page yesterday",
    "start": "757810",
    "end": "765009"
  },
  {
    "text": "and he's still at the University of Washington of course as I mentioned they're famous for winning the Hicks",
    "start": "765009",
    "end": "771819"
  },
  {
    "text": "machine learning challenge in 2014 there are implementations and this is",
    "start": "771819",
    "end": "777490"
  },
  {
    "text": "separately managed by his group on in C++ Python our Java Scala Giulia and",
    "start": "777490",
    "end": "785740"
  },
  {
    "text": "even node so now let's focus on XG boost itself so gradient boosted trees work by",
    "start": "785740",
    "end": "794860"
  },
  {
    "text": "combining predictions from many simple models each of which tries to address the weakness of the previous model as I",
    "start": "794860",
    "end": "800949"
  },
  {
    "text": "as I've Illustrated so XG boost is an implement",
    "start": "800949",
    "end": "806230"
  },
  {
    "text": "of those boosted trees and what happens is as we go through cycles that",
    "start": "806230",
    "end": "811720"
  },
  {
    "text": "repeatedly build new models and combine them into an ensemble model we start the",
    "start": "811720",
    "end": "817930"
  },
  {
    "text": "cycle by taking an existing model and calculating errors for each observation",
    "start": "817930",
    "end": "823300"
  },
  {
    "text": "in the data set we then build a new model to predict these errors then we",
    "start": "823300",
    "end": "829390"
  },
  {
    "text": "add predictions from this error predicting model to the ensemble of models to make a prediction we add the",
    "start": "829390",
    "end": "836860"
  },
  {
    "text": "predictions from all the previous models and we can use these predictions to calculate new errors build the next",
    "start": "836860",
    "end": "843490"
  },
  {
    "text": "model and add it to the ensemble that's the cycle but there's one piece that's",
    "start": "843490",
    "end": "849420"
  },
  {
    "text": "missing we need a base prediction to get started in practice initial predictions",
    "start": "849420",
    "end": "855100"
  },
  {
    "text": "can be pretty naive even if it's predictions are wildly inaccurate subsequent additions to the ensemble",
    "start": "855100",
    "end": "862150"
  },
  {
    "text": "will address those errors so what are the advantages of X G boost probably the",
    "start": "862150",
    "end": "868960"
  },
  {
    "text": "top point has number one heterogeneous data think about a spreadsheet a sequel",
    "start": "868960",
    "end": "874450"
  },
  {
    "text": "statement you know you really don't need to go through all the effort of feature",
    "start": "874450",
    "end": "880720"
  },
  {
    "text": "scaling you don't have to do a lot of feature engineering I mean you should",
    "start": "880720",
    "end": "885810"
  },
  {
    "text": "but you don't have to it handles missing values and outliers it can deal with irrelevant inputs I've",
    "start": "885810",
    "end": "894790"
  },
  {
    "text": "seen a lot of tests where people just add noise to xgu boost models and the",
    "start": "894790",
    "end": "900910"
  },
  {
    "text": "final models are almost not affected it can handled mixed predictors both",
    "start": "900910",
    "end": "907930"
  },
  {
    "text": "qualitative and quantitative it also has support for a lot of different loss functions it can automatically detect",
    "start": "907930",
    "end": "914590"
  },
  {
    "text": "feature interactions it's very fast at prediction the results are interpretable",
    "start": "914590",
    "end": "920920"
  },
  {
    "text": "this is really valuable so interpret all that you can create independent rule sets it also distributes training in",
    "start": "920920",
    "end": "928750"
  },
  {
    "text": "training very easily which can reduce dramatically training time a few more",
    "start": "928750",
    "end": "935050"
  },
  {
    "text": "advantages so you to find the best so finding split algorithms to find it",
    "start": "935050",
    "end": "941920"
  },
  {
    "text": "uh split over a continuous feature data needs to be sorted and fit entirely into",
    "start": "941920",
    "end": "947819"
  },
  {
    "text": "memory which can be a problem in the case of large datasets so an approximate",
    "start": "947819",
    "end": "954160"
  },
  {
    "text": "algorithm is used for this candidate split points are proposed based on the",
    "start": "954160",
    "end": "959350"
  },
  {
    "text": "percentiles of feature distribution that continuous features are binned",
    "start": "959350",
    "end": "964420"
  },
  {
    "text": "into buckets that are split based on candidate split points the best solution for candidates flick points are chosen",
    "start": "964420",
    "end": "971529"
  },
  {
    "text": "from the aggregate statistics for the buckets column block for apparel our pal",
    "start": "971529",
    "end": "977529"
  },
  {
    "text": "our excuse me parallel learning so in this situation we're sorting the data",
    "start": "977529",
    "end": "983819"
  },
  {
    "text": "and sorting the data is the most common time-consuming aspect of tree learning",
    "start": "983819",
    "end": "989369"
  },
  {
    "text": "to reduce sorting costs data is stored in memory units called blocks each block",
    "start": "989369",
    "end": "995829"
  },
  {
    "text": "has data columns sorted by the corresponding feature value this computation needs to be done only once",
    "start": "995829",
    "end": "1002730"
  },
  {
    "text": "before training and can be you reused later sorting of blocks can be done independently and then can be divided",
    "start": "1002730",
    "end": "1009779"
  },
  {
    "text": "between parallel threads of the CPU the split finding can be parallelized as a",
    "start": "1009779",
    "end": "1015689"
  },
  {
    "text": "collection of statistics for each column when each cup for each column is done in",
    "start": "1015689",
    "end": "1021269"
  },
  {
    "text": "parallel weighted quantile sketch for approximate tree learning to propose",
    "start": "1021269",
    "end": "1027360"
  },
  {
    "text": "candidate split points amongst weighted data sets the weighted quintile sketch algorithm is used it carries out merge",
    "start": "1027360",
    "end": "1035130"
  },
  {
    "text": "and prune operations on quantile summaries over the data also sparsity",
    "start": "1035130",
    "end": "1040678"
  },
  {
    "text": "aware algorithms are used input may be sparse due to reasons such as one hot",
    "start": "1040679",
    "end": "1046350"
  },
  {
    "text": "encoding missing values and zero entries XG boo sizzle is aware of the sparsity",
    "start": "1046350",
    "end": "1052409"
  },
  {
    "text": "pattern in the data and visits only the default direction in each node cache",
    "start": "1052409",
    "end": "1058559"
  },
  {
    "text": "aware access is an algorithm designed to minimize the movement of memory pages in",
    "start": "1058559",
    "end": "1065250"
  },
  {
    "text": "and out of the processors on chip memory cache out of core computation for data",
    "start": "1065250",
    "end": "1071399"
  },
  {
    "text": "that does not fit in two main memory you can divide the data into multiple blocks and store each",
    "start": "1071399",
    "end": "1077260"
  },
  {
    "text": "block on disk you can compress the block by columns and decompress on-the-fly in",
    "start": "1077260",
    "end": "1083650"
  },
  {
    "text": "an independent thread while disk reading finally to measure the performance of a",
    "start": "1083650",
    "end": "1091300"
  },
  {
    "text": "model you are given a set of parameters and we need to define an objective function an objective function must",
    "start": "1091300",
    "end": "1098590"
  },
  {
    "text": "always contain two parts a training loss and regularization so the regularization",
    "start": "1098590",
    "end": "1104200"
  },
  {
    "text": "term panel eise's the complexity of the model so we're Amega here is the",
    "start": "1104200",
    "end": "1112150"
  },
  {
    "text": "regularization term where algorithms forget to include an objective function",
    "start": "1112150",
    "end": "1117190"
  },
  {
    "text": "however XG boost includes regularization thus controlling the complexity of the",
    "start": "1117190",
    "end": "1122710"
  },
  {
    "text": "model and preventing overfitting so the above six features may be individually",
    "start": "1122710",
    "end": "1128650"
  },
  {
    "text": "present in some algorithms but XG boost combines all of these techniques to make an end-to-end system that provides",
    "start": "1128650",
    "end": "1135550"
  },
  {
    "text": "scalability and effective resource utilization okay there's a few disadvantages training can be onerous it",
    "start": "1135550",
    "end": "1147640"
  },
  {
    "text": "can be slow and importantly it extrapolates very poorly let me give you",
    "start": "1147640",
    "end": "1153400"
  },
  {
    "text": "example here it's possible if you are given out-of-bounds data on inference",
    "start": "1153400",
    "end": "1161740"
  },
  {
    "text": "that you can get illogical values so if",
    "start": "1161740",
    "end": "1167110"
  },
  {
    "text": "you take a look at the chart on the upper left here gradient boosting has to",
    "start": "1167110",
    "end": "1174820"
  },
  {
    "text": "minimize the loss function you use you might and you might have bounds for your",
    "start": "1174820",
    "end": "1180130"
  },
  {
    "text": "labels but gradient boosting does not look at those bounds and does not bound the predictions so you can see clearly",
    "start": "1180130",
    "end": "1187150"
  },
  {
    "text": "in this upper left that with one variable although you have one variable X and only positive observations the if",
    "start": "1187150",
    "end": "1196810"
  },
  {
    "text": "if you were to predict five you actually get a negative value I don't know if you can see my pointer there or not so that",
    "start": "1196810",
    "end": "1204250"
  },
  {
    "text": "the same as the next story on the chart below but you're given you know a positive",
    "start": "1204250",
    "end": "1209920"
  },
  {
    "text": "value that is that isn't correct so generally if you take a look at other",
    "start": "1209920",
    "end": "1215530"
  },
  {
    "text": "machine learning algorithms we may or may not have that problem but it's",
    "start": "1215530",
    "end": "1220960"
  },
  {
    "text": "particularly important with XG boost to recognize that out of bound inference",
    "start": "1220960",
    "end": "1228460"
  },
  {
    "text": "can give you you know poor results unusable results so I mentioned there's",
    "start": "1228460",
    "end": "1235150"
  },
  {
    "text": "a lot of hyper parameters this is another really big problem now I want to",
    "start": "1235150",
    "end": "1241809"
  },
  {
    "text": "point out a handful there's more than 40 parameters that you can modify on Sage",
    "start": "1241809",
    "end": "1248350"
  },
  {
    "text": "makers version of XG boost but three are very important so let me just walk",
    "start": "1248350",
    "end": "1254740"
  },
  {
    "text": "through those just a little bit so first of all max death the max depth of a tree",
    "start": "1254740",
    "end": "1260860"
  },
  {
    "text": "if set to small can under fit the data you might recall my chart just a few",
    "start": "1260860",
    "end": "1267400"
  },
  {
    "text": "slides back with the the bold green line while increasing it will make it more complex and thus more likely to overfit",
    "start": "1267400",
    "end": "1274990"
  },
  {
    "text": "now the min child weight hyper parameter is the minimum sum of instance weight or",
    "start": "1274990",
    "end": "1282280"
  },
  {
    "text": "the Haitian derivative of the gradient needed in a child if the tree partition",
    "start": "1282280",
    "end": "1287590"
  },
  {
    "text": "step results in a leaf node with the sum of instance weight less than min child",
    "start": "1287590",
    "end": "1293470"
  },
  {
    "text": "weight the building process gives up further partitioning in linear regression models this simply",
    "start": "1293470",
    "end": "1299650"
  },
  {
    "text": "corresponds to the number of instances needed in each node the gamma is a",
    "start": "1299650",
    "end": "1306010"
  },
  {
    "text": "regularization control and it prevents overfitting the higher the gamma is the higher the regularization the default",
    "start": "1306010",
    "end": "1313179"
  },
  {
    "text": "value is zero which is no regularization I'll mention one more which is sort of",
    "start": "1313179",
    "end": "1319110"
  },
  {
    "text": "highlighted in the the second section there which is ETA so that is the step",
    "start": "1319110",
    "end": "1325390"
  },
  {
    "text": "size shrinkage used in updates to prevent overfitting after each boosting",
    "start": "1325390",
    "end": "1330700"
  },
  {
    "text": "step you can directly get the weights of new features that ETA hyper parameter",
    "start": "1330700",
    "end": "1336400"
  },
  {
    "text": "actually shrinks the feature weights to make the boosting process more conservative but don't worry we have a",
    "start": "1336400",
    "end": "1345520"
  },
  {
    "text": "whole lot going on with the AWS implementation of X G boost that takes a",
    "start": "1345520",
    "end": "1352600"
  },
  {
    "text": "lot of that off of your shoulders so first of all XG boost on Sage maker is",
    "start": "1352600",
    "end": "1361440"
  },
  {
    "text": "really designed for high scale high-volume classification or regression",
    "start": "1361440",
    "end": "1368520"
  },
  {
    "text": "so with distributed training XG boost on Sage maker allows customers to train",
    "start": "1368520",
    "end": "1375130"
  },
  {
    "text": "massive data sets on multiple machines just specify the number and the size of machines that you want and we will scale",
    "start": "1375130",
    "end": "1382120"
  },
  {
    "text": "out Amazon will automatically take care of distributing the data and the training process now charted by Amazon",
    "start": "1382120",
    "end": "1389919"
  },
  {
    "text": "s3 key training what that that refers to is that requires that you partition your",
    "start": "1389919",
    "end": "1397390"
  },
  {
    "text": "data on s3 in advance of training this allows Sage maker to download each",
    "start": "1397390",
    "end": "1402760"
  },
  {
    "text": "partition of the data set to individual nodes rather than downloading all the",
    "start": "1402760",
    "end": "1408340"
  },
  {
    "text": "data on all the nodes this saves time in downloading the data set from s3 and",
    "start": "1408340",
    "end": "1413559"
  },
  {
    "text": "ultimately speeds up training jobs instance weighted training using XG",
    "start": "1413559",
    "end": "1419559"
  },
  {
    "text": "boost on Sage maker allows you to add weights to individual data points also",
    "start": "1419559",
    "end": "1426130"
  },
  {
    "text": "referred to as instances while you're training this allows customers to differentiate the importance of",
    "start": "1426130",
    "end": "1433000"
  },
  {
    "text": "different instances during model training by assigning them weight values and just had a conversation about that",
    "start": "1433000",
    "end": "1439390"
  },
  {
    "text": "last night so SPARC integration is key with the",
    "start": "1439390",
    "end": "1444820"
  },
  {
    "text": "SPARC SDK we provide a concise API for developers to interact with XG boost",
    "start": "1444820",
    "end": "1451750"
  },
  {
    "text": "from SPARC developers can't first pre process data on Apache spark then call",
    "start": "1451750",
    "end": "1457780"
  },
  {
    "text": "XG boost and sage maker directly from their spark environment I'm going to have a quick demo of that in just a few",
    "start": "1457780",
    "end": "1464230"
  },
  {
    "text": "moments this spins up the Amazon sage maker training instance and uses them to train on the data that was already pre",
    "start": "1464230",
    "end": "1471040"
  },
  {
    "text": "processed with spark so easy deployment and managed model",
    "start": "1471040",
    "end": "1476250"
  },
  {
    "text": "hosting after a model is trained you only need one API call to deploy in",
    "start": "1476250",
    "end": "1481470"
  },
  {
    "text": "production at scale Sage maker hosting is managed and can be configured for",
    "start": "1481470",
    "end": "1487470"
  },
  {
    "text": "auto scaling which reduces the overhead in running a hosting environment and a B",
    "start": "1487470",
    "end": "1493500"
  },
  {
    "text": "testing once you're in production using sage maker you can run multiple XG boost",
    "start": "1493500",
    "end": "1499770"
  },
  {
    "text": "models with different weights of inference in other words the incoming traffic can be pointed to different XG",
    "start": "1499770",
    "end": "1510240"
  },
  {
    "text": "boost models so as a new model a new trained model can go into production you",
    "start": "1510240",
    "end": "1515820"
  },
  {
    "text": "can behind the scenes put that model into place or even just test it on a small fraction of traffic without",
    "start": "1515820",
    "end": "1522960"
  },
  {
    "text": "touching the endpoint that your applications are all accessing all right",
    "start": "1522960",
    "end": "1529830"
  },
  {
    "text": "so that's the the theory and the introduction part",
    "start": "1529830",
    "end": "1535070"
  },
  {
    "text": "let's move on to taking a look now at some of sample notebooks so I've started",
    "start": "1535860",
    "end": "1543179"
  },
  {
    "text": "up a sage maker instance here and I just need to clear some of this webinar stuff",
    "start": "1543179",
    "end": "1549780"
  },
  {
    "text": "off on my screen so I can see what I'm doing almost there all right there we go alright so I've",
    "start": "1549780",
    "end": "1558990"
  },
  {
    "text": "started in a notebook instance on sage maker and Here I am in Jupiter and I",
    "start": "1558990",
    "end": "1566490"
  },
  {
    "text": "have a number of notebooks that I'd like to go through we probably won't get",
    "start": "1566490",
    "end": "1572370"
  },
  {
    "text": "through all of these but because of the versatility of x/g boost we have more",
    "start": "1572370",
    "end": "1579360"
  },
  {
    "text": "models in our sample in our sage make Alex sage maker example set than almost",
    "start": "1579360",
    "end": "1587760"
  },
  {
    "text": "any other algorithm I'm going to start with the simple and get in and certainly before the top of the hour or I guess",
    "start": "1587760",
    "end": "1595020"
  },
  {
    "text": "the the bottom of the hour which would be our one one hour mark here get to hyper parameter optimization which is",
    "start": "1595020",
    "end": "1601559"
  },
  {
    "text": "one of the key benefits of running XG boost on on sage maker so start with",
    "start": "1601559",
    "end": "1607950"
  },
  {
    "text": "this simple example it is the abalone dataset so what we're doing here in this",
    "start": "1607950",
    "end": "1615780"
  },
  {
    "text": "particular notebook oh and if and if it wasn't a hundred percent clear every one",
    "start": "1615780",
    "end": "1621330"
  },
  {
    "text": "of the notebooks that I'm going to open up right now is available in the sage",
    "start": "1621330",
    "end": "1626760"
  },
  {
    "text": "maker example set which comes with every sage maker implementation so you can use",
    "start": "1626760",
    "end": "1633090"
  },
  {
    "text": "this as a reference you can use it as a cut and paste way to start your own",
    "start": "1633090",
    "end": "1638160"
  },
  {
    "text": "models often in these seminars we will create new content but there's so much",
    "start": "1638160",
    "end": "1644130"
  },
  {
    "text": "excellent content that's just in the sage maker that's given that we give away for free and and the issues are so",
    "start": "1644130",
    "end": "1650580"
  },
  {
    "text": "complex I just thought it would be annoying or distracting to create something original for this demo so what",
    "start": "1650580",
    "end": "1658559"
  },
  {
    "text": "we want to do with the abalone dataset is instead of cracking open the shells and and counting the rings you know we",
    "start": "1658559",
    "end": "1666090"
  },
  {
    "text": "want to be able to use characteristics the physical measurements of the abalone",
    "start": "1666090",
    "end": "1671760"
  },
  {
    "text": "to guess its age right that's that's the the problem that's being addressed here so in typical stage maker style the very",
    "start": "1671760",
    "end": "1680340"
  },
  {
    "text": "first cell we're importing whatever Python modules we need along with sage",
    "start": "1680340",
    "end": "1685410"
  },
  {
    "text": "makers execution roll we're allocating an s3 bucket to both download our data",
    "start": "1685410",
    "end": "1692850"
  },
  {
    "text": "set and to put our trained model in when it's done okay so job number one",
    "start": "1692850",
    "end": "1700410"
  },
  {
    "text": "we're gonna download the data set and then we're gonna split it into a training a validation and a test set",
    "start": "1700410",
    "end": "1706610"
  },
  {
    "text": "whenever you're using sage maker you always want to have at least a training and validation sets when you provide a",
    "start": "1706610",
    "end": "1714000"
  },
  {
    "text": "validation set to Sage maker built-in models you automatically get hyper",
    "start": "1714000",
    "end": "1720180"
  },
  {
    "text": "parameter optimization and like I said we're going to go into the parameters",
    "start": "1720180",
    "end": "1725640"
  },
  {
    "text": "that we expose for you at the end of this presentation so we upload the data to s3 we divide it as far as ingesting",
    "start": "1725640",
    "end": "1734580"
  },
  {
    "text": "the data let's see I guess we did a few methods first so we actually do download it here and then we make the calls to",
    "start": "1734580",
    "end": "1742170"
  },
  {
    "text": "divide the data set okay so next up one",
    "start": "1742170",
    "end": "1748050"
  },
  {
    "text": "thing that's important for everyone to know using sage maker is in training your data is always going to be in s3 so",
    "start": "1748050",
    "end": "1755010"
  },
  {
    "text": "even though we have many storage options available in AWS when you're using sage",
    "start": "1755010",
    "end": "1760830"
  },
  {
    "text": "maker always plan to have your data set in s3 in fact if you're using spark in",
    "start": "1760830",
    "end": "1768720"
  },
  {
    "text": "our spark EMR our PI spark implementation what we're doing for you",
    "start": "1768720",
    "end": "1774480"
  },
  {
    "text": "behind the scenes without ever leaving spark we're saving the data from your",
    "start": "1774480",
    "end": "1779670"
  },
  {
    "text": "data frame into s3 calling the the built-in models from Sage Maker",
    "start": "1779670",
    "end": "1784800"
  },
  {
    "text": "returning the results to s3 and then delivering that back to spark for you so",
    "start": "1784800",
    "end": "1790350"
  },
  {
    "text": "in fact you're always training out of s3 even though some of our utilities provide convenience functions for you so",
    "start": "1790350",
    "end": "1797190"
  },
  {
    "text": "you just don't quite see that so as three is critical the other thing that's critical our docker",
    "start": "1797190",
    "end": "1803370"
  },
  {
    "text": "Tanner's all of the built-in algorithms are in docker containers and in this",
    "start": "1803370",
    "end": "1808950"
  },
  {
    "text": "line right here what we're doing is we're grabbing the container that has the XG boost built-in algorithm the next",
    "start": "1808950",
    "end": "1816480"
  },
  {
    "text": "step here is specifying our training parameters the first section is just saying okay",
    "start": "1816480",
    "end": "1822260"
  },
  {
    "text": "which one are we going to run where is the s3 output path what type of machine",
    "start": "1822260",
    "end": "1829830"
  },
  {
    "text": "are we going to run this on and then here we see our training parameters for this particular job and as mentioned we",
    "start": "1829830",
    "end": "1836910"
  },
  {
    "text": "have max depth ETA and gamma probably the most important along with min child",
    "start": "1836910",
    "end": "1844380"
  },
  {
    "text": "wait for XG boost we have a few others including subsample silence our",
    "start": "1844380",
    "end": "1850130"
  },
  {
    "text": "objective here is a linear regressor and number and is set to 50 so we also have",
    "start": "1850130",
    "end": "1859650"
  },
  {
    "text": "a stopping condition if our training job runs away we don't want it just billing us you know forever so that's another",
    "start": "1859650",
    "end": "1866730"
  },
  {
    "text": "feature within Sage maker and then finally we have the details for our s3 buckets for our input data and yeah and",
    "start": "1866730",
    "end": "1875429"
  },
  {
    "text": "that's it now the sage maker call that actually trains the job isn't fit it's",
    "start": "1875429",
    "end": "1881460"
  },
  {
    "text": "described training job so what you're doing when you call describe training job I'm sorry a create training job went",
    "start": "1881460",
    "end": "1888720"
  },
  {
    "text": "to the wrong line there create training job what that's actually doing is it's",
    "start": "1888720",
    "end": "1894059"
  },
  {
    "text": "taking the hyper parameters that you just set marrying it with the container that you just accessed and sending that",
    "start": "1894059",
    "end": "1901920"
  },
  {
    "text": "off to a separate job queue so in our Jupiter notebook instance that we're",
    "start": "1901920",
    "end": "1907559"
  },
  {
    "text": "looking at right now this can run on a very inexpensive machine not one of the",
    "start": "1907559",
    "end": "1912660"
  },
  {
    "text": "big P 3s with all the GPUs in them now when you send your job off to train you could send that to any machine now up in",
    "start": "1912660",
    "end": "1919500"
  },
  {
    "text": "our parameters here we chose I believe it was an m4 yeah we did so this really",
    "start": "1919500",
    "end": "1926640"
  },
  {
    "text": "is a cost optimization feature and also it's what gives us the ability to",
    "start": "1926640",
    "end": "1932780"
  },
  {
    "text": "automatically scale not just the memory within a single oisin but also to cluster out and share",
    "start": "1932780",
    "end": "1940259"
  },
  {
    "text": "memory through some Bayesian techniques that we have to grow your training job",
    "start": "1940259",
    "end": "1946860"
  },
  {
    "text": "to almost any number of systems in a cluster so in this particular instance",
    "start": "1946860",
    "end": "1952559"
  },
  {
    "text": "our wall time was three minutes our CPU time was pretty small so I'm often asked",
    "start": "1952559",
    "end": "1959190"
  },
  {
    "text": "what the Delta is between those two numbers that's just loading up the container and getting your processing",
    "start": "1959190",
    "end": "1965669"
  },
  {
    "text": "ready sitting in the job queue loading the container this thing ran really really quickly but the wall time",
    "start": "1965669",
    "end": "1971460"
  },
  {
    "text": "happened to be three minutes so the lesson there is to recognize that when",
    "start": "1971460",
    "end": "1976860"
  },
  {
    "text": "you're using the job training queue and you're actually training with Sage maker",
    "start": "1976860",
    "end": "1982139"
  },
  {
    "text": "this is really for you know your your bigger training jobs when you're sending the big data",
    "start": "1982139",
    "end": "1987330"
  },
  {
    "text": "I'm very often I'll use scikit-learn version of XG boost when I'm working on",
    "start": "1987330",
    "end": "1993269"
  },
  {
    "text": "small sample sets and I'm just developing models in the very very beginning all right so now we the models",
    "start": "1993269",
    "end": "2000289"
  },
  {
    "text": "been trained as as you can see I've already run this notebook fully so for the sake of time now we're ready to host",
    "start": "2000289",
    "end": "2008000"
  },
  {
    "text": "the model so when we create our model we're essentially just getting the",
    "start": "2008000",
    "end": "2014289"
  },
  {
    "text": "getting the training job getting the model out of the training job that we created we're going to create this model",
    "start": "2014289",
    "end": "2020149"
  },
  {
    "text": "right here and then we're going to marry it to what's called a production variant right so the variant name here is going",
    "start": "2020149",
    "end": "2026750"
  },
  {
    "text": "to be all traffic and what that means you can see here when it says initial variant wait we're going to give it all",
    "start": "2026750",
    "end": "2033620"
  },
  {
    "text": "of the traffic so the wait is a float it goes from 0 to 1 and it's essentially",
    "start": "2033620",
    "end": "2039860"
  },
  {
    "text": "the percentage of tract of traffic that's coming to that particular end point to create the end point it's",
    "start": "2039860",
    "end": "2046639"
  },
  {
    "text": "really simple it's just one command create end point and then that end point will go into production and then we can",
    "start": "2046639",
    "end": "2054500"
  },
  {
    "text": "begin to do our predictions off of it in this particular example we iterated",
    "start": "2054500",
    "end": "2060858"
  },
  {
    "text": "through our our labels and through our test sets and first on a single",
    "start": "2060859",
    "end": "2069589"
  },
  {
    "text": "prediction then we ran through the whole set and believe we have a confusion matrix at the end here no but",
    "start": "2069589",
    "end": "2076290"
  },
  {
    "text": "we do have a a percentage error so",
    "start": "2076290",
    "end": "2081388"
  },
  {
    "text": "that's the first simple example with the abalone data set let's move on to",
    "start": "2081389",
    "end": "2088560"
  },
  {
    "text": "something a little bit more interesting which is ensemble learning using census",
    "start": "2088560",
    "end": "2094020"
  },
  {
    "text": "data so the purpose of this particular notebook is to use information from the",
    "start": "2094020",
    "end": "2100320"
  },
  {
    "text": "Census Bureau to determine whether a person makes more than $50,000 a year so",
    "start": "2100320",
    "end": "2106320"
  },
  {
    "text": "you know we're not going to look at their income but as we see down here below and I'm going to skip some of the",
    "start": "2106320",
    "end": "2111540"
  },
  {
    "text": "stuff that I spent a little bit more time on on the first notebook let's get",
    "start": "2111540",
    "end": "2116790"
  },
  {
    "text": "right to the data exploration so we're using pandas to open up this data set",
    "start": "2116790",
    "end": "2121830"
  },
  {
    "text": "pandas is typically alias to PD and if you look at the actual data here we have aged the working classification",
    "start": "2121830",
    "end": "2131420"
  },
  {
    "text": "education how how far you got your education marital status etc right and a lot of",
    "start": "2131420",
    "end": "2139020"
  },
  {
    "text": "these values are texts right so we see immediately in our data here that we've",
    "start": "2139020",
    "end": "2146070"
  },
  {
    "text": "got to at least do perhaps one hot encoding if you're not familiar with this concept basically everything you",
    "start": "2146070",
    "end": "2153060"
  },
  {
    "text": "have to send to xg-- boost has to be a number so typically one hot encoding is used to to do that you take those labels",
    "start": "2153060",
    "end": "2161310"
  },
  {
    "text": "those categories give them a 1 or a 0 say that your class is in there that's what we've done on this next set using",
    "start": "2161310",
    "end": "2166920"
  },
  {
    "text": "pandas pandas makes it really really easy to do that let's just move down a",
    "start": "2166920",
    "end": "2172560"
  },
  {
    "text": "little bit here and see what the information is 4xg boost itself so in",
    "start": "2172560",
    "end": "2178260"
  },
  {
    "text": "term in terms of once again looking at the training parameters we have the name the container s3 locations and then our",
    "start": "2178260",
    "end": "2186210"
  },
  {
    "text": "algorithm hyper parameters our data is coming in s in in CSV format you know",
    "start": "2186210",
    "end": "2192480"
  },
  {
    "text": "here's something that is important to know live SVM support vector machine",
    "start": "2192480",
    "end": "2199200"
  },
  {
    "text": "format is the typical input for XG boost but we also accept CSV format",
    "start": "2199200",
    "end": "2207060"
  },
  {
    "text": "so in this instance we're going to use CSV formats we're taking our training data once again we're going to set up",
    "start": "2207060",
    "end": "2213750"
  },
  {
    "text": "our hyper parameters here's the top hyper parameters as mentioned before our input and output we",
    "start": "2213750",
    "end": "2221880"
  },
  {
    "text": "create the training job we run the training job again it ran in 76 milliseconds",
    "start": "2221880",
    "end": "2227940"
  },
  {
    "text": "once we got there we set up our training container for our hosting container then",
    "start": "2227940",
    "end": "2233760"
  },
  {
    "text": "we create the endpoint which is in production now and then we do our",
    "start": "2233760",
    "end": "2239310"
  },
  {
    "text": "prediction based on our test set so at",
    "start": "2239310",
    "end": "2246210"
  },
  {
    "text": "the end of this we're gonna we're going to determine how much how accurate our",
    "start": "2246210",
    "end": "2252510"
  },
  {
    "text": "training set was using an a AUC predictor so that is going to determine",
    "start": "2252510",
    "end": "2259530"
  },
  {
    "text": "well it's sort of a Gaussian approach to taking a look at a confusion matrix so let's just put it that way",
    "start": "2259530",
    "end": "2266780"
  },
  {
    "text": "so we set up the approach at the end we convert our training data to protobuf",
    "start": "2266780",
    "end": "2274290"
  },
  {
    "text": "format and then we finally specify some",
    "start": "2274290",
    "end": "2279780"
  },
  {
    "text": "images here and then come down to the end I want to get to the bottom of this",
    "start": "2279780",
    "end": "2285360"
  },
  {
    "text": "one because I think there we go then we finally save our prediction results all",
    "start": "2285360",
    "end": "2292440"
  },
  {
    "text": "right so this particular notebook covers a lot of issues that are a little bit",
    "start": "2292440",
    "end": "2297450"
  },
  {
    "text": "more complex and a little bit more relevant to what I call tabular data or the kind of data that we deal with in",
    "start": "2297450",
    "end": "2303900"
  },
  {
    "text": "you know most enterprises everyday in other words when you're working with deep learning frequently the topics are",
    "start": "2303900",
    "end": "2310230"
  },
  {
    "text": "all around identifying cats and dogs however you don't need to do that with",
    "start": "2310230",
    "end": "2317610"
  },
  {
    "text": "XG boost you can do image processing with XG boost in fact I have one example",
    "start": "2317610",
    "end": "2323430"
  },
  {
    "text": "right here if I come back here one second I come back here is that that's",
    "start": "2323430",
    "end": "2330240"
  },
  {
    "text": "not it I think it's this one one moment here we go just wanna make sure",
    "start": "2330240",
    "end": "2336280"
  },
  {
    "text": "yes and it's not it sorry",
    "start": "2336280",
    "end": "2342610"
  },
  {
    "text": "so I did run it on amnesty here it is and now I'm gonna skip all of the setup",
    "start": "2342610",
    "end": "2349930"
  },
  {
    "text": "and just go to the predictive capacity of XG boot oh wait oh that wasn't run",
    "start": "2349930",
    "end": "2356230"
  },
  {
    "text": "either oh I apologize okay so I don't have a pre run version but you can use",
    "start": "2356230",
    "end": "2362350"
  },
  {
    "text": "extra boost on images and we do have that notebook available okay",
    "start": "2362350",
    "end": "2368590"
  },
  {
    "text": "moving into a customer churn so this particular notebook seeks to determine",
    "start": "2368590",
    "end": "2374920"
  },
  {
    "text": "which customers are most likely to leave your business it's using a cell phone",
    "start": "2374920",
    "end": "2381930"
  },
  {
    "text": "data set from UCI and it's also well",
    "start": "2381930",
    "end": "2388600"
  },
  {
    "text": "documented in this book by Daniel LaRoche called discovering knowledge and data and it's not from UCI actually it's",
    "start": "2388600",
    "end": "2395380"
  },
  {
    "text": "from data mining consultant com but as we take a look at the data here",
    "start": "2395380",
    "end": "2400860"
  },
  {
    "text": "we can see that there's a lot of interesting stuff here right it has the",
    "start": "2400860",
    "end": "2406090"
  },
  {
    "text": "location area code the actual phone number whether there's an international",
    "start": "2406090",
    "end": "2411130"
  },
  {
    "text": "plan or not the amount of billable time on this particular cell phone so it's",
    "start": "2411130",
    "end": "2417370"
  },
  {
    "text": "it's it's a pretty meaningful data set but not all this data is relevant right",
    "start": "2417370",
    "end": "2422380"
  },
  {
    "text": "and so it's pretty well described right here so what we do in the next couple of cells here is begin to explore this data",
    "start": "2422380",
    "end": "2429910"
  },
  {
    "text": "so first we're looking for geographical distribution and we can see that it's relatively well distributed if we move",
    "start": "2429910",
    "end": "2439540"
  },
  {
    "text": "into some other features I probably should have mentioned that one of the",
    "start": "2439540",
    "end": "2444670"
  },
  {
    "text": "features because this is a labeled data set right this is supervised learning is whether there was actual turn on this",
    "start": "2444670",
    "end": "2451150"
  },
  {
    "text": "account so as we dig into that data we",
    "start": "2451150",
    "end": "2456370"
  },
  {
    "text": "get the following observations right it's fairly evenly distributed geographically people who leave are more",
    "start": "2456370",
    "end": "2463360"
  },
  {
    "text": "likely to have an international plan less likely to have voicemail etc so on",
    "start": "2463360",
    "end": "2468790"
  },
  {
    "text": "that data we're going to start eliminating some of the columns right we do scatterplot here of these particular",
    "start": "2468790",
    "end": "2475690"
  },
  {
    "text": "features which in my humble opinion kind of looks good on the notebook but it's a little bit hard to read with this with",
    "start": "2475690",
    "end": "2481750"
  },
  {
    "text": "this much data so here we are dropping some of the not meaningful columns in",
    "start": "2481750",
    "end": "2488770"
  },
  {
    "text": "the data set we're splitting our data into training validation and test now we",
    "start": "2488770",
    "end": "2495850"
  },
  {
    "text": "go to train again off of the XG boost container we do so by creating this",
    "start": "2495850",
    "end": "2501690"
  },
  {
    "text": "estimator we set the hyper parameters here and then we set the job off for",
    "start": "2501690",
    "end": "2507460"
  },
  {
    "text": "training now one thing to note about training and sage maker is of course you get the visual representations here",
    "start": "2507460",
    "end": "2514410"
  },
  {
    "text": "right in your notebook but if we go to the sage maker console all this information is available on the left",
    "start": "2514410",
    "end": "2520360"
  },
  {
    "text": "hand side so when I go to training jobs and I take a look at the Train jobs that",
    "start": "2520360",
    "end": "2526810"
  },
  {
    "text": "have run here I can click on any of these and I'll just choose this one",
    "start": "2526810",
    "end": "2532150"
  },
  {
    "text": "right here and you can see that there's a lot of information on it including access at the bottom to both the cloud",
    "start": "2532150",
    "end": "2540250"
  },
  {
    "text": "watch and the instance specific logs so I click on this so all this stuff is",
    "start": "2540250",
    "end": "2546820"
  },
  {
    "text": "retained right so as you train this job these are all the messages that were generated in the training of the job so",
    "start": "2546820",
    "end": "2553360"
  },
  {
    "text": "to pop back into our turn model when we",
    "start": "2553360",
    "end": "2558880"
  },
  {
    "text": "deployed this particular model we're not choosing as many options in the other",
    "start": "2558880",
    "end": "2564100"
  },
  {
    "text": "models now I've also often called model hosting or model deployment sort of the Forgotten child of of machine",
    "start": "2564100",
    "end": "2571930"
  },
  {
    "text": "learning there's so much attention on how do we create a model how do we explore the data how do we create the",
    "start": "2571930",
    "end": "2577540"
  },
  {
    "text": "data set that sometimes we forget how do you deploy this thing at scale globally",
    "start": "2577540",
    "end": "2582730"
  },
  {
    "text": "well with sage maker it is literally as simple as this one command just this one",
    "start": "2582730",
    "end": "2588190"
  },
  {
    "text": "command can do it and from the sage maker console you can do this all just with a GUI right you can create an",
    "start": "2588190",
    "end": "2594700"
  },
  {
    "text": "endpoint configuration that says you know what kind of hosts do I want this machine on what's the model and then I",
    "start": "2594700",
    "end": "2601420"
  },
  {
    "text": "can creat an endpoint here just by clicking this create endpoint and finding that endpoint configuration so",
    "start": "2601420",
    "end": "2609070"
  },
  {
    "text": "that is how you deploy a model not just from Python within a sage maker notebook",
    "start": "2609070",
    "end": "2615340"
  },
  {
    "text": "but also graphically and I'd be remiss if I didn't mention that you can do it with bash using the AWS CLI alright so",
    "start": "2615340",
    "end": "2623830"
  },
  {
    "text": "finally we're gonna call predict that's the whole point of launching this model",
    "start": "2623830",
    "end": "2629560"
  },
  {
    "text": "in the first place and in this particular notebook our confusion matrix correctly predicts 39",
    "start": "2629560",
    "end": "2636010"
  },
  {
    "text": "out of 48 actual churners so what they get into next here is you know this is",
    "start": "2636010",
    "end": "2643990"
  },
  {
    "text": "doing a cut off at point O 5 right we're just just you know doing a binary cut off at point O 5 and that might not be",
    "start": "2643990",
    "end": "2650950"
  },
  {
    "text": "the right measure so the next few cells here are looking for a better value to",
    "start": "2650950",
    "end": "2656800"
  },
  {
    "text": "cross tabulate to get a better predictive value so you know you run that algorithm algorithm doesn't",
    "start": "2656800",
    "end": "2662680"
  },
  {
    "text": "necessarily do it all in fact most machine learning algorithms are set up in a pipeline so the rest of this",
    "start": "2662680",
    "end": "2670410"
  },
  {
    "text": "notebook actually walks through the process of determining what is the correct cutoff point to get true",
    "start": "2670410",
    "end": "2677050"
  },
  {
    "text": "negatives true positives and differentiate from especially the false",
    "start": "2677050",
    "end": "2682900"
  },
  {
    "text": "negatives which would be the most costly all right last but not least certainly",
    "start": "2682900",
    "end": "2688030"
  },
  {
    "text": "not least let's take a look at let's get back to my Jupiter head here I might",
    "start": "2688030",
    "end": "2696460"
  },
  {
    "text": "just make it easy by reopening this",
    "start": "2696460",
    "end": "2701520"
  },
  {
    "text": "let's take a look at a really phenomenal notebook that not only creates a really",
    "start": "2701520",
    "end": "2709270"
  },
  {
    "text": "nice market segmentation model using x-g boost but you can also analyze the",
    "start": "2709270",
    "end": "2714310"
  },
  {
    "text": "results using of hyper parameter optimization and try to do that in five",
    "start": "2714310",
    "end": "2721600"
  },
  {
    "text": "minutes so this particular notebook and we have three variants of this so I",
    "start": "2721600",
    "end": "2727000"
  },
  {
    "text": "don't know if you want to know specifically just send me an email or just you know explore around the sample",
    "start": "2727000",
    "end": "2733900"
  },
  {
    "text": "notebooks but what this what we're doing here is we're creating characteristics of our",
    "start": "2733900",
    "end": "2740120"
  },
  {
    "text": "customers from a transaction log it happens to be a banking database so that",
    "start": "2740120",
    "end": "2745280"
  },
  {
    "text": "we more accurately target customers with direct marketing isn't that a joy so",
    "start": "2745280",
    "end": "2752900"
  },
  {
    "text": "here we are once again I'm just gonna very go very quickly over the things I've already covered including",
    "start": "2752900",
    "end": "2758900"
  },
  {
    "text": "downloading the data set but pause a little bit to take a look at the actual data right so this is the bank this is",
    "start": "2758900",
    "end": "2766250"
  },
  {
    "text": "the information the bank has actually there's a few data sets when you get when you download this there's",
    "start": "2766250",
    "end": "2771770"
  },
  {
    "text": "transactions themselves but then this Bank additional full dot CSV has all",
    "start": "2771770",
    "end": "2777380"
  },
  {
    "text": "this qualitative information around the individuals who are in the data set",
    "start": "2777380",
    "end": "2782840"
  },
  {
    "text": "right so job housemaid marital status education etc so there's a lot of good",
    "start": "2782840",
    "end": "2790130"
  },
  {
    "text": "data here and and once again you know most a lot of that's a categorical and it's gonna get hot one hot and coded but",
    "start": "2790130",
    "end": "2797960"
  },
  {
    "text": "a lot of this data varies widely right so as we're going through and we're",
    "start": "2797960",
    "end": "2803150"
  },
  {
    "text": "looking some of these well some of these are just like I said categorical but some of the observations might be you",
    "start": "2803150",
    "end": "2809180"
  },
  {
    "text": "know how much did this particular customer spend and that could be in the thousands of dollars to you know pennies",
    "start": "2809180",
    "end": "2816350"
  },
  {
    "text": "and then they might talk about you know whether they have a college education or not so here graphically what we're",
    "start": "2816350",
    "end": "2822650"
  },
  {
    "text": "illustrating is you know what are the actual contents and averages them that",
    "start": "2822650",
    "end": "2827990"
  },
  {
    "text": "are found in the data set itself very easy to illustrate so what I love about",
    "start": "2827990",
    "end": "2834860"
  },
  {
    "text": "this notebook too is some of the language around data exploration right almost ninety percent of the values of",
    "start": "2834860",
    "end": "2840950"
  },
  {
    "text": "the target values are yes so you know most customers did not subscribe for a",
    "start": "2840950",
    "end": "2848150"
  },
  {
    "text": "term deposit that's the objective right for this particular marketing program many of the features are unknown so you",
    "start": "2848150",
    "end": "2856820"
  },
  {
    "text": "know without getting into too many of the details here because I encourage you to read this on your own we need to",
    "start": "2856820",
    "end": "2862070"
  },
  {
    "text": "determine which of these features are useful and predictive and which aren't which can be",
    "start": "2862070",
    "end": "2868520"
  },
  {
    "text": "correlated or combined into other more useful features once again I'm just",
    "start": "2868520",
    "end": "2874910"
  },
  {
    "text": "going to illustrate that a lot of this data is here and then finally we take a",
    "start": "2874910",
    "end": "2881600"
  },
  {
    "text": "look at the data through that scatter plot now this one actually does bear a little bit of attention which you can do",
    "start": "2881600",
    "end": "2888440"
  },
  {
    "text": "on your own finally now we're gonna do a little data cleaning in fact if you happen to be in",
    "start": "2888440",
    "end": "2895340"
  },
  {
    "text": "New York City tonight at the 34th Street we're going to have our deep-learning",
    "start": "2895340",
    "end": "2901520"
  },
  {
    "text": "meet up and trifectas presenting they are one of our competency partners that",
    "start": "2901520",
    "end": "2907640"
  },
  {
    "text": "specializes in cleaning data and feature engineering so they clean up the data",
    "start": "2907640",
    "end": "2913340"
  },
  {
    "text": "here now we're dividing our training validation set etc most of this now",
    "start": "2913340",
    "end": "2920450"
  },
  {
    "text": "around training is the usual and we're setting our hyper parameters etc now",
    "start": "2920450",
    "end": "2926540"
  },
  {
    "text": "this is sort of a plain vanilla version of using XG boost to clarify who we want",
    "start": "2926540",
    "end": "2934700"
  },
  {
    "text": "to communicate with the second version remember I said there's multiple versions of this well actually and when",
    "start": "2934700",
    "end": "2942200"
  },
  {
    "text": "you get to the training part you're actually going to specify hyper parameters that you would like varied so",
    "start": "2942200",
    "end": "2949190"
  },
  {
    "text": "we expose our hyper parameter optimization API for you and let me show",
    "start": "2949190",
    "end": "2956450"
  },
  {
    "text": "you exactly what that looks like all right so we're essentially in the same notebook but this time before training",
    "start": "2956450",
    "end": "2962330"
  },
  {
    "text": "we're setting up hyper parameter optimization and what we're saying here is for ETA we want to explore the whole",
    "start": "2962330",
    "end": "2969290"
  },
  {
    "text": "range of values between zero and one for min child weight we want to do between 1",
    "start": "2969290",
    "end": "2974840"
  },
  {
    "text": "and 10 alpha zero and two max depth one and ten so we chose one two three four",
    "start": "2974840",
    "end": "2981290"
  },
  {
    "text": "hyper parameters now that will set off separate parallel training jobs which",
    "start": "2981290",
    "end": "2988250"
  },
  {
    "text": "will be examined and then it will help us determine what the best possible and",
    "start": "2988250",
    "end": "2993980"
  },
  {
    "text": "that that's the most accurate and most regularized model that can be produced",
    "start": "2993980",
    "end": "3000940"
  },
  {
    "text": "with the information that we then we launch hyper parameter tuning",
    "start": "3000940",
    "end": "3006130"
  },
  {
    "text": "it's a single command and then we have our tuning job and now we want to see the results all right so we haven't",
    "start": "3006130",
    "end": "3012790"
  },
  {
    "text": "really trained our model here yet right we've just explored all of the options for hyper parameter tuning we have a",
    "start": "3012790",
    "end": "3020260"
  },
  {
    "text": "really nice built in notebook here that helps you analyze the results of any",
    "start": "3020260",
    "end": "3027460"
  },
  {
    "text": "training job so I'm going to open this up right now and this will go in and for",
    "start": "3027460",
    "end": "3034150"
  },
  {
    "text": "each of the hyper parameters that you chose you'll get specific metrics all right so",
    "start": "3034150",
    "end": "3040660"
  },
  {
    "text": "you get final objective value etc and and they're nicely illustrated below so",
    "start": "3040660",
    "end": "3047050"
  },
  {
    "text": "here we could see where's the label on this one you scroll down just a little",
    "start": "3047050",
    "end": "3052060"
  },
  {
    "text": "bit mark here we go so here we see max depths here we see ETA min child weight",
    "start": "3052060",
    "end": "3058690"
  },
  {
    "text": "etc now my recommendation in using this particular notebook when you're just starting using hyper parameter",
    "start": "3058690",
    "end": "3065290"
  },
  {
    "text": "optimization on AWS in the beginning to develop intuition just do one at a time",
    "start": "3065290",
    "end": "3071170"
  },
  {
    "text": "okay because when you're doing one at a time you can really get a strong",
    "start": "3071170",
    "end": "3076390"
  },
  {
    "text": "intuition of what those actual results are now if you go back to the console",
    "start": "3076390",
    "end": "3082000"
  },
  {
    "text": "and we look at the details of these training jobs of course every tuning job",
    "start": "3082000",
    "end": "3087310"
  },
  {
    "text": "that you asked for is preserved so you can look at it and up here the best",
    "start": "3087310",
    "end": "3092380"
  },
  {
    "text": "training job is highlighted it gets its own tab I mean it's part of this set",
    "start": "3092380",
    "end": "3097420"
  },
  {
    "text": "with all the training jobs but you with the the best training job is highlighted",
    "start": "3097420",
    "end": "3103180"
  },
  {
    "text": "on here you could see what the values are how they were actually chosen what the max depth was for example turned out",
    "start": "3103180",
    "end": "3109570"
  },
  {
    "text": "to be 3 min child weight 4.61 and this is just a really handy tool for",
    "start": "3109570",
    "end": "3117880"
  },
  {
    "text": "XG boost so so that is it these are all",
    "start": "3117880",
    "end": "3123850"
  },
  {
    "text": "the topics I wanted to cover today I want to leave a few minutes at the",
    "start": "3123850",
    "end": "3129640"
  },
  {
    "text": "bottom of the hour here for Q&A but yeah those were although those were",
    "start": "3129640",
    "end": "3135010"
  },
  {
    "text": "all the key topics that I wanted to cover if I could just do one more recommendation as you go",
    "start": "3135010",
    "end": "3142850"
  },
  {
    "text": "through these notebooks you're gonna see a lot of data wrangling and data",
    "start": "3142850",
    "end": "3149060"
  },
  {
    "text": "cleaning if you're relatively new to this if you're a developer coming into",
    "start": "3149060",
    "end": "3154220"
  },
  {
    "text": "machine learning situation even if you're a data scientist with many years",
    "start": "3154220",
    "end": "3159290"
  },
  {
    "text": "but somewhat new 2xg boost and all of",
    "start": "3159290",
    "end": "3164540"
  },
  {
    "text": "the deep learning algorithms I could not more highly recommend this particular book Python for data analysis which",
    "start": "3164540",
    "end": "3171290"
  },
  {
    "text": "covers principally it's a deep dive on X G boost I also have a set of references",
    "start": "3171290",
    "end": "3178280"
  },
  {
    "text": "if you email me at the email address below there I can send you a number of",
    "start": "3178280",
    "end": "3184280"
  },
  {
    "text": "other references on all of the sage maker algorithms in depth stuff that you",
    "start": "3184280",
    "end": "3190310"
  },
  {
    "text": "can share with customers or with your colleagues in-house so don't be afraid to ask that's exactly what we're here",
    "start": "3190310",
    "end": "3197030"
  },
  {
    "text": "for and that's the purpose of this of this call so um we do have some time for",
    "start": "3197030",
    "end": "3203240"
  },
  {
    "text": "questions and I know that my friend Pratap has been patiently waiting so",
    "start": "3203240",
    "end": "3210590"
  },
  {
    "text": "let's let's go to that interesting",
    "start": "3210590",
    "end": "3216410"
  },
  {
    "text": "questions that I want you to pick up in",
    "start": "3216410",
    "end": "3221870"
  },
  {
    "text": "the meantime which were like quick references right now the first question that I want to take up is how does X G",
    "start": "3221870",
    "end": "3229610"
  },
  {
    "text": "boost solve the imbalance class problem in the data set do you have any specific parameter to tune sure it's sort of",
    "start": "3229610",
    "end": "3239900"
  },
  {
    "text": "built into the nature of an ensemble model so the first of all there are many",
    "start": "3239900",
    "end": "3245720"
  },
  {
    "text": "hyper parameters that you can tune that will help especially if you have over",
    "start": "3245720",
    "end": "3253640"
  },
  {
    "text": "weighted problems but the general question how is it done it's sort of in the nature of a decision tree and",
    "start": "3253640",
    "end": "3261770"
  },
  {
    "text": "ensemble Excel as you begin to build these trees and one thing about gradient",
    "start": "3261770",
    "end": "3268730"
  },
  {
    "text": "boosting that I didn't meant earlier we nxg boost is as you create a model a surrogate model and you begin to",
    "start": "3268730",
    "end": "3276890"
  },
  {
    "text": "refine that model with the next iteration you're actually only focusing on the errors so as you're focusing on",
    "start": "3276890",
    "end": "3284690"
  },
  {
    "text": "the errors and improving on the errors of the previous model in other words you're not doing a whole tree from the",
    "start": "3284690",
    "end": "3289849"
  },
  {
    "text": "circuitry when you go to the next model you just focus on on the errors there's a refinement that occurs in that process",
    "start": "3289849",
    "end": "3296829"
  },
  {
    "text": "so it's I hope that's helpful but it's right in the mechanism of the algorithm",
    "start": "3296829",
    "end": "3304609"
  },
  {
    "text": "itself so usually like in deep learning you're always going to do some kind of normalization you know batch",
    "start": "3304609",
    "end": "3310339"
  },
  {
    "text": "normalization but yeah you don't have to you certainly could here and it's always",
    "start": "3310339",
    "end": "3315619"
  },
  {
    "text": "worth it to take a deep look at your data set and do proper feature",
    "start": "3315619",
    "end": "3322069"
  },
  {
    "text": "engineering which is why I brought up that pandas book at the end the next",
    "start": "3322069",
    "end": "3332720"
  },
  {
    "text": "question is does xgu support local training and hosting by local I'm not",
    "start": "3332720",
    "end": "3341089"
  },
  {
    "text": "sure if you mean if you can download the sage maker algorithm to say your laptop",
    "start": "3341089",
    "end": "3346940"
  },
  {
    "text": "or a local machine or whether you're saying can do it on an ec2 the short answer to that question if",
    "start": "3346940",
    "end": "3352220"
  },
  {
    "text": "that's the question is no so ya know like I said I usually use the",
    "start": "3352220",
    "end": "3359630"
  },
  {
    "text": "scikit-learn version of XG boost for my model development that could be 80 even",
    "start": "3359630",
    "end": "3366170"
  },
  {
    "text": "90 percent of the time you know I'm just sitting there in scikit-learn but then",
    "start": "3366170",
    "end": "3372140"
  },
  {
    "text": "when I'm ready to do the large data set training I almost always you know flip the switch use the mechanisms in Sage",
    "start": "3372140",
    "end": "3379489"
  },
  {
    "text": "Maker that I just showed you to send off the job to the job queue to get trained in the cloud so yeah when I'm using",
    "start": "3379489",
    "end": "3387049"
  },
  {
    "text": "scikit-learn on my machine I usually have like a you know small data set you know at the most you know 20 gig and I",
    "start": "3387049",
    "end": "3394700"
  },
  {
    "text": "just play with it on my machine I like to think of it as playing and and then",
    "start": "3394700",
    "end": "3400279"
  },
  {
    "text": "you know when I'm ready to do you know much larger datasets I go to the cloud",
    "start": "3400279",
    "end": "3407140"
  },
  {
    "text": "okay so so what he's asking is so",
    "start": "3407890",
    "end": "3413330"
  },
  {
    "text": "there's a beautiful hook for that question but we'll get to that I get the full question the next question I want",
    "start": "3413330",
    "end": "3420830"
  },
  {
    "text": "to go back to the next question maybe come back to this the next question is do you recommend doing HBO on every new",
    "start": "3420830",
    "end": "3428360"
  },
  {
    "text": "train job for example as more data becomes available every quarter which I",
    "start": "3428360",
    "end": "3434000"
  },
  {
    "text": "wanting to as part of your model or only once for each new problem space hmm yeah",
    "start": "3434000",
    "end": "3439910"
  },
  {
    "text": "so um you know I one of the reasons I highlighted those four hyper parameters",
    "start": "3439910",
    "end": "3445580"
  },
  {
    "text": "in particular is they have the most influence on the outcome of your model",
    "start": "3445580",
    "end": "3452350"
  },
  {
    "text": "now as far as HBO which I do it on every iteration no first of all you know it's",
    "start": "3452350",
    "end": "3460100"
  },
  {
    "text": "costly you should be careful especially on large data sets hyper parameter optimization can take hours also in the",
    "start": "3460100",
    "end": "3469460"
  },
  {
    "text": "beginning if you're not you know really super familiar with the way we do hyper parameter optimization it's it's a good",
    "start": "3469460",
    "end": "3477260"
  },
  {
    "text": "idea to just tune one see what the results are tune a different one see what the",
    "start": "3477260",
    "end": "3482300"
  },
  {
    "text": "results are iterate just a little bit on smaller datasets until you develop an intuition",
    "start": "3482300",
    "end": "3487820"
  },
  {
    "text": "of the capabilities what's going to happen over time is you're going to have more intuition about your data matching",
    "start": "3487820",
    "end": "3496850"
  },
  {
    "text": "with relevant hyper parameters I mean I did mention we were just talking about this last night gentleman at the loft",
    "start": "3496850",
    "end": "3503750"
  },
  {
    "text": "yesterday I had a financial data set with 4,000 features and he wanted to",
    "start": "3503750",
    "end": "3509840"
  },
  {
    "text": "take advantage of the ability to overweight one particular feature when",
    "start": "3509840",
    "end": "3515330"
  },
  {
    "text": "running the model well that's one of the things you could do with sage maker and he wanted to rerun the model 4,000 times",
    "start": "3515330",
    "end": "3522650"
  },
  {
    "text": "with an individual feature overweighted I said okay you know have at it but yeah so you want to you know",
    "start": "3522650",
    "end": "3529280"
  },
  {
    "text": "obviously everybody solving very different problems so I'm trying to answer the question generally for",
    "start": "3529280",
    "end": "3534500"
  },
  {
    "text": "everybody what I'd say though is you know to be mindful of cost because hyper parameter",
    "start": "3534500",
    "end": "3539840"
  },
  {
    "text": "optimization is costly also we're not the only ones that do this we have a",
    "start": "3539840",
    "end": "3545150"
  },
  {
    "text": "competency partner named sig opt if hyper parameter optimization is really",
    "start": "3545150",
    "end": "3550220"
  },
  {
    "text": "important to you and I'm sure it's important to everybody I mean it's who would create any code without",
    "start": "3550220",
    "end": "3556100"
  },
  {
    "text": "optimization I would invite you to look at our partner sig oft as well okay I",
    "start": "3556100",
    "end": "3567380"
  },
  {
    "text": "have the confirmation from the person who has all the local do you have answer",
    "start": "3567380",
    "end": "3573500"
  },
  {
    "text": "the question all right so that's all the questions that we have oh okay all right",
    "start": "3573500",
    "end": "3579560"
  },
  {
    "text": "well extra boost is a rock star algorithm for tabular data you you know",
    "start": "3579560",
    "end": "3586670"
  },
  {
    "text": "it's useful not only for creating production models but also for just exploring your data so thank you for",
    "start": "3586670",
    "end": "3593750"
  },
  {
    "text": "your time here today and let's see you've got one more slide here there's my email address and Twitter handle",
    "start": "3593750",
    "end": "3600550"
  },
  {
    "text": "eager to help anyone with any follow-on questions from the presentation I don't",
    "start": "3600550",
    "end": "3613820"
  },
  {
    "text": "know if comments at the end sue do you want to chime in No",
    "start": "3613820",
    "end": "3621060"
  },
  {
    "text": "if not then once again just thank you very much I appreciate your time and have fun with XG boost",
    "start": "3621060",
    "end": "3629210"
  },
  {
    "text": "you",
    "start": "3636230",
    "end": "3638290"
  },
  {
    "text": "a Pratap do we have any more questions that we need to get answered are we all",
    "start": "3641590",
    "end": "3646900"
  },
  {
    "text": "good to go awesome thanks so much everybody for being here we really",
    "start": "3646900",
    "end": "3652090"
  },
  {
    "text": "appreciate it and in the email that you get after this webinar you'll get the link to register for the next webinar so",
    "start": "3652090",
    "end": "3658810"
  },
  {
    "text": "look forward to seeing you again thank you",
    "start": "3658810",
    "end": "3662640"
  }
]