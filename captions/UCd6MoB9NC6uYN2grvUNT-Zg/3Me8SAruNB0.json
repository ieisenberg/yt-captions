[
  {
    "text": "if your developers have been building a",
    "start": "240",
    "end": "2159"
  },
  {
    "text": "purchase Park applications on Amazon",
    "start": "2159",
    "end": "3659"
  },
  {
    "text": "redshift they may be using spark",
    "start": "3659",
    "end": "6000"
  },
  {
    "text": "redshift open source connector",
    "start": "6000",
    "end": "8220"
  },
  {
    "text": "it is often a manual and cumbersome",
    "start": "8220",
    "end": "10440"
  },
  {
    "text": "process to set up that connector",
    "start": "10440",
    "end": "12719"
  },
  {
    "text": "the new Amazon redshift integration for",
    "start": "12719",
    "end": "14759"
  },
  {
    "text": "Apache spark eliminates the need for",
    "start": "14759",
    "end": "17160"
  },
  {
    "text": "complex manual setup",
    "start": "17160",
    "end": "19080"
  },
  {
    "text": "it helps your developers to seamlessly",
    "start": "19080",
    "end": "21359"
  },
  {
    "text": "build and run Apache spark applications",
    "start": "21359",
    "end": "23640"
  },
  {
    "text": "to read from and write to Amazon",
    "start": "23640",
    "end": "25800"
  },
  {
    "text": "redshift data warehouse",
    "start": "25800",
    "end": "28619"
  },
  {
    "text": "you can get started using your favorite",
    "start": "28619",
    "end": "30599"
  },
  {
    "text": "AWS analytics Services Amazon EMR 6.9",
    "start": "30599",
    "end": "34440"
  },
  {
    "text": "EMR serverless or AWS glue 4.0",
    "start": "34440",
    "end": "37800"
  },
  {
    "text": "you only need to specify the connection",
    "start": "37800",
    "end": "39660"
  },
  {
    "text": "and you can start working with Amazon",
    "start": "39660",
    "end": "41579"
  },
  {
    "text": "redshift from your Apache spark based",
    "start": "41579",
    "end": "44040"
  },
  {
    "text": "applications within minutes",
    "start": "44040",
    "end": "46559"
  },
  {
    "text": "using this integration you can use",
    "start": "46559",
    "end": "48600"
  },
  {
    "text": "iim-based authentication to make your",
    "start": "48600",
    "end": "50879"
  },
  {
    "text": "Apache spark applications more secure",
    "start": "50879",
    "end": "54600"
  },
  {
    "text": "you can use familiar spark data frame",
    "start": "54600",
    "end": "56699"
  },
  {
    "text": "apis or spark SQL apis",
    "start": "56699",
    "end": "60120"
  },
  {
    "text": "several spark data frame API operations",
    "start": "60120",
    "end": "62280"
  },
  {
    "text": "are pushed down to Amazon redshift which",
    "start": "62280",
    "end": "64680"
  },
  {
    "text": "results in Superior performance",
    "start": "64680",
    "end": "67860"
  },
  {
    "text": "let's now see a demo on how Amazon",
    "start": "67860",
    "end": "69720"
  },
  {
    "text": "redshift integration for Apache spark",
    "start": "69720",
    "end": "71880"
  },
  {
    "text": "simplifies running a purchase spark",
    "start": "71880",
    "end": "73920"
  },
  {
    "text": "applications using Amazon EMR and EMR",
    "start": "73920",
    "end": "77100"
  },
  {
    "text": "serverless",
    "start": "77100",
    "end": "79020"
  },
  {
    "text": "we will see how to use this integration",
    "start": "79020",
    "end": "81060"
  },
  {
    "text": "to read data from and write data into",
    "start": "81060",
    "end": "83580"
  },
  {
    "text": "Amazon redshift using EMR cluster on ec2",
    "start": "83580",
    "end": "87780"
  },
  {
    "text": "we will then see how to submit an Apache",
    "start": "87780",
    "end": "90180"
  },
  {
    "text": "spark job on Amazon redshift using EMR",
    "start": "90180",
    "end": "93000"
  },
  {
    "text": "serverless",
    "start": "93000",
    "end": "94860"
  },
  {
    "text": "let's start with EMR on ec2 I created an",
    "start": "94860",
    "end": "97860"
  },
  {
    "text": "EMR cluster with the name my cluster",
    "start": "97860",
    "end": "101220"
  },
  {
    "text": "it is using EMR version 6.9.0 and Spark",
    "start": "101220",
    "end": "105119"
  },
  {
    "text": "version 3.3.0",
    "start": "105119",
    "end": "107340"
  },
  {
    "text": "for redshift I'm using Amazon redshift",
    "start": "107340",
    "end": "109500"
  },
  {
    "text": "serverless have pre-created a serverless",
    "start": "109500",
    "end": "112439"
  },
  {
    "text": "namespace and serverless work group",
    "start": "112439",
    "end": "115140"
  },
  {
    "text": "I'm using an EMR notebook connected to",
    "start": "115140",
    "end": "117659"
  },
  {
    "text": "my cluster that I've previously shown to",
    "start": "117659",
    "end": "120180"
  },
  {
    "text": "run the spark commands",
    "start": "120180",
    "end": "122759"
  },
  {
    "text": "first provide redshift jdbc and Spark",
    "start": "122759",
    "end": "125579"
  },
  {
    "text": "redshift connector jars to the spark",
    "start": "125579",
    "end": "127320"
  },
  {
    "text": "application",
    "start": "127320",
    "end": "128700"
  },
  {
    "text": "these jars are locally available on EMR",
    "start": "128700",
    "end": "131099"
  },
  {
    "text": "clusters using version 6.9 and above and",
    "start": "131099",
    "end": "133860"
  },
  {
    "text": "EMR serverless",
    "start": "133860",
    "end": "135420"
  },
  {
    "text": "you can use them using a local path as",
    "start": "135420",
    "end": "137760"
  },
  {
    "text": "shown here",
    "start": "137760",
    "end": "140299"
  },
  {
    "text": "next import the spark modules and",
    "start": "142800",
    "end": "145260"
  },
  {
    "text": "initiate a spark session",
    "start": "145260",
    "end": "147959"
  },
  {
    "text": "next provide redshift options to",
    "start": "147959",
    "end": "151080"
  },
  {
    "text": "establish a connection between Apache",
    "start": "151080",
    "end": "152760"
  },
  {
    "text": "spark and Amazon redshift",
    "start": "152760",
    "end": "155099"
  },
  {
    "text": "you can authenticate using username and",
    "start": "155099",
    "end": "157140"
  },
  {
    "text": "password or an IAM role IEM",
    "start": "157140",
    "end": "160020"
  },
  {
    "text": "authentication is more secure as you",
    "start": "160020",
    "end": "162120"
  },
  {
    "text": "don't explicitly provide a username and",
    "start": "162120",
    "end": "164099"
  },
  {
    "text": "password",
    "start": "164099",
    "end": "165360"
  },
  {
    "text": "provide a jdbc URL that uses IAM",
    "start": "165360",
    "end": "168060"
  },
  {
    "text": "authentication as shown here",
    "start": "168060",
    "end": "171060"
  },
  {
    "text": "next provide an S3 path that acts as a",
    "start": "171060",
    "end": "173700"
  },
  {
    "text": "temporary directory and an IAM role that",
    "start": "173700",
    "end": "176640"
  },
  {
    "text": "redshift uses to connect to Amazon S3",
    "start": "176640",
    "end": "179940"
  },
  {
    "text": "I'm also setting a query group to spark",
    "start": "179940",
    "end": "183060"
  },
  {
    "text": "redshift in order to label the queries",
    "start": "183060",
    "end": "185160"
  },
  {
    "text": "executed using this connection",
    "start": "185160",
    "end": "187739"
  },
  {
    "text": "that is all you need to set up now we",
    "start": "187739",
    "end": "189900"
  },
  {
    "text": "are all set to use spark data frame apis",
    "start": "189900",
    "end": "192180"
  },
  {
    "text": "and Spark SQL apis",
    "start": "192180",
    "end": "195659"
  },
  {
    "text": "the new Amazon redshift spark",
    "start": "195659",
    "end": "197459"
  },
  {
    "text": "integration will push down many",
    "start": "197459",
    "end": "198840"
  },
  {
    "text": "operations to Amazon redshift so that",
    "start": "198840",
    "end": "201180"
  },
  {
    "text": "only the relevant data is moved from",
    "start": "201180",
    "end": "203340"
  },
  {
    "text": "Amazon redshift data warehouse to the",
    "start": "203340",
    "end": "205319"
  },
  {
    "text": "consuming spark application",
    "start": "205319",
    "end": "207599"
  },
  {
    "text": "I will now run some spark data frame",
    "start": "207599",
    "end": "209760"
  },
  {
    "text": "apis to show examples of some push terms",
    "start": "209760",
    "end": "213000"
  },
  {
    "text": "in my first example we will see push",
    "start": "213000",
    "end": "215580"
  },
  {
    "text": "down for join filter Aggregate and sort",
    "start": "215580",
    "end": "219360"
  },
  {
    "text": "we will use sales and date tables from",
    "start": "219360",
    "end": "222000"
  },
  {
    "text": "ticket data set to get total quantity",
    "start": "222000",
    "end": "224340"
  },
  {
    "text": "sold for each quarter in 2008.",
    "start": "224340",
    "end": "229080"
  },
  {
    "text": "I created a data frame sales DF for",
    "start": "229080",
    "end": "231659"
  },
  {
    "text": "sales table",
    "start": "231659",
    "end": "233340"
  },
  {
    "text": "similarly I have created a data frame",
    "start": "233340",
    "end": "235680"
  },
  {
    "text": "date DF for date table",
    "start": "235680",
    "end": "238440"
  },
  {
    "text": "I then performed a join between the date",
    "start": "238440",
    "end": "241379"
  },
  {
    "text": "data frame and sales data frame using",
    "start": "241379",
    "end": "243360"
  },
  {
    "text": "date ID column",
    "start": "243360",
    "end": "245099"
  },
  {
    "text": "and perform a filter for year 2008.",
    "start": "245099",
    "end": "249420"
  },
  {
    "text": "I then grouped by quarter and did a sum",
    "start": "249420",
    "end": "253140"
  },
  {
    "text": "on quantity sold",
    "start": "253140",
    "end": "256440"
  },
  {
    "text": "I then sorted the output on quarter",
    "start": "256440",
    "end": "260519"
  },
  {
    "text": "I'm running this code",
    "start": "260519",
    "end": "263660"
  },
  {
    "text": "it has executed and returned the result",
    "start": "267900",
    "end": "272060"
  },
  {
    "text": "I have created a function to get the",
    "start": "276540",
    "end": "278580"
  },
  {
    "text": "last issued query from redshift for the",
    "start": "278580",
    "end": "280800"
  },
  {
    "text": "query label spark underscore redshift",
    "start": "280800",
    "end": "283620"
  },
  {
    "text": "I'll execute the function now",
    "start": "283620",
    "end": "287600"
  },
  {
    "text": "as you can see in this query the inner",
    "start": "288300",
    "end": "291120"
  },
  {
    "text": "join the filter",
    "start": "291120",
    "end": "293160"
  },
  {
    "text": "the group by",
    "start": "293160",
    "end": "295259"
  },
  {
    "text": "the order by",
    "start": "295259",
    "end": "297419"
  },
  {
    "text": "the aggregation sum are all pushed down",
    "start": "297419",
    "end": "300240"
  },
  {
    "text": "to redshift and only relevant data is",
    "start": "300240",
    "end": "302580"
  },
  {
    "text": "sent back to spark",
    "start": "302580",
    "end": "305539"
  },
  {
    "text": "let's see another example of a push down",
    "start": "305639",
    "end": "308520"
  },
  {
    "text": "in this example we are counting the",
    "start": "308520",
    "end": "310800"
  },
  {
    "text": "distinct buyers in sales table",
    "start": "310800",
    "end": "313620"
  },
  {
    "text": "from the sales data frame I have",
    "start": "313620",
    "end": "315600"
  },
  {
    "text": "selected the buyer ID column and",
    "start": "315600",
    "end": "317580"
  },
  {
    "text": "performed a distinct and count",
    "start": "317580",
    "end": "319860"
  },
  {
    "text": "I'm going to execute this query now",
    "start": "319860",
    "end": "323400"
  },
  {
    "text": "here is the result I'm going to get the",
    "start": "323400",
    "end": "326039"
  },
  {
    "text": "last query issued to redshift one more",
    "start": "326039",
    "end": "327960"
  },
  {
    "text": "time to see the push Downs",
    "start": "327960",
    "end": "330240"
  },
  {
    "text": "and you can see that the count is pushed",
    "start": "330240",
    "end": "332639"
  },
  {
    "text": "down to redshift and distinct is",
    "start": "332639",
    "end": "334919"
  },
  {
    "text": "converted into more optimized Group by",
    "start": "334919",
    "end": "337979"
  },
  {
    "text": "also data is unloaded into optimized",
    "start": "337979",
    "end": "340380"
  },
  {
    "text": "parquet format",
    "start": "340380",
    "end": "342479"
  },
  {
    "text": "this is how you perform read operations",
    "start": "342479",
    "end": "344520"
  },
  {
    "text": "on redshift efficiently using Apache",
    "start": "344520",
    "end": "347280"
  },
  {
    "text": "spark to Amazon redshift integration",
    "start": "347280",
    "end": "351060"
  },
  {
    "text": "next let's see how to write data into",
    "start": "351060",
    "end": "353460"
  },
  {
    "text": "Amazon redshift",
    "start": "353460",
    "end": "355380"
  },
  {
    "text": "the raw data that we are going to load",
    "start": "355380",
    "end": "357240"
  },
  {
    "text": "is available in Json format in Amazon S3",
    "start": "357240",
    "end": "361500"
  },
  {
    "text": "the Json has data about customers",
    "start": "361500",
    "end": "363840"
  },
  {
    "text": "customer orders and the line items for",
    "start": "363840",
    "end": "366780"
  },
  {
    "text": "each order",
    "start": "366780",
    "end": "367919"
  },
  {
    "text": "let's create a data frame to read data",
    "start": "367919",
    "end": "370560"
  },
  {
    "text": "from Json",
    "start": "370560",
    "end": "373139"
  },
  {
    "text": "now let's look at the schema of the Json",
    "start": "373139",
    "end": "376979"
  },
  {
    "text": "as you can see the Json is a nested Json",
    "start": "376979",
    "end": "380100"
  },
  {
    "text": "it has data for each customer",
    "start": "380100",
    "end": "382979"
  },
  {
    "text": "an array of orders for each customer and",
    "start": "382979",
    "end": "386639"
  },
  {
    "text": "each order has an array of line items",
    "start": "386639",
    "end": "390600"
  },
  {
    "text": "we are going to load each of these",
    "start": "390600",
    "end": "392340"
  },
  {
    "text": "nested levels into three Separate Tables",
    "start": "392340",
    "end": "396500"
  },
  {
    "text": "we'll first create customer data frame",
    "start": "398699",
    "end": "400620"
  },
  {
    "text": "with the required fields from the",
    "start": "400620",
    "end": "402840"
  },
  {
    "text": "underlying Json data frame",
    "start": "402840",
    "end": "405960"
  },
  {
    "text": "we will then write the customer data",
    "start": "405960",
    "end": "408000"
  },
  {
    "text": "frame into Amazon redshift to a table",
    "start": "408000",
    "end": "410460"
  },
  {
    "text": "called customer",
    "start": "410460",
    "end": "413180"
  },
  {
    "text": "I'm executing this code",
    "start": "413220",
    "end": "416660"
  },
  {
    "text": "I'll now get the last issued query from",
    "start": "417360",
    "end": "419340"
  },
  {
    "text": "redshift as we have used the right mode",
    "start": "419340",
    "end": "421620"
  },
  {
    "text": "append spark would create a table if it",
    "start": "421620",
    "end": "424380"
  },
  {
    "text": "doesn't already exist and then copy the",
    "start": "424380",
    "end": "427139"
  },
  {
    "text": "data from S3 into redshift",
    "start": "427139",
    "end": "431240"
  },
  {
    "text": "similarly let's create an orders data",
    "start": "431520",
    "end": "433860"
  },
  {
    "text": "frame by selecting the required columns",
    "start": "433860",
    "end": "435840"
  },
  {
    "text": "for the orders table",
    "start": "435840",
    "end": "438060"
  },
  {
    "text": "we'll then write the orders data frame",
    "start": "438060",
    "end": "440039"
  },
  {
    "text": "into a table called order with append",
    "start": "440039",
    "end": "442800"
  },
  {
    "text": "mode",
    "start": "442800",
    "end": "445039"
  },
  {
    "text": "finally let's create a line item data",
    "start": "449280",
    "end": "452039"
  },
  {
    "text": "frame with the required columns for the",
    "start": "452039",
    "end": "453599"
  },
  {
    "text": "line item and write the line item data",
    "start": "453599",
    "end": "456479"
  },
  {
    "text": "frame to line item table",
    "start": "456479",
    "end": "459360"
  },
  {
    "text": "in this write operation we are going to",
    "start": "459360",
    "end": "461400"
  },
  {
    "text": "add a post action to refresh a",
    "start": "461400",
    "end": "463560"
  },
  {
    "text": "materialized view called MV total price",
    "start": "463560",
    "end": "465720"
  },
  {
    "text": "segment by ear",
    "start": "465720",
    "end": "467340"
  },
  {
    "text": "this materialized view is created on top",
    "start": "467340",
    "end": "470039"
  },
  {
    "text": "of customer orders and line items table",
    "start": "470039",
    "end": "472800"
  },
  {
    "text": "and the data is aggregated by segment",
    "start": "472800",
    "end": "475440"
  },
  {
    "text": "and year",
    "start": "475440",
    "end": "477919"
  },
  {
    "text": "with this the data load is complete and",
    "start": "480000",
    "end": "483000"
  },
  {
    "text": "materialized view is refreshed using",
    "start": "483000",
    "end": "485340"
  },
  {
    "text": "these steps we have loaded customer",
    "start": "485340",
    "end": "487139"
  },
  {
    "text": "table orders table line item table and",
    "start": "487139",
    "end": "491039"
  },
  {
    "text": "refresh the materialized view that is",
    "start": "491039",
    "end": "492720"
  },
  {
    "text": "created on top of these three tables",
    "start": "492720",
    "end": "495180"
  },
  {
    "text": "let's look at the data from the",
    "start": "495180",
    "end": "496620"
  },
  {
    "text": "materialized view",
    "start": "496620",
    "end": "499400"
  },
  {
    "text": "here is the data that has been loaded",
    "start": "501900",
    "end": "503940"
  },
  {
    "text": "and summarized",
    "start": "503940",
    "end": "506220"
  },
  {
    "text": "this is how you use Apache spark",
    "start": "506220",
    "end": "508620"
  },
  {
    "text": "integration to Amazon redshift to load",
    "start": "508620",
    "end": "511440"
  },
  {
    "text": "data into Amazon redshift",
    "start": "511440",
    "end": "513779"
  },
  {
    "text": "you have to use EMR serverless to submit",
    "start": "513779",
    "end": "516419"
  },
  {
    "text": "a spark job on Amazon redshift I'm going",
    "start": "516419",
    "end": "519300"
  },
  {
    "text": "to use AWS CLI for this purpose",
    "start": "519300",
    "end": "523200"
  },
  {
    "text": "I am using start job run command to",
    "start": "523200",
    "end": "525360"
  },
  {
    "text": "execute the job",
    "start": "525360",
    "end": "527339"
  },
  {
    "text": "to the start job run command I provide",
    "start": "527339",
    "end": "529560"
  },
  {
    "text": "an execution role with the required",
    "start": "529560",
    "end": "531120"
  },
  {
    "text": "permissions",
    "start": "531120",
    "end": "533600"
  },
  {
    "text": "I also provide the script that I would",
    "start": "533760",
    "end": "535680"
  },
  {
    "text": "like to execute",
    "start": "535680",
    "end": "537779"
  },
  {
    "text": "and the jars redshift jdbc jar and the",
    "start": "537779",
    "end": "540540"
  },
  {
    "text": "spark redshift jars",
    "start": "540540",
    "end": "543720"
  },
  {
    "text": "I provide optional configuration",
    "start": "543720",
    "end": "545640"
  },
  {
    "text": "overrides such as the log path",
    "start": "545640",
    "end": "549360"
  },
  {
    "text": "once I execute the CLI command I would",
    "start": "549360",
    "end": "552480"
  },
  {
    "text": "get a job run ID",
    "start": "552480",
    "end": "554279"
  },
  {
    "text": "I can check the status of this job using",
    "start": "554279",
    "end": "556680"
  },
  {
    "text": "get job run command",
    "start": "556680",
    "end": "559320"
  },
  {
    "text": "it is this simple to submit spark jobs",
    "start": "559320",
    "end": "561720"
  },
  {
    "text": "on Amazon redshift using EMR serverless",
    "start": "561720",
    "end": "565620"
  },
  {
    "text": "you can submit spark jobs on Amazon",
    "start": "565620",
    "end": "567540"
  },
  {
    "text": "redshift using this native integration",
    "start": "567540",
    "end": "569519"
  },
  {
    "text": "using AWS glue 4.0 as well",
    "start": "569519",
    "end": "572820"
  },
  {
    "text": "please go ahead and try the Amazon",
    "start": "572820",
    "end": "574560"
  },
  {
    "text": "redshift to spark integration",
    "start": "574560",
    "end": "577500"
  },
  {
    "text": "thanks for watching the demo have a good",
    "start": "577500",
    "end": "580019"
  },
  {
    "text": "day",
    "start": "580019",
    "end": "582200"
  }
]