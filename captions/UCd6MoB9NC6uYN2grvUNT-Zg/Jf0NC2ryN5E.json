[
  {
    "start": "0",
    "end": "119000"
  },
  {
    "text": "good afternoon so who's on their cell phone tweeting facebooking sending text messages",
    "start": "1760",
    "end": "8800"
  },
  {
    "text": "yes yeah we all love to multitask",
    "start": "8800",
    "end": "14639"
  },
  {
    "text": "so statistics actually show that four out of five americans actually multitask while watching tv",
    "start": "14719",
    "end": "22320"
  },
  {
    "text": "and that can be from sharing commentary on the content they're watching web browsing related to the the content",
    "start": "22320",
    "end": "29279"
  },
  {
    "text": "they're watching or totally something different like cooking dinner for the family working out in the gym",
    "start": "29279",
    "end": "36800"
  },
  {
    "text": "that's eighty percent of the total population that is watching on average four hours of tv per",
    "start": "36800",
    "end": "43920"
  },
  {
    "text": "day and the total ad spend in 2013 is estimated to be",
    "start": "43920",
    "end": "50280"
  },
  {
    "text": "503 billion dollars hi my name is usman shakil",
    "start": "50280",
    "end": "55920"
  },
  {
    "text": "and i'll be talking about maximizing audience engagement in this media delivery",
    "start": "55920",
    "end": "62800"
  },
  {
    "text": "so what do our customers of today want the customers really want",
    "start": "62960",
    "end": "68400"
  },
  {
    "text": "to watch the content that matters to them from anywhere for free",
    "start": "68400",
    "end": "74159"
  },
  {
    "text": "whatever type of device they're using uh they want to be able to easily",
    "start": "74159",
    "end": "80080"
  },
  {
    "text": "access the content it has to be nicely organized so it does not take me you know i don't know hours",
    "start": "80080",
    "end": "87040"
  },
  {
    "text": "just browsing through the content what i want to really watch and it has to be really high quality",
    "start": "87040",
    "end": "92799"
  },
  {
    "text": "without any sort of irrelevant interruptions like annoying ads so obviously some kind of",
    "start": "92799",
    "end": "99840"
  },
  {
    "text": "personalized ads will be great the things that i'm really i really care about or i'm interested in",
    "start": "99840",
    "end": "106159"
  },
  {
    "text": "and i would also like to multitask as well while watching tv for interactivity like second screen",
    "start": "106159",
    "end": "112960"
  },
  {
    "text": "social media and all that kind of stuff so really an ultimate customer",
    "start": "112960",
    "end": "118320"
  },
  {
    "text": "experience would entail an ultimate content discovery as well as the",
    "start": "118320",
    "end": "123840"
  },
  {
    "start": "119000",
    "end": "119000"
  },
  {
    "text": "ultimate content delivery and those are both things that we'll talk about today so starting with",
    "start": "123840",
    "end": "130239"
  },
  {
    "text": "content discovery if we take a step back and we take the",
    "start": "130239",
    "end": "136400"
  },
  {
    "start": "133000",
    "end": "133000"
  },
  {
    "text": "content choices evolution you know from the number of channels that we used to",
    "start": "136400",
    "end": "141520"
  },
  {
    "text": "have back in those good old days to today you know we've got a lot of",
    "start": "141520",
    "end": "148000"
  },
  {
    "text": "choices and even more in the case of you know uh the event of digital media that we have",
    "start": "148000",
    "end": "154879"
  },
  {
    "text": "all these uh online uh mediums and channels now that we can",
    "start": "154879",
    "end": "160000"
  },
  {
    "text": "watch and re-watch or replay that content",
    "start": "160000",
    "end": "165360"
  },
  {
    "text": "so yes a lot of content choices what about content discovery",
    "start": "165360",
    "end": "171840"
  },
  {
    "start": "169000",
    "end": "169000"
  },
  {
    "text": "again used to be those good old days of you know printed tv guides to the",
    "start": "171840",
    "end": "177920"
  },
  {
    "text": "epgs of your cable providers to you know uh second screen now you",
    "start": "177920",
    "end": "184879"
  },
  {
    "text": "know where you can actually look at the the um the the programming guide and you",
    "start": "184879",
    "end": "191040"
  },
  {
    "text": "know all the different programs that are gonna be in different channels etc while you're actually watching",
    "start": "191040",
    "end": "197519"
  },
  {
    "text": "so what that has what that means is really a lot of options a lot of choices out there",
    "start": "197519",
    "end": "204640"
  },
  {
    "text": "that's why we see a lot of our uh media companies today you know",
    "start": "204640",
    "end": "210640"
  },
  {
    "text": "spending a lot of resources a lot of effort on really making this a personalized experience in terms of",
    "start": "210640",
    "end": "217599"
  },
  {
    "text": "content discovery so we see in the case of samsung where they have unified search you know you have all your",
    "start": "217599",
    "end": "223200"
  },
  {
    "text": "favorites you have all your um different pieces of content that you want to watch and you can search across all these",
    "start": "223200",
    "end": "230400"
  },
  {
    "text": "different channels to netflix where you know they're they're spending a lot of resources to create things like",
    "start": "230400",
    "end": "236400"
  },
  {
    "text": "personalized role displays and similarity algorithms where it's it's really sort of almost like",
    "start": "236400",
    "end": "243599"
  },
  {
    "text": "rocket science you know to build these uh highly um",
    "start": "243599",
    "end": "249439"
  },
  {
    "text": "smart algorithms that that you know look at what a person is or what an audience",
    "start": "250080",
    "end": "256000"
  },
  {
    "text": "is really watching or wants to see and what their taste is based on their viewing habits and what to show or what",
    "start": "256000",
    "end": "262560"
  },
  {
    "text": "content really means to them and then the second piece is content delivery so yeah once i am you know",
    "start": "262560",
    "end": "270080"
  },
  {
    "text": "i search for the content that i'm looking for i'm interested in watching it",
    "start": "270080",
    "end": "275360"
  },
  {
    "text": "i need to have a great content uh viewing experience as well",
    "start": "275360",
    "end": "280639"
  },
  {
    "text": "so conviva a viewer experience report of 2013 shows that uh the",
    "start": "280639",
    "end": "287360"
  },
  {
    "text": "average play time per buffering percent um",
    "start": "287360",
    "end": "292400"
  },
  {
    "start": "288000",
    "end": "288000"
  },
  {
    "text": "is is really directly related to the number of the amount of buffering that you see so basically",
    "start": "292400",
    "end": "297680"
  },
  {
    "text": "the the better the quality the more viewing time you're gonna get",
    "start": "297680",
    "end": "303040"
  },
  {
    "text": "out of your audience and what this really entails is more",
    "start": "303040",
    "end": "308240"
  },
  {
    "text": "time that the end user has on that screen or watching your content",
    "start": "308240",
    "end": "314639"
  },
  {
    "text": "more time with you much more monetization opportunities and of course",
    "start": "314639",
    "end": "320000"
  },
  {
    "text": "who likes these right it'd be great to see something that i",
    "start": "320000",
    "end": "325039"
  },
  {
    "text": "really care about or i'm interested in so really what's happening is that there is",
    "start": "325039",
    "end": "333199"
  },
  {
    "text": "actually mountains of raw data from what a user is doing on the screen when",
    "start": "333199",
    "end": "340479"
  },
  {
    "text": "they are trying to search for a particular piece of content uh you know what different parts of a",
    "start": "340479",
    "end": "346639"
  },
  {
    "text": "website or device or within that page they've gone and you know uh browsed to",
    "start": "346639",
    "end": "353759"
  },
  {
    "text": "actually uh you know what are they doing during uh while viewing that piece of content so",
    "start": "353759",
    "end": "360479"
  },
  {
    "text": "that leads us to two different distinctions between content discovery and content delivery that within content",
    "start": "360479",
    "end": "366960"
  },
  {
    "text": "discovery you have the metadata the session logs uh for that particular",
    "start": "366960",
    "end": "372639"
  },
  {
    "text": "session that you can capture and you can find out how did a end user reached to a conclusion or a",
    "start": "372639",
    "end": "379840"
  },
  {
    "text": "piece of content that they really want to watch to content delivery that what their viewing habits were did they watch the",
    "start": "379840",
    "end": "386080"
  },
  {
    "text": "entire piece of content did they skip in the middle did they close the session before that",
    "start": "386080",
    "end": "392000"
  },
  {
    "text": "did they suffer any sort of like buffering or quality issues um any sort of page click events what happened at",
    "start": "392000",
    "end": "398960"
  },
  {
    "text": "the cdn layer you know to application logs if you have some sort of like a smart player that is streaming that",
    "start": "398960",
    "end": "405520"
  },
  {
    "text": "piece of content so in other words a lot of data",
    "start": "405520",
    "end": "410880"
  },
  {
    "text": "so some numbers from netflix nearly uh 38 million customers",
    "start": "410880",
    "end": "417199"
  },
  {
    "text": "that are you know or unique customers that netflix has in more than 50 countries and territories",
    "start": "417199",
    "end": "425199"
  },
  {
    "text": "that translates to hundreds of billion of events",
    "start": "425199",
    "end": "430479"
  },
  {
    "text": "basically that are being generated uh in a very very short period of time so just imagine these 38 million customers how",
    "start": "430479",
    "end": "438160"
  },
  {
    "text": "many of those customers are actually watching content across 50 different countries",
    "start": "438160",
    "end": "443280"
  },
  {
    "text": "and what are the different events that are happening while that content is being streamed",
    "start": "443280",
    "end": "448400"
  },
  {
    "text": "at the same time there is over a hundred billion metadata events that are being triggered due to just due to the",
    "start": "448400",
    "end": "455039"
  },
  {
    "text": "activity that people have on say the netflix website or you know the the the",
    "start": "455039",
    "end": "460639"
  },
  {
    "text": "device where where they're searching this",
    "start": "460639",
    "end": "465240"
  },
  {
    "text": "and how can the challenges how can we take this mountains of raw data that we just talked about",
    "start": "465840",
    "end": "471840"
  },
  {
    "text": "to useful information as soon as possible kind of very similar to you know i have",
    "start": "471840",
    "end": "477120"
  },
  {
    "text": "a bunch of something meaningless like the analogy i used here like trash and i want to sort it out on you know how i",
    "start": "477120",
    "end": "484400"
  },
  {
    "text": "want to use it for some something useful and that useful could be either historical data that i can use for batch",
    "start": "484400",
    "end": "491520"
  },
  {
    "text": "analysis or it could be live data as it comes in for some real-time analysis",
    "start": "491520",
    "end": "498720"
  },
  {
    "text": "so looking at the historical versus real-time analytics continuum here",
    "start": "498720",
    "end": "504879"
  },
  {
    "start": "502000",
    "end": "502000"
  },
  {
    "text": "you know i could either do real time which is 100 dynamic meaning as the data",
    "start": "504879",
    "end": "510960"
  },
  {
    "text": "is or as those events are being generated uh during a live session i",
    "start": "510960",
    "end": "516719"
  },
  {
    "text": "somehow capture those events and somehow process those events and i'm able to um",
    "start": "516719",
    "end": "523360"
  },
  {
    "text": "you know get some useful information out of it or it could be batch where it's a",
    "start": "523360",
    "end": "528560"
  },
  {
    "text": "you know i capture this information over a period of time and then i i could",
    "start": "528560",
    "end": "534000"
  },
  {
    "text": "basically uh have some an analytics processing happening behind",
    "start": "534000",
    "end": "539040"
  },
  {
    "text": "the scenes and i could get some meaningful information out of it and then i could use this analysis for",
    "start": "539040",
    "end": "544160"
  },
  {
    "text": "future purposes so yes obviously i would love to have",
    "start": "544160",
    "end": "549200"
  },
  {
    "text": "real-time information but there are a lot of challenges with it that is that it has to be computed on",
    "start": "549200",
    "end": "554560"
  },
  {
    "text": "the fly it's very flexible but processing can take a lot of time right and a lot of",
    "start": "554560",
    "end": "561200"
  },
  {
    "text": "power we all know that in this at this scale so really it becomes a scale problem that scale is very very hard",
    "start": "561200",
    "end": "568959"
  },
  {
    "text": "on in the case of batch um you know the reason i can't really use batch for everything is because it's predefined",
    "start": "568959",
    "end": "576560"
  },
  {
    "text": "queries it's rigid i can't really run a query on the fly on a real time or set of events that are",
    "start": "576560",
    "end": "583760"
  },
  {
    "text": "happening so what i tried to do here was i really tried to divide it",
    "start": "583760",
    "end": "589760"
  },
  {
    "text": "based on the use case so on the real-time side that's really on the content delivery that i want to do some",
    "start": "589760",
    "end": "596399"
  },
  {
    "text": "sort of real-time capture real-time analytics and use that very useful information in my content delivery use",
    "start": "596399",
    "end": "603200"
  },
  {
    "text": "case whereas i can use this historical metadata and do some processing on the",
    "start": "603200",
    "end": "609600"
  },
  {
    "text": "in the back end and use this analysis for my content discovery purposes",
    "start": "609600",
    "end": "614880"
  },
  {
    "text": "so then defining the challenge here mountains of raw data coming from these events that",
    "start": "614880",
    "end": "621120"
  },
  {
    "start": "620000",
    "end": "620000"
  },
  {
    "text": "are happening from very very large number of user sessions out there",
    "start": "621120",
    "end": "626320"
  },
  {
    "text": "i need a way to s a scalable way to ingest and stream",
    "start": "626320",
    "end": "631680"
  },
  {
    "text": "these mountains of raw data then i need a way to do real-time processing at scale",
    "start": "631680",
    "end": "639120"
  },
  {
    "text": "and then store it in a durable way and in a way that it is accessible to me",
    "start": "639120",
    "end": "645279"
  },
  {
    "text": "for batch processing and then i could create either dashboards trigger events and stuff like",
    "start": "645279",
    "end": "651920"
  },
  {
    "text": "that and take very useful actions on the fly or based on the user sessions to create",
    "start": "651920",
    "end": "659279"
  },
  {
    "text": "really personalized experience for my end user so really that's going to be the track",
    "start": "659279",
    "end": "665680"
  },
  {
    "text": "that we talk about how we do ultimate content discovery and i leave it to our",
    "start": "665680",
    "end": "673360"
  },
  {
    "text": "to from netflix who and she will talk about how netflix creates personalized content",
    "start": "673360",
    "end": "680240"
  },
  {
    "text": "and the power of metadata and ultimate content delivery where i'll talk about",
    "start": "680240",
    "end": "685360"
  },
  {
    "text": "the tool set for real real-time big data processing and how it can be used in the context of media delivery so i'll",
    "start": "685360",
    "end": "692160"
  },
  {
    "text": "introduce shobna from netflix",
    "start": "692160",
    "end": "697040"
  },
  {
    "text": "thanks osman um how many of you have a netflix service",
    "start": "701600",
    "end": "706959"
  },
  {
    "text": "or have used it most of you uh almost all um so you probably know that we are",
    "start": "706959",
    "end": "713360"
  },
  {
    "text": "extremely passionate about making sure that the users have the most compelling choices in front of them when they're",
    "start": "713360",
    "end": "719519"
  },
  {
    "text": "deciding what to watch on an evening so why are we so obsessed with",
    "start": "719519",
    "end": "725440"
  },
  {
    "text": "personalization the two key metrics we care about as a streaming service are the median viewing",
    "start": "725440",
    "end": "732000"
  },
  {
    "text": "hours which is an indication of how much our subscribers like what they chose to see",
    "start": "732000",
    "end": "737360"
  },
  {
    "text": "and our net subscribers so we want to grow add more subscribers but we also want to retain subscribers and keep them",
    "start": "737360",
    "end": "744240"
  },
  {
    "text": "happy with our service and through our a b tests as well as our different product rollouts it's been",
    "start": "744240",
    "end": "750399"
  },
  {
    "text": "clear to us that personalization consistently improves both of these metrics",
    "start": "750399",
    "end": "755600"
  },
  {
    "text": "and also a significant percentage of what people watch comes from recommendations provided by",
    "start": "755600",
    "end": "761279"
  },
  {
    "text": "us so how do we do this a critical piece of",
    "start": "761279",
    "end": "766480"
  },
  {
    "text": "enabling this personalization is making sure that all the user facing applications have access to",
    "start": "766480",
    "end": "772399"
  },
  {
    "text": "all the data possible about our tv shows and movies and this is referred to as video metadata",
    "start": "772399",
    "end": "778880"
  },
  {
    "text": "simply put video metadata is anything you have seen about movies it could be genre cast rating just contract",
    "start": "778880",
    "end": "786160"
  },
  {
    "text": "information do we have an exclusive contract are we allowed to show it in all countries how long does this run for what",
    "start": "786160",
    "end": "792560"
  },
  {
    "text": "languages are we allowed to show even information like that and all the information about what",
    "start": "792560",
    "end": "797680"
  },
  {
    "text": "devices we have encoding for what bit rates we can support for each of the shows how many episodes do we have",
    "start": "797680",
    "end": "802800"
  },
  {
    "text": "trailers for so all this is referred to as streaming information any information about assets or the actual streams",
    "start": "802800",
    "end": "809440"
  },
  {
    "text": "and also the deployment information itself which origin service or is this content hosted which one is closest to",
    "start": "809440",
    "end": "814959"
  },
  {
    "text": "the locale that's interested most in this movie even those are referred to as metadata",
    "start": "814959",
    "end": "820880"
  },
  {
    "text": "and a number of pieces are actually localized for example the subtitles dubbing",
    "start": "820880",
    "end": "826320"
  },
  {
    "text": "trailers stills even the titles on the billboards all of those are localized and therefore tailored again to the",
    "start": "826320",
    "end": "832480"
  },
  {
    "text": "audience this is also referred to as metadata so there is a wealth of data about the",
    "start": "832480",
    "end": "837760"
  },
  {
    "text": "movies that we can use to make sure we provide the best possible viewing experience",
    "start": "837760",
    "end": "844639"
  },
  {
    "text": "what is it used for providing user using user specific choices like",
    "start": "846639",
    "end": "852720"
  },
  {
    "text": "language or even the viewing behavior of the specific culture of their locale their taste preferences",
    "start": "852720",
    "end": "859600"
  },
  {
    "text": "all our recommendation algorithms you probably know about similarity and a number of like top 10 recommendations",
    "start": "859600",
    "end": "864800"
  },
  {
    "text": "from your netflix interface pretty much every row that's presented to the user is running some algorithm",
    "start": "864800",
    "end": "870800"
  },
  {
    "text": "which heavily leverages our metadata device rendering and playback as we talked about earlier",
    "start": "870800",
    "end": "876959"
  },
  {
    "text": "cdn deployments all the assets related to the movie or the show",
    "start": "876959",
    "end": "883440"
  },
  {
    "text": "original programming has been a big part of our focus recently how many of you have seen house of cards or know about",
    "start": "883440",
    "end": "889519"
  },
  {
    "text": "it how many of you know that we won emmys for house of cards",
    "start": "889519",
    "end": "895360"
  },
  {
    "text": "great so you can see why we focus on this part of our business as well so international",
    "start": "895360",
    "end": "900480"
  },
  {
    "text": "expansion as well as original programming are have been our focus and are going to continue to be a big part of our focus",
    "start": "900480",
    "end": "906639"
  },
  {
    "text": "so basically everything anything that's related to providing a rich experience for the user is going to",
    "start": "906639",
    "end": "913040"
  },
  {
    "text": "heavily use metadata so suspend was talking about earlier we have a number of data sources the",
    "start": "913040",
    "end": "920000"
  },
  {
    "text": "processing that it takes to enter this data into our systems whether it's encoding or just review of the content",
    "start": "920000",
    "end": "925279"
  },
  {
    "text": "automatic generation re-encoding of the content all these generate metadata there are distributed workflows done by",
    "start": "925279",
    "end": "931279"
  },
  {
    "text": "several teams and tools that feed metadata into our system now our job in our video metadata",
    "start": "931279",
    "end": "938079"
  },
  {
    "text": "infrastructure is to make sure that we extract the relevant data that each application cares about and provide it",
    "start": "938079",
    "end": "945199"
  },
  {
    "text": "in the most efficient way possible for these applications because as you know availability is a",
    "start": "945199",
    "end": "950639"
  },
  {
    "text": "big deal for us we guarantee four nines for our streaming service which heavily uses metadata so the",
    "start": "950639",
    "end": "956160"
  },
  {
    "text": "playback uses metadata so we need to be really efficient about this so at high level our solution was uh to",
    "start": "956160",
    "end": "962639"
  },
  {
    "start": "957000",
    "end": "957000"
  },
  {
    "text": "have a publishing engine which understands the different sources of truth and is able to operate on that piece of data make it very efficient",
    "start": "962639",
    "end": "969839"
  },
  {
    "text": "generate slices of the data that our applications care about and write it to s3 and then on the client side we",
    "start": "969839",
    "end": "976880"
  },
  {
    "text": "provide an object cache so our all the applications netflix all they have to do is pull this jar in and they're able to",
    "start": "976880",
    "end": "983759"
  },
  {
    "text": "access the objects they care about they don't even know where it comes from how it gets refreshed into memory all they",
    "start": "983759",
    "end": "990000"
  },
  {
    "text": "know is they get a very efficient access and they have guaranteed in memory access to all of this data",
    "start": "990000",
    "end": "996880"
  },
  {
    "text": "and their availability is not at risk with a solution that's really all they should care about",
    "start": "996880",
    "end": "1003519"
  },
  {
    "text": "so our architecture looks like this as i said earlier the snapshot files are also referred to",
    "start": "1004000",
    "end": "1010720"
  },
  {
    "text": "as col start files because we can in seconds pin uh systems to an old",
    "start": "1010720",
    "end": "1016880"
  },
  {
    "text": "known good snapshot of the data if we detect that there's anything bad about the data and that's why it's also",
    "start": "1016880",
    "end": "1022959"
  },
  {
    "text": "referred to as colstar so with every cycle the publishing engine generates about 10 facets per",
    "start": "1022959",
    "end": "1029360"
  },
  {
    "text": "country and writes them to s3 in this architecture um and most of our",
    "start": "1029360",
    "end": "1034640"
  },
  {
    "text": "application so when we started off there were a couple of thousand instances across our applications all of them",
    "start": "1034640",
    "end": "1040480"
  },
  {
    "text": "using ec2 and they used to access this data on s3",
    "start": "1040480",
    "end": "1045520"
  },
  {
    "text": "during cache refresh so in terms of the data flow uh just a",
    "start": "1045520",
    "end": "1052880"
  },
  {
    "text": "simple sequencing diagram to demonstrate that so metadata gets entered through these variety of sources",
    "start": "1052880",
    "end": "1058880"
  },
  {
    "text": "there are hourly data snapshots again the frequency can be adjusted uh we cannot we also now support event based",
    "start": "1058880",
    "end": "1065360"
  },
  {
    "text": "generation of data on the site the publishing engine periodically",
    "start": "1065360",
    "end": "1070640"
  },
  {
    "text": "checks do you have any updates for me it looks at the deltas",
    "start": "1070640",
    "end": "1075679"
  },
  {
    "text": "it generates and writes uh also the artifacts to s3 and then the",
    "start": "1076000",
    "end": "1081120"
  },
  {
    "text": "cache through its periodic refresh is going to get this data from s3 what this means also is uh since we",
    "start": "1081120",
    "end": "1088320"
  },
  {
    "text": "extensively use auto scaling from aws we need to make sure that our cache warm-up",
    "start": "1088320",
    "end": "1093600"
  },
  {
    "text": "is very quick very efficient because our instances are being dynamically started all the time for all our applications so",
    "start": "1093600",
    "end": "1099520"
  },
  {
    "text": "this cache can't slow down the startup time so that is a big deal for us and the periodic refreshes need to be even",
    "start": "1099520",
    "end": "1105840"
  },
  {
    "text": "more efficient and non-invasive so at this point all the applications",
    "start": "1105840",
    "end": "1110880"
  },
  {
    "text": "have to do is make java api calls and they get access to the data and serve the users",
    "start": "1110880",
    "end": "1118320"
  },
  {
    "text": "so our initial landscapers we establish ourselves in u.s and canada first and as",
    "start": "1119360",
    "end": "1124880"
  },
  {
    "text": "i said earlier a couple of thousand ec2 instances were accessing us so this",
    "start": "1124880",
    "end": "1130160"
  },
  {
    "text": "architecture worked really well 100 of the data is always in memory so you're guaranteed to have the in-memory latency",
    "start": "1130160",
    "end": "1136400"
  },
  {
    "text": "for all of the data so the target application looks something like this in terms of file",
    "start": "1136400",
    "end": "1141919"
  },
  {
    "text": "size number of instances you can do the math in terms of the s3 reads per cycle",
    "start": "1141919",
    "end": "1148240"
  },
  {
    "text": "and our availability goals are pretty stringent um so our solution was to provide 100 of the access in memory",
    "start": "1148240",
    "end": "1157440"
  },
  {
    "text": "and then with an ever-growing business like netflix nothing stays static as we all know so a few things happened that",
    "start": "1158880",
    "end": "1165600"
  },
  {
    "text": "made us uh enhance this further so we went to latin america which added",
    "start": "1165600",
    "end": "1171039"
  },
  {
    "text": "a bunch of countries uh to the mix and we've expanded since we most recently launched in netherlands in the past",
    "start": "1171039",
    "end": "1176840"
  },
  {
    "text": "september and we also had more applications in netflix that started using uh this to personalize their",
    "start": "1176840",
    "end": "1183520"
  },
  {
    "text": "algorithms even more so the number of instances that needed to access this data also grew",
    "start": "1183520",
    "end": "1188559"
  },
  {
    "text": "significantly so we had a problem to solve or scale",
    "start": "1188559",
    "end": "1196720"
  },
  {
    "text": "so from the previous slide these are the corrections so with the same architecture as before where we wrote 10 facets per country per cycle",
    "start": "1197120",
    "end": "1204320"
  },
  {
    "start": "1200000",
    "end": "1200000"
  },
  {
    "text": "now we're writing every cycle we have 500 writes um and a large number of instances",
    "start": "1204320",
    "end": "1210960"
  },
  {
    "text": "trying to access all these files and refresh themselves but we still have the same availability",
    "start": "1210960",
    "end": "1216480"
  },
  {
    "text": "availability guarantees to work with the same make sure that the cache can warm up quickly and applications can start up",
    "start": "1216480",
    "end": "1222799"
  },
  {
    "text": "quickly for auto scaling so as you might expect with that uh with",
    "start": "1222799",
    "end": "1228799"
  },
  {
    "text": "the initial implementation as the data grew and the number of fetches grew the file rights we saw were slower",
    "start": "1228799",
    "end": "1235120"
  },
  {
    "text": "because just a lot more data being written longer publish time because the publishing engine is trying to write a",
    "start": "1235120",
    "end": "1241200"
  },
  {
    "text": "lot more things with each cycle and also it's processing deduping filtering through a lot more data",
    "start": "1241200",
    "end": "1249039"
  },
  {
    "text": "and since there was a lot of client-side processing to read all these facets dedupe and then figure out this is the",
    "start": "1249039",
    "end": "1254799"
  },
  {
    "text": "part i need there was overhead involved in the startup and refresh as well so there was a higher",
    "start": "1254799",
    "end": "1260960"
  },
  {
    "text": "cost on the client side so a couple of key things we did is one",
    "start": "1260960",
    "end": "1267440"
  },
  {
    "text": "we re-architected the publishing engine to be region based instead of just a simple one per country",
    "start": "1267440",
    "end": "1273679"
  },
  {
    "text": "or it can even be configured to be one global server which would uh crunch all the data dupe and and do a lot of processing in",
    "start": "1273679",
    "end": "1281440"
  },
  {
    "text": "the pre-write phase which is before even writing the facets to s3 and we also came up with this design of",
    "start": "1281440",
    "end": "1288400"
  },
  {
    "text": "blob images which are not just slices of data generated per country but it supported a combination of both",
    "start": "1288400",
    "end": "1294240"
  },
  {
    "text": "horizontal and vertical slicing for example you could say i want only data for lat m or i want only data for this",
    "start": "1294240",
    "end": "1300000"
  },
  {
    "text": "region because you may be a regionally distributed service or you could say don't give me streams",
    "start": "1300000",
    "end": "1305120"
  },
  {
    "text": "data because i i don't care about that because maybe i'm an algorithm that doesn't use that data so we allowed a more intelligent slicing",
    "start": "1305120",
    "end": "1312080"
  },
  {
    "text": "of the data so the number of images we produced also could go down for satisfying all",
    "start": "1312080",
    "end": "1317679"
  },
  {
    "text": "the needs so from 500 we were able to bring it to basically writing 10 blobs per cycle to",
    "start": "1317679",
    "end": "1323679"
  },
  {
    "text": "s3 and as i said earlier a lot of the deduping a lot of the processing was",
    "start": "1323679",
    "end": "1328960"
  },
  {
    "text": "moved to the pre-write which means the client side processing has become much simpler now and so the uh cash warm-up",
    "start": "1328960",
    "end": "1336400"
  },
  {
    "text": "as in start-up time and refresh times went down significantly another thing we did which was not",
    "start": "1336400",
    "end": "1342480"
  },
  {
    "text": "necessarily a change later that's the way we did it from the initial is the cold start files are written in zipped",
    "start": "1342480",
    "end": "1347840"
  },
  {
    "text": "formats so the smaller the files you write to s3 the fewer the files you write the more efficient your",
    "start": "1347840",
    "end": "1353360"
  },
  {
    "text": "application is going to be um we also use the trach from the publishing engine",
    "start": "1353360",
    "end": "1359840"
  },
  {
    "text": "down to the client since after the initial startup applications are more interested in",
    "start": "1359840",
    "end": "1365039"
  },
  {
    "text": "knowing what has changed since i last tried they're not interested in the entire snapshot the snapshot is really",
    "start": "1365039",
    "end": "1370159"
  },
  {
    "text": "for fallback or for recovery so we prioritized to write all the deltas first so that we could get that quickly",
    "start": "1370159",
    "end": "1376480"
  },
  {
    "text": "in the hands of the application and the snapshot could come later so we used even small tricks like this to make sure",
    "start": "1376480",
    "end": "1382320"
  },
  {
    "text": "our cross region publishing as well as our delta and snapshot publishing was uh very efficient",
    "start": "1382320",
    "end": "1389639"
  },
  {
    "text": "so we did observe a significant uh reduction in the average memory footprint of the cache",
    "start": "1390880",
    "end": "1396000"
  },
  {
    "text": "in the application itself because a lot of the deduping and optimization is happening on the server side",
    "start": "1396000",
    "end": "1402559"
  },
  {
    "text": "we saw as a result of this significant reduction in the application startup times",
    "start": "1402559",
    "end": "1407679"
  },
  {
    "text": "and also shorter publish times because we were writing fewer files per cycle",
    "start": "1407679",
    "end": "1415240"
  },
  {
    "text": "a few things we learned in the process and in memory cache we leverage",
    "start": "1416159",
    "end": "1423520"
  },
  {
    "text": "netflix graph which is actually open sourced if you're interested there is a write up on this on our tech blog",
    "start": "1423520",
    "end": "1429600"
  },
  {
    "text": "this is very effective for high availability because in our design we have basically eliminated network",
    "start": "1429600",
    "end": "1434880"
  },
  {
    "text": "latency for real-time requests with this solution",
    "start": "1434880",
    "end": "1440399"
  },
  {
    "text": "started time is is critical when you're employing aws auto scaling so that's always been a key metric for us to",
    "start": "1440480",
    "end": "1446320"
  },
  {
    "text": "optimize in fact now we are on a continuous deployment cycle where we actually measure the startup time for",
    "start": "1446320",
    "end": "1452960"
  },
  {
    "text": "each of our blob image combinations for every check-in so you can detect regression pretty quickly and adjust it",
    "start": "1452960",
    "end": "1458559"
  },
  {
    "text": "so every release goes out uh with us knowing exactly how the footprint and the behavioral characteristics are going",
    "start": "1458559",
    "end": "1465200"
  },
  {
    "text": "to be when our cache runs within an application uh definitely use the best practices",
    "start": "1465200",
    "end": "1472480"
  },
  {
    "text": "documented uh by amazon a couple of other things this is an",
    "start": "1472480",
    "end": "1477600"
  },
  {
    "text": "extremely uh distributed system where there's a lot of data processing going on at every stage so we employed circuit",
    "start": "1477600",
    "end": "1483919"
  },
  {
    "text": "breakers both as you saw it's a mix of both offline processing and real-time access so we had checks and balances to make",
    "start": "1483919",
    "end": "1490640"
  },
  {
    "text": "sure that the data didn't deviate much from what we would expect so when we saw a trend of data where for example say the",
    "start": "1490640",
    "end": "1497120"
  },
  {
    "text": "number of titles going live was much more than expected or much less than expected it would it would alert",
    "start": "1497120",
    "end": "1502799"
  },
  {
    "text": "automatically and trigger a whole on-call process to fix well before a customer could notice it",
    "start": "1502799",
    "end": "1508640"
  },
  {
    "text": "and as i said earlier we could pin back to a known good snapshot in seconds so as long as there was a very good",
    "start": "1508640",
    "end": "1515200"
  },
  {
    "text": "monitoring across the board and there was a clear way of alerting crisply we were able to roll all these fundamental",
    "start": "1515200",
    "end": "1522000"
  },
  {
    "text": "changes out without any impact to the customers and also for uh because we have a very",
    "start": "1522000",
    "end": "1528880"
  },
  {
    "text": "lightweight coordination culture um even though more than 25 teams use our systems and we work with almost 10 teams",
    "start": "1528880",
    "end": "1535360"
  },
  {
    "text": "on the source side we were able to roll this these changes out in a matter of weeks because most of the teams in fact",
    "start": "1535360",
    "end": "1542480"
  },
  {
    "text": "all the teams have uh automation and continuous deployment as part of their uh life cycle",
    "start": "1542480",
    "end": "1547679"
  },
  {
    "text": "so it's it's actually very lightweight to make even pretty fundamental changes uh such as this one",
    "start": "1547679",
    "end": "1554400"
  },
  {
    "text": "um so with that i just wanted to share a few things we're thinking about for even further scale if you think of say five",
    "start": "1555440",
    "end": "1562799"
  },
  {
    "text": "years from now or ten years from now the growth for netflix as well as what we will support a lot more",
    "start": "1562799",
    "end": "1568559"
  },
  {
    "text": "data is going to go through the system we're going to be serving a much higher number of requests from users",
    "start": "1568559",
    "end": "1574640"
  },
  {
    "text": "so a few things we are already starting to think about is providing a number of dimensions to dynamically control the",
    "start": "1574640",
    "end": "1581360"
  },
  {
    "text": "caches behavior because each of our applications has a different access pattern so it's not possible for one team to",
    "start": "1581360",
    "end": "1587200"
  },
  {
    "text": "think about everything and provide one solution that fits all so we're going to do a lot more of that",
    "start": "1587200",
    "end": "1594000"
  },
  {
    "text": "we're experimenting with dynamic code inclusion where different people can contribute and shape how the optimization works",
    "start": "1594000",
    "end": "1600880"
  },
  {
    "text": "and also we are looking to parallelize our publishing engine itself because as more data goes through it and more complex relationships between the data",
    "start": "1600880",
    "end": "1607679"
  },
  {
    "text": "develop it's going to become more complex and probably more heavyweight to do the publishing side as",
    "start": "1607679",
    "end": "1613440"
  },
  {
    "text": "well so these are three angles along which we're already thinking about for further expansion",
    "start": "1613440",
    "end": "1620480"
  },
  {
    "text": "great with that let me turn back to osman",
    "start": "1620480",
    "end": "1625960"
  },
  {
    "text": "all right so with these great techniques metadata so",
    "start": "1631840",
    "end": "1637200"
  },
  {
    "text": "assuming with the great customer uh or content discovery the customer is now",
    "start": "1637200",
    "end": "1644400"
  },
  {
    "text": "you know already set to what they want to watch now time comes for content delivery we want to give them the ultimate content",
    "start": "1644400",
    "end": "1651120"
  },
  {
    "text": "delivery and what i want to really talk about here is take this opportunity to talk about the tool set that's available",
    "start": "1651120",
    "end": "1657760"
  },
  {
    "text": "out there on aws that will allow you to build a real-time big data processing",
    "start": "1657760",
    "end": "1663679"
  },
  {
    "text": "application that you can then use for real-time decision making to get to this",
    "start": "1663679",
    "end": "1669200"
  },
  {
    "text": "ultimate content delivery so back to the challenge again we have",
    "start": "1669200",
    "end": "1676240"
  },
  {
    "text": "mountains of raw data coming in have to somehow ingest stream into real-time",
    "start": "1676240",
    "end": "1681520"
  },
  {
    "text": "processing and then use it for dashboards or personalized user experience",
    "start": "1681520",
    "end": "1687039"
  },
  {
    "text": "so first comes ingest and stream so here going back to our real-time versus batch continuum i",
    "start": "1687039",
    "end": "1694840"
  },
  {
    "text": "have on the right hand side starting from batch use cases amazon s3",
    "start": "1694840",
    "end": "1701520"
  },
  {
    "start": "1698000",
    "end": "1698000"
  },
  {
    "text": "you're probably familiar with amazon s3 highly scalable simple storage service allows you to store static objects so",
    "start": "1701520",
    "end": "1709520"
  },
  {
    "text": "basically one thing would be as those events are being captured as those events are being",
    "start": "1709520",
    "end": "1715520"
  },
  {
    "text": "happening while the content is being delivered you keep on putting those things into s3 start dumping into s3 one",
    "start": "1715520",
    "end": "1722240"
  },
  {
    "text": "beautiful thing about s3 is that it scales automatically for your use so",
    "start": "1722240",
    "end": "1727360"
  },
  {
    "text": "you know you can just start this fire hose off these logs going into s3 and start dumping them in there",
    "start": "1727360",
    "end": "1733600"
  },
  {
    "text": "next down the list is sqs and dynamodb if you notice i put an arrow in",
    "start": "1733600",
    "end": "1739120"
  },
  {
    "text": "there because these are dynamic because sqs scales based on",
    "start": "1739120",
    "end": "1744720"
  },
  {
    "text": "the based on the the traffic or based on you know how you how you're putting or the",
    "start": "1744720",
    "end": "1750480"
  },
  {
    "text": "the messages into the queue and dynamodb you can really provision it for the throughput that you're looking for so",
    "start": "1750480",
    "end": "1756960"
  },
  {
    "text": "you can really scale these things as needed but",
    "start": "1756960",
    "end": "1762320"
  },
  {
    "text": "s3 sqs dynamodb really work for the batch use case where you have a lot of",
    "start": "1762720",
    "end": "1768240"
  },
  {
    "text": "data that you are somehow capturing into these uh you know storage uh backhand",
    "start": "1768240",
    "end": "1773679"
  },
  {
    "text": "systems and then you are using some sort of like a you know batch processing like hadoop",
    "start": "1773679",
    "end": "1780000"
  },
  {
    "text": "etc to run some sort of analytics on it and then use it uh for for your batch",
    "start": "1780000",
    "end": "1785360"
  },
  {
    "text": "processing needs how about real time so i like to talk about apache kafka",
    "start": "1785360",
    "end": "1791840"
  },
  {
    "text": "it's an open source project basically gives you the capability to build",
    "start": "1791840",
    "end": "1799120"
  },
  {
    "text": "scalable messaging clusters so basically you can define different partitions and then",
    "start": "1799120",
    "end": "1805840"
  },
  {
    "text": "from there on push these event logs getting into these partitions and scale based on the demands of your application",
    "start": "1805840",
    "end": "1812000"
  },
  {
    "text": "or based on the throughput that you're looking for and the new uh thing amazon kinesis that",
    "start": "1812000",
    "end": "1817360"
  },
  {
    "text": "we just introduced this morning which is a highly uh scalable highly available",
    "start": "1817360",
    "end": "1824960"
  },
  {
    "text": "and durable uh messaging service so basically you can",
    "start": "1824960",
    "end": "1830080"
  },
  {
    "text": "it gives you the capability to again scale based on the demands how much uh you know events how many",
    "start": "1830080",
    "end": "1837600"
  },
  {
    "text": "events that you're looking for that are going to be pushed into this messaging queue",
    "start": "1837600",
    "end": "1842720"
  },
  {
    "text": "so i'd like to talk a bit about uh amazon kinesis as you know it's a new service that",
    "start": "1842720",
    "end": "1848000"
  },
  {
    "text": "scales elastically for near real-time processing of streaming data",
    "start": "1848000",
    "end": "1853520"
  },
  {
    "text": "so the server stores large amounts of data in a durable consistent storage",
    "start": "1853520",
    "end": "1860080"
  },
  {
    "text": "which you can scale and it is good for real-time uh processing um that as i mentioned that",
    "start": "1860080",
    "end": "1867840"
  },
  {
    "text": "that that that you can scale so the way that that works is really is that you have a bunch of streams that could be",
    "start": "1867840",
    "end": "1874799"
  },
  {
    "text": "any data sources uh in our example back to the media delivery case we have uh",
    "start": "1874799",
    "end": "1880240"
  },
  {
    "text": "you know a stream that is being play played and you know all those events are being captured",
    "start": "1880240",
    "end": "1885919"
  },
  {
    "text": "so those are from everywhere they are talking to via the amazon uh web into the",
    "start": "1885919",
    "end": "1891520"
  },
  {
    "text": "amazon kinesis back-end engine which is really uh you know a storage system so",
    "start": "1891520",
    "end": "1896799"
  },
  {
    "text": "what it is is it allows the developers to define the number of shards that they",
    "start": "1896799",
    "end": "1902320"
  },
  {
    "text": "want to have you know so based on the shards it will scale and that will define the eventual throughput",
    "start": "1902320",
    "end": "1908960"
  },
  {
    "text": "that you can have from kinesis and from uh so you use the kinesis put api to put",
    "start": "1908960",
    "end": "1914960"
  },
  {
    "text": "these event logs into these shards and then use the get from the kinesis api to actually get",
    "start": "1914960",
    "end": "1922480"
  },
  {
    "text": "those those event logs run it into your application for",
    "start": "1922480",
    "end": "1928720"
  },
  {
    "start": "1926000",
    "end": "1926000"
  },
  {
    "text": "processing which you can use again amazon kinesis enabled application or api to do certain things",
    "start": "1928720",
    "end": "1935679"
  },
  {
    "text": "like filtering find uniques doing aggregations and stuff like that",
    "start": "1935679",
    "end": "1940720"
  },
  {
    "text": "and then once the processing is done actually delete it from kinesis and then",
    "start": "1940720",
    "end": "1945840"
  },
  {
    "text": "you can also store it into a durable storage system like s3",
    "start": "1945840",
    "end": "1952000"
  },
  {
    "text": "dynamodb or redshift for further analysis or further usage as well",
    "start": "1952000",
    "end": "1959279"
  },
  {
    "text": "so quick intro to to kinesis so at it at its core kinesis is a storage system uh that is",
    "start": "1959279",
    "end": "1967200"
  },
  {
    "start": "1964000",
    "end": "1964000"
  },
  {
    "text": "kinesis storage system is actually a highly high performance and strongly consistent uh replicated",
    "start": "1967200",
    "end": "1974480"
  },
  {
    "text": "log system so it also employs chain replication which",
    "start": "1974480",
    "end": "1980240"
  },
  {
    "text": "means that it maintains high throughput and availability without sacrificing the",
    "start": "1980240",
    "end": "1987760"
  },
  {
    "text": "strong consistency guarantees so basically as you can see here",
    "start": "1987760",
    "end": "1993120"
  },
  {
    "text": "it's a managed service as i mentioned transports data streams it enables the users",
    "start": "1993120",
    "end": "1999200"
  },
  {
    "text": "to launch multiple charts or you can specify how many multiples of one megabyte per",
    "start": "1999200",
    "end": "2005519"
  },
  {
    "text": "second charge you want to have each chart is capable of doing one megabyte per second so basically you are",
    "start": "2005519",
    "end": "2012240"
  },
  {
    "text": "you can you can scale it based on the throughput that you're looking for",
    "start": "2012240",
    "end": "2019519"
  },
  {
    "text": "each one of these charts is stored for 24 hours and it's highly durable and it is",
    "start": "2019519",
    "end": "2025679"
  },
  {
    "text": "achieved by a three wave application so behind the scenes we are replicating it in multiple availability zones",
    "start": "2025679",
    "end": "2033360"
  },
  {
    "text": "and each data record that you put into the kinesis shard is actually assigned a sequence",
    "start": "2033360",
    "end": "2040559"
  },
  {
    "text": "number so based on that sequence number uh then you will be able to get it so in",
    "start": "2040559",
    "end": "2046320"
  },
  {
    "text": "the picture then there are the producers basically that generate these streams of data",
    "start": "2046320",
    "end": "2051760"
  },
  {
    "text": "and they use the kinesis port api to put this event logs or data into these",
    "start": "2051760",
    "end": "2057679"
  },
  {
    "text": "shards and they use a partition key which is supplied by the",
    "start": "2057679",
    "end": "2063679"
  },
  {
    "text": "developer or the producer and places the record into a specific chart based on this",
    "start": "2063679",
    "end": "2069520"
  },
  {
    "text": "partition key so once all this data is in there so that's the ingest piece that you know",
    "start": "2069520",
    "end": "2074960"
  },
  {
    "text": "all this millions of events or you know however many hundreds of millions of events that are happening at any given time you're",
    "start": "2074960",
    "end": "2082158"
  },
  {
    "text": "storing those throwing those into these shards and then there is the the consumer layer",
    "start": "2082159",
    "end": "2090720"
  },
  {
    "text": "or the worker layer which is uh basically a processing layer running on ec2",
    "start": "2090720",
    "end": "2097839"
  },
  {
    "text": "that again a developer owns and controls and uses the sequence number that we",
    "start": "2097839",
    "end": "2102960"
  },
  {
    "text": "talked about earlier to get these records from any one of these shards as possible so the idea is",
    "start": "2102960",
    "end": "2109119"
  },
  {
    "text": "that you can really scale up to gigabytes of data per second",
    "start": "2109119",
    "end": "2114640"
  },
  {
    "text": "literally or hundreds of millions of records per second and then you could do",
    "start": "2114640",
    "end": "2120560"
  },
  {
    "text": "real-time processing in the matter of few seconds so that was the ingest piece now talk",
    "start": "2120560",
    "end": "2126800"
  },
  {
    "text": "about let's talk about processing so once the data is in there in our durable highly performant uh",
    "start": "2126800",
    "end": "2134240"
  },
  {
    "text": "storage system kinesis these are the different processing uh tools that are available out there that",
    "start": "2134240",
    "end": "2140400"
  },
  {
    "text": "you can run on amazon ec2 and and do real-time processing of this",
    "start": "2140400",
    "end": "2145599"
  },
  {
    "text": "um this event data so again on that uh",
    "start": "2145599",
    "end": "2150880"
  },
  {
    "text": "on the right-hand side the batch uh side of things you know you could run hadoop",
    "start": "2150880",
    "end": "2156240"
  },
  {
    "text": "using amazon emr um you can have you can scale amazon emr uh you know based on",
    "start": "2156240",
    "end": "2162400"
  },
  {
    "text": "the number of worker nodes that you want to have and based on the urgency of this analysis that you want to do you can",
    "start": "2162400",
    "end": "2169040"
  },
  {
    "text": "scale it up or down and and it you know it's a managed service within amazon services stack",
    "start": "2169040",
    "end": "2176400"
  },
  {
    "text": "once the data is analyzed using the mapreduce or some other processing technique you can then push it into",
    "start": "2176400",
    "end": "2181760"
  },
  {
    "text": "redshift and then it becomes readily available for you to query it you can also use redshift's",
    "start": "2181760",
    "end": "2189040"
  },
  {
    "text": "processing power so basically use if it's simple aggregations or simple queries within redshift redshift tends",
    "start": "2189040",
    "end": "2195839"
  },
  {
    "text": "to perform very well but again the idea there is that you can scale both emr and redshift",
    "start": "2195839",
    "end": "2201760"
  },
  {
    "text": "based on how quickly you want to analyze this data but again both of these go",
    "start": "2201760",
    "end": "2206960"
  },
  {
    "text": "into the batch specific use case where you are working on it on a historical set of data",
    "start": "2206960",
    "end": "2214160"
  },
  {
    "text": "storing it and then using the analysis for uh personalization type of of use cases",
    "start": "2214160",
    "end": "2221520"
  },
  {
    "text": "let's talk about storm and spark so a quick introduction to to storm",
    "start": "2221520",
    "end": "2228800"
  },
  {
    "text": "a storm storm is really similar to a hadoop cluster basically uh storm is",
    "start": "2228800",
    "end": "2234480"
  },
  {
    "text": "open source um and there is a concept of topology versus a job so when you're",
    "start": "2234480",
    "end": "2239920"
  },
  {
    "text": "running a hadoop job basically you have a a data set for a particular timeline",
    "start": "2239920",
    "end": "2247040"
  },
  {
    "text": "that you're running a processing on whereas in the case of storm it's really the incoming stream of data that we",
    "start": "2247040",
    "end": "2254240"
  },
  {
    "text": "captured using say kinesis or kafka or some other technology like that and it",
    "start": "2254240",
    "end": "2260560"
  },
  {
    "text": "is a continuous job that you're running or continuous analysis that you're running",
    "start": "2260560",
    "end": "2265839"
  },
  {
    "text": "unless you really go and kill the topology very simple uh to run it",
    "start": "2265839",
    "end": "2271599"
  },
  {
    "text": "and it what it does is it really uh the streams like unborn sequence of tuples",
    "start": "2271599",
    "end": "2277280"
  },
  {
    "text": "and i'll talk about it these tuples there are basically there is a concept of a stream a spout and a bolt and what",
    "start": "2277280",
    "end": "2284480"
  },
  {
    "text": "i did was i actually took uh the example uh from the storm documentation and kind of",
    "start": "2284480",
    "end": "2290960"
  },
  {
    "text": "modified it into our particular use case so in this particular scenario i'm really interested in getting the count",
    "start": "2290960",
    "end": "2297920"
  },
  {
    "text": "of ads that were clicked on and watched in a particular stream so just imagine",
    "start": "2297920",
    "end": "2303760"
  },
  {
    "text": "you know in if i am if i have a bunch of streams in a",
    "start": "2303760",
    "end": "2310480"
  },
  {
    "text": "channel or in a media delivery workflow and each one of these streams have an ad",
    "start": "2310480",
    "end": "2316000"
  },
  {
    "text": "playing that could be a personalized ad or whatever i want to make sure i want to get a capture of how many ads are",
    "start": "2316000",
    "end": "2322320"
  },
  {
    "text": "being being played so first the first line actually i go create a topology as i mentioned",
    "start": "2322320",
    "end": "2330079"
  },
  {
    "text": "topology is sort of analogous to a job but it runs forever and then i use that topology to add a",
    "start": "2330079",
    "end": "2337119"
  },
  {
    "text": "bolt so bolt is a processing piece within storm uh and what this bolt is gonna do",
    "start": "2337119",
    "end": "2343839"
  },
  {
    "text": "is it's gonna get the stream so it it gets all the streams and it is",
    "start": "2343839",
    "end": "2349359"
  },
  {
    "text": "gonna transform the id uh transform a stream of ids and ads to ids and streams",
    "start": "2349359",
    "end": "2357040"
  },
  {
    "text": "the next piece is to filter out the viewers or get the viewers of a",
    "start": "2357040",
    "end": "2362320"
  },
  {
    "text": "particular ad within a stream so in the first step we got which ad is specific to a stream",
    "start": "2362320",
    "end": "2369520"
  },
  {
    "text": "and then the second one we got the viewers of a particular ad okay the",
    "start": "2369520",
    "end": "2375280"
  },
  {
    "text": "third one here uh basically does a unique count of the viewers of that ad and a full uh view of a particular ad",
    "start": "2375280",
    "end": "2383760"
  },
  {
    "text": "and then it finally it computes the aggregate of the viewers so",
    "start": "2383760",
    "end": "2388880"
  },
  {
    "text": "behind the scenes it is you know it has all these different uh methods like counting the aggregate doing the",
    "start": "2388880",
    "end": "2394560"
  },
  {
    "text": "aggregations or doing the partial unique etc those are defined but the idea is",
    "start": "2394560",
    "end": "2400000"
  },
  {
    "text": "it's very very simple to implement simple to code so putting it all together",
    "start": "2400000",
    "end": "2405920"
  },
  {
    "text": "in terms of you know using kinesis along with storm here i have a bunch of",
    "start": "2405920",
    "end": "2411200"
  },
  {
    "start": "2409000",
    "end": "2409000"
  },
  {
    "text": "streams of these events that could be as i mentioned you know multiple different streams playing and producing these",
    "start": "2411200",
    "end": "2417200"
  },
  {
    "text": "event logs that are shoved into the kinesis shards or kinesis storage and then i use",
    "start": "2417200",
    "end": "2424800"
  },
  {
    "text": "the kinesis api to create these spouts that are consumed by storm",
    "start": "2424800",
    "end": "2431920"
  },
  {
    "text": "bolts that are actually processing nodes that could be running on ec2 and",
    "start": "2431920",
    "end": "2437280"
  },
  {
    "text": "scalable to do real-time analytics of the data",
    "start": "2437280",
    "end": "2443040"
  },
  {
    "text": "so that was real-time analytics of the data as it comes in basically it's running",
    "start": "2443040",
    "end": "2449440"
  },
  {
    "text": "all the time the topology is running all the time now let's talk about spark for a bit spark is",
    "start": "2449440",
    "end": "2454960"
  },
  {
    "text": "also open source started from the berkeley labs and really what spark is is um you know it's",
    "start": "2454960",
    "end": "2462880"
  },
  {
    "text": "it's it's sort of in-memory batch processing so the big difference between spark and hadoop would be where hadoop",
    "start": "2462880",
    "end": "2469839"
  },
  {
    "text": "runs on a disk based processing nodes spark could be in",
    "start": "2469839",
    "end": "2475200"
  },
  {
    "text": "memory so the real benefit there is that um you know it it it you can use the",
    "start": "2475200",
    "end": "2482560"
  },
  {
    "text": "data set that is in memory for iterative type of uh processing needs that you",
    "start": "2482560",
    "end": "2487760"
  },
  {
    "text": "have so again taking an example from spark and kind of putting it into our sort of example so in this case i'm",
    "start": "2487760",
    "end": "2495359"
  },
  {
    "text": "interested in counting the buffering events from a streaming log so again i",
    "start": "2495359",
    "end": "2500640"
  },
  {
    "text": "could use you know kinesis or something similar to capture the event logs from a stream in",
    "start": "2500640",
    "end": "2507440"
  },
  {
    "text": "this case i have the stream that is stored in some location as a text file",
    "start": "2507440",
    "end": "2513440"
  },
  {
    "start": "2508000",
    "end": "2508000"
  },
  {
    "text": "and i simply filter these logs for the line that starts with a buffer",
    "start": "2513440",
    "end": "2520319"
  },
  {
    "text": "you know and then i persist that in memory and then i count those so behind the scenes what's happening is",
    "start": "2520319",
    "end": "2527760"
  },
  {
    "text": "um when i have i run this spark job at first action that is involving buffer",
    "start": "2527760",
    "end": "2535839"
  },
  {
    "text": "it it runs the spark process and the driver will take",
    "start": "2535839",
    "end": "2541280"
  },
  {
    "text": "the um the data set and put it into the processing node so each one of the in",
    "start": "2541280",
    "end": "2547040"
  },
  {
    "text": "this case there are three different processing nodes and each one of those has the input data it is read in memory",
    "start": "2547040",
    "end": "2553520"
  },
  {
    "text": "and then processed and then it is uh basically persisted in memory for",
    "start": "2553520",
    "end": "2558640"
  },
  {
    "text": "further processing so that's that's the key piece that i want to bring to front here is that it's the",
    "start": "2558640",
    "end": "2564800"
  },
  {
    "start": "2564000",
    "end": "2564000"
  },
  {
    "text": "in-memory processing of say a smaller micro uh set of the",
    "start": "2564800",
    "end": "2570000"
  },
  {
    "text": "data set that is being processed in this case so in our logistic regression performance",
    "start": "2570000",
    "end": "2575599"
  },
  {
    "text": "uh example basically that was a test run between hadoop and spark as you can see",
    "start": "2575599",
    "end": "2581920"
  },
  {
    "text": "in the very first iteration it actually took 174",
    "start": "2581920",
    "end": "2588000"
  },
  {
    "text": "seconds for spark to run the logistic regression whereas for hadoop it took 127 seconds",
    "start": "2588000",
    "end": "2595760"
  },
  {
    "text": "so the very first iteration it's actually faster to run in hadoop but for ongoing cases since now the data set is",
    "start": "2595760",
    "end": "2603359"
  },
  {
    "text": "persisted in memory and you can bring more data set as it grows and basically re-run",
    "start": "2603359",
    "end": "2608960"
  },
  {
    "text": "or do additional iterations it is much faster in spark just by virtue of the fact that it is",
    "start": "2608960",
    "end": "2615359"
  },
  {
    "text": "persisted in memory so again a really more relevant example in",
    "start": "2615359",
    "end": "2621200"
  },
  {
    "text": "the case of canva geo report aggregations are many keys basically if",
    "start": "2621200",
    "end": "2626319"
  },
  {
    "text": "you if you look at hive versus spark so to run the the particular report it took",
    "start": "2626319",
    "end": "2633520"
  },
  {
    "text": "20 hours on hive where hive is sequel on top of a",
    "start": "2633520",
    "end": "2638640"
  },
  {
    "text": "sql interface on top of hadoop and in the case of spark it took half an hour",
    "start": "2638640",
    "end": "2644400"
  },
  {
    "text": "so that 40 times gain actually comes from not having to read this data set",
    "start": "2644400",
    "end": "2649839"
  },
  {
    "text": "over again you know in order to do the iteration rather use the data set that is already",
    "start": "2649839",
    "end": "2656000"
  },
  {
    "text": "in memory and persisted in memory so different tool set in the case of processing from say storm for",
    "start": "2656000",
    "end": "2664079"
  },
  {
    "text": "continuous processing of incoming streams to small micro level batches in",
    "start": "2664079",
    "end": "2669440"
  },
  {
    "text": "the case of spark and you can scale spark for much smaller micro level batches to sort of give you this",
    "start": "2669440",
    "end": "2675839"
  },
  {
    "text": "real-time processing needs as you're looking for so from processing moving on to the",
    "start": "2675839",
    "end": "2680880"
  },
  {
    "text": "back-end storage so i have ingest i have a way to do scalable ingest basically as",
    "start": "2680880",
    "end": "2686640"
  },
  {
    "start": "2683000",
    "end": "2683000"
  },
  {
    "text": "the real-time events are being generated i have a way to process my events uh",
    "start": "2686640",
    "end": "2692240"
  },
  {
    "text": "from you know batch all the way to real time from as i talked about from emr hadoop",
    "start": "2692240",
    "end": "2698079"
  },
  {
    "text": "to storm and spark and so once i have this data set that's being processed that's already processed now i can store",
    "start": "2698079",
    "end": "2704720"
  },
  {
    "text": "it into um s3 where i can access it at given time but from s3 perspective it's",
    "start": "2704720",
    "end": "2711520"
  },
  {
    "text": "going to be you know an object which is sitting there and i can access it there is obviously uh you know the depending on",
    "start": "2711520",
    "end": "2719040"
  },
  {
    "text": "how i access it there is uh it's it's it's good for the batch use cases then there is rds relational database amazon",
    "start": "2719040",
    "end": "2726480"
  },
  {
    "text": "rds service where uh you know it could be it could have a relational schema and could be",
    "start": "2726480",
    "end": "2731920"
  },
  {
    "text": "stored inside of a database dynamodb where you know i could provision the",
    "start": "2731920",
    "end": "2737520"
  },
  {
    "text": "throughput and based on my needs i could simply go and do a nosql type of a",
    "start": "2737520",
    "end": "2742560"
  },
  {
    "text": "get from dynamodb hdfs on amazon emr again i can scale it",
    "start": "2742560",
    "end": "2748240"
  },
  {
    "text": "based on the number of nodes the size of nodes that i have uh you know how much performance how much size that i want on",
    "start": "2748240",
    "end": "2753599"
  },
  {
    "text": "hdfs and store it and then finally redshift um redshift i can use for you know sort of",
    "start": "2753599",
    "end": "2759680"
  },
  {
    "text": "data warehousing type of needs where i have processed uh done some pre-processing on",
    "start": "2759680",
    "end": "2765040"
  },
  {
    "text": "the data set on a real-time basis and then put it into redshift that then i can use for historical",
    "start": "2765040",
    "end": "2773200"
  },
  {
    "text": "historical uh processing historical data processing as well uh using uh redshift's aggregation uh",
    "start": "2773200",
    "end": "2779599"
  },
  {
    "text": "processing uh performance uh gains there as well so finally uh some batch processing",
    "start": "2779599",
    "end": "2785760"
  },
  {
    "text": "techniques um we could obviously use emr with hive on emr it's good for custom user defined",
    "start": "2785760",
    "end": "2792800"
  },
  {
    "start": "2788000",
    "end": "2788000"
  },
  {
    "text": "functions and if there are needs for say like data warehousing type of needs and redshift for more traditional type",
    "start": "2792800",
    "end": "2799440"
  },
  {
    "text": "of data warehousing needs where i could run the etl process either as real time or in the batch put stuff in redshift",
    "start": "2799440",
    "end": "2806160"
  },
  {
    "text": "and then consume it later so putting it all together the challenge was mountains of raw data how do i",
    "start": "2806160",
    "end": "2812480"
  },
  {
    "text": "ingest it how do i process it uh real time store it in a more scalable way do some",
    "start": "2812480",
    "end": "2819920"
  },
  {
    "text": "batch processing and be able to run you know make decisions on the fly and",
    "start": "2819920",
    "end": "2825599"
  },
  {
    "text": "take actions so from a tool set perspective from ingest and uh",
    "start": "2825599",
    "end": "2831760"
  },
  {
    "start": "2827000",
    "end": "2827000"
  },
  {
    "text": "streaming side of things we talked about from s3 simple queueing service amazon dynamodb",
    "start": "2831760",
    "end": "2837839"
  },
  {
    "text": "kafka and amazon kinesis on real-time processing side of things we talked",
    "start": "2837839",
    "end": "2843440"
  },
  {
    "text": "about amazon emr redshift we talked about storm and spark",
    "start": "2843440",
    "end": "2849520"
  },
  {
    "text": "for storage data warehousing amazon s3 amazon rds service dynamodb and then",
    "start": "2849520",
    "end": "2856480"
  },
  {
    "text": "redshift and then finally i could use emr for back-end processing and all that",
    "start": "2856480",
    "end": "2862160"
  },
  {
    "text": "gets to gets me to useful information that i can use to",
    "start": "2862160",
    "end": "2867760"
  },
  {
    "text": "make decisions decisions like dynamic personalized ads now that i know that you know how",
    "start": "2867760",
    "end": "2874319"
  },
  {
    "text": "somebody is uh consuming those ads or you know where are they watching from what are they they're watching what are",
    "start": "2874319",
    "end": "2880880"
  },
  {
    "text": "the sort of viewing habits on the fly i can i can take actions in terms of how",
    "start": "2880880",
    "end": "2886560"
  },
  {
    "text": "or what type of ads or personalized ads that i want to display in that stream dynamic cdn switching that's a big use",
    "start": "2886560",
    "end": "2893520"
  },
  {
    "text": "case we see you know some companies already doing that using kafka storm type of technologies",
    "start": "2893520",
    "end": "2899760"
  },
  {
    "text": "where basically based on the user experience or the quality of the stream",
    "start": "2899760",
    "end": "2905119"
  },
  {
    "text": "i could do cdn switching on the fly switch them from one cdn to another based on how one cdn is performing in a",
    "start": "2905119",
    "end": "2911680"
  },
  {
    "text": "particular geographic location there are other things like interactive streaming experiences as well where you know while",
    "start": "2911680",
    "end": "2919440"
  },
  {
    "text": "somebody is watching a stream they are able to say do some sort of like uh you",
    "start": "2919440",
    "end": "2925119"
  },
  {
    "text": "know social interaction like via social media or interact directly have analytics on the",
    "start": "2925119",
    "end": "2931599"
  },
  {
    "text": "on the content itself and then finally uh stream quality in terms of you know",
    "start": "2931599",
    "end": "2936800"
  },
  {
    "text": "again better stream quality better viewing experience for the end consumer",
    "start": "2936800",
    "end": "2942720"
  },
  {
    "text": "so finally um you know what we talked about in this kind of recapping this whole thing what",
    "start": "2942720",
    "end": "2948319"
  },
  {
    "text": "we talked about or tried to convey in this is the tool set around you know the time frame when somebody sits at that",
    "start": "2948319",
    "end": "2954960"
  },
  {
    "text": "you know couch picks up the remote and wants to watch something to they pick the content that they start watching all",
    "start": "2954960",
    "end": "2962319"
  },
  {
    "text": "the way to an ultimate inter entertainment experience where you know they they love consuming that that",
    "start": "2962319",
    "end": "2968880"
  },
  {
    "text": "material so for more information for amazon kinesis the web page is up and",
    "start": "2968880",
    "end": "2974079"
  },
  {
    "text": "running uh take a look at it try it out it's a very very cheap i'm happy to say",
    "start": "2974079",
    "end": "2979280"
  },
  {
    "text": "that other resources apache kafka website is",
    "start": "2979280",
    "end": "2984319"
  },
  {
    "text": "there a storm and apache spark good technologies to take a look at it",
    "start": "2984319",
    "end": "2990000"
  },
  {
    "text": "for real-time processing thank you very much for joining us today and i really hope that",
    "start": "2990000",
    "end": "2997040"
  },
  {
    "text": "you find this useful myself and shaupna will make ourselves available here to",
    "start": "2997040",
    "end": "3002319"
  },
  {
    "text": "take any questions and answer the questions that you guys have thank you very much",
    "start": "3002319",
    "end": "3008760"
  }
]