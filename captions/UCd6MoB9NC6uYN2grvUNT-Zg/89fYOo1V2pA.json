[
  {
    "start": "0",
    "end": "24000"
  },
  {
    "text": "g'day my name is brynan Gregg and welcome to my talk I work at Netflix and",
    "start": "800",
    "end": "7859"
  },
  {
    "text": "we're deployed on AWS for our cloud it's very large you may have heard of it and",
    "start": "7859",
    "end": "13139"
  },
  {
    "text": "that's running Ubuntu Linux we also have a CDN that's based on free D FreeBSD",
    "start": "13139",
    "end": "19680"
  },
  {
    "text": "that's physical service Netflix is an awesome place to work and the team I'm on is the performance and operating",
    "start": "19680",
    "end": "26789"
  },
  {
    "start": "24000",
    "end": "24000"
  },
  {
    "text": "systems team our job is to evaluate technology and times like reinvent keep",
    "start": "26789",
    "end": "32640"
  },
  {
    "text": "us all very busy with all the new announcements we come up with recommendations and best practices for",
    "start": "32640",
    "end": "38040"
  },
  {
    "text": "the Netflix cloud as part of the work that we do engineering work we often",
    "start": "38040",
    "end": "44520"
  },
  {
    "text": "have to create new tools especially observability tools to fill in missing gaps and we will often open-source those",
    "start": "44520",
    "end": "51149"
  },
  {
    "text": "tools we help with projects new rollouts of technologies and applications and we",
    "start": "51149",
    "end": "58199"
  },
  {
    "text": "can also do Incident Response if there is a performance issue my manager at hunter is in the room is here reinvent",
    "start": "58199",
    "end": "65040"
  },
  {
    "text": "and we're actually hiring a couple of positions so you can come join me and work on my team at Netflix we'll all be",
    "start": "65040",
    "end": "71520"
  },
  {
    "text": "at the booth after this talk the Netflix booth in this talk I'm going to go through how",
    "start": "71520",
    "end": "77460"
  },
  {
    "text": "we do performance tuning of ec2 at Netflix I give many talks where I'm",
    "start": "77460",
    "end": "83729"
  },
  {
    "text": "covering what I do with kernel analysis and kernel tuning and I'm getting into the nuts and bolts of the system this",
    "start": "83729",
    "end": "90689"
  },
  {
    "text": "talks a little bit different instead of covering what I do this is more covering",
    "start": "90689",
    "end": "95759"
  },
  {
    "text": "what my team does the team that I'm on the performance and operating systems team and so this isn't just about kernel",
    "start": "95759",
    "end": "101939"
  },
  {
    "text": "tuning it's also about instant selection it's about understanding Amazon ec2 features tuning methodologies and all",
    "start": "101939",
    "end": "108750"
  },
  {
    "text": "the observability and analysis that we do I'm going to cover some Chernobyl's",
    "start": "108750",
    "end": "115409"
  },
  {
    "text": "in the next hour this is think of think of it as this is what's in our medicine cabinet",
    "start": "115409",
    "end": "120810"
  },
  {
    "text": "these are best before 2018 if you're looking at this slide deck from the distant future",
    "start": "120810",
    "end": "126180"
  },
  {
    "text": "these Chernobyl's may not be a good idea so cannabis is something that does need",
    "start": "126180",
    "end": "131190"
  },
  {
    "text": "to be revisited so instant selection is the first topic",
    "start": "131190",
    "end": "137370"
  },
  {
    "text": "I'll cover just as background for the next flicks cloud we do have many",
    "start": "137370",
    "end": "143820"
  },
  {
    "start": "139000",
    "end": "139000"
  },
  {
    "text": "different workloads so we have Cassandra evey cash applications we're using a lbs",
    "start": "143820",
    "end": "150800"
  },
  {
    "text": "within the applications as many workloads as well so we have proxy servers we have API servers we do media",
    "start": "150800",
    "end": "157800"
  },
  {
    "text": "encoding recommendations and big data",
    "start": "157800",
    "end": "162440"
  },
  {
    "start": "163000",
    "end": "163000"
  },
  {
    "text": "now a out Amazon AWS environment is",
    "start": "163100",
    "end": "168290"
  },
  {
    "text": "wonderful to do performance analysis and tuning on because of how its configured",
    "start": "168290",
    "end": "173520"
  },
  {
    "text": "to do red black or they're scaling groups and how we can take traffic from",
    "start": "173520",
    "end": "180900"
  },
  {
    "text": "an elastic load balancer and send it to a canary we can look at it performance",
    "start": "180900",
    "end": "188040"
  },
  {
    "text": "tunable and begin by setting it to a canary that may be just one instance and then see how that performs with some",
    "start": "188040",
    "end": "195630"
  },
  {
    "text": "real workload Netflix has a library hystrix and other libraries to make sure",
    "start": "195630",
    "end": "201630"
  },
  {
    "text": "that we do fault tolerance if as part of doing a performance experiment we",
    "start": "201630",
    "end": "208650"
  },
  {
    "text": "actually break those requests so those customer requests these will automatically timeout and then go to a",
    "start": "208650",
    "end": "214110"
  },
  {
    "text": "backup service and this can happen so quick if you're browsing Netflix you may not even notice that your session was",
    "start": "214110",
    "end": "221670"
  },
  {
    "text": "carried because we're doing a tuning experiment if it failed over within 100 milliseconds you may not notice so it's",
    "start": "221670",
    "end": "229470"
  },
  {
    "text": "a very safe environment for doing these experiments that's great for a performance engineer we can spin up a",
    "start": "229470",
    "end": "235770"
  },
  {
    "text": "canary we can try out some tunable parameters if it looks good as part of the regular deployment model we launched",
    "start": "235770",
    "end": "242760"
  },
  {
    "text": "a new on a scaling group with instances and then over time we shift work the",
    "start": "242760",
    "end": "248490"
  },
  {
    "text": "workload from the old auto scaling group to the new one and if things go wrong we can move it back this is great it's we",
    "start": "248490",
    "end": "256079"
  },
  {
    "text": "also do use micro benchmarking but micro benchmarking isn't the only tool we also get to play with real load workloads and",
    "start": "256080",
    "end": "264030"
  },
  {
    "text": "it's because of those libraries and our architecture that enables to do that so we have many different",
    "start": "264030",
    "end": "269760"
  },
  {
    "text": "ways of understanding where the two nobles are going to be effective for",
    "start": "269760",
    "end": "275370"
  },
  {
    "start": "274000",
    "end": "274000"
  },
  {
    "text": "instance election there are many instance types and the instance type",
    "start": "275370",
    "end": "282180"
  },
  {
    "text": "itself in some ways is a tunable parameter before cloud we had mainframes",
    "start": "282180",
    "end": "288630"
  },
  {
    "text": "and data centers and you deployed the you purchased some big iron a very large",
    "start": "288630",
    "end": "295260"
  },
  {
    "text": "server the size of a fridge and you had to tune it you couldn't swap it out because you discovered a month later you bought the",
    "start": "295260",
    "end": "301950"
  },
  {
    "text": "wrong server but with Amazon ec2 the",
    "start": "301950",
    "end": "308330"
  },
  {
    "text": "instance types can be replaced very quickly and so we aren't locked obviously we're locked in two years of a",
    "start": "308330",
    "end": "314850"
  },
  {
    "text": "support contract for that instance type was selected and so that way the",
    "start": "314850",
    "end": "320160"
  },
  {
    "text": "instance type itself is a very major tunable parameter what instance type are we going to pick for each of the",
    "start": "320160",
    "end": "325620"
  },
  {
    "text": "different workloads and so we at Netflix we're actually using over 30 different",
    "start": "325620",
    "end": "331470"
  },
  {
    "text": "instance types to run the different applications and I've listed some of them here but we should be pretty",
    "start": "331470",
    "end": "336690"
  },
  {
    "text": "familiar with how do we actually pick an instance type for a workload there's a",
    "start": "336690",
    "end": "345900"
  },
  {
    "start": "345000",
    "end": "345000"
  },
  {
    "text": "few different ways and the first one I've drawn an instance selection flow chart although at Netflix this is",
    "start": "345900",
    "end": "353250"
  },
  {
    "text": "something that the engine is having our heads we don't draw this out on a whiteboard we know we know this stuff",
    "start": "353250",
    "end": "358650"
  },
  {
    "text": "off by heart and this way we're looking",
    "start": "358650",
    "end": "363870"
  },
  {
    "text": "at characteristics that would point to a particular instance like if you really need large disk capacity terabytes of",
    "start": "363870",
    "end": "370470"
  },
  {
    "text": "disk we can deploy on details and so have those many terabytes of hard disk",
    "start": "370470",
    "end": "376110"
  },
  {
    "text": "storage if you don't need that much disk capacity and only some Netflix micro",
    "start": "376110",
    "end": "381630"
  },
  {
    "text": "services heavy users of discs like the databases then is a disk I about is the",
    "start": "381630",
    "end": "389340"
  },
  {
    "text": "performance workload bound or limited by disk i/o details if it is then we look",
    "start": "389340",
    "end": "397800"
  },
  {
    "text": "at whether it's cashable or not what is the working set which is much of the data is accessed frequently",
    "start": "397800",
    "end": "404689"
  },
  {
    "text": "say within an hour if a can cache we may",
    "start": "404689",
    "end": "409740"
  },
  {
    "text": "be better off trying to cash it in main memory DRAM and then picking an instance",
    "start": "409740",
    "end": "414930"
  },
  {
    "text": "type like an m4 and r3 if a car cache then we're going to pick the best storage instance type available",
    "start": "414930",
    "end": "421349"
  },
  {
    "text": "currently in i3 with nvme SSD devices",
    "start": "421349",
    "end": "426530"
  },
  {
    "text": "but for some of these it's going to end up being a choice between whether we want to go memory or CPU and if it's",
    "start": "426530",
    "end": "433199"
  },
  {
    "text": "memories are three instances and if it's CPU at c5 and Netflix we don't actually have a lot of m4 instances so they're",
    "start": "433199",
    "end": "439860"
  },
  {
    "text": "very popular a different way of picking an instance type is to determine what is",
    "start": "439860",
    "end": "446580"
  },
  {
    "text": "the bounding resource and so whether that's CP disk or network i/o and pick",
    "start": "446580",
    "end": "452129"
  },
  {
    "text": "the instance type just based on that alone that can be found using estimation",
    "start": "452129",
    "end": "457710"
  },
  {
    "text": "I know it's a data base I know it's going to have higher ups I know I should probably pick an i3 we can use an",
    "start": "457710",
    "end": "465839"
  },
  {
    "text": "existing real work like we can canary a work load onto an m4 and then see which",
    "start": "465839",
    "end": "472500"
  },
  {
    "text": "resource that's really hitting and then pick it based on that and we can also do",
    "start": "472500",
    "end": "477690"
  },
  {
    "text": "a benchmark or load test experimentation to figure out what should be the deep bounding resource for a synthetic",
    "start": "477690",
    "end": "483089"
  },
  {
    "text": "workload and then we choose the instance",
    "start": "483089",
    "end": "488129"
  },
  {
    "text": "type based on that we have a tool Norma Graham visualization tool written by",
    "start": "488129",
    "end": "493589"
  },
  {
    "start": "492000",
    "end": "492000"
  },
  {
    "text": "Scott is here and that lets us pick the",
    "start": "493589",
    "end": "498919"
  },
  {
    "text": "characteristics were interested in so the attributes there may be the performance bounding resource and then",
    "start": "498919",
    "end": "505770"
  },
  {
    "text": "the classes were interested in and then it will tell us the options available it",
    "start": "505770",
    "end": "511650"
  },
  {
    "text": "also gives us the prices which I've had to read act because I'm not sharing PI",
    "start": "511650",
    "end": "517140"
  },
  {
    "text": "that top to Amazon to share prices that's not Netflix to share so that helps us pick which of these instance",
    "start": "517140",
    "end": "524430"
  },
  {
    "text": "types best suits workload given the attributes we know important there's",
    "start": "524430",
    "end": "530970"
  },
  {
    "start": "530000",
    "end": "530000"
  },
  {
    "text": "another way we could pick instance types as well which is fun to mention and that's brutal a brute-force approach in the past",
    "start": "530970",
    "end": "538589"
  },
  {
    "text": "Netflix has done this and that is where we're able to run the load on all available instance types spin up one",
    "start": "538589",
    "end": "544680"
  },
  {
    "text": "instance type from for one instance for each instance type and then use the EOB",
    "start": "544680",
    "end": "550980"
  },
  {
    "text": "to fan out the workload and then measure price performance and then find out using a brute force approach which",
    "start": "550980",
    "end": "558029"
  },
  {
    "text": "instance type delivers the best price performance that's interesting we don't",
    "start": "558029",
    "end": "564630"
  },
  {
    "text": "currently use it I think that tool failing to fill out of repair one",
    "start": "564630",
    "end": "571050"
  },
  {
    "start": "571000",
    "end": "571000"
  },
  {
    "text": "problem with it that you'd have to pay attention to is you may find you've",
    "start": "571050",
    "end": "576839"
  },
  {
    "text": "picked an instance with the best price performance but you've neglected some other attribute that's important for",
    "start": "576839",
    "end": "582660"
  },
  {
    "text": "example latency requirements you might find this as the best price performance but all the API requests are taking over",
    "start": "582660",
    "end": "590040"
  },
  {
    "text": "one second and so customers wouldn't be happy with it because latency isn't part",
    "start": "590040",
    "end": "595290"
  },
  {
    "text": "of price performance is that's the only thing you're looking at and so it's important to think that there are other",
    "start": "595290",
    "end": "600620"
  },
  {
    "text": "requirements as well and I just brought this to show that as you increase",
    "start": "600620",
    "end": "606300"
  },
  {
    "text": "throughput there's a point where it's clearly unacceptable and there's a point",
    "start": "606300",
    "end": "613050"
  },
  {
    "text": "we'd like to roll it back so that we have some Headroom that's really where we're looking for the way we'd want to",
    "start": "613050",
    "end": "620370"
  },
  {
    "text": "measure price performance this is an acceptable latency for price performance just in case you want to try this",
    "start": "620370",
    "end": "625500"
  },
  {
    "text": "approach that don't we pick instance types to start with what about Reese",
    "start": "625500",
    "end": "631050"
  },
  {
    "start": "628000",
    "end": "628000"
  },
  {
    "text": "election we have tools that we can use",
    "start": "631050",
    "end": "637680"
  },
  {
    "start": "634000",
    "end": "634000"
  },
  {
    "text": "to see our instance usage and I've had to redact the instance types but this",
    "start": "637680",
    "end": "644730"
  },
  {
    "text": "quickly does a list of this is the most common in society Netflix whether that's an M 4.8 excel or whatever down the list",
    "start": "644730",
    "end": "652050"
  },
  {
    "text": "and we can use this tool to see if there's any old instance types that need to be upgraded because performance and",
    "start": "652050",
    "end": "657509"
  },
  {
    "text": "price performance always gets better instances cost which we also track",
    "start": "657509",
    "end": "663480"
  },
  {
    "text": "regularly by auto scaling group and application and instance types and I've had to heavily redact",
    "start": "663480",
    "end": "669790"
  },
  {
    "text": "that we have tools for this as well so we can understand the instance custom to",
    "start": "669790",
    "end": "675670"
  },
  {
    "text": "see if see where our tuning efforts are best spent there's a performance",
    "start": "675670",
    "end": "680860"
  },
  {
    "text": "engineer you there's lots of things you can do to tune applications and ec2 and",
    "start": "680860",
    "end": "685960"
  },
  {
    "text": "so it's important for our team to focus on the highest priority or the biggest",
    "start": "685960",
    "end": "691270"
  },
  {
    "text": "spend applications and to tune those first another thing to look out for is",
    "start": "691270",
    "end": "699220"
  },
  {
    "start": "697000",
    "end": "697000"
  },
  {
    "text": "instance variance so we may pick an instance type everything looks fine but there is a problem a few weeks later",
    "start": "699220",
    "end": "707050"
  },
  {
    "text": "when main memory gets populated or when the storage gets full and then things to",
    "start": "707050",
    "end": "713290"
  },
  {
    "text": "perform badly or some instances begin to perform badly and so we have ways to quickly look at the performance of",
    "start": "713290",
    "end": "721090"
  },
  {
    "text": "different nodes within a auto scaling group and to see if there's any outliers that may also tell us that we should",
    "start": "721090",
    "end": "730120"
  },
  {
    "text": "revisit instance selection and that there's a problem and once on ec2",
    "start": "730120",
    "end": "737320"
  },
  {
    "text": "features is another important area of tuning that we pay attention to and this",
    "start": "737320",
    "end": "743980"
  },
  {
    "text": "is also an area that became quite confusing because then over time the Xen",
    "start": "743980",
    "end": "751570"
  },
  {
    "text": "hypervisor has provided different modes for operations so there's been para",
    "start": "751570",
    "end": "758020"
  },
  {
    "text": "virtualization PV and hardware virtual virtual machines h vm and then you could",
    "start": "758020",
    "end": "764410"
  },
  {
    "text": "also boot a h VM instance but then use a PV driver and and then there's pv h vm",
    "start": "764410",
    "end": "771930"
  },
  {
    "text": "to try and make sense of this I've drawn a diagram so that as part of our team is",
    "start": "771930",
    "end": "778600"
  },
  {
    "text": "to always understand Amazon ec2 features and hype and especially hypervisor",
    "start": "778600",
    "end": "784450"
  },
  {
    "text": "features for performance and so this is something that I've spent time on had",
    "start": "784450",
    "end": "789520"
  },
  {
    "text": "help from other people the diagram looks like this and it's based on an original Xen modes diagram from Lars Cerf and",
    "start": "789520",
    "end": "798750"
  },
  {
    "start": "793000",
    "end": "793000"
  },
  {
    "text": "George Dunlap then published diagram on the Zen wiki there's a blog",
    "start": "798750",
    "end": "805089"
  },
  {
    "text": "post a few years ago and what I've done I've been editing this diagram and",
    "start": "805089",
    "end": "810519"
  },
  {
    "text": "tweaking and moving things around I've added extra columns so that we properly understand the state of hypervisor and",
    "start": "810519",
    "end": "818139"
  },
  {
    "text": "virtualization today this really tells the story in one slide to start with the",
    "start": "818139",
    "end": "826990"
  },
  {
    "text": "first row of got there shows what happens if we were fully emulated so",
    "start": "826990",
    "end": "832689"
  },
  {
    "text": "imagine the the bad old days of the first eighty x86 hypervisors it was",
    "start": "832689",
    "end": "840160"
  },
  {
    "text": "great that it was possible but these had to use binary emulation emulation and",
    "start": "840160",
    "end": "845620"
  },
  {
    "text": "binary substitution of instructions in order to work in those days this was before processors had hardware",
    "start": "845620",
    "end": "854079"
  },
  {
    "text": "virtualization support like intel vt-x hardware virtual machines got a very bad",
    "start": "854079",
    "end": "861370"
  },
  {
    "text": "rap from those early days and those only days I mean virtualization x86 virtualization started in the late 1990s",
    "start": "861370",
    "end": "868750"
  },
  {
    "text": "with VMware 1998 but over time both in",
    "start": "868750",
    "end": "875529"
  },
  {
    "text": "software of the hypervisor and also in hardware for the CPUs thus the network interface cards the storage interface",
    "start": "875529",
    "end": "882939"
  },
  {
    "text": "cards storage adapters these have all come up with ways to improve performance",
    "start": "882939",
    "end": "890050"
  },
  {
    "text": "in software ways to improve performance usually meant para virtualization",
    "start": "890050",
    "end": "896920"
  },
  {
    "text": "support and that's where the instead of running an unmodified guest the guest",
    "start": "896920",
    "end": "903370"
  },
  {
    "text": "now runs code that is aware that is it is running as a is an instance under",
    "start": "903370",
    "end": "910389"
  },
  {
    "text": "hypervisor and that hypervisor can expose hyper calls which are efficient",
    "start": "910389",
    "end": "916059"
  },
  {
    "text": "ways to access resources and then a para virtualized guest of code in a pair of para virtualized code in the guest can",
    "start": "916059",
    "end": "922689"
  },
  {
    "text": "then make hyper calls and improve efficiency the first line I've got here",
    "start": "922689",
    "end": "931240"
  },
  {
    "text": "beyond fully emulated is Zen 3.0 vintage and this is showing when",
    "start": "931240",
    "end": "940760"
  },
  {
    "text": "I first joined Netflix this is actually showing what most when frustrated Netflix is in 2014 this is what most of",
    "start": "940760",
    "end": "946640"
  },
  {
    "text": "our instances looked like so going along that line we've got CPU memory at this",
    "start": "946640",
    "end": "953300"
  },
  {
    "text": "point we're past VTX so the processors had support for hardware virtualization",
    "start": "953300",
    "end": "960589"
  },
  {
    "text": "of privileged instructions and also for page table operations and so the",
    "start": "960589",
    "end": "966020"
  },
  {
    "text": "performance overhead was very small it's near metal performance then on those",
    "start": "966020",
    "end": "972589"
  },
  {
    "text": "instances we had para ver drivers for network and storage devices also Netflix",
    "start": "972589",
    "end": "981110"
  },
  {
    "text": "when we began rolling out H VM instances using pv h vm drivers pv h vm drivers",
    "start": "981110",
    "end": "990250"
  },
  {
    "text": "para virtualization drivers that are designed in a post VTX",
    "start": "990250",
    "end": "995510"
  },
  {
    "text": "era v TX is the process or hardware virtualization technology and so these",
    "start": "995510",
    "end": "1000970"
  },
  {
    "text": "drivers can make use of hardware virtualization so in a way it's getting the best of both worlds you're getting",
    "start": "1000970",
    "end": "1007290"
  },
  {
    "text": "software optimizations para ver annual making use of the hardware optimizations",
    "start": "1007290",
    "end": "1013300"
  },
  {
    "text": "from the CPUs so that was pv h vm drivers we haven't got those pv h vm",
    "start": "1013300",
    "end": "1018730"
  },
  {
    "text": "drivers public confusion was that the way amazon exposed was in terms of how",
    "start": "1018730",
    "end": "1024760"
  },
  {
    "text": "the instances booted and so you needed to select between pv or h vm when the",
    "start": "1024760",
    "end": "1030790"
  },
  {
    "text": "instance was up and running your h vm instance may be running with pv drivers which is what i'm showing here and your",
    "start": "1030790",
    "end": "1037270"
  },
  {
    "text": "h vm instance may be running with pv h vm drivers i wrote a blog post because this is all so confusing there's so many",
    "start": "1037270",
    "end": "1043449"
  },
  {
    "text": "terms here and at netflix part of our team is we have to understand this when",
    "start": "1043449",
    "end": "1048970"
  },
  {
    "text": "we have to explain to other engineers what you should be picking and an epiphany is in some ways this is not",
    "start": "1048970",
    "end": "1056170"
  },
  {
    "text": "picking options to deploy instances this is showing the transition over time so",
    "start": "1056170",
    "end": "1062140"
  },
  {
    "text": "as Xen improved performance and added new capabilities and as Amazon and the ec2 teams engineers",
    "start": "1062140",
    "end": "1070559"
  },
  {
    "text": "also added capabilities instead of things you should pick it's simply what version hypervisor are you want and so",
    "start": "1070559",
    "end": "1077100"
  },
  {
    "text": "that's why I've ordered these rows from ulta new moving forward to the Zen AWS",
    "start": "1077100",
    "end": "1083190"
  },
  {
    "text": "2013 offerings this is where we then had I two instances with the SR io v",
    "start": "1083190",
    "end": "1092190"
  },
  {
    "text": "networking so now I can take the network i/o column and that has hardware",
    "start": "1092190",
    "end": "1097559"
  },
  {
    "text": "virtualization support so Network IO workloads became very fast and then this",
    "start": "1097559",
    "end": "1102809"
  },
  {
    "text": "year for sin a Tubbs 2017 the i3 instance types also let you do local",
    "start": "1102809",
    "end": "1110309"
  },
  {
    "text": "storage as SR Iove so this is near metal access so this is where the card itself",
    "start": "1110309",
    "end": "1117600"
  },
  {
    "text": "for Hardware card understands virtualization and can create virtualized instances that guests can",
    "start": "1117600",
    "end": "1123450"
  },
  {
    "text": "directly access that's great and I've been working on this diagram and I was",
    "start": "1123450",
    "end": "1129779"
  },
  {
    "text": "really happy it's like finally I can explain this all I can write this blog post and explain this transition from",
    "start": "1129779",
    "end": "1136049"
  },
  {
    "text": "the fully emulated work world which some people still when I hate hardware virtualization they think fully emulator",
    "start": "1136049",
    "end": "1142139"
  },
  {
    "text": "that's not what we're running these days - coming up - today - 2017 and then AWS",
    "start": "1142139",
    "end": "1148440"
  },
  {
    "text": "threw a spanner in my works in terms of writing a blog posts that explain it all by releasing a new hypervisor type very",
    "start": "1148440",
    "end": "1154320"
  },
  {
    "text": "recently for the c5 s nothing studying that it's really interesting there is a",
    "start": "1154320",
    "end": "1161610"
  },
  {
    "text": "talk tomorrow by AWS so Anthony Liguori is giving a talk on the new c5",
    "start": "1161610",
    "end": "1167730"
  },
  {
    "text": "hypervisor which I recommend attending as he's he's the authority and he'll be able to explain exactly what they've now",
    "start": "1167730",
    "end": "1175409"
  },
  {
    "text": "done since the c5 has been launched I have been using them and I and I can say",
    "start": "1175409",
    "end": "1180720"
  },
  {
    "text": "it looks like some of the key advantages are with the previous instance types you",
    "start": "1180720",
    "end": "1186480"
  },
  {
    "text": "had to make sure you selected to use the hardware virtualization drivers but this",
    "start": "1186480",
    "end": "1191580"
  },
  {
    "text": "now looks like that is the default so it's easier to use and it also looks",
    "start": "1191580",
    "end": "1198779"
  },
  {
    "text": "like interrupts have-have virtual hardware virtualization support as well and if",
    "start": "1198779",
    "end": "1204720"
  },
  {
    "text": "you've been following virtualization interrupts have been described as the",
    "start": "1204720",
    "end": "1210090"
  },
  {
    "text": "last battleground for making hardware virtual machines perform near metal at",
    "start": "1210090",
    "end": "1217379"
  },
  {
    "text": "near metal space the previous battleground was s r io v making sure that we could have direct access to",
    "start": "1217379",
    "end": "1224039"
  },
  {
    "text": "devices we could have DMA ring buffers to talk to devices but then when devices",
    "start": "1224039",
    "end": "1229200"
  },
  {
    "text": "talked back to the guests they're making interrupts and it looks like that's now",
    "start": "1229200",
    "end": "1234539"
  },
  {
    "text": "been fixed with the c5 hypervisor Amazon's also mentioned that that hypervisor may be used for other instance families",
    "start": "1234539",
    "end": "1241860"
  },
  {
    "text": "as well which is really exciting so there I said there's a talk about this tomorrow and I am going to do a blog",
    "start": "1241860",
    "end": "1249210"
  },
  {
    "text": "post after that talk where I cover this all in more detail so please check that",
    "start": "1249210",
    "end": "1255659"
  },
  {
    "text": "out so at Netflix what does it mean we should pick it means we should just pick",
    "start": "1255659",
    "end": "1262080"
  },
  {
    "text": "the latest so the latest offering if it makes sense for that workload so if we want an m4 am i picking pv am i picking",
    "start": "1262080",
    "end": "1271679"
  },
  {
    "text": "HP m with pv drivers no I'm gonna pick the best I'm gonna pick Bhutan hvm with",
    "start": "1271679",
    "end": "1277470"
  },
  {
    "text": "pv h vm + sr io v or if it's the new hypervisor i'll go with that because i'll get those things set up",
    "start": "1277470",
    "end": "1283379"
  },
  {
    "text": "automatically so i talked about things",
    "start": "1283379",
    "end": "1288629"
  },
  {
    "start": "1287000",
    "end": "1287000"
  },
  {
    "text": "for networking sr IV that's where the network cards can expose virtual",
    "start": "1288629",
    "end": "1296850"
  },
  {
    "text": "instances so the guests can have near metal access I noticed on some of the vendors who make these cards if you read",
    "start": "1296850",
    "end": "1304139"
  },
  {
    "text": "the spec sheets they call it bare metal access and and they're the ones that actually built the card so maybe I",
    "start": "1304139",
    "end": "1309149"
  },
  {
    "text": "should be calling a bare metal access so I've called an Ian metal access and for",
    "start": "1309149",
    "end": "1315450"
  },
  {
    "text": "a C to the IX GB driver supports up to 10 gigabits per second in the ene driver is 25 which is pretty great for a cloud",
    "start": "1315450",
    "end": "1321990"
  },
  {
    "text": "instance and then storage sr IV that's what was launched this year and Amazon",
    "start": "1321990",
    "end": "1331529"
  },
  {
    "text": "called the network s r io v enhanced networking I think we should call this enhanced storage so far some instance",
    "start": "1331529",
    "end": "1338220"
  },
  {
    "text": "types only and does use nvme and vtd for IO virtualization so been which should",
    "start": "1338220",
    "end": "1345390"
  },
  {
    "text": "be called bare metal disk access or near metal disk disk access and we've already on our team we've done benchmarking and",
    "start": "1345390",
    "end": "1351990"
  },
  {
    "text": "found and I three can exceed three million disk storage I ops which is pretty exciting",
    "start": "1351990",
    "end": "1358970"
  },
  {
    "text": "Colonel tuning so if picked an instance we've picked our SRA avi drivers or",
    "start": "1360710",
    "end": "1368970"
  },
  {
    "text": "we're using the latest hypervisor now comes time to tuning the kernel itself",
    "start": "1368970",
    "end": "1375920"
  },
  {
    "start": "1375000",
    "end": "1375000"
  },
  {
    "text": "to start with with kernel tuning we're typically looking at one to thirty",
    "start": "1376220",
    "end": "1381600"
  },
  {
    "text": "percent wins for for average performance finds this adds up to significant",
    "start": "1381600",
    "end": "1387300"
  },
  {
    "text": "savings for the Netflix cloud so we do care about it but in context if you're doing application analysis you can find",
    "start": "1387300",
    "end": "1393240"
  },
  {
    "text": "two X wins and ten X wins and so at Netflix wild our performance team is",
    "start": "1393240",
    "end": "1398429"
  },
  {
    "text": "doing is always doing analysis around the stack many engineers and Netflix on",
    "start": "1398429",
    "end": "1404160"
  },
  {
    "text": "the application development teams they're doing their own application performance analysis and so they're finding those very large application",
    "start": "1404160",
    "end": "1411150"
  },
  {
    "text": "wins eliminating unnecessary work but we still care about performance across all",
    "start": "1411150",
    "end": "1416490"
  },
  {
    "text": "the layers down to those small wins now one to thirty percent for Netflix is a",
    "start": "1416490",
    "end": "1422820"
  },
  {
    "text": "lot for you it may not be much so you may think I don't care too much about kernel tuning you do get bigger wins",
    "start": "1422820",
    "end": "1428580"
  },
  {
    "text": "when you're reducing latency outliers and that may be important because you're trying to meet an SLA I need to take my",
    "start": "1428580",
    "end": "1435090"
  },
  {
    "text": "99th percentile latency down and I there's no way around it I'm have to tune everything and make this work and",
    "start": "1435090",
    "end": "1442110"
  },
  {
    "text": "so if that's a problem for you that's a situation where everyone's doing kernel",
    "start": "1442110",
    "end": "1448800"
  },
  {
    "text": "tuning because you have to we deploy tuning by baking it into our base ami so",
    "start": "1448800",
    "end": "1455520"
  },
  {
    "text": "everyone gets in we have had experimental tuning package add-ons in",
    "start": "1455520",
    "end": "1460830"
  },
  {
    "text": "the past to play with and we do work with specific tuning for applications",
    "start": "1460830",
    "end": "1467230"
  },
  {
    "text": "got to remember to tune the workload with the Chernobyl's if I improve the performance of the storage devices if I",
    "start": "1467230",
    "end": "1473559"
  },
  {
    "text": "improve the buffer sizes or read ahead storage i/o sizes I may need to easily",
    "start": "1473559",
    "end": "1479080"
  },
  {
    "text": "offer to you in the application so that makes use of that new capacity so I",
    "start": "1479080",
    "end": "1484360"
  },
  {
    "text": "wanted to mention that I'll go through these tuning targets quickly the cpu",
    "start": "1484360",
    "end": "1492070"
  },
  {
    "start": "1487000",
    "end": "1487000"
  },
  {
    "text": "scheduler you can see in the scheduler class priorities migration latency and",
    "start": "1492070",
    "end": "1497530"
  },
  {
    "text": "task sets some applications benefit from",
    "start": "1497530",
    "end": "1502720"
  },
  {
    "text": "this including things like tuning the scheduler migration cost the kernel",
    "start": "1502720",
    "end": "1508000"
  },
  {
    "text": "tries to the Linux kernel tries to honor cpu affinity and that is where a thread is running on a CPU it's running with",
    "start": "1508000",
    "end": "1514900"
  },
  {
    "text": "hot CPU caches so level one level two TLB cache if the thread is context",
    "start": "1514900",
    "end": "1521049"
  },
  {
    "text": "switched off and then wants to because there's an interrupt or another",
    "start": "1521049",
    "end": "1526809"
  },
  {
    "text": "application needs to run and then that thread needs to run again but that CPU is busy do we pick an idle CPU and",
    "start": "1526809",
    "end": "1534040"
  },
  {
    "text": "migrate the thread do we wait a little bit to see if the CPU will free up this",
    "start": "1534040",
    "end": "1539049"
  },
  {
    "text": "but performance benefit from waiting a little bit because those CPUs are warm on that CPU and so some of this can be",
    "start": "1539049",
    "end": "1546190"
  },
  {
    "text": "tuned this the scheduler migration cost nanoseconds and that can have a",
    "start": "1546190",
    "end": "1551679"
  },
  {
    "text": "performance boost especially for large numerous systems because now your caches are even more important to avoid talking",
    "start": "1551679",
    "end": "1557830"
  },
  {
    "text": "over the remote memory bus some Java",
    "start": "1557830",
    "end": "1563080"
  },
  {
    "text": "applications were found have Bennett benefited from schedule skid bash to",
    "start": "1563080",
    "end": "1568090"
  },
  {
    "text": "reduce context switching as well which you can set up you seeing shed tall task",
    "start": "1568090",
    "end": "1574120"
  },
  {
    "text": "set is another one which we can I often use task set for experimentations and",
    "start": "1574120",
    "end": "1580570"
  },
  {
    "text": "that's where I can reduce how many CPUs an application can use if you're ever in",
    "start": "1580570",
    "end": "1587380"
  },
  {
    "text": "a situation I had this last week where it was a micro benchmark and it was",
    "start": "1587380",
    "end": "1593140"
  },
  {
    "text": "running at 1.7 million ups per second sounds good running across 72 CPUs",
    "start": "1593140",
    "end": "1600559"
  },
  {
    "text": "I use tasks set and I reduced it to only run on one CPU and then it ran at 3.2",
    "start": "1600559",
    "end": "1605840"
  },
  {
    "text": "million ops per second as a performance engineer I would not be deploying that",
    "start": "1605840",
    "end": "1610909"
  },
  {
    "text": "in production because it's it tells me something bad has happened something really bad has happened if I'm reduce it",
    "start": "1610909",
    "end": "1616940"
  },
  {
    "text": "from going from 72 CPUs to 1 and now it has double the performance and the thing that was bad which I did a profile on",
    "start": "1616940",
    "end": "1622970"
  },
  {
    "text": "the flame graph laws was that we were in lock contention we're in lock hell so the more CPUs to added that you added",
    "start": "1622970",
    "end": "1628820"
  },
  {
    "text": "the worse things got and so it's one of those classical experiments you do as a performance engineer of reducing the",
    "start": "1628820",
    "end": "1634730"
  },
  {
    "text": "CPUs and if things go faster you've probably got a lock problem and you would fix the lock problem so my point",
    "start": "1634730",
    "end": "1640460"
  },
  {
    "text": "is task sets are useful for investigating CPU scheduler issues but I don't think it's part of the final",
    "start": "1640460",
    "end": "1645740"
  },
  {
    "text": "solution the final solution is you fix the locking problem virtual memory so we",
    "start": "1645740",
    "end": "1652490"
  },
  {
    "start": "1649000",
    "end": "1649000"
  },
  {
    "text": "can change swap enos over commit the behavior swap enos we set it to zero to",
    "start": "1652490",
    "end": "1659059"
  },
  {
    "text": "disable swapping and we don't have swap",
    "start": "1659059",
    "end": "1666619"
  },
  {
    "text": "devices configured for our base am I so when application and we tune",
    "start": "1666619",
    "end": "1671990"
  },
  {
    "text": "applications so that they don't run out of memory we do have to look out for",
    "start": "1671990",
    "end": "1677110"
  },
  {
    "text": "behavior behavior out of memory killer because what can happen is an application can grow by mistake and then",
    "start": "1677110",
    "end": "1684470"
  },
  {
    "text": "get them killed and then we have to go around and unit but that's another area",
    "start": "1684470",
    "end": "1689600"
  },
  {
    "text": "that we continue can the tunable for swapping helps you pick between the",
    "start": "1689600",
    "end": "1697090"
  },
  {
    "text": "ditching memory and favoring the file system cache or favoring keeping",
    "start": "1697090",
    "end": "1705110"
  },
  {
    "text": "application memory in main memory so do I want to make sure the application is",
    "start": "1705110",
    "end": "1710450"
  },
  {
    "text": "in main memory or do I want to move some F to a soft device to freed up for file system cache we just don't play that",
    "start": "1710450",
    "end": "1716090"
  },
  {
    "text": "game in Netflix we don't have the soft devices configure at least under my although micro services I'm familiar with huge pages is another terrible",
    "start": "1716090",
    "end": "1724460"
  },
  {
    "start": "1722000",
    "end": "1722000"
  },
  {
    "text": "parameter to be aware of the CPU operates on memory in units of pages",
    "start": "1724460",
    "end": "1733340"
  },
  {
    "text": "which depends on the architecture for until x86 its 4k that can be inefficient for",
    "start": "1733340",
    "end": "1739669"
  },
  {
    "text": "large memory applications and so processors for a long time have supported large pages or huge pages to",
    "start": "1739669",
    "end": "1745820"
  },
  {
    "text": "mix or form X instead of 4k there were problems in the past so when we upgraded",
    "start": "1745820",
    "end": "1752419"
  },
  {
    "text": "to a previous Ubuntu release we found that the performance was worse because",
    "start": "1752419",
    "end": "1760850"
  },
  {
    "text": "of transparent huge pages had some some outright performance issues and so we",
    "start": "1760850",
    "end": "1767899"
  },
  {
    "text": "found that turning off transparent huge pages improved performance that was the",
    "start": "1767899",
    "end": "1773450"
  },
  {
    "text": "previous release that now that was trusty now we're going to xenial we're turning this back on so you're getting",
    "start": "1773450",
    "end": "1779179"
  },
  {
    "text": "back to M advice but it's a tunable parameter to be aware of it can affect performance so whether it's using huge",
    "start": "1779179",
    "end": "1786919"
  },
  {
    "text": "pages transparent huge pages is whether unless the kernel manage it so you don't have to explicitly set up regions Numa",
    "start": "1786919",
    "end": "1796370"
  },
  {
    "start": "1794000",
    "end": "1794000"
  },
  {
    "text": "balancing another tunable to be aware of the worst I've seen was a-there",
    "start": "1796370",
    "end": "1802720"
  },
  {
    "text": "fortunately many instance types are so small they don't have different Newman Newman notes so if you run numerous",
    "start": "1802720",
    "end": "1810110"
  },
  {
    "text": "stack you'll only get one node but for the very largest types they are multi",
    "start": "1810110",
    "end": "1817070"
  },
  {
    "text": "Numa systems and you also have multiple nodes and then the kernel can spend time moving pages from one node to another",
    "start": "1817070",
    "end": "1823039"
  },
  {
    "text": "and when I say node it could be a group a bank of memory attached to one socket processor and then there's another Bank",
    "start": "1823039",
    "end": "1829490"
  },
  {
    "text": "of memory attached to another socket processor and if your application is busy running on one socket you'd rather",
    "start": "1829490",
    "end": "1835759"
  },
  {
    "text": "talk to local memory than going over the CPU interconnect so the kernel doing",
    "start": "1835759",
    "end": "1841370"
  },
  {
    "text": "this effort of moving pages of memory around to keep it close to where the application is makes sense in theory but",
    "start": "1841370",
    "end": "1847190"
  },
  {
    "text": "we did have it runs so aggressively the the system was consuming 60% of all CPU",
    "start": "1847190",
    "end": "1853309"
  },
  {
    "text": "cycles doing Numa page rebalancing so the system itself was I mean the",
    "start": "1853309",
    "end": "1858799"
  },
  {
    "text": "application was secondary this was a Numa page rebalancing service that Netflix was spending a lot of money to",
    "start": "1858799",
    "end": "1864049"
  },
  {
    "text": "run so here we turned that off it's like that's that's a bug there in later kernel versions",
    "start": "1864049",
    "end": "1869930"
  },
  {
    "text": "there have been improving this so we can we have more fine-grained controls but another one to be aware of file system",
    "start": "1869930",
    "end": "1876950"
  },
  {
    "start": "1874000",
    "end": "1874000"
  },
  {
    "text": "cannibals so dirty ratio to dirty",
    "start": "1876950",
    "end": "1883010"
  },
  {
    "text": "background ratio and expires any sex these control how quickly the write-back",
    "start": "1883010",
    "end": "1888110"
  },
  {
    "text": "hashing works and so when the application does write to the file system it doesn't immediately engage the",
    "start": "1888110",
    "end": "1893630"
  },
  {
    "text": "storage devices it can keep it in memory and then flush it later and when you can tune how aggressively it flushes that",
    "start": "1893630",
    "end": "1899560"
  },
  {
    "text": "buffering is good it helps the application continue and we could also mount things using say no a time and",
    "start": "1899560",
    "end": "1907130"
  },
  {
    "text": "discard no barrier various other file system cannibals as well you should check and we actually have CFS on Linux",
    "start": "1907130",
    "end": "1915860"
  },
  {
    "text": "running for our container micro service and we also have butter professed running for the container micro surface",
    "start": "1915860",
    "end": "1923090"
  },
  {
    "text": "as well we're trying at both and both of them come with many many more to Nobles that you need to pay attention to so if",
    "start": "1923090",
    "end": "1929540"
  },
  {
    "text": "you have a workload that is filesystem bound have a look at the to know balls because there should be many storage IO",
    "start": "1929540",
    "end": "1939340"
  },
  {
    "start": "1936000",
    "end": "1936000"
  },
  {
    "text": "reader head size the number of in-flight requests the i/o scheduler volume stripe with these are all tunable parameters if",
    "start": "1939340",
    "end": "1946010"
  },
  {
    "text": "you have a workload that is storage bound this can make a big difference we've had that in the past with",
    "start": "1946010",
    "end": "1951530"
  },
  {
    "text": "Cassandra being very sensitive to the reader head size and tuning that to",
    "start": "1951530",
    "end": "1957470"
  },
  {
    "text": "match Cassandra improved performance considerably SSDs can perform better",
    "start": "1957470",
    "end": "1962720"
  },
  {
    "text": "with a know up scheduler we don't need to have an elevator scheduler that worries about the rotational disks disk",
    "start": "1962720",
    "end": "1969440"
  },
  {
    "text": "haired as it moves around we can turn that off because SSDs are different and then tuning the MD chunk size and stripe",
    "start": "1969440",
    "end": "1976610"
  },
  {
    "text": "mixed with to match the workload so there's a lot that we can do when we have done some of this for Cassandra and",
    "start": "1976610",
    "end": "1982940"
  },
  {
    "text": "we've done some of this for the storage workloads networking we've done a lot",
    "start": "1982940",
    "end": "1988130"
  },
  {
    "text": "with and there's TCP buffer sizes TCP",
    "start": "1988130",
    "end": "1993290"
  },
  {
    "text": "backlog device backlog and so on very common to tune these this is the current enables we're using again this would",
    "start": "1993290",
    "end": "1999890"
  },
  {
    "text": "this could be out of date next year and that involves increasing the buffer sizes so that we can improve",
    "start": "1999890",
    "end": "2008020"
  },
  {
    "text": "network throughput and the hypervisor",
    "start": "2008020",
    "end": "2014780"
  },
  {
    "start": "2014000",
    "end": "2014000"
  },
  {
    "text": "itself whether we're using the PV or HTM instances PV or HBM AMI",
    "start": "2014780",
    "end": "2023350"
  },
  {
    "text": "these days it's HBM in the very distant past when HTM was first introduced there",
    "start": "2023350",
    "end": "2029750"
  },
  {
    "text": "were some performance issues and so there became this some people had found that PV could run faster than HTM and",
    "start": "2029750",
    "end": "2037400"
  },
  {
    "text": "you'll still find this on the internet because nothing ever dies on the internet all the old tuning advice is",
    "start": "2037400",
    "end": "2042620"
  },
  {
    "text": "still out there so if you try and search for a PV versus h vm you'll see some very out of date advice telling you the",
    "start": "2042620",
    "end": "2048020"
  },
  {
    "text": "pv pv s faster pv may be faster for some very corner case workloads but nowadays",
    "start": "2048020",
    "end": "2053060"
  },
  {
    "text": "it's the h vm instances and well of course as I said earlier once they but they may be using pv h vm the sauce",
    "start": "2053060",
    "end": "2062090"
  },
  {
    "text": "is something that we have chin for awhile and so there are different clock",
    "start": "2062090",
    "end": "2067158"
  },
  {
    "text": "sources available and with we in fact we",
    "start": "2067159",
    "end": "2072290"
  },
  {
    "text": "encountered a Zen clock sauce regression in the past and the best case was reducing CP usage by 30 percent and",
    "start": "2072290",
    "end": "2079010"
  },
  {
    "text": "average application latency by 43 percent just by changing the clock type that our instances are using and so",
    "start": "2079010",
    "end": "2085340"
  },
  {
    "text": "that's T SC and you can set that up why wouldn't that be default in the distant",
    "start": "2085340",
    "end": "2091550"
  },
  {
    "text": "past processors couldn't guarantee that TS c monotonically increased across all",
    "start": "2091550",
    "end": "2097640"
  },
  {
    "text": "cores and so there could be clock skew and clock could go backwards sometimes",
    "start": "2097640",
    "end": "2103550"
  },
  {
    "text": "there were various concerns about ta talk to Intel talk to aim they talk to your processor manufacturer and say and",
    "start": "2103550",
    "end": "2110960"
  },
  {
    "text": "these concerns relevant now in 2017 or is this old is this fruit like 1997 and",
    "start": "2110960",
    "end": "2117830"
  },
  {
    "text": "so my belief is that's that wisdom is quite old-fashioned now and so we can",
    "start": "2117830",
    "end": "2124100"
  },
  {
    "text": "trust TSA but beware of be aware of that check it out",
    "start": "2124100",
    "end": "2131470"
  },
  {
    "text": "methodologies for tuning so I want to cover some methodologies for understanding the performance of our",
    "start": "2132140",
    "end": "2138120"
  },
  {
    "text": "workloads since a lot of the Chernobyl's will end up using whether it's kernel or application Chernobyl's it can be",
    "start": "2138120",
    "end": "2144900"
  },
  {
    "text": "derived from how the system is performing the observability metrics we",
    "start": "2144900",
    "end": "2151590"
  },
  {
    "start": "2150000",
    "end": "2150000"
  },
  {
    "text": "have dashboards at Netflix for example a Netflix / finals dashboard which shows",
    "start": "2151590",
    "end": "2158520"
  },
  {
    "text": "all sorts of statistics for a auto",
    "start": "2158520",
    "end": "2163980"
  },
  {
    "text": "scaling group or an application so requests per second how many instances it's using which changes there's a",
    "start": "2163980",
    "end": "2171300"
  },
  {
    "text": "performance engineer on auto scaling groups when you see performance change it may not be that there was a",
    "start": "2171300",
    "end": "2177950"
  },
  {
    "text": "application change or a workload change it could be an instance change hood a",
    "start": "2177950",
    "end": "2183510"
  },
  {
    "text": "scaling may have added 10 more instances and then fanned out work amongst a larger pool which can cause a",
    "start": "2183510",
    "end": "2189480"
  },
  {
    "text": "performance issue so we always have to look at the instance count when we're trying to understand performs in the cloud so this is using the Atlas cloud",
    "start": "2189480",
    "end": "2196260"
  },
  {
    "text": "wide observability tool and so there's lots of metrics we can use for drilling into where the performance problems are",
    "start": "2196260",
    "end": "2205910"
  },
  {
    "text": "Java heap whether its latency load average 99th percentile so that",
    "start": "2205910",
    "end": "2216140"
  },
  {
    "start": "2213000",
    "end": "2213000"
  },
  {
    "text": "performance methodology is to look at a dashboard which is almost like a checklist of options another methodology",
    "start": "2216140",
    "end": "2223020"
  },
  {
    "text": "is to either to work load analysis or resource analysis it's in I think it's",
    "start": "2223020",
    "end": "2228540"
  },
  {
    "text": "helpful to separate them and realize be conscious that you're doing it resource analysis is where you're thinking of",
    "start": "2228540",
    "end": "2234540"
  },
  {
    "text": "how's the cpu performance how's the disk performance has the network performance oh the disk performance is bad let's",
    "start": "2234540",
    "end": "2240300"
  },
  {
    "text": "work our way up the stack is file system performance also battles all asynchronous now how's the application",
    "start": "2240300",
    "end": "2246660"
  },
  {
    "text": "affected it's in some ways it's it's nice to do that analysis approach",
    "start": "2246660",
    "end": "2252180"
  },
  {
    "text": "because these resource types are well-known and well-understood whereas your application may be custom",
    "start": "2252180",
    "end": "2258960"
  },
  {
    "text": "and there may be there's no one else you can talk to about how that application works we",
    "start": "2258960",
    "end": "2264109"
  },
  {
    "text": "you can talk to everyone about how CPUs work and disks work so there's some benefits from that approach and",
    "start": "2264109",
    "end": "2269779"
  },
  {
    "text": "disadvantages you don't have application context so you may often see a disk i/o performance issue but you don't know",
    "start": "2269779",
    "end": "2275209"
  },
  {
    "text": "whether it's affecting the application or the application requests the other way around is to start top-down and",
    "start": "2275209",
    "end": "2280459"
  },
  {
    "text": "begin with the workload analysis what's the application doing and then drill down into the libraries in the system",
    "start": "2280459",
    "end": "2285559"
  },
  {
    "text": "called McConnel that's useful as you can then expose latency in terms of an",
    "start": "2285559",
    "end": "2291109"
  },
  {
    "text": "application request so you can always keep track of does this matter is it actually hurting our requests the use",
    "start": "2291109",
    "end": "2299930"
  },
  {
    "start": "2299000",
    "end": "2299000"
  },
  {
    "text": "method is another methodology for doing performance analysis and tuning this is when I came up with a long time ago it's",
    "start": "2299930",
    "end": "2306229"
  },
  {
    "text": "where I take a functional diagram of a system where I'll plot out the CPUs storage devices network devices all the",
    "start": "2306229",
    "end": "2312859"
  },
  {
    "text": "busses and interconnects bridge devices and then for every hardware and software",
    "start": "2312859",
    "end": "2318049"
  },
  {
    "text": "resource I want to check three metrics only which is utilization saturation and errors if you look at a lot of",
    "start": "2318049",
    "end": "2325039"
  },
  {
    "text": "performance monitoring products and if you look at the kernel itself we're drowning in metrics there's so many metrics this is an approach that reduces",
    "start": "2325039",
    "end": "2333890"
  },
  {
    "text": "that set to maybe 48 so just three metrics for each resource it also",
    "start": "2333890",
    "end": "2341319"
  },
  {
    "text": "reveals things you may not have looked at so it reveals blind spots",
    "start": "2341319",
    "end": "2346989"
  },
  {
    "text": "how many performance monitoring solutions look at bus performance look at CPU to memory bus or CPU interconnect",
    "start": "2346989",
    "end": "2354289"
  },
  {
    "text": "performance I've had lots of issues with buses in the distant past but many",
    "start": "2354289",
    "end": "2359449"
  },
  {
    "text": "performance monitoring products don't and I'll analyze it at that level and",
    "start": "2359449",
    "end": "2364900"
  },
  {
    "text": "it's a consumer of it you wouldn't even know of it you'd look at the phone's monitoring product and say well there's a thousand metrics in here everything",
    "start": "2364900",
    "end": "2371479"
  },
  {
    "text": "must be covered in here but that's not true because it does have blind spots this methodology helps you reveal and",
    "start": "2371479",
    "end": "2378680"
  },
  {
    "text": "become aware of those blind spots because you can say well I don't have",
    "start": "2378680",
    "end": "2384009"
  },
  {
    "text": "saturation for the CPU interconnect so now I need to go find out how to measure that it's a use method method poses",
    "start": "2384009",
    "end": "2391670"
  },
  {
    "text": "questions to answer and another methodology that I love to use and",
    "start": "2391670",
    "end": "2397609"
  },
  {
    "start": "2393000",
    "end": "2393000"
  },
  {
    "text": "it's unsafe you enough CPU analysis and we also use it on that team with uncie P",
    "start": "2397609",
    "end": "2404029"
  },
  {
    "text": "analysis that's explained what's happening on the CPU and so flame graphs are great at that don't do a flame graph",
    "start": "2404029",
    "end": "2410660"
  },
  {
    "text": "C P profile I mentioned flame graphs in a bit but if you're not on CPU but you've got a performance issue",
    "start": "2410660",
    "end": "2416630"
  },
  {
    "text": "why are you blocking are you blocking because of resources disco storage are",
    "start": "2416630",
    "end": "2421700"
  },
  {
    "text": "you blocking for locks are you blocking to simply sleep is the application developer added some sleeps",
    "start": "2421700",
    "end": "2428450"
  },
  {
    "text": "in there that shouldn't be in there are you waiting for work which is not a performance issue it's just idle and so",
    "start": "2428450",
    "end": "2435200"
  },
  {
    "text": "being able to analyze the reasons why we block is useful as a methodology because",
    "start": "2435200",
    "end": "2443779"
  },
  {
    "text": "once you know you're not CPU bound you're thinking why am i blocking and",
    "start": "2443779",
    "end": "2449239"
  },
  {
    "text": "then you're using tools to explain why I'm blocking and to measure the magnitude of why you're blocking I'll",
    "start": "2449239",
    "end": "2454970"
  },
  {
    "text": "mention those tools a bit so the observability tools how do we apply",
    "start": "2454970",
    "end": "2460700"
  },
  {
    "text": "these methodologies this is for discovering system wins that might be five to twenty five percent and",
    "start": "2460700",
    "end": "2466339"
  },
  {
    "text": "application wins which are much larger there are lots of statistics statistical",
    "start": "2466339",
    "end": "2474049"
  },
  {
    "start": "2471000",
    "end": "2471000"
  },
  {
    "text": "tools that Linux comes with we don't often use these at the command-line because we have over a hundred thousand",
    "start": "2474049",
    "end": "2481249"
  },
  {
    "text": "instances and I mean which instance would you log into it's it's unusual that we we know we have one instance",
    "start": "2481249",
    "end": "2487849"
  },
  {
    "text": "that's a problem and for some reason work or it hasn't automatically migrated off that instance so statistical tools",
    "start": "2487849",
    "end": "2497799"
  },
  {
    "text": "it's important to understand this however because the graphical tools that we do use to examine cloud performance",
    "start": "2498099",
    "end": "2505249"
  },
  {
    "text": "cloud wide often use the same metrics so if you look at Sony you understand",
    "start": "2505249",
    "end": "2510410"
  },
  {
    "text": "what's being exposed you'll see the same metrics in GUI tools so sorry is great",
    "start": "2510410",
    "end": "2515839"
  },
  {
    "text": "it has lots and lots of metrics so here I'm looking at TCP throughput I'm looking at active and passive TCP",
    "start": "2515839",
    "end": "2523299"
  },
  {
    "text": "requests go connections so active is where I'm establishing an outbound connection passive as words were where",
    "start": "2523299",
    "end": "2529670"
  },
  {
    "text": "high ethics ceptin it and then metrics like retransmissions and we use these if we",
    "start": "2529670",
    "end": "2536400"
  },
  {
    "text": "use these on the instance which is rare we do use these mostly normally so the vm stats and the page stats and the MP",
    "start": "2536400",
    "end": "2541560"
  },
  {
    "text": "stats and SARS and so on if you're in the situation where you do have to log",
    "start": "2541560",
    "end": "2547470"
  },
  {
    "start": "2544000",
    "end": "2544000"
  },
  {
    "text": "into an instance I did write a blog post with the performance team a while ago on",
    "start": "2547470",
    "end": "2552620"
  },
  {
    "text": "performance analysis in 60 seconds what's the top ten commands you would run if you had to SSH on to an instance",
    "start": "2552620",
    "end": "2559110"
  },
  {
    "text": "so uptime to look at the load averages because they they're not a great metric but at least they tell you if things are",
    "start": "2559110",
    "end": "2565290"
  },
  {
    "text": "getting worse or better D message because the kernel will often tell you if it's unhappy if it's done something",
    "start": "2565290",
    "end": "2571080"
  },
  {
    "text": "sad and then vmstat and MP stat and paid stat and so on system profilers we do",
    "start": "2571080",
    "end": "2578970"
  },
  {
    "start": "2577000",
    "end": "2577000"
  },
  {
    "text": "use a lot we ought to make these through gooeys so we don't have to log in for",
    "start": "2578970",
    "end": "2584400"
  },
  {
    "text": "example using linux perth to do cpu flame graphs perf record - f 49 for 49",
    "start": "2584400",
    "end": "2591330"
  },
  {
    "text": "Hertz that's across all CPUs of - a - G to get coal graphs or stack traces that",
    "start": "2591330",
    "end": "2598230"
  },
  {
    "text": "writes a perfect data file I can then feed it into my flame graph software and generate an interactive flame graph on a",
    "start": "2598230",
    "end": "2606360"
  },
  {
    "text": "team mountain spear has just published a new flame graph type you might notice my",
    "start": "2606360",
    "end": "2613920"
  },
  {
    "text": "flank my old flame graph suffered is written in Perl who likes Perl there's",
    "start": "2613920",
    "end": "2619440"
  },
  {
    "text": "still some of us how about that so man has written flame graphs in a new",
    "start": "2619440",
    "end": "2625380"
  },
  {
    "text": "language d3 which is kind of exciting because with D 3 D 3 gives us more",
    "start": "2625380",
    "end": "2631050"
  },
  {
    "text": "interactivity so I'm really excited for it and my Perl program may become a",
    "start": "2631050",
    "end": "2636660"
  },
  {
    "text": "thing of the past and we all might be using the d3 version once we get it into our tools so Martin finish that work",
    "start": "2636660",
    "end": "2642780"
  },
  {
    "text": "recently an open source Ted and put it into the Google paper off profile it's pretty exciting so if you notice Perl and you're a bit",
    "start": "2642780",
    "end": "2651030"
  },
  {
    "text": "worried don't worry the d3 one is in the works",
    "start": "2651030",
    "end": "2655130"
  },
  {
    "text": "when I talked about flame graphs at Breen fan 2014 this was the state of",
    "start": "2658660",
    "end": "2665609"
  },
  {
    "text": "doing a sepia performance profile of Java it was okay because we could look",
    "start": "2665609",
    "end": "2672309"
  },
  {
    "text": "into the kernel and we could look into the Jade operation of the JVM but the Java context was missing so the stacks",
    "start": "2672309",
    "end": "2679900"
  },
  {
    "text": "were all broken because Java didn't honor the frame pointer and the frame pointer is how the perf tool finds the",
    "start": "2679900",
    "end": "2690849"
  },
  {
    "text": "stack trace and the frame pointer wasn't honored as a performance optimization",
    "start": "2690849",
    "end": "2696220"
  },
  {
    "text": "which provides a miniscule tiny win on modern processors I looked at hacking in",
    "start": "2696220",
    "end": "2704319"
  },
  {
    "text": "the frame planar and so now we're in 2017 I can show how it looks this is",
    "start": "2704319",
    "end": "2711759"
  },
  {
    "text": "with the JVM with what became so article",
    "start": "2711759",
    "end": "2717309"
  },
  {
    "text": "took my patch and rewrote it and integrated it as a tunable parameter sense now minus xx plus preserved frame",
    "start": "2717309",
    "end": "2723339"
  },
  {
    "text": "pointer and so now we get to have the Java stacks so if Lane graph isn't completely busted and you can see the",
    "start": "2723339",
    "end": "2730359"
  },
  {
    "text": "this is a mixed mode flan graph you get to see everything from when the thread starts we're running in the JVM with",
    "start": "2730359",
    "end": "2736599"
  },
  {
    "text": "doing Java methods then when we're going into the kernel hands up if you've used",
    "start": "2736599",
    "end": "2741789"
  },
  {
    "text": "flame graphs before all right so we have maybe 15% flame graphs are also we use",
    "start": "2741789",
    "end": "2748869"
  },
  {
    "text": "them in Netflix all the time the x-axis is the population it's not the passage",
    "start": "2748869",
    "end": "2755740"
  },
  {
    "text": "of time the x-axis shows our full profile the y-axis is the stack depth and the color here I'm using hue to",
    "start": "2755740",
    "end": "2764349"
  },
  {
    "text": "identify different types of code GREEN for java orange for kernel yellow for the for c++ and what you do to interpret",
    "start": "2764349",
    "end": "2772660"
  },
  {
    "text": "a flame graph is you just look for their largest towers things that are wider the wider it is the more it was in the",
    "start": "2772660",
    "end": "2778299"
  },
  {
    "text": "profile the white tower on the right looks like it's I guess 60% of the",
    "start": "2778299",
    "end": "2783490"
  },
  {
    "text": "profile that's consuming 60% of the CPUs so this is directly proportional to the",
    "start": "2783490",
    "end": "2789819"
  },
  {
    "text": "performance pane so you simply look for the widest Towler's and then you can understand the profile we use them all the time we've automated",
    "start": "2789819",
    "end": "2796810"
  },
  {
    "text": "them they're used by other companies as well so Facebook has an implementation I just mentioned we contributed flank",
    "start": "2796810",
    "end": "2802180"
  },
  {
    "text": "grafts to Google's paper off but I think they already had an implementation if",
    "start": "2802180",
    "end": "2810880"
  },
  {
    "start": "2807000",
    "end": "2807000"
  },
  {
    "text": "trace is a tool for getting into other types of analysis so if Lane grafts",
    "start": "2810880",
    "end": "2816310"
  },
  {
    "text": "we've used them so far for CPU analysis F trace let's is getting to things like",
    "start": "2816310",
    "end": "2822010"
  },
  {
    "text": "blocking and IO analysis and deep kernel issues it's been around for a long time",
    "start": "2822010",
    "end": "2828840"
  },
  {
    "text": "so it was first added in Linux to 6:27 it didn't have a great front end it did",
    "start": "2828840",
    "end": "2836920"
  },
  {
    "text": "have some front ends I wrote some very simple easy-to-use tools and shared them",
    "start": "2836920",
    "end": "2843100"
  },
  {
    "text": "as perf tools which we've been putting in our base ami and I've been using to",
    "start": "2843100",
    "end": "2848110"
  },
  {
    "text": "solve various issues so just as an example of that this is is snoop that uses F trace and that's showing disk IO",
    "start": "2848110",
    "end": "2856650"
  },
  {
    "start": "2851000",
    "end": "2851000"
  },
  {
    "text": "so it's like TCP dump but for disk so instead of showing app every packet it's",
    "start": "2856650",
    "end": "2863110"
  },
  {
    "text": "showing every district west and then I can look at the latency usually useful for identifying patterns and really",
    "start": "2863110",
    "end": "2870100"
  },
  {
    "text": "understanding what the disks do imagine trying to understand what the network interfaces are doing but without using",
    "start": "2870100",
    "end": "2875230"
  },
  {
    "text": "TCP dump this is kind of what it's like to understand what disks are doing but without using a tracer it's crazy you",
    "start": "2875230",
    "end": "2881080"
  },
  {
    "text": "need to use a trace you need to see the the device by device requests and to see the latency perf is another tool that's",
    "start": "2881080",
    "end": "2890170"
  },
  {
    "start": "2888000",
    "end": "2888000"
  },
  {
    "text": "been around for a long time that's what I was using for the CPU fan graphs as perfect ndu profiling or time sampling",
    "start": "2890170",
    "end": "2896470"
  },
  {
    "text": "perf can also do tracing and that's where Instruments software and we get an event per invocation of that software so",
    "start": "2896470",
    "end": "2904090"
  },
  {
    "text": "here i'm instrumenting consume skb for when i'm consuming a socket buffer in",
    "start": "2904090",
    "end": "2909970"
  },
  {
    "text": "the linux kernel for networking and now the stack trace is showing this is how I",
    "start": "2909970",
    "end": "2915250"
  },
  {
    "text": "got to that kernel function great for debugging kernel internals you can also do user level programs and applications",
    "start": "2915250",
    "end": "2922300"
  },
  {
    "text": "as well the latest one in this field for tracing",
    "start": "2922300",
    "end": "2927540"
  },
  {
    "start": "2924000",
    "end": "2924000"
  },
  {
    "text": "his BP f so BPF the enhanced Berkeley packet filter it has been enhanced so",
    "start": "2927540",
    "end": "2936180"
  },
  {
    "text": "that it's now an internal virtual machine that can take trace information",
    "start": "2936180",
    "end": "2941280"
  },
  {
    "text": "and then summarize it in kernel where it's efficient and then only asynchronously put the summary at to use",
    "start": "2941280",
    "end": "2947670"
  },
  {
    "text": "a level previously we had things like F trace and we had per for trace points and made K probes and new probes for",
    "start": "2947670",
    "end": "2954270"
  },
  {
    "text": "dynamic tracing but to do a lot with them as a performance engineer you had to worry about the other head and",
    "start": "2954270",
    "end": "2960270"
  },
  {
    "text": "whether you could do this in production a BPF means we can do a lot of things we",
    "start": "2960270",
    "end": "2966450"
  },
  {
    "text": "couldn't before so we I could do much more of CPU analysis and I can look at scheduler I can trace schedule events",
    "start": "2966450",
    "end": "2972900"
  },
  {
    "text": "more confidently knowing that I can summarize it efficiently and I don't have to worry too much about overheads",
    "start": "2972900",
    "end": "2979110"
  },
  {
    "text": "you always still have to worry about overheads but it helps a lot and so I've got a diagram just of the internals but",
    "start": "2979110",
    "end": "2985290"
  },
  {
    "text": "it shows you can write a BPF program the kernel can attach it to trace points dynamic tracing or sampling and then can",
    "start": "2985290",
    "end": "2993390"
  },
  {
    "text": "export the information back to user level as per event data or a summary statistics just as an example of a tool",
    "start": "2993390",
    "end": "3002320"
  },
  {
    "text": "this is TCP life and it's showing each",
    "start": "3002320",
    "end": "3009890"
  },
  {
    "text": "event summarizes one session on a system and if I said well I want to see a list",
    "start": "3009890",
    "end": "3017690"
  },
  {
    "text": "of what sessions are happening on an instance you may might say well you can use TCP dump for that surely there's a",
    "start": "3017690",
    "end": "3023900"
  },
  {
    "text": "flag what's special about this is it's showing things that don't go on the wire so I can see the process ID in the",
    "start": "3023900",
    "end": "3029840"
  },
  {
    "text": "command responsible for that session I can see the millisecond duration this is",
    "start": "3029840",
    "end": "3035360"
  },
  {
    "text": "actually a nice tool uses BPF and it's open source as part of the BCC repository it's also nice in that IU is",
    "start": "3035360",
    "end": "3042740"
  },
  {
    "text": "able to write this without tracing Center C if I instrumented it just a kernel function called TCP set state to",
    "start": "3042740",
    "end": "3049100"
  },
  {
    "text": "keep the overhead a minimum TCP set state is relative to the rate of connections not the rate of packets so",
    "start": "3049100",
    "end": "3056300"
  },
  {
    "text": "this is great so it's we're starting to run this and so different teams that Netflix have started to run this to get events",
    "start": "3056300",
    "end": "3063350"
  },
  {
    "text": "whether they're interested in security or network engineering so they can understand details of all the flows",
    "start": "3063350",
    "end": "3071690"
  },
  {
    "text": "around applications and Netflix hardware counters is another observability tool",
    "start": "3071690",
    "end": "3079220"
  },
  {
    "start": "3075000",
    "end": "3075000"
  },
  {
    "text": "worth mentioning and there's two types there's MSRs and PMC's Amazonas have",
    "start": "3079220",
    "end": "3085220"
  },
  {
    "text": "been enabled for a long time and they give us basic details there's modern specific registers and so I've used them",
    "start": "3085220",
    "end": "3090740"
  },
  {
    "text": "to look on to investigate things like turbo boost performance monitoring counters PMC's let us do all sorts of",
    "start": "3090740",
    "end": "3098180"
  },
  {
    "text": "cool things but they've not been enabled in the past on ec2 here's an example",
    "start": "3098180",
    "end": "3105590"
  },
  {
    "text": "open source tool I wrote for using MSRs that shows the real CPU megahertz but",
    "start": "3105590",
    "end": "3116270"
  },
  {
    "start": "3115000",
    "end": "3115000"
  },
  {
    "text": "PMC's let us do so much more early this year Amazon launched a feature for the",
    "start": "3116270",
    "end": "3125360"
  },
  {
    "text": "m4 at 16x ELLs and some other instance types that expose the architectural set",
    "start": "3125360",
    "end": "3131690"
  },
  {
    "text": "of PMC's so these are seven PMC's that give you a very high-level view of what",
    "start": "3131690",
    "end": "3137900"
  },
  {
    "text": "the processor is doing so you can investigate stall cycles by looking at",
    "start": "3137900",
    "end": "3143870"
  },
  {
    "text": "the CP cycles and instructions retired and looking for IPC ratio between them",
    "start": "3143870",
    "end": "3150980"
  },
  {
    "text": "and figuring out how much the CPU is stalled you can look at last level cache",
    "start": "3150980",
    "end": "3156260"
  },
  {
    "text": "references and misses and branch retired branch prediction as well and how well",
    "start": "3156260",
    "end": "3162320"
  },
  {
    "text": "that's working this is great I wrote a blog post on it I actually wrote the patch for Xen originally to add this to",
    "start": "3162320",
    "end": "3168170"
  },
  {
    "text": "Xen but Amazon when you did their own thing which is great but I'm having to",
    "start": "3168170",
    "end": "3173660"
  },
  {
    "text": "rethink this all because now with the new c5 hypervisor the c5 hypervisor has",
    "start": "3173660",
    "end": "3179750"
  },
  {
    "text": "access to all the PMC's which is awesome so we now get to look at I mean I've just run a very simple tool here but",
    "start": "3179750",
    "end": "3186890"
  },
  {
    "text": "there are hundreds of PMC's so that you can understand what the TLB s are doing what all levels of cache they call it on corn and uncor events",
    "start": "3186890",
    "end": "3194360"
  },
  {
    "start": "3190000",
    "end": "3190000"
  },
  {
    "text": "when you will off the cpu and you're talking to devices or memory hands up if you've done pmc worker before so we've",
    "start": "3194360",
    "end": "3202790"
  },
  {
    "text": "got like five people so if your based on the cloud I get I guess I understand why you haven't because you haven't had",
    "start": "3202790",
    "end": "3208670"
  },
  {
    "text": "access but it's something really cool it's especially important because workloads are often moving to being",
    "start": "3208670",
    "end": "3215600"
  },
  {
    "text": "memory bound in the distant past it was always disk IO disk IO was always",
    "start": "3215600",
    "end": "3221600"
  },
  {
    "text": "killing performance but now with after decades of engineering discs a fast SSD",
    "start": "3221600",
    "end": "3227570"
  },
  {
    "text": "as fast main memories huge and caches things and so a lot of the performance issues and now the memory buses and the",
    "start": "3227570",
    "end": "3233540"
  },
  {
    "text": "CPU interconnects and PMC's let you investigate that that is a major area of",
    "start": "3233540",
    "end": "3239300"
  },
  {
    "text": "performance issues nowadays so there's a tool that we can now run on the cloud",
    "start": "3239300",
    "end": "3246920"
  },
  {
    "text": "intel vtune so just as an example of a new tool that you might hear about in the future but it uses the PMC's to do",
    "start": "3246920",
    "end": "3253670"
  },
  {
    "text": "this sort of analysis but there's a lot of open source tools as well now flicks an atlas I did mention that's",
    "start": "3253670",
    "end": "3260060"
  },
  {
    "start": "3257000",
    "end": "3257000"
  },
  {
    "text": "our cloud wide monitoring tool that we use to look at our over a hundred",
    "start": "3260060",
    "end": "3265400"
  },
  {
    "text": "thousand instances and that's pretty great it's very important that it works",
    "start": "3265400",
    "end": "3271640"
  },
  {
    "text": "we'd use it for Incident Response and it",
    "start": "3271640",
    "end": "3277220"
  },
  {
    "text": "has a lot of the OS metrics in there for analysis and we quickly can do filters",
    "start": "3277220",
    "end": "3282859"
  },
  {
    "text": "and breakdowns by region application ASG or metric or instance Netflix vector is",
    "start": "3282859",
    "end": "3289490"
  },
  {
    "start": "3288000",
    "end": "3288000"
  },
  {
    "text": "a newer tool that we've been developing for instance analysis and so this has a",
    "start": "3289490",
    "end": "3295250"
  },
  {
    "text": "use method inspired dashboard showing utilization saturation and errors but",
    "start": "3295250",
    "end": "3301460"
  },
  {
    "text": "vector is also great because it it's where we can launch on-demand flame graphs so we can click a button and get",
    "start": "3301460",
    "end": "3306890"
  },
  {
    "start": "3302000",
    "end": "3302000"
  },
  {
    "text": "a sepia flame graph in a minute and quickly understand CPU performance and",
    "start": "3306890",
    "end": "3314650"
  },
  {
    "text": "I've recently been adding more flame graph types to do the office EPO analysis using EBP F which which is",
    "start": "3314650",
    "end": "3321260"
  },
  {
    "text": "awesome so vector is also open source like Atlas for analyzing low-level",
    "start": "3321260",
    "end": "3327440"
  },
  {
    "text": "performance in so many there was a tour",
    "start": "3327440",
    "end": "3333170"
  },
  {
    "start": "3331000",
    "end": "3331000"
  },
  {
    "text": "of what our team does from instant selection understanding Amazon ec2",
    "start": "3333170",
    "end": "3339050"
  },
  {
    "text": "features tuning and then methodologies for tuning and analysis and many of the",
    "start": "3339050",
    "end": "3345800"
  },
  {
    "text": "observability tools a team does a lot more as well because we often do get into Java internals and work with",
    "start": "3345800",
    "end": "3351680"
  },
  {
    "text": "applications and understand performance in detail this site will be shared and",
    "start": "3351680",
    "end": "3360470"
  },
  {
    "start": "3357000",
    "end": "3357000"
  },
  {
    "text": "I've got lots of references and links and I did mention I will do a blog post tomorrow where I do an extended version",
    "start": "3360470",
    "end": "3367670"
  },
  {
    "text": "of the hypervisor diagram and I'll explain each of the rows in detail because things are so confusing and and",
    "start": "3367670",
    "end": "3373700"
  },
  {
    "text": "the new hypervisor c5 hypervisor is adding even more things for us to think",
    "start": "3373700",
    "end": "3379100"
  },
  {
    "text": "about but I'll do a blog post to explain that of course there is a talk tomorrow about the c5 hypervisor which I",
    "start": "3379100",
    "end": "3385160"
  },
  {
    "text": "recommend seeing by Anthony Liguori and there's also other Netflix talks at reinvent that I recommend seeing as well",
    "start": "3385160",
    "end": "3393130"
  },
  {
    "start": "3388000",
    "end": "3388000"
  },
  {
    "text": "I'll finish with I can take a couple of questions and then many of us will be at the",
    "start": "3393130",
    "end": "3399920"
  },
  {
    "text": "Netflix booth today and I'll be the Netflix booth so you can contact us then",
    "start": "3399920",
    "end": "3405020"
  },
  {
    "text": "and the slides will be online thank you [Applause]",
    "start": "3405020",
    "end": "3415010"
  }
]