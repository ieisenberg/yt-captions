[
  {
    "start": "0",
    "end": "43000"
  },
  {
    "text": "hi everybody so welcome again to reinvent happy to be here my name is",
    "start": "0",
    "end": "6330"
  },
  {
    "text": "Yaniv donenfeld I'm a business development manager for container services at AWS and we are in con 301",
    "start": "6330",
    "end": "14130"
  },
  {
    "text": "mastering kubernetes on AWS so we're gonna split this session to two parts",
    "start": "14130",
    "end": "20430"
  },
  {
    "text": "one is going to be the mastering kubernetes on AWS which we'll talk about kubernetes in general and give you some",
    "start": "20430",
    "end": "25680"
  },
  {
    "text": "tips and best practices and how some aspects of kubernetes like networking and security looks like and then we're",
    "start": "25680",
    "end": "33420"
  },
  {
    "text": "gonna have Carl a demo from snap who's gonna talk to you about snap and their",
    "start": "33420",
    "end": "38610"
  },
  {
    "text": "journey through kubernetes in general and what they do on the platform so this",
    "start": "38610",
    "end": "44309"
  },
  {
    "start": "43000",
    "end": "83000"
  },
  {
    "text": "is a rough idea of the agenda for today just kidding yeah so we're going to talk",
    "start": "44309",
    "end": "50219"
  },
  {
    "text": "about kubernetes and when when it comes to talk about kubernetes when you start off building your workloads on",
    "start": "50219",
    "end": "56520"
  },
  {
    "text": "containers and I'm pretty sure a lot of you know this it's really oh one more",
    "start": "56520",
    "end": "62219"
  },
  {
    "text": "thing I forgot sorry the breakup repeat that's today so that deck is done okay",
    "start": "62219",
    "end": "67380"
  },
  {
    "text": "so when starting to talk about the local development that's easy that's usually",
    "start": "67380",
    "end": "73229"
  },
  {
    "text": "just relying on docker or just running a simple kubernetes cluster either on your laptop through",
    "start": "73229",
    "end": "78299"
  },
  {
    "text": "things like mini cube or just running them in the cloud in a very simple way however when it comes to running things",
    "start": "78299",
    "end": "84930"
  },
  {
    "start": "83000",
    "end": "142000"
  },
  {
    "text": "at scale that's when things become more complicated so talking about separation",
    "start": "84930",
    "end": "90270"
  },
  {
    "text": "between control plane and the data plane control train is all the kubernetes components like masters at CD in which",
    "start": "90270",
    "end": "97470"
  },
  {
    "text": "by themselves run a few components on the master like the controllers the api server dns and a bunch of other things",
    "start": "97470",
    "end": "104430"
  },
  {
    "text": "that are required for the cluster to function correctly and then you have your worker nodes these are your ec2",
    "start": "104430",
    "end": "110939"
  },
  {
    "text": "instances on AWS on which you're going to run your docker containers and then",
    "start": "110939",
    "end": "116369"
  },
  {
    "text": "that poses a set of challenges on its own because now you have to kind of scale your workload on top of a lot of",
    "start": "116369",
    "end": "122880"
  },
  {
    "text": "instances which are spread across multiple disease and you have to figure out things like how to schedule them",
    "start": "122880",
    "end": "128009"
  },
  {
    "text": "properly on your platform how to connect monitoring and logging and tracing debugging",
    "start": "128009",
    "end": "133690"
  },
  {
    "text": "and generally speaking managing that fleet which is an operational burden by its own so those are like two different",
    "start": "133690",
    "end": "139390"
  },
  {
    "text": "challenges and we'll talk about those two aspects so starting with the control plane and I think we're gonna spend majority of the time on that really when",
    "start": "139390",
    "end": "148390"
  },
  {
    "start": "142000",
    "end": "206000"
  },
  {
    "text": "you're moving your stuff to production you need to have a highly available cluster that means your masters usually",
    "start": "148390",
    "end": "155320"
  },
  {
    "text": "won't be a single replica but rather multiple ones and then the HCD database",
    "start": "155320",
    "end": "161410"
  },
  {
    "text": "which is the persistence layer every object in kubernetes is getting persisted onto @cd so that is kind of a",
    "start": "161410",
    "end": "168460"
  },
  {
    "text": "critical piece on its own because if etsy DS down then well whatever is running is still gonna keep running but",
    "start": "168460",
    "end": "174700"
  },
  {
    "text": "you cannot provision anything new on your cluster and you're going to lose the state of your cluster which means",
    "start": "174700",
    "end": "180610"
  },
  {
    "text": "the desired capacity of each of your services and so on and so forth any secrets that you had stored so really",
    "start": "180610",
    "end": "187600"
  },
  {
    "text": "when I kind of started to look into how do I convey the message of how complex it is to run those fingers scale I ran",
    "start": "187600",
    "end": "194050"
  },
  {
    "text": "into one of the articles an interview by one of the CNCs ambassadors and he",
    "start": "194050",
    "end": "199180"
  },
  {
    "text": "really knows a lot about kubernetes but just by judging them based on what he said you can tell how complex that is to run",
    "start": "199180",
    "end": "205870"
  },
  {
    "text": "at scale because you have a multiple you have multiple things to consider things",
    "start": "205870",
    "end": "211870"
  },
  {
    "text": "like for example if you want to have a quorum you need to be resilient to at least one failover as a server so you",
    "start": "211870",
    "end": "218710"
  },
  {
    "text": "need to have a quorum of three if you want to be resilient to two fails of a master node then you already have to",
    "start": "218710",
    "end": "224890"
  },
  {
    "text": "have a quorum of five and that's masters you can actually replicate those but what about components like the DNS",
    "start": "224890",
    "end": "230950"
  },
  {
    "text": "server for example that's only a single replica so what happens if that goes down then you don't have service",
    "start": "230950",
    "end": "236290"
  },
  {
    "text": "discovery in your cluster anymore not to mention the fact that you have things",
    "start": "236290",
    "end": "241330"
  },
  {
    "text": "like controller managers that are like working in active passive mode that means that whenever one is failing the",
    "start": "241330",
    "end": "248440"
  },
  {
    "text": "other the rest five for three components do you have are going to start racing towards hey I'm gonna pick it up instead",
    "start": "248440",
    "end": "255760"
  },
  {
    "text": "of the previous one that failed so this thing is complex to run and manage at scale that's kind of the bottom line of",
    "start": "255760",
    "end": "262510"
  },
  {
    "start": "262000",
    "end": "289000"
  },
  {
    "text": "it and when we start to kind of talk to customers and understand and what they're doing two things popped",
    "start": "262510",
    "end": "268250"
  },
  {
    "text": "up one we really have a significant amount of customers running kubernetes on EWS the majority of workloads",
    "start": "268250",
    "end": "275540"
  },
  {
    "text": "actually and the other one was they all asked us or a majority of the Masters to",
    "start": "275540",
    "end": "281090"
  },
  {
    "text": "run this thing for them at least that particular control plane which is hard to configure how to provision hard to",
    "start": "281090",
    "end": "287570"
  },
  {
    "text": "manage hard to scale so with Amazon eks the first thing that you get when you're",
    "start": "287570",
    "end": "293420"
  },
  {
    "start": "289000",
    "end": "327000"
  },
  {
    "text": "building that is a fully managed control plane which is being built when you're",
    "start": "293420",
    "end": "299150"
  },
  {
    "text": "creating an e KS cluster so on e KS every cluster actually has its own control pane which is made of multiple",
    "start": "299150",
    "end": "305780"
  },
  {
    "text": "masters and multiple EDD instances that are running across multiple AZ's and",
    "start": "305780",
    "end": "310840"
  },
  {
    "text": "what you're getting is a managed AWS endpoint that is basically your master",
    "start": "310840",
    "end": "316760"
  },
  {
    "text": "endpoint so you will point all your worker nodes and all your configuration tools like you cuddle to point to that",
    "start": "316760",
    "end": "323750"
  },
  {
    "text": "particular endpoint when it comes to communicating between the workers and",
    "start": "323750",
    "end": "330440"
  },
  {
    "start": "327000",
    "end": "412000"
  },
  {
    "text": "the masters this is kind of a cross account provisioning which means the",
    "start": "330440",
    "end": "335510"
  },
  {
    "text": "control plane components are being provisioned on our AWS service account whereas your worker nodes your ec2",
    "start": "335510",
    "end": "342680"
  },
  {
    "text": "instances that you're bringing in to run the pods are being provisioned in your V PC and your account and so the way that",
    "start": "342680",
    "end": "349670"
  },
  {
    "text": "they talk to each other is the workers talk to the master through the public endpoint which is public today and we",
    "start": "349670",
    "end": "356660"
  },
  {
    "text": "also already announced that we have plans to make or have available a private link endpoint and we're going to",
    "start": "356660",
    "end": "362510"
  },
  {
    "text": "do that next year too and device versa so when the Masters need to talk to the",
    "start": "362510",
    "end": "369100"
  },
  {
    "text": "workers for commands like queue cuttle proxy or cube catalogs or Cupit alexa",
    "start": "369100",
    "end": "374930"
  },
  {
    "text": "k-- so that's being provisioned by us creating what we call a cross account",
    "start": "374930",
    "end": "380030"
  },
  {
    "text": "eni so whenever you are creating a kubernetes cluster you need to specify a",
    "start": "380030",
    "end": "385790"
  },
  {
    "text": "list of subnets and a V PC in which you will run your workloads in now we are",
    "start": "385790",
    "end": "391820"
  },
  {
    "text": "taking those subnets tagging them and provisioning cross account en eyes in",
    "start": "391820",
    "end": "397730"
  },
  {
    "text": "each of those subnets so that the master we'll be able to talk to them for that",
    "start": "397730",
    "end": "403070"
  },
  {
    "text": "particular communication so that is the way that is being provisioned behind the scenes when you guys are creating your",
    "start": "403070",
    "end": "408860"
  },
  {
    "text": "ich es clusters so I want to shed a bit more light on the networking aspect of",
    "start": "408860",
    "end": "415729"
  },
  {
    "start": "412000",
    "end": "467000"
  },
  {
    "text": "how kubernetes networking runs on Amazon so first of all from a pod perspective",
    "start": "415729",
    "end": "422199"
  },
  {
    "text": "when you guys have a bunch of pods on your machine they need to be able to communicate kubernetes is not",
    "start": "422199",
    "end": "428740"
  },
  {
    "text": "opinionated as to how you implement your networking as long as whatever network",
    "start": "428740",
    "end": "433970"
  },
  {
    "text": "implementation you have will sustain those three simple rules which are all the pods need to communicate with each",
    "start": "433970",
    "end": "440810"
  },
  {
    "text": "other directly known adding involved all the pods and all the nodes need to be able to talk to each other",
    "start": "440810",
    "end": "446720"
  },
  {
    "text": "interchangeably again no natty involved and then whatever IP the pod sees itself as",
    "start": "446720",
    "end": "452930"
  },
  {
    "text": "should be the same IP that other pods in the system sees it as as long as your",
    "start": "452930",
    "end": "459099"
  },
  {
    "text": "calico or flannel or whatever implementation you have and all of those do maintain those you're all good to go",
    "start": "459099",
    "end": "465470"
  },
  {
    "text": "and so we actually created an AWS V PCC and I plug in C and I for those of you",
    "start": "465470",
    "end": "472789"
  },
  {
    "start": "467000",
    "end": "637000"
  },
  {
    "text": "who doesn't know its container network interface it's kind of a generic interface that most of the network",
    "start": "472789",
    "end": "479419"
  },
  {
    "text": "implementations for kubernetes basically follow as a guideline to how to implement networking and so AWS created",
    "start": "479419",
    "end": "486620"
  },
  {
    "text": "an specific CNI plugin that uses our V PC as a way to communicate between your",
    "start": "486620",
    "end": "493310"
  },
  {
    "text": "pods so what happens is every time you're spinning up a worker node there",
    "start": "493310",
    "end": "499729"
  },
  {
    "text": "is a CNI plugin that runs as a daemon or as a daemon set inside that node it",
    "start": "499729",
    "end": "505250"
  },
  {
    "text": "contains a component called IP MD or IP manager which means that particular",
    "start": "505250",
    "end": "511300"
  },
  {
    "text": "manager will maintain a list of Ian's eyes on that node and a list of IP",
    "start": "511300",
    "end": "516589"
  },
  {
    "text": "addresses that are associated with them so for every E and I we actually provision multiple secondary IP",
    "start": "516589",
    "end": "522919"
  },
  {
    "text": "addresses and kubernetes will be able to leverage those secondary IP addresses to",
    "start": "522919",
    "end": "528410"
  },
  {
    "text": "use for scheduling your pods so for example if you have an instance that can bind",
    "start": "528410",
    "end": "534350"
  },
  {
    "text": "for Eni is based on that instance type and each of those for instance sorry each of those en is can have eight",
    "start": "534350",
    "end": "540890"
  },
  {
    "text": "secondary IP addresses that's the multiplier of how many pods you can actually run in that node so that's the",
    "start": "540890",
    "end": "547940"
  },
  {
    "text": "kind of max density that you can get for that particular node and obviously you",
    "start": "547940",
    "end": "553040"
  },
  {
    "text": "can scale that with bringing in more nodes to the cluster but that's not the end of the story so that C&I plugin",
    "start": "553040",
    "end": "559490"
  },
  {
    "text": "actually makes VPC calls to the are VPC endpoints and fetching en eyes and IP",
    "start": "559490",
    "end": "564830"
  },
  {
    "text": "addresses as kind of a warm pool to be able to allocate to any running pod and then any part that is getting scheduled",
    "start": "564830",
    "end": "572900"
  },
  {
    "text": "basically takes one of those specific instance pool en eyes and secondary IP",
    "start": "572900",
    "end": "579980"
  },
  {
    "text": "addresses and associates them with the underlying pod the daemon runs in its",
    "start": "579980",
    "end": "586880"
  },
  {
    "text": "own so it runs in the root filesystem and it's connected to the pod namespace",
    "start": "586880",
    "end": "592610"
  },
  {
    "text": "through what we call a VF that bridges them and connects them together so what",
    "start": "592610",
    "end": "601670"
  },
  {
    "text": "does that all mean it means that essentially what you have is you have V PC networking running in your pods unlike solutions like calico or flannel",
    "start": "601670",
    "end": "609470"
  },
  {
    "text": "or any other network implementation for kubernetes this is not an overlay",
    "start": "609470",
    "end": "614870"
  },
  {
    "text": "Network there is no overhead of running an additional networking layer but rather just using the standard V PC with",
    "start": "614870",
    "end": "621770"
  },
  {
    "text": "all the implications of a V PC you get like a high performance low jitter network that you can use with nat",
    "start": "621770",
    "end": "627350"
  },
  {
    "text": "gateways with router tables everything that the V PC offers is now available to you through your kubernetes workloads so",
    "start": "627350",
    "end": "639530"
  },
  {
    "start": "637000",
    "end": "776000"
  },
  {
    "text": "just to shed some light on how the IP allocation works or how does your V PC",
    "start": "639530",
    "end": "646310"
  },
  {
    "text": "gets allocated to the multiple workloads that we create for you on eks so when",
    "start": "646310",
    "end": "652520"
  },
  {
    "text": "you're creating a primary V PC which is to be used for your eks cluster you're",
    "start": "652520",
    "end": "659150"
  },
  {
    "text": "using a specific primary side arrange for your V PC that can be one of the RFC",
    "start": "659150",
    "end": "665210"
  },
  {
    "text": "1918 rages and then those ranges will be part of the subnets that you're",
    "start": "665210",
    "end": "671150"
  },
  {
    "text": "providing to us on cluster creation the one that we are tagging and using so",
    "start": "671150",
    "end": "676790"
  },
  {
    "text": "those will be used for pods as I mentioned earlier will allocate those secondary IPS to pods from that but",
    "start": "676790",
    "end": "683720"
  },
  {
    "text": "we're also going to create the cross account en eyes and assign those IPS to them too and then the third thing is",
    "start": "683720",
    "end": "690280"
  },
  {
    "text": "when you're creating kubernetes services and we'll talk about Community Services",
    "start": "690280",
    "end": "695450"
  },
  {
    "text": "in just a bit those services has what we call a cluster IP that is an internal virtual",
    "start": "695450",
    "end": "701390"
  },
  {
    "text": "IP created by Cuba natives and the range that we assigned to those internal Service IPS is going to be based on",
    "start": "701390",
    "end": "708620"
  },
  {
    "text": "whatever you have selected for your V PC but it's going to be a different one so",
    "start": "708620",
    "end": "714470"
  },
  {
    "text": "what I mean by that is if you've created any other than 10.0 to your V PC then",
    "start": "714470",
    "end": "722450"
  },
  {
    "text": "we'll allocate 10 dot 100 to your service range for creating the internal",
    "start": "722450",
    "end": "728090"
  },
  {
    "text": "IPS whereas if you created the 10.0 for your IPS in your V PC",
    "start": "728090",
    "end": "734870"
  },
  {
    "text": "we will not use that for your internal services in order to give you the maximum IP space that you can use or",
    "start": "734870",
    "end": "741140"
  },
  {
    "text": "leverage for your pods but rather use an alternate V PC I'm sorry an alternate IP range which is 170 2.20 so in some cases",
    "start": "741140",
    "end": "749840"
  },
  {
    "text": "you will create an e KS cluster and you will see the internal IPS as 10 100 in other cases based on what you've",
    "start": "749840",
    "end": "756410"
  },
  {
    "text": "configured to your V PC you will see a 172 dot 20 how does that all work again",
    "start": "756410",
    "end": "763480"
  },
  {
    "text": "creating the e KS cluster providing the V PC and the subnets that you want us to",
    "start": "763480",
    "end": "768950"
  },
  {
    "text": "use and then everything else will be created by and provisioned by our eks control plane",
    "start": "768950",
    "end": "776679"
  },
  {
    "text": "there is a new feature however that allows you to extend or further extend",
    "start": "776679",
    "end": "782569"
  },
  {
    "text": "the range of IPs that you will use for your pods so some customers have been",
    "start": "782569",
    "end": "788689"
  },
  {
    "text": "asking us to kind of say hey we have exhausted the list of IPs that you can use from our cider or I primary side",
    "start": "788689",
    "end": "795860"
  },
  {
    "text": "arranged for our V PC and we would still like to be able to run more IPs because we have more paused on that so for those",
    "start": "795860",
    "end": "802399"
  },
  {
    "text": "we actually enabled a new feature which allows you to add secondary cider ranges",
    "start": "802399",
    "end": "808009"
  },
  {
    "text": "of your V PC and allocate those specific ranges to you're running pods only not",
    "start": "808009",
    "end": "813679"
  },
  {
    "text": "for your across the county and is not for your internal IP services but what you care about we do deliver is the",
    "start": "813679",
    "end": "821149"
  },
  {
    "text": "ability to assign more IP space to your pods now how does that get provisioned",
    "start": "821149",
    "end": "827319"
  },
  {
    "text": "first you have to use our latest CNI plug-in version which is 12.1 or",
    "start": "827319",
    "end": "832970"
  },
  {
    "text": "anything above that and then what you need to do is you're enabling that",
    "start": "832970",
    "end": "838100"
  },
  {
    "text": "feature and the documentation will state how to enable that feature once you did you need to create something called a",
    "start": "838100",
    "end": "844699"
  },
  {
    "text": "custom resource definition now that's a kubernetes construct or CRD a CRT is a",
    "start": "844699",
    "end": "851179"
  },
  {
    "text": "kubernetes construct that allows you to make specific definitions that can then be made available to your running",
    "start": "851179",
    "end": "857299"
  },
  {
    "text": "services and pods so in this case we will create a custom resource definition called eni config there is basically",
    "start": "857299",
    "end": "864470"
  },
  {
    "text": "just a snippet of configuration that says what is the IPS what are the",
    "start": "864470",
    "end": "870679"
  },
  {
    "text": "security groups and the subnets that you want to further allocate to your running pods and that's going to be configured",
    "start": "870679",
    "end": "877009"
  },
  {
    "text": "on a host basis so for each of your hosts you're gonna be annotating that",
    "start": "877009",
    "end": "882379"
  },
  {
    "text": "host with the specific eni configuration creating those additional subnets",
    "start": "882379",
    "end": "888589"
  },
  {
    "text": "or enabling those additional subnets and additional security groups to be used by",
    "start": "888589",
    "end": "893839"
  },
  {
    "text": "the pods that are running on that pod on that host specifically naturally those",
    "start": "893839",
    "end": "899209"
  },
  {
    "text": "subnets would have to be subnets that are in the same AV as the instance that you are running on obviously",
    "start": "899209",
    "end": "906878"
  },
  {
    "start": "907000",
    "end": "1029000"
  },
  {
    "text": "so just to highlight a few of the other kind of newer functionality that we have",
    "start": "908600",
    "end": "914089"
  },
  {
    "text": "enabled on our latest version of the cni plugin for those of you who haven't heard so in addition to the custom",
    "start": "914089",
    "end": "920959"
  },
  {
    "text": "network configuration which I just talked about we also enabled what we call external s net so s net is a source",
    "start": "920959",
    "end": "929269"
  },
  {
    "text": "network address translation what that means is that we specified earlier that",
    "start": "929269",
    "end": "935000"
  },
  {
    "text": "each of your pods is getting its own IP address in your V PC that's an internal V PC address which will not allow it to",
    "start": "935000",
    "end": "942079"
  },
  {
    "text": "communicate with the outside world with the internet now if your pod is running",
    "start": "942079",
    "end": "947480"
  },
  {
    "text": "on a host which runs in a public subnet and needs to communicate with the Internet what we do by default is",
    "start": "947480",
    "end": "954110"
  },
  {
    "text": "whenever it makes that request there is an S net component that translates or",
    "start": "954110",
    "end": "959449"
  },
  {
    "text": "the cni plugin translates that internal V PC address to the hosts public IP",
    "start": "959449",
    "end": "965540"
  },
  {
    "text": "address which then in turn allows it to communicate with the Internet so that's",
    "start": "965540",
    "end": "972170"
  },
  {
    "text": "basically the way it's going to work and for other use cases that is not going to",
    "start": "972170",
    "end": "978949"
  },
  {
    "text": "be very helpful for example if you have a need to connect your pod to an external traffic coming from a separate",
    "start": "978949",
    "end": "985790"
  },
  {
    "text": "IP space which could be a VPN connection or a direct connect that's not gonna",
    "start": "985790",
    "end": "992180"
  },
  {
    "text": "work so well for you so for those kind of use cases you would put your pods on",
    "start": "992180",
    "end": "997309"
  },
  {
    "text": "a post that's running in a private subnet and you would disable ass net by",
    "start": "997309",
    "end": "1002439"
  },
  {
    "text": "specifying the external s net flag now what that means is that external means",
    "start": "1002439",
    "end": "1008470"
  },
  {
    "text": "that you have to now take care of providing an S net if you wanted that particular node part to be able to talk",
    "start": "1008470",
    "end": "1015370"
  },
  {
    "text": "to the Internet and that can be in the form of any network address translation service for example the AWS management",
    "start": "1015370",
    "end": "1021779"
  },
  {
    "text": "or any other netting service that you guys are using but that's something that you will now have to configure yourself",
    "start": "1021779",
    "end": "1030270"
  },
  {
    "start": "1029000",
    "end": "1061000"
  },
  {
    "text": "so this is the basics of how the part 2 pod communication works or part 2 the",
    "start": "1030329",
    "end": "1035949"
  },
  {
    "text": "outside world I want to go over one the very important aspects of running",
    "start": "1035949",
    "end": "1042529"
  },
  {
    "text": "kubernetes which is services so I'm pretty sure most of you know this but services are kind of an abstraction that",
    "start": "1042529",
    "end": "1048980"
  },
  {
    "text": "allows you to take incoming traffic and actually route that to a bunch of pods that are representing your application",
    "start": "1048980",
    "end": "1055309"
  },
  {
    "text": "or your micro service now there are a number of ways to do that on kubernetes so the first one is the cluster IP and",
    "start": "1055309",
    "end": "1064250"
  },
  {
    "start": "1061000",
    "end": "1191000"
  },
  {
    "text": "were you creating a service you will have to specify what is the service type that you want us to implement and as you",
    "start": "1064250",
    "end": "1071960"
  },
  {
    "text": "will see this is kind of a layered architecture and I'll show that in a second but the first most basic mode",
    "start": "1071960",
    "end": "1078559"
  },
  {
    "text": "would be a cluster IP remember earlier when we talked about we can allocate",
    "start": "1078559",
    "end": "1083919"
  },
  {
    "text": "specific ranges to our internal services on kubernetes that is those internal",
    "start": "1083919",
    "end": "1090289"
  },
  {
    "text": "network ranges that I've talked about so every time you're creating a community service one of those IPS will get",
    "start": "1090289",
    "end": "1096529"
  },
  {
    "text": "allocated as a cluster IP which again a cluster IP is a virtual IP it's only",
    "start": "1096529",
    "end": "1102889"
  },
  {
    "text": "accessible from within the cluster you cannot access the the cluster IP from outside the cluster but it's really good",
    "start": "1102889",
    "end": "1109970"
  },
  {
    "text": "if you wanted to communicate between your micro services internally now there",
    "start": "1109970",
    "end": "1115220"
  },
  {
    "text": "is a proxying that takes place whenever you're making a call to the cluster IP the proxying can be of several types the",
    "start": "1115220",
    "end": "1123289"
  },
  {
    "text": "most common one being used is IP tables and that is being managed by a component called queue proxy so queue proxy is a",
    "start": "1123289",
    "end": "1131419"
  },
  {
    "text": "component that sits on each of your nodes and manipulates the rules or the",
    "start": "1131419",
    "end": "1137179"
  },
  {
    "text": "traffic in a way that will actually forward or route that to the right pod",
    "start": "1137179",
    "end": "1142750"
  },
  {
    "text": "so in IP tables mode queue proxy itself is not involved in the request itself",
    "start": "1142750",
    "end": "1149179"
  },
  {
    "text": "but rather what it does is it manipulates the IP tables on that particular host and making sure that the",
    "start": "1149179",
    "end": "1156110"
  },
  {
    "text": "IP tables rule will actually forward the request that you have hit with that particular node to the right part that",
    "start": "1156110",
    "end": "1162350"
  },
  {
    "text": "you have so that is the IP tables mode there are other modes like users face in",
    "start": "1162350",
    "end": "1167720"
  },
  {
    "text": "which the queue proxy itself will take an active participation in in the request and then there is a newer",
    "start": "1167720",
    "end": "1175070"
  },
  {
    "text": "mode called IP vs which started to get picked more and more because of its increased efficiency so those are like",
    "start": "1175070",
    "end": "1182629"
  },
  {
    "text": "different modes of how you can proxy a request from hitting these cluster IP on",
    "start": "1182629",
    "end": "1188119"
  },
  {
    "text": "to your running pods so that's one option the other option you have is you",
    "start": "1188119",
    "end": "1194149"
  },
  {
    "start": "1191000",
    "end": "1301000"
  },
  {
    "text": "can use a new port an old port basically exposes your service on each of the",
    "start": "1194149",
    "end": "1199190"
  },
  {
    "text": "cluster nodes regardless of whether it's running the specific pods or not running",
    "start": "1199190",
    "end": "1204679"
  },
  {
    "text": "the specific pods each of your cluster nodes will now have a specific port open to each of your note port services what",
    "start": "1204679",
    "end": "1212719"
  },
  {
    "text": "that means is that there is a certain routing that could take place if for",
    "start": "1212719",
    "end": "1218479"
  },
  {
    "text": "example you're hitting a node but that node does not even have the part that you're trying to reach so there will be",
    "start": "1218479",
    "end": "1223909"
  },
  {
    "text": "a redirect and forwarding of that request to the right place using a cluster IP service so basically the node",
    "start": "1223909",
    "end": "1232339"
  },
  {
    "text": "ports are all forwarding the request to a cluster IP service which we talked about earlier which gets created every",
    "start": "1232339",
    "end": "1239419"
  },
  {
    "text": "time you're creating a node port so you can look at it like we have an external endpoint on each of our nodes on a",
    "start": "1239419",
    "end": "1246169"
  },
  {
    "text": "specific port so it's going to be the same port for a specific service on all the nodes that we have and that request",
    "start": "1246169",
    "end": "1253549"
  },
  {
    "text": "no matter which node it hit will get routed to the cluster IP and from there on the same path that I explained",
    "start": "1253549",
    "end": "1260719"
  },
  {
    "text": "earlier and one thing I forgot to mention is that cluster IPs are",
    "start": "1260719",
    "end": "1267349"
  },
  {
    "text": "accessible from the outside so you can actually call the node IP and the specific port and you will be able to",
    "start": "1267349",
    "end": "1274309"
  },
  {
    "text": "access that service but it does leave upon you to do kind of the load balancing aspect of hitting the right",
    "start": "1274309",
    "end": "1281239"
  },
  {
    "text": "set of nodes or node as to make your request so there's no load balancer",
    "start": "1281239",
    "end": "1287959"
  },
  {
    "text": "involved there to kind of be the single endpoint and forward the requests to multiple nodes but rather it leaves upon",
    "start": "1287959",
    "end": "1295099"
  },
  {
    "text": "you the responsibility to choose whoever node you want it to call for that particular service so the third mode",
    "start": "1295099",
    "end": "1304279"
  },
  {
    "start": "1301000",
    "end": "1394000"
  },
  {
    "text": "that we have is a load balancer that is a mode that's available only on the cloud provider like AWS and that",
    "start": "1304279",
    "end": "1311210"
  },
  {
    "text": "will in turn when you're creating a load balancer service will cause a creation",
    "start": "1311210",
    "end": "1316490"
  },
  {
    "text": "of a load balancer in AWS s case it would be an e lb or an N lb that would",
    "start": "1316490",
    "end": "1322669"
  },
  {
    "text": "be elastic load balancer or a network load balancer which is kind of the newer generation and then that load balancer",
    "start": "1322669",
    "end": "1329330"
  },
  {
    "text": "will forward the requests back to the node ports the node port service that",
    "start": "1329330",
    "end": "1335630"
  },
  {
    "text": "gets created which has its own cluster IP to so you can see how that's all being layered one on top of each other",
    "start": "1335630",
    "end": "1342250"
  },
  {
    "text": "but obviously the load balancer can give you the benefit of having that load",
    "start": "1342250",
    "end": "1347330"
  },
  {
    "text": "balancer sustain all the traffic that you can that can come in in whatever",
    "start": "1347330",
    "end": "1352580"
  },
  {
    "text": "place and kind of be the shock absorber of that traffic and then offload that to",
    "start": "1352580",
    "end": "1359630"
  },
  {
    "text": "your other to the pods that are running behind it now obviously you also get benefits from",
    "start": "1359630",
    "end": "1366230"
  },
  {
    "text": "the load balancer itself which are whatever the yield B can offer in terms of encryption certificates security so",
    "start": "1366230",
    "end": "1373669"
  },
  {
    "text": "those things can be provisioned as kind of annotations that you can configure in kubernetes so if you go to the",
    "start": "1373669",
    "end": "1380090"
  },
  {
    "text": "kubernetes documentation you can actually see a lot of the annotations that you can put on provisioning",
    "start": "1380090",
    "end": "1385460"
  },
  {
    "text": "features for your NLB or sorry for your LB as to how to make those available on",
    "start": "1385460",
    "end": "1392990"
  },
  {
    "text": "running on AWS now the NLB is a kind of a newer option",
    "start": "1392990",
    "end": "1398000"
  },
  {
    "start": "1394000",
    "end": "1523000"
  },
  {
    "text": "it's been made available ever since kubernetes 1.9 but it's still an alpha feature so it's not yet kind of",
    "start": "1398000",
    "end": "1404510"
  },
  {
    "text": "endorsed as a recommended production service to use the way to provision in",
    "start": "1404510",
    "end": "1410330"
  },
  {
    "text": "NLB rather than an lb is done by a simple annotation I think the big benefit of that is one we will keep",
    "start": "1410330",
    "end": "1418250"
  },
  {
    "text": "adding more and more features into that which will allow you to use and take",
    "start": "1418250",
    "end": "1423889"
  },
  {
    "text": "advantage of the enemy's advantages over ELB and then the other thing is that it",
    "start": "1423889",
    "end": "1429950"
  },
  {
    "text": "will support other more refined kubernetes options one of such would be",
    "start": "1429950",
    "end": "1435500"
  },
  {
    "text": "the external traffic policy so basically external policy allows you to override this",
    "start": "1435500",
    "end": "1443410"
  },
  {
    "text": "forwarding that I've talked about earlier that's happening between nodes it means only the specific node that",
    "start": "1443410",
    "end": "1450670"
  },
  {
    "text": "will get hit by the request will get routed that requests so if you have parts running on that node good if you",
    "start": "1450670",
    "end": "1457840"
  },
  {
    "text": "don't have parts running of that on that node that node would fail the NLB health check and would get you registered from",
    "start": "1457840",
    "end": "1464650"
  },
  {
    "text": "the target group that it actually uses to route your requests so the bottom",
    "start": "1464650",
    "end": "1470290"
  },
  {
    "text": "line of it is that if you're using an NLB only nodes that are having active",
    "start": "1470290",
    "end": "1476710"
  },
  {
    "text": "pods that belong to that service are part of that game or part of that backends that that enemy can forward",
    "start": "1476710",
    "end": "1483220"
  },
  {
    "text": "traffic to now what that means though is that that could lead to an uneven distribution of traffic because if you",
    "start": "1483220",
    "end": "1489640"
  },
  {
    "text": "have two nodes that have running pods for that service one has a single part and the other has four pods the traffic",
    "start": "1489640",
    "end": "1496660"
  },
  {
    "text": "will go 50/50 between those two nodes but that obviously would lead to an uneven balance between the pods getting",
    "start": "1496660",
    "end": "1502660"
  },
  {
    "text": "the traffic so a way to kind of override that would be to either use a daemon set to make sure that you only have one part",
    "start": "1502660",
    "end": "1510250"
  },
  {
    "text": "running for each of your applications on each of your nodes and then or use anti",
    "start": "1510250",
    "end": "1515800"
  },
  {
    "text": "affinity rules that means that two pods of the same type cannot run or cannot coexist on the same node there is a",
    "start": "1515800",
    "end": "1525250"
  },
  {
    "start": "1523000",
    "end": "1557000"
  },
  {
    "text": "fourth type that's not being I think as well known as the rest of three which is",
    "start": "1525250",
    "end": "1530650"
  },
  {
    "text": "called external named external name is actually a service that creates a DNS entry and it returns a cname that will",
    "start": "1530650",
    "end": "1539470"
  },
  {
    "text": "correspond to whatever dns resolution or whatever dns name you specified on creating that service so there is no",
    "start": "1539470",
    "end": "1547360"
  },
  {
    "text": "routing or forwarding or proxying that takes place when it comes to this it's just a simple DNS cname resolution",
    "start": "1547360",
    "end": "1556860"
  },
  {
    "start": "1557000",
    "end": "1740000"
  },
  {
    "text": "now another construct that customers used to run or to in to have provision",
    "start": "1558800",
    "end": "1565430"
  },
  {
    "text": "incoming traffic onto your kubernetes cluster is called an ingress so an ingress is you can look at it like an",
    "start": "1565430",
    "end": "1572510"
  },
  {
    "text": "entry point to your cluster which can receive l7 traffic and then route that traffic and propagate that to the",
    "start": "1572510",
    "end": "1578960"
  },
  {
    "text": "multiple services that you have based on things like host-based or path based",
    "start": "1578960",
    "end": "1584360"
  },
  {
    "text": "routing now there are multiple implementations of ingress ingress azan",
    "start": "1584360",
    "end": "1591740"
  },
  {
    "text": "kubernetes on AWS including nginx and f5 zalando has an implementation there is",
    "start": "1591740",
    "end": "1599270"
  },
  {
    "text": "an implementation that we kind of took ownership on which is called the alb",
    "start": "1599270",
    "end": "1604340"
  },
  {
    "text": "ingress controller so the alb ingress controller is a component that",
    "start": "1604340",
    "end": "1610220"
  },
  {
    "text": "provisions in alb in order to implement an ingress resource that you guys define so when you're creating that ingress",
    "start": "1610220",
    "end": "1617150"
  },
  {
    "text": "resource and alb gets provisioned a target group and listeners are getting",
    "start": "1617150",
    "end": "1622490"
  },
  {
    "text": "created for each of those services that you need to route traffic to and that in turn allows you to use things like host",
    "start": "1622490",
    "end": "1629090"
  },
  {
    "text": "base or path based routing in order to determine which sort of traffic will route to which of the services now we",
    "start": "1629090",
    "end": "1636530"
  },
  {
    "text": "also enable the new operation proxying mode which is called an IP mode that",
    "start": "1636530",
    "end": "1643010"
  },
  {
    "text": "allows you to not only forward the requests back to the services but also override the node port mode that I've",
    "start": "1643010",
    "end": "1651050"
  },
  {
    "text": "talked about earlier and go straight to pods so remember in communities each part has its own IP address so those",
    "start": "1651050",
    "end": "1658550"
  },
  {
    "text": "pots can actually be directly configured as the backends in that target group",
    "start": "1658550",
    "end": "1664160"
  },
  {
    "text": "that gets created and this leads to further efficiency routing your request because you don't get to do you don't",
    "start": "1664160",
    "end": "1670160"
  },
  {
    "text": "need to do that proxying through a node port anymore but can just go straight to pots so that's",
    "start": "1670160",
    "end": "1675890"
  },
  {
    "text": "kind of a new feature that the team has developed over the last few weeks we also made the alb ingress controller in",
    "start": "1675890",
    "end": "1682820"
  },
  {
    "text": "its new form so it's a 1.0 version of it which means it is now much more stable",
    "start": "1682820",
    "end": "1688610"
  },
  {
    "text": "much more performant and also is officially supported by either us so if you guys are using a lb",
    "start": "1688610",
    "end": "1694670"
  },
  {
    "text": "controller ingress controller you can actually open support tickets to us have",
    "start": "1694670",
    "end": "1700040"
  },
  {
    "text": "our guidance and training and how to use it it's going to be part of our official docs for kubernetes and so this is one",
    "start": "1700040",
    "end": "1707360"
  },
  {
    "text": "kind of improved option that we offer for you to implement your ingress resources so a recap of what we have so",
    "start": "1707360",
    "end": "1715940"
  },
  {
    "text": "far so we have a cluster we have a managed control plane we have pods that",
    "start": "1715940",
    "end": "1721220"
  },
  {
    "text": "are running in that cluster with multiple services that can have different types in different forms and",
    "start": "1721220",
    "end": "1726610"
  },
  {
    "text": "then what we now want to do is talk about security because we want to make sure that pods in the cluster can be",
    "start": "1726610",
    "end": "1734480"
  },
  {
    "text": "accessed in the secured way and can access other services in a secure way - so when it comes to runtime security",
    "start": "1734480",
    "end": "1742400"
  },
  {
    "start": "1740000",
    "end": "2413000"
  },
  {
    "text": "this is kind of a broad topic that I can spend like two hours easy just talking",
    "start": "1742400",
    "end": "1747650"
  },
  {
    "text": "about that so I won't be able to cover all the aspects in this talk but one thing I wanted to highlight on security",
    "start": "1747650",
    "end": "1754540"
  },
  {
    "text": "that's something that a lot of customers I spoke to this year asked is like we",
    "start": "1754540",
    "end": "1759710"
  },
  {
    "text": "want to be able to access other AWS services with our applications because we're part of you know running on AWS so",
    "start": "1759710",
    "end": "1765770"
  },
  {
    "text": "we leverage things like DynamoDB we leverage things like s3 or our EMR or",
    "start": "1765770",
    "end": "1770930"
  },
  {
    "text": "any other AWS service and we want our pod to have those fine-grained permissions to be able to use that the",
    "start": "1770930",
    "end": "1777770"
  },
  {
    "text": "same way like you get with ECS or Fargate we want that to be available for pods on kubernetes and previously or",
    "start": "1777770",
    "end": "1786890"
  },
  {
    "text": "even currently we don't have any native solution to provide iam rules for pods",
    "start": "1786890",
    "end": "1792620"
  },
  {
    "text": "so first I want to go over what we do have today or what the industry and the",
    "start": "1792620",
    "end": "1797840"
  },
  {
    "text": "community has which are options that you can use here and today to provide",
    "start": "1797840",
    "end": "1803920"
  },
  {
    "text": "granular permissions using I am - you're running pods",
    "start": "1803920",
    "end": "1809210"
  },
  {
    "text": "so basically there are four I think projects that kind of stand out in terms of how you can do that and they all have",
    "start": "1809210",
    "end": "1816170"
  },
  {
    "text": "different characteristics and different trade-offs cube - I am is actually I think the oldest one it's pretty mature",
    "start": "1816170",
    "end": "1824150"
  },
  {
    "text": "it's being used in production by several companies including zalando Atlassian and others",
    "start": "1824150",
    "end": "1831020"
  },
  {
    "text": "one thing about qim though is that it's running as a daemon set on each of your cluster nodes and that means that each",
    "start": "1831020",
    "end": "1838820"
  },
  {
    "text": "of your cluster nodes needs to have an assumed role permission to assume any",
    "start": "1838820",
    "end": "1844520"
  },
  {
    "text": "role that it needs to which your pod may require and that kind of leads to a",
    "start": "1844520",
    "end": "1849860"
  },
  {
    "text": "security issue in which pods that are running before cube - I am initializers",
    "start": "1849860",
    "end": "1857539"
  },
  {
    "text": "could actually get underlying access now the way that that works is those",
    "start": "1857539",
    "end": "1863120"
  },
  {
    "text": "solutions especially cube - I am they proxy every request you make to the ec2",
    "start": "1863120",
    "end": "1869120"
  },
  {
    "text": "metadata ape endpoint and they provide those credentials themselves so they",
    "start": "1869120",
    "end": "1875299"
  },
  {
    "text": "actually make the request fetch the credentials and then hand them off to the part that needs those",
    "start": "1875299",
    "end": "1881030"
  },
  {
    "text": "credentials that's a fair way to do that but there are better ways so for example I am for",
    "start": "1881030",
    "end": "1887480"
  },
  {
    "text": "cube is running as a specific deployment that's running in a separate node not",
    "start": "1887480",
    "end": "1893240"
  },
  {
    "text": "the nodes that you guys are running your workers on but other nodes so only that particular set of nodes has those",
    "start": "1893240",
    "end": "1900650"
  },
  {
    "text": "assumed role permissions so if some some pod in your system managed to breach the",
    "start": "1900650",
    "end": "1906409"
  },
  {
    "text": "container of security boundaries it would still not get any underlying security access to the ec2 metadata API",
    "start": "1906409",
    "end": "1913840"
  },
  {
    "text": "through the assumed role that got installed because there is none the way",
    "start": "1913840",
    "end": "1919429"
  },
  {
    "text": "that it works is that all the requests are getting routed by an IP tables manipulation to that particular server",
    "start": "1919429",
    "end": "1926630"
  },
  {
    "text": "that runs elsewhere so only that service is the one that makes the calls to the ec2 metadata so that kind of reduces",
    "start": "1926630",
    "end": "1934190"
  },
  {
    "text": "significantly the blast radius that you have for I am for cube the K I am one is",
    "start": "1934190",
    "end": "1941480"
  },
  {
    "text": "taking a different approach to it actually has agents that are running on",
    "start": "1941480",
    "end": "1946549"
  },
  {
    "text": "each of the nodes but those are not the ones that are fetching the credentials so there is a separation between a",
    "start": "1946549",
    "end": "1953330"
  },
  {
    "text": "server that runs only one instance which is the one that has all the assumed role",
    "start": "1953330",
    "end": "1958640"
  },
  {
    "text": "ownerships is the one that fetches the credentials and then the agents that are actually",
    "start": "1958640",
    "end": "1964340"
  },
  {
    "text": "only proxying the requests and getting those credentials from them that that one single server so again those",
    "start": "1964340",
    "end": "1970879"
  },
  {
    "text": "solutions take different approaches as to how to mitigate different security issues there also could be race",
    "start": "1970879",
    "end": "1978110"
  },
  {
    "text": "conditions in those solutions which leads to again in cube - I am that can actually lead to a pod getting",
    "start": "1978110",
    "end": "1984440"
  },
  {
    "text": "underlying access to credentials it doesn't need to but in other cases it could just lead to kind of a race in",
    "start": "1984440",
    "end": "1991249"
  },
  {
    "text": "which your pod got scheduled before cube before for example ki a.m. which means it just doesn't get any access it will",
    "start": "1991249",
    "end": "1998419"
  },
  {
    "text": "make a request to get the credentials but there is nobody to answer that yet because it hasn't been initialized so it",
    "start": "1998419",
    "end": "2005110"
  },
  {
    "text": "will result in I can't fetch credential credentials for certain period of time upon initialization the force solution",
    "start": "2005110",
    "end": "2011830"
  },
  {
    "text": "cube AWS I am controller actually takes an entire different approach to that that's actually a single instance that",
    "start": "2011830",
    "end": "2019690"
  },
  {
    "text": "only makes a request to fetch the credentials but makes them available to pods by injecting them as mounted",
    "start": "2019690",
    "end": "2027249"
  },
  {
    "text": "secrets so that kind of relies on the trait that means that every time a pod",
    "start": "2027249",
    "end": "2033249"
  },
  {
    "text": "is launching it has to have the mounted secrets available to him before they",
    "start": "2033249",
    "end": "2038259"
  },
  {
    "text": "start running which means that by default every time you have a pod running you will have that secret",
    "start": "2038259",
    "end": "2044950"
  },
  {
    "text": "available to you so you will have the credential that you have so there are no race conditions in that but there are",
    "start": "2044950",
    "end": "2050589"
  },
  {
    "text": "other trade-offs so for example production experience we have seen significant or at least some production",
    "start": "2050589",
    "end": "2058210"
  },
  {
    "text": "experience with cube - I am and slightly less than that with K I am but none at least not that we have heard on using I",
    "start": "2058210",
    "end": "2065169"
  },
  {
    "text": "am for cube or the cube AWS I am controller so those are not has not been",
    "start": "2065169",
    "end": "2070358"
  },
  {
    "text": "tested yet at scale and real production scenarios one other thing to call out",
    "start": "2070359",
    "end": "2076599"
  },
  {
    "text": "and that's kind of a benefit for I am for cube it's the only construct that uses the service account as a way to",
    "start": "2076599",
    "end": "2084520"
  },
  {
    "text": "inject the credentials to pods so when you're annotating your when you're",
    "start": "2084520",
    "end": "2090339"
  },
  {
    "text": "asking for your credentials in most cases in most of Solutions you will get that by annotating the pod with whatever I am",
    "start": "2090339",
    "end": "2097780"
  },
  {
    "text": "role you wanted to have whereas with cube was I am for cube you actually annotate that on a service account level",
    "start": "2097780",
    "end": "2104500"
  },
  {
    "text": "now a service account is actually kind of a construct or an abstraction that",
    "start": "2104500",
    "end": "2109750"
  },
  {
    "text": "provide identity to running pots so you can have multiple pots that are running under the same service account and will",
    "start": "2109750",
    "end": "2115950"
  },
  {
    "text": "enjoy the same credentials being offered so I think as a design decision this is a really good one that we like now",
    "start": "2115950",
    "end": "2124450"
  },
  {
    "text": "having said all of that and and by the way if you guys wanted more details on comparing those solutions on the top",
    "start": "2124450",
    "end": "2130870"
  },
  {
    "text": "left corner there is a QR code that if you scan will lead you to a kind of an article that will give you a table of",
    "start": "2130870",
    "end": "2137260"
  },
  {
    "text": "comparison that shows the different trade-offs and design decisions between those for now kubernetes itself is",
    "start": "2137260",
    "end": "2145720"
  },
  {
    "text": "working on providing native support for the api server for what we call Oh IDC",
    "start": "2145720",
    "end": "2151870"
  },
  {
    "text": "or open ID Connect now we have our own proposal on how to provide native I am",
    "start": "2151870",
    "end": "2158260"
  },
  {
    "text": "support for pods and we're going to implement that next year and we would love to rely upon having the ability to",
    "start": "2158260",
    "end": "2165160"
  },
  {
    "text": "have oh I DC support on the API server as a construct for us to use on that solution so there will be more to come",
    "start": "2165160",
    "end": "2172510"
  },
  {
    "text": "on that going forward but for now those four solutions are available to you and",
    "start": "2172510",
    "end": "2178480"
  },
  {
    "text": "you can choose whatever of them seems more right for your use case based on those criterias that I defined one more",
    "start": "2178480",
    "end": "2187930"
  },
  {
    "text": "thing I wanted to kind of highlight in terms of the data plane so we talked a lot about the control plane we talked",
    "start": "2187930",
    "end": "2193450"
  },
  {
    "text": "about about how you can configure networking how you can configure security monitoring logging and tracing",
    "start": "2193450",
    "end": "2202120"
  },
  {
    "text": "so for monitoring I won't be able to cover that much but we have a lot of",
    "start": "2202120",
    "end": "2207520"
  },
  {
    "text": "tools that are being widely used in the communities community and I think the two that stand out are Prometheus and",
    "start": "2207520",
    "end": "2213310"
  },
  {
    "text": "Griffin ax so those tools should be available for you to use with dks2 if you guys or for those of you who",
    "start": "2213310",
    "end": "2220150"
  },
  {
    "text": "doesn't know eks is a 100% kubernetes upstream conformance service which means",
    "start": "2220150",
    "end": "2226420"
  },
  {
    "text": "we actually go through the kubernetes official conformance testing on every version that we support",
    "start": "2226420",
    "end": "2231450"
  },
  {
    "text": "we've done that with one point nine and one point ten and very soon with 111",
    "start": "2231450",
    "end": "2237230"
  },
  {
    "text": "with what that means is he should be able to run pretty much any tool from",
    "start": "2237230",
    "end": "2242760"
  },
  {
    "text": "the ecosystem that supports monitoring logging security should be able to run on e KS so Prometheus and Griffin are",
    "start": "2242760",
    "end": "2250650"
  },
  {
    "text": "our two that would kind of stand out if I had the time to show you how to provision that I would but I think there",
    "start": "2250650",
    "end": "2256079"
  },
  {
    "text": "were maybe other sessions this week that kind of will maybe dive deeper into that what I wanted to show is about logging",
    "start": "2256079",
    "end": "2262530"
  },
  {
    "text": "I've had a few customers recently that asked like what is your recommended approach on making those kubernetes logs",
    "start": "2262530",
    "end": "2269970"
  },
  {
    "text": "those application logs that we have running on multiple pods available for us with eks and on AWS so",
    "start": "2269970",
    "end": "2277079"
  },
  {
    "text": "again we're not opinionated as to which solution you guys want to use you can use commercial solutions like Splunk or",
    "start": "2277079",
    "end": "2284570"
  },
  {
    "text": "open source solutions one pattern don't that kind of has stood out as a lot of",
    "start": "2284570",
    "end": "2290099"
  },
  {
    "text": "customers are using that is the efk stack efk stands for elasticsearch flew",
    "start": "2290099",
    "end": "2295829"
  },
  {
    "text": "and D and Cabana so fluently is actually a very widely known and popular",
    "start": "2295829",
    "end": "2301200"
  },
  {
    "text": "component for collecting your data it has a lot of plugins that you can use to",
    "start": "2301200",
    "end": "2307859"
  },
  {
    "text": "stream that data straight to whatever centralized mechanism you want to have",
    "start": "2307859",
    "end": "2312900"
  },
  {
    "text": "including cloud watch logs elastic search and many others elastic search",
    "start": "2312900",
    "end": "2318030"
  },
  {
    "text": "itself is a very commonly used and distributed analytics engine that you can use to kind of aggregate all of your",
    "start": "2318030",
    "end": "2324810"
  },
  {
    "text": "logs and then analyze them and then Cabana is a really good way to visualize all of that so a typical workflow on",
    "start": "2324810",
    "end": "2333329"
  },
  {
    "text": "your eks cluster or on your kubernetes cluster would be having the flu in de run as a daemon set on each of your",
    "start": "2333329",
    "end": "2340020"
  },
  {
    "text": "nodes and then sending and here we have two options one option is you could use",
    "start": "2340020",
    "end": "2346230"
  },
  {
    "text": "cloud watch logs and flu indeed does have a cloud watch long locks plug-in so",
    "start": "2346230",
    "end": "2351780"
  },
  {
    "text": "you can forward the request to clever clogs and then cloud load logs by itself have a subscription that allows you to",
    "start": "2351780",
    "end": "2359670"
  },
  {
    "text": "forward those long straight to a managed elastics of service if you're running one on AWS so",
    "start": "2359670",
    "end": "2367000"
  },
  {
    "text": "if you're running a managed elasticsearch service that could be like a good way for you to provision that",
    "start": "2367000",
    "end": "2372329"
  },
  {
    "text": "assuming that you wanted the benefits of getting cloud watch logs in the middle in terms of being able to do some",
    "start": "2372329",
    "end": "2378220"
  },
  {
    "text": "filtering searches before you kind of make the final transition to elastic",
    "start": "2378220",
    "end": "2384160"
  },
  {
    "text": "search so it's kind of like the first line of filtering that you get when when",
    "start": "2384160",
    "end": "2390430"
  },
  {
    "text": "you're actually provisioning that flow of logs another way would be to just avoid cloud",
    "start": "2390430",
    "end": "2395650"
  },
  {
    "text": "watch logs and just use the flu and D connector plug-in to elasticsearch itself in which you can use any elastic",
    "start": "2395650",
    "end": "2402640"
  },
  {
    "text": "search cluster that you want and have that be visualized with cabaña whether it's a managed service or elsewhere so",
    "start": "2402640",
    "end": "2415530"
  },
  {
    "text": "these are just a few of the related breakouts that we have this week that could provide some further information",
    "start": "2415530",
    "end": "2422079"
  },
  {
    "text": "from all of these I would actually call out the one on Friday that's a deep dive on Amazon eks there",
    "start": "2422079",
    "end": "2429220"
  },
  {
    "text": "has been a session yesterday but for those of you who have not attended that and you want to do get that there is",
    "start": "2429220",
    "end": "2435700"
  },
  {
    "text": "another session that's happening on Friday it will basically cover some of the other aspects of eks like how do you",
    "start": "2435700",
    "end": "2442750"
  },
  {
    "text": "provision your worker nodes how do you provision your a.m. eyes and other things as well as well as kind of touch",
    "start": "2442750",
    "end": "2448299"
  },
  {
    "text": "upon the new items that we have released recently so with that I want to hand it",
    "start": "2448299",
    "end": "2455500"
  },
  {
    "text": "over to Carl and thank you so much and enjoy the rest of presentation",
    "start": "2455500",
    "end": "2461880"
  },
  {
    "text": "thanks and Eve let's see it's not",
    "start": "2466099",
    "end": "2471599"
  },
  {
    "text": "working there we go Oh too many slides so I'm gonna take a step up a little bit",
    "start": "2471599",
    "end": "2478319"
  },
  {
    "start": "2473000",
    "end": "2518000"
  },
  {
    "text": "and actually give like our perspective on our transition to kubernetes in eks and I'm gonna specifically talk a little",
    "start": "2478319",
    "end": "2483990"
  },
  {
    "text": "bit about vanilla kubernetes versus on ec2 or wherever versus you Cass and and",
    "start": "2483990",
    "end": "2489869"
  },
  {
    "text": "our decisions around that so I'll take a step to just step back to kind of give some context in history on snap as a",
    "start": "2489869",
    "end": "2495240"
  },
  {
    "text": "company there get to make the rest of my talk makes sense so we were actually founded in 2011 and really starting from",
    "start": "2495240",
    "end": "2501690"
  },
  {
    "text": "2012 on we had really rapid growth in employees and software engineers and so",
    "start": "2501690",
    "end": "2506759"
  },
  {
    "text": "a lot of the problems that I'll talk about that we went through are really organizational problems or like fast growth problems so for customers that",
    "start": "2506759",
    "end": "2513059"
  },
  {
    "text": "are going through similar transitions some of that might be relevant so when",
    "start": "2513059",
    "end": "2519299"
  },
  {
    "start": "2518000",
    "end": "2700000"
  },
  {
    "text": "we think about kind of I guess any technical decision but certainly infrastructure decisions we generally start with first principles and each",
    "start": "2519299",
    "end": "2525990"
  },
  {
    "text": "particular example we make different trade-offs but these are at a high level probably to some of the top five things",
    "start": "2525990",
    "end": "2531869"
  },
  {
    "text": "that we think about for any project you know I mentioned our rapid growth there was a time where flexibility was the",
    "start": "2531869",
    "end": "2538380"
  },
  {
    "text": "most important thing all the time even sacrificing that of a security in some situations right we wanted development team to be able to move as quickly as",
    "start": "2538380",
    "end": "2544410"
  },
  {
    "text": "possible not have to worry about provisioning infrastructure just building on top of what we already had",
    "start": "2544410",
    "end": "2549859"
  },
  {
    "text": "even today that's a really important goal for us we've actually gone through some organizational transitions where",
    "start": "2549859",
    "end": "2557069"
  },
  {
    "text": "instead of having you know a bunch of independent teams that work separately we've we kind of have some central",
    "start": "2557069",
    "end": "2562140"
  },
  {
    "text": "platform teams that provide higher level services to internal development teams right now they're probably about maybe a",
    "start": "2562140",
    "end": "2568200"
  },
  {
    "text": "fifth or so of our software engineers work on teams like that that supports both infrastructure as well as mobile",
    "start": "2568200",
    "end": "2573569"
  },
  {
    "text": "which is actually most of our energy goes into security I kind of said it what sometimes wasn't important well",
    "start": "2573569",
    "end": "2579210"
  },
  {
    "text": "these days it's it's probably always one of the most important concerns and I think we look at project products like",
    "start": "2579210",
    "end": "2585599"
  },
  {
    "text": "kubernetes and eks with the security lens right we were initially a little worried about using containers versus",
    "start": "2585599",
    "end": "2591269"
  },
  {
    "text": "fully virtualized instances but obviously like a lot of what you need was talking about like fool I am support for pods would be an enormous",
    "start": "2591269",
    "end": "2597600"
  },
  {
    "text": "thing that we would love to start using once it comes out little anecdote so I",
    "start": "2597600",
    "end": "2602850"
  },
  {
    "text": "started at snap actually came from AWS in like summer 2014 and I kind of my",
    "start": "2602850",
    "end": "2609090"
  },
  {
    "text": "style is just observed for a while and see what the land the land looks like and then make recommendations so about",
    "start": "2609090",
    "end": "2614640"
  },
  {
    "text": "two weeks in I went to my boss and I said availability is the most important thing we have to work on we just had",
    "start": "2614640",
    "end": "2619770"
  },
  {
    "text": "five you know two minute outages in the past week that were totally avoidable and he's you know this was 2014 he said",
    "start": "2619770",
    "end": "2626220"
  },
  {
    "text": "yeah I agree it's important but on my list it's probably number 20 and the reason it was number 20 is there were so many so many product changes so many",
    "start": "2626220",
    "end": "2632730"
  },
  {
    "text": "things we wanted to do with with the app itself that were more important that that wasn't initially that important but",
    "start": "2632730",
    "end": "2637830"
  },
  {
    "text": "over the years our expectations from our users have gone ly gone up and and performance has I think always been",
    "start": "2637830",
    "end": "2644250"
  },
  {
    "text": "important to us but performance availability are becoming you know almost as important and security and our perspective as with employee growth",
    "start": "2644250",
    "end": "2652440"
  },
  {
    "text": "comes infrastructure cost growth as well initially efficient infrastructure wasn't higher in our list but as the",
    "start": "2652440",
    "end": "2659190"
  },
  {
    "text": "absolute amount of money we spent on infrastructure has grown that's become more and more important we actually have several teams now dedicated to measuring",
    "start": "2659190",
    "end": "2665100"
  },
  {
    "text": "and thinking about cost reduction kubernetes is a big part of that for us like where I'll talk about some of our specific transitions but obviously",
    "start": "2665100",
    "end": "2671610"
  },
  {
    "text": "getting the most out of an instance scheduling kind of different kinds of tasks on instances is a good way to",
    "start": "2671610",
    "end": "2677910"
  },
  {
    "text": "improve efficiency and finally you know we have a lot of younger engineers that",
    "start": "2677910",
    "end": "2683220"
  },
  {
    "text": "are that haven't had to kind of operate their an infrastructure so a lot of our teams are really looking for simple",
    "start": "2683220",
    "end": "2688500"
  },
  {
    "text": "operational solutions and for teams like that where you know operational insight isn't going to give them much most",
    "start": "2688500",
    "end": "2694260"
  },
  {
    "text": "useful information about how their service works we're happy to provide kind of higher level operational paradigms for them okay so here's the",
    "start": "2694260",
    "end": "2702450"
  },
  {
    "start": "2700000",
    "end": "2772000"
  },
  {
    "text": "history lesson so I'll talk a little bit about the way things we're a couple years ago and then I'll transition to how we're changing have we've changed a",
    "start": "2702450",
    "end": "2708720"
  },
  {
    "text": "lot won't continue to change things so I think a lot of small companies that grow really rapidly take this approach we",
    "start": "2708720",
    "end": "2714480"
  },
  {
    "text": "operated Wiimote with most our infrastructure in a single large monolithic application actually a couple",
    "start": "2714480",
    "end": "2719580"
  },
  {
    "text": "large monolithic applications and I mean monolithic I guess in two senses monolithic and a runtime sense like",
    "start": "2719580",
    "end": "2726000"
  },
  {
    "text": "literally we have we used to have an application that was like a 600 megabyte waar and and with hundreds of api's and actually",
    "start": "2726000",
    "end": "2732930"
  },
  {
    "text": "contained inside that wire but I also mean from a code based perspective and obviously for most companies that",
    "start": "2732930",
    "end": "2739380"
  },
  {
    "text": "continue to grow this becomes an impediment after a certain amount of time we were starting to see problems a couple years ago where projects most of",
    "start": "2739380",
    "end": "2746760"
  },
  {
    "text": "our projects involve a mobile mobile work as well as infrastructure work it should pretty much never be the case",
    "start": "2746760",
    "end": "2751920"
  },
  {
    "text": "that the infrastructure work is the long pole in any projects because mobile engineering if people haven't worked on as much harder than infrastructure it's",
    "start": "2751920",
    "end": "2758400"
  },
  {
    "text": "much more like traditional shrink-wrap software where if you make a mistake you're stuck with it for a long period of time but we were starting to see",
    "start": "2758400",
    "end": "2764370"
  },
  {
    "text": "projects where that was not true that the infrastructure was actually the long pole so that was a good signal for us to",
    "start": "2764370",
    "end": "2769530"
  },
  {
    "text": "start thinking about how to change from a large monolith secondly as we grew we",
    "start": "2769530",
    "end": "2776310"
  },
  {
    "start": "2772000",
    "end": "2836000"
  },
  {
    "text": "obviously didn't grow by just single teams getting larger and larger we subdivided responsibility and ownership",
    "start": "2776310",
    "end": "2781580"
  },
  {
    "text": "that created a secondary problem which was often teams had to collaborate in order to ship something and we evolved",
    "start": "2781580",
    "end": "2789360"
  },
  {
    "text": "sort of defense mechanisms over the years where because a monolith it has a lot of strengths but one of the big",
    "start": "2789360",
    "end": "2795000"
  },
  {
    "text": "weaknesses is if one person makes a mistake it can affect everyone we evolved kind of a set of processes around kind of gatekeeping into code",
    "start": "2795000",
    "end": "2802260"
  },
  {
    "text": "changes into that model if for things that were deemed high risk or potentially large and scoped and that",
    "start": "2802260",
    "end": "2808200"
  },
  {
    "text": "was a good strategy but it create a lot of tension right no one wants to be on the team whose job it is is to police",
    "start": "2808200",
    "end": "2813540"
  },
  {
    "text": "what other development teams are doing symmetrically no one wants to kind of ask for permission to do something we'd",
    "start": "2813540",
    "end": "2818670"
  },
  {
    "text": "rather just have teams work as independently as possible and maybe another element to that is we started",
    "start": "2818670",
    "end": "2824070"
  },
  {
    "text": "having geographically distributed teams that had sort of different cultures and different histories about how they",
    "start": "2824070",
    "end": "2829140"
  },
  {
    "text": "approached engineering and they were probably some of the most vocal teams in terms of not wanting to ask for permission to do certain things and",
    "start": "2829140",
    "end": "2836300"
  },
  {
    "start": "2836000",
    "end": "2884000"
  },
  {
    "text": "finally in terms of problems although it was possible and we had a fair amount of",
    "start": "2836300",
    "end": "2842210"
  },
  {
    "text": "individual services that were regionalised it wasn't possible to make coherent application wide",
    "start": "2842210",
    "end": "2848190"
  },
  {
    "text": "regionalization decisions everything was kind of siloed and for us during this period i 2015/2016 was really when user",
    "start": "2848190",
    "end": "2855060"
  },
  {
    "text": "growth that snapchat was really at the highest growth rate ever and that was primarily occurring outside of the United States most of our infrastructure",
    "start": "2855060",
    "end": "2861150"
  },
  {
    "text": "was actually in the at that point so regionalization was a big like thing we knew we needed to work",
    "start": "2861150",
    "end": "2866400"
  },
  {
    "text": "on rather than like take our big monolithic thing try to regionalize that we decided to start to decompose that region and regionalize the component",
    "start": "2866400",
    "end": "2872610"
  },
  {
    "text": "parts and obviously performance is the number one reason why we would regionalize obviously availability is a",
    "start": "2872610",
    "end": "2877800"
  },
  {
    "text": "secondary reason but for us it's really about delivering media and delivering metadata our clients as quickly as possible so this slides probably a",
    "start": "2877800",
    "end": "2886440"
  },
  {
    "start": "2884000",
    "end": "2951000"
  },
  {
    "text": "little boring I mean obviously SOA is a good solution to the five or six problems I just identified you know it",
    "start": "2886440",
    "end": "2892560"
  },
  {
    "text": "really does solve a lot of those things or at least help dramatically with a lot of those things letting a team or a team",
    "start": "2892560",
    "end": "2899190"
  },
  {
    "text": "own a set of services that do one thing but do that one thing very well you're just gonna end up with better software more efficient software that has fewer",
    "start": "2899190",
    "end": "2906390"
  },
  {
    "text": "bugs security was a big driver though part of our Montt we had built a lot of infrastructure around the monolithic",
    "start": "2906390",
    "end": "2912750"
  },
  {
    "text": "application to compartmentalize and not allow any any random developer to get access to things but that created some",
    "start": "2912750",
    "end": "2918810"
  },
  {
    "text": "tension it basically meant that when you did need access to certain things it was harder to get access obviously a set of",
    "start": "2918810",
    "end": "2924990"
  },
  {
    "text": "services that have different databases and different sets of access controls is a much cleaner way to do that and again",
    "start": "2924990",
    "end": "2931320"
  },
  {
    "text": "SOA I'm a big fan of having your organization org structure map closely",
    "start": "2931320",
    "end": "2936660"
  },
  {
    "text": "to what your application SOA structure looks like probably not always right to have it perfectly match but we we",
    "start": "2936660",
    "end": "2943440"
  },
  {
    "text": "thought that moving to SOI would give us a better like growth trajectory when it cut when it came to launching new",
    "start": "2943440",
    "end": "2948570"
  },
  {
    "text": "services and launching new products so those that's kind of the obvious slide I",
    "start": "2948570",
    "end": "2953670"
  },
  {
    "start": "2951000",
    "end": "2988000"
  },
  {
    "text": "think the the question that is like what what do we do like what's the actual path from big millions of lines of code",
    "start": "2953670",
    "end": "2960120"
  },
  {
    "text": "monolithic applications where we want to go containers I think were the obvious solution in fact it solved a different",
    "start": "2960120",
    "end": "2966000"
  },
  {
    "text": "problem we had it used to be impossible to even run and test our application on your laptop because the 600 megabyte war",
    "start": "2966000",
    "end": "2972030"
  },
  {
    "text": "is too big and pus required all these dependent services that we didn't have knocked out containers help with that too but they're not even probably close",
    "start": "2972030",
    "end": "2980130"
  },
  {
    "text": "to half the battle for a big set of services like ours right how are you gonna run those containers how you're gonna deploy them how are you gonna",
    "start": "2980130",
    "end": "2985650"
  },
  {
    "text": "manage them in an ongoing basis an efficient way is the real question so solutions like kubernetes were I",
    "start": "2985650",
    "end": "2991020"
  },
  {
    "start": "2988000",
    "end": "3154000"
  },
  {
    "text": "think obvious like it was sort of lucky I think for us in a sense that kubernetes started to get traction",
    "start": "2991020",
    "end": "2996320"
  },
  {
    "text": "around the time when we were thinking about making this transition we actually invested a lot prior to the transition",
    "start": "2996320",
    "end": "3001690"
  },
  {
    "text": "to kubernetes in a framework that was internal to snapchat that solved a lot of these same problems and actually a lot of that stuff that we built then has",
    "start": "3001690",
    "end": "3008200"
  },
  {
    "text": "actually been carried on and has run as part of our kubernetes clusters but we saw like there's so much momentum behind",
    "start": "3008200",
    "end": "3014290"
  },
  {
    "text": "kubernetes we wanted to kind of switch over to that solution because we saw it as the future but we're presented with this question like should we run it",
    "start": "3014290",
    "end": "3020410"
  },
  {
    "text": "ourselves on ec2 should we rent ourselves on other cloud providers or should we use to manage service philosophically if you if I went back to",
    "start": "3020410",
    "end": "3027220"
  },
  {
    "text": "my first slide you know we definitely have we prefer to use a managed solution",
    "start": "3027220",
    "end": "3032260"
  },
  {
    "text": "when it's only a little bit more expensive and it provides some significant value or it replaces work",
    "start": "3032260",
    "end": "3037780"
  },
  {
    "text": "that we could do ourselves but that is not going to be valuing for us so to us a lot of problems of kubernetes solves",
    "start": "3037780",
    "end": "3043030"
  },
  {
    "text": "fit that definition right yes we could figure out our own way to like carve up instances into smaller units and manage",
    "start": "3043030",
    "end": "3049510"
  },
  {
    "text": "where pods are deployed or where containers are deployed but that's a solved problem right or we could come up",
    "start": "3049510",
    "end": "3055360"
  },
  {
    "text": "with our own solution about how to react to datacenter failures own failure and and scale up in another zone again",
    "start": "3055360",
    "end": "3061510"
  },
  {
    "text": "that's it's a problem that's gonna be solved at kubernetes but the question remains like eks versus vanilla",
    "start": "3061510",
    "end": "3067450"
  },
  {
    "text": "kubernetes alb ingress controller I think that you need mentioned is a good example of what we think is going to",
    "start": "3067450",
    "end": "3074380"
  },
  {
    "text": "continue to happen in the ecosystem I think Amazon will will likely continue to launch integrations with services",
    "start": "3074380",
    "end": "3080410"
  },
  {
    "text": "that we want to use and load-balancing structured storage the blob storage these are services that we generally",
    "start": "3080410",
    "end": "3086290"
  },
  {
    "text": "philosophically want to use the cloud provider offered version they're really hard to operate it's super hard to not lose data in those systems and so we",
    "start": "3086290",
    "end": "3093640"
  },
  {
    "text": "expect to see kind of this leapfrog effect where services or better integrations with database services are",
    "start": "3093640",
    "end": "3098920"
  },
  {
    "text": "launched first in the manage products like ETS and ECS and then do eventually make their way back into vanilla kubernetes but we want we want access",
    "start": "3098920",
    "end": "3106030"
  },
  {
    "text": "those things as soon as possible like load balancing is a good example we were before we I'll get into some of the real",
    "start": "3106030",
    "end": "3111850"
  },
  {
    "text": "production workloads that we have running but we were running individual like kubernetes style load balancers onto individual services or nodes just",
    "start": "3111850",
    "end": "3118990"
  },
  {
    "text": "to start out and we knew that was not going to be a viable solution where we launched products to production luckily",
    "start": "3118990",
    "end": "3124000"
  },
  {
    "text": "a lot of the load balancing features did launch in time for us should use them one other point on that",
    "start": "3124000",
    "end": "3129800"
  },
  {
    "text": "I'm we actually don't think that the slight portability differences between it's not really working okay here we go",
    "start": "3129800",
    "end": "3137360"
  },
  {
    "text": "the slight portability less portability that you would have by building on top of ETS is that big of a deal at the end",
    "start": "3137360",
    "end": "3142790"
  },
  {
    "text": "of the day I think you're gonna be more there's gonna be more inertia and switching away from something like dynamodb to a different solution than",
    "start": "3142790",
    "end": "3149030"
  },
  {
    "text": "there is when reconfiguring your containers to run in some different version of the UK s so where are we",
    "start": "3149030",
    "end": "3155210"
  },
  {
    "start": "3154000",
    "end": "3244000"
  },
  {
    "text": "right now and where are we going so we currently have about six services in production those we're gonna see",
    "start": "3155210",
    "end": "3161510"
  },
  {
    "text": "actually something like 14 or so by end of year so there's actually a lot of growth happening in the last month of the year well everything goes right next year we",
    "start": "3161510",
    "end": "3169130"
  },
  {
    "text": "we hope to have about 50 services again if teams kind of meet their commitments that they made and the end state we have",
    "start": "3169130",
    "end": "3175340"
  },
  {
    "text": "something like 1600 ap is that our mobile application talks to in the old monolith and we've migrated maybe I'm",
    "start": "3175340",
    "end": "3181760"
  },
  {
    "text": "guessing here 70 to 80 already we think we'll end up with several honey through those api's operate on different data",
    "start": "3181760",
    "end": "3187850"
  },
  {
    "text": "right so our general segmentation strategy is you know if a bunch of API is operate in the same data those should",
    "start": "3187850",
    "end": "3193280"
  },
  {
    "text": "probably be in the same ultimate service but we expect to have several hundred services mostly running on eks in",
    "start": "3193280",
    "end": "3198650"
  },
  {
    "text": "several more years and we don't have a detailed timeline but so I'm kind of being a little vague there but a lot of",
    "start": "3198650",
    "end": "3204260"
  },
  {
    "text": "things we're gonna be doing over next year and this year is focusing on making decisions about regionalization we want",
    "start": "3204260",
    "end": "3209300"
  },
  {
    "text": "to provide I think it's hard right like most teams would want a over-provision to the point where their service would",
    "start": "3209300",
    "end": "3216230"
  },
  {
    "text": "always be available in the face of any infrastructure failure and and from a business perspective that's actually not",
    "start": "3216230",
    "end": "3221450"
  },
  {
    "text": "the right thing to do right some services really do we have a concept of service tearing internally and so for",
    "start": "3221450",
    "end": "3226670"
  },
  {
    "text": "high availability services that really need to be up in all cases that probably is the right strategy but for example for a batch processing workload that you",
    "start": "3226670",
    "end": "3233150"
  },
  {
    "text": "could just delay for an hour or two if Amazon did have a zone down maybe that's not the best strategy so we want to",
    "start": "3233150",
    "end": "3238490"
  },
  {
    "text": "provide some guidance and recommendations and tooling for teams to use to make these decisions internally yes ma'am so a little a few more stats",
    "start": "3238490",
    "end": "3247510"
  },
  {
    "start": "3244000",
    "end": "3309000"
  },
  {
    "text": "we currently have about 7500 cores running various we think we have a couple ets clusters right now if you sum",
    "start": "3247510",
    "end": "3254510"
  },
  {
    "text": "up all the the average transaction rates per second for those services it's about 250,000 QPS",
    "start": "3254510",
    "end": "3260059"
  },
  {
    "text": "so we are using n lb and a lb for a lot of these services one thing that's I think worth mentioning from our",
    "start": "3260059",
    "end": "3266089"
  },
  {
    "text": "experience so far is we have ended up with relatively large instances with lots of pods packed onto them and that",
    "start": "3266089",
    "end": "3272029"
  },
  {
    "text": "worked out to be the most cost-effective way for us to run to run things I think the most interesting thing is with",
    "start": "3272029",
    "end": "3277579"
  },
  {
    "text": "kubernetes and eks this is a decision that one team or one person in your company can make who's responsible for",
    "start": "3277579",
    "end": "3283339"
  },
  {
    "text": "optimizing the clusters right so in fact the development teams that run on those clusters don't even need to know necessarily that we're making a change",
    "start": "3283339",
    "end": "3290059"
  },
  {
    "text": "to how which instances type we're using what good example that is actually c5 and m5 instances which we found to be",
    "start": "3290059",
    "end": "3297199"
  },
  {
    "text": "significantly more performant if you sort of measure it by cost and we were able to replace a bunch they know it's pretty easily actually that was a case",
    "start": "3297199",
    "end": "3303890"
  },
  {
    "text": "of using ECS by just doing that under the covers without the service one or really being involved so I'll take in",
    "start": "3303890",
    "end": "3311180"
  },
  {
    "start": "3309000",
    "end": "3396000"
  },
  {
    "text": "the last couple minutes take a step up to stack a little bit and talk a little bit about what we're doing at the application layer so I mentioned earlier",
    "start": "3311180",
    "end": "3317689"
  },
  {
    "text": "we had developed a bunch of stuff to make it easy to deploy services to physical infrastructure at snap and so",
    "start": "3317689",
    "end": "3323569"
  },
  {
    "text": "we were able to take a lot of our custom logging agents custom metric agents and just carry those over to kubernetes obviously there was a little bit of work",
    "start": "3323569",
    "end": "3329449"
  },
  {
    "text": "to repackage those but it was it was a lot easier than doing it all over again we're using envoy for sort of",
    "start": "3329449",
    "end": "3336319"
  },
  {
    "text": "application application layer proxying I think for a couple of reasons we actually have a fair amount of languages in use in production and so a proxy like",
    "start": "3336319",
    "end": "3343429"
  },
  {
    "text": "envoy allows us to have very consistent rules and guidelines around how RPC should be made between services without",
    "start": "3343429",
    "end": "3349640"
  },
  {
    "text": "having to implement libraries in each language but we also want to have kind of governance around service calls so I",
    "start": "3349640",
    "end": "3356089"
  },
  {
    "text": "mentioned tiering we don't yet really have these rules in place because we only have six services in eks but we do",
    "start": "3356089",
    "end": "3362209"
  },
  {
    "text": "want to create rules that actually prohibit or maybe more like whitelist certain API calls from service to",
    "start": "3362209",
    "end": "3367549"
  },
  {
    "text": "service so that we can kind of avoid people from taking on unplanned and SS dependencies or maybe dependencies on a",
    "start": "3367549",
    "end": "3374269"
  },
  {
    "text": "layer that that is above that the tearing that your application is in on droids gonna allow us to do that I think",
    "start": "3374269",
    "end": "3379999"
  },
  {
    "text": "another cool thing about eks is envoy I think it'll probably be around for a long time seems like there's a lot of momentum behind it as well if that turns",
    "start": "3379999",
    "end": "3386119"
  },
  {
    "text": "out to not be the solution that people are moving towards in a couple years it should be relatively easy to swap that out I'd imagine any future",
    "start": "3386119",
    "end": "3393359"
  },
  {
    "text": "would probably even provide some kind of backwards pen ability so how does this",
    "start": "3393359",
    "end": "3398730"
  },
  {
    "text": "help the company so I mentioned earlier just like packing more pods on instances has already saved us a lot of money",
    "start": "3398730",
    "end": "3403890"
  },
  {
    "text": "which is great a snapchat has a really simple business like our all of our costs are basically infrastructure and",
    "start": "3403890",
    "end": "3409230"
  },
  {
    "text": "people and so people are not really a an elastic cost in the same way the infrastructure is so this has really",
    "start": "3409230",
    "end": "3414509"
  },
  {
    "text": "been a big a big boon for the actual business secondly on goes a good example",
    "start": "3414509",
    "end": "3420420"
  },
  {
    "text": "but I think we'll see this this continue to be an advantage right kubernetes it's just there's so much momentum behind it",
    "start": "3420420",
    "end": "3427140"
  },
  {
    "text": "there's so many different products that are being built and integrated with it that when someone has a cool idea and wants to use the next next thing it",
    "start": "3427140",
    "end": "3433289"
  },
  {
    "text": "should be relatively easy to integrate that to a given service once that team likes it it's also relatively easy to",
    "start": "3433289",
    "end": "3438900"
  },
  {
    "text": "integrate it with their entire set of services we are moving or really",
    "start": "3438900",
    "end": "3443999"
  },
  {
    "text": "implementing a model that's kind of like we hope one big cluster per region one",
    "start": "3443999",
    "end": "3449489"
  },
  {
    "text": "big multi tender cluster per region and again the main reason for that is efficiency we can probably just end less money if we're using the same",
    "start": "3449489",
    "end": "3455309"
  },
  {
    "text": "infrastructure there are a couple examples where we might not do that for security reasons or kind of risk",
    "start": "3455309",
    "end": "3460739"
  },
  {
    "text": "aversion reasons but for the bulk of what we're doing but that's our plan and we've already seen actually significant",
    "start": "3460739",
    "end": "3466769"
  },
  {
    "text": "performance improvements we had teams running running kubernetes themselves on ec2 and they would port their",
    "start": "3466769",
    "end": "3472019"
  },
  {
    "text": "application really there's very little work to do over the eks and they saw immediate performance improvements a lot of that's probably like kernel tuning",
    "start": "3472019",
    "end": "3478559"
  },
  {
    "text": "and things that we could have figured out ourselves but the point is it's kind of better for Amazon or to least start with the baseline of what Amazon thinks",
    "start": "3478559",
    "end": "3484650"
  },
  {
    "text": "is a good configuration and get to give a sense we already have 10 teams so we have 6 production services but actually",
    "start": "3484650",
    "end": "3491039"
  },
  {
    "text": "10 teams already using NGS and at snap so some of those obviously haven't launched yet but they're some of the 14",
    "start": "3491039",
    "end": "3496650"
  },
  {
    "text": "they expect so our next steps are mostly maybe not so interesting for this",
    "start": "3496650",
    "end": "3503069"
  },
  {
    "start": "3498000",
    "end": "3539000"
  },
  {
    "text": "audience but with 16-yard API is to migrate each of which does require some amount of testing and porting is a lot",
    "start": "3503069",
    "end": "3508440"
  },
  {
    "text": "of work so that's what we're going to be spending the next couple years doing but again as I mentioned earlier we want to",
    "start": "3508440",
    "end": "3513539"
  },
  {
    "text": "come up with a couple of kind of canned strategies around how to regionalize applications and give that to our",
    "start": "3513539",
    "end": "3518700"
  },
  {
    "text": "internal team so they can kind of pick and choose what they think is the right thing to do okay I think that's it so a reminder",
    "start": "3518700",
    "end": "3526579"
  },
  {
    "text": "fill out your survey or you need will kill me one of those was that slide but will actually be available for a few",
    "start": "3526579",
    "end": "3531769"
  },
  {
    "text": "questions over here people have it we ran out of time but thank you everyone [Applause]",
    "start": "3531769",
    "end": "3541820"
  }
]