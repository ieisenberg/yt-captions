[
  {
    "start": "0",
    "end": "179000"
  },
  {
    "text": "[Applause] right good afternoon everyone my name is",
    "start": "170",
    "end": "5400"
  },
  {
    "text": "Manos I'm a Solutions Architect on AWS specializing in big data and analytics so today we're gonna we're gonna be",
    "start": "5400",
    "end": "12599"
  },
  {
    "text": "speaking about really about big data we're going to go through architecture part the principles and some design",
    "start": "12599",
    "end": "18270"
  },
  {
    "text": "patterns we have to cover quite quite a lot so the way the way we're going to",
    "start": "18270",
    "end": "24180"
  },
  {
    "text": "play today and I'm gonna talk to you some of the big data challenges that I",
    "start": "24180",
    "end": "29250"
  },
  {
    "text": "see and I discuss with with my customers and then I'm gonna discuss through some",
    "start": "29250",
    "end": "34770"
  },
  {
    "text": "architectural principles some things that you you need to remember when you do your your architecture and then we",
    "start": "34770",
    "end": "41430"
  },
  {
    "text": "will simplify the big data processing into smaller steps and then we're gonna",
    "start": "41430",
    "end": "46620"
  },
  {
    "text": "dive deeper into each step and I will start identifying some some tools that you you could use on those on those",
    "start": "46620",
    "end": "54510"
  },
  {
    "text": "steps then we'll combine all those steps together into some sort of reference",
    "start": "54510",
    "end": "59730"
  },
  {
    "text": "architectures and in the end we'll we'll finish with some customer examples now",
    "start": "59730",
    "end": "65460"
  },
  {
    "text": "the types of big data analytics we're going to discuss today it's going to be bad and interactive type of analytics",
    "start": "65460",
    "end": "72409"
  },
  {
    "text": "streaming analytics and some predictive analytics some sort of that are powered",
    "start": "72409",
    "end": "78479"
  },
  {
    "text": "by some sort of machine learning algorithm now because we're gonna speak",
    "start": "78479",
    "end": "84479"
  },
  {
    "text": "about big data and and the cloud and I'd like to step back a little bit and think",
    "start": "84479",
    "end": "90030"
  },
  {
    "text": "how the cloud has evolved since AWS pioneered the cloud started with",
    "start": "90030",
    "end": "95100"
  },
  {
    "text": "virtualized instances where customers were able to spin up virtual machines",
    "start": "95100",
    "end": "100170"
  },
  {
    "text": "and then you were restoring your applications and then you will start running and your code but then that",
    "start": "100170",
    "end": "106170"
  },
  {
    "text": "evolved into managed services where AWS took more responsibilities AWS then started to manage more on the",
    "start": "106170",
    "end": "114270"
  },
  {
    "text": "software stack so you didn't have anymore to upgrade lyc√©e application it was managed by AWS but still you had to",
    "start": "114270",
    "end": "121020"
  },
  {
    "text": "provision as a customer you had to provision to think about servers you had to think about memory CPU you have to",
    "start": "121020",
    "end": "126299"
  },
  {
    "text": "think about all this stuff and this where serverless paradigm comes this is where the serverless and cluster less",
    "start": "126299",
    "end": "131520"
  },
  {
    "text": "computing comes which is the evolution we're customers don't have to think anymore about about capacity instead you",
    "start": "131520",
    "end": "140130"
  },
  {
    "text": "really focus on on developing your application code and it comes to me when",
    "start": "140130",
    "end": "145170"
  },
  {
    "text": "it comes to analytics that that means you're you know focusing to develop those analytical queries that will make",
    "start": "145170",
    "end": "150270"
  },
  {
    "text": "a difference to your applications rather than focusing on upgrading frameworks",
    "start": "150270",
    "end": "155970"
  },
  {
    "text": "and upgrading the applications so today we're going to cover and we're going to",
    "start": "155970",
    "end": "161400"
  },
  {
    "text": "dive into some tools and there will be quite a lot of tools a lot of which are coming from the open source kind of",
    "start": "161400",
    "end": "169640"
  },
  {
    "text": "stack of tools and especially around the hadoop a kind of ecosystem but also I'm",
    "start": "169640",
    "end": "175530"
  },
  {
    "text": "going to describe a lot about some AWS services around the analytics so let's",
    "start": "175530",
    "end": "181560"
  },
  {
    "start": "179000",
    "end": "380000"
  },
  {
    "text": "let's start with the the challenges first so really what what the customers",
    "start": "181560",
    "end": "186630"
  },
  {
    "text": "wants to understand is 2002 Spanish when we have a problem right they want to the standard what is a reference architecture but that I can use to solve",
    "start": "186630",
    "end": "194010"
  },
  {
    "text": "my problem and what tools that architecture has what tools should I use",
    "start": "194010",
    "end": "200220"
  },
  {
    "text": "and then different tools how to bring those tools together and then why to",
    "start": "200220",
    "end": "205440"
  },
  {
    "text": "choose one tool over the other so today's session is about about about this really about understanding the",
    "start": "205440",
    "end": "211440"
  },
  {
    "text": "different options and help you choose the right tools okay before we start all",
    "start": "211440",
    "end": "218579"
  },
  {
    "text": "the tools conversations let's let's define some architectural principles so",
    "start": "218579",
    "end": "223940"
  },
  {
    "text": "we have kind of a common framework as we're going to go through those those options the first one is a general kind",
    "start": "223940",
    "end": "234930"
  },
  {
    "text": "of good practice and good architecture and principles in general not just when you design big data systems is when you",
    "start": "234930",
    "end": "241380"
  },
  {
    "text": "design a pretty much any system is to build the coupled systems really separate the concerns",
    "start": "241380",
    "end": "247350"
  },
  {
    "text": "instead of having big monolithic application separate the concerns into smaller systems and then what that",
    "start": "247350",
    "end": "253620"
  },
  {
    "text": "allows you to do it allows you to iterate on each and every subsystem and this is how you can truly build and",
    "start": "253620",
    "end": "260880"
  },
  {
    "text": "evolve over time now from a big data point of view and really what",
    "start": "260880",
    "end": "266060"
  },
  {
    "text": "means it comes breakdowns into storage processing and analysis of the data and their the the way you store the data",
    "start": "266060",
    "end": "273080"
  },
  {
    "text": "really shouldn't really mandate the way and you process the data and equally the way you process the data sitting really",
    "start": "273080",
    "end": "280220"
  },
  {
    "text": "Monday the way you analyze the data so they should be independent now the second architectural principles",
    "start": "280220",
    "end": "288080"
  },
  {
    "text": "is really use the right the right tool for the job and in order to identify what is the right tool there are a",
    "start": "288080",
    "end": "294290"
  },
  {
    "text": "couple of questions that you kind of need to ask yourself and it really comes down to the data structure and to",
    "start": "294290",
    "end": "300880"
  },
  {
    "text": "latency requirements as you might have the third one might not be an",
    "start": "300880",
    "end": "307370"
  },
  {
    "text": "architectural principle but certainly something that definitely we recommend our customers do and this is about using",
    "start": "307370",
    "end": "314169"
  },
  {
    "text": "magnet services and server less services where possible by doing that and you",
    "start": "314169",
    "end": "321110"
  },
  {
    "text": "inherit a lot a lot of engineering effort that AWS has put in place to build those services you inherit a lot",
    "start": "321110",
    "end": "327650"
  },
  {
    "text": "of best practices AWS put in place to make this those services scalable to",
    "start": "327650",
    "end": "332960"
  },
  {
    "text": "make them highly available reliable secure and you save also some admin cost",
    "start": "332960",
    "end": "339260"
  },
  {
    "text": "because you don't have to again to upgrade applications you don't have to manage service behind the scenes and the",
    "start": "339260",
    "end": "346760"
  },
  {
    "text": "other one is to prefer for when it comes to analytical systems and tuinal to to big data systems to to prefer immutable",
    "start": "346760",
    "end": "353479"
  },
  {
    "text": "data sets like a data leak and where you want to capture the state of an",
    "start": "353479",
    "end": "360200"
  },
  {
    "text": "application to load them as materialized views rather than updating records that",
    "start": "360200",
    "end": "365870"
  },
  {
    "text": "introduce complexity and finally and cost is part of any architecture so we",
    "start": "365870",
    "end": "371720"
  },
  {
    "text": "will talk today about some ways to optimize cost when you architect systems",
    "start": "371720",
    "end": "376850"
  },
  {
    "text": "analytic systems on AWS okay now let's let's move on to think",
    "start": "376850",
    "end": "386270"
  },
  {
    "start": "380000",
    "end": "430000"
  },
  {
    "text": "the simplification of big data processing so what we're going to do now we're going to go on each and every step and the steps are collection storage",
    "start": "386270",
    "end": "394610"
  },
  {
    "text": "process and consumption and you see those arrows the",
    "start": "394610",
    "end": "399889"
  },
  {
    "text": "those student shouldn't name doesn't have to be a waterfall process right so",
    "start": "399889",
    "end": "405590"
  },
  {
    "text": "storage could you could store the data then analyze the data then you cannot",
    "start": "405590",
    "end": "410889"
  },
  {
    "text": "kind of the process data said you can then store it back and then analyze so it's kind of iterative as well so what",
    "start": "410889",
    "end": "419779"
  },
  {
    "text": "we're going to do now in the next few slides we're going to go on each and every step and then we're going to start",
    "start": "419779",
    "end": "426740"
  },
  {
    "text": "discussing the technology options when it comes to architecture so we'll start with collection so when it comes to",
    "start": "426740",
    "end": "433669"
  },
  {
    "start": "430000",
    "end": "497000"
  },
  {
    "text": "collection and the first thing you have to think about is about the type of the data that you are collecting so what are",
    "start": "433669",
    "end": "440060"
  },
  {
    "text": "the types of the data I'm Carly I'm collecting so first we have data",
    "start": "440060",
    "end": "445250"
  },
  {
    "text": "structures database records these are data that typically are coming from mobile applications from web",
    "start": "445250",
    "end": "452000"
  },
  {
    "text": "applications and let's call those transactions and second category is",
    "start": "452000",
    "end": "458080"
  },
  {
    "text": "object or all files and the third is",
    "start": "458080",
    "end": "463129"
  },
  {
    "text": "about data streams let's call those events okay now I'm going to dive deeper",
    "start": "463129",
    "end": "469849"
  },
  {
    "text": "on each and every category okay so in",
    "start": "469849",
    "end": "476479"
  },
  {
    "text": "terms of storage the first the transactions really typically we would",
    "start": "476479",
    "end": "481879"
  },
  {
    "text": "store them into a database system either sequel no sequel in-memory cast it's a database system the medial the the files",
    "start": "481879",
    "end": "489949"
  },
  {
    "text": "you really would go to an object store and then we'll discuss the options there as well and then the data stream really",
    "start": "489949",
    "end": "495800"
  },
  {
    "text": "will go to a stream storage so again now let's dive deeper a bit and we go now",
    "start": "495800",
    "end": "501050"
  },
  {
    "start": "497000",
    "end": "794000"
  },
  {
    "text": "and it's in every category too let's start with the stream storage so when it comes to stream storage what are the",
    "start": "501050",
    "end": "506120"
  },
  {
    "text": "options I have on AWS so the first I'd like to kind of introduce Kinesis Amazon",
    "start": "506120",
    "end": "512000"
  },
  {
    "text": "Kinesis communities is a family of services and there are two services that I'm going to discuss today it's the",
    "start": "512000",
    "end": "518828"
  },
  {
    "text": "Genesis data streams and knishes data firehose and I will explain what is the use case of using those so Amazon",
    "start": "518829",
    "end": "526160"
  },
  {
    "text": "Kinesis Amazon Genesis data streams is a distributed and durable string storage",
    "start": "526160",
    "end": "532490"
  },
  {
    "text": "that you can ingest thousands or millions of events per second and what it allows you",
    "start": "532490",
    "end": "538910"
  },
  {
    "text": "to do it allows you to have multiple kind of producers - pushing data to the stream and then multiple consumers",
    "start": "538910",
    "end": "544550"
  },
  {
    "text": "reading from the stream now Amazon Kinesis is fully service it kind of fits",
    "start": "544550",
    "end": "550160"
  },
  {
    "text": "into that category with discussed earlier and what that means when you provision the provision streams and you",
    "start": "550160",
    "end": "557060"
  },
  {
    "text": "don't have to you know to think about kind of CPU memory and resources instead",
    "start": "557060",
    "end": "562580"
  },
  {
    "text": "you provision charge not they're called and one shard kind of corresponds to",
    "start": "562580",
    "end": "567800"
  },
  {
    "text": "1,000 writes per second so let's say if your application has 10,000 messages per",
    "start": "567800",
    "end": "573410"
  },
  {
    "text": "second that you need to write then you need to provision 10 charts for your stream then what Kinesis allows you will",
    "start": "573410",
    "end": "581360"
  },
  {
    "text": "store the data up to up to a week and then you can have your consumers application readings from the stream and",
    "start": "581360",
    "end": "587930"
  },
  {
    "text": "that will allow you to do real-time analytics now we have a lot of customers though who don't necessarily want to do",
    "start": "587930",
    "end": "595010"
  },
  {
    "text": "real-time analytics what they want to do instead they want to capture the data from the stream and they want to deliver",
    "start": "595010",
    "end": "601459"
  },
  {
    "text": "it to a data lake for further analytics later on and this is where you should",
    "start": "601459",
    "end": "606500"
  },
  {
    "text": "consider Kinesis data firehose which is a delivery stream think it as a pipe that will deliver your data to a target",
    "start": "606500",
    "end": "614390"
  },
  {
    "text": "and then that target then will be your system that you will do your analytics later on now when it comes to streaming",
    "start": "614390",
    "end": "621830"
  },
  {
    "text": "and you know the conversation always will have the word Apache Kafka and Apache Kafka is also high you know a",
    "start": "621830",
    "end": "629950"
  },
  {
    "text": "distributed streaming platform it is durable and it's coming from the open source community so there is huge",
    "start": "629950",
    "end": "635660"
  },
  {
    "text": "support for Apache Kafka out there so when it comes to Apache Kafka as of now",
    "start": "635660",
    "end": "640970"
  },
  {
    "text": "you can you can kind of fit to the virtualized kind of service where you",
    "start": "640970",
    "end": "646490"
  },
  {
    "text": "could install Apache Kafka on Institute's which that means you you you know you have all the configurability by",
    "start": "646490",
    "end": "652610"
  },
  {
    "text": "configuring Kafka applications however",
    "start": "652610",
    "end": "657650"
  },
  {
    "text": "that means that you have to kind of have admins and around the CAF that you need to administer the Kafka clusters and",
    "start": "657650",
    "end": "664220"
  },
  {
    "text": "this is where we released in reinvent Amazon msk which",
    "start": "664220",
    "end": "670250"
  },
  {
    "text": "stands msk stands for magnet service for Kafka so we heard our customers asking",
    "start": "670250",
    "end": "676399"
  },
  {
    "text": "us to help them to manage more on the Kafka lair and we have now a service at",
    "start": "676399",
    "end": "683720"
  },
  {
    "text": "the moment is in preview but it's in public preview meaning that actually you can go if you're an AWS customers and",
    "start": "683720",
    "end": "689630"
  },
  {
    "text": "try it out in fact three days ago it was released in Ireland as well in Europe now Amazon msk will fully manage",
    "start": "689630",
    "end": "698089"
  },
  {
    "text": "zookeeper for you and then you know with you know the console or it or the API as",
    "start": "698089",
    "end": "704089"
  },
  {
    "text": "you can create character clusters in minutes moving on and we will have quite",
    "start": "704089",
    "end": "711050"
  },
  {
    "text": "a lot of those tables some of them I will skip but the presentation slides will be made available and the key point",
    "start": "711050",
    "end": "717200"
  },
  {
    "text": "here when you select and you look over all the options really think of the use",
    "start": "717200",
    "end": "722720"
  },
  {
    "text": "case first and if the use case is just a one single message to unconsumed one",
    "start": "722720",
    "end": "728209"
  },
  {
    "text": "producer and when you have the consumer reading from the message you want the visibility time out this sort of",
    "start": "728209",
    "end": "734209"
  },
  {
    "text": "behavior then sqs is the right service to go for you know to those queuing",
    "start": "734209",
    "end": "739850"
  },
  {
    "text": "systems are still are still valid but when you've got like multiple consumers multiple producers and then is when you",
    "start": "739850",
    "end": "747830"
  },
  {
    "text": "have the kinases or Kafka and how to choose between kinases or Kafka what kinases will give you Kinesis will give",
    "start": "747830",
    "end": "754820"
  },
  {
    "text": "you a tight AWS integration it's a fully managed service you don't have to think about servers what Kafka will give you",
    "start": "754820",
    "end": "760790"
  },
  {
    "text": "the open source kind of tooling around it full configurability with the kind of",
    "start": "760790",
    "end": "766450"
  },
  {
    "text": "overhead of the admin overhead which is get reduced let's say when you go to an",
    "start": "766450",
    "end": "772910"
  },
  {
    "text": "escape ok moving to the next cutting to the category the file category and the",
    "start": "772910",
    "end": "779750"
  },
  {
    "text": "server is really the de-facto service to store files into AWS is really Amazon s3",
    "start": "779750",
    "end": "786770"
  },
  {
    "text": "and Amazon s3 it will allow you to to store any amount of data and you can",
    "start": "786770",
    "end": "792920"
  },
  {
    "text": "retrieve any amount of data there are a lot of use cases for allows ministry in fact is one of the oldest service",
    "start": "792920",
    "end": "799410"
  },
  {
    "start": "794000",
    "end": "1054000"
  },
  {
    "text": "integrated with the rest of AWS services but one of the use case is very strong use case is to build to use Amazon s3 as",
    "start": "799410",
    "end": "806759"
  },
  {
    "text": "the center of your data Lake and there are a couple of reasons why a three is actually good for being you know the",
    "start": "806759",
    "end": "814829"
  },
  {
    "text": "storage for your data Lake it's open and comprehensive and it really that means that you know apart from integrating",
    "start": "814829",
    "end": "820290"
  },
  {
    "text": "with the rest of the AWS services actually outside of AWS like Hadoop",
    "start": "820290",
    "end": "825329"
  },
  {
    "text": "clusters like our partners cloud era Hortonworks they speak natively to to to",
    "start": "825329",
    "end": "830519"
  },
  {
    "text": "s3 now what this trick will allow you by by using it as a storage it will really",
    "start": "830519",
    "end": "836790"
  },
  {
    "text": "allow you to decouple storage and compute and this is quite powerful when it comes to big data analytics and I",
    "start": "836790",
    "end": "842129"
  },
  {
    "text": "will try to explain a little bit more about that so by having a storage",
    "start": "842129",
    "end": "847379"
  },
  {
    "text": "outside of your let's say Hadoop cluster what that allows you to do it allows you",
    "start": "847379",
    "end": "852569"
  },
  {
    "text": "to run what we call transient clusters over your data set and what is the benefit there by having transient",
    "start": "852569",
    "end": "858839"
  },
  {
    "text": "clusters excuse me by having to transit clusters you could be fine clusters that are short-lived and you can introduce",
    "start": "858839",
    "end": "866819"
  },
  {
    "text": "some sort of steps when you configure them and when the steps complete then the clusters you that cluster can",
    "start": "866819",
    "end": "873029"
  },
  {
    "text": "terminate which means that you you've done your analytical job you've stored the output again back to history and the",
    "start": "873029",
    "end": "879720"
  },
  {
    "text": "cluster has completed now what's the benefit there the first apparent the benefit is the cost will aw as you pay",
    "start": "879720",
    "end": "885149"
  },
  {
    "text": "on before what you use meaning that if you have a cluster for ten minutes you pay for ten minutes and goes on now the other benefit though",
    "start": "885149",
    "end": "893189"
  },
  {
    "text": "which is it's around really the innovation that allows and the flexibility that gives you because you",
    "start": "893189",
    "end": "899730"
  },
  {
    "text": "could now you can actually run multiple clusters multiple Hadoop clusters and they might have different configurations",
    "start": "899730",
    "end": "905939"
  },
  {
    "text": "so for instance and you might have like SPARC Apache spark that really when you're doing interactive analytical at a",
    "start": "905939",
    "end": "912720"
  },
  {
    "text": "party spark really wants that memory so you can have what you know those are five instances that are memory optimized",
    "start": "912720",
    "end": "918269"
  },
  {
    "text": "so that you can have a Hadoop cluster like that but also you can have GPU and Hadoop clusters we to run tensor flow",
    "start": "918269",
    "end": "924779"
  },
  {
    "text": "for machine learning so with the same data set to different clusters and really that allows you to",
    "start": "924779",
    "end": "931930"
  },
  {
    "text": "and to really build the right Hadoop cluster for that for the job you need to to complete and the regarding the cost",
    "start": "931930",
    "end": "941980"
  },
  {
    "text": "I've mentioned also this kind of an is the cost saving is enhanced by Amazon ec2 spot so there is a really great use",
    "start": "941980",
    "end": "948520"
  },
  {
    "text": "case of of spot instances with Hadoop",
    "start": "948520",
    "end": "953890"
  },
  {
    "text": "with Hadoop a Amazon EMR and why is that if we think about Hadoop in general",
    "start": "953890",
    "end": "959470"
  },
  {
    "text": "Hadoop has or has been architected to tolerate individual or even multiple node loss even when jobs are running and",
    "start": "959470",
    "end": "967380"
  },
  {
    "text": "this is a great if you think about the spot instance the spot instances means that you are bidding for unused capacity",
    "start": "967380",
    "end": "973120"
  },
  {
    "text": "in AWS data centers and if you combine those two together means that you can do a combination and create like on-demand",
    "start": "973120",
    "end": "979149"
  },
  {
    "text": "instances which is the standard with you know they don't get interrupted is the",
    "start": "979149",
    "end": "984190"
  },
  {
    "text": "only money instances and with combinations with spot instances and then you can have your transit cluster",
    "start": "984190",
    "end": "990160"
  },
  {
    "text": "and spot instances will give you up to 90% saving when it comes to cost this does matter when you have clusters of 10",
    "start": "990160",
    "end": "996640"
  },
  {
    "text": "20 hundreds of nodes makes a big difference other reasons for s3 being",
    "start": "996640",
    "end": "1004670"
  },
  {
    "text": "ideal for the data lake to be the storage of the data lake is its durability and it has been designed for",
    "start": "1004670",
    "end": "1011160"
  },
  {
    "text": "a level nines of durability and one of the reasons that it achieves that durability s3 actually s3 behind the",
    "start": "1011160",
    "end": "1019440"
  },
  {
    "text": "scenes is using our availability zones so within a region when you put data into history s3 will replicate the data",
    "start": "1019440",
    "end": "1026428"
  },
  {
    "text": "automatically for you there is no additional charge for there for that it's it's the default behavior so that",
    "start": "1026429",
    "end": "1031949"
  },
  {
    "text": "is how you know s3 is durable one of the ways that we're on the region's s3 is",
    "start": "1031949",
    "end": "1036990"
  },
  {
    "text": "durable it's secure so s3 will allow you to encrypt data in transit will allow",
    "start": "1036990",
    "end": "1042720"
  },
  {
    "text": "you to encrypt data at rest and it's cost-effective and I would like to",
    "start": "1042720",
    "end": "1049710"
  },
  {
    "text": "actually expand a little bit more of ways to cost optimize Amazon s3 and one",
    "start": "1049710",
    "end": "1055830"
  },
  {
    "start": "1054000",
    "end": "1214000"
  },
  {
    "text": "of the ways to optimize Amazon s3 is to use those tiers those data tiers so for the kind of the",
    "start": "1055830",
    "end": "1062760"
  },
  {
    "text": "default standard tier is for frequently accessed data so for the data that you are doing analytics frequently",
    "start": "1062760",
    "end": "1068410"
  },
  {
    "text": "that will go to the Amazon you know standard s3 tier and but then what s3",
    "start": "1068410",
    "end": "1074650"
  },
  {
    "text": "actually gives you the option to move data to infrequent access and then this is where you save cost by moving the",
    "start": "1074650",
    "end": "1081520"
  },
  {
    "text": "data down to infrequent access and there are let's say that cold storage which is",
    "start": "1081520",
    "end": "1088240"
  },
  {
    "text": "like archives this is the Amazon glacier which is kind of very cold storage this",
    "start": "1088240",
    "end": "1094930"
  },
  {
    "text": "not doesn't necessarily make the villa the data available for you to do analytics at the same time but with",
    "start": "1094930",
    "end": "1100270"
  },
  {
    "text": "Amazon s3 standard infrequent access actually you don't compromise performance so you can actually have the data available to do analytics it's just",
    "start": "1100270",
    "end": "1107410"
  },
  {
    "text": "the pricing model that works differently but Amazon glacier though you save on cost significantly but the data is",
    "start": "1107410",
    "end": "1115030"
  },
  {
    "text": "stored as archive last week we made also available the deep archive which is the",
    "start": "1115030",
    "end": "1122230"
  },
  {
    "text": "cheapest way to store data basically on AWS but you're the trade-off there is the retrieval times is hours rather than",
    "start": "1122230",
    "end": "1129330"
  },
  {
    "text": "minutes okay and about HDFS so a lot of",
    "start": "1129330",
    "end": "1135390"
  },
  {
    "text": "some discussions with customers about HDFS like if you're familiar with with",
    "start": "1135390",
    "end": "1140470"
  },
  {
    "text": "Hadoop HDFS is the Hadoop distributed file system is where Hadoop by default",
    "start": "1140470",
    "end": "1145630"
  },
  {
    "text": "would store the data and and customers ask like okay I have the data industry do I need high DFS anymore and the",
    "start": "1145630",
    "end": "1152710"
  },
  {
    "text": "answer is maybe and there are some use cases that you could actually use HDFS",
    "start": "1152710",
    "end": "1158710"
  },
  {
    "text": "so what are the use cases if you are doing analytics over the same data set actually you could benefit by loading",
    "start": "1158710",
    "end": "1165850"
  },
  {
    "text": "the data into HDFS our - Hadoop you know EMR it comes with HDFS so you could",
    "start": "1165850",
    "end": "1172150"
  },
  {
    "text": "leverage HDFS and if you engineer Hadoop you could potentially get a use of the",
    "start": "1172150",
    "end": "1177670"
  },
  {
    "text": "kind of data locality although it's hard because HDFS is also distributed so there will be data flowing around the",
    "start": "1177670",
    "end": "1183520"
  },
  {
    "text": "nodes but there is that that room for HDFS that can't be used if you if you if",
    "start": "1183520",
    "end": "1190720"
  },
  {
    "text": "you do analytics over the kind of the same rather than fetching it from s3 all the time do analytics over",
    "start": "1190720",
    "end": "1197070"
  },
  {
    "text": "astray over hates DFS and there is actually a little utility that comes with EMR called Street DCP dist copy",
    "start": "1197070",
    "end": "1205260"
  },
  {
    "text": "that will use a MapReduce job to fetch the data distributed in parallel from",
    "start": "1205260",
    "end": "1210810"
  },
  {
    "text": "from s3 okay I'd like also to mention",
    "start": "1210810",
    "end": "1216450"
  },
  {
    "start": "1214000",
    "end": "1400000"
  },
  {
    "text": "about metadata in order to fully decouple your date your data though your data from from compute is data is once",
    "start": "1216450",
    "end": "1224310"
  },
  {
    "text": "one part of the equation also metadata is the other part you need to have both outside of the cluster to be able to do",
    "start": "1224310",
    "end": "1231810"
  },
  {
    "text": "those transit claim our clusters and what are the services about the metadata",
    "start": "1231810",
    "end": "1237030"
  },
  {
    "text": "I'd like to highlight firstly AWS glue catalog so AWS glue catalog is a fully",
    "start": "1237030",
    "end": "1242220"
  },
  {
    "text": "managed data catalog and it's hive metal store compliant now what is the purpose",
    "start": "1242220",
    "end": "1248100"
  },
  {
    "text": "of route galaga glue catalog really it it's there to be the unified central data catalog across your data sources",
    "start": "1248100",
    "end": "1255300"
  },
  {
    "text": "that you want to do analytics on and what then you have the Athena EMR and",
    "start": "1255300",
    "end": "1261690"
  },
  {
    "text": "redshift all the kind of group of AWS services that there are integrated with",
    "start": "1261690",
    "end": "1267720"
  },
  {
    "text": "the glue catalog so you can explore those data sources right away there will",
    "start": "1267720",
    "end": "1272820"
  },
  {
    "text": "be no effort to to plug them together a glue has a also a kind of a utility let's say the the",
    "start": "1272820",
    "end": "1279270"
  },
  {
    "text": "glue crawlers which is an application that you can point to your data sources and that could be industry that could be",
    "start": "1279270",
    "end": "1285810"
  },
  {
    "text": "also databases and then it will explore the data and will try to identify the metadata for you so that it will create",
    "start": "1285810",
    "end": "1291960"
  },
  {
    "text": "those tables to make it a little bit easier to you know to find those schemas it kind of makes it easier when it comes",
    "start": "1291960",
    "end": "1301860"
  },
  {
    "text": "to data legs and okay I say that so hive Medicaid when it comes to to data legs",
    "start": "1301860",
    "end": "1308390"
  },
  {
    "text": "you still you still have to go to a few places let's say to create it so we've",
    "start": "1308390",
    "end": "1314550"
  },
  {
    "text": "discussed about s3 for the storage we've discussed about the metadata the glue",
    "start": "1314550",
    "end": "1320190"
  },
  {
    "text": "catalog there are also you know what we hold the I am identity and access",
    "start": "1320190",
    "end": "1325620"
  },
  {
    "text": "management to do security so there are a couple of places that you need to go in in order to create a data Lake so we",
    "start": "1325620",
    "end": "1332220"
  },
  {
    "text": "identified that and then we've released lake formation and that was that is",
    "start": "1332220",
    "end": "1337260"
  },
  {
    "text": "currently in preview so lake formation is there to help you assist you to create a data lake in AWS within a few",
    "start": "1337260",
    "end": "1345300"
  },
  {
    "text": "minutes so it does build over those services that describe so it will use s3",
    "start": "1345300",
    "end": "1350640"
  },
  {
    "text": "behind the scenes it will use glue catalog it enhances those and those services with additional metadata and",
    "start": "1350640",
    "end": "1358680"
  },
  {
    "text": "also it will it will introduce some more also some data quality jobs that you",
    "start": "1358680",
    "end": "1365070"
  },
  {
    "text": "will do on the data to kind of deduplicate the data or find matches where there is no common idea available",
    "start": "1365070",
    "end": "1371040"
  },
  {
    "text": "so I'd like to highlight that this is a private preview at the moment so you need to sign up and then it's kind of a",
    "start": "1371040",
    "end": "1377580"
  },
  {
    "text": "white listing so will be some conversation about your news case before you get your hands let's say - like lake",
    "start": "1377580",
    "end": "1383130"
  },
  {
    "text": "formation and for completeness just to highlight the hide meta store you know glue catalog is one of the options it is",
    "start": "1383130",
    "end": "1390480"
  },
  {
    "text": "the easiest option but for completeness also I would like to point out that you have also have metal store that you",
    "start": "1390480",
    "end": "1395760"
  },
  {
    "text": "could build for instance and you can host it on Amazon RDS ok and moving to",
    "start": "1395760",
    "end": "1402840"
  },
  {
    "start": "1400000",
    "end": "1430000"
  },
  {
    "text": "databases there was a session on database so I'll go through a bit quick from databases databases from an",
    "start": "1402840",
    "end": "1408780"
  },
  {
    "text": "analytics point of view could be either our source of the data that we pull the data from the databases so that we can",
    "start": "1408780",
    "end": "1414840"
  },
  {
    "text": "do later analytics can be also the target of analytics you might want to you might have built some analysis some",
    "start": "1414840",
    "end": "1421110"
  },
  {
    "text": "data and then you want to push that data back to to a database so that you have",
    "start": "1421110",
    "end": "1426180"
  },
  {
    "text": "like a real-time dashboard or something really the key message here is the best",
    "start": "1426180",
    "end": "1435120"
  },
  {
    "start": "1430000",
    "end": "1451000"
  },
  {
    "text": "the best practice released you know you use there right there are so many database options on AWS you need to use",
    "start": "1435120",
    "end": "1441060"
  },
  {
    "text": "the right tool for the job and just to help you identify the right tool is back",
    "start": "1441060",
    "end": "1447360"
  },
  {
    "text": "to those questions that you need to ask yourself and the main the main two questions is about the data structure",
    "start": "1447360",
    "end": "1453930"
  },
  {
    "start": "1451000",
    "end": "1503000"
  },
  {
    "text": "and how the data will be accessed so if you are if your data structure is still",
    "start": "1453930",
    "end": "1459570"
  },
  {
    "text": "is kind of a fixed schema then probably sequel is the way to go especially if you have complex",
    "start": "1459570",
    "end": "1466240"
  },
  {
    "text": "relationships and if you have table joints if this is the way you access the data then you know you should evaluate",
    "start": "1466240",
    "end": "1472000"
  },
  {
    "text": "SQL databases first if your data though has no fixed schema or it has a simple",
    "start": "1472000",
    "end": "1477529"
  },
  {
    "text": "key value pair then you should evaluate first and no sequel database and if your latency requirements are really low to",
    "start": "1477529",
    "end": "1484639"
  },
  {
    "text": "the microsecond then yes you need to think of in-memory cache in front of your database system or the in front of",
    "start": "1484639",
    "end": "1492320"
  },
  {
    "text": "the no sequel database and if you have graph kind of traversal for queering or",
    "start": "1492320",
    "end": "1497990"
  },
  {
    "text": "if you have graph structures in your data you need to evaluate first a graph database so there are quite a lot of",
    "start": "1497990",
    "end": "1506149"
  },
  {
    "text": "characteristic again those slides will be made available and the main two characteristics its line one which is",
    "start": "1506149",
    "end": "1513139"
  },
  {
    "text": "the use case line three the shape so these two are the ones that we really",
    "start": "1513139",
    "end": "1518169"
  },
  {
    "text": "will hint we will show you what sort of database systems to use okay moving to",
    "start": "1518169",
    "end": "1528580"
  },
  {
    "start": "1526000",
    "end": "1785000"
  },
  {
    "text": "to the next step processing analysis so when it comes to personalities I'd like",
    "start": "1528580",
    "end": "1533629"
  },
  {
    "text": "to highlight now the services what they do and then some use cases first we start with interactive and but analytics",
    "start": "1533629",
    "end": "1541759"
  },
  {
    "text": "so some service a couple of options there so we've got Amazon elastic search and Amazon Elastic search will allow you",
    "start": "1541759",
    "end": "1547639"
  },
  {
    "text": "to spin up elastic search clusters in minutes and what are the use cases for elastic search well the strongest use",
    "start": "1547639",
    "end": "1554179"
  },
  {
    "text": "case I am saying is about logs logs analysis so if you have logs and you want to have dashboards with kibana",
    "start": "1554179",
    "end": "1562009"
  },
  {
    "text": "this l.k stack Amazon elastic search is great the other use case I see as well",
    "start": "1562009",
    "end": "1567590"
  },
  {
    "text": "for for data lakes is also some metadata indexing so if you will because elastic",
    "start": "1567590",
    "end": "1573080"
  },
  {
    "text": "search is a search system and it will allow you to actually query using some sort of search so you could actually do",
    "start": "1573080",
    "end": "1578240"
  },
  {
    "text": "a lot of stuff like even tolerate typos you could do this sort of experience you know search experience to your to your",
    "start": "1578240",
    "end": "1584539"
  },
  {
    "text": "queries you could index metadata to elastic search about your data and then you can",
    "start": "1584539",
    "end": "1589879"
  },
  {
    "text": "have an additional search ability over your data sources in the data Lake this is also another use cases",
    "start": "1589879",
    "end": "1595740"
  },
  {
    "text": "have to be you know something that you are querying for analytics it could also play a role to help you explore your",
    "start": "1595740",
    "end": "1601260"
  },
  {
    "text": "data Amazon redshift and spare ft spectrum the redshift is our fully",
    "start": "1601260",
    "end": "1606960"
  },
  {
    "text": "managed data warehouse and Amazon redshift is a what we call an OLAP",
    "start": "1606960",
    "end": "1612750"
  },
  {
    "text": "system and online analytical processing systems or in a massive parallel processing system alright so it will",
    "start": "1612750",
    "end": "1618150"
  },
  {
    "text": "allow you to query terabytes of data petabytes of data sorry and Amazon",
    "start": "1618150",
    "end": "1624840"
  },
  {
    "text": "redshift really it's a kind of you know you it would be like a schema and read",
    "start": "1624840",
    "end": "1631320"
  },
  {
    "text": "kind of a system which means you define the scheme upfront kind of highly structured data what you put in in",
    "start": "1631320",
    "end": "1636630"
  },
  {
    "text": "Amazon redshift now there is also an extension Armas Interactive spectrum and",
    "start": "1636630",
    "end": "1643410"
  },
  {
    "text": "Amazon redshift spectrum will allow you to query data in a three I would like to explain a little bit more what is the",
    "start": "1643410",
    "end": "1648900"
  },
  {
    "text": "use case of spectrum so let's say let's imagine that you have a data warehouse and this is where you drive your monthly",
    "start": "1648900",
    "end": "1655230"
  },
  {
    "text": "or weekly or daily reports from so the data there is high quality data and highly structured data but there is",
    "start": "1655230",
    "end": "1661950"
  },
  {
    "text": "still a lot of the day a lot of data that reside in your data Lake in Amazon s3 and for one reason or the other they",
    "start": "1661950",
    "end": "1669420"
  },
  {
    "text": "didn't make it all the way to the day to the data warehouse maybe was lower quality but they still hold value so",
    "start": "1669420",
    "end": "1675150"
  },
  {
    "text": "what rates if spectrum will allow you to do it will allow you to combine data from that side of the data like with",
    "start": "1675150",
    "end": "1681750"
  },
  {
    "text": "your high quality data from the data warehouse Amazon relative to this one single interface and actually spectrum",
    "start": "1681750",
    "end": "1688410"
  },
  {
    "text": "will be able to to query to the limits beyond redshift because redshift has you",
    "start": "1688410",
    "end": "1696179"
  },
  {
    "text": "know to petabyte actually to spectrum will go to exabyte because you will utilize the nodes closer to a3 not",
    "start": "1696179",
    "end": "1703380"
  },
  {
    "text": "necessarily your your redshift cluster will just bring the results back to your cluster just to present it back and",
    "start": "1703380",
    "end": "1710010"
  },
  {
    "text": "Amazon Athena another great service for a kind of the service category that will",
    "start": "1710010",
    "end": "1715020"
  },
  {
    "text": "allow you to do a sequel queries over your data in s3 and the skill set that",
    "start": "1715020",
    "end": "1720720"
  },
  {
    "text": "you will require to be able to query with Athena is really if you have presto skill or you know presto the interface",
    "start": "1720720",
    "end": "1728370"
  },
  {
    "text": "would be an unsecured kind of interface so you know you will do select statements and this sort of stuff and",
    "start": "1728370",
    "end": "1734480"
  },
  {
    "text": "then of course Amazon EMR Amazonia merrily and it's a great service into",
    "start": "1734480",
    "end": "1739680"
  },
  {
    "text": "the manage Hadoop our manaslu Duke service and it will give you all the frameworks up to date including presto",
    "start": "1739680",
    "end": "1747360"
  },
  {
    "text": "spark hive Heights baits and and and what is great about the Ammar and it's a",
    "start": "1747360",
    "end": "1754559"
  },
  {
    "text": "kind of an exception let's say because actually amazon EMR it's like most of managed services you don't have access",
    "start": "1754559",
    "end": "1760170"
  },
  {
    "text": "like root access you we don't offer that in most managed services that that's not the case for EMR you can actually get",
    "start": "1760170",
    "end": "1766650"
  },
  {
    "text": "root access on EMR and then you can bootstrap it and you can install your own applications and it's a great way to",
    "start": "1766650",
    "end": "1773010"
  },
  {
    "text": "get used to use you know the there is a huge ecosystem on the hadoop",
    "start": "1773010",
    "end": "1778370"
  },
  {
    "text": "technologies in general outside of AWS you could integrate those with Amazon",
    "start": "1778370",
    "end": "1783390"
  },
  {
    "text": "EMR ok going to stream analytics so",
    "start": "1783390",
    "end": "1789480"
  },
  {
    "start": "1785000",
    "end": "1823000"
  },
  {
    "text": "regards the stream analytics there are a couple of options again spark streaming with EMR and there is also impact the",
    "start": "1789480",
    "end": "1795720"
  },
  {
    "text": "kinases family the kinases data analytics that you can configure data analytics kinases data analytics to read",
    "start": "1795720",
    "end": "1801929"
  },
  {
    "text": "from Kinesis data streams and then again with some sort of sequel interface to aggregate or do analytics on the streams",
    "start": "1801929",
    "end": "1808410"
  },
  {
    "text": "and you can always create your own applications using the Kinesis client library all with lambdas I think there",
    "start": "1808410",
    "end": "1815550"
  },
  {
    "text": "was some similar use case in the previous session about how to do you",
    "start": "1815550",
    "end": "1821100"
  },
  {
    "text": "know to read from lamda notifications and moving to predictive analytics and I",
    "start": "1821100",
    "end": "1827910"
  },
  {
    "start": "1823000",
    "end": "1910000"
  },
  {
    "text": "won't explain every single service I think there was another session earlier which great session from Neil about",
    "start": "1827910",
    "end": "1832980"
  },
  {
    "text": "predictive analytics the key message here I would like to point out and you've got the option of you know you",
    "start": "1832980",
    "end": "1841110"
  },
  {
    "text": "have those three layers from frameworks platforms and applications when it comes to frameworks is for machine learning",
    "start": "1841110",
    "end": "1846750"
  },
  {
    "text": "practitioners you get the Machine the the images with those frameworks pre-installed but you have to do the",
    "start": "1846750",
    "end": "1852660"
  },
  {
    "text": "machine learning you have to do you know all the complexity of the hyper",
    "start": "1852660",
    "end": "1858300"
  },
  {
    "text": "parameter tuning traditional machine learning kind of tasks the the platforms it's kind of the helping those machine",
    "start": "1858300",
    "end": "1865890"
  },
  {
    "text": "learning experts to do tasks like you know giving them a Jupiter notebook a",
    "start": "1865890",
    "end": "1872520"
  },
  {
    "text": "familiar interface and then you can use the API say it's make your API to create",
    "start": "1872520",
    "end": "1878250"
  },
  {
    "text": "for instance models and then you can deploy the model so it's one kind of place that you can do all your machine",
    "start": "1878250",
    "end": "1884160"
  },
  {
    "text": "learning tasks for the platforms and for applications then it's just for for",
    "start": "1884160",
    "end": "1889170"
  },
  {
    "text": "developers who don't want to who want to use machine learning in their application but they don't want to implement machine learning so we have",
    "start": "1889170",
    "end": "1896250"
  },
  {
    "text": "some algorithm that you know they're successful at a amazon.com like comprehend the language a language the",
    "start": "1896250",
    "end": "1903900"
  },
  {
    "text": "natural language processing and you can integrate that with either calling those",
    "start": "1903900",
    "end": "1909060"
  },
  {
    "text": "api's and just a couple of examples before we go to before we move on so",
    "start": "1909060",
    "end": "1915930"
  },
  {
    "start": "1910000",
    "end": "1982000"
  },
  {
    "text": "when it comes to math analytics think of it as a report that are running monthly",
    "start": "1915930",
    "end": "1921150"
  },
  {
    "text": "weekly or daily and the first service that you know who comes into mind is",
    "start": "1921150",
    "end": "1926610"
  },
  {
    "text": "Amazon EMR with with tools like hive or or spark interactive analytics think of",
    "start": "1926610",
    "end": "1934620"
  },
  {
    "text": "it like as thyself self-service dashboard something that you know maybe you want to get some answers from the system within seconds",
    "start": "1934620",
    "end": "1941520"
  },
  {
    "text": "so the service is there will be redshift afina or EMR running spark or am i running",
    "start": "1941520",
    "end": "1947430"
  },
  {
    "text": "presto when it comes to stream analytics again EMR you can utilize spark spark",
    "start": "1947430",
    "end": "1953790"
  },
  {
    "text": "streaming or those kinases data analytics services and when it comes to",
    "start": "1953790",
    "end": "1959670"
  },
  {
    "text": "predictive predictive analytics will be either bats or or streaming depends which service you use and there are so",
    "start": "1959670",
    "end": "1965970"
  },
  {
    "text": "many services there from AWS but you know the key key message I would like to say is like the Amazon change maker",
    "start": "1965970",
    "end": "1971790"
  },
  {
    "text": "really is worth noting that because it's the single single place that will form",
    "start": "1971790",
    "end": "1977250"
  },
  {
    "text": "for machine learning practitioners to to do all the machine learning from one place",
    "start": "1977250",
    "end": "1983450"
  },
  {
    "start": "1982000",
    "end": "2063000"
  },
  {
    "text": "so I'll skip through those slides quickly because we're gonna discuss",
    "start": "1985830",
    "end": "1992410"
  },
  {
    "text": "about the next topic which is the ETL so ETL quite a lot of discussion around it",
    "start": "1992410",
    "end": "1997570"
  },
  {
    "text": "and sometimes we think like when you do ETL which stands from extract transform load",
    "start": "1997570",
    "end": "2003630"
  },
  {
    "text": "you might lose value as you change the data but what sum is what is a good years for for ETL is really to prepare",
    "start": "2003630",
    "end": "2011700"
  },
  {
    "text": "the data in a format that is really more suitable to do analytics like partition the data converting from a text format",
    "start": "2011700",
    "end": "2018090"
  },
  {
    "text": "into a parka format or an RC format and there are a couple of options when it comes to ETL I'd like to highlight blue",
    "start": "2018090",
    "end": "2025200"
  },
  {
    "text": "cup a blue ETL blue ETL will allow you to run spark or Python shell and it's",
    "start": "2025200",
    "end": "2031080"
  },
  {
    "text": "kind of a serverless type of category where it kind of abstracts the service server complexity from you you just",
    "start": "2031080",
    "end": "2036930"
  },
  {
    "text": "define data processing unit and then you can schedule those jobs to run if you",
    "start": "2036930",
    "end": "2042630"
  },
  {
    "text": "want to capture a change data capture from from a database then database",
    "start": "2042630",
    "end": "2048389"
  },
  {
    "text": "migration service also an option gluant is not able to do that or you you can look under the partner solutions",
    "start": "2048390",
    "end": "2054540"
  },
  {
    "text": "especially when it comes to change datasets from change data capture from",
    "start": "2054540",
    "end": "2059820"
  },
  {
    "text": "relational databases ok moving to",
    "start": "2059820",
    "end": "2065580"
  },
  {
    "start": "2063000",
    "end": "2123000"
  },
  {
    "text": "consumption so when it comes to consumption of the data which is kind of our last step when it comes to",
    "start": "2065580",
    "end": "2071340"
  },
  {
    "text": "consumption I say to cut two main categories one category is really those",
    "start": "2071340",
    "end": "2076770"
  },
  {
    "text": "business users that they want to make sense of the data and what the service there is really you know applications",
    "start": "2076770",
    "end": "2083760"
  },
  {
    "text": "like visualization visual visualization applications like tableau or we have",
    "start": "2083760",
    "end": "2089250"
  },
  {
    "text": "also an Amazon quick site which are our fully managed business intelligence tool",
    "start": "2089250",
    "end": "2095280"
  },
  {
    "text": "and also Cabana Cabana will allow you to run visualizations in front of Amazon",
    "start": "2095280",
    "end": "2100530"
  },
  {
    "text": "Elastic search and the other categories those data scientists that they want to get an access to an endpoint and they",
    "start": "2100530",
    "end": "2107580"
  },
  {
    "text": "wanted us to you know do statistical analysis using our studio for instance for those you know Athena for instance",
    "start": "2107580",
    "end": "2113580"
  },
  {
    "text": "of the redshift will have a JDBC you know driver that you can make to your data and you can start",
    "start": "2113580",
    "end": "2119640"
  },
  {
    "text": "pulling data from there or you can just directly query them okay and the next",
    "start": "2119640",
    "end": "2125340"
  },
  {
    "start": "2123000",
    "end": "2228000"
  },
  {
    "text": "now I'm going to put all of them together and it will start actually discussing some some design patterns for",
    "start": "2125340",
    "end": "2132900"
  },
  {
    "text": "the data so we kind of saw that slide before also notice this ETL arrow there",
    "start": "2132900",
    "end": "2137910"
  },
  {
    "text": "that storage and processing you know can go you can analyze and then box store",
    "start": "2137910",
    "end": "2143790"
  },
  {
    "text": "and then analyze again okay let's see",
    "start": "2143790",
    "end": "2149040"
  },
  {
    "text": "some some design patterns okay",
    "start": "2149040",
    "end": "2156080"
  },
  {
    "text": "that kind of goes back to the temperature kind of data so on the left",
    "start": "2156080",
    "end": "2162360"
  },
  {
    "text": "hand side you see the hot data meaning the data that are short-lived own data but kind of lose value as they age and",
    "start": "2162360",
    "end": "2169530"
  },
  {
    "text": "data that you typically want to retrieve faster and all the other end is the cold data and the ultra Cola says those",
    "start": "2169530",
    "end": "2176880"
  },
  {
    "text": "archives so as the data arrives you see Keaney sees as storage for the day for",
    "start": "2176880",
    "end": "2184650"
  },
  {
    "text": "the kind of a hot data and then for for analysis spark streaming lambda KCl",
    "start": "2184650",
    "end": "2192050"
  },
  {
    "text": "applications for interactive analytics redshift to query data from to load data",
    "start": "2192050",
    "end": "2201390"
  },
  {
    "text": "from from RDS or Amazon s3 and then allow you to query data and also Amazon",
    "start": "2201390",
    "end": "2206730"
  },
  {
    "text": "Athena or EMR with spark entresto running over s3 and for the bats",
    "start": "2206730",
    "end": "2214230"
  },
  {
    "text": "analytics hive is a kind of a typical tool running over EMR so the lower those",
    "start": "2214230",
    "end": "2219840"
  },
  {
    "text": "tools are the the the bigger is the response time back to to you okay when",
    "start": "2219840",
    "end": "2229710"
  },
  {
    "start": "2228000",
    "end": "2305000"
  },
  {
    "text": "it comes to streaming analytics and a stream typically a right to Kinesis Amazon Kinesis data streams and and then",
    "start": "2229710",
    "end": "2237840"
  },
  {
    "text": "you might have the data analytics Kinesis data analytics to do real-time analytics other options you have there",
    "start": "2237840",
    "end": "2243990"
  },
  {
    "text": "it would be lambda to read from the stream will be a KCl your your library or you could actually do like micro",
    "start": "2243990",
    "end": "2249960"
  },
  {
    "text": "batching as a spark streaming does with Amazon EMR and you could utilize when you could utilize those ml",
    "start": "2249960",
    "end": "2256530"
  },
  {
    "text": "applications so for instance if you want to start do some ml as you process the",
    "start": "2256530",
    "end": "2261600"
  },
  {
    "text": "stream you can utilize those endpoints that are already there for you and you",
    "start": "2261600",
    "end": "2267990"
  },
  {
    "text": "can let's say you identified something and you want to do like a fraud alert then you can use like SNS to push a notification out to users and then you",
    "start": "2267990",
    "end": "2275700"
  },
  {
    "text": "store the data to s3 which is your data leak for further analytics later on and",
    "start": "2275700",
    "end": "2280710"
  },
  {
    "text": "if you want to capture the data just to have a real-time dashboard or something",
    "start": "2280710",
    "end": "2286170"
  },
  {
    "text": "that will feed the dashboard then is a good practice to actually export like an app state and have another system that",
    "start": "2286170",
    "end": "2293400"
  },
  {
    "text": "could be a database like you know DynamoDB for instance and then have some KPI tables or sort of visualizations in",
    "start": "2293400",
    "end": "2301740"
  },
  {
    "text": "front of the streaming data I'd like",
    "start": "2301740",
    "end": "2307050"
  },
  {
    "start": "2305000",
    "end": "2356000"
  },
  {
    "text": "also to in that case now I'd like to introduce like a customer use case which",
    "start": "2307050",
    "end": "2312300"
  },
  {
    "text": "is Hearst which is a media and information company among other things they're managing websites for those",
    "start": "2312300",
    "end": "2318330"
  },
  {
    "text": "magazines and they're using Kinesis data stream and I would like to highlight here the use case that you can have",
    "start": "2318330",
    "end": "2324690"
  },
  {
    "text": "multiple consumers from the same stream so with Kinesis data streams one stream one one is we're using lambdas to pull",
    "start": "2324690",
    "end": "2333150"
  },
  {
    "text": "the data from the stream and then doing the analytics and then push the results of the analyses into DynamoDB and then",
    "start": "2333150",
    "end": "2339930"
  },
  {
    "text": "you expose then those tables they are exposed as from an API and that could drive dashboard but on the other the",
    "start": "2339930",
    "end": "2347160"
  },
  {
    "text": "other route is with key missus data firehose to capture the streaming event and deliver them into s3 for feather",
    "start": "2347160",
    "end": "2354210"
  },
  {
    "text": "analytics later on moving to Interactive",
    "start": "2354210",
    "end": "2359820"
  },
  {
    "start": "2356000",
    "end": "2446000"
  },
  {
    "text": "now in baton oolitic kind of design patterns again we start with first stream the fact that you might do",
    "start": "2359820",
    "end": "2366360"
  },
  {
    "text": "interactive in but analytics doesn't mean to say that you don't have streaming data so you could have stream",
    "start": "2366360",
    "end": "2371730"
  },
  {
    "text": "data and again in order to capture them and put them to s3 Kinesis data firehose",
    "start": "2371730",
    "end": "2377130"
  },
  {
    "text": "is a great service but you also might have files that you can deliver directly",
    "start": "2377130",
    "end": "2382530"
  },
  {
    "text": "to s3 and from Kinesis the data firehose and it's a delivery stream think it has just",
    "start": "2382530",
    "end": "2389600"
  },
  {
    "text": "a pipe that delivers so the supported target is apart from s3 is also redshift",
    "start": "2389600",
    "end": "2395840"
  },
  {
    "text": "for data warehousing or elastic search to do those kind of key bond and our sports or doing analysis and from from",
    "start": "2395840",
    "end": "2405260"
  },
  {
    "text": "to do interactive analytics over s3 great a great tool is Athena as well but",
    "start": "2405260",
    "end": "2411620"
  },
  {
    "text": "also running EMR clusters with presto or or spark for interactive analytics for",
    "start": "2411620",
    "end": "2419510"
  },
  {
    "text": "batch analytics now EMR again is a great great tool or the glute jobs is also is",
    "start": "2419510",
    "end": "2426110"
  },
  {
    "text": "also possible and with EMR you can have the main the main two frameworks that",
    "start": "2426110",
    "end": "2431390"
  },
  {
    "text": "I'm seeing now is a spark and hive not Pig for instance but primarily spark and",
    "start": "2431390",
    "end": "2436670"
  },
  {
    "text": "hive yet so the top layer is the interactive analytics the bottom layer",
    "start": "2436670",
    "end": "2442820"
  },
  {
    "text": "is the batch analytics ok another",
    "start": "2442820",
    "end": "2447980"
  },
  {
    "start": "2446000",
    "end": "2536000"
  },
  {
    "text": "customer example that I would like also to highlight is FINRA so FINRA is the financial regulator regulatory authority",
    "start": "2447980",
    "end": "2455240"
  },
  {
    "text": "over in the USA and they're using they're using AWS for for a lot of their",
    "start": "2455240",
    "end": "2461120"
  },
  {
    "text": "analytics and they are actually ingesting everyday even billions of records daily and data are coming from",
    "start": "2461120",
    "end": "2469400"
  },
  {
    "text": "various sources into you know s3 and from s3 they have multiple systems doing",
    "start": "2469400",
    "end": "2475730"
  },
  {
    "text": "analytics and this is this is back to where we say you can have multiple clusters doing analytics and there's",
    "start": "2475730",
    "end": "2481640"
  },
  {
    "text": "clusters would be specific to the workload that you want to support so they have like for instance redshift for",
    "start": "2481640",
    "end": "2488080"
  },
  {
    "text": "for a kind of predefined queries and they have been different EMR clusters they have multiple EMR classes not just",
    "start": "2488080",
    "end": "2494330"
  },
  {
    "text": "to actually as this diagram you have multiple EMR clusters to do interactive analytics or serve various analytics to",
    "start": "2494330",
    "end": "2501080"
  },
  {
    "text": "detect fraud etc now finner has strict kind of security requirements and they",
    "start": "2501080",
    "end": "2506450"
  },
  {
    "text": "were utilizing things like a VP see that the virtual private cloud and EMR can be hosted into a V PC or redshift can be",
    "start": "2506450",
    "end": "2514040"
  },
  {
    "text": "hosted into a BBC and they also use encryption at rest and in transit from",
    "start": "2514040",
    "end": "2519710"
  },
  {
    "text": "s3 and of course they're using a cloud trail crowd AWS cloud trail is for audit",
    "start": "2519710",
    "end": "2525170"
  },
  {
    "text": "it will capture every single API call that you do to AWS and then you can have",
    "start": "2525170",
    "end": "2530599"
  },
  {
    "text": "an audit trail for any any action that happened on your infrastructure and then",
    "start": "2530599",
    "end": "2539300"
  },
  {
    "start": "2536000",
    "end": "2627000"
  },
  {
    "text": "I'd like also to say like the kind of the final pattern is a data like architecture a reference architecture so",
    "start": "2539300",
    "end": "2546380"
  },
  {
    "text": "you can say here the Amazon s3 is really the center of the data Lake plays a central role I'd like to say also with",
    "start": "2546380",
    "end": "2554300"
  },
  {
    "text": "with glue catalog together because you need to have those metadata as well to be able to query so there are again",
    "start": "2554300",
    "end": "2561079"
  },
  {
    "text": "multiple ways to put data into a straight and if you have a relational",
    "start": "2561079",
    "end": "2566900"
  },
  {
    "text": "databases and you want to have those data change data captures then with",
    "start": "2566900",
    "end": "2573190"
  },
  {
    "text": "database migration service is a great way the other way is with glue ETL jobs to put the data into s3 and streaming",
    "start": "2573190",
    "end": "2580819"
  },
  {
    "text": "data arriving data firehose is a great way you could use Kinesis data analytics",
    "start": "2580819",
    "end": "2586069"
  },
  {
    "text": "to do maybe analytics and then put them back to stream and then put it back to to s3 from Amazon s3 you can then use",
    "start": "2586069",
    "end": "2594740"
  },
  {
    "text": "lambdas or spark streaming or again those KCl applications to to do a",
    "start": "2594740",
    "end": "2600920"
  },
  {
    "text": "real-time kind of of analytics and then you can load the data to some sort of application state or a materialized view",
    "start": "2600920",
    "end": "2608540"
  },
  {
    "text": "so that you drive those dashboards or you can have them the batch and interactive layer above your data Lake",
    "start": "2608540",
    "end": "2614690"
  },
  {
    "text": "so that you can explore data in the data like services there when we discuss here you know redshift Athena presto spark",
    "start": "2614690",
    "end": "2622160"
  },
  {
    "text": "hive all these sort of tools for budget and interactive analytics okay to",
    "start": "2622160",
    "end": "2629869"
  },
  {
    "start": "2627000",
    "end": "2702000"
  },
  {
    "text": "summarize I would like to play back those architectural principles that we discussed in the start of the presentation we starting from building",
    "start": "2629869",
    "end": "2637099"
  },
  {
    "text": "decouple systems and really today we discussed how you know those steps from collection to storage to processing and",
    "start": "2637099",
    "end": "2643310"
  },
  {
    "text": "to analysis and you know what 10 tools you have to use so use the right tool for",
    "start": "2643310",
    "end": "2648400"
  },
  {
    "text": "the right job and then own it and every step remember those kind of options you have and those kind of decisions you",
    "start": "2648400",
    "end": "2655059"
  },
  {
    "text": "have to make we discuss also today about managed services and their service",
    "start": "2655059",
    "end": "2660099"
  },
  {
    "text": "services and what are the options that AWS gives you where possible do leverage you inherit a lot of best practice and",
    "start": "2660099",
    "end": "2665740"
  },
  {
    "text": "you do save a lot of a lot of you know reduce the cost especially admin cost",
    "start": "2665740",
    "end": "2670890"
  },
  {
    "text": "and do prefer immutable data sets immutable logs data lay kind of",
    "start": "2670890",
    "end": "2677200"
  },
  {
    "text": "architecture that you append to the to the data set rather than upgrade updating and overwriting and of course",
    "start": "2677200",
    "end": "2684550"
  },
  {
    "text": "big cost-conscious big data systems don't necessarily have to be expensive and have big cost with that I'd like to",
    "start": "2684550",
    "end": "2692380"
  },
  {
    "text": "thank you for your attention [Applause]",
    "start": "2692380",
    "end": "2700969"
  }
]