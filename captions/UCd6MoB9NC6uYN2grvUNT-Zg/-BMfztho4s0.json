[
  {
    "start": "0",
    "end": "17000"
  },
  {
    "text": "In this video, you'll see how you can stream analytics\nusing a modern data architecture from AWS.",
    "start": "240",
    "end": "5397"
  },
  {
    "text": "With this approach, you can move data from a \ndata store into a data lake (outside-in),",
    "start": "6000",
    "end": "10686"
  },
  {
    "text": "from one data store to another (around-the-perimeter), \nand from a data lake to a data store (inside-out).",
    "start": "10686",
    "end": "16450"
  },
  {
    "start": "17000",
    "end": "85000"
  },
  {
    "text": "Let’s begin by navigating to Amazon Kinesis \nto look at an outside-in data movement.",
    "start": "17594",
    "end": "21954"
  },
  {
    "text": "In this example, Amazon Kinesis is ingesting real-time data\nstreaming from Amazon Elastic Compute Cloud (Amazon EC2).",
    "start": "24305",
    "end": "30865"
  },
  {
    "text": "The stream is collecting data about taxi trips.",
    "start": "31536",
    "end": "33885"
  },
  {
    "text": "While the dashboard is loading, we can go to Systems Manager Session Manager\nto see the data that’s being processed and sent to Kinesis.",
    "start": "34375",
    "end": "40693"
  },
  {
    "text": "Here we can see that the code is \ngenerating a series of events.",
    "start": "41339",
    "end": "44201"
  },
  {
    "text": "The events are then ingested into the \nstream and sent to Kinesis in real time.",
    "start": "45062",
    "end": "48582"
  },
  {
    "text": "Amazon Kinesis Data Streams ingests a large amount \nof data in real time, durably stores the data,",
    "start": "51655",
    "end": "56957"
  },
  {
    "text": "and makes the data available for consumption.",
    "start": "56957",
    "end": "58905"
  },
  {
    "text": "On the Monitoring tab, we can see what metrics are being monitored.",
    "start": "59358",
    "end": "62290"
  },
  {
    "text": "The “put record sum” captures how many data records\nare contained in the data stream at a given time.",
    "start": "63890",
    "end": "68451"
  },
  {
    "text": "Before we continue with\nthis outside-in data movement,",
    "start": "69988",
    "end": "72882"
  },
  {
    "text": "let's use our data stream to illustrate an around-the-perimeter\ndata movement, which sends data between data stores.",
    "start": "72882",
    "end": "78394"
  },
  {
    "text": "In this case, we’ll move data from our Kinesis Data Stream\nto Kinesis Data Analytics for SQL Applications.",
    "start": "79169",
    "end": "84445"
  },
  {
    "start": "85000",
    "end": "156000"
  },
  {
    "text": "Let’s select the initials-taxi-trips application to view\nan analytics event that we have already created.",
    "start": "85601",
    "end": "90505"
  },
  {
    "text": "On the Source tab, we can \nsee the application source.",
    "start": "91711",
    "end": "94311"
  },
  {
    "text": "Kinesis Data Analytics is the easiest way to process data\nin real time with SQL without having to learn the language.",
    "start": "97037",
    "end": "102717"
  },
  {
    "text": "Let's take a closer look at the configuration.",
    "start": "103474",
    "end": "105428"
  },
  {
    "text": "In the SQL code we have two streams, “cleaned_trips”\nand “trip_statistics,” each with an associated pump.",
    "start": "106325",
    "end": "112126"
  },
  {
    "text": "A pump serves as a continuous \nquery to ensure the stream runs.",
    "start": "112671",
    "end": "115859"
  },
  {
    "text": "The cleaned-trips pump selects type values \nthat have a longitude and latitude value.",
    "start": "116911",
    "end": "120994"
  },
  {
    "text": "The trip_statistics pump calculates the \nsummary statistics in two-second intervals.",
    "start": "122166",
    "end": "126166"
  },
  {
    "text": "Let's save and run the application.",
    "start": "127345",
    "end": "129000"
  },
  {
    "text": "The application has finished running \nand we can see output streams listed.",
    "start": "132041",
    "end": "135120"
  },
  {
    "text": "Let's look at output for \nthe trip statistics stream.",
    "start": "135508",
    "end": "137907"
  },
  {
    "text": "Every two seconds, a SQL query \nruns that gives the total trips,",
    "start": "139837",
    "end": "143037"
  },
  {
    "text": "passenger count, and amount spent for trips. ",
    "start": "143037",
    "end": "145050"
  },
  {
    "text": "Let’s return to our outside-in data movement \npattern and move data into a data lake.",
    "start": "146166",
    "end": "150240"
  },
  {
    "text": "We’ll begin in Amazon Simple \nStorage Service (Amazon S3).",
    "start": "151043",
    "end": "154573"
  },
  {
    "text": "Amazon S3 is organized into buckets \nthat receive data from Amazon EC2.",
    "start": "156483",
    "end": "160702"
  },
  {
    "text": "We can run queries of our Amazon EC2 data \n(the outside) from Amazon S3 (the inside).",
    "start": "161348",
    "end": "166628"
  },
  {
    "text": "The NYC taxi trips bucket has been created \nfor our initials-taxi-trips data stream.",
    "start": "168434",
    "end": "172754"
  },
  {
    "text": "The bucket is organized into folders for the \nyear, month, day, and time when data is collected.",
    "start": "173597",
    "end": "178077"
  },
  {
    "text": "The data objects are stored \nin Apache Parquet files.",
    "start": "182699",
    "end": "185419"
  },
  {
    "text": "We can query an individual file with S3 Select.",
    "start": "186120",
    "end": "188942"
  },
  {
    "text": "We can choose our output format.",
    "start": "191944",
    "end": "193559"
  },
  {
    "text": "Here we see the first five records of input data from\nour query to Amazon EC2 from our Amazon S3 data lake.",
    "start": "200585",
    "end": "206397"
  },
  {
    "start": "207000",
    "end": "292000"
  },
  {
    "text": "The third data movement in our \narchitecture is from the inside out.",
    "start": "207622",
    "end": "210754"
  },
  {
    "text": "This pattern is used to send selected material \nfrom the data lake to outside systems,",
    "start": "211456",
    "end": "215569"
  },
  {
    "text": "like a data warehouse or database, to be analyzed.",
    "start": "215569",
    "end": "218107"
  },
  {
    "text": "We’ll use AWS Glue, a serverless data integration service,\nto prepare our data and send it to Amazon Redshift.",
    "start": "219000",
    "end": "225313"
  },
  {
    "text": "We have already created a crawler we can \nrun to catalog the latest data in Amazon S3.",
    "start": "227268",
    "end": "231588"
  },
  {
    "text": "Let’s open the NYC taxi trips database table to confirm\nthat the crawler connected to our data lake.",
    "start": "237382",
    "end": "242265"
  },
  {
    "text": "Here we can see our schema definitions.",
    "start": "244213",
    "end": "245944"
  },
  {
    "text": "The table is ready to use.",
    "start": "246416",
    "end": "247834"
  },
  {
    "text": "Now we'll use Amazon Glue to send the data in real time\nto Amazon Redshift via our Amazon S3 data bucket.",
    "start": "250342",
    "end": "256044"
  },
  {
    "text": "Let's open the Amazon \nRedshift console in a new tab.",
    "start": "256911",
    "end": "259502"
  },
  {
    "text": "For demonstration purposes, we've already \nconstructed a cluster for Redshift.",
    "start": "265206",
    "end": "268886"
  },
  {
    "text": "Returning to AWS Glue, we’ll use a connection \nwe already created to integrate with Redshift.",
    "start": "270572",
    "end": "275371"
  },
  {
    "text": "Let's test the connection.",
    "start": "277720",
    "end": "278928"
  },
  {
    "text": "Our connection between Glue and Redshift \nhas been successfully established.",
    "start": "287800",
    "end": "291194"
  },
  {
    "start": "292000",
    "end": "348000"
  },
  {
    "text": "Next, we'll build an extract, transform, and load (ETL) job\nin AWS Glue Studio that will load Glue data into Redshift.",
    "start": "292492",
    "end": "299544"
  },
  {
    "text": "For demonstration purposes, let’s copy an \nexisting job and clone it to run it in real-time.",
    "start": "301278",
    "end": "305758"
  },
  {
    "text": "Here we see the structure of our new job.",
    "start": "308034",
    "end": "310034"
  },
  {
    "text": "We'll leave the data source as our Amazon S3 \ndata bucket and retain the default properties.",
    "start": "311056",
    "end": "315296"
  },
  {
    "text": "We'll also keep the default settings \nin the Transform and Data target nodes.",
    "start": "316896",
    "end": "320496"
  },
  {
    "text": "Let's review the job details \nand then save and run the job.",
    "start": "323065",
    "end": "325865"
  },
  {
    "text": "As the job is running, we can monitor its status.",
    "start": "331465",
    "end": "333785"
  },
  {
    "text": "Our job run has a 100% success rate.",
    "start": "337098",
    "end": "339505"
  },
  {
    "text": "This means we successfully transformed and loaded data from the Amazon S3\ndata bucket to our Amazon Redshift catalog using Amazon Glue.",
    "start": "339959",
    "end": "346877"
  },
  {
    "start": "348000",
    "end": "393000"
  },
  {
    "text": "Now that our job is running, let’s see how an end user\ncan query the data using the Redshift query editor.",
    "start": "348291",
    "end": "353224"
  },
  {
    "text": "From the query editor, we'll connect to \nour database and select a Redshift schema.",
    "start": "354317",
    "end": "358156"
  },
  {
    "text": "Let's view the table that appears, which is \nloading the data schema in the column groups.",
    "start": "363616",
    "end": "367478"
  },
  {
    "text": "When we query the table for a count, we'll \nget an output from the Amazon Redshift job.",
    "start": "368203",
    "end": "372043"
  },
  {
    "text": "Here's our query result, which confirms that we have\nestablished the inside-out data movement architecture.",
    "start": "372788",
    "end": "377908"
  },
  {
    "text": "You've just seen how you can stream analytics \nusing a modern data architecture from AWS.",
    "start": "379739",
    "end": "384244"
  },
  {
    "text": "You can learn more about this topic in \nthe description and links for this video.",
    "start": "385686",
    "end": "388901"
  },
  {
    "text": "Thanks for watching. Now it’s your turn to try.",
    "start": "389179",
    "end": "391124"
  }
]