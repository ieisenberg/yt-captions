[
  {
    "start": "0",
    "end": "17000"
  },
  {
    "text": "all right can everyone hear me good so as Edgar said my name is Randi",
    "start": "1070",
    "end": "6690"
  },
  {
    "text": "Ridgely and I'm here with with Mike and Ralph from pnnl we're gonna talk a lot about Kinesis today I'm not just Kinesis",
    "start": "6690",
    "end": "13500"
  },
  {
    "text": "analytics but essentially three of the four parts to Kinesis if we just take a",
    "start": "13500",
    "end": "19260"
  },
  {
    "start": "17000",
    "end": "17000"
  },
  {
    "text": "look at the agenda really quickly we're going to talk about the characteristics of streaming data we're going to dive",
    "start": "19260",
    "end": "25140"
  },
  {
    "text": "into what the Kinesis service suite looks like and then I'll bring pnnl up to discuss how they're using Kinesis to",
    "start": "25140",
    "end": "31800"
  },
  {
    "text": "ingest data into their data like just a really quick show of hands how many people are using real-time streaming",
    "start": "31800",
    "end": "38520"
  },
  {
    "text": "today whether it's Kinesis kafka other streaming services okay good good number",
    "start": "38520",
    "end": "47070"
  },
  {
    "text": "of people so this should be a really good session for not only an overview and introduction of our real-time",
    "start": "47070",
    "end": "53250"
  },
  {
    "text": "streaming services but going into some of the more advanced features that are available so when we talk about",
    "start": "53250",
    "end": "60120"
  },
  {
    "start": "58000",
    "end": "58000"
  },
  {
    "text": "real-time streaming we're really talking about the pace of the data and what I mean by that is you know traditionally",
    "start": "60120",
    "end": "66659"
  },
  {
    "text": "you know when you're doing batch processing you're periodically pulling some data",
    "start": "66659",
    "end": "71760"
  },
  {
    "text": "maybe you're waking a service up every hour to grab some logs push it into some",
    "start": "71760",
    "end": "77400"
  },
  {
    "text": "service and then run some analysis on it from there you know typically those",
    "start": "77400",
    "end": "82770"
  },
  {
    "text": "files are very large files that you're pulling because there's such a long period of time on returning that data in",
    "start": "82770",
    "end": "89340"
  },
  {
    "text": "real time streaming you're gonna have a very large volume of small messages",
    "start": "89340",
    "end": "94500"
  },
  {
    "text": "funneling through a service to do oh do the processing on that so so when you",
    "start": "94500",
    "end": "101070"
  },
  {
    "text": "start thinking about what are the differences between batch processing and stream processing it's really the",
    "start": "101070",
    "end": "107490"
  },
  {
    "text": "continuous processing of data in real time streaming versus the periodic processing of larger data as I'm going",
    "start": "107490",
    "end": "114750"
  },
  {
    "text": "through you can see a couple of examples up here you might be processing server logs you know you want to do that in",
    "start": "114750",
    "end": "122610"
  },
  {
    "text": "real time you're going to be processing those events right away not waiting an hour to see what's in those logs and",
    "start": "122610",
    "end": "128910"
  },
  {
    "text": "then react to that and that kind of takes me to this slide here where you know the the",
    "start": "128910",
    "end": "134260"
  },
  {
    "start": "131000",
    "end": "131000"
  },
  {
    "text": "value of data can decrease over time depending on how you're leveraging that data and what I mean by that is you know",
    "start": "134260",
    "end": "141519"
  },
  {
    "text": "all the way on the right there that historical data still has a lot of value for a lot of the traditional analytics",
    "start": "141519",
    "end": "147670"
  },
  {
    "text": "you would run on that but if you're trying to do things like fraud detection or you're trying to you know process",
    "start": "147670",
    "end": "154390"
  },
  {
    "text": "clickstream data in real time to react back to the the browser or the end user",
    "start": "154390",
    "end": "159940"
  },
  {
    "text": "you really want to be processing that in milliseconds seconds to minutes not into",
    "start": "159940",
    "end": "165459"
  },
  {
    "text": "those hour month and day ranges so over time that data quickly decreases so this",
    "start": "165459",
    "end": "171579"
  },
  {
    "text": "is where streaming concepts really come into play and Kinesis itself is really the the driver from an AWS perspective",
    "start": "171579",
    "end": "179519"
  },
  {
    "start": "179000",
    "end": "179000"
  },
  {
    "text": "some of the common use cases for this streaming into data Lakes which we'll be talking about a little later here IOT",
    "start": "179519",
    "end": "187389"
  },
  {
    "text": "analytics maybe you want to capture information on you know temperature sensors or flow meters for water flowing",
    "start": "187389",
    "end": "195819"
  },
  {
    "text": "through systems and react to that in real time you know especially from like a flow meter or temperature perspective",
    "start": "195819",
    "end": "202449"
  },
  {
    "text": "you want to know when something's going wrong right away not wait an hour or a week or a month later and go I had a",
    "start": "202449",
    "end": "209049"
  },
  {
    "text": "leak I probably should have reacted to that faster smart cities you know being able to you know build intelligence into",
    "start": "209049",
    "end": "216910"
  },
  {
    "text": "all of your devices and be able to provide feedback to the backend of process it becomes key over time where",
    "start": "216910",
    "end": "223959"
  },
  {
    "text": "you really want to do that as fast as possible not not waiting for a later date so looking at the Kinesis suite of",
    "start": "223959",
    "end": "233109"
  },
  {
    "start": "228000",
    "end": "228000"
  },
  {
    "text": "services we actually have four four main categories of services today I'm going to talk about the first three here we",
    "start": "233109",
    "end": "239799"
  },
  {
    "text": "have Kinesis data streams which allows you to have data consumers and data",
    "start": "239799",
    "end": "244840"
  },
  {
    "text": "producers set you know data producers sending data to these streams for",
    "start": "244840",
    "end": "250540"
  },
  {
    "text": "consumers to pick up in process we have Kinesis data firehose which is going to",
    "start": "250540",
    "end": "256000"
  },
  {
    "text": "allow you to descend data as fast as possible and then buffer data to push into a persistent data store we have",
    "start": "256000",
    "end": "263139"
  },
  {
    "text": "Kinesis data analytics which allows you to tap into those streams and run sequel queries against that data in",
    "start": "263139",
    "end": "269750"
  },
  {
    "text": "real time to provide aggregates and in statistical analysis of that data and then finally we do have Kinesis video",
    "start": "269750",
    "end": "276770"
  },
  {
    "text": "streams where you can actually capture video data in real time and process that",
    "start": "276770",
    "end": "283360"
  },
  {
    "start": "283000",
    "end": "283000"
  },
  {
    "text": "so if we talk about Kinesis data streams it's really easy to get started you know",
    "start": "284050",
    "end": "292520"
  },
  {
    "text": "anybody that's that's taking a look at this before what you'll do is you'll go into the council or the sdk you define a",
    "start": "292520",
    "end": "299210"
  },
  {
    "text": "Kinesis data stream with a name and you'll give it a number of shards that you want for that particular stream and",
    "start": "299210",
    "end": "306169"
  },
  {
    "text": "that sharding is is what's going to determine how much volume that particular stream can take it's highly",
    "start": "306169",
    "end": "313789"
  },
  {
    "text": "durable it's it's across multiple availability zones within the region so",
    "start": "313789",
    "end": "319280"
  },
  {
    "text": "you have that durability and then you you're gonna have data consumers on the back end of that to pick up those",
    "start": "319280",
    "end": "325850"
  },
  {
    "text": "messages and process them as need be we have a number of tools and there's a number of open source tools for both",
    "start": "325850",
    "end": "332479"
  },
  {
    "text": "ends of that that that really help you move along in in the process so if we",
    "start": "332479",
    "end": "339560"
  },
  {
    "start": "338000",
    "end": "338000"
  },
  {
    "text": "look at some of those producers if we take a look at the left-hand side here you know we do have you know many ways",
    "start": "339560",
    "end": "345169"
  },
  {
    "text": "we have the AWS SDK and the CL is you can do direct puts against that stream",
    "start": "345169",
    "end": "350419"
  },
  {
    "text": "with the payloads that you want to send we have a Kinesis producer library that's going to do some aggregates on",
    "start": "350419",
    "end": "356990"
  },
  {
    "text": "the the on the service side to package up multiple messages in a highly",
    "start": "356990",
    "end": "363199"
  },
  {
    "text": "performant manner to compress that data and send to the stream to really optimize the usage of that there's a lot",
    "start": "363199",
    "end": "369410"
  },
  {
    "text": "of open source tools like fluent D or flume that have adapters to connect to",
    "start": "369410",
    "end": "374479"
  },
  {
    "text": "Kinesis streams and send that data to you right away that fits within the patterns that you may have already out",
    "start": "374479",
    "end": "380330"
  },
  {
    "text": "there and then finally one of the other pieces we have is that we have a Kinesis agent if you don't know about the",
    "start": "380330",
    "end": "386810"
  },
  {
    "text": "Kinesis agent it's a great tool that that is an agent you would install on your services or on your ec2 instances",
    "start": "386810",
    "end": "393949"
  },
  {
    "text": "that can pick up and aggregate the data and send it off to that Kinesis stream",
    "start": "393949",
    "end": "399669"
  },
  {
    "text": "as you can it all goes into the stream and then there's a number of consumers again AWS",
    "start": "399669",
    "end": "405560"
  },
  {
    "text": "SDK CLI the API is to leverage that data we have a Kinesis client library this",
    "start": "405560",
    "end": "411980"
  },
  {
    "text": "client library allows you to build applications and deploy them onto ec2 instances and when you deploy them on",
    "start": "411980",
    "end": "418850"
  },
  {
    "text": "those ec2 instances it can scale out how many consumers you need to process that",
    "start": "418850",
    "end": "424310"
  },
  {
    "text": "message those messages on the stream you can leverage lambda so if you're looking",
    "start": "424310",
    "end": "430280"
  },
  {
    "text": "down the road of you know I want to build out a server less architecture and to end for my you know whatever workload",
    "start": "430280",
    "end": "437420"
  },
  {
    "text": "that I'm working on you can leverage AWS lambda and pull the data from the",
    "start": "437420",
    "end": "443060"
  },
  {
    "text": "Kinesis streams and process that in real time as well and then there's a few open source technologies that are available",
    "start": "443060",
    "end": "449000"
  },
  {
    "text": "to you you can leverage apache storm and set it up and process that data or what",
    "start": "449000",
    "end": "455510"
  },
  {
    "text": "a lot of people have been leveraging and it is one of the more popular frameworks out there is utilizing apache spark to",
    "start": "455510",
    "end": "462440"
  },
  {
    "text": "be able to pull the data off process it and push it into the places that that you're looking for it to go so Kinesis",
    "start": "462440",
    "end": "475340"
  },
  {
    "start": "472000",
    "end": "472000"
  },
  {
    "text": "data streams was the first service that we provided about four ish or so years ago and as we were working with",
    "start": "475340",
    "end": "482510"
  },
  {
    "text": "customers we found they were using Kinesis data streams in the biggest use case they were doing was they wanted to",
    "start": "482510",
    "end": "489350"
  },
  {
    "text": "process data and be able to handle the volume of data coming through whether it was click stream or iot data and just",
    "start": "489350",
    "end": "496430"
  },
  {
    "text": "get it into a persistent store so that somebody could process that data after",
    "start": "496430",
    "end": "501470"
  },
  {
    "text": "the fact and so what we built was Kinesis data firehose this with this service it's 0 administration and it'll",
    "start": "501470",
    "end": "509420"
  },
  {
    "text": "scale to the volume of data that you're pushing to it and what you end up doing is picking one of four sources of",
    "start": "509420",
    "end": "516349"
  },
  {
    "text": "persistent stores today you can say I want to I want to send data to s3 and",
    "start": "516350",
    "end": "522560"
  },
  {
    "text": "get it in there as fast as possible I want to push it into my redshift data warehouse for my bi folks or or people",
    "start": "522560",
    "end": "529730"
  },
  {
    "text": "doing visualizations off of that data push it into elasticsearch then one service that we have that isn't",
    "start": "529730",
    "end": "536500"
  },
  {
    "text": "part of the AWS ecosystem but one of our partners because of the popularity of",
    "start": "536500",
    "end": "541510"
  },
  {
    "text": "Splunk we have an adapter for pushing data into Splunk as well the nice thing about Kinesis data firehose is you've",
    "start": "541510",
    "end": "549220"
  },
  {
    "text": "got a couple of lovers and what to do buffering and what i mean by that is as",
    "start": "549220",
    "end": "554410"
  },
  {
    "text": "you're pushing that those individual small packets of data into this Kinesis",
    "start": "554410",
    "end": "559960"
  },
  {
    "text": "data firehose you want to be able to aggregate those and push them into s3 so",
    "start": "559960",
    "end": "565000"
  },
  {
    "text": "that you can utilize some of the strengths of the the the analytics engines on the back end there so the two",
    "start": "565000",
    "end": "570940"
  },
  {
    "text": "lovers that you have for Kinesis data firehose is i want to stream data up to",
    "start": "570940",
    "end": "576010"
  },
  {
    "text": "say 15 minutes by time or by size I want a hundred Meg files before I persist",
    "start": "576010",
    "end": "583510"
  },
  {
    "text": "that data out to that data store so what you'll have for example with s3 you may",
    "start": "583510",
    "end": "589060"
  },
  {
    "text": "you may allow some latency within the streaming portion of your data but you",
    "start": "589060",
    "end": "594970"
  },
  {
    "text": "want to bulk up and push larger files to s3 you might say I want every hour to",
    "start": "594970",
    "end": "601420"
  },
  {
    "text": "push out that data to that persistent store AWS is going to take care of the durability of that firehose while it's",
    "start": "601420",
    "end": "608440"
  },
  {
    "text": "up and running it's a lot it's it's serverless and it scales elastically",
    "start": "608440",
    "end": "614380"
  },
  {
    "text": "with the the load that you have one side note that I'll put in here there's actually two features I'd like to talk",
    "start": "614380",
    "end": "620680"
  },
  {
    "text": "about that are relatively new this year you have lambda functions that you can",
    "start": "620680",
    "end": "626200"
  },
  {
    "text": "apply to do transformations of that data before it goes to that persistent store",
    "start": "626200",
    "end": "631240"
  },
  {
    "text": "and then just about about a month ago we announced pushing and transforming that",
    "start": "631240",
    "end": "638710"
  },
  {
    "text": "raw data which might be CSV files XML JSON into park' format and registering",
    "start": "638710",
    "end": "645370"
  },
  {
    "text": "it with our Glu data catalog so essentially making it a one-stop shop to get your data in as fast as possible get",
    "start": "645370",
    "end": "652690"
  },
  {
    "text": "it into that columnar format in your data Lake and be able to run processing with multiple analytics engines on the",
    "start": "652690",
    "end": "658990"
  },
  {
    "text": "backend and you know in real time and as as fast as possible the next service",
    "start": "658990",
    "end": "665710"
  },
  {
    "start": "664000",
    "end": "664000"
  },
  {
    "text": "which I'm going to dive into a little deeper here just - you know let people see how easy it is to build these aggregates out is is",
    "start": "665710",
    "end": "672730"
  },
  {
    "text": "leveraging Kinesis data analytics so with Kinesis data analytics essentially",
    "start": "672730",
    "end": "677980"
  },
  {
    "text": "what you're doing is tapping into those Kinesis data streams or the Kinesis data",
    "start": "677980",
    "end": "683709"
  },
  {
    "text": "firehose in real time while the data is going through there and acting on it",
    "start": "683709",
    "end": "689019"
  },
  {
    "text": "leveraging standard and z sequel it's it's scales itself as well - the the the",
    "start": "689019",
    "end": "696130"
  },
  {
    "text": "volume of data that you're pushing to it and it's very easy to get set up and started there's really three steps that",
    "start": "696130",
    "end": "704500"
  },
  {
    "start": "701000",
    "end": "701000"
  },
  {
    "text": "you'll have with creating a Kinesis data analytics application the first thing is",
    "start": "704500",
    "end": "710380"
  },
  {
    "text": "you're going to have your your source input which is one of those Kinesis data streams or data firehoses",
    "start": "710380",
    "end": "717630"
  },
  {
    "text": "you're going to write some sequel code that allows you to ingest that data off",
    "start": "717630",
    "end": "723100"
  },
  {
    "text": "of that stream and then start running your transformations in there and then",
    "start": "723100",
    "end": "729009"
  },
  {
    "text": "you're going to continuously deliver those results to another Kinesis data stream or firehose to be able to use",
    "start": "729009",
    "end": "734769"
  },
  {
    "text": "downstream somewhere else so what you'll see is you can start chaining these services together to build a really",
    "start": "734769",
    "end": "741639"
  },
  {
    "text": "in-depth rich pipeline for the data flowing through your systems what",
    "start": "741639",
    "end": "752079"
  },
  {
    "start": "748000",
    "end": "748000"
  },
  {
    "text": "happens with Kinesis data analytics is as i mentioned you have a stream a data",
    "start": "752079",
    "end": "759550"
  },
  {
    "text": "stream or data firehose that you're connecting to if the data say for",
    "start": "759550",
    "end": "764920"
  },
  {
    "text": "example is CSV data or JSON data a number of formats that we support what",
    "start": "764920",
    "end": "770470"
  },
  {
    "text": "is actually happening you know under the covers is it's converting that data into",
    "start": "770470",
    "end": "775569"
  },
  {
    "text": "a table like format for you to then build those sequel queries off of and it's it's very familiar to people that",
    "start": "775569",
    "end": "782470"
  },
  {
    "text": "are comfortable working in sequel and at that point it supports a number of functions statistical functions nested",
    "start": "782470",
    "end": "789910"
  },
  {
    "text": "nested data within the the JSON structure allowing you to then build out",
    "start": "789910",
    "end": "794980"
  },
  {
    "text": "multiple step processes of processing that sequel data all of this happens",
    "start": "794980",
    "end": "801889"
  },
  {
    "start": "799000",
    "end": "799000"
  },
  {
    "text": "sub-second and to end depending on how in-depth those queries are and the types",
    "start": "801889",
    "end": "806929"
  },
  {
    "text": "of aggregates you're doing for example maybe you have a click stream coming in off of your website and you want to know",
    "start": "806929",
    "end": "813829"
  },
  {
    "text": "how many active users are on the site at any given time you can tap in this Kinesis analytics service and do some of",
    "start": "813829",
    "end": "823519"
  },
  {
    "text": "those summations or aggregates based on the data coming in in real time and",
    "start": "823519",
    "end": "828649"
  },
  {
    "text": "produce those outputs to another stream there's pre-built functions for all of",
    "start": "828649",
    "end": "834769"
  },
  {
    "text": "that and there's also there I'm going to touch on a little bit and pnn now it's going to go into some more detail on this as well is there's some built in",
    "start": "834769",
    "end": "841429"
  },
  {
    "text": "machine learning functions that you can leverage with Kinesis analytics so you don't necessarily have to have that",
    "start": "841429",
    "end": "847339"
  },
  {
    "text": "background of understanding of the the deep knowledge required to build these algorithms you get the ability to",
    "start": "847339",
    "end": "853999"
  },
  {
    "text": "leverage them for things like fraud detection or sorry anomaly detection",
    "start": "853999",
    "end": "859329"
  },
  {
    "text": "hotspot detection and things of that nature so the way you do that is there's",
    "start": "859329",
    "end": "869149"
  },
  {
    "start": "864000",
    "end": "864000"
  },
  {
    "text": "two main concepts with Kinesis analytics you're going to create a stream and that stream is that stream of data that you",
    "start": "869149",
    "end": "876350"
  },
  {
    "text": "have funneling in that comes up in table format so in this example here you can see we're creating a table of the number",
    "start": "876350",
    "end": "883610"
  },
  {
    "text": "of calls per IP coming through that stream it's got very familiar sequel for",
    "start": "883610",
    "end": "889220"
  },
  {
    "text": "you you've got the data types that you can leverage and what that's going to do is create an in-memory table to start",
    "start": "889220",
    "end": "896029"
  },
  {
    "text": "funneling that data into the second piece to that is you're going to have",
    "start": "896029",
    "end": "901910"
  },
  {
    "text": "what we call a pump and what the pump does is does the actual insert",
    "start": "901910",
    "end": "907309"
  },
  {
    "text": "information so that you create these select statements of the data you want to ingest into that stream that you've",
    "start": "907309",
    "end": "914299"
  },
  {
    "text": "defined and it's going to do that in real time and aggregate all of that data",
    "start": "914299",
    "end": "919579"
  },
  {
    "text": "up so so then you know things like doing a count on the number of distinct users",
    "start": "919579",
    "end": "925879"
  },
  {
    "text": "that are currently in this 5-minute window we can process that day to see that data in real time and report back",
    "start": "925879",
    "end": "933079"
  },
  {
    "text": "or utilize that data other systems and the way we do that is",
    "start": "933079",
    "end": "939050"
  },
  {
    "start": "937000",
    "end": "937000"
  },
  {
    "text": "through defining windows of time when you know so traditionally with standard",
    "start": "939050",
    "end": "944810"
  },
  {
    "text": "sequel you already know what the size of the data is within my tables because I have that structure defined because the",
    "start": "944810",
    "end": "952130"
  },
  {
    "text": "data is real-time streaming it's constantly changing so you have to put some bounds around",
    "start": "952130",
    "end": "957800"
  },
  {
    "text": "how much data do I want to ingest in process in a particular time frame the",
    "start": "957800",
    "end": "963950"
  },
  {
    "text": "way you do that is with these windows there's a few types of windows and then that are built in and you can also",
    "start": "963950",
    "end": "970040"
  },
  {
    "text": "create custom windows the first window is a sliding window so think of that as",
    "start": "970040",
    "end": "975230"
  },
  {
    "text": "I may have a 10 minute sliding window that I'm working off of this window slides are all you know every 10 minutes",
    "start": "975230",
    "end": "983150"
  },
  {
    "text": "so there'll be some overlap of data between the windows that you may need to",
    "start": "983150",
    "end": "988490"
  },
  {
    "text": "work with the next one is our tumbling windows these windows are distinct sets",
    "start": "988490",
    "end": "994070"
  },
  {
    "text": "of time that you define that tumble over each other that won't have overlapping",
    "start": "994070",
    "end": "999560"
  },
  {
    "text": "data coming in across those windows so depending on the the implementation that",
    "start": "999560",
    "end": "1005650"
  },
  {
    "text": "you're looking for will help you determine what types of windows you want to leverage you can create session",
    "start": "1005650",
    "end": "1011860"
  },
  {
    "text": "windows so that you have a window of time per those sessions that you may be defining within your clickstream",
    "start": "1011860",
    "end": "1018610"
  },
  {
    "text": "environments and then as I mentioned you can create some custom windows as well and then finally the last thing that I",
    "start": "1018610",
    "end": "1027880"
  },
  {
    "start": "1024000",
    "end": "1024000"
  },
  {
    "text": "want to talk about is we have these built-in algorithms machine learning",
    "start": "1027880",
    "end": "1033160"
  },
  {
    "text": "algorithms for you to leverage day one the great thing about them is is again",
    "start": "1033160",
    "end": "1038890"
  },
  {
    "text": "like I mentioned you don't have to have that deep understanding as long as you understand how to utilize those types of",
    "start": "1038890",
    "end": "1045310"
  },
  {
    "text": "algorithms you can leverage them in your Kinesis analytics applications the three",
    "start": "1045310",
    "end": "1051280"
  },
  {
    "text": "that we're calling out here are the anomaly detection so there's certain parameters that you would pass in when",
    "start": "1051280",
    "end": "1056740"
  },
  {
    "text": "you're defining that select statement to this algorithm to determine the window",
    "start": "1056740",
    "end": "1061990"
  },
  {
    "text": "size that I want to leverage as well as some of the metrics for the parameters",
    "start": "1061990",
    "end": "1067240"
  },
  {
    "text": "around you that algorithm and with anomaly detection you you'll be able to see",
    "start": "1067240",
    "end": "1073360"
  },
  {
    "text": "those those unique spikes and be able to report on those in real time as opposed",
    "start": "1073360",
    "end": "1078490"
  },
  {
    "text": "to trying to process that data later and go oh I did have something that looks odd here I want to report on that now",
    "start": "1078490",
    "end": "1085720"
  },
  {
    "text": "not later and then we also have hotspot detection so if you're doing things like a bunch of IOT sensors or temperature",
    "start": "1085720",
    "end": "1093790"
  },
  {
    "text": "sensors and things yeah you've got a large data collection in a particular",
    "start": "1093790",
    "end": "1099580"
  },
  {
    "text": "group you may want to report on that and be able to provide value back to downstream systems to let them know all",
    "start": "1099580",
    "end": "1107980"
  },
  {
    "text": "of these machine learning algorithms are unsupervised if you're familiar with the concepts of machine learning so what",
    "start": "1107980",
    "end": "1113650"
  },
  {
    "text": "happens is they're going to learn over time based on the data that you're feeding it so it is really critical to",
    "start": "1113650",
    "end": "1119530"
  },
  {
    "text": "understand your data but then be able to feed that algorithm appropriately they're all adaptive and they're all",
    "start": "1119530",
    "end": "1125980"
  },
  {
    "text": "done in real time so with that I'm going",
    "start": "1125980",
    "end": "1130990"
  },
  {
    "text": "to bring up the Ralphie going first Ralph who's going to talk a little bit about ingesting data into his data Lake",
    "start": "1130990",
    "end": "1137940"
  },
  {
    "text": "leveraging Kinesis thank you thank you",
    "start": "1137940",
    "end": "1145540"
  },
  {
    "text": "ready like Randy said my name is Ralph purko and I'm with my garden Ellie were from Pacific Northwest National Lab",
    "start": "1145540",
    "end": "1151660"
  },
  {
    "text": "we're both software engineers and we're been out on AWS for several years now more and more we continue to move",
    "start": "1151660",
    "end": "1158559"
  },
  {
    "text": "towards a serverless environment so we can talk a little bit about how we've implemented a serverless data Lake in",
    "start": "1158559",
    "end": "1165130"
  },
  {
    "text": "addition to how we're using various aspects of Kinesis and then also we'll",
    "start": "1165130",
    "end": "1170260"
  },
  {
    "text": "follow up with what we're doing next is regarding serverless and in our path forward there so I thought I'd start",
    "start": "1170260",
    "end": "1177730"
  },
  {
    "start": "1176000",
    "end": "1176000"
  },
  {
    "text": "with setting a base definition for a data Lake you know data Lake is a buzzword you see it advertised in",
    "start": "1177730",
    "end": "1184809"
  },
  {
    "text": "various places so for the sake of this presentation we'll call a data Lake in architecture that allows you to collect",
    "start": "1184809",
    "end": "1190270"
  },
  {
    "text": "store process and analyze consume all data that flows into your organization",
    "start": "1190270",
    "end": "1195970"
  },
  {
    "text": "now that's pretty general below it a more practical definition perhaps",
    "start": "1195970",
    "end": "1201700"
  },
  {
    "text": "it's a collection of technologies services and best practices around a large repository of raw data I think",
    "start": "1201700",
    "end": "1208240"
  },
  {
    "text": "that raw data part is the important piece to call out here because if you think about this you're like well why",
    "start": "1208240",
    "end": "1214539"
  },
  {
    "text": "don't you just use a database or something to store all your data and and the reason is is because we you have this raw data coming in one of the core",
    "start": "1214539",
    "end": "1221350"
  },
  {
    "text": "tenants of a data Lake is the raw data is coming in and you never get rid of it you store it as is because you never",
    "start": "1221350",
    "end": "1226929"
  },
  {
    "text": "know when you're going to need it and that's why it's usually associated with something like a hadoop system originally at least where you have",
    "start": "1226929",
    "end": "1232779"
  },
  {
    "text": "distributed file system and now the next evolution of the data Lake is out on the",
    "start": "1232779",
    "end": "1238360"
  },
  {
    "text": "out on the cloud AWS s3 is a fantastic candidate for that so what does a data",
    "start": "1238360",
    "end": "1246669"
  },
  {
    "text": "like do exactly right so there's some core core features that you need to have in a data Lake you need to be able to",
    "start": "1246669",
    "end": "1251950"
  },
  {
    "text": "get the data into the data Lake ingest the data you need to be able to transform it so even though you're saving that raw data you you're",
    "start": "1251950",
    "end": "1258159"
  },
  {
    "text": "eventually going to want to do something with that data so you're gonna want to transform it into some other format and make it more usable you want to be able",
    "start": "1258159",
    "end": "1264309"
  },
  {
    "text": "to analyze the data that's why you have it in there correlate it with other data sources and whatnot store that data or",
    "start": "1264309",
    "end": "1270760"
  },
  {
    "text": "archive that data that's where s3 again is just fantastic because with the lifecycle rules you can just have this",
    "start": "1270760",
    "end": "1275980"
  },
  {
    "text": "stuff roll off in the glacier I think our rules are set up by default to 90 days then extract the data you want to",
    "start": "1275980",
    "end": "1282190"
  },
  {
    "text": "be able get the data out and that can be something literally you know somebody downloading the data to pushing that into something like an enterprise data",
    "start": "1282190",
    "end": "1288429"
  },
  {
    "text": "warehouse or a relational database elasticsearch something like that skipping replay I'll come back to that",
    "start": "1288429",
    "end": "1294279"
  },
  {
    "text": "and then got data provenance data security other enterprise features you know based on what you need and then",
    "start": "1294279",
    "end": "1300669"
  },
  {
    "text": "going back to the replay data that's something that's that we take advantage of quite a bit and what I mean by replaying data is the ability to take",
    "start": "1300669",
    "end": "1307240"
  },
  {
    "text": "some well-defined data set that is around an event or a time frame or whatnot and make that data available",
    "start": "1307240",
    "end": "1313139"
  },
  {
    "text": "particularly to researchers who might be working on a particular algorithm doing some data science and they need some notion of ground truth to be able to",
    "start": "1313139",
    "end": "1319330"
  },
  {
    "text": "test the work that they're doing and we take advantage of that building that capability in so what do we use our data",
    "start": "1319330",
    "end": "1326889"
  },
  {
    "text": "Lake for first it's a centralized data repository we have a lot of different",
    "start": "1326889",
    "end": "1331929"
  },
  {
    "text": "disparate data sources that we have and there's some examples there of course data science and analytics having",
    "start": "1331929",
    "end": "1339040"
  },
  {
    "text": "the data in s3 you have the entire suite of AWS services at your disposal then to interact with that data and that and",
    "start": "1339040",
    "end": "1345370"
  },
  {
    "text": "that's really powerful and then third is collaboration which is always near and dear to our hearts we're a National Lab",
    "start": "1345370",
    "end": "1351850"
  },
  {
    "text": "we do a lot of collaborative work internally we're collaborating with researchers and other engineers and then",
    "start": "1351850",
    "end": "1357400"
  },
  {
    "text": "also with our government sponsors or customers and external partners like universities or other national labs so",
    "start": "1357400",
    "end": "1366400"
  },
  {
    "text": "what does our server list data like look like this picture here it's not ours it's uh from an AWS blog that I grabbed",
    "start": "1366400",
    "end": "1373900"
  },
  {
    "text": "this from this is our inspiration if you will so first a little bit of background we did have we do have a data lake internally we have several Hadoop",
    "start": "1373900",
    "end": "1380380"
  },
  {
    "text": "systems at the lab and one our program in particular it's I don't know 42 or 44",
    "start": "1380380",
    "end": "1385990"
  },
  {
    "text": "node cluster it's a decent size and so we'd already had a data lake internally we're already familiar with",
    "start": "1385990",
    "end": "1391480"
  },
  {
    "text": "the concepts and the best practices around that which translated very well out to out to the cloud but as more and",
    "start": "1391480",
    "end": "1399040"
  },
  {
    "text": "more of our data was out on AWS and we were realizing that wow it's an extra",
    "start": "1399040",
    "end": "1404320"
  },
  {
    "text": "step to continue to bring it in we dist and there's so much capability available we decided let's just go ahead and",
    "start": "1404320",
    "end": "1410980"
  },
  {
    "text": "formalize this a little more and make our make our data like external out on AWS now we do still have our internal",
    "start": "1410980",
    "end": "1417730"
  },
  {
    "text": "Hadoop cluster and we will continue to use that there's just some data for example that we just need to keep internal but by and by we we've all all",
    "start": "1417730",
    "end": "1427840"
  },
  {
    "text": "the data that we ingest is usually out on the cloud and so we have moved in that direction just talking through this",
    "start": "1427840",
    "end": "1433540"
  },
  {
    "text": "really quickly from left to right you know the data is coming in on those arrows you see those buckets and the",
    "start": "1433540",
    "end": "1439630"
  },
  {
    "text": "little spider icons right these blue web crawlers are being used to analyze the data and an s3 bucket making that data",
    "start": "1439630",
    "end": "1447250"
  },
  {
    "text": "then available on Athena and then you can use a variety of services to access that data and that that's pretty similar",
    "start": "1447250",
    "end": "1454090"
  },
  {
    "text": "to how we've set up ours so taking a quick look at the the technologies we",
    "start": "1454090",
    "end": "1460450"
  },
  {
    "text": "use in particular and what our implementation actually looks like just calling out there are some of the the main technology so",
    "start": "1460450",
    "end": "1467460"
  },
  {
    "text": "excuse me first on the in Jess you use a patching knife I quite a bit and also a",
    "start": "1467460",
    "end": "1474870"
  },
  {
    "text": "WS lambda so the one thing with data ingest and actually all these services it's very use case dependent there's not",
    "start": "1474870",
    "end": "1481470"
  },
  {
    "text": "a one-size-fits-all typically for a data Lake unless all your data is just coming from one source but when you have a lot",
    "start": "1481470",
    "end": "1487590"
  },
  {
    "text": "of disparate sources make it come in to a lot of unique situations so from",
    "start": "1487590",
    "end": "1493140"
  },
  {
    "text": "ingest perspective you know the first questions you need to ask yourself is what kind of control and access do you",
    "start": "1493140",
    "end": "1498390"
  },
  {
    "text": "have to the data do you control the data is the data are you pulling the data in or is it being pushed to you is it on",
    "start": "1498390",
    "end": "1505230"
  },
  {
    "text": "your network that somebody can do a DVD and you need to get it out there so these are all questions to consider when",
    "start": "1505230",
    "end": "1510539"
  },
  {
    "text": "you're thinking about how to get your data into your data Lake and depending on the situation we typically go with",
    "start": "1510539",
    "end": "1516419"
  },
  {
    "text": "either knife I or lambda from there we push our data on to a messaging cube",
    "start": "1516419",
    "end": "1522240"
  },
  {
    "text": "system typically either Kinesis or sqs we also use SNS as well internally most",
    "start": "1522240",
    "end": "1528809"
  },
  {
    "text": "of our systems are message driven our architectures that keeps them decoupled",
    "start": "1528809",
    "end": "1534539"
  },
  {
    "text": "and then also makes them much much simpler to distribute and scale so we've found that as effective pattern for us",
    "start": "1534539",
    "end": "1540649"
  },
  {
    "text": "from an ETL perspective then we use AWS glue to do transformations from JSON",
    "start": "1540649",
    "end": "1547529"
  },
  {
    "text": "into park' quite a bit and although we'll talk about the fire hose and how",
    "start": "1547529",
    "end": "1552600"
  },
  {
    "text": "we use that as well to do that very same thing but the nice thing about glue is you can also write your own custom spark",
    "start": "1552600",
    "end": "1558029"
  },
  {
    "text": "jobs which we have done to take care of some different news nuances again Apache",
    "start": "1558029",
    "end": "1563220"
  },
  {
    "text": "knife I we have a suite of enrichments that we have written over the years and",
    "start": "1563220",
    "end": "1569390"
  },
  {
    "text": "we haven't completely ported all those to lambda although as as opportunities",
    "start": "1569390",
    "end": "1576149"
  },
  {
    "text": "come up we're moving more and more to the server list just so we don't have that image administrative overhead of a",
    "start": "1576149",
    "end": "1582990"
  },
  {
    "text": "server storage I have s3 here that is the foundation however you could there's there's other storage mechanisms",
    "start": "1582990",
    "end": "1589590"
  },
  {
    "text": "mechanisms we use as well the data is pushed oftentimes in RDS or elasticsearch or in",
    "start": "1589590",
    "end": "1595950"
  },
  {
    "text": "the redshift so there's a variety of technologies we use there but s3 is by and large the the",
    "start": "1595950",
    "end": "1601110"
  },
  {
    "text": "that we lay and then from the analytic side again just calling out some examples here Kinesis analytics lambda",
    "start": "1601110",
    "end": "1606929"
  },
  {
    "text": "Athena Athena is great that's really pretty much almost entirely replaced",
    "start": "1606929",
    "end": "1613549"
  },
  {
    "text": "internally our hive implementation the ability to use glue to crawl the data",
    "start": "1613549",
    "end": "1619620"
  },
  {
    "text": "and get a schema and be able to then access it quickly with a theme has just been huge and then Amazon EMR course",
    "start": "1619620",
    "end": "1626130"
  },
  {
    "text": "which is MapReduce you know you got who dupes part Zeppelin are you know whole",
    "start": "1626130",
    "end": "1631320"
  },
  {
    "text": "variety Python variety of tools that you can use out there so alright with that",
    "start": "1631320",
    "end": "1637080"
  },
  {
    "start": "1635000",
    "end": "1635000"
  },
  {
    "text": "I'm just going to walk through a dataflow really quick this is one we have right now and just discuss how",
    "start": "1637080",
    "end": "1642870"
  },
  {
    "text": "we're using some of these technologies so it says news feed it probably better to call an RSS feed it's uh not a lot of",
    "start": "1642870",
    "end": "1649890"
  },
  {
    "text": "it is news but we're also we also have read it another data that's coming in here and so let me just describe this",
    "start": "1649890",
    "end": "1656100"
  },
  {
    "text": "diagram really quick on the top we have an arrow going left to right that's the data as it's coming in it's flowing into",
    "start": "1656100",
    "end": "1661320"
  },
  {
    "text": "the system below we have a you see Amazon s3 and the data's being stored there and then the variety of services",
    "start": "1661320",
    "end": "1668100"
  },
  {
    "text": "that are available to you once the data is in there and then the arrows going down is the various places in the pipeline where we're pushing the data",
    "start": "1668100",
    "end": "1673830"
  },
  {
    "text": "into s3 so the data is coming in in this case they're RSS feeds so they're in XML",
    "start": "1673830",
    "end": "1679169"
  },
  {
    "text": "and we're ingesting it using Apache knife I and then trying to stay true to true to the tenants of a data Lake we",
    "start": "1679169",
    "end": "1686130"
  },
  {
    "text": "immediately push that data into s3 so that data is then available for future use",
    "start": "1686130",
    "end": "1691290"
  },
  {
    "text": "if we need it then we put that payload onto sqs again decoupling that",
    "start": "1691290",
    "end": "1696690"
  },
  {
    "text": "ingest from the the processing in case things go bad sigh go sideways",
    "start": "1696690",
    "end": "1702960"
  },
  {
    "text": "downstream by separating your ingest from the process and even sure that your streaming data continues to flow in it's",
    "start": "1702960",
    "end": "1710580"
  },
  {
    "text": "at this point that we start applying some enrichments again using an Apache knife I and this is where we start taking advantage of Kinesis firehose to",
    "start": "1710580",
    "end": "1717840"
  },
  {
    "text": "push that data into s3 well it's really nice about Kinesis firehose and in this respect is that the way it puts the data",
    "start": "1717840",
    "end": "1724559"
  },
  {
    "text": "into s3 there's a really nice directory structure that partitions really well for you for use with Athena and the web",
    "start": "1724559",
    "end": "1731450"
  },
  {
    "text": "and we take full advantage of that that replaces some knife iflows that we have that we were doing that very thing so",
    "start": "1731450",
    "end": "1737659"
  },
  {
    "text": "now it's just one just one call into into the fire hose and then on onward to",
    "start": "1737659",
    "end": "1744200"
  },
  {
    "text": "the right here we got a variety of other functions that we're using we're using",
    "start": "1744200",
    "end": "1749269"
  },
  {
    "text": "lambda then to do some other enrichments what's really cool about this is that it's dynamic and that's what we one of",
    "start": "1749269",
    "end": "1756169"
  },
  {
    "text": "the benefits that we found is that you know we in the past a lot of our lambda",
    "start": "1756169",
    "end": "1761450"
  },
  {
    "text": "functions would be written in Java and we deploy them out there but more and more were taking advantage of Python and",
    "start": "1761450",
    "end": "1766669"
  },
  {
    "text": "so we're able to then just tie into our pipeline fairly easily to do some new analytic and then you see where these",
    "start": "1766669",
    "end": "1774049"
  },
  {
    "text": "this this data is being pushed in the s3 you can kind of you can pick and choose where in the pipeline you want to do",
    "start": "1774049",
    "end": "1780679"
  },
  {
    "text": "that so there may be a specific use case where we want we've called some lambda functions and we want to go ahead and push that data in or perhaps that use",
    "start": "1780679",
    "end": "1788539"
  },
  {
    "text": "case goes away and we just abandon that and let the data go through so in this case here we got starting with that",
    "start": "1788539",
    "end": "1794000"
  },
  {
    "text": "first lambda we're doing an enrichment there it gets pushed to Kinesis streams at that point it also goes to the fire",
    "start": "1794000",
    "end": "1800450"
  },
  {
    "text": "hose we have another lambda function that one's actually a preprocessor to the Kinesis analytics and Mike's going",
    "start": "1800450",
    "end": "1806330"
  },
  {
    "text": "to talk a little bit more about that and when those analytics come out it goes to push it into to the fire hose and then",
    "start": "1806330",
    "end": "1812570"
  },
  {
    "text": "once all the data is in there we can use a variety of services below AWS glue",
    "start": "1812570",
    "end": "1817850"
  },
  {
    "text": "athena EMR quick side and then our own custom analytics as well so with that",
    "start": "1817850",
    "end": "1823279"
  },
  {
    "text": "I'm gonna hand this off to my garden Ellie and he's gonna talk a little bit more about Kinesis all right thanks",
    "start": "1823279",
    "end": "1831200"
  },
  {
    "text": "Ralph make sure I don't trip here so what I'm going to do is I'm going to",
    "start": "1831200",
    "end": "1837720"
  },
  {
    "text": "recap the suite of Kinesis services that we're actually using which is the fire hose to streams in the analytics I'm",
    "start": "1837720",
    "end": "1844320"
  },
  {
    "text": "gonna actually walk you through an actual use case that we actually put together that Ralph touched on",
    "start": "1844320",
    "end": "1849900"
  },
  {
    "text": "previously in the air in that previous diagram so start with the fire hose and Ralph touch on some of this but you know we're starting to really move towards",
    "start": "1849900",
    "end": "1855990"
  },
  {
    "text": "using fire hose for a lot of our ingest pipelines and a lot of the value-add that we get out of it is the the how",
    "start": "1855990",
    "end": "1863100"
  },
  {
    "text": "easy it is to just do the bundling compression and partitioning and things like that to really set up the data for",
    "start": "1863100",
    "end": "1868320"
  },
  {
    "text": "analysis and other needs that we have in our space and one of the the key parts there is that we do use knife I quite",
    "start": "1868320",
    "end": "1874290"
  },
  {
    "text": "heavily Apache knife I we love it but there are some things about it that have started to become really problematic for",
    "start": "1874290",
    "end": "1880080"
  },
  {
    "text": "us and some of that's just the administration part of it the administrative overhead that you have along with some of the configuration",
    "start": "1880080",
    "end": "1886530"
  },
  {
    "text": "management and things as you start to deploy out these solutions for these pipelines and Kinesis Benham just a",
    "start": "1886530",
    "end": "1891600"
  },
  {
    "text": "better fit for us there and then the other part is as i mentioned is just you know trying to really quickly get it in",
    "start": "1891600",
    "end": "1897450"
  },
  {
    "text": "the hands of our researchers and scientists and analysts so they can help assess and help us understand if the",
    "start": "1897450",
    "end": "1902760"
  },
  {
    "text": "data we're bringing in is positioned the way that they want if there's other modifications or issuance or whatever",
    "start": "1902760",
    "end": "1907860"
  },
  {
    "text": "that we need to do to actually more in richer or better position i guess whatever it is that they want to do and",
    "start": "1907860",
    "end": "1913920"
  },
  {
    "text": "then the last piece i just want to mention is you know you can do quite a bit with the actual firehose piece",
    "start": "1913920",
    "end": "1918930"
  },
  {
    "text": "itself meaning the transformations and optimizations and things like that but one of the things we encountered when we were bringing some of our stuff in is",
    "start": "1918930",
    "end": "1925020"
  },
  {
    "text": "that some of the data sources we had had some fairly complicated json nested structures and we actually had to use",
    "start": "1925020",
    "end": "1931410"
  },
  {
    "text": "spark jobs instead just because we were having problems actually getting into the park' format so just something to note if you're going through that",
    "start": "1931410",
    "end": "1936750"
  },
  {
    "text": "process just keep that in mind and with streams so somewhat similar to firehose",
    "start": "1936750",
    "end": "1941970"
  },
  {
    "text": "just in that we're starting to look at seeing if we are trying to look at if we can actually port some of our",
    "start": "1941970",
    "end": "1947610"
  },
  {
    "text": "preexisting knife eye pipelines we were doing a lot of enrichments analytics and transformations into leveraging Kinesis",
    "start": "1947610",
    "end": "1953160"
  },
  {
    "text": "streams in analytics instead for a lot of the same reasons the administrative overhead configuration management and",
    "start": "1953160",
    "end": "1960150"
  },
  {
    "text": "then just simply because of the analytics and things that our teams want to run are pretty diverse in lamda in",
    "start": "1960150",
    "end": "1965370"
  },
  {
    "text": "particular we're very well suited for that and then one of the other key parts is",
    "start": "1965370",
    "end": "1970760"
  },
  {
    "text": "making sure that we can try to deliver more targeted directed data streams so a lot of the models machine learning",
    "start": "1970760",
    "end": "1977000"
  },
  {
    "text": "implementations applications things like that are better suited and provide a lot more I guess accurate results if you",
    "start": "1977000",
    "end": "1983120"
  },
  {
    "text": "will if you're providing them more directed streams and using nya and other things you can do that but it seems to",
    "start": "1983120",
    "end": "1989000"
  },
  {
    "text": "be harder seems to be a little more difficult and Kinesis streams with the the dynamic routing and things like that you can do it's better position at least",
    "start": "1989000",
    "end": "1994910"
  },
  {
    "text": "from what we've seen to do that and then the last piece the Kinesis analytics so we are starting to leverage a lot of the",
    "start": "1994910",
    "end": "2001720"
  },
  {
    "text": "functionality out of the analytics side of Kinesis where we can actually build these directed these targeted streams so",
    "start": "2001720",
    "end": "2007120"
  },
  {
    "text": "one of the things we do is we try to eliminate what we call the junk write like eliminate the stuff that people don't care about try to actually provide",
    "start": "2007120",
    "end": "2013120"
  },
  {
    "text": "them with the relevant data the relevant content that they actually are looking for and then use the filtering logic",
    "start": "2013120",
    "end": "2019540"
  },
  {
    "text": "also to do things like route things that require specific contents like the models themselves like language is a",
    "start": "2019540",
    "end": "2025600"
  },
  {
    "text": "good example of that right if you have you know like a Spanish based base model or something you obviously don't want to",
    "start": "2025600",
    "end": "2030610"
  },
  {
    "text": "send it English right and you can use the criteria or sequel logic directly in Kinesis analytics to do that and then",
    "start": "2030610",
    "end": "2036940"
  },
  {
    "text": "one of the other things is we're starting to add a lot more capability to bring users into the loop like you know",
    "start": "2036940",
    "end": "2042310"
  },
  {
    "text": "allow them to engage in these flows and these data streams so they can help us build up these training sets so that we",
    "start": "2042310",
    "end": "2048310"
  },
  {
    "text": "can actually then build out these machine learning and Stan she ations and leverage the content that that's being derived from that tagging the labeling",
    "start": "2048310",
    "end": "2054879"
  },
  {
    "text": "all that stuff to do additional filtering and things and then the aggregations is is really powerful so a",
    "start": "2054880",
    "end": "2061240"
  },
  {
    "text": "lot of what we've had to do in the past is of course sort of the details run the aggregations perform the summarizations",
    "start": "2061240",
    "end": "2067000"
  },
  {
    "text": "all those kinds of things so we can actually get the data into a state that's needed for alerting or outlier detection or whatever analytic it is",
    "start": "2067000",
    "end": "2073480"
  },
  {
    "text": "that it needs that type of input signature instead now though instead of having to actually persist at first we",
    "start": "2073480",
    "end": "2079360"
  },
  {
    "text": "can actually do some of these aggregations in line in the stream directly especially if we actually are",
    "start": "2079360",
    "end": "2085090"
  },
  {
    "text": "doing some up filtering and enrichments and modifications to us so it's really really powerful not have to store at",
    "start": "2085090",
    "end": "2090700"
  },
  {
    "text": "first we can actually persistent in an aggregated state or pass it directly to the analytic and then the the other part",
    "start": "2090700",
    "end": "2097210"
  },
  {
    "text": "is just taking advantage and Randi touched on this taking advantage of some of the out-of-the-box capabilities things like anomaly detection we have our own we",
    "start": "2097210",
    "end": "2103990"
  },
  {
    "text": "have our own custom ones and things like that that we build but it's nice that we can attach new ones or different ones I guess if you will with within the",
    "start": "2103990",
    "end": "2109900"
  },
  {
    "text": "Kinesis suite alright so I actually you know it always helped at least ralph let me talk about this a lot it always helps",
    "start": "2109900",
    "end": "2115270"
  },
  {
    "text": "to actually have a use case to walk through or to describe you know how you can actually leverage these tools and in",
    "start": "2115270",
    "end": "2121000"
  },
  {
    "text": "this case we were it started to use fire hose and things to do our ingest and we actually wanted to take a look at the",
    "start": "2121000",
    "end": "2126430"
  },
  {
    "text": "other suite of tools at least the the streams and the analytics and we had something that come up this this news",
    "start": "2126430",
    "end": "2131500"
  },
  {
    "text": "feed that Ralph was talking about you know where we actually did one in aggregate the content we needed to make enrichments and modifications to perform",
    "start": "2131500",
    "end": "2138250"
  },
  {
    "text": "ETL and then we actually wanted to integrate it with some of our excuse me existing an anomaly detection",
    "start": "2138250",
    "end": "2144460"
  },
  {
    "text": "capabilities that we had built and we thought hey let's just just just attach the out-of-the-box one to so just as a",
    "start": "2144460",
    "end": "2149620"
  },
  {
    "text": "side note there within the Kinesis suite it's really easy to just attach a template or grab a template and use it",
    "start": "2149620",
    "end": "2155200"
  },
  {
    "text": "as starting place so I really highly encourage you to do that that's that's what we did here so the first part of",
    "start": "2155200",
    "end": "2161110"
  },
  {
    "text": "this build out for this use case is constructing the schemas so just like you do with a relational database it's",
    "start": "2161110",
    "end": "2167230"
  },
  {
    "text": "really important to understand for your use case what the data structures need to look like and that so that's what we",
    "start": "2167230",
    "end": "2172930"
  },
  {
    "text": "had done is that we knew that we needed to position this raw content are getting into a state where we could actually perform the aggregations we wanted to",
    "start": "2172930",
    "end": "2178990"
  },
  {
    "text": "have and then also make sure we had the the data elements we needed to actually attach the anomaly detection there",
    "start": "2178990",
    "end": "2185590"
  },
  {
    "text": "actually is one flaw on this slide that I noticed later and that's the the roll-up timestamp there on that uragan scheme was actually a timestamps not an",
    "start": "2185590",
    "end": "2191620"
  },
  {
    "text": "integer so I saw that after the fact just as a side note so in order to",
    "start": "2191620",
    "end": "2197110"
  },
  {
    "text": "actually get the data into the structure that we wanted we actually are attaching what's called a preprocessor to one of",
    "start": "2197110",
    "end": "2203410"
  },
  {
    "text": "our Kinesis streams and this is really awesome I mean we basically are able to take a lambda function generate the",
    "start": "2203410",
    "end": "2209710"
  },
  {
    "text": "logic or create the logic within that lambda function and then attach it to this stream so that as data is coming through this stream it's actually",
    "start": "2209710",
    "end": "2215650"
  },
  {
    "text": "running this logic on it performing the enrichments doing the extractions doing some of the transformations we needed to",
    "start": "2215650",
    "end": "2220810"
  },
  {
    "text": "actually get it into that payload get it into that state which is really nice if you haven't used lambda hopefully most",
    "start": "2220810",
    "end": "2226210"
  },
  {
    "text": "of you in here have it's it's phenomenal we use it all over the place now for a lot of our logic and processing just because of how easy it is to build these",
    "start": "2226210",
    "end": "2232690"
  },
  {
    "text": "containers and deploy them and reuse so this and this is just a snippet of what the actual lambda code looks like",
    "start": "2232690",
    "end": "2239230"
  },
  {
    "text": "it's a it's a python-based lambda function and what we did is we built just a deployment package locally where",
    "start": "2239230",
    "end": "2244930"
  },
  {
    "text": "we could quickly evaluate if the logic was right just run some basic testings test through it and make sure that it",
    "start": "2244930",
    "end": "2250750"
  },
  {
    "text": "was converting it and reaching it the way that we wanted and then we actually deployed that out and we're using the",
    "start": "2250750",
    "end": "2255820"
  },
  {
    "text": "the built-in test harness that's out there that actually has an input signature that matches what you get from",
    "start": "2255820",
    "end": "2261970"
  },
  {
    "text": "the Kinesis stream and we just made some minor modifications to it so we actually could test it as it was deployed make",
    "start": "2261970",
    "end": "2267850"
  },
  {
    "text": "some minor adjustments in the console and then once we got it into that state it was ready to go and we used the",
    "start": "2267850",
    "end": "2273550"
  },
  {
    "text": "deployment package mainly just because there was some libraries and things that we wanted to take advantage of that that we wanted to package up and use that as",
    "start": "2273550",
    "end": "2279610"
  },
  {
    "text": "part of the deployment so then the next step so we define our schemas we perform",
    "start": "2279610",
    "end": "2284920"
  },
  {
    "start": "2281000",
    "end": "2281000"
  },
  {
    "text": "the pre-processing where we're doing in their enrichment the ETL and the data the next step was actually to generate the actual aggregations that we wanted",
    "start": "2284920",
    "end": "2291490"
  },
  {
    "text": "out of the stream and then Randi touched on this it's just sequel it's just pure sequel that we were able to write against the streams enriched to",
    "start": "2291490",
    "end": "2297850"
  },
  {
    "text": "transform data and so what you see is that we're actually doing groupings or roll-ups by the source which is the URL",
    "start": "2297850",
    "end": "2303130"
  },
  {
    "text": "on some of the other elements or components that we were exposing within this data so aside from the source",
    "start": "2303130",
    "end": "2308140"
  },
  {
    "text": "counts who are actually looking at named entity recognition so it's identifying the people places organizations things",
    "start": "2308140",
    "end": "2313990"
  },
  {
    "text": "like that within the content based on one of the deployments that we had and and the other couple things to know here",
    "start": "2313990",
    "end": "2320260"
  },
  {
    "text": "one is that we're using the tumbling window so this is the non overlap we want to actually have these very",
    "start": "2320260",
    "end": "2325450"
  },
  {
    "text": "segmented roll-ups of data that's what the input signature was required ever",
    "start": "2325450",
    "end": "2330760"
  },
  {
    "text": "that we want to have required for the anomaly detection and then our internal custom ones also rely on that so we use",
    "start": "2330760",
    "end": "2336880"
  },
  {
    "text": "the tumbling window approach and if you look at the end of the sequel where it says 600 a second that's where we were",
    "start": "2336880",
    "end": "2342520"
  },
  {
    "text": "describing that time frame that we wanted to actually group by which is roughly ten minutes at least for our other anomaly detection capabilities we",
    "start": "2342520",
    "end": "2349120"
  },
  {
    "text": "found that ten minutes is about the the most granular signal we could we can agree gate on where we're an analyst or",
    "start": "2349120",
    "end": "2355480"
  },
  {
    "text": "a researcher could actually get make some sense of when it normally occurs so that's why we use that and then the other thing to note is we actually",
    "start": "2355480",
    "end": "2361660"
  },
  {
    "text": "dropped in the current row timestamp function in here because one of the things that we want to make sure and",
    "start": "2361660",
    "end": "2366970"
  },
  {
    "text": "enable that if it's somebody did see something out of whack or out of norm so either the aggregate data itself or",
    "start": "2366970",
    "end": "2372859"
  },
  {
    "text": "the anomaly's they could actually drill into the rough timeframe and do some searches and queries on the raw data just to see if they could better",
    "start": "2372859",
    "end": "2379069"
  },
  {
    "text": "understand what was going on and then the screenshot there is from the aggregated data that we're building or",
    "start": "2379069",
    "end": "2384920"
  },
  {
    "text": "deriving here we're actually writing it to a destination which is another Kinesis stream in this case and what's really powerful about that is you can",
    "start": "2384920",
    "end": "2390890"
  },
  {
    "text": "actually build these new streams where other applications or consumers can leverage what you're doing here directly",
    "start": "2390890",
    "end": "2396529"
  },
  {
    "text": "on the stream really really nice really really powerful and this is this is kind of in line with what I mentioned earlier about some of the complexity we would",
    "start": "2396529",
    "end": "2403250"
  },
  {
    "text": "have some of our pre-existing flows that this is helping us with so then the next",
    "start": "2403250",
    "end": "2408680"
  },
  {
    "start": "2407000",
    "end": "2407000"
  },
  {
    "text": "step is okay we derived the schemas we applied our ETL we actually performing the aggregations now we actually wanted",
    "start": "2408680",
    "end": "2414380"
  },
  {
    "text": "to attach or apply the anomaly detection and in this case and I highlighted in red just so it was apparent what we were",
    "start": "2414380",
    "end": "2419539"
  },
  {
    "text": "doing is we're actually using the random cut forest with explanation anomaly detection and the reason we chose that",
    "start": "2419539",
    "end": "2425210"
  },
  {
    "text": "over there just the random cut forests one is that we actually wanted the metadata that comes with the anomaly",
    "start": "2425210",
    "end": "2430430"
  },
  {
    "text": "score to be part of the feed and the metadata actually gives you insight into the the direction and and the reasons",
    "start": "2430430",
    "end": "2436789"
  },
  {
    "text": "for which the anomaly is being derived right so I we were using it not that we will keep it in the fee but we were just doing it so we had a better",
    "start": "2436789",
    "end": "2442460"
  },
  {
    "text": "understanding of why the scores are coming out the way they were and then the other part of it is just like we did",
    "start": "2442460",
    "end": "2447859"
  },
  {
    "text": "with the aggregations is we were building another destination so we're building another output feed that we could leverage or used for the actual",
    "start": "2447859",
    "end": "2454400"
  },
  {
    "text": "analysis itself once we had or now that we had the aggregations and the anomalies and so this is this is",
    "start": "2454400",
    "end": "2460520"
  },
  {
    "text": "essentially redundant to my previous slide it's just that I wanted to show you what it looks like directly in the console so the console is really",
    "start": "2460520",
    "end": "2466430"
  },
  {
    "text": "powerful you can actually of course apply your sequel you can run it you can evaluate the source data as it's coming",
    "start": "2466430",
    "end": "2472309"
  },
  {
    "text": "in just so you get an idea of what the data looks like and then you can of course evaluate the logic that you've",
    "start": "2472309",
    "end": "2477650"
  },
  {
    "text": "applied in that real-time analysis window down there at the bottom so what it's doing is it's periodically running samples on the logic that you applied",
    "start": "2477650",
    "end": "2484039"
  },
  {
    "text": "and you can actually see what's being spit out just to see if it's in line with what you're expecting and so you know we're we're rolling up on the",
    "start": "2484039",
    "end": "2489650"
  },
  {
    "text": "sources the counts the the named entities and then the time stamp which is being derived and then a little",
    "start": "2489650",
    "end": "2495950"
  },
  {
    "text": "snippet there is the anomaly score itself and then the next the next piece I just",
    "start": "2495950",
    "end": "2502070"
  },
  {
    "start": "2500000",
    "end": "2500000"
  },
  {
    "text": "want to highlight is very similar to the aggregations which is then we're actually deriving or pushing this data to a firehose delivery stream in this",
    "start": "2502070",
    "end": "2508160"
  },
  {
    "text": "case instead of a Kinesis stream and the reason that we were pushing it to a firehose stream is we actually wanted to load this output this data actually back",
    "start": "2508160",
    "end": "2515030"
  },
  {
    "text": "into our data Lake the data like ralph was talking about and make it available for other processing and analysis that",
    "start": "2515030",
    "end": "2520310"
  },
  {
    "text": "we want to have happen or enable for our researchers and scientists and things like that that for their ability to take",
    "start": "2520310",
    "end": "2525440"
  },
  {
    "text": "a look at the content and see if it's in line with what they wanted and so the the other part is that and",
    "start": "2525440",
    "end": "2530570"
  },
  {
    "start": "2528000",
    "end": "2528000"
  },
  {
    "text": "I'm it's unfortunate I left it out I wish I would have put it in here is that ralph was talking about the glue crawlers and so the next thing that we",
    "start": "2530570",
    "end": "2536750"
  },
  {
    "text": "did is once we start landing the data into our rs3 repositories that we ran glue crawlers on it to auto-generate the",
    "start": "2536750",
    "end": "2543080"
  },
  {
    "text": "schema and so once you have that auto-generated schema then you can actually start to write sequel through",
    "start": "2543080",
    "end": "2549110"
  },
  {
    "text": "the Athena to actually further evaluate your data which is really really powerful we found actually pretty pretty",
    "start": "2549110",
    "end": "2556130"
  },
  {
    "text": "quickly up front that I think that I'd made some mistakes on like the the time windowing and then I'd realized that I'd",
    "start": "2556130",
    "end": "2562640"
  },
  {
    "text": "actually left out one of the named entities that I want to apply I think it was location and I was I should have noticed that I recognized that right in",
    "start": "2562640",
    "end": "2569180"
  },
  {
    "text": "the Kinesis console but I didn't and I was actually then able to run some Athena queries and things to identify",
    "start": "2569180",
    "end": "2574700"
  },
  {
    "text": "that both in the schema structure and then the roll-ups and it's really really powerful within it seriously like I think was like a matter of minutes oh",
    "start": "2574700",
    "end": "2580820"
  },
  {
    "text": "it's dumping the data actually could come out here and take a look at the data and do the roll-ups and start to see or identify if the data the",
    "start": "2580820",
    "end": "2587030"
  },
  {
    "text": "signature was what I wanted and then I came back the next day and looked at the actual results and just something to know it's kind of interesting and I",
    "start": "2587030",
    "end": "2593210"
  },
  {
    "text": "don't know if there's anything of value here or not but if you look at the second and third rows and it's maybe a little hard to see you can tell that the",
    "start": "2593210",
    "end": "2599390"
  },
  {
    "text": "anomaly score is a lot higher on the second row even though the URL counts less and the reason for that is the entities that are being identified from",
    "start": "2599390",
    "end": "2606710"
  },
  {
    "text": "that source within that time frame are a lot higher again does that identify anything of importance not sure but the",
    "start": "2606710",
    "end": "2613280"
  },
  {
    "text": "the reason for doing this or the reason we're applying these algorithms to this data is we want to highlight cases of",
    "start": "2613280",
    "end": "2620150"
  },
  {
    "text": "where the sources are having things occur out of norm and so what you would expect is that if you're starting to see a lot more people or locations or",
    "start": "2620150",
    "end": "2626000"
  },
  {
    "text": "organizations or maybe a combination of them mention more frequently it's usually an indication of some",
    "start": "2626000",
    "end": "2631730"
  },
  {
    "text": "sort of event occurring right or at least that's the intention and so we've gotten a lot of just a ton of value out",
    "start": "2631730",
    "end": "2637340"
  },
  {
    "text": "of bringing these things together and I think we did it and it was a the initial attempt at least was pretty quick right",
    "start": "2637340",
    "end": "2642440"
  },
  {
    "text": "in a matter of days or something like that we're actually able to assemble these things pretty quickly and get it in front of people and have them start to give us feedback instead of have it",
    "start": "2642440",
    "end": "2649250"
  },
  {
    "text": "spending a lot of time on the engineering part or did we find out that you've made a lot of you know issues are",
    "start": "2649250",
    "end": "2654260"
  },
  {
    "text": "you encountered a lot of issues in the way that you put it together that maybe doesn't match their needs or meet their needs so you know what's next for us so",
    "start": "2654260",
    "end": "2660980"
  },
  {
    "text": "we I think we're really or I guess Ralph would probably agree that we really just kind of scratching the surface on what we can do with some of these Kinesis streams the analytics and the firehose",
    "start": "2660980",
    "end": "2667820"
  },
  {
    "text": "and all that and so we want to start and we're going to continue to look at other areas where you can make adjustments or",
    "start": "2667820",
    "end": "2672859"
  },
  {
    "text": "migrations into this platform we do a lot of enrichments and analytic integrations and things like that and so",
    "start": "2672859",
    "end": "2678170"
  },
  {
    "text": "we're gonna be looking at that we also use a lot of pre-existing AWS services so it seems like a DBS recognition which",
    "start": "2678170",
    "end": "2684470"
  },
  {
    "text": "is identifying image classes or image classifiers within content are within the imagery themselves I mean and so",
    "start": "2684470",
    "end": "2691310"
  },
  {
    "text": "because of the data you get out of those feeds or out of that service we actually can start to use those for our filtering",
    "start": "2691310",
    "end": "2697010"
  },
  {
    "text": "and aggregations and even maybe anomaly detection I start to determine if you've got imagery of certain kinds that that",
    "start": "2697010",
    "end": "2703070"
  },
  {
    "text": "stand out from a feed if you will and then the other part is making it easier starting to make incorporate a lot of",
    "start": "2703070",
    "end": "2709190"
  },
  {
    "text": "the input that we're getting again from our users that I touched on earlier there they're helping us build them out",
    "start": "2709190",
    "end": "2715100"
  },
  {
    "text": "these training sets and things like that and then applying the machine learning aspects to it and the taking and leggo",
    "start": "2715100",
    "end": "2720109"
  },
  {
    "text": "me again that we get from that it's starting to use it so anyways that's the that's the end of the use case that's",
    "start": "2720109",
    "end": "2725990"
  },
  {
    "text": "the end of our presentation just wanted to say thanks I really appreciate you know Randy and Cindy asking us to come",
    "start": "2725990",
    "end": "2732410"
  },
  {
    "text": "up here and talk to you and really appreciate your time so thank you and if you guys could really quickly if",
    "start": "2732410",
    "end": "2741080"
  },
  {
    "text": "you guys could",
    "start": "2741080",
    "end": "2743650"
  }
]