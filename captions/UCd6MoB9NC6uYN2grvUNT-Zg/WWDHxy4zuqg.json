[
  {
    "start": "0",
    "end": "36000"
  },
  {
    "text": "hey all right so I've been pretty excited about some of the announcements today as you might imagine from the talk",
    "start": "680",
    "end": "8069"
  },
  {
    "text": "and one thing before I even start that I want to mention was that I had some conversations with the lambda product",
    "start": "8069",
    "end": "13769"
  },
  {
    "text": "team and they completed for four out of the six things that they announced today",
    "start": "13769",
    "end": "19080"
  },
  {
    "text": "we're things that I'm sure not just myself but I had specifically asked for and believe me as an AWS customer that",
    "start": "19080",
    "end": "25470"
  },
  {
    "text": "is a huge deal when you know that they're out there doing a lot of this engineering for you so before I even",
    "start": "25470",
    "end": "30869"
  },
  {
    "text": "start I just want to say thank you very much that lambda team they've done some amazing work here all right so if you're",
    "start": "30869",
    "end": "38190"
  },
  {
    "start": "36000",
    "end": "96000"
  },
  {
    "text": "here thank you we're going to talk about today will be an expert level session however I'm going to get into some",
    "start": "38190",
    "end": "44129"
  },
  {
    "text": "higher-level topics at the end that I think everyone expert and novice will appreciate so please don't run away if",
    "start": "44129",
    "end": "50039"
  },
  {
    "text": "you were expecting something else I think that this talk is going to cover some pretty unique things that none of",
    "start": "50039",
    "end": "55920"
  },
  {
    "text": "the other lambda talks are going to cover there's some really awesome stuff you can do with a sink lambda and those",
    "start": "55920",
    "end": "61050"
  },
  {
    "text": "are all really important things to understand about architecting and nothing not taking anything away from",
    "start": "61050",
    "end": "66090"
  },
  {
    "text": "there but this is probably only place you're going to see some of this specifically I don't know if you guys caught the cascading stuff that Warner",
    "start": "66090",
    "end": "73020"
  },
  {
    "text": "brought up in the keynote I was I was very surprised to see that that's actually slide from the stack so we're",
    "start": "73020",
    "end": "78600"
  },
  {
    "text": "going to be covering that in depth so the other last part of this that I hope",
    "start": "78600",
    "end": "84180"
  },
  {
    "text": "you guys get out of this is the importance of interactive big data so this isn't sort of stuff that happens",
    "start": "84180",
    "end": "89909"
  },
  {
    "text": "behind the scenes but how important it is to be able to get that as a very tactile feel for the end user all right",
    "start": "89909",
    "end": "97770"
  },
  {
    "start": "96000",
    "end": "139000"
  },
  {
    "text": "so what was our problem so I work for fire I and our mission is to find evil and sometimes evil is in a giant",
    "start": "97770",
    "end": "104640"
  },
  {
    "text": "mountain of data and I'll get into just how big and a little bit some of this in",
    "start": "104640",
    "end": "110189"
  },
  {
    "text": "fact of you know majority of our working data is in hot indexes but quite a bit",
    "start": "110189",
    "end": "115500"
  },
  {
    "text": "of it by time period is in cold storage on s3 I suspect this is a pretty common use case for a lot of people and this is",
    "start": "115500",
    "end": "122369"
  },
  {
    "text": "sort of a lambda architecture and the the more big data sense of it not the EWS lambda architecture so our problem",
    "start": "122369",
    "end": "130229"
  },
  {
    "text": "is we needed to be able to take full advantage of all the data that we had both hot and cold and we needed to be",
    "start": "130229",
    "end": "135370"
  },
  {
    "text": "able to do this in a way that's not going to hurt or user experience so",
    "start": "135370",
    "end": "140860"
  },
  {
    "start": "139000",
    "end": "288000"
  },
  {
    "text": "security about boils down to what I call questions and answers there's a lot of ways to look at it but this is over the",
    "start": "140860",
    "end": "147190"
  },
  {
    "text": "last few years been really important to me to define it this way i have about 10 years of incident response experience so",
    "start": "147190",
    "end": "154570"
  },
  {
    "text": "i'm used to speaking out security conferences this is kind of a new one for me to be at just a general tech conference but i suspect that this is no",
    "start": "154570",
    "end": "161260"
  },
  {
    "text": "different than a lot of you have to deal with in fact i was talking to a company yesterday and what they were doing was",
    "start": "161260",
    "end": "166330"
  },
  {
    "text": "very similar but they were operating on mechanical failures and they were trying to go back and figure out why a",
    "start": "166330",
    "end": "173290"
  },
  {
    "text": "particular piece of machinery failed and in a very similar way that's what we're doing we're trying to figure out what",
    "start": "173290",
    "end": "179170"
  },
  {
    "text": "went wrong and back-trace so you have two sides of the equation you have questions and you have answers the",
    "start": "179170",
    "end": "184690"
  },
  {
    "text": "questions are sort of the typical security product that's going to say hey I've got an alert for you and the way",
    "start": "184690",
    "end": "191320"
  },
  {
    "text": "that we do it is through some more traditional means we have EMR analytic output I'll get into the specifics on",
    "start": "191320",
    "end": "197320"
  },
  {
    "text": "the next slide we also have our ec2 based proprietary detection that we may look at you moving that to other forms",
    "start": "197320",
    "end": "204010"
  },
  {
    "text": "within AWS but primarily that's going to be things like traditional rules and our",
    "start": "204010",
    "end": "209520"
  },
  {
    "text": "giant amount of Intel that we have from doing all of our investigations goes into our rules so that you're getting",
    "start": "209520",
    "end": "215220"
  },
  {
    "text": "all of your data checked against all the investigations that we've ever done and we've done some really big",
    "start": "215220",
    "end": "220390"
  },
  {
    "text": "investigations if you've seen in the newspaper there's a very good chance that it'll say investigated by fire I so",
    "start": "220390",
    "end": "226540"
  },
  {
    "text": "we have a lot of interesting things that we look for upfront but that only generates a question so what do we do",
    "start": "226540",
    "end": "233080"
  },
  {
    "text": "once we have that question so that's the right hand side these now we have to find answers and this has been kind of a",
    "start": "233080",
    "end": "239800"
  },
  {
    "text": "mystery to me why this isn't the first thing people talk about insecurity to me",
    "start": "239800",
    "end": "244990"
  },
  {
    "text": "when a product says like like if you're Sam and you say hey I've got an alert for you that's kind of like a police",
    "start": "244990",
    "end": "250270"
  },
  {
    "text": "investigator showing up and saying yep this victim is definitely dead I guess my job is done here that's where the job",
    "start": "250270",
    "end": "256299"
  },
  {
    "text": "starts that's what in the investigation starts you have to find your answers and that's the the interesting thing that",
    "start": "256299",
    "end": "261519"
  },
  {
    "text": "that we had to work on so we have two ways of doing that we have our ec2 based index",
    "start": "261520",
    "end": "266670"
  },
  {
    "text": "search I've recently learned we have the largest elasticsearch cluster on AWS this is this is not easy and it's",
    "start": "266670",
    "end": "273960"
  },
  {
    "text": "something that we're very proud of and it's a big part of our product however",
    "start": "273960",
    "end": "279150"
  },
  {
    "text": "we can only fit so much data from a customer in a hot index so sometimes we need to go to the cold stuff and that",
    "start": "279150",
    "end": "285630"
  },
  {
    "text": "requires lambda so what are we doing specifically so any mr we have our",
    "start": "285630",
    "end": "292760"
  },
  {
    "start": "288000",
    "end": "374000"
  },
  {
    "text": "traditional Big Data kinds of things so we're doing our clustering or doing things like linear regression one of the",
    "start": "292760",
    "end": "298980"
  },
  {
    "text": "interesting things that we do that I find easiest to explain to the novice and big data is geographic timeline and",
    "start": "298980",
    "end": "305820"
  },
  {
    "text": "call it geo feasibility so if we have to log ins from two different cities at the same time on a mobile device that seems",
    "start": "305820",
    "end": "311670"
  },
  {
    "text": "pretty improbable and we flagged as an alert and in this context that's a question the question is why did this",
    "start": "311670",
    "end": "317490"
  },
  {
    "text": "person log in from two different places at the same time so that triggers our investigation and that's where lambda",
    "start": "317490",
    "end": "324180"
  },
  {
    "text": "can help us out here again we have majority of work is on that hot index but sometimes you got to go and go much",
    "start": "324180",
    "end": "330510"
  },
  {
    "text": "farther back in time and for that you might need to do some basic group by style reporting and those might be a",
    "start": "330510",
    "end": "335700"
  },
  {
    "text": "multi-column things you might have to use some more in-depth aggregation the visualizations help will show that in a",
    "start": "335700",
    "end": "341970"
  },
  {
    "text": "second and you might need to do some statistics on there as well and this is the part when i was working Flander that",
    "start": "341970",
    "end": "347100"
  },
  {
    "text": "really surprised me i assume that because there wasn't much code involved in what I was working on that this",
    "start": "347100",
    "end": "352350"
  },
  {
    "text": "wasn't going to be you know very powerful and what I found was just using even some of the node libraries out",
    "start": "352350",
    "end": "358620"
  },
  {
    "text": "there for things like k-means and linear regression and deep neural networks there's a fair amount of libraries out",
    "start": "358620",
    "end": "364230"
  },
  {
    "text": "there for doing some serious data science and when you put it on lambda it's actually pretty fast and I find it",
    "start": "364230",
    "end": "369960"
  },
  {
    "text": "to be as or easier to use than your traditional spark based data so when",
    "start": "369960",
    "end": "377130"
  },
  {
    "start": "374000",
    "end": "411000"
  },
  {
    "text": "you're doing big data a lot of the times a bar chart in a table is the best way to show it but I feel obligated to show",
    "start": "377130",
    "end": "383910"
  },
  {
    "text": "some of the cool things everyone here I'm sure it was d3 loves d3 and i do want to point out how important it is to",
    "start": "383910",
    "end": "390480"
  },
  {
    "text": "sometimes show you've got outliers there and these form questions of themselves",
    "start": "390480",
    "end": "396120"
  },
  {
    "text": "so sometimes you get this loop where you have a question comes in the form of a lert and you go find an answer and then",
    "start": "396120",
    "end": "402000"
  },
  {
    "text": "that forms more questions and so you keep going and going and going and this becomes a really important workflow to understand later because it explains why",
    "start": "402000",
    "end": "408539"
  },
  {
    "text": "batch processing doesn't help us very much so everybody talks about Big Data",
    "start": "408539",
    "end": "414690"
  },
  {
    "start": "411000",
    "end": "521000"
  },
  {
    "text": "right this is there's a million really funny sites out there that spoof kind of some of the work that we do right and",
    "start": "414690",
    "end": "420599"
  },
  {
    "text": "because some of it's really ridiculous we use math to do magic right happens a lot sometimes some of its deserve some",
    "start": "420599",
    "end": "427680"
  },
  {
    "text": "of it isn't so here's here's how big our big is just for level setting and I'm",
    "start": "427680",
    "end": "432780"
  },
  {
    "text": "sure there's much bigger and I'm sure there's much smaller I've seen many cases where people call a few million",
    "start": "432780",
    "end": "438690"
  },
  {
    "text": "records big data and like okay that's that's cute and then there's there's",
    "start": "438690",
    "end": "444300"
  },
  {
    "text": "places like Netflix will just laugh at this I mean I'm sure they have you know hundreds maybe thousands of petah bites",
    "start": "444300",
    "end": "449460"
  },
  {
    "text": "right so that's that's a whole different ballgame but this is this is to frame",
    "start": "449460",
    "end": "454590"
  },
  {
    "text": "your context for the kinds of things I'm describing so you're saying like okay well that maybe works in your use case but mine looks like this so this should",
    "start": "454590",
    "end": "461819"
  },
  {
    "text": "sort of help you so we work with security event data these are the mountains of data points that we're going through and each event is about on",
    "start": "461819",
    "end": "469080"
  },
  {
    "text": "average three thousand bytes and again this is for an average customer and this is what I want to point out this is not",
    "start": "469080",
    "end": "474479"
  },
  {
    "text": "the entirety of our data this is one customer and I'm not at liberty to say how many customers it is but it's enough",
    "start": "474479",
    "end": "480180"
  },
  {
    "text": "that this becomes very important to be able to do right and do well at scale so we have three thousand bytes and an",
    "start": "480180",
    "end": "485969"
  },
  {
    "text": "average customer has about 20,000 of these events coming in per second so that's 60 Meg about five terabytes a day",
    "start": "485969",
    "end": "492319"
  },
  {
    "text": "it's like okay that's a lot of data right but that's one day so even searching one week you're talking 12",
    "start": "492319",
    "end": "497969"
  },
  {
    "text": "billion events and 35 terabytes now if you've got that an index no problem",
    "start": "497969",
    "end": "503039"
  },
  {
    "text": "right we can go through that in under a second for most searches on our hot index and that that's not that's not",
    "start": "503039",
    "end": "510000"
  },
  {
    "text": "nothing to shake a stick at that's very important however what happens when that rolls the cold storage what are you",
    "start": "510000",
    "end": "515640"
  },
  {
    "text": "going to do with 35 terabytes that's a lot to get through when you don't have an index so how long would that take",
    "start": "515640",
    "end": "523500"
  },
  {
    "text": "it's like well alright well spin up grab right we got we'll just grab it so this",
    "start": "523500",
    "end": "528779"
  },
  {
    "text": "is how long it would take it would take you four days to grab seven days worth of data this is clearly not going to work for us",
    "start": "528779",
    "end": "535200"
  },
  {
    "text": "so it's like okay well we'll throw some stuff at it maybe we'll auto scale maybe we'll do an EMR it's like okay what is",
    "start": "535200",
    "end": "541140"
  },
  {
    "text": "it going to take well 10 you're looking at six hours and this is where that loop that I was talking about is so important",
    "start": "541140",
    "end": "546779"
  },
  {
    "text": "because when you're doing an investigation you don't know that what you're going to ask is going to be the",
    "start": "546779",
    "end": "552390"
  },
  {
    "text": "right answer you might have to ask a number of questions so the longer you have to wait to find the answer for what",
    "start": "552390",
    "end": "557430"
  },
  {
    "text": "you just asked is going to lengthen and lengthen and lengthen your investigation when often time is of the essence so if",
    "start": "557430",
    "end": "563850"
  },
  {
    "text": "you're waiting six hours during your investigation that's not going to be very helpful I mean that's an overnight job right so can you imagine doing",
    "start": "563850",
    "end": "570360"
  },
  {
    "text": "investigation say well I'd be really interested to know what this host did I'll check on it tomorrow I mean that's",
    "start": "570360",
    "end": "575790"
  },
  {
    "text": "tough that's tough so okay we'll throw a hundred processes at it well now it's an hour okay now we're talking a long lunch",
    "start": "575790",
    "end": "581940"
  },
  {
    "text": "right we're still doing pretty well here but let's think about that 100 well",
    "start": "581940",
    "end": "587190"
  },
  {
    "text": "let's see biggest boxing get is what 44 TV CPUs so you're talking two and a half",
    "start": "587190",
    "end": "594149"
  },
  {
    "text": "there so that's not nothing but it's pretty doable but still you're waiting an hour for that and that's that's",
    "start": "594149",
    "end": "599850"
  },
  {
    "text": "significant so okay let's get this down to something where you can ask multiple questions without getting up from your desk well now we need a thousand of",
    "start": "599850",
    "end": "606390"
  },
  {
    "text": "these so now we're down to five minutes this is somewhat tenable still not awesome but it's it's you can actually",
    "start": "606390",
    "end": "612899"
  },
  {
    "text": "use this an investigation at that point and if you really want to you could do 10,000 and now we're now we're cooking",
    "start": "612899",
    "end": "618690"
  },
  {
    "text": "now you've got a few seconds now you're sipping your coffee for a second while you're watching results come in so that's pretty important right that's",
    "start": "618690",
    "end": "624810"
  },
  {
    "text": "this directly ties to the length of your investigation so let me pose that again how you gonna do an auto scaling at a",
    "start": "624810",
    "end": "630720"
  },
  {
    "text": "thousand processes you can do it right I mean the answer is always yes yes we can do that but it's going to be kind of",
    "start": "630720",
    "end": "637079"
  },
  {
    "text": "tricky and it's going to be a little bit painful and it's not something you're used to invoking by a user from a web",
    "start": "637079",
    "end": "642810"
  },
  {
    "text": "interface that's kind of a shoehorn at that point right there's not a lot of times where you have your customer",
    "start": "642810",
    "end": "648269"
  },
  {
    "text": "invoking EMR jobs that are expected to finish within a couple of minutes it's",
    "start": "648269",
    "end": "653820"
  },
  {
    "text": "not saying you can't do it or that you aren't but I'm saying there is a better way so that's where lambda comes in so",
    "start": "653820",
    "end": "659459"
  },
  {
    "start": "657000",
    "end": "732000"
  },
  {
    "text": "what if you could spend up 10,000 CPUs in 100 milliseconds we've ever been able",
    "start": "659459",
    "end": "664740"
  },
  {
    "text": "to do this before and i'll get i'll get two more what this means and at the end of the talk some of the higher level stuff but this is a big",
    "start": "664740",
    "end": "670350"
  },
  {
    "text": "deal so you be you're able to do the standard MapReduce pattern without any of the hassle of the EMR stuff and it's",
    "start": "670350",
    "end": "677940"
  },
  {
    "text": "not that emrs bad it gives you the time windowing and spark and there's a lot of reasons why you knew that on the other",
    "start": "677940",
    "end": "683100"
  },
  {
    "text": "hand there's a lot of use cases for analytics that don't require the full spectrum that spark gives you or that",
    "start": "683100",
    "end": "689610"
  },
  {
    "text": "you can roll your own and when you have something that can spin up this quickly there's a very nice trade-off that",
    "start": "689610",
    "end": "695460"
  },
  {
    "text": "happens and primarily you get much much simpler code and this is what I loved",
    "start": "695460",
    "end": "700770"
  },
  {
    "text": "about it because I am an average programmer at best and I guarantee you if I can do this anybody can do this and",
    "start": "700770",
    "end": "706980"
  },
  {
    "text": "that was kind of what was interesting to me as suddenly I could mask really really big questions and they would all",
    "start": "706980",
    "end": "712350"
  },
  {
    "text": "just come right to me and this had never been quite so easy before I didn't have to go and have an EMR cluster",
    "start": "712350",
    "end": "718470"
  },
  {
    "text": "provisioned I didn't have to set up all this stuff and have to take the weeks of prep to set this all up I said hey this",
    "start": "718470",
    "end": "723840"
  },
  {
    "text": "worked on this one file let's try it on all of them and then about five minutes later I had my result and that's when it",
    "start": "723840",
    "end": "730650"
  },
  {
    "text": "started to get really really cool so this is the cascade process and I",
    "start": "730650",
    "end": "736460"
  },
  {
    "start": "732000",
    "end": "776000"
  },
  {
    "text": "noticed they didn't change a whole lot from the keynote this morning and I kind of if I had if I had known about it I",
    "start": "736460",
    "end": "741570"
  },
  {
    "text": "would have offered to change a couple of things on it but this is basically the the pattern I'll go in-depth on each one",
    "start": "741570",
    "end": "749250"
  },
  {
    "text": "of these in just a second the thing I want to point out here is at the bottom this is straight from my blog post I did",
    "start": "749250",
    "end": "755670"
  },
  {
    "text": "for the AWS Big Data blog and what this means to you is that the source code is available there on github for you guys",
    "start": "755670",
    "end": "761550"
  },
  {
    "text": "to go check out literally check out and you can run any of the stuff so i did a",
    "start": "761550",
    "end": "766590"
  },
  {
    "text": "word counting function you know the hello world of big data and i didn't do any of our normal stuff on there but",
    "start": "766590",
    "end": "772350"
  },
  {
    "text": "that gives you that sort of bootstrap to say like oh I can do this too so let's",
    "start": "772350",
    "end": "778980"
  },
  {
    "start": "776000",
    "end": "903000"
  },
  {
    "text": "go through component by component I tried to simplify this a significant amount just to show where the the",
    "start": "778980",
    "end": "784980"
  },
  {
    "text": "mapping and reducing comes from if you want to use the classic terms right so the the initial request comes in on a",
    "start": "784980",
    "end": "792120"
  },
  {
    "text": "very basic web app now when i first started on this api gateway was not avail so I did this all on node with",
    "start": "792120",
    "end": "798630"
  },
  {
    "text": "expressing I just kind of standard basic setup for node but you could use API gateway which remove all the servers",
    "start": "798630",
    "end": "805380"
  },
  {
    "text": "entirely from this which becomes even more interesting and again they'll talk about that in a little bit but that",
    "start": "805380",
    "end": "810990"
  },
  {
    "text": "doesn't really matter in this setup the idea is that you have a josh mo customer and user is going to be invoking all",
    "start": "810990",
    "end": "818460"
  },
  {
    "text": "this and they're going to be expecting a response back immediately not in a few minutes just right away they want to",
    "start": "818460",
    "end": "825120"
  },
  {
    "text": "know what they're getting so the basic web app handles that and then streams the results back and we'll get more into",
    "start": "825120",
    "end": "831150"
  },
  {
    "text": "the streaming at towards IAM the cascade function this is going to invoke a bunch",
    "start": "831150",
    "end": "836310"
  },
  {
    "text": "of child workers so this is where you spread out this is the map part right and this part can be made recursive so",
    "start": "836310",
    "end": "842460"
  },
  {
    "text": "if you needed if you asked for a hundred thousand lambdas to be provisioned I don't mean that would be pretty tall",
    "start": "842460",
    "end": "847830"
  },
  {
    "text": "order ten thousand was not a problem a hundred thousand who knows depending on the region what you get but",
    "start": "847830",
    "end": "853650"
  },
  {
    "text": "theoretically you could ask for that and you're going to be able to do that if you just have another layer of cascading",
    "start": "853650",
    "end": "859050"
  },
  {
    "text": "so the answer is going to be always yes on this it just depends how many cascade layers you want and the important thing",
    "start": "859050",
    "end": "864900"
  },
  {
    "text": "here is going to do two things it's going to aggregate the results which means your initial requester EG the web",
    "start": "864900",
    "end": "870300"
  },
  {
    "text": "app node this poor little single threaded node is going to not be inundated with a thousand responses it's",
    "start": "870300",
    "end": "877140"
  },
  {
    "text": "going to be inundated with maybe a hundred responses or even fewer depending on how you have your cascading",
    "start": "877140",
    "end": "882390"
  },
  {
    "text": "so there's your reduction right and then of course your work or function this is",
    "start": "882390",
    "end": "887700"
  },
  {
    "text": "the cool part you just write that as you know your basic little lambda function and if it works on one file it works on",
    "start": "887700",
    "end": "893520"
  },
  {
    "text": "all files and that's really the fun part of developing this is that your that thing is going to grow to be giant when",
    "start": "893520",
    "end": "899490"
  },
  {
    "text": "it gets invoked by the cascade function and i'll talk more about the DevOps part of that and a little bit so because it's",
    "start": "899490",
    "end": "906420"
  },
  {
    "start": "903000",
    "end": "1105000"
  },
  {
    "text": "expert level we're going to just a teeny bit of code I don't expect you to really care about the code itself here I'm",
    "start": "906420",
    "end": "912330"
  },
  {
    "text": "going to go over just a few of the finer points on node and y know it is kind of a helpful way to do this although I was",
    "start": "912330",
    "end": "917610"
  },
  {
    "text": "really excited and again I asked for the Python thing so I like both note in Python and I think they're they both",
    "start": "917610",
    "end": "924060"
  },
  {
    "text": "have their strengths so one of the strengths of node is that it's you know streaming is",
    "start": "924060",
    "end": "929310"
  },
  {
    "text": "is kind of one of the things that does very very well so the event stream library has been with it for a long time and the key part about streaming is that",
    "start": "929310",
    "end": "936120"
  },
  {
    "text": "you can be working on any large data set but you're only ever operating on chunks of it so your RAM usage is always going",
    "start": "936120",
    "end": "942300"
  },
  {
    "text": "to be very small even though you're working on something very large right the idea is you're not like downloading a whole file right here and then",
    "start": "942300",
    "end": "948240"
  },
  {
    "text": "churning through the whole file and then operating on the next step like a waterfall you're working on just a",
    "start": "948240",
    "end": "953279"
  },
  {
    "text": "little bit of window of the data the whole time and that's critical because you want to be returning responses to",
    "start": "953279",
    "end": "959220"
  },
  {
    "text": "the user during this time so let's walk through this so the initial thing is the list stream this is a really really dumb",
    "start": "959220",
    "end": "966300"
  },
  {
    "text": "node module that goes out and says hey I'm looking for all the keys under this prefix and s3 Mike deck and AWS",
    "start": "966300",
    "end": "974130"
  },
  {
    "text": "solutions expert blue that's his title has another talk out there hopefully got to it in which he showed how to index s3",
    "start": "974130",
    "end": "981300"
  },
  {
    "text": "and put it out on DynamoDB so you have a nice directory which would make this much better on our current",
    "start": "981300",
    "end": "987150"
  },
  {
    "text": "implementation we are actually limited by the speed at which we can list keys off of s3 that's the overall bottleneck",
    "start": "987150",
    "end": "992700"
  },
  {
    "text": "and of course there's lots of ways we can we can deal with that in the future in addition to just having a normal",
    "start": "992700",
    "end": "999240"
  },
  {
    "text": "directory in any case the fruit is all starts out with getting a stream of keys that we want to search right so we're",
    "start": "999240",
    "end": "1004850"
  },
  {
    "text": "getting a queue of our work and that's going into the actual land of stream thing that's going to be invoking the",
    "start": "1004850",
    "end": "1010250"
  },
  {
    "text": "Cascade workers and here you can tell it how many workers do invoke at maximum and that's important because you want to",
    "start": "1010250",
    "end": "1016010"
  },
  {
    "text": "stay under your provision threshold which is 100 by default you have to request a limit raised to go up to the",
    "start": "1016010",
    "end": "1022100"
  },
  {
    "text": "thousand or ten thousand so when you pipe it all together you have a list of keys coming into lambda and then we have",
    "start": "1022100",
    "end": "1027860"
  },
  {
    "text": "those end equals false thing here and this is actually this trip me up for several days this one is is a little bit",
    "start": "1027860",
    "end": "1033709"
  },
  {
    "text": "annoying I'll get to that in a second the results then are sent over just a",
    "start": "1033709",
    "end": "1038720"
  },
  {
    "text": "tiny little five line thing that does a server sent extreme that's the html5 server sent framework that's very",
    "start": "1038720",
    "end": "1046160"
  },
  {
    "text": "similar to a WebSocket but with a little bit less hassle and it's just unidirectional back to the user for for",
    "start": "1046160",
    "end": "1052280"
  },
  {
    "text": "this that's all i needed i didn't need any bidirectional stuff it's it's very very easy to use and that was my choice",
    "start": "1052280",
    "end": "1060380"
  },
  {
    "text": "because it fit the bill and seriously like 10 lines the whole thing and then it goes right into the HTTP",
    "start": "1060380",
    "end": "1066039"
  },
  {
    "text": "response so the key here is that the web user is seeing a new line separated feed in real time as that whole pipeline is",
    "start": "1066039",
    "end": "1073720"
  },
  {
    "text": "operating so there may be hundreds of thousands of s3 keys that we're going to",
    "start": "1073720",
    "end": "1078789"
  },
  {
    "text": "search through and if we can get the first couple of them done those results are streamed back to the user in under a",
    "start": "1078789",
    "end": "1084429"
  },
  {
    "text": "second even though the whole job won't be done for maybe a minute or two and that's great because then the user gets",
    "start": "1084429",
    "end": "1089500"
  },
  {
    "text": "something to look at right away and for instance if they see that they made a typo they can cancel the search and",
    "start": "1089500",
    "end": "1094870"
  },
  {
    "text": "mentor what they really wanted instead of finding out at the end of a giant batch that it was all wrong and they can",
    "start": "1094870",
    "end": "1100059"
  },
  {
    "text": "also more importantly get approximated results as things are coming back through so the batch to async execution",
    "start": "1100059",
    "end": "1108850"
  },
  {
    "start": "1105000",
    "end": "1181000"
  },
  {
    "text": "this is the tricky part so the pipeline stuff works great in node the problem in the trick is it's an false because so we",
    "start": "1108850",
    "end": "1116980"
  },
  {
    "text": "have all these keys coming through and they're going to lambda and then it's doing its thing this lambdas are out",
    "start": "1116980",
    "end": "1122110"
  },
  {
    "text": "there parallel invoking in the returning results to the to the end user but that tail that caboose on the those that's",
    "start": "1122110",
    "end": "1129639"
  },
  {
    "text": "going to be a problem is going to get trimmed off because as soon as that list stream says I sent all the keys those",
    "start": "1129639",
    "end": "1134830"
  },
  {
    "text": "things that are asynchronously out there working in lambda land those are going to get cut off because the pipeline is",
    "start": "1134830",
    "end": "1140080"
  },
  {
    "text": "done so you have to stick an end equals false on there to say I will tell you when we're done and that's a very simple",
    "start": "1140080",
    "end": "1146260"
  },
  {
    "text": "state tracking to say era did all the lambdas come home yet okay great then we can issue false or sorry issue end and",
    "start": "1146260",
    "end": "1153100"
  },
  {
    "text": "then it'll it'll close the pipeline correctly so that was a little bit tricky that was the only thing that was",
    "start": "1153100",
    "end": "1158440"
  },
  {
    "text": "tripping me up in the event stream stuff otherwise it was a very good pattern to use the nice thing about the pipeline is",
    "start": "1158440",
    "end": "1164740"
  },
  {
    "text": "as i showed you can tell it the maximum you want to be going at one time and that will keep you under your provision",
    "start": "1164740",
    "end": "1170440"
  },
  {
    "text": "limits you're not dealing with like a retry avalanche where you're sending a whole bunch of lambdas it says hey",
    "start": "1170440",
    "end": "1176649"
  },
  {
    "text": "already sent 100 your provision for 100 these are all failing you don't want those to backup all right so into the",
    "start": "1176649",
    "end": "1183909"
  },
  {
    "start": "1181000",
    "end": "1203000"
  },
  {
    "text": "Cascade function this is the middle so this code is ridiculously simple I'm just bringing it up to show manages a",
    "start": "1183909",
    "end": "1189820"
  },
  {
    "text": "for loop that creates a batch of batches right but that's all we're doing here we're taking a giant list of keys that",
    "start": "1189820",
    "end": "1195340"
  },
  {
    "text": "goes a cascade function we're going to break that down into smaller batches and each one of those small batches is going to",
    "start": "1195340",
    "end": "1200560"
  },
  {
    "text": "be given out to a worker and we're going to give them to the workers in parallel",
    "start": "1200560",
    "end": "1207280"
  },
  {
    "start": "1203000",
    "end": "1227000"
  },
  {
    "text": "so we're not going to wait you know we're not going to do it in serial we're waiting for one to come back and we'll do another right we're going to do them",
    "start": "1207280",
    "end": "1212800"
  },
  {
    "text": "all at the same time so the entire execution is only as slow as the slowest worker and generally speaking when you",
    "start": "1212800",
    "end": "1218290"
  },
  {
    "text": "have this much data that all pretty much averages out there aren't too many outliers but it doesn't really matter if there are generally speaking you're",
    "start": "1218290",
    "end": "1225130"
  },
  {
    "text": "going to you're not going to have a problem here all right so as I mentioned",
    "start": "1225130",
    "end": "1231280"
  },
  {
    "start": "1227000",
    "end": "1322000"
  },
  {
    "text": "before the key thing here is that we have a point to collect these results and when you're tuning this the thing to",
    "start": "1231280",
    "end": "1238240"
  },
  {
    "text": "understand is this is what you have to tune for your data set based on your workload this has got to stay under the",
    "start": "1238240",
    "end": "1244090"
  },
  {
    "text": "provisioned amount of memory that you're allocating so you have to kind of understand what's going on there do some",
    "start": "1244090",
    "end": "1250030"
  },
  {
    "text": "error checking and most of the time I have not run you know I've not run into problems once you get your basic pattern",
    "start": "1250030",
    "end": "1256690"
  },
  {
    "text": "going and your files you're going to know ahead of time from s3 how big your files are so that's usually enough of a",
    "start": "1256690",
    "end": "1262210"
  },
  {
    "text": "track in fact you can split your batches up by size instead of number if you want to do it that way and what I have found",
    "start": "1262210",
    "end": "1268090"
  },
  {
    "text": "is that you can go full-bore 100 to one for a cascade to work or a ratio and you",
    "start": "1268090",
    "end": "1273520"
  },
  {
    "text": "can't actually go higher than that what I have found is that the maximum number of API calls you can make is about 100",
    "start": "1273520",
    "end": "1279940"
  },
  {
    "text": "per second and that's purely limited in node by nodes HTTP library how fast it",
    "start": "1279940",
    "end": "1286060"
  },
  {
    "text": "can go and that's because behind behind the scenes you're just making HTTP requests so that's actually your",
    "start": "1286060",
    "end": "1291370"
  },
  {
    "text": "limiting factor there's how many HTTP requests can I make at once and so now you see the importance of being able to",
    "start": "1291370",
    "end": "1297100"
  },
  {
    "text": "a cascade is not just to aggregate the workload but it's also to distribute the work of invoking the functions",
    "start": "1297100",
    "end": "1303160"
  },
  {
    "text": "themselves if you tried to just at that original nodejs process invoke a",
    "start": "1303160",
    "end": "1308290"
  },
  {
    "text": "thousand lambdas at once from one spot not only would you have too many results coming back to to handle from one you",
    "start": "1308290",
    "end": "1314410"
  },
  {
    "text": "wouldn't be able to invoke more than 100 at a time so you got though is going to be at least 10 seconds before all of",
    "start": "1314410",
    "end": "1320260"
  },
  {
    "text": "them are even invoked alright so the worker function against from the blog",
    "start": "1320260",
    "end": "1325690"
  },
  {
    "start": "1322000",
    "end": "1397000"
  },
  {
    "text": "this is almost hello world ish but basically we're going to take s3 and we're going to stream the files off of",
    "start": "1325690",
    "end": "1332960"
  },
  {
    "text": "s3 and this is really critical that we're not downloading the whole file because remember we're being paid per",
    "start": "1332960",
    "end": "1338809"
  },
  {
    "text": "hundred milliseconds so we do not want to be waiting around at any point we need to be saturating the CPU all the",
    "start": "1338809",
    "end": "1345260"
  },
  {
    "text": "time or paying too much and that's that's really your tuning factor is at any time did I have extra CPU that's a",
    "start": "1345260",
    "end": "1350929"
  },
  {
    "text": "bad sign so I want to make sure that I'm saturating at CPU this is great because you're not waiting for the whole file",
    "start": "1350929",
    "end": "1356840"
  },
  {
    "text": "the second that the head of the file arrives you get to start working so",
    "start": "1356840",
    "end": "1362240"
  },
  {
    "text": "you're not waiting for the tail you and if you just get the head and that's usually I was seeing 15 to 20 milliseconds from s 3 2 lambda I mean",
    "start": "1362240",
    "end": "1369169"
  },
  {
    "text": "we're talking really really fast especially if you've accessed that file recently and there's a little bit of caching going on got the FCS three team",
    "start": "1369169",
    "end": "1375590"
  },
  {
    "text": "how that works but I've seen some ridiculously fast downloads and that's great so the other thing that you want",
    "start": "1375590",
    "end": "1381200"
  },
  {
    "text": "to do is do this all currently and I'll get to that in a second so the the",
    "start": "1381200",
    "end": "1386240"
  },
  {
    "text": "basics line up here though we have our s3 it's Kurt you just to create read",
    "start": "1386240",
    "end": "1391340"
  },
  {
    "text": "stream you're going to unzip it in our case and then we'll split it on new lines and now we're operating on actual",
    "start": "1391340",
    "end": "1396740"
  },
  {
    "text": "events all right so these are some of the main tips and tricks here so we",
    "start": "1396740",
    "end": "1401809"
  },
  {
    "start": "1397000",
    "end": "1556000"
  },
  {
    "text": "talked about downloading concurrently this is a really interesting one you want to make sure that you are using for",
    "start": "1401809",
    "end": "1407659"
  },
  {
    "text": "a workload like this as much memory as you can throw at it if any of you use lambda before you know that they chart",
    "start": "1407659",
    "end": "1414020"
  },
  {
    "text": "that they allocate vcpu time in accordance with the RAM so you're not",
    "start": "1414020",
    "end": "1419630"
  },
  {
    "text": "necessarily going to use much of this memory at all but you're asking for basically how much percentage of one cpu",
    "start": "1419630",
    "end": "1425240"
  },
  {
    "text": "time are you going to use so unless you want to be waiting around for no reason if you're going to saturate that cpu ask",
    "start": "1425240",
    "end": "1430880"
  },
  {
    "text": "for the whole the whole thing let me give an example let's say you did 250 Meg for your your lambda sighs you're",
    "start": "1430880",
    "end": "1439850"
  },
  {
    "text": "going to run at a hundred percent and it's going to take four times as long as running a hundred percent at one gig",
    "start": "1439850",
    "end": "1445370"
  },
  {
    "text": "right so there's no reason if you can saturate the CPU to ever go less than full throttle in our experience",
    "start": "1445370",
    "end": "1452990"
  },
  {
    "text": "downloading 223 Meg as three keys is a you do 5 10 15 somewhere in that range works pretty well again whenever you do",
    "start": "1452990",
    "end": "1460520"
  },
  {
    "text": "whatever you can do to make sure you're saturating the CPU this was kind of interesting so our team did some",
    "start": "1460520",
    "end": "1465750"
  },
  {
    "text": "research on compression and generally speaking z live is always your best bet for the size 22 decompression speed",
    "start": "1465750",
    "end": "1472559"
  },
  {
    "text": "trade-off and in this case since you're paying for storage and s3 you that's usually where you want to be is in that",
    "start": "1472559",
    "end": "1478259"
  },
  {
    "text": "sweet spot however if you find you're in a use case where you're doing a lot more",
    "start": "1478259",
    "end": "1483480"
  },
  {
    "text": "access than you are storage like maybe you're not storing thanks for very long LZ for with that compression might be",
    "start": "1483480",
    "end": "1489360"
  },
  {
    "text": "worth your time and the reason is while the compression speed is about the same and the files are slightly larger we're",
    "start": "1489360",
    "end": "1495419"
  },
  {
    "text": "talking 25 maybe forty percent larger which is not good for storage you can get really fast decompress and that",
    "start": "1495419",
    "end": "1502649"
  },
  {
    "text": "means a lot less lambda CPU being used so that might be worth your time to look at again if you're not going to access",
    "start": "1502649",
    "end": "1508259"
  },
  {
    "text": "these files very often though you're probably going to be paying too much in s3 storage to outweigh the CPU costs but",
    "start": "1508259",
    "end": "1513720"
  },
  {
    "text": "that's something to look out for your your basic use case and the last thing and this should be obvious to everyone",
    "start": "1513720",
    "end": "1519269"
  },
  {
    "text": "but it's really important that in this cascade structure that your errors are getting boiled all the way up so even",
    "start": "1519269",
    "end": "1525539"
  },
  {
    "text": "the the weird ones all the way down need to be trickled all the way up so that the user knows in our case it's very important if there was an error anywhere",
    "start": "1525539",
    "end": "1532049"
  },
  {
    "text": "along the way that contains our result because if you say have we ever seen this bad guy in this giant mountain of",
    "start": "1532049",
    "end": "1537960"
  },
  {
    "text": "data and there's an error we want to know two things okay yes we want to know if there's an error because that might",
    "start": "1537960",
    "end": "1543059"
  },
  {
    "text": "invalidate a result and we can't be sure about it but we also if this is a big job we want to know where that error was",
    "start": "1543059",
    "end": "1548190"
  },
  {
    "text": "because we don't want have to run the whole batch again we want to only have to run that sliver where the error was so really full error reporting is really",
    "start": "1548190",
    "end": "1555299"
  },
  {
    "text": "important here now the last thing I want",
    "start": "1555299",
    "end": "1560669"
  },
  {
    "start": "1556000",
    "end": "1600000"
  },
  {
    "text": "or not the last thing but the next time we talk about is the fact that you don't have to pull this off of s3 so everything I've shown so far has been s3",
    "start": "1560669",
    "end": "1566940"
  },
  {
    "text": "or anything cuz that's how we use it but it doesn't have to be that way these are just computers they're just computers",
    "start": "1566940",
    "end": "1572370"
  },
  {
    "text": "running for a short amount of time and doing whatever you're telling them to do and as we learned today they could be",
    "start": "1572370",
    "end": "1577559"
  },
  {
    "text": "running in a VPC so you can go out to the internet you can do dynamo you already asked you do Kinesis and very",
    "start": "1577559",
    "end": "1583470"
  },
  {
    "text": "interestingly you can hit your own api's maybe you've got your own cloud front-rear aren't ec2 instances running your own you know proprietary stuff",
    "start": "1583470",
    "end": "1590940"
  },
  {
    "text": "there's no reason that these guys can't come and hit that and you use the data there it doesn't have to be s3 so anything that you need extra",
    "start": "1590940",
    "end": "1597509"
  },
  {
    "text": "cpu for you can grab that in a hundred milliseconds so as it was a little example and i didn't just lose code was",
    "start": "1597509",
    "end": "1604470"
  },
  {
    "text": "not worth releasing but i thought it was an interesting test case so one day I",
    "start": "1604470",
    "end": "1610080"
  },
  {
    "text": "was thought I want to know what certain follow what do my followers feel about something and I thought I thought of",
    "start": "1610080",
    "end": "1617820"
  },
  {
    "start": "1612000",
    "end": "1689000"
  },
  {
    "text": "this because i came across the sentiment analysis engine and i thought has kind of cool natural language processing that's always a fun you know something",
    "start": "1617820",
    "end": "1625139"
  },
  {
    "text": "to look at and for us it's not really super importance is more a toy but for a lot of places it's you know bread and",
    "start": "1625139",
    "end": "1630629"
  },
  {
    "text": "butter and so there's a lot of good libraries out there and I thought well wouldn't be interesting to see if you",
    "start": "1630629",
    "end": "1636059"
  },
  {
    "text": "give it any user name on twitter go dump all their followers using the twitter",
    "start": "1636059",
    "end": "1641399"
  },
  {
    "text": "api and then we'll vocal anda to operate to to run the sentiment analysis on each",
    "start": "1641399",
    "end": "1646739"
  },
  {
    "text": "one so the inputs are username and a",
    "start": "1646739",
    "end": "1652350"
  },
  {
    "text": "term to look for and we're going to so we're going to search twitter for that user name and this term and just we're",
    "start": "1652350",
    "end": "1659190"
  },
  {
    "text": "going to analyze it the sentiment in lambda for positive negative and neutral and hopefully this animation shows that",
    "start": "1659190",
    "end": "1666269"
  },
  {
    "text": "looks like it is there we go so this is pretty cool I mean it's fairly useless",
    "start": "1666269",
    "end": "1671970"
  },
  {
    "text": "but it's a it's important to show that we're getting data back to the user right away in a very visceral visceral",
    "start": "1671970",
    "end": "1679649"
  },
  {
    "text": "way and I'm just looping this but that was kind of I ran through a bunch of",
    "start": "1679649",
    "end": "1685529"
  },
  {
    "text": "these and this was probably the best so",
    "start": "1685529",
    "end": "1691109"
  },
  {
    "start": "1689000",
    "end": "1701000"
  },
  {
    "text": "let's talk about the importance of streaming results and this is this is really the different the difference between doing your standard Big Data EMR",
    "start": "1691109",
    "end": "1698940"
  },
  {
    "text": "style batches versus something like lambda and that is progressive results so in New X time if you see a spinner",
    "start": "1698940",
    "end": "1707190"
  },
  {
    "start": "1701000",
    "end": "1786000"
  },
  {
    "text": "for 30 seconds your app is broken that thing probably never coming back if it is that's a terrible UX right 30 seconds",
    "start": "1707190",
    "end": "1714840"
  },
  {
    "text": "is ridiculous I mean three seconds is ridiculous most of the time when people are used to doing search results they",
    "start": "1714840",
    "end": "1719909"
  },
  {
    "text": "may tolerate you know two three four five seconds but 30 seconds that's a long time so even in our small Orlando",
    "start": "1719909",
    "end": "1726389"
  },
  {
    "text": "search we still need something to come back right away and we don't want to dump progress bar because the progress bar",
    "start": "1726389",
    "end": "1731720"
  },
  {
    "text": "says go do something else that's not what we want we want our users to be looking at the app and we might actually",
    "start": "1731720",
    "end": "1737120"
  },
  {
    "text": "be able to tell them something very helpful right now so we want to go beyond a progress bar we want to show the results that we have so far because",
    "start": "1737120",
    "end": "1744170"
  },
  {
    "text": "they may already be very interesting case in point you're looking for a bad guy you put in let's say it's a bad IP",
    "start": "1744170",
    "end": "1750500"
  },
  {
    "text": "address and something pops up right away and you start getting a bazillion results and you realize like okay either",
    "start": "1750500",
    "end": "1755570"
  },
  {
    "text": "this is very bad or i typo'd this so that's that's something you can answer within three seconds and then you know",
    "start": "1755570",
    "end": "1761120"
  },
  {
    "text": "okay well either continue this and let it run and see like the actual final number because maybe we need like an exact count and that's really important",
    "start": "1761120",
    "end": "1767660"
  },
  {
    "text": "for the investigation or we know we're not interested in this at all and we don't need to let the whole thing finish so that's not a brand new idea but I",
    "start": "1767660",
    "end": "1774020"
  },
  {
    "text": "think it's pretty new from a big data standpoint and I think it's really",
    "start": "1774020",
    "end": "1782060"
  },
  {
    "text": "important that you're not just getting a stream of text back that you actually have graphics that are showing what's going on so this is a really really",
    "start": "1782060",
    "end": "1789110"
  },
  {
    "start": "1786000",
    "end": "1846000"
  },
  {
    "text": "rough prototype I had a while back and this is what it looks like when the things come back right so you guys are",
    "start": "1789110",
    "end": "1795680"
  },
  {
    "text": "probably all seen this in some other products but the fact that this is coming across something like thirty five",
    "start": "1795680",
    "end": "1801350"
  },
  {
    "text": "terabytes is a little bit more interesting and one one thing that's kind of cool about as you can see right away that okay the first thing you know",
    "start": "1801350",
    "end": "1807650"
  },
  {
    "text": "is we had results in all of these time periods right there was no time period that we didn't have a result for that's that's an immediate assertion that we",
    "start": "1807650",
    "end": "1814910"
  },
  {
    "text": "can make and if we were going to just have a spinning progress bar you would never know that the second thing is that",
    "start": "1814910",
    "end": "1820310"
  },
  {
    "text": "it's kind of like a fractal where you things get less and less blurry the more you have and so you can see that there",
    "start": "1820310",
    "end": "1826880"
  },
  {
    "text": "are spikes that show up and they get more and more defined as time goes on but there's still value in the immediate spikes and on the left you can see that",
    "start": "1826880",
    "end": "1833540"
  },
  {
    "text": "we have well maybe you can't but there's distinct values and field counts that grow as the as the search runs and so at",
    "start": "1833540",
    "end": "1841280"
  },
  {
    "text": "the final and then you see that the most granular search results and says hey we're all done",
    "start": "1841280",
    "end": "1847120"
  },
  {
    "text": "alright so that's that's most of the super technical stuff but I really wanted to spend some time this talk to",
    "start": "1850480",
    "end": "1856009"
  },
  {
    "text": "really emphasize how important lambda is and I think Werner's still a little bit",
    "start": "1856009",
    "end": "1861529"
  },
  {
    "text": "of my thunder in the the keynote today because what what he said was so true",
    "start": "1861529",
    "end": "1866960"
  },
  {
    "text": "that we've never had this power before this is the first time we've ever been able to spin up this much compute this",
    "start": "1866960",
    "end": "1872299"
  },
  {
    "text": "quickly this easily there's nothing like it and I don't know that everyone is",
    "start": "1872299",
    "end": "1877519"
  },
  {
    "text": "really wrap their head around the fact that they don't need to do a lot of work to get a lot of CPU really fast but",
    "start": "1877519",
    "end": "1883460"
  },
  {
    "text": "that's only where it starts so you bunch of other talks talking about service architecture and these are huge these",
    "start": "1883460",
    "end": "1888649"
  },
  {
    "text": "are such a big deal because let's think about what it means if we didn't have servers we don't have operating systems",
    "start": "1888649",
    "end": "1894950"
  },
  {
    "text": "and we don't have networking what are we left working on just the stuff that matters to our customers and that's",
    "start": "1894950",
    "end": "1901009"
  },
  {
    "text": "what's so powerful to me is that the only code that I'm writing is code that is directly benefiting the customer",
    "start": "1901009",
    "end": "1907129"
  },
  {
    "text": "there's no overhead code there's no infrastructure code I don't care how or where it's executing it's going to get",
    "start": "1907129",
    "end": "1912379"
  },
  {
    "text": "done and it's going to be just the bare minimum that I wrote so we can focus on",
    "start": "1912379",
    "end": "1918559"
  },
  {
    "text": "our core competencies the other nice part is on the DevOps I'd so when you're right in the code if it worked once it",
    "start": "1918559",
    "end": "1924409"
  },
  {
    "start": "1919000",
    "end": "1970000"
  },
  {
    "text": "always works unit testing is really really easy somebody has a node project",
    "start": "1924409",
    "end": "1930289"
  },
  {
    "text": "out there I saw it on Twitter and it will emulate a lambda run just from a",
    "start": "1930289",
    "end": "1935570"
  },
  {
    "text": "unit testing perspective so that's available if you want to look at using that for something that's either not on",
    "start": "1935570",
    "end": "1940730"
  },
  {
    "text": "the internet or something like that if you need to do air-gapped unit tests but honestly when you're only doing one I",
    "start": "1940730",
    "end": "1946999"
  },
  {
    "text": "mean that we're talking you know hundreds of one cent worth of execution you can afford to have like Jenkins or",
    "start": "1946999",
    "end": "1953210"
  },
  {
    "text": "Travis or something actually spin up a real lambda in that case it's not going to it's going to cause any sort of",
    "start": "1953210",
    "end": "1958249"
  },
  {
    "text": "significant costs so you can actually run the real lambda and see it go and this kind of reminds me of containers",
    "start": "1958249",
    "end": "1968590"
  },
  {
    "text": "so you have the same situation where the great thing about docker is that inter",
    "start": "1971810",
    "end": "1976940"
  },
  {
    "text": "settled I think it's actually an onion article where they said dev guy doesn't",
    "start": "1976940",
    "end": "1983720"
  },
  {
    "text": "understand why ops guy doesn't care at work locally right and so like I worked for me I don't know why it's not working",
    "start": "1983720",
    "end": "1989300"
  },
  {
    "text": "in production and so doctor helps a lot with that right and say well I have this exact environment i'm going to put it",
    "start": "1989300",
    "end": "1994310"
  },
  {
    "text": "out here I mean we go bond beyond that with lambda 2 lambda says it's the exact same code running in the exact same",
    "start": "1994310",
    "end": "2000220"
  },
  {
    "text": "environment and you only have to worry about what the environment looks like or what the networking looks like or any of that stuff we always know where that",
    "start": "2000220",
    "end": "2006880"
  },
  {
    "text": "code is and that code can be updated without any patching when you're executing a lambda it's the same code",
    "start": "2006880",
    "end": "2013030"
  },
  {
    "text": "that you said update lambda function and it's right up here and whenever a lambda execute they go and they grab it that's",
    "start": "2013030",
    "end": "2019180"
  },
  {
    "text": "the exact opposite of something like ansible or puppet where you're pushing down all your patches to everything and",
    "start": "2019180",
    "end": "2025180"
  },
  {
    "text": "hoping that it gets to everything this is a situation where you update it once and you're guaranteed that every time that runs from then on is going to be",
    "start": "2025180",
    "end": "2031300"
  },
  {
    "text": "running the exact code that you uploaded all right so I'm coming up to the end",
    "start": "2031300",
    "end": "2037690"
  },
  {
    "start": "2035000",
    "end": "2129000"
  },
  {
    "text": "here it occurred to me that when I was talking about well what do we need servers for in the last job that i",
    "start": "2037690",
    "end": "2044890"
  },
  {
    "text": "worked at i was at a data center and we had a couple of really giant mainframes",
    "start": "2044890",
    "end": "2049929"
  },
  {
    "text": "and some people twice my age that operated them and their stuff pretty",
    "start": "2049930",
    "end": "2055600"
  },
  {
    "text": "much never broke and it was really interesting it didn't do anything that i thought was cool i mean it's just you",
    "start": "2055600",
    "end": "2060820"
  },
  {
    "text": "know finance stuff or whatever you know nothing super flashy like d3 right but",
    "start": "2060820",
    "end": "2066220"
  },
  {
    "text": "it was important and it always worked and they didn't have to care about networking they made some help our stuff but generally speaking they didn't care",
    "start": "2066220",
    "end": "2072790"
  },
  {
    "text": "how their code is getting executed they just wrote it now why isn't that super prevalent in the 80 in 80's and 90's",
    "start": "2072790",
    "end": "2079629"
  },
  {
    "text": "well people had graphical user interfaces and that's kind of anti mainframe because they don't have any",
    "start": "2079630",
    "end": "2084909"
  },
  {
    "text": "bandwidth back then right so you're on your 2400 baud modem if you're lucky back then you're not going to have any",
    "start": "2084910",
    "end": "2090580"
  },
  {
    "text": "kind of significant graphical experience that way so now fast forward we have",
    "start": "2090580",
    "end": "2096820"
  },
  {
    "text": "both right we have the ability we have the cloud we have these computers that you don't see on a daily",
    "start": "2096820",
    "end": "2102280"
  },
  {
    "text": "basis and we have broadband that's actually going to bring down all that rich content as quickly as it can and so",
    "start": "2102280",
    "end": "2109030"
  },
  {
    "text": "we have the centralized compute and we have everything we need to from there and I think it's important to just kind",
    "start": "2109030",
    "end": "2114670"
  },
  {
    "text": "of look back and say I mean so off we think are things brandy or links brand new but you know we've got a lot of",
    "start": "2114670",
    "end": "2119830"
  },
  {
    "text": "repetition and there's a lot of lessons that we can learn from from back then on the other hand things are much improved and that now it's much easier to get",
    "start": "2119830",
    "end": "2126760"
  },
  {
    "text": "things deployed and there's a couple related sessions I think this one ended",
    "start": "2126760",
    "end": "2133210"
  },
  {
    "start": "2129000",
    "end": "2151000"
  },
  {
    "text": "up being scheduled alongside so that was kind of unfortunate but I do encourage you to check that out when I had submit",
    "start": "2133210",
    "end": "2139390"
  },
  {
    "text": "the slides they didn't have the schedule down all right so please remember to put your evaluations and are there any",
    "start": "2139390",
    "end": "2146140"
  },
  {
    "text": "questions",
    "start": "2146140",
    "end": "2148319"
  }
]