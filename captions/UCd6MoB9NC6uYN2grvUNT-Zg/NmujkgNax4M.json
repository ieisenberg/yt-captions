[
  {
    "start": "0",
    "end": "26000"
  },
  {
    "text": "and so please help me welcome to the stage door Senora combs [Music]",
    "start": "359",
    "end": "10300"
  },
  {
    "text": "so I'm here both of my role as faculty member at Cornell as well as Amazon",
    "start": "12410",
    "end": "17850"
  },
  {
    "text": "scholar working with Amazon music because in both roles what I do is do",
    "start": "17850",
    "end": "23160"
  },
  {
    "text": "research on learning from interaction logs and what I mean by intellectual logs it's the logs from these systems that",
    "start": "23160",
    "end": "29789"
  },
  {
    "start": "26000",
    "end": "26000"
  },
  {
    "text": "we're all collectively building and using and you know app placement systems search engines ecommerce sites",
    "start": "29789",
    "end": "37260"
  },
  {
    "text": "recommender systems and more and more also systems that kind of interface with the physical worlds like smart homes and",
    "start": "37260",
    "end": "43469"
  },
  {
    "text": "self-driving cars all of these systems lock the interactions that our users",
    "start": "43469",
    "end": "49559"
  },
  {
    "text": "have with them and clearly these logs contain a lot of interesting information because they reveal the preferences that",
    "start": "49559",
    "end": "56250"
  },
  {
    "text": "our users have but dealing with these logs is also tricky because our systems",
    "start": "56250",
    "end": "61469"
  },
  {
    "text": "themselves bias the data that we are collecting now but if we can get these",
    "start": "61469",
    "end": "66840"
  },
  {
    "text": "this this bias out of the data then these interaction logs are really a",
    "start": "66840",
    "end": "72000"
  },
  {
    "text": "great source for measuring how our systems are doing for learning and improving these systems and for",
    "start": "72000",
    "end": "78390"
  },
  {
    "text": "personalization so let me walk you through a couple of examples what I mean",
    "start": "78390",
    "end": "83460"
  },
  {
    "start": "81000",
    "end": "81000"
  },
  {
    "text": "by interaction logs and they all have this structure that we have a context X and action Y that our current system is",
    "start": "83460",
    "end": "89850"
  },
  {
    "text": "taking and then we observe some feedback Delta so for an ad placement system like",
    "start": "89850",
    "end": "95009"
  },
  {
    "text": "the one where that be F here the context may be the user and the page that we're dealing with our current system places a",
    "start": "95009",
    "end": "102570"
  },
  {
    "text": "particular ad like this Malaysia Airlines ad on the top right corner there and then for that ad we get to",
    "start": "102570",
    "end": "108810"
  },
  {
    "text": "observe the feedback whether the user clicks on it or not but what we don't get to observe is the counterfactual",
    "start": "108810",
    "end": "115439"
  },
  {
    "text": "outcomes what would have happened if our current system had placed a different ad there we don't get to see what whether",
    "start": "115439",
    "end": "122100"
  },
  {
    "text": "user would have clicked or not or think about a news recommender system right puts together a personalized newspaper",
    "start": "122100",
    "end": "128629"
  },
  {
    "text": "here the context X is some user profile our current system takes an action why",
    "start": "128629",
    "end": "135750"
  },
  {
    "text": "puts together this personalized front page the selection of articles and the layout and then for that particular page",
    "start": "135750",
    "end": "142320"
  },
  {
    "text": "we get to observe let's say the reading time which we use as a product as a proxy for user satisfaction but again we",
    "start": "142320",
    "end": "149340"
  },
  {
    "text": "don't get to see what would have happened if we had put together a different front page here or last",
    "start": "149340",
    "end": "157350"
  },
  {
    "start": "157000",
    "end": "157000"
  },
  {
    "text": "example of search engine here the context X is a query the action that the system is taking is presenting this",
    "start": "157350",
    "end": "163200"
  },
  {
    "text": "particular ranking and then we have some click metric Delta that we want to optimize but again we don't get to see",
    "start": "163200",
    "end": "168660"
  },
  {
    "text": "what would have happened if you had presented a different ranking so in a sense what we are getting here machine",
    "start": "168660",
    "end": "175080"
  },
  {
    "text": "learning terms is bandit feedback these triples X Y and Delta but it's biased by",
    "start": "175080",
    "end": "183420"
  },
  {
    "text": "the actions that our current system is taking right we only get to observe the feedback for the actions that we try so",
    "start": "183420",
    "end": "192930"
  },
  {
    "text": "nevertheless what we want to do is you want to learn a new system an improved system in you rank and function let's",
    "start": "192930",
    "end": "198060"
  },
  {
    "text": "say that selects actions why with even better deltas so how can we do this",
    "start": "198060",
    "end": "203430"
  },
  {
    "text": "that's the question that I want to address in this talk you think about it",
    "start": "203430",
    "end": "208620"
  },
  {
    "start": "207000",
    "end": "207000"
  },
  {
    "text": "what we're trying to do here is batch learning from bandit feedback right if",
    "start": "208620",
    "end": "214350"
  },
  {
    "text": "you kind of partition machine learning into these four quadrants we have online learning with interactive control and",
    "start": "214350",
    "end": "219510"
  },
  {
    "text": "batch learning where we're just learning from existing data and we have full information feedback where we have like",
    "start": "219510",
    "end": "225450"
  },
  {
    "text": "hand labelled data and disbanded feedback then we are in that bottom right corner right and if you think",
    "start": "225450",
    "end": "233610"
  },
  {
    "text": "about it all these other quadrants are very well supplied with machine learning methods but what's happening in that",
    "start": "233610",
    "end": "240420"
  },
  {
    "text": "bottom right corner that's what I want to explore in this talk and it's not that there isn't enough data actually",
    "start": "240420",
    "end": "245730"
  },
  {
    "text": "lots of data terabytes of data all of these locks files low in that corner ok so what I want to do in this talk is",
    "start": "245730",
    "end": "253650"
  },
  {
    "text": "first think about you know what is actually the problem and get at the heart of the problem and I'm going to",
    "start": "253650",
    "end": "259859"
  },
  {
    "text": "bring it down into what we call counterfactual risk minimization which is a way of constructing learning out",
    "start": "259859",
    "end": "266340"
  },
  {
    "text": "it's for this particular setting that's rating from bandit feedback and then put",
    "start": "266340",
    "end": "271680"
  },
  {
    "text": "these kind of theoretical framework into practice and show you two algorithms that we've developed one for training",
    "start": "271680",
    "end": "277710"
  },
  {
    "text": "conditional random fields called poem and the other one called bandit net for training deep models based on this log",
    "start": "277710",
    "end": "284280"
  },
  {
    "text": "data okay so where to start well it's",
    "start": "284280",
    "end": "290880"
  },
  {
    "start": "287000",
    "end": "287000"
  },
  {
    "text": "always a good idea to start and think well maybe you've solved problems that are kind of like that already and then",
    "start": "290880",
    "end": "297180"
  },
  {
    "text": "we can maybe adapt some of the methods that we have right and if I think about you know the setup of context X",
    "start": "297180",
    "end": "303570"
  },
  {
    "text": "extra-wide effect delta that we have in our online systems it actually sounds a lot like what you would have in a",
    "start": "303570",
    "end": "310710"
  },
  {
    "start": "309000",
    "end": "309000"
  },
  {
    "text": "medical treatment setting right so they would have a context X which maybe",
    "start": "310710",
    "end": "315720"
  },
  {
    "text": "you're patient have described by lab tests let's say then you have to decide on a treatment Y you take an action and",
    "start": "315720",
    "end": "323130"
  },
  {
    "text": "then for that particular treatment maybe surgery you get to see the outcome right",
    "start": "323130",
    "end": "328620"
  },
  {
    "text": "and you want to improve the outcome but again you don't get to see what would have happened if you had given that",
    "start": "328620",
    "end": "333720"
  },
  {
    "text": "person a different treatment right that's counterfactual outcomes are not observed so how do we typically deal",
    "start": "333720",
    "end": "340229"
  },
  {
    "text": "with these settings in the medical domain right and the gold standard there is to do causal inference we are",
    "start": "340229",
    "end": "346289"
  },
  {
    "text": "controlled randomized trials now what I",
    "start": "346289",
    "end": "351660"
  },
  {
    "text": "want to do in this talk is rethink what we're doing in online systems in terms of counterfactual and causal inference",
    "start": "351660",
    "end": "358020"
  },
  {
    "text": "and controlled randomized trials and clearly you know many of the parallels",
    "start": "358020",
    "end": "363150"
  },
  {
    "text": "break down like you know the action space in the medical domain is typically you know just have a handful of different treatments",
    "start": "363150",
    "end": "369330"
  },
  {
    "text": "whereas in in our cases we have like combinatorial space of all rankings that are all treatments but nevertheless",
    "start": "369330",
    "end": "376229"
  },
  {
    "text": "maybe we can adopt some of the ideas there so let me distort by defining a couple",
    "start": "376229",
    "end": "383310"
  },
  {
    "text": "of things in particular whatever we want to learn what we really want to learn is we want to learn a policy and a policy is just a fancy word for a mapping pie",
    "start": "383310",
    "end": "391139"
  },
  {
    "text": "that takes a context and says what to do why which action to take and in",
    "start": "391139",
    "end": "396479"
  },
  {
    "text": "particular we'll think of stochastic policies for reasons which will become clear later",
    "start": "396479",
    "end": "401699"
  },
  {
    "text": "and so a policy is just a conditional probability distribution given a context",
    "start": "401699",
    "end": "407100"
  },
  {
    "text": "X draw actions from this distribution PI you want to visualize that think of the",
    "start": "407100",
    "end": "415169"
  },
  {
    "text": "gray box there as all the possible actions that we could take in our contacts current context X and then",
    "start": "415169",
    "end": "421680"
  },
  {
    "text": "policy PI one would draw actions from that distribution and policy PI 2 there's a different policy with draw",
    "start": "421680",
    "end": "428880"
  },
  {
    "text": "actions from a different distribution and clearly deterministic policies are just a special case of this you put all",
    "start": "428880",
    "end": "434820"
  },
  {
    "text": "the probability mass on one of the actions right ok so that's what we want to learn how do we tell whether you know",
    "start": "434820",
    "end": "442050"
  },
  {
    "text": "how do we measure performance and we'll do that in terms of utility which is just the expectation of our feedback",
    "start": "442050",
    "end": "449490"
  },
  {
    "text": "Delta the thing that we want to maximize or optimize and so again if I you know",
    "start": "449490",
    "end": "456030"
  },
  {
    "text": "it's just computing this expectation here if you do that a graphical terms the policy PI 1",
    "start": "456030",
    "end": "461759"
  },
  {
    "text": "you know the shading indicates the value of our feedback red being bad green being good and so like the policy PI 1",
    "start": "461759",
    "end": "470490"
  },
  {
    "text": "here would sample so it takes mostly red actions so it's not so good the expectation is kind of reddish and",
    "start": "470490",
    "end": "477650"
  },
  {
    "text": "policy PI 2 takes mostly green actions so the you know expectation is greenish it's better right so that's what we want",
    "start": "477650",
    "end": "486870"
  },
  {
    "text": "to optimize but unfortunately just like it's just like prediction error right",
    "start": "486870",
    "end": "491969"
  },
  {
    "text": "it's something we can compute Y can't be computer well the PI the policy that's not the problem",
    "start": "491969",
    "end": "499530"
  },
  {
    "text": "right we built this this is our system that we want to evaluate so we actually know this so that's not the problem the",
    "start": "499530",
    "end": "507090"
  },
  {
    "text": "P of X yeah that's our distribution of context like users coming to our side we",
    "start": "507090",
    "end": "512339"
  },
  {
    "text": "don't exactly know that but we can draw a large sample so that's not a big problem either the problem is that we",
    "start": "512339",
    "end": "518578"
  },
  {
    "text": "don't know most of the deltas right we don't know the feedback for most of the contacts action pairs so we have a lot",
    "start": "518579",
    "end": "527040"
  },
  {
    "text": "of missing data so what are we going to do well the standard thing to do",
    "start": "527040",
    "end": "532889"
  },
  {
    "text": "to run an a/b test right then we can estimate those utility and maybe test",
    "start": "532889",
    "end": "538980"
  },
  {
    "text": "just means we take our current policy our corn system field it give it to a",
    "start": "538980",
    "end": "546029"
  },
  {
    "text": "random sample of our users that's basically a controlled randomized trial right and then we just take the average",
    "start": "546029",
    "end": "553109"
  },
  {
    "text": "of the deltas and that's an unbiased estimate of the performance of our",
    "start": "553109",
    "end": "558299"
  },
  {
    "text": "system of the utility of the policy this is great in terms of like cause of",
    "start": "558299",
    "end": "565529"
  },
  {
    "text": "validity and you know both in in medical settings and in in the online system",
    "start": "565529",
    "end": "572339"
  },
  {
    "text": "setting that's kind of the the gold standard for establishing how good a policy is how good a system is right but",
    "start": "572339",
    "end": "578249"
  },
  {
    "text": "unfortunately in terms of efficiency it's really bad right so what's",
    "start": "578249",
    "end": "584819"
  },
  {
    "start": "582000",
    "end": "582000"
  },
  {
    "text": "happening here if you were if you actually have a lot of policies that we want to evaluate then what we would need",
    "start": "584819",
    "end": "590459"
  },
  {
    "text": "to do we need to implement the policy productionize it filled in our system",
    "start": "590459",
    "end": "596489"
  },
  {
    "text": "and run it for at least a week to get you know it's a you know get rid of the",
    "start": "596489",
    "end": "602059"
  },
  {
    "text": "cyclical nature of our data and so if we do that for many different policies that",
    "start": "602059",
    "end": "608160"
  },
  {
    "text": "takes forever right this will give us a really low kind of turnaround cycle development cycle but we're much rather",
    "start": "608160",
    "end": "616889"
  },
  {
    "text": "do is something like offline a be testing right it's the online testing is",
    "start": "616889",
    "end": "622649"
  },
  {
    "text": "also really wasteful we collect the data once and then we never use it again so",
    "start": "622649",
    "end": "629279"
  },
  {
    "text": "couldn't we just collect a whole bunch of data first actually we already have that we have terabytes of log data lying",
    "start": "629279",
    "end": "635040"
  },
  {
    "text": "around probably right including the data from the old a B tests and then using",
    "start": "635040",
    "end": "641399"
  },
  {
    "text": "just the old data evaluate a whole bunch of new policies kind of in an offline a",
    "start": "641399",
    "end": "647069"
  },
  {
    "text": "B test if you were able to do with that we could get you know turnaround times",
    "start": "647069",
    "end": "652829"
  },
  {
    "text": "in seconds and we could actually do learning right if we could do that for a",
    "start": "652829",
    "end": "658079"
  },
  {
    "text": "lot of policies we could basically use this as a form of training error that we could optimize",
    "start": "658079",
    "end": "663649"
  },
  {
    "text": "so what we want is we want counterfactual estimates of what would",
    "start": "663990",
    "end": "671040"
  },
  {
    "text": "have happened or what would have my performance been if I had used a different policy instead of the policy",
    "start": "671040",
    "end": "677250"
  },
  {
    "text": "that actually collected the data or by policy PI now sounds great right but",
    "start": "677250",
    "end": "685520"
  },
  {
    "text": "what's the problem here the problem is that we don't actually have the full",
    "start": "685520",
    "end": "690720"
  },
  {
    "start": "687000",
    "end": "687000"
  },
  {
    "text": "data to do this right so in particular we you know that the",
    "start": "690720",
    "end": "696420"
  },
  {
    "text": "situation doesn't look like this where you know we know the whole Delta but more looks like this right in our data",
    "start": "696420",
    "end": "702030"
  },
  {
    "text": "in our log file however big it is we will only see some dots right those are the actions that we've actually tried",
    "start": "702030",
    "end": "709610"
  },
  {
    "text": "but this you know if we have a new policy PI that we want to evaluate that you know when we wallet it wants to",
    "start": "709610",
    "end": "716130"
  },
  {
    "text": "evaluate what the black action is like chances are it's not in our sample right",
    "start": "716130",
    "end": "722340"
  },
  {
    "text": "and we don't actually know what the Delta what the payoff for this action is",
    "start": "722340",
    "end": "728810"
  },
  {
    "text": "so the first thought that you may have is well let's build an imputation model",
    "start": "728810",
    "end": "734720"
  },
  {
    "text": "so let's go through that thought experiment and see what happens so we",
    "start": "734720",
    "end": "740760"
  },
  {
    "start": "735000",
    "end": "735000"
  },
  {
    "text": "could take all of our observed triples of data embed them in some nice feature",
    "start": "740760",
    "end": "746460"
  },
  {
    "text": "space and then a regression model that would then be able Delta hat that would",
    "start": "746460",
    "end": "752610"
  },
  {
    "text": "then be able to impute the value of Delta everywhere right so here this is",
    "start": "752610",
    "end": "759210"
  },
  {
    "text": "visualized again you would learn our model and now we can impute a value",
    "start": "759210",
    "end": "764310"
  },
  {
    "text": "everywhere so now if we have a black action here that we could just fill in",
    "start": "764310",
    "end": "770190"
  },
  {
    "text": "the value of the predicted value from our regression model unfortunately that",
    "start": "770190",
    "end": "775650"
  },
  {
    "text": "can go horribly wrong right especially if we are now optimizing over our model of the reward",
    "start": "775650",
    "end": "783360"
  },
  {
    "text": "you know it will tell us I'll go into that bottom right corner there that's where they're kind of greenness area of",
    "start": "783360",
    "end": "789540"
  },
  {
    "text": "the spaces so we're really at the mercy of how our model extrapolates and here",
    "start": "789540",
    "end": "796140"
  },
  {
    "text": "in this particular case there's actually no down there so we're really at the mercy of you know what how you know if they",
    "start": "796140",
    "end": "802860"
  },
  {
    "text": "have a mismatch model the predictions there could be way off so let's not go",
    "start": "802860",
    "end": "808710"
  },
  {
    "text": "this route because we get into biases that we that are difficult to control let's try something else",
    "start": "808710",
    "end": "814580"
  },
  {
    "text": "so the approach that we're actually going to use is going through a counterfactual estimator and in",
    "start": "814580",
    "end": "821790"
  },
  {
    "start": "816000",
    "end": "816000"
  },
  {
    "text": "particular using inverse propensity score waiting to watch IPS for short",
    "start": "821790",
    "end": "826980"
  },
  {
    "text": "and so IPS waiting just us the following it looks a lot like what we have in an a/b test but it waits the the feedback",
    "start": "826980",
    "end": "835110"
  },
  {
    "text": "Delta by this ratio of what's the probability and the new policy that we",
    "start": "835110",
    "end": "840270"
  },
  {
    "text": "want to evaluate divided by what's called the propensity and that's the probability of the action under the",
    "start": "840270",
    "end": "847410"
  },
  {
    "text": "policy that locked the data so again to",
    "start": "847410",
    "end": "853110"
  },
  {
    "text": "visualize that you know Pinal collected the data now we want to evaluate this",
    "start": "853110",
    "end": "859110"
  },
  {
    "text": "other policy pi what this inverse propensity score IPS estimator does it up weights the terms over on the right",
    "start": "859110",
    "end": "866790"
  },
  {
    "text": "and it down weights the observations on the left and if you do it in this way",
    "start": "866790",
    "end": "872970"
  },
  {
    "text": "there's a three line proof that shows that this is an unbiased estimate of the online performance so you get an",
    "start": "872970",
    "end": "880800"
  },
  {
    "text": "estimate the same you're estimating the same quantity that you would estimate in an online ad test but you're doing it by",
    "start": "880800",
    "end": "887910"
  },
  {
    "text": "reusing old data now one restriction",
    "start": "887910",
    "end": "893970"
  },
  {
    "text": "here is in the fine print is this is only an unbiased estimate if the new",
    "start": "893970",
    "end": "900060"
  },
  {
    "text": "policy is restricted to the actions that had nonzero probability under the",
    "start": "900060",
    "end": "905820"
  },
  {
    "text": "logging policy Pinal so from that perspective you know coming back to controlled randomized trials it's",
    "start": "905820",
    "end": "912750"
  },
  {
    "text": "actually beneficial to lock your data with some kind of random behavior in it",
    "start": "912750",
    "end": "920210"
  },
  {
    "start": "920000",
    "end": "920000"
  },
  {
    "text": "and well the simplest way of getting randomness into your system is to simply",
    "start": "920210",
    "end": "927030"
  },
  {
    "text": "make your long key policy a little bit random and I'll show an X sample of how to do this on a later",
    "start": "927030",
    "end": "933360"
  },
  {
    "text": "slide but even if you're not doing this your system probably is pretty random",
    "start": "933360",
    "end": "939089"
  },
  {
    "text": "already like the features X that describe your context are probably",
    "start": "939089",
    "end": "944310"
  },
  {
    "text": "somewhat random you've run a B test so the choice and the assignment of users to a policy is randomized user behavior",
    "start": "944310",
    "end": "953400"
  },
  {
    "text": "in your system is random and maybe a system is actually a little bit flaky and you know what was preceded about",
    "start": "953400",
    "end": "960630"
  },
  {
    "text": "before is now a feature right at least you would hope so if you have a little",
    "start": "960630",
    "end": "968370"
  },
  {
    "text": "bit of randomness in your system you can exploit this just like you would exploit this in a controlled randomized trial so",
    "start": "968370",
    "end": "976140"
  },
  {
    "text": "this gives us a way to run these offline a B tests and get unbiased estimates and",
    "start": "976140",
    "end": "982110"
  },
  {
    "text": "that's our pathway that's important for it's in itself right you can actually",
    "start": "982110",
    "end": "987150"
  },
  {
    "text": "use this to greatly speed up your development cycle but it's also a pathway for doing learning so what you",
    "start": "987150",
    "end": "996180"
  },
  {
    "text": "can basically do is you can use this inverse propensity score estimator as a",
    "start": "996180",
    "end": "1001940"
  },
  {
    "start": "998000",
    "end": "998000"
  },
  {
    "text": "form of unbiased estimate of training error right and then you can do learning",
    "start": "1001940",
    "end": "1008089"
  },
  {
    "text": "by minimizing training error or maximizing utility so basically what you",
    "start": "1008089",
    "end": "1014089"
  },
  {
    "text": "would do is you would find the policy that optimizes that estimated performance just like most machine",
    "start": "1014089",
    "end": "1021980"
  },
  {
    "text": "learning algorithms actually do empirical risk minimization but you do have to be a little careful because what",
    "start": "1021980",
    "end": "1028938"
  },
  {
    "text": "can happen is the following if you're searching the large policy space then",
    "start": "1028939",
    "end": "1035178"
  },
  {
    "text": "you can get into a particular new type of overfitting so let's say the policy PI one here that we're now evaluating",
    "start": "1035179",
    "end": "1042678"
  },
  {
    "text": "against our log data is kind of close to the existing policy that locked the data",
    "start": "1042679",
    "end": "1048470"
  },
  {
    "text": "and so we will take an average over many kind of small weights and so action our",
    "start": "1048470",
    "end": "1055220"
  },
  {
    "text": "estimate is going to be you know reasonably accurate in terms of variance but we may have some policy in our",
    "start": "1055220",
    "end": "1062120"
  },
  {
    "text": "policy space that's way up in the corner will be a very little de and if you do inverse propensity score",
    "start": "1062120",
    "end": "1067560"
  },
  {
    "text": "waiting there it would basically put all of the weight on that one point so we still have an unbiased estimate but over",
    "start": "1067560",
    "end": "1074190"
  },
  {
    "text": "one sample so it's going to have huge variance so you have to be careful about this and if you look at kind of a",
    "start": "1074190",
    "end": "1080970"
  },
  {
    "start": "1080000",
    "end": "1080000"
  },
  {
    "text": "learning theory behind these learning problems then you can actually derive generalization error bounds that tell",
    "start": "1080970",
    "end": "1086700"
  },
  {
    "text": "you that this is going on so in particular if you want to say something about the true utility like the",
    "start": "1086700",
    "end": "1093000"
  },
  {
    "text": "generalization error of the system you have to look at the training error you",
    "start": "1093000",
    "end": "1099000"
  },
  {
    "text": "have the typical capacity overfitting like vc-dimension term in there but you",
    "start": "1099000",
    "end": "1104940"
  },
  {
    "text": "have this new term which kind of counts for the variance of your estimator and that is different for different policies",
    "start": "1104940",
    "end": "1111150"
  },
  {
    "text": "so get an additional regularizer now the the numbers that come of these bounds",
    "start": "1111150",
    "end": "1117690"
  },
  {
    "text": "are typically not that accurate right but they these bounds do tell you what a",
    "start": "1117690",
    "end": "1122700"
  },
  {
    "text": "learning algorithm should be optimizing and so what we take away from that is this counterfactual risk minimization",
    "start": "1122700",
    "end": "1129480"
  },
  {
    "start": "1125000",
    "end": "1125000"
  },
  {
    "text": "principle that tells you what your algorithm for Batchelor need for bannard feedback should be optimizing and it's",
    "start": "1129480",
    "end": "1136800"
  },
  {
    "text": "basically that you should be optimizing this objective right optimize your training utility as measured by IPs and",
    "start": "1136800",
    "end": "1145880"
  },
  {
    "text": "you should account for variance regularization and for capacity regulation so so far for the theory",
    "start": "1145880",
    "end": "1156930"
  },
  {
    "text": "but can be actually take this and turn this into practical machine learning",
    "start": "1156930",
    "end": "1164190"
  },
  {
    "text": "algorithms so what I'm going to have two examples for doing this the first",
    "start": "1164190",
    "end": "1170670"
  },
  {
    "text": "example is called poem and it's basically way of training conditional random fields for structured prediction",
    "start": "1170670",
    "end": "1177120"
  },
  {
    "text": "but not on full information hand labelled data but on data that we get from your log files obviously much",
    "start": "1177120",
    "end": "1183900"
  },
  {
    "text": "cheaper so what do we need to do well we",
    "start": "1183900",
    "end": "1190200"
  },
  {
    "text": "have to define the policy space the the type of function that we want to learn and I said we wanted to learn a conditional random field and if you know",
    "start": "1190200",
    "end": "1196020"
  },
  {
    "text": "what a conditional random field is that's what it is right you have this kind of softmax function that's normalized to sum to 1",
    "start": "1196020",
    "end": "1203010"
  },
  {
    "text": "by this partition function z and then you have a joint feature map fee of x",
    "start": "1203010",
    "end": "1208380"
  },
  {
    "text": "and y that comes from a a conditional random field and then you want to learn this weight vector W and that's the",
    "start": "1208380",
    "end": "1217350"
  },
  {
    "text": "that's the thing that you're adjusting in your learning algorithm so you would",
    "start": "1217350",
    "end": "1223470"
  },
  {
    "start": "1223000",
    "end": "1223000"
  },
  {
    "text": "take this and plug it into this counterfactual risk minimization objective and then you basically plug",
    "start": "1223470",
    "end": "1231240"
  },
  {
    "text": "this into the IPS estimator as the capacity regularization we use the",
    "start": "1231240",
    "end": "1236670"
  },
  {
    "text": "typical l2 regularization that you have in a support vector machine or an a conditional random field typically but",
    "start": "1236670",
    "end": "1242760"
  },
  {
    "text": "then we have this additional variance organization term where you basically just have to look up an estimator for the variance there I'm sweeping another",
    "start": "1242760",
    "end": "1249750"
  },
  {
    "text": "rug how to optimize this but you can actually was a couple of optimization tricks you can do stochastic gradient",
    "start": "1249750",
    "end": "1254790"
  },
  {
    "text": "descent on this and you can do this large-scale so here's a sanity check and",
    "start": "1254790",
    "end": "1262370"
  },
  {
    "text": "kind of checking whether we can get the same performance training on lock data",
    "start": "1262370",
    "end": "1269640"
  },
  {
    "text": "that we get from full information data kind of training a conditional random",
    "start": "1269640",
    "end": "1275460"
  },
  {
    "text": "field in the traditional way so we took a text classification data set where we had manual labels for multi-class",
    "start": "1275460",
    "end": "1282290"
  },
  {
    "text": "multi-label classification tasks so we predicting a bit vector basically you know is it about sports is about",
    "start": "1282290",
    "end": "1287790"
  },
  {
    "text": "politics so there's a bit vector that detecting and then between a conditional random field in the traditional way on",
    "start": "1287790",
    "end": "1294120"
  },
  {
    "text": "this full information feedback and that's our skyline that's how good we could hope to possibly get now from this",
    "start": "1294120",
    "end": "1303420"
  },
  {
    "text": "phone information data we're now generating artificial Banat feedback data in the in the following way we pick",
    "start": "1303420",
    "end": "1311820"
  },
  {
    "text": "a document from the collection we have a login policy PI one that's just a",
    "start": "1311820",
    "end": "1316980"
  },
  {
    "text": "classifier oh that's just a classifier",
    "start": "1316980",
    "end": "1322130"
  },
  {
    "start": "1319000",
    "end": "1319000"
  },
  {
    "text": "I'll classify and makes a prediction it says oh it's a the correct label vector",
    "start": "1322130",
    "end": "1328320"
  },
  {
    "text": "is one zero one zero and my propensity of making that prediction is 0.3 and",
    "start": "1328320",
    "end": "1335030"
  },
  {
    "text": "then we observe what's the hamming loss of this vector so this basically means",
    "start": "1335030",
    "end": "1341190"
  },
  {
    "text": "two bits are wrong it doesn't tell you which ones just that two are wrong so this is this kind of contextual bandit",
    "start": "1341190",
    "end": "1347280"
  },
  {
    "text": "feedback in the simulated environment and then we give more and more of this",
    "start": "1347280",
    "end": "1352940"
  },
  {
    "text": "contextual bandit feedback to our poem algorithm now training conditional random field same structure of model but",
    "start": "1352940",
    "end": "1359850"
  },
  {
    "text": "trained based on this log data and what you can see in the plot is we start in terms of the performance in at our",
    "start": "1359850",
    "end": "1366330"
  },
  {
    "text": "logging poly log logging policy which was pretty crummy no where it's better that's the red line up there and then",
    "start": "1366330",
    "end": "1374120"
  },
  {
    "text": "poem get if you give it more and more bandit feedback it eventually approaches",
    "start": "1374120",
    "end": "1379560"
  },
  {
    "text": "the performance of the skyline right that's how good we could hope to get so that's a sanity check that we can",
    "start": "1379560",
    "end": "1385860"
  },
  {
    "text": "actually do learning in this way from log data so let's look at some real",
    "start": "1385860",
    "end": "1391920"
  },
  {
    "text": "world experiments here so we've worked with a couple of companies trying this out on on real systems and I want to",
    "start": "1391920",
    "end": "1398670"
  },
  {
    "start": "1394000",
    "end": "1394000"
  },
  {
    "text": "talk about one application here that we did with a large media company in New York City they had the problem of there",
    "start": "1398670",
    "end": "1405630"
  },
  {
    "text": "was a particular kind of question-answering type of problem where",
    "start": "1405630",
    "end": "1410820"
  },
  {
    "text": "there was a query and they wanted to pick a high quality answer and there was",
    "start": "1410820",
    "end": "1417810"
  },
  {
    "text": "a kind of candidate set formation stage which Capel ey firm of X and what they wanted",
    "start": "1417810",
    "end": "1423650"
  },
  {
    "text": "to do is maximize the number of correct answers roughly - the number of",
    "start": "1423650",
    "end": "1429110"
  },
  {
    "text": "incorrect answers and we were allowed to be able to abstain and that would be",
    "start": "1429110",
    "end": "1434390"
  },
  {
    "text": "kind of a zero right here just abstain so their existing system they actually",
    "start": "1434390",
    "end": "1440180"
  },
  {
    "text": "had collected some hand labeled data full information data where somebody in",
    "start": "1440180",
    "end": "1445400"
  },
  {
    "text": "our editors had gone in and said that for this this is the correct response but it's actually really hard to do this",
    "start": "1445400",
    "end": "1451460"
  },
  {
    "text": "because people typically I mean these editors don't know what the correct response is they're really only the user does right but they had this data they",
    "start": "1451460",
    "end": "1459110"
  },
  {
    "text": "collect the training logistic regression score on the stadium and then what they",
    "start": "1459110",
    "end": "1465680"
  },
  {
    "text": "feel that was basically an arc max so the the action that had the highest",
    "start": "1465680",
    "end": "1472310"
  },
  {
    "text": "score at prediction time that's the thing that the system would present and",
    "start": "1472310",
    "end": "1477650"
  },
  {
    "text": "abstaining was one of the possible actions now they weren't satisfied with",
    "start": "1477650",
    "end": "1482840"
  },
  {
    "text": "the performance of this so what we did is we still Casta fight this rule and",
    "start": "1482840",
    "end": "1488510"
  },
  {
    "text": "it's actually very simple in this case you replace the arc max with a soft Max",
    "start": "1488510",
    "end": "1493550"
  },
  {
    "text": "and so by putting this parameter lambda",
    "start": "1493550",
    "end": "1498590"
  },
  {
    "text": "in there we were able to control how stochastic our login policy is if you",
    "start": "1498590",
    "end": "1506210"
  },
  {
    "text": "pick lambda to be very large it's basically the arc max if you pick the lambda to be zero it's the uniform",
    "start": "1506210",
    "end": "1512120"
  },
  {
    "text": "distribution and now you can tweak the lambda and we treat it until the company was comfortable with the performance of",
    "start": "1512120",
    "end": "1517880"
  },
  {
    "text": "the prediction that the system made but where it was still sufficiently stochastic to give us a reliable results",
    "start": "1517880",
    "end": "1525080"
  },
  {
    "text": "from in from IPS estimation then we collect the data from the stochastic",
    "start": "1525080",
    "end": "1530540"
  },
  {
    "text": "login policy train poem and came up with a new policy just using about 5,000 kind",
    "start": "1530540",
    "end": "1538670"
  },
  {
    "text": "of examples that gave us about a 30% improvement over the existing policy and",
    "start": "1538670",
    "end": "1544070"
  },
  {
    "text": "now we can iterate that right you can now feel that policy collect more data retrain and so on so at that point",
    "start": "1544070",
    "end": "1552230"
  },
  {
    "text": "people typically ask oh can you also do deep Lewin with us and the answer is yes really not",
    "start": "1552230",
    "end": "1558529"
  },
  {
    "text": "that much changes except for the optimization so let me quickly talk also",
    "start": "1558529",
    "end": "1566509"
  },
  {
    "text": "about bandit net but it's really almost",
    "start": "1566509",
    "end": "1572539"
  },
  {
    "text": "the same right so instead of having the CRF model where it's W times fie we just",
    "start": "1572539",
    "end": "1579649"
  },
  {
    "text": "plug a deep network in there and then the following will use arrest net 20 but",
    "start": "1579649",
    "end": "1585349"
  },
  {
    "text": "you could put any deep learning model that's referring triple in there then what you would do is you put this into",
    "start": "1585349",
    "end": "1592249"
  },
  {
    "text": "your counterfactual risk minimization objective here we use slightly different estimators but effectively it's it's the",
    "start": "1592249",
    "end": "1599989"
  },
  {
    "text": "same and then if you do the same experiment comparing the two ways of",
    "start": "1599989",
    "end": "1605330"
  },
  {
    "text": "training the same ResNet 20 full information versus collecting bandit",
    "start": "1605330",
    "end": "1613009"
  },
  {
    "text": "feedback in the same manner as we've done before if you give it enough bandit feedback the performance does converge",
    "start": "1613009",
    "end": "1619429"
  },
  {
    "text": "to the performance of the skyline so again very fine that more generally you",
    "start": "1619429",
    "end": "1626269"
  },
  {
    "text": "can train basically any differentiable model this way alright so let me wrap up",
    "start": "1626269",
    "end": "1634159"
  },
  {
    "text": "by stepping back a little bit and pointing out that this batch learning",
    "start": "1634159",
    "end": "1639320"
  },
  {
    "text": "from Bennett feedback is just one specific case of learning with partial information in particular if you're",
    "start": "1639320",
    "end": "1648139"
  },
  {
    "text": "thinking about learning research and then you probably wouldn't want to formulate that as batch learning from Bennett feedback and you may not be",
    "start": "1648139",
    "end": "1654619"
  },
  {
    "text": "comfortable having a stochastic ranking function but here you can actually exploit the stochasticity of your users",
    "start": "1654619",
    "end": "1661669"
  },
  {
    "text": "the users are somewhat random so what we came up with is actually explore methods",
    "start": "1661669",
    "end": "1667940"
  },
  {
    "text": "for exploiting the randomness in user behavior and using that 2d bias the results and coming up as unbiased",
    "start": "1667940",
    "end": "1674090"
  },
  {
    "text": "learning to rank algorithms and as a side note this whole framework about you",
    "start": "1674090",
    "end": "1679909"
  },
  {
    "text": "know thinking about this as counterfactual inference and thinking about this as controlled randomized trials it's also a really great way of",
    "start": "1679909",
    "end": "1686629"
  },
  {
    "text": "thinking about is in the sense of fairness of these systems in particular you know thinking",
    "start": "1686629",
    "end": "1694490"
  },
  {
    "text": "about is my policy fair to different constituencies in particular the users of my systems the people who type in the",
    "start": "1694490",
    "end": "1701090"
  },
  {
    "text": "queries but also to the items that are being ranked where the items being ranked could be candidates for a job for",
    "start": "1701090",
    "end": "1707450"
  },
  {
    "text": "example right and last example of a",
    "start": "1707450",
    "end": "1713690"
  },
  {
    "start": "1711000",
    "end": "1711000"
  },
  {
    "text": "partial information learning problem is matrix factorization there again you have a lot of missing data people reveal",
    "start": "1713690",
    "end": "1719990"
  },
  {
    "text": "some of their ratings but they reveal it in a very biased way right they most reveal ratings that are four and five",
    "start": "1719990",
    "end": "1725840"
  },
  {
    "text": "stars because they pick the movies that they think they will like then they watch it and then they rate them right",
    "start": "1725840",
    "end": "1731300"
  },
  {
    "text": "so you have to deal that the missingness in this observation matrix is not",
    "start": "1731300",
    "end": "1736690"
  },
  {
    "text": "completely at random and but if you model this you can use the same kind of inverse propensity",
    "start": "1736690",
    "end": "1742040"
  },
  {
    "text": "scoring techniques to track of these problems as well so to wrap up what I wanted to make as",
    "start": "1742040",
    "end": "1752840"
  },
  {
    "text": "main point and the main takeaway here is that if you think about these systems",
    "start": "1752840",
    "end": "1758500"
  },
  {
    "text": "from the point of counterfactual inference and in terms of controlled randomized trials that opens up a new",
    "start": "1758500",
    "end": "1764960"
  },
  {
    "text": "view of looking at this and gives you a new methodology to solve some of the",
    "start": "1764960",
    "end": "1770300"
  },
  {
    "text": "buyers problems that we have in these systems in particular it gives you this ability to do offline a B testing which",
    "start": "1770300",
    "end": "1776660"
  },
  {
    "text": "can really speed up your development cycle and it gives you the ability to do learning in these settings in an",
    "start": "1776660",
    "end": "1782990"
  },
  {
    "text": "unbiased way if you're interested in more of the technical details there's a",
    "start": "1782990",
    "end": "1788059"
  },
  {
    "text": "tutorial on my homepage from Sakai or two years ago that goes into a lot more",
    "start": "1788059",
    "end": "1794059"
  },
  {
    "text": "detail and you also find papers and software and data there thank you very much",
    "start": "1794059",
    "end": "1799600"
  },
  {
    "text": "[Applause]",
    "start": "1799600",
    "end": "1804480"
  }
]