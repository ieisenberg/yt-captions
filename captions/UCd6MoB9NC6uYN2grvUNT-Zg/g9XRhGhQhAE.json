[
  {
    "text": "- [Instructor] This video walks",
    "start": "160",
    "end": "1319"
  },
  {
    "text": "through a large-scale inference solution",
    "start": "1320",
    "end": "4030"
  },
  {
    "text": "on EKS.",
    "start": "4030",
    "end": "5600"
  },
  {
    "text": "Here is a summary of the problem.",
    "start": "5600",
    "end": "7509"
  },
  {
    "text": "We need to deploy a large number of models",
    "start": "7510",
    "end": "10620"
  },
  {
    "text": "for inference on AWS.",
    "start": "10620",
    "end": "12730"
  },
  {
    "text": "Each model needs to be\nindividually accessible",
    "start": "12730",
    "end": "15469"
  },
  {
    "text": "through an API,",
    "start": "15470",
    "end": "17050"
  },
  {
    "text": "and each model needs to\nrespond within 100 milliseconds",
    "start": "17050",
    "end": "21550"
  },
  {
    "text": "of receiving a request.",
    "start": "21550",
    "end": "23210"
  },
  {
    "text": "The solution needs to be\ncost-effective and scalable.",
    "start": "23210",
    "end": "27759"
  },
  {
    "text": "To solve the problem, we use\naccelerator-enabled EKS nodes.",
    "start": "27760",
    "end": "32590"
  },
  {
    "text": "We pack as many models as\npossible into each node.",
    "start": "32590",
    "end": "36520"
  },
  {
    "text": "This allows us to serve\na large number of models",
    "start": "36520",
    "end": "39920"
  },
  {
    "text": "at sub-100-millisecond latency",
    "start": "39920",
    "end": "42350"
  },
  {
    "text": "while achieving sub-$100-per-hour cost",
    "start": "42350",
    "end": "45629"
  },
  {
    "text": "according to on-demand pricing.",
    "start": "45630",
    "end": "47640"
  },
  {
    "text": "To deploy this solution,\nexecute the following steps.",
    "start": "47640",
    "end": "50723"
  },
  {
    "text": "First, clone the AWS-do-inference\nGitHub repository.",
    "start": "51750",
    "end": "56750"
  },
  {
    "text": "Then configure the project",
    "start": "59470",
    "end": "61260"
  },
  {
    "text": "by executing the config.sh script.",
    "start": "61260",
    "end": "64659"
  },
  {
    "text": "To enable deployment to Kubernetes,",
    "start": "64660",
    "end": "66640"
  },
  {
    "text": "you'll have to specify\nthe registry setting.",
    "start": "66640",
    "end": "69273"
  },
  {
    "text": "For 3,000 models,",
    "start": "70710",
    "end": "72390"
  },
  {
    "text": "you will need to configure\n160 model server pods,",
    "start": "72390",
    "end": "77390"
  },
  {
    "text": "which are spread over\n40 inf1.6xl instances.",
    "start": "78120",
    "end": "83120"
  },
  {
    "text": "Next, build the base container",
    "start": "85860",
    "end": "89330"
  },
  {
    "text": "by executing the build.sh script.",
    "start": "89330",
    "end": "92623"
  },
  {
    "text": "This container image has\nall the necessary tooling",
    "start": "94180",
    "end": "97860"
  },
  {
    "text": "and will be used later to\ncompile and package your model.",
    "start": "97860",
    "end": "101833"
  },
  {
    "text": "To compile the model and enable it to run",
    "start": "104750",
    "end": "107710"
  },
  {
    "text": "on Inferentia or GPU accelerators,",
    "start": "107710",
    "end": "111110"
  },
  {
    "text": "execute the trace.sh script.",
    "start": "111110",
    "end": "113980"
  },
  {
    "text": "The compiled model is saved\nin a PyTorch trace file.",
    "start": "113980",
    "end": "118330"
  },
  {
    "text": "Execute pack.sh to package this trace file",
    "start": "118330",
    "end": "123260"
  },
  {
    "text": "into a container image\ntogether with a model server.",
    "start": "123260",
    "end": "126707"
  },
  {
    "text": "Then authenticate with the\nregistry by executing login.sh,",
    "start": "127830",
    "end": "132050"
  },
  {
    "text": "and push the container image\nby running pack.sh push.",
    "start": "132050",
    "end": "137050"
  },
  {
    "text": "Similarly, we need to build",
    "start": "141290",
    "end": "144430"
  },
  {
    "text": "and push the test container image,",
    "start": "144430",
    "end": "147459"
  },
  {
    "text": "which we're going to use later\nto verify our deployment.",
    "start": "147460",
    "end": "152460"
  },
  {
    "text": "Execute test.sh build,",
    "start": "152620",
    "end": "155683"
  },
  {
    "text": "login.sh,",
    "start": "156860",
    "end": "158410"
  },
  {
    "text": "and test.sh push to accomplish this task.",
    "start": "158410",
    "end": "163283"
  },
  {
    "text": "Next, we'll scale up the EKS cluster.",
    "start": "167320",
    "end": "170053"
  },
  {
    "text": "To do that, modify the\ncorresponding node group",
    "start": "170940",
    "end": "175870"
  },
  {
    "text": "and set the desired capacity\nto 40 inf1.6x large nodes.",
    "start": "175870",
    "end": "180870"
  },
  {
    "text": "Here, we use the open\nsource tool Qbox view",
    "start": "184050",
    "end": "187700"
  },
  {
    "text": "to visualize the cluster and its workload.",
    "start": "187700",
    "end": "190483"
  },
  {
    "text": "We are now ready to deploy our models.",
    "start": "192410",
    "end": "195500"
  },
  {
    "text": "To do that execute deploy.sh.",
    "start": "195500",
    "end": "199120"
  },
  {
    "text": "Kubernetes manifests are\nautomatically generated",
    "start": "199120",
    "end": "202409"
  },
  {
    "text": "and applied to the cluster.",
    "start": "202410",
    "end": "204770"
  },
  {
    "text": "160 model server pods\nserving 19 models each,",
    "start": "204770",
    "end": "209770"
  },
  {
    "text": "equaling total 3,040 models,",
    "start": "210740",
    "end": "213780"
  },
  {
    "text": "get evenly distributed\namong the cluster nodes.",
    "start": "213780",
    "end": "217600"
  },
  {
    "text": "We are now ready to run some tests.",
    "start": "217600",
    "end": "220610"
  },
  {
    "text": "We will use the open source toolkit KL",
    "start": "220610",
    "end": "222970"
  },
  {
    "text": "to aggregate the logs\nof all model server pods",
    "start": "222970",
    "end": "226380"
  },
  {
    "text": "in the lower right.",
    "start": "226380",
    "end": "227800"
  },
  {
    "text": "Each model server log is\ndisplayed with a different color.",
    "start": "227800",
    "end": "231640"
  },
  {
    "text": "We will execute the tests",
    "start": "231640",
    "end": "233550"
  },
  {
    "text": "in the order they are described\nin the test script Help.",
    "start": "233550",
    "end": "237713"
  },
  {
    "text": "First, let's execute the sequential test.",
    "start": "238580",
    "end": "242100"
  },
  {
    "text": "In this test, a request is sent",
    "start": "242100",
    "end": "244480"
  },
  {
    "text": "to each of the models in the deployment",
    "start": "244480",
    "end": "247599"
  },
  {
    "text": "in a sequential order.",
    "start": "247600",
    "end": "249283"
  },
  {
    "text": "We can see that the\nresponses from the models",
    "start": "250460",
    "end": "253420"
  },
  {
    "text": "are grouped by color.",
    "start": "253420",
    "end": "255599"
  },
  {
    "text": "This is because requests\nare being sent sequentially",
    "start": "255600",
    "end": "259820"
  },
  {
    "text": "to each of the models being served",
    "start": "259820",
    "end": "263140"
  },
  {
    "text": "by each individual model server pod",
    "start": "263140",
    "end": "266770"
  },
  {
    "text": "before moving on to the next.",
    "start": "266770",
    "end": "268633"
  },
  {
    "text": "We can see each request\nis receiving a response",
    "start": "269920",
    "end": "273931"
  },
  {
    "text": "within 21 or 22 milliseconds.",
    "start": "273931",
    "end": "276348"
  },
  {
    "text": "Next, we will run the random test.",
    "start": "280334",
    "end": "283167"
  },
  {
    "text": "In this test, requests are\nbeing sent to a random server",
    "start": "284287",
    "end": "289287"
  },
  {
    "text": "and a random model within that server.",
    "start": "289601",
    "end": "292768"
  },
  {
    "text": "The randomness of this test",
    "start": "299201",
    "end": "301628"
  },
  {
    "text": "reflects in the colors in the lower right,",
    "start": "301628",
    "end": "304465"
  },
  {
    "text": "as each individual server and model",
    "start": "304465",
    "end": "307382"
  },
  {
    "text": "responds to a random request.",
    "start": "308290",
    "end": "310707"
  },
  {
    "text": "We can see that the\nlatency of the responses",
    "start": "312891",
    "end": "316485"
  },
  {
    "text": "is holding within 22 to 23 milliseconds.",
    "start": "316485",
    "end": "319818"
  },
  {
    "text": "Next, we'll run the benchmark test,",
    "start": "323533",
    "end": "326250"
  },
  {
    "text": "which uses Python multi-threading",
    "start": "326250",
    "end": "329030"
  },
  {
    "text": "to send random requests to the system",
    "start": "329030",
    "end": "333410"
  },
  {
    "text": "and measure throughput as well as latency",
    "start": "333410",
    "end": "337800"
  },
  {
    "text": "in the P50, P90, and P95 percentiles.",
    "start": "337800",
    "end": "342362"
  },
  {
    "text": "Once the test is complete,",
    "start": "343440",
    "end": "344880"
  },
  {
    "text": "we run the benchmark analysis job",
    "start": "344880",
    "end": "347760"
  },
  {
    "text": "to aggregate the results.",
    "start": "347760",
    "end": "349723"
  },
  {
    "text": "The results show that we\nare achieving a throughput",
    "start": "351840",
    "end": "355050"
  },
  {
    "text": "of more than 75 requests per second,",
    "start": "355050",
    "end": "359080"
  },
  {
    "text": "at latency of less than 36 milliseconds",
    "start": "359080",
    "end": "364080"
  },
  {
    "text": "on the P95 percentile.",
    "start": "365170",
    "end": "367513"
  },
  {
    "text": "Finally, let's clean up our deployment.",
    "start": "371698",
    "end": "375190"
  },
  {
    "text": "To do that, execute deploy.sh stop.",
    "start": "375190",
    "end": "378733"
  },
  {
    "text": "Once the model server pods are terminated,",
    "start": "380150",
    "end": "383350"
  },
  {
    "text": "scale down the EKS cluster by\nadjusting the desired capacity",
    "start": "383350",
    "end": "387670"
  },
  {
    "text": "of the auto-scaling group back to zero.",
    "start": "387670",
    "end": "390773"
  },
  {
    "text": "All of this source code and\ndocumentation is available",
    "start": "401330",
    "end": "404490"
  },
  {
    "text": "in the AWS-do-inference repository",
    "start": "404490",
    "end": "408099"
  },
  {
    "text": "on GitHub AWS Samples.",
    "start": "408100",
    "end": "410593"
  },
  {
    "text": "Thank you for watching.",
    "start": "411590",
    "end": "412803"
  }
]