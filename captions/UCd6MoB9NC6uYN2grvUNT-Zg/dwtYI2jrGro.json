[
  {
    "text": "yeah here we go all right cool so one of",
    "start": "30",
    "end": "5100"
  },
  {
    "text": "the hard parts about HPC is that it's complex so there's lots of different ways to characterize HPC there's people",
    "start": "5100",
    "end": "12269"
  },
  {
    "text": "who are chiefly concerned about just IO there's people in that category we'll",
    "start": "12269",
    "end": "17369"
  },
  {
    "text": "call them like the National Labs customers for example genomics cares a lot about IO over other elements right",
    "start": "17369",
    "end": "24600"
  },
  {
    "text": "we'll talk about those other elements as well there's the analytics guys you know AI and ml type type workloads",
    "start": "24600",
    "end": "33059"
  },
  {
    "text": "there's the EDA space so traditionally that's been you know a lot of these are",
    "start": "33059",
    "end": "38940"
  },
  {
    "text": "in the small file workloads and then there's the people who care chiefly about cost and that's a lot of",
    "start": "38940",
    "end": "45390"
  },
  {
    "text": "times research groups because they're working with grant money and maybe they don't have a bottomless pit of money like federal labs to utilities are often",
    "start": "45390",
    "end": "54329"
  },
  {
    "text": "worried about margin there's usually fairly low tolerances for having having high margin in the work that they're",
    "start": "54329",
    "end": "59850"
  },
  {
    "text": "doing inside those projects the median entertainment space cares about cost because they have a business to run",
    "start": "59850",
    "end": "66060"
  },
  {
    "text": "they can't you know it's not just a science project for them and then there's people who are chiefly concerned",
    "start": "66060",
    "end": "71549"
  },
  {
    "text": "with the time to results finance is one of those they have opportunities that they have to take advantage of and when",
    "start": "71549",
    "end": "78420"
  },
  {
    "text": "that window is gone you know it won't matter anymore whether or not they get the results late different kinds of",
    "start": "78420",
    "end": "86880"
  },
  {
    "text": "workloads inside of those that you can associate with things you can associate you know technologies like luster and",
    "start": "86880",
    "end": "92400"
  },
  {
    "text": "MPI i/o and an NFS with a lot of the i/o oriented workloads we see a lot of HDFS",
    "start": "92400",
    "end": "98759"
  },
  {
    "text": "in the in the finance world and EDA world for for I would say you know more",
    "start": "98759",
    "end": "106560"
  },
  {
    "text": "recently you know traditionally that's been an NFS place and then we see a lot of things like GPFS in in all over the",
    "start": "106560",
    "end": "113640"
  },
  {
    "text": "place as well as well as NFS and the reason that that HPC is difficult to",
    "start": "113640",
    "end": "120030"
  },
  {
    "text": "sort of nail down is because everybody has a slightly different idea about what",
    "start": "120030",
    "end": "125640"
  },
  {
    "text": "HPC means to them and when you go talk with customers I'm just a show of hands out of curiosity who's a who's a partner",
    "start": "125640",
    "end": "132280"
  },
  {
    "text": "and who's a customer it's like half in half ok cool so this year we have open",
    "start": "132280",
    "end": "137630"
  },
  {
    "text": "it's an open forum so it used to be that GPS talks were limited to people on the partner track and so you never know so I",
    "start": "137630",
    "end": "143120"
  },
  {
    "text": "want to make sure I'm giving you guys information that's useful too so it's hard to nail this down every customer",
    "start": "143120",
    "end": "148220"
  },
  {
    "text": "has a different sense of what HBC means to them and that's consistent with you know other kinds of storage workloads",
    "start": "148220",
    "end": "153709"
  },
  {
    "text": "too but especially so in HPC you get a lot of legacy applications with weird i/o patterns and a lot of really",
    "start": "153709",
    "end": "159350"
  },
  {
    "text": "interesting problems because they're solving interesting challenges and they often have to create new things in order",
    "start": "159350",
    "end": "166489"
  },
  {
    "text": "to do that so we see computer-aided engineering in CAE also in here it's",
    "start": "166489",
    "end": "174140"
  },
  {
    "text": "it's just it's all over the map and it's it's interesting actually because when you look at like the greenfield",
    "start": "174140",
    "end": "179690"
  },
  {
    "text": "opportunities versus sort of the Sun setting data center world the HPC market",
    "start": "179690",
    "end": "185209"
  },
  {
    "text": "is one of the areas of growth the cloud and HPC are they sort of two areas that",
    "start": "185209",
    "end": "191150"
  },
  {
    "text": "are growing and storage according to the you know the various IDC and Gartner",
    "start": "191150",
    "end": "196160"
  },
  {
    "text": "group so those kinds of things so when people go to the cloud especially with HPC it always starts off like this I'm",
    "start": "196160",
    "end": "202070"
  },
  {
    "text": "gonna roll my own I'm gonna do it myself and this works fine for like a day and a half and then it starts to go and screw",
    "start": "202070",
    "end": "209209"
  },
  {
    "text": "grow in scope and pretty soon they are occupying multiple AZ's and they have to",
    "start": "209209",
    "end": "214760"
  },
  {
    "text": "worry about all kinds of other challenges now associated with scale out and data synchronization and pretty soon",
    "start": "214760",
    "end": "223130"
  },
  {
    "text": "they have clients that aren't just talking to the servers and their ACS but they're talking to other AC systems too",
    "start": "223130",
    "end": "229850"
  },
  {
    "text": "and EFS helps to simplify that EFS says you know what we're gonna back that off",
    "start": "229850",
    "end": "234859"
  },
  {
    "text": "we're going to give you one mount target we're gonna have a single namespace and we're gonna do it through effectively",
    "start": "234859",
    "end": "240739"
  },
  {
    "text": "through a load balancer and so EFS is great for that and if you have a ton of data EFS is your best friend you can get",
    "start": "240739",
    "end": "247459"
  },
  {
    "text": "amazing performance off of EFS but for a big portion of the HPC market you have a",
    "start": "247459",
    "end": "253310"
  },
  {
    "text": "lot people who have not that much data frankly or they have lots of small files or their write heavy right latency in",
    "start": "253310",
    "end": "260539"
  },
  {
    "text": "EFS because of the desire to do durable writes is slow frankly so you know if you have",
    "start": "260539",
    "end": "269880"
  },
  {
    "text": "small file workloads and you're right heavy the FS usually isn't isn't what you're going to go after you're going to",
    "start": "269880",
    "end": "275370"
  },
  {
    "text": "look for something else but it works well for customers because it helps simplify things and it helps simplify",
    "start": "275370",
    "end": "280980"
  },
  {
    "text": "things so much so that a lot of people will use it because it's just you know there's no infrastructure to manage",
    "start": "280980",
    "end": "287240"
  },
  {
    "text": "excuse me there's no infrastructure to manage and that simplification is a",
    "start": "287240",
    "end": "292800"
  },
  {
    "text": "trade-off that people are consciously making but there's other ways to do it",
    "start": "292800",
    "end": "298890"
  },
  {
    "text": "since this is a partner track we're gonna feature some partner technology but first I want to talk about HPC",
    "start": "298890",
    "end": "306390"
  },
  {
    "text": "cluster node anatomy so there's these concepts of you know what should I node",
    "start": "306390",
    "end": "312630"
  },
  {
    "text": "look like and what are the parts that are important there's different ways of",
    "start": "312630",
    "end": "318060"
  },
  {
    "text": "doing deployments if you deploy the same way today that you've had deployed for the last 15 years you'll probably have a",
    "start": "318060",
    "end": "326160"
  },
  {
    "text": "mixture of applications and storage resources on on the same systems there's good arguments for that on Prem in the",
    "start": "326160",
    "end": "333090"
  },
  {
    "text": "cloud it doesn't really matter as much you can disaggregate things but what what is consistent is that when you",
    "start": "333090",
    "end": "339210"
  },
  {
    "text": "start looking at the node and Matt I mean the things that you lose a little bit of efficiency on on an individual",
    "start": "339210",
    "end": "345810"
  },
  {
    "text": "node will deliver a huge loss of efficiency at scale so it's important to understand what's going on at the data",
    "start": "345810",
    "end": "353250"
  },
  {
    "text": "layer where are your bottlenecks and how to test for them what's going on with the metadata layer what's you know where",
    "start": "353250",
    "end": "359310"
  },
  {
    "text": "are the bottlenecks and how to test for that we'll talk about that if you should tear I would say most people should tear",
    "start": "359310",
    "end": "367080"
  },
  {
    "text": "there's not a lot of reason for folks necessarily to not tear unless you know that your datasets is changing very",
    "start": "367080",
    "end": "372300"
  },
  {
    "text": "quickly and you you want a hundred percent of it hot and we'll talk about",
    "start": "372300",
    "end": "377430"
  },
  {
    "text": "some some of the trade-offs there as well routing between cluster nodes goes to",
    "start": "377430",
    "end": "383160"
  },
  {
    "text": "your availability metric so whatever you think your cluster can provide for you",
    "start": "383160",
    "end": "388190"
  },
  {
    "text": "from an availability perspective the ability for you to regardless of what kind of job scheduler you're using",
    "start": "388190",
    "end": "395900"
  },
  {
    "text": "if you think that those requests are being able to be handled all the time you may be mistaken if you ask the",
    "start": "395900",
    "end": "402139"
  },
  {
    "text": "question what happens when a node dies and I have something that's in flight if",
    "start": "402139",
    "end": "407660"
  },
  {
    "text": "the answer is you have to wait a huge amount of time for something to timeout that's going to negatively impact your",
    "start": "407660",
    "end": "413479"
  },
  {
    "text": "your availability so understanding how requests get routed inside of the cluster is super important and then on",
    "start": "413479",
    "end": "420620"
  },
  {
    "text": "the front end the access protocols really kind of go toward how much administrative headache you're going to have one of the reasons that people",
    "start": "420620",
    "end": "427190"
  },
  {
    "text": "really liked the sort of hyper-converged element is that it was easier from an administrative perspective to kind of",
    "start": "427190",
    "end": "433490"
  },
  {
    "text": "deploy everything all in the same box everywhere but in the cloud all of the other arguments therefore",
    "start": "433490",
    "end": "439280"
  },
  {
    "text": "hyper-converged kind of fall over you know you don't you don't have to worry about deploying one-size-fits-all",
    "start": "439280",
    "end": "444440"
  },
  {
    "text": "because you can tune the instance types that we there's so many instance types these days you can tune the instance",
    "start": "444440",
    "end": "450530"
  },
  {
    "text": "types to whatever it is that you need and then the other argument from a sales",
    "start": "450530",
    "end": "455630"
  },
  {
    "text": "perspective is oh it just you know it's easier to sell you know these building blocks well with AWS with the utility",
    "start": "455630",
    "end": "462410"
  },
  {
    "text": "based pricing model you only pay for what you're using anyway so that argument sort of falls down and then",
    "start": "462410",
    "end": "467780"
  },
  {
    "text": "from an application development perspective the idea that you would have the same resources everywhere well you",
    "start": "467780",
    "end": "473930"
  },
  {
    "text": "get that anyway when you start using api's and you start to move your workload to be a little bit more cloud friendly so access protocol is super",
    "start": "473930",
    "end": "481250"
  },
  {
    "text": "important so I guess I'll back up for",
    "start": "481250",
    "end": "488570"
  },
  {
    "text": "just a second to talk talk one more one more bit about this so when you when you",
    "start": "488570",
    "end": "493699"
  },
  {
    "text": "find wastes in in small amounts and you look at okay well it's not a big deal if",
    "start": "493699",
    "end": "499400"
  },
  {
    "text": "I have you know half a pet percent and you know inefficiency or something on a node and you start looking at deploying",
    "start": "499400",
    "end": "505599"
  },
  {
    "text": "tens of thousands of those things as a lot of HPC customers do that start it",
    "start": "505599",
    "end": "511729"
  },
  {
    "text": "starts to add up so what you get out of each instance should be as close to what",
    "start": "511729",
    "end": "517310"
  },
  {
    "text": "the hardware is physically possible delivering as possible there's great",
    "start": "517310",
    "end": "523370"
  },
  {
    "text": "tools for testing things there's myths around you know which tools are better for what",
    "start": "523370",
    "end": "529629"
  },
  {
    "text": "things I'll talk about some of the tools that our HPC smees advocate using",
    "start": "529629",
    "end": "535319"
  },
  {
    "text": "specifically testing like the metadata layer you use a different tool for",
    "start": "535319",
    "end": "540730"
  },
  {
    "text": "example than what you would do if you were gonna test the data layer the way",
    "start": "540730",
    "end": "545829"
  },
  {
    "text": "you organize your server nodes versus your client nodes it doesn't much matter most of the job schedulers that have",
    "start": "545829",
    "end": "552429"
  },
  {
    "text": "been around forever or you can work with in the cloud - so I would say probably",
    "start": "552429",
    "end": "558610"
  },
  {
    "text": "the most common one that I see from the university spaces slurm any slurm users in the okay alright good yeah so slurm",
    "start": "558610",
    "end": "566679"
  },
  {
    "text": "works great in AWS there's no reason not to use it especially if you've already got everything configured the way you",
    "start": "566679",
    "end": "571989"
  },
  {
    "text": "want it to be works pretty well with CFT to the cloud formation template service um so if we just characterize sort of",
    "start": "571989",
    "end": "579459"
  },
  {
    "text": "our approach that what we're trying to do is eliminate hot spots or at least discover them we can set up what it is",
    "start": "579459",
    "end": "585339"
  },
  {
    "text": "that we want to test for now a lot of folks actually say inside of our org anyway you know benchmarks are useless",
    "start": "585339",
    "end": "591670"
  },
  {
    "text": "give us real workloads like if you you know if you're gonna go through all the trouble to do a benchmark you might as",
    "start": "591670",
    "end": "597129"
  },
  {
    "text": "well go through all the trouble to actually do the real thing and there's there's a lot of merit to that but",
    "start": "597129",
    "end": "602290"
  },
  {
    "text": "because in the HPC world everything is sort of all over the place as far as what constitutes HPC it's difficult to",
    "start": "602290",
    "end": "609819"
  },
  {
    "text": "come up with one workload that would work well for everybody so we're gonna do a little bit of benchmarking I'm",
    "start": "609819",
    "end": "615309"
  },
  {
    "text": "gonna focus primarily on on one partner product today I'm gonna compare it against some other things so lustre is",
    "start": "615309",
    "end": "621399"
  },
  {
    "text": "probably the you know the gold standard that everybody thinks about when they think storage and HPC maybe gpfs is in",
    "start": "621399",
    "end": "628149"
  },
  {
    "text": "there too it's very common for people to hang you know NFS clients or samba",
    "start": "628149",
    "end": "634119"
  },
  {
    "text": "clients off of these things in order to support things like oh you know Windows Windows and that sort of thing in the in",
    "start": "634119",
    "end": "641499"
  },
  {
    "text": "mini space there's a ton of Windows clients excuse me the deadline scheduler",
    "start": "641499",
    "end": "646869"
  },
  {
    "text": "by think box which is one of Amazon's acquisitions actually a great example they're all Windows users and so they",
    "start": "646869",
    "end": "652329"
  },
  {
    "text": "they're hurting for great HPC storage solutions so without naming names",
    "start": "652329",
    "end": "659110"
  },
  {
    "text": "we'll talk about some customers so a semiconductor customer ran a really",
    "start": "659110",
    "end": "664959"
  },
  {
    "text": "interesting workload actually they they they had serve they had two challenges",
    "start": "664959",
    "end": "670600"
  },
  {
    "text": "one challenge was and they were sort of juxtaposition just juxtaposed the juxtaposition of the",
    "start": "670600",
    "end": "678550"
  },
  {
    "text": "two because of the two challenges was that they want a great performance but they also needed centralized",
    "start": "678550",
    "end": "684220"
  },
  {
    "text": "manageability and so they ran their their builds against a local SSD on a",
    "start": "684220",
    "end": "691060"
  },
  {
    "text": "super high end you know EDA workstation and they got great performance you know",
    "start": "691060",
    "end": "697980"
  },
  {
    "text": "this is some minutes for for an elapsed elapsed time for a build and then they",
    "start": "697980",
    "end": "703630"
  },
  {
    "text": "ran it on just a standard nfsv4 open source product and the results were not",
    "start": "703630",
    "end": "710199"
  },
  {
    "text": "so good and then they used an AWS technology partner product called Weka",
    "start": "710199",
    "end": "715449"
  },
  {
    "text": "and the results were almost as good as local SSD the catch is is that Weka is a",
    "start": "715449",
    "end": "721660"
  },
  {
    "text": "distributed parallel file system so all of the things that people think about from an IT perspective of a rogue SS you",
    "start": "721660",
    "end": "729639"
  },
  {
    "text": "know rogue workloads on you know non centralized data those challenges got solved by using Weka the data was",
    "start": "729639",
    "end": "736750"
  },
  {
    "text": "centralized and manageable and the performance requirements that the engineering groups had so that they",
    "start": "736750",
    "end": "742389"
  },
  {
    "text": "could actually get you know more than one build done in a day was were met as",
    "start": "742389",
    "end": "747670"
  },
  {
    "text": "well and so that's that was actually that was really interesting so there's the solution was to implement Weka and",
    "start": "747670",
    "end": "754480"
  },
  {
    "text": "it was it wasn't even a very very big cluster actually but the the performance was on par with the local SSD the next",
    "start": "754480",
    "end": "762819"
  },
  {
    "text": "customer is a genomics customer and it was a similar kind of workload where it was a small file work laid but they had",
    "start": "762819",
    "end": "768910"
  },
  {
    "text": "some large files in the mix and so mixed workloads are particularly difficult for most storage systems to to handle well a",
    "start": "768910",
    "end": "777160"
  },
  {
    "text": "lot of it just has to do with sort of a queueing architecture so it's you know if you've ever had a small file workload",
    "start": "777160",
    "end": "783310"
  },
  {
    "text": "it's coming along and then something oriented on big file IO comes along and",
    "start": "783310",
    "end": "788350"
  },
  {
    "text": "just throws it out of whack or vice-versa a lot of big your clothes and then another client",
    "start": "788350",
    "end": "793749"
  },
  {
    "text": "comes over and does a bunch of small I hope and your performance goes in the toilet that doesn't that is not limited to the",
    "start": "793749",
    "end": "799749"
  },
  {
    "text": "HPC world that is the problem that database administrators around the world suffer from as well pretty much anybody",
    "start": "799749",
    "end": "805480"
  },
  {
    "text": "who has performance requirements has hit that kind of problem and so handling a",
    "start": "805480",
    "end": "810759"
  },
  {
    "text": "mixed file workload is is a particularly difficult challenge and in in this case",
    "start": "810759",
    "end": "816449"
  },
  {
    "text": "they they bought some equipment this is the I mean this wasn't this was a start",
    "start": "816449",
    "end": "821649"
  },
  {
    "text": "out as an on-prem you know test they bought some equipment and and their incumbent vendor said well you know what",
    "start": "821649",
    "end": "830740"
  },
  {
    "text": "are you gonna go use this stuff for and they told them and and so the the vendor",
    "start": "830740",
    "end": "836110"
  },
  {
    "text": "in addition to giving them the that hardware for doing the testing since some nice all flash arrays with it for",
    "start": "836110",
    "end": "842019"
  },
  {
    "text": "free just you know so they wouldn't have to do the test and I think that that's funny because even when you look at the",
    "start": "842019",
    "end": "849879"
  },
  {
    "text": "results the results were that a single run of the of the genomics workload",
    "start": "849879",
    "end": "855790"
  },
  {
    "text": "which was a format conversion job ran really fast on the all flash array did",
    "start": "855790",
    "end": "860980"
  },
  {
    "text": "really really well but when you started having concurrent jobs that mixed file",
    "start": "860980",
    "end": "866499"
  },
  {
    "text": "workload problem starts to show up and it starts to elongate the elapsed time",
    "start": "866499",
    "end": "872519"
  },
  {
    "text": "for the for the on prim all flash array doing doing its test so again Weka",
    "start": "872519",
    "end": "879839"
  },
  {
    "text": "worked really well there because they have a good software architecture under the covers that lets them handle",
    "start": "879839",
    "end": "885850"
  },
  {
    "text": "different different types of workloads the you know one conversion completed in",
    "start": "885850",
    "end": "891999"
  },
  {
    "text": "the same amount of time as six conversions which you know if you think about her main PC perspective you think okay that means that the limits of the",
    "start": "891999",
    "end": "898629"
  },
  {
    "text": "of the of what of the gear that was deployed weren't found yet so a lot of",
    "start": "898629",
    "end": "904209"
  },
  {
    "text": "this has to do with just old software versus new software in some ways there's also who's doing it like who's writing",
    "start": "904209",
    "end": "910540"
  },
  {
    "text": "it most software like the most software that we deal with on a daily basis including the stuff on our telephones",
    "start": "910540",
    "end": "916899"
  },
  {
    "text": "and on our laptops is old like really old so you know son RPC goes back a bit",
    "start": "916899",
    "end": "924890"
  },
  {
    "text": "standard libraries are you know the basic system calls and UNIX haven't been updated in a long time there's news",
    "start": "924890",
    "end": "931710"
  },
  {
    "text": "there's more system calls now than there were you know 30 years ago but a lot of this stuff is getting a little long on",
    "start": "931710",
    "end": "937470"
  },
  {
    "text": "the tooth so the assumptions are actually different now the the things that people think about it used to be",
    "start": "937470",
    "end": "943680"
  },
  {
    "text": "that people just sort of you know they they put a lot of money into hardware into engineering hardware engineering",
    "start": "943680",
    "end": "949140"
  },
  {
    "text": "and and the hardware engineering mantra sort of was will make it reliable and",
    "start": "949140",
    "end": "954270"
  },
  {
    "text": "then the software didn't have to work as much and now we know that when you deploy in large quantities especially",
    "start": "954270",
    "end": "960630"
  },
  {
    "text": "for HPC use cases where you're using lots of systems maybe as your clients",
    "start": "960630",
    "end": "967500"
  },
  {
    "text": "and maybe not as many systems b√©zier servers we know that the hardware really isn't that reliable another thing was",
    "start": "967500",
    "end": "974610"
  },
  {
    "text": "that the operating system was considered the sort of the rock that the bedrock of stability people would deploy operating",
    "start": "974610",
    "end": "981480"
  },
  {
    "text": "systems depending on the reputation for stability I remember deploying like OSF one on",
    "start": "981480",
    "end": "987810"
  },
  {
    "text": "alpha and it was rock-solid man it was like out of the box no bugs I loved it",
    "start": "987810",
    "end": "993240"
  },
  {
    "text": "there was a great series called the the UNIX haters handbook where they talked about symbolic systems versus Sun",
    "start": "993240",
    "end": "999900"
  },
  {
    "text": "systems in the early days and they were like the good thing about Suns is that they boot fast and that's good because",
    "start": "999900",
    "end": "1005180"
  },
  {
    "text": "they boot a lot okay we're not very stable and so now we know that that",
    "start": "1005180",
    "end": "1012020"
  },
  {
    "text": "isn't always the right choice that the way that things like Linux are developed and even FreeBSD to some extent you know",
    "start": "1012020",
    "end": "1020210"
  },
  {
    "text": "frankly speaking there's a there's a lot in there and it's moving so fast that sometimes trusting the kernel isn't the",
    "start": "1020210",
    "end": "1025670"
  },
  {
    "text": "right choice and so we have mechanisms now to bypass those things things like DB D K s and s P D K these are",
    "start": "1025670",
    "end": "1032890"
  },
  {
    "text": "development kits that Intel puts out around how to literally how to bypass the kernel things like",
    "start": "1032890",
    "end": "1040250"
  },
  {
    "text": "rocky help lower latency and that's an on-prem already American Virgie thir net",
    "start": "1040250",
    "end": "1047150"
  },
  {
    "text": "it's an on-prem technology that doesn't exist in the cloud but you can get the same latency levels in the cloud using",
    "start": "1047150",
    "end": "1053540"
  },
  {
    "text": "things like DB DK as you can on Prem using things like rocky we used to",
    "start": "1053540",
    "end": "1058760"
  },
  {
    "text": "believe that MTBF was millions of hours because manufacturers told us that and then we got bent out of shape when we",
    "start": "1058760",
    "end": "1064670"
  },
  {
    "text": "only got a few hundred thousand hours out of something and we thought oh it must be a fluke now we you see enough hardware that we know that it's totally",
    "start": "1064670",
    "end": "1072200"
  },
  {
    "text": "normal for I have to die or a system to die after two hundred thousand hours you know it doesn't mean that it's dead",
    "start": "1072200",
    "end": "1077330"
  },
  {
    "text": "forever it just means it's sick and it needs help it used to be that you would",
    "start": "1077330",
    "end": "1083240"
  },
  {
    "text": "lease hardware and you'd leave it up until it was time to return it or until it died if you did like a $1 buyout or",
    "start": "1083240",
    "end": "1088370"
  },
  {
    "text": "something like that HPC customers especially in the cloud especially in AWS use easy to spot a lot",
    "start": "1088370",
    "end": "1097300"
  },
  {
    "text": "so much so that like you know you might have a job that uses a hundred thousand",
    "start": "1097300",
    "end": "1103010"
  },
  {
    "text": "cores but you might only spend up a couple thousand dollars and there's one of the great things about easy to spot",
    "start": "1103010",
    "end": "1108980"
  },
  {
    "text": "and these phenomenal stories about the the kid who got the National Science Foundation grant and came up with a new",
    "start": "1108980",
    "end": "1115370"
  },
  {
    "text": "way to to identify a certain type of skin cancer cell and he needed a lot of",
    "start": "1115370",
    "end": "1122570"
  },
  {
    "text": "course to do it though so he used ec2 spot to exercise several hundred thousand and he was able to",
    "start": "1122570",
    "end": "1128780"
  },
  {
    "text": "complete in the weekend what the National Science Foundation said they would have to spend like 38 million dollars building a lab to do and there",
    "start": "1128780",
    "end": "1136850"
  },
  {
    "text": "was no capex component so so with the ec2 spots software needs to have different expectations around how long",
    "start": "1136850",
    "end": "1143630"
  },
  {
    "text": "the hardware is going to be up and if you're not using easy to spot should ask the question why not give you a 90%",
    "start": "1143630",
    "end": "1150410"
  },
  {
    "text": "savings on average it's sometimes it's even more over what you can by using",
    "start": "1150410",
    "end": "1156830"
  },
  {
    "text": "reserved instances or even using hardware on-prem really no reason not to use it it's really quite cool so",
    "start": "1156830",
    "end": "1164410"
  },
  {
    "text": "so those are the assumptions now it's an it's really likely that when you",
    "start": "1166120",
    "end": "1171940"
  },
  {
    "text": "start looking at your HPC jobs that you have a job scheduler this resilient and",
    "start": "1171940",
    "end": "1177970"
  },
  {
    "text": "that you don't care if some nodes die and that's cool but you should probably know anyway what it is that you're",
    "start": "1177970",
    "end": "1184330"
  },
  {
    "text": "deploying so bluster for example after all these years no real good durability",
    "start": "1184330",
    "end": "1190270"
  },
  {
    "text": "options there's a few patches here and there but at the end of the day what",
    "start": "1190270",
    "end": "1196420"
  },
  {
    "text": "makes something durable that's a question right how do you measure it and the answer is that you need to have",
    "start": "1196420",
    "end": "1204040"
  },
  {
    "text": "special logic in order to handle the failures that you're gonna see and so",
    "start": "1204040",
    "end": "1209110"
  },
  {
    "text": "when you look at availability versus durability it's important to separate them a lot of people conflate them and",
    "start": "1209110",
    "end": "1215860"
  },
  {
    "text": "they'll call it resiliency or reliability and they really mean two separate things so so availability is",
    "start": "1215860",
    "end": "1222490"
  },
  {
    "text": "you know you sent your request and it was received successfully and if it was",
    "start": "1222490",
    "end": "1227680"
  },
  {
    "text": "received successfully it was available durability means that you wrote some",
    "start": "1227680",
    "end": "1232720"
  },
  {
    "text": "data and then some time later you came back and it was there or it wasn't there it's lost or it's present so their",
    "start": "1232720",
    "end": "1240400"
  },
  {
    "text": "ability means that you wrote the data and it wasn't lost so at the bottom line",
    "start": "1240400",
    "end": "1245490"
  },
  {
    "text": "that's eleven nines that's s3s durability so what that means is that",
    "start": "1245490",
    "end": "1251470"
  },
  {
    "text": "there's a one in 100 billion chance that it will lose data and we explain this in",
    "start": "1251470",
    "end": "1257110"
  },
  {
    "text": "the in the s3 fact like how how we go about calculating that but s3 actually",
    "start": "1257110",
    "end": "1263110"
  },
  {
    "text": "be these numbers because it has trillions of objects it's never lost an object and advertently people have",
    "start": "1263110",
    "end": "1269380"
  },
  {
    "text": "deleted objects of course but the the support tickets associated with",
    "start": "1269380",
    "end": "1274690"
  },
  {
    "text": "restoring those objects usually have happy endings of people had version versioning turned on availability is in",
    "start": "1274690",
    "end": "1283510"
  },
  {
    "text": "the context of AWS services how often you get a positive response so it's",
    "start": "1283510",
    "end": "1290290"
  },
  {
    "text": "entirely possible for example that you might submit an HTTP request over",
    "start": "1290290",
    "end": "1295450"
  },
  {
    "text": "one of our restful api is and you might get response that says I'm busy try again later",
    "start": "1295450",
    "end": "1300510"
  },
  {
    "text": "that is a tantamount to it not being available but then you may resubmit your",
    "start": "1300510",
    "end": "1307480"
  },
  {
    "text": "request and on the next try it could say okay here you go here's here's the",
    "start": "1307480",
    "end": "1312820"
  },
  {
    "text": "response that you were looking for so that part's important because the protocol that you use can dictate what",
    "start": "1312820",
    "end": "1318340"
  },
  {
    "text": "the semantics are and if you're using something like RPC it's not so simple because you may simply wait for the i/o",
    "start": "1318340",
    "end": "1326050"
  },
  {
    "text": "to come back availability may mean that your application is hung and that's the downside to having old software that",
    "start": "1326050",
    "end": "1332230"
  },
  {
    "text": "expects things like POSIX compliancy so under a note about that actually anybody",
    "start": "1332230",
    "end": "1339490"
  },
  {
    "text": "have like POSIX requirements and the workloads that they're were running on like anybody does like and like directory locking or atomic writes or",
    "start": "1339490",
    "end": "1347410"
  },
  {
    "text": "anything like that yeah so in the HPC world traditionally requirements are",
    "start": "1347410",
    "end": "1353160"
  },
  {
    "text": "strict and the tools that you're using are fairly strict and it's law it's",
    "start": "1353160",
    "end": "1358390"
  },
  {
    "text": "often the case that things like NFS don't work sometimes they do there are definitely plenty of scenarios",
    "start": "1358390",
    "end": "1365650"
  },
  {
    "text": "where NFS works fine but there is also this section of HPC where they need more",
    "start": "1365650",
    "end": "1371410"
  },
  {
    "text": "POSIX compliancy than NFS can provide and it's just it really comes down to the fact that NFS wasn't really built to",
    "start": "1371410",
    "end": "1377830"
  },
  {
    "text": "do all the things that are a quote unquote real filesystem were built to do so we'll talk about failures and then",
    "start": "1377830",
    "end": "1386320"
  },
  {
    "text": "we'll we'll go into a demo so why",
    "start": "1386320",
    "end": "1391720"
  },
  {
    "text": "doesn't matter what you use like why can't you just go use lustre you can totally go use luster luster doesn't offer you any durability but it can live",
    "start": "1391720",
    "end": "1397540"
  },
  {
    "text": "on top of things that do but if you had something they did offer durability but",
    "start": "1397540",
    "end": "1403600"
  },
  {
    "text": "maybe you had some sort of thing on prem for example that might protect your data",
    "start": "1403600",
    "end": "1409120"
  },
  {
    "text": "for you it could be the case that every 22 years something might die but when",
    "start": "1409120",
    "end": "1415930"
  },
  {
    "text": "you start having lots of those things the probability adds up and it's entirely likely that you're going to",
    "start": "1415930",
    "end": "1421090"
  },
  {
    "text": "experience a failure sooner than that so a failure every 22 years when you have",
    "start": "1421090",
    "end": "1426190"
  },
  {
    "text": "one of something translates to a failure every 1.9 years when you have 12 of them and that number",
    "start": "1426190",
    "end": "1435300"
  },
  {
    "text": "continues to get smaller and smaller and smaller when you have more and more and more so if you have a hundred or 200 or",
    "start": "1435300",
    "end": "1441450"
  },
  {
    "text": "300 you can expect to have failures if you have 300 you can expect to have failures every month right if you have",
    "start": "1441450",
    "end": "1448220"
  },
  {
    "text": "600 you can expect to have failures a lot sooner than that all right so when",
    "start": "1448220",
    "end": "1453450"
  },
  {
    "text": "you think about the size of your HPC clusters think about that like what is the what is your what is your rate of",
    "start": "1453450",
    "end": "1458940"
  },
  {
    "text": "failure been on Prem and is it gonna and it'll probably get worse frankly when you when you start using things like ec2",
    "start": "1458940",
    "end": "1466380"
  },
  {
    "text": "spot because if you put in a bid for something and you lose your bid that's the equivalent of a failed node because",
    "start": "1466380",
    "end": "1473010"
  },
  {
    "text": "the the node gets taken away from you so understanding how these elements impact the software that you're using is super",
    "start": "1473010",
    "end": "1478770"
  },
  {
    "text": "important so if it takes you eight and a half hours to repair that problem that",
    "start": "1478770",
    "end": "1486150"
  },
  {
    "text": "you had maybe you have some sort of raid system or some sort of erasure coding what that means is that for the duration",
    "start": "1486150",
    "end": "1492840"
  },
  {
    "text": "of that I'll just put these bill belts up there for the duration of that that",
    "start": "1492840",
    "end": "1498480"
  },
  {
    "text": "rebuild you are you're having a different kind of risk the way you",
    "start": "1498480",
    "end": "1503760"
  },
  {
    "text": "characterize risk is different anybody ever had a to disks fail at the same time I have okay so it statistically",
    "start": "1503760",
    "end": "1511290"
  },
  {
    "text": "it's really improbable but that doesn't mean it doesn't happen so these statistics are sort of like guidelines",
    "start": "1511290",
    "end": "1516810"
  },
  {
    "text": "that you can use to help yourself but really what it boils down to is you have to ask the question can I afford to not",
    "start": "1516810",
    "end": "1523080"
  },
  {
    "text": "have something durable so a lot of people say oh I'll just if it's an HPC job I'll just rerun the job if I have a",
    "start": "1523080",
    "end": "1528660"
  },
  {
    "text": "failure and this is what a lot of people do with with things like lustre if",
    "start": "1528660",
    "end": "1533880"
  },
  {
    "text": "they're they want really good performance they go and deploy on instance stores rather than EBS and then",
    "start": "1533880",
    "end": "1538940"
  },
  {
    "text": "when they have an instance store failure or a node failure they have to start all over that means they have to build out",
    "start": "1538940",
    "end": "1544050"
  },
  {
    "text": "their entire environment again it's not fun so so thinking about that is super",
    "start": "1544050",
    "end": "1550170"
  },
  {
    "text": "important so that's actually one of the reasons why I like Weka because it uses the instance store but it has a",
    "start": "1550170",
    "end": "1555240"
  },
  {
    "text": "durability mechanism that gives you the 11 nines of durability that you get from s3 it also supports tier",
    "start": "1555240",
    "end": "1561360"
  },
  {
    "text": "yes three and and and we'll actually talk about how that works functionally from a performance perspective MD test",
    "start": "1561360",
    "end": "1569549"
  },
  {
    "text": "is sort of the de facto test for metadata performance studies and it's used in a lot of really large system",
    "start": "1569549",
    "end": "1575549"
  },
  {
    "text": "RFPs so file creation rate is one of the I would say one of the most important",
    "start": "1575549",
    "end": "1581120"
  },
  {
    "text": "HPC application issues right that it often is the case that a large volume of",
    "start": "1581120",
    "end": "1588360"
  },
  {
    "text": "files are created not only at the beginning in order to read like input datasets for maybe some data",
    "start": "1588360",
    "end": "1594900"
  },
  {
    "text": "decomposition among them like parallel processes but also at intervals throughout the course of of application",
    "start": "1594900",
    "end": "1602130"
  },
  {
    "text": "runs and a lot of that has to do with the fact that most applications in the HPC space have some form of",
    "start": "1602130",
    "end": "1607559"
  },
  {
    "text": "checkpointing system where they write out intermediate datasets and if there's a problem with you out with the software",
    "start": "1607559",
    "end": "1614070"
  },
  {
    "text": "they can restart from their checkpoint it's an easily an internal component to the HPC app it's also the case that",
    "start": "1614070",
    "end": "1621860"
  },
  {
    "text": "usually those checkpoints are written out to the same storage system that they're using to do everything else so",
    "start": "1621860",
    "end": "1627480"
  },
  {
    "text": "there's some pretty big assumptions that are about durability so the metadata operations are critical function of",
    "start": "1627480",
    "end": "1633540"
  },
  {
    "text": "distributed file systems and I would say more often than not it ends up being a pretty major bottleneck to system",
    "start": "1633540",
    "end": "1640740"
  },
  {
    "text": "scalability and that's just sort of by nature of how client requests are serviced and typical metadata operations",
    "start": "1640740",
    "end": "1647820"
  },
  {
    "text": "involve gathering file system actor or file attributes from many many systems",
    "start": "1647820",
    "end": "1653130"
  },
  {
    "text": "or possibly even from in some architecture as a single metadata server and unfortunately you know neither of",
    "start": "1653130",
    "end": "1661140"
  },
  {
    "text": "those things bode well for for scalability if you're getting from a single system and every clients waiting",
    "start": "1661140",
    "end": "1666210"
  },
  {
    "text": "on it that's not good examples of these kinds of operations are file or directory creation and",
    "start": "1666210",
    "end": "1673260"
  },
  {
    "text": "removal for that matter stat operations in order to get things like sizes and a times and sometimes permissions and",
    "start": "1673260",
    "end": "1680370"
  },
  {
    "text": "those sorts of things so you can see there that I've got a slurm you see my s",
    "start": "1680370",
    "end": "1685799"
  },
  {
    "text": "run command and I'm using MD test against my test directory so this this",
    "start": "1685799",
    "end": "1691169"
  },
  {
    "text": "graph compares weka against luster and the reason that it compares those two and not something",
    "start": "1691169",
    "end": "1697369"
  },
  {
    "text": "else is because both of them present a fully POSIX compliant implementation you",
    "start": "1697369",
    "end": "1703700"
  },
  {
    "text": "could do it I suppose also with something like gpfs with the datapath",
    "start": "1703700",
    "end": "1709399"
  },
  {
    "text": "limits things are a little different so and I have a problem with the lustre run",
    "start": "1709399",
    "end": "1715039"
  },
  {
    "text": "here actually at in that's in 24 column but it's important to understand the i/o data path performance of your HPC",
    "start": "1715039",
    "end": "1721700"
  },
  {
    "text": "platform because usually HPC i/o workloads adopt one of two i/o modes a file per process model where each",
    "start": "1721700",
    "end": "1728869"
  },
  {
    "text": "process of a parallel application rights to and reads from its own file or a",
    "start": "1728869",
    "end": "1734899"
  },
  {
    "text": "shared file model we're a large set of parallel processes perform a collective",
    "start": "1734899",
    "end": "1740269"
  },
  {
    "text": "i/o to a single file at non-overlapping offsets the scalability of the shared",
    "start": "1740269",
    "end": "1747950"
  },
  {
    "text": "file workload since it involves locking usually is super important so it's often",
    "start": "1747950",
    "end": "1755149"
  },
  {
    "text": "limited in distributed file systems and this is true by the way not just for POSIX i/o but also for other kinds of",
    "start": "1755149",
    "end": "1761360"
  },
  {
    "text": "i/o that are maybe more aligned and more structured like in piao or hdf5 libraries and so for this exercise the",
    "start": "1761360",
    "end": "1769820"
  },
  {
    "text": "plan was to use the i/o our benchmark I or to study low-level sequential and",
    "start": "1769820",
    "end": "1776360"
  },
  {
    "text": "random i/o performance characteristics of the file systems this was done on AWS instances and we used the max IO IO",
    "start": "1776360",
    "end": "1786049"
  },
  {
    "text": "proxy application to study the represented application level performance so the nice thing was we",
    "start": "1786049",
    "end": "1794990"
  },
  {
    "text": "gathered a lot of data and we were able to see the sort of expected linear scalability up to the point where we hit",
    "start": "1794990",
    "end": "1801919"
  },
  {
    "text": "the limits of the hardware that we were running on",
    "start": "1801919",
    "end": "1805600"
  },
  {
    "text": "got to hit the right button you can do other types of things too so you know",
    "start": "1808909",
    "end": "1815269"
  },
  {
    "text": "you can do by giving different options to tie or it'll test different elements",
    "start": "1815269",
    "end": "1820850"
  },
  {
    "text": "of posix there's really just a question of you know how complete is your application require POSIX compliancy and",
    "start": "1820850",
    "end": "1826279"
  },
  {
    "text": "if it requires you know 100% compliance then things like NFS aren't going to work for you and so you may you may",
    "start": "1826279",
    "end": "1833029"
  },
  {
    "text": "think well I'd love to take my HPC workload to the cloud but I don't necessarily have the option of going to",
    "start": "1833029",
    "end": "1839870"
  },
  {
    "text": "something like EFS so what should I do so what you should do is go back and",
    "start": "1839870",
    "end": "1851950"
  },
  {
    "text": "we'll go here what you should do is you should go here - there we go",
    "start": "1851950",
    "end": "1862899"
  },
  {
    "text": "you should go here start got lucky oh you can you can do it today you know you",
    "start": "1865760",
    "end": "1870980"
  },
  {
    "text": "don't have to you know it's not like a special Amazon only thing and so what",
    "start": "1870980",
    "end": "1878210"
  },
  {
    "text": "this is is a self-service portal that I could put together and it's got actually",
    "start": "1878210",
    "end": "1883490"
  },
  {
    "text": "some really nice features so on the left it gives you sort of like what your price would be with on-demand pricing if",
    "start": "1883490",
    "end": "1889490"
  },
  {
    "text": "you have reserved instances you can use that to do self deploys as well you can specify what the capacity is whether or",
    "start": "1889490",
    "end": "1897020"
  },
  {
    "text": "not you want it to automatically tear to s3 or not you can get figure it out for there afterwards you can tell it whether",
    "start": "1897020",
    "end": "1903920"
  },
  {
    "text": "or not you want a hyper-converged deployment which is shared or whether or",
    "start": "1903920",
    "end": "1909080"
  },
  {
    "text": "not you want to do dedicated it'll turn on clients for you as well not huge numbers you can like if you have a CF",
    "start": "1909080",
    "end": "1916100"
  },
  {
    "text": "tea set up and slurm or something like that and you have thousands of clients that you're deploying you know you",
    "start": "1916100",
    "end": "1921110"
  },
  {
    "text": "definitely probably still do that but for the for this for the server component this was a nice self-service",
    "start": "1921110",
    "end": "1927770"
  },
  {
    "text": "way of doing cloud formation style deployments you can take their CFT and",
    "start": "1927770",
    "end": "1934580"
  },
  {
    "text": "modify it the other thing that you can do is you can if you want to you can do",
    "start": "1934580",
    "end": "1940070"
  },
  {
    "text": "things like specify you know what performance levels you you say you need and they do really good guessing based",
    "start": "1940070",
    "end": "1946370"
  },
  {
    "text": "on what kind of instance type you use about whether or not you can handle like the ena for example if you have bursty",
    "start": "1946370",
    "end": "1953000"
  },
  {
    "text": "Network requirements they will do some calculations about allowing you to use a",
    "start": "1953000",
    "end": "1958160"
  },
  {
    "text": "newer instance type for example like a like a a nice series or or like the i3",
    "start": "1958160",
    "end": "1964640"
  },
  {
    "text": "for example has a ena and so the smaller ene instances have have bursty network characteristics so when you're ready to",
    "start": "1964640",
    "end": "1972290"
  },
  {
    "text": "go deploy you tell it how many clients you want and what the type of the clients are and you click deploy cluster",
    "start": "1972290",
    "end": "1978640"
  },
  {
    "text": "and it will use a feature in confirmation template to just bring up",
    "start": "1978640",
    "end": "1984410"
  },
  {
    "text": "that stack it'll ask you what you want to call the stack name what your SSH key should be what some that you want to",
    "start": "1984410",
    "end": "1990290"
  },
  {
    "text": "deploy and what the VPC ID is you check the box at the bottom and away you go so when that's done",
    "start": "1990290",
    "end": "1996509"
  },
  {
    "text": "it looks like this and you get this one output it's one output is a lb and if we go",
    "start": "1996509",
    "end": "2003589"
  },
  {
    "text": "back down here and we look at the events is this big enough for you guys to see let me make this bigger we go look at",
    "start": "2003589",
    "end": "2010249"
  },
  {
    "text": "the events and all the things that they've done for us they set up the right security groups they set up",
    "start": "2010249",
    "end": "2016419"
  },
  {
    "text": "placement groups they setup log groups they set up security groups they set up",
    "start": "2016419",
    "end": "2022549"
  },
  {
    "text": "the backend systems they set up the two clients that I asked them to set up and",
    "start": "2022549",
    "end": "2027709"
  },
  {
    "text": "then they set up a load balancer in front of the nodes for the administrative aspect so all of those",
    "start": "2027709",
    "end": "2033979"
  },
  {
    "text": "things were done and then they're in the cluster is a ssin of the of the nodes as well so that they form a cluster that",
    "start": "2033979",
    "end": "2040009"
  },
  {
    "text": "was all done via CloudFormation template and I aim for the purposes of this demo",
    "start": "2040009",
    "end": "2046279"
  },
  {
    "text": "I feel like live demos have more credibility so I wiped out my demo environment and we're gonna do it",
    "start": "2046279",
    "end": "2052220"
  },
  {
    "text": "together so so you can see here that from start to finish to 2:00 p.m. the",
    "start": "2052220",
    "end": "2060500"
  },
  {
    "text": "cloud formation template was done at 2:13 so it is quick and you can actually",
    "start": "2060500",
    "end": "2066529"
  },
  {
    "text": "set it up and you can experiment so it's the best part okay so when it's done you click on that",
    "start": "2066529",
    "end": "2072679"
  },
  {
    "text": "link it takes you to their login page and that takes you to this system",
    "start": "2072679",
    "end": "2078049"
  },
  {
    "text": "overview so this is a relatively small cluster it's just just six systems and",
    "start": "2078049",
    "end": "2085210"
  },
  {
    "text": "inside of there they're gooey which is all client-side render oriented there's",
    "start": "2085210",
    "end": "2092628"
  },
  {
    "text": "a really great event system statistics file systems objects store definitions",
    "start": "2092629",
    "end": "2099500"
  },
  {
    "text": "so we're gonna do an object store definition together so I have two two clusters of systems I have one cluster",
    "start": "2099500",
    "end": "2104869"
  },
  {
    "text": "in u.s. East and I have another cluster in u.s. West so I've turned both of",
    "start": "2104869",
    "end": "2110660"
  },
  {
    "text": "those up so what we're gonna do is we are going to create we're going to",
    "start": "2110660",
    "end": "2116839"
  },
  {
    "text": "configure the clusters to talk to s3 we're gonna demonstrate that the filesystem is a shared file system we're",
    "start": "2116839",
    "end": "2123769"
  },
  {
    "text": "going to take a snapshot we're gonna then upload the snapshot to s3 so that we you like when at the point that we upload",
    "start": "2123769",
    "end": "2129990"
  },
  {
    "text": "that that snapshot to s3 we some we could blow up our cluster we could delete the whole thing we could roll back our or delete our entire cloud",
    "start": "2129990",
    "end": "2136890"
  },
  {
    "text": "formation stack and the data and the cluster state are safe and s3 which",
    "start": "2136890",
    "end": "2143460"
  },
  {
    "text": "means that now that snapshot out that data is there is durable it was durable already on Weka but if I if I want to",
    "start": "2143460",
    "end": "2150840"
  },
  {
    "text": "prostrate and replicate that to another region or if I want to have a process ami a business process or a development",
    "start": "2150840",
    "end": "2158040"
  },
  {
    "text": "process around dev and test that involves taking snapshots of stuff and then bringing them up as clones this is",
    "start": "2158040",
    "end": "2164460"
  },
  {
    "text": "a great way to do that so so we'll first we'll configure the object store so it",
    "start": "2164460",
    "end": "2171930"
  },
  {
    "text": "will select AWS s3 will pull up my",
    "start": "2171930",
    "end": "2177120"
  },
  {
    "text": "credentials my bucket name what was my book at name o for this we're gonna",
    "start": "2177120",
    "end": "2186360"
  },
  {
    "text": "create a new bucket that way will will be a hundred percent sure that everything is empty",
    "start": "2186360",
    "end": "2192890"
  },
  {
    "text": "amazingly hpc demo one is available",
    "start": "2196470",
    "end": "2201000"
  },
  {
    "text": "we create the bucket will verify the bucket exists",
    "start": "2204090",
    "end": "2211050"
  },
  {
    "text": "here it is HPC demo one okay so we'll go back over to our to our object store",
    "start": "2224480",
    "end": "2234530"
  },
  {
    "text": "config and we'll call this US East US three that's the bucket name I'll",
    "start": "2234530",
    "end": "2240800"
  },
  {
    "text": "specify the correct region and then we will go grab our access key and our",
    "start": "2240800",
    "end": "2247130"
  },
  {
    "text": "secret key and we'll hit validate and it says it's validated so now we'll save it",
    "start": "2247130",
    "end": "2253690"
  },
  {
    "text": "so now that the object stores been configured we can configure features that Weka has like tearing and saving",
    "start": "2253690",
    "end": "2260510"
  },
  {
    "text": "snapshots to objects so then we go into file systems we have a default local",
    "start": "2260510",
    "end": "2268400"
  },
  {
    "text": "file system and a default group that",
    "start": "2268400",
    "end": "2273680"
  },
  {
    "text": "file systems live in that's the air construct and if we go to one of our clients we can see that the wireless",
    "start": "2273680",
    "end": "2286280"
  },
  {
    "text": "connection here is flaky what a surprise we can see that this is a special",
    "start": "2286280",
    "end": "2295360"
  },
  {
    "text": "lucke FS type of file system and if we go to another client in the same place",
    "start": "2295360",
    "end": "2303130"
  },
  {
    "text": "you might ask the question well how do I know if those are the clients in the same place we go down here to clients",
    "start": "2303130",
    "end": "2309730"
  },
  {
    "text": "and this is us used yeah so 1897 178 22",
    "start": "2309730",
    "end": "2319420"
  },
  {
    "text": "1897 178 22 or 222 so they both have the",
    "start": "2322030",
    "end": "2328640"
  },
  {
    "text": "file system and I'll go here and you can see that there's nothing in there so I'm",
    "start": "2328640",
    "end": "2334400"
  },
  {
    "text": "going to set up watch and we'll do watch with a second of one and we'll do LS minus L over here to check out the",
    "start": "2334400",
    "end": "2345050"
  },
  {
    "text": "permissions I can write to it so immediately it shows up on the other",
    "start": "2345050",
    "end": "2351050"
  },
  {
    "text": "side so these are two separate systems they both have the special kernel driver or driver to mount the file system",
    "start": "2351050",
    "end": "2357380"
  },
  {
    "text": "and so we know that this is a shared file system okay so we go back over here to the file system view I'm going to",
    "start": "2357380",
    "end": "2368960"
  },
  {
    "text": "just shrink the capacity here so I can use some of the flash capacity for",
    "start": "2368960",
    "end": "2376370"
  },
  {
    "text": "something else okay so I'm gonna add a new group four-tiered file systems and",
    "start": "2376370",
    "end": "2384050"
  },
  {
    "text": "we'll call this tiered and this is the",
    "start": "2384050",
    "end": "2390590"
  },
  {
    "text": "object store that's configured and we'll keep our cash set to a month or space",
    "start": "2390590",
    "end": "2399800"
  },
  {
    "text": "oriented and we will send new data that's written to the cloud after say 10",
    "start": "2399800",
    "end": "2406220"
  },
  {
    "text": "minutes so because it's client-side rendering you kind of have to do like",
    "start": "2406220",
    "end": "2411950"
  },
  {
    "text": "one 1000 two 1000 on the network here anyway and then a lot of file system so",
    "start": "2411950",
    "end": "2417740"
  },
  {
    "text": "this will be called to upload so now",
    "start": "2417740",
    "end": "2425660"
  },
  {
    "text": "that I've created that file system I'll just verify that it that it exists here they have their own CLI called W CLI and",
    "start": "2425660",
    "end": "2433700"
  },
  {
    "text": "I'll give it the file systems list view and you can see here that there's the",
    "start": "2433700",
    "end": "2440570"
  },
  {
    "text": "two file systems one is called default the one that we touch that file on and the other one is called to upload so I'm",
    "start": "2440570",
    "end": "2447290"
  },
  {
    "text": "going to mount the to upload",
    "start": "2447290",
    "end": "2450370"
  },
  {
    "text": "now I can see that to upload is mounted I'm going to go repeat that process with",
    "start": "2462670",
    "end": "2467890"
  },
  {
    "text": "to upload and I'm gonna create it let's",
    "start": "2467890",
    "end": "2478810"
  },
  {
    "text": "call it a 10 Meg file",
    "start": "2478810",
    "end": "2481650"
  },
  {
    "text": "so now I have this file here and I should see it over here if I mount that file system because the second system",
    "start": "2489340",
    "end": "2496160"
  },
  {
    "text": "doesn't have the the new one mounted",
    "start": "2496160",
    "end": "2499869"
  },
  {
    "text": "oh I have to actually spell things properly mmm that's weird I don't know",
    "start": "2525900",
    "end": "2531000"
  },
  {
    "text": "what's up with that I'm on the right system I feel like I'm on the right system this is actually the beauty of live demos okay so what I am gonna do is",
    "start": "2531000",
    "end": "2540120"
  },
  {
    "text": "verify that I can see the systems from this side I can see the system from",
    "start": "2540120",
    "end": "2546060"
  },
  {
    "text": "those side okay I have to assume that there was a some until I did the list",
    "start": "2546060",
    "end": "2553140"
  },
  {
    "text": "that the cache didn't know about it I don't necessarily consider that to be a bad thing so now I'll go into the to",
    "start": "2553140",
    "end": "2559590"
  },
  {
    "text": "upload and I should already see the file there I do so I have this 10 Meg file here so the next thing I'm gonna do is",
    "start": "2559590",
    "end": "2566270"
  },
  {
    "text": "I'm going to take a snapshot so there's a couple different ways to do the whole",
    "start": "2566270",
    "end": "2575850"
  },
  {
    "text": "snapshot process well what we're gonna do is we're gonna take a normal snapshot but then we're gonna the second thing",
    "start": "2575850",
    "end": "2580950"
  },
  {
    "text": "that we're gonna do is we're gonna actually upload it to test 3 so if we do that my cheat sheet of commands here so",
    "start": "2580950",
    "end": "2592710"
  },
  {
    "text": "I'm gonna create the snapshot the file system is called to upload I'm gonna say",
    "start": "2592710",
    "end": "2601710"
  },
  {
    "text": "that the access point that I want to access it on it's called uploaded snapshot and the name of the snapshot",
    "start": "2601710",
    "end": "2607200"
  },
  {
    "text": "that I'm going to the actual name that I'm going to give it it's upload a snapshot so the reason there's a differentiation between uploaded",
    "start": "2607200",
    "end": "2614460"
  },
  {
    "text": "snapshot and the name is that in DevOps environments where you have some sort of CI CD type process it's very often the",
    "start": "2614460",
    "end": "2622170"
  },
  {
    "text": "case that you might have some limitations in the name of a snapshot but the metadata about it you want to",
    "start": "2622170",
    "end": "2630000"
  },
  {
    "text": "include there so the quote unquote name of the snapshot might be the access point like a mount point but the name",
    "start": "2630000",
    "end": "2635760"
  },
  {
    "text": "might be much more descriptive so we're gonna take this snapshot the data that",
    "start": "2635760",
    "end": "2641550"
  },
  {
    "text": "was returned to me the thing that is interesting to us is the stove status so",
    "start": "2641550",
    "end": "2646590"
  },
  {
    "text": "this has not been stowed in NS 3 yet but it is present and so now we're gonna go",
    "start": "2646590",
    "end": "2652980"
  },
  {
    "text": "back over to that other system just to verify that as present we snap shots list and we see that we have",
    "start": "2652980",
    "end": "2659900"
  },
  {
    "text": "the uploaded snapshot it is present there so now we're gonna upload it to",
    "start": "2659900",
    "end": "2665269"
  },
  {
    "text": "test three got everything in the file",
    "start": "2665269",
    "end": "2673910"
  },
  {
    "text": "system properly okay so this next part is a called a slug and what's",
    "start": "2673910",
    "end": "2681829"
  },
  {
    "text": "interesting about this is the way that they implemented snapped object is that",
    "start": "2681829",
    "end": "2687769"
  },
  {
    "text": "this is an s3 path and let me show you what I mean everybody see this hexadecimal bit at the front it ends in",
    "start": "2687769",
    "end": "2694880"
  },
  {
    "text": "F F of zero we're gonna go over to s3",
    "start": "2694880",
    "end": "2701720"
  },
  {
    "text": "and find that bucket that we just created and inside of it wouldn't you",
    "start": "2701720",
    "end": "2709910"
  },
  {
    "text": "know it in F F zero so that's the ID of the cluster that was configured to talk",
    "start": "2709910",
    "end": "2717829"
  },
  {
    "text": "to s3 and so there's d4 data and inside of there there's all of these different layers of key key space management and",
    "start": "2717829",
    "end": "2727579"
  },
  {
    "text": "so like this file here is small 44 bytes that's not big but I didn't put very much data in there either right so every",
    "start": "2727579",
    "end": "2733430"
  },
  {
    "text": "piece of data that I gave it it broke it up into pieces that it likes to manage",
    "start": "2733430",
    "end": "2738980"
  },
  {
    "text": "stuff in distributed it across its parallel distributed system and then when I'm uploading the snapshot to ask",
    "start": "2738980",
    "end": "2745640"
  },
  {
    "text": "three it's taking basically scraping it off in that native format and putting it in s3 so in it in AWS we often talk",
    "start": "2745640",
    "end": "2752720"
  },
  {
    "text": "about smart bits versus dumb bits but if you put bits into s3 you want them to be",
    "start": "2752720",
    "end": "2758299"
  },
  {
    "text": "smart because if they're dumb you can't use other services with them you have that limitation here too but the",
    "start": "2758299",
    "end": "2763970"
  },
  {
    "text": "trade-off is lustre with no durability no persistence to s3 at best persistence",
    "start": "2763970",
    "end": "2770000"
  },
  {
    "text": "CBS but with no way to state fully take a snapshot across all of the nodes or",
    "start": "2770000",
    "end": "2775119"
  },
  {
    "text": "something that gives you all of the features that you would typically get on like a NetApp filer with the performance",
    "start": "2775119",
    "end": "2780740"
  },
  {
    "text": "that you expect that of lustre so that trade-off in my mind is an easy one to make",
    "start": "2780740",
    "end": "2785880"
  },
  {
    "text": "so now that we've seen that these bits are there in an s3 we will go to our",
    "start": "2785880",
    "end": "2791940"
  },
  {
    "text": "second cluster so when I turn up the stack I did two stacks I did one in",
    "start": "2791940",
    "end": "2797090"
  },
  {
    "text": "Oregon and I did one in Virginia so I have two of these things here so this is",
    "start": "2797090",
    "end": "2805430"
  },
  {
    "text": "this is the East one where the file systems are are configured and here's the West one where the file systems",
    "start": "2805430",
    "end": "2813120"
  },
  {
    "text": "aren't configured the other thing that isn't configured is the object store so the objects two are not being configured",
    "start": "2813120",
    "end": "2818400"
  },
  {
    "text": "is analogous to I've got a new cluster I've got a new workload or I had some sort of disaster recovery scenario or",
    "start": "2818400",
    "end": "2824460"
  },
  {
    "text": "I'm going to be turning things up because the workflow operation here is I wrote data to my HPC cluster I persisted",
    "start": "2824460",
    "end": "2831900"
  },
  {
    "text": "it to s3 and now I want to restore it so the first thing I have to go do is again",
    "start": "2831900",
    "end": "2838920"
  },
  {
    "text": "reconfigure the object store for the US West one go back to my cheat sheet",
    "start": "2838920",
    "end": "2848270"
  },
  {
    "text": "I'll be deleting that after this show and then the object store is still in",
    "start": "2854560",
    "end": "2861520"
  },
  {
    "text": "u.s. East one you could use cross region replication if you want to remember with cross region replication you have to",
    "start": "2861520",
    "end": "2866619"
  },
  {
    "text": "turn on versioning on both sides which means that you have to when objects get deleted you have to make sure that",
    "start": "2866619",
    "end": "2872560"
  },
  {
    "text": "things get expunged properly so it's important to make sure that you have lifecycle policies to clean up deleted",
    "start": "2872560",
    "end": "2877630"
  },
  {
    "text": "objects otherwise they'll look it'll look funny you'll see one thing on the console you see another thing with AWS CLI tools",
    "start": "2877630",
    "end": "2883770"
  },
  {
    "text": "that kind of thing okay so this is",
    "start": "2883770",
    "end": "2889080"
  },
  {
    "text": "tiered I'll validate it validated",
    "start": "2889080",
    "end": "2894550"
  },
  {
    "text": "properly I'll click Save so you might ask the question why is this AWS guy",
    "start": "2894550",
    "end": "2901330"
  },
  {
    "text": "talking about this software so much Ceres is one never in the history of",
    "start": "2901330",
    "end": "2907030"
  },
  {
    "text": "POSIX compliant stuff has there ever been anything so simple and so easy to use did you know that there's like 400",
    "start": "2907030",
    "end": "2914950"
  },
  {
    "text": "plus user facing tunable zin gpfs and another 250 plus that engineering knows",
    "start": "2914950",
    "end": "2920920"
  },
  {
    "text": "about that customers are not supposed to use like that's madness that's crazy",
    "start": "2920920",
    "end": "2926650"
  },
  {
    "text": "like under what circumstances are you supposed to to really go in tune those things the second thing is they did such",
    "start": "2926650",
    "end": "2934510"
  },
  {
    "text": "a great job on the self-service portal and users can actually turn things up on their own it means you don't have to",
    "start": "2934510",
    "end": "2940089"
  },
  {
    "text": "talk to a sales guy it's great to come to reinvent and learn it's not great to be forced to sit through sales",
    "start": "2940089",
    "end": "2945369"
  },
  {
    "text": "conversations having been on the customer side I can attest to that some conversations are great some conversations not so good either way",
    "start": "2945369",
    "end": "2951820"
  },
  {
    "text": "you've got more important things to do so very much in sort of the vein of AWS",
    "start": "2951820",
    "end": "2956920"
  },
  {
    "text": "being able to do self provisioning self services is present here the second ii",
    "start": "2956920",
    "end": "2963250"
  },
  {
    "text": "believe the third the third thing is we don't have anything natively that meets",
    "start": "2963250",
    "end": "2969250"
  },
  {
    "text": "the HPC requirements around high performance with small file workloads",
    "start": "2969250",
    "end": "2974730"
  },
  {
    "text": "and and having something that's distributed you can use EBS and you get",
    "start": "2974730",
    "end": "2980589"
  },
  {
    "text": "pretty good performance on a per node basis you can use EFS and you'll get pretty great performance for read workloads you'll get great performance",
    "start": "2980589",
    "end": "2987010"
  },
  {
    "text": "on read clothes if you have a lot of data but if you have a small amount of data or if you have primarily small file workloads",
    "start": "2987010",
    "end": "2994720"
  },
  {
    "text": "or again if you have mixed small and large file workloads we don't have anything natively that that'll do the",
    "start": "2994720",
    "end": "3000210"
  },
  {
    "text": "trick so it's important to not be you know not have the blinders on too much",
    "start": "3000210",
    "end": "3005810"
  },
  {
    "text": "around what are what we're natively able to offer so we you know it may change in the future but at least for now there's",
    "start": "3005810",
    "end": "3012570"
  },
  {
    "text": "nothing out there that really hits the sweet spot for most of the HPC workloads except this so it's it's great that it's",
    "start": "3012570",
    "end": "3018840"
  },
  {
    "text": "available and and you know that the fully POSIX compliant point is actually super important okay so now this is the",
    "start": "3018840",
    "end": "3026760"
  },
  {
    "text": "u.s. West one we know it's us West because in the URL it says u.s. West so",
    "start": "3026760",
    "end": "3032880"
  },
  {
    "text": "things to ELB I know where I am now I'm gonna go back to my file systems and you",
    "start": "3032880",
    "end": "3037920"
  },
  {
    "text": "can see that I'm back where I started where I've got all of my flash capacity is deployed in my default and that",
    "start": "3037920",
    "end": "3046110"
  },
  {
    "text": "that's perfectly fine for most people I'm just going to edit that real quick and I'm gonna make it like ten megabytes",
    "start": "3046110",
    "end": "3051720"
  },
  {
    "text": "so I have the rest of it here so I have a tiered file system I'm going to add",
    "start": "3051720",
    "end": "3056850"
  },
  {
    "text": "the group already and slip they're tired",
    "start": "3056850",
    "end": "3064580"
  },
  {
    "text": "okay I'm gonna do the same thing where I want to keep it for a month cashed and",
    "start": "3067090",
    "end": "3073100"
  },
  {
    "text": "I'm gonna get get rid of it off the cache after ten minutes so now I have this tiered file system group and I can",
    "start": "3073100",
    "end": "3080540"
  },
  {
    "text": "add a a file system so I'm gonna add a file system and the other way you don't",
    "start": "3080540",
    "end": "3086450"
  },
  {
    "text": "have to do this via the GUI you can do it via CLI I've got my nice cheat sheet right here I've got I will actually do",
    "start": "3086450",
    "end": "3097520"
  },
  {
    "text": "that because I don't have the cheat sheet that had a file system and we'll call this um downloaded FS and we'll",
    "start": "3097520",
    "end": "3107120"
  },
  {
    "text": "give it the rest of the capacity so now we go back to our terminal",
    "start": "3107120",
    "end": "3112670"
  },
  {
    "text": "so these systems that we were working on or the clients associated with the first cluster and we'll verify that when we go",
    "start": "3112670",
    "end": "3122000"
  },
  {
    "text": "to the clients associated with the second cluster that they see the second cluster which as far as we know only has",
    "start": "3122000",
    "end": "3128420"
  },
  {
    "text": "this new empty file system and the default language is also empty so I'll go over to our second tab here see if",
    "start": "3128420",
    "end": "3135380"
  },
  {
    "text": "our connections are still there and how do we know that we're on the right ones",
    "start": "3135380",
    "end": "3140510"
  },
  {
    "text": "this one ends in two two six five eight this one ends in one seven one two three two we'll go back over here and we'll click",
    "start": "3140510",
    "end": "3146420"
  },
  {
    "text": "on clients two two six five eight one",
    "start": "3146420",
    "end": "3152960"
  },
  {
    "text": "seven one two three - sweet so we look at what's mounted just the",
    "start": "3152960",
    "end": "3158960"
  },
  {
    "text": "default file system will do a file systems list will do a file systems list",
    "start": "3158960",
    "end": "3169610"
  },
  {
    "text": "on both systems so we see they showed us the same thing they both have the default and they have that empty file",
    "start": "3169610",
    "end": "3175370"
  },
  {
    "text": "system called downloaded FS now that thing that was outputted before here",
    "start": "3175370",
    "end": "3181190"
  },
  {
    "text": "this is what we need this is the specification for that snapshot so we're",
    "start": "3181190",
    "end": "3187400"
  },
  {
    "text": "gonna use that to restore so here's my",
    "start": "3187400",
    "end": "3192590"
  },
  {
    "text": "slug this is gonna be super exciting",
    "start": "3192590",
    "end": "3200350"
  },
  {
    "text": "so we're saying download the file system from this s3 specification to this file",
    "start": "3200350",
    "end": "3207170"
  },
  {
    "text": "system called downloaded FS and the group name is actually teared and we'll",
    "start": "3207170",
    "end": "3215660"
  },
  {
    "text": "give it like 50 Meg total capacity and we'll give it a SSD capacity of 50 mega",
    "start": "3215660",
    "end": "3221480"
  },
  {
    "text": "as well hopefully the capacities that I put in before won't break it if it does we'll go fix it oh it already exists let",
    "start": "3221480",
    "end": "3227690"
  },
  {
    "text": "me delete it real quick",
    "start": "3227690",
    "end": "3230290"
  },
  {
    "text": "cool so now it's not there now it's not",
    "start": "3239050",
    "end": "3244360"
  },
  {
    "text": "there and we'll just download it so now we're gonna go back to the the events",
    "start": "3244360",
    "end": "3251950"
  },
  {
    "text": "list and the statistics so here we go so these statistics just for the last",
    "start": "3251950",
    "end": "3257680"
  },
  {
    "text": "hour we're gonna pull up more stats by the way there's so many statistics in this thing it's ridiculous",
    "start": "3257680",
    "end": "3262870"
  },
  {
    "text": "you can metric anything so specifically what we want is we want to look at objects we want to look at object",
    "start": "3262870",
    "end": "3268980"
  },
  {
    "text": "downloads so it just added this and it's shown up on the graph two things you can",
    "start": "3268980",
    "end": "3275320"
  },
  {
    "text": "infer from that one the data was being collected the whole time and to all of",
    "start": "3275320",
    "end": "3281770"
  },
  {
    "text": "the normal things that you to expect from modern software around crafting stuff nicely exists this is not a graph",
    "start": "3281770",
    "end": "3287410"
  },
  {
    "text": "that's being rendered on a server and presented to us this is client-side rendering that means that the data is in",
    "start": "3287410",
    "end": "3293740"
  },
  {
    "text": "JSON format and that means that you can process it if you want to via cloud watch events you can process it via",
    "start": "3293740",
    "end": "3299760"
  },
  {
    "text": "Nagios via you name it right so was the",
    "start": "3299760",
    "end": "3305860"
  },
  {
    "text": "one ganglia as the classic one for like HPC clusters so you can actually go you",
    "start": "3305860",
    "end": "3311710"
  },
  {
    "text": "can go and look at this data um it's got its on auto refresh we can you can even",
    "start": "3311710",
    "end": "3317710"
  },
  {
    "text": "take the we'll hide they like the other operations we'll just double click on this to see in more granularity what's",
    "start": "3317710",
    "end": "3325690"
  },
  {
    "text": "going on there and you can see there that it was it 318 it started so",
    "start": "3325690",
    "end": "3335520"
  },
  {
    "text": "downloading that stuff so the right one yeah I'm on the right one so now we'll",
    "start": "3335520",
    "end": "3340600"
  },
  {
    "text": "go look at file systems list and see we have now the downloaded FS we now need",
    "start": "3340600",
    "end": "3346150"
  },
  {
    "text": "to mount the downloaded FS",
    "start": "3346150",
    "end": "3349410"
  },
  {
    "text": "and we know the name of the file system is downloaded FS so now it's mounted we",
    "start": "3356170",
    "end": "3361780"
  },
  {
    "text": "go there and we should see that technic",
    "start": "3361780",
    "end": "3370060"
  },
  {
    "text": "file so we're in the second cluster in u.s. West and Oregon we've downloaded",
    "start": "3370060",
    "end": "3375640"
  },
  {
    "text": "the entirety of the snapshot from s3 and it's been restored to our local cluster",
    "start": "3375640",
    "end": "3381730"
  },
  {
    "text": "think about that for a second what that would mean for you and your HPC workloads when you deploy your clusters",
    "start": "3381730",
    "end": "3388540"
  },
  {
    "text": "or your and maybe your application as checkpointing and maybe it doesn't when you deploy your clusters you're usually",
    "start": "3388540",
    "end": "3396280"
  },
  {
    "text": "leaving them on if it's on Prem and if it's in the cloud you're you're gonna",
    "start": "3396280",
    "end": "3401410"
  },
  {
    "text": "hurry to tear things down because it's expensive to leave stuff up imagine if",
    "start": "3401410",
    "end": "3406810"
  },
  {
    "text": "you were using ec2 spot and you put in a bid for all of your systems and you got",
    "start": "3406810",
    "end": "3412360"
  },
  {
    "text": "that two-minute notification that said hey guys you're gonna lose your bid",
    "start": "3412360",
    "end": "3417960"
  },
  {
    "text": "isn't that a great time to upload the snapshot that you want to take take a snapshot immediately and then upload it",
    "start": "3417960",
    "end": "3425350"
  },
  {
    "text": "to s3 that's your checkpoint your app can checkpoint whatever the contents of the file system are but for your",
    "start": "3425350",
    "end": "3431650"
  },
  {
    "text": "purposes you can take the snapshot of your entire HPC storage cluster upload it to s3 and come back to it when you're",
    "start": "3431650",
    "end": "3438040"
  },
  {
    "text": "ready if the spot price doesn't come down in the next hour that's fine you can come back when it comes down if you",
    "start": "3438040",
    "end": "3443950"
  },
  {
    "text": "have systems that you're gonna turn off because you want to go on vacation and not leave them up you can do that too if",
    "start": "3443950",
    "end": "3449140"
  },
  {
    "text": "you have a desire to spin up ten more clusters because you have some you know dev or test workload that you want to be",
    "start": "3449140",
    "end": "3456370"
  },
  {
    "text": "able to run things on by the way you don't have to just use this for HBCUs for anything you can do that too and you can do it based on the same slug",
    "start": "3456370",
    "end": "3462850"
  },
  {
    "text": "once it's in s3 an object is immutable which means that that snapshot that you've taken until you delete it from s3",
    "start": "3462850",
    "end": "3469240"
  },
  {
    "text": "is going to contain everything that was in your cluster at that point in time that level of integration doesn't exist",
    "start": "3469240",
    "end": "3476470"
  },
  {
    "text": "in any other solution and so I'm super happy to be able to present it to you so",
    "start": "3476470",
    "end": "3481980"
  },
  {
    "text": "so now we'll go back with our remaining couple of minutes go back to PowerPoint",
    "start": "3481980",
    "end": "3487859"
  },
  {
    "text": "a couple things to talk about so this is sort of a summary of the stuff that we talked about will come up in a second",
    "start": "3487859",
    "end": "3494400"
  },
  {
    "text": "all right cool so EBS performance is fine but it doesn't scale and you know",
    "start": "3494400",
    "end": "3500489"
  },
  {
    "text": "the way that distributed file systems scale EFS is fine but it doesn't handle",
    "start": "3500489",
    "end": "3505739"
  },
  {
    "text": "writes with low latency and it doesn't give you full POSIX compliancy no NFS platform will and the ability to use",
    "start": "3505739",
    "end": "3516180"
  },
  {
    "text": "things like lustre exists in fact you can go to the AWS marketplace and you can find a luster on EBS Quick Start",
    "start": "3516180",
    "end": "3522630"
  },
  {
    "text": "basically and you can start up that cluster and you'll see it like like it'll be there it would be the real thing but how do you know that you've",
    "start": "3522630",
    "end": "3529259"
  },
  {
    "text": "tuned it properly how do you know that one of those 652 Nobles isn't wrong you want something",
    "start": "3529259",
    "end": "3535470"
  },
  {
    "text": "that's going to do the right thing without you having to tell it to do the right thing that's more important than ever in the cloud because in the cloud",
    "start": "3535470",
    "end": "3542099"
  },
  {
    "text": "not everything works right all the time systems get turned off due to things like ec2 spot bidding going away",
    "start": "3542099",
    "end": "3549239"
  },
  {
    "text": "EBS volumes fail like we have we have you know trillions of them in you know",
    "start": "3549239",
    "end": "3554640"
  },
  {
    "text": "invariably you're going to find that some something that you've deployed you know has failed and you weren't aware of",
    "start": "3554640",
    "end": "3560640"
  },
  {
    "text": "it so you want a system that's going to provide a level of durability beyond what you would traditionally deploy you",
    "start": "3560640",
    "end": "3567059"
  },
  {
    "text": "know like with a traditional sand or something like that I mean so whack I can actually do that for you it's got",
    "start": "3567059",
    "end": "3572249"
  },
  {
    "text": "its own durability mechanisms built in under the covers but with the snapshot to object you don't have to even worry",
    "start": "3572249",
    "end": "3578400"
  },
  {
    "text": "about that you can just take as many snapshots the object as you want and though all they'll all be there so for for other",
    "start": "3578400",
    "end": "3587279"
  },
  {
    "text": "things if you're interested in getting started don't forget start got like a die I Oh as well as check out our competency",
    "start": "3587279",
    "end": "3594869"
  },
  {
    "text": "program and we have some other programmatic elements like we have quickstarts that are cloud formation",
    "start": "3594869",
    "end": "3599880"
  },
  {
    "text": "template based ways for you to spin up third-party applications and the ADA",
    "start": "3599880",
    "end": "3605489"
  },
  {
    "text": "base marketplace as well thank you very much for attending if you have any questions I'll be sticking around",
    "start": "3605489",
    "end": "3611450"
  },
  {
    "text": "afterwards before having to run to the Venetian for my next session thanks very",
    "start": "3611450",
    "end": "3617180"
  },
  {
    "text": "much",
    "start": "3617180",
    "end": "3619270"
  }
]