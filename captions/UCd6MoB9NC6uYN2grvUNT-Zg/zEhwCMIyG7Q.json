[
  {
    "start": "0",
    "end": "36000"
  },
  {
    "text": "all right good afternoon everybody thank you I'm gonna move this down all right so I'm gonna talk about a couple things",
    "start": "0",
    "end": "9510"
  },
  {
    "text": "today the kind of the two key takeaways I'm going to lay these out now and then kind of get there is I want to talk",
    "start": "9510",
    "end": "14790"
  },
  {
    "text": "about the idea of dealing with data of this scale of this volume and how we can",
    "start": "14790",
    "end": "20640"
  },
  {
    "text": "kind of make it consumable for smaller groups I'm gonna talk about archives of convenience and then I'm going to talk about kind of using AWS infrastructure",
    "start": "20640",
    "end": "27330"
  },
  {
    "text": "to get there and how it enables a whole other class of participants that really weren't able to take advantage of this kind of data at scale so with that kind",
    "start": "27330",
    "end": "35309"
  },
  {
    "text": "of setting some context we sincerely believe that organizations that can leverage data are going to devour ones",
    "start": "35309",
    "end": "41489"
  },
  {
    "start": "36000",
    "end": "60000"
  },
  {
    "text": "that can't you've probably heard the expression that Netflix is a metrics platform that just happens to serve",
    "start": "41489",
    "end": "46890"
  },
  {
    "text": "video there's a tremendous truth to that and we're generating data at an",
    "start": "46890",
    "end": "52649"
  },
  {
    "text": "unprecedented rate and the tools and capabilities to handle that and deal with that have to grow with it have to",
    "start": "52649",
    "end": "59219"
  },
  {
    "text": "be able to scale so as part of that there are billions of dollars worth of publicly funded data sitting there",
    "start": "59219",
    "end": "66630"
  },
  {
    "start": "60000",
    "end": "83000"
  },
  {
    "text": "waited to be you to be used right data is a national asset right now too much",
    "start": "66630",
    "end": "71640"
  },
  {
    "text": "of that is locked up it's in formats that are kind of archival friendly but not readily usable they're in massive",
    "start": "71640",
    "end": "77490"
  },
  {
    "text": "massive sizes it requires us it requires a skill set to get at it and it's one of",
    "start": "77490",
    "end": "83820"
  },
  {
    "start": "83000",
    "end": "126000"
  },
  {
    "text": "those things that it's just not working right now an estimate is that earth",
    "start": "83820",
    "end": "89130"
  },
  {
    "text": "scientists spend about 60% of their time getting at that data I'm trying to prepare that data so to put that in",
    "start": "89130",
    "end": "95189"
  },
  {
    "text": "context like to make that a little bit more real this is a 50 minute talk I would spend the next 30 minutes up here",
    "start": "95189",
    "end": "100770"
  },
  {
    "text": "playing with my laptop getting these slides ready to then talk to you for 20 it's absolutely unacceptable in pretty",
    "start": "100770",
    "end": "107159"
  },
  {
    "text": "much any industry except here and data wrangling is like a whole career path",
    "start": "107159",
    "end": "112290"
  },
  {
    "text": "that you can go down and it's just if you can go at this if we could have this",
    "start": "112290",
    "end": "117899"
  },
  {
    "text": "like not even make it bigs exist they're still spending a third of their day doing it but if you could have this you could double the amount of science you",
    "start": "117899",
    "end": "123240"
  },
  {
    "text": "could double the amount of research that's going on to get there so this kind of throws this out there is the problem but it's getting compounded so",
    "start": "123240",
    "end": "129599"
  },
  {
    "start": "126000",
    "end": "178000"
  },
  {
    "text": "one of the things that we do is we support NASA and some of their cloud transitions and part of what we're looking at now",
    "start": "129599",
    "end": "135300"
  },
  {
    "text": "are some of the upcoming missions they have some new missions coming up SWAT and nice are there gonna be generating data at just an insane rate jet NASA is",
    "start": "135300",
    "end": "142470"
  },
  {
    "text": "looking to move hundreds of petabytes to data of data to the cloud up to AWS in the near future nice are one of the new",
    "start": "142470",
    "end": "150299"
  },
  {
    "text": "missions coming up there's 300 gigabyte granules so this data is carved up into pieces accessible pieces at 300 gigs",
    "start": "150299",
    "end": "157440"
  },
  {
    "text": "each gonna be 150 petabytes of data put up there 80 terabytes a day new data",
    "start": "157440",
    "end": "162599"
  },
  {
    "text": "being generated right this is just an insane amount of data but this stuff is used for things like volcanoes floods",
    "start": "162599",
    "end": "171950"
  },
  {
    "text": "forest monitoring like there's tons of incredibly important uses for this information but it's not it's just very",
    "start": "171950",
    "end": "179160"
  },
  {
    "start": "178000",
    "end": "199000"
  },
  {
    "text": "difficult to get to which sets up this kind of weird situation if you have the resources you can figure out how to",
    "start": "179160",
    "end": "185069"
  },
  {
    "text": "harness big data but what happens if you don't what happens if you're not the group that either has the resources",
    "start": "185069",
    "end": "191129"
  },
  {
    "text": "whether that's dollars people skills time pick your problem what do you do how do you avoid kind of that situation",
    "start": "191129",
    "end": "197940"
  },
  {
    "text": "where you get consumed by other groups so for all the work we've done and this is a little honestly a little painful",
    "start": "197940",
    "end": "204389"
  },
  {
    "start": "199000",
    "end": "220000"
  },
  {
    "text": "for me but of all the work we've done in trying to push data out there we still hear from big-name people I don't know",
    "start": "204389",
    "end": "211440"
  },
  {
    "text": "what data exists I can't use it even if it is out there I don't know how to get it or I can't find it like I know this",
    "start": "211440",
    "end": "217739"
  },
  {
    "text": "thing exists but I don't know how to get there I don't know how to find it I don't know what to do with it so if you",
    "start": "217739",
    "end": "222870"
  },
  {
    "start": "220000",
    "end": "245000"
  },
  {
    "text": "step back and we think about how we approach problems we approach them kind of threefold we want to better",
    "start": "222870",
    "end": "229139"
  },
  {
    "text": "understand the problem we're going after through data we want to be able to bring the data that's relevant to the problem",
    "start": "229139",
    "end": "234450"
  },
  {
    "text": "and understand what's really going on and make factually based decisions we want to build solutions around that and",
    "start": "234450",
    "end": "240450"
  },
  {
    "text": "actually be able to affect that problem and we need to measure how we did with analytics well that leaves us in a bad",
    "start": "240450",
    "end": "247290"
  },
  {
    "start": "245000",
    "end": "264000"
  },
  {
    "text": "spot because now what we have or we have clients that we're working with that we sincerely care about their mission they're really trying to make a major",
    "start": "247290",
    "end": "253829"
  },
  {
    "text": "impact we know that there's billions of dollars worth of data there's hundreds of petabytes of data sitting out there",
    "start": "253829",
    "end": "259680"
  },
  {
    "text": "waiting to be consumed how do we bridge this gap so we decided to take a piece",
    "start": "259680",
    "end": "266400"
  },
  {
    "start": "264000",
    "end": "355000"
  },
  {
    "text": "of it here so we set out a couple of goals we want to make the data discovery easier and",
    "start": "266400",
    "end": "271830"
  },
  {
    "text": "interactive and we wanted to target kind of lower bandwidth lower resolution lower processing devices we want to try",
    "start": "271830",
    "end": "278340"
  },
  {
    "text": "to make this stuff as accessible and discoverable as possible we need to be highly configurable so both in terms of",
    "start": "278340",
    "end": "284520"
  },
  {
    "text": "the data that we're using as well as the processing that we're actually going to be applying to it we're a small company",
    "start": "284520",
    "end": "292320"
  },
  {
    "text": "we're not NASA and so we really had to look at the budget and how do we do this",
    "start": "292320",
    "end": "297780"
  },
  {
    "text": "in a cost-effective way and this is where kind of the AWS managed services pieces come into it in particular serval",
    "start": "297780",
    "end": "303060"
  },
  {
    "text": "is computing we need to be able to scale as close to zero as possible when it's not actively doing something but then",
    "start": "303060",
    "end": "308340"
  },
  {
    "text": "when we actually we want to scale this up to really work with clients who are literally trying to save the world in",
    "start": "308340",
    "end": "313740"
  },
  {
    "text": "some manner how do we do it how do we scale up and it set up kind of this little bit of a philosophical one at the",
    "start": "313740",
    "end": "319110"
  },
  {
    "text": "bottom stop putting data in things there's a lot of efforts in terms of ok I'm gonna",
    "start": "319110",
    "end": "324720"
  },
  {
    "text": "take this source data I'm gonna reprocess it in some way I'm going to jam it into my analytics tool ok but it",
    "start": "324720",
    "end": "331410"
  },
  {
    "text": "only goes that kind of a part of the problem if that tool or if that application is not consuming all of the",
    "start": "331410",
    "end": "336990"
  },
  {
    "text": "data that you might want to use well now you just have another place to go look another archive that you need to deal",
    "start": "336990",
    "end": "342570"
  },
  {
    "text": "with and figure out how to effectively extract data out of or merge data into you haven't really solved the problem so",
    "start": "342570",
    "end": "348690"
  },
  {
    "text": "the asterisks on the things is because you should put it in the cloud but don't put it in a box when we get out there all right so this is kind of what we set",
    "start": "348690",
    "end": "354990"
  },
  {
    "text": "out so we had this idea of kind of being this personal shopper for data so you",
    "start": "354990",
    "end": "363060"
  },
  {
    "start": "355000",
    "end": "404000"
  },
  {
    "text": "know if I'm using something like trunk club I tell them kind of who I wish I was and they go and they somebody who",
    "start": "363060",
    "end": "370169"
  },
  {
    "text": "dresses better than I do shops and puts together a thing sends me some outfits and I can kind of go from there so that's kind of the philosophy behind",
    "start": "370169",
    "end": "375570"
  },
  {
    "text": "this I want to study a hurricane I want to study a volcano and be able to pull together kind of heterogeneous",
    "start": "375570",
    "end": "382010"
  },
  {
    "text": "interdisciplinary curated data together into these things and give me a data set that I can work with now if you're the",
    "start": "382010",
    "end": "389159"
  },
  {
    "text": "kind of user who knows I need mato to 1km data like it's probably not",
    "start": "389159",
    "end": "394909"
  },
  {
    "text": "targeting you you you're you know if you're on the instrument team this is not your thing",
    "start": "394909",
    "end": "400530"
  },
  {
    "text": "we're trying to go after how do we make this data more accessible more readily available so at a high level what we",
    "start": "400530",
    "end": "406260"
  },
  {
    "start": "404000",
    "end": "484000"
  },
  {
    "text": "talked about is something like this kind of an overall data flow there is some data in the cloud now public datasets",
    "start": "406260",
    "end": "412080"
  },
  {
    "text": "NASA NOAA USGS I learned today that apparently there's a marvel universe",
    "start": "412080",
    "end": "417650"
  },
  {
    "text": "interrelated data set up there too but there's a lot of data available in the cloud now but it's kind of distributed",
    "start": "417650",
    "end": "424530"
  },
  {
    "text": "to anyone who put it up there and how it's up there there's also a ton of data that still exists on Prem and even with",
    "start": "424530",
    "end": "429630"
  },
  {
    "text": "efforts to do this transition those products are not going to show up tomorrow this is going to take some time",
    "start": "429630",
    "end": "434700"
  },
  {
    "text": "to move these there's data exposed through opened app there's data exposed to the OGC wxs services there's lots of",
    "start": "434700",
    "end": "441240"
  },
  {
    "text": "capabilities to expose this data FTP even still but how do we make that and kind of bring those things together and",
    "start": "441240",
    "end": "446700"
  },
  {
    "text": "so we had this idea of okay we're going to go after some discovery pieces and then we're going to be able to create kind of on the right-hand side here this",
    "start": "446700",
    "end": "452880"
  },
  {
    "text": "idea of an archive of convenience so when people talk about the cloud that you always hear a lot of talk about kind",
    "start": "452880",
    "end": "458430"
  },
  {
    "text": "of the elasticity of computing right we can do an elastic we can scale up and down from a compute perspective what we",
    "start": "458430",
    "end": "463919"
  },
  {
    "text": "wanted to take advantage of is kind of the elasticity of storage we can create basically ephemeral archives that",
    "start": "463919",
    "end": "469169"
  },
  {
    "text": "contain the data you want in the way you want to use it and you can then process that or combine that with other things",
    "start": "469169",
    "end": "475380"
  },
  {
    "text": "whatever you want to do with it and then when you're done we can destroy that and clean that back up and then layering on",
    "start": "475380",
    "end": "481169"
  },
  {
    "text": "top of this kind of integration with other points I'll talk about that as we go all right so here's what we put",
    "start": "481169",
    "end": "486540"
  },
  {
    "start": "484000",
    "end": "616000"
  },
  {
    "text": "together this is a demo piece I'm using gos data for this as is go 16 data",
    "start": "486540",
    "end": "492200"
  },
  {
    "text": "honestly because it's just beautiful data but everything I'm talking about applies to other products so the idea",
    "start": "492200",
    "end": "499169"
  },
  {
    "text": "here is you can come and hit this tool you can see goes this is the entire goes archive this renders I'll show you some",
    "start": "499169",
    "end": "507570"
  },
  {
    "text": "more when we flip over to the tool this is the entire archive it updates daily",
    "start": "507570",
    "end": "513270"
  },
  {
    "text": "so as new data is ingested we can trigger it and rip and and continue to extend the video but everything is scrub",
    "start": "513270",
    "end": "519300"
  },
  {
    "text": "Abul we can move through this we can select events and again I'll show you some other features here from here",
    "start": "519300",
    "end": "524550"
  },
  {
    "text": "though once you've selected your event you can ask the stage some data in s3 so what we're showing here are bands within",
    "start": "524550",
    "end": "529620"
  },
  {
    "text": "goes but again think about this across multiple products so I want to be I want some Landsat data I want some",
    "start": "529620",
    "end": "536220"
  },
  {
    "text": "Sentinel data I want some gos data I want it in this area of interest I want in this time of interest and I want you",
    "start": "536220",
    "end": "542490"
  },
  {
    "text": "to stage it for me in this output format I'm showings are there a couple other things I'll talk about when we get a",
    "start": "542490",
    "end": "547980"
  },
  {
    "text": "little deeper and then an email address because it's the Internet so what",
    "start": "547980",
    "end": "553410"
  },
  {
    "text": "happens here is this thing will run and it's going to produce an s3 bucket for you in that s3 bucket we're gonna put",
    "start": "553410",
    "end": "560399"
  },
  {
    "text": "out the clip of the actual area we're looking for because again we're trying to figure out we're trying to make this",
    "start": "560399",
    "end": "565829"
  },
  {
    "text": "data accessible so if what you want is just a pretty screenshot of Maria or a pretty video of Maria to embed we can",
    "start": "565829",
    "end": "571889"
  },
  {
    "text": "give you that we can clip it and drop it in there if if what you want our images high resolution images PNG s we spit",
    "start": "571889",
    "end": "578670"
  },
  {
    "text": "those out in there as well and then ultimately we give you a data archive that you can attach a more sophisticated analysis tool or use that to feed",
    "start": "578670",
    "end": "585959"
  },
  {
    "text": "something like redshift a Hadoop cluster or whatever the case may be to kind of do the analysis you want to do we can",
    "start": "585959",
    "end": "592019"
  },
  {
    "text": "plug into these things what we set out to do is not try to solve the general",
    "start": "592019",
    "end": "597180"
  },
  {
    "text": "case you know hundreds of petabytes thousands tens of thousands of products out there instead we're prioritizing the",
    "start": "597180",
    "end": "604439"
  },
  {
    "text": "ones that are interest to people which ones need are going to have the broadest user base what tools do we need to be",
    "start": "604439",
    "end": "610860"
  },
  {
    "text": "able to feed this into and kind of stop trying to solve this overall general case of it once we've got that data",
    "start": "610860",
    "end": "618240"
  },
  {
    "start": "616000",
    "end": "645000"
  },
  {
    "text": "there now users can take it and kind of extend it out wherever they want to go this is a Jupiter notebook I'll show you some more of this in a minute but it's",
    "start": "618240",
    "end": "624839"
  },
  {
    "text": "kind of a jupiter notebook plug into that s3 bucket we're not feeding this into a custom and analytics tool we're",
    "start": "624839",
    "end": "630449"
  },
  {
    "text": "not feeding this into something proprietary we're making or staging this data from whatever source we need to get",
    "start": "630449",
    "end": "636240"
  },
  {
    "text": "it from applying whatever processing we need to do to make it usable and then sticky making available in s3 or plugged",
    "start": "636240",
    "end": "642059"
  },
  {
    "text": "into your tool of choice as long as we support it I'll talk about pan geo in a minute all right so behind the scenes",
    "start": "642059",
    "end": "647490"
  },
  {
    "start": "645000",
    "end": "651000"
  },
  {
    "text": "and how this stuff works what we have is we had to go after the discovery problem",
    "start": "647490",
    "end": "655259"
  },
  {
    "text": "first we had to figure out all right how do we make this approachable there are",
    "start": "655259",
    "end": "660269"
  },
  {
    "text": "plenty of tools out there that let you search a specific archive how do we go at kind of making this look like something you can get to",
    "start": "660269",
    "end": "666430"
  },
  {
    "start": "664000",
    "end": "759000"
  },
  {
    "text": "one of the things that we've done kind of kind of unrelated or kind of",
    "start": "666430",
    "end": "671680"
  },
  {
    "text": "tangentially to their science base is we spend a lot of time doing video streaming processing as well right now",
    "start": "671680",
    "end": "676900"
  },
  {
    "text": "it's something like 70 some-odd percent of Internet traffic is streaming video um if you're screaming a 4k movie these",
    "start": "676900",
    "end": "685480"
  },
  {
    "text": "things are huge right so a single scene and goes is fifty four hundred by fifty four hundred right like they're big",
    "start": "685480",
    "end": "690940"
  },
  {
    "text": "tiles but streaming 4k video isn't that far off and when you break down kind of",
    "start": "690940",
    "end": "696940"
  },
  {
    "text": "how streaming video works you have this video and it's made up of these individual frames right",
    "start": "696940",
    "end": "702820"
  },
  {
    "text": "lots of frames are together there those are run through a coding that's designed to produce high compression across",
    "start": "702820",
    "end": "710710"
  },
  {
    "text": "transition between frames so now you produce this streaming video capable thing but this is still a relatively",
    "start": "710710",
    "end": "717880"
  },
  {
    "text": "large file alright so how do we kind of break that down so that gets fed into this is again a solved problem in the",
    "start": "717880",
    "end": "724000"
  },
  {
    "text": "video space that gets broken down using something like HLS HTTP Live Streaming -",
    "start": "724000",
    "end": "729790"
  },
  {
    "text": "is another one and what they do is they take this high this relatively high resolution video generate a manifest",
    "start": "729790",
    "end": "736390"
  },
  {
    "text": "file for it split it up into very small chunks two-second segments something along those lines and what this lets you",
    "start": "736390",
    "end": "742240"
  },
  {
    "text": "do is now fetch prefetch based on bandwidth how much you need to be able to stream to show what you're showing",
    "start": "742240",
    "end": "747790"
  },
  {
    "text": "but also let you arbitrarily jump into the archive so we took advantage of that and we crawled the entire go sixteen",
    "start": "747790",
    "end": "755020"
  },
  {
    "text": "archive and produced multiple resolution videos off of it I'll talk some more about that later so it's not it doesn't",
    "start": "755020",
    "end": "763150"
  },
  {
    "start": "759000",
    "end": "777000"
  },
  {
    "text": "just work for gos though right and again I used goes because it's really pretty this is Himawari data showing the",
    "start": "763150",
    "end": "769029"
  },
  {
    "text": "Eclipse next time it passes over you'll be able to see the Sun pass over and the Eclipse kind of chase it in the other",
    "start": "769029",
    "end": "774910"
  },
  {
    "text": "direction this is Himawari data this is a Solar Dynamics Observatory data again",
    "start": "774910",
    "end": "780880"
  },
  {
    "start": "777000",
    "end": "814000"
  },
  {
    "text": "assembling same concept we're applying kind of video codec video technologies I gave a talk several years ago at AG you",
    "start": "780880",
    "end": "787680"
  },
  {
    "text": "where it really bothered me that there were tools like Final Cut Pro and video",
    "start": "787680",
    "end": "792910"
  },
  {
    "text": "editing software that can do histograms that can do live filters and applications against video streams at 4k",
    "start": "792910",
    "end": "799180"
  },
  {
    "text": "resolution on you know your iPad and moving around a set of data in the science space that",
    "start": "799180",
    "end": "805570"
  },
  {
    "text": "are like okay they're five K by five K not 4k but still this was kind of this insurmountable problem and it really",
    "start": "805570",
    "end": "811360"
  },
  {
    "text": "bothered me and so we applied a lot of the video technologies to it this is rosetta data from ISA and in",
    "start": "811360",
    "end": "818410"
  },
  {
    "start": "814000",
    "end": "827000"
  },
  {
    "text": "this case what you're seeing is kind of the you know that the asteroid orbiting or rotating but same concept being",
    "start": "818410",
    "end": "825370"
  },
  {
    "text": "applied here all right so how this works under the",
    "start": "825370",
    "end": "830740"
  },
  {
    "start": "827000",
    "end": "917000"
  },
  {
    "text": "covers how we make this happen NOAA data is the go date goes 16 date is",
    "start": "830740",
    "end": "835959"
  },
  {
    "text": "available in the public data set so there's an s3 bucket there's an event",
    "start": "835959",
    "end": "841839"
  },
  {
    "text": "that triggered whatever new data is added to it we hang on for that it's one of those kind of embarrassing ly",
    "start": "841839",
    "end": "847540"
  },
  {
    "text": "parallel problems but we use lambda to then take those individual scenes and produce pngs",
    "start": "847540",
    "end": "852940"
  },
  {
    "text": "so we can scale that out kind of at whatever level however fast we want to go through the archive we can kind of",
    "start": "852940",
    "end": "858820"
  },
  {
    "text": "scale that horizontally it's really budget driven not anything else these pngs are then put together and we",
    "start": "858820",
    "end": "866500"
  },
  {
    "text": "can leverage the elastic transcoder service to start reassembling these we take these pngs we spin up a cluster",
    "start": "866500",
    "end": "873430"
  },
  {
    "text": "that converts those into small h.264 videos again using that video encoding and then feed this all into the elastic",
    "start": "873430",
    "end": "879850"
  },
  {
    "text": "transcoder a managed service which then reassembles this up into a video carves it back into the tiny two-second chunks",
    "start": "879850",
    "end": "888519"
  },
  {
    "text": "that lets us do the streaming kind of the arbitrary seeking into the data and dumps that into an s3 bucket all right",
    "start": "888519",
    "end": "894190"
  },
  {
    "text": "so now we have a video the first time we did this with goes I think it ran for somewhere around 48 hours 48 50 hours to",
    "start": "894190",
    "end": "900370"
  },
  {
    "text": "do the entire 20-some terabyte goes archive once we we felt reasonably comfortable at this worked we turned the",
    "start": "900370",
    "end": "906579"
  },
  {
    "text": "volume up and it took a little over 2 hours to kind of reprocess the entire archive when we were making tweaks to how we are doing the frame generation",
    "start": "906579",
    "end": "912579"
  },
  {
    "text": "and everything else all right so we have a video stuffed in s3 so now we look at",
    "start": "912579",
    "end": "919120"
  },
  {
    "text": "okay how do we get at this again low cost simple we put together a little",
    "start": "919120",
    "end": "924880"
  },
  {
    "text": "static site host on s3 with with CloudFront runs completely client-side there's no server back-end for this this",
    "start": "924880",
    "end": "931209"
  },
  {
    "text": "is available to distributed there it's serving up that HLS videostream with two second chunks and then wrote some",
    "start": "931209",
    "end": "937820"
  },
  {
    "text": "JavaScript to be able to translate from a time sequence in the video frame back to the underlying science frames this",
    "start": "937820",
    "end": "944630"
  },
  {
    "text": "lessons map back into the actual underlying goes data all right so some",
    "start": "944630",
    "end": "951139"
  },
  {
    "start": "949000",
    "end": "985000"
  },
  {
    "text": "idea of the numbers ghost 16 full disk archive is 20 terabytes at 640 P it's",
    "start": "951139",
    "end": "956930"
  },
  {
    "text": "about 540 Meg's for that video all the way out to kind of the other end at 3 K",
    "start": "956930",
    "end": "962000"
  },
  {
    "text": "4 K ish depending on how you define that it's about 12 gigs one of the features",
    "start": "962000",
    "end": "967550"
  },
  {
    "text": "of HLS video streaming and - as well is you can make these available and based on bandwidth usage it'll automatically",
    "start": "967550",
    "end": "974269"
  },
  {
    "text": "fall to different resolutions and so we can move through and kind of get that user experience the way we wanted it we're doing it with HLS again you can",
    "start": "974269",
    "end": "981560"
  },
  {
    "text": "compliant you can do all this with - all through the managed service alright so",
    "start": "981560",
    "end": "987440"
  },
  {
    "start": "985000",
    "end": "995000"
  },
  {
    "text": "then we move to the staging piece so now we have an application we can find some data of interest we can pick an event we can mark it and now we got to make this",
    "start": "987440",
    "end": "994220"
  },
  {
    "text": "data available for users so when you click that staging button we actually create two buckets we create a public",
    "start": "994220",
    "end": "999980"
  },
  {
    "start": "995000",
    "end": "1053000"
  },
  {
    "text": "access bucket and in there we drop the index that HTML file we drop a copy of the video again we use the elastic",
    "start": "999980",
    "end": "1006070"
  },
  {
    "text": "transcoder service to slice out that little clip and make it available we drop in and metadata file we can drop in",
    "start": "1006070",
    "end": "1012220"
  },
  {
    "text": "a jupiter notebook and i'll show you an example of one and then we create a second bucket and this is the in region access bucket this is where the actual",
    "start": "1012220",
    "end": "1018699"
  },
  {
    "text": "data goes this is the raw stuff I get thumbnails are also in the public one the the in region one is where the real",
    "start": "1018699",
    "end": "1025630"
  },
  {
    "text": "data goes this is going to be terabytes of data potentially depending on what you've selected the reason we do in region access is again from a cost",
    "start": "1025630",
    "end": "1031928"
  },
  {
    "text": "perspective we don't want to be a position where we're creating a bucket and somebody's egressing it back out so",
    "start": "1031929",
    "end": "1036970"
  },
  {
    "text": "we're paying for storage so we don't want to pay for like somebody puts up a link to this and now we're paying for distribution so we spin up a separate",
    "start": "1036970",
    "end": "1042610"
  },
  {
    "text": "bucket we put access controls on it so you can do in region access which is free there's no cost for data movement there but you can get to it do whatever",
    "start": "1042610",
    "end": "1048580"
  },
  {
    "text": "you want only if you want to serve it out that's your prerogative but it's not something that we're going to deal with so the way this works is similar you",
    "start": "1048580",
    "end": "1057220"
  },
  {
    "text": "click the button we use AWS step functions to orchestrate this and I'll show you that in a second but this is",
    "start": "1057220",
    "end": "1062470"
  },
  {
    "text": "another one of those kind of embarrassing lis parallel problems we have these underlying frames we have this I know and although we need to do some",
    "start": "1062470",
    "end": "1068080"
  },
  {
    "text": "processing on it and we can do it in in parallel so what we have is a step function that walks through this as soon",
    "start": "1068080",
    "end": "1075040"
  },
  {
    "start": "1071000",
    "end": "1129000"
  },
  {
    "text": "as the step function kicks off at Forks first thing we do is we generate the bit video clip that's done in parallel it's",
    "start": "1075040",
    "end": "1080080"
  },
  {
    "text": "done through elastic transcoder service dropped in the bucket once some content exists in that bucket the web page is",
    "start": "1080080",
    "end": "1086320"
  },
  {
    "text": "visible we can start providing you some basic information and in the background what we'll do is then start divvying up",
    "start": "1086320",
    "end": "1091990"
  },
  {
    "text": "this processing as much as we kind of want to scale it horizontally so we'll start partitioning the key space if",
    "start": "1091990",
    "end": "1097570"
  },
  {
    "text": "you're gonna pull in thousand frames ten thousand frames we'll divide those up into chunks and then farm that out using",
    "start": "1097570",
    "end": "1104230"
  },
  {
    "text": "batch across a spot instant or a cluster of spot instances this is another one of",
    "start": "1104230",
    "end": "1109810"
  },
  {
    "text": "those kind of scalable as people are willing to pay for the more we can blow",
    "start": "1109810",
    "end": "1116260"
  },
  {
    "text": "it out horizontally we can do in theory you could go all the way to a frame per instance if you wanted to the cost would",
    "start": "1116260",
    "end": "1121540"
  },
  {
    "text": "be crazy but you can do it once this happens the data gets pushed into a common archive we can clean up the data",
    "start": "1121540",
    "end": "1127300"
  },
  {
    "text": "and we can send out a notification email alright a little tangent so wise are for",
    "start": "1127300",
    "end": "1134380"
  },
  {
    "start": "1129000",
    "end": "1214000"
  },
  {
    "text": "our output format one is we aren't trying to play favorites here there are",
    "start": "1134380",
    "end": "1140470"
  },
  {
    "text": "other options the source data for gos is in netcdf so if you want tiled access to netcdf you can use it it's there",
    "start": "1140470",
    "end": "1147490"
  },
  {
    "text": "what's our gets us is kind of this open format for n dimensional arrays and chunking in n dimensions so we can chunk",
    "start": "1147490",
    "end": "1155140"
  },
  {
    "text": "accordingly it's I'm not saying czar is necessarily the right solution it is a",
    "start": "1155140",
    "end": "1161350"
  },
  {
    "text": "solution and it's one we're using one of the big pluses it gets us as it's fully parallelized read and write so again",
    "start": "1161350",
    "end": "1167890"
  },
  {
    "text": "that horizontal scalability we can begin dumping data into this concurrently across as much as we want to scale out",
    "start": "1167890",
    "end": "1173700"
  },
  {
    "text": "there's also no infrastructure pieces sitting in front of this there's no kind of machines we have to route this data through or API is kind of at from that",
    "start": "1173700",
    "end": "1180610"
  },
  {
    "text": "perspective we can write directly to it using the libraries and then it's compatible with pan geo if you've never",
    "start": "1180610",
    "end": "1186190"
  },
  {
    "text": "used pan geo pangaea's can an open source stack for kind of debt for earth science data analytics and data processing I have an asterisk there",
    "start": "1186190",
    "end": "1193450"
  },
  {
    "text": "because it's kind of compatible with pan geo right now pan geo has a limitation that it can only",
    "start": "1193450",
    "end": "1198659"
  },
  {
    "text": "seed czar files created by x-ray its own kind of the pie that you're familiar",
    "start": "1198659",
    "end": "1204330"
  },
  {
    "text": "with it Python x-ray it can only read or a data that it had written so we actually have to treat that as kind of a",
    "start": "1204330",
    "end": "1209700"
  },
  {
    "text": "separate format we can either out put it in a format that's support go by hip angio or we can output it the format this that we can use within that archive",
    "start": "1209700",
    "end": "1217320"
  },
  {
    "start": "1214000",
    "end": "1239000"
  },
  {
    "text": "now what you have is this this is our file this archive of the data you want",
    "start": "1217320",
    "end": "1223499"
  },
  {
    "text": "it we've organized it by groups we have it by bands within those groups each data sets are represented and it can be",
    "start": "1223499",
    "end": "1229649"
  },
  {
    "text": "heterogeneous if we wanted it to you we can embed metadata we can embed information about the bands the wavelengths and everything else all of",
    "start": "1229649",
    "end": "1234809"
  },
  {
    "text": "that's in there as attributes within the data and you can start to hit it with your own tool so I'm going to switch",
    "start": "1234809",
    "end": "1240479"
  },
  {
    "start": "1239000",
    "end": "1241000"
  },
  {
    "text": "over here and just give you kind of a sense that I'm not making this up so here's refresh this",
    "start": "1240479",
    "end": "1247309"
  },
  {
    "start": "1246000",
    "end": "1305000"
  },
  {
    "text": "so here's hitting this thing live right now this is streaming the goes archive you",
    "start": "1247309",
    "end": "1254369"
  },
  {
    "text": "can see down at the bottom kind of the prefetching that's happening on the little progress bar I can arbitrarily",
    "start": "1254369",
    "end": "1261029"
  },
  {
    "text": "jump to some location or some time in the archive and now we're there down at",
    "start": "1261029",
    "end": "1268739"
  },
  {
    "text": "the bottom we have a date/time stamp again that's being reversed out of kind of the time position seek position within that data right now we're using",
    "start": "1268739",
    "end": "1275279"
  },
  {
    "text": "the NOAA hurricane feed but there's other ones we can pull from to get some basic information so I can hit Maria and",
    "start": "1275279",
    "end": "1282029"
  },
  {
    "text": "what will happen is this is going to fetch the Maria start/end information and then jump to that time loop and just",
    "start": "1282029",
    "end": "1290039"
  },
  {
    "text": "loop that from kind of the beginning of Hurricane Maria to the end and so you can kind of see Hurricane Maria making",
    "start": "1290039",
    "end": "1295349"
  },
  {
    "text": "its way over Puerto Rico there and that okay that full animation is there you",
    "start": "1295349",
    "end": "1300509"
  },
  {
    "text": "can tweak that we can tune that if we wanted to but what I'll do is I'm gonna go ahead and hit stage so with staging",
    "start": "1300509",
    "end": "1306629"
  },
  {
    "start": "1305000",
    "end": "1338000"
  },
  {
    "text": "it now I can go ahead and pick the bands that I'm interested in I can do multiple bands again ideally what we'd have here",
    "start": "1306629",
    "end": "1312779"
  },
  {
    "text": "is multiple products I'm only showing goes but we can do like low level water vapor our output format either czar or",
    "start": "1312779",
    "end": "1319710"
  },
  {
    "text": "x-rays are",
    "start": "1319710",
    "end": "1322518"
  },
  {
    "text": "and stage that that's going to kick off that background process that I mentioned",
    "start": "1326860",
    "end": "1332420"
  },
  {
    "text": "and start putting that together it gives me a URL of where my data is going to be available this is that public bucket this is that first s3 bucket we spun",
    "start": "1332420",
    "end": "1339170"
  },
  {
    "start": "1338000",
    "end": "1355000"
  },
  {
    "text": "that up if I hit that right now it's just staging data it's a placeholder thing until that bucket gets put",
    "start": "1339170",
    "end": "1344570"
  },
  {
    "text": "together the video clips in there I'm gonna refresh this and hopefully it will be fast enough that I can show you this",
    "start": "1344570",
    "end": "1351820"
  },
  {
    "text": "I'll come back all right so in here this is the basically your archive that we've",
    "start": "1354760",
    "end": "1361550"
  },
  {
    "start": "1355000",
    "end": "1385000"
  },
  {
    "text": "just created this is that archive of convenience there's the video loop that it's been subset out of the overall goes",
    "start": "1361550",
    "end": "1368240"
  },
  {
    "text": "archive and again this is goes data we can apply this to whatever it is you're looking at we have the bands you",
    "start": "1368240",
    "end": "1374270"
  },
  {
    "text": "selected the output formats and basic information the links to it we have the actuals our s3 bucket information that",
    "start": "1374270",
    "end": "1380210"
  },
  {
    "text": "we've made available so I'm gonna switch over to here so this is an example",
    "start": "1380210",
    "end": "1388520"
  },
  {
    "start": "1385000",
    "end": "1493000"
  },
  {
    "text": "Jupiter notebook dropped in alright so now what I'm going to do is I'm actually gonna come down and hit run through this",
    "start": "1388520",
    "end": "1394880"
  },
  {
    "text": "and so what we can do is look at this archive this is a small archive here I've only put one day's worth of data in",
    "start": "1394880",
    "end": "1400640"
  },
  {
    "text": "it but we can go through and we can say ok what information is available in this archive right again we've staged this",
    "start": "1400640",
    "end": "1406490"
  },
  {
    "text": "data this is a subset of the overall information zarg is the ability to kind of represent it like this what you're",
    "start": "1406490",
    "end": "1411770"
  },
  {
    "text": "seeing here are the individual bands 5k 54 24 by 54:24 there's 16-bit integers",
    "start": "1411770",
    "end": "1417980"
  },
  {
    "text": "that data has now been staged and is ready to be it's accessible we can pull",
    "start": "1417980",
    "end": "1423200"
  },
  {
    "text": "out some metadata information out of this and all the way down to things like what's the wavelength from here now",
    "start": "1423200",
    "end": "1432590"
  },
  {
    "text": "we're just interacting with the data in kind of the way that we want to interact with that data we can take it and we can",
    "start": "1432590",
    "end": "1438440"
  },
  {
    "text": "render different bands it's water vapor I'm gonna go ahead and do this one right this is using a different bandpass or a",
    "start": "1438440",
    "end": "1446030"
  },
  {
    "text": "different color spectrum that we're using to render this I'm doing this in Jupiter notebooks but you could do this",
    "start": "1446030",
    "end": "1451850"
  },
  {
    "text": "in kind of whatever it is your your tool of choice would be I'm doing this against a local bazaar",
    "start": "1451850",
    "end": "1458960"
  },
  {
    "text": "archive I wasn't sure of network connectivity but the whole thing is the same hitting the actual s3 bucket just",
    "start": "1458960",
    "end": "1464480"
  },
  {
    "text": "by pointing to a different s3 route and then lastly if I take a an x-ray",
    "start": "1464480",
    "end": "1470419"
  },
  {
    "text": "compatible one you can see it's just the extent of metadata that was pulled out of the original goes data that's",
    "start": "1470419",
    "end": "1477169"
  },
  {
    "text": "embedded in there and it's available so this is where you could take this archive plug it right into something",
    "start": "1477169",
    "end": "1482270"
  },
  {
    "text": "like Pangaea leveraged ask and now you can do kind of parallel execution parallel computation against this as you",
    "start": "1482270",
    "end": "1489440"
  },
  {
    "text": "go forward all right",
    "start": "1489440",
    "end": "1492278"
  },
  {
    "start": "1493000",
    "end": "1589000"
  },
  {
    "text": "so so we put this together we learned a lot there are some serious bottlenecks",
    "start": "1495330",
    "end": "1501989"
  },
  {
    "text": "with this approach this is a breakdown of kind of the time we spent doing this or not the time we spent doing the time",
    "start": "1501989",
    "end": "1508769"
  },
  {
    "text": "it spends processing so the biggest hunk of this time is spent moving data around",
    "start": "1508769",
    "end": "1514860"
  },
  {
    "text": "this is a real issue data movement within AWS is the biggest chunk of time",
    "start": "1514860",
    "end": "1522330"
  },
  {
    "text": "spent in here 30% to kind of read off of the original data and then about 55% of",
    "start": "1522330",
    "end": "1527399"
  },
  {
    "text": "the time was been restaging that into an s3 bucket on the way out only about 15 percent of our time was actually spent",
    "start": "1527399",
    "end": "1533730"
  },
  {
    "text": "doing processing this is for the archive generation not for the video generation",
    "start": "1533730",
    "end": "1539269"
  },
  {
    "text": "the kind of related to this is this idea of auto scaling our batch processing the",
    "start": "1539269",
    "end": "1545639"
  },
  {
    "text": "idea would be that we have this batch processing capability as people kick off jobs as they spin these things up it",
    "start": "1545639",
    "end": "1550769"
  },
  {
    "text": "scales out horizontally and we can start doing some of that processing under there they the spot instance that",
    "start": "1550769",
    "end": "1557879"
  },
  {
    "text": "cluster spin up time is really really painful and so we ended up having to switch to a basically a minimum sized",
    "start": "1557879",
    "end": "1563730"
  },
  {
    "text": "auto scaling group that we can kind of adjust the volume on a little bit to kind of deal with roughly how long we want this to execute lots of different",
    "start": "1563730",
    "end": "1571019"
  },
  {
    "text": "options we could take here there's chunk sizing and compression that you can tune within as our archive we can take",
    "start": "1571019",
    "end": "1576539"
  },
  {
    "text": "advantage of those capabilities depending on your format we can actually avoid this entirely and feed it into something like a Kinesis stream depending what you want to do if there's",
    "start": "1576539",
    "end": "1582600"
  },
  {
    "text": "lots of optimization opportunities in there and then this idea of kind of local caching of hot files this idea of hitting local data and then ultimately",
    "start": "1582600",
    "end": "1588840"
  },
  {
    "text": "smarter archive creation which gets there but really this is a question of kind of okay this is height I don't mean",
    "start": "1588840",
    "end": "1594600"
  },
  {
    "start": "1589000",
    "end": "1793000"
  },
  {
    "text": "to put up a lot of bullet points but this one was just kinda like aha we can do this and we could do this and we can do this the philosophical idea here is",
    "start": "1594600",
    "end": "1601129"
  },
  {
    "text": "stop trying to solve kind of the general availability of the data like okay we're gonna put it in cogs cloud optimized geo",
    "start": "1601129",
    "end": "1607739"
  },
  {
    "text": "tips and then all the sudden everything's magical it really needs to be there's so many valid use cases",
    "start": "1607739",
    "end": "1612899"
  },
  {
    "text": "whether you're doing machine learning or whether you're doing statistical analysis or whether you're doing time series over long archives there are a",
    "start": "1612899",
    "end": "1619409"
  },
  {
    "text": "variety of needs all that vary and how you want to get at that data varies and",
    "start": "1619409",
    "end": "1624450"
  },
  {
    "text": "so fundamentally what we want to do is go at that elasticity of storage and be able to Bend this up and compute it and dump it",
    "start": "1624450",
    "end": "1630360"
  },
  {
    "text": "out there and so what we want to start doing is looking at okay additional products in providing data bundles we",
    "start": "1630360",
    "end": "1635400"
  },
  {
    "text": "want to look at additional output formats and what else we can use what else is useful for the community and put out there optimizing the the actual",
    "start": "1635400",
    "end": "1642840"
  },
  {
    "text": "build time so local caching horizontal scaling czar tuning time-lapse video",
    "start": "1642840",
    "end": "1648630"
  },
  {
    "text": "generation if you wanted to do a time series analysis we could show you the loop over Maria but if you're looking at",
    "start": "1648630",
    "end": "1653880"
  },
  {
    "text": "something longer if you're looking at land change or land use information we may have to show this over years we have",
    "start": "1653880",
    "end": "1659760"
  },
  {
    "text": "access to that data that's no problem but we need to be able to render things that actually help make that understandable and comprehendible",
    "start": "1659760",
    "end": "1666080"
  },
  {
    "text": "additional bands or video scrubbing so I'm scrubbing through false-color so you can see what that imagery looks like but",
    "start": "1666080",
    "end": "1672240"
  },
  {
    "text": "let's scrub through something else let's look at scrub through far fire anomalies or let's scrub through water vapor but",
    "start": "1672240",
    "end": "1678990"
  },
  {
    "text": "to be able to give me the idea of like let me see the data I want to see it and let me see the loops associated with it",
    "start": "1678990",
    "end": "1684020"
  },
  {
    "text": "once we move into the video space and kind of the ability to do that well now we can start looking at GPU based video",
    "start": "1684020",
    "end": "1689910"
  },
  {
    "text": "filters we can start taking advantage of kind of all the technology space associated with 4k video processing",
    "start": "1689910",
    "end": "1697140"
  },
  {
    "text": "let's start applying those in the frame generation we can expand that out with",
    "start": "1697140",
    "end": "1702840"
  },
  {
    "text": "kind of Python modules that you can drop in there Python code you can drop in there to be able to do compute on frame",
    "start": "1702840",
    "end": "1708780"
  },
  {
    "text": "generation so you've got your source video you're looking to transfer it into an archive of convenience let me not let me do more than just kind of",
    "start": "1708780",
    "end": "1713940"
  },
  {
    "text": "transformation let me do reprojection let me do some basic analysis you could",
    "start": "1713940",
    "end": "1719250"
  },
  {
    "text": "potentially do model training machine learning model training off of this but I think you'd almost want to go the other way where you could actually do",
    "start": "1719250",
    "end": "1724350"
  },
  {
    "text": "model execution let me take these individual frames as I'm producing them into the archive I'm gonna run it",
    "start": "1724350",
    "end": "1729360"
  },
  {
    "text": "through for object detection or whatever I've trained my monologue once you get to that point now we can start talking",
    "start": "1729360",
    "end": "1735720"
  },
  {
    "text": "about things like overlays and annotations I know where the hurricane is I know the hurricane path",
    "start": "1735720",
    "end": "1741030"
  },
  {
    "text": "let me annotate that let me overlay that let me overlay that with additional information let me do object-- detection",
    "start": "1741030",
    "end": "1746670"
  },
  {
    "text": "and generate an overlay image that I can now do compositing when I play that back or when I want to look at that again it depends on your use case but there are",
    "start": "1746670",
    "end": "1752880"
  },
  {
    "text": "just a ton of opportunities once you kind of take this idea of there's something between I've dumped petabytes",
    "start": "1752880",
    "end": "1759060"
  },
  {
    "text": "of data in the cloud and I'm an end-user trying to figure out deforestation like",
    "start": "1759060",
    "end": "1764280"
  },
  {
    "text": "there's this whole chunk that we tend to skip over as a community of like how do",
    "start": "1764280",
    "end": "1769410"
  },
  {
    "text": "I map how do I connect those two there's a huge gap there and so we're trying to do is get at that subframe rendering so",
    "start": "1769410",
    "end": "1776040"
  },
  {
    "text": "this is kind of pulling out small pieces instead of doing 5k pieces common projections for heterogeneous products",
    "start": "1776040",
    "end": "1781440"
  },
  {
    "text": "let me pick products I don't know how to get these two together let me take a data recipe that's blessed by NASA this",
    "start": "1781440",
    "end": "1786840"
  },
  {
    "text": "is the right way to reproject this data I'm gonna we can provide that implementation and be able to do this reprojection so these two products are",
    "start": "1786840",
    "end": "1792300"
  },
  {
    "text": "usable together so kind of in summary here we leveraged AWS and the spot",
    "start": "1792300",
    "end": "1800070"
  },
  {
    "start": "1793000",
    "end": "1854000"
  },
  {
    "text": "market ECS elastic transcoder service to take what was 20 terabytes of data and",
    "start": "1800070",
    "end": "1805110"
  },
  {
    "text": "make this visible and scrub a ball and accessible from an iPad and iPhone all",
    "start": "1805110",
    "end": "1811320"
  },
  {
    "text": "the way up to a machine to a you know your full machine into by several orders",
    "start": "1811320",
    "end": "1816750"
  },
  {
    "text": "of magnitude reducing that overall data set you're not doing science against the video necessarily but you're able to get at the data that way and depending on",
    "start": "1816750",
    "end": "1822960"
  },
  {
    "text": "your use case that might be all that you need this works across lots of different products AWS in batch parallel wise is",
    "start": "1822960",
    "end": "1829470"
  },
  {
    "text": "this it makes it again a kind of almost embarrassingly parallel to get there and scalable and then what we get is this",
    "start": "1829470",
    "end": "1836970"
  },
  {
    "text": "kind of highly elastic access to the data targeted at use cases we have trigger set up on these buckets to clean",
    "start": "1836970",
    "end": "1843660"
  },
  {
    "text": "them up after a week that's all obviously tunable but again this is all kind of leveraging that elasticity of storage and then ultimately other than",
    "start": "1843660",
    "end": "1850350"
  },
  {
    "text": "the actual data that we have sitting in a bucket this all goes back to zero when nobody's using it so kind of from a call",
    "start": "1850350",
    "end": "1857190"
  },
  {
    "start": "1854000",
    "end": "1922000"
  },
  {
    "text": "for participation or a call from help perspective I sincerely want to hear from people if it's if it's not nice",
    "start": "1857190",
    "end": "1864720"
  },
  {
    "text": "then wait till after the question session but I do sincerely want to hear feedback of do you have data that you",
    "start": "1864720",
    "end": "1871650"
  },
  {
    "text": "want to make available what formats do you make it available in now what formats would you like it to be in what",
    "start": "1871650",
    "end": "1877440"
  },
  {
    "text": "information out of that data do you want to know how do you find it how do you subset it what other ways can we potentially make some of this available",
    "start": "1877440",
    "end": "1883740"
  },
  {
    "text": "to you and then if you're in the science community help us not break this data you know algorithms that are established",
    "start": "1883740",
    "end": "1891440"
  },
  {
    "text": "blessed we peer reviewed anything in that space that would be",
    "start": "1891440",
    "end": "1897760"
  },
  {
    "text": "fantastic again we don't want to do anything that kind of disturbs the underlying data but really feedback here",
    "start": "1897760",
    "end": "1904660"
  },
  {
    "text": "this is this is you know kind of putting out there here's the concept we've got a while before we have this kind of utopia",
    "start": "1904660",
    "end": "1911830"
  },
  {
    "text": "of all the data sitting there for everybody to use and exactly the format you want and so this is something to bridge that gap between where we are now",
    "start": "1911830",
    "end": "1917950"
  },
  {
    "text": "and getting this data out to people who do not have the infrastructure or the capabilities to put it together so I think this is just a beautiful video so",
    "start": "1917950",
    "end": "1924520"
  },
  {
    "start": "1922000",
    "end": "1928000"
  },
  {
    "text": "I put it in twice and then with that I will open it up for questions oh this is",
    "start": "1924520",
    "end": "1929770"
  },
  {
    "start": "1928000",
    "end": "1975000"
  },
  {
    "text": "out there this is live it is absolutely the version that's out there is just",
    "start": "1929770",
    "end": "1936520"
  },
  {
    "text": "exposing gos data the bucket will be deleted after a short period of time",
    "start": "1936520",
    "end": "1941740"
  },
  {
    "text": "five days I think is what we said it's the bucket will be cleaned up egress should be blocked off but please",
    "start": "1941740",
    "end": "1949120"
  },
  {
    "text": "be gentle but this link is available right and with that I will take any",
    "start": "1949120",
    "end": "1955810"
  },
  {
    "text": "questions [Applause]",
    "start": "1955810",
    "end": "1965380"
  },
  {
    "text": "yeah",
    "start": "1967679",
    "end": "1970679"
  },
  {
    "start": "1975000",
    "end": "2126000"
  },
  {
    "text": "yeah so the the chunking so the two-second the question repeat the question was if two seconds is the wrong",
    "start": "1975070",
    "end": "1981580"
  },
  {
    "text": "chunk size how do we regenerate or reach chunk that data so the chunking size",
    "start": "1981580",
    "end": "1987280"
  },
  {
    "text": "there at the two-second chunk piece is is on the video generation and so that's",
    "start": "1987280",
    "end": "1993010"
  },
  {
    "text": "actually generated off of the master video which is in the gigabytes range Meg's two gigs after we've done the",
    "start": "1993010",
    "end": "1999070"
  },
  {
    "text": "original video assembly and division video encoding generally from a streaming perspective to seconds is",
    "start": "1999070",
    "end": "2004920"
  },
  {
    "text": "about right it's small enough that you can jump kind of anywhere but large enough that usually are even reasonable",
    "start": "2004920",
    "end": "2011040"
  },
  {
    "text": "bandwidth at a low resolution you can keep up with playback the the trickier",
    "start": "2011040",
    "end": "2016920"
  },
  {
    "text": "chunking is in the archive generation we're actually talk about this this",
    "start": "2016920",
    "end": "2022470"
  },
  {
    "text": "morning at a roundtable we talked a lot about cloud optimized UTS and the idea",
    "start": "2022470",
    "end": "2028320"
  },
  {
    "text": "that they provide tiling within data if your area of interest is sufficiently",
    "start": "2028320",
    "end": "2034560"
  },
  {
    "text": "large you're going to need not just the entire cog the cloud optimized geo TIFF you might need multiple ones which means",
    "start": "2034560",
    "end": "2040530"
  },
  {
    "text": "that internal chunking of those cogs really doesn't get you anything or if there's just kind of this arbitrary chunking that's done inside of the COG",
    "start": "2040530",
    "end": "2045540"
  },
  {
    "text": "if you need multiple say Landsat tiles these you have to stitch these things together you don't get any advantage to",
    "start": "2045540",
    "end": "2050760"
  },
  {
    "text": "that the flip is true too if you need a really small area and your tile size is large well the chunking may not actually",
    "start": "2050760",
    "end": "2056550"
  },
  {
    "text": "help you all that much because you're you're so far down inside of that so this processing step again that",
    "start": "2056550",
    "end": "2061919"
  },
  {
    "text": "intermediate step going at how do we get at your use case gives us the ability to tune that based on potentially the area",
    "start": "2061919",
    "end": "2068550"
  },
  {
    "text": "of interest and the data resolution that you're using so if you're using a 250",
    "start": "2068550",
    "end": "2073919"
  },
  {
    "text": "meter scene then and you're looking at a",
    "start": "2073919",
    "end": "2079320"
  },
  {
    "text": "very tiny place well we can partition appropriately if you're using if you have very high resolution data using",
    "start": "2079320",
    "end": "2085740"
  },
  {
    "text": "centimeter resolution data and you're looking at the city we can potentially collapse these are the bigger chunks to make it a little more accessible and",
    "start": "2085740",
    "end": "2090899"
  },
  {
    "text": "you're not doing lots of repeated requests against s3 but that's done during processing",
    "start": "2090900",
    "end": "2096440"
  },
  {
    "text": "yeah you mentioned at one point that you",
    "start": "2097270",
    "end": "2102520"
  },
  {
    "text": "were making a financial decision I believed about the processing based upon",
    "start": "2102520",
    "end": "2107860"
  },
  {
    "text": "the data that you are working on do you don't have to go into details but could you talk a little bit more about that",
    "start": "2107860",
    "end": "2114610"
  },
  {
    "text": "process that perhaps you guys are using the process to make the financial",
    "start": "2114610",
    "end": "2120340"
  },
  {
    "text": "decision or yeah yeah so it's it really",
    "start": "2120340",
    "end": "2129940"
  },
  {
    "text": "comes down to kind of how much do you paralyze it how big is the cluster and what are the Machine sizes that are in",
    "start": "2129940",
    "end": "2136540"
  },
  {
    "text": "there there's a couple different factors that come into it so when we talk about",
    "start": "2136540",
    "end": "2142230"
  },
  {
    "text": "that graph I showed the pie chart showing where our time's going you could use something like the elastic container",
    "start": "2142230",
    "end": "2148060"
  },
  {
    "text": "service and throw these into kind of big nodes but ultimately you're moving through the network interface on that",
    "start": "2148060",
    "end": "2154090"
  },
  {
    "text": "node and so you have to really metric it to figure out where where is it the actual bottleneck like we're throwing",
    "start": "2154090",
    "end": "2160300"
  },
  {
    "text": "more nodes at this go speed things up or would size make a difference ultimately kind of what the driver for us is how",
    "start": "2160300",
    "end": "2167050"
  },
  {
    "text": "long can we have people wait for data and so if it's you know we had some",
    "start": "2167050",
    "end": "2172390"
  },
  {
    "text": "discussions around you know once we cross five minutes to create the bucket five minutes versus four minutes it",
    "start": "2172390",
    "end": "2179350"
  },
  {
    "text": "doesn't really make a lot of difference now if we're talking five minutes to come back tomorrow it's a different",
    "start": "2179350",
    "end": "2184720"
  },
  {
    "text": "discussion and obviously that also depends on how much data is being requested so kind of the knobs we have",
    "start": "2184720",
    "end": "2189880"
  },
  {
    "text": "in turn is when we spin down to a some",
    "start": "2189880",
    "end": "2196180"
  },
  {
    "text": "of the guys here who worked on this can tell me Andrew it's five dollars an hour for our smaller cluster is that right that's our bigger clusters about five",
    "start": "2196180",
    "end": "2202660"
  },
  {
    "text": "dollars an hour to keep that cluster that's about 20 nodes it's about five dollars an hour to keep those hot but",
    "start": "2202660",
    "end": "2210150"
  },
  {
    "text": "again if it's Network throttled having bigger instances is it gonna do any difference for us it's not gonna help us",
    "start": "2210150",
    "end": "2216090"
  },
  {
    "text": "and then if it's actually data fetch well we could actually go out that by caching some of this unlucky BS or",
    "start": "2216090",
    "end": "2221680"
  },
  {
    "text": "something like less o'clock storage and putting some data local to that node which basically makes that 30% time just",
    "start": "2221680",
    "end": "2226720"
  },
  {
    "text": "disappear and fall off so then we have to do the cost we locally cache this data versus how",
    "start": "2226720",
    "end": "2232819"
  },
  {
    "text": "much does that speed up the delivery so it really I mean it bottom line to your question is how long are people going to wait for their data and they trade that",
    "start": "2232819",
    "end": "2239180"
  },
  {
    "text": "against costs and then looking from the cost model local caching Network transfers processing time and then kind",
    "start": "2239180",
    "end": "2246170"
  },
  {
    "text": "of auto scaling group time if it's under heavy load we can let it auto scale up and then just spin back down but it's too slow to do that for spike load were",
    "start": "2246170",
    "end": "2260059"
  },
  {
    "text": "you able to come up with a heuristics to define the forcing function to win to",
    "start": "2260059",
    "end": "2265190"
  },
  {
    "text": "horizontally scale a generic function no",
    "start": "2265190",
    "end": "2270230"
  },
  {
    "text": "no that's the one we're trying to solve yeah no I I don't have a good answer for that Andrew I'll put you on the spot if",
    "start": "2270230",
    "end": "2277640"
  },
  {
    "text": "you have anything you wanted to if there's any kind of heuristic that you would implement here but",
    "start": "2277640",
    "end": "2283869"
  },
  {
    "text": "thanks I got the microphone to give a long answer that is no we didn't",
    "start": "2290530",
    "end": "2297520"
  },
  {
    "text": "optimize this to level where were comfortable with our scaling policy so",
    "start": "2297520",
    "end": "2302890"
  },
  {
    "text": "our scaling is really just our partition size two nodes and we're saying the",
    "start": "2302890",
    "end": "2307990"
  },
  {
    "text": "smaller the partitions per process the happier we are but there are some very obvious problems with that the",
    "start": "2307990",
    "end": "2314170"
  },
  {
    "text": "bottlenecks being one of those and you can get larger instances that do have better network interfaces but the value",
    "start": "2314170",
    "end": "2321820"
  },
  {
    "text": "at that point you have to consider because they're a bit more expensive than the smaller slower but cheaper ones",
    "start": "2321820",
    "end": "2329220"
  },
  {
    "text": "that's a really unsatisfying answer I'm sorry but that's where we are yeah I'm",
    "start": "2329220",
    "end": "2334570"
  },
  {
    "text": "sorry these are C files right thank you",
    "start": "2334570",
    "end": "2344170"
  },
  {
    "text": "so for the video building signed sighs those are c5 they just came out and made",
    "start": "2344170",
    "end": "2350170"
  },
  {
    "text": "the c5 D class 4x larges for these our generation because it's really not that",
    "start": "2350170",
    "end": "2357120"
  },
  {
    "text": "compute intensive it's really just deep copies and some small transformations from the netcdf into the Czar's those we",
    "start": "2357120",
    "end": "2364600"
  },
  {
    "text": "just used em five larges and we scaled up to a cluster of 60-80 yeah",
    "start": "2364600",
    "end": "2376830"
  },
  {
    "text": "the know the the Tsar supports kind of",
    "start": "2377220",
    "end": "2386410"
  },
  {
    "start": "2383000",
    "end": "2637000"
  },
  {
    "text": "data chunking based on chunk sizes and then that kind of also affects s three rights so the question is how do we do",
    "start": "2386410",
    "end": "2392860"
  },
  {
    "text": "the data chunking within the archive in this bit so key partitioning is different key partitioning is in house",
    "start": "2392860",
    "end": "2398410"
  },
  {
    "text": "we kind of figure out how we want to parallelize this the actual right into the data is configurable when you create",
    "start": "2398410",
    "end": "2403630"
  },
  {
    "text": "those our archive you set chunk sizes and then I don't think I can actually if I could pull one up if I can I'm happy",
    "start": "2403630",
    "end": "2410410"
  },
  {
    "text": "to show you one it's real obvious kind of what's happening is it's creating basically this tree group underneath when you use the nested route so czar",
    "start": "2410410",
    "end": "2417160"
  },
  {
    "text": "supports pluggable backends and one of them is s3 there's also Google file system you can also do zip as your",
    "start": "2417160",
    "end": "2423190"
  },
  {
    "text": "back-end for the s3 one it has inherent support for key value mappings and so it",
    "start": "2423190",
    "end": "2429220"
  },
  {
    "text": "went based on that chunk size it will then create a hierarchy of keys and then blow it out across that the problem is",
    "start": "2429220",
    "end": "2435340"
  },
  {
    "text": "you can't change the chunk size once you've created the archive or once you've start once you open to the archive and so you really need to tune",
    "start": "2435340",
    "end": "2440860"
  },
  {
    "text": "that knowing what you're about to put into it and there's tons of optimizations could be done in there too based on the reading patterns not just",
    "start": "2440860",
    "end": "2447400"
  },
  {
    "text": "the writing patterns you could optimize for writing but then depending on what you're doing with read you may actually want a slightly different chunk size",
    "start": "2447400",
    "end": "2454170"
  },
  {
    "text": "yeah thanks really interesting Dan just a question so if I understand correctly",
    "start": "2458990",
    "end": "2464260"
  },
  {
    "text": "if I run this currently this is on your bill yeah I presume eventually you'd",
    "start": "2464260",
    "end": "2473870"
  },
  {
    "text": "want this to be you know paid by the customer so as a future user of some",
    "start": "2473870",
    "end": "2480170"
  },
  {
    "text": "service have you done any testing with being able to approximate like the cost overall cost of a job depending on what",
    "start": "2480170",
    "end": "2486470"
  },
  {
    "text": "the tuning is and some way to convey that to users yes and no yes we know",
    "start": "2486470",
    "end": "2491960"
  },
  {
    "text": "where we are now no it's not something I'd want to convey because I don't think we've got it right yet there's lots of",
    "start": "2491960",
    "end": "2497810"
  },
  {
    "text": "tuning and kind of cost optimizations right now kind of what we have is this",
    "start": "2497810",
    "end": "2503810"
  },
  {
    "text": "cluster available to do this processing but it's not necessarily it's not optimal for kind of it depends on your load it depends on the size of the data",
    "start": "2503810",
    "end": "2509960"
  },
  {
    "text": "it depends on how what does sustain load versus spike load looks like things along those lines right now this is",
    "start": "2509960",
    "end": "2518840"
  },
  {
    "text": "something that we're tuning and so I don't feel comfortable that the costs",
    "start": "2518840",
    "end": "2524060"
  },
  {
    "text": "we're seeing right now are representative of where we want to be okay but I guess in general like if I",
    "start": "2524060",
    "end": "2531740"
  },
  {
    "text": "were a future user of the service I would want to know say if I press this button absolutely",
    "start": "2531740",
    "end": "2536870"
  },
  {
    "text": "yes yes you know sets in limits so no question some other options there so like there's compute time that we're",
    "start": "2536870",
    "end": "2542510"
  },
  {
    "text": "paying for but right now we're also paying for storage costs there's no reason why you couldn't also give us a",
    "start": "2542510",
    "end": "2548630"
  },
  {
    "text": "bucket or we could create a bucket or you know delegated rolls or something to that effect where we could actually",
    "start": "2548630",
    "end": "2554270"
  },
  {
    "text": "generate directly into your space and then there's a compute cost but there's",
    "start": "2554270",
    "end": "2559850"
  },
  {
    "text": "no kind of ongoing data hosting cost or anything like that that would be part of it as well that's another model",
    "start": "2559850",
    "end": "2566079"
  },
  {
    "text": "yeah so it really depends on data right so the data size once the output",
    "start": "2571500",
    "end": "2576660"
  },
  {
    "text": "archived it depends on the length of the clip you've taken that's just Recker 8's 3 storage that's",
    "start": "2576660",
    "end": "2583530"
  },
  {
    "text": "just depends on the size of the data in there which is you know pennies on the gigs the compute size at our 20 node",
    "start": "2583530",
    "end": "2592109"
  },
  {
    "text": "cluster it's like five dollars an hour 60 new cluster yeah",
    "start": "2592109",
    "end": "2603559"
  },
  {
    "text": "so for that example so for like Murray no no no it's nowhere near that no from",
    "start": "2605079",
    "end": "2611440"
  },
  {
    "text": "so you know five dollars for our 60 node cluster five dollars an hour that's $60",
    "start": "2611440",
    "end": "2617329"
  },
  {
    "text": "for 12 hours that that Maria segment which is a couple days worth of data maybe it's a couple weeks I think is",
    "start": "2617329",
    "end": "2622670"
  },
  {
    "text": "like two weeks worth the day it's like September to October that puts it in the order of I think it's it's a couple",
    "start": "2622670",
    "end": "2629960"
  },
  {
    "text": "hours right to kind of stage that data yeah so tens of dollars yeah hey Dan I",
    "start": "2629960",
    "end": "2636890"
  },
  {
    "text": "had a quick question all the way in the back Oh gotcha so this is awesome and I think you copy audited that you know it's a",
    "start": "2636890",
    "end": "2643910"
  },
  {
    "start": "2637000",
    "end": "2728000"
  },
  {
    "text": "prototype in a demo all that a lot but it's obviously a working system in some context and it works on stage yeah so",
    "start": "2643910",
    "end": "2651319"
  },
  {
    "text": "would you be willing to share sort of a rough estimate of time that it's taken for you to get this far yeah sure so it",
    "start": "2651319",
    "end": "2659450"
  },
  {
    "text": "depends on pieces of it some of it obviously is kind of bringing the knowledge of getting at that data I",
    "start": "2659450",
    "end": "2665420"
  },
  {
    "text": "couldn't tell you that this kind of came out of some work that Andrew had started around Himawari data",
    "start": "2665420",
    "end": "2671750"
  },
  {
    "text": "and then we applied it to goes talking the audience for here the front end I",
    "start": "2671750",
    "end": "2678829"
  },
  {
    "text": "mean we're talking I don't know maybe a month to kind of put together give it a",
    "start": "2678829",
    "end": "2684890"
  },
  {
    "text": "little little three weeks to kind of give the GUI here that you need it for this stringing together the kind of the",
    "start": "2684890",
    "end": "2690829"
  },
  {
    "text": "the services in the processing a lot of that is kind of it's one of those once",
    "start": "2690829",
    "end": "2695990"
  },
  {
    "text": "you've done it for other systems so when we built things like this for you know processing NASA data or whatever the",
    "start": "2695990",
    "end": "2701750"
  },
  {
    "text": "case may be it's it's pretty directly applicable to repeat that so but it's not years worth of work by any stretch",
    "start": "2701750",
    "end": "2710140"
  },
  {
    "text": "okay thank you [Applause]",
    "start": "2712730",
    "end": "2720980"
  },
  {
    "text": "somebody can have the information and thank you for working hard to make this data usable and accessible so we can",
    "start": "2720980",
    "end": "2727140"
  },
  {
    "text": "better understand their so we're going to take a short break I asked everybody to come back at 5:00 of and our next",
    "start": "2727140",
    "end": "2735480"
  },
  {
    "start": "2728000",
    "end": "2922000"
  },
  {
    "text": "session is gonna be machine learning with Earth Observation imagery thank you all oh and don't forget to comment rate the",
    "start": "2735480",
    "end": "2742140"
  },
  {
    "text": "session on your app",
    "start": "2742140",
    "end": "2744890"
  }
]