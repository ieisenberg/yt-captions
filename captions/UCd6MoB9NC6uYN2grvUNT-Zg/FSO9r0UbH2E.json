[
  {
    "start": "0",
    "end": "27000"
  },
  {
    "text": "welcome everyone I'm Edward name I'm the GM of Amazon fsx and I'm here today with",
    "start": "30",
    "end": "6750"
  },
  {
    "text": "Darrel Osborne who will be on stage in a little bit to do some demos and show you fsx for lustre in in action we're really",
    "start": "6750",
    "end": "16020"
  },
  {
    "text": "excited to be here today to talk to you about Amazon fsx for lustre which is a new service that was announced yesterday",
    "start": "16020",
    "end": "22650"
  },
  {
    "text": "and launched yesterday during Andy Jesse's keynote and in the session I'm",
    "start": "22650",
    "end": "29369"
  },
  {
    "start": "27000",
    "end": "75000"
  },
  {
    "text": "gonna start with a very brief overview of AWS storage and the options available across our storage portfolio I'm then",
    "start": "29369",
    "end": "37500"
  },
  {
    "text": "going to talk to you about running compute intensive workloads on AWS because that's really the goal of fsx",
    "start": "37500",
    "end": "44129"
  },
  {
    "text": "for Luster's to support compute intensive workloads will then introduce fsx for lustre why we built it what it",
    "start": "44129",
    "end": "51149"
  },
  {
    "text": "offers and how it can help you run high-performance computing machine learning media processing and other",
    "start": "51149",
    "end": "57120"
  },
  {
    "text": "compute intensive workloads Darrell will then lead us through a part of the",
    "start": "57120",
    "end": "62579"
  },
  {
    "text": "session that will show fsx in action so he'll do some cool hands-on demos of",
    "start": "62579",
    "end": "67650"
  },
  {
    "text": "using fsx and he's going to demonstrate the performance that you can get from it and then we'll save time at the end for",
    "start": "67650",
    "end": "73140"
  },
  {
    "text": "Q&A so let's get into it and let's start with an overview of AWS as storage",
    "start": "73140",
    "end": "79409"
  },
  {
    "start": "75000",
    "end": "125000"
  },
  {
    "text": "offerings and the summary is we have a rich set of storage offerings to support",
    "start": "79409",
    "end": "85470"
  },
  {
    "text": "whatever workloads you want to run on AWS we have object in archival storage so s3 and s3 glacier we have locally",
    "start": "85470",
    "end": "94229"
  },
  {
    "text": "attached storage and that's ec2 instance local storage and in EBS and it's been",
    "start": "94229",
    "end": "99900"
  },
  {
    "text": "an exciting week for our file offerings we announced yesterday not only fsx for",
    "start": "99900",
    "end": "105509"
  },
  {
    "text": "lustre but also another fsx file system fsx for windows file server and we also",
    "start": "105509",
    "end": "111240"
  },
  {
    "text": "announced that EFS is getting a new infrequent access class of storage it's coming soon we also have a wide set of",
    "start": "111240",
    "end": "118320"
  },
  {
    "text": "data movement services for moving your data into AWS out of AWS and across services and most of you are here",
    "start": "118320",
    "end": "127950"
  },
  {
    "start": "125000",
    "end": "607000"
  },
  {
    "text": "because you're looking for storage to use with your compute intensive workloads so I want to talk a little bit",
    "start": "127950",
    "end": "133050"
  },
  {
    "text": "about running intensive workloads on AWS to tee up the discussion and so let's start with a",
    "start": "133050",
    "end": "140050"
  },
  {
    "text": "definition what do we mean by a compute intensive workload well it's any workload that processes",
    "start": "140050",
    "end": "146410"
  },
  {
    "text": "data at a rapid pace with lots of compute power so another way to say it is a workload that processes large",
    "start": "146410",
    "end": "153190"
  },
  {
    "text": "amounts of data with large amounts of compute and typically in these compute intensive workloads you'll have really",
    "start": "153190",
    "end": "159550"
  },
  {
    "text": "massive data sets and large clusters of compute and you want to use the compute",
    "start": "159550",
    "end": "165130"
  },
  {
    "text": "to process the data that's in the data sets and commonly you need to have high speed and network between the storage",
    "start": "165130",
    "end": "171730"
  },
  {
    "text": "and the compute and AWS provides a wide breadth of services for these types of",
    "start": "171730",
    "end": "178299"
  },
  {
    "text": "computer intensive workloads a bunch of compute services we have services like",
    "start": "178299",
    "end": "183610"
  },
  {
    "text": "easy to spot we have automation and orchestration services like AWS batch which has been",
    "start": "183610",
    "end": "189220"
  },
  {
    "text": "very popular with our HPC customers we have a number of networking services we actually announced EF a network",
    "start": "189220",
    "end": "195910"
  },
  {
    "text": "interface for compute instances this week we also announced a c5n compute",
    "start": "195910",
    "end": "202989"
  },
  {
    "text": "network optimized instance that gives you a hundred gigabit network and we",
    "start": "202989",
    "end": "209410"
  },
  {
    "text": "have a number of visualization services as well other customers are using to really do do more with their data and so",
    "start": "209410",
    "end": "218079"
  },
  {
    "text": "what does the typical compute intensive workload on AWS look like so commonly",
    "start": "218079",
    "end": "223720"
  },
  {
    "text": "what customers do is they store their data sets in s3 which provides highly",
    "start": "223720",
    "end": "229269"
  },
  {
    "text": "durable very cheap storage for large data sets fully elastic and then there's",
    "start": "229269",
    "end": "236319"
  },
  {
    "text": "two ways of making that data available to compute clusters so the first way is",
    "start": "236319",
    "end": "241690"
  },
  {
    "text": "that you copy data to the into the ec2 instance storage or to EBS volumes",
    "start": "241690",
    "end": "247810"
  },
  {
    "text": "attached to the ec2 instances that you're using in your compute cluster the",
    "start": "247810",
    "end": "254650"
  },
  {
    "text": "other way is you copy data into some sort of self-managed high-performance shared file system and then your compute",
    "start": "254650",
    "end": "261789"
  },
  {
    "text": "cluster has access to the data on that file system so those are the two common ways to work with your s3",
    "start": "261789",
    "end": "267490"
  },
  {
    "text": "data for compute workload and then once",
    "start": "267490",
    "end": "272620"
  },
  {
    "text": "your workloads complete you write results back to s3 and sometimes you might do that at intermediate points to",
    "start": "272620",
    "end": "279099"
  },
  {
    "text": "do some sort of check pointing and then when you're Daniel you'll delete the compute cluster and the local storage or",
    "start": "279099",
    "end": "285669"
  },
  {
    "text": "the shared file system so it's a very typical type of workflow now you'll",
    "start": "285669",
    "end": "293050"
  },
  {
    "text": "notice that I mentioned that for both options one an option two you're using a filesystem you might be wondering why why do you",
    "start": "293050",
    "end": "299319"
  },
  {
    "text": "need a filesystem and it's either a filesystem that is local to the EBS volumes or to the ec2 local storage or",
    "start": "299319",
    "end": "305949"
  },
  {
    "text": "it's a filesystem that's a shared file system well there's really three reasons the first is latencies so a lot of",
    "start": "305949",
    "end": "313000"
  },
  {
    "text": "compute intensive workloads are latency sensitive and so you'll want to have the data on a filesystem that's local or on",
    "start": "313000",
    "end": "319960"
  },
  {
    "text": "the local network close to your computer the second our request costs so for a",
    "start": "319960",
    "end": "325509"
  },
  {
    "text": "lot of these workloads if you're going back to s3 every time you want to read data a lot of times you're reading the",
    "start": "325509",
    "end": "330880"
  },
  {
    "text": "same data over and over again you're paying requests cost and so those quickly add up for workloads that are",
    "start": "330880",
    "end": "336520"
  },
  {
    "text": "processing a lot of data and a data set and then finally a lot of the applications that that people are",
    "start": "336520",
    "end": "342639"
  },
  {
    "text": "running to process their data require a file interface so they're written for filesystem API s operating system API is",
    "start": "342639",
    "end": "349509"
  },
  {
    "text": "that provider that provide file functionality so that's generally why",
    "start": "349509",
    "end": "354699"
  },
  {
    "text": "file systems involved in between your s3 and your computer and so option two",
    "start": "354699",
    "end": "361750"
  },
  {
    "text": "which I said is a shared file system is great because it allows you to expose your data to your instances as local",
    "start": "361750",
    "end": "367750"
  },
  {
    "text": "files but you don't need to shard the data across instances and volumes like you do in an option one it's just a",
    "start": "367750",
    "end": "373330"
  },
  {
    "text": "shared file system that all of your compute instances can connect to and access the same set of data but for a",
    "start": "373330",
    "end": "379270"
  },
  {
    "text": "shared file system to work for most of these workloads it needs to really provide two key things first as a POSIX",
    "start": "379270",
    "end": "385060"
  },
  {
    "text": "interface so files and directories will appear and work just as they would on local storage and all of the API is the",
    "start": "385060",
    "end": "391930"
  },
  {
    "text": "operating system provides to work with a file system will work with it so your applications will just work and it needs",
    "start": "391930",
    "end": "398770"
  },
  {
    "text": "to provide very high levels of throughput in AI ops while providing very low latencies and",
    "start": "398770",
    "end": "403870"
  },
  {
    "text": "the high levels of throughput in AI ops is because you have a lot of compute processing a lot of data and so you need",
    "start": "403870",
    "end": "410380"
  },
  {
    "text": "a lot of throughput a lot of AI ops at in aggregate and and most shared file",
    "start": "410380",
    "end": "418330"
  },
  {
    "text": "systems that are deployed for these compute intensive workloads are what we call parallel file systems and so with a",
    "start": "418330",
    "end": "424690"
  },
  {
    "text": "traditional file system you'll have a single file server that all of your your compute nodes connect to and that ends",
    "start": "424690",
    "end": "431890"
  },
  {
    "text": "up becoming a bottleneck so you can't really get the high levels of throughput that you need for these workloads and so",
    "start": "431890",
    "end": "437380"
  },
  {
    "text": "parallel file systems exist to solve that problem and what they do is they store data across multiple file servers",
    "start": "437380",
    "end": "445620"
  },
  {
    "text": "so to maximize the performance and reduce bottlenecks so the way it works is you'll have your data spread across",
    "start": "445620",
    "end": "451840"
  },
  {
    "text": "servers clients each individually talk to each data server and there's a mapping that the clients are aware of",
    "start": "451840",
    "end": "458290"
  },
  {
    "text": "for where data is whenever they need to request it and so the bottlenecks that",
    "start": "458290",
    "end": "463810"
  },
  {
    "text": "you get from everything going to a central server go away and you also get low latencies because you have a single",
    "start": "463810",
    "end": "468940"
  },
  {
    "text": "network hop between the client and the server but it's really hard to run a",
    "start": "468940",
    "end": "477430"
  },
  {
    "text": "parallel file system it requires specialized expertise and lots of time",
    "start": "477430",
    "end": "482950"
  },
  {
    "text": "this is something that customers have been telling us over and over they don't want to run and manage their own parallel file systems they have to",
    "start": "482950",
    "end": "489790"
  },
  {
    "text": "configure and maintain cumbersome software there's a ton of performance parameters that generally need to be",
    "start": "489790",
    "end": "495970"
  },
  {
    "text": "tuned nobody can just bring up a parallel file system and run it for their workload and it works like it just doesn't happen and",
    "start": "495970",
    "end": "503470"
  },
  {
    "text": "then keeping it alive ends up being really challenging especially as these things grow when you have multiple",
    "start": "503470",
    "end": "508660"
  },
  {
    "text": "servers and you have multiple disks attached to each server it can become a full-time job just to keep a cluster alive and so as a result until today",
    "start": "508660",
    "end": "518169"
  },
  {
    "text": "high-performance parallel file systems have mostly been out of reach for all but the largest institutions and",
    "start": "518169",
    "end": "524530"
  },
  {
    "text": "projects so and really it's out of reach unless you're lucky enough to have experts managing the file system for you",
    "start": "524530",
    "end": "531100"
  },
  {
    "text": "and most of these file systems are edible it's in size because what companies",
    "start": "531100",
    "end": "537400"
  },
  {
    "text": "generally do is they build them for peak capacity across teams and so now you're talking about clusters that are maybe",
    "start": "537400",
    "end": "542500"
  },
  {
    "text": "tens or hundreds of servers with thousands or tens of thousands of disks and so you're really talking about a",
    "start": "542500",
    "end": "549040"
  },
  {
    "text": "very challenging thing to keep alive and what customers have told us is that they have entire teams that are dedicated",
    "start": "549040",
    "end": "555820"
  },
  {
    "text": "just to keep these parallel file systems alive that's great if you have the luxury of having those teams available",
    "start": "555820",
    "end": "562240"
  },
  {
    "text": "to do that but for most projects that's that's not a reality and then it's not",
    "start": "562240",
    "end": "570700"
  },
  {
    "text": "trivial to move data into and out of a parallel file system talking about large amounts of data so on AWS for example to",
    "start": "570700",
    "end": "578290"
  },
  {
    "text": "move your data from s3 onto a parallel file system and you want to do it fast generally you're going to need to set up",
    "start": "578290",
    "end": "584140"
  },
  {
    "text": "data management tools to move the data over you're going to want to spread the the movement of data across multiple",
    "start": "584140",
    "end": "589930"
  },
  {
    "text": "nodes across multiple threads you're going to need to track what data is moved over and if it's successfully",
    "start": "589930",
    "end": "594970"
  },
  {
    "text": "moved over and all of this actually gates you're being able to start processing your data so there's a ton of",
    "start": "594970",
    "end": "601630"
  },
  {
    "text": "complexity and a ton of time that that that introduces and so based on keep",
    "start": "601630",
    "end": "608560"
  },
  {
    "start": "607000",
    "end": "772000"
  },
  {
    "text": "feedback from customers about how to better enable compute intense workloads from a storage perspective we thought",
    "start": "608560",
    "end": "614410"
  },
  {
    "text": "how can we do better and that's what we set out to deliver with fsx for lustre and that's why we built it and fsx for",
    "start": "614410",
    "end": "623620"
  },
  {
    "text": "lustre is a fully managed high-performance parallel file system on AWS it makes high-performance lustre file",
    "start": "623620",
    "end": "631030"
  },
  {
    "text": "systems accessible and simple for the first time it seamlessly integrates with",
    "start": "631030",
    "end": "636760"
  },
  {
    "text": "your existing data repositories both s3 and on-prem data stores and it's really",
    "start": "636760",
    "end": "642610"
  },
  {
    "text": "easy to launch and run with virtually unlimited scale and those are really our design goals for this that's what we've",
    "start": "642610",
    "end": "648700"
  },
  {
    "text": "delivered so let me start with what is lustre because actually show of hands",
    "start": "648700",
    "end": "653710"
  },
  {
    "text": "how many people are familiar with lustre as a file system ok so maybe what a",
    "start": "653710",
    "end": "658810"
  },
  {
    "text": "third of the folks here so it's one of the most popular parallel file systems it's open source it was started in 1999",
    "start": "658810",
    "end": "667510"
  },
  {
    "text": "at Carnegie Mellon University and since then it's matured into a file system",
    "start": "667510",
    "end": "673180"
  },
  {
    "text": "that's heavily used by companies by research institutions by government agencies for a pretty broad set of use",
    "start": "673180",
    "end": "679420"
  },
  {
    "text": "cases including things like seismic processing financial modeling EDA it's",
    "start": "679420",
    "end": "685240"
  },
  {
    "text": "used across a really wide variety of use cases and it's because of the performance that it delivers in fact 60%",
    "start": "685240",
    "end": "691870"
  },
  {
    "text": "of the top 100 fastest supercomputers leveraged lustre for data storage and",
    "start": "691870",
    "end": "696899"
  },
  {
    "text": "again for the most part it's it's been out of reach for most people to be able to run a lustre cluster and with fsx for",
    "start": "696899",
    "end": "706510"
  },
  {
    "text": "lustre there's really six key benefits that we're providing and I'll go through each of these but it's massively",
    "start": "706510",
    "end": "712570"
  },
  {
    "text": "scalable performance seamless access to your data repositories so you can link your file system to your long-term data",
    "start": "712570",
    "end": "718870"
  },
  {
    "text": "stores it's simple and fully managed it provides a native file system interface",
    "start": "718870",
    "end": "724300"
  },
  {
    "text": "it's cops to optimized for compute intensive workloads and it's secure and",
    "start": "724300",
    "end": "729610"
  },
  {
    "text": "compliant so let me talk about each so first in terms of performance the lustre",
    "start": "729610",
    "end": "737440"
  },
  {
    "text": "file system itself provides throughput of up to hundreds of gigabytes per second and millions of AI ops and it",
    "start": "737440",
    "end": "745329"
  },
  {
    "text": "provides consistent sub-millisecond latency and that's really due to two things the first is the parallel",
    "start": "745329",
    "end": "750339"
  },
  {
    "text": "architecture where clients and servers are just one network hop away and the second is that we're building this on",
    "start": "750339",
    "end": "756130"
  },
  {
    "text": "SSD storage and so you get the benefit the latency benefits of that and it also",
    "start": "756130",
    "end": "762100"
  },
  {
    "text": "supports hundreds of thousands of cores so you can really have a really large compute clusters accessing data set and",
    "start": "762100",
    "end": "772560"
  },
  {
    "start": "772000",
    "end": "1054000"
  },
  {
    "text": "with FS x4 lustre we've designed it so the file system throughput and IAP scale linearly with storage capacity and",
    "start": "772560",
    "end": "779890"
  },
  {
    "text": "that's generally because with the more data you have the more you need aggregate access to that data and so",
    "start": "779890",
    "end": "786610"
  },
  {
    "text": "specifically each terabyte of storage provides 200 megabytes per second of throughput and that's 100 percent of the",
    "start": "786610",
    "end": "794440"
  },
  {
    "text": "time you can get that 200 gigabytes per second of throughput you may even see higher levels depending on your i/o",
    "start": "794440",
    "end": "800260"
  },
  {
    "text": "pattern and then as I mentioned filesystems can scale to hundreds of gigabytes per",
    "start": "800260",
    "end": "805430"
  },
  {
    "text": "second and millions of AI ops so let me",
    "start": "805430",
    "end": "811160"
  },
  {
    "text": "talk about how the s3 integration works because I've alluded to it a couple times so the way it works is you'll have",
    "start": "811160",
    "end": "818000"
  },
  {
    "text": "your data stored in s3 like you do with a typical one of these computer intensive workloads and you'll create",
    "start": "818000",
    "end": "825860"
  },
  {
    "text": "your file system and when you create the file system you link it to the s3 bucket where your data is and as part of the",
    "start": "825860",
    "end": "833420"
  },
  {
    "text": "creation of that file system what we do is we build all of the file names and",
    "start": "833420",
    "end": "839180"
  },
  {
    "text": "the directory names in your file system that map to what's in your bucket so when you when you mount your file system",
    "start": "839180",
    "end": "846230"
  },
  {
    "text": "for the first time you're actually able to see a full directory structure with all your directories and files listed in",
    "start": "846230",
    "end": "852890"
  },
  {
    "text": "your file system and then when you access a file for the first time from the file system it pulls the data in",
    "start": "852890",
    "end": "859700"
  },
  {
    "text": "real time from s3 so we call that lazy loading the data from s3 and so what",
    "start": "859700",
    "end": "865970"
  },
  {
    "text": "that allows you to do is let's say that you have a really large bucket you only need to use a subset of the data in that",
    "start": "865970",
    "end": "871100"
  },
  {
    "text": "bucket you don't have to move all of the data over you don't have to specify here's the data that I need to move over it",
    "start": "871100",
    "end": "876920"
  },
  {
    "text": "just automatically moves when you're when your application needs to access the data and then there's also a API",
    "start": "876920",
    "end": "883790"
  },
  {
    "text": "that's provided for you to push data back to s3 and we track what data has",
    "start": "883790",
    "end": "889520"
  },
  {
    "text": "changed since your import from s3 and we only push the changes back so it's only",
    "start": "889520",
    "end": "894680"
  },
  {
    "text": "incremental changes and it's actually incremental since the last time you ran that commit API now if you have a",
    "start": "894680",
    "end": "901310"
  },
  {
    "text": "workload where it's really latency sensitive for that first access you could also do a batch load of your data",
    "start": "901310",
    "end": "907430"
  },
  {
    "text": "from s3 onto the file system it would provide a command for doing that as well but I think for most folks to lazy load",
    "start": "907430",
    "end": "913520"
  },
  {
    "text": "as will make most sense so just to be like super clear on this so let me walk",
    "start": "913520",
    "end": "920839"
  },
  {
    "text": "through an example so let's say that you have your filesystem up you're linked it to an s3 bucket and you want to access",
    "start": "920839",
    "end": "927230"
  },
  {
    "text": "file one dot text and so what happens is your compute node tries to",
    "start": "927230",
    "end": "934550"
  },
  {
    "text": "sit lustre fsx for lustre de Texas not on your file system it goes to s3 it",
    "start": "934550",
    "end": "940370"
  },
  {
    "text": "gets that file from s3 pulls it onto the file system and delivers it to the to",
    "start": "940370",
    "end": "946310"
  },
  {
    "text": "the compute node and there's a bit of latency there because it's going to s3 to get your data so now let's say that",
    "start": "946310",
    "end": "953019"
  },
  {
    "text": "another you want to access it again now this time it's already on the file system because it's been pulled and so",
    "start": "953019",
    "end": "959630"
  },
  {
    "text": "it just delivers it back to your computer and so you don't have the round-trip to s3 with the latency that",
    "start": "959630",
    "end": "966320"
  },
  {
    "text": "that entails and then we've optimized",
    "start": "966320",
    "end": "974810"
  },
  {
    "text": "the performance of the movement from s3 back and forth to make it really fast and so we're really doing a lot of",
    "start": "974810",
    "end": "981500"
  },
  {
    "text": "parallel stuff to make the movement fast so first of all you're fsx file system",
    "start": "981500",
    "end": "987500"
  },
  {
    "text": "is spread but a lot of them are spread across multiple file servers and multiple disks depending on the size of the filesystem and we really leverage",
    "start": "987500",
    "end": "993680"
  },
  {
    "text": "that parallelism to send parallel requests to s3 and then even from a single server we're using",
    "start": "993680",
    "end": "999260"
  },
  {
    "text": "multi-threading as well so if you do want to do a batch load and you issue that command we're gonna be pushing it",
    "start": "999260",
    "end": "1004660"
  },
  {
    "text": "at pretty high throughput because of that parallelism and that's important because a lot of times this is gating",
    "start": "1004660",
    "end": "1010959"
  },
  {
    "text": "the start of your workload the start of your compute and you want the data to move as fast as possible to the file system and then cloudbursting is also",
    "start": "1010959",
    "end": "1020050"
  },
  {
    "text": "supported so the scenario here is you have your data on prem and you want to spin up some compute on the cloud burst",
    "start": "1020050",
    "end": "1026829"
  },
  {
    "text": "to the cloud to process the data and so what you do is you would spin up a fsx",
    "start": "1026829",
    "end": "1032260"
  },
  {
    "text": "for lustre filesystem you would mount it from an on-prem server over direct-connect over or over VPN you",
    "start": "1032260",
    "end": "1040120"
  },
  {
    "text": "would batch move the data into your lustre filesystem you'd spin up your compute cluster on AWS you process the",
    "start": "1040120",
    "end": "1045640"
  },
  {
    "text": "data and then you'd move your data back so that would be the path for",
    "start": "1045640",
    "end": "1050800"
  },
  {
    "text": "cloudbursting and so as I mentioned simple and fully",
    "start": "1050800",
    "end": "1057070"
  },
  {
    "start": "1054000",
    "end": "1402000"
  },
  {
    "text": "managed we provision and set up the file servers and storage volumes we configure",
    "start": "1057070",
    "end": "1062740"
  },
  {
    "text": "and maintain the lustre software and with those two we're really putting the power of lustre and the reach of everybody fully managing it for you it",
    "start": "1062740",
    "end": "1073360"
  },
  {
    "text": "provides a POSIX interface so works like any file system would with your your",
    "start": "1073360",
    "end": "1079120"
  },
  {
    "text": "Linux applications you don't need to change your applications it provides read after write sorry read after write",
    "start": "1079120",
    "end": "1086740"
  },
  {
    "text": "and read after close consistency which is really important when you're accessing data from multiple nodes and",
    "start": "1086740",
    "end": "1092440"
  },
  {
    "text": "you want to have a guaranteed consistency model and also supports file locking also important when you have",
    "start": "1092440",
    "end": "1098890"
  },
  {
    "text": "multiple compute nodes accessing a set of data and it's a computer that secure",
    "start": "1098890",
    "end": "1105370"
  },
  {
    "text": "and compliant data is automatically encrypted at rest PCI DSS ISO compliant",
    "start": "1105370",
    "end": "1110590"
  },
  {
    "text": "HIPAA eligible you access your file system from your V pcs you set up",
    "start": "1110590",
    "end": "1117340"
  },
  {
    "text": "endpoints in in your V pcs and you apply security groups to them so kind of a typical V PC model admin API is is",
    "start": "1117340",
    "end": "1127090"
  },
  {
    "text": "controlled through I am and then monitoring and logging API calls are",
    "start": "1127090",
    "end": "1132700"
  },
  {
    "text": "through cloud Rail and we've really cost",
    "start": "1132700",
    "end": "1137980"
  },
  {
    "text": "optimized it for compute intensive workloads so your data on your lustre file system is not replicated and the",
    "start": "1137980",
    "end": "1145570"
  },
  {
    "text": "reason we're not replicating it is because it's designed for these workflows where you have your durable data somewhere you spin up a file system",
    "start": "1145570",
    "end": "1153580"
  },
  {
    "text": "that's around for hours days weeks in some cases months you can write your you",
    "start": "1153580",
    "end": "1159190"
  },
  {
    "text": "can commit data back to s3 at any point or back to your data your durable data store at any point and you spin down",
    "start": "1159190",
    "end": "1166360"
  },
  {
    "text": "your file system when you're done and by not replicating it we have the benefit of lower cost for us which we pass on is",
    "start": "1166360",
    "end": "1172960"
  },
  {
    "text": "lower price for you and we also have significant performance benefits by not",
    "start": "1172960",
    "end": "1178120"
  },
  {
    "text": "replicating the data so it's really optimized for these compute intensive",
    "start": "1178120",
    "end": "1183220"
  },
  {
    "text": "type workflows and the price is 14 cents per gigabyte",
    "start": "1183220",
    "end": "1188350"
  },
  {
    "text": "per month a interesting way to think about that if you think about these as kind of ephemeral workloads is 20 cents",
    "start": "1188350",
    "end": "1194679"
  },
  {
    "text": "per terabyte per hour and so if you have a few terabytes for a few hours you can do the math and it comes out to a few",
    "start": "1194679",
    "end": "1200770"
  },
  {
    "text": "dollars for running these these massively large file systems so as an",
    "start": "1200770",
    "end": "1208270"
  },
  {
    "text": "example I think that the billing is fairly straightforward we'll go through an example just to make it super clear",
    "start": "1208270",
    "end": "1214809"
  },
  {
    "text": "so let's say that you have a total data set of 250 terabytes of data and this is",
    "start": "1214809",
    "end": "1220480"
  },
  {
    "text": "in your s3 bucket and you want to process 25 terabytes of that data every",
    "start": "1220480",
    "end": "1226000"
  },
  {
    "text": "day and you need to do it at you know something like 5 gigabytes per second so you decide to use the Celestra file",
    "start": "1226000",
    "end": "1232240"
  },
  {
    "text": "system and you run it for 10 hours each day so what would your bill look like",
    "start": "1232240",
    "end": "1237510"
  },
  {
    "text": "well again it's 20 cents per terabyte per hour times the 25 terabytes per job",
    "start": "1237510",
    "end": "1243100"
  },
  {
    "text": "and every job is 10 hours you have 30 of those a month so you're talking about",
    "start": "1243100",
    "end": "1248130"
  },
  {
    "text": "$1,500 a month of spend that's in addition to the 5700 on s3 for your full",
    "start": "1248130",
    "end": "1254530"
  },
  {
    "text": "data set so for for most use cases this is gonna be a pretty small percentage of your overall storage bill they'll have",
    "start": "1254530",
    "end": "1261070"
  },
  {
    "text": "your larger data set in s3 and you'll be able to selectively process portions of",
    "start": "1261070",
    "end": "1266559"
  },
  {
    "text": "it and and pay a relatively small amount storage wise for doing that and I've",
    "start": "1266559",
    "end": "1274659"
  },
  {
    "text": "mentioned a couple of these workloads but high performance computing lustre is is basically the standard and the HPC",
    "start": "1274659",
    "end": "1280900"
  },
  {
    "text": "space machine learning often requires as these large compute clusters requires",
    "start": "1280900",
    "end": "1286840"
  },
  {
    "text": "very high levels of throughput parallel access media rendering and transcoding those both tend to be throughput heavy",
    "start": "1286840",
    "end": "1292750"
  },
  {
    "text": "and very latency sensitive processing big data EDA is another use case that we",
    "start": "1292750",
    "end": "1300130"
  },
  {
    "text": "expect to be very popular with this and then there's a whole bunch of kind of financial analytics use cases oil and",
    "start": "1300130",
    "end": "1306700"
  },
  {
    "text": "gas seismic processing it's a pretty bud broad spectrum of lots of data need to",
    "start": "1306700",
    "end": "1312280"
  },
  {
    "text": "compute it and I do that in kind of these these workflows",
    "start": "1312280",
    "end": "1318059"
  },
  {
    "text": "and we announced we announced the service yesterday and it was available immediately and it's in these four",
    "start": "1318100",
    "end": "1324880"
  },
  {
    "text": "regions us West Oregon us East Northern Virginia us East Ohio and EU Ireland and",
    "start": "1324880",
    "end": "1331890"
  },
  {
    "text": "we will be rolling out to more regions pretty quickly and let me just talk",
    "start": "1331890",
    "end": "1339850"
  },
  {
    "text": "about fsx more broadly so on Wednesday we generally introduced the Amazon fsx",
    "start": "1339850",
    "end": "1345940"
  },
  {
    "text": "family and it provides fully managed third-party file systems that are",
    "start": "1345940",
    "end": "1351070"
  },
  {
    "text": "optimized for a variety of different environments or different use cases so",
    "start": "1351070",
    "end": "1356530"
  },
  {
    "text": "the other one that we announced yesterday was a fully managed Windows file server and so that's for kind of",
    "start": "1356530",
    "end": "1362080"
  },
  {
    "text": "traditional Windows applications that need native Windows file storage makes it super easy to spin up and store your",
    "start": "1362080",
    "end": "1368260"
  },
  {
    "text": "data on Windows file servers and not have to worry about managing those so kind of the core goals of fsx is simple",
    "start": "1368260",
    "end": "1376120"
  },
  {
    "text": "and fully managed the native compatibility and features and performance of these third-party file",
    "start": "1376120",
    "end": "1381640"
  },
  {
    "text": "systems but we integrate them with AWS services to make them much more useful",
    "start": "1381640",
    "end": "1386920"
  },
  {
    "text": "than they otherwise it would be an example is what we're doing with s3 with lustre and then cost optimized for the",
    "start": "1386920",
    "end": "1392680"
  },
  {
    "text": "use cases that they're targeting so again with lustre for example with the non replicated version of the product so",
    "start": "1392680",
    "end": "1403120"
  },
  {
    "start": "1402000",
    "end": "1587000"
  },
  {
    "text": "let me hand it over to Daryl now to share some more details about sex for lustre and show show a demo",
    "start": "1403120",
    "end": "1410610"
  },
  {
    "text": "great thanks then appreciate it so you",
    "start": "1413110",
    "end": "1422110"
  },
  {
    "text": "know I think the best way to share more about how we've made Leicester file",
    "start": "1422110",
    "end": "1427360"
  },
  {
    "text": "systems accessible easy to launch easy",
    "start": "1427360",
    "end": "1432429"
  },
  {
    "text": "to run and actually seamlessly integrated with existing datasets is to",
    "start": "1432429",
    "end": "1437710"
  },
  {
    "text": "see it in action so I'm gonna go ahead and switch over to my laptop really",
    "start": "1437710",
    "end": "1446500"
  },
  {
    "text": "quick so what we're gonna do I have a",
    "start": "1446500",
    "end": "1452980"
  },
  {
    "text": "CLI command here I'm gonna go ahead and copy into my terminal window so as you",
    "start": "1452980",
    "end": "1463330"
  },
  {
    "text": "can see this is a very simple command it's a create filesystem command if you",
    "start": "1463330",
    "end": "1468790"
  },
  {
    "text": "don't if you haven't downloaded the latest CLI we made this push yesterday",
    "start": "1468790",
    "end": "1474220"
  },
  {
    "text": "so download the latest you'll have access to the fsx api we identify what",
    "start": "1474220",
    "end": "1481000"
  },
  {
    "text": "type of file system it is again there's as Edie mentioned there's two different Amazon fsx file systems one for Windows",
    "start": "1481000",
    "end": "1487750"
  },
  {
    "text": "file server in one for lustre so we need to identify what type of file system",
    "start": "1487750",
    "end": "1492970"
  },
  {
    "text": "then we also identify the storage capacity so in this example I'm gonna have a 3,600 gigabyte file system then",
    "start": "1492970",
    "end": "1502660"
  },
  {
    "text": "I'm going to create I identify the subnet this is the subnet where my file system is going to reside the optionally",
    "start": "1502660",
    "end": "1511390"
  },
  {
    "text": "I am linking this to an s3 bucket the NASA NEX bucket this is an open data",
    "start": "1511390",
    "end": "1517540"
  },
  {
    "text": "source bucket we'll talk about a little later and that's it that's really how",
    "start": "1517540",
    "end": "1524320"
  },
  {
    "text": "simple it is it is going to have the default VPC security group associated",
    "start": "1524320",
    "end": "1530260"
  },
  {
    "text": "with it if I wanted to add additional security groups I could do that at this",
    "start": "1530260",
    "end": "1535299"
  },
  {
    "text": "command as well so we're going to go ahead and just run that as you can see",
    "start": "1535299",
    "end": "1541050"
  },
  {
    "text": "within just a matter of seconds the API is finished and never",
    "start": "1541050",
    "end": "1547090"
  },
  {
    "text": "has it been easier to launch a lustre filesystem it will take a few minutes",
    "start": "1547090",
    "end": "1555700"
  },
  {
    "text": "for the the file system to be created because I linked it with an s3 bucket as",
    "start": "1555700",
    "end": "1562990"
  },
  {
    "text": "Edie mentioned we are going to load the object database or not that data itself",
    "start": "1562990",
    "end": "1570160"
  },
  {
    "text": "but the the metadata the the object name and the structure of these objects into",
    "start": "1570160",
    "end": "1575620"
  },
  {
    "text": "the file system what we call a a metadata load and that's going to happen as a part of the filesystem creation so",
    "start": "1575620",
    "end": "1589090"
  },
  {
    "start": "1587000",
    "end": "1705000"
  },
  {
    "text": "earlier I created a file system it's a little larger than the 3600 gigabyte",
    "start": "1589090",
    "end": "1595780"
  },
  {
    "text": "file system here but what I did was I also linked it to this bucket and as we",
    "start": "1595780",
    "end": "1600910"
  },
  {
    "text": "can see once the file system is created and we start loading the object metadata",
    "start": "1600910",
    "end": "1607120"
  },
  {
    "text": "into the file system we can see the the activity so this is a CloudFormation template or I use a cloud formation",
    "start": "1607120",
    "end": "1613900"
  },
  {
    "text": "template to launch a cloud watch dashboard for my file system the very",
    "start": "1613900",
    "end": "1618940"
  },
  {
    "text": "top is the available storage next we have the throughput in megabytes per second then finally we have the",
    "start": "1618940",
    "end": "1624970"
  },
  {
    "text": "operations per second so as you can see we loaded metadata for seven hundred",
    "start": "1624970",
    "end": "1632230"
  },
  {
    "text": "three hundred and seventy three thousand objects and that load took only eleven",
    "start": "1632230",
    "end": "1638620"
  },
  {
    "text": "minutes so we were getting roughly five hundred and sixty seven objects per",
    "start": "1638620",
    "end": "1643930"
  },
  {
    "text": "second so very easy fast to go ahead and load that really associate it with an s3",
    "start": "1643930",
    "end": "1651970"
  },
  {
    "text": "bucket and load the metadata so we can have access to that to that metadata so",
    "start": "1651970",
    "end": "1657490"
  },
  {
    "text": "the data set that I'm using today is it's in the open open open data",
    "start": "1657490",
    "end": "1664720"
  },
  {
    "text": "repository on AWS NEX it's really a platform for scientific collaboration",
    "start": "1664720",
    "end": "1670030"
  },
  {
    "text": "knowledge sharing and research of earth sciences so it contains objects of",
    "start": "1670030",
    "end": "1675460"
  },
  {
    "text": "different sizes and file types we have net CDF or",
    "start": "1675460",
    "end": "1680679"
  },
  {
    "text": "our network common data form format files they're really designed for storing multi-dimensional data we have",
    "start": "1680679",
    "end": "1687700"
  },
  {
    "text": "some TIF files and we have some hdf files these are hierarchical data format really designed to organize large",
    "start": "1687700",
    "end": "1694539"
  },
  {
    "text": "amounts of data we also have some PDFs and some text files so they range from you know 4 kilobytes all the way up to",
    "start": "1694539",
    "end": "1701679"
  },
  {
    "text": "you know hundreds and hundreds of megabytes so what we're going to do is",
    "start": "1701679",
    "end": "1707110"
  },
  {
    "start": "1705000",
    "end": "2092000"
  },
  {
    "text": "we're going to what I'm going to do is is lazy load this data into my",
    "start": "1707110",
    "end": "1713139"
  },
  {
    "text": "filesystem it because fsx for lustre is a POSIX compliant file system there's",
    "start": "1713139",
    "end": "1719110"
  },
  {
    "text": "really nothing special I need to do so what I'm gonna do is just hop over",
    "start": "1719110",
    "end": "1726730"
  },
  {
    "text": "everyone can see that great I'm gonna go back to my my screen here I just have a",
    "start": "1726730",
    "end": "1733389"
  },
  {
    "text": "simple LS of a of a file so this file is",
    "start": "1733389",
    "end": "1740200"
  },
  {
    "text": "roughly it's a 181 megabytes it's a tiff file it's sitting in the metadata is on",
    "start": "1740200",
    "end": "1747669"
  },
  {
    "text": "my my cluster to my lustre filesystem but the data still resides in Amazon s3",
    "start": "1747669",
    "end": "1754590"
  },
  {
    "text": "so up at the very top I'm gonna go ahead and just run and load so we can monitor",
    "start": "1754590",
    "end": "1760090"
  },
  {
    "text": "network traffic to this instance so I'm",
    "start": "1760090",
    "end": "1768700"
  },
  {
    "text": "going to simulate an application reading this file so I'm actually going to just",
    "start": "1768700",
    "end": "1774220"
  },
  {
    "text": "do a cat into temp FS so I'm going to read all of the the data of this file",
    "start": "1774220",
    "end": "1780100"
  },
  {
    "text": "and we're gonna see what happens",
    "start": "1780100",
    "end": "1783539"
  },
  {
    "text": "so you noticed I kicked it off and it",
    "start": "1787090",
    "end": "1792429"
  },
  {
    "text": "took roughly 4.2 seconds but you notice for about the first three and a half",
    "start": "1792429",
    "end": "1798340"
  },
  {
    "text": "seconds we didn't see any network traffic why why not well at that time",
    "start": "1798340",
    "end": "1807940"
  },
  {
    "text": "the object data was being loaded from s3 into the fsx for lustre file system once",
    "start": "1807940",
    "end": "1815710"
  },
  {
    "text": "it was loaded to the file system then at the very end we loaded it into into the",
    "start": "1815710",
    "end": "1821889"
  },
  {
    "text": "client and the client was able to able to get it so if we read it again",
    "start": "1821889",
    "end": "1831179"
  },
  {
    "text": "how fast is that gonna be what one one six seconds very fast so where did I",
    "start": "1832110",
    "end": "1840999"
  },
  {
    "text": "read this from cache it's on my client",
    "start": "1840999",
    "end": "1847659"
  },
  {
    "text": "cache okay so I did not as you look at the the network traffic and my end load screen up above you notice then when I",
    "start": "1847659",
    "end": "1854860"
  },
  {
    "text": "did that cat to subsequent cat data was that loaded from the F is X file system",
    "start": "1854860",
    "end": "1863309"
  },
  {
    "text": "so what we want to do is we want to go",
    "start": "1863309",
    "end": "1869200"
  },
  {
    "text": "ahead and flush drop our cache so it's not gonna reside there now that we've",
    "start": "1869200",
    "end": "1878110"
  },
  {
    "text": "dropped our cache let's go ahead and run that again and take a look at the end load output so just a little bit we",
    "start": "1878110",
    "end": "1888309"
  },
  {
    "text": "loaded that now in point four four two seconds so it wasn't the four seconds",
    "start": "1888309",
    "end": "1894759"
  },
  {
    "text": "that it did that it took to load that data in from s3 we know that it loaded",
    "start": "1894759",
    "end": "1899980"
  },
  {
    "text": "it from the FS x 4 lustre filesystem because again it was very fast and we",
    "start": "1899980",
    "end": "1905139"
  },
  {
    "text": "know that that file actually resides on that file says the data resides on that filesystem now",
    "start": "1905139",
    "end": "1912119"
  },
  {
    "text": "so let's go ahead and just take a look at where my file is so if we do a DF on",
    "start": "1916300",
    "end": "1925460"
  },
  {
    "text": "my file system if we scroll up we can actually see that that 181 megabyte file",
    "start": "1925460",
    "end": "1932540"
  },
  {
    "text": "is loaded on one of my osts my object storage target of the lustre",
    "start": "1932540",
    "end": "1940940"
  },
  {
    "text": "file system so that's where it resides so I know that that data has been loaded which is great we scroll up we can",
    "start": "1940940",
    "end": "1946730"
  },
  {
    "text": "actually see the metadata in the MDT so we've got 4 gigs of metadata that describes all of those objects in the",
    "start": "1946730",
    "end": "1955250"
  },
  {
    "text": "file system so what if we wanted to",
    "start": "1955250",
    "end": "1963860"
  },
  {
    "text": "recover that stores 181 megabytes isn't much but if you had a lot of data in",
    "start": "1963860",
    "end": "1969140"
  },
  {
    "text": "there and you wanted to load more data what could we do so we could actually release that data from the file system",
    "start": "1969140",
    "end": "1976880"
  },
  {
    "text": "so we can add more data in if we wanted to so by issuing a simple HSM release",
    "start": "1976880",
    "end": "1982700"
  },
  {
    "text": "command we can release that file let's go back to the DF we still see it hasn't",
    "start": "1982700",
    "end": "1990620"
  },
  {
    "text": "been released yet and we issue it again now the file data is gone so it's no",
    "start": "1990620",
    "end": "1998450"
  },
  {
    "text": "longer in my fsx for lustre file system upon the next read what happens we're",
    "start": "1998450",
    "end": "2012910"
  },
  {
    "text": "going back up to s3 it's going to take for 4.8 seconds again we just see the",
    "start": "2012910",
    "end": "2018760"
  },
  {
    "text": "network activity on this instance once it hits the HSH once it hits the FS x",
    "start": "2018760",
    "end": "2023950"
  },
  {
    "text": "for the lustre filesystem then it'll load that data into the client",
    "start": "2023950",
    "end": "2029370"
  },
  {
    "text": "we can do it again we'll notice that we're reading from cache we'll flush",
    "start": "2033690",
    "end": "2040450"
  },
  {
    "text": "that cache will read it one more time",
    "start": "2040450",
    "end": "2046380"
  },
  {
    "text": "and now we know that we're getting it from the the lustre filesystem so a very",
    "start": "2046380",
    "end": "2052599"
  },
  {
    "text": "easy way to to bring did it in now this is simulating what your application",
    "start": "2052599",
    "end": "2058898"
  },
  {
    "text": "would be doing upon first access you will have higher latency because again",
    "start": "2058899",
    "end": "2064300"
  },
  {
    "text": "we need to go and retrieve that object from s3 loaded in the filesystem and",
    "start": "2064300",
    "end": "2069820"
  },
  {
    "text": "then deliver it to the client so sometimes you may your application your",
    "start": "2069820",
    "end": "2075608"
  },
  {
    "text": "your workload may be sensitive to that latency so maybe you want to load it in",
    "start": "2075609",
    "end": "2081398"
  },
  {
    "text": "load the data in a batch sort of a batch format as ed mentioned so we can do that",
    "start": "2081399",
    "end": "2087849"
  },
  {
    "text": "as well so I actually ran this earlier",
    "start": "2087849",
    "end": "2093190"
  },
  {
    "text": "oh well actually let me let me go back and describe to you what we just did so",
    "start": "2093190",
    "end": "2098589"
  },
  {
    "text": "again when the upon first read of that file it loads from s3 into the file",
    "start": "2098589",
    "end": "2105220"
  },
  {
    "text": "system and then it delivers it to the client in my example that I that I recorded it was four point two nine",
    "start": "2105220",
    "end": "2111160"
  },
  {
    "text": "three seconds so my next read where did it come from cache very fast 0.109",
    "start": "2111160",
    "end": "2118960"
  },
  {
    "text": "seconds I dropped my cache then I read it again fast still fast but this time",
    "start": "2118960",
    "end": "2126819"
  },
  {
    "text": "it came from fsx and it was in my example it was point three nine six seconds that I ran a little earlier so again very very fast",
    "start": "2126819",
    "end": "2136500"
  },
  {
    "text": "access to your filesystem and it allows you to get high levels of operations per",
    "start": "2136500",
    "end": "2142180"
  },
  {
    "text": "second and high levels of throughput so",
    "start": "2142180",
    "end": "2147329"
  },
  {
    "text": "when we talk about batch loading files you could issue a",
    "start": "2147329",
    "end": "2154329"
  },
  {
    "text": "single command like this basically identifying all of the the files in your file system and then issuing this HSM",
    "start": "2154329",
    "end": "2162040"
  },
  {
    "text": "restore command for all of those so by using a simple command like this you're able to do these batch",
    "start": "2162040",
    "end": "2167500"
  },
  {
    "text": "loads and this is really what it looks like again the the client itself isn't",
    "start": "2167500",
    "end": "2172690"
  },
  {
    "text": "it's none of the data is passing through the client it's going directly from the s3 bucket to the fsx for Windows sorry",
    "start": "2172690",
    "end": "2181270"
  },
  {
    "text": "fsx for lustre file system and it's not going through a specific node the way",
    "start": "2181270",
    "end": "2188020"
  },
  {
    "text": "that lustre works is all of the OST s that we have and you saw the list of my",
    "start": "2188020",
    "end": "2193480"
  },
  {
    "text": "osts those are actually getting the data directly from s3 so it's highly parallel",
    "start": "2193480",
    "end": "2198990"
  },
  {
    "text": "so that's how we're able to deliver high levels of throughput in operations per",
    "start": "2198990",
    "end": "2205060"
  },
  {
    "text": "second as a part of this that part of the slow process so with my first",
    "start": "2205060",
    "end": "2211300"
  },
  {
    "start": "2209000",
    "end": "2289000"
  },
  {
    "text": "example you see here that that is the metadata load of the file system and",
    "start": "2211300",
    "end": "2217599"
  },
  {
    "text": "then what I did was actually loaded I did a batch load to load all the data in",
    "start": "2217599",
    "end": "2223990"
  },
  {
    "text": "this file system as well and as you can see the amount of throughput in operations per second",
    "start": "2223990",
    "end": "2230140"
  },
  {
    "text": "basically dwarfed what that made a metadata load was it you can barely even",
    "start": "2230140",
    "end": "2235690"
  },
  {
    "text": "see it on the bottom chart which is the operations per second so we have 370",
    "start": "2235690",
    "end": "2244530"
  },
  {
    "text": "3572 files it took 32 minutes to load this data so we were able to achieve",
    "start": "2244530",
    "end": "2250440"
  },
  {
    "text": "fifteen point eight gigabytes per second at our peak we had a peak of five",
    "start": "2250440",
    "end": "2256330"
  },
  {
    "text": "hundred and twenty eight thousand operations per second and as a part of",
    "start": "2256330",
    "end": "2261970"
  },
  {
    "text": "this process we load at twenty two point seven terabytes of data so again very fast I didn't have to spin up a fleet",
    "start": "2261970",
    "end": "2270690"
  },
  {
    "text": "ec2 instances to ingest that data I could have a simple simple ec2 instance",
    "start": "2270690",
    "end": "2277720"
  },
  {
    "text": "issue a simple command and then all of the work is done by Amazon fsx for",
    "start": "2277720",
    "end": "2283060"
  },
  {
    "text": "lustre as we import all that data into the file system",
    "start": "2283060",
    "end": "2288329"
  },
  {
    "start": "2289000",
    "end": "2389000"
  },
  {
    "text": "so you'll also have workloads where you'll need to generate lots amount of",
    "start": "2291710",
    "end": "2298529"
  },
  {
    "text": "data as well so you know Tiff's you know",
    "start": "2298529",
    "end": "2304650"
  },
  {
    "text": "it could be terabytes of data it could be petabytes of data but you're gonna have lots of you know an opportunity to",
    "start": "2304650",
    "end": "2312779"
  },
  {
    "text": "to ingest and and generate lots of data so in the same example I have my file",
    "start": "2312779",
    "end": "2320130"
  },
  {
    "text": "system it's a hundred and eight terabyte file system so if you do the math with you know 200 megabytes per second per",
    "start": "2320130",
    "end": "2327089"
  },
  {
    "text": "terabyte of storage capacity that I prevent we should be able to achieve",
    "start": "2327089",
    "end": "2332460"
  },
  {
    "text": "very high levels of throughput so during my my ingest of data that I generated",
    "start": "2332460",
    "end": "2341369"
  },
  {
    "text": "from a fleet of these ec2 instances you know we can see that we're able to achieve high levels of throughput even",
    "start": "2341369",
    "end": "2348119"
  },
  {
    "text": "well above the throughput that we achieved when we were importing the data directly from from s3 so I generated 6.4",
    "start": "2348119",
    "end": "2357660"
  },
  {
    "text": "terabytes of data it only took four point five minutes and it was roughly in",
    "start": "2357660",
    "end": "2363029"
  },
  {
    "text": "that peak there we were getting around 20 3.9 gigabytes per second and I think",
    "start": "2363029",
    "end": "2369000"
  },
  {
    "text": "besides my file system I should have been getting 23 point 1 2 or something",
    "start": "2369000",
    "end": "2374519"
  },
  {
    "text": "like that so slightly higher than what the throughput capacity should be based",
    "start": "2374519",
    "end": "2381029"
  },
  {
    "text": "on the metrics of 200 megabytes per second per per terabyte of data stored",
    "start": "2381029",
    "end": "2387588"
  },
  {
    "start": "2389000",
    "end": "2419000"
  },
  {
    "text": "so as that mentioned we just launched this yesterday we were very excited about it we do have a video and seblak",
    "start": "2390130",
    "end": "2397450"
  },
  {
    "text": "posts about the service we are going to be having white papers and reference",
    "start": "2397450",
    "end": "2402489"
  },
  {
    "text": "architectures around how to use fsx for lustre our documentation is out there as",
    "start": "2402489",
    "end": "2407950"
  },
  {
    "text": "well so the commands that you saw me run today those are available in our user guide and you know we encourage you to",
    "start": "2407950",
    "end": "2415539"
  },
  {
    "text": "go out take a look at it and test it out we do want to thank you we do have time",
    "start": "2415539",
    "end": "2422079"
  },
  {
    "start": "2419000",
    "end": "2695000"
  },
  {
    "text": "for Q&A so what I like to do is open it up to the floor so that you can ask",
    "start": "2422079",
    "end": "2427660"
  },
  {
    "text": "questions before everyone leaves though do complete the surveys of the session",
    "start": "2427660",
    "end": "2432970"
  },
  {
    "text": "so we can gather feedback from you so we'll be able to improve our sessions year-over-year question",
    "start": "2432970",
    "end": "2448440"
  },
  {
    "text": "I'm the on the on the client-side we're on the server-side okay yeah so this is",
    "start": "2452770",
    "end": "2460910"
  },
  {
    "text": "a standard version of fluster server and client so it's kind of a few views",
    "start": "2460910",
    "end": "2467000"
  },
  {
    "text": "lustre before it's the same type of performance the same functionality that you would get from a typical yeah yep so",
    "start": "2467000",
    "end": "2474170"
  },
  {
    "text": "it's the latest stable version of it question up here",
    "start": "2474170",
    "end": "2481390"
  },
  {
    "text": "we'll start with that okay so the question is what happens when the bucket",
    "start": "2493810",
    "end": "2499450"
  },
  {
    "text": "size and it could be a bucket or even a prefix inside that bucket as well so you could reduce it to a smaller set of",
    "start": "2499450",
    "end": "2505600"
  },
  {
    "text": "objects so what happens if the s3 bucket or the prefix is much larger than your",
    "start": "2505600",
    "end": "2511930"
  },
  {
    "text": "file system in every case you may want to have an environment like that because you have a large data set you know what",
    "start": "2511930",
    "end": "2518110"
  },
  {
    "text": "you don't need to process all that data so what's going to happen is it's going to load it's going to fill up your your",
    "start": "2518110",
    "end": "2523240"
  },
  {
    "text": "file system and then it's not going to load any more data in there so what you'd have to do is release that the the",
    "start": "2523240",
    "end": "2529230"
  },
  {
    "text": "files the data of those files from your file system in order to ingest anymore",
    "start": "2529230",
    "end": "2534670"
  },
  {
    "text": "and one thing to clarify if you're linking your file system to your bucket it only when you first do that link it",
    "start": "2534670",
    "end": "2540790"
  },
  {
    "text": "only brings in the file names and the directory entries so you could have like a multi petabyte bucket spin up a you",
    "start": "2540790",
    "end": "2549160"
  },
  {
    "text": "know five terabyte lustre file system you're not going to consume the full bucket unless you start pulling data in",
    "start": "2549160",
    "end": "2555820"
  },
  {
    "text": "and so it's really if you're pulling data in and you end up filling up your file system you can release data back to",
    "start": "2555820",
    "end": "2561130"
  },
  {
    "text": "the s3 bucket question back there",
    "start": "2561130",
    "end": "2566339"
  },
  {
    "text": "so the question is do you need to do that by hand or is there some type of caching mechanism cache efficient ok",
    "start": "2568800",
    "end": "2575530"
  },
  {
    "text": "cache eviction 20 cover that so yeah you would do it by hand you'd use the",
    "start": "2575530",
    "end": "2581980"
  },
  {
    "text": "command that we provide but it's not a cache eviction type of so the command",
    "start": "2581980",
    "end": "2587710"
  },
  {
    "text": "that he saw me run earlier the hsm release so basically we released that data and we saw it actually be removed",
    "start": "2587710",
    "end": "2593500"
  },
  {
    "text": "from that that OST and then you could import that in so you could do it individually or you can actually run a",
    "start": "2593500",
    "end": "2600220"
  },
  {
    "text": "similar batch command instead of batch loading data you can batch the vector as",
    "start": "2600220",
    "end": "2606070"
  },
  {
    "text": "well by whatever whenever however you want to sort of define that find or",
    "start": "2606070",
    "end": "2611680"
  },
  {
    "text": "search criteria down here",
    "start": "2611680",
    "end": "2617069"
  },
  {
    "text": "okay so the first question was around permissions of your of your objects okay",
    "start": "2638190",
    "end": "2645000"
  },
  {
    "text": "and the second question was around soft links what happens when you have when",
    "start": "2645000",
    "end": "2650190"
  },
  {
    "text": "you have soft links okay so for the permissions right now when we push the",
    "start": "2650190",
    "end": "2656369"
  },
  {
    "text": "incremental changes back to s3 we're not maintaining the permissions in the object we've gotten some feedback from",
    "start": "2656369",
    "end": "2663150"
  },
  {
    "text": "customers this week that that would be useful so we're looking at doing that and we can basically just store it as",
    "start": "2663150",
    "end": "2669599"
  },
  {
    "text": "metadata and your s3 object so that's it would be good to understand a use case",
    "start": "2669599",
    "end": "2675420"
  },
  {
    "text": "that you have in a little more detail to understand you know what the need is but we're hearing that so well try to",
    "start": "2675420",
    "end": "2681000"
  },
  {
    "text": "respond quickly to that one what was that what was the second one sorry ah do",
    "start": "2681000",
    "end": "2687750"
  },
  {
    "text": "we support South links yes yep back there so the question is how frequent do",
    "start": "2687750",
    "end": "2698190"
  },
  {
    "start": "2695000",
    "end": "2756000"
  },
  {
    "text": "you sync back to to s3 so that's a that's a manual command that you would issue so you would actually issue a",
    "start": "2698190",
    "end": "2704549"
  },
  {
    "text": "command to write that file back to s3 as an object and it is incremental as well",
    "start": "2704549",
    "end": "2711930"
  },
  {
    "text": "it's incremented yeah and we actually originally we were planning to automatically sync data back and we",
    "start": "2711930",
    "end": "2719910"
  },
  {
    "text": "talked to customers about that and they said actually don't do that because we don't want to keep sending data back and you know wait for that and pay the",
    "start": "2719910",
    "end": "2727109"
  },
  {
    "text": "request costs we'd rather batch do it and so it's really you know giving you the command to do that and and we detect",
    "start": "2727109",
    "end": "2733140"
  },
  {
    "text": "what the incremental changes worth let's come over here",
    "start": "2733140",
    "end": "2738829"
  },
  {
    "start": "2756000",
    "end": "2792000"
  },
  {
    "text": "so the question is Linux is going to be dropping lustre support from starting at",
    "start": "2756210",
    "end": "2764520"
  },
  {
    "text": "Linux kernel 418 what does that mean for our service basically for fsx for",
    "start": "2764520",
    "end": "2770670"
  },
  {
    "text": "Leicester so I would say that we're committed to keeping luster going",
    "start": "2770670",
    "end": "2776790"
  },
  {
    "text": "forward and so we're gonna continue to work with the community and invest in making sure that's available",
    "start": "2776790",
    "end": "2782869"
  },
  {
    "text": "good thank you",
    "start": "2782869",
    "end": "2786529"
  },
  {
    "start": "2792000",
    "end": "2851000"
  },
  {
    "text": "so two questions what happens when the filesystem is full do you need to evict",
    "start": "2792830",
    "end": "2799040"
  },
  {
    "text": "objects do you have to manually purge it or automatically so you would have to",
    "start": "2799040",
    "end": "2805340"
  },
  {
    "text": "manually release the the file data from your filesystem in order to in order to",
    "start": "2805340",
    "end": "2811130"
  },
  {
    "text": "recover more data to load in more data yeah and your last question is it",
    "start": "2811130",
    "end": "2819470"
  },
  {
    "text": "possible to resize the filesystem so today it is not possible to resize the filesystem when you create your",
    "start": "2819470",
    "end": "2824900"
  },
  {
    "text": "filesystem you identify the storage capacity and then we launch that luster",
    "start": "2824900",
    "end": "2829970"
  },
  {
    "text": "cluster to support that that that size and the part of the reason for that is",
    "start": "2829970",
    "end": "2837080"
  },
  {
    "text": "because we really do see this as you know kind of first for shorter term like it's not gonna be you're gonna have this",
    "start": "2837080",
    "end": "2842780"
  },
  {
    "text": "single thing around for years and you need to scale it so but again this feedback if if people feel that's",
    "start": "2842780",
    "end": "2849500"
  },
  {
    "text": "important it's good feedback for us yeah really within minutes you can have a you know tens of terabytes hundreds of",
    "start": "2849500",
    "end": "2855650"
  },
  {
    "text": "terabytes lustre filesystem use it for a couple of hours maybe a day or so tear back down if you have more data spin",
    "start": "2855650",
    "end": "2862400"
  },
  {
    "text": "back up again you know we have per second pricing billing yeah per second billing",
    "start": "2862400",
    "end": "2869180"
  },
  {
    "text": "on the amount of data that you know that you created as a part of your filesystem",
    "start": "2869180",
    "end": "2875780"
  },
  {
    "text": "so yeah we I really think of it as more of a on-demand scratch filesystem for",
    "start": "2875780",
    "end": "2882640"
  },
  {
    "text": "compute intensive workloads I don't see it sticking around for a long time right",
    "start": "2882640",
    "end": "2888110"
  },
  {
    "text": "here so the question is what happens if",
    "start": "2888110",
    "end": "2895400"
  },
  {
    "text": "you have multi terabyte objects does it pull down the entire object when you do",
    "start": "2895400",
    "end": "2901550"
  },
  {
    "text": "the when you do the read the answer is yes it does you know we're looking at an optimization where we would you know",
    "start": "2901550",
    "end": "2908060"
  },
  {
    "text": "support some form of bite range pulling portions but for now it does pull the",
    "start": "2908060",
    "end": "2914300"
  },
  {
    "text": "full objectivity great great",
    "start": "2914300",
    "end": "2918250"
  },
  {
    "text": "okay so the question is they have an on-prem luster cluster is it not-it's",
    "start": "2935930",
    "end": "2944490"
  },
  {
    "start": "2936000",
    "end": "3026000"
  },
  {
    "text": "just an on-prem data set in emc isilon on-prem appliance they have all their",
    "start": "2944490",
    "end": "2952109"
  },
  {
    "text": "data petabytes and petabytes of data they want to go and see if they can link that did repository to an Amazon fsx for",
    "start": "2952109",
    "end": "2960329"
  },
  {
    "text": "lustre file system so the the way that you would connect the two is over a",
    "start": "2960329",
    "end": "2967410"
  },
  {
    "text": "Direct Connect connection you would have your on-prem filesystem mounted to a",
    "start": "2967410",
    "end": "2973170"
  },
  {
    "text": "server or several servers and you would mount your lustre filesystem to the same set of servers and then you'd be able to",
    "start": "2973170",
    "end": "2978960"
  },
  {
    "text": "copy data between the two so that's that's what the solution would be it would be more of a more of a manual",
    "start": "2978960",
    "end": "2985799"
  },
  {
    "text": "process than our s3 integration but it is it is doable in that way right back",
    "start": "2985799",
    "end": "2992880"
  },
  {
    "text": "there",
    "start": "2992880",
    "end": "2995088"
  },
  {
    "text": "okay the question is is it possible to have intersect clients or connections between the clients and the resources",
    "start": "3002220",
    "end": "3009060"
  },
  {
    "text": "that support the fsx for leicester file system I don't believe so we can check",
    "start": "3009060",
    "end": "3014940"
  },
  {
    "text": "but I don't believe so so but that's that's also good good feedback if that's",
    "start": "3014940",
    "end": "3020580"
  },
  {
    "text": "a requirement question right here the",
    "start": "3020580",
    "end": "3027570"
  },
  {
    "start": "3026000",
    "end": "3107000"
  },
  {
    "text": "question is can you have multiple file systems or objects linked to the same bucket yes",
    "start": "3027570",
    "end": "3035720"
  },
  {
    "text": "I'll clear if you're writing to the same objects you're saying from both file",
    "start": "3042390",
    "end": "3047650"
  },
  {
    "text": "systems so the way that it works is we",
    "start": "3047650",
    "end": "3052839"
  },
  {
    "text": "actually when you're when you call the when you when you commit the data back to s3 we actually write it to a separate",
    "start": "3052839",
    "end": "3061150"
  },
  {
    "text": "prefix we kind of maintain the Dirk the same prefix structure within that separate prefix but we we write the",
    "start": "3061150",
    "end": "3067630"
  },
  {
    "text": "incremental results back there so that you can then decide if you want to kind of batch move it back to original objects or not so it avoids the",
    "start": "3067630",
    "end": "3073869"
  },
  {
    "text": "conflicts that you're that you know",
    "start": "3073869",
    "end": "3082059"
  },
  {
    "text": "there you can't write here",
    "start": "3082059",
    "end": "3086880"
  },
  {
    "start": "3107000",
    "end": "3186000"
  },
  {
    "text": "yep so the question is during the initial or during the data load of",
    "start": "3107029",
    "end": "3113069"
  },
  {
    "text": "objects there is higher latency when we're actually ingesting this data from the s3 bucket what are we doing to",
    "start": "3113069",
    "end": "3120240"
  },
  {
    "text": "address that to try to improve the performance so couple things so during",
    "start": "3120240",
    "end": "3126779"
  },
  {
    "text": "the initial creation again the data is not moved over it's just the think about",
    "start": "3126779",
    "end": "3131970"
  },
  {
    "text": "it as a filesystem metadata and if you have you know I forget exactly how many objects you yeah we had three hundred",
    "start": "3131970",
    "end": "3139019"
  },
  {
    "text": "seventy-two thousand five hundred seventy-two objects which consumed about twenty two",
    "start": "3139019",
    "end": "3144390"
  },
  {
    "text": "point seven terabytes of data okay so if so couple options so the first is you",
    "start": "3144390",
    "end": "3151500"
  },
  {
    "text": "can actually link your filesystem to a specific prefix within your bucket so",
    "start": "3151500",
    "end": "3156599"
  },
  {
    "text": "you don't have to wait for like everything to load and the second is you know if lazy loading is an option if the",
    "start": "3156599",
    "end": "3162839"
  },
  {
    "text": "if the latencies are tolerable then that's that's a path where you don't need to wait before you know your full",
    "start": "3162839",
    "end": "3169380"
  },
  {
    "text": "data set to move over in order to start processing and you can do the lazy load for you know portions of your your data",
    "start": "3169380",
    "end": "3176190"
  },
  {
    "text": "set as well it doesn't need to be it for the full it's the number the the initial",
    "start": "3176190",
    "end": "3181680"
  },
  {
    "text": "time for moving that for building the metadata is all about the number of files not the size of the files yeah yeah but you could really go from when I",
    "start": "3181680",
    "end": "3189089"
  },
  {
    "start": "3186000",
    "end": "3236000"
  },
  {
    "text": "did my test I had it some some minutes in between the metadata load and then",
    "start": "3189089",
    "end": "3194730"
  },
  {
    "text": "the actual data load but when really you add it up around eleven minutes to create the the hardware or provision the",
    "start": "3194730",
    "end": "3202920"
  },
  {
    "text": "the resources to support the file system another 11 minutes to actually move that metadata in from my 307 3,000 files and",
    "start": "3202920",
    "end": "3211769"
  },
  {
    "text": "then another 32 minutes to to ingest so you're a little over you're under an hour to ingest twenty two point seven",
    "start": "3211769",
    "end": "3219329"
  },
  {
    "text": "terabytes of data from s3 with literally two commands a create file system",
    "start": "3219329",
    "end": "3225150"
  },
  {
    "text": "command and a batch load command right here",
    "start": "3225150",
    "end": "3231060"
  },
  {
    "text": "okay so the question is when you start being charged do you are you charged for",
    "start": "3236450",
    "end": "3241970"
  },
  {
    "text": "the metadata load or once the data has already been pulled in so you're charged",
    "start": "3241970",
    "end": "3248660"
  },
  {
    "text": "by the amount of data you provision for your file system you set that when you create the file system you identify a",
    "start": "3248660",
    "end": "3254510"
  },
  {
    "text": "storage capacity and that's when as we're building this the the resources",
    "start": "3254510",
    "end": "3259940"
  },
  {
    "text": "out you will be billed when it's when it's available the when it's when it's",
    "start": "3259940",
    "end": "3265760"
  },
  {
    "text": "started yeah so so think of it as there's kind of two things that are happening the first is we're provisioning the file system for you",
    "start": "3265760",
    "end": "3272090"
  },
  {
    "text": "once it's provisioned you do start getting billed and then we start moving the metadata in you know usually that's",
    "start": "3272090",
    "end": "3279560"
  },
  {
    "text": "a couple minutes if not less so but you are getting billed during that that time",
    "start": "3279560",
    "end": "3284900"
  },
  {
    "text": "period question back there so the",
    "start": "3284900",
    "end": "3293660"
  },
  {
    "start": "3292000",
    "end": "3330000"
  },
  {
    "text": "question is are there any plans to integrate this with EMR for big data movers I'd love to hear kind of the",
    "start": "3293660",
    "end": "3299870"
  },
  {
    "text": "scenario that you see this being useful for",
    "start": "3299870",
    "end": "3304390"
  },
  {
    "text": "okay so it's good if we can actually if you have time few minutes after I'd love",
    "start": "3313530",
    "end": "3318780"
  },
  {
    "text": "to talk a little bit more with you about that and maybe talk back there good idea",
    "start": "3318780",
    "end": "3326299"
  },
  {
    "start": "3330000",
    "end": "3383000"
  },
  {
    "text": "so the question is is the s3 sinking a standard s3 API operation okay so it is",
    "start": "3330479",
    "end": "3343269"
  },
  {
    "text": "it's the standard limits that you you have for your bucket and for operations associated with your bucket you don't",
    "start": "3343269",
    "end": "3350229"
  },
  {
    "text": "issue that that's a part of the the FS x4 luster service will do that you issue",
    "start": "3350229",
    "end": "3355630"
  },
  {
    "text": "the the command to to ingest that data in and then fsx for lustre does the the",
    "start": "3355630",
    "end": "3362950"
  },
  {
    "text": "magic force all of those you parallel get object API calls do you do you see",
    "start": "3362950",
    "end": "3368979"
  },
  {
    "text": "that as a challenger okay just cares okay yeah okay okay question back there",
    "start": "3368979",
    "end": "3380430"
  },
  {
    "text": "the question is can you link a files can you link multiple s3 buckets to the",
    "start": "3382529",
    "end": "3390219"
  },
  {
    "start": "3383000",
    "end": "3439000"
  },
  {
    "text": "single file system so the answer is no and that's really because we're aiming",
    "start": "3390219",
    "end": "3397809"
  },
  {
    "text": "to do this kind of one-to-one mapping of your your kind of prefixes and your set",
    "start": "3397809",
    "end": "3402940"
  },
  {
    "text": "of objects to the file system metadata so for the direct linking it's to a single bucket however if you have data",
    "start": "3402940",
    "end": "3409630"
  },
  {
    "text": "in multiple buckets that you want on the same file system you can you can copy the you can get the data from your s3",
    "start": "3409630",
    "end": "3416440"
  },
  {
    "text": "you can just make get calls from your client it has the file system mounted get the objects and write them to the file system so it's I mean it's a",
    "start": "3416440",
    "end": "3422799"
  },
  {
    "text": "standard file system and you can just get data from other buckets and kind of that way but the direct linking is with",
    "start": "3422799",
    "end": "3428259"
  },
  {
    "text": "there's a one to one question",
    "start": "3428259",
    "end": "3434009"
  },
  {
    "text": "so the question is as fsx for leicester evolves are we gonna have policy turning",
    "start": "3438520",
    "end": "3444280"
  },
  {
    "start": "3439000",
    "end": "3493000"
  },
  {
    "text": "as a part of it and what what kind of policies would you see as useful",
    "start": "3444280",
    "end": "3450670"
  },
  {
    "text": "I say so basically Penn objects are pinned files so that it can't be it",
    "start": "3459109",
    "end": "3464239"
  },
  {
    "text": "won't be released and just keep it there",
    "start": "3464239",
    "end": "3468819"
  },
  {
    "text": "Invicta okay that's a good idea okay that's",
    "start": "3479190",
    "end": "3485490"
  },
  {
    "text": "Thanks that's good yeah good question back there so the question is how does",
    "start": "3485490",
    "end": "3495059"
  },
  {
    "text": "it scale when you have lots of clients accessing the same file so there's a",
    "start": "3495059",
    "end": "3500160"
  },
  {
    "text": "couple ways to think about that luster provides a options for you to specify at",
    "start": "3500160",
    "end": "3508200"
  },
  {
    "text": "the file level how you stripe your your file across multiple disks and so if you",
    "start": "3508200",
    "end": "3514710"
  },
  {
    "text": "have let's say you know a sizable file and you know that there's gonna be a",
    "start": "3514710",
    "end": "3520230"
  },
  {
    "text": "bunch of of clients accessing different portions of that file you can stripe it",
    "start": "3520230",
    "end": "3525509"
  },
  {
    "text": "across the multiple disks and that's going to reduce the contention that you have on on that file if they're all",
    "start": "3525509",
    "end": "3532920"
  },
  {
    "text": "hitting like exactly the same you know same set of bytes all at exactly the same time there's just laws of physics",
    "start": "3532920",
    "end": "3538470"
  },
  {
    "text": "in terms of you know being able to serve that but lustre really is designed for",
    "start": "3538470",
    "end": "3544920"
  },
  {
    "text": "kind of this like high concurrent access to two single files by supporting the",
    "start": "3544920",
    "end": "3550200"
  },
  {
    "text": "striping so it's I think most people would would say that it does a really good job of supporting that type of",
    "start": "3550200",
    "end": "3556200"
  },
  {
    "text": "access okay we have one minute left so maybe two more questions these two",
    "start": "3556200",
    "end": "3562559"
  },
  {
    "text": "questions and we'll we'll finish it very here",
    "start": "3562559",
    "end": "3566838"
  },
  {
    "text": "you mean if like if it's a new version of sir good repeat the question sure so",
    "start": "3579109",
    "end": "3587059"
  },
  {
    "text": "this is the question is what happens if you you have a an s3 bucket you link to",
    "start": "3587059",
    "end": "3592670"
  },
  {
    "text": "a file system the object in the you imported there the data is actually in",
    "start": "3592670",
    "end": "3599029"
  },
  {
    "text": "the file system you make a change to that object what happens in that scenario you've got an object that has",
    "start": "3599029",
    "end": "3604910"
  },
  {
    "text": "the latest data your file system has in a sense stale data because it's not the",
    "start": "3604910",
    "end": "3611450"
  },
  {
    "text": "same version of the object as you have a new bucket yes so if the object has",
    "start": "3611450",
    "end": "3617029"
  },
  {
    "text": "already been pulled in then it's already on a file system so if you have a new version of the object you would need to",
    "start": "3617029",
    "end": "3624349"
  },
  {
    "text": "and you want it to pull that in and you would delete the file from the lustre file system and do a read again of the",
    "start": "3624349",
    "end": "3629420"
  },
  {
    "text": "object and pull the latest one last question",
    "start": "3629420",
    "end": "3635680"
  },
  {
    "text": "so does it support memory map operations I believe so",
    "start": "3637089",
    "end": "3642859"
  },
  {
    "text": "but I'd have to check I don't know yeah",
    "start": "3642859",
    "end": "3648038"
  },
  {
    "text": "so it yeah it'll load the full file from s3 so",
    "start": "3655630",
    "end": "3662240"
  },
  {
    "text": "that's that's ours yeah it's an optimization we're thinking about yeah yeah all right well thank you so much",
    "start": "3662240",
    "end": "3668930"
  },
  {
    "text": "for your attendance and for all the great questions thank you guys have a great day [Applause]",
    "start": "3668930",
    "end": "3676019"
  }
]