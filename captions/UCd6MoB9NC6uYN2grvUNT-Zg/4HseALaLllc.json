[
  {
    "start": "0",
    "end": "46000"
  },
  {
    "text": "everyone thanks very much for coming my name is Rahul potluck",
    "start": "410",
    "end": "5520"
  },
  {
    "text": "I run the EMR business for AWS so it's really a pleasure to be here to to talk",
    "start": "5520",
    "end": "10980"
  },
  {
    "text": "to you about the service I'm also here with Scott Donaldson and Clayton Kivar from FINRA one of our",
    "start": "10980",
    "end": "17640"
  },
  {
    "text": "customers they'll be speaking about how they're using EMR at scale as a petabyte scale data warehouse in their",
    "start": "17640",
    "end": "23369"
  },
  {
    "text": "environment and just before we get going a quick show of hands how many people",
    "start": "23369",
    "end": "28590"
  },
  {
    "text": "here are existing EMR customers use EMR today ok about two thirds how many",
    "start": "28590",
    "end": "36719"
  },
  {
    "text": "people use Hadoop in some fashion ok just about everyone great and so we'll",
    "start": "36719",
    "end": "44190"
  },
  {
    "text": "get started what we're gonna cover is I'm gonna give you an update on the latest EMR release this is something",
    "start": "44190",
    "end": "50700"
  },
  {
    "start": "46000",
    "end": "46000"
  },
  {
    "text": "that we shipped last week we'll then talk about some of the key capabilities of EMR some of this will be familiar to",
    "start": "50700",
    "end": "57539"
  },
  {
    "text": "some of you but we'll try and get into some actionable tips that you can use in your environments we will also talk a",
    "start": "57539",
    "end": "64768"
  },
  {
    "text": "little bit about how to lower your EMR costs so one of the things about AWS is that we want you to spend as little as",
    "start": "64769",
    "end": "71400"
  },
  {
    "text": "possible to get the work done that you need to and so we'll talk about how to get more value out of the service but",
    "start": "71400",
    "end": "77820"
  },
  {
    "text": "really the bulk of the talk is going to be Scott the senior director of technology at FINRA talking about how they've been able to",
    "start": "77820",
    "end": "84090"
  },
  {
    "text": "use EMR in their environments to run large volume queries on large data sets",
    "start": "84090",
    "end": "89100"
  },
  {
    "text": "at scale in a secure environment and then we will have time for questions at",
    "start": "89100",
    "end": "94140"
  },
  {
    "text": "the end so diving into EMR I think that",
    "start": "94140",
    "end": "99930"
  },
  {
    "text": "most of the room is familiar with it but essentially we are managed clusters for applications in the Hadoop stack and",
    "start": "99930",
    "end": "105540"
  },
  {
    "text": "really this is anything that's in the Apache ecosystem you can use with EMR so",
    "start": "105540",
    "end": "111149"
  },
  {
    "text": "whether it's spark or presto or hive HBase everything will run we also extend",
    "start": "111149",
    "end": "117780"
  },
  {
    "text": "the Hadoop stack with EMR FS which is a file system that essentially looks like",
    "start": "117780",
    "end": "123030"
  },
  {
    "text": "HDFS to the applications and this allows your apps to extend into s3 which is one",
    "start": "123030",
    "end": "129599"
  },
  {
    "text": "of the dominant storage tiers and what we'll talk about a fair bit today but also into die eebee Kinesis and redshift and so you",
    "start": "129599",
    "end": "137430"
  },
  {
    "text": "can run jobs that run across these data stores and run them transparently in",
    "start": "137430",
    "end": "143099"
  },
  {
    "text": "your applications and we also integrate with the AWS security infrastructure so V pcs iam controls encryption at the",
    "start": "143099",
    "end": "151470"
  },
  {
    "text": "object level that we'll talk about on the AWS key management service and from",
    "start": "151470",
    "end": "157920"
  },
  {
    "text": "a security perspective all of the things that you're familiar with in AWS is supported here we also completed HIPAA",
    "start": "157920",
    "end": "165090"
  },
  {
    "text": "eligibility for EMR that was done in July and so if you're running applications that require HIPAA",
    "start": "165090",
    "end": "172220"
  },
  {
    "text": "compliance that's also supported and were covered under the AWS BAA and we can follow up on that if you have",
    "start": "172220",
    "end": "178319"
  },
  {
    "text": "specific questions and and then EMR also integrates with spot who's everyone",
    "start": "178319",
    "end": "184110"
  },
  {
    "text": "familiar with the spot market I could not as many people so we'll talk a",
    "start": "184110",
    "end": "190200"
  },
  {
    "text": "little bit about what that is and how you can leverage it in EMR as well so",
    "start": "190200",
    "end": "195329"
  },
  {
    "start": "194000",
    "end": "194000"
  },
  {
    "text": "last week we released EMR version 4.1 this was actually turned out to be a",
    "start": "195329",
    "end": "201480"
  },
  {
    "text": "pretty big release for us so we added support for transparent hdfs encryption",
    "start": "201480",
    "end": "206609"
  },
  {
    "text": "using the Hadoop kms that allows you to plug in your own key materials provider and so this encrypts if the efest data",
    "start": "206609",
    "end": "214200"
  },
  {
    "text": "it encrypts the shuffle in MapReduce and transit between nodes and it also",
    "start": "214200",
    "end": "219420"
  },
  {
    "text": "encrypts straps based on the local disk so you can actually run a fully secured local environment we added SPARC 1.5 dot",
    "start": "219420",
    "end": "227340"
  },
  {
    "text": "oh and I know 1.51 just came out and we'll add that pretty quickly in the",
    "start": "227340",
    "end": "233459"
  },
  {
    "text": "next next few weeks and we also added Zeppelin Zeppelin if you're not familiar with it is essentially a notebook like",
    "start": "233459",
    "end": "239940"
  },
  {
    "text": "environment where you can run interactive queries and spark do visualizations and collaborate with",
    "start": "239940",
    "end": "245549"
  },
  {
    "text": "other users on the system we added presto presto is sequel on a hadoop",
    "start": "245549",
    "end": "251579"
  },
  {
    "text": "technology that came out of facebook its apache license and it can out great sequel queries directly against s3 so it",
    "start": "251579",
    "end": "259289"
  },
  {
    "text": "lets you run very large scale there and we also add support for air pal and this",
    "start": "259289",
    "end": "264630"
  },
  {
    "text": "is a project from Airbnb which is a friend frontend - - presto and then the hive",
    "start": "264630",
    "end": "273360"
  },
  {
    "text": "one is in there Lucy's in there we added hue which is a user interface for Hadoop and that we improved some api's for",
    "start": "273360",
    "end": "279960"
  },
  {
    "text": "launching and configuring an EMR clusters and so generally focused on ease-of-use from a development",
    "start": "279960",
    "end": "285539"
  },
  {
    "text": "integration environment and will continue to see currency with applications in the Hadoop ecosystem and",
    "start": "285539",
    "end": "292500"
  },
  {
    "text": "then you always have the ability to add anything that's not part of the core distribution using custom install",
    "start": "292500",
    "end": "297990"
  },
  {
    "text": "actions and so if there's custom code that you have or packages or applications those can all be brought",
    "start": "297990",
    "end": "303900"
  },
  {
    "text": "onto the cluster as well in addition to the distribution we also added support",
    "start": "303900",
    "end": "309930"
  },
  {
    "text": "for intelligent resize so if for example you wanted to launch a thousand node cluster instead of having to wait for a",
    "start": "309930",
    "end": "316560"
  },
  {
    "text": "thousand nodes to become available EMR will actually dis incremental e take whatever capacity is there and start the",
    "start": "316560",
    "end": "322020"
  },
  {
    "text": "work on your cluster and then opportunistically add it as it becomes available so your time to getting jobs",
    "start": "322020",
    "end": "327719"
  },
  {
    "text": "executed as much shorter in this environment especially if there's any temporary capacity blitz and then on the",
    "start": "327719",
    "end": "334949"
  },
  {
    "text": "resize down what we added was the ability to wait for work to be drained",
    "start": "334949",
    "end": "340020"
  },
  {
    "text": "from notes if you want to cut your cluster in half we will wait for works two jobs to finish with an optional",
    "start": "340020",
    "end": "345659"
  },
  {
    "text": "timeout on half the nodes before decommissioning them so you can scale down without introducing job latency by",
    "start": "345659",
    "end": "351870"
  },
  {
    "text": "having to retry and then we also as part of this added the ability to scale down your core node so an EMR core nodes are",
    "start": "351870",
    "end": "358979"
  },
  {
    "text": "where you have HDFS storage and you can now grow and shrink your HDFS storage and we just will replicate onto a small",
    "start": "358979",
    "end": "366569"
  },
  {
    "text": "amount of space we'll check to see that you have enough HDFS on your target size to house the data that you have",
    "start": "366569",
    "end": "372569"
  },
  {
    "text": "allocated and then in that scenario we'll be able to resize down so a lot more flexibility in dealing with",
    "start": "372569",
    "end": "378300"
  },
  {
    "text": "elasticity and EMR so one of the core",
    "start": "378300",
    "end": "383699"
  },
  {
    "text": "parts of EMR that makes it interesting and typically different from on-premise Anoop environments is is EMR FS and so",
    "start": "383699",
    "end": "391349"
  },
  {
    "text": "what EMR FS does is it allows you to extend your Hadoop and Apache jobs into",
    "start": "391349",
    "end": "396419"
  },
  {
    "text": "the AWS ecosystem the primary one here is s3 and the biggest benefit of s3 with EMR",
    "start": "396419",
    "end": "402880"
  },
  {
    "start": "400000",
    "end": "400000"
  },
  {
    "text": "is that you can decouple your storage and compute so if you think about classic Hadoop data lakes that might run",
    "start": "402880",
    "end": "409120"
  },
  {
    "text": "on ec2 or on-premise typically your data will get cold pretty quickly and you find yourself adding nodes to your",
    "start": "409120",
    "end": "415660"
  },
  {
    "text": "cluster just to house your data and those nodes aren't really helping you with compute jobs because you're not using that capacity and so by separating",
    "start": "415660",
    "end": "423550"
  },
  {
    "text": "storage and compute and using s3 directly you can essentially have your storage grow infinitely independently of",
    "start": "423550",
    "end": "430360"
  },
  {
    "text": "your compute at s3 storage rates so three cents a gigabyte a month about $360 a terabyte a year which is much",
    "start": "430360",
    "end": "438070"
  },
  {
    "text": "lower cost typically than you can achieve with three-way replicated data sitting in HDFS and as three as you know",
    "start": "438070",
    "end": "444160"
  },
  {
    "text": "is designed for 11 9s of your ability so you don't need to re replicate once your data is there it can be used directly",
    "start": "444160",
    "end": "450460"
  },
  {
    "text": "and with EMR FS you don't have to ingest data into HDFS before you can use it it",
    "start": "450460",
    "end": "457180"
  },
  {
    "text": "can stream directly into your jobs and so you can stream data in stream it out and go that way and so this allows you",
    "start": "457180",
    "end": "463390"
  },
  {
    "text": "to resize your clusters turn them off turn them on without having any impact on your core data sets the other big",
    "start": "463390",
    "end": "470020"
  },
  {
    "text": "benefit is that it allows you to point multiple clusters at the same source of truth which is an s3 so if you have",
    "start": "470020",
    "end": "475930"
  },
  {
    "text": "different departments that want to operate different jobs they can act in isolation their clusters can be charged",
    "start": "475930",
    "end": "481810"
  },
  {
    "text": "to their business units and their departments and they don't affect the core production operations of your environment this also allows you to",
    "start": "481810",
    "end": "488410"
  },
  {
    "text": "split interactive query workloads from ETL type workloads and again gives you more flexibility in how you operate",
    "start": "488410",
    "end": "494670"
  },
  {
    "text": "another big thing and Scott will talk about this as well is it allows you the flexibility to evolve your analytic",
    "start": "494670",
    "end": "501010"
  },
  {
    "text": "infrastructure so if you're you know you're all familiar with the Hadoop ecosystem there's a new version of",
    "start": "501010",
    "end": "506410"
  },
  {
    "text": "something every week or two and and some of these technologies are interesting and you want to experiment with them in",
    "start": "506410",
    "end": "512890"
  },
  {
    "text": "an EMR environment you can just spin up a new cluster with that new technology operate in parallel on your data with",
    "start": "512890",
    "end": "518409"
  },
  {
    "text": "your core production environment and then once you make a decision about which technology you want to adopt you can easily cut over from one to the",
    "start": "518410",
    "end": "524650"
  },
  {
    "text": "other so it buys you a lot of future proofing and option value because you can keep pace as the analytic toolset",
    "start": "524650",
    "end": "531700"
  },
  {
    "text": "evolves your infrastructure can evolve with it we any expensive Reap lat forming a retransfer mation of data and with the",
    "start": "531700",
    "end": "541850"
  },
  {
    "start": "541000",
    "end": "541000"
  },
  {
    "text": "MRFs we've also done some things to make s3 more performant and easier to use and",
    "start": "541850",
    "end": "546920"
  },
  {
    "text": "so there's readout for right consistency so you don't have to worry about eventual consistency in s3 there's also",
    "start": "546920",
    "end": "553790"
  },
  {
    "text": "we've sped up list operations so if you think about s3 buckets with potentially millions of small files listing them as",
    "start": "553790",
    "end": "561230"
  },
  {
    "text": "you're doing jobs can take some time and with the MRFs i'll talk about what we've done we've essentially built a map in",
    "start": "561230",
    "end": "567380"
  },
  {
    "text": "dynamo DB that allows for very fast list operations and so that can speed up job",
    "start": "567380",
    "end": "572540"
  },
  {
    "text": "throughput overall and this is transparent to your applications again you're just writing jobs as you normally",
    "start": "572540",
    "end": "578330"
  },
  {
    "text": "would and I'm going from there and then as you know s3 has three tiers of",
    "start": "578330",
    "end": "583700"
  },
  {
    "text": "encryption there's server side which is checkbox there is you can also do",
    "start": "583700",
    "end": "589460"
  },
  {
    "text": "client-side encryption and you can do client side with keys that you provide either from an auth from an on AWS key",
    "start": "589460",
    "end": "595670"
  },
  {
    "text": "management system or you can integrate with AWS kms and that's all transparently supported through MRFs so",
    "start": "595670",
    "end": "602240"
  },
  {
    "text": "you can run jobs that will read and write encrypted data using your key provider of choice without having to do",
    "start": "602240",
    "end": "608270"
  },
  {
    "text": "a lot of expensive configuration or deal with complexity there and when we talk about transparent to applications if you",
    "start": "608270",
    "end": "614840"
  },
  {
    "text": "think of you know a hive job on HDFS you've got a path to a local data set if",
    "start": "614840",
    "end": "620990"
  },
  {
    "text": "you're going to s3 it's just a simple matter of pointing to the different location you're not actually changing",
    "start": "620990",
    "end": "626900"
  },
  {
    "text": "anything in how you operate so you get the flexibility of s3 and decoupled storage and compute without having to",
    "start": "626900",
    "end": "633460"
  },
  {
    "text": "change or replumb your applications and if you're running technologies like",
    "start": "633460",
    "end": "639920"
  },
  {
    "text": "spark for example on EMR they take advantage of encrypted data on s3 you",
    "start": "639920",
    "end": "645500"
  },
  {
    "text": "can run secure jobs in spark through this layer using spark sequel and have access to the overall ecosystem you can",
    "start": "645500",
    "end": "651980"
  },
  {
    "text": "even write spark queries that traverse s3 DynamoDB and Kinesis streams join",
    "start": "651980",
    "end": "657860"
  },
  {
    "text": "them together or pull those into rdd's so a lot of flexibility in terms of extending into the AWS ecosystem beyond",
    "start": "657860",
    "end": "663920"
  },
  {
    "text": "what sits in HDFS and with the fast ting layer might be a little hard to",
    "start": "663920",
    "end": "669770"
  },
  {
    "text": "read but typically if you've got something like a consider a case of a million objects listing them might take",
    "start": "669770",
    "end": "675440"
  },
  {
    "text": "150 seconds with s3 directly 30 seconds going through a MRFs so you can",
    "start": "675440",
    "end": "681260"
  },
  {
    "text": "dramatically speed up access to jobs using the the tooling that we've",
    "start": "681260",
    "end": "686450"
  },
  {
    "text": "provided and then with EMR FS and support for client-side encryption",
    "start": "686450",
    "end": "691839"
  },
  {
    "start": "689000",
    "end": "689000"
  },
  {
    "text": "essentially in this model you can encrypt data at the object level going into s3 using a key provider of your",
    "start": "691839",
    "end": "699050"
  },
  {
    "text": "choice could be your own crypto could be SafeNet boxes sitting outside of AWS as",
    "start": "699050",
    "end": "704089"
  },
  {
    "text": "I think was talked about in the prior session or it could be the AWS key management service which lets you define",
    "start": "704089",
    "end": "709940"
  },
  {
    "text": "keys set up I am policies to out for those keys connect with other services and audit the use of those keys",
    "start": "709940",
    "end": "716089"
  },
  {
    "text": "those can encrypt your objects the objects are encrypted using envelope encryption and the metadata is a pointer",
    "start": "716089",
    "end": "721640"
  },
  {
    "text": "to the key essentially an EMR FS can then use this transparently to encrypt and decrypt objects transparently for",
    "start": "721640",
    "end": "729260"
  },
  {
    "text": "your processing and so this is capability that's built in and lets you run very secure workloads using EMR now",
    "start": "729260",
    "end": "738260"
  },
  {
    "text": "EMR also HDFS is always available so when you provision clusters you",
    "start": "738260",
    "end": "743510"
  },
  {
    "text": "designate a certain number of core nodes your HDFS exists in that environment and",
    "start": "743510",
    "end": "748899"
  },
  {
    "text": "and you can use them for iterative workloads typically if you think about it s3 is a network access there's an",
    "start": "748899",
    "end": "755209"
  },
  {
    "text": "overhead to that connection if you're reusing very small files of a lot of iterative workloads",
    "start": "755209",
    "end": "761029"
  },
  {
    "text": "you're not amortized costs of the connection over a large volume of data transfer so you might see some latency",
    "start": "761029",
    "end": "766630"
  },
  {
    "text": "relative to running locally in HDFS and so HDFS is still available for you for",
    "start": "766630",
    "end": "772040"
  },
  {
    "text": "those kinds of jobs and you may want to consider technologies like SPARC and pulling that data into memory for it",
    "start": "772040",
    "end": "777950"
  },
  {
    "text": "iterative processing of rdd's that's something that we see more and more and we provide a s3 disk CP utility so this",
    "start": "777950",
    "end": "785450"
  },
  {
    "text": "is a distributed copy from s3 that lets you ingest data into HDFS in parallel",
    "start": "785450",
    "end": "790670"
  },
  {
    "text": "from s3 at very high throughput rates so if you need a PFS it's it's there for",
    "start": "790670",
    "end": "796250"
  },
  {
    "text": "you and you know when you're thinking about storage formats",
    "start": "796250",
    "end": "801620"
  },
  {
    "text": "a wide range of choices available to you again all of this can be supported you choose how you format your data on - on",
    "start": "801620",
    "end": "808550"
  },
  {
    "text": "s3 for consumption our typical advice to customers is when you're using s3 as",
    "start": "808550",
    "end": "814010"
  },
  {
    "text": "your data storage here just bring in your data and whatever format it's being generated in and then process it into",
    "start": "814010",
    "end": "820790"
  },
  {
    "text": "whatever format you need for the systems that you're gonna use to query it and so this way you have an archive of your raw",
    "start": "820790",
    "end": "826130"
  },
  {
    "text": "data you have a data that's ready in a format for processing you know these could be row oriented",
    "start": "826130",
    "end": "831920"
  },
  {
    "text": "column column oriented formats typically your use cases in the nature of your data will determine what you choose",
    "start": "831920",
    "end": "838220"
  },
  {
    "text": "there so for analytic type workloads column formats can be great because typically you're operating analytic",
    "start": "838220",
    "end": "844010"
  },
  {
    "text": "queries on a subset of the columns in a table and so you're able to get better performance because you're only accessing portions of the data row",
    "start": "844010",
    "end": "851420"
  },
  {
    "text": "stores can be great if you're trying to look up all of the information about a particular key and so those options are",
    "start": "851420",
    "end": "856460"
  },
  {
    "text": "all available to you in the s3 environment and if you've got schema that's evolving many of the",
    "start": "856460",
    "end": "861620"
  },
  {
    "text": "self-describing formats like Avro or thrift or protobuf can also help you keep track of schema as it changes",
    "start": "861620",
    "end": "867529"
  },
  {
    "text": "giving you flexibility in that regard and when you're using s3 as your primary",
    "start": "867529",
    "end": "873320"
  },
  {
    "text": "data store what we recommend is use the hive meta store and have it running in something like RDF so that way it's",
    "start": "873320",
    "end": "879290"
  },
  {
    "text": "running and persistent and highly available off cluster and this allows you to then spin up spin down multiple",
    "start": "879290",
    "end": "884630"
  },
  {
    "text": "clusters pointed the same hive meta store and then have access to all the tables and data sitting on s3 other",
    "start": "884630",
    "end": "893330"
  },
  {
    "start": "893000",
    "end": "893000"
  },
  {
    "text": "factors to think about are the tool sets that you're using and so different technologies will work better or worse",
    "start": "893330",
    "end": "898700"
  },
  {
    "text": "with different data formats and as with all things databases you've really got",
    "start": "898700",
    "end": "904310"
  },
  {
    "text": "to just try it out and see what works best so for example in the case of presto in the earlier session Nasdaq",
    "start": "904310",
    "end": "911510"
  },
  {
    "text": "found that with the encrypted data Parkay files worked better than orz and",
    "start": "911510",
    "end": "917720"
  },
  {
    "text": "it just depends on the technology that you use and you always have the option to experiment with multiple variants and",
    "start": "917720",
    "end": "924140"
  },
  {
    "text": "then set set your sights on the ones that work best for your workloads one",
    "start": "924140",
    "end": "930020"
  },
  {
    "start": "930000",
    "end": "930000"
  },
  {
    "text": "other tip when dealing with s3 is that small files can be problematic we talked about the fixed overhead of get that data and so you really want to",
    "start": "930020",
    "end": "938580"
  },
  {
    "text": "avoid small files where possible and typically have files that map to your block size because ultimately those",
    "start": "938580",
    "end": "945300"
  },
  {
    "text": "files will feed into a mapper that mapper is gonna be a JVM spinning that up also has a fixed cost and you want to",
    "start": "945300",
    "end": "951570"
  },
  {
    "text": "try and amortize that over larger volumes of data to maximize the overall throughput of the cluster and with small",
    "start": "951570",
    "end": "957840"
  },
  {
    "text": "files you have a couple of options you can either shrink the HDFS block size and that can be done when you're",
    "start": "957840",
    "end": "963480"
  },
  {
    "start": "958000",
    "end": "958000"
  },
  {
    "text": "spinning up the cluster or adding nodes to it or the what we think is the better",
    "start": "963480",
    "end": "968670"
  },
  {
    "text": "option is used as three disk CP to combine the files into sizes that are large enough to operate efficiently I",
    "start": "968670",
    "end": "974310"
  },
  {
    "text": "think the default block size we provide an EMR is 128 megabytes and you want to use that wherever you can another tip",
    "start": "974310",
    "end": "983400"
  },
  {
    "text": "when dealing with s3 and s3 back data stores is compression generally speaking compression is the",
    "start": "983400",
    "end": "989160"
  },
  {
    "text": "way to go because the savings of the network transfer more than offsets the cost of encryption and decryption sorry",
    "start": "989160",
    "end": "996150"
  },
  {
    "text": "have compression and decompression and you know the choices in that scenario",
    "start": "996150",
    "end": "1001220"
  },
  {
    "text": "are thinking about when you're thinking about what compression types to use ultimately it's a trade-off between time",
    "start": "1001220",
    "end": "1007310"
  },
  {
    "text": "and space so if your space sensitive then you want to use something that compresses more efficiently if you're",
    "start": "1007310",
    "end": "1013460"
  },
  {
    "start": "1008000",
    "end": "1008000"
  },
  {
    "text": "time-sensitive you can use something where speed is paramount the tables there for reference one other",
    "start": "1013460",
    "end": "1019280"
  },
  {
    "text": "thing to think about is the split ability of your compression so typically if you have very large files you want to",
    "start": "1019280",
    "end": "1025850"
  },
  {
    "text": "think about splittable compression models so that you can provide splits to the mappers and not have to essentially",
    "start": "1025850",
    "end": "1032120"
  },
  {
    "text": "bottleneck while processing one large file because that data skew will lead to essentially your cluster not operating with as much parallel",
    "start": "1032120",
    "end": "1038510"
  },
  {
    "text": "throughput as possible you're effectively hot spotting and then from a",
    "start": "1038510",
    "end": "1044240"
  },
  {
    "start": "1044000",
    "end": "1044000"
  },
  {
    "text": "cost perspective we really do want you to spend as little as possible in our services and as three is your persistent",
    "start": "1044240",
    "end": "1051170"
  },
  {
    "text": "data store automatically saves we've seen customers save 70 to 90 percent relative to keeping data live in HDFS",
    "start": "1051170",
    "end": "1058630"
  },
  {
    "text": "simply because you don't have to replicate it you can shrink down your compute if you're running batch",
    "start": "1058630",
    "end": "1064040"
  },
  {
    "text": "workloads that compute doesn't have to be always on so for example climate corporation which does weather",
    "start": "1064040",
    "end": "1071270"
  },
  {
    "text": "modeling for simulation for selling weather insurance so they run simulations using soil samples weather",
    "start": "1071270",
    "end": "1077720"
  },
  {
    "text": "sample is to try and predict what they should price their contracts at and they run these simulations everyday so they",
    "start": "1077720",
    "end": "1083000"
  },
  {
    "text": "actually spin up several thousand node cluster nightly to rerun the scenarios process data from s3 simulate right data",
    "start": "1083000",
    "end": "1090500"
  },
  {
    "text": "back out to s3 and that cluster only runs for a few hours and so that flexibility with s3 is your data store",
    "start": "1090500",
    "end": "1097190"
  },
  {
    "text": "just saves you money across the board and with spot so we talked about spot",
    "start": "1097190",
    "end": "1102260"
  },
  {
    "text": "briefly so the spot market on ec2 is a market for unused ec2 capacity and you",
    "start": "1102260",
    "end": "1107750"
  },
  {
    "text": "can get up to a 90% discount relative to on-demand rates with spot and essentially it's it's a market so you",
    "start": "1107750",
    "end": "1114950"
  },
  {
    "text": "pick a bid price the market price fluctuates based on supply and demand and with EMR you can build in",
    "start": "1114950",
    "end": "1122210"
  },
  {
    "text": "essentially a bidding strategy you can pick a max bid price you can pick the number of nodes you want to operate at",
    "start": "1122210",
    "end": "1127340"
  },
  {
    "text": "that price and so if you have your cluster size for 10 nodes because that's your SLA and it gets your job done by 2",
    "start": "1127340",
    "end": "1133490"
  },
  {
    "text": "a.m. but you want to opportunistically add up to 10 more at 90 percent off when",
    "start": "1133490",
    "end": "1139190"
  },
  {
    "text": "that's available EMR will add those to your cluster your job gets done in half the time you're lower and you have a",
    "start": "1139190",
    "end": "1144650"
  },
  {
    "text": "lower average cost per job and so this is just capability that's built in and if you're not using spot for batch",
    "start": "1144650",
    "end": "1150650"
  },
  {
    "text": "processing workloads you're definitely probably paying more than you need to so we definitely recommend taking advantage",
    "start": "1150650",
    "end": "1156200"
  },
  {
    "text": "of that and then if you have steady-state workloads for always-on ad hoc clusters or interactive models than",
    "start": "1156200",
    "end": "1163310"
  },
  {
    "text": "EMR works perfectly with ec2 reserved instances and we even have customers",
    "start": "1163310",
    "end": "1168320"
  },
  {
    "text": "that will use their instances for app servers during the day and then at night when they're not serving as much traffic",
    "start": "1168320",
    "end": "1173570"
  },
  {
    "text": "they'll bring those instances into their EMR clusters and we can opportunistically take advantage of that capacity so wherever possible",
    "start": "1173570",
    "end": "1180530"
  },
  {
    "text": "you can match costs to your workloads and now with that I'm going to turn over to Scott and we'll walk you through how",
    "start": "1180530",
    "end": "1187220"
  },
  {
    "text": "FINRA has been using EMR at scale and then we'll come back for questions so thank you",
    "start": "1187220",
    "end": "1193120"
  },
  {
    "text": "Thank You Rahul before I get started I just do want to give a great deal of",
    "start": "1198110",
    "end": "1204270"
  },
  {
    "text": "thanks to rahul in the EMR team we started on our migration of moving to the cloud started experimenting about",
    "start": "1204270",
    "end": "1210810"
  },
  {
    "text": "two years ago and really about a year year and a half ago came into a full force of moving our market data and the",
    "start": "1210810",
    "end": "1217530"
  },
  {
    "text": "EMR team both in terms of the sport product feature class and the engineering staff have been just",
    "start": "1217530",
    "end": "1223580"
  },
  {
    "text": "top-notch in terms of making this journey possible so so before we get",
    "start": "1223580",
    "end": "1230070"
  },
  {
    "text": "started let me just give a quick background of who is Fenner and what is it that we we do so fen Rosa we ran on",
    "start": "1230070",
    "end": "1237860"
  },
  {
    "text": "we're a private regulator of the financial markets we bring in data from",
    "start": "1237860",
    "end": "1244200"
  },
  {
    "text": "all of the equities exchanges or 99% of the equities exchanges and over two-thirds of the options exchanges as",
    "start": "1244200",
    "end": "1250590"
  },
  {
    "text": "well as regulate the fixed income markets when you look at that from a dollar volume perspective that",
    "start": "1250590",
    "end": "1255900"
  },
  {
    "text": "represents over 600 billion dollars of trading activity a day and up to 75 billion events per day that we bring in",
    "start": "1255900",
    "end": "1262740"
  },
  {
    "text": "that we're ingesting and these rows are typically quite large they're order reports and very detailed some of these",
    "start": "1262740",
    "end": "1269040"
  },
  {
    "text": "records have anywhere from 200 to 300 attributes and these can be quite wide in terms of the details that we have on",
    "start": "1269040",
    "end": "1276150"
  },
  {
    "text": "that and currently within s3 we're storing over five petabytes of that data currently online and we continue every",
    "start": "1276150",
    "end": "1282270"
  },
  {
    "text": "day to add more and more to that what we do with that data is we reconstruct it",
    "start": "1282270",
    "end": "1288060"
  },
  {
    "text": "after the fact and what we look at are any ways of looking for basically market integrity and looking for malfeasance in",
    "start": "1288060",
    "end": "1294750"
  },
  {
    "text": "the market so these things can range from items such as layering and spoofing any sort of market manipulation in that",
    "start": "1294750",
    "end": "1300990"
  },
  {
    "text": "regard also to items like insider trading and fraud a recent example of what would the",
    "start": "1300990",
    "end": "1307410"
  },
  {
    "text": "work that we do at FINRA there was a big press release a little over a month ago from the SEC on an insider trading case",
    "start": "1307410",
    "end": "1313440"
  },
  {
    "text": "that was over a hundred million dollars that involved 32 traders that were ended up being charged between the US Canada",
    "start": "1313440",
    "end": "1320100"
  },
  {
    "text": "and in Eastern Europe and what that was was an insider trading ring where some Ukrainian hackers were hacking into news",
    "start": "1320100",
    "end": "1326580"
  },
  {
    "text": "wire services getting into basically unannounced public",
    "start": "1326580",
    "end": "1331740"
  },
  {
    "text": "earnings announcements feeding that information back over the traders they would open up accounts take an advantage position on that sell it and they ended",
    "start": "1331740",
    "end": "1338730"
  },
  {
    "text": "up with over a hundred million dollars in illegal profits what FINRA does is we our jurisdiction",
    "start": "1338730",
    "end": "1344370"
  },
  {
    "text": "is not that that falls into the area of the SEC the FBI and other foreign agencies like the FSA but we're sort of",
    "start": "1344370",
    "end": "1350430"
  },
  {
    "text": "one of the front lines of defense on the US markets in that regard so and our",
    "start": "1350430",
    "end": "1357960"
  },
  {
    "start": "1356000",
    "end": "1356000"
  },
  {
    "text": "move to the cloud EMR is ubiquitous within our architecture and before we kind of jump into where we're at we",
    "start": "1357960",
    "end": "1364110"
  },
  {
    "text": "should understand where we came from one of the aspects of where we are running within our own data center it's probably",
    "start": "1364110",
    "end": "1369990"
  },
  {
    "text": "a familiar sort of format that many anybody working in big data has we used",
    "start": "1369990",
    "end": "1375780"
  },
  {
    "text": "a variety of very disparate relational databases of Oracle and sand we also had",
    "start": "1375780",
    "end": "1381600"
  },
  {
    "text": "a variety of different data warehouse appliances and we had many of these boxes one of the issue of bringing in",
    "start": "1381600",
    "end": "1387450"
  },
  {
    "text": "this much data and on any particular day the market volatility can change you we",
    "start": "1387450",
    "end": "1392790"
  },
  {
    "text": "can end up with spikes where the volume can Lily double in the span of a day or two and being able to scale to that",
    "start": "1392790",
    "end": "1397950"
  },
  {
    "text": "capacity and provision that much capital is very cost prohibitive and so what we",
    "start": "1397950",
    "end": "1404130"
  },
  {
    "text": "need to be able to do is we were always optimizing on a couple of different variables of both of space and storage and compute and often dealing with a lot",
    "start": "1404130",
    "end": "1411330"
  },
  {
    "text": "of trade-offs and that sometimes we were are we optimizing for our ETL or for our series of batch analytics or sometimes",
    "start": "1411330",
    "end": "1417750"
  },
  {
    "text": "we would be disadvantaging in terms of interactive queries because you're optimizing around these other variables",
    "start": "1417750",
    "end": "1423300"
  },
  {
    "text": "as opposed to laying out the data and optimizing as to what is the particular use case that you're trying to do when",
    "start": "1423300",
    "end": "1429870"
  },
  {
    "text": "we moved over as we're moving up to the cloud we wanted to remove and break that constraint and move through EMR and s3",
    "start": "1429870",
    "end": "1435450"
  },
  {
    "text": "we're able to achieve that so what we have is that we use EMR literally throughout our entire architecture",
    "start": "1435450",
    "end": "1441030"
  },
  {
    "text": "everywhere from where we bring in the data to where we are doing ETL and data ingest and ingress and data",
    "start": "1441030",
    "end": "1447150"
  },
  {
    "text": "normalization we run a variety of batch analytic programs every day we're constantly looking at daily weekly",
    "start": "1447150",
    "end": "1452820"
  },
  {
    "text": "monthly quarterly annually look-back periods around us looking for these items like layering and spoofing and",
    "start": "1452820",
    "end": "1458940"
  },
  {
    "text": "insider trading and then ultimately what we do with that is generate a series of events",
    "start": "1458940",
    "end": "1464160"
  },
  {
    "text": "that a regulatory analyst looked at and now they need to actually investigate this so there's a workflow around this",
    "start": "1464160",
    "end": "1469260"
  },
  {
    "text": "they have to go they have to investigate this and they use things where we provide various different query clusters",
    "start": "1469260",
    "end": "1474390"
  },
  {
    "text": "sometimes they're doing targeted queries other times they're doing very large-scale investigations and they need",
    "start": "1474390",
    "end": "1479970"
  },
  {
    "text": "a wide or a very large slice of the market in that case we use EMR and hive and we put that into a redshift and",
    "start": "1479970",
    "end": "1486480"
  },
  {
    "text": "create little private data Mart's for the users so now they can query on that particular slice and these slices are",
    "start": "1486480",
    "end": "1492120"
  },
  {
    "text": "taking Lily from several trillions that we have an s3 several trillion several",
    "start": "1492120",
    "end": "1497490"
  },
  {
    "text": "trillion records within our database within s3 putting that and we create slices that might range from a couple",
    "start": "1497490",
    "end": "1504060"
  },
  {
    "text": "hundred thousand rows to actually several billion rows we've created data Mart's for individual users that have",
    "start": "1504060",
    "end": "1509850"
  },
  {
    "text": "ranged in the two hundred and fifty billion and they're still doing that for an analysis where they're doing that over a range of time so at for us at",
    "start": "1509850",
    "end": "1520200"
  },
  {
    "start": "1518000",
    "end": "1518000"
  },
  {
    "text": "FINRA as Rahul is alluding to it's always about like the data right that's",
    "start": "1520200",
    "end": "1525480"
  },
  {
    "text": "ultimately what we're trying to be able to do an s3 is that durable source of record and what we do with that is and",
    "start": "1525480",
    "end": "1532140"
  },
  {
    "text": "then EMR FF provides that consistent view for us the key for us is what we do is that we've separated what we're doing",
    "start": "1532140",
    "end": "1538980"
  },
  {
    "text": "in terms of archiving the data versus what we do in terms of query for use and we'll create two or three copies of that",
    "start": "1538980",
    "end": "1545010"
  },
  {
    "text": "data and have it partitioned in different ways on top of an archival format and to help manage on orchestrate",
    "start": "1545010",
    "end": "1551730"
  },
  {
    "text": "all of this Finnair has created what we call our fin or data manager and actually yesterday peer of mine tigran",
    "start": "1551730",
    "end": "1558530"
  },
  {
    "text": "announced yesterday during the s3 we've Fenner has open source staff in our data manager it's called herd and you can",
    "start": "1558530",
    "end": "1565260"
  },
  {
    "text": "actually find that out on github to be able to pull that and what that does is provides a unified catalog of all of our",
    "start": "1565260",
    "end": "1571170"
  },
  {
    "text": "data objects and data sets across s3 and wherever they're being provisioned it provides lineage and tracking and usage",
    "start": "1571170",
    "end": "1577260"
  },
  {
    "text": "and it also allows us to do cluster management so we've abstracted all these different services at an infrastructure",
    "start": "1577260",
    "end": "1582540"
  },
  {
    "text": "layer so that we don't actually have to so individual development teams don't need to be able to be worried about that",
    "start": "1582540",
    "end": "1587670"
  },
  {
    "text": "and they can focus on the matter at hand of what their particular item that they're trying to develop along with",
    "start": "1587670",
    "end": "1594510"
  },
  {
    "start": "1592000",
    "end": "1592000"
  },
  {
    "text": "that you have to look at like what is the right file format and the right compression for us because",
    "start": "1594510",
    "end": "1600419"
  },
  {
    "text": "we're dealing with such volumes of data within the archive what we selected was visa because of the high compression",
    "start": "1600419",
    "end": "1606000"
  },
  {
    "text": "that's really out there we keep it in s3 for a short period of time and then move that over in the glacier and I think at",
    "start": "1606000",
    "end": "1611490"
  },
  {
    "text": "this time we're sitting with not just we have over five petabytes of data just sitting inside of Glacier and several",
    "start": "1611490",
    "end": "1617460"
  },
  {
    "text": "other and several more petabytes over an s3 as we archive this data our requirements as a regulator of these",
    "start": "1617460",
    "end": "1623190"
  },
  {
    "text": "financial markets is that we have to keep this data for over seven years and we have to keep full fidelity on these",
    "start": "1623190",
    "end": "1628350"
  },
  {
    "text": "records you can't bring a legal case against somebody and be missing one record out of a million when you're trying to prove a case that's the first",
    "start": "1628350",
    "end": "1634830"
  },
  {
    "text": "thing the lawyer is going to point at and get the case dismissed so that chain of custody and that",
    "start": "1634830",
    "end": "1639840"
  },
  {
    "text": "evidence control is critical and paramount with everything that we do once we have that data an archive format",
    "start": "1639840",
    "end": "1646169"
  },
  {
    "text": "we under we take a look at what are the usages that we're doing of that data and then put it into the right format and",
    "start": "1646169",
    "end": "1652169"
  },
  {
    "text": "partition it and lay it out for the particular purpose that we're trying to serve now some of the nuances that we",
    "start": "1652169",
    "end": "1658950"
  },
  {
    "start": "1658000",
    "end": "1658000"
  },
  {
    "text": "have within our data is that we don't get all the data for a particular event date on the date we received it so we",
    "start": "1658950",
    "end": "1665610"
  },
  {
    "text": "had some unique challenges as we are laying out and trying to partition looking at that and optimizing our query",
    "start": "1665610",
    "end": "1671059"
  },
  {
    "text": "so what we have in the in the equities market for instance we have over 20,000",
    "start": "1671059",
    "end": "1677130"
  },
  {
    "text": "ticker symbols being traded between NASDAQ nicey and over-the-counter we have over 10,000 firms that are trading",
    "start": "1677130",
    "end": "1683429"
  },
  {
    "text": "on these activities that range from the Goldman Sachs Bank of America's and Merrill Lynch's to a mom-and-pop shop that literally you know a two-person",
    "start": "1683429",
    "end": "1690299"
  },
  {
    "text": "firm that operates out of their house there are participants in these markets so if you're looking at that from a",
    "start": "1690299",
    "end": "1695970"
  },
  {
    "text": "partitioning we always require a date and an and or an issue or a firm typically even with most of our query",
    "start": "1695970",
    "end": "1701880"
  },
  {
    "text": "use cases that would represent over 200 million partitions and our data is highly skewed because Bank of America",
    "start": "1701880",
    "end": "1707700"
  },
  {
    "text": "treating and Amazon stock is much higher than some small mom-and-pop shop trading in and over-the-counter security so what",
    "start": "1707700",
    "end": "1714450"
  },
  {
    "text": "we ended up doing is that that violates the one tenant which is having small files out on s3 so what we ended up",
    "start": "1714450",
    "end": "1720929"
  },
  {
    "text": "coming up with is a hashing algorithm both where we grouped the issue symbols into over a hundred different buckets",
    "start": "1720929",
    "end": "1726450"
  },
  {
    "text": "and looking at the number of firms and given the size of that we ended up kind of like with our sweet spot of",
    "start": "1726450",
    "end": "1732240"
  },
  {
    "text": "trying to have our data file sizes within these buckets to be in the",
    "start": "1732240",
    "end": "1737370"
  },
  {
    "text": "hundred and twenty-eight to 256 megabytes range and that provided us sort of the throughput that we are",
    "start": "1737370",
    "end": "1742590"
  },
  {
    "text": "looking for that optimize within there are various different queries so as you were doing any sort of queries around",
    "start": "1742590",
    "end": "1747809"
  },
  {
    "text": "these if you're doing a symbol query you would hit at most ten partitions plus a late partition so we get this 0.03",
    "start": "1747809",
    "end": "1754470"
  },
  {
    "text": "percent of the data that can trickle in slowly over the course of a year to two years and every day we have to continue",
    "start": "1754470",
    "end": "1760350"
  },
  {
    "text": "to add that into it to make sure that it's relevant within that query if you're doing a firm query you had hit a",
    "start": "1760350",
    "end": "1765720"
  },
  {
    "text": "hundred and one partitions as opposed to the millions of millions of partitions that you have and if you're doing a simple query you'd hit one part you",
    "start": "1765720",
    "end": "1771720"
  },
  {
    "text": "would actually hit just two partitions the single one with all of the late data so one of the things that we had",
    "start": "1771720",
    "end": "1778110"
  },
  {
    "text": "experimented in hi was actually trying to use bucketing but that didn't actually work with sort of the",
    "start": "1778110",
    "end": "1783179"
  },
  {
    "text": "multi-dimensional analysis that we were trying to be able to do with this nor did it work with sort of Union all what",
    "start": "1783179",
    "end": "1788580"
  },
  {
    "text": "we are doing is that oftentimes we are merging orders information trading and quoting so a variety of different event",
    "start": "1788580",
    "end": "1794760"
  },
  {
    "text": "types and we needed to be able to Union these across different time so an",
    "start": "1794760",
    "end": "1799830"
  },
  {
    "start": "1799000",
    "end": "1799000"
  },
  {
    "text": "example of creating that hive table is that we would create it we would in our",
    "start": "1799830",
    "end": "1804899"
  },
  {
    "text": "case we had chosen to use orc and this time with snappy compression so snappy gives you very fast compress and",
    "start": "1804899",
    "end": "1811409"
  },
  {
    "text": "decompress and we were using orc with all these partitions and we would create these and we would have basically the",
    "start": "1811409",
    "end": "1816750"
  },
  {
    "text": "table name the event date and then actually the hash partition which define that particular table basically that",
    "start": "1816750",
    "end": "1822029"
  },
  {
    "text": "filed and represented that partitioning scheme now what we were competing against as I was stating earlier was the",
    "start": "1822029",
    "end": "1829620"
  },
  {
    "text": "data appliances and so you hear a lot of questions around like hive and the performance of what you're able to provide so the incumbent in our regard",
    "start": "1829620",
    "end": "1837360"
  },
  {
    "text": "that we were battling against were the types of queries we were running on our data warehouse appliance is our initial release that we put out in February this",
    "start": "1837360",
    "end": "1843840"
  },
  {
    "text": "year we were in the ballpark we spent a lot of time partitioning and laying out and testing and tuning both the query",
    "start": "1843840",
    "end": "1850590"
  },
  {
    "text": "and the cluster around this and we got it within the ballpark and we were able to go out the door with that now",
    "start": "1850590",
    "end": "1855799"
  },
  {
    "text": "partitioning is great but right literally three days before we are going",
    "start": "1855799",
    "end": "1861000"
  },
  {
    "start": "1856000",
    "end": "1856000"
  },
  {
    "text": "live with this we are back processing and adding more data literally we have tens of millions of partitions in our high meta",
    "start": "1861000",
    "end": "1867360"
  },
  {
    "text": "store where we're loading three or four years worth of this data and a typical query that we were running we had",
    "start": "1867360",
    "end": "1873269"
  },
  {
    "text": "embedded the hash partitioning into the sequel query itself so you didn't have to enumerate problem with that is that",
    "start": "1873269",
    "end": "1879179"
  },
  {
    "text": "the hive meta store can't actually interpret the p-mod functions in terms",
    "start": "1879179",
    "end": "1884639"
  },
  {
    "text": "of that so what it was doing is that the hive meta store couldn't do that predicate push down that Lily returned millions of partitions back to the hive",
    "start": "1884639",
    "end": "1891749"
  },
  {
    "text": "server and all of a sudden a query that was taking 30 45 seconds and now was taking 10 minutes 2 hours because of the",
    "start": "1891749",
    "end": "1898049"
  },
  {
    "text": "number of partitions that actually had the prune in the hive server to process so what we ended up having to do is",
    "start": "1898049",
    "end": "1903720"
  },
  {
    "start": "1902000",
    "end": "1902000"
  },
  {
    "text": "literally two days before launch around this is refactor our code and come back in and actually layout and specifically",
    "start": "1903720",
    "end": "1910710"
  },
  {
    "text": "enumerate the hash partitions and within that we couldn't use an in clause because that was just sort of the",
    "start": "1910710",
    "end": "1916080"
  },
  {
    "text": "limitation within hive so as you're building this data you always have to be aware of what are the limitations of the",
    "start": "1916080",
    "end": "1922769"
  },
  {
    "text": "particular tools that you're using and what are the different query use cases around the app now putting the data up",
    "start": "1922769",
    "end": "1929309"
  },
  {
    "start": "1927000",
    "end": "1927000"
  },
  {
    "text": "there is great and being able to query it but it's financial records and they have to be secured Nate Salmons was",
    "start": "1929309",
    "end": "1935789"
  },
  {
    "text": "actually from Nasdaq was in the prior session and he spoke actually quite at depth at this and we use many of the",
    "start": "1935789",
    "end": "1942119"
  },
  {
    "text": "same techniques that Nasdaq is doing where they are actually providing this data one is that we require all of our",
    "start": "1942119",
    "end": "1948869"
  },
  {
    "text": "data at rest to be encrypted all of our data in flight is encrypted and all our data on inside of a server has to be",
    "start": "1948869",
    "end": "1954960"
  },
  {
    "text": "encrypted so s3 we had internally walked our early on within our process of",
    "start": "1954960",
    "end": "1961409"
  },
  {
    "text": "moving to the cloud we involved are in first information security folks and they came up with the assessment we had",
    "start": "1961409",
    "end": "1967289"
  },
  {
    "text": "to sell that not only to our IT management but our business management to all the exchanges that we regulate so",
    "start": "1967289",
    "end": "1973379"
  },
  {
    "text": "Nasdaq nicey the CBOE as well as the SEC and to our Board of Governors which represents all of the firms that oversee",
    "start": "1973379",
    "end": "1980009"
  },
  {
    "text": "what FINRA does and what we had found is that we actually believe that running ourselves up on the cloud was actually",
    "start": "1980009",
    "end": "1986669"
  },
  {
    "text": "more secure than actually running it within our own data centers that we were operating given the amount of data that",
    "start": "1986669",
    "end": "1991799"
  },
  {
    "text": "we bring up and shuffle and move around one of the aspects and sorry I forgot the touch on within",
    "start": "1991799",
    "end": "1998730"
  },
  {
    "text": "our architecture is that every day that we're very very transient we bring up anywhere from nine thousand to twelve",
    "start": "1998730",
    "end": "2005240"
  },
  {
    "text": "thousand nodes per day where we're doing these analytics in the batch and the ETL and those nodes go away at the end of",
    "start": "2005240",
    "end": "2012049"
  },
  {
    "text": "the day so we're constantly bringing up and shutting down on a daily basis the ones we do have a server up we're using",
    "start": "2012049",
    "end": "2019040"
  },
  {
    "text": "Lux encryption with a random memory key and so if we were to have to lose it a particular node or lose a server you've",
    "start": "2019040",
    "end": "2025490"
  },
  {
    "text": "lost that data but it doesn't really matter for us because the golden source of truth is always sitting down in s3 and it's sitting in those external",
    "start": "2025490",
    "end": "2031760"
  },
  {
    "text": "tables we just reinitialize the cluster and bring it back up and do it again and then we also use security groups and I",
    "start": "2031760",
    "end": "2037730"
  },
  {
    "text": "am and now with the advent of the Hadoop transparent encryption that's now in the",
    "start": "2037730",
    "end": "2043580"
  },
  {
    "text": "EMR for one we're going to be evaluating that and looking at taking advantage of a lot of those different features at VM",
    "start": "2043580",
    "end": "2048980"
  },
  {
    "text": "our team that provides for us now we go back to why did we end up selecting EMR",
    "start": "2048980",
    "end": "2054830"
  },
  {
    "start": "2049000",
    "end": "2049000"
  },
  {
    "text": "in the first place we had been running internal Hadoop ourselves and we're looking at doing running Hadoop up in",
    "start": "2054830",
    "end": "2061520"
  },
  {
    "text": "the cloud some of our tables I was talking of our different events one or just one of our tables is over six",
    "start": "2061520",
    "end": "2067490"
  },
  {
    "text": "hundred terabytes to represent that data so to just put that one table in the",
    "start": "2067490",
    "end": "2072590"
  },
  {
    "text": "HDFS on a set of ETA two clusters would have taken at least thirty D to a deck cells that would represents over a",
    "start": "2072590",
    "end": "2078858"
  },
  {
    "text": "million and a half and cost annually to be able to operate that cluster whereas storing that data on s3 and",
    "start": "2078859",
    "end": "2084800"
  },
  {
    "text": "bringing up an D telling it or querying it when we needed it to it was 120 thousand dollars is what we're paying",
    "start": "2084800",
    "end": "2089929"
  },
  {
    "text": "effectively on that in terms of the storage and the clusters when we extend that to our other data sets that we have",
    "start": "2089929",
    "end": "2095240"
  },
  {
    "text": "we're you know we're tripling over the number of nodes of these high capacity nodes that you need so it goes back to",
    "start": "2095240",
    "end": "2101840"
  },
  {
    "text": "the point of having that separation of the storage versus the compute we don't need that much compute power for the",
    "start": "2101840",
    "end": "2107450"
  },
  {
    "text": "types of queries that were typically are running that we don't need that much because the queries age over time",
    "start": "2107450",
    "end": "2112760"
  },
  {
    "text": "there's most of the volume that they're looking at is over the first few months but we have to be able to query any of the data that goes back up to seven",
    "start": "2112760",
    "end": "2119150"
  },
  {
    "text": "years the other item that goes along with that you'll always hear the argument of within Hadoop is that you",
    "start": "2119150",
    "end": "2125180"
  },
  {
    "text": "have to have data locality now if you perfectly sort of lay out your data and get",
    "start": "2125180",
    "end": "2130390"
  },
  {
    "text": "all the data locality which is a ton of work you might be able to achieve up to like two times performance over what",
    "start": "2130390",
    "end": "2136210"
  },
  {
    "text": "you're doing in general where you're just laying out the partitions and working through external tables what we",
    "start": "2136210",
    "end": "2141250"
  },
  {
    "text": "saw is about a 15 to an 18 percent degradation in performance in general unless you were willing just put in all",
    "start": "2141250",
    "end": "2146529"
  },
  {
    "text": "of that extra effort to get perfect data locality within your HDFS nodes so you",
    "start": "2146529",
    "end": "2151779"
  },
  {
    "text": "know the trade-off of that given the cost was well worth it of using EMR and s3 one of the other items that we",
    "start": "2151779",
    "end": "2158710"
  },
  {
    "text": "considered to help deal with that or where we do have our most it is the most",
    "start": "2158710",
    "end": "2164079"
  },
  {
    "text": "recent data is typically the most queried data we debated actually storing that data into locally HDFS using s3",
    "start": "2164079",
    "end": "2170529"
  },
  {
    "text": "disk copy and putting that in there but that provided an operational issue that as we had that cluster the partition",
    "start": "2170529",
    "end": "2177010"
  },
  {
    "text": "maintenance you couldn't actually have a separate hive meta store around that it actually had to be on cluster and so hydrating the cluster with millions of",
    "start": "2177010",
    "end": "2183490"
  },
  {
    "text": "millions of partitions the initialization of that cluster could actually take hours to onboard that whereas if you already connect to an",
    "start": "2183490",
    "end": "2189549"
  },
  {
    "text": "existing hive meta store that connection and materialized in a matter of minutes one of the other beauties of just moving",
    "start": "2189549",
    "end": "2196000"
  },
  {
    "start": "2193000",
    "end": "2193000"
  },
  {
    "text": "on to the cloud one the advantages that we're looking to take advantage of was the ability to move and change instance",
    "start": "2196000",
    "end": "2202000"
  },
  {
    "text": "types as opposed to when you're running inside of your own data center the fact that you are basically committed to at",
    "start": "2202000",
    "end": "2207579"
  },
  {
    "text": "least three years maybe five years of sitting on that particular hardware platform we went live initially with",
    "start": "2207579",
    "end": "2213849"
  },
  {
    "text": "based on the types of queries and analyzing what we were doing we went live with one of our career clusters",
    "start": "2213849",
    "end": "2219099"
  },
  {
    "text": "with about a hundred and forty nodes using m3 class machines and that was based on the different types of queries now one of the constraints that we had",
    "start": "2219099",
    "end": "2225880"
  },
  {
    "text": "within our own internal environment of dealing with these appliances is that they couldn't deal with concurrency we",
    "start": "2225880",
    "end": "2230950"
  },
  {
    "text": "had to build a whole governance around that they were limited in the amount of data and the record sets that we would through the application returned to them",
    "start": "2230950",
    "end": "2236619"
  },
  {
    "text": "so that we basically time shared the appliances across hundreds and across hundreds even thousands of users asking",
    "start": "2236619",
    "end": "2242920"
  },
  {
    "text": "their different questions once we turn once we separated this concern and basically unleashed a level of untapped",
    "start": "2242920",
    "end": "2249579"
  },
  {
    "text": "demand literally it wasn't a move down the curve it was a shift in the curve right it shifted outward in terms of what types of questions they're asking",
    "start": "2249579",
    "end": "2255849"
  },
  {
    "text": "what size of data that they're asking for and so we ended up learning from that and we were receiving various",
    "start": "2255849",
    "end": "2262329"
  },
  {
    "text": "different hive and query errors around and so we started refactoring the code and looking at the various different",
    "start": "2262329",
    "end": "2268000"
  },
  {
    "text": "instance types the new c3 for s had came out and we were testing and",
    "start": "2268000",
    "end": "2273730"
  },
  {
    "text": "experimenting around that and we actually found that by going with smaller larger nodes small fewer nodes",
    "start": "2273730",
    "end": "2279280"
  },
  {
    "text": "in your cluster of larger nodes given the memory in the CPU ended up having better throughput and we could actually deliver greater concurrency than what we",
    "start": "2279280",
    "end": "2286450"
  },
  {
    "text": "had and the net result of that is that we actually now weren't just competing with the incumbent we were actually",
    "start": "2286450",
    "end": "2292090"
  },
  {
    "start": "2288000",
    "end": "2288000"
  },
  {
    "text": "beating it we ended up improving our overall query performance across all of our various different test cases by over",
    "start": "2292090",
    "end": "2299140"
  },
  {
    "text": "36% we ended up with a greater level of concurrency now we are supporting anywhere from 40 to sometimes 40 to 50",
    "start": "2299140",
    "end": "2305860"
  },
  {
    "text": "concurrent queries were in our old appliances we would support 8 to 10 and all of this at a much lower cost both of",
    "start": "2305860",
    "end": "2312190"
  },
  {
    "text": "what we are running beforehand on our initial version of EMR and a much greater cost efficiency than what we are",
    "start": "2312190",
    "end": "2317830"
  },
  {
    "text": "doing off of our internal environment so one of the other aspects is you're",
    "start": "2317830",
    "end": "2322960"
  },
  {
    "start": "2320000",
    "end": "2320000"
  },
  {
    "text": "always looking at cost as an aspect that's always the selling point as you're trying to move up data to the cloud is right sizing your cluster and",
    "start": "2322960",
    "end": "2330310"
  },
  {
    "text": "our case I touched on it just a little bit ago is that you look at your ETL or your batch analytics and these things",
    "start": "2330310",
    "end": "2335530"
  },
  {
    "text": "are very very transient and here we heavily use the spot market typically within our SaaS we've had very few spot",
    "start": "2335530",
    "end": "2342340"
  },
  {
    "text": "failures and so when you're architecting your batch analytics your DTLS you have to understand that like the job may not",
    "start": "2342340",
    "end": "2347800"
  },
  {
    "text": "complete if you would get all bid and then the cluster gets taken away from you and in reality we've seen very few failures within that where that actually",
    "start": "2347800",
    "end": "2354610"
  },
  {
    "text": "occurs and what you can do as I look - like bit above the market price or even",
    "start": "2354610",
    "end": "2361030"
  },
  {
    "text": "above the on-demand price because you only actually pay at the price of what the market is doing as a strategy around",
    "start": "2361030",
    "end": "2366520"
  },
  {
    "text": "that within the analytics perspective what we had to look at is what were a",
    "start": "2366520",
    "end": "2372340"
  },
  {
    "text": "couple of factors what is the amount of queries that we needed to serve at the low period as opposed to the peak period",
    "start": "2372340",
    "end": "2377650"
  },
  {
    "text": "so it changes your mentality of how you're trying to architect your solution before we are always architecting and",
    "start": "2377650",
    "end": "2383020"
  },
  {
    "text": "capacity sizing for peak now we are looking at what is the minimum that we needed service level that we needed to",
    "start": "2383020",
    "end": "2388660"
  },
  {
    "text": "provide and how do we grow up and scale up to the particular use cases that the",
    "start": "2388660",
    "end": "2393820"
  },
  {
    "text": "business was demanding and if we needed 50 nodes and we needed to go to 200 nodes we easily add that to the capacity so it",
    "start": "2393820",
    "end": "2400930"
  },
  {
    "text": "was really a balance of what was that minimum set and also what is that minimum that you need to put into HDFS no matter what you're doing you still",
    "start": "2400930",
    "end": "2407500"
  },
  {
    "text": "have to have some level of HDFS within each use within your core nodes that's for various different statistics and",
    "start": "2407500",
    "end": "2413320"
  },
  {
    "text": "logging and all the aspects that kind of run with that so what we found is so the",
    "start": "2413320",
    "end": "2420850"
  },
  {
    "text": "way you can kind of minimize your cost around that is then going out and reserving your master in your core nodes that minimum size and then growing your",
    "start": "2420850",
    "end": "2427060"
  },
  {
    "text": "core and your caps notes up on top of that to scale up and then at when the peak demand goes away then we scale that",
    "start": "2427060",
    "end": "2433000"
  },
  {
    "text": "back down right now we're doing that primarily on a timing basis and we",
    "start": "2433000",
    "end": "2438100"
  },
  {
    "text": "constantly monitor and look at that and we're looking to take advantage of some more of the intelligent resizing capabilities with the new EMR release as",
    "start": "2438100",
    "end": "2444610"
  },
  {
    "text": "we go forward one of the funny items that we found is that as we scaled up you needed to keep roughly about a",
    "start": "2444610",
    "end": "2450550"
  },
  {
    "text": "five to one ratio otherwise your core nodes end up becoming a bit of a bottleneck with all the logging aspects",
    "start": "2450550",
    "end": "2456340"
  },
  {
    "text": "that you need to be able to do that just HDFS requires so one of the other things",
    "start": "2456340",
    "end": "2462430"
  },
  {
    "start": "2460000",
    "end": "2460000"
  },
  {
    "text": "that Rahul both touched on as well as Nate and you see this as a common practice in terms of architectures is",
    "start": "2462430",
    "end": "2468940"
  },
  {
    "text": "basically having a single meta store and that meta store should be off cluster the advantages of this are several fold",
    "start": "2468940",
    "end": "2476530"
  },
  {
    "text": "one is that you get a high level of fault tolerance in terms of having a multi zone AZ with it with your meta",
    "start": "2476530",
    "end": "2482800"
  },
  {
    "text": "store and that's always on and that your initialization of your servers is much faster now they're just basically",
    "start": "2482800",
    "end": "2488050"
  },
  {
    "text": "initiating a connection to your RDS instance around this and now onboarding",
    "start": "2488050",
    "end": "2494020"
  },
  {
    "text": "that before if you're trying to set up the meta store as part of your cluster you're literally where we are doing",
    "start": "2494020",
    "end": "2499330"
  },
  {
    "text": "millions and millions of partitions it can take about seven minutes per day to on board the partitions and if you're doing it over a course of a month or a",
    "start": "2499330",
    "end": "2505840"
  },
  {
    "text": "year that that cluster may take hours for it to initialize one of the nuances that you have to be aware of is looking",
    "start": "2505840",
    "end": "2511960"
  },
  {
    "text": "at the tool sets that you're doing as much as we would love to have in terms of open source and interoperability between file sets whether it be or core",
    "start": "2511960",
    "end": "2519370"
  },
  {
    "text": "part Kay and the various different tools out there of hive and presto there's nuances and there's differences in the data types and what sort of",
    "start": "2519370",
    "end": "2525880"
  },
  {
    "text": "semantics that they support hi 13 is different than hive one which is different than presto what we are",
    "start": "2525880",
    "end": "2532030"
  },
  {
    "text": "ultimately trying to be able to have is that one copy of the data and eventually you know these products are starting to",
    "start": "2532030",
    "end": "2537730"
  },
  {
    "text": "converge around some of those nuances around the data types and we and as that comes in but in the meantime what we do",
    "start": "2537730",
    "end": "2543400"
  },
  {
    "text": "is that we keep a different version we keep a single meta store on a single RDS instance but we keep multiple versions",
    "start": "2543400",
    "end": "2549520"
  },
  {
    "text": "of that and we keep that updated on a daily basis and as part of our fin our data manager what we do is as new data",
    "start": "2549520",
    "end": "2556210"
  },
  {
    "text": "comes in or new versions of that come in that those partitions will automatically get added and updated and so you as a",
    "start": "2556210",
    "end": "2562450"
  },
  {
    "text": "consumer don't have to deal with all of that maintenance right if you are a team that's focused on analytics or batch",
    "start": "2562450",
    "end": "2568780"
  },
  {
    "text": "analytics you now no longer have to deal with all of that infrastructure in the pit and the partition maintenance that",
    "start": "2568780",
    "end": "2574270"
  },
  {
    "text": "you normally would even if you're running like your own database or swapping partitions in and out of your appliances or your other relational data",
    "start": "2574270",
    "end": "2580990"
  },
  {
    "text": "store so one of the last points said I guess I'd like to be able to kind of",
    "start": "2580990",
    "end": "2586990"
  },
  {
    "start": "2583000",
    "end": "2583000"
  },
  {
    "text": "like touch on in terms of best practices is that you have to be constantly monitoring learning and optimizing and",
    "start": "2586990",
    "end": "2592090"
  },
  {
    "text": "adapting around this you know one of the when we initially went out the door we weren't doing anything we primarily had",
    "start": "2592090",
    "end": "2598090"
  },
  {
    "text": "a single queue set up within our EMR and we were basically clogging the highways",
    "start": "2598090",
    "end": "2603280"
  },
  {
    "text": "we would have a lot of smaller queries that might be asking for 5,000 rows 10,000 rows a hundred thousand rows a",
    "start": "2603280",
    "end": "2609070"
  },
  {
    "text": "million rows somebody would come in and ask for a dataset that would actually be like several billion so everybody's",
    "start": "2609070",
    "end": "2615850"
  },
  {
    "text": "smaller queries would get clogged up behind that so what we did is that we implemented that and we basically route",
    "start": "2615850",
    "end": "2620860"
  },
  {
    "text": "the query in terms of the various different queues inside of the EMR cluster to separate that concern secondly is that you always have to be",
    "start": "2620860",
    "end": "2627820"
  },
  {
    "text": "refactoring your code you got to be looking at where your bottlenecks and redesigning and refactoring and taking",
    "start": "2627820",
    "end": "2633220"
  },
  {
    "text": "advantage of the new tools and the new versions and what comes out and what can you do in terms of like caching or other",
    "start": "2633220",
    "end": "2638800"
  },
  {
    "text": "mechanisms around that one always will be looking to optimize on your cost in",
    "start": "2638800",
    "end": "2644050"
  },
  {
    "text": "terms of transient clusters and one of the things that if you're looking at your ETL your batch analytics so within",
    "start": "2644050",
    "end": "2649930"
  },
  {
    "text": "Amazon you're paying on the hour of those servers take a look at your size and the worst thing that you'd want to be able to do is have a job that runs",
    "start": "2649930",
    "end": "2655930"
  },
  {
    "text": "like you know 62 minutes when if you added ten more nodes to it it would complete in 55 so then you're paying",
    "start": "2655930",
    "end": "2662740"
  },
  {
    "text": "that extra hour on those expert so if you're looking to optimize on cost you have to play around with that and you got to look at what your sizing is",
    "start": "2662740",
    "end": "2668590"
  },
  {
    "text": "and be able to to try to manage inside of that one other item that was a unique",
    "start": "2668590",
    "end": "2674050"
  },
  {
    "text": "that we had ran into of running at least in terms of some of our hive queries over s3 was basically expected of",
    "start": "2674050",
    "end": "2681340"
  },
  {
    "text": "execution so some of our queries are very very long running over the EMR and through s3 and what speculative",
    "start": "2681340",
    "end": "2687640"
  },
  {
    "text": "execution does is that at Lund as high of the C's if the process had taken too",
    "start": "2687640",
    "end": "2693010"
  },
  {
    "text": "long it'll launch a new thread to try to be able to do that the first and the first process would ultimately finish",
    "start": "2693010",
    "end": "2698080"
  },
  {
    "text": "while the second one was still looking at it and it would overwrite some of the results and we picked up on the completion of the first thread not",
    "start": "2698080",
    "end": "2703870"
  },
  {
    "text": "knowing that the other one was was unaware so it's just basically it was just a little item and we earned up catching that relatively early on and",
    "start": "2703870",
    "end": "2711280"
  },
  {
    "text": "some other things you can do is using like broadcast joins in terms of being able to take small you know small",
    "start": "2711280",
    "end": "2716920"
  },
  {
    "text": "dimension or very certain fact tables and kind of caching those in there and you can use things like the EMR step API",
    "start": "2716920",
    "end": "2723100"
  },
  {
    "text": "when you're basically setting up various different job queues but using items like oszi where you have more complex",
    "start": "2723100",
    "end": "2728440"
  },
  {
    "text": "jobs the net result of all of this is that we ended up actually removing",
    "start": "2728440",
    "end": "2733720"
  },
  {
    "start": "2729000",
    "end": "2729000"
  },
  {
    "text": "obstacles one of the tenants that we had within sort of FINRA technology is get technology out of the way it's to",
    "start": "2733720",
    "end": "2739590"
  },
  {
    "text": "empower and provide self-service to the individual business consumers to let them ask whatever questions of the data",
    "start": "2739590",
    "end": "2745840"
  },
  {
    "text": "that they wanted to I kind of call it the Ganesh effect who's a remover of obstacles and that's basically getting",
    "start": "2745840",
    "end": "2751600"
  },
  {
    "text": "technology out of the way and let them do what they want to do the other aspect is that we've lowered that cost of",
    "start": "2751600",
    "end": "2756610"
  },
  {
    "text": "curiosity so now what we are able to do is provide multiple clusters and separation of concerns we have various",
    "start": "2756610",
    "end": "2762850"
  },
  {
    "text": "different business departments and they are looking at the data in different ways we have office of chief economist and they're looking for trends and",
    "start": "2762850",
    "end": "2768760"
  },
  {
    "text": "profiles we have market regulation that are looking for regularities and events and sequences within the data and so we",
    "start": "2768760",
    "end": "2774550"
  },
  {
    "text": "can separate those concerns we can optimize around that and they can get to the types of queries that they're",
    "start": "2774550",
    "end": "2779830"
  },
  {
    "text": "looking to be able to do the other aspect around this is that when you're in a fixed capacity environment in your",
    "start": "2779830",
    "end": "2786790"
  },
  {
    "text": "own data center is that you're always dealing with I kind of call we called it the mainframe effect you're always",
    "start": "2786790",
    "end": "2791860"
  },
  {
    "text": "shuffling data in and out we were dealing with a fixed capacity of whatever box and even as you moved from",
    "start": "2791860",
    "end": "2797240"
  },
  {
    "text": "one box to another bigger box you're always still living inside of a box what EMR an s3 has allowed us to do is",
    "start": "2797240",
    "end": "2803210"
  },
  {
    "text": "to get outside of that we often times you end up with data fixes or data Corrections you have to reprocess you",
    "start": "2803210",
    "end": "2808580"
  },
  {
    "text": "have to rerun there's a whole series of analytics and decisions that have already been made on that but you've still got to meet your production SLA as",
    "start": "2808580",
    "end": "2814910"
  },
  {
    "text": "of all your other daily jobs that need to execute before it was just actually a few weeks ago we ended up having like a",
    "start": "2814910",
    "end": "2821990"
  },
  {
    "text": "big data correction from one of the exchanges that needed to correct over like six months worth of data which mean all of that analytics all of the ETL",
    "start": "2821990",
    "end": "2828380"
  },
  {
    "text": "that were done on top of that needed to be corrected and so what that happened if we were doing that in the old",
    "start": "2828380",
    "end": "2833930"
  },
  {
    "text": "environment that might have been at two or three months a little mini project to shuffle this data in and out we did that in the span of three or four days and",
    "start": "2833930",
    "end": "2840170"
  },
  {
    "text": "EMR spinning up separate clusters around this and actually executing that we're delivering the updated results to the",
    "start": "2840170",
    "end": "2845750"
  },
  {
    "text": "business so they could move on and the other the final aspect of this is that you're looking at this and you separate",
    "start": "2845750",
    "end": "2851690"
  },
  {
    "text": "these concerns and look at these different services you end up ultimately increasing your team's delivery velocity",
    "start": "2851690",
    "end": "2857630"
  },
  {
    "text": "so we've picked up the pace in terms of what we were able to deliver in terms of business value in terms of the various",
    "start": "2857630",
    "end": "2862820"
  },
  {
    "text": "different services and features we can be able to deliver so just to kind of recap a few of the different items as",
    "start": "2862820",
    "end": "2869060"
  },
  {
    "start": "2865000",
    "end": "2865000"
  },
  {
    "text": "obviously take advantage of EMR and s3 and resize and definitely take advantage of the spot market you can save a lot of",
    "start": "2869060",
    "end": "2875630"
  },
  {
    "text": "dollars around that which makes your finance and you're purchasing people very happy you need to have you need to",
    "start": "2875630",
    "end": "2881270"
  },
  {
    "text": "look at always kind of having that shared persistent meta store that's out there one of the other items I guess the",
    "start": "2881270",
    "end": "2886580"
  },
  {
    "text": "last two are kind of near and dear to us is that all within this ecosystem all of these things are changing and they you",
    "start": "2886580",
    "end": "2893300"
  },
  {
    "text": "need to be able to adapt and to move around this having your data separated from your compute and being able to adapt and move to different items",
    "start": "2893300",
    "end": "2899300"
  },
  {
    "text": "currently today we use we're using hive we have smaller patches of presto and Rican and we're growing and evolving our",
    "start": "2899300",
    "end": "2905780"
  },
  {
    "text": "use of presto we have some smaller patches of where we're using like spark and that continues to grow it allows you",
    "start": "2905780",
    "end": "2911840"
  },
  {
    "text": "to these engines and these tools are going to continue to evolve as long as you can run them in a Hadoop environment and run and put them out there with your",
    "start": "2911840",
    "end": "2918920"
  },
  {
    "text": "data separated you're not tied to that particular format and it goes back to how you do that and ultimately you need",
    "start": "2918920",
    "end": "2924440"
  },
  {
    "text": "to make sure that you're budgeting time to experiment in terms of looking at these engines right sizing the cluster",
    "start": "2924440",
    "end": "2930500"
  },
  {
    "text": "and op them around that you need to make sure you have that built into your delivery schedules as you're working those working these items out some other",
    "start": "2930500",
    "end": "2938310"
  },
  {
    "start": "2936000",
    "end": "2936000"
  },
  {
    "text": "related sessions that were out there and with that we'll be happy to take some",
    "start": "2938310",
    "end": "2944430"
  },
  {
    "text": "questions we have a few minutes left",
    "start": "2944430",
    "end": "2947930"
  }
]