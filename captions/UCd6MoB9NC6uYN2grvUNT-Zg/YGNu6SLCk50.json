[
  {
    "start": "0",
    "end": "53000"
  },
  {
    "text": "y hello everybody Welcome to the session and welcome to Vegas for those from outside here like myself so let me",
    "start": "1719",
    "end": "8760"
  },
  {
    "text": "introduce myself I'm Richard Freeman um I'm based in London and I work for",
    "start": "8760",
    "end": "13879"
  },
  {
    "text": "justgiving and today's session is about serverless data pipelines event driven",
    "start": "13879",
    "end": "18880"
  },
  {
    "text": "ETL and stream processing just a little bit about my background so I have a PhD",
    "start": "18880",
    "end": "24640"
  },
  {
    "text": "in machine learning and natural language processing um back in the day uh when",
    "start": "24640",
    "end": "31400"
  },
  {
    "text": "being a data scientist wasn't actually as sexy as nowadays um so I got that in Manchester",
    "start": "31400",
    "end": "37040"
  },
  {
    "text": "University back in the UK after that I I got a job in a consultancy Cap Gemini so",
    "start": "37040",
    "end": "42440"
  },
  {
    "text": "I've got six years experience moving from a developer up to a Solutions architect then I worked for various",
    "start": "42440",
    "end": "47920"
  },
  {
    "text": "organizations before joining justgiving um around three years",
    "start": "47920",
    "end": "53719"
  },
  {
    "start": "53000",
    "end": "105000"
  },
  {
    "text": "ago so this is what we're going to be covering today in the session uh so",
    "start": "53719",
    "end": "58879"
  },
  {
    "text": "we're going to have a recap on some of the AWS managed Services then we're going to be talking about the different",
    "start": "58879",
    "end": "65080"
  },
  {
    "text": "challenges that we faced and the requirements that we needed to meet in terms of Big Data um so this were the",
    "start": "65080",
    "end": "71920"
  },
  {
    "text": "initial requirements about three years ago that we were looking to move a lot of the processes and data science",
    "start": "71920",
    "end": "78880"
  },
  {
    "text": "forward uh then I'll be giving a brief summary summary on the existing event",
    "start": "78880",
    "end": "84159"
  },
  {
    "text": "driven um data pipelines that exist on the market um and then we'll be covering",
    "start": "84159",
    "end": "90079"
  },
  {
    "text": "our data platform and from that actually we'll be abstracting that down into five different patterns that you can",
    "start": "90079",
    "end": "95560"
  },
  {
    "text": "Implement within your organization um I'm going to conclude on some recommendations around serverless",
    "start": "95560",
    "end": "101399"
  },
  {
    "text": "technology and the best practices that you can use within your organization so first a very brief recap",
    "start": "101399",
    "end": "108240"
  },
  {
    "start": "105000",
    "end": "330000"
  },
  {
    "text": "on some of the AWS services and before I do this I just want to get a show of hands for how many people have used um",
    "start": "108240",
    "end": "114520"
  },
  {
    "text": "serverless technology or Landers it's great lots of people how many people have use red shift um yeah",
    "start": "114520",
    "end": "122840"
  },
  {
    "text": "good show of hands and finally how many people have an automated data pipeline to actually load data into red",
    "start": "122840",
    "end": "129479"
  },
  {
    "text": "shift okay there's fewer hands but obviously people interested so we're going to cover that today um so you've",
    "start": "129479",
    "end": "137680"
  },
  {
    "text": "probably heard some of this already but I'm going to reiterate it for others that aren't aware of it um so on the",
    "start": "137680",
    "end": "142920"
  },
  {
    "text": "left we've got AWS S3 uh Amazon S3 which is a distributed web store for objects",
    "start": "142920",
    "end": "149840"
  },
  {
    "text": "so it's immutable so once you add an object it's persisted um until it gets removed it's highly scalable it supports",
    "start": "149840",
    "end": "157760"
  },
  {
    "text": "um almost petabytes of data or unlimited amounts of data anything you can throw it almost and it's a very cheap way of",
    "start": "157760",
    "end": "164560"
  },
  {
    "text": "doing it as you only pay for the actual storage that you consume on S3 itself um",
    "start": "164560",
    "end": "170159"
  },
  {
    "text": "it supports encryption you can obviously compress those files um and to reduce the storage cost AWS Lander so obviously",
    "start": "170159",
    "end": "177280"
  },
  {
    "text": "for those who are using servus technology you probably used it before um we started using it um a few years",
    "start": "177280",
    "end": "183720"
  },
  {
    "text": "ago already and I've written some blogs about it um it allows you to run specific code based on some trigger so",
    "start": "183720",
    "end": "191200"
  },
  {
    "text": "the trigger could either be based on some events added into Kinesis streams",
    "start": "191200",
    "end": "196400"
  },
  {
    "text": "or a specific object that's added to an S3 bucket and that will trigger the Lander function within the Lander you",
    "start": "196400",
    "end": "202400"
  },
  {
    "text": "can actually run any of the analytics code or any type of code that you want to do it also supports the Amazon API",
    "start": "202400",
    "end": "208920"
  },
  {
    "text": "Gateway which is you could think of it as a rest endpoint that you can call and that would invoke a Lander in a serous",
    "start": "208920",
    "end": "214159"
  },
  {
    "text": "way the Lander would do some computation or some lookup and then return the results at um through your rest API the",
    "start": "214159",
    "end": "221560"
  },
  {
    "text": "beauty is you only pay for the amount of code um the amount of time that your",
    "start": "221560",
    "end": "226840"
  },
  {
    "text": "code executes down to the 100 milliseconds um it's highly scalable",
    "start": "226840",
    "end": "232439"
  },
  {
    "text": "highly available again that's all managed for you by AWS Amazon red shift is um a massively parallel processing",
    "start": "232439",
    "end": "239480"
  },
  {
    "text": "dat data warehouse which allows you to run SQL and csql on top of large amounts",
    "start": "239480",
    "end": "244519"
  },
  {
    "text": "of data it's highly optimized to actually load data from S3 and Dynamo",
    "start": "244519",
    "end": "249560"
  },
  {
    "text": "and elastic map produce it's got extensive security it supports rols um",
    "start": "249560",
    "end": "254959"
  },
  {
    "text": "encryption at rest and encryption and Transit um it also has um this is one of",
    "start": "254959",
    "end": "261079"
  },
  {
    "text": "the very good benefits actually why I like it it's got jdbc and odbc connectivity supports anti-l and also",
    "start": "261079",
    "end": "268199"
  },
  {
    "text": "there's a postgress interface actually so you can um communicate using any of the tools that exist on the market",
    "start": "268199",
    "end": "273880"
  },
  {
    "text": "including vendor tools for bi so you're not tied to a specific MPP um so on the",
    "start": "273880",
    "end": "279400"
  },
  {
    "text": "right we've got Amazon elastic map produce um which allows you to do either batch processing at scale or real-time",
    "start": "279400",
    "end": "286759"
  },
  {
    "text": "analytics um a lot of people will be talking about spark and this is where um Amazon supports out of the box the open",
    "start": "286759",
    "end": "293600"
  },
  {
    "text": "source platforms installs all that on the EMR cluster and you have sparken up and running in 10 minutes basically",
    "start": "293600",
    "end": "300199"
  },
  {
    "text": "uh it supports both long and transient running clusters so you could imagine you've got a huge ETL job you need to",
    "start": "300199",
    "end": "306680"
  },
  {
    "text": "process a parab a petabyte of data you can spin up a thousand machines just for the duration of that job and then shut",
    "start": "306680",
    "end": "312800"
  },
  {
    "text": "them all down um the beauty for me is the spot instances this is where you can",
    "start": "312800",
    "end": "317840"
  },
  {
    "text": "actually bid for spare capacity in Amazon at a very low price so you can spin up a thousand nodes again at a",
    "start": "317840",
    "end": "324639"
  },
  {
    "text": "reduced cost rather than the um more expensive on demand prices",
    "start": "324639",
    "end": "330639"
  },
  {
    "start": "330000",
    "end": "388000"
  },
  {
    "text": "so the other three I want to talk about that are relevant to the talk is um Amazon Kinesis so we started off with",
    "start": "330639",
    "end": "338319"
  },
  {
    "text": "Amazon Kinesis streams um and this is a way of um ingesting a lot of data very",
    "start": "338319",
    "end": "343440"
  },
  {
    "text": "fast fast and making it available for other other services so you could think of thousands of or millions of iot",
    "start": "343440",
    "end": "349880"
  },
  {
    "text": "devices pushing in sensor data into Kinesis streams and that's what it's built for um in just giving we use it",
    "start": "349880",
    "end": "356440"
  },
  {
    "text": "for ingesting web analytics data um if you pipe the data out of Kinesis you can",
    "start": "356440",
    "end": "361919"
  },
  {
    "text": "actually use Amazon Kinesis analytics and run SQL commands on the stream of data um and that's all integrated",
    "start": "361919",
    "end": "369080"
  },
  {
    "text": "already Amazon Kinesis fire hose is an endpoint you could think of it where you",
    "start": "369080",
    "end": "374960"
  },
  {
    "text": "actually push a lot of events those will get buffered in memory and then written out to S3 um and they can also be loaded",
    "start": "374960",
    "end": "382160"
  },
  {
    "text": "into red shift or Amazon um elastic search",
    "start": "382160",
    "end": "388080"
  },
  {
    "start": "388000",
    "end": "547000"
  },
  {
    "text": "service Okay so now we've covered some of the background just for those who weren't aware of it so I'm going to be talking about our challenges and",
    "start": "388080",
    "end": "394120"
  },
  {
    "text": "requirements at just giving um I'm going to first introduce just giving so we are",
    "start": "394120",
    "end": "399759"
  },
  {
    "text": "tech for good company uh we work in event-based fundraising in the charity space and and crowdfunding for good so",
    "start": "399759",
    "end": "408199"
  },
  {
    "text": "these are four good causes um we are the number one platform for online social",
    "start": "408199",
    "end": "413479"
  },
  {
    "text": "giving in the world and this is based on the amount of money that we've raised which is $4.2 billion in donation to",
    "start": "413479",
    "end": "420599"
  },
  {
    "text": "date so yeah that's a huge amount when you think about 4.2 billion helping for those in needs helping the animals",
    "start": "420599",
    "end": "427720"
  },
  {
    "text": "helping your local communities um and we've got a massive user base also so we've got 28.5 million users who",
    "start": "427720",
    "end": "434319"
  },
  {
    "text": "actually transacted on the platform in 196 countries and our platform supports 27,000 good causes which are Charities",
    "start": "434319",
    "end": "442560"
  },
  {
    "text": "or crowdfunding for good projects um and what we like to do is uh apply data",
    "start": "442560",
    "end": "448520"
  },
  {
    "text": "science also so so my backgrounds uh PhD in machine learning so obviously I've got an interest in data science and so",
    "start": "448520",
    "end": "455400"
  },
  {
    "text": "do understanding how um people donate and almost our motto I would say is to",
    "start": "455400",
    "end": "461879"
  },
  {
    "text": "ensure that no good cause goes unfunded um and we found that in order to do this in a data scientific way we actually",
    "start": "461879",
    "end": "469000"
  },
  {
    "text": "need to provide content that is highly relevant to the user and that is very",
    "start": "469000",
    "end": "475400"
  },
  {
    "text": "engaging for them relevant um and we found it it was difficult at the start",
    "start": "475400",
    "end": "480639"
  },
  {
    "text": "to do that um we have a lot of data we need to understand almost how how that",
    "start": "480639",
    "end": "486000"
  },
  {
    "text": "works so what we found is we created a graph a graph of relationships between the users so people who create a page to",
    "start": "486000",
    "end": "493800"
  },
  {
    "text": "raise money for a specific cause you're actually running for an a marathon for example and people who actually sponsor",
    "start": "493800",
    "end": "499520"
  },
  {
    "text": "them um and actually creating what we call a give graph which is a",
    "start": "499520",
    "end": "505319"
  },
  {
    "text": "relationship um between nodes so we have about 91 million nodes so these are the",
    "start": "505319",
    "end": "510680"
  },
  {
    "text": "users the Charities and also a decomposition of the charity which means",
    "start": "510680",
    "end": "515760"
  },
  {
    "text": "for example if it's um um dogs trust in the UK we decompose it into an animal",
    "start": "515760",
    "end": "521120"
  },
  {
    "text": "charity specifically dogs and in specific locations around the country so we're actually decomposing the whole",
    "start": "521120",
    "end": "527920"
  },
  {
    "text": "essence of what the charity does um so those are nodes and Inter relationship between all those nodes we have half a",
    "start": "527920",
    "end": "534279"
  },
  {
    "text": "billion relationships and that helps you understand and provide more engaging content content we found um and that",
    "start": "534279",
    "end": "541240"
  },
  {
    "text": "actually integrates with Facebook and it is the largest giving graph in the",
    "start": "541240",
    "end": "547160"
  },
  {
    "start": "547000",
    "end": "637000"
  },
  {
    "text": "world just taking a step back so um this is what your journey would look like if you go on just giving uh so here we have",
    "start": "547480",
    "end": "554279"
  },
  {
    "text": "an example of one of our products which is a fundraising page so this is Sophie's Royal Parks half marathon so",
    "start": "554279",
    "end": "562000"
  },
  {
    "text": "she's run running for um a charity that's really close to her heart which is around the mental health so she wants",
    "start": "562000",
    "end": "569360"
  },
  {
    "text": "to raise money she set a Target um a value that uh colleagues friends and",
    "start": "569360",
    "end": "575160"
  },
  {
    "text": "families are going to sponsor her and then she hopes to reach so you can see there's a Target there's also updates",
    "start": "575160",
    "end": "580560"
  },
  {
    "text": "that she provides at the bottom she's able to share this on social media Facebook Twitter or by email and you can",
    "start": "580560",
    "end": "587240"
  },
  {
    "text": "see a list of the supporters on the right and actually behind all of this there's a lot of data science that we do",
    "start": "587240",
    "end": "593680"
  },
  {
    "text": "so we can actually we actually know um based on the estimates based on your past history with the platform form how",
    "start": "593680",
    "end": "599880"
  },
  {
    "text": "much we think you're going to be able to raise so again this is maximizing the revenue for that specific charity um we",
    "start": "599880",
    "end": "606800"
  },
  {
    "text": "also have different mechanisms to show more engaging content to the specific users um so you can see different",
    "start": "606800",
    "end": "613320"
  },
  {
    "text": "products so behind the scenes we're capturing a lot of the analytics data see these are page views different",
    "start": "613320",
    "end": "619440"
  },
  {
    "text": "Impressions um different clicks for share for example on Facebook and that allows the um us to analyze all of that",
    "start": "619440",
    "end": "626880"
  },
  {
    "text": "and to run various tests so you can think of that a fundraising product we have various features that we released",
    "start": "626880",
    "end": "632320"
  },
  {
    "text": "such as the updates um that's a fairly recent feature for example uh so our requirements were at",
    "start": "632320",
    "end": "639680"
  },
  {
    "text": "the time that our SQL Server data warehouse wasn't able to cope with long running queries and the complexity that",
    "start": "639680",
    "end": "646279"
  },
  {
    "text": "queries required for data scientists such as the graph queries that we talked about earlier in addition we were",
    "start": "646279",
    "end": "652839"
  },
  {
    "text": "receiving more and more data sources and we needed a way to ingest that data and to run processes on it so if you think",
    "start": "652839",
    "end": "660000"
  },
  {
    "text": "about the clickstream data that's one source so we're getting clickstream at user level um we're also getting API",
    "start": "660000",
    "end": "666680"
  },
  {
    "text": "data so from the likes of Facebook Survey Monkey exact target and other",
    "start": "666680",
    "end": "672120"
  },
  {
    "text": "news feeds and data sources um in addition we're getting log data so this",
    "start": "672120",
    "end": "677240"
  },
  {
    "text": "is the App log or the web server logs uh coming in as well as what I talked about earlier the behavioral data how do you",
    "start": "677240",
    "end": "683000"
  },
  {
    "text": "users actually interact with the specific products what do they like if we make recommendations do they actually",
    "start": "683000",
    "end": "688800"
  },
  {
    "text": "uh click on those recommendations so we're capturing a lot more data sources our data warehouse wasn't able to cope",
    "start": "688800",
    "end": "694079"
  },
  {
    "text": "so we were looking for other Solutions almost on the other side we also looking for a way to add new data sources very",
    "start": "694079",
    "end": "699360"
  },
  {
    "text": "easily and create u a platform that allows us to create a pipeline an ETL",
    "start": "699360",
    "end": "704600"
  },
  {
    "text": "pipeline or elt pipeline for data preparation and you can see I've put a very simplified view of what we have in",
    "start": "704600",
    "end": "710680"
  },
  {
    "text": "our automated pipeline so you have a data ingestion that we talked about the data preparation as well as the machine",
    "start": "710680",
    "end": "716279"
  },
  {
    "text": "learning graph processing and NLP pipelines in addition um we are",
    "start": "716279",
    "end": "722000"
  },
  {
    "text": "providing streaming analytics so this allows us to ingest data analyze it on",
    "start": "722000",
    "end": "727040"
  },
  {
    "text": "real time for example the page view data and provide some metrics back to the users on the right um You can see this",
    "start": "727040",
    "end": "734920"
  },
  {
    "text": "is more of the outcome so we have a great platform that works and um the data driven approach that we wanted to",
    "start": "734920",
    "end": "741279"
  },
  {
    "text": "use was to actually measure um and make Insight available to end users as well",
    "start": "741279",
    "end": "747839"
  },
  {
    "text": "as provide pred itions based on all the data that we have um recommendations are",
    "start": "747839",
    "end": "754120"
  },
  {
    "text": "also important so as I said we're recommending a Target value we're recommending default values to the user",
    "start": "754120",
    "end": "759519"
  },
  {
    "text": "and those are all based dynamically on our data science models so I'm just going to go over uh",
    "start": "759519",
    "end": "767199"
  },
  {
    "start": "764000",
    "end": "848000"
  },
  {
    "text": "some of the key notes some of the key points that exist in the um existing Big Data ETL platforms um so we talked about",
    "start": "767199",
    "end": "776440"
  },
  {
    "text": "some of this already but I just want to reiterate some of it so there is the the concept of an automated data pipeline",
    "start": "776440",
    "end": "782760"
  },
  {
    "text": "where you want to automate the execution of your machine learning algorithms there's always a way of querying that",
    "start": "782760",
    "end": "788440"
  },
  {
    "text": "data and manipulating it efficiently um that's why um a data pipel is required",
    "start": "788440",
    "end": "794639"
  },
  {
    "text": "if you're doing ad hoc analysis as a data scientist it doesn't matter too much when you want to automate and productionize the reporting you need to",
    "start": "794639",
    "end": "800399"
  },
  {
    "text": "have this Automation in place there's always some sort of data schema some models around the actual data um the",
    "start": "800399",
    "end": "807000"
  },
  {
    "text": "specific columns or the specific key pairs that is that is always available",
    "start": "807000",
    "end": "812040"
  },
  {
    "text": "um typically how data pipelines work they support either scheduled job so it",
    "start": "812040",
    "end": "817399"
  },
  {
    "text": "runs at a specific time for example at midnight every day uh or we have triggered jobs based on some other types",
    "start": "817399",
    "end": "823920"
  },
  {
    "text": "of events so this could be a file is added to S3 for example um there's",
    "start": "823920",
    "end": "829680"
  },
  {
    "text": "always a requirement for monitoring and for looking at any failures and reloading that data almost continuously",
    "start": "829680",
    "end": "836240"
  },
  {
    "text": "and typically what I found is uh looking look at all the open source looking at the other vendor products there's always",
    "start": "836240",
    "end": "842040"
  },
  {
    "text": "some sort of concept of a workflow or directed ayc graph or dags for short",
    "start": "842040",
    "end": "847600"
  },
  {
    "text": "which I'll show you next um so the dags in ETL I I believe this is my view can",
    "start": "847600",
    "end": "855040"
  },
  {
    "start": "848000",
    "end": "904000"
  },
  {
    "text": "go very complex imagine you have a thousand nodes and you end up with this huge um almost flow of",
    "start": "855040",
    "end": "863199"
  },
  {
    "text": "dependencies um sometimes depends on which products you you look at uh these are actually",
    "start": "863199",
    "end": "869519"
  },
  {
    "text": "handdrawn graphically um there are abstractions where you abstract different layers and you call different layers but again there's a lot of",
    "start": "869519",
    "end": "875680"
  },
  {
    "text": "dependency um in the actual flow also I found it very difficult if you're moving",
    "start": "875680",
    "end": "881079"
  },
  {
    "text": "from one workflow engine or one dag to another it's not it's not obvious you need to redraw it you need to recreate",
    "start": "881079",
    "end": "886720"
  },
  {
    "text": "it um again there's there's such a variety of works out there but this is a this is a typical pattern in the market",
    "start": "886720",
    "end": "892240"
  },
  {
    "text": "right now um so if in the green boxes we're introducing a change to one of the tables for example or the data formats",
    "start": "892240",
    "end": "899240"
  },
  {
    "text": "um that would have to get propagated in workflow so you'd have to go and edit it and change",
    "start": "899240",
    "end": "904440"
  },
  {
    "start": "904000",
    "end": "978000"
  },
  {
    "text": "everything um also what I found is a lot of the integration that exists right now",
    "start": "904440",
    "end": "909680"
  },
  {
    "text": "is point-to-point integration so this is where if you make one change um that",
    "start": "909680",
    "end": "915000"
  },
  {
    "text": "gets added into red shift directly so there's no um you need to actually modify your workflow if you want to load",
    "start": "915000",
    "end": "921079"
  },
  {
    "text": "it into a different cluster also they don't tend to support",
    "start": "921079",
    "end": "926279"
  },
  {
    "text": "very well uh spinning up different EMR clusters dynamically loading the data in",
    "start": "926279",
    "end": "931720"
  },
  {
    "text": "and then um spinning them down so I found some of the some of the solutions and they don't necessarily integrate",
    "start": "931720",
    "end": "937360"
  },
  {
    "text": "directly um with spark in an advanced way to create the machine learning pipelines it's also complex to reload so",
    "start": "937360",
    "end": "945000"
  },
  {
    "text": "some vendors um products Open Source Products have a way of dashboard of viewing and reloading but again that",
    "start": "945000",
    "end": "950480"
  },
  {
    "text": "introduces some complexity um so i' say in general if you're doing any big data",
    "start": "950480",
    "end": "955720"
  },
  {
    "text": "analytics batch analytics or batch loading ETL is very easy incremental is a lot harder you need to take into",
    "start": "955720",
    "end": "961839"
  },
  {
    "text": "account duplicates so obviously you don't want duplicates you don't want to drop any data so there's always this incremental concept um uh usually they",
    "start": "961839",
    "end": "969399"
  },
  {
    "text": "use different types of user interfaces and you need to spin up a cluster of machines almost to to create um your big",
    "start": "969399",
    "end": "977440"
  },
  {
    "text": "data pipeline so now I'm just going to talk about the event driven data platform in",
    "start": "977440",
    "end": "983800"
  },
  {
    "start": "978000",
    "end": "1076000"
  },
  {
    "text": "dis giving so just giving we created our",
    "start": "983800",
    "end": "989759"
  },
  {
    "text": "in-house data analytics platform which we call Raven which stands for reporting analytics visualization experimental",
    "start": "989759",
    "end": "998120"
  },
  {
    "text": "networks so rather than use dags or workflows we're actually using event",
    "start": "998600",
    "end": "1004880"
  },
  {
    "text": "driven architecture and serverless pipelines event-driven um architecture",
    "start": "1004880",
    "end": "1010079"
  },
  {
    "text": "is probably more familiar amongst yourselves so who are or people who are developers and Architects and maybe less",
    "start": "1010079",
    "end": "1016800"
  },
  {
    "text": "for the traditional ETL bi type people um it relies on",
    "start": "1016800",
    "end": "1021920"
  },
  {
    "text": "messaging um cues publish subscribe mechanisms so probably developers who",
    "start": "1021920",
    "end": "1027520"
  },
  {
    "text": "used entprise service buses will be aware of what I'm talking about in terms of messaging being more Dynamic responding to events sending a message",
    "start": "1027520",
    "end": "1034600"
  },
  {
    "text": "and decoupling almost um one system from another um so it's all about loose",
    "start": "1034600",
    "end": "1040400"
  },
  {
    "text": "coupling um and we also separate the data which is stored in S3 from all the",
    "start": "1040400",
    "end": "1046280"
  },
  {
    "text": "Computing so rather than doing for example ex Le an ETL in red shift we're actually doing the ETL in spark or other",
    "start": "1046280",
    "end": "1053400"
  },
  {
    "text": "services and then loading it in um it supports a scalable ETL machine learning",
    "start": "1053400",
    "end": "1058520"
  },
  {
    "text": "natural language processing V spark and graph processing and at the end it allows us to consume the the raw data in",
    "start": "1058520",
    "end": "1065480"
  },
  {
    "text": "red shift or in spark um data blocks which is uh join between all the data sources such as the clickstream the",
    "start": "1065480",
    "end": "1071200"
  },
  {
    "text": "transactional data and other data sources and provide insight and dashboard metrics so maybe for the more",
    "start": "1071200",
    "end": "1077960"
  },
  {
    "start": "1076000",
    "end": "1329000"
  },
  {
    "text": "technical audience this is an overview of the platform so you can see on the top left we have the web analytics so",
    "start": "1077960",
    "end": "1085360"
  },
  {
    "text": "I'll decompose this into three types of web analytics which I think every company should have so we capture the",
    "start": "1085360",
    "end": "1091520"
  },
  {
    "text": "client side analytics So currently using kiss metrics to do that for the server side we're using Kinesis streams so each",
    "start": "1091520",
    "end": "1100360"
  },
  {
    "text": "of our microservices micro sites actually sends analytics events to Kinesis streams and we also have log",
    "start": "1100360",
    "end": "1105720"
  },
  {
    "text": "stach so that's the web logs the app logs where we capture that data um we",
    "start": "1105720",
    "end": "1111960"
  },
  {
    "text": "use um clickstream data in kissmetrics and Kinesis streams because we have the",
    "start": "1111960",
    "end": "1117400"
  },
  {
    "text": "detail of the user so we're not getting Aggregates we're actually getting the specific user this is their Journey on on the website or the mobile app so we",
    "start": "1117400",
    "end": "1124320"
  },
  {
    "text": "actually have an understanding of the actions that they're taking um in terms of mini surveys we're using queru",
    "start": "1124320",
    "end": "1130120"
  },
  {
    "text": "currently um so those will pop up within a specific context so um they're very context sensitive a very small survey",
    "start": "1130120",
    "end": "1136159"
  },
  {
    "text": "that appears at the bottom uh that you can dismiss if you're if you're not interested um we also have you see on",
    "start": "1136159",
    "end": "1142640"
  },
  {
    "text": "the middle left we have different API Integrations so integrate with exact target Survey Monkey Twitter other open",
    "start": "1142640",
    "end": "1149720"
  },
  {
    "text": "sources news feeds um and that we have a process to actually put all that data",
    "start": "1149720",
    "end": "1154760"
  },
  {
    "text": "from their apis uh the third pattern that we have is actually to pull data from a data warehouse so we pull facts",
    "start": "1154760",
    "end": "1160840"
  },
  {
    "text": "and dimensions ltp um rather than doing a pull we're doing a push actually to not overwhelm our data warehouse",
    "start": "1160840",
    "end": "1166799"
  },
  {
    "text": "currently um and done all through the the next layer the data integration um",
    "start": "1166799",
    "end": "1172799"
  },
  {
    "text": "where we have different processes to read all that data to process it to run ETL jobs and we're going to go into drill down into that detail um for",
    "start": "1172799",
    "end": "1179400"
  },
  {
    "text": "yourselves afterwards that's all done in a secure way so we're using rols as part of the I am and um we're also using what",
    "start": "1179400",
    "end": "1187360"
  },
  {
    "text": "we we'll see how how we do the event driven using cues in addition we've got",
    "start": "1187360",
    "end": "1192600"
  },
  {
    "text": "a Lander function also to do some of the lightweight ETL which we'll we'll cover in great depth the data storage so we",
    "start": "1192600",
    "end": "1198760"
  },
  {
    "text": "have uh red shift everything um how we when we started off everything was centered around red shift so the whole",
    "start": "1198760",
    "end": "1205760"
  },
  {
    "text": "orchestration of loading data running the ETL into red shift um red shift Centric and then we started to um scale",
    "start": "1205760",
    "end": "1213760"
  },
  {
    "text": "out even further we wanted to use the um spot instances so we started using Amazon elastic map reduce as well as uh",
    "start": "1213760",
    "end": "1221240"
  },
  {
    "text": "introducing more of the machine learning at scale uh and you can see on the right uh so what our data scientists would do",
    "start": "1221240",
    "end": "1227559"
  },
  {
    "text": "uh is use R and Python and they found it really really useful to connect to Red",
    "start": "1227559",
    "end": "1233799"
  },
  {
    "text": "shift straight because you can load data straight into Data frames and you've got it all available already cleaned up all",
    "start": "1233799",
    "end": "1240200"
  },
  {
    "text": "the pre-processing um that the the data science Community don't always talk about but you know they spend maybe 80%",
    "start": "1240200",
    "end": "1246679"
  },
  {
    "text": "of their time preparing the data shaping the data cleansing the data enriching the data um and they don't need to do",
    "start": "1246679",
    "end": "1252559"
  },
  {
    "text": "that anymore which is great um other integration points that we have is Tableau so you run automated",
    "start": "1252559",
    "end": "1258760"
  },
  {
    "text": "reports Tableau allows our product managers and maybe the non it people to",
    "start": "1258760",
    "end": "1263799"
  },
  {
    "text": "actually visualize um and graph the results so these could be aggregate accounts it could be the the specific",
    "start": "1263799",
    "end": "1269919"
  },
  {
    "text": "page use for example for the fundraising Pages understanding a new product release um yeah any of the AB testing is",
    "start": "1269919",
    "end": "1276520"
  },
  {
    "text": "available in there in addition our uh machine learning Engineers also use",
    "start": "1276520",
    "end": "1282159"
  },
  {
    "text": "spark and EMR um and that's a great way I think to spin up a cluster during",
    "start": "1282159",
    "end": "1287679"
  },
  {
    "text": "office hours maybe you can run all your ETL jobs um on that automatically and",
    "start": "1287679",
    "end": "1292960"
  },
  {
    "text": "your machine learning and then shut it down we're mastering the data as as part of the data Lake within",
    "start": "1292960",
    "end": "1299480"
  },
  {
    "text": "S3 um what's funny is we also have SQL Server taking aggates out of red shift",
    "start": "1299480",
    "end": "1305320"
  },
  {
    "text": "so it almost Loops back um because we're able to support the hundreds of terabyt",
    "start": "1305320",
    "end": "1312360"
  },
  {
    "text": "scale oh sorry last one uh so we've got AWS Lander also for streaming analytics and we're going to talk about this as",
    "start": "1312360",
    "end": "1318200"
  },
  {
    "text": "part of the patterns so from this so the data integration layer is uh what I'm going to be covering in a bit more depth",
    "start": "1318200",
    "end": "1324400"
  },
  {
    "text": "um and we've derived five different patterns that you can use in your",
    "start": "1324400",
    "end": "1329600"
  },
  {
    "start": "1329000",
    "end": "1399000"
  },
  {
    "text": "organization so the first well the first you need to understand uh how we view event driven",
    "start": "1329600",
    "end": "1336360"
  },
  {
    "text": "ETL um and a way of loading the data on multiple clusters so we actually um I've",
    "start": "1336360",
    "end": "1341799"
  },
  {
    "text": "got simplified example here so we've got an external API uh and we're fetching data using a microservice so you can",
    "start": "1341799",
    "end": "1348720"
  },
  {
    "text": "think of this triggered every hour maybe and you're fing a subset of the data an incremental subset you're writing that",
    "start": "1348720",
    "end": "1354159"
  },
  {
    "text": "data into an S3 bucket and then sending a message to a specific topic um that",
    "start": "1354159",
    "end": "1360720"
  },
  {
    "text": "topic has two subscribers we've got two cues where it gets fored on it get",
    "start": "1360720",
    "end": "1366120"
  },
  {
    "text": "broadcast on and then we have a loader so we've got a microservice that actually initiates a load that's pulling",
    "start": "1366120",
    "end": "1371559"
  },
  {
    "text": "the queue and then loads that into red shift and you can see if you want to add another cluster so here we've got",
    "start": "1371559",
    "end": "1376880"
  },
  {
    "text": "analytics one Analytics 2 if you want to add analytics 3 the only thing you need to do is add another micros service and",
    "start": "1376880",
    "end": "1383159"
  },
  {
    "text": "put a subscriber to the topic and we've totally decoupled almost the extraction of the data the downloading of the data",
    "start": "1383159",
    "end": "1389400"
  },
  {
    "text": "writing test free from the actual loading into red shift this also could be used using spark um and another",
    "start": "1389400",
    "end": "1396240"
  },
  {
    "text": "process is we could have Hive tables on EMR so the pros are there's a lot of",
    "start": "1396240",
    "end": "1402039"
  },
  {
    "start": "1399000",
    "end": "1497000"
  },
  {
    "text": "flexibility on the loading supports incremental load on multiple clusters multiple environments um almost not tied",
    "start": "1402039",
    "end": "1408679"
  },
  {
    "text": "anymore to where it's getting written um again it's this concept of separating",
    "start": "1408679",
    "end": "1415240"
  },
  {
    "text": "the storage from the compute which is really valuable um it's simple to",
    "start": "1415240",
    "end": "1420320"
  },
  {
    "text": "understand I think as a pattern you can see that for example just going back um",
    "start": "1420320",
    "end": "1425400"
  },
  {
    "text": "if red shift is resizing and goes into read only for example or if it's in maintenance mode then uh that service",
    "start": "1425400",
    "end": "1432039"
  },
  {
    "text": "will just shut down and wait until red shift comes back and the items remain on the Queue so you can actually view the",
    "start": "1432039",
    "end": "1437120"
  },
  {
    "text": "workload almost of red shift you can choose to load one message at a time or many so that's why I I find that",
    "start": "1437120",
    "end": "1444159"
  },
  {
    "text": "incremental loads work really well using this system and using messaging and I think a lot of developers understand",
    "start": "1444159",
    "end": "1449360"
  },
  {
    "text": "this concept um so I'd recommend looking into it more um basic sequencing can be",
    "start": "1449360",
    "end": "1456200"
  },
  {
    "text": "done within a message for example you could load some data and then run some SQL on red shift again you're decoupling",
    "start": "1456200",
    "end": "1462039"
  },
  {
    "text": "creating a huge workflow you're just loading it incrementally running the prices that you need and then you're done the cons are obviously if you if",
    "start": "1462039",
    "end": "1470600"
  },
  {
    "text": "you want to create a huge workflow and your workflow is a thousand steps then obviously it's not really meant for that",
    "start": "1470600",
    "end": "1477480"
  },
  {
    "text": "um also there wasn't any fiveo support so which is the first in first out in",
    "start": "1477480",
    "end": "1483559"
  },
  {
    "text": "sqs but actually about a week ago that's been introduced in the US but not not in the west yet so that's no longer an",
    "start": "1483559",
    "end": "1491279"
  },
  {
    "text": "issue so if you have an order in which you want to load data you're able to use that functionality in the sqs",
    "start": "1491279",
    "end": "1498960"
  },
  {
    "start": "1497000",
    "end": "1571000"
  },
  {
    "text": "okay so that pattern of loading was important uh for the next five patterns that we're going to talk about in terms",
    "start": "1498960",
    "end": "1504840"
  },
  {
    "text": "of data pipelines first pattern um so this is um",
    "start": "1504840",
    "end": "1511559"
  },
  {
    "text": "we're using what we call a cluster based Big Data pipeline this is pattern number one so we have all the data already in",
    "start": "1511559",
    "end": "1518799"
  },
  {
    "text": "an S3 bucket we prepare the incremental load using what we call the content manager um so we know the exact files",
    "start": "1518799",
    "end": "1526120"
  },
  {
    "text": "that have been prepared we know almost the input folder um and then we're sending a message to uh an sqs um",
    "start": "1526120",
    "end": "1534600"
  },
  {
    "text": "cue which has the size of the cluster the target um input and output folders",
    "start": "1534600",
    "end": "1541760"
  },
  {
    "text": "and then that message gets read by The Raven EMR job Runner which spins up the",
    "start": "1541760",
    "end": "1546799"
  },
  {
    "text": "EMR cluster submits almost uh the specific jobs that we want to do on the",
    "start": "1546799",
    "end": "1551880"
  },
  {
    "text": "data um once that's executed and the cluster will shut down and then that job",
    "start": "1551880",
    "end": "1557679"
  },
  {
    "text": "Runner will then send a message onto a topic um and then we talked about the",
    "start": "1557679",
    "end": "1562799"
  },
  {
    "text": "pattern for loading that data so we'd have a loading process to load that data we know where those files are because",
    "start": "1562799",
    "end": "1568360"
  },
  {
    "text": "it's part of the metadata of the message so the pros are it supports",
    "start": "1568360",
    "end": "1574440"
  },
  {
    "start": "1571000",
    "end": "1658000"
  },
  {
    "text": "spark jobs natural language processing you can actually do your machine learning so spark has the machine learning pipeline now so you can",
    "start": "1574440",
    "end": "1580640"
  },
  {
    "text": "actually sequence specific steps in your machine learning process and um graph x is also there if you want to run graph",
    "start": "1580640",
    "end": "1587880"
  },
  {
    "text": "analytics at scale on a cluster um things have improved a lot uh since um",
    "start": "1587880",
    "end": "1595320"
  },
  {
    "text": "since previous Frameworks um compared to the previous Frameworks I'd say now with",
    "start": "1595320",
    "end": "1600440"
  },
  {
    "text": "spark things are a lot a lot simpler um and this allows you to run also ETL the",
    "start": "1600440",
    "end": "1606120"
  },
  {
    "text": "more traditional ETL at scale so that could be uh enrichment or um any paing",
    "start": "1606120",
    "end": "1611440"
  },
  {
    "text": "of specific data fields uh the spark jobs itself can contain many steps but",
    "start": "1611440",
    "end": "1616960"
  },
  {
    "text": "then EMR you also have a set of steps so you've got two ways almost of sequencing data um which is which is really good so",
    "start": "1616960",
    "end": "1625880"
  },
  {
    "text": "the downside of having an EMR cluster it does take 10 minutes to spin up um and obviously you're paying per hour so",
    "start": "1625880",
    "end": "1632880"
  },
  {
    "text": "imagine if your huge ETL job takes an hour one minute then you would actually pay for two hours so obviously that's a",
    "start": "1632880",
    "end": "1640039"
  },
  {
    "text": "limitation and there's always this concept of how many nodes do I have is it better to execute um using more nodes",
    "start": "1640039",
    "end": "1646840"
  },
  {
    "text": "to reduce the time is it cheaper than having less nodes and taking longer and",
    "start": "1646840",
    "end": "1652679"
  },
  {
    "text": "sometimes it's actually to have more nodes um especially with the on demand I'd",
    "start": "1652679",
    "end": "1658159"
  },
  {
    "start": "1658000",
    "end": "1689000"
  },
  {
    "text": "say Okay pattern number two is very similar to the first pattern except that we have streaming data so you can think",
    "start": "1658159",
    "end": "1665080"
  },
  {
    "text": "of it as our click stream for page views for the fundraising pages and we have a",
    "start": "1665080",
    "end": "1670120"
  },
  {
    "text": "spark process that actually just reads the Kinesis stream directly and then this could run any of the machine",
    "start": "1670120",
    "end": "1676000"
  },
  {
    "text": "learning it could run just simple counts on the actual data itself and then on a regular schedule it write those results",
    "start": "1676000",
    "end": "1682480"
  },
  {
    "text": "out into S3 send a message on the topic and then that data would get loaded as before using the the patterns of the red",
    "start": "1682480",
    "end": "1689360"
  },
  {
    "start": "1689000",
    "end": "1783000"
  },
  {
    "text": "shift loader so the pros are the process we can stream in parallel this is a",
    "start": "1689360",
    "end": "1695480"
  },
  {
    "text": "fault tolerant way of processing the data we benefit already out of the box from a huge open source community in",
    "start": "1695480",
    "end": "1702080"
  },
  {
    "text": "spark for streaming analytics a lot of the machine learning already works on that so we got like things like the streaming K means um you can actually",
    "start": "1702080",
    "end": "1709480"
  },
  {
    "text": "analyze clusters on the Fly using that and use Python Scala Java for that so",
    "start": "1709480",
    "end": "1718240"
  },
  {
    "text": "it's it's a great way of doing it you've got a huge Community behind it um you've got EMR spun up supported by Amazon it's",
    "start": "1718240",
    "end": "1726360"
  },
  {
    "text": "realtime analytics pipeline so you can also anything you can do on the IMR cluster um allows you to get the",
    "start": "1726360",
    "end": "1733200"
  },
  {
    "text": "analytics out very fast almost and you can adjust all of that um downside art",
    "start": "1733200",
    "end": "1738480"
  },
  {
    "text": "it's always on so the cluster's got to be always on because the data is always coming in so you can't turn it off um so",
    "start": "1738480",
    "end": "1744159"
  },
  {
    "text": "you got this dilemma or do I create a big cluster do I resize it based on the amount of data that's added that's",
    "start": "1744159",
    "end": "1750360"
  },
  {
    "text": "streaming in so at peak hours on your website do you actually make the cluster bigger do you downsize it or do you",
    "start": "1750360",
    "end": "1756080"
  },
  {
    "text": "leave it constant more expensive that's not out of the box actually so you need to think about that when you have some",
    "start": "1756080",
    "end": "1763279"
  },
  {
    "text": "of this code deployed on a streaming platform and you want to swap it out because you want tock how you count the events that's not a straightforward",
    "start": "1763279",
    "end": "1769919"
  },
  {
    "text": "process um and you'll see on blogs they talk about this also so you need to think about how you checko the existing",
    "start": "1769919",
    "end": "1776279"
  },
  {
    "text": "data so that you do not double count or you do not undercount so that's that's not a straightforward thing to do on",
    "start": "1776279",
    "end": "1783840"
  },
  {
    "text": "spark so now I'm going to talk a bit about the serverless I've got three more patterns to to cover I think this covers",
    "start": "1783840",
    "end": "1790320"
  },
  {
    "text": "quite a broad range so um you'll be very interested um so just going back to the",
    "start": "1790320",
    "end": "1795399"
  },
  {
    "text": "serverless concepts so this allows us to run machines or run our code based on",
    "start": "1795399",
    "end": "1801640"
  },
  {
    "text": "some trigger so the trigger could be a file gets dropped into S3 or um specific",
    "start": "1801640",
    "end": "1808559"
  },
  {
    "text": "records are written to Kinesis streams so and then these records are actually passed onto the Lander function as micro",
    "start": "1808559",
    "end": "1815799"
  },
  {
    "text": "batches where you can actually process them in parallel and all the autoscaling everything's done for you there's",
    "start": "1815799",
    "end": "1821279"
  },
  {
    "text": "there's no maintenance it's almost just deploying the code and everything works so's no you can't remote onto the Box um",
    "start": "1821279",
    "end": "1828039"
  },
  {
    "text": "it's a lot simpler even than containers I'll say um it alsoo scales with capacity and it's very very cheap so you",
    "start": "1828039",
    "end": "1835480"
  },
  {
    "text": "only pay in 100 millisecond increments rather than per hour for the ec2 instances um it's highly secure also as",
    "start": "1835480",
    "end": "1843279"
  },
  {
    "text": "it supports the IM roles um so a role could be for example to actually read",
    "start": "1843279",
    "end": "1849159"
  },
  {
    "text": "from Kinesis streams but not uh right to the Kinesis streams and it and you could",
    "start": "1849159",
    "end": "1856039"
  },
  {
    "text": "allow it basically in that way lock down the resources so very secure highly",
    "start": "1856039",
    "end": "1861720"
  },
  {
    "text": "scalable no maintenance so this is the first pattern",
    "start": "1861720",
    "end": "1867240"
  },
  {
    "start": "1864000",
    "end": "2020000"
  },
  {
    "text": "I'd say there are scenarios where actually people talk about big data but sometimes you have small data so you",
    "start": "1867240",
    "end": "1872840"
  },
  {
    "text": "have lots of files that arrive um sometimes and you don't necessarily want to spin up a cluster or have a machine",
    "start": "1872840",
    "end": "1878799"
  },
  {
    "text": "running on all the time um so as the file gets added you can have a trigger a",
    "start": "1878799",
    "end": "1885399"
  },
  {
    "text": "trigger which is an event source that you provide in the Lander function to say if an object is checked in is added",
    "start": "1885399",
    "end": "1892000"
  },
  {
    "text": "to S3 then trigger this Lander function the Lander in this case will load the data in memory process and run the ETL",
    "start": "1892000",
    "end": "1899720"
  },
  {
    "text": "so this could be for example very simple transformation of a URL extracting the domain name extracting specific patterns",
    "start": "1899720",
    "end": "1906000"
  },
  {
    "text": "within the URL and then pushing those results out in in adjacent format for example into S3 and then following the",
    "start": "1906000",
    "end": "1912720"
  },
  {
    "text": "same pattern as earlier we just send a message onto a specific topic for further processes to actually load that",
    "start": "1912720",
    "end": "1919000"
  },
  {
    "text": "data in so into red shift or into a spark cluster or Hive table um in",
    "start": "1919000",
    "end": "1924639"
  },
  {
    "text": "addition what's interesting you can actually also scan S3 for all those files using a client and then send",
    "start": "1924639",
    "end": "1930880"
  },
  {
    "text": "messages uh via a topic to the Lander so each Lander will be invoked for each file",
    "start": "1930880",
    "end": "1937720"
  },
  {
    "text": "almost so again the pros are it supports both the batch and the incremental",
    "start": "1939880",
    "end": "1945240"
  },
  {
    "text": "process um it's very useful for small files and frequently added files ones where you don't spin up a huge cluster",
    "start": "1945240",
    "end": "1950840"
  },
  {
    "text": "or have a machine running it also has the ability to preserve the file name so",
    "start": "1950840",
    "end": "1955919"
  },
  {
    "text": "those of you who use spark or the old map produce jobs you know that you end with you end up with part Zero part one",
    "start": "1955919",
    "end": "1962519"
  },
  {
    "text": "part two part free even if you just had one file on the input this one will preserve the same file on the input and output if you want it to it's a very",
    "start": "1962519",
    "end": "1969919"
  },
  {
    "text": "good patent I think for projecting specific columns enriching specific rows and doing paing on the Fly and again",
    "start": "1969919",
    "end": "1976039"
  },
  {
    "text": "this is you can do this in in node in Java 8 and in",
    "start": "1976039",
    "end": "1981039"
  },
  {
    "text": "Python um there are some limitations obviously to be aware of uh so there's 500 megabytes dis on a Landa and up to",
    "start": "1981080",
    "end": "1988159"
  },
  {
    "text": "1.5 gigabytes of RAM and the whole process needs to complete within five minutes um complex joins are a lot",
    "start": "1988159",
    "end": "1995039"
  },
  {
    "text": "harder to do because you're actually invoked with one specific file uh it is",
    "start": "1995039",
    "end": "2001039"
  },
  {
    "text": "possible but I would recommend using other approaches which I'll talk about afterwards um you may not want one file",
    "start": "2001039",
    "end": "2007559"
  },
  {
    "text": "an input and output so for example if you have one file added every minute you may not want to have one file on the",
    "start": "2007559",
    "end": "2013519"
  },
  {
    "text": "output you may want to aggregate them up almost to make it more easy to actually query and that's where we use the next",
    "start": "2013519",
    "end": "2019320"
  },
  {
    "text": "pattern pattern number four which I call the serverless streamify file pattern",
    "start": "2019320",
    "end": "2025039"
  },
  {
    "start": "2020000",
    "end": "2087000"
  },
  {
    "text": "which allows you to merge many small files that frequently added into a stream and then you can create the stream either using some analytics",
    "start": "2025039",
    "end": "2031919"
  },
  {
    "text": "processes or persist it into S3 so that's what I'm showing here so files are added into",
    "start": "2031919",
    "end": "2038120"
  },
  {
    "text": "S3 that will basically trigger a Lander the Lander will read the file in memory go for each of the rows and send them",
    "start": "2038120",
    "end": "2044519"
  },
  {
    "text": "off as records into the Kinesis streams or if you want to persist them in memory",
    "start": "2044519",
    "end": "2050638"
  },
  {
    "text": "um for fast querying then you can use Dynamo DB which is a nosql store um in",
    "start": "2050639",
    "end": "2056480"
  },
  {
    "text": "addition you can actually persist millions of files into smaller files using the fire hose so the Lander",
    "start": "2056480",
    "end": "2063079"
  },
  {
    "text": "function will read all those files streamify it and then those events into a Kinesis fire hose the fire hose will",
    "start": "2063079",
    "end": "2070118"
  },
  {
    "text": "actually capture up to 128 megabytes and upt 15 minutes of data and that will be",
    "start": "2070119",
    "end": "2076320"
  },
  {
    "text": "written into S3 and then we have a process as soon as that file gets written we can trigger another Lander",
    "start": "2076320",
    "end": "2082079"
  },
  {
    "text": "function to send an SS topic for further loading further down the",
    "start": "2082079",
    "end": "2087560"
  },
  {
    "start": "2087000",
    "end": "2128000"
  },
  {
    "text": "line so the pros are this is um a way you can support batch and incremental uh",
    "start": "2088320",
    "end": "2095440"
  },
  {
    "text": "in terms of loading so if I just go back um so you see at the top we've got the",
    "start": "2095440",
    "end": "2101000"
  },
  {
    "text": "Amazon SNS topic that triggers the Lander function so what you can do is if",
    "start": "2101000",
    "end": "2106079"
  },
  {
    "text": "You' got lots of files that you want to streamify you can just scan all of S3 um and send me one message per object in S3",
    "start": "2106079",
    "end": "2113400"
  },
  {
    "text": "send that as part of the topic and the Lander functions would execute in parallel read all the files in parallel",
    "start": "2113400",
    "end": "2118440"
  },
  {
    "text": "run all that and send it into Amazon Kinesis streams so that's the batch process the incremental one is when the",
    "start": "2118440",
    "end": "2124720"
  },
  {
    "text": "file dis gets added and then triggers a draw um so it's very useful for small and",
    "start": "2124720",
    "end": "2131520"
  },
  {
    "text": "frequently added files it's a very nice pattern you've got a seress way to also use a Lander to do a transformation",
    "start": "2131520",
    "end": "2138040"
  },
  {
    "text": "transform lots of files um into almost a smaller subset and use the fire hose to",
    "start": "2138040",
    "end": "2143720"
  },
  {
    "text": "actually persist that for further analysis or loading um conar again there",
    "start": "2143720",
    "end": "2149000"
  },
  {
    "text": "there similar limitations um for the Lander again I would re architect it if if it takes longer or if it's larger",
    "start": "2149000",
    "end": "2155359"
  },
  {
    "text": "files um the fire hose also has some limits I talked about the 15 minute window and 138 megabytes uh in buffer so",
    "start": "2155359",
    "end": "2164240"
  },
  {
    "text": "if one of those gets hit the file gets written but at least you're aggregating you know up to that level i' say for the",
    "start": "2164240",
    "end": "2170280"
  },
  {
    "text": "map produce spark 128 is is a good number to have um typically it doesn't",
    "start": "2170280",
    "end": "2176400"
  },
  {
    "text": "like small files so actually it's a good way to to um reduce the size of your files complex joins are also possible",
    "start": "2176400",
    "end": "2184480"
  },
  {
    "text": "they include introduce a lot more complexity in the architecture because I wouldn't recommend it for this specific",
    "start": "2184480",
    "end": "2190800"
  },
  {
    "start": "2191000",
    "end": "2341000"
  },
  {
    "text": "approach so next pattern is pattern number five which is the serverless",
    "start": "2191920",
    "end": "2197000"
  },
  {
    "text": "streaming analytics and persisting the",
    "start": "2197000",
    "end": "2201160"
  },
  {
    "text": "Stream So if you think about um what we talked about earlier so we had the streaming analytics um coming in so we",
    "start": "2202079",
    "end": "2209440"
  },
  {
    "text": "had our clickstream coming into Kinesis streams or into Dynamo if you want um and then micro batches would actually",
    "start": "2209440",
    "end": "2216240"
  },
  {
    "text": "get sent sent to the Lander function so it would receive all the clickstream data so that could be the page views The",
    "start": "2216240",
    "end": "2221960"
  },
  {
    "text": "Impressions the clicks for share on Facebook or specific scrolling events on on the website so all of that gets",
    "start": "2221960",
    "end": "2227880"
  },
  {
    "text": "received as a microbatch in a Lander the Lander itself in this case in this scenario does a running count so it",
    "start": "2227880",
    "end": "2234880"
  },
  {
    "text": "actually counts the number of page USS and impressions for a specific page and sends those into at the top we've got um",
    "start": "2234880",
    "end": "2243160"
  },
  {
    "text": "the ways of visualizing the data so we send that data into cloudwatch metrics",
    "start": "2243160",
    "end": "2248359"
  },
  {
    "text": "cloudwatch metrics is probably the first protocol so where you want to um have a seress way of charting data over time um",
    "start": "2248359",
    "end": "2256520"
  },
  {
    "text": "so it's very simple to send metrics I've written a blog post about this so uh you'll be able to find it I've got",
    "start": "2256520",
    "end": "2261640"
  },
  {
    "text": "details afterwards um and then you can chart over time you'll see the page views and the specific events um for",
    "start": "2261640",
    "end": "2268160"
  },
  {
    "text": "that if you want something more external facing um or even internal facing again",
    "start": "2268160",
    "end": "2274880"
  },
  {
    "text": "in a serverless fashion I'd recommend you create um an S3 bucket that acts as",
    "start": "2274880",
    "end": "2281240"
  },
  {
    "text": "a static website so you can actually run um node.js or JavaScript code to",
    "start": "2281240",
    "end": "2286560"
  },
  {
    "text": "actually chart that data um and the data counts the counts actually are in Dynamo",
    "start": "2286560",
    "end": "2293240"
  },
  {
    "text": "DB um equally you can create just a raw table and show the page counts um or",
    "start": "2293240",
    "end": "2299000"
  },
  {
    "text": "expose it via some API so as an alternative you can also run uh using",
    "start": "2299000",
    "end": "2305200"
  },
  {
    "text": "Kinesis and itics so you can run SQL on your click stream or data and write",
    "start": "2305200",
    "end": "2310640"
  },
  {
    "text": "those results into the fire hose and that will get written into S3 the Lander can also write to the fire hose and I've",
    "start": "2310640",
    "end": "2316760"
  },
  {
    "text": "got um I've got some code to do that also part of my blog so same pattern as number four the",
    "start": "2316760",
    "end": "2323680"
  },
  {
    "text": "data gets persisted into S3 and then we have a process where as soon as the file's written the object is written",
    "start": "2323680",
    "end": "2328800"
  },
  {
    "text": "into S3 we trigger another Lander function that then sends um an SNS message topic to a specific topic",
    "start": "2328800",
    "end": "2337560"
  },
  {
    "text": "um that will then propagate onto sqs and then get loaded so the pros of this approach are",
    "start": "2337560",
    "end": "2345119"
  },
  {
    "start": "2341000",
    "end": "2460000"
  },
  {
    "text": "we're stream processing without any running cluster um so there's there's no machines running actually everything is",
    "start": "2345119",
    "end": "2351440"
  },
  {
    "text": "managed by Amazon you've got freedom also of how you do everything um you can mix and match different parts of the of",
    "start": "2351440",
    "end": "2358000"
  },
  {
    "text": "this architecture um using the Lander and clinis analytics you also benefit from autoscaling um so again as more",
    "start": "2358000",
    "end": "2365480"
  },
  {
    "text": "machine more data arrives in your clickstream there's no need to actually",
    "start": "2365480",
    "end": "2370960"
  },
  {
    "text": "you know think about how you scale out for that peak times and traffic and that's something we've benefited a lot",
    "start": "2370960",
    "end": "2376720"
  },
  {
    "text": "at just giving so actually using this autoscaling facility um without having to worry about this especially we get",
    "start": "2376720",
    "end": "2384040"
  },
  {
    "text": "spikes of data sometimes so think about the ice bucket challenge for example um",
    "start": "2384040",
    "end": "2389359"
  },
  {
    "text": "no makeup selfie um there's various events that are huge in terms of the",
    "start": "2389359",
    "end": "2394400"
  },
  {
    "text": "traffic volumes so we need to be able to cope with those so this is a this is a good thing for other organizations looking at spikes of",
    "start": "2394400",
    "end": "2400880"
  },
  {
    "text": "traffic um so you can actually serve those metrics up using a static website",
    "start": "2400880",
    "end": "2406640"
  },
  {
    "text": "and the metrics themselves are in Dynamo DB so again there's a way of quering that really fast getting the aggregate",
    "start": "2406640",
    "end": "2411760"
  },
  {
    "text": "values out there's a choice of language also which is quite important so you can use Python SQL nodejs and Java 8 So",
    "start": "2411760",
    "end": "2419599"
  },
  {
    "text": "within um this process and the code can be changed without interruption do you remember for pattern number two I was",
    "start": "2419599",
    "end": "2425640"
  },
  {
    "text": "talking about Spark and swapping one application for another so if you're changing the way you're counting different events that's there's always",
    "start": "2425640",
    "end": "2431520"
  },
  {
    "text": "this checkpointing that you need to do you need to swap it over very carefully to make sure you don't lose data using",
    "start": "2431520",
    "end": "2436920"
  },
  {
    "text": "this pattern you don't need to worry about that the Lander also automatically switches over almost in terms of the micro batch data that it gets um the the",
    "start": "2436920",
    "end": "2445240"
  },
  {
    "text": "cons are um obviously there's some Lander limits we talked about the fire hose and any complex drains between",
    "start": "2445240",
    "end": "2451640"
  },
  {
    "text": "multiple streams such just how it's architected um it's probably not the best pattern for this and there's",
    "start": "2451640",
    "end": "2456880"
  },
  {
    "text": "Alternatives I'll talk about afterwards so based on this five",
    "start": "2456880",
    "end": "2463599"
  },
  {
    "start": "2460000",
    "end": "2715000"
  },
  {
    "text": "patterns I'm just going to talk about some of the serverless recommendations and the best practices that you can use",
    "start": "2463599",
    "end": "2468640"
  },
  {
    "text": "in your organization so this is a table showing when serverless is good and when",
    "start": "2468640",
    "end": "2473720"
  },
  {
    "text": "we maybe we could use EMR instead so pattern number three uh the small data",
    "start": "2473720",
    "end": "2479359"
  },
  {
    "text": "pipeline so this is where we take one file that isn't added that frequently and um we're pausing it process ing it",
    "start": "2479359",
    "end": "2487240"
  },
  {
    "text": "running transformation projecting specific rows um so this is a great pattern for that um also when you want",
    "start": "2487240",
    "end": "2493640"
  },
  {
    "text": "to preserve the file names then this is a good pattern if you have files that are bigger than 400 megabytes and that",
    "start": "2493640",
    "end": "2500040"
  },
  {
    "text": "take maybe longer than 5 minutes to process um I'd recommend you use spark on",
    "start": "2500040",
    "end": "2505599"
  },
  {
    "text": "EMR if you have so this is one file to one file almost if you have many files",
    "start": "2505599",
    "end": "2511520"
  },
  {
    "text": "um to one file I'd recommend using spark also so if you're doing a complex drain um",
    "start": "2511520",
    "end": "2516760"
  },
  {
    "text": "I'd recommend that but if you think about clickstream you're always you've got a consistency almost it's almost",
    "start": "2516760",
    "end": "2521839"
  },
  {
    "text": "when you want to do complex joins um there's other other patterns so spark would be",
    "start": "2521839",
    "end": "2527760"
  },
  {
    "text": "better uh also not recommended for pattern number three to use for merging",
    "start": "2527760",
    "end": "2532800"
  },
  {
    "text": "small files into one pattern number four so this is where we streamify the file and then merge",
    "start": "2532800",
    "end": "2539800"
  },
  {
    "text": "them into a larger file so if you have millions of files you've got this ability to use the fire host to actually",
    "start": "2539800",
    "end": "2545800"
  },
  {
    "text": "compress the data into smaller amounts um and this is a great way uh this is something one of the limitations of",
    "start": "2545800",
    "end": "2552760"
  },
  {
    "text": "spark where if you have lots of files it's not as efficient to to process so this is a pattern almost to reduce your",
    "start": "2552760",
    "end": "2559160"
  },
  {
    "text": "data volumes um so if you have files again greater than 400 megabytes or if you",
    "start": "2559160",
    "end": "2565720"
  },
  {
    "text": "want to do complex joins between the streams I'd recommend using spark on on the right pattern number five so this is",
    "start": "2565720",
    "end": "2572559"
  },
  {
    "text": "a way of um running ser analytics using Lander functions and",
    "start": "2572559",
    "end": "2580400"
  },
  {
    "text": "Dynamo DB and persisting the stream also further down further down line the",
    "start": "2580400",
    "end": "2585599"
  },
  {
    "text": "streaming analytics actually is the results are persisted in Dynamo DB and",
    "start": "2585599",
    "end": "2592640"
  },
  {
    "text": "those results are then presented in real time on the dashboard using node.js I've",
    "start": "2592640",
    "end": "2598280"
  },
  {
    "text": "used in my blog you'll read about it I'm using charts.js to do that and also just",
    "start": "2598280",
    "end": "2604119"
  },
  {
    "text": "drawing out some tables so you can use almost any of the JavaScript Frameworks to actually integrate with Dynamo DB",
    "start": "2604119",
    "end": "2609559"
  },
  {
    "text": "that's that's a great way to do it so if you want to do complex joins",
    "start": "2609559",
    "end": "2615280"
  },
  {
    "text": "again that's more of a spark so you can see you want to save money you don't want to pay per hour per node uh there's",
    "start": "2615280",
    "end": "2621720"
  },
  {
    "text": "a serverless pattern that's available for you so Lander functions are really good",
    "start": "2621720",
    "end": "2628680"
  },
  {
    "text": "for any transformation at row or event level for paing projecting enriching and",
    "start": "2628680",
    "end": "2634800"
  },
  {
    "text": "filtering the data uh so you can think of it as a record level like this and you're doing Transformations adding more columns um",
    "start": "2634800",
    "end": "2642720"
  },
  {
    "text": "rather than downwards so working with other AWS",
    "start": "2642720",
    "end": "2648200"
  },
  {
    "text": "offerings like S3 SNS cloudwatch I am um that's that's it's really well",
    "start": "2648200",
    "end": "2653280"
  },
  {
    "text": "integrated I'd say so you benefit from the ecosystem already um so for example for the triggers if an object is added",
    "start": "2653280",
    "end": "2659839"
  },
  {
    "text": "to S3 that will trigger a Lander SNS triggers Landers um micro batches in",
    "start": "2659839",
    "end": "2665040"
  },
  {
    "text": "Kinesis streams trigger Landers also um as I talked about the IM roles also so",
    "start": "2665040",
    "end": "2670880"
  },
  {
    "text": "these are specific policies that can be assigned to the role for example where the Lander function can only read from Kinesis it's not allowed to write or um",
    "start": "2670880",
    "end": "2679040"
  },
  {
    "text": "it can only write to Dynamo DB for the counters for example and it supports VPC actually so you can totally isolate um",
    "start": "2679040",
    "end": "2684920"
  },
  {
    "text": "the Lander function within the service they're really good also for in",
    "start": "2684920",
    "end": "2690520"
  },
  {
    "text": "terms of packaging so you can package up all your dependencies um and then using Comm line deploy those in terms of of",
    "start": "2690520",
    "end": "2696160"
  },
  {
    "text": "versions so you can actually Version Control a lot of that as I said earlier the stream also",
    "start": "2696160",
    "end": "2704240"
  },
  {
    "text": "um stream processing using spark is not easy when you want to swap over the applications uh with the Lander",
    "start": "2704240",
    "end": "2709720"
  },
  {
    "text": "functions that's that's fairly straightforward it will just swap over a different microbatch as they",
    "start": "2709720",
    "end": "2715400"
  },
  {
    "text": "arrive so it's important to understand some of the limits also of the Lander so we talked about some of the the memory",
    "start": "2715400",
    "end": "2721839"
  },
  {
    "text": "the local disc and the execution time um if you think about real time analytics so if you processing something that",
    "start": "2721839",
    "end": "2728440"
  },
  {
    "text": "takes more than 5 minutes then it's not going to be real time anymore so you need to think about you need to re",
    "start": "2728440",
    "end": "2734400"
  },
  {
    "text": "architect that so it's real time so i' say anything less than a minute maybe um you've also got this concurrency",
    "start": "2734400",
    "end": "2742079"
  },
  {
    "text": "limits so you need to be aware of which is per account again that can get increased also if you ask Amazon um",
    "start": "2742079",
    "end": "2749240"
  },
  {
    "text": "Dynamo and Amazon Kinesis also have a different set of Shard iterators so if",
    "start": "2749240",
    "end": "2754680"
  },
  {
    "text": "you need more capacity you you can add more shards to Kinesis um and as almost one Shard is linked to one Lander so you",
    "start": "2754680",
    "end": "2761599"
  },
  {
    "text": "need to be aware of that also um but effectively we' taken all of our production click stream we ran it through Landers and we didn't have any",
    "start": "2761599",
    "end": "2768160"
  },
  {
    "text": "any issues with that so again we talked about the complex joints are harder so that's more",
    "start": "2768160",
    "end": "2773680"
  },
  {
    "text": "suited for using spark on EMR again using our pattern number one um and the",
    "start": "2773680",
    "end": "2780680"
  },
  {
    "text": "reason for that is uh spark has more context it can read everything anything from S3 whereas here with trying to",
    "start": "2780680",
    "end": "2786160"
  },
  {
    "text": "process just a subset of the data almost as fast in real time um in addition if",
    "start": "2786160",
    "end": "2791839"
  },
  {
    "text": "you want to use orc orc or Park which are the big data formats so these are",
    "start": "2791839",
    "end": "2798400"
  },
  {
    "text": "colum formats where the data is compressed um similar to Red shift in some ways um and allows you to quw a",
    "start": "2798400",
    "end": "2805640"
  },
  {
    "text": "specific column it's very fast it's also supports compression um the actual data",
    "start": "2805640",
    "end": "2811079"
  },
  {
    "text": "is um you've got almost the name of the column and you have the data data type of the column and whether it's mandatory",
    "start": "2811079",
    "end": "2816839"
  },
  {
    "text": "or not as in terms of a field so that's better done in spark because it's got access to more of the",
    "start": "2816839",
    "end": "2823359"
  },
  {
    "start": "2823000",
    "end": "2896000"
  },
  {
    "text": "data so Lander recommendations I'd recommend you create a Sandbox",
    "start": "2823920",
    "end": "2829040"
  },
  {
    "text": "environment and load all your production data in that to make sure everything's running smoothly think about this",
    "start": "2829040",
    "end": "2835040"
  },
  {
    "text": "deployment how do you automate the deployment process of your code um because Landers are serverless it's very",
    "start": "2835040",
    "end": "2841480"
  },
  {
    "text": "easy to deploy the code the simplest method is to actually log onto production paste code in but obviously",
    "start": "2841480",
    "end": "2846800"
  },
  {
    "text": "that's not the Ops wouldn't be happy with you if you do that um it's also important to use the Version Control to",
    "start": "2846800",
    "end": "2854280"
  },
  {
    "text": "actually deploy Lander so you can roll back very quickly um and for me um yeah",
    "start": "2854280",
    "end": "2860119"
  },
  {
    "text": "one of the big things is about reducing that execution time so you've got that five minutes you want to minimize almost",
    "start": "2860119",
    "end": "2865359"
  },
  {
    "text": "the time that's taken to actually run through the data and do the ETL or the analysis of the data um so I found that",
    "start": "2865359",
    "end": "2873640"
  },
  {
    "text": "minimizing the actual logging so logging only when you really need it it sped up the the speed of the Lander optimizing",
    "start": "2873640",
    "end": "2880119"
  },
  {
    "text": "codes so for example rather than writing every record out you would backat from up into memory in terms of counters and",
    "start": "2880119",
    "end": "2885800"
  },
  {
    "text": "then write that into Dynamo there's other optimizations I talk about in my blog posts so I've got",
    "start": "2885800",
    "end": "2893520"
  },
  {
    "text": "links afterwards for",
    "start": "2893520",
    "end": "2896480"
  },
  {
    "start": "2896000",
    "end": "2970000"
  },
  {
    "text": "that so event driven pipelines so again think about how maybe you can move away",
    "start": "2899000",
    "end": "2904280"
  },
  {
    "text": "from workflow for some scenario or move away from the directed as click graphs into more incremental loads so if",
    "start": "2904280",
    "end": "2911559"
  },
  {
    "text": "you think about clickstream data it's not something you run overnight it's arriving all the time continuously so",
    "start": "2911559",
    "end": "2917240"
  },
  {
    "text": "you might as well use that data immediately and make it available um",
    "start": "2917240",
    "end": "2922680"
  },
  {
    "text": "think about using some of the managed Services also they're there they're highly scalable highly available and",
    "start": "2922680",
    "end": "2928000"
  },
  {
    "text": "that's all built in for you with the security um and all the Lander functions for that the state is not maintaining",
    "start": "2928000",
    "end": "2935720"
  },
  {
    "text": "within the Lander you can maintain it outside the Lander so in Dynamo DB in S3 or in RDS and often you'll see if you",
    "start": "2935720",
    "end": "2943359"
  },
  {
    "text": "actually break down a directed AC click graph you can probably flatten most of it out and that that's what we found in",
    "start": "2943359",
    "end": "2950400"
  },
  {
    "text": "our organization especially if you're loading data and then running some SQL after it straight after it um you're",
    "start": "2950400",
    "end": "2956280"
  },
  {
    "text": "almost simplifying rather than having this huge workflow that executes every night a big data pipeline you can actually run smaller batches of data",
    "start": "2956280",
    "end": "2963400"
  },
  {
    "text": "continuously incrementally and that allows you to have the data available in closer to real",
    "start": "2963400",
    "end": "2970000"
  },
  {
    "start": "2970000",
    "end": "3082000"
  },
  {
    "text": "time think about uh so this is probably one of the key messages think about how you can decouple the loading in the ETL",
    "start": "2970000",
    "end": "2977160"
  },
  {
    "text": "elt so elt is probably the Big Data term where the schema is on the read rather",
    "start": "2977160",
    "end": "2982640"
  },
  {
    "text": "than creating the schema up front um so if you think about using messaging you can loosly couple actually all the",
    "start": "2982640",
    "end": "2988839"
  },
  {
    "text": "systems so the data producer from almost the one that loads the data into other clusters so you can actually load it",
    "start": "2988839",
    "end": "2995319"
  },
  {
    "text": "into into for example how we use it in just giving uh in production we have four different clusters and each cluster",
    "start": "2995319",
    "end": "3002559"
  },
  {
    "text": "of red shift has different purpose one of them is to serve analytics the other one is to run really complex queries",
    "start": "3002559",
    "end": "3009559"
  },
  {
    "text": "that data scientists that really push the cluster to the limits the other one is purely for graph analytics so we're",
    "start": "3009559",
    "end": "3015119"
  },
  {
    "text": "running some of the graph analytics in red shift so obviously that one's always Computing um and then we have more gen",
    "start": "3015119",
    "end": "3022400"
  },
  {
    "text": "General red shift cluster used by the developers and Ops when we want to investigate so you can almost have",
    "start": "3022400",
    "end": "3028280"
  },
  {
    "text": "you're taking it a level further you not just having this massive single cluster you've got many clusters and the Dr is",
    "start": "3028280",
    "end": "3034119"
  },
  {
    "text": "built in another another way around I'd say um because the data is replicated and loaded on all the Clusters and it's",
    "start": "3034119",
    "end": "3039200"
  },
  {
    "text": "all the same same scripts are executed all of them so the ETL is the same we benefit from the ability to add new",
    "start": "3039200",
    "end": "3045960"
  },
  {
    "text": "clusters on demand and equally if a cluster goes down I can spin up a cluster reload all",
    "start": "3045960",
    "end": "3052640"
  },
  {
    "text": "the data purely using messaging so without running any types of workflows or",
    "start": "3052640",
    "end": "3057839"
  },
  {
    "text": "anything so think about exceptions also that's really important so if um an EMR",
    "start": "3057839",
    "end": "3063880"
  },
  {
    "text": "job run fails or a Lander function fails for some reason you need to think about how you reload that so it's important to",
    "start": "3063880",
    "end": "3070319"
  },
  {
    "text": "have uh notifications to do that and a dashboard and actually if there is a",
    "start": "3070319",
    "end": "3076200"
  },
  {
    "text": "failure what we do is just send a message and that's it and it's done we don't need to replay any workflow or",
    "start": "3076200",
    "end": "3082960"
  },
  {
    "start": "3082000",
    "end": "3168000"
  },
  {
    "text": "anything so I'd recommend you master all your data in S3 so have a single source of Truth in S3 and make that available",
    "start": "3082960",
    "end": "3090520"
  },
  {
    "text": "um to any of the the services that want to run ETL or machine learning at scale",
    "start": "3090520",
    "end": "3097079"
  },
  {
    "text": "um yeah we like to think of the EMR clusters spark clusters Hive clusters",
    "start": "3097079",
    "end": "3102359"
  },
  {
    "text": "Amazon red shift clusters as disposable because we have this ability to actually reload any of the data on the",
    "start": "3102359",
    "end": "3109280"
  },
  {
    "text": "Fly uh and incrementally as well as in batch if we need it um data storage so",
    "start": "3109280",
    "end": "3116240"
  },
  {
    "text": "these is just more traditional patterns uh there are best practices these are the useful ones so I have one file per",
    "start": "3116240",
    "end": "3122559"
  },
  {
    "text": "file type in terms of the prefix um use an incremental loading notation so if",
    "start": "3122559",
    "end": "3128440"
  },
  {
    "text": "you want to load a specific year month day hour that really helps um if you've",
    "start": "3128440",
    "end": "3133480"
  },
  {
    "text": "got data such as comma separated or tab separated from an external vendor and that's the only format that they support",
    "start": "3133480",
    "end": "3139440"
  },
  {
    "text": "or some CSV file for some analyst for example it's best to have another file",
    "start": "3139440",
    "end": "3144799"
  },
  {
    "text": "that actually describes the file in terms of metadata or some metadata layer to do that um if it's oral par that's",
    "start": "3144799",
    "end": "3152359"
  },
  {
    "text": "actually built into so you don't need to worry about that it's more for those other formats and stick with one consistent datetime format for all your",
    "start": "3152359",
    "end": "3158960"
  },
  {
    "text": "data so you know that no matter what you do everything's in UTC and consistent uh",
    "start": "3158960",
    "end": "3164319"
  },
  {
    "text": "compress and encrypt your data also so here's some of my uh blog posts",
    "start": "3164319",
    "end": "3171040"
  },
  {
    "start": "3168000",
    "end": "3184000"
  },
  {
    "text": "so I've got one on the uh compute blog another one on the Big Data Amazon blog",
    "start": "3171040",
    "end": "3176559"
  },
  {
    "text": "I've got an medium article we've actually also open sourced some of the code so it's on github.com",
    "start": "3176559",
    "end": "3183520"
  },
  {
    "text": "justgiving so just wrapping up so um today we talked about some of the challenges of the existing data",
    "start": "3183760",
    "end": "3189440"
  },
  {
    "start": "3184000",
    "end": "3219000"
  },
  {
    "text": "pipelines um and we proposed an alternative way of doing ETL using event driven and serverless pipelines and we",
    "start": "3189440",
    "end": "3197440"
  },
  {
    "text": "covered the five different patterns that you can Implement in your organization um so the first two",
    "start": "3197440",
    "end": "3202880"
  },
  {
    "text": "required a cluster the other three actually purely serverless the recommendations I would say is uh use",
    "start": "3202880",
    "end": "3208920"
  },
  {
    "text": "serverless where possible and manage services think about how also you",
    "start": "3208920",
    "end": "3214280"
  },
  {
    "text": "decouple the loading in the ETL or",
    "start": "3214280",
    "end": "3218760"
  },
  {
    "start": "3219000",
    "end": "3245000"
  },
  {
    "text": "elt so I'd like to to thank everybody today so I hope you find everything",
    "start": "3220119",
    "end": "3225880"
  },
  {
    "text": "useful and you can reuse it in your organization as well as maybe if you can think about the tech for good sector and",
    "start": "3225880",
    "end": "3232400"
  },
  {
    "text": "how you can use those skills in that area and here are my contact details if you want to connect with me on LinkedIn",
    "start": "3232400",
    "end": "3238319"
  },
  {
    "text": "or drop me an email thank you very much for your",
    "start": "3238319",
    "end": "3242680"
  },
  {
    "text": "attention",
    "start": "3244839",
    "end": "3247839"
  }
]