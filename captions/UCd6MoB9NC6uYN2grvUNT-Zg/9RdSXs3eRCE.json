[
  {
    "start": "0",
    "end": "14000"
  },
  {
    "text": "In this video, you'll see how you can build a modern\ndata architecture using native AWS services. ",
    "start": "160",
    "end": "5420"
  },
  {
    "text": "With this approach, you can seamlessly \nintegrate your data lake and data warehouse,",
    "start": "6000",
    "end": "9840"
  },
  {
    "text": "enabling easy data movement and\nunified governance over your data.",
    "start": "9840",
    "end": "13120"
  },
  {
    "start": "14000",
    "end": "131000"
  },
  {
    "text": "In the first use case, we’ll show low-code \nand no-code approaches for ingesting,",
    "start": "14400",
    "end": "18080"
  },
  {
    "text": "transforming, moving data into a \ndata warehouse ready for analytics.",
    "start": "18080",
    "end": "21633"
  },
  {
    "text": "We’ll begin with a MySQL instance \ncontaining a sample database in Amazon RDS.",
    "start": "22341",
    "end": "26529"
  },
  {
    "text": "In AWS Glue, we created a Java Database Connectivity\n(JDBC) connection pointing to the sample database.",
    "start": "27173",
    "end": "33742"
  },
  {
    "text": "The database table was then crawled to generate\nmetadata in an AWS Glue Data Catalog table.",
    "start": "34237",
    "end": "38889"
  },
  {
    "text": "We then created a job in Amazon Glue Studio to \ningest data from the AWS Glue Data Catalog table",
    "start": "41020",
    "end": "46860"
  },
  {
    "text": "into an Amazon Simple Storage \nService (Amazon S3) data lake.",
    "start": "46860",
    "end": "50653"
  },
  {
    "text": "In the Data source node, we can see the \nschema of the table and preview the records.",
    "start": "52800",
    "end": "56790"
  },
  {
    "text": "In the Transform node, we can view the \ntargets, adjust the data types as necessary,",
    "start": "59200",
    "end": "63680"
  },
  {
    "text": "and preview the results.",
    "start": "63680",
    "end": "64948"
  },
  {
    "text": "Here, we can see that the data target is \nthe S3 bucket, and the format is Parquet.",
    "start": "67161",
    "end": "71099"
  },
  {
    "text": "This job will also create a new \ntable in the AWS Glue Data Catalog.",
    "start": "71641",
    "end": "75301"
  },
  {
    "text": "AWS Glue supports many data sources, \ntargets, and transformations.",
    "start": "77512",
    "end": "81261"
  },
  {
    "text": "Additional data sources are \navailable in AWS Marketplace.",
    "start": "82240",
    "end": "85442"
  },
  {
    "text": "For instance, we could bring in \ndata from Salesforce or Splunk.",
    "start": "87617",
    "end": "90770"
  },
  {
    "text": "Back in Amazon Glue Studio, \nlet's view the job runs.",
    "start": "92880",
    "end": "95933"
  },
  {
    "text": "Here’s confirmation that the \nmysql_to_datalake job had a successful run.",
    "start": "98125",
    "end": "101645"
  },
  {
    "text": "Let's visit the S3 bucket \nwhere the data was ingested.",
    "start": "102201",
    "end": "104681"
  },
  {
    "text": "The bucket already contains \na number of Parquet files.",
    "start": "106989",
    "end": "109498"
  },
  {
    "text": "Going to the AWS Glue Data Catalog, we can see that the\ndata_lake_source_table was generated as expected.",
    "start": "110584",
    "end": "116305"
  },
  {
    "text": "Now that the data has been ingested into S3, let’s see\nhow it can be transformed using AWS Glue DataBrew.",
    "start": "117444",
    "end": "123184"
  },
  {
    "text": "Glue DataBrew is a visual tool that enables users to quickly\nprepare data for analytics without writing any code.",
    "start": "123760",
    "end": "129645"
  },
  {
    "text": "Here, we have a dataset called “data-lake-source-table,”\nwhich is connected to the AWS Glue Data Catalog table with the same name.",
    "start": "131832",
    "end": "138149"
  },
  {
    "text": "We have also created a demo project,",
    "start": "140429",
    "end": "142429"
  },
  {
    "text": "which provides an interactive workspace \nfor applying data transformations.",
    "start": "142429",
    "end": "145914"
  },
  {
    "text": "Our demo-project lets us see the \ndata we ingested into the S3 bucket.",
    "start": "148053",
    "end": "151573"
  },
  {
    "text": "On the right, is the “recipe” of \ntransformation steps we’ll apply.",
    "start": "152084",
    "end": "155294"
  },
  {
    "text": "AWS Glue DataBrew includes more \nthan 250 prebuilt transformations.",
    "start": "156428",
    "end": "160853"
  },
  {
    "text": "Each recipe can include up to 100 transformations.",
    "start": "161217",
    "end": "164049"
  },
  {
    "text": "Let's review the first step.",
    "start": "165061",
    "end": "166261"
  },
  {
    "text": "This \"merge columns\" step transforms \nthe data from three columns—first_name,",
    "start": "168560",
    "end": "172560"
  },
  {
    "text": "middle_name, and last_name—into a \nsingle column represented as full_name.",
    "start": "172560",
    "end": "176469"
  },
  {
    "text": "We can preview the transformation \nbefore applying it to the table.",
    "start": "178189",
    "end": "181004"
  },
  {
    "text": "Let's review the second step of the recipe.",
    "start": "184605",
    "end": "186444"
  },
  {
    "text": "This step creates a column called derived_sex to \nreflect a person’s sex as either male or female.",
    "start": "187628",
    "end": "192522"
  },
  {
    "text": "Here we can see a preview of the transformation.",
    "start": "192981",
    "end": "194944"
  },
  {
    "text": "The third step uses logical conditions \nto define three cases related to age.",
    "start": "198909",
    "end": "203034"
  },
  {
    "text": "The first case filters the age values between 21 and 30,\nthe second between 31 and 40, and the third between 41 and 50.",
    "start": "205209",
    "end": "217394"
  },
  {
    "text": "Anyone younger or older is filtered out of the table.",
    "start": "219706",
    "end": "222260"
  },
  {
    "text": "When we preview the changes, we \nsee the new age_group column.",
    "start": "223449",
    "end": "226542"
  },
  {
    "text": "The fourth and fifth steps in this recipe delete the columns\nmade redundant by the previous three steps.",
    "start": "232489",
    "end": "237121"
  },
  {
    "text": "With AWS Glue DataBrew, we can publish a \nrecipe and apply it to multiple datasets.",
    "start": "237905",
    "end": "242450"
  },
  {
    "start": "244000",
    "end": "291000"
  },
  {
    "text": "On the Jobs tab, we see our published recipe jobs.",
    "start": "244589",
    "end": "247614"
  },
  {
    "text": "We can select one to review \nits run history and details.",
    "start": "249805",
    "end": "252681"
  },
  {
    "text": "We can also edit the output settings to \napply the job to a different dataset.",
    "start": "254913",
    "end": "258432"
  },
  {
    "text": "In the current configuration, the data output is \nan AWS Glue Data Catalog table.",
    "start": "259585",
    "end": "263947"
  },
  {
    "text": "This eliminates the steps needed to crawl the S3 bucket and \napply the recipe to the table in Glue DataBrew.",
    "start": "264429",
    "end": "269608"
  },
  {
    "text": "This is how the output table appears \nin the AWS Glue Data Catalog.",
    "start": "271832",
    "end": "275465"
  },
  {
    "text": "We can query this data \ndirectly using Amazon Athena.",
    "start": "277713",
    "end": "280561"
  },
  {
    "text": "The final piece of this architecture \nis the Amazon Redshift data warehouse,",
    "start": "282853",
    "end": "286613"
  },
  {
    "text": "where we've created a cluster \nto store our transformed data.",
    "start": "286613",
    "end": "289313"
  },
  {
    "start": "291000",
    "end": "339000"
  },
  {
    "text": "On the Editor tab, we can see the schema \nattributes represented on the redshift_demo_table.",
    "start": "291561",
    "end": "296130"
  },
  {
    "text": "In AWS Glue, we can see the JDBC connection \npointing to the Redshift demo table,",
    "start": "296761",
    "end": "301560"
  },
  {
    "text": "which has been crawled as before.",
    "start": "301561",
    "end": "302960"
  },
  {
    "text": "Here we see the metadata that appears in the AWS Glue\nData Catalog after crawling the redshift_demo_table.",
    "start": "305437",
    "end": "310746"
  },
  {
    "text": "In Amazon Glue Studio, we can see the ETL job\nthat loads the transformed data into the data warehouse.",
    "start": "313073",
    "end": "318714"
  },
  {
    "text": "From the Data source node, we can review \nthe output schema and preview the results.",
    "start": "319641",
    "end": "323504"
  },
  {
    "text": "Let's do the same for the Data target node.",
    "start": "324421",
    "end": "326421"
  },
  {
    "text": "Let's review the most recent job run.",
    "start": "328577",
    "end": "330494"
  },
  {
    "text": "The data_lake_to_data_warehouse \nrun completed successfully.",
    "start": "332681",
    "end": "335703"
  },
  {
    "text": "Let's return to Amazon Redshift.",
    "start": "336149",
    "end": "337875"
  },
  {
    "start": "339000",
    "end": "487000"
  },
  {
    "text": "The data is ready to be queried using \nthe Amazon Redshift query editor.",
    "start": "340121",
    "end": "343529"
  },
  {
    "text": "Now that we have reviewed the steps for \nbuilding a modern data architecture,",
    "start": "344693",
    "end": "348052"
  },
  {
    "text": "let's see how we can orchestrate \nand automate this entire workflow.",
    "start": "348053",
    "end": "351068"
  },
  {
    "text": "Here we can see our flow visualized \nin AWS Step Functions Workflow Studio.",
    "start": "353313",
    "end": "357473"
  },
  {
    "text": "The first node orchestrates the ingestion \nof data from MySQL into the data lake.",
    "start": "359757",
    "end": "363668"
  },
  {
    "text": "The second node orchestrates the AWS Glue \nDataBrew job that transforms the data.",
    "start": "365837",
    "end": "370157"
  },
  {
    "text": "The third node orchestrates the AWS Glue job that loads the transformed\ndata from the data lake into the Redshift data warehouse.",
    "start": "372489",
    "end": "379019"
  },
  {
    "text": "Once we've executed the steps, we can monitor \ntheir progress using the Graph inspector.",
    "start": "381493",
    "end": "385585"
  },
  {
    "text": "Now let’s look at a different use case that involves\nsharing the data lake with other accounts.",
    "start": "388133",
    "end": "392054"
  },
  {
    "text": "We can use AWS Lake Formation to \nregister the location of the data lake.",
    "start": "392857",
    "end": "396617"
  },
  {
    "text": "This requires selecting the path to the S3 bucket and assigning\nan AWS Identity and Access Management (AWS IAM) role.",
    "start": "399105",
    "end": "406368"
  },
  {
    "text": "In this case, we’ve already \nregistered the data lake location.",
    "start": "408841",
    "end": "411590"
  },
  {
    "text": "Note that AWS Lake Formation \nshares the AWS Glue Data Catalog.",
    "start": "412053",
    "end": "416171"
  },
  {
    "text": "Let's view the tables.",
    "start": "416533",
    "end": "417650"
  },
  {
    "text": "When sharing data with consumer accounts, we can \nshare the entire database or just parts of it,",
    "start": "420000",
    "end": "424240"
  },
  {
    "text": "such as a few tables or even \na few columns within a table.",
    "start": "424240",
    "end": "427245"
  },
  {
    "text": "To share the data_lake_source_table,",
    "start": "429677",
    "end": "431517"
  },
  {
    "text": "we can simply select it and then grant \ndata permissions to specific accounts.",
    "start": "431517",
    "end": "435357"
  },
  {
    "text": "Consumer accounts are external accounts.",
    "start": "437025",
    "end": "439225"
  },
  {
    "text": "We can choose which table permissions to grant.",
    "start": "442397",
    "end": "444438"
  },
  {
    "text": "Here’s where we choose whether to allow \nall data access or column-based access.",
    "start": "449165",
    "end": "453150"
  },
  {
    "text": "Because we’ve already granted access to \nthis table, we'll cancel and move on.",
    "start": "453641",
    "end": "457121"
  },
  {
    "text": "Under Permissions, we can see that \nthe source table has been shared with",
    "start": "459520",
    "end": "462240"
  },
  {
    "text": "the AWS account corresponding to the principal ID.",
    "start": "462240",
    "end": "465162"
  },
  {
    "text": "We can view the list of shared objects in \nthe AWS Resource Access Manager console.",
    "start": "467657",
    "end": "471897"
  },
  {
    "text": "Now that the table has been \nshared with a consumer account,",
    "start": "474441",
    "end": "476761"
  },
  {
    "text": "permissions can be granted \nto users within that account.",
    "start": "476761",
    "end": "479280"
  },
  {
    "text": "In the IAM console, we have two users, \ndata_analyst and data_scientist,",
    "start": "480000",
    "end": "484157"
  },
  {
    "text": "who require access.",
    "start": "484157",
    "end": "485494"
  },
  {
    "start": "487000",
    "end": "665000"
  },
  {
    "text": "We'll grant them access to this \nlocal database in AWS Glue.",
    "start": "487861",
    "end": "491199"
  },
  {
    "text": "When the consumer account is not part of the \nsame organization as the data lake account,",
    "start": "493713",
    "end": "497633"
  },
  {
    "text": "it must accept the resource \nshare in Resource Access Manager.",
    "start": "497633",
    "end": "500697"
  },
  {
    "text": "In this case, the consumer account has already \naccepted the resource share. Let's view it.",
    "start": "501253",
    "end": "505478"
  },
  {
    "text": "The Glue data lake source table, Data Catalog, \nand local database are all shared resources.",
    "start": "507997",
    "end": "512666"
  },
  {
    "text": "The next step is to create a sharable \nresource link from AWS Lake Formation.",
    "start": "515061",
    "end": "518990"
  },
  {
    "text": "This can be done for the entire \ndatabase or a single table.",
    "start": "519497",
    "end": "522057"
  },
  {
    "text": "Now we can fill in any fields not \nalready populated and create the link.",
    "start": "524521",
    "end": "528000"
  },
  {
    "text": "A link has already been created, \nso let's cancel and view it.",
    "start": "528729",
    "end": "531689"
  },
  {
    "text": "We can see the underlying table \nobject and its attributes.",
    "start": "533369",
    "end": "536234"
  },
  {
    "text": "Now that we have the link, \nwe can grant access to it.",
    "start": "537537",
    "end": "539922"
  },
  {
    "text": "We'll grant permission to \nthe data_analyst IAM user.",
    "start": "545793",
    "end": "548593"
  },
  {
    "text": "We’ll choose the database.",
    "start": "552469",
    "end": "553766"
  },
  {
    "text": "Let's assign the table and grantable permissions.",
    "start": "559005",
    "end": "561357"
  },
  {
    "text": "For this user, we'll grant column-based access.",
    "start": "564189",
    "end": "566794"
  },
  {
    "text": "Let's select the columns account_id, \nfirst_name, middle_name, and last_name.",
    "start": "569757",
    "end": "573547"
  },
  {
    "text": "Then we can finish and grant the permissions.",
    "start": "583541",
    "end": "585446"
  },
  {
    "text": "We can repeat the same process to grant \npermissions for the data_scientist user.",
    "start": "587897",
    "end": "591657"
  },
  {
    "text": "For this user, we'll share the \naccount_id, sex, and age columns.",
    "start": "594133",
    "end": "597625"
  },
  {
    "text": "Now we’ll sign out and log in using the \nusers’ AWS accounts to validate their access.",
    "start": "606109",
    "end": "610874"
  },
  {
    "text": "Let's open Amazon Athena.",
    "start": "613325",
    "end": "614845"
  },
  {
    "text": "We’ll run a query to demonstrate we have \naccess to the data_lake_source_table.",
    "start": "617245",
    "end": "620714"
  },
  {
    "text": "The query revealed data from the account_id, \nfirst_name, middle_name, and last_name columns.",
    "start": "622045",
    "end": "626634"
  },
  {
    "text": "Now let’s validate the \ndata_scientist account's permissions.",
    "start": "628433",
    "end": "631308"
  },
  {
    "text": "Again, we’ll run a query in Athena.",
    "start": "633665",
    "end": "635425"
  },
  {
    "text": "As you can see, the account_id, sex,",
    "start": "640397",
    "end": "642240"
  },
  {
    "text": "and age columns were successfully\ngranted to the data_scientist.",
    "start": "642240",
    "end": "645482"
  },
  {
    "text": "You've just seen how you can build \na modern data architecture with",
    "start": "646821",
    "end": "649279"
  },
  {
    "text": "AWS services using low-code and \nno-code approaches for ingesting,",
    "start": "649280",
    "end": "653200"
  },
  {
    "text": "transforming, and moving data into a \ndata warehouse ready for analytics.",
    "start": "653200",
    "end": "657150"
  },
  {
    "text": "You can learn more about this topic in \nthe discription and links for this video.",
    "start": "658409",
    "end": "661503"
  },
  {
    "text": "Thanks for watching now it's your turn to try.",
    "start": "661721",
    "end": "663799"
  }
]