[
  {
    "start": "0",
    "end": "48000"
  },
  {
    "text": "my name is una pelea and I'm a solution architect with Amazon Web Services focusing on big data and analytics so",
    "start": "0",
    "end": "6960"
  },
  {
    "text": "today we had a bunch of sessions for you where we spoke about analytic services on AWS and how you can build data",
    "start": "6960",
    "end": "14009"
  },
  {
    "text": "pipelines data lakes and how do you drive business value using analytic services on AWS so in this session I'm",
    "start": "14009",
    "end": "20760"
  },
  {
    "text": "going to talk about in the next 40 minutes how can you build an type line a data pipeline end-to-end and build a",
    "start": "20760",
    "end": "26310"
  },
  {
    "text": "data link by the end of it so before we get started with that we need to understand why data leaks are important",
    "start": "26310",
    "end": "33180"
  },
  {
    "text": "and and why we should be building them and why you should be thinking about it from a business standpoint and what are",
    "start": "33180",
    "end": "39989"
  },
  {
    "text": "the business values that it gives you so if you so if you if you think about the",
    "start": "39989",
    "end": "51449"
  },
  {
    "start": "48000",
    "end": "116000"
  },
  {
    "text": "way the amount of data has been growing the velocity the volume and the variety of data that we are dealing with on a",
    "start": "51449",
    "end": "57719"
  },
  {
    "text": "day to day basis is increasing so that being said you also need to have systems",
    "start": "57719",
    "end": "62879"
  },
  {
    "text": "that will be able to that will be able to cater to that kind of scale and cater to that kind of requirements that you",
    "start": "62879",
    "end": "69180"
  },
  {
    "text": "have within your organization's so a report that was done by IDC shows that",
    "start": "69180",
    "end": "74909"
  },
  {
    "text": "the amount of data that companies are generating and companies are accumulating is increasing 10 10 times",
    "start": "74909",
    "end": "81780"
  },
  {
    "text": "but every five years and a usual a usual lifetime of a Data Platform within an",
    "start": "81780",
    "end": "87990"
  },
  {
    "text": "organization is usually for 15 years so Data Platform when you build it it lives in the company for 15 years the tools",
    "start": "87990",
    "end": "94770"
  },
  {
    "text": "that you use the architecture that you have might change and it might evolve over a period of time but whatever you",
    "start": "94770",
    "end": "100860"
  },
  {
    "text": "build needs to be built to scale up 2,000 times for the next so in when you",
    "start": "100860",
    "end": "106079"
  },
  {
    "text": "when you are in 15 years after veneer 15 years from today it needs to have the",
    "start": "106079",
    "end": "111600"
  },
  {
    "text": "scale that is a thousand times bigger than what it is today that being said you also have tools that have evolved",
    "start": "111600",
    "end": "118170"
  },
  {
    "text": "over a period of time we have 11 years back Apache there was a batch a project",
    "start": "118170",
    "end": "124079"
  },
  {
    "text": "that came up called Apache Hadoop where it made it easier for companies to do analytics large-scale analytics so it",
    "start": "124079",
    "end": "132060"
  },
  {
    "text": "was definitely a step up from data warehousing because in data warehousing you had a bunch of servers",
    "start": "132060",
    "end": "137939"
  },
  {
    "text": "and you had data that was distributed across those servers and you were able to run parallel queries but the major",
    "start": "137939",
    "end": "144150"
  },
  {
    "text": "problem with that was your architecture was very tightly coupled with the the",
    "start": "144150",
    "end": "149670"
  },
  {
    "text": "computer was very tightly coupled with the storage and because of that what happened was it became very difficult and very expensive to manage and run in",
    "start": "149670",
    "end": "159239"
  },
  {
    "text": "a large scale and Hadoop helped solve that problem but if you look now no matter what the problem is people say",
    "start": "159239",
    "end": "165510"
  },
  {
    "text": "that SPARC is a solution for anything to do with big data and over the last four years SPARC has taken a has become the de",
    "start": "165510",
    "end": "172290"
  },
  {
    "text": "facto standard to do annelid analytics workload and because and because of that",
    "start": "172290",
    "end": "179099"
  },
  {
    "text": "you see most of the products today supporting SPARC as an execution execution environment the the the key",
    "start": "179099",
    "end": "187470"
  },
  {
    "text": "thing to notice that whatever architecture that you build VI told you 15 years is when a usual data platform",
    "start": "187470",
    "end": "193769"
  },
  {
    "text": "will live live and over that 15 years if you look at in this slide we have showing last 11 years what are the tools",
    "start": "193769",
    "end": "199680"
  },
  {
    "text": "that came and went and the platform would still be there and the way you need to build your architecture the way",
    "start": "199680",
    "end": "206069"
  },
  {
    "text": "you need to build your data Lake would be in a way where you should be able to plug out tools that are no more useful",
    "start": "206069",
    "end": "211980"
  },
  {
    "text": "and plug in tools that you want to replace them with and doing this will help you innovate faster because you'll",
    "start": "211980",
    "end": "218819"
  },
  {
    "text": "be using the latest and the greatest tools available out there so today we have spark that is used for everything",
    "start": "218819",
    "end": "224639"
  },
  {
    "text": "but in the future there might be some other tool that might replace spark so your architecture that you build should",
    "start": "224639",
    "end": "230609"
  },
  {
    "text": "be built in a way where you are able to take out spark and plug it in with another set of tools that are coming in",
    "start": "230609",
    "end": "237209"
  },
  {
    "text": "so people who are from machine learning background have had done data science in the past people used to do data science",
    "start": "237209",
    "end": "243150"
  },
  {
    "text": "using R and Python and bunch of other stuff Python is still used today are still used today but then over the last couple",
    "start": "243150",
    "end": "250290"
  },
  {
    "text": "of years M X net has really taken over tensorflow has almost become a de facto standard for doing deep learning deep",
    "start": "250290",
    "end": "259019"
  },
  {
    "text": "learning workloads that being said the way you should be architecting you should be able to plug these tools in",
    "start": "259019",
    "end": "264690"
  },
  {
    "text": "without massive leary architecting or application so with that let's take a look at a",
    "start": "264690",
    "end": "269970"
  },
  {
    "text": "couple of things that people want or companies want from the data like ten",
    "start": "269970",
    "end": "275100"
  },
  {
    "text": "years back if you would have asked a data professional what is the purpose of building data platforms they would have",
    "start": "275100",
    "end": "281280"
  },
  {
    "text": "they would have told you business intelligence use cases that was pretty much the only thing that they did so",
    "start": "281280",
    "end": "287340"
  },
  {
    "text": "somebody from marketing somebody from sales would tell them hey I want to report for this the BI guy would go and",
    "start": "287340",
    "end": "293460"
  },
  {
    "text": "build a bunch of dashboards and give it to the business users and they would analyze data but if you if you if you",
    "start": "293460",
    "end": "299910"
  },
  {
    "text": "look at what how data teams have been structured over the last few years it has changed drastically now you have",
    "start": "299910",
    "end": "307280"
  },
  {
    "text": "these primary four personas within every organization you still have the BI user",
    "start": "307280",
    "end": "312720"
  },
  {
    "text": "that does the reporting and dashboard and visualization for things but we have",
    "start": "312720",
    "end": "318330"
  },
  {
    "text": "now data scientists analyst and application integrations that are happening so application integrations",
    "start": "318330",
    "end": "324390"
  },
  {
    "text": "could be your company's sharing data with other company an example of that would be your company's loyalty",
    "start": "324390",
    "end": "331590"
  },
  {
    "text": "management system sharing data with its partners so that they can run joint promotions or join sales activities you",
    "start": "331590",
    "end": "338340"
  },
  {
    "text": "still and you have data scientists who require access to raw data or curated data based on the workloads that they",
    "start": "338340",
    "end": "345030"
  },
  {
    "text": "are working on or use cases that they're working on and when they want something like that giving them a data warehouse",
    "start": "345030",
    "end": "351450"
  },
  {
    "text": "is not going to help so a data warehouse will give them access to data that is in a structured form that has been curated",
    "start": "351450",
    "end": "357750"
  },
  {
    "text": "for them and they will be very limited they will feel very limited by that kind of data so you need to give them a",
    "start": "357750",
    "end": "364230"
  },
  {
    "text": "self-service platform where they have the ability to go in scan the data that",
    "start": "364230",
    "end": "369240"
  },
  {
    "text": "they have look at the data that they want to use and extract the data out and do the analysis on top of it and build",
    "start": "369240",
    "end": "374880"
  },
  {
    "text": "their models so the personas the personas each of these personas in an environment would be the requirements",
    "start": "374880",
    "end": "382470"
  },
  {
    "text": "for them would be very different so what a data scientist needs is probably a notebook environment and a self-service",
    "start": "382470",
    "end": "388320"
  },
  {
    "text": "analytics tool what a bi user needs is a data warehouse a bi tool and querying",
    "start": "388320",
    "end": "394410"
  },
  {
    "text": "capability where they can go and explore data and build dashboards whereas application would need api's and",
    "start": "394410",
    "end": "400980"
  },
  {
    "text": "is DK's that you should be able to connect to and they should be able to share information with each other and while",
    "start": "400980",
    "end": "407170"
  },
  {
    "text": "you need to cater to these personas you also need to make sure that these these users are able to access it securely",
    "start": "407170",
    "end": "413560"
  },
  {
    "text": "there is a proper governance structure in place so that people who do not have access to people who should not have",
    "start": "413560",
    "end": "419800"
  },
  {
    "text": "access to things do not have access to that while all of this is being done you",
    "start": "419800",
    "end": "426700"
  },
  {
    "text": "also need to think about how I'm going to bring in fast-moving data data that",
    "start": "426700",
    "end": "432520"
  },
  {
    "text": "is coming in very high velocity and high volume how do you ingest that data so that is your real-time systems how do",
    "start": "432520",
    "end": "438550"
  },
  {
    "text": "you ingest your real-time data into into the platform and how do you make it available to the users to consume so",
    "start": "438550",
    "end": "445750"
  },
  {
    "text": "being able to do real-time predictions being able to do alerting on data that is coming in and analytics around it",
    "start": "445750",
    "end": "452770"
  },
  {
    "text": "and while scalability is the biggest thing that you should be focusing on when building a data lake so how are you",
    "start": "452770",
    "end": "459880"
  },
  {
    "text": "going to make something that is going to be scalable and doesn't increase the cost in a linear way when the amount of",
    "start": "459880",
    "end": "466090"
  },
  {
    "text": "volume or the amount of data increases so if you think about the textbook",
    "start": "466090",
    "end": "472870"
  },
  {
    "start": "470000",
    "end": "545000"
  },
  {
    "text": "definition of data Lake this is what I think a data Lake should actually mean so it's an architecture approach that",
    "start": "472870",
    "end": "478900"
  },
  {
    "text": "allows you to store massive amount of data in a central location and when you store the data in a central location you",
    "start": "478900",
    "end": "485170"
  },
  {
    "text": "should create mechanisms within the platform to be able to cater to be able",
    "start": "485170",
    "end": "490360"
  },
  {
    "text": "to serve multiple set of users be data scientists be it bi users or be application users from a single place so",
    "start": "490360",
    "end": "497860"
  },
  {
    "text": "common use cases that we see around data lakes are people wanting to do advanced analytics so they say I have data coming",
    "start": "497860",
    "end": "505030"
  },
  {
    "text": "in from system a I have data coming in from system B and now I want to combine",
    "start": "505030",
    "end": "510280"
  },
  {
    "text": "these two data and do some Intel do some intelligent analytics around it so that's why we see those are some of the",
    "start": "510280",
    "end": "516909"
  },
  {
    "text": "examples that we see and the data usually comes in from two places if you have an on-premise infrastructure you",
    "start": "516910",
    "end": "523360"
  },
  {
    "text": "would build data ingestion tools or use data ingestion tools that are available out there to bring the data or push the",
    "start": "523360",
    "end": "528940"
  },
  {
    "text": "data to a data lake and do analytics there while you also need to combine the data with",
    "start": "528940",
    "end": "534760"
  },
  {
    "text": "a real-time ingestion that is happening so you need to combine the data with the real-time data that is coming in so",
    "start": "534760",
    "end": "540160"
  },
  {
    "text": "these are these are things that you need to focus on when building a data like so",
    "start": "540160",
    "end": "546610"
  },
  {
    "start": "545000",
    "end": "634000"
  },
  {
    "text": "a characteristics of a data Lake are a few things that you need to focus on as",
    "start": "546610",
    "end": "552630"
  },
  {
    "text": "it should be able to work with relational data sets and run relational data sets so things like structured data",
    "start": "552630",
    "end": "560560"
  },
  {
    "text": "and unstructured data it should it's you should not be locking yourself into a particular technology like some of the",
    "start": "560560",
    "end": "567400"
  },
  {
    "text": "database vendors when you buy the product you pretty much feel logged in so you know you should not be thinking you should not be building it in a way",
    "start": "567400",
    "end": "573370"
  },
  {
    "text": "where you get logged in into the technology and everything will be cool",
    "start": "573370",
    "end": "578740"
  },
  {
    "text": "when you build a little egg but then if the cost is not low somebody from your finance team would come in and ask you",
    "start": "578740",
    "end": "584590"
  },
  {
    "text": "to shut that down so you need to also be cost cognizant and make sure that whatever you build is is built for scale",
    "start": "584590",
    "end": "591880"
  },
  {
    "text": "is built for cost and scale so customers have been building data leaks on AWS",
    "start": "591880",
    "end": "598260"
  },
  {
    "text": "from the very beginning so the largest customers that we have on the platform",
    "start": "598260",
    "end": "603730"
  },
  {
    "text": "have built their data leaks on AWS like likes of Na Nasdaq FINRA in the region",
    "start": "603730",
    "end": "610330"
  },
  {
    "text": "we have AI flakes today in ren√©e's session you heard from FWD on how they",
    "start": "610330",
    "end": "615370"
  },
  {
    "text": "built their data leak on AWS you ha was here talking with NTUC on how they built",
    "start": "615370",
    "end": "621220"
  },
  {
    "text": "that italic on AWS we have customers in the region that have build italics on AWS and we are",
    "start": "621220",
    "end": "626770"
  },
  {
    "text": "happy to also work with customers who have not started on the journey yet but would like to begin and we are happy to",
    "start": "626770",
    "end": "632380"
  },
  {
    "text": "engage with you guys on that a usual a usual process that you follow when you",
    "start": "632380",
    "end": "637900"
  },
  {
    "text": "build the data like is is is something that I have on the screen now so it's a multi-step process you begin with",
    "start": "637900",
    "end": "644680"
  },
  {
    "text": "identifying what are the data sets what are the data sets that you have you figure you figure out a way to",
    "start": "644680",
    "end": "652000"
  },
  {
    "text": "ingest that data you work on tools that will help you bring that data into the platform and store that data process the",
    "start": "652000",
    "end": "659170"
  },
  {
    "text": "data catalog it and then finally make it available for users to consume but then",
    "start": "659170",
    "end": "664180"
  },
  {
    "text": "the problem is once you have done all of this this is all of this a very manual so you need to think about",
    "start": "664180",
    "end": "670479"
  },
  {
    "text": "how am I going to automate all of this is there something that I can do to make all of this much easier so that's where",
    "start": "670479",
    "end": "677999"
  },
  {
    "text": "that's where aw is worked on whew the like a service and that is what we are going to talk about today so we did we",
    "start": "677999",
    "end": "685869"
  },
  {
    "text": "did a product teams data research and they did a survey and we found out that data engineers and data scientists are",
    "start": "685869",
    "end": "692439"
  },
  {
    "text": "spending a good amount of time on their day to day job trying to clean up data",
    "start": "692439",
    "end": "697989"
  },
  {
    "text": "and do ETL so this is stuff that is important but it is not the most important thing that they are supposed",
    "start": "697989",
    "end": "703839"
  },
  {
    "text": "to do in their day to day job they would rather be spending time building models and building analytics building",
    "start": "703839",
    "end": "710559"
  },
  {
    "text": "analytics models on top of the data then actually working with infrastructure around it so that's why we launched the",
    "start": "710559",
    "end": "717699"
  },
  {
    "text": "service called lake formation in reinvent last year in 2018 the lake",
    "start": "717699",
    "end": "723160"
  },
  {
    "start": "718000",
    "end": "732000"
  },
  {
    "text": "formation is a service that allows you to build data lakes build data lakes in a matter of clicks and what you get is",
    "start": "723160",
    "end": "730600"
  },
  {
    "text": "you set up the ingestion you set up the storage you set up the security",
    "start": "730600",
    "end": "736509"
  },
  {
    "start": "732000",
    "end": "762000"
  },
  {
    "text": "procedures around the storage and then you say how am I going to allow users to",
    "start": "736509",
    "end": "741910"
  },
  {
    "text": "collaborate on the platform and the cherry on the top of it it allows you to",
    "start": "741910",
    "end": "746980"
  },
  {
    "text": "do monitoring and auditing of these monitoring of the access kinetic and",
    "start": "746980",
    "end": "752289"
  },
  {
    "text": "access control around the data sets as well so let's take a look at how do you",
    "start": "752289",
    "end": "757449"
  },
  {
    "text": "actually start how do you actually start building a data like using lake formation the first thing that you do is",
    "start": "757449",
    "end": "764139"
  },
  {
    "start": "762000",
    "end": "800000"
  },
  {
    "text": "set up an s3 bucket and say it and tell the lake formation that this is the storage layer for the data lake and you",
    "start": "764139",
    "end": "771579"
  },
  {
    "text": "can your register your existing s3 buckets or you can create a new one you can import existing data that you have",
    "start": "771579",
    "end": "777459"
  },
  {
    "text": "within your bucket and or you can use something called blueprints lake formation blueprints to import the data",
    "start": "777459",
    "end": "784569"
  },
  {
    "text": "so we'll talk about blueprints in the next couple of slides and once the data",
    "start": "784569",
    "end": "789639"
  },
  {
    "text": "is brought in you can use your usual ways to access the data using the API SDK AWS console and whatever other tools",
    "start": "789639",
    "end": "797049"
  },
  {
    "text": "that you have been using in the past this is how a usual flow of lake",
    "start": "797049",
    "end": "803440"
  },
  {
    "start": "800000",
    "end": "854000"
  },
  {
    "text": "formation injection pipeline looks like on the extreme left hand side you have data sources so you have RDS data",
    "start": "803440",
    "end": "811959"
  },
  {
    "text": "sources which are a relational database service that we have you have you can",
    "start": "811959",
    "end": "817570"
  },
  {
    "text": "bring the data in from an AR DBMS system in a single shot or you can you can also",
    "start": "817570",
    "end": "823240"
  },
  {
    "text": "do incremental loads into it once the data comes into the data like you lik formation will store the data into s3 it",
    "start": "823240",
    "end": "830290"
  },
  {
    "text": "will it you you can go in and specify the access control around it and we have something called blue crawlers that",
    "start": "830290",
    "end": "837040"
  },
  {
    "text": "allow you that allows you to auto discover the data and create a catalog around it so all of this is automated",
    "start": "837040",
    "end": "843100"
  },
  {
    "text": "the only manual step that you need to do is setting up your source and setting up the access control rest everything all",
    "start": "843100",
    "end": "849370"
  },
  {
    "text": "the way from data import to accessing or the data is is automated let's take a",
    "start": "849370",
    "end": "855279"
  },
  {
    "start": "854000",
    "end": "917000"
  },
  {
    "text": "look at what actually a blueprint is made of so blueprints are templates they",
    "start": "855279",
    "end": "860740"
  },
  {
    "text": "are predefined templates that AWS is giving as of today we support blueprints",
    "start": "860740",
    "end": "865870"
  },
  {
    "text": "for our DBMS systems so if you are if you are using an RDBMS database either on Prem or on AWS that can talk about",
    "start": "865870",
    "end": "872440"
  },
  {
    "text": "JDBC we can have we can have blue sorry",
    "start": "872440",
    "end": "878589"
  },
  {
    "text": "we can have Lake formation ingest the data using a blueprint blueprints in the",
    "start": "878589",
    "end": "883600"
  },
  {
    "text": "backend use glue workflows which chain events together and it uses the data catalog to store the metadata about your",
    "start": "883600",
    "end": "889959"
  },
  {
    "text": "information and then you have glue jobs that act as the data ingestion tools and",
    "start": "889959",
    "end": "895420"
  },
  {
    "text": "also act as where you'll do your ETL and then you have your database connections",
    "start": "895420",
    "end": "901959"
  },
  {
    "text": "and tables that are data core structure to thee to the glue environment and to",
    "start": "901959",
    "end": "909370"
  },
  {
    "text": "top on top of that we have monitoring capability that sits across all of this to give you the ability to monitor and",
    "start": "909370",
    "end": "914769"
  },
  {
    "text": "see how things are going and how things are progressing we also are we also added with with lake",
    "start": "914769",
    "end": "922930"
  },
  {
    "start": "917000",
    "end": "958000"
  },
  {
    "text": "formation a new feature called ml transform so ml transforms are our way to help you clean your data better",
    "start": "922930",
    "end": "928990"
  },
  {
    "text": "so one of the ML transforms that we have added is the ability to do 2d to placate data deduplication",
    "start": "928990",
    "end": "934660"
  },
  {
    "text": "so if you have records that are like you have duplicate records within your data sets you can train your own machine learning model and tell it that this",
    "start": "934660",
    "end": "941860"
  },
  {
    "text": "record is a duplicate of this and then going forward once the model is ready you'll be able to plug it into your",
    "start": "941860",
    "end": "947860"
  },
  {
    "text": "pipeline and it will do automatic data cleaning for you so it's a pretty cool feature in the future we'll be adding",
    "start": "947860",
    "end": "953500"
  },
  {
    "text": "more transforms but right now the one that we have is for data deduplication so once the data leak is once the data",
    "start": "953500",
    "end": "962200"
  },
  {
    "start": "958000",
    "end": "980000"
  },
  {
    "text": "leak is created using lick formation lake formation acts as a central place where the security happens so it becomes",
    "start": "962200",
    "end": "968560"
  },
  {
    "text": "a central place where all your access control is managed all your you the user",
    "start": "968560",
    "end": "974050"
  },
  {
    "text": "management is done and then the metadata management is done so let's take a look",
    "start": "974050",
    "end": "979840"
  },
  {
    "text": "at how do you access how do you provide security features with lake formation so",
    "start": "979840",
    "end": "985810"
  },
  {
    "start": "980000",
    "end": "1013000"
  },
  {
    "text": "what you see on the screen is the on the top the image that you see is actually showing the list of tables that I have",
    "start": "985810",
    "end": "991630"
  },
  {
    "text": "and once I click on the table and I say grant permissions here on the on the image on the bottom you can see that",
    "start": "991630",
    "end": "998410"
  },
  {
    "text": "I've added three users there and giving them acts and I've given them access to create delete and alter tables so this",
    "start": "998410",
    "end": "1005490"
  },
  {
    "text": "is this is how easy it is to do data Lake security so anybody who has built a data lake knows how difficult it is to",
    "start": "1005490",
    "end": "1011340"
  },
  {
    "text": "do this in practice the next challenge that we saw our customers facing was hey",
    "start": "1011340",
    "end": "1017790"
  },
  {
    "start": "1013000",
    "end": "1030000"
  },
  {
    "text": "I've got one table and I've got two set of users who want to access this table but then I want to give one user access",
    "start": "1017790",
    "end": "1023880"
  },
  {
    "text": "to everything and the other user access to only the limited set of data and that is exactly what Lake formation allows",
    "start": "1023880",
    "end": "1029250"
  },
  {
    "text": "you to do so it allows you to set column level permissions for every user so user",
    "start": "1029250",
    "end": "1034438"
  },
  {
    "text": "could be talking to lake formation and lake formation will decide who has access to what and you can control that",
    "start": "1034439",
    "end": "1041130"
  },
  {
    "text": "and add that data governance but in there and lake formation has integration",
    "start": "1041130",
    "end": "1047610"
  },
  {
    "start": "1045000",
    "end": "1056000"
  },
  {
    "text": "with Athena as well so within the lake formation console you can run the athena now you can run the athena queries as",
    "start": "1047610",
    "end": "1054540"
  },
  {
    "text": "well and and we have the ability to",
    "start": "1054540",
    "end": "1059730"
  },
  {
    "start": "1056000",
    "end": "1077000"
  },
  {
    "text": "audit and monitor things in real time so if you somebody changes a permission for a particular table a new data set is",
    "start": "1059730",
    "end": "1066060"
  },
  {
    "text": "added all of that is available with the within the console for you to audit and take remediate remediation actions",
    "start": "1066060",
    "end": "1074020"
  },
  {
    "text": "so I think enough of me talking now so I should we should get into a demo so let",
    "start": "1074020",
    "end": "1080450"
  },
  {
    "start": "1077000",
    "end": "1123000"
  },
  {
    "text": "me show you an architecture that we have for the demo so I have pre-recorded the demo for you guys so what we are going",
    "start": "1080450",
    "end": "1086420"
  },
  {
    "text": "to do is we have our MySQL database that I have so it is an employee database I'm",
    "start": "1086420",
    "end": "1091550"
  },
  {
    "text": "going to bring that data into lick formation and lake formation is going to crawl that data input that data and I'm",
    "start": "1091550",
    "end": "1098480"
  },
  {
    "text": "going to access the data using Athena which is our interactive query tool and we are going to have two users HR user",
    "start": "1098480",
    "end": "1104510"
  },
  {
    "text": "and the bi user that will access this data so HR user should be able to see salary information and bi user should",
    "start": "1104510",
    "end": "1111560"
  },
  {
    "text": "not be able to see salary information from the same table so that is what we are trying to demonstrate team can we",
    "start": "1111560",
    "end": "1122570"
  },
  {
    "text": "have the video so here I am on my lake formation console I clicked on ad",
    "start": "1122570",
    "end": "1127940"
  },
  {
    "text": "storage and I put in my storage bucket name what I'm specifying here is the",
    "start": "1127940",
    "end": "1133190"
  },
  {
    "text": "role that my lake formation will be using to to run this job next I'll have",
    "start": "1133190",
    "end": "1139280"
  },
  {
    "text": "to give access to Lake formation to actually modify this bucket so I'll go to data locations and I'll grant",
    "start": "1139280",
    "end": "1145940"
  },
  {
    "text": "permissions here so I'll click on grant permissions and here I'll specify my",
    "start": "1145940",
    "end": "1153130"
  },
  {
    "start": "1149000",
    "end": "1177000"
  },
  {
    "text": "specify the name of the role so I'll add myself that is the admin user so I am admin in this account so I'll add admin",
    "start": "1153130",
    "end": "1160160"
  },
  {
    "text": "and I'll also add the lake formation user here at the lake formation role here so once lake formation role is added and",
    "start": "1160160",
    "end": "1166460"
  },
  {
    "text": "I add the s3 bucket named Lake formation and myself that is admin we'll be able",
    "start": "1166460",
    "end": "1171560"
  },
  {
    "text": "to control that bucket and delegate access to other users within my data",
    "start": "1171560",
    "end": "1177020"
  },
  {
    "start": "1177000",
    "end": "1206000"
  },
  {
    "text": "Lake environment so now this is done let's take a look at how I actually bring the data into the platform so you",
    "start": "1177020",
    "end": "1183500"
  },
  {
    "text": "can see that admin and Lake formation role now have access to that s3 resource",
    "start": "1183500",
    "end": "1189490"
  },
  {
    "text": "so I'll go to data importers so data importers are nothing but blueprints so",
    "start": "1195760",
    "end": "1201050"
  },
  {
    "text": "I'll click on add data importer and when I click on add on data importer it will",
    "start": "1201050",
    "end": "1206210"
  },
  {
    "start": "1206000",
    "end": "1233000"
  },
  {
    "text": "show me multiple blueprint options that are available so there is a database snapshot option which is basically go to",
    "start": "1206210",
    "end": "1213290"
  },
  {
    "text": "the database bring everything put it back in there is an incremental database option blueprint that allows me to go in",
    "start": "1213290",
    "end": "1220610"
  },
  {
    "text": "there bring everything and then continuously replicate the data that is changing on the source so this is what",
    "start": "1220610",
    "end": "1226730"
  },
  {
    "text": "you'll be doing if you have a CDC kind of a use case a change data capture kind of for use case and then I have got a",
    "start": "1226730",
    "end": "1231950"
  },
  {
    "text": "cloud trail as well so here I have already got a database connection within my environment so I'll just use the",
    "start": "1231950",
    "end": "1237830"
  },
  {
    "start": "1233000",
    "end": "1263000"
  },
  {
    "text": "database connection that I have called employ dB so I'll choose employ DB here and tell employ and tell lick formation",
    "start": "1237830",
    "end": "1245810"
  },
  {
    "text": "that bring everything that is sitting inside the employee schema so I'll say employee slash percentage so percentage",
    "start": "1245810",
    "end": "1253100"
  },
  {
    "text": "is basically a wild-card that allows me to bring in everything so I'll say employee slash percentage and I don't",
    "start": "1253100",
    "end": "1258830"
  },
  {
    "text": "want to exclude any tables because there's only one table in my database anyway so here in the target the import",
    "start": "1258830",
    "end": "1265670"
  },
  {
    "start": "1263000",
    "end": "1299000"
  },
  {
    "text": "target here I'm telling lake formation that which glue database should it update with this data so here what I'm",
    "start": "1265670",
    "end": "1273140"
  },
  {
    "text": "going to add is I'm going to create a new database in lake formation and I'll call the database employee and I'll add",
    "start": "1273140",
    "end": "1286550"
  },
  {
    "text": "a description for it so I'll just call it employee DB import from RDS and I'll",
    "start": "1286550",
    "end": "1295790"
  },
  {
    "text": "click on create database so this will create Mike this will create metadata a database in my Glu metadata storm so",
    "start": "1295790",
    "end": "1303950"
  },
  {
    "start": "1299000",
    "end": "1330000"
  },
  {
    "text": "once this is done I cannot directly go and put data into this database because",
    "start": "1303950",
    "end": "1309020"
  },
  {
    "text": "I don't have access to this so what I'll have to do now is grant access so this actually shows you how much granular",
    "start": "1309020",
    "end": "1315140"
  },
  {
    "text": "level of security Lake formation provides out-of-the-box so I'm obviously adding myself that is admin and the lake",
    "start": "1315140",
    "end": "1321020"
  },
  {
    "text": "formation role to be able to do all the operations so I want myself to be able to",
    "start": "1321020",
    "end": "1326420"
  },
  {
    "text": "delete the table create the table and also alter the table information so I'll create now everything is set I think I",
    "start": "1326420",
    "end": "1333470"
  },
  {
    "start": "1330000",
    "end": "1343000"
  },
  {
    "text": "can go ahead and start inputting the data so admin and lake formation role both can create drop and alter",
    "start": "1333470",
    "end": "1340030"
  },
  {
    "text": "information in this database so here now I'll refresh this and I should be able",
    "start": "1340030",
    "end": "1346250"
  },
  {
    "start": "1343000",
    "end": "1432000"
  },
  {
    "text": "to see my employee database here so the employee database is here and I'll just add the s3 location where Lake formation",
    "start": "1346250",
    "end": "1354740"
  },
  {
    "text": "was going to dump the data into so this will this will be my raw data Lake so",
    "start": "1354740",
    "end": "1359840"
  },
  {
    "text": "this is the place where the initial data dump happens a lot of people call it staging database so staging data Lake so",
    "start": "1359840",
    "end": "1366680"
  },
  {
    "text": "whatever then your normal clay sure would be this is where the initial set of data comes in so to keep it clean",
    "start": "1366680",
    "end": "1373130"
  },
  {
    "text": "I just want to remove the raw part from there because it just creates a mess with my environment so I'm just going to",
    "start": "1373130",
    "end": "1378950"
  },
  {
    "text": "keep it to data and employee and park' is the creator format that we are using Parker is a columnar data format it",
    "start": "1378950",
    "end": "1385670"
  },
  {
    "text": "allows it allows you to analyze data in a faster way so it's a columnar datastore and because this is a bulk",
    "start": "1385670",
    "end": "1392600"
  },
  {
    "text": "upload I don't want to change the frequency I will just say run it on demand here I'm going to give our data",
    "start": "1392600",
    "end": "1402170"
  },
  {
    "text": "importer name so there's no there's not much significance of what is the data input a name that you give it's just a",
    "start": "1402170",
    "end": "1407180"
  },
  {
    "text": "label for you to identify on the console and using the API and the important the other important thing that you need to",
    "start": "1407180",
    "end": "1413180"
  },
  {
    "text": "specify here is the I am role so I am role will be the lake formation service",
    "start": "1413180",
    "end": "1418460"
  },
  {
    "text": "role so so I'll be selecting the lake formation service role the dpu",
    "start": "1418460",
    "end": "1426050"
  },
  {
    "text": "information that is the scaling unit for Glu so data processing unit it's an optional it's an optional parameter so",
    "start": "1426050",
    "end": "1431720"
  },
  {
    "text": "I'll not choose that and I'll click on add importer once this is ready and you know you can see that it is creating so",
    "start": "1431720",
    "end": "1438230"
  },
  {
    "start": "1432000",
    "end": "1472000"
  },
  {
    "text": "if I click on actions I will not I will not be able to start it so I'll just have to wait for a couple of seconds for",
    "start": "1438230",
    "end": "1443630"
  },
  {
    "text": "it to for it to become available or in in a ready ready mode so that I can go",
    "start": "1443630",
    "end": "1450020"
  },
  {
    "text": "and start the data import process so I clicked on the database I click on",
    "start": "1450020",
    "end": "1455510"
  },
  {
    "text": "actions so I click on the data importer click on actions start and starts the data input service the next",
    "start": "1455510",
    "end": "1462770"
  },
  {
    "text": "thing that it does is it creates a bunch of things in back-end for you to actually import the data crawl the data",
    "start": "1462770",
    "end": "1469520"
  },
  {
    "text": "and get the data into s3 so first thing that it does is it creates a glue",
    "start": "1469520",
    "end": "1475700"
  },
  {
    "start": "1472000",
    "end": "1514000"
  },
  {
    "text": "crawler so glue crawler is a tool within AWS glue that allows you to go to your",
    "start": "1475700",
    "end": "1480980"
  },
  {
    "text": "data source catalog that autodiscover that data and catalog it into glue",
    "start": "1480980",
    "end": "1486910"
  },
  {
    "text": "metadata store so here you can see that this crawler is running so it created a crawler and it has an automated name to",
    "start": "1486910",
    "end": "1493460"
  },
  {
    "text": "it and it shows the status running and it created a temporary data base you can",
    "start": "1493460",
    "end": "1498470"
  },
  {
    "text": "see there with tilde data Lake import and it's using the service and you can see the data store here is my JDBC",
    "start": "1498470",
    "end": "1505250"
  },
  {
    "text": "employee database that we saw that we that we put in in the in the previous steps now this this usually takes close",
    "start": "1505250",
    "end": "1513530"
  },
  {
    "text": "to 40 seconds to a minute to run and once that is done it will be able to now",
    "start": "1513530",
    "end": "1519350"
  },
  {
    "start": "1514000",
    "end": "1554000"
  },
  {
    "text": "you should be able to see the tables created the other things that it does is it creates a bunch of jobs in Glu so",
    "start": "1519350",
    "end": "1526880"
  },
  {
    "text": "these are PI spark jobs that Lake formation has created for me and each job has a function or within the whole",
    "start": "1526880",
    "end": "1533330"
  },
  {
    "text": "workflow and all of this is tied together using a feature and lake formation in glue called triggers so you",
    "start": "1533330",
    "end": "1541910"
  },
  {
    "text": "can see when I click on the trigger it will show me a chain of events that are tacked together so when job zero completes job one runs",
    "start": "1541910",
    "end": "1549530"
  },
  {
    "text": "when one finishes two runs so when all of this runs in one sequence is when my",
    "start": "1549530",
    "end": "1554600"
  },
  {
    "start": "1554000",
    "end": "1585000"
  },
  {
    "text": "daytime put a job would be finished let's go back to our chloral and see what happened here so if i keep",
    "start": "1554600",
    "end": "1561140"
  },
  {
    "text": "refreshing this now you'll see that the crawler would would have updated the",
    "start": "1561140",
    "end": "1566870"
  },
  {
    "text": "table so now you can see on the right hand side it updated one table so if i go to the if i go back to the lake",
    "start": "1566870",
    "end": "1573950"
  },
  {
    "text": "formation page now so if i go to the lake formation page now it will actually",
    "start": "1573950",
    "end": "1579260"
  },
  {
    "text": "show me the table that has been created for that particular crawler so under",
    "start": "1579260",
    "end": "1586130"
  },
  {
    "start": "1585000",
    "end": "1598000"
  },
  {
    "text": "databases that's a temporary database where it actually updated it so we'll open the temporary database and wait if",
    "start": "1586130",
    "end": "1592010"
  },
  {
    "text": "when we click on view tables you can see that this is this is a staging database so when I",
    "start": "1592010",
    "end": "1597860"
  },
  {
    "text": "click on view tables it actually has the data in here and you can see the",
    "start": "1597860",
    "end": "1604160"
  },
  {
    "start": "1598000",
    "end": "1624000"
  },
  {
    "text": "metadata about this information about this table as well so classification is MySQL because the data is coming in from",
    "start": "1604160",
    "end": "1610220"
  },
  {
    "text": "MySQL and those are the some other properties from my table and these are",
    "start": "1610220",
    "end": "1615530"
  },
  {
    "text": "the columns that is inside my database so I never went to the database I never",
    "start": "1615530",
    "end": "1620840"
  },
  {
    "text": "logged into the database server glue the glue crawler did all of this for me so this is what blueprints do for you so",
    "start": "1620840",
    "end": "1629000"
  },
  {
    "start": "1624000",
    "end": "1674000"
  },
  {
    "text": "next thing that we'll see is if we go to jobs page back in glue and you can see that these jobs are running so now the",
    "start": "1629000",
    "end": "1635330"
  },
  {
    "text": "data import process has happened so it was a two-step process one it creates a crawler but which goes and discovers the",
    "start": "1635330",
    "end": "1641930"
  },
  {
    "text": "data and puts that information into a glue catalog then the glue job runs that",
    "start": "1641930",
    "end": "1647330"
  },
  {
    "text": "actually bringing that that is bringing the data in in from the our DBMS system into s3 and storing it and showing it in",
    "start": "1647330",
    "end": "1655190"
  },
  {
    "text": "my data Lake so here if I go back to s3 so I should be able to see the files",
    "start": "1655190",
    "end": "1661910"
  },
  {
    "text": "that have been that all that has landed here so here I am on my three console",
    "start": "1661910",
    "end": "1668230"
  },
  {
    "text": "I'm going a bit faster than my recording so we click on data they click on",
    "start": "1673059",
    "end": "1680030"
  },
  {
    "text": "employee and you should be able to see the table information here so it says employees is the database and the name",
    "start": "1680030",
    "end": "1686659"
  },
  {
    "text": "of the table is also employees and here I have the version 0 of my data like that is already created so this is my",
    "start": "1686659",
    "end": "1693559"
  },
  {
    "text": "raw data and if you can see the data is added as in parque format that is the one that I chose now that that's the",
    "start": "1693559",
    "end": "1700460"
  },
  {
    "start": "1699000",
    "end": "1718000"
  },
  {
    "text": "format that I chose do it when I created the data import on now I'll go back to",
    "start": "1700460",
    "end": "1705860"
  },
  {
    "text": "the main database I'm not going to the temporary database I'm going to the main database and I should be able to see the",
    "start": "1705860",
    "end": "1711860"
  },
  {
    "text": "tables from here so you can see there is a temporary table created and a main table created if I click on the main",
    "start": "1711860",
    "end": "1717919"
  },
  {
    "text": "table this time you'll see the classification is parque and the data location has changed to s3 it's not my",
    "start": "1717919",
    "end": "1724940"
  },
  {
    "start": "1718000",
    "end": "1728000"
  },
  {
    "text": "SQL anymore so the data is now sitting in s3 and there is a bunch of other properties that are available here that",
    "start": "1724940",
    "end": "1733280"
  },
  {
    "start": "1728000",
    "end": "1764000"
  },
  {
    "text": "you can take a look when the recording is made available for you later and if",
    "start": "1733280",
    "end": "1738770"
  },
  {
    "text": "you scroll down there's a bunch of that there are a bunch of columns that are",
    "start": "1738770",
    "end": "1744140"
  },
  {
    "text": "that were imported that will also be shown so you can see that the source type is JDBC so the data came in using a",
    "start": "1744140",
    "end": "1750380"
  },
  {
    "text": "JDBC connection is what it is telling and you still have all the information in there so you have the department name",
    "start": "1750380",
    "end": "1756080"
  },
  {
    "text": "the employee the employee ID and the salary information all of that information as a that has been imported",
    "start": "1756080",
    "end": "1763070"
  },
  {
    "text": "for you next thing that we'll be doing is and this is the most difficult part usually in a manual data leak creation",
    "start": "1763070",
    "end": "1769940"
  },
  {
    "start": "1764000",
    "end": "1784000"
  },
  {
    "text": "process is doing security around it so take a look at how easy it is to actually do it with leak formation I",
    "start": "1769940",
    "end": "1775419"
  },
  {
    "text": "choose the database name I choose a table name and I say grant permission so",
    "start": "1775419",
    "end": "1784340"
  },
  {
    "text": "I should be able to grant permission to this particular table to a specific set of users with just a couple of clicks so",
    "start": "1784340",
    "end": "1791960"
  },
  {
    "text": "here I go in and add the demo demo HR user and what I'll tell lake formation",
    "start": "1791960",
    "end": "1800450"
  },
  {
    "text": "to do is give access to demo HR user for employee database to employee stay",
    "start": "1800450",
    "end": "1806370"
  },
  {
    "text": "and columns I say give it access to everything except except star sorry",
    "start": "1806370",
    "end": "1814530"
  },
  {
    "text": "except blank so when I do that it says I say that's the wrong option so I'll",
    "start": "1814530",
    "end": "1820470"
  },
  {
    "text": "change that again so I'll change that to all except extruded columns and I say",
    "start": "1820470",
    "end": "1826980"
  },
  {
    "text": "blank so HR user will be able to access everything so technically it has access to star columns so all the columns",
    "start": "1826980",
    "end": "1834180"
  },
  {
    "text": "within the database it will be able to access and now I will add the permission for the BI user that we spoke about",
    "start": "1834180",
    "end": "1840480"
  },
  {
    "text": "earlier so I'm going to go to grant permissions again and I'll select the BI",
    "start": "1840480",
    "end": "1845670"
  },
  {
    "text": "user there so if we look for demo bi",
    "start": "1845670",
    "end": "1853500"
  },
  {
    "text": "user and here I'm choosing the employed database again choose the employee table and here I will say has has access to",
    "start": "1853500",
    "end": "1863700"
  },
  {
    "text": "everything except these columns so now I can filter out sensitive columns that I feel the BI user should not be able to",
    "start": "1863700",
    "end": "1870420"
  },
  {
    "text": "see so first of all I don't want the person to identify who is the employee so I remove the employee ID I don't want",
    "start": "1870420",
    "end": "1876240"
  },
  {
    "text": "to filter them out I want to filter data birth and I want to filter out obviously the salary so these are the columns that",
    "start": "1876240",
    "end": "1882840"
  },
  {
    "text": "I'm interested in that they should not have access to and then I say grant permission and I say select and grant",
    "start": "1882840",
    "end": "1888180"
  },
  {
    "text": "permission so now you can see bi user has access to everything except those",
    "start": "1888180",
    "end": "1894690"
  },
  {
    "text": "three record those three columns and they can run select statement on that particular table whereas with the HR",
    "start": "1894690",
    "end": "1901230"
  },
  {
    "text": "with the HR user they can run select star from the table and it will work fine let's take a look at actually how",
    "start": "1901230",
    "end": "1908130"
  },
  {
    "text": "does this work we are going to use Athena - Athena to run two queries one",
    "start": "1908130",
    "end": "1914880"
  },
  {
    "text": "using the bi user and one using the HR user and we'll see what is the difference that we'll get when now when",
    "start": "1914880",
    "end": "1922170"
  },
  {
    "text": "we run now when we run a query like that so this is my this is my HR user so you",
    "start": "1922170",
    "end": "1928170"
  },
  {
    "start": "1924000",
    "end": "2051000"
  },
  {
    "text": "can see on the top right it says it demo HR and this is my bi user so so this is",
    "start": "1928170",
    "end": "1935520"
  },
  {
    "text": "the Athena console from where I'll be where I'll be connecting to Lake formations and bringing the data in and acquiring",
    "start": "1935520",
    "end": "1943100"
  },
  {
    "text": "the data that I want to access so here I am querying a department name gender count of department and average salary",
    "start": "1943100",
    "end": "1950360"
  },
  {
    "text": "so what I am running is give me give me a summary of average salary based on department for a specific gender within",
    "start": "1950360",
    "end": "1958309"
  },
  {
    "text": "within that company and because this is using savage of salary the HR user has",
    "start": "1958309",
    "end": "1963590"
  },
  {
    "text": "access to this so this query should work fine so when I click on run query it",
    "start": "1963590",
    "end": "1969200"
  },
  {
    "text": "will take a couple of seconds for the query to run and it will show me that the query ran successfully so we'll just",
    "start": "1969200",
    "end": "1974390"
  },
  {
    "text": "wait for the query to finish so the",
    "start": "1974390",
    "end": "1984350"
  },
  {
    "text": "query has finished and now I can see the salary information is available so I can see that customer service team the",
    "start": "1984350",
    "end": "1991490"
  },
  {
    "text": "average salary for females is 67,000 or whatever the number is right whereas on",
    "start": "1991490",
    "end": "1996799"
  },
  {
    "text": "the other hand with the BI user if I run the same query so I just copy/paste the query into the BI user and when I run it",
    "start": "1996799",
    "end": "2003659"
  },
  {
    "text": "we'll see what happens so it immediately fails saying column column employee",
    "start": "2003659",
    "end": "2012549"
  },
  {
    "text": "salary is not available for this user to access so that column itself is not visible for the user so this is how you",
    "start": "2012549",
    "end": "2020140"
  },
  {
    "text": "implement column level or attribute level security using like formation for your data leaks in AWS so I'm just going",
    "start": "2020140",
    "end": "2028390"
  },
  {
    "text": "to get rid of that query put in a new query without the salary information and that should be able to run fine without",
    "start": "2028390",
    "end": "2035289"
  },
  {
    "text": "any issues so this query doesn't have any salary information so when I run it that query will run successfully and I",
    "start": "2035289",
    "end": "2041980"
  },
  {
    "text": "should be able to see the data so there",
    "start": "2041980",
    "end": "2049388"
  },
  {
    "text": "so the query has run and I think we are good so with that demo I wanted to",
    "start": "2049389",
    "end": "2055720"
  },
  {
    "start": "2051000",
    "end": "2173000"
  },
  {
    "text": "basically show you what is the flow of building a data Lake on AWS using Lake formation and one of the key things to",
    "start": "2055720",
    "end": "2062020"
  },
  {
    "text": "notice and this is what really excites me are talking about the service to you guys is this service comes with no",
    "start": "2062020",
    "end": "2068888"
  },
  {
    "text": "charges so you don't pay anything for Lake formation itself you pay for the services that uses in the",
    "start": "2068889",
    "end": "2074050"
  },
  {
    "text": "ken so you pay for the data that it is storing in s3 you pay for any glue jobs",
    "start": "2074050",
    "end": "2079419"
  },
  {
    "text": "and crawlers that are running and catalog that it is using in glue other than that the service itself is pretty",
    "start": "2079420",
    "end": "2085540"
  },
  {
    "text": "much free of charge so I wanted to end the session with a couple of best",
    "start": "2085540",
    "end": "2090639"
  },
  {
    "text": "practices so we have always built systems that are decoupled and you have",
    "start": "2090640",
    "end": "2095679"
  },
  {
    "text": "the storage system the processing system and then use the storage layer as your decoupling layer use the right tools for",
    "start": "2095679",
    "end": "2102220"
  },
  {
    "text": "the right job make sure you're using the tools that are built for purpose so don't use a data warehouse to run your",
    "start": "2102220",
    "end": "2108340"
  },
  {
    "text": "OLTP workloads and don't use your MySQL's as your data warehouses so you don't want to do that and build build",
    "start": "2108340",
    "end": "2117010"
  },
  {
    "text": "your systems in a way that are that are good on your pocket and go easy on the CFO when they are signing the check so",
    "start": "2117010",
    "end": "2123640"
  },
  {
    "text": "make sure that your cost conscious and if you have issues around how do you optimize for cost feel free to reach out",
    "start": "2123640",
    "end": "2129370"
  },
  {
    "text": "to anybody from your AWS team and we'll be happy to help you guys with the cost optimization but and always talk to your",
    "start": "2129370",
    "end": "2136480"
  },
  {
    "text": "consumer groups within the company to make sure that you understand the requirements before you start building",
    "start": "2136480",
    "end": "2141760"
  },
  {
    "text": "out the lake and these are some of the partners in the region that have done some amazing work with data warehouses",
    "start": "2141760",
    "end": "2149050"
  },
  {
    "text": "with data lakes on AWS and all of those partners have worked with our customers",
    "start": "2149050",
    "end": "2155530"
  },
  {
    "text": "in the region to help them build the data platforms on AWS so guys that's all I have for you today",
    "start": "2155530",
    "end": "2161310"
  },
  {
    "text": "please take your time to fill the feedback and thank you very much for spending time with us today and you guys",
    "start": "2161310",
    "end": "2168340"
  },
  {
    "text": "have a good evening",
    "start": "2168340",
    "end": "2170850"
  }
]