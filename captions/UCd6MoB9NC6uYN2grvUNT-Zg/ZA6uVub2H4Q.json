[
  {
    "start": "0",
    "end": "73000"
  },
  {
    "text": "am I live okay hi so I hope everybody has the time to",
    "start": "2480",
    "end": "8240"
  },
  {
    "text": "settle down good it's getting a little uh quieter so I'd like to welcome you",
    "start": "8240",
    "end": "14519"
  },
  {
    "text": "all here uh it's it's a big crowd I'm really happy to see all of you here I hope kues would sparked interest it",
    "start": "14519",
    "end": "20080"
  },
  {
    "text": "certainly has been a pretty exciting technology for us over the past 2 years",
    "start": "20080",
    "end": "25279"
  },
  {
    "text": "today I'm going to be talking about kubernetes in a in a in a specific way and that's running all of your kues",
    "start": "25279",
    "end": "31400"
  },
  {
    "text": "workloads on AWS ec2 spot instances I think the main the reason",
    "start": "31400",
    "end": "38559"
  },
  {
    "text": "why we started playing with spot instances was naturally the cost uh and the savings which we could realize",
    "start": "38559",
    "end": "45320"
  },
  {
    "text": "actually on our ec2 part of the AWS bill but the twist I want to make to the presentation and which I want to spend a",
    "start": "45320",
    "end": "51800"
  },
  {
    "text": "little bit of time today is that during that we learned a lot about the resiliency of our apps and that is is",
    "start": "51800",
    "end": "60199"
  },
  {
    "text": "actually going to be I think the biggest takeaway to this that adapting to what spot to the",
    "start": "60199",
    "end": "66479"
  },
  {
    "text": "behavior of spot insist can actually make your apps uh a lot better A lot more resilient so let me maybe just kind",
    "start": "66479",
    "end": "74280"
  },
  {
    "start": "73000",
    "end": "73000"
  },
  {
    "text": "of stop very very quickly over where I come from I come from delivery hero I",
    "start": "74280",
    "end": "79400"
  },
  {
    "text": "manage the logistics engineering team and we manage our own delivery feeds so",
    "start": "79400",
    "end": "85000"
  },
  {
    "text": "we're a food delivery company so you know us as Leaf foral as furora as Pizza in in in Germany but we also",
    "start": "85000",
    "end": "92560"
  },
  {
    "text": "operate uh in 40 countries all over the world many continents and about 20% of",
    "start": "92560",
    "end": "98520"
  },
  {
    "text": "our business we employ or we have Riders as freelance Partners which deliver the food for us so it's not the restaurant",
    "start": "98520",
    "end": "105399"
  },
  {
    "text": "and in Berlin we have an engineering team which writes all the code supporting that where as far as AWS goes",
    "start": "105399",
    "end": "114680"
  },
  {
    "text": "uh we have workloads across four continents North America South America Europe",
    "start": "114680",
    "end": "120840"
  },
  {
    "text": "Asia actually also up until recently Australia and just to give you an idea",
    "start": "120840",
    "end": "126000"
  },
  {
    "text": "of let's say the scale we're talking about to bring this a little bit closer to you uh our workloads for purely",
    "start": "126000",
    "end": "131800"
  },
  {
    "text": "Logistics is around a few hundred kubernetes running uh nodes running simultaneously of course it can go up",
    "start": "131800",
    "end": "138120"
  },
  {
    "text": "and down quite a lot depending if we have some batch workloads but this is let's say the scale at which we made",
    "start": "138120",
    "end": "144879"
  },
  {
    "text": "this uh the contents of this presentation work so to give a little",
    "start": "144879",
    "end": "149959"
  },
  {
    "text": "bit more practical and to kind of just bring Logistics Tech uh from delivery hero a bit closer nothing extremely",
    "start": "149959",
    "end": "156840"
  },
  {
    "text": "surprising we have rails and spring for web workloads doing their job very well no need to for us to experiment with",
    "start": "156840",
    "end": "162920"
  },
  {
    "text": "something more exotic we do however have Scala and AA which are doing a really",
    "start": "162920",
    "end": "168640"
  },
  {
    "text": "good job for our real-time apps especially our dispatching algorithm and also in the past year or two we've",
    "start": "168640",
    "end": "176360"
  },
  {
    "text": "really become data driven and we use a lot of python we we use a lot of R for more badge",
    "start": "176360",
    "end": "182319"
  },
  {
    "text": "tasks uh and also a lot of a lot of solvers for some of our optimization problems persistence wise we use mostly",
    "start": "182319",
    "end": "189640"
  },
  {
    "text": "Amazon uh RDS with postgress and uh more recently we're uh switching a lot of uh",
    "start": "189640",
    "end": "195920"
  },
  {
    "text": "use cases to Dynamo DB when we realized a lot of our usage just doesn't fit the",
    "start": "195920",
    "end": "202319"
  },
  {
    "text": "rdbms's anymore and we just realize how much more uh Dynamo DB is scalable and",
    "start": "202319",
    "end": "208480"
  },
  {
    "text": "as far as kubernetes goes right now we actually don't use Amazon eks yet but our system Engineers they're actually",
    "start": "208480",
    "end": "214560"
  },
  {
    "text": "sitting somewhere around here are I think they actually this week transitioned uh our QA environment to",
    "start": "214560",
    "end": "219720"
  },
  {
    "text": "eks currently we use cops but for the purposes of this presentations it will work no matter what way of uh",
    "start": "219720",
    "end": "227519"
  },
  {
    "text": "provisioning kubernetes you use so if you are from Germany you might",
    "start": "227519",
    "end": "233120"
  },
  {
    "text": "know fura so fura Riders is is uh what we and they're supporting them is what",
    "start": "233120",
    "end": "238280"
  },
  {
    "text": "we spend most of our time with this is me uh delivering food somewhere near hosal plot uh I think this was almost a",
    "start": "238280",
    "end": "244879"
  },
  {
    "text": "year ago and so what do we do uh essentially we manage the whole end to-end experience for the rider it",
    "start": "244879",
    "end": "251560"
  },
  {
    "text": "starts with hiring them a huge part of our business is actually Staffing the right amount of riders so it's not",
    "start": "251560",
    "end": "258320"
  },
  {
    "text": "everybody thinks it's just you know finding the best route and assigning the best orders but a lot of uh",
    "start": "258320",
    "end": "263759"
  },
  {
    "text": "optimizations we do actually are about Staffing the Riders as well we developed",
    "start": "263759",
    "end": "268880"
  },
  {
    "text": "the mobile app the tracking of the Riders to see where they are and being able to assign them the best orders or",
    "start": "268880",
    "end": "274560"
  },
  {
    "text": "make them be able to choose the best orders that they can deliver and of course following up to payroll to",
    "start": "274560",
    "end": "280600"
  },
  {
    "text": "actually kind of going full circle with the whole employment or freelance",
    "start": "280600",
    "end": "286639"
  },
  {
    "text": "experience so this I picked up my that microphone picked me up so this is a",
    "start": "286639",
    "end": "292960"
  },
  {
    "text": "little bit how our day looks like in terms of load so this is a typical day of a logistics and not just Logistics",
    "start": "292960",
    "end": "299520"
  },
  {
    "text": "but the delivery hero engineer right so it starts uh at midnight and around 4:00 or",
    "start": "299520",
    "end": "306039"
  },
  {
    "text": "5 p.m. Asia lunch picks up so there you have the first Peak then around noon our",
    "start": "306039",
    "end": "311880"
  },
  {
    "text": "time Asia dinner suddenly coincides with Europeans going to lunch and ordering food then and then you can see the",
    "start": "311880",
    "end": "319199"
  },
  {
    "text": "rest uh why I'm actually trying to show this and what I'm actually trying to uh to to show this on a graph is that cloud",
    "start": "319199",
    "end": "329440"
  },
  {
    "text": "and the elasticity has been always kind of like a a really basic thing for us",
    "start": "329440",
    "end": "336240"
  },
  {
    "text": "we've actually never really thought of operating on on uh pre-provisioned hardware and we've always been using to",
    "start": "336240",
    "end": "342880"
  },
  {
    "text": "use autoscaling because when you have these usage patterns it's just really so hard to expect Peaks so for the purposes",
    "start": "342880",
    "end": "349960"
  },
  {
    "text": "of this presentation again I think I'll assume most of you know what an natos scaling group is so who uses autoscaling",
    "start": "349960",
    "end": "357919"
  },
  {
    "text": "actually in the crowd who kind of goes to the EC to the ec2",
    "start": "357919",
    "end": "363639"
  },
  {
    "text": "console and clicks the blue button launch new instance and that's most of his workload a few it's okay i' recommend",
    "start": "363639",
    "end": "371560"
  },
  {
    "text": "actually converting that to an Autos Skilling group with a size one like the moment you come home but and then if I",
    "start": "371560",
    "end": "377800"
  },
  {
    "text": "pull out like a single country you can see the traffic INF flux can actually be pretty rapid so I think this is actually",
    "start": "377800",
    "end": "384319"
  },
  {
    "text": "my home country this is this is Cia so you can see at 11:30 everybody lcks",
    "start": "384319",
    "end": "389840"
  },
  {
    "text": "their computers and start and and goes to lunch or better for our case orders food online so at 11:30 suddenly we have",
    "start": "389840",
    "end": "396639"
  },
  {
    "text": "these influxes of traffic so going to what I was telling in the",
    "start": "396639",
    "end": "403639"
  },
  {
    "text": "beginning why did we start playing with spences in the first place and that was to save money uh it is I think I'm",
    "start": "403639",
    "end": "411080"
  },
  {
    "text": "pretty sure it is the biggest component of our it spend of course and that was our primary goal if you look at a",
    "start": "411080",
    "end": "419840"
  },
  {
    "text": "typical delivery hero AWS account bill it looks oversimplified a little bit",
    "start": "419840",
    "end": "425160"
  },
  {
    "text": "like this this ec2 typically is always more than 50% of our spend and it's",
    "start": "425160",
    "end": "431840"
  },
  {
    "text": "always the strongest contributor to to to our cost RDS usually follows in some",
    "start": "431840",
    "end": "437120"
  },
  {
    "text": "accounts RDS actually attacks first place and this graph already takes into",
    "start": "437120",
    "end": "442960"
  },
  {
    "text": "account that you just did not do this naive migration to Amazon but you have",
    "start": "442960",
    "end": "448199"
  },
  {
    "text": "also uh already started to using a lot of the application Services the manage",
    "start": "448199",
    "end": "453360"
  },
  {
    "text": "services that are metered by usage not just by an idle instance uh running time",
    "start": "453360",
    "end": "459039"
  },
  {
    "text": "so imagine Dynamo DB sqs which is just paid per message or by how often you pull AWS Lambda which is really just",
    "start": "459039",
    "end": "465639"
  },
  {
    "text": "measured by the amount of RAM and CPU time you consume but still ec2 very",
    "start": "465639",
    "end": "470840"
  },
  {
    "text": "consistently pops up as the biggest contributor towards our AWS",
    "start": "470840",
    "end": "476159"
  },
  {
    "text": "Bill and again that's why we want to focus on this here because all pretty",
    "start": "476680",
    "end": "482199"
  },
  {
    "start": "479000",
    "end": "479000"
  },
  {
    "text": "much all of our ec2 usage is on kubernetes notes we've made an all-in choice that kubernetes is going to power",
    "start": "482199",
    "end": "487800"
  },
  {
    "text": "our web workloads our worker workloads our batch workloads our model training workloads everything that we have have",
    "start": "487800",
    "end": "494240"
  },
  {
    "text": "in the bag so any Improvement we make on this front pretty much applies to every",
    "start": "494240",
    "end": "499360"
  },
  {
    "text": "application we have every use case we have and we just need to basically make",
    "start": "499360",
    "end": "504440"
  },
  {
    "text": "this effort once it's not so it's not as as it sounds but in in practice it actually ended up being pretty very",
    "start": "504440",
    "end": "510800"
  },
  {
    "text": "close to this uh one common question is why not",
    "start": "510800",
    "end": "516120"
  },
  {
    "text": "use reserved instances which was usually the traditional way of saving money on ec2 where essentially you go to Amazon",
    "start": "516120",
    "end": "523000"
  },
  {
    "text": "and you commit for one two or 3 years uh to use an ec2 instance and no matter if",
    "start": "523000",
    "end": "528120"
  },
  {
    "text": "you use it or not you're going to be paying it it has become a lot more flexible in the recent months they have",
    "start": "528120",
    "end": "533200"
  },
  {
    "text": "become much more convertible and you can basically change instance uh sizes but at the same time you still have that",
    "start": "533200",
    "end": "539440"
  },
  {
    "text": "that commitment for us one of the reasons why I was showing that load graph at the beginning was uh to show",
    "start": "539440",
    "end": "548000"
  },
  {
    "text": "basically the daily change in uh in load and we could only let's say what are we supposed to do we can either reserve the",
    "start": "548000",
    "end": "554560"
  },
  {
    "text": "base load or we can Bas reserve the over-provisioned peak load we expect we",
    "start": "554560",
    "end": "559959"
  },
  {
    "text": "could also deploy a hybrid just reserve the base load and doing on demand or doing reserved and then combine it with",
    "start": "559959",
    "end": "566160"
  },
  {
    "text": "spots but then exactly that's why we just say okay let's just go with spots all the way the business itself so delivery hero",
    "start": "566160",
    "end": "575120"
  },
  {
    "text": "uh is still has a lot of startup DNA in it and the business is too volatile so",
    "start": "575120",
    "end": "580560"
  },
  {
    "text": "for example you might have read in the news in December we have sold furora and leaf and pizza it's still subject to",
    "start": "580560",
    "end": "586800"
  },
  {
    "text": "approval but it's there's a very good chance it's going to be going through and suddenly that means all of your",
    "start": "586800",
    "end": "591839"
  },
  {
    "text": "German traffic will be out one day to another and what do you do with all that reserved capacity in Europe we're not",
    "start": "591839",
    "end": "597480"
  },
  {
    "text": "opening any new countries there and also I think most interestingly the",
    "start": "597480",
    "end": "603480"
  },
  {
    "text": "workloads and the way we use cloud changes over time very apparent it's very apparent in machine learning for",
    "start": "603480",
    "end": "609600"
  },
  {
    "text": "example where some of the model training is suddenly one month very CPU heavy one month it's very memory heavy and one",
    "start": "609600",
    "end": "615839"
  },
  {
    "text": "month we're going to need actually gpus to train our models and again trying to figure out which types of capacity we",
    "start": "615839",
    "end": "623000"
  },
  {
    "text": "want to reserve somehow then locks us into having that capacity and then just",
    "start": "623000",
    "end": "628320"
  },
  {
    "text": "finding trying to find the ways to use it rather than trying to think about the problems we have and then trying just",
    "start": "628320",
    "end": "634079"
  },
  {
    "text": "buying the capacity for cheap uh to run what we need an interesting small use",
    "start": "634079",
    "end": "639720"
  },
  {
    "text": "case though we use it for kubernetes Masters currently so we're not on eks yet so that's a really nice use case for",
    "start": "639720",
    "end": "646360"
  },
  {
    "text": "us it's running 24/7 and uh and at a very constant load",
    "start": "646360",
    "end": "651920"
  },
  {
    "text": "so we do use uh reserved instances for our kues masters which is going to go away the moment we manage to migrate to",
    "start": "651920",
    "end": "658680"
  },
  {
    "text": "eks which I hope is going to be any any week now actually very very nice experience I think for us right now",
    "start": "658680",
    "end": "664639"
  },
  {
    "text": "after the last batch of changes we're pretty confident to go with it and we already have production applications on",
    "start": "664639",
    "end": "669920"
  },
  {
    "text": "eks in delivery hero uh money right so how does our",
    "start": "669920",
    "end": "675480"
  },
  {
    "text": "actual AWS Bill uh look like I think a lot of people find this the most interesting thing and are always most",
    "start": "675480",
    "end": "681440"
  },
  {
    "text": "curious right so uh let me uh if you see the laser pointer the time when we",
    "start": "681440",
    "end": "688279"
  },
  {
    "text": "started talking about instances was around here second half of 2017 and we",
    "start": "688279",
    "end": "693839"
  },
  {
    "text": "realized our AWS cost goes up linearly with our order volume and we kind of",
    "start": "693839",
    "end": "700560"
  },
  {
    "text": "figured out if it goes up and up and up it's just become to become a Hot Topic taking our time and so we did invest",
    "start": "700560",
    "end": "707240"
  },
  {
    "text": "that time and tried to figure out what is the best ways to tackle this and you can see even after the first waves here",
    "start": "707240",
    "end": "713360"
  },
  {
    "text": "of optimization where it quickly goes down that was pot instances for us and it also bought us a good quarter of a",
    "start": "713360",
    "end": "720760"
  },
  {
    "text": "year's time to basically Implement some other measures and then the really nice thing over time is it has also manage to",
    "start": "720760",
    "end": "727760"
  },
  {
    "text": "keep our uh AWS Bill pretty constant because we even with spots we can be",
    "start": "727760",
    "end": "733199"
  },
  {
    "text": "over provisioned and uh we're just growing into that capacity over time the",
    "start": "733199",
    "end": "738600"
  },
  {
    "text": "nice thing of course is then uh that the cost per order is getting lower and roll over which in",
    "start": "738600",
    "end": "744160"
  },
  {
    "text": "business terms mean it's becoming a less of and less of a topic with Management in budgeting and in with Finance so this",
    "start": "744160",
    "end": "749600"
  },
  {
    "text": "is of course like a nice thing giving you energy and time to spend on other places this is a screenshot of a recent",
    "start": "749600",
    "end": "756480"
  },
  {
    "text": "addition to the ec2 spot console so what you can see is actually a real example",
    "start": "756480",
    "end": "762839"
  },
  {
    "start": "759000",
    "end": "759000"
  },
  {
    "text": "of savings so this is a snapshot I let's say from our uh Us East one uh cluster",
    "start": "762839",
    "end": "770000"
  },
  {
    "text": "and it takes your currently running spot instances so I see I have 77 instances running you see spread over a bunch of",
    "start": "770000",
    "end": "776760"
  },
  {
    "text": "different machine types and it tells you the saving and I think the 70% saving is consistently what I see",
    "start": "776760",
    "end": "784160"
  },
  {
    "text": "across the whole world and it really applies so a fleet I would have paid for or the whole lifetime of that Fleet I",
    "start": "784160",
    "end": "791000"
  },
  {
    "text": "would have paid $1,800 I paid a little bit under 500 so which is 72% savings",
    "start": "791000",
    "end": "796760"
  },
  {
    "text": "and I mean that's huge reserved instances will give you 20 30% and here you can suddenly jump into completely",
    "start": "796760",
    "end": "803480"
  },
  {
    "text": "different territories in terms of savings it also applies for example when you have very very intense workloads you",
    "start": "803480",
    "end": "809360"
  },
  {
    "text": "want to basically pre-process all your historical data and you want to spin up a huge massive Fleet of data processing",
    "start": "809360",
    "end": "815360"
  },
  {
    "text": "machines again spots will give you an that same rate overall if I actually I don't have",
    "start": "815360",
    "end": "822800"
  },
  {
    "text": "it in the presentation but if for us that for example meant on a monthly AWS bill for the Department goes around from",
    "start": "822800",
    "end": "830600"
  },
  {
    "text": "50 to 60,000 a month will go down to around 20 20 around 20 which is for us",
    "start": "830600",
    "end": "838440"
  },
  {
    "text": "again really really huge because it buys you a lot of time to optimize",
    "start": "838440",
    "end": "843600"
  },
  {
    "text": "elsewhere who uses pod instances good I actually",
    "start": "843600",
    "end": "849800"
  },
  {
    "text": "have one or I think or two to share so just to have a quick refresher and",
    "start": "849800",
    "end": "855279"
  },
  {
    "text": "everybody be on the let everybody be on the same page uh I just want to explain very very",
    "start": "855279",
    "end": "861920"
  },
  {
    "text": "briefly how spot instances work so what you if you go to Amazon and you launch an instance by default it's called an uh",
    "start": "861920",
    "end": "869560"
  },
  {
    "text": "too much text I'll say it first um it's called on demand and essentially you just rent per uh per second nowadays I",
    "start": "869560",
    "end": "876839"
  },
  {
    "text": "think uh and a virtual machine on which you can run things Amazon will not take that machine",
    "start": "876839",
    "end": "883079"
  },
  {
    "text": "away it's going to run for you and unless the underlying Hardware fails that's Al also the reason why you should",
    "start": "883079",
    "end": "889480"
  },
  {
    "text": "always run a single instance as in an Autos scaling group of size one it'll stay uh it'll stay there for you whereas",
    "start": "889480",
    "end": "896519"
  },
  {
    "text": "with spot fleets what you do is you bid on on the unused capacity I mean at least that's how Amazon explains it to",
    "start": "896519",
    "end": "902600"
  },
  {
    "start": "897000",
    "end": "897000"
  },
  {
    "text": "you on the free capacity in the data center there's one big catch to that",
    "start": "902600",
    "end": "909519"
  },
  {
    "text": "they give you that those instances their free capacity that they would not use otherwise more than 50% cheaper you just",
    "start": "909519",
    "end": "915560"
  },
  {
    "text": "saw it's usually around 70% but the catch is they can take away that instance with a two-minute warning so",
    "start": "915560",
    "end": "922560"
  },
  {
    "text": "they're going to say hey I have an ond demand customer or I have a reserved instance customer who's going to need that instance and you're just paying for",
    "start": "922560",
    "end": "929160"
  },
  {
    "text": "free usage right for our free capacity so you give it back to us and the second",
    "start": "929160",
    "end": "935399"
  },
  {
    "text": "catch of course is you're bidding on free capacity there might not be any it basically the market can be completely",
    "start": "935399",
    "end": "941800"
  },
  {
    "text": "saturated and everybody the C5 2x large is sold out in this availability Zone in",
    "start": "941800",
    "end": "947800"
  },
  {
    "text": "Frankfurt so this is essentially the only problem and I want to spend a little bit of time with that actually",
    "start": "947800",
    "end": "953519"
  },
  {
    "text": "means for applications in production a spot instance Market is",
    "start": "953519",
    "end": "959160"
  },
  {
    "text": "defined by an availability Zone and an instance type so you always bid within an availability zone for a specific",
    "start": "959160",
    "end": "966279"
  },
  {
    "text": "instance type so C4 2x large in EU West 1A is a separate market and that could",
    "start": "966279",
    "end": "972920"
  },
  {
    "text": "be sold out so what you want to do is you want to actually diversify and bid",
    "start": "972920",
    "end": "979040"
  },
  {
    "text": "in many markets at the same time so you say okay I can use a 2X large 4X large uh different instance families and I",
    "start": "979040",
    "end": "985920"
  },
  {
    "text": "want to use uh the a availability Zone B availability Zone the C availability",
    "start": "985920",
    "end": "991600"
  },
  {
    "text": "Zone and that suddenly for example generates 24 bidding market so it's actually the soldout problem is actually",
    "start": "991600",
    "end": "998480"
  },
  {
    "text": "not so big uh Amazon also in my opinion this is",
    "start": "998480",
    "end": "1003560"
  },
  {
    "text": "my uh still old school with RSS this is my reader and if you look at the new uh",
    "start": "1003560",
    "end": "1008680"
  },
  {
    "text": "a lot of the work if you look at the work going into spot instances it's very much active and we could see a lot of",
    "start": "1008680",
    "end": "1015360"
  },
  {
    "text": "things that make using them a lot easier compared to for example a 18 months ago",
    "start": "1015360",
    "end": "1020440"
  },
  {
    "text": "there's uh offerings now which are able to give you hybrid fleets part on demand",
    "start": "1020440",
    "end": "1027360"
  },
  {
    "text": "part Reserve part spot you can specify everything in a visual console",
    "start": "1027360",
    "end": "1032678"
  },
  {
    "text": "the uh if you're not super used to infrastructure as code but that's of",
    "start": "1032679",
    "end": "1038280"
  },
  {
    "start": "1038000",
    "end": "1038000"
  },
  {
    "text": "course also possible so today if you provision spot fleets you the SP the",
    "start": "1038280",
    "end": "1044079"
  },
  {
    "text": "console experience is very good it's actually been it's it's very comfortable but of course I'll always be the",
    "start": "1044079",
    "end": "1049880"
  },
  {
    "text": "advocate for using infrastructure as code and the support there is good as well you can use AWS cloud formation you",
    "start": "1049880",
    "end": "1055559"
  },
  {
    "text": "can use terraform very well there the some of the resource types are a little confusing there's a",
    "start": "1055559",
    "end": "1062039"
  },
  {
    "text": "product called spot Fleet there's a product called ec2 Fleet but after a few pages of documentations it's not very",
    "start": "1062039",
    "end": "1067840"
  },
  {
    "text": "hard to wrap your head around so what I want to move on to is",
    "start": "1067840",
    "end": "1073520"
  },
  {
    "text": "what does that 2-minute warning actually mean for your applications and what do you actually need to take care of",
    "start": "1073520",
    "end": "1081880"
  },
  {
    "text": "so a timer starts 2 minutes countdown and what you need to do is close all",
    "start": "1081880",
    "end": "1088520"
  },
  {
    "text": "connections to the to the application right so any clients connected any keep alive connections you have to do you",
    "start": "1088520",
    "end": "1093880"
  },
  {
    "text": "need to gracefully disconnect them maybe your application is doing long polling on the other hand you need to close that",
    "start": "1093880",
    "end": "1099400"
  },
  {
    "text": "close those connections as well you need to stop in progress worker jobs your Sidekick your cues your celery and stuff",
    "start": "1099400",
    "end": "1105799"
  },
  {
    "text": "like that you need to actually cleanly terminate your pod your Docker containers and kuties you need to",
    "start": "1105799",
    "end": "1112400"
  },
  {
    "text": "deregister uh the pods from the service from the load balancer within kuity so any traff no traffic rooted to that and",
    "start": "1112400",
    "end": "1119520"
  },
  {
    "text": "very importantly you need to rescale to the previous capacity since there will be pods running on the instance that is",
    "start": "1119520",
    "end": "1125799"
  },
  {
    "text": "being taken away you need to place them somewhere else in kues kues will do that for you but that's what I want to talk",
    "start": "1125799",
    "end": "1132000"
  },
  {
    "text": "about a little bit more to understand what components are good to make that work",
    "start": "1132000",
    "end": "1137240"
  },
  {
    "text": "reliably uh uh maybe just to stop for a few seconds how do you actually know",
    "start": "1137240",
    "end": "1142320"
  },
  {
    "text": "that instance will go away there is a special metadata API for Amazon this is",
    "start": "1142320",
    "end": "1148200"
  },
  {
    "text": "exactly the same place where you get your temporary I am credentials from and there's an endpoint which you simply",
    "start": "1148200",
    "end": "1153440"
  },
  {
    "text": "pull every one or two seconds uh I think 2 seconds AWS recommends and it returns",
    "start": "1153440",
    "end": "1158919"
  },
  {
    "text": "a simple Json response and it will tell you scheduled for termination not scheduled for termination if yes what is",
    "start": "1158919",
    "end": "1165360"
  },
  {
    "text": "the deadline so that's the way Amazon will let you know recently a few weeks",
    "start": "1165360",
    "end": "1171600"
  },
  {
    "text": "or months ago they also added an SNS event as far as I know but this is the way we go and I'll stop by that in just",
    "start": "1171600",
    "end": "1177559"
  },
  {
    "start": "1175000",
    "end": "1175000"
  },
  {
    "text": "a second so something I hear very often is like hey that's like chaos monkey it's going to terminate random instances",
    "start": "1177559",
    "end": "1184559"
  },
  {
    "text": "all over the time right I mean chaos monkey I should have that installed or being able to uh handle that",
    "start": "1184559",
    "end": "1190520"
  },
  {
    "text": "anyway yes but those terminations will happen not just in business hours like",
    "start": "1190520",
    "end": "1196000"
  },
  {
    "text": "people usually run chaos monkey they also can kill several instances at the",
    "start": "1196000",
    "end": "1201559"
  },
  {
    "text": "same time which means if that's a significant part of the capacity of your cluster you can run into problems",
    "start": "1201559",
    "end": "1208240"
  },
  {
    "text": "and one of the biggest things I think I was fighting or was a my own insecurity",
    "start": "1208240",
    "end": "1214919"
  },
  {
    "text": "in like knowing that this is going to be a reliable solution for the company but also with management with my",
    "start": "1214919",
    "end": "1220960"
  },
  {
    "text": "CTO and that's exactly what I kind of want to demystify in the rest of the presentation and and explain hey we're",
    "start": "1220960",
    "end": "1227280"
  },
  {
    "text": "able to handle L of these things and while we're doing that we can actually make our apps a lot more resilient so if",
    "start": "1227280",
    "end": "1234120"
  },
  {
    "text": "we break it down a little bit applications not terminating gracefully you can have uh this connection reset by",
    "start": "1234120",
    "end": "1242039"
  },
  {
    "text": "Pier you might know from when when you browse you can have stuck jobs which never propably uh properly uh report a",
    "start": "1242039",
    "end": "1249280"
  },
  {
    "text": "success or error uh status you can have too much of the target capacity being taken out at the same time so if I have",
    "start": "1249280",
    "end": "1255919"
  },
  {
    "text": "a service which has uh a pot autoscaler and is running at four instances three",
    "start": "1255919",
    "end": "1260960"
  },
  {
    "text": "of them will be taken out because they were on the same node that service is probably going to be under capacity and you're going to see request cheing or",
    "start": "1260960",
    "end": "1268280"
  },
  {
    "text": "you might be running your cluster very tightly in terms of uh scaling and there's just not going to be enough uh",
    "start": "1268280",
    "end": "1276000"
  },
  {
    "text": "enough room in the rest of the cluster to reschedule your pods I wanted to",
    "start": "1276000",
    "end": "1281200"
  },
  {
    "start": "1280000",
    "end": "1280000"
  },
  {
    "text": "actually stop by an a little practical case study and it's one of the things that we had to learn uh while doing this",
    "start": "1281200",
    "end": "1288640"
  },
  {
    "text": "one of the most misbehaving or the worst behaving apps on boot were are Java apps",
    "start": "1288640",
    "end": "1294440"
  },
  {
    "text": "written in Spring Boot and what if what you can see on this profile or graph at the top spring boot pretty much consumes",
    "start": "1294440",
    "end": "1300960"
  },
  {
    "text": "all the all the CPU it can get its hand on so you can see we had it throttle at about 40 40% or so for uh until all the",
    "start": "1300960",
    "end": "1309080"
  },
  {
    "text": "classes are loaded and then it basically kind of chills out at about 1% and so we never actually tackled this problem",
    "start": "1309080",
    "end": "1315520"
  },
  {
    "text": "before and we had to understand why in this case it was actually uh a",
    "start": "1315520",
    "end": "1320640"
  },
  {
    "text": "combination of uh an APM instrumentation tool so something like New Relic where basically it instruments by bite code",
    "start": "1320640",
    "end": "1327279"
  },
  {
    "text": "and and hooks into the bite code and it takes a lot of time and we also uh learned a little bit about jit and Java",
    "start": "1327279",
    "end": "1334120"
  },
  {
    "text": "how it works and essentially you can trade off a little bit of performance over the lifetime of the app getting a",
    "start": "1334120",
    "end": "1340440"
  },
  {
    "text": "faster boot Time by actually disabling some higher levels of jitting uh the bigger problem we would",
    "start": "1340440",
    "end": "1347039"
  },
  {
    "text": "normally not care that the app application starts uh start so slow but the problem was in case spot instances",
    "start": "1347039",
    "end": "1353640"
  },
  {
    "text": "took out too many instances at the same time when several of these spring boot applications started fighting for CPU it",
    "start": "1353640",
    "end": "1360200"
  },
  {
    "text": "actually made the note too busy and started affecting other apps so by",
    "start": "1360200",
    "end": "1365279"
  },
  {
    "text": "Rolling Out spot instances we suddenly learned about this usage which would bite Us in the long term anyway and we",
    "start": "1365279",
    "end": "1371000"
  },
  {
    "text": "found good ways to fix it so now to actually the most",
    "start": "1371000",
    "end": "1376159"
  },
  {
    "text": "interesting uh part of it and that's actually how do you handle spot instances in kubernetes",
    "start": "1376159",
    "end": "1382039"
  },
  {
    "text": "itself so there's like a basic toolkit which you should always uh take with",
    "start": "1382039",
    "end": "1387840"
  },
  {
    "text": "yourself um and that's the and I'm going to go component by component the first",
    "start": "1387840",
    "end": "1393919"
  },
  {
    "text": "one is uh called a spot termination notice Handler and if you remember the",
    "start": "1393919",
    "end": "1399400"
  },
  {
    "start": "1397000",
    "end": "1397000"
  },
  {
    "text": "metadata API which I was telling you about that's all this does I think it's a ruby script or or some or a python",
    "start": "1399400",
    "end": "1405320"
  },
  {
    "text": "script which essentially just every 2 minutes uh every two seconds pulls that metadata API and in case it finds out",
    "start": "1405320",
    "end": "1412400"
  },
  {
    "text": "that node is um uh getting terminated it will drain it maybe let me just do a",
    "start": "1412400",
    "end": "1418240"
  },
  {
    "text": "little check on how much I should explain this who knows what a demon set is who knows what draining a node is",
    "start": "1418240",
    "end": "1425400"
  },
  {
    "text": "pretty much same crowd so just to make sure that I'm not leaving anybody behind a demon set is a kubernetes uh a",
    "start": "1425400",
    "end": "1433279"
  },
  {
    "text": "container in kues a pod which will run on every node in the machine so so for",
    "start": "1433279",
    "end": "1438720"
  },
  {
    "text": "example a lock collector very often it is or something that is you have a",
    "start": "1438720",
    "end": "1443880"
  },
  {
    "text": "server monitoring tool and you want it collect metrics on every server so this is something that runs on every machine",
    "start": "1443880",
    "end": "1449919"
  },
  {
    "text": "and draining a node means that for kubernetes what kubernetes does for you",
    "start": "1449919",
    "end": "1455039"
  },
  {
    "text": "essentially and and all the core of it is basically you tell kubernetes please launch a Docker container in this",
    "start": "1455039",
    "end": "1460799"
  },
  {
    "text": "cluster of machines and it has a component called auler which will decide oh this is the best machine to run it on",
    "start": "1460799",
    "end": "1466480"
  },
  {
    "text": "based on some metrics and so what it does it taints the node which marks hey",
    "start": "1466480",
    "end": "1471679"
  },
  {
    "text": "don't schedule any pods here this this node is going away it's not healthy there's no don't please schedule any new",
    "start": "1471679",
    "end": "1476760"
  },
  {
    "text": "Docker containers here and starts immediately rescheduling those pods on another node within the same cluster",
    "start": "1476760",
    "end": "1483840"
  },
  {
    "text": "it's going to do it gracefully so if that it's basically going to issue a nice termination signal to the docker",
    "start": "1483840",
    "end": "1490440"
  },
  {
    "text": "container and if your application can handle this it's going to close all connections properly close all workers",
    "start": "1490440",
    "end": "1496279"
  },
  {
    "text": "and die I think this is something that no matter uh if you run spots or not you should be doing but spots again made us",
    "start": "1496279",
    "end": "1504360"
  },
  {
    "text": "focus on this a little bit more because if you have your standard on demand instances you will not experience them",
    "start": "1504360",
    "end": "1509760"
  },
  {
    "text": "going up and down so much so you don't focus on this and you will just ignore though 500 errors uh which sometimes pop",
    "start": "1509760",
    "end": "1516399"
  },
  {
    "text": "up from time to time in your logs when a uh a doctor container is dying but it really made us spend a little bit of",
    "start": "1516399",
    "end": "1522480"
  },
  {
    "text": "time on examining the behavior of all our applications where when they are getting sent gracefully termination",
    "start": "1522480",
    "end": "1529120"
  },
  {
    "text": "signals it also gives you uh an optional slack notification which has become our",
    "start": "1529120",
    "end": "1534159"
  },
  {
    "text": "go-to debugging tool in the very beginning it looks a little bit like this we have a slack Channel and it",
    "start": "1534159",
    "end": "1539480"
  },
  {
    "start": "1538000",
    "end": "1538000"
  },
  {
    "text": "tells us okay uh in Asia Pacific we have a instance terminating at 840 actually",
    "start": "1539480",
    "end": "1545000"
  },
  {
    "text": "to and this also kind of made us um demystified spots a little bit so we",
    "start": "1545000",
    "end": "1551399"
  },
  {
    "text": "didn't know if it's like you know you get it always for 12 minutes you always uh and it you you kind of only have them",
    "start": "1551399",
    "end": "1556720"
  },
  {
    "text": "for a few minutes at the same time how many will disappear at the same time all these little questions to see I",
    "start": "1556720",
    "end": "1562640"
  },
  {
    "text": "don't have a definitive answer it does look a lot like this so a lot of random",
    "start": "1562640",
    "end": "1567919"
  },
  {
    "text": "uh terminations over the whole day but from time to time I would say we can see for example up to five machines leaving",
    "start": "1567919",
    "end": "1574600"
  },
  {
    "text": "the cluster at the same time but I think that's a more extreme case which doesn't happen uh so often but today we are",
    "start": "1574600",
    "end": "1581600"
  },
  {
    "text": "perfectly able to handle that again our current usual sizes in one cabes",
    "start": "1581600",
    "end": "1587039"
  },
  {
    "text": "clusters will be around 50 to 150 nodes so you see at the bottom some",
    "start": "1587039",
    "end": "1593880"
  },
  {
    "text": "somebody from the data science team actually using that channel to debug uh noticing that four in four instances",
    "start": "1593880",
    "end": "1599919"
  },
  {
    "text": "were terminated and he had 25 tasks uh running on airflow at the same time which got stopped interrupted in the",
    "start": "1599919",
    "end": "1606480"
  },
  {
    "text": "middle airflow is a is a framework for running badge tasks imagine Jenkins if",
    "start": "1606480",
    "end": "1612080"
  },
  {
    "text": "if you're not familiar with air flow which just allows you to run periodically tasks and for example that",
    "start": "1612080",
    "end": "1617279"
  },
  {
    "text": "means machine learning models for us machine learning model training for us",
    "start": "1617279",
    "end": "1622440"
  },
  {
    "text": "and so for example that batch use case is actually one of the things we do have on On Demand right now and we're still",
    "start": "1622440",
    "end": "1629399"
  },
  {
    "text": "that's kind of let's say the last thing we still have on on demand which we would like to figure out how to use pots for",
    "start": "1629399",
    "end": "1635200"
  },
  {
    "text": "correctly second component first was termination notice Handler the second component which we have is the",
    "start": "1635200",
    "end": "1642880"
  },
  {
    "start": "1642000",
    "end": "1642000"
  },
  {
    "text": "deser so on the screenshot uh you see an interesting",
    "start": "1642880",
    "end": "1648440"
  },
  {
    "text": "thing we keep talking how spot instances can disappear that they're going to go away within 2 minutes but they can also",
    "start": "1648440",
    "end": "1655520"
  },
  {
    "text": "stay stick around for a remarkably long time so you can see this is uh one of",
    "start": "1655520",
    "end": "1660760"
  },
  {
    "text": "the production clusters in the US regions and you see we commissioned it 250 days ago 2 weeks later a node scaled",
    "start": "1660760",
    "end": "1667559"
  },
  {
    "text": "up based on usage and it still hasn't gone away it's been there almost the year so spot",
    "start": "1667559",
    "end": "1673200"
  },
  {
    "text": "instances are not don't have to be necessarily epimeral and what the",
    "start": "1673200",
    "end": "1678320"
  },
  {
    "text": "problem with this is kubernetes will only look at your Docker containers and where to schedule them on creation time",
    "start": "1678320",
    "end": "1685600"
  },
  {
    "text": "so what happens is over time the that node will typically like uh accumulate",
    "start": "1685600",
    "end": "1691760"
  },
  {
    "text": "pods especially for example if you have some applications which you don't deploy that often you don't upgrade and they",
    "start": "1691760",
    "end": "1697000"
  },
  {
    "text": "will become full to the full capacity that you can reserve on those nodes and",
    "start": "1697000",
    "end": "1702519"
  },
  {
    "text": "that is a problem for one reason and that's basically when that node will go away there's going to be a lot of doctor",
    "start": "1702519",
    "end": "1709360"
  },
  {
    "text": "containers to reschedule on other nodes and going back to that Java example for example that can cause a lot of CPU",
    "start": "1709360",
    "end": "1715200"
  },
  {
    "text": "stress on that node because some applications need a little bit more CPU when they're booting up so this is a",
    "start": "1715200",
    "end": "1722240"
  },
  {
    "text": "good component to have no matter if you use uh spot instances or you don't it simply prevents a few things it prevents",
    "start": "1722240",
    "end": "1729159"
  },
  {
    "text": "PODS of the same deployment so imagine I have a service serving my customer data in",
    "start": "1729159",
    "end": "1734640"
  },
  {
    "text": "kues um a deployment is basic Bally a collection of horizontally scaled PODS",
    "start": "1734640",
    "end": "1740919"
  },
  {
    "text": "of horizontally scaled containers so it prevents making sure that the scaled copies of that container run on the same",
    "start": "1740919",
    "end": "1747799"
  },
  {
    "text": "node it will also uh try to reduce the overall reserved or you or reserved or",
    "start": "1747799",
    "end": "1754120"
  },
  {
    "text": "act used CPU or memory usage on the nodes and make sure that they're more evenly spread out which is a good thing",
    "start": "1754120",
    "end": "1760200"
  },
  {
    "text": "in any case the additional bonus to this being is of course then if one of those nodes go down there's a lot more room uh",
    "start": "1760200",
    "end": "1767720"
  },
  {
    "text": "spread across the cluster to launch those new pods so there's a bigger chance the new the pods will be starting",
    "start": "1767720",
    "end": "1774080"
  },
  {
    "text": "spread out across the cluster not just on one node which happens to be very free like like like like uh can't show",
    "start": "1774080",
    "end": "1781679"
  },
  {
    "text": "like over here uh third so that was the second component first termination no his",
    "start": "1781679",
    "end": "1787600"
  },
  {
    "text": "Handler second deser third being the Autos scaling strategy so again that's why I was in",
    "start": "1787600",
    "end": "1794760"
  },
  {
    "start": "1794000",
    "end": "1794000"
  },
  {
    "text": "the beginning showing our load graph with all those lunch and dinner Peaks AC across the whole world we always need to",
    "start": "1794760",
    "end": "1800799"
  },
  {
    "text": "have enough capacity to launch new PA you have four machines going away you need to have enough room for pods which",
    "start": "1800799",
    "end": "1807360"
  },
  {
    "text": "used to run on those for machines and other places on the cluster in delivery hero we use two strategies that I know",
    "start": "1807360",
    "end": "1813960"
  },
  {
    "text": "of probably we might use more I know the first one which my team uses a little bit better but I think the second one is",
    "start": "1813960",
    "end": "1820320"
  },
  {
    "text": "also also uh very nice and it's it's been a pretty it's a pretty nice idea",
    "start": "1820320",
    "end": "1825360"
  },
  {
    "text": "from another team in delivery hero so the key point for us is we do not scale",
    "start": "1825360",
    "end": "1831279"
  },
  {
    "text": "by CPU usage it is generally discouraged in kuties to do that in in general but",
    "start": "1831279",
    "end": "1837000"
  },
  {
    "text": "the way for us was to actually collect how much uh again maybe let's do one",
    "start": "1837000",
    "end": "1842279"
  },
  {
    "text": "question do you know the difference between requests and limits on pods in kues who",
    "start": "1842279",
    "end": "1849159"
  },
  {
    "text": "does okay I'll still maybe do a little refresher on a Docker container level in",
    "start": "1849159",
    "end": "1854559"
  },
  {
    "text": "kues you are able to specify two values one is called requests which is how much",
    "start": "1854559",
    "end": "1860600"
  },
  {
    "text": "CPU or Ram that pod will always have reserved it will always have half a core",
    "start": "1860600",
    "end": "1866039"
  },
  {
    "text": "equivalent and it will always have 600 megabytes of memory limits is how it can how much it can like go above that",
    "start": "1866039",
    "end": "1873240"
  },
  {
    "text": "reservation and burst so if you reserve very little and let the limit go very",
    "start": "1873240",
    "end": "1878720"
  },
  {
    "text": "high your applications will be just fighting for resources you can also set limits the same as reserved and you will",
    "start": "1878720",
    "end": "1884279"
  },
  {
    "text": "always have dedicated resources for your pods across the cluster so uh that is the key thing for us that",
    "start": "1884279",
    "end": "1892360"
  },
  {
    "text": "we scale the spot Fleet based on CPU and or Ram reservations depends on the usage",
    "start": "1892360",
    "end": "1897919"
  },
  {
    "text": "type we have so imagine we have a a fleet of CPU bound machines we say",
    "start": "1897919",
    "end": "1903600"
  },
  {
    "text": "we always want to have that machine at a 70% level reservation so that 30% is",
    "start": "1903600",
    "end": "1910240"
  },
  {
    "text": "left for bursting but we always are able to place that it's more important with",
    "start": "1910240",
    "end": "1915320"
  },
  {
    "text": "memory because if the Pod does not have enough memory it will simply terminate with an out of memory",
    "start": "1915320",
    "end": "1921399"
  },
  {
    "text": "error uh I'll stop by at the first strategy in the in just a second a little bit more another nice uh way to",
    "start": "1921399",
    "end": "1929080"
  },
  {
    "text": "do this is over-provisioning essentially you put placeholder pods in your cluster which do nothing it's just like you know they",
    "start": "1929080",
    "end": "1935240"
  },
  {
    "text": "just sit there but they take all those resources but they have a low priority which means if kubernetes has to decide",
    "start": "1935240",
    "end": "1941600"
  },
  {
    "text": "hey I have another clust another container which wants to start but there's no no um free room this one has",
    "start": "1941600",
    "end": "1948159"
  },
  {
    "text": "such a low priority it will actually terminate it to make room for those new ones so that's also like a nice way to",
    "start": "1948159",
    "end": "1954440"
  },
  {
    "text": "keep like kind of an additional free buffer on top of your cluster capacity at all times so in general it looks a",
    "start": "1954440",
    "end": "1961200"
  },
  {
    "text": "little bit like this we have a custom written demon Set uh the secret is actually we use that termination notice",
    "start": "1961200",
    "end": "1966919"
  },
  {
    "text": "Handler we patched it a little bit and the same time it checks for that um",
    "start": "1966919",
    "end": "1972039"
  },
  {
    "text": "termination notice it also uh reports metrics to AWS Cloud watch it reports",
    "start": "1972039",
    "end": "1977679"
  },
  {
    "text": "the the number of CPU reserved uh and and the and the percentage of ram reserved on the Node and directly in AWS",
    "start": "1977679",
    "end": "1985159"
  },
  {
    "text": "you can specify that you want to scale a spot Fleet based on custom cloudwatch metrics in our case we will launch two",
    "start": "1985159",
    "end": "1992679"
  },
  {
    "text": "new machines when more than 80% of the cluster CPU is reserved and we will terminate one machine",
    "start": "1992679",
    "end": "1998760"
  },
  {
    "text": "when uh when the used CPU is under 50% this is exactly a screenshot uh from uh",
    "start": "1998760",
    "end": "2006840"
  },
  {
    "text": "DWS console doesn't make me too proud because you can see it's still manual so this is one thing we're just right now rewriting uh with eks into into",
    "start": "2006840",
    "end": "2016120"
  },
  {
    "text": "terraform so that is essentially for us it's always super important for us to base to be to be sure that we have",
    "start": "2016120",
    "end": "2022639"
  },
  {
    "text": "enough capacity to launch new pods in the whole cluster one little thing I was",
    "start": "2022639",
    "end": "2027760"
  },
  {
    "text": "talking about Java I'll still stay with gvm uh one of the interesting exercises",
    "start": "2027760",
    "end": "2033559"
  },
  {
    "text": "when looking deeply into our applications how they behave on termination was our AA applications so",
    "start": "2033559",
    "end": "2041519"
  },
  {
    "text": "AKA is a jvm based framework for building realtime apps it's based on an actor",
    "start": "2041519",
    "end": "2047280"
  },
  {
    "text": "model uh and it doesn't really matter if if if you know it or not but the big",
    "start": "2047280",
    "end": "2053599"
  },
  {
    "text": "difference compared to your typical web application is it's stateful it holds data in memory between requests it's",
    "start": "2053599",
    "end": "2059760"
  },
  {
    "text": "synced uh by messages between the actors it knows how to basically rein or",
    "start": "2059760",
    "end": "2065440"
  },
  {
    "text": "rebuild that state in case the Pod goes down and it can also run in a cluster so it's not like your HTTP request in response",
    "start": "2065440",
    "end": "2073760"
  },
  {
    "text": "out I forgot what I was doing two seconds ago it actually keeps all that state in memory so you can imagine a PO",
    "start": "2073760",
    "end": "2079599"
  },
  {
    "text": "termination affects that a lot because you need to make sure that all the me the data which was in memory before",
    "start": "2079599",
    "end": "2085398"
  },
  {
    "text": "determination is reinstantiate somewhere else properly and we learned a lot about the",
    "start": "2085399",
    "end": "2093839"
  },
  {
    "text": "cluster formation so it has a non-trivial cluster formation mechanism it needs to keep it quum there's several",
    "start": "2093839",
    "end": "2099720"
  },
  {
    "text": "strategies you can run into split brain situations and I think one of the really",
    "start": "2099720",
    "end": "2104760"
  },
  {
    "text": "really uh proud moments for me in the last in last year was when uh an",
    "start": "2104760",
    "end": "2109800"
  },
  {
    "text": "availability Zone in in um Us East one had connectivity problems and that's always something that we were always",
    "start": "2109800",
    "end": "2116240"
  },
  {
    "text": "yeah we can never do anything about this it's just an infrastructure issue and we'll blame it on AWS but we actually",
    "start": "2116240",
    "end": "2121920"
  },
  {
    "text": "found that the optimizations we did to be able to cope with those spot inance",
    "start": "2121920",
    "end": "2127839"
  },
  {
    "text": "is being terminated all the time we had to implement a split brain resolver which sees if the cluster has enough",
    "start": "2127839",
    "end": "2133520"
  },
  {
    "text": "members to form a quum to form a majority and suddenly when we were looking at our logs debugging what",
    "start": "2133520",
    "end": "2140440"
  },
  {
    "text": "exactly happened during that networking outage we could we seen that the application by itself has migrated to",
    "start": "2140440",
    "end": "2146800"
  },
  {
    "text": "the healthy availability Zone ignoring all the machines in that unhealthy one and that was a really like nice thing",
    "start": "2146800",
    "end": "2152839"
  },
  {
    "text": "for us that okayy availability zones make sense we can actually use them and it's not not just it's probably better",
    "start": "2152839",
    "end": "2159119"
  },
  {
    "text": "to run in one availability Zone since we don't want to pay for the ENT a traffic so this is like another another case uh",
    "start": "2159119",
    "end": "2165960"
  },
  {
    "text": "where this has become a really nice exercise for us to learn how our apps uh",
    "start": "2165960",
    "end": "2171920"
  },
  {
    "text": "behave in under these situations I think the what I want to",
    "start": "2171920",
    "end": "2177880"
  },
  {
    "text": "recommend if you want to try this there's no need to basically do everything at once and you can take it step by step kues allows you to do",
    "start": "2177880",
    "end": "2185079"
  },
  {
    "text": "multiple instance groups so you can have a set of spot Fleet spot instance uh",
    "start": "2185079",
    "end": "2190960"
  },
  {
    "text": "spot instances some on demand instances reserved instances you can have more memory based machines you can have more",
    "start": "2190960",
    "end": "2197000"
  },
  {
    "text": "CPU based machines and then on a pod level you can actually specify which of those instances you want to run those",
    "start": "2197000",
    "end": "2203560"
  },
  {
    "text": "apps on so what do you do first you take your most simple rest API with no persistence the one that boots up in 2",
    "start": "2203560",
    "end": "2210319"
  },
  {
    "text": "seconds has no persistence layer has no state in memory and move it to spot instances see how you go see how your",
    "start": "2210319",
    "end": "2216280"
  },
  {
    "text": "monitoring noise goes up then you take your applications which have persistence and then you maybe end",
    "start": "2216280",
    "end": "2222160"
  },
  {
    "text": "up like in our case with AA with the applications which actually have State uh based in memory I think the 80% goal",
    "start": "2222160",
    "end": "2231319"
  },
  {
    "text": "if I look at our February usage down below here in dark blue spot instances in red you see the airflow tasks I was",
    "start": "2231319",
    "end": "2238560"
  },
  {
    "text": "talking about on on demand and the reserved instances for cernes Masters and maybe a little you other use case so",
    "start": "2238560",
    "end": "2244960"
  },
  {
    "text": "you can see there's a bit more unreserved instances that took us 6 months it was not from",
    "start": "2244960",
    "end": "2250359"
  },
  {
    "text": "one day to another we of course uncovered problems going going forward with it spend a little bit of time",
    "start": "2250359",
    "end": "2256079"
  },
  {
    "text": "fixing them but it really opened eyes our eyes in a lot of in a lot of in a lot of things so you can see right now",
    "start": "2256079",
    "end": "2263000"
  },
  {
    "text": "we're running if I sum up the whole data for February which I had I know up until",
    "start": "2263000",
    "end": "2268280"
  },
  {
    "text": "a week ago you can see it's just about under 80% running on spot instances and if I again take that uh if I combine",
    "start": "2268280",
    "end": "2276040"
  },
  {
    "text": "that with the 70% saving which one on average we're always achieving with spot instances this is a huge huge thing for",
    "start": "2276040",
    "end": "2282960"
  },
  {
    "text": "us so I can only say that our uh experience with with this was eye",
    "start": "2282960",
    "end": "2290040"
  },
  {
    "text": "opening it was also a really big cost saavor reducing a lot of energy on that",
    "start": "2290040",
    "end": "2295880"
  },
  {
    "text": "and uh looking at the time I think I'll be available rather for questions right over at the side and that's it for me so",
    "start": "2295880",
    "end": "2302440"
  },
  {
    "text": "very much thanks you for for being here and thank you",
    "start": "2302440",
    "end": "2308880"
  },
  {
    "text": "I do have one actually last message one request we do have a booth in the startup uh section right over there at",
    "start": "2315720",
    "end": "2322160"
  },
  {
    "text": "the end so uh I was asked to to send you over and I think from what I stopped by",
    "start": "2322160",
    "end": "2327800"
  },
  {
    "text": "it's really worth taking a bit of time to stop there thank you again",
    "start": "2327800",
    "end": "2333480"
  }
]