[
  {
    "text": "hi everyone my name is Dimitri sadkin I",
    "start": "4480",
    "end": "7799"
  },
  {
    "text": "am a senior ml specialist solution",
    "start": "7799",
    "end": "10000"
  },
  {
    "text": "architect with Amazon web services and",
    "start": "10000",
    "end": "13080"
  },
  {
    "text": "today I'm going to demonstrate how to",
    "start": "13080",
    "end": "15759"
  },
  {
    "text": "use various deployment options and",
    "start": "15759",
    "end": "18680"
  },
  {
    "text": "optimization techniques when deploying",
    "start": "18680",
    "end": "21199"
  },
  {
    "text": "Foundation models on Amazon Sage",
    "start": "21199",
    "end": "24800"
  },
  {
    "text": "maker in this session I'm going to show",
    "start": "24800",
    "end": "28439"
  },
  {
    "text": "you how to choose different deployment",
    "start": "28439",
    "end": "31199"
  },
  {
    "text": "options for different use cases when you",
    "start": "31199",
    "end": "34079"
  },
  {
    "text": "deploying models on a sage maker we will",
    "start": "34079",
    "end": "37160"
  },
  {
    "text": "cover two use cases first open-ended",
    "start": "37160",
    "end": "41280"
  },
  {
    "text": "generation typically this use case",
    "start": "41280",
    "end": "43800"
  },
  {
    "text": "happens in a chatboard and QA types of",
    "start": "43800",
    "end": "47520"
  },
  {
    "text": "applications main feature of this use",
    "start": "47520",
    "end": "50239"
  },
  {
    "text": "case is unpredictable number of input on",
    "start": "50239",
    "end": "53239"
  },
  {
    "text": "output",
    "start": "53239",
    "end": "54760"
  },
  {
    "text": "tokens second it's a tech summarization",
    "start": "54760",
    "end": "57920"
  },
  {
    "text": "use case we usually have some idea on a",
    "start": "57920",
    "end": "61680"
  },
  {
    "text": "number of input tokens and desired",
    "start": "61680",
    "end": "64158"
  },
  {
    "text": "number of output tokens during this use",
    "start": "64159",
    "end": "67840"
  },
  {
    "text": "case let's take a look at what we have",
    "start": "67840",
    "end": "71000"
  },
  {
    "text": "in our deployment",
    "start": "71000",
    "end": "73119"
  },
  {
    "text": "toolkit LMI containers are a set of high",
    "start": "73119",
    "end": "76680"
  },
  {
    "text": "performance Docker containers purposely",
    "start": "76680",
    "end": "79680"
  },
  {
    "text": "built for large language model",
    "start": "79680",
    "end": "82119"
  },
  {
    "text": "inference with these containers you can",
    "start": "82119",
    "end": "85000"
  },
  {
    "text": "leverage high performance open-source",
    "start": "85000",
    "end": "87680"
  },
  {
    "text": "inference libraries like V LM tensor RT",
    "start": "87680",
    "end": "91560"
  },
  {
    "text": "llm and deep speed to deploy llms on",
    "start": "91560",
    "end": "95560"
  },
  {
    "text": "Amazon Sage maker",
    "start": "95560",
    "end": "97560"
  },
  {
    "text": "endpoints these containers bundle",
    "start": "97560",
    "end": "100600"
  },
  {
    "text": "together a model server with open-source",
    "start": "100600",
    "end": "103799"
  },
  {
    "text": "inference libraries to deliver allinone",
    "start": "103799",
    "end": "107320"
  },
  {
    "text": "llm serving",
    "start": "107320",
    "end": "109439"
  },
  {
    "text": "solution we also provide quick start",
    "start": "109439",
    "end": "112640"
  },
  {
    "text": "notebooks to get you deployed popular",
    "start": "112640",
    "end": "115600"
  },
  {
    "text": "open source models in minutes and",
    "start": "115600",
    "end": "119039"
  },
  {
    "text": "advanced guide",
    "start": "119039",
    "end": "120680"
  },
  {
    "text": "to maximize performance of your uh end",
    "start": "120680",
    "end": "124479"
  },
  {
    "text": "point the llm is a fast and easy to ous",
    "start": "124479",
    "end": "128840"
  },
  {
    "text": "library for llm inference it doesn't",
    "start": "128840",
    "end": "132319"
  },
  {
    "text": "require model compilation and it's more",
    "start": "132319",
    "end": "135800"
  },
  {
    "text": "flexible with respect to input and",
    "start": "135800",
    "end": "138280"
  },
  {
    "text": "output length",
    "start": "138280",
    "end": "141080"
  },
  {
    "text": "variability vlm has the following",
    "start": "141080",
    "end": "143920"
  },
  {
    "text": "features it has state-ofthe-art serving",
    "start": "143920",
    "end": "147920"
  },
  {
    "text": "throughput efficient management",
    "start": "147920",
    "end": "150360"
  },
  {
    "text": "of attention key and value memory with",
    "start": "150360",
    "end": "153160"
  },
  {
    "text": "the mechanism called paged",
    "start": "153160",
    "end": "155720"
  },
  {
    "text": "attention continuous batching of",
    "start": "155720",
    "end": "158280"
  },
  {
    "text": "incoming",
    "start": "158280",
    "end": "159879"
  },
  {
    "text": "request support for several models",
    "start": "159879",
    "end": "162519"
  },
  {
    "text": "quantization techniques like gptq and",
    "start": "162519",
    "end": "167560"
  },
  {
    "text": "awq tensor",
    "start": "167560",
    "end": "169879"
  },
  {
    "text": "rtlm is the open-source library that",
    "start": "169879",
    "end": "173120"
  },
  {
    "text": "provides users with the easy to use",
    "start": "173120",
    "end": "176120"
  },
  {
    "text": "Python API to Define l language models",
    "start": "176120",
    "end": "180080"
  },
  {
    "text": "and build tensor RT engines that",
    "start": "180080",
    "end": "183200"
  },
  {
    "text": "contains state-of-the-art optimization",
    "start": "183200",
    "end": "186200"
  },
  {
    "text": "to perform inference efficiently on",
    "start": "186200",
    "end": "188920"
  },
  {
    "text": "Nvidia",
    "start": "188920",
    "end": "190319"
  },
  {
    "text": "gpus tensor RTM is a good choice for low",
    "start": "190319",
    "end": "194959"
  },
  {
    "text": "variability use cases like information",
    "start": "194959",
    "end": "198400"
  },
  {
    "text": "instruction and",
    "start": "198400",
    "end": "201400"
  },
  {
    "text": "summarization finally quantization",
    "start": "201400",
    "end": "204280"
  },
  {
    "text": "quantization is the model compression",
    "start": "204280",
    "end": "206799"
  },
  {
    "text": "technique the goal is to reduce the",
    "start": "206799",
    "end": "209799"
  },
  {
    "text": "computational and memory cost of running",
    "start": "209799",
    "end": "212840"
  },
  {
    "text": "inference by representing the weights",
    "start": "212840",
    "end": "215200"
  },
  {
    "text": "and activation with low Precision data",
    "start": "215200",
    "end": "218000"
  },
  {
    "text": "types like floating 16 bits or even 8bit",
    "start": "218000",
    "end": "223040"
  },
  {
    "text": "integers instead of using full Precision",
    "start": "223040",
    "end": "226439"
  },
  {
    "text": "32 bits floating Point",
    "start": "226439",
    "end": "228840"
  },
  {
    "text": "numbers there are different techniques",
    "start": "228840",
    "end": "232519"
  },
  {
    "text": "gptq awq and smooth",
    "start": "232519",
    "end": "236040"
  },
  {
    "text": "Quant in this demo we are going to",
    "start": "236040",
    "end": "238959"
  },
  {
    "text": "deploy a awq quantise",
    "start": "238959",
    "end": "243280"
  },
  {
    "text": "model LMI offers multiple inference",
    "start": "243519",
    "end": "247480"
  },
  {
    "text": "backend options each backend offers",
    "start": "247480",
    "end": "251159"
  },
  {
    "text": "support for certain model architectures",
    "start": "251159",
    "end": "254000"
  },
  {
    "text": "and Exhibits unique features and",
    "start": "254000",
    "end": "256519"
  },
  {
    "text": "performance",
    "start": "256519",
    "end": "257919"
  },
  {
    "text": "characteristics in order to achieve the",
    "start": "257919",
    "end": "260479"
  },
  {
    "text": "best performance for your use case we",
    "start": "260479",
    "end": "264120"
  },
  {
    "text": "recommend evaluating multiple backends",
    "start": "264120",
    "end": "267040"
  },
  {
    "text": "to understand which one works best for",
    "start": "267040",
    "end": "269440"
  },
  {
    "text": "you",
    "start": "269440",
    "end": "271680"
  },
  {
    "text": "to help you with choosing the right",
    "start": "271680",
    "end": "273960"
  },
  {
    "text": "hosting",
    "start": "273960",
    "end": "275320"
  },
  {
    "text": "option we created the following flow",
    "start": "275320",
    "end": "279840"
  },
  {
    "text": "chart let's open the demo Notebook on",
    "start": "279840",
    "end": "282800"
  },
  {
    "text": "Amazon sagemaker studio and look at code",
    "start": "282800",
    "end": "286199"
  },
  {
    "text": "examples in this notebook we will deploy",
    "start": "286199",
    "end": "289440"
  },
  {
    "text": "Lama 2 70 billion parameter model on",
    "start": "289440",
    "end": "292800"
  },
  {
    "text": "Amazon Sage maker using large model",
    "start": "292800",
    "end": "295520"
  },
  {
    "text": "inference",
    "start": "295520",
    "end": "296759"
  },
  {
    "text": "containers first we will deploy this mod",
    "start": "296759",
    "end": "299720"
  },
  {
    "text": "model with VM back end for chatboard use",
    "start": "299720",
    "end": "303800"
  },
  {
    "text": "case then we will use tensor RT llm to",
    "start": "303800",
    "end": "308560"
  },
  {
    "text": "deploy this model for text summarization",
    "start": "308560",
    "end": "311199"
  },
  {
    "text": "use case and finally we will deploy awq",
    "start": "311199",
    "end": "315800"
  },
  {
    "text": "quantise model with VM and compare the",
    "start": "315800",
    "end": "319759"
  },
  {
    "text": "performance to our first use case let's",
    "start": "319759",
    "end": "323080"
  },
  {
    "text": "get started in the first few",
    "start": "323080",
    "end": "326160"
  },
  {
    "text": "cells we install required python",
    "start": "326160",
    "end": "328800"
  },
  {
    "text": "libraries and an initialize Sage maker",
    "start": "328800",
    "end": "333400"
  },
  {
    "text": "environment LMI containers offer low",
    "start": "333680",
    "end": "338360"
  },
  {
    "text": "code deployment options all you need is",
    "start": "338360",
    "end": "341800"
  },
  {
    "text": "to set several environment variables",
    "start": "341800",
    "end": "344520"
  },
  {
    "text": "like model ID and rolling badge and LMI",
    "start": "344520",
    "end": "348639"
  },
  {
    "text": "container will take care of the",
    "start": "348639",
    "end": "350680"
  },
  {
    "text": "inference",
    "start": "350680",
    "end": "352240"
  },
  {
    "text": "logic so let's deploy our model using",
    "start": "352240",
    "end": "355560"
  },
  {
    "text": "LMI container and VM backand",
    "start": "355560",
    "end": "360479"
  },
  {
    "text": "using AWS python",
    "start": "360479",
    "end": "362919"
  },
  {
    "text": "SDK it takes three steps to deploy the",
    "start": "362919",
    "end": "366319"
  },
  {
    "text": "model to sagemaker",
    "start": "366319",
    "end": "369280"
  },
  {
    "text": "endpoint first we need to create a model",
    "start": "369280",
    "end": "372639"
  },
  {
    "text": "object that specify the model weight",
    "start": "372639",
    "end": "375759"
  },
  {
    "text": "location and which container to use",
    "start": "375759",
    "end": "378560"
  },
  {
    "text": "during",
    "start": "378560",
    "end": "379560"
  },
  {
    "text": "inference for the purpose of this demo",
    "start": "379560",
    "end": "382720"
  },
  {
    "text": "we already uploaded the model to S3",
    "start": "382720",
    "end": "385319"
  },
  {
    "text": "bucket and we will use version 0.2 27 of",
    "start": "385319",
    "end": "390880"
  },
  {
    "text": "the LMI",
    "start": "390880",
    "end": "393599"
  },
  {
    "text": "container second step we need to create",
    "start": "393880",
    "end": "397560"
  },
  {
    "text": "a endpoint configuration which defines",
    "start": "397560",
    "end": "400720"
  },
  {
    "text": "the instance type timeouts and routing",
    "start": "400720",
    "end": "406080"
  },
  {
    "text": "strategy we will use G5 2x SL instance",
    "start": "406080",
    "end": "411199"
  },
  {
    "text": "in our latest optimization routing",
    "start": "411199",
    "end": "414360"
  },
  {
    "text": "strategy which in our internal test",
    "start": "414360",
    "end": "417479"
  },
  {
    "text": "reduces latency by 30 33% for certain",
    "start": "417479",
    "end": "421199"
  },
  {
    "text": "use",
    "start": "421199",
    "end": "423560"
  },
  {
    "text": "cases and finally we will create the end",
    "start": "423599",
    "end": "427280"
  },
  {
    "text": "point and wait for it to be",
    "start": "427280",
    "end": "430240"
  },
  {
    "text": "ready please note that the provisioning",
    "start": "430240",
    "end": "433520"
  },
  {
    "text": "of the endpoint can take up to 10",
    "start": "433520",
    "end": "436800"
  },
  {
    "text": "minutes we are going to pause this video",
    "start": "436800",
    "end": "439360"
  },
  {
    "text": "and resume when it's",
    "start": "439360",
    "end": "441440"
  },
  {
    "text": "ready okay the end point is",
    "start": "441440",
    "end": "445000"
  },
  {
    "text": "ready Let's test it I'm going to ask the",
    "start": "445000",
    "end": "448520"
  },
  {
    "text": "model if it knows know anything about",
    "start": "448520",
    "end": "451080"
  },
  {
    "text": "Sage",
    "start": "451080",
    "end": "453478"
  },
  {
    "text": "maker and as you can see it does and the",
    "start": "455759",
    "end": "459720"
  },
  {
    "text": "answer is fairly good no",
    "start": "459720",
    "end": "464120"
  },
  {
    "text": "hallucinations next we are going to test",
    "start": "464120",
    "end": "466639"
  },
  {
    "text": "the performance of the endpoint we are",
    "start": "466639",
    "end": "469240"
  },
  {
    "text": "going to send exactly the same prompt 10",
    "start": "469240",
    "end": "472199"
  },
  {
    "text": "times and see what would be average",
    "start": "472199",
    "end": "474879"
  },
  {
    "text": "response",
    "start": "474879",
    "end": "477360"
  },
  {
    "text": "time running the",
    "start": "478000",
    "end": "481039"
  },
  {
    "text": "test and we got the results so on",
    "start": "481039",
    "end": "484039"
  },
  {
    "text": "average it will it took 8 seconds to get",
    "start": "484039",
    "end": "487319"
  },
  {
    "text": "the response from the",
    "start": "487319",
    "end": "490440"
  },
  {
    "text": "model just a reminder don't forget to",
    "start": "490440",
    "end": "493319"
  },
  {
    "text": "clean up all the resources after you",
    "start": "493319",
    "end": "495520"
  },
  {
    "text": "done with this demo so you won't get",
    "start": "495520",
    "end": "497840"
  },
  {
    "text": "charged for the resources you don't",
    "start": "497840",
    "end": "501479"
  },
  {
    "text": "use let's deploy the model for the text",
    "start": "501919",
    "end": "504759"
  },
  {
    "text": "summarization use case we are going to",
    "start": "504759",
    "end": "507520"
  },
  {
    "text": "use LMI container with tzor RT llm back",
    "start": "507520",
    "end": "512518"
  },
  {
    "text": "end we will repeat the same three",
    "start": "512519",
    "end": "516479"
  },
  {
    "text": "steps model creation and point",
    "start": "516479",
    "end": "519240"
  },
  {
    "text": "configuration and end point but this",
    "start": "519240",
    "end": "522360"
  },
  {
    "text": "time we are going to use a different Ser",
    "start": "522360",
    "end": "525160"
  },
  {
    "text": "serving",
    "start": "525160",
    "end": "526600"
  },
  {
    "text": "container and we are going to use a",
    "start": "526600",
    "end": "528920"
  },
  {
    "text": "bigger instance in this case 16x",
    "start": "528920",
    "end": "533440"
  },
  {
    "text": "large the only reason we need a bigger",
    "start": "533440",
    "end": "536680"
  },
  {
    "text": "instance is because Sage maker will will",
    "start": "536680",
    "end": "539600"
  },
  {
    "text": "need to compile the model into optimize",
    "start": "539600",
    "end": "542880"
  },
  {
    "text": "tensor RT format during the deployment",
    "start": "542880",
    "end": "545839"
  },
  {
    "text": "process the compilation process requires",
    "start": "545839",
    "end": "549200"
  },
  {
    "text": "more CPU memory and it will take",
    "start": "549200",
    "end": "552079"
  },
  {
    "text": "slightly longer to wait till the end",
    "start": "552079",
    "end": "554720"
  },
  {
    "text": "point is",
    "start": "554720",
    "end": "556240"
  },
  {
    "text": "ready we are doing this um just in time",
    "start": "556240",
    "end": "560720"
  },
  {
    "text": "compilation for the demo purpose only",
    "start": "560720",
    "end": "564120"
  },
  {
    "text": "you can choose to compile the model",
    "start": "564120",
    "end": "566760"
  },
  {
    "text": "ahead of time and if you do that you",
    "start": "566760",
    "end": "569760"
  },
  {
    "text": "won't need a bigger instance and you",
    "start": "569760",
    "end": "571920"
  },
  {
    "text": "won't have this additional weight",
    "start": "571920",
    "end": "576200"
  },
  {
    "text": "time okay let's go through the steps",
    "start": "578000",
    "end": "580880"
  },
  {
    "text": "model endpoint configuration and the",
    "start": "580880",
    "end": "585240"
  },
  {
    "text": "endpoint end point is",
    "start": "586600",
    "end": "589519"
  },
  {
    "text": "ready let's ask uh this model exactly",
    "start": "589519",
    "end": "593160"
  },
  {
    "text": "the same question what does it know",
    "start": "593160",
    "end": "595320"
  },
  {
    "text": "about Amazon Sage maker",
    "start": "595320",
    "end": "599839"
  },
  {
    "text": "it's hard to say but it looks like",
    "start": "601640",
    "end": "603680"
  },
  {
    "text": "result",
    "start": "603680",
    "end": "604760"
  },
  {
    "text": "is almost",
    "start": "604760",
    "end": "607240"
  },
  {
    "text": "identical and let us run performance",
    "start": "607240",
    "end": "612519"
  },
  {
    "text": "test okay we got the results back the",
    "start": "616200",
    "end": "620120"
  },
  {
    "text": "run time is almost",
    "start": "620120",
    "end": "623680"
  },
  {
    "text": "identical in our final test",
    "start": "626680",
    "end": "630680"
  },
  {
    "text": "we will deploy awq quantise model using",
    "start": "630680",
    "end": "634959"
  },
  {
    "text": "LMI container and vlm back end and",
    "start": "634959",
    "end": "639120"
  },
  {
    "text": "compare performance to our first use",
    "start": "639120",
    "end": "643240"
  },
  {
    "text": "case you should be familiar with the",
    "start": "643240",
    "end": "645720"
  },
  {
    "text": "process by now we are creating the model",
    "start": "645720",
    "end": "649279"
  },
  {
    "text": "objects not that we have to specify that",
    "start": "649279",
    "end": "652560"
  },
  {
    "text": "we use quantise",
    "start": "652560",
    "end": "655519"
  },
  {
    "text": "model then we create the endpoint",
    "start": "655519",
    "end": "658079"
  },
  {
    "text": "configuration the this time again we are",
    "start": "658079",
    "end": "660600"
  },
  {
    "text": "going to use two x slat",
    "start": "660600",
    "end": "663920"
  },
  {
    "text": "instance and finally we'll create then",
    "start": "663920",
    "end": "666639"
  },
  {
    "text": "point and wait for it to be",
    "start": "666639",
    "end": "670399"
  },
  {
    "text": "ready our point is",
    "start": "671800",
    "end": "675440"
  },
  {
    "text": "ready Let's test if our inpoint is",
    "start": "675440",
    "end": "681120"
  },
  {
    "text": "working okay we got the results back and",
    "start": "682560",
    "end": "686800"
  },
  {
    "text": "now we are going to run the performance",
    "start": "686800",
    "end": "688959"
  },
  {
    "text": "test",
    "start": "688959",
    "end": "691240"
  },
  {
    "text": "as you can see the quanti model is a lot",
    "start": "698040",
    "end": "702000"
  },
  {
    "text": "faster but it might be less accurate is",
    "start": "702000",
    "end": "706320"
  },
  {
    "text": "a tradeoff you have to consider",
    "start": "706320",
    "end": "709399"
  },
  {
    "text": "increased performance versus potential",
    "start": "709399",
    "end": "711959"
  },
  {
    "text": "loss of",
    "start": "711959",
    "end": "713320"
  },
  {
    "text": "accuracy we advise you to evaluate this",
    "start": "713320",
    "end": "716560"
  },
  {
    "text": "tradeoff very carefully and test the the",
    "start": "716560",
    "end": "719360"
  },
  {
    "text": "model accuracy on your data before you",
    "start": "719360",
    "end": "721680"
  },
  {
    "text": "make a",
    "start": "721680",
    "end": "723360"
  },
  {
    "text": "decision okay we are done with the Dema",
    "start": "723360",
    "end": "726440"
  },
  {
    "text": "just a reminder don't forget to clean up",
    "start": "726440",
    "end": "728800"
  },
  {
    "text": "the resources so you not charged for",
    "start": "728800",
    "end": "731040"
  },
  {
    "text": "anything you don't use here's the link",
    "start": "731040",
    "end": "734160"
  },
  {
    "text": "to the notebook we used in this demo and",
    "start": "734160",
    "end": "737839"
  },
  {
    "text": "several links to the documentation if",
    "start": "737839",
    "end": "740399"
  },
  {
    "text": "you want to learn more about the topics",
    "start": "740399",
    "end": "742519"
  },
  {
    "text": "we discuss through this",
    "start": "742519",
    "end": "745760"
  },
  {
    "text": "demo thank you very much for your time",
    "start": "746040",
    "end": "748839"
  },
  {
    "text": "hope hopefully you've enjoyed the",
    "start": "748839",
    "end": "750360"
  },
  {
    "text": "session and if you have some free time",
    "start": "750360",
    "end": "752600"
  },
  {
    "text": "please join us in the next session",
    "start": "752600",
    "end": "757040"
  }
]