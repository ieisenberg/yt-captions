[
  {
    "start": "0",
    "end": "60000"
  },
  {
    "text": "good morning everyone it's great to see so many people here for a talk that was just announced just a few minutes ago",
    "start": "1319",
    "end": "8360"
  },
  {
    "text": "obviously you're on a mission like us to bring the world forward from batch into stream processing my name's Roger bar",
    "start": "8360",
    "end": "14759"
  },
  {
    "text": "I'm the general manager for Kinesis I'm here together with my colleague AI Christian the product manager for",
    "start": "14759",
    "end": "22320"
  },
  {
    "text": "Kinesis now if you've heard in the Keynotes Andy and also vender today we actually",
    "start": "23560",
    "end": "29400"
  },
  {
    "text": "broadening the family of Kinesis the family now includes three services so one of our objectives for today is to",
    "start": "29400",
    "end": "36320"
  },
  {
    "text": "make sure that you leave the room understanding what each one of these services do and how they can be combined together",
    "start": "36320",
    "end": "43960"
  },
  {
    "text": "um to actually build even more powerful stream processing applications but of course the real",
    "start": "43960",
    "end": "49160"
  },
  {
    "text": "Focus for this session oh sorry gentlemen",
    "start": "49160",
    "end": "54640"
  },
  {
    "text": "okay the visual aids do help okay all right we got the glitch passed say the",
    "start": "63680",
    "end": "69600"
  },
  {
    "text": "real Focus today guys is also fire hose so after a little bit of introduction and talking about the services we're going to dive deep on fire hose",
    "start": "69600",
    "end": "75640"
  },
  {
    "start": "75000",
    "end": "111000"
  },
  {
    "text": "specifically we want to make sure you understand the key concepts of fire hose talk about what the experience is from",
    "start": "75640",
    "end": "81280"
  },
  {
    "text": "S3 to Red shift and because this is a simple and easy to use service you'll actually find it's a very seamless and",
    "start": "81280",
    "end": "87000"
  },
  {
    "text": "very easy to use experience talk about how you put data into into it with the Kinesis agent which is something new that we've just launched we'll talk",
    "start": "87000",
    "end": "93560"
  },
  {
    "text": "about that and the key API should you actually want to integrate put data yourself directly into fire hose I'm",
    "start": "93560",
    "end": "99920"
  },
  {
    "text": "talk about pricing demo God's willing we're going to run a demonstration for you and show it in action and then",
    "start": "99920",
    "end": "105119"
  },
  {
    "text": "deeper dive into the delivery patterns understanding the key metrics and how you actually troubleshoot your fire",
    "start": "105119",
    "end": "110719"
  },
  {
    "text": "hose all right you know there's a tenant for each one of the services that we're building that we adhere to it's actually",
    "start": "110719",
    "end": "117119"
  },
  {
    "start": "111000",
    "end": "183000"
  },
  {
    "text": "worth going through one through all of them first and foremost easy to provision and deploy and manage with a few clicks from our console we want you",
    "start": "117119",
    "end": "123479"
  },
  {
    "text": "to get your service up and running elastically scalable these services are going to scale in different ways and",
    "start": "123479",
    "end": "128920"
  },
  {
    "text": "we'll talk about how what scalability means for each one of these Services real-time latency because we're going to",
    "start": "128920",
    "end": "134959"
  },
  {
    "text": "continue to squash latencies and I talk about developments or advances we've made in Kinesis we've reduced the latencies there but we want these",
    "start": "134959",
    "end": "141400"
  },
  {
    "text": "services to be able to support real-time processing and it's pay as you go no upfront costs when you get to the we get",
    "start": "141400",
    "end": "147480"
  },
  {
    "text": "to the fire host pricing you'll find that you actually pay exactly for what you get out of the service for exactly",
    "start": "147480",
    "end": "152879"
  },
  {
    "text": "the data that you're putting into and this it's the Last Tenant that's really driven the development of these two new",
    "start": "152879",
    "end": "157920"
  },
  {
    "text": "Services the right services for your specific use cases we've been in Market with Kinesis now for two years we're",
    "start": "157920",
    "end": "164360"
  },
  {
    "text": "learning a lot from our customers we're hearing how they're using streaming how they're using Kinesis and we're identifying opportunities to actually",
    "start": "164360",
    "end": "170879"
  },
  {
    "text": "jump in and actually build services that take that undifferentiated heavy lifting off our developers so they can actually start building more powerful",
    "start": "170879",
    "end": "177000"
  },
  {
    "text": "applications on top of it and it's that Last Tenant that has really motivated both fire hose and Kinesis",
    "start": "177000",
    "end": "183319"
  },
  {
    "start": "183000",
    "end": "283000"
  },
  {
    "text": "analytics and I know some of you in the audience this may be your first time hearing about Kinesis you're new to streaming and you're here to learn so",
    "start": "183319",
    "end": "189599"
  },
  {
    "text": "let's go all the way back to the beginning we announced and launched Kinesis what we call Kinesis streams now",
    "start": "189599",
    "end": "195280"
  },
  {
    "text": "a reinvent 2013 we were using it internally for our batching and our metering data um it was",
    "start": "195280",
    "end": "201599"
  },
  {
    "text": "running at Great scale we brought it out as a customer facing service let's just make sure we have a good mental model of",
    "start": "201599",
    "end": "206920"
  },
  {
    "text": "how it actually gets used we have the AWS SD K also just very simple put",
    "start": "206920",
    "end": "212360"
  },
  {
    "text": "record API that allows you to put hundreds of thousands or millions or billions of events into Kinesis um you",
    "start": "212360",
    "end": "219159"
  },
  {
    "text": "can in fact put a partition key into each record when you create your stream",
    "start": "219159",
    "end": "224959"
  },
  {
    "text": "you can think about the io that you need to have because each Shard which is a component of a stream handles up to",
    "start": "224959",
    "end": "230560"
  },
  {
    "text": "1,000 transactions per second or one megabyte of Ingress per second and double that on the back end so as a",
    "start": "230560",
    "end": "236439"
  },
  {
    "text": "developer you'd say okay what kind of data rates am I expecting I'll go ahead and create my stream with the appropriate number of shards and again",
    "start": "236439",
    "end": "242720"
  },
  {
    "text": "attended to scalability as your business grows You' add more shards but you might say you know what I actually want this",
    "start": "242720",
    "end": "248640"
  },
  {
    "text": "when I put this data in I actually want to be able to associate specific processing with certain pieces of data",
    "start": "248640",
    "end": "253840"
  },
  {
    "text": "you can actually put a partition key in with each record and A Shard has a range of partition keys that it catches the",
    "start": "253840",
    "end": "260359"
  },
  {
    "text": "data for it so every time you write to that partition key range it goes to the same Shard and also we we we make it",
    "start": "260359",
    "end": "268120"
  },
  {
    "text": "available and three availabilities zones before we allow you to read it so the data is highly durable and it's ridiculously inexpensive a penny a",
    "start": "268120",
    "end": "275479"
  },
  {
    "text": "little over one penny for a billion put payload records which is 25 kilobytes and when we launched our payload Max",
    "start": "275479",
    "end": "281479"
  },
  {
    "text": "payload was 50 kilobytes and then on the back end you basically have a 24-hour period of time",
    "start": "281479",
    "end": "288240"
  },
  {
    "start": "283000",
    "end": "343000"
  },
  {
    "text": "in which you could process the data we kept it durable for you for 24 hours and the secret SAA Kinesis is the Kinesis",
    "start": "288240",
    "end": "294919"
  },
  {
    "text": "client library that a developer building their application on ec2 could use the Kinesis Cent",
    "start": "294919",
    "end": "300560"
  },
  {
    "text": "Library um to to write the to run Builder application the Kinesis client Library allows you to get records from",
    "start": "300560",
    "end": "307160"
  },
  {
    "text": "the stream but it keeps track of exactly where each record processing application is at if it fails it can restart pick",
    "start": "307160",
    "end": "313840"
  },
  {
    "text": "right back up where it left off and also because of autoscaling you can set an autoscaling rule you say look if we're",
    "start": "313840",
    "end": "319120"
  },
  {
    "text": "falling behind or a big surge of data's come in I want my back end to scale automatically you could set an autoscaling Rule and the KCl because of",
    "start": "319120",
    "end": "326680"
  },
  {
    "text": "the metadata it keeps in Dynamo DB would allow a new instance to your application to start and together they would pick up",
    "start": "326680",
    "end": "332120"
  },
  {
    "text": "and jointly tackle this stream so this is what our infrastructure is and we we saw all sorts of customers building",
    "start": "332120",
    "end": "338880"
  },
  {
    "text": "different backend custom applications reading data off of the Kinesis stream we've been in market now for two years",
    "start": "338880",
    "end": "345160"
  },
  {
    "start": "343000",
    "end": "372000"
  },
  {
    "text": "we have hundreds of customers worldwide who have deployed into production thousands of applications in adte and",
    "start": "345160",
    "end": "351360"
  },
  {
    "text": "gaming and iot just to name a few media math bzo sushiro these are all case",
    "start": "351360",
    "end": "356840"
  },
  {
    "text": "studies that if you want to learn more you can actually find on the AWS site and actually read how they've actually how they've built their applications how",
    "start": "356840",
    "end": "363120"
  },
  {
    "text": "they've architected it and in fact if you just simply Google Kinesis you'll actually find a number of articles that our customers have written themselves",
    "start": "363120",
    "end": "369039"
  },
  {
    "text": "just sharing best practices typical AWS fashion we listen",
    "start": "369039",
    "end": "375919"
  },
  {
    "start": "372000",
    "end": "386000"
  },
  {
    "text": "to our customers 90% of what we have built is exactly what our customers have been talking to and asking for so we",
    "start": "375919",
    "end": "382080"
  },
  {
    "text": "have continued to invest in kesa streams and will continue to do so going forward and just look what we've done over the",
    "start": "382080",
    "end": "387599"
  },
  {
    "start": "386000",
    "end": "435000"
  },
  {
    "text": "last year some of our customers are saying you know an HTTP put call is kind of expensive for me I'd really like to",
    "start": "387599",
    "end": "394039"
  },
  {
    "text": "batch my records up and actually do a single call we introduced a new API call called put records which allows you to",
    "start": "394039",
    "end": "400160"
  },
  {
    "text": "put up to 500 records or 5 megabyte per payload and each one of those records could have the partition key so the",
    "start": "400160",
    "end": "406080"
  },
  {
    "text": "model is the same just simple one big batch put we increased the individual",
    "start": "406080",
    "end": "411240"
  },
  {
    "text": "payload size from 50 kilobytes up to 1 Megabyte our customers were saying actually we're using Kinesis not for",
    "start": "411240",
    "end": "416440"
  },
  {
    "text": "these iot but we're actually doing some other interesting applications pushing larger data so we increased the payload",
    "start": "416440",
    "end": "422120"
  },
  {
    "text": "this year we also significantly reduced the endend latency specifically from the",
    "start": "422120",
    "end": "427240"
  },
  {
    "text": "time Kinesis caught it to the time you could actually process it is way down in double- digit milliseconds now",
    "start": "427240",
    "end": "432479"
  },
  {
    "text": "significant reduction and we're going to continue to push that down we introduced the Kinesis client",
    "start": "432479",
    "end": "437759"
  },
  {
    "start": "435000",
    "end": "521000"
  },
  {
    "text": "library because some of our developers are saying hey we're we're a python shop we're a node we're a ruby shop and we're going to continue to do that as well we",
    "start": "437759",
    "end": "443960"
  },
  {
    "text": "want to meet the developer where they are today we also introduced the Kinesis producer Library this is is a library",
    "start": "443960",
    "end": "450039"
  },
  {
    "text": "that you can embed into your code it implements best practices for putting data into Kinesis it provides metrics so",
    "start": "450039",
    "end": "456440"
  },
  {
    "text": "you can track exactly how your producer is doing and monitor how how the Kinesis producer is doing again trying to make",
    "start": "456440",
    "end": "462639"
  },
  {
    "text": "it even simpler to put data into your your streaming data into Kinesis and last month we introduced serers side",
    "start": "462639",
    "end": "468919"
  },
  {
    "text": "timestamp so as your data arrives into Kinesis we put a Tim stamp because you may actually want to do something a reason about the time so if you're",
    "start": "468919",
    "end": "475240"
  },
  {
    "text": "unable to put time in yourself we're putting a time stamp on it for you we're also pleased to announce today verer",
    "start": "475240",
    "end": "481680"
  },
  {
    "text": "mentioned this keynote I just want to make sure we hit it we in announcing extended stream retention we're going",
    "start": "481680",
    "end": "487479"
  },
  {
    "text": "now Beyond 24 hours to a full 7-Day retention period and 3 days before the",
    "start": "487479",
    "end": "493400"
  },
  {
    "text": "before this event we're going to launch it we actually had a customer call us frantically and they said we didn't",
    "start": "493400",
    "end": "498720"
  },
  {
    "text": "realize our our processing application's been down and and we're 22 hours behind and we're afraid we're going to lose our",
    "start": "498720",
    "end": "504560"
  },
  {
    "text": "data and so we turned it on for them and they successfully have now actually used the feature to catch up with with their",
    "start": "504560",
    "end": "510360"
  },
  {
    "text": "with their data it's working so thank you thank you thank",
    "start": "510360",
    "end": "516680"
  },
  {
    "text": "you so the team the team has been has been really busy on kesa streams but we",
    "start": "516680",
    "end": "521919"
  },
  {
    "start": "521000",
    "end": "555000"
  },
  {
    "text": "kept we kept seeing and yeah here's I want to summarize this guys when you need that record by record processing to",
    "start": "521919",
    "end": "527560"
  },
  {
    "text": "build a custom application you want to have control over exactly the capacity that you that you provision this is what",
    "start": "527560",
    "end": "534480"
  },
  {
    "text": "Kinesis streams is for and it's not just the KCl we have aache spark storm that you can actually process your data with",
    "start": "534480",
    "end": "540760"
  },
  {
    "text": "AWS Lambda your it's it's only limited by your creativity and we're going to continue to actually innovate on this so",
    "start": "540760",
    "end": "546720"
  },
  {
    "text": "that you can build your custom streaming applications using kesa streams in fact we're going home with a great list of",
    "start": "546720",
    "end": "552320"
  },
  {
    "text": "ideas and we hope to even get more feedback from you while you're here but we also started to notice this",
    "start": "552320",
    "end": "558040"
  },
  {
    "start": "555000",
    "end": "588000"
  },
  {
    "text": "pattern it was in fact this time last year talking to customers at reinvent I'll just put toir I love this",
    "start": "558040",
    "end": "563760"
  },
  {
    "text": "application because they're really bringing the latency down by they monitoring their customers and how they're eating sushi these RFID devices",
    "start": "563760",
    "end": "570680"
  },
  {
    "text": "on the bottom of each plate you pick that plate up they know what was on it they know how long it was on the Carousel and they back at headquarters",
    "start": "570680",
    "end": "577120"
  },
  {
    "text": "can see demand across all 340 or 380 stores and place an order for tuna and have it shipped to the store because",
    "start": "577120",
    "end": "583240"
  },
  {
    "text": "they realize it's selling like hot cakes um good real-time business processing using Kinesis but if you double click",
    "start": "583240",
    "end": "589079"
  },
  {
    "text": "and look at their architecture here we go um basically they're feeding it in Amon Kinesis they have a an application",
    "start": "589079",
    "end": "596240"
  },
  {
    "text": "that they had to write they put it into red shift and Tableau is their user experience and if we think back about",
    "start": "596240",
    "end": "601959"
  },
  {
    "text": "data Zoo they were data zoo was actually dumping all of their data do we move the",
    "start": "601959",
    "end": "607920"
  },
  {
    "start": "607000",
    "end": "641000"
  },
  {
    "text": "data Zoo example sorry guys I'm think those slides got lost um data zoo is another example that we're looking at",
    "start": "607920",
    "end": "614120"
  },
  {
    "text": "and then their Golden Rule is to dump all data into S3 going in and out of the business almost all of our customers",
    "start": "614120",
    "end": "620480"
  },
  {
    "text": "Kinesis customers are putting data into S3 it is the archive for their business they're doing analytics they're also",
    "start": "620480",
    "end": "626440"
  },
  {
    "text": "putting it into red shift as well we said wait a second that takes takes time to actually they have to write an app",
    "start": "626440",
    "end": "631720"
  },
  {
    "text": "they have to write an app they have to provision the resources to actually do this loading and how many customers",
    "start": "631720",
    "end": "636839"
  },
  {
    "text": "might actually start getting excited about streaming if they didn't even have to write an app this is exactly what",
    "start": "636839",
    "end": "642279"
  },
  {
    "start": "641000",
    "end": "708000"
  },
  {
    "text": "motivated Amazon Kinesis fire hose there's absolutely zero Administration required today we'll move your data into",
    "start": "642279",
    "end": "649760"
  },
  {
    "text": "S3 and red shift soon it will be to every destination that our customers care about um but you don't have to",
    "start": "649760",
    "end": "656320"
  },
  {
    "text": "write an app you don't have to spin up any ec2 instances it just works okay in addition just the high",
    "start": "656320",
    "end": "662720"
  },
  {
    "text": "points here you can batch it before we send it we'll compress you can compress",
    "start": "662720",
    "end": "667920"
  },
  {
    "text": "the data with gzip zip and snappy you can encrypt it with KMS and in as little",
    "start": "667920",
    "end": "673720"
  },
  {
    "text": "as 30 seconds we'll pick that data up and move it into S3 and into red shift we'll talk more about the details in the",
    "start": "673720",
    "end": "679200"
  },
  {
    "text": "talk today and seamless elasticity this is a big one and that we're prepared to",
    "start": "679200",
    "end": "685519"
  },
  {
    "text": "take whatever data that you throw at us the moment you create a fire hose we're provisioned to handle the 998th 97th",
    "start": "685519",
    "end": "691519"
  },
  {
    "text": "percentile of customers that we see today if you're one of those top one or two percenters let us know and we'll even provision more for your fire hose",
    "start": "691519",
    "end": "698279"
  },
  {
    "text": "but basically we will automatically scale you do not have to worry about getting throttled if you're under those limits you do not have to worry about",
    "start": "698279",
    "end": "704040"
  },
  {
    "text": "managing partitions or anything it's just ready to go just throw the data at us so the customer experience just to",
    "start": "704040",
    "end": "711120"
  },
  {
    "text": "talk a little bit about that there only three simple Concepts that you have to be aware of you create a delivery stream",
    "start": "711120",
    "end": "718440"
  },
  {
    "text": "a delivery stream is associated with the destination and it's this this delivery stream which is actually the underlying",
    "start": "718440",
    "end": "724480"
  },
  {
    "text": "unit which is actually moving data you don't have to create or provision shards",
    "start": "724480",
    "end": "730120"
  },
  {
    "text": "you don't have to specify partition Keys you simply do a create delivery stream there's a notion of Records these are",
    "start": "730120",
    "end": "736560"
  },
  {
    "text": "the things that are being transmitted across to your delivery stream up to one megabyte payload P per record that gets",
    "start": "736560",
    "end": "743399"
  },
  {
    "text": "transmitted they're data producers and we'll talk about that in a second but they're the things that actually send",
    "start": "743399",
    "end": "748639"
  },
  {
    "text": "the data to into into the delivery stream so it will then appear into S3 that's it very three very simple",
    "start": "748639",
    "end": "754680"
  },
  {
    "text": "Concepts you have to understand and later when you see the API call just see how it maps to these",
    "start": "754680",
    "end": "759880"
  },
  {
    "start": "759000",
    "end": "776000"
  },
  {
    "text": "Concepts so today you can find Kinesis under the single Kinesis console right",
    "start": "759880",
    "end": "765720"
  },
  {
    "text": "now you see two options Kinesis streams Kinesis fire hose when Kinesis analytics is available it too will be under the",
    "start": "765720",
    "end": "771240"
  },
  {
    "text": "family um on the console you simply say I'm going to create a fire hose first thing",
    "start": "771240",
    "end": "778000"
  },
  {
    "text": "you're asked you're really confused figing the delivery stream what's your destination you specify Amazon S3 you",
    "start": "778000",
    "end": "784760"
  },
  {
    "text": "name your stream CU I we assume you're going to have lots of these As you move your streaming data into AWS you tell us",
    "start": "784760",
    "end": "789880"
  },
  {
    "text": "what bucket it's going to appear in what's the prefix and your IM am role next thing that you do what's the",
    "start": "789880",
    "end": "796800"
  },
  {
    "text": "buffer size that that you want to have delivered before you actually flush and actually push it in and what's the",
    "start": "796800",
    "end": "802320"
  },
  {
    "text": "what's the buffer interval anywhere from 60 seconds up to 900 seconds if either of those two thresholds are crossed the",
    "start": "802320",
    "end": "808519"
  },
  {
    "text": "data flot fled this is real continuous delivery of streaming data you'll see",
    "start": "808519",
    "end": "813839"
  },
  {
    "text": "we're we have an agent I'll talk about in a second but literally you're going to point at the files where this data the log files where this data is being",
    "start": "813839",
    "end": "820199"
  },
  {
    "text": "created you could have it running up every every 60 seconds or you could say look when I just get us when I get two",
    "start": "820199",
    "end": "825920"
  },
  {
    "text": "gigabytes when I get a few megabytes I want it uploaded you specify what you what method you wish to use for",
    "start": "825920",
    "end": "831519"
  },
  {
    "text": "compression and how if you in fact wish to wish to encrypt it what KMS Keys you'd like to use that's it what about",
    "start": "831519",
    "end": "838720"
  },
  {
    "start": "838000",
    "end": "878000"
  },
  {
    "text": "what about the red shift experience little bit different slightly first and foremost we use a customer provided S3",
    "start": "838720",
    "end": "846199"
  },
  {
    "text": "bucket as an intermediate destination this is still the most efficient way to do large scale loads up to Red shift not",
    "start": "846199",
    "end": "853800"
  },
  {
    "text": "only that there's an absolutely no way we can lose the data should red shift be unavailable should you be getting",
    "start": "853800",
    "end": "858920"
  },
  {
    "text": "throttled or have some other issue because one of your developers are working on red shift or doing something your data is going to be safely put into",
    "start": "858920",
    "end": "865079"
  },
  {
    "text": "S3 and yes we will try to repeat if we if we encounter errors when you talk more in the deep dive about how often we",
    "start": "865079",
    "end": "870199"
  },
  {
    "text": "do that not only that some of our customers are saying actually we actually want to do processing on S3 with EMR so having that in S3 is",
    "start": "870199",
    "end": "876800"
  },
  {
    "text": "actually kind of a bonus and what we'll do fire hose will issue your customer provided copy",
    "start": "876800",
    "end": "882360"
  },
  {
    "start": "878000",
    "end": "900000"
  },
  {
    "text": "command synchronously and it will continuously issue a copy command once the previous copy command is finished",
    "start": "882360",
    "end": "888000"
  },
  {
    "text": "and acknowledge back from the red shift so again data is coming into S3 as the as into buckets basically we're issuing",
    "start": "888000",
    "end": "894519"
  },
  {
    "text": "your copy Command right in parallel one right after the other until we get your data into red shift",
    "start": "894519",
    "end": "901040"
  },
  {
    "text": "only thing to point out on this experience and I realize in the back of the room this may be a little small but guys all you have to do and it's",
    "start": "901240",
    "end": "906560"
  },
  {
    "text": "actually also worth noting on the service you specify your red shift cluster your red shift database your red",
    "start": "906560",
    "end": "911720"
  },
  {
    "text": "shift table the previous information on the top is your S3 bucket your username your password and then and your copy",
    "start": "911720",
    "end": "918399"
  },
  {
    "text": "command which you can test and see how it actually works now at present a limitation is",
    "start": "918399",
    "end": "926199"
  },
  {
    "start": "922000",
    "end": "949000"
  },
  {
    "text": "that we a fire hose or a delivery stream is associated with with one red shift cluster one database and one table so",
    "start": "926199",
    "end": "932120"
  },
  {
    "text": "for those of you who are actually streaming your data to multiple clusters or to multiple databases fire hoses are",
    "start": "932120",
    "end": "937759"
  },
  {
    "text": "are free to create only when you use them so you can actually create multiple fire hoses over time we'll actually see about actually coales in that so you can",
    "start": "937759",
    "end": "943600"
  },
  {
    "text": "actually have a single fire hose putting into multiple databases and multiple tables but this is how we",
    "start": "943600",
    "end": "949600"
  },
  {
    "start": "949000",
    "end": "981000"
  },
  {
    "text": "start all right we're also introducing the Amazon Kinesis agent this is an",
    "start": "949600",
    "end": "955759"
  },
  {
    "text": "agent that you can download install it will monitor the file files that you're saying here you tell it these are where",
    "start": "955759",
    "end": "961160"
  },
  {
    "text": "my log files are being created it monitors and sends new records to your delivery stream either based on time or",
    "start": "961160",
    "end": "967399"
  },
  {
    "text": "amount of data in in that file it handles file rotations checkpointing and it does retry upon failures again it's",
    "start": "967399",
    "end": "974319"
  },
  {
    "text": "implementing best practices for putting data into Kinesis it'll do retries and handle all all the corner cases for you",
    "start": "974319",
    "end": "981519"
  },
  {
    "start": "981000",
    "end": "1023000"
  },
  {
    "text": "and it delivers data in a reliable timely and simple manner it also emits AWS cloudwatch metrics to help you",
    "start": "981519",
    "end": "988480"
  },
  {
    "text": "better monitor troubleshoot the streaming process and AUD is going to go into the kind of things issues that can happen and basically what the metrics",
    "start": "988480",
    "end": "994839"
  },
  {
    "text": "would be telling you and it's supported today on Amazon Linux Ali version 2015.",
    "start": "994839",
    "end": "1000079"
  },
  {
    "text": "n or later Red Hat Enterprise Linux version 7 um and again you the ideas you",
    "start": "1000079",
    "end": "1005199"
  },
  {
    "text": "install this on your Linux environment such as web servers frontend or log servers and it's also enabled for",
    "start": "1005199",
    "end": "1011720"
  },
  {
    "text": "Kinesis streams so for those of you using Kinesis today and would actually like to have an agent that would automatically start pumping data into",
    "start": "1011720",
    "end": "1017920"
  },
  {
    "text": "your Kinesis streams please bow me and start using that give us feedback on it okay now maybe you don't want to use",
    "start": "1017920",
    "end": "1025880"
  },
  {
    "start": "1023000",
    "end": "1059000"
  },
  {
    "text": "the agent maybe you've got some code written you'd actually just simply like to embed fire hose into your existing",
    "start": "1025880",
    "end": "1031558"
  },
  {
    "text": "application here you see the API for fire hose and the first four they're",
    "start": "1031559",
    "end": "1037000"
  },
  {
    "text": "parallels with Kinesis create stream create delivery stream create delete stream well we have a delete delivery",
    "start": "1037000",
    "end": "1043160"
  },
  {
    "text": "stream list delivery streams update the destinations for a delivery stream then",
    "start": "1043160",
    "end": "1048240"
  },
  {
    "text": "finally put record and put batch record into your delivery stream very simple API if you actually want to roll your",
    "start": "1048240",
    "end": "1054400"
  },
  {
    "text": "own like some of our Kinesis customers have done with the Kinesis API the",
    "start": "1054400",
    "end": "1060480"
  },
  {
    "start": "1059000",
    "end": "1076000"
  },
  {
    "text": "price 3.5 cents per gigabyte ingested I didn't say delivered ingested because",
    "start": "1060480",
    "end": "1067160"
  },
  {
    "text": "that's the same price if you go to S3 or to Red shift or to both okay so hopefully that we hopefully",
    "start": "1067160",
    "end": "1073480"
  },
  {
    "text": "that price point actually tickles you that it's actually a relatively low price point so at this point just to stop before we do a deeper dive say okay",
    "start": "1073480",
    "end": "1080960"
  },
  {
    "start": "1076000",
    "end": "1085000"
  },
  {
    "text": "when would I actually use kesa streams when would I use fire host it's it's real simple I mean if you want to do a",
    "start": "1080960",
    "end": "1087480"
  },
  {
    "start": "1085000",
    "end": "1135000"
  },
  {
    "text": "record by record processing with custom logic to feed your own dashboards to do",
    "start": "1087480",
    "end": "1093000"
  },
  {
    "text": "your own custom processing you want to use kesa streams you get subsec latency",
    "start": "1093000",
    "end": "1098159"
  },
  {
    "text": "on that processing you get your your choice of stream processing Frameworks that you can support on on an ec2",
    "start": "1098159",
    "end": "1103760"
  },
  {
    "text": "instance you have complete artistic control as a developer to actually build your own stream processing application",
    "start": "1103760",
    "end": "1109000"
  },
  {
    "text": "we continue we'll continue to see developers using that fire hose maybe you're finding value out of red shift S3",
    "start": "1109000",
    "end": "1115200"
  },
  {
    "text": "your apps are already reading and working on those today you can now immediately with a no Hands-On approach",
    "start": "1115200",
    "end": "1120679"
  },
  {
    "text": "start delivering your data through fire hose within within seconds within a minute or more up to Red shift and S3 no",
    "start": "1120679",
    "end": "1128559"
  },
  {
    "text": "application to write nothing to manage don't have to worry about the heat don't have to worry about capacity management it will take care of it for",
    "start": "1128559",
    "end": "1135240"
  },
  {
    "text": "you so with that I'm going to turn it over to Audi it's going to take you down for a deeper",
    "start": "1135240",
    "end": "1140880"
  },
  {
    "text": "dive thank you Roger hello everybody thank you so much for for joining us this uh this morning uh I know it's it",
    "start": "1140880",
    "end": "1148320"
  },
  {
    "text": "was an embargo session that comes up jumps in late um let's uh so hopefully",
    "start": "1148320",
    "end": "1153400"
  },
  {
    "text": "you have a good idea for what Kinesis spose is uh kind of the Genesis and why",
    "start": "1153400",
    "end": "1159000"
  },
  {
    "text": "we we built uh build the service and you know how to kind of compare contrast it if you will uh with kesa streams let's",
    "start": "1159000",
    "end": "1165880"
  },
  {
    "start": "1165000",
    "end": "1198000"
  },
  {
    "text": "look at some specifics uh about uh about the delivery mechanism into S3 first and",
    "start": "1165880",
    "end": "1172960"
  },
  {
    "text": "before we before we jump in there uh how many of you use S3 Today Show hands yeah",
    "start": "1172960",
    "end": "1178320"
  },
  {
    "text": "I would have been shocked if it was lesser than that uh how many of you use red shift for any sort of analytics neat",
    "start": "1178320",
    "end": "1184919"
  },
  {
    "text": "uh how many of you have some other system some analytical system that is uh",
    "start": "1184919",
    "end": "1190120"
  },
  {
    "text": "relies on s34 it's kind of durable persistence right great thank you so",
    "start": "1190120",
    "end": "1197880"
  },
  {
    "text": "let's let's look into details of what it means to deliver data into",
    "start": "1197880",
    "end": "1203240"
  },
  {
    "text": "S3 so as Roger mentioned one delivery stream loads data into one of your",
    "start": "1203240",
    "end": "1211280"
  },
  {
    "text": "buckets the buffer size and intervals as rajer mentioned are what ultimately control the size and frequency at which",
    "start": "1211280",
    "end": "1219080"
  },
  {
    "text": "the data more specifically the S3 object is created and put",
    "start": "1219080",
    "end": "1224480"
  },
  {
    "text": "into uh the dimensions range from 1 MB to 128 MB on the on the size of of of",
    "start": "1224480",
    "end": "1230640"
  },
  {
    "text": "the resulting buffered file or from uh 1 minute to up to uh 15 minutes and the",
    "start": "1230640",
    "end": "1237640"
  },
  {
    "text": "buffer operation is effectively a concatenation operation so as you put data using either the put apis or",
    "start": "1237640",
    "end": "1243640"
  },
  {
    "text": "perhaps using the agent or any of the AWS sdks each record if you will is",
    "start": "1243640",
    "end": "1249120"
  },
  {
    "text": "getting concatenated with the with the one that follows until your buffer condition evaluates to True either one",
    "start": "1249120",
    "end": "1256200"
  },
  {
    "text": "of those conditions can satis the first one that that satisfies the condition um triggers the actual put to S3 so so you",
    "start": "1256200",
    "end": "1265039"
  },
  {
    "text": "know from from a pricing perspective think of the the puts into S3 and the S3 storage as distinct from the core fire",
    "start": "1265039",
    "end": "1272159"
  },
  {
    "text": "host cost in case there were some of you uh who had you know perhaps had that confusion the compression operation for",
    "start": "1272159",
    "end": "1279200"
  },
  {
    "text": "fire hose is is optional uh there are three algorithms that we support today",
    "start": "1279200",
    "end": "1284919"
  },
  {
    "text": "gzip zip and snappy some of you are probably thinking about l zo which is a",
    "start": "1284919",
    "end": "1290240"
  },
  {
    "text": "compression that uh red sh really likes as you can imagine is something that that we that we'd like to deliver fairly",
    "start": "1290240",
    "end": "1297039"
  },
  {
    "text": "soon and as you can imagine since the compression is happening after the buffer operation",
    "start": "1297039",
    "end": "1303600"
  },
  {
    "text": "itself the actual size of the S3 object could be smaller than the amount of data",
    "start": "1303600",
    "end": "1309640"
  },
  {
    "text": "you've actually put the raw data you've put into fir hols kind of obvious but you know to to prevent you from",
    "start": "1309640",
    "end": "1316080"
  },
  {
    "text": "surprising yourself um and for Amazon on red shift you'll notice that the only compression option that's supported",
    "start": "1316080",
    "end": "1322039"
  },
  {
    "text": "right now is is gzm okay so the next next step uh fire",
    "start": "1322039",
    "end": "1329120"
  },
  {
    "start": "1325000",
    "end": "1393000"
  },
  {
    "text": "hose because it's delivering data on your behalf you need to Grant it permissions via I roles uh the the fire",
    "start": "1329120",
    "end": "1337279"
  },
  {
    "text": "hose underlying entity which is the delivery stream that is created uh will assume the specific role to gain access",
    "start": "1337279",
    "end": "1343720"
  },
  {
    "text": "to your target S3 bucket uh there are default delivery roles uh in in the",
    "start": "1343720",
    "end": "1349159"
  },
  {
    "text": "experience but you can write your own craft your own specific delivery role and then we can go into depths later on",
    "start": "1349159",
    "end": "1355840"
  },
  {
    "text": "uh on what exactly are the minimal set of permissions you must you need to provide for fire host to successfully",
    "start": "1355840",
    "end": "1361440"
  },
  {
    "text": "put data into the3 bucket fire hoses also enables encryption via uh the AWS",
    "start": "1361440",
    "end": "1368559"
  },
  {
    "text": "Key Management Service so how many of you here use uh KMS for the Key Management Service Great uh KMS is is is",
    "start": "1368559",
    "end": "1377720"
  },
  {
    "text": "a newer Serv service that we announced I believe last year or earlier this year maybe um and it's a fully managed",
    "start": "1377720",
    "end": "1384080"
  },
  {
    "text": "service that makes it easy to create and control the uh the encryption keys that",
    "start": "1384080",
    "end": "1389400"
  },
  {
    "text": "are used for encryption on a variety of different services so on the S3",
    "start": "1389400",
    "end": "1395320"
  },
  {
    "start": "1393000",
    "end": "1459000"
  },
  {
    "text": "front um what firehost does is that you give it",
    "start": "1395320",
    "end": "1400600"
  },
  {
    "text": "access more specifically fire hose will pass the KMS",
    "start": "1400600",
    "end": "1406120"
  },
  {
    "text": "ID that you've already created for your S3 bucket and then uses the encryption",
    "start": "1406120",
    "end": "1412600"
  },
  {
    "text": "context that S3 already has typically using uh the bucket name and the object",
    "start": "1412600",
    "end": "1417679"
  },
  {
    "text": "name to encrypt your data so this is still the",
    "start": "1417679",
    "end": "1423039"
  },
  {
    "text": "S3 KMS encryption integration that firehose is able to rely on so just to",
    "start": "1423039",
    "end": "1429480"
  },
  {
    "text": "be clear if you're already using KMS you should not see any difference in how this operates um just like for your S3",
    "start": "1429480",
    "end": "1436799"
  },
  {
    "text": "bucket fire hose uh needs to be given permission to access the specific key",
    "start": "1436799",
    "end": "1443159"
  },
  {
    "text": "the KMS ID needed for driving through the S3",
    "start": "1443159",
    "end": "1448240"
  },
  {
    "text": "encryption again uh the the encryption is is optional so if in your use case",
    "start": "1448880",
    "end": "1454360"
  },
  {
    "text": "that makes sense uh you can go ahead and use it uh let's now talk about what does it",
    "start": "1454360",
    "end": "1461400"
  },
  {
    "start": "1459000",
    "end": "1649000"
  },
  {
    "text": "mean uh what does the the object look like as it gets delivered uh into the S3",
    "start": "1461400",
    "end": "1467039"
  },
  {
    "text": "bucket and and this is this is going to be interesting because you're going to have a bunch of Downstream flows that rely on",
    "start": "1467039",
    "end": "1473679"
  },
  {
    "text": "the the pattern of the naming Convention of the file itself to drive your Downstream workflow this could be uh",
    "start": "1473679",
    "end": "1480080"
  },
  {
    "text": "running a classical map reduce job this could be running a presto High impella",
    "start": "1480080",
    "end": "1485399"
  },
  {
    "text": "query uh this could be uh running your own custom application this could be triggering a Lambda function as soon as",
    "start": "1485399",
    "end": "1492480"
  },
  {
    "text": "the put as in as fire host completes uh the S3 put successfully whatever the",
    "start": "1492480",
    "end": "1497600"
  },
  {
    "text": "downstream service or application is that is picking up the data from S3 it's good to",
    "start": "1497600",
    "end": "1504159"
  },
  {
    "text": "have a sense for how is the data going to be laid out inside of your S3",
    "start": "1504159",
    "end": "1510360"
  },
  {
    "text": "bucket so uh firose adds a UTC time in the format that's shown in the screen",
    "start": "1510399",
    "end": "1516200"
  },
  {
    "text": "before putting objects uh into into the bucket and and as you all know uh you",
    "start": "1516200",
    "end": "1521600"
  },
  {
    "text": "know the the prefix effectively translates this into a a folder structure of shorts where each label",
    "start": "1521600",
    "end": "1528799"
  },
  {
    "text": "separated by the forward slash becomes uh a bit of a subfolder now you can modify this",
    "start": "1528799",
    "end": "1535840"
  },
  {
    "text": "mechanism uh or at least add to it by also optionally specifying a prefix so",
    "start": "1535840",
    "end": "1541559"
  },
  {
    "text": "which effectively becomes your own uh top level folder U if you add the a",
    "start": "1541559",
    "end": "1546600"
  },
  {
    "text": "forward slash to it then it becomes the form you know my app forward slash followed by the existing prefix or you",
    "start": "1546600",
    "end": "1552919"
  },
  {
    "text": "can or you can do that do so just as a prepend something that's truly prefixed to the to the existing convention",
    "start": "1552919",
    "end": "1559559"
  },
  {
    "text": "without having a a folder structure if you will associated with it and expect the the the delivery pattern to be of",
    "start": "1559559",
    "end": "1566120"
  },
  {
    "text": "the kind noted it's it's a pretty long one so I'm not going to read it the random string towards the very end is a goid that that fire host generates uh",
    "start": "1566120",
    "end": "1573799"
  },
  {
    "text": "and the one thing to to to note here is is that the delivery stream version is",
    "start": "1573799",
    "end": "1580520"
  },
  {
    "text": "will get updated each time you make a configuration change into your fir hose so maybe you went from instead of",
    "start": "1580520",
    "end": "1587080"
  },
  {
    "text": "buffering from 5 megab maybe went to 50 megabytes or maybe you dial down from a 15minute interval down to a 5 minute",
    "start": "1587080",
    "end": "1594279"
  },
  {
    "text": "interval so those are all examples of config changes those config changes",
    "start": "1594279",
    "end": "1599559"
  },
  {
    "text": "result in a new underlying uh delivery uh stream version update it is still",
    "start": "1599559",
    "end": "1605120"
  },
  {
    "text": "your same delivery stream nothing changes in the way you put data and nothing changes in the way the data is",
    "start": "1605120",
    "end": "1611000"
  },
  {
    "text": "being delivered in a timely fashion because firos will update those configurations on your behalf you'll",
    "start": "1611000",
    "end": "1617039"
  },
  {
    "text": "just see the data appear let's say later if you dialed up the buffer interval or see you might see the data become",
    "start": "1617039",
    "end": "1623840"
  },
  {
    "text": "smaller as you have let's say included compression but the key thing is that",
    "start": "1623840",
    "end": "1629279"
  },
  {
    "text": "the the file the naming pattern will be updated to take into account the new",
    "start": "1629279",
    "end": "1635200"
  },
  {
    "text": "delivery stream version number the reason I'm flagging this for you up front is that if your analytical",
    "start": "1635200",
    "end": "1641559"
  },
  {
    "text": "consumption process relies on that stream name you want to make sure that that it it can account for this this",
    "start": "1641559",
    "end": "1647120"
  },
  {
    "text": "variance so so as you can imagine one of one of the core things right off of that is",
    "start": "1647120",
    "end": "1653480"
  },
  {
    "text": "well uh you are relying on firehost to do its job of delivery into S3 in a zero",
    "start": "1653480",
    "end": "1659960"
  },
  {
    "text": "touch complete Peace of Mind manner but given this is a fully managed",
    "start": "1659960",
    "end": "1666080"
  },
  {
    "text": "service that is interfacing with another fully managed service the there could be a there could",
    "start": "1666080",
    "end": "1673640"
  },
  {
    "text": "be a case where your S3 bucket cannot be reached by firose and and there there",
    "start": "1673640",
    "end": "1679120"
  },
  {
    "text": "are variety of of good reasonable conditions under which this can happen but if it but if it does happen",
    "start": "1679120",
    "end": "1687080"
  },
  {
    "text": "the fire hose behavior is as follows it'll try to access the S3 bucket every 5 seconds until the issue is resolved so",
    "start": "1687080",
    "end": "1693960"
  },
  {
    "text": "it's going to effectively try to to to put that data and and it needs to do that because you're continuing to slam",
    "start": "1693960",
    "end": "1700000"
  },
  {
    "text": "data into firehose fire hose is continuing to buffer up objects uh if you will as it's trying to then L the",
    "start": "1700000",
    "end": "1706200"
  },
  {
    "text": "data into into your S3 bucket let's say that you uh the S3 bucket isn't",
    "start": "1706200",
    "end": "1712360"
  },
  {
    "text": "available for hours for whatever other good reason fire hose will continue executing",
    "start": "1712360",
    "end": "1719480"
  },
  {
    "text": "the way it's supposed to and buffer up all data for 24 hours this is where the the shared ancestry of Kinesis phos and",
    "start": "1719480",
    "end": "1726960"
  },
  {
    "text": "Kinesis streams really come comes into play and so you know rest assured all your data is safe for 24 hours until the",
    "start": "1726960",
    "end": "1733760"
  },
  {
    "text": "situation itself is resolved at which point fireos will resume delivery per your desired configuration so if you set",
    "start": "1733760",
    "end": "1740919"
  },
  {
    "text": "you know 5 MB uh buffer size you will notice that there are that the",
    "start": "1740919",
    "end": "1747039"
  },
  {
    "text": "configuration conditions are not violated you still get 5 MB objects for the naming configuration and firos will",
    "start": "1747039",
    "end": "1753960"
  },
  {
    "text": "try to catch up as quickly as it can as soon as the bucket comes back up and of course you know kind of the",
    "start": "1753960",
    "end": "1761080"
  },
  {
    "text": "the caveat here is that it is a 24-hour window so there is a shared responsibility in making sure that the",
    "start": "1761080",
    "end": "1767200"
  },
  {
    "text": "S3 bucket uh does indeed come back online and is accessible uh for fire",
    "start": "1767200",
    "end": "1772919"
  },
  {
    "text": "hose so the core fire hose to S3 mechanism is relatively simple and let's",
    "start": "1772919",
    "end": "1778200"
  },
  {
    "start": "1777000",
    "end": "1823000"
  },
  {
    "text": "switch gears a little bit and talk about uh fire host to Red shift so as as Roger mentioned this mechanism is is a",
    "start": "1778200",
    "end": "1785720"
  },
  {
    "text": "two-step process still uses your S3 bucket as an intermediate uh staging",
    "start": "1785720",
    "end": "1792519"
  },
  {
    "text": "area before fire hose issues the red shift copy command command which several",
    "start": "1792519",
    "end": "1799279"
  },
  {
    "text": "of you are are familiar with if your users of red shift loading data from S3 into red shift remains one of the most",
    "start": "1799279",
    "end": "1806000"
  },
  {
    "text": "popular ways and arguably some of the most efficient ways to do large scale data loads uh into red shift and and and",
    "start": "1806000",
    "end": "1814159"
  },
  {
    "text": "that's the procedure so fire hose isn't uh at least yet doing some uh direct uh",
    "start": "1814159",
    "end": "1819440"
  },
  {
    "text": "direct writing into into your red shift nodes if you will so let's let's discuss now what it",
    "start": "1819440",
    "end": "1825480"
  },
  {
    "start": "1823000",
    "end": "1898000"
  },
  {
    "text": "means uh to uh what is the timeliness around data delivery into red shift so",
    "start": "1825480",
    "end": "1833559"
  },
  {
    "text": "the first step is since the data is landing inside of S3 whatever specific buffer size or",
    "start": "1833559",
    "end": "1839799"
  },
  {
    "text": "interval that you've set will first apply so let's say you've set a one",
    "start": "1839799",
    "end": "1845360"
  },
  {
    "text": "minute uh one minute interval so it'll data that's put into fir hose will take a minute until it shows up in your S3",
    "start": "1845360",
    "end": "1852640"
  },
  {
    "text": "bucket and then it'll do so continuously on a minute by- minute basis the next next thing uh that uh",
    "start": "1852640",
    "end": "1860480"
  },
  {
    "text": "that the fire hose accomplishes on your behalf is is running the red shift copy",
    "start": "1860480",
    "end": "1866840"
  },
  {
    "text": "command the mechanism for the red shift copy command is one of firehost continuously synchronously executing the",
    "start": "1866840",
    "end": "1873639"
  },
  {
    "text": "copy command waiting for the cluster to come back with a positive act and then",
    "start": "1873639",
    "end": "1878840"
  },
  {
    "text": "uploading the next file into it and for efficiency actually firehost does a",
    "start": "1878840",
    "end": "1884880"
  },
  {
    "text": "manifest copy so as the files are being generated it's maintaining a manifest of each file that needs to be written and",
    "start": "1884880",
    "end": "1891840"
  },
  {
    "text": "then executes a manifest copy inside of",
    "start": "1891840",
    "end": "1896278"
  },
  {
    "text": "redshift so let's let's dig a Little Deeper on the red shift copy command mechanics as fire hose uses it so the",
    "start": "1898480",
    "end": "1906799"
  },
  {
    "text": "the the the governing principle for fire hose is really one around recency so get",
    "start": "1906799",
    "end": "1913200"
  },
  {
    "text": "the latest data in quickly and the second governing principle is one of Z error tolerance so if a single record",
    "start": "1913200",
    "end": "1921799"
  },
  {
    "text": "within uh the file fails to load the whole batch of files in that manifest",
    "start": "1921799",
    "end": "1927200"
  },
  {
    "text": "and therefore the entire copy command will fail so that is the the r the the fire",
    "start": "1927200",
    "end": "1934320"
  },
  {
    "text": "hose model which is which is one of the two models that the red shift actually offers to customers uh you can set",
    "start": "1934320",
    "end": "1940960"
  },
  {
    "text": "either conditions on how many files you're allowed to fail inside of the red shift copy command or you can choose to",
    "start": "1940960",
    "end": "1946120"
  },
  {
    "text": "fail the entire batch the the model that fire hose aligns with is a second one so",
    "start": "1946120",
    "end": "1951840"
  },
  {
    "text": "it chooses to fail the entire batch all the objects that were skipped",
    "start": "1951840",
    "end": "1958480"
  },
  {
    "text": "as a result of this are delivered into the same S3 bucket under an errors",
    "start": "1958480",
    "end": "1964320"
  },
  {
    "text": "folder so the objects the Manifest are both if they were skipped and the copy",
    "start": "1964320",
    "end": "1969919"
  },
  {
    "text": "command did not succeed are simply placed into the errors folder uh inside",
    "start": "1969919",
    "end": "1976840"
  },
  {
    "text": "of your S3 box socket that is the intermediate staging",
    "start": "1976840",
    "end": "1981760"
  },
  {
    "text": "area so a natural kind of question is well okay so what if now my red shift cluster is not accessible for any good",
    "start": "1982679",
    "end": "1989639"
  },
  {
    "start": "1983000",
    "end": "2052000"
  },
  {
    "text": "reason now on the good news side the data is still available in S3 so the data is still over",
    "start": "1989639",
    "end": "1995960"
  },
  {
    "text": "there but the fire hose mechanism around around dealing with uh lack of access to",
    "start": "1995960",
    "end": "2001639"
  },
  {
    "text": "the red shift to your red shift clusters as follows if the cluster is not accessible it'll try it'll retry every 5",
    "start": "2001639",
    "end": "2008399"
  },
  {
    "text": "minutes up for up to a 60-minute window and if the cluster is still not reachable uh for a period of 60 minutes",
    "start": "2008399",
    "end": "2016200"
  },
  {
    "text": "It'll skip the current batch of objects that it was meaning to write and move on to the next one and like before those",
    "start": "2016200",
    "end": "2023480"
  },
  {
    "text": "skipped objects along with the Manifest is placed in the ER in the errors folder",
    "start": "2023480",
    "end": "2028960"
  },
  {
    "text": "now on again on the good news side you have access to that data but I guess on the not so good news side there is a",
    "start": "2028960",
    "end": "2036720"
  },
  {
    "text": "somewhat more potenti entially complex backfilling procedure that that you will have to run to reintegrate the the",
    "start": "2036720",
    "end": "2042799"
  },
  {
    "text": "missing data if you will inside of your redri cluster more specifically the table inside of your red",
    "start": "2042799",
    "end": "2050440"
  },
  {
    "start": "2052000",
    "end": "2225000"
  },
  {
    "text": "database now we realize that that part of making things super",
    "start": "2052200",
    "end": "2057320"
  },
  {
    "text": "simple is also making sure that the set of metrics and monitoring mechanisms",
    "start": "2057320",
    "end": "2063679"
  },
  {
    "text": "that we ship with give you a pretty good clear transparent idea of what is going",
    "start": "2063679",
    "end": "2069638"
  },
  {
    "text": "on in the system and we frequently hear this from customers at least tell me if it was you or was it me right and and",
    "start": "2069639",
    "end": "2077200"
  },
  {
    "text": "that's kind of we want to make sure that we can at the very minimum go there and then do a little bit",
    "start": "2077200",
    "end": "2082720"
  },
  {
    "text": "more all the apis that are ultimately being used have a core set of metrics as usual",
    "start": "2082720",
    "end": "2090760"
  },
  {
    "text": "that are available inside the firehost console and inside of cloudwatch where I'm going to focus is on the specific",
    "start": "2090760",
    "end": "2098960"
  },
  {
    "text": "service level if you will metrics that make identification of some of the key problems easy so let's have a look at",
    "start": "2098960",
    "end": "2106200"
  },
  {
    "text": "the metrics really quickly some of them should be pretty obvious so incoming bytes and incoming records regardless of",
    "start": "2106200",
    "end": "2112079"
  },
  {
    "text": "what put API has been used to get the data in uh we capture we capture counts",
    "start": "2112079",
    "end": "2117200"
  },
  {
    "text": "of both of those uh metrics uh from an S3 perspective we also have delivery to",
    "start": "2117200",
    "end": "2122240"
  },
  {
    "text": "S3 bytes and delivery to S3 records again that makes sense uh and it's exact",
    "start": "2122240",
    "end": "2128000"
  },
  {
    "text": "what the what the metric indicates in its name uh delivery to S3 data freshness is kind of a interesting one",
    "start": "2128000",
    "end": "2134160"
  },
  {
    "text": "its goal in life is to tell you uh the age of the oldest record",
    "start": "2134160",
    "end": "2139599"
  },
  {
    "text": "inside a firose so when you look at look at that metric which will be in seconds",
    "start": "2139599",
    "end": "2145640"
  },
  {
    "text": "you can say hey firehose has been delivering the oldest data that was delivered is 30 seconds",
    "start": "2145640",
    "end": "2153520"
  },
  {
    "text": "old or 1 minute old or 90 seconds old and any data older than that is guaranteed",
    "start": "2153520",
    "end": "2161839"
  },
  {
    "text": "to have already been transferred inside into your S3 bucket so so this is a good",
    "start": "2161839",
    "end": "2167400"
  },
  {
    "text": "way to say hey am I slowing down is the system slowing down is there data that I should be seeing but I'm currently not",
    "start": "2167400",
    "end": "2173760"
  },
  {
    "text": "seeing so the data freshness metric will help you get a good idea of what that is and then lastly because firose is",
    "start": "2173760",
    "end": "2180680"
  },
  {
    "text": "executing a put call into your s bucket that's how the the actual data delivery",
    "start": "2180680",
    "end": "2186599"
  },
  {
    "text": "part is happening uh we have a simple success metric which is a ratio of of all which is a ratio of successful puts",
    "start": "2186599",
    "end": "2193599"
  },
  {
    "text": "uh above all s3p put commands that's been executed so whenever that that",
    "start": "2193599",
    "end": "2198960"
  },
  {
    "text": "ratio starts trending in a in a if it's not trending towards one then we know",
    "start": "2198960",
    "end": "2204400"
  },
  {
    "text": "that there have been some failures similarly for red shift there are three analogous metrics so delivery",
    "start": "2204400",
    "end": "2211640"
  },
  {
    "text": "bytes records and a success metric the success metric just like for puts is actually red copy command so successful",
    "start": "2211640",
    "end": "2218240"
  },
  {
    "text": "Richard copy commands Over All Copy commands that were executed by fire hose on your",
    "start": "2218240",
    "end": "2224640"
  },
  {
    "text": "behalf okay so now let's look at some key questions that you'll have to you'll have to you know you might have to",
    "start": "2224640",
    "end": "2230800"
  },
  {
    "start": "2225000",
    "end": "2284000"
  },
  {
    "text": "confront with during a troubleshooting scenario the first one and this is regardless of a troubleshooting thing this could just be an operational best",
    "start": "2230800",
    "end": "2237560"
  },
  {
    "text": "practices is my data flowing end to endend right so whatever I've put into my fire hose uh from any any number of",
    "start": "2237560",
    "end": "2243480"
  },
  {
    "text": "different or any diversity of different data producers is it actually showing up up in",
    "start": "2243480",
    "end": "2248760"
  },
  {
    "text": "S3 um and for this you know the the the metrics that we reviewed specifically",
    "start": "2248760",
    "end": "2254240"
  },
  {
    "text": "incoming bytes and incoming records along with delivery to S3 success or delivery to Red shift success so the",
    "start": "2254240",
    "end": "2259960"
  },
  {
    "text": "data is landing inside of red shift can be used to confirm at a glance what I put in is it actually flowing through",
    "start": "2259960",
    "end": "2266200"
  },
  {
    "text": "and Landing inside of the destination S3 red of both and of course because all",
    "start": "2266200",
    "end": "2271599"
  },
  {
    "text": "the core API level metrics are also available for those of you who are more inclined to look at that can can have",
    "start": "2271599",
    "end": "2277680"
  },
  {
    "text": "can access that as well to kind of verify uh verify and cross",
    "start": "2277680",
    "end": "2283800"
  },
  {
    "start": "2284000",
    "end": "2422000"
  },
  {
    "text": "check okay so what if the key question is you know how much data is being captured and delivered uh to the",
    "start": "2284000",
    "end": "2291400"
  },
  {
    "text": "destination so here too you know you can look at the put records bytes uh put",
    "start": "2291400",
    "end": "2296839"
  },
  {
    "text": "records the bytes and the requests regardless of which API you're using the put record or the put record batch that",
    "start": "2296839",
    "end": "2303200"
  },
  {
    "text": "would be the data volume that is captured by firose and then you can check the delivery to S3 bytes and",
    "start": "2303200",
    "end": "2310160"
  },
  {
    "text": "Records or delivery to Red shift bytes and Records to know that okay what was",
    "start": "2310160",
    "end": "2315319"
  },
  {
    "text": "being captured is it really being delivered of course you'll have to bear in mind here that in case you are",
    "start": "2315319",
    "end": "2320560"
  },
  {
    "text": "running choosing uh one of the compression options that what you deliver will be smaller than what you've",
    "start": "2320560",
    "end": "2327359"
  },
  {
    "text": "put inside of the of of the firehose so so you know don't freak out um and and",
    "start": "2327359",
    "end": "2332880"
  },
  {
    "text": "you can obviously also verify all incoming records and incoming bytes again to see that and and get it get a",
    "start": "2332880",
    "end": "2338200"
  },
  {
    "text": "check for is data really flowing what is being catch and what is being delivered the next question that you",
    "start": "2338200",
    "end": "2344960"
  },
  {
    "text": "might want to ask is uh is data being delivered in a timely manner and over",
    "start": "2344960",
    "end": "2350079"
  },
  {
    "text": "here again the the data freshness metric uh will be will be useful uh because it",
    "start": "2350079",
    "end": "2357839"
  },
  {
    "text": "because it shows the maximum age in seconds of the oldest undelivered record you know that anything uh anything later",
    "start": "2357839",
    "end": "2364160"
  },
  {
    "text": "than that has been delivered anything after that is still being uh still being",
    "start": "2364160",
    "end": "2369440"
  },
  {
    "text": "buffered or processed by fire",
    "start": "2369440",
    "end": "2372880"
  },
  {
    "text": "hose this one can can happen can happen fairly frequently",
    "start": "2376560",
    "end": "2381839"
  },
  {
    "text": "potentially this this one is around well what is going wrong is it is it with fire hose is it with my bucket or is it",
    "start": "2381839",
    "end": "2388319"
  },
  {
    "text": "with my cluster and you want to always start with the success metric the S3 or the",
    "start": "2388319",
    "end": "2396400"
  },
  {
    "text": "red shift version of the delivery success metric to add a very minimum get",
    "start": "2396400",
    "end": "2401520"
  },
  {
    "text": "okay assuming that the data is indeed being captured is it actually being loaded and if let's say in a scenario",
    "start": "2401520",
    "end": "2408040"
  },
  {
    "text": "where the S3 S3 success metric is trending below one then it suggest that",
    "start": "2408040",
    "end": "2413760"
  },
  {
    "text": "there is a potential issue that phos is having in putting data into your s bucket and similarly uh for red",
    "start": "2413760",
    "end": "2421640"
  },
  {
    "text": "shift so what are some of the key steps that you might take if the data is not",
    "start": "2421640",
    "end": "2427319"
  },
  {
    "start": "2422000",
    "end": "2555000"
  },
  {
    "text": "showing up in S3 so you might want to do something like firstly you know you check the incoming uh incoming bytes and Records",
    "start": "2427319",
    "end": "2435000"
  },
  {
    "text": "okay is my data being put successfully the the second thing you might do is check the success metric as we discussed",
    "start": "2435000",
    "end": "2441000"
  },
  {
    "text": "to make sure data is actually being put you want to ensure that the S3",
    "start": "2441000",
    "end": "2447640"
  },
  {
    "text": "bucket that you specified in the delivery stream configuration is indeed that that something hasn't changed about",
    "start": "2447640",
    "end": "2454640"
  },
  {
    "text": "the S3 bucket or it hasn't been taken offline or it hasn't been uh it's not",
    "start": "2454640",
    "end": "2459800"
  },
  {
    "text": "currently being used in some other manner that you did not know any someplace else and you want to always",
    "start": "2459800",
    "end": "2465319"
  },
  {
    "text": "make sure that the IM role and the permissions that associated with it that fireos is assumed to do puts have not",
    "start": "2465319",
    "end": "2471640"
  },
  {
    "text": "been changed such that fire Hol is now not permitted to put data into your S3",
    "start": "2471640",
    "end": "2478520"
  },
  {
    "text": "bucket somewhat analogously uh there are some key steps you can take when data",
    "start": "2478520",
    "end": "2484400"
  },
  {
    "text": "isn't loading inside of red shift now because fyos is loading data today",
    "start": "2484400",
    "end": "2490520"
  },
  {
    "text": "only in a single red shrift cluster single database within that",
    "start": "2490520",
    "end": "2495560"
  },
  {
    "text": "cluster and a single table within that database um there is it operates in a",
    "start": "2495560",
    "end": "2502079"
  },
  {
    "text": "pretty straightforward manner now having said that though um like before you want to check your incoming bytes and",
    "start": "2502079",
    "end": "2507760"
  },
  {
    "text": "incoming records data's coming again you want to check your S3 uh make sure that the data is in fact being staged inside",
    "start": "2507760",
    "end": "2513960"
  },
  {
    "text": "of S3 that you can do with the S3 success metrics uh and similarly on The Wretched success",
    "start": "2513960",
    "end": "2520599"
  },
  {
    "text": "metrics when either of those are trending under one you know there's a there's a problem there uh next you can",
    "start": "2520599",
    "end": "2527000"
  },
  {
    "text": "check the red shift's uh load errors table to verify the actual reason of the copy failure now for those of you who",
    "start": "2527000",
    "end": "2533040"
  },
  {
    "text": "are using red shift are uh probably know this already and you want to make sure",
    "start": "2533040",
    "end": "2539240"
  },
  {
    "text": "that all the configurations that you specified inside of fire hose available both within the",
    "start": "2539240",
    "end": "2545000"
  },
  {
    "text": "console uh and via the API the configurations are are accurate including as before the IM roles",
    "start": "2545000",
    "end": "2554200"
  },
  {
    "text": "themselves so I'm going to switch a little bit and give you just a flavor",
    "start": "2554520",
    "end": "2560079"
  },
  {
    "start": "2555000",
    "end": "2585000"
  },
  {
    "text": "for what was announced today about Amazon Kinesis analytics and before we",
    "start": "2560079",
    "end": "2565880"
  },
  {
    "text": "go there fyos is available uh for all customers it's available in our Us East",
    "start": "2565880",
    "end": "2573000"
  },
  {
    "text": "US West 2 and our EU West regions and it's open for all customers to use um",
    "start": "2573000",
    "end": "2579599"
  },
  {
    "text": "use its scale Amazon Kinesis analytics was announced today it is not currently",
    "start": "2579599",
    "end": "2587400"
  },
  {
    "start": "2585000",
    "end": "2752000"
  },
  {
    "text": "available uh for you to try out it will be so shortly but I do want to give you",
    "start": "2587400",
    "end": "2592800"
  },
  {
    "text": "flavor for the three things that we're targeting uh with with analytics now as",
    "start": "2592800",
    "end": "2597880"
  },
  {
    "text": "you can imagine um you know or given what what we shared at the keynote the",
    "start": "2597880",
    "end": "2603079"
  },
  {
    "text": "key thing is stream processing has been Out Of Reach for for many many customers",
    "start": "2603079",
    "end": "2613359"
  },
  {
    "text": "um and it's beyond merely managing infrastructure the fundamental thing",
    "start": "2613359",
    "end": "2619240"
  },
  {
    "text": "that we've learned from customers is that it's a new programming model it's a new distributed programming model that",
    "start": "2619240",
    "end": "2626480"
  },
  {
    "text": "that that imposes a very steep ramp what we've heard and and they will always be",
    "start": "2626480",
    "end": "2631599"
  },
  {
    "text": "a place for the right tool for the right job something we strongly believe in and in a variety of stream process ing",
    "start": "2631599",
    "end": "2637319"
  },
  {
    "text": "Frameworks exist will continue to exist and new ones will come up but the one that that came back to us time and time",
    "start": "2637319",
    "end": "2643760"
  },
  {
    "text": "again was was the ability to express SQL directly over streaming data as you can",
    "start": "2643760",
    "end": "2650359"
  },
  {
    "text": "imagine SQL is available uh broadly known by large sets of developers uh",
    "start": "2650359",
    "end": "2656760"
  },
  {
    "text": "with or without distributed system skills and what Kinesis analytics will do is enable you to connect in to Amazon",
    "start": "2656760",
    "end": "2664680"
  },
  {
    "text": "kesa streams and Amazon canes as firose as as at least or two streaming data",
    "start": "2664680",
    "end": "2672680"
  },
  {
    "text": "sources we make the task of connecting extremely easy you can parse and schematize the data and you get the",
    "start": "2672680",
    "end": "2679119"
  },
  {
    "text": "ability to apply uh templated uh functions SQL functions onto that data",
    "start": "2679119",
    "end": "2685480"
  },
  {
    "text": "stream directly because this is still a stream processing engine we do care about",
    "start": "2685480",
    "end": "2691960"
  },
  {
    "text": "latency uh we will continue to Target sub 1 second processing latencies uh",
    "start": "2691960",
    "end": "2697280"
  },
  {
    "text": "possibly end to endend and lastly uh because a lot of",
    "start": "2697280",
    "end": "2702359"
  },
  {
    "text": "the underlying um underlying distributed assistance coordination uh including scaling including doing things like um",
    "start": "2702359",
    "end": "2710079"
  },
  {
    "text": "item potency or exactly one semantics that today if you want to get to that it is an incredible amount of work is",
    "start": "2710079",
    "end": "2717040"
  },
  {
    "text": "things that that we we will execute underneath the cars as as a best practice um you do none of that and we",
    "start": "2717040",
    "end": "2724319"
  },
  {
    "text": "will enable elastic scaling of the engine as well so imagine putting data into your Kinesis stream you have",
    "start": "2724319",
    "end": "2730160"
  },
  {
    "text": "complete control over how the Kinesis stream scales in response you've got your Kinesis analytics infrastructure",
    "start": "2730160",
    "end": "2736000"
  },
  {
    "text": "that will scale as your incoming stream scales and size I know a lot of you are probably",
    "start": "2736000",
    "end": "2741280"
  },
  {
    "text": "excited about Kinesis analytics is an announcement only happy to take questions uh after we're done uh but",
    "start": "2741280",
    "end": "2748040"
  },
  {
    "text": "fire hos is available today Kinesis analytics uh soon so just want to round",
    "start": "2748040",
    "end": "2753559"
  },
  {
    "start": "2752000",
    "end": "2831000"
  },
  {
    "text": "things up here uh and talk to you about the Amazon family as Roger mentioned two",
    "start": "2753559",
    "end": "2759319"
  },
  {
    "text": "years ago at this very venue we started on that Journey um and the family has grown Amazon Kinesis is now a family of",
    "start": "2759319",
    "end": "2766800"
  },
  {
    "text": "streaming data services that help you capture deliver and process real-time data streams on AWS Amazon Kinesis",
    "start": "2766800",
    "end": "2774960"
  },
  {
    "text": "streams is is where it all started targeted towards Developers for complete control over how you partition the data",
    "start": "2774960",
    "end": "2782280"
  },
  {
    "text": "how you scale the stream and then complete control over building your custom stream processing applications",
    "start": "2782280",
    "end": "2787800"
  },
  {
    "text": "using any framework or tool of choice Amazon can firehose as hopefully",
    "start": "2787800",
    "end": "2793319"
  },
  {
    "text": "you learned a little bit today gives you a fully managed zero touch mechanism to",
    "start": "2793319",
    "end": "2798599"
  },
  {
    "text": "capture and deliver data into destinations like S3 and red shift and in the future Amazon Kinesis analytics",
    "start": "2798599",
    "end": "2805440"
  },
  {
    "text": "will enable you to apply SQL queries directly to the streaming data sources namely Kinesis streams and firehose so",
    "start": "2805440",
    "end": "2812839"
  },
  {
    "text": "hopefully that gave you a good flavor of of the of the of Amazon kinesis fire hose and uh hopefully you can all get",
    "start": "2812839",
    "end": "2819400"
  },
  {
    "text": "started really quickly I'll I'll take questions uh for the next uh 10 minutes or so",
    "start": "2819400",
    "end": "2827119"
  },
  {
    "text": "[Applause]",
    "start": "2828080",
    "end": "2833449"
  }
]