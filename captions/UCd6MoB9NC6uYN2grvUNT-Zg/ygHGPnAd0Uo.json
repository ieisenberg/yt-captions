[
  {
    "text": "well first I want to say thank you for joining me today I'm really excited to talk with you about developing a data",
    "start": "30",
    "end": "6299"
  },
  {
    "text": "ingestion pipeline that gives you real-time access to your data without having to deploy a single server or ec2",
    "start": "6299",
    "end": "12090"
  },
  {
    "text": "instance let's go ahead and jump in we'll talk a little bit about who I am",
    "start": "12090",
    "end": "17279"
  },
  {
    "text": "my name's Steve Abraham I'm an AWS Solutions Architect in a previous life I was the lead sequel server DBA for",
    "start": "17279",
    "end": "23519"
  },
  {
    "text": "t-mobile for a number of years I then went into independent consulting I did were a lot of work actually for the US",
    "start": "23519",
    "end": "29849"
  },
  {
    "text": "State Department to help them build their communications platform and a lot of work for Hasbro most recently before",
    "start": "29849",
    "end": "36390"
  },
  {
    "text": "coming to AWS I owned my own software company in Tampa Florida and I joined AWS last year so what we're gonna talk",
    "start": "36390",
    "end": "45390"
  },
  {
    "text": "about today is like I said a data ingestion pipeline and we're gonna collect a billion data points over the",
    "start": "45390",
    "end": "51210"
  },
  {
    "text": "course of a given month we're gonna collect it from varied clients that means mobile devices PCs etc we want",
    "start": "51210",
    "end": "58050"
  },
  {
    "text": "near real-time access to that data and we need high performance and high availability something you would",
    "start": "58050",
    "end": "63270"
  },
  {
    "text": "normally get by building out ec2 instances with elastic load balancer and auto scaling groups and we are also",
    "start": "63270",
    "end": "69330"
  },
  {
    "text": "looking for low cost and low maintenance and then later we're gonna be joined by Brian Phillip who the director of business intelligence from Zillow who's",
    "start": "69330",
    "end": "76110"
  },
  {
    "text": "going to talk about his particular implementation of this so there are two",
    "start": "76110",
    "end": "82860"
  },
  {
    "text": "end states that we're going to take a look at today the first one is the end state of Amazon redshift and you can see",
    "start": "82860",
    "end": "88500"
  },
  {
    "text": "on the Left we have a number of a number of different sources of data and they're gonna send their data through the API",
    "start": "88500",
    "end": "93600"
  },
  {
    "text": "gateway API gateway is going to leverage lambda and then that lambda function is going to funnel that data into a Kinesis",
    "start": "93600",
    "end": "100049"
  },
  {
    "text": "stream Kinesis then is going to use lambda functions again to read that data",
    "start": "100049",
    "end": "105390"
  },
  {
    "text": "off of there and push it into s3 when that data hits s3 then it's going to put",
    "start": "105390",
    "end": "110670"
  },
  {
    "text": "a record into sqs queue that says that that file exists and then we'll talk about something that I like to call a",
    "start": "110670",
    "end": "116729"
  },
  {
    "text": "looping lambda function that will load that data in real time into redshift the",
    "start": "116729",
    "end": "122820"
  },
  {
    "text": "other architecture we're going to look at is very similar it's the same general ingestion pattern but instead of dealing",
    "start": "122820",
    "end": "128849"
  },
  {
    "text": "with s3 and SQS we're gonna be loading that directly into Aurora our new relational",
    "start": "128849",
    "end": "134110"
  },
  {
    "text": "database server and I'm gonna talk a little bit about the difference between redshift and Aurora and when it would be",
    "start": "134110",
    "end": "139390"
  },
  {
    "text": "appropriate to use each so let's go ahead and get started with the API gateway so in the diagram here we're",
    "start": "139390",
    "end": "146890"
  },
  {
    "text": "gonna kind of build out from left to right and so we have the clients they're all writing through to the API gateway",
    "start": "146890",
    "end": "152620"
  },
  {
    "text": "what is the API gateway it allows you to create rest based endpoints and it's fully managed there are no servers",
    "start": "152620",
    "end": "158950"
  },
  {
    "text": "there's no pre provisioning of servers you don't need to scale anything that happens for you it enables rapid",
    "start": "158950",
    "end": "166090"
  },
  {
    "text": "development and I'm really excited for Brian to share about his particular story about just how rapidly they were",
    "start": "166090",
    "end": "171370"
  },
  {
    "text": "able to develop using this and there are flexible security controls in place so",
    "start": "171370",
    "end": "176710"
  },
  {
    "text": "you can use I am you can use a pre shared key a number of different options there are four different integration",
    "start": "176710",
    "end": "183430"
  },
  {
    "text": "types one is lambda which we'll talk about today another is to proxy - AWS",
    "start": "183430",
    "end": "188980"
  },
  {
    "text": "services it is possible to just use API gateway as a proxy into Kinesis or into",
    "start": "188980",
    "end": "195490"
  },
  {
    "text": "sqs or many other amazon services directly you can proxy to an existing",
    "start": "195490",
    "end": "202210"
  },
  {
    "text": "service maybe you have a soap based service right now and you want to create rest based endpoints but you don't want",
    "start": "202210",
    "end": "207550"
  },
  {
    "text": "to you don't want to rewrite your code you don't want to stand up new servers this allows you to very quickly and",
    "start": "207550",
    "end": "212920"
  },
  {
    "text": "easily set that up and we also provide mocks so if you have a software development team that needs to integrate",
    "start": "212920",
    "end": "219250"
  },
  {
    "text": "with your API but you don't have the time yet you haven't gotten around to developing the core of the functions in",
    "start": "219250",
    "end": "224920"
  },
  {
    "text": "that API you can use our mocks so they can continue to do their development against it you can also with the API",
    "start": "224920",
    "end": "232630"
  },
  {
    "text": "gateway deploy to stages and you can define what those stages are but for example dev test production but those",
    "start": "232630",
    "end": "238870"
  },
  {
    "text": "are flexible you can create as many and named them whatever you like we support cross-origin resource sharing which",
    "start": "238870",
    "end": "245410"
  },
  {
    "text": "allows for api gateway to work nicely with an s3 and CloudFront combination maybe a third-party library like",
    "start": "245410",
    "end": "251830"
  },
  {
    "text": "angularjs to allow you to host websites that don't require a single ec2 instance",
    "start": "251830",
    "end": "256840"
  },
  {
    "text": "that are instantly distributed across the globe without managing of server farm at all and something that i think",
    "start": "256840",
    "end": "263080"
  },
  {
    "text": "is particularly great about the API gateway is that when you define your API you can export an SDK for Android",
    "start": "263080",
    "end": "270759"
  },
  {
    "text": "iOS or JavaScript for your particular API you don't have to write that it",
    "start": "270759",
    "end": "275979"
  },
  {
    "text": "comes right out of the box for you the cost for API gateway is three dollars",
    "start": "275979",
    "end": "281080"
  },
  {
    "text": "and fifty cents per million calls data in as with most Amazon services is free",
    "start": "281080",
    "end": "286330"
  },
  {
    "text": "the data transfer out is between five to nine cents per gigabyte in our particular scenario we're not pulling",
    "start": "286330",
    "end": "292569"
  },
  {
    "text": "data out we're just pushing data in so there is no fee there and in this particular scenario for a billion calls",
    "start": "292569",
    "end": "298419"
  },
  {
    "text": "we're looking at $3,500 AWS lamda so we",
    "start": "298419",
    "end": "305949"
  },
  {
    "text": "have our producers on the Left they're funneling into the gateway when they are received in the gateway we've attached a",
    "start": "305949",
    "end": "311740"
  },
  {
    "text": "lambda function to those gateway those gateway calls so that in this case it's a post and so when this post comes",
    "start": "311740",
    "end": "318759"
  },
  {
    "text": "through to our gateway it invokes this lambda function lambda is a fully",
    "start": "318759",
    "end": "323919"
  },
  {
    "text": "managed server list compute platform its event-driven but there are a couple of different models and we'll talk about",
    "start": "323919",
    "end": "329800"
  },
  {
    "text": "that here in a minute the platform that you get for your function so it's important to note that lambda allows you to write a function",
    "start": "329800",
    "end": "336280"
  },
  {
    "text": "and focus on the code you don't have to worry about the server and these services that run on that server but it",
    "start": "336280",
    "end": "342639"
  },
  {
    "text": "still runs on a server and so the platform the context in which it runs is Amazon Linux currently the support of",
    "start": "342639",
    "end": "349300"
  },
  {
    "text": "languages are no js' and Java and for those of you that went to the keynote you probably also notice that we picked",
    "start": "349300",
    "end": "354550"
  },
  {
    "text": "up Python today which is a welcome addition you can configure the memory for your lambda functions from 128",
    "start": "354550",
    "end": "362139"
  },
  {
    "text": "megabytes up to one and a half gigabytes and then there's a corresponding CPU allocation that goes along with that but",
    "start": "362139",
    "end": "368050"
  },
  {
    "text": "you just control that one dial with that one number and then there's a timeout which until today was 60 seconds but",
    "start": "368050",
    "end": "374979"
  },
  {
    "text": "also announced today it is now five minutes so the way that lambda gets",
    "start": "374979",
    "end": "381699"
  },
  {
    "text": "invoked there's the direct invocation model this would apply to the Amazon API gateway or custom code you can hook",
    "start": "381699",
    "end": "389110"
  },
  {
    "text": "right into the lambda API and invoke lambda functions directly yourself and",
    "start": "389110",
    "end": "394630"
  },
  {
    "text": "the way that works is the application sends the request into lambda they go through an execution role which grants",
    "start": "394630",
    "end": "400750"
  },
  {
    "text": "permissions to execute the code and then your lambda function is executed there's",
    "start": "400750",
    "end": "406360"
  },
  {
    "text": "them the lamda pull model and this is applied to Kinesis and dynamodb streams",
    "start": "406360",
    "end": "411460"
  },
  {
    "text": "and what this looks like is you have in this case what we're seeing here is there's a Kinesis stream and as the data",
    "start": "411460",
    "end": "417430"
  },
  {
    "text": "is going in there lamda will pull the that data source in this case Kinesis",
    "start": "417430",
    "end": "423370"
  },
  {
    "text": "and then when it detects that there are records in there then it will pull those records into the lambda function and",
    "start": "423370",
    "end": "429280"
  },
  {
    "text": "allow you to do things like maybe ETL on those records that you've pulled in so that you can handle that immediately the",
    "start": "429280",
    "end": "437950"
  },
  {
    "text": "push model is complementary to s3 SNS Cognito and echo and what this is is",
    "start": "437950",
    "end": "444190"
  },
  {
    "text": "that this is really in response to an event so with s3 for example when a file",
    "start": "444190",
    "end": "449320"
  },
  {
    "text": "is uploaded into an s3 bucket then that lambda function gets executed in response to that event and it passes",
    "start": "449320",
    "end": "456130"
  },
  {
    "text": "along metadata like the bucket name the key name etc so that you can respond to that as that data shows up so AWS lambda",
    "start": "456130",
    "end": "466420"
  },
  {
    "text": "on the API gateway they can work well together in if you're looking for something that's very fast and easy to",
    "start": "466420",
    "end": "472570"
  },
  {
    "text": "deploy it's automatically scaling and I can't emphasize this enough that you know it's a hundred percent utilized and",
    "start": "472570",
    "end": "478600"
  },
  {
    "text": "managed that means that you don't have ec2 instances to log into you don't have to patch them you don't have to share",
    "start": "478600",
    "end": "484210"
  },
  {
    "text": "SSH keys this it is you just write the code and deploy the code and it runs for",
    "start": "484210",
    "end": "489400"
  },
  {
    "text": "you as far as the hundred percent utilization I mean even if you're really great at running a load balancer and an",
    "start": "489400",
    "end": "495250"
  },
  {
    "text": "auto scaling group you're always going to have some buffer at the top that you're not using with these two models",
    "start": "495250",
    "end": "501160"
  },
  {
    "text": "then you're you only pay for exactly what you use now if you do have an",
    "start": "501160",
    "end": "507340"
  },
  {
    "text": "existing infrastructure that runs on ec2 and you already have a code base there and you would do achieve high",
    "start": "507340",
    "end": "513280"
  },
  {
    "text": "utilization maybe somewhere north of 90% then it may make sense to just you know",
    "start": "513280",
    "end": "518530"
  },
  {
    "text": "leverage that existing infrastructure but if you're looking to create something out of the gate then these are",
    "start": "518530",
    "end": "524440"
  },
  {
    "text": "a great alternative to so lambda it's 20 cents for the first million or per",
    "start": "524440",
    "end": "530740"
  },
  {
    "text": "million request the first million are on the house for our particular scenario of a billion requests we're looking at about $200 and",
    "start": "530740",
    "end": "537710"
  },
  {
    "text": "then for the gigabyte second so you know I talked earlier about allocating the",
    "start": "537710",
    "end": "543290"
  },
  {
    "text": "memory to a function and so a gigabyte second would be you take the sum total of all of the seconds and we break it",
    "start": "543290",
    "end": "550010"
  },
  {
    "text": "down to 100 millisecond increments we sum that up for the month and we multiply that by the amount of memory",
    "start": "550010",
    "end": "556310"
  },
  {
    "text": "that you've allocated that function and that's your gigabyte seconds and so that is very inexpensive in this case it's",
    "start": "556310",
    "end": "564200"
  },
  {
    "text": "12:35 for this particular design but like I said earlier we could also",
    "start": "564200",
    "end": "569500"
  },
  {
    "text": "potentially just proxy directly into Kinesis which would eliminate this cost altogether today I'd like to just show",
    "start": "569500",
    "end": "575840"
  },
  {
    "text": "you a number of different ways that you can do this and give you the LEGO pieces so to speak so that you can make up your mind about",
    "start": "575840",
    "end": "581930"
  },
  {
    "text": "what works best for you but this in this particular scenario that's what we're looking at Amazon",
    "start": "581930",
    "end": "587390"
  },
  {
    "text": "Kinesis so again producers on the Left API gateway and lambda taking that and",
    "start": "587390",
    "end": "593690"
  },
  {
    "text": "packaging it up and pushing it into a Kinesis stream Kinesis is a fully",
    "start": "593690",
    "end": "599360"
  },
  {
    "text": "managed data aggregator that can handle terabytes of data per hour and you know",
    "start": "599360",
    "end": "604460"
  },
  {
    "text": "it the way that it's designed is you have a stream and then the stream is comprised of different shards when data",
    "start": "604460",
    "end": "610970"
  },
  {
    "text": "comes into that stream on a put then before you get that 200 request back",
    "start": "610970",
    "end": "616010"
  },
  {
    "text": "from the service that data has already been replicated across three different facilities so there's a high degree of",
    "start": "616010",
    "end": "621590"
  },
  {
    "text": "data durability when you write to Kinesis we until today had a 24-hour retention period that retention period",
    "start": "621590",
    "end": "629060"
  },
  {
    "text": "is now I believe it's seven days and then the shards shards are how you how",
    "start": "629060",
    "end": "636560"
  },
  {
    "text": "you add additional capacity so a single shard will support a thousand put requests per second and one megabyte",
    "start": "636560",
    "end": "643400"
  },
  {
    "text": "worth of throughput and then of course two megabytes worth of read and you can think of each of these shards is an",
    "start": "643400",
    "end": "649460"
  },
  {
    "text": "individual thread and I'll explain what that means here in a little bit that'll make more sense in just a second so",
    "start": "649460",
    "end": "655160"
  },
  {
    "text": "traditionally a Kinesis application you have these producer applications on the Left they write to the stream and then",
    "start": "655160",
    "end": "661430"
  },
  {
    "text": "that data is in the stream and the consumer applications read from that stream and the can",
    "start": "661430",
    "end": "666440"
  },
  {
    "text": "tomber applications in here run on ec2 well with lambda we can get rid of those consumer applications and we can put a",
    "start": "666440",
    "end": "674210"
  },
  {
    "text": "lambda function on the stream now when I said earlier that you can think of a shard as a single thread in this",
    "start": "674210",
    "end": "679730"
  },
  {
    "text": "particular scenario we have five shards and they're all executing the same code but they're executing independently of",
    "start": "679730",
    "end": "687770"
  },
  {
    "text": "one another so in this case we have effectively five threads all reading simultaneously from that shard on their",
    "start": "687770",
    "end": "694040"
  },
  {
    "text": "from it stream on each of their own dedicated shards for shard management",
    "start": "694040",
    "end": "700040"
  },
  {
    "text": "you can split a shard so that you can add additional capacity to your stream you can merge shards if you find that",
    "start": "700040",
    "end": "706340"
  },
  {
    "text": "you're not utilizing them and bring it back down to reduce cost and then you",
    "start": "706340",
    "end": "711350"
  },
  {
    "text": "can also if you want to automate this you can use the Amazon Kinesis scaling utility this was a tool that was",
    "start": "711350",
    "end": "716990"
  },
  {
    "text": "developed by one of my colleagues and it's currently available on github so",
    "start": "716990",
    "end": "722510"
  },
  {
    "text": "Amazon Kinesis with the API gateway you can you can if you need to have a",
    "start": "722510",
    "end": "728840"
  },
  {
    "text": "rest-based API you can put the API gateway in front of it and this is usually the case if you have maybe a",
    "start": "728840",
    "end": "735050"
  },
  {
    "text": "third party you're working with or somebody else where they have a specific requirement of what that API needs to",
    "start": "735050",
    "end": "741080"
  },
  {
    "text": "look like if you are working internally and you have control over the code that's accessing Kinesis then you can",
    "start": "741080",
    "end": "747500"
  },
  {
    "text": "write directly to the Kinesis using the Kinesis api it's the least expensive so",
    "start": "747500",
    "end": "754160"
  },
  {
    "text": "and you'll see that right here the cost for Kinesis in this particular scenario to size this out it's one and a half",
    "start": "754160",
    "end": "761030"
  },
  {
    "text": "pennies per shard hours so a shard is about eleven dollars and sixteen cents a month we're taking our billion request",
    "start": "761030",
    "end": "767150"
  },
  {
    "text": "divided by 31 days 86,400 seconds we're averaging about 373 requests per second",
    "start": "767150",
    "end": "773270"
  },
  {
    "text": "we're gonna go ahead and over provision a little bit so that we can support up to 3,000 per second and our total cost",
    "start": "773270",
    "end": "779570"
  },
  {
    "text": "for that stream is 33 dollars and 48 cents for the month so incredibly inexpensive it doesn't even make sense",
    "start": "779570",
    "end": "785720"
  },
  {
    "text": "to run something else on another instance when you have something that's",
    "start": "785720",
    "end": "790880"
  },
  {
    "text": "inexpensive now the put payloads anything that's less than 25 kilobytes in size counts as one put payload in our",
    "start": "790880",
    "end": "797810"
  },
  {
    "text": "particular scenario for a billion put it's going to cost us about $14 a month so our total cost for Kinesis is $47.48",
    "start": "797810",
    "end": "807269"
  },
  {
    "text": "it's really hard to beat Amazon s3 and SQS so you know we still have the",
    "start": "807269",
    "end": "814500"
  },
  {
    "text": "producers they run through the Gateway they hit the stream now these lambda functions that are attached to the stream what they're doing is they're",
    "start": "814500",
    "end": "820560"
  },
  {
    "text": "iterating through the set of records that have been provided to them and they're combining them into a single",
    "start": "820560",
    "end": "826589"
  },
  {
    "text": "stream gzipping them and putting them into an s3 bucket when they hit that s3 bucket like I said there's a lambda",
    "start": "826589",
    "end": "832980"
  },
  {
    "text": "function that detects that and it takes the name and it puts it into an sqs queue the Amazon s3 simple storage",
    "start": "832980",
    "end": "842550"
  },
  {
    "text": "service is secure you can encrypt your data in flight using HTTPS you can",
    "start": "842550",
    "end": "848069"
  },
  {
    "text": "encrypt it rest using either our provided Amazon s3 key you can provide your own encryption key or you can",
    "start": "848069",
    "end": "854639"
  },
  {
    "text": "leverage the Amazon key management service kms it's durable it's been designed for 11 nines of durability and",
    "start": "854639",
    "end": "861329"
  },
  {
    "text": "it's scalable we routinely happen handle millions of requests per second and currently stored trillions of objects in",
    "start": "861329",
    "end": "867930"
  },
  {
    "text": "s3 the key management service manages encryption keys for you you know no",
    "start": "867930",
    "end": "874560"
  },
  {
    "text": "matter how strong your encryption algorithm is if you don't manage your keys those keys fall into the wrong",
    "start": "874560",
    "end": "879569"
  },
  {
    "text": "hands it doesn't matter what kind of encryption algorithm you use because they can just use those keys to unlock",
    "start": "879569",
    "end": "884910"
  },
  {
    "text": "your secrets so the key management service will keep those managed for you",
    "start": "884910",
    "end": "890069"
  },
  {
    "text": "they allow you to encrypt and decrypt data directly without having to keep the key on local instances or machines and",
    "start": "890069",
    "end": "896910"
  },
  {
    "text": "it directly integrates with s 3 RDS redshift and a number of other services but these are the ones that are most",
    "start": "896910",
    "end": "902160"
  },
  {
    "text": "pertinent to our talk today there's also if you want to do it with AWS lambda you",
    "start": "902160",
    "end": "907740"
  },
  {
    "text": "can access it using the API and we actually have an example of that that I'll show during the demo portion of this presentation so when you're working",
    "start": "907740",
    "end": "916620"
  },
  {
    "text": "with us 3 it's important the name that you pick for your files is important because when the files come in if",
    "start": "916620",
    "end": "922529"
  },
  {
    "text": "they're named 1 2 3 4 5 and sequential like that you can create a hotspot so what's recommended is that you create a",
    "start": "922529",
    "end": "928740"
  },
  {
    "text": "more randomized at least prefix to those names in our particular scenario and",
    "start": "928740",
    "end": "933960"
  },
  {
    "text": "the code that I'll show you later we're actually using a good for the whole thing because the name itself doesn't really matter to us it's being logged in",
    "start": "933960",
    "end": "941490"
  },
  {
    "text": "sqs we're just processing that data anyway something that's important to note too is that you know you're",
    "start": "941490",
    "end": "946710"
  },
  {
    "text": "ingesting all of this data right that data is not always necessary to have all",
    "start": "946710",
    "end": "952200"
  },
  {
    "text": "of the time so we have an s3 a life cycle policy and the life cycle policy allows you to say after this many days I",
    "start": "952200",
    "end": "960330"
  },
  {
    "text": "would like to either delete these objects or I would like to move these objects to Amazon glacier and glacier",
    "start": "960330",
    "end": "967470"
  },
  {
    "text": "you can think of as something like a tape drive in the cloud it's designed for data that you don't need on a",
    "start": "967470",
    "end": "973350"
  },
  {
    "text": "regular basis maybe it's financial records or something that you need to keep for auditing purposes but you're",
    "start": "973350",
    "end": "979470"
  },
  {
    "text": "not really likely going to access that data anytime soon it's very cost",
    "start": "979470",
    "end": "985020"
  },
  {
    "text": "efficient at 7/10 of a penny per gigabyte and it's secure and durable much like s3 the Amazon simple queue",
    "start": "985020",
    "end": "992880"
  },
  {
    "text": "service is simple easy to set up as the name would imply it's secure you encrypt your data using HTTPS and flight it's",
    "start": "992880",
    "end": "999510"
  },
  {
    "text": "durable before you get that response back it's already been copied to multiple data centers and servers and",
    "start": "999510",
    "end": "1005810"
  },
  {
    "text": "then it also automatically scales it's it's one of the easiest services to set",
    "start": "1005810",
    "end": "1011000"
  },
  {
    "text": "up it only takes a few seconds pricing for s3 it varies based on tier you know",
    "start": "1011000",
    "end": "1018110"
  },
  {
    "text": "the volume of data you have it varies based on the region you do pay for put",
    "start": "1018110",
    "end": "1023330"
  },
  {
    "text": "requests and get requests but for our particular use case here for a billion requests we're talking three dollars and",
    "start": "1023330",
    "end": "1029360"
  },
  {
    "text": "87 cents okay Amazon SQS pricing is 50 cents for a",
    "start": "1029360",
    "end": "1036079"
  },
  {
    "text": "million requests the first million are free and in our particular scenario based on how many requests we plan to",
    "start": "1036080",
    "end": "1042740"
  },
  {
    "text": "make we'll be making less than a million per month and so there will be no cost to us for using sqs Amazon redshift so",
    "start": "1042740",
    "end": "1053120"
  },
  {
    "text": "now that we've got the data in s3 and we've got those file names stored in sqs we have this looping lambda function",
    "start": "1053120",
    "end": "1059840"
  },
  {
    "text": "that is going to go to sqs it's going to grab X number of file names and then it's going to process those and load",
    "start": "1059840",
    "end": "1065780"
  },
  {
    "text": "those into redshift and it will continue to loop from there Amazon redshift itself is our fully",
    "start": "1065780",
    "end": "1073260"
  },
  {
    "text": "managed petabyte scale data warehouse it's very fast we leverage columnar storage and data compression it's",
    "start": "1073260",
    "end": "1079530"
  },
  {
    "text": "scalable you can scale up from one node up to a number of nodes depending in",
    "start": "1079530",
    "end": "1084960"
  },
  {
    "text": "different types of nodes and you can do that dynamically you don't you're not set with just the data warehouse size",
    "start": "1084960",
    "end": "1090330"
  },
  {
    "text": "that you pick initially it's fault tolerant the data is replicated across multiple nodes and backed up to s3 and",
    "start": "1090330",
    "end": "1097050"
  },
  {
    "text": "that's very familiar to use you can connect using ODBC and JDBC and it uses a sequel to query the database there's",
    "start": "1097050",
    "end": "1105480"
  },
  {
    "text": "nothing unfamiliar about that the general architecture of redshift is there's a leader node which is what your",
    "start": "1105480",
    "end": "1112620"
  },
  {
    "text": "applications communicate with which receives the queries and figures out what the query plan is going to be and",
    "start": "1112620",
    "end": "1118410"
  },
  {
    "text": "then it distributes that note across compute nodes those compute nodes are then split up into what are known as",
    "start": "1118410",
    "end": "1124260"
  },
  {
    "text": "slices and so if you look at a compute you have a different slice for every virtual CPU you have so the virtual CPU",
    "start": "1124260",
    "end": "1131970"
  },
  {
    "text": "and a slice of memory comprises one slice so that workload is distributed across that and that's one of the ways",
    "start": "1131970",
    "end": "1138000"
  },
  {
    "text": "that we get such incredible performance out of redshift so loading this data",
    "start": "1138000",
    "end": "1144240"
  },
  {
    "text": "into redshift how are we going to do that we're going to use the copy command and the copy command will will give a",
    "start": "1144240",
    "end": "1150720"
  },
  {
    "text": "list it will use a manifest file that has a list of files that need to be processed and then it will allow a",
    "start": "1150720",
    "end": "1156270"
  },
  {
    "text": "redshift to parallel eyes that load you don't need to spin up multiple threads and send all kinds of data and in fact",
    "start": "1156270",
    "end": "1162660"
  },
  {
    "text": "it's better to have a single thread that issues a single copy command it's sequentially with a copy command in",
    "start": "1162660",
    "end": "1170010"
  },
  {
    "text": "it and then all of the files that are specified in that manifest they're all part of a single transaction if one of",
    "start": "1170010",
    "end": "1176880"
  },
  {
    "text": "those files does not load for some reason the whole transaction is rolled back you can encrypt credentials in your",
    "start": "1176880",
    "end": "1182850"
  },
  {
    "text": "source code using kms and we'll see an example of that in our code later redshift and the copy command supports",
    "start": "1182850",
    "end": "1190200"
  },
  {
    "text": "various file types including delimited fixed with JSON and Avro and we also",
    "start": "1190200",
    "end": "1195570"
  },
  {
    "text": "support compression gzip and Elza we're actually using gzip for the files in our particular",
    "start": "1195570",
    "end": "1200909"
  },
  {
    "text": "scenario here micro batch loading is the process of loading lots of very small",
    "start": "1200909",
    "end": "1206519"
  },
  {
    "text": "files we'd like to have the number of files that we're loading at a time equal to a multiple of the slices or virtual",
    "start": "1206519",
    "end": "1212729"
  },
  {
    "text": "cores on that and that redshift cluster we can define the compression type for each of the columns we're importing",
    "start": "1212729",
    "end": "1218279"
  },
  {
    "text": "ahead of time in the table so we can make sure that we have the right the right one picked if possible it's good",
    "start": "1218279",
    "end": "1224399"
  },
  {
    "text": "to load the data in key sort order and then we have the SSD node types the dc1 instance types that also have really",
    "start": "1224399",
    "end": "1231210"
  },
  {
    "text": "fast throughput that will help with speeding up your redshift loads our",
    "start": "1231210",
    "end": "1236609"
  },
  {
    "text": "infinite loop the way we create this is we create a Amazon Kinesis stream with",
    "start": "1236609",
    "end": "1241710"
  },
  {
    "text": "one shard because we want to do things serially here we want to do it on one thread we're gonna attach a lambda",
    "start": "1241710",
    "end": "1247649"
  },
  {
    "text": "function to that Kinesis stream and then we're going to exit and then within that function the first portion of it is",
    "start": "1247649",
    "end": "1253889"
  },
  {
    "text": "going to execute this whole copy command and it's gonna load this data a batch of data into redshift and when that piece",
    "start": "1253889",
    "end": "1260220"
  },
  {
    "text": "is done inside the same function it's gonna put just a dummy record back into that stream that it's attached to what",
    "start": "1260220",
    "end": "1266580"
  },
  {
    "text": "will that do that will trigger that function to be fired again and so it will continue to run in an infinite loop",
    "start": "1266580",
    "end": "1271590"
  },
  {
    "text": "what happens if there's a failure Kinesis with the pull model whenever you're using the pull model with Kinesis",
    "start": "1271590",
    "end": "1277229"
  },
  {
    "text": "it will retry that record so long as it's there so in Kinesis is case 24",
    "start": "1277229",
    "end": "1283049"
  },
  {
    "text": "hours before it times out if you do have a workload where you need to run",
    "start": "1283049",
    "end": "1288720"
  },
  {
    "text": "multiple threads you can apply the same pattern and simply use multiple shards on your stream so walking through it",
    "start": "1288720",
    "end": "1297049"
  },
  {
    "text": "lambda is going to create a manifest file which is a text file with a list of files in s3 that it got out of sq s it",
    "start": "1297049",
    "end": "1304919"
  },
  {
    "text": "will put that manifest into s3 it will call the copy command on the redshift cluster redshift will then load the data",
    "start": "1304919",
    "end": "1311820"
  },
  {
    "text": "directly from s3 into redshift Amazon redshift like I said earlier it's easy",
    "start": "1311820",
    "end": "1318179"
  },
  {
    "text": "to spin up spin down I have one customer that they only need to do data processing at the end of the month and they feel more comfortable with redshift",
    "start": "1318179",
    "end": "1324539"
  },
  {
    "text": "than using something like EMR and so they have they have a redshift cluster",
    "start": "1324539",
    "end": "1329789"
  },
  {
    "text": "they load a bunch of data in it they run some reports they just shut it down they don't pay for it well it's off the next month they",
    "start": "1329789",
    "end": "1335640"
  },
  {
    "text": "started back up and pick up right where they left off in our case we're looking at a two terabyte data warehouse just to",
    "start": "1335640",
    "end": "1340799"
  },
  {
    "text": "get started because we can scale up later there's no reason to over provision and for our cost we're gonna",
    "start": "1340799",
    "end": "1346110"
  },
  {
    "text": "go with a with a reserved instance capacity we're looking at about three",
    "start": "1346110",
    "end": "1351179"
  },
  {
    "text": "hundred and sixty-five dollars a month Amazon Arora so similar design here the",
    "start": "1351179",
    "end": "1357929"
  },
  {
    "text": "difference is is that our lambda functions instead of writing files out to s3 they're going to directly load the",
    "start": "1357929",
    "end": "1363690"
  },
  {
    "text": "data straight into Aurora Aurora is our fully managed relational database it's",
    "start": "1363690",
    "end": "1369600"
  },
  {
    "text": "based on my sequel five six in fact your applications will not know the difference between Aurora and my sequel",
    "start": "1369600",
    "end": "1375900"
  },
  {
    "text": "five six when they're run when they run because it's wire compatible we use the nodd storage engine and one of the",
    "start": "1375900",
    "end": "1382500"
  },
  {
    "text": "things that i think is particularly interesting about Aurora is that we're able to achieve up to five times better",
    "start": "1382500",
    "end": "1388650"
  },
  {
    "text": "performance than my sequel and when I say that I don't mean that an individual query will execute five times faster",
    "start": "1388650",
    "end": "1394860"
  },
  {
    "text": "what I mean is that you can handle five times the load in this case we have a white paper that details how we were",
    "start": "1394860",
    "end": "1400650"
  },
  {
    "text": "able to achieve over 500,000 selects per second and a hundred thousand updates per second Aurora also can leverage",
    "start": "1400650",
    "end": "1408270"
  },
  {
    "text": "multi AZ so that you have high availability if you need to if one of your servers one of your instances needs",
    "start": "1408270",
    "end": "1414929"
  },
  {
    "text": "to be patched it will automatically failover your application won't know the difference you don't have to do anything",
    "start": "1414929",
    "end": "1420179"
  },
  {
    "text": "to manage that it's all automatic and then another thing that I think sets",
    "start": "1420179",
    "end": "1425400"
  },
  {
    "text": "Aurora apart from a lot of other engines is before you before your command finishes when you're inserting or",
    "start": "1425400",
    "end": "1431700"
  },
  {
    "text": "updating data into an Aurora table that data has been replicated six ways across",
    "start": "1431700",
    "end": "1437460"
  },
  {
    "text": "three different availability zones which gives it a very high rate of durability",
    "start": "1437460",
    "end": "1442520"
  },
  {
    "text": "so now the question is do I use Aurora or do I use redshift so redshift is a",
    "start": "1442520",
    "end": "1448620"
  },
  {
    "text": "data warehouse and data warehouses are specifically designed for handling enormous volumes of data and have",
    "start": "1448620",
    "end": "1455940"
  },
  {
    "text": "relatively few queries performed on them in a given time period in fact we have there there's a limit of 50 concurrent",
    "start": "1455940",
    "end": "1463169"
  },
  {
    "text": "queries that can be executed against redshift but if you have a lot of data a to a petabyte of data redshift is a",
    "start": "1463169",
    "end": "1470400"
  },
  {
    "text": "great a great place to put that Aurora on the other hand is designed for OLTP",
    "start": "1470400",
    "end": "1475740"
  },
  {
    "text": "workloads so that's a lot of fast transactions hundreds thousands millions",
    "start": "1475740",
    "end": "1481380"
  },
  {
    "text": "of transactions at a time there is a limit of 64 terabytes on that but that's",
    "start": "1481380",
    "end": "1487559"
  },
  {
    "text": "still quite generous and like I said earlier 500,000 selects or a hundred thousand updates per second",
    "start": "1487559",
    "end": "1494539"
  },
  {
    "text": "so for Aurora there are two different price components there there's the",
    "start": "1494539",
    "end": "1499679"
  },
  {
    "text": "compute component in our case we're looking at a DVR 3x large taking the",
    "start": "1499679",
    "end": "1506130"
  },
  {
    "text": "reserved instance option we're looking at about two hundred and thirty-five dollars a month for storage storage is a",
    "start": "1506130",
    "end": "1513000"
  },
  {
    "text": "little bit different than the other RDS options out there frequently when you run it when you run a database on AWS",
    "start": "1513000",
    "end": "1521270"
  },
  {
    "text": "it's it's very I o intensive right so you will use provisioned I ops often",
    "start": "1521270",
    "end": "1527250"
  },
  {
    "text": "times for your disks that your database sit on which is great provision die ups are great but you your pre provisioning",
    "start": "1527250",
    "end": "1534149"
  },
  {
    "text": "for those I ops with Aurora you only pay by the iOS there's nothing to pre provision so if you have a database even",
    "start": "1534149",
    "end": "1541169"
  },
  {
    "text": "a 64 terabyte database that's sitting out there you will pay the 10 cents per gigabyte but then you don't pay anything",
    "start": "1541169",
    "end": "1547770"
  },
  {
    "text": "for the i/o request unless you use it in this particular scenario here we're looking at about 93 gigabytes worth of",
    "start": "1547770",
    "end": "1554399"
  },
  {
    "text": "data a month and we're just ballparking we'll say 2 billion i/o requests which honestly is probably a little bit on the",
    "start": "1554399",
    "end": "1560669"
  },
  {
    "text": "high side the the total price out the door is about 644 so with that I'd like",
    "start": "1560669",
    "end": "1568860"
  },
  {
    "text": "to introduce Brian Phil pou brian is the director of business intelligence with Zillow and he's gonna tell you a little",
    "start": "1568860",
    "end": "1574409"
  },
  {
    "text": "bit about their experience working with us thank you Steve can you guys hear me okay thank you",
    "start": "1574409",
    "end": "1582860"
  },
  {
    "text": "so Zillow for those of you who don't know we are the leading real estate and",
    "start": "1583000",
    "end": "1588039"
  },
  {
    "text": "home related information marketplace dedicated to empowering consumers with data inspiration knowledge around the",
    "start": "1588039",
    "end": "1594370"
  },
  {
    "text": "place that you guys call home so Who am I Brian Phil pou as I mentioned director of business intelligence at Zillow I've",
    "start": "1594370",
    "end": "1601419"
  },
  {
    "text": "been in till oh just about eight years eight years come this December previous life I spent around six years consulting",
    "start": "1601419",
    "end": "1608110"
  },
  {
    "text": "throughout North America Canada Caribbean as well so let's talk about my",
    "start": "1608110",
    "end": "1613270"
  },
  {
    "text": "the use case we had at Zillow so somebody came to me and said Brian we have something that shipping in three",
    "start": "1613270",
    "end": "1619030"
  },
  {
    "text": "weeks we need you to collect millions of millions of real-time data points we're",
    "start": "1619030",
    "end": "1624580"
  },
  {
    "text": "going to send it to a URL I need you to create that URL and again it's already",
    "start": "1624580",
    "end": "1630669"
  },
  {
    "text": "the codes already written its shipping in three weeks so you better get your act together we have to aggregate this",
    "start": "1630669",
    "end": "1636280"
  },
  {
    "text": "data throughout the day and the business owners need data feedback they need information on reports throughout the",
    "start": "1636280",
    "end": "1643150"
  },
  {
    "text": "day so who what did we what did we do right I knew right away that it was going to be hard to set this up in-house",
    "start": "1643150",
    "end": "1649740"
  },
  {
    "text": "write procure servers work with other teams and get this up we already run a",
    "start": "1649740",
    "end": "1655059"
  },
  {
    "text": "large infrastructure in AWS our data warehouse is primarily running AWS so we",
    "start": "1655059",
    "end": "1660370"
  },
  {
    "text": "knew we wanted to use Kinesis so we happen to reach out to our Solutions Architect who just so happens to be Steve so after an hour conference",
    "start": "1660370",
    "end": "1667150"
  },
  {
    "text": "call we had our game plan in place we knew what we wanted to do which was you",
    "start": "1667150",
    "end": "1673210"
  },
  {
    "text": "guessed it API gateway we created a custom URL endpoint in Amazon gateway to start with",
    "start": "1673210",
    "end": "1678940"
  },
  {
    "text": "we're doing about 16 million posts a day we send that data from the API gateway using lambda functions so we wrote a",
    "start": "1678940",
    "end": "1686500"
  },
  {
    "text": "lambda function that puts the data from the gateway into Kinesis we then use",
    "start": "1686500",
    "end": "1691690"
  },
  {
    "text": "another lambda function to pull the data out of the Kinesis and we encrypt that data using kms and we store it in s3 we",
    "start": "1691690",
    "end": "1700150"
  },
  {
    "text": "analyze our data with spark on EMR we kind of run several spark jobs throughout the day to run a whole bunch",
    "start": "1700150",
    "end": "1706809"
  },
  {
    "text": "of analysis on this data aggregations and so forth we also have the ability in",
    "start": "1706809",
    "end": "1712840"
  },
  {
    "text": "the option to consume it in real time if we want right we're kind of doing more of a for our batch processing at the moment and",
    "start": "1712840",
    "end": "1719590"
  },
  {
    "text": "that's just due to the nature of the data we're looking at but if we ever need to get there we have the ability to just analyze it analyzer kind of in real",
    "start": "1719590",
    "end": "1726700"
  },
  {
    "text": "time so here's a here's just kind of a quick architecture diagram I think Steve has been over this a bit we have a",
    "start": "1726700",
    "end": "1733899"
  },
  {
    "text": "mobile client we're calling the API gateway from there we have three shard",
    "start": "1733899",
    "end": "1738970"
  },
  {
    "text": "setup and Kinesis and from there we're going to our s3 bucket and then running spark jobs so what were some of the",
    "start": "1738970",
    "end": "1744940"
  },
  {
    "text": "challenges and the problems that we we ran into in developing this I would say you know we wrote our solution and",
    "start": "1744940",
    "end": "1750429"
  },
  {
    "text": "nodejs we we've never really written anything and no js' before but it's not overly complicated you know we had to",
    "start": "1750429",
    "end": "1757210"
  },
  {
    "text": "figure out how do we how do we concatenate lines and put a carriage between carriage return between them",
    "start": "1757210",
    "end": "1762279"
  },
  {
    "text": "well quick Google search write solved it for us the hard part with lambda was it's it's just a relatively new product",
    "start": "1762279",
    "end": "1768549"
  },
  {
    "text": "and there's not a ton of documentation or Stack Overflow articles or blog posts",
    "start": "1768549",
    "end": "1774639"
  },
  {
    "text": "so we actually spent a bit of time trying to figure out how to read data from Kinesis encrypt it with KMS and",
    "start": "1774639",
    "end": "1782919"
  },
  {
    "text": "then write it out to s3 that by far took us the most amount of time and that was probably half a day at most in all it",
    "start": "1782919",
    "end": "1789279"
  },
  {
    "text": "took us two weeks to develop this entire solution that was with one developer and honestly that developer was probably",
    "start": "1789279",
    "end": "1795730"
  },
  {
    "text": "spending about 50% of their time working on the solution and it was actually 57 lines of code for us to do this and that",
    "start": "1795730",
    "end": "1803200"
  },
  {
    "text": "57 lines of code includes error handling creating references to kms s3 various",
    "start": "1803200",
    "end": "1809830"
  },
  {
    "text": "Amazon services air handling etc so in reality it was probably about ten lines of code for us to collect sixteen",
    "start": "1809830",
    "end": "1816789"
  },
  {
    "text": "million bits of information throughout the day record it store it in an s3 so",
    "start": "1816789",
    "end": "1822250"
  },
  {
    "text": "extremely quick extremely fast very very little development time for us so let's",
    "start": "1822250",
    "end": "1829029"
  },
  {
    "text": "talk a little bit about the data collection costs using you know three Amazon Kinesis shards cost us about a",
    "start": "1829029",
    "end": "1836409"
  },
  {
    "text": "dollar 30 a day which includes our hourly put costs lambda we allocated 128",
    "start": "1836409",
    "end": "1842549"
  },
  {
    "text": "MB for our for our memory it runs a little under a little about six dollars",
    "start": "1842549",
    "end": "1847870"
  },
  {
    "text": "a day kind of depends our traffic and what we have going on so I mean you can see that this is just",
    "start": "1847870",
    "end": "1853000"
  },
  {
    "text": "extremely cost effective right I think that's about the third of the price of my what my dinner cost me last night so",
    "start": "1853000",
    "end": "1858840"
  },
  {
    "text": "very very very cheap in just two weeks two weeks to development I'd say the",
    "start": "1858840",
    "end": "1864190"
  },
  {
    "text": "other great parts are you know right now we're doing 16 million puts a day when",
    "start": "1864190",
    "end": "1869649"
  },
  {
    "text": "we want to go to 160 million or 500 million a day all I have to do is reach are my Kinesis dream no additional work",
    "start": "1869649",
    "end": "1876309"
  },
  {
    "text": "I don't have to patch servers I don't have to allocate new hardware I don't I don't have to worry about load balancing",
    "start": "1876309",
    "end": "1882580"
  },
  {
    "text": "I'm done that's it that's all I have to do so huge huge just time savings worked",
    "start": "1882580",
    "end": "1890139"
  },
  {
    "text": "worked perfectly for us so on the data analysis side let me let me just talk about that for a minute you know I",
    "start": "1890139",
    "end": "1895899"
  },
  {
    "text": "mentioned that we use spark to perform our ETL data cleanup analysis you know we run some ETL on this data we convert",
    "start": "1895899",
    "end": "1903070"
  },
  {
    "text": "it to park' we do our data partitioning store it in s3 etc you know a Zillo we",
    "start": "1903070",
    "end": "1908710"
  },
  {
    "text": "run a ton of EMR we run presto kind of for our long-term querying of the data we also use Amazon redshift we use the",
    "start": "1908710",
    "end": "1916570"
  },
  {
    "text": "Amazon redshift kind of for our call that our hot data our most recent data that's very very important to us we tend",
    "start": "1916570",
    "end": "1922990"
  },
  {
    "text": "to try to keep that to about a year and then we store you know the rest of it on s3 and using s3 is our HDFS and that's",
    "start": "1922990",
    "end": "1930010"
  },
  {
    "text": "where you see presto for your long term queries along with along with some people data science doing doing spark as",
    "start": "1930010",
    "end": "1937000"
  },
  {
    "text": "I mentioned we're running about four jobs a day using a data pipeline to launch the our spark jobs to kind of",
    "start": "1937000",
    "end": "1942070"
  },
  {
    "text": "process this data that we're recording with Amazon lambda Kinesis cetera so",
    "start": "1942070",
    "end": "1948850"
  },
  {
    "text": "that's that's what we did over at Zillow what else do we use well we use redshift",
    "start": "1948850",
    "end": "1954309"
  },
  {
    "text": "I mentioned that for fast access we are huge users of presto EMR we also run",
    "start": "1954309",
    "end": "1959440"
  },
  {
    "text": "Amazon sqs data pipelines SNS s3 you know kms we want to encrypt our data",
    "start": "1959440",
    "end": "1965230"
  },
  {
    "text": "right our users data is super sensitive we don't want to leave it out there unencrypted gateway we run ec2 hopefully",
    "start": "1965230",
    "end": "1971919"
  },
  {
    "text": "everybody heard the firehose announcement that's something that's not up here but that is something that myself and my team is super excited",
    "start": "1971919",
    "end": "1978549"
  },
  {
    "text": "about basically allows us to take the data right out of to stream and just put it right into s3",
    "start": "1978549",
    "end": "1986250"
  },
  {
    "text": "so before I before I finish just one kind of shout out to the Zillo here we",
    "start": "1986250",
    "end": "1992260"
  },
  {
    "text": "are hiring we are hiring ETL engineers software developers certainly all open",
    "start": "1992260",
    "end": "1997810"
  },
  {
    "text": "positions can be found at Zillow com more than happy I'll stick around answer any questions you have you guys can also find me on LinkedIn my last name is",
    "start": "1997810",
    "end": "2004800"
  },
  {
    "text": "super unique there's only one Brian Phil poo in the world you shouldn't have any problems finding me thank you Steve",
    "start": "2004800",
    "end": "2010170"
  },
  {
    "text": "uh-huh thank you Brian",
    "start": "2010170",
    "end": "2013700"
  },
  {
    "text": "all right so let's get on to what is normally everyone's favorite part of",
    "start": "2016559",
    "end": "2023470"
  },
  {
    "text": "these presentations which is a demo one second here",
    "start": "2023470",
    "end": "2030240"
  },
  {
    "text": "okay so a lot of these a lot of these things that we've talked about today",
    "start": "2037400",
    "end": "2043250"
  },
  {
    "text": "many of you are already very familiar with s3 probably Kinesis a lot of these things some of the things that maybe",
    "start": "2043250",
    "end": "2049280"
  },
  {
    "text": "you're not as familiar with our Amazon API gateway and lambda so we're gonna",
    "start": "2049280",
    "end": "2056628"
  },
  {
    "text": "spend the bulk of our time talking about this and looking at a real world actual implementation of this so what we're",
    "start": "2056629",
    "end": "2062658"
  },
  {
    "text": "looking at here is the Amazon API gateway console from here you can see you can just click here and create an",
    "start": "2062659",
    "end": "2068960"
  },
  {
    "text": "API but we're gonna take a look at this API that I built for what we were talking about in the slides so in here",
    "start": "2068960",
    "end": "2077120"
  },
  {
    "text": "we have our API you can create resources off of that API with the create resource",
    "start": "2077120",
    "end": "2082429"
  },
  {
    "text": "button right here you can specify the path and then you can attach methods off",
    "start": "2082429",
    "end": "2088398"
  },
  {
    "text": "of those in our particular API we are simply using a post right off of the",
    "start": "2088399",
    "end": "2093770"
  },
  {
    "text": "route we're not going to create separate resources once I've created this post method then I'll go into the method",
    "start": "2093770",
    "end": "2100790"
  },
  {
    "text": "request and I'm going to specify in here three different parameters they're going to come in on the query string you can",
    "start": "2100790",
    "end": "2107540"
  },
  {
    "text": "also accept post data you can accept however you need to pass the data but in this particular scenario we're just",
    "start": "2107540",
    "end": "2113990"
  },
  {
    "text": "going to pass the data on the query string so per am one two and three",
    "start": "2113990",
    "end": "2119170"
  },
  {
    "text": "once we've specified that we come over here to the integration request we suspect we take the lambda function here",
    "start": "2119620",
    "end": "2127700"
  },
  {
    "text": "the advance just shows the AWS service proxy but these are the four that we talked about earlier specify my region",
    "start": "2127700",
    "end": "2133610"
  },
  {
    "text": "the lambda function that we're going to execute when we receive this and then we're gonna get into the mapping",
    "start": "2133610",
    "end": "2139700"
  },
  {
    "text": "template so the mapping template we're gonna specify here that for the content type JSON this is how we're going to",
    "start": "2139700",
    "end": "2148130"
  },
  {
    "text": "transform it so you can see we just have a simple template here it takes and",
    "start": "2148130",
    "end": "2153200"
  },
  {
    "text": "takes the input parameters for paramah and 3 and then creates these actor",
    "start": "2153200",
    "end": "2159110"
  },
  {
    "text": "actually they show up as variables once they hit the lambda function so once this is all put into place come back",
    "start": "2159110",
    "end": "2167600"
  },
  {
    "text": "here like test your method say a B C and so",
    "start": "2167600",
    "end": "2176450"
  },
  {
    "text": "you can see right here that our status was this is the request we sent in here's our status back this is how long",
    "start": "2176450",
    "end": "2183050"
  },
  {
    "text": "it took in the particular lambda function that we're using I just have it echo back the payload that I've received",
    "start": "2183050",
    "end": "2189080"
  },
  {
    "text": "so you can see how it comes through as a JSON object with the parameters a B and",
    "start": "2189080",
    "end": "2194510"
  },
  {
    "text": "C you can see the response headers and then here's a more detailed log of",
    "start": "2194510",
    "end": "2200200"
  },
  {
    "text": "exactly what just happened so you can see setting up the actual API endpoint",
    "start": "2200200",
    "end": "2207290"
  },
  {
    "text": "it's very simple there's no provisioning of servers and this will scale as much",
    "start": "2207290",
    "end": "2212930"
  },
  {
    "text": "as I need it to once you've designed your entire API and you're ready to deploy it you can click",
    "start": "2212930",
    "end": "2219770"
  },
  {
    "text": "on deploy API you can select a deployment stage and then or create a",
    "start": "2219770",
    "end": "2225410"
  },
  {
    "text": "new one and then you can click deploy we've already deployed it so I'm not",
    "start": "2225410",
    "end": "2230780"
  },
  {
    "text": "going to play it right now but it's already out there so then let's take a look at what happens once we receive",
    "start": "2230780",
    "end": "2237740"
  },
  {
    "text": "that request so I have five lambda functions here that cover the entire project and we're gonna go through and",
    "start": "2237740",
    "end": "2244670"
  },
  {
    "text": "talk about each of these so the first one the one that's connected to that function or to that API gateway is the",
    "start": "2244670",
    "end": "2250670"
  },
  {
    "text": "data collection lambda in this one it's they're all written in node so they all",
    "start": "2250670",
    "end": "2256430"
  },
  {
    "text": "export Handler there's an event object and a context object these are always passed into the lambda functions we're",
    "start": "2256430",
    "end": "2264230"
  },
  {
    "text": "going to go ahead and specify our settings US East the Kinesis stream that we're going to write - we're gonna",
    "start": "2264230",
    "end": "2270080"
  },
  {
    "text": "stringify this payload from a so that we can have a data object to pass through",
    "start": "2270080",
    "end": "2275330"
  },
  {
    "text": "and then you can see it's very straightforward we're just calling it put record and then we return Kinesis",
    "start": "2275330",
    "end": "2281420"
  },
  {
    "text": "put failed or succeeded once it's in there then we can take a look over here",
    "start": "2281420",
    "end": "2288609"
  },
  {
    "text": "um sure I forgot how you zoom in Chrome it's in",
    "start": "2290650",
    "end": "2301520"
  },
  {
    "text": "the middle control plus all right is",
    "start": "2301520",
    "end": "2308030"
  },
  {
    "text": "that better all right great so in here you can get a glance at how your function is",
    "start": "2308030",
    "end": "2314869"
  },
  {
    "text": "performing and if you want to look in more detail you can click on the the",
    "start": "2314869",
    "end": "2320240"
  },
  {
    "text": "cloud watch logs this will show you an actual breakdown of you know your",
    "start": "2320240",
    "end": "2326090"
  },
  {
    "text": "console dot log messages that you put out it also tells you the memory size",
    "start": "2326090",
    "end": "2331220"
  },
  {
    "text": "that you're using and the memory that you've used during the execution and something else to note is the duration",
    "start": "2331220",
    "end": "2337040"
  },
  {
    "text": "the duration right here so it took sixty nine point five five milliseconds but we're billing it out at 100 milliseconds",
    "start": "2337040",
    "end": "2343369"
  },
  {
    "text": "because you're 200 millisecond increments okay so let's go on the",
    "start": "2343369",
    "end": "2353810"
  },
  {
    "text": "larger font makes it a little difficult to see everything here but everybody",
    "start": "2353810",
    "end": "2360320"
  },
  {
    "text": "back there alright the Kinesis reader function this is what",
    "start": "2360320",
    "end": "2365570"
  },
  {
    "text": "happens when we're reading from that Kinesis stream right so we've taken this function and attached it so when records",
    "start": "2365570",
    "end": "2371930"
  },
  {
    "text": "come in we get a batch of those records they come through in the event object as records note that the event and the",
    "start": "2371930",
    "end": "2378320"
  },
  {
    "text": "context objects they change depending on what they're attached to whether it's a Kinesis stream or its API gateway or how",
    "start": "2378320",
    "end": "2385130"
  },
  {
    "text": "it's being used we're gonna for each through those we're going to create the string we'll use e live which ships",
    "start": "2385130",
    "end": "2392000"
  },
  {
    "text": "natively with with lambda once it zipped",
    "start": "2392000",
    "end": "2397609"
  },
  {
    "text": "we're gonna put it into s3 it's the same sort of thing so you can see these are very short very simple functions and a",
    "start": "2397609",
    "end": "2404960"
  },
  {
    "text": "lot of your lambda functions are going to be exactly that once it hits s3",
    "start": "2404960",
    "end": "2411790"
  },
  {
    "text": "we're just gonna take and extract a bucket name and the key and we're gonna make sure that it's not a manifest file",
    "start": "2415600",
    "end": "2421570"
  },
  {
    "text": "because we don't want to put manifest files into sqs because they're not data files we only want to put the data files",
    "start": "2421570",
    "end": "2426910"
  },
  {
    "text": "that are in there and then we push that into sq s and then we return out of",
    "start": "2426910",
    "end": "2433000"
  },
  {
    "text": "there now these three are very simple very straightforward so what happens if you",
    "start": "2433000",
    "end": "2438070"
  },
  {
    "text": "want to use something with you know the node package manager or something else what if you want to connect to Aurora",
    "start": "2438070",
    "end": "2443650"
  },
  {
    "text": "and you want to upload your data through Aurora or to Aurora or redshift for those it's a little bit more involved so",
    "start": "2443650",
    "end": "2451750"
  },
  {
    "text": "let's take a look at well let's take a look first at the my sequel 1 still it's",
    "start": "2451750",
    "end": "2458650"
  },
  {
    "text": "fairly similar I'm specifying a number of settings here you can see that I've",
    "start": "2458650",
    "end": "2463870"
  },
  {
    "text": "specified my encrypted password I've encrypted this using kms and so I'm not storing any passwords in this lambda",
    "start": "2463870",
    "end": "2471100"
  },
  {
    "text": "function at all and we're going to you know we're going to time the execution of these things so again you know when",
    "start": "2471100",
    "end": "2480280"
  },
  {
    "text": "when that function is fired on Kinesis we're gonna for each through those and instead of putting them together into a",
    "start": "2480280",
    "end": "2486370"
  },
  {
    "text": "string that we're gonna write into into s3 we're gonna take and create an insert statement so insert it into reinvent",
    "start": "2486370",
    "end": "2493750"
  },
  {
    "text": "table values we append those here and then we're going to decrypt the password",
    "start": "2493750",
    "end": "2499320"
  },
  {
    "text": "using kms you can see I'll show you what that looks like you simply create a kms",
    "start": "2499320",
    "end": "2508300"
  },
  {
    "text": "client you call decrypt you pass in the cipher text blob inside that cipher text",
    "start": "2508300",
    "end": "2513580"
  },
  {
    "text": "blob it specifies which key to use and assuming that the execution role that you have applied to your lambda function",
    "start": "2513580",
    "end": "2519850"
  },
  {
    "text": "has sufficient permissions it will allow you to access that key and decrypt that",
    "start": "2519850",
    "end": "2525760"
  },
  {
    "text": "key I'm sorry what's that",
    "start": "2525760",
    "end": "2533210"
  },
  {
    "text": "nope not necessarily and I'll show you that too I'll show you you can look in in RDS and it'll show you especially if",
    "start": "2533820",
    "end": "2541080"
  },
  {
    "text": "you close those connections out and we'll show you just how fast these things are getting put in there so we'll",
    "start": "2541080",
    "end": "2547320"
  },
  {
    "text": "go ahead and create the connection right here we're gonna execute the insert query and then we're gonna disconnect",
    "start": "2547320",
    "end": "2552840"
  },
  {
    "text": "right here now how do you deploy this I",
    "start": "2552840",
    "end": "2558270"
  },
  {
    "text": "mean you have you have a folder that has the additional just this my sequel",
    "start": "2558270",
    "end": "2564740"
  },
  {
    "text": "library right here so you can't really do that through the web console so the other way you can do that is you can use",
    "start": "2564740",
    "end": "2570660"
  },
  {
    "text": "the CLI to upload it and so using the CLI I just have a PowerShell script here",
    "start": "2570660",
    "end": "2577500"
  },
  {
    "text": "and you can also use the PowerShell tools as well but I wrote this using the CLI so that it's a little more portable",
    "start": "2577500",
    "end": "2583560"
  },
  {
    "text": "between platforms but here we're going to we're going to take and zip up that",
    "start": "2583560",
    "end": "2589290"
  },
  {
    "text": "entire folder structure we're gonna delete the function that's already out there",
    "start": "2589290",
    "end": "2594420"
  },
  {
    "text": "we'll call create on the function and we'll pass that zip file directly up there that's got everything all packaged",
    "start": "2594420",
    "end": "2601440"
  },
  {
    "text": "into it including the my sequel library that you use to connect to Aurora and then we're going to attach it to the",
    "start": "2601440",
    "end": "2607080"
  },
  {
    "text": "Kinesis stream right here so let's take a look at what this looks like once it's",
    "start": "2607080",
    "end": "2612120"
  },
  {
    "text": "deployed so the Aurora loader you can",
    "start": "2612120",
    "end": "2618990"
  },
  {
    "text": "look right here at the event sources you can see that I've attached it to this reinvent Kinesis data stream it's",
    "start": "2618990",
    "end": "2625440"
  },
  {
    "text": "currently enabled and it shows that the current status of it is okay click on",
    "start": "2625440",
    "end": "2631320"
  },
  {
    "text": "monitoring like we did before view the cloud watch logs and you can see right",
    "start": "2631320",
    "end": "2639960"
  },
  {
    "text": "here as we go through it shows the execution time for each of these things 39 seconds 98 milliseconds 98",
    "start": "2639960",
    "end": "2646770"
  },
  {
    "text": "milliseconds etc this right here is how you debug these functions is by looking",
    "start": "2646770",
    "end": "2651960"
  },
  {
    "text": "in these logs also it's worth noting that when you deploy them you can invoke",
    "start": "2651960",
    "end": "2657210"
  },
  {
    "text": "them if you call the invoke function like we're doing right here then once you've uploaded that function",
    "start": "2657210",
    "end": "2664569"
  },
  {
    "text": "to lambda you can revoke it from your local workstation in fact if you happen to do this in Java and you use Eclipse",
    "start": "2664569",
    "end": "2671349"
  },
  {
    "text": "Eclipse actually has a plug-in that will allow you to do all of this directly from within the confines of Eclipse so",
    "start": "2671349",
    "end": "2682089"
  },
  {
    "text": "let's take a look at what we did for redshift",
    "start": "2682089",
    "end": "2688380"
  },
  {
    "text": "so for redshift we're gonna use the Postgres library for loading data into",
    "start": "2688380",
    "end": "2693789"
  },
  {
    "text": "there and there are in and I'll just walk through each of these functions about how we did this this first",
    "start": "2693789",
    "end": "2700089"
  },
  {
    "text": "function right here is going to query SQS it's gonna pull the number of records that it needs and it's going to",
    "start": "2700089",
    "end": "2706599"
  },
  {
    "text": "when it returns with the call back it's gonna pass the files the list of files",
    "start": "2706599",
    "end": "2711789"
  },
  {
    "text": "the next one is going to create a manifest which will just be a string in this particular case the next function",
    "start": "2711789",
    "end": "2718329"
  },
  {
    "text": "is going to take that manifest can you guys see this is this too small as well",
    "start": "2718329",
    "end": "2724140"
  },
  {
    "text": "let me see if you bottom-left oh yes",
    "start": "2725579",
    "end": "2733740"
  },
  {
    "text": "is that better is that too big all right so maybe this will make more sense as",
    "start": "2738070",
    "end": "2745150"
  },
  {
    "text": "I'm going through it all right so we want to get the list of files of process",
    "start": "2745150",
    "end": "2751600"
  },
  {
    "text": "so we're going to create a new sqs client and we're in the specified region and we're going to loop and we're going",
    "start": "2751600",
    "end": "2758440"
  },
  {
    "text": "to go ahead and you can you can get messages out of sqs 10 at a time so",
    "start": "2758440",
    "end": "2763570"
  },
  {
    "text": "we're going to do a little bit of arithmetic to figure out you know how many executions we need to do but the",
    "start": "2763570",
    "end": "2770050"
  },
  {
    "text": "bottom line is that we're going to loop through do ten at a time and then do the modulus at the end to get the last one",
    "start": "2770050",
    "end": "2775180"
  },
  {
    "text": "to round it out and that we're putting them into an array of ID and body and",
    "start": "2775180",
    "end": "2781750"
  },
  {
    "text": "we'll need those later on when we delete these records out of sqs once we have",
    "start": "2781750",
    "end": "2788110"
  },
  {
    "text": "that list of files then we'll loop through those and we'll create the manifest file the manifest file it",
    "start": "2788110",
    "end": "2793870"
  },
  {
    "text": "really starts with entries and a bracket and a bunch of URLs and then ends with another bracket we'll put that manifest",
    "start": "2793870",
    "end": "2801070"
  },
  {
    "text": "into s3 it's two lines of code to do that again decrypting decrypting the red",
    "start": "2801070",
    "end": "2808900"
  },
  {
    "text": "shift credentials now with aurora I just encrypted the password with red shift",
    "start": "2808900",
    "end": "2814660"
  },
  {
    "text": "there are two sets of credentials and so I just went ahead and encrypted the entire copy command and along with the",
    "start": "2814660",
    "end": "2821230"
  },
  {
    "text": "credentials to log in to the red shift cluster directly and so I put those into a JSON object so we're gonna create a",
    "start": "2821230",
    "end": "2828970"
  },
  {
    "text": "kms client right here well specify the cipher text blob and then when it comes",
    "start": "2828970",
    "end": "2836290"
  },
  {
    "text": "back then we'll just parse it into an object from the plaintext that was returned and so now we'll see later on",
    "start": "2836290",
    "end": "2842710"
  },
  {
    "text": "that we can use that for our authentication into redshift and then we call the load data into redshift and so",
    "start": "2842710",
    "end": "2849970"
  },
  {
    "text": "we will connect to the redshift cluster once we connect successfully we will call the copy command",
    "start": "2849970",
    "end": "2855880"
  },
  {
    "text": "so this credentials object is what we just decrypted using kms so you'll note",
    "start": "2855880",
    "end": "2861970"
  },
  {
    "text": "there are no credentials stored in here anywhere except well actually later on I did leave them in the comment so I could",
    "start": "2861970",
    "end": "2867880"
  },
  {
    "text": "show you what the copy command looks like but again we'll call client dot n to disconnect and then do",
    "start": "2867880",
    "end": "2875380"
  },
  {
    "text": "our callback once we've once we have committed those records to the database then we needed to leave them out of SQS",
    "start": "2875380",
    "end": "2881290"
  },
  {
    "text": "so earlier I was saying that we needed to make sure that we have the ID and receipt handle so we're going to go",
    "start": "2881290",
    "end": "2888910"
  },
  {
    "text": "through and pull those out create the messages that we need to send back to sqs and say delete these so we don't",
    "start": "2888910",
    "end": "2895120"
  },
  {
    "text": "reprocess them again and then we'll delete those and then the very last thing we do is we put our dummy record",
    "start": "2895120",
    "end": "2902200"
  },
  {
    "text": "into the Kinesis stream that this lambda function is running on and so we just put the letter A using a partition key a",
    "start": "2902200",
    "end": "2909930"
  },
  {
    "text": "into that Kinesis stream in here this is what the connection string looks like to",
    "start": "2909930",
    "end": "2917380"
  },
  {
    "text": "connect to the redshift cluster so it's",
    "start": "2917380",
    "end": "2923020"
  },
  {
    "text": "fairly straightforward but this is part of what was encrypted and the other thing that was encrypted is this copy",
    "start": "2923020",
    "end": "2928420"
  },
  {
    "text": "command here where we are specifying the table or loading it to we're saying we're going to load it directly from this s3 manifest file the data that's in",
    "start": "2928420",
    "end": "2936520"
  },
  {
    "text": "there is json encoded so i've specified that that it is json encoded and that",
    "start": "2936520",
    "end": "2941620"
  },
  {
    "text": "we're gonna use a JSON path document to specify how to decode that these are the",
    "start": "2941620",
    "end": "2946690"
  },
  {
    "text": "credentials that we're using which will be which are no longer useful and then",
    "start": "2946690",
    "end": "2954490"
  },
  {
    "text": "the time format and then we're specifying that we're using a manifest and that it's been gzipped",
    "start": "2954490",
    "end": "2959710"
  },
  {
    "text": "and so what that all looks like without the commented sections below which you",
    "start": "2959710",
    "end": "2965200"
  },
  {
    "text": "would not put into your application is this base64 encoded string which is",
    "start": "2965200",
    "end": "2970960"
  },
  {
    "text": "rather unremarkable and then as far as executing these functions we're just",
    "start": "2970960",
    "end": "2976240"
  },
  {
    "text": "going through these and calling back and",
    "start": "2976240",
    "end": "2981390"
  },
  {
    "text": "so let's take a look at what the output looks like in redshift so I think that's where we",
    "start": "2981390",
    "end": "2988810"
  },
  {
    "text": "are no go back to log groups",
    "start": "2988810",
    "end": "2995250"
  },
  {
    "text": "go to redshift loader so you can see in",
    "start": "2997099",
    "end": "3006109"
  },
  {
    "text": "here that we've requested 64 files from sqs because we want a multiple of our of",
    "start": "3006109",
    "end": "3011719"
  },
  {
    "text": "the number of slices that we have on our cluster we actually received nine and we're going to go ahead and load them if",
    "start": "3011719",
    "end": "3017569"
  },
  {
    "text": "you wanted to optimize that you would probably just use only say eight if you if you have two or four slices and then",
    "start": "3017569",
    "end": "3024739"
  },
  {
    "text": "you can see the time that it took to create put the manifest 73 milliseconds and then on and on the rest of that is",
    "start": "3024739",
    "end": "3030349"
  },
  {
    "text": "fairly unimportant now once you have all of this set up you want to know how is",
    "start": "3030349",
    "end": "3036200"
  },
  {
    "text": "this pipeline doing how can I fine tune this is this working all right so we",
    "start": "3036200",
    "end": "3043609"
  },
  {
    "text": "will click away from this we'll go into here so this is another new thing that",
    "start": "3043609",
    "end": "3051079"
  },
  {
    "text": "was just released today and this is the dashboards for cloud watch this is something I'm personally pretty excited",
    "start": "3051079",
    "end": "3056509"
  },
  {
    "text": "about because it allows for you to create things like this so if we're looking at these if these charts right",
    "start": "3056509",
    "end": "3063319"
  },
  {
    "text": "here on this dashboard along the Left what we have are the number of invocations of our API gateway lambda",
    "start": "3063319",
    "end": "3069469"
  },
  {
    "text": "function so you can see we're running pretty close to 40,000 invocations per minute all of these are based on",
    "start": "3069469",
    "end": "3075440"
  },
  {
    "text": "permanent aggregates so we're taking in about 40,000 records a minute pretty consistently then the reader",
    "start": "3075440",
    "end": "3082400"
  },
  {
    "text": "function the Kinesis reader function you can see we're only executing about 180 why is that it's because we're",
    "start": "3082400",
    "end": "3088039"
  },
  {
    "text": "processing that in batches so you spend on AWS lambda for processing those as",
    "start": "3088039",
    "end": "3093410"
  },
  {
    "text": "practically no the Aurora loader similar and that you would expect as much",
    "start": "3093410",
    "end": "3098420"
  },
  {
    "text": "because it's feeding off the same data 180 executions per minute the s3 to sqs",
    "start": "3098420",
    "end": "3104930"
  },
  {
    "text": "invocations also similar it looks like there was a small spike to around 210 and then the redshift loader is a little",
    "start": "3104930",
    "end": "3112339"
  },
  {
    "text": "bit slower because it takes longer to load all those data all that data into redshift so we're looking at about 27 of",
    "start": "3112339",
    "end": "3119329"
  },
  {
    "text": "those and then along the right you can see the duration the average duration of each of the invocations now where this",
    "start": "3119329",
    "end": "3126349"
  },
  {
    "text": "is important is in a couple of ways so one thing is is you want how well is this performing you can see",
    "start": "3126349",
    "end": "3132560"
  },
  {
    "text": "along the left right up here you can see how this was at a hundred and forty milliseconds that was when I was playing",
    "start": "3132560",
    "end": "3138920"
  },
  {
    "text": "around a little bit with the memory allocation that I was giving to that lambda function I believe that was",
    "start": "3138920",
    "end": "3144170"
  },
  {
    "text": "running at a hundred and twenty eight megabytes and then I dialed it up to I think five hundred and twelve and so you",
    "start": "3144170",
    "end": "3149870"
  },
  {
    "text": "can see that the the duration dropped dramatically so it gives you even",
    "start": "3149870",
    "end": "3155660"
  },
  {
    "text": "tighter control of getting that in there the other thing that you want to consider is cost because lambda obviously you pay by the hundred",
    "start": "3155660",
    "end": "3162230"
  },
  {
    "text": "millisecond slice and so you want to try and find that balance between how much memory do I allocate and how many",
    "start": "3162230",
    "end": "3168290"
  },
  {
    "text": "milliseconds is it taking to run okay so",
    "start": "3168290",
    "end": "3176540"
  },
  {
    "text": "that's where we are with the demo we'll switch back so just to recap on the top",
    "start": "3176540",
    "end": "3185780"
  },
  {
    "text": "left is what Zillow has in place right now they have mobile clients writing the API gateway API gateway uses lambda to",
    "start": "3185780",
    "end": "3192740"
  },
  {
    "text": "write into a Kinesis stream lambda functions on there that are feeding into s3 and then they're using EMR and spark",
    "start": "3192740",
    "end": "3199730"
  },
  {
    "text": "to read from there we went over the Aurora one which is very similar except we're feeding data directly into Aurora",
    "start": "3199730",
    "end": "3205070"
  },
  {
    "text": "and then the third use case is also leveraging s3 sqs and the looping lambda function to load data into redshift",
    "start": "3205070",
    "end": "3212180"
  },
  {
    "text": "one other thing I forgot to mention as far as how real time this is this is my",
    "start": "3212180",
    "end": "3220010"
  },
  {
    "text": "Aurora instance right here and note that the time down here is 2:23 if you can't",
    "start": "3220010",
    "end": "3226700"
  },
  {
    "text": "see that and when I execute this to look at the the latest records oh it looks",
    "start": "3226700",
    "end": "3232490"
  },
  {
    "text": "like we just rolled over to 224 and the newest record in there was created at 224 so we are receiving these records",
    "start": "3232490",
    "end": "3241130"
  },
  {
    "text": "from the point of ingestion to the point they show up in Aurora in about a second",
    "start": "3241130",
    "end": "3248110"
  },
  {
    "text": "so very real time how about redshift redshifts the data warehouse right normally you know the traditional",
    "start": "3248110",
    "end": "3255470"
  },
  {
    "text": "thinking is is that you need to run run batch loads every hour every six hours",
    "start": "3255470",
    "end": "3260510"
  },
  {
    "text": "something like that well it's 2:24 still and the latest",
    "start": "3260510",
    "end": "3267220"
  },
  {
    "text": "record is to 24 and 32 seconds so we're still certainly certainly less than a",
    "start": "3267220",
    "end": "3273640"
  },
  {
    "text": "minute on that there are a couple of",
    "start": "3273640",
    "end": "3279460"
  },
  {
    "text": "other related sessions I encourage you guys to check out don't forget to fill",
    "start": "3279460",
    "end": "3285910"
  },
  {
    "text": "out your evaluations and then I'd like to take a few minutes here if anybody has any questions",
    "start": "3285910",
    "end": "3293490"
  }
]