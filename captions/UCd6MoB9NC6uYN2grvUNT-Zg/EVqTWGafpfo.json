[
  {
    "text": "- Hey, folks, Emily here. So in the first lecture in our series,",
    "start": "780",
    "end": "5819"
  },
  {
    "text": "you got a gentle primer\non foundation models. You learned what they are,\nwhy they're interesting,",
    "start": "5820",
    "end": "12540"
  },
  {
    "text": "and what you can do with them. In this lecture, we're gonna learn how to pick the right foundation model.",
    "start": "12540",
    "end": "18060"
  },
  {
    "text": "So we're gonna understand what it means to evaluate a foundation model and we'll learn some guiding criteria",
    "start": "18060",
    "end": "24119"
  },
  {
    "text": "that we can use to just\npick the right one, and then we'll close that\nwith a hands-on walkthrough",
    "start": "24120",
    "end": "29670"
  },
  {
    "text": "of running model evaluation\nbenchmarks on AWS and especially with\nSageMaker, let's get rolling.",
    "start": "29670",
    "end": "35030"
  },
  {
    "text": "All right, so today, again,\nwe're gonna cover the basics of foundation model selection.",
    "start": "38043",
    "end": "43773"
  },
  {
    "text": "I get this question a lot and it's actually pretty,",
    "start": "44640",
    "end": "50579"
  },
  {
    "text": "once you learn just a couple\nof the guiding criteria, you're gonna be able to pick\nthe right foundation model.",
    "start": "50580",
    "end": "57210"
  },
  {
    "text": "It may seem overwhelming because you might think oh, my gosh, there's this massive\nuniverse of foundation models",
    "start": "57210",
    "end": "64080"
  },
  {
    "text": "that are growing constantly\nand all these new flavors, and that's true, there are.",
    "start": "64080",
    "end": "69570"
  },
  {
    "text": "That will continue to be true. But there's sort of this core foundation",
    "start": "69570",
    "end": "75300"
  },
  {
    "text": "to picking the right foundation model. There's a core framework, mental framework that you can learn",
    "start": "75300",
    "end": "81360"
  },
  {
    "text": "to help you be discerning in picking the right foundation model. It's a little bit like picking\nfood at buffet if you will.",
    "start": "81360",
    "end": "87990"
  },
  {
    "text": "Once you know what you're hungry for and once you know what's healthy, it makes it a lot easier\nto pick food at a buffet.",
    "start": "87990",
    "end": "94380"
  },
  {
    "text": "And so in any case, some of\nthe considerations for you. Different modalities\nacross language and vision.",
    "start": "94380",
    "end": "101100"
  },
  {
    "text": "Different downstream tasks\nthat you're interested in. The size of those foundation models",
    "start": "101100",
    "end": "106200"
  },
  {
    "text": "which then impact the accuracy of those. How easy it is to use a\nfoundation model licensing,",
    "start": "106200",
    "end": "112560"
  },
  {
    "text": "previous examples, external\nbenchmarks, et cetera. We'll kind of have this edgy question",
    "start": "112560",
    "end": "117870"
  },
  {
    "text": "which is does it even matter\nto start with the right model? 'Cause arguably you just do\neverything in the prompting",
    "start": "117870",
    "end": "123810"
  },
  {
    "text": "or you do everything in\nthe fine tuning, so TBD.",
    "start": "123810",
    "end": "126573"
  },
  {
    "text": "So the basics of foundation model section. So really there are two,",
    "start": "129060",
    "end": "133862"
  },
  {
    "text": "there are four dimensions really. You've got the two edges of your x-axis here that I'm proposing",
    "start": "134850",
    "end": "140970"
  },
  {
    "text": "and then you've got two\nedges of your y-axis. So here in this horizontal line,",
    "start": "140970",
    "end": "146820"
  },
  {
    "text": "basically you have two modalities\nthat you're looking at. One is language and then\nthe other one is vision.",
    "start": "146820",
    "end": "154860"
  },
  {
    "text": "Now is the interplay between language",
    "start": "154860",
    "end": "159860"
  },
  {
    "text": "and vision really a line or is it more like a binary? It's unclear, you could say",
    "start": "160530",
    "end": "166980"
  },
  {
    "text": "that hey, it's just the one or the other. I'm just choosing the axis here because it's kind of cool. But so in any case,",
    "start": "166980",
    "end": "172709"
  },
  {
    "text": "some models are language only. So the GPT version of models, BERT models.",
    "start": "172710",
    "end": "179840"
  },
  {
    "text": "Many language models as you might imagine are strictly language only.",
    "start": "180108",
    "end": "185760"
  },
  {
    "text": "However vision has realized\nthat LLMs are awesome,",
    "start": "185760",
    "end": "190760"
  },
  {
    "text": "that language models are really demonstrating some\ninteresting performance. And so the last four years have seen",
    "start": "191070",
    "end": "197880"
  },
  {
    "text": "more and more vision models\nadopting some of the techniques and adopting some of the domains\nof these language models.",
    "start": "197880",
    "end": "205200"
  },
  {
    "text": "And so CLIP is a image-to-text model.",
    "start": "205200",
    "end": "210200"
  },
  {
    "text": "So CLIP is a model that you\ncan use to upload an image and then get the text out of this.",
    "start": "210390",
    "end": "216690"
  },
  {
    "text": "Originally CLIP was just\ndoing text classification, so using the captions of\nimages to then classify those.",
    "start": "216690",
    "end": "224220"
  },
  {
    "text": "There have been many open\nsource versions of CLIP that have then been pushed into a number",
    "start": "224220",
    "end": "229260"
  },
  {
    "text": "of different downstream tasks\nbeyond just classification, but larger captioning,\nvision question answering,",
    "start": "229260",
    "end": "238293"
  },
  {
    "text": "and some other applications. And then Stable Diffusion of course, is another foundation model",
    "start": "239520",
    "end": "245849"
  },
  {
    "text": "that is text to image. So Stable Diffusion of course,",
    "start": "245850",
    "end": "251220"
  },
  {
    "text": "lets you add a couple prompts, add some language as\nan input to your model, and then it will produce\nthis novel model for you.",
    "start": "251220",
    "end": "258867"
  },
  {
    "text": "And so when you're picking\na foundation model, at a bare minimum you\nwanna know what the inputs",
    "start": "258867",
    "end": "265080"
  },
  {
    "text": "and outputs of the models are. Does the model take language\nand produce language?",
    "start": "265080",
    "end": "270870"
  },
  {
    "text": "Does it take an image? Does it take text? Does it output text, et cetera.",
    "start": "270870",
    "end": "276030"
  },
  {
    "text": "And so once you know that\nbasis about the domains, then that obviously limits down your scope",
    "start": "276030",
    "end": "282150"
  },
  {
    "text": "to something a little bit more chewable. Then you have this second dimension here",
    "start": "282150",
    "end": "287220"
  },
  {
    "text": "which is generally small to large. So there is a huge variation",
    "start": "287220",
    "end": "293160"
  },
  {
    "text": "in the size of foundation models. Some foundation models are quite minute",
    "start": "293160",
    "end": "298380"
  },
  {
    "text": "like our trusty BERT model is just about 100 million parameters",
    "start": "298380",
    "end": "304710"
  },
  {
    "text": "which sounds like a big number, but it actually on disc is quite small and it can fit on your GPU,",
    "start": "304710",
    "end": "310379"
  },
  {
    "text": "on a single GPU or a single accelerator really almost without fail.",
    "start": "310380",
    "end": "315630"
  },
  {
    "text": "BERT is a very small model, especially BERT base is is quite tiny. However depending on the use\ncase that you're interested in,",
    "start": "315630",
    "end": "325430"
  },
  {
    "text": "BERT may be useful or may not be useful especially if you're looking at open-ended language generation.",
    "start": "325560",
    "end": "332370"
  },
  {
    "text": "Obviously you're not gonna go to BERT. You're gonna go to a GPT or some other type of language generator.",
    "start": "332370",
    "end": "339690"
  },
  {
    "text": "So as you move more and more into open-ended language generation, you're gonna want a larger model.",
    "start": "339690",
    "end": "346500"
  },
  {
    "text": "The model itself has more\nparameters in the neural network. The largest models have\nover 175 billion parameters.",
    "start": "346500",
    "end": "356280"
  },
  {
    "text": "The human brain has more\nthan 100 billion parameters. I forget exactly how many, but it's certainly more than 100 billion.",
    "start": "356280",
    "end": "362070"
  },
  {
    "text": "It's a lot, and so language\nmodels are inspired by this",
    "start": "362070",
    "end": "367070"
  },
  {
    "text": "and then again, are increasing in size. And so these are these two core dimensions",
    "start": "367920",
    "end": "375660"
  },
  {
    "text": "that you're caring about\nis the domain of the model and then the core size of the model.",
    "start": "375660",
    "end": "381480"
  },
  {
    "text": "So once you have those four, there are a couple other modalities or considerations that\nwe wanna grapple with.",
    "start": "381480",
    "end": "387780"
  },
  {
    "text": "And so to pick the right foundation model, again, the first tenet,",
    "start": "387780",
    "end": "392910"
  },
  {
    "text": "the first step to consider is just the modality of this model.",
    "start": "392910",
    "end": "397173"
  },
  {
    "text": "And again, as we learned, the modality of the\nmodel drives its output. So if I am a text-to-text model",
    "start": "398220",
    "end": "405660"
  },
  {
    "text": "or if I'm working with\na text-to-text model, then I'll give it an input which is say if I have two\nmimosas and one samosa,",
    "start": "405660",
    "end": "414030"
  },
  {
    "text": "how many vehicles do I have to sort of again, throw\nat this red herring and see if it understands?",
    "start": "414030",
    "end": "419340"
  },
  {
    "text": "And so a good LLM will tell\nme that having two samosas and one mimosa actually doesn't\nimply ownership of vehicles.",
    "start": "419340",
    "end": "429200"
  },
  {
    "text": "It does however if it's a\nvehicle for entertainment which I had some fun with and so in any case, that's your good LLM.",
    "start": "429210",
    "end": "436919"
  },
  {
    "text": "And then for my text to image, right? I'm gonna give it an input",
    "start": "436920",
    "end": "441930"
  },
  {
    "text": "and maybe my input is a samosa sitting next to a mimosa on a table.",
    "start": "441930",
    "end": "447210"
  },
  {
    "text": "And then I'll give it my negative prompts which is say some curry for fun and then some flowers.",
    "start": "447210",
    "end": "454410"
  },
  {
    "text": "And so this is what I got. So DeepFloyd which is\nanother foundation model",
    "start": "454410",
    "end": "460003"
  },
  {
    "text": "I was working with from Hugging Face and essentially so DeepFloyd's rendition",
    "start": "460003",
    "end": "465720"
  },
  {
    "text": "of samosas with a mimosa and then I added upscaling. And so here are these delicious samosas.",
    "start": "465720",
    "end": "473250"
  },
  {
    "text": "They look pretty good, and then here's this\nrendition of a mimosa.",
    "start": "473250",
    "end": "478937"
  },
  {
    "text": "I can tell you this doesn't look like any mimosa that I've had and so I was curious about this.",
    "start": "480360",
    "end": "487530"
  },
  {
    "text": "And for me, I was able\nto get better performance when I separated them out. With this particular model, DeepFloyd,",
    "start": "487530",
    "end": "495153"
  },
  {
    "text": "it didn't seem to understand\nsort of novel concepts and putting them together. It was much happier when\nyou took novel concepts",
    "start": "495990",
    "end": "503940"
  },
  {
    "text": "and kept them apart. So it's not common for mimosas\nto appear with samosas.",
    "start": "503940",
    "end": "510470"
  },
  {
    "text": "It's common for them to appear separately. And so when I put them\nin two different prompts,",
    "start": "510810",
    "end": "516300"
  },
  {
    "text": "it performed much better, and so now here is our samosas\nsitting next to a mango lassi",
    "start": "516300",
    "end": "523039"
  },
  {
    "text": "which so this is still kind of getting the mango lassi, right? Because we have this,",
    "start": "523080",
    "end": "528690"
  },
  {
    "text": "certainly this looks like a normal lassi and then my mango flavor is down here. Obviously you would prefer them together. So there's some room for improvement.",
    "start": "528690",
    "end": "535740"
  },
  {
    "text": "And so in any case, when we're picking the\nright foundation model, the first thing we'll do\nis start with the modality",
    "start": "535740",
    "end": "541830"
  },
  {
    "text": "that we're interested in and generally that's language, or vision, or some unknown wild card, right?",
    "start": "541830",
    "end": "549000"
  },
  {
    "text": "You can use foundation models with really any digital modality and my only word of caution here",
    "start": "549000",
    "end": "555630"
  },
  {
    "text": "is that the quality of that model and the ease of execution of that model",
    "start": "555630",
    "end": "560790"
  },
  {
    "text": "is really gonna vary as a function of how widely adopted that model is. So language models and vision\nmodels are so good today",
    "start": "560790",
    "end": "570390"
  },
  {
    "text": "because of years of really, really hard and dedicated R&D to make them awesome.",
    "start": "570390",
    "end": "576810"
  },
  {
    "text": "And so for any other arbitrary modalities, definitely you can try it but it might be hard unless",
    "start": "576810",
    "end": "584070"
  },
  {
    "text": "and until it's been a little\nbit more widely adopted. And so as again, we learned previously,",
    "start": "584070",
    "end": "590220"
  },
  {
    "text": "you can recast your ML task as generative. So you can take your prompt,",
    "start": "590220",
    "end": "595590"
  },
  {
    "text": "send your prompt to the model, and in traditional classification\nit'll just add a label.",
    "start": "595590",
    "end": "600750"
  },
  {
    "text": "In this new world of generative AI, we're actually recasting it as generative.",
    "start": "600750",
    "end": "607230"
  },
  {
    "text": "And so this is a decision point for you in picking the right foundation model",
    "start": "607230",
    "end": "612420"
  },
  {
    "text": "is how deep down the\ngenerative AI rabbit hole do you want to go?",
    "start": "612420",
    "end": "617700"
  },
  {
    "text": "And so in particular, you certainly can try\nany one of these tasks",
    "start": "617700",
    "end": "624330"
  },
  {
    "text": "as using a generative model, using an LLM. You can try doing classification\nwith a generative model.",
    "start": "624330",
    "end": "631620"
  },
  {
    "text": "You can try forecasting\nwith a generative model. You can try recommendation. You can try anomaly detection.",
    "start": "631620",
    "end": "638190"
  },
  {
    "text": "Certainly translation, style transfer, visual search arguably,",
    "start": "638190",
    "end": "643320"
  },
  {
    "text": "and so it's interesting to explore these because you're using\nthat foundation model.",
    "start": "643320",
    "end": "649050"
  },
  {
    "text": "So you're still using that\npre-trained base artifact that has multiple terabytes\nof images and text,",
    "start": "649050",
    "end": "657150"
  },
  {
    "text": "possibly with lots of neural networks and trained on 1,000 plus accelerators.",
    "start": "657150",
    "end": "662967"
  },
  {
    "text": "And so it's this really powerful thing and so theoretically you should be able to get some type of boost",
    "start": "662967",
    "end": "669330"
  },
  {
    "text": "when you plug this foundation model into your other downstream task. However you don't always need to",
    "start": "669330",
    "end": "676980"
  },
  {
    "text": "and what's interesting is that if you are sort of less on open-ended generation",
    "start": "676980",
    "end": "684450"
  },
  {
    "text": "and if you're more interested\nin doing just summarization or doing just style transfer,\ndoing just classification,",
    "start": "684450",
    "end": "691470"
  },
  {
    "text": "you can get away with a\nsmaller model actually. And so there's a benefit in\ntrying to use a smaller model",
    "start": "691470",
    "end": "698430"
  },
  {
    "text": "as I'm saying here because when you use a smaller model, obviously your costs go down",
    "start": "698430",
    "end": "704160"
  },
  {
    "text": "and your model inferencing\nruntime is also going to go down. Some benefits from\nusing a foundation model",
    "start": "704160",
    "end": "712050"
  },
  {
    "text": "again, is just streamlining operations. When you have all of your\npipelines, all of your datasets,",
    "start": "712050",
    "end": "718410"
  },
  {
    "text": "all of your team resources and human brainpower focused on making",
    "start": "718410",
    "end": "723779"
  },
  {
    "text": "this one foundation model spectacular, in many cases, it's just more efficient.",
    "start": "723780",
    "end": "729540"
  },
  {
    "text": "It's just easier to put everything into one really big project which you then send",
    "start": "729540",
    "end": "735210"
  },
  {
    "text": "into many different\ndownstream applications versus having to build, and maintain,",
    "start": "735210",
    "end": "740730"
  },
  {
    "text": "and update 40, 50, 100 different pipelines for all of these different models",
    "start": "740730",
    "end": "746640"
  },
  {
    "text": "which I know some of you do. I know it's really frustrating. And so when you run\neverything again in one model,",
    "start": "746640",
    "end": "754410"
  },
  {
    "text": "it's just very efficient. It's just very streamlined.",
    "start": "754410",
    "end": "758133"
  },
  {
    "text": "And so again, when you're\nusing this foundation model, accuracy may actually\njust jump up as a boost.",
    "start": "759480",
    "end": "766920"
  },
  {
    "text": "When you just use this modern pre-trained\nbackend for your application, you may just see an an\nuptick in performance.",
    "start": "766920",
    "end": "775649"
  },
  {
    "text": "And it's fun because when\nyou're using classification, you can still use all of these\nnormal evaluation metrics.",
    "start": "775650",
    "end": "782760"
  },
  {
    "text": "So you can use precision,\nyou can use recall, you can use F1, et cetera.",
    "start": "782760",
    "end": "788880"
  },
  {
    "text": "It's really fun to just run sort of the normal classification stack",
    "start": "788880",
    "end": "796050"
  },
  {
    "text": "of eval just using\nthese generative models.",
    "start": "796050",
    "end": "800133"
  },
  {
    "text": "And so that's a big decision point, right? Is picking the right foundation model is how generative do you\nneed your model to be?",
    "start": "801570",
    "end": "810750"
  },
  {
    "text": "Do you want it to be\nwriting multiple pages of really interesting text?",
    "start": "810750",
    "end": "817110"
  },
  {
    "text": "Or do you want it to be just generating a sentence,\ngenerating a paragraph?",
    "start": "817110",
    "end": "823259"
  },
  {
    "text": "Doing summarization, doing extraction? There are a lot of different\ntypes of NLP use cases",
    "start": "823260",
    "end": "829920"
  },
  {
    "text": "and so if you're not interested in generic open-ended generation",
    "start": "829920",
    "end": "835150"
  },
  {
    "text": "but just using some of the\nboosts of your foundation model to solve a task, still use a decoder model,",
    "start": "836040",
    "end": "842277"
  },
  {
    "text": "but you can get away with a much smaller, and therefore less expensive, and therefore faster language model",
    "start": "842277",
    "end": "850740"
  },
  {
    "text": "versus if you really want something that's gonna help you write a novel or that will help you write a screenplay,",
    "start": "850740",
    "end": "858029"
  },
  {
    "text": "then you probably want to\nuse a larger language model because the quality of that\ntax is just gonna be higher.",
    "start": "858030",
    "end": "864430"
  },
  {
    "text": "All right, and so we come to\nthis other discussion point which is the impact of\nsize on foundation models",
    "start": "865650",
    "end": "872730"
  },
  {
    "text": "and so two dimensions here\nthat we're interested in. One is the number of accelerators",
    "start": "872730",
    "end": "878610"
  },
  {
    "text": "or the number of GPUs. And so basically again, foundation models take physical space",
    "start": "878610",
    "end": "886350"
  },
  {
    "text": "and because they take physical space, your accelerators actually, these things,",
    "start": "886350",
    "end": "892440"
  },
  {
    "text": "these things, these\nthings have a physical, they're bound, right? There's a card that's sitting in here",
    "start": "892440",
    "end": "899760"
  },
  {
    "text": "that can literally only\nhold so many bytes. Same with here, you've got two of them and in this card, right?",
    "start": "899760",
    "end": "906330"
  },
  {
    "text": "Maybe I can fit a model with 3GB, right? So I've got 3GB and 3GB,",
    "start": "906330",
    "end": "913110"
  },
  {
    "text": "so maybe I have two models\nthat's sitting on this thing. One model here, one model there,",
    "start": "913110",
    "end": "918780"
  },
  {
    "text": "and that's a smaller model. So that's one model that's\nparked on my single GPU.",
    "start": "918780",
    "end": "923790"
  },
  {
    "text": "Alternatively for a larger model, I might shard it across these two GPUs.",
    "start": "923790",
    "end": "930390"
  },
  {
    "text": "So I might have a model\nthat's maybe 10GB on disc",
    "start": "930390",
    "end": "935390"
  },
  {
    "text": "and maybe six billion parameters, and then I'm gonna run that on two GPUs.",
    "start": "937260",
    "end": "943920"
  },
  {
    "text": "My larger model on average is going to be more accurate. Very, very well evidenced in the field",
    "start": "943920",
    "end": "952650"
  },
  {
    "text": "and in the industry that larger models on average tend to be more accurate,",
    "start": "952650",
    "end": "957720"
  },
  {
    "text": "but they also tend to be more expensive. So you see this core trade-off in the size of a foundation model.",
    "start": "957720",
    "end": "963450"
  },
  {
    "text": "That as the number of accelerators, so the number of GPUs goes up,",
    "start": "963450",
    "end": "968970"
  },
  {
    "text": "accuracy goes up, right? So you're gonna see a more accurate model",
    "start": "968970",
    "end": "974019"
  },
  {
    "text": "that when that model generally\nneeds more accelerators, you're also gonna see that cost go up",
    "start": "974940",
    "end": "981360"
  },
  {
    "text": "because you literally need to provision more GPUs, more accelerators, and the cost for all of\nthat is gonna increase.",
    "start": "981360",
    "end": "988470"
  },
  {
    "text": "You're also gonna see the\ninference runtime increase. So again, because you\nliterally have to communicate",
    "start": "988470",
    "end": "994500"
  },
  {
    "text": "with multiple GPUs, multiple accelerators, the inference runtime of that\nis just gonna go up there.",
    "start": "994500",
    "end": "1001519"
  },
  {
    "text": "There's just no, there's no way around it. Communication takes time,\nand humans are people.",
    "start": "1001520",
    "end": "1006560"
  },
  {
    "text": "Communication takes time, and so yeah, so the inference cost is gonna go up and then as a result of that,",
    "start": "1006560",
    "end": "1013130"
  },
  {
    "text": "you have fewer deployment options. So you then become limited\nto only environments",
    "start": "1013130",
    "end": "1019280"
  },
  {
    "text": "where you can provision and maintain all of those GPUs. Most commonly obviously on the cloud.",
    "start": "1019280",
    "end": "1025640"
  },
  {
    "text": "Some people will try and do this on-prem, but most commonly on the cloud. And then on the flip side",
    "start": "1025640",
    "end": "1031790"
  },
  {
    "text": "when you're looking at smaller models, you get more deployment options, right?",
    "start": "1031790",
    "end": "1037279"
  },
  {
    "text": "Because your model, when it\nfits only on a single GPU, then you can actually plug it into devices",
    "start": "1037280",
    "end": "1044750"
  },
  {
    "text": "that already have a GPU. So I can run it on my laptop. Maybe I can run some super\nsmall ones on my phone.",
    "start": "1044750",
    "end": "1052252"
  },
  {
    "text": "There are other disconnected edge devices where I can take my model and park it.",
    "start": "1053150",
    "end": "1058157"
  },
  {
    "text": "And so when you have a smaller model, obviously you have more deployment options which makes it less\nexpensive to run your model",
    "start": "1058157",
    "end": "1066620"
  },
  {
    "text": "because you're physically paying for fewer devices to host\nit so it's less expensive.",
    "start": "1066620",
    "end": "1073370"
  },
  {
    "text": "The inference runtime is faster because it's communicating\nwith fewer devices. The layers in the network\nare literally fewer",
    "start": "1073370",
    "end": "1081080"
  },
  {
    "text": "so it takes literally less time to just run a forward\npass through the thing.",
    "start": "1081080",
    "end": "1086750"
  },
  {
    "text": "And so yeah, so it's\nless expensive, faster, and you have more options, but your accuracy may be lower.",
    "start": "1086750",
    "end": "1093620"
  },
  {
    "text": "So there's this core trade-off here that you need to grapple with. There's no single answer for this today.",
    "start": "1093620",
    "end": "1100463"
  },
  {
    "text": "You'll need to look at this, consider in your business and your teams what makes sense for you,",
    "start": "1101720",
    "end": "1107300"
  },
  {
    "text": "and then come to some sort of core set of model size, accuracy,\nperformance, cost, runtime,",
    "start": "1107300",
    "end": "1115400"
  },
  {
    "text": "and deployment options that you are all happy with and onboard with. And so two ways to think about this.",
    "start": "1115400",
    "end": "1122300"
  },
  {
    "text": "So the impact on size\nand foundation models is a fascinating debate.",
    "start": "1122300",
    "end": "1127850"
  },
  {
    "text": "I love to just look at\nboth sides of this debate, so let's poke at it. 86 billion, yeah, yeah, yeah,",
    "start": "1127850",
    "end": "1133760"
  },
  {
    "text": "so the case for large foundation models. So why should we care",
    "start": "1133760",
    "end": "1139100"
  },
  {
    "text": "about increasing large foundation models? And theoretically why is this\nsort of more interesting?",
    "start": "1139100",
    "end": "1145340"
  },
  {
    "text": "So the human brain itself\nhas 86 billion neurons and so theoretically if we're trying",
    "start": "1145340",
    "end": "1151669"
  },
  {
    "text": "to replicate human level performance, then we should see there's some,",
    "start": "1151670",
    "end": "1158210"
  },
  {
    "text": "86 should be the number, right? That should be the target. Unless we're trying to replicate",
    "start": "1158210",
    "end": "1164690"
  },
  {
    "text": "the performance of multiple humans, in which case theoretically\nmaybe the numbers should go up, but that's just one data point.",
    "start": "1164690",
    "end": "1170480"
  },
  {
    "text": "There are so many foundation models that are just breathtakingly large, right? That are just well north\nof 100 billion parameters.",
    "start": "1170480",
    "end": "1179029"
  },
  {
    "text": "Some cases hitting multiple hundreds of\nbillions of parameters. Some of them are actually\nin the trillion phase",
    "start": "1179030",
    "end": "1186710"
  },
  {
    "text": "and so there's this wish, theoretical hope that oh, well,\nmaybe if we just scale it,",
    "start": "1186710",
    "end": "1193250"
  },
  {
    "text": "maybe we get better performance. Also logically you would\nthink that a larger dataset",
    "start": "1193250",
    "end": "1199490"
  },
  {
    "text": "should just have more information. As long as I'm not just\ncopying the same thing,",
    "start": "1199490",
    "end": "1205340"
  },
  {
    "text": "logically if I have multiple terabytes of rich, diverse, non-duplicated records,",
    "start": "1205340",
    "end": "1214450"
  },
  {
    "text": "then there should be more\ninformation in my dataset. So just logically a larger\ndataset should be better.",
    "start": "1214580",
    "end": "1221242"
  },
  {
    "text": "And then as I have a larger dataset, theoretically I should\nalso have a larger model.",
    "start": "1222560",
    "end": "1228980"
  },
  {
    "text": "So in order to not overfit this thing and to actually capture\nall of the information that's in my dataset,",
    "start": "1228980",
    "end": "1234920"
  },
  {
    "text": "logically I should also\njust have a larger model. And there's also this\ntrend that larger projects",
    "start": "1234920",
    "end": "1241310"
  },
  {
    "text": "are just sort of\ninherently more inspiring. There's quite a few of us\ntechies who are just more excited",
    "start": "1241310",
    "end": "1247820"
  },
  {
    "text": "when you're running a\nmassive distributed system and we just find that\ninherently more interesting.",
    "start": "1247820",
    "end": "1254000"
  },
  {
    "text": "But you'll notice that\nthat's not the top point, but it is a point. And then the case against\nlarge foundation models.",
    "start": "1254000",
    "end": "1261533"
  },
  {
    "text": "So Stable Diffusion came in and was really fascinating",
    "start": "1262670",
    "end": "1267830"
  },
  {
    "text": "because it was both really\nhigh performance, right? The quality of Stable Diffusion was great,",
    "start": "1267830",
    "end": "1274372"
  },
  {
    "text": "and it was also single accelerator. So Stable Diffusion came out with a model",
    "start": "1275270",
    "end": "1280730"
  },
  {
    "text": "that was again, amazing and runs extremely well\non a single accelerator",
    "start": "1280730",
    "end": "1286760"
  },
  {
    "text": "and a single GPU. And so that was sort of a shock, right? 'Cause in LLMs we'd been\nmoving for a while, for years.",
    "start": "1286760",
    "end": "1295067"
  },
  {
    "text": "Most of us thought oh, we'll\njust get larger and larger and there were some trends\nto decrease the size,",
    "start": "1296450",
    "end": "1303290"
  },
  {
    "text": "but none of them were hitting\nstate-of-the-art performance. And so Stable Diffusion was a big shock,",
    "start": "1303290",
    "end": "1309982"
  },
  {
    "text": "so that was a really exciting moment. And then actually there were\nsome updated scaling laws",
    "start": "1311090",
    "end": "1317190"
  },
  {
    "text": "in 2022 that suggested\nactually increasing datasets",
    "start": "1318050",
    "end": "1323050"
  },
  {
    "text": "much larger than previously which meant we could use\nsomewhat of a smaller model.",
    "start": "1324860",
    "end": "1330919"
  },
  {
    "text": "So instead of maybe needing 175 billion, maybe we could get away with\nsomething that was smaller",
    "start": "1330920",
    "end": "1337520"
  },
  {
    "text": "but just trained on more data. And then there have been other models",
    "start": "1337520",
    "end": "1342890"
  },
  {
    "text": "that outperformed GPT-3 in many cases. So both the AlexaTM was a 20\nbillion model, parameter model",
    "start": "1342890",
    "end": "1351860"
  },
  {
    "text": "available on SageMaker\nthrough foundation models. Same with Stable Diffusion. But in any case,",
    "start": "1351860",
    "end": "1357193"
  },
  {
    "text": "the Alexa Teacher Model\nis better than GPT-3 in some use cases, right? So in some slice of the universe,",
    "start": "1357193",
    "end": "1364490"
  },
  {
    "text": "the Alexa Teacher Model\nwas actually outperforming GPT-3 on some downstream tasks.",
    "start": "1364490",
    "end": "1371510"
  },
  {
    "text": "And then InstructGPT\ncame out and was shocking because InstructGPT was\n1.3 billion parameters.",
    "start": "1371510",
    "end": "1380720"
  },
  {
    "text": "So this tiny model was\nthen outperforming GPT-3 on their win rates",
    "start": "1380720",
    "end": "1386780"
  },
  {
    "text": "with literally 1% of the parameter counts. So maybe you really can get away with a tiny fraction\nof the parameter size.",
    "start": "1386780",
    "end": "1395060"
  },
  {
    "text": "And then in vision, certainly CNNs are still really strong.",
    "start": "1395060",
    "end": "1400490"
  },
  {
    "text": "There's a lot of performance to be had just through convolutional\nneural networks,",
    "start": "1400490",
    "end": "1407300"
  },
  {
    "text": "so CNNs are just great for vision. That's because they capture\nthe visual structure.",
    "start": "1407300",
    "end": "1412760"
  },
  {
    "text": "So you've got your picture and then your CNN starts in the top left,",
    "start": "1412760",
    "end": "1418280"
  },
  {
    "text": "and it sort of learns the\npixels in that top left and then it moves across, and it convolves across the image, right?",
    "start": "1418280",
    "end": "1425050"
  },
  {
    "text": "So it goes left to right, top to bottom, and it learns the structural\nrelationship of the pixels",
    "start": "1425050",
    "end": "1432740"
  },
  {
    "text": "and of the objects that the\npixels describe which is great,",
    "start": "1432740",
    "end": "1437740"
  },
  {
    "text": "and transformers don't necessarily\ndo that as well, so CNNs, and then encoders are still still great.",
    "start": "1438680",
    "end": "1447560"
  },
  {
    "text": "Obviously having a lower cost makes projects more accessible,",
    "start": "1447560",
    "end": "1452570"
  },
  {
    "text": "makes development more accessible, and then the lower carbon emission is always strictly preferred.",
    "start": "1452570",
    "end": "1458270"
  },
  {
    "text": "And so this debates will continue to wage.",
    "start": "1458270",
    "end": "1463270"
  },
  {
    "text": "I wholeheartedly expect\nthe next couple years to continue to see edges in this",
    "start": "1464120",
    "end": "1472670"
  },
  {
    "text": "and I would say this space is your creative space to innovate.",
    "start": "1472670",
    "end": "1478309"
  },
  {
    "text": "So as you are looking\nat interesting projects, interesting ways to\ndevelop novel capabilities,",
    "start": "1478310",
    "end": "1486610"
  },
  {
    "text": "think about this dialogue and think about sort of being\ninspired from both of these, and then try something new",
    "start": "1488510",
    "end": "1494720"
  },
  {
    "text": "and show us all what's up. All right, and so to pick\nthe right foundation model,",
    "start": "1494720",
    "end": "1501470"
  },
  {
    "text": "we learned about modalities. We learned about language\nvision and this wild card.",
    "start": "1501470",
    "end": "1507170"
  },
  {
    "text": "We learned about thinking about the model as being generative and sort of how deep down the generative,",
    "start": "1507170",
    "end": "1513920"
  },
  {
    "text": "the open-ended language generation, how deep down that rabbit\nhole we want to go.",
    "start": "1513920",
    "end": "1518663"
  },
  {
    "text": "And that should be if you actually use decoder-based autoregressive models. And then these are sort of the categories",
    "start": "1519590",
    "end": "1526940"
  },
  {
    "text": "that I tend to see for parameter counts. So you have some models",
    "start": "1526940",
    "end": "1532250"
  },
  {
    "text": "that are solidly in the\none billion parameters like your GPT-2 in some\nof the different flavors.",
    "start": "1532250",
    "end": "1538970"
  },
  {
    "text": "And so you can get some performance with the one billion parameter model.",
    "start": "1538970",
    "end": "1544040"
  },
  {
    "text": "I'm not gonna write a novel with it, but I might do some\nclassification with that thing. Your seven billion parameters\nis gonna do a little bit more.",
    "start": "1544040",
    "end": "1551840"
  },
  {
    "text": "You might need your two\naccelerators for your seven billion and it's gonna do a little more.",
    "start": "1551840",
    "end": "1557240"
  },
  {
    "text": "So maybe I'm not just classifying. Maybe I am generating a headline or generating a summary.",
    "start": "1557240",
    "end": "1563900"
  },
  {
    "text": "Your 11 billion parameter\ncan answer some questions. It can answer questions\nthat are quite obvious,",
    "start": "1563900",
    "end": "1572000"
  },
  {
    "text": "that are well understood,\nthat are non-ambiguous. Your 20 million parameter model, especially if you do some\ninstruction fine tuning,",
    "start": "1572000",
    "end": "1579710"
  },
  {
    "text": "you do some RLHF on that thing, you get some really high\nquality alignment data.",
    "start": "1579710",
    "end": "1586520"
  },
  {
    "text": "You can have a conversation with a 20 billion parameter model. It should understand questions",
    "start": "1586520",
    "end": "1592669"
  },
  {
    "text": "and then you'll chain\nthe questions, right? So you can use LangChain",
    "start": "1592670",
    "end": "1597890"
  },
  {
    "text": "to just take the prompts and the answers, chain these things together and then have a rich\ndialogue with your model.",
    "start": "1597890",
    "end": "1605630"
  },
  {
    "text": "So 20 million is a nice spot because you still need maybe four GPUs,",
    "start": "1605630",
    "end": "1613900"
  },
  {
    "text": "maybe eight GPUs or accelerators which is still a single\nbox, a single instance.",
    "start": "1614540",
    "end": "1620630"
  },
  {
    "text": "So you can run maybe one P4d, one P4d instance on AWS,",
    "start": "1620630",
    "end": "1626363"
  },
  {
    "text": "and park that in a 20\nbillion parameter model. Now that would be tight.",
    "start": "1627860",
    "end": "1632870"
  },
  {
    "text": "I would probably try and\nget a couple instances and then maybe will it down from there,",
    "start": "1632870",
    "end": "1639080"
  },
  {
    "text": "but at least you're in a feasible range. 50 billion is a lot.",
    "start": "1639080",
    "end": "1646100"
  },
  {
    "text": "That's a large model. You're gonna need many\nmore instances for that. I would probably expect\nyou to be using eight,",
    "start": "1646100",
    "end": "1655550"
  },
  {
    "text": "maybe 16 P4d instances, P4des,",
    "start": "1655550",
    "end": "1659843"
  },
  {
    "text": "and you're gonna be\nrunning some compilation to hit that with Trainium so,",
    "start": "1660710",
    "end": "1665840"
  },
  {
    "text": "but it's gonna be very good. Particularly when you train\na 50 billion parameter model",
    "start": "1665840",
    "end": "1671810"
  },
  {
    "text": "on an excellent dataset, you will have a very\ngood model at the end.",
    "start": "1671810",
    "end": "1677390"
  },
  {
    "text": "It won't just be able to\nhave a conversation with you. It can probably pass tests.",
    "start": "1677390",
    "end": "1684260"
  },
  {
    "text": "You can fine tune it and then again design it to\nhandle very complex tasks.",
    "start": "1684260",
    "end": "1689570"
  },
  {
    "text": "And then this last category is sort of north of 100 billion parameters and this is like I'm passing\nthe SATs, I'm writing novels.",
    "start": "1689570",
    "end": "1698860"
  },
  {
    "text": "I am translating these\nvery complex languages.",
    "start": "1700100",
    "end": "1705100"
  },
  {
    "text": "I'm doing very complex tasks, and so what I want you to see from this",
    "start": "1705440",
    "end": "1710480"
  },
  {
    "text": "is that you can pick a model based on the task that\nyou're interested in and based on the complexity of that task.",
    "start": "1710480",
    "end": "1718820"
  },
  {
    "text": "So for the task that you have, if it's gonna take human beings an hour to think about this,",
    "start": "1718820",
    "end": "1726260"
  },
  {
    "text": "translate it, write a good response, really grapple with the\nnuances of the question,",
    "start": "1726260",
    "end": "1732140"
  },
  {
    "text": "100 billion parameters. You need a really big, really good model. But if your model is just answering",
    "start": "1732140",
    "end": "1740480"
  },
  {
    "text": "a really simple question, or extracting a phrase, or doing a really obvious translation,",
    "start": "1740480",
    "end": "1748370"
  },
  {
    "text": "then you can pick a model that's smaller which is gonna be less expensive, easier,",
    "start": "1748370",
    "end": "1753860"
  },
  {
    "text": "and faster at inference runtime. So those are some of your spectrums in foundation model size.",
    "start": "1753860",
    "end": "1760283"
  },
  {
    "text": "And then again, the impact of accuracy on foundation models is always a thing,",
    "start": "1761210",
    "end": "1767120"
  },
  {
    "text": "so we care about how\ngood these models are. You want a model to be good, but it is not obvious to\ndetermine what good means.",
    "start": "1767120",
    "end": "1776770"
  },
  {
    "text": "Good means different\nthings to different people. I might say a good driver\nis someone who's safe",
    "start": "1776810",
    "end": "1783259"
  },
  {
    "text": "and you might say a good driver is someone who's clocking 1 wins, so good means a lot of different things",
    "start": "1783260",
    "end": "1792110"
  },
  {
    "text": "and so we wanna quantify that. And so in foundation models\nand machine learning, so accuracy is a common metric",
    "start": "1792110",
    "end": "1798590"
  },
  {
    "text": "where you'll try to see oh,\nhow accurate is this model? Does it answer this correctly?",
    "start": "1798590",
    "end": "1805279"
  },
  {
    "text": "Accuracy is actually a pretty\nmisleading term however, so it's more common to use precision,",
    "start": "1805280",
    "end": "1811700"
  },
  {
    "text": "to use recall, to use F1, and that's because accuracy",
    "start": "1811700",
    "end": "1816740"
  },
  {
    "text": "is dependent on how many samples you have. So if you're running the\ntest out of 10 samples",
    "start": "1816740",
    "end": "1823580"
  },
  {
    "text": "and you get three right out of 10, then your accuracy is only 30%",
    "start": "1823580",
    "end": "1828830"
  },
  {
    "text": "which sounds really bad. But maybe it picked the\nmost important three,",
    "start": "1828830",
    "end": "1835010"
  },
  {
    "text": "so your precision was really, really good and that matters more in your use case. So all I'm saying here is think about it.",
    "start": "1835010",
    "end": "1841102"
  },
  {
    "text": "Dive deep into it and try\nand pick the right metric. You can use labeled data to\nevaluate your foundation models.",
    "start": "1841940",
    "end": "1851620"
  },
  {
    "text": "So we talked about\nsummarizing call transcripts or generating content, updating content.",
    "start": "1852020",
    "end": "1861150"
  },
  {
    "text": "All of these you can\nascend to human beings to produce some label data for you",
    "start": "1862280",
    "end": "1869059"
  },
  {
    "text": "and it doesn't have to be massive. You can construct a label\ndata dataset of 30 samples,",
    "start": "1869060",
    "end": "1876080"
  },
  {
    "text": "50 samples, 100 samples. Maybe that takes you an hour. Maybe it takes you a couple hours",
    "start": "1876080",
    "end": "1882530"
  },
  {
    "text": "with two of your team members. So you can create, what I'm saying is you\ncan create a dataset",
    "start": "1882530",
    "end": "1889429"
  },
  {
    "text": "that is already labeled and then you can use that labeled dataset",
    "start": "1889430",
    "end": "1895039"
  },
  {
    "text": "to evaluate your language model. So say you take your call\ntranscripts from your call center",
    "start": "1895040",
    "end": "1901610"
  },
  {
    "text": "and your customers are calling and they're like, \"Oh,\nhey, I loved this piece, but I didn't like that piece",
    "start": "1901610",
    "end": "1907580"
  },
  {
    "text": "and there's something\nwrong with my payment. What do I do?\" And then your call agent\nis talking to the customer",
    "start": "1907580",
    "end": "1913760"
  },
  {
    "text": "and they're trying to\nget them back on track. And then after the call completes,",
    "start": "1913760",
    "end": "1919520"
  },
  {
    "text": "your team members go\nphysically write a summary. They're like yes, this is what happened,",
    "start": "1919520",
    "end": "1925700"
  },
  {
    "text": "this is what we saw, and you get a five sentence\nsummary of the call. The next thing you can do",
    "start": "1925700",
    "end": "1932150"
  },
  {
    "text": "is run a metric, it's called rouge, and the rouge metric is\na way to basically see",
    "start": "1932150",
    "end": "1939860"
  },
  {
    "text": "how accurate the generated text is. So you take the text, send it to your LLM,",
    "start": "1939860",
    "end": "1945170"
  },
  {
    "text": "get a response back, and see how like the ground truth the response from that LLM is.",
    "start": "1945170",
    "end": "1952340"
  },
  {
    "text": "Again, you can construct a labeled dataset and then compute metrics\nto actually quantify",
    "start": "1952340",
    "end": "1960980"
  },
  {
    "text": "how good your model is, how accurate your model is. Again, human feedback always wins.",
    "start": "1960980",
    "end": "1967430"
  },
  {
    "text": "At the end of the day when you have more\nresponses from human beings, when you aggregate the\nresponses from human beings",
    "start": "1967430",
    "end": "1974930"
  },
  {
    "text": "using reward modeling plus some RLHF, that's always gonna\ngive you a better model at the end of the day.",
    "start": "1974930",
    "end": "1980870"
  },
  {
    "text": "And in terms of picking the right foundation model as a base, again, I'm assuming you're\ngonna evaluate this.",
    "start": "1980870",
    "end": "1987260"
  },
  {
    "text": "You're gonna pick the\nmetrics that you care about. You're gonna construct a\ndataset that you care about.",
    "start": "1987260",
    "end": "1993140"
  },
  {
    "text": "You're gonna put the model\nin front of your users, your stakeholders to see what\nthey think, what they like.",
    "start": "1993140",
    "end": "1999500"
  },
  {
    "text": "And then when you pick\na higher quality model, it always means less work for you.",
    "start": "1999500",
    "end": "2004929"
  },
  {
    "text": "So when you pick a higher quality model, it means you don't have to worry about the prompting as much.",
    "start": "2004930",
    "end": "2010870"
  },
  {
    "text": "You don't have to worry about the prompt engineering\nas much, the prompt templates. You're fine tuning doesn't\nhave to be as aggressive.",
    "start": "2010870",
    "end": "2019240"
  },
  {
    "text": "You don't have to be\ncrazy about keeping it constantly up to date. It's just generally a good model",
    "start": "2019240",
    "end": "2025000"
  },
  {
    "text": "and so that means you have to do less work because you're just starting with a higher model, higher quality model.",
    "start": "2025000",
    "end": "2032679"
  },
  {
    "text": "And so that's how the\nimpact of both accuracy and generally the evaluation of models",
    "start": "2032680",
    "end": "2038770"
  },
  {
    "text": "can really impact your workflows. So when we're picking the\nright foundation model,",
    "start": "2038770",
    "end": "2043870"
  },
  {
    "text": "again, we start with modalities\nacross vision, language, and this wild card.",
    "start": "2043870",
    "end": "2048703"
  },
  {
    "text": "If it's generative, we use decoder-based autoregressive models and then we have these\ndifferent parameter sizes.",
    "start": "2049540",
    "end": "2059139"
  },
  {
    "text": "We'll set our accuracy thresholds. And then there's this sort of continuum",
    "start": "2059140",
    "end": "2065770"
  },
  {
    "text": "across open source and proprietary models, and so we've all seen that\nsome language model providers",
    "start": "2065770",
    "end": "2073270"
  },
  {
    "text": "are quite happy open sourcing their models like Falcon is the most recent LLM",
    "start": "2073270",
    "end": "2078909"
  },
  {
    "text": "that's at the top of the leaderboard that the TII has very graciously\ndecided to open source.",
    "start": "2078910",
    "end": "2085980"
  },
  {
    "text": "We're all very happy about that, and so using open source models",
    "start": "2086770",
    "end": "2092470"
  },
  {
    "text": "frees you from relying on a vendor, right? That saves you from getting locked into a certain model vendor",
    "start": "2092470",
    "end": "2098960"
  },
  {
    "text": "and it lets you be a\nlittle bit more flexible with how you develop\nand how you maintain it. At the same time, when you\npick a proprietary model,",
    "start": "2100300",
    "end": "2108040"
  },
  {
    "text": "it means you're offloading\nsome of that model development. It means you don't need to\nbe as obsessed about the R&D",
    "start": "2108040",
    "end": "2114100"
  },
  {
    "text": "which I mean, might impact you 'cause I care about it, but not everyone does. But so in any case,",
    "start": "2114100",
    "end": "2120250"
  },
  {
    "text": "when you pick a proprietary model, again, it gives you access to possibly more performant\nmodels more quickly.",
    "start": "2120250",
    "end": "2127030"
  },
  {
    "text": "And so yeah, so again,\nthere's this vibrant and creative economy of\nfoundation models out there",
    "start": "2127030",
    "end": "2133539"
  },
  {
    "text": "in the universe today and each of them have\nunique pros and cons. But most companies that I work with,",
    "start": "2133540",
    "end": "2140260"
  },
  {
    "text": "most customers will have\na really clear opinion about they only want to do open source",
    "start": "2140260",
    "end": "2147250"
  },
  {
    "text": "or they only want to\ndo proprietary models. If you don't have a clear opinion on that,",
    "start": "2147250",
    "end": "2152380"
  },
  {
    "text": "then just run a cost benefit analysis. Look at the quality of different models",
    "start": "2152380",
    "end": "2158770"
  },
  {
    "text": "based on the quality of the models, how well they work for you, how easily they integrate into your apps.",
    "start": "2158770",
    "end": "2165849"
  },
  {
    "text": "Then sort of make a\ndecision about open source and proprietary for you.",
    "start": "2165850",
    "end": "2170443"
  },
  {
    "text": "So again, this becomes our\nsecond sort of decision criteria and the most impactful\npro tip I can give you",
    "start": "2171910",
    "end": "2180900"
  },
  {
    "text": "is to find a working example. So there are tons of\npapers on archive.org.",
    "start": "2181300",
    "end": "2187569"
  },
  {
    "text": "There's work published at top conferences. There's so many GitHub repositories. There's so many benchmarks\non Papers With Code,",
    "start": "2187570",
    "end": "2195070"
  },
  {
    "text": "online curriculum and coursework. Spend time finding a working example.",
    "start": "2195070",
    "end": "2200290"
  },
  {
    "text": "Go spend the day or\neven just a couple hours looking for working examples that do",
    "start": "2200290",
    "end": "2207070"
  },
  {
    "text": "whatever it is you are trying to do and use that as your starting point",
    "start": "2207070",
    "end": "2213010"
  },
  {
    "text": "because if it works already, then that mitigates so much of your risk because you're starting\nwith a working example.",
    "start": "2213010",
    "end": "2220240"
  },
  {
    "text": "And I would say just anecdotally, a working example beats a concept any day.",
    "start": "2220240",
    "end": "2227233"
  },
  {
    "text": "This is true for all of us. Any day we come up with some crazy idea of something that we're\ntrying to build out,",
    "start": "2228460",
    "end": "2235720"
  },
  {
    "text": "it is always more powerful if\nyou start that conversation with an example of it\nactually up and running",
    "start": "2235720",
    "end": "2243369"
  },
  {
    "text": "because that helps you win stakeholder buy-in early and often. All right, and so that leads us",
    "start": "2243370",
    "end": "2251350"
  },
  {
    "text": "to the last part of picking\nthe right foundation model where we also want this working\nexample to get started with.",
    "start": "2251350",
    "end": "2258490"
  },
  {
    "text": "And then the last concept for\nyou here is really find, use, and master external benchmarks.",
    "start": "2258490",
    "end": "2264040"
  },
  {
    "text": "So obviously our friends at Hugging Face have an open LLM leaderboard",
    "start": "2264040",
    "end": "2269320"
  },
  {
    "text": "and then Stanford's HELM project from the, yeah, from their foundation\nmodel research team",
    "start": "2269320",
    "end": "2275440"
  },
  {
    "text": "is really awesome. They built this amazing\ntaxonomy of ways to evaluate",
    "start": "2275440",
    "end": "2282100"
  },
  {
    "text": "and consider the performance\nof language models and created this great robust repository",
    "start": "2282100",
    "end": "2288430"
  },
  {
    "text": "for all of us to use. So both of those are really good. You wanna get very comfortable at understanding these benchmarks,",
    "start": "2288430",
    "end": "2295330"
  },
  {
    "text": "diving deep in them, spending\ntime to learn about them, thinking about the different statistics,",
    "start": "2295330",
    "end": "2302140"
  },
  {
    "text": "the different methods that they use, and understand what that\nmeans for your model.",
    "start": "2302140",
    "end": "2307540"
  },
  {
    "text": "And then develop unit tests and edge cases for your domain.",
    "start": "2307540",
    "end": "2313360"
  },
  {
    "text": "So not just the hella\nswags of the universe, but your healthcare life sciences,",
    "start": "2313360",
    "end": "2319060"
  },
  {
    "text": "or your financial services, or your media and\nentertainment, your hospitality.",
    "start": "2319060",
    "end": "2324609"
  },
  {
    "text": "The problem that you are\ntrying to solve right now, start to develop your\nown edge cases for that,",
    "start": "2324610",
    "end": "2331900"
  },
  {
    "text": "your own unit tasks. Build a prompt database for yourself of oh, in my use case,\nhere is what I care about.",
    "start": "2331900",
    "end": "2339640"
  },
  {
    "text": "I need to see summarization\nat this standard or I need to see classification\nat this standard,",
    "start": "2339640",
    "end": "2345549"
  },
  {
    "text": "translation at this standard. And then once you have your\nown sort of library of prompts,",
    "start": "2345550",
    "end": "2352470"
  },
  {
    "text": "that becomes your source of truth. Then eventually that\nbecomes much more powerful than the external benchmarks",
    "start": "2352840",
    "end": "2358990"
  },
  {
    "text": "because they don't have\naccess to your data. They don't see what you're doing, and so that's ultimately the\nbest way that you can evaluate",
    "start": "2358990",
    "end": "2367180"
  },
  {
    "text": "and then find the best foundation model. And then this last question is",
    "start": "2367180",
    "end": "2372610"
  },
  {
    "text": "well, what if a better\nmodel comes along tomorrow? Unambiguously this will happen.",
    "start": "2372610",
    "end": "2377680"
  },
  {
    "text": "You should anticipate\nthat this will happen because it's too cool. The universe is too big to put\neverything in a single model,",
    "start": "2377680",
    "end": "2385330"
  },
  {
    "text": "so plan to update your\nML ops pipeline plan,",
    "start": "2385330",
    "end": "2390330"
  },
  {
    "text": "to refresh your training, set your hosting sets to\nensure that you get the benefit",
    "start": "2390520",
    "end": "2398730"
  },
  {
    "text": "and you expose yourself\nto getting your hands on a better model tomorrow",
    "start": "2399910",
    "end": "2404920"
  },
  {
    "text": "so that you can easily expose those benefits to your consumer. And with that, let's\ntake a look at the demo.",
    "start": "2404920",
    "end": "2412720"
  },
  {
    "text": "So in this notebook, we are going to evaluate\na foundation model.",
    "start": "2412720",
    "end": "2418810"
  },
  {
    "text": "In particular we're gonna look at Falcon, so this is Falcon 40B and we are going use label data.",
    "start": "2418810",
    "end": "2426580"
  },
  {
    "text": "So we're going to bring a dataset. This is actually the CNN Daily Mail",
    "start": "2426580",
    "end": "2432279"
  },
  {
    "text": "from the Hugging Face datasets, and in particular the dataset\ncontains a number of articles",
    "start": "2432280",
    "end": "2439380"
  },
  {
    "text": "and then the highlights\nfrom those articles. And what we're gonna do is compute",
    "start": "2440200",
    "end": "2447550"
  },
  {
    "text": "what's called a rouge metric where rouge is a statistic",
    "start": "2447550",
    "end": "2454030"
  },
  {
    "text": "that tells us how similar two strings are, and so this is the rouge one",
    "start": "2454030",
    "end": "2460060"
  },
  {
    "text": "which basically a one means the two strings are perfectly identical and a zero means they\nhave nothing in common.",
    "start": "2460060",
    "end": "2466900"
  },
  {
    "text": "And so this metric will let us see how well our models are doing",
    "start": "2466900",
    "end": "2473410"
  },
  {
    "text": "at summarizing this particular dataset. And so the hope is that you\ncan of course modify this",
    "start": "2473410",
    "end": "2479920"
  },
  {
    "text": "to work with any summarization dataset you're interested in to\nevaluate how well that model",
    "start": "2479920",
    "end": "2486730"
  },
  {
    "text": "is performing at summarizing your data. And so just as before,",
    "start": "2486730",
    "end": "2491740"
  },
  {
    "text": "you are welcome to use a short URL here, the Bitly, to point to this notebook.",
    "start": "2491740",
    "end": "2498640"
  },
  {
    "text": "So this time we're in the SageMaker Distributed\nTraining Workshop Repository and this is number 10 LLM Eval.",
    "start": "2498640",
    "end": "2507460"
  },
  {
    "text": "And then the notebook here is Falcon 40B and then this is the Rouge notebook,",
    "start": "2507460",
    "end": "2512470"
  },
  {
    "text": "and you're welcome to go to\nthe same link using the QR code depending on where you are.",
    "start": "2512470",
    "end": "2518559"
  },
  {
    "text": "So let's check it out.",
    "start": "2518560",
    "end": "2519660"
  },
  {
    "text": "All right, so just as before, we are in SageMaker Studio",
    "start": "2526630",
    "end": "2532750"
  },
  {
    "text": "and in the first notebook\nactually we created a Falcon 40B endpoint\nusing SageMaker JumpStart.",
    "start": "2532750",
    "end": "2540960"
  },
  {
    "text": "So we'll do the same thing here. Excuse me, we'll do the same thing here.",
    "start": "2543550",
    "end": "2548710"
  },
  {
    "text": "The endpoint has already been created and we're gonna point to that\nendpoint using actually Boto3.",
    "start": "2548710",
    "end": "2556410"
  },
  {
    "text": "So the notebook link points\nto a pre-created endpoint.",
    "start": "2556720",
    "end": "2561720"
  },
  {
    "text": "It does have the code to create it. So if you'd like to use the\nnotebook to create the endpoint",
    "start": "2562870",
    "end": "2569619"
  },
  {
    "text": "and then invoke it in the same session, you're welcome to do that. You'll just use this predictor syntax.",
    "start": "2569620",
    "end": "2577360"
  },
  {
    "text": "I wasn't in that scenario. I had a preexisting endpoint and then just needed to point to it,",
    "start": "2577360",
    "end": "2582880"
  },
  {
    "text": "and so I ended up using Boto3 for that. So this notebook uses Boto3 as a result.",
    "start": "2582880",
    "end": "2588670"
  },
  {
    "text": "So here we're using\nBoto3 and AWS Python SDK",
    "start": "2588670",
    "end": "2593670"
  },
  {
    "text": "and then this is pointing\nto not just SageMaker, but the SageMaker Runtime\nwhich is scope to hosting.",
    "start": "2594190",
    "end": "2602220"
  },
  {
    "text": "So all of the hosting APIs are then available through the runtime",
    "start": "2602290",
    "end": "2608920"
  },
  {
    "text": "and that doesn't mean sort\nof managing the endpoints, but it means just interacting\nwith the endpoints directly.",
    "start": "2608920",
    "end": "2615070"
  },
  {
    "text": "And then to basically invoke our endpoint, we're going to just pass the name of it.",
    "start": "2615070",
    "end": "2623080"
  },
  {
    "text": "So here I'm just passing\nthe name of the endpoint and then again, if you're doing this in the same Jupyter session,",
    "start": "2623080",
    "end": "2629049"
  },
  {
    "text": "you can just use the\npredictor.predict payload. If you're using that handy\nmodel.deploy JumpStart model.",
    "start": "2629050",
    "end": "2637470"
  },
  {
    "text": "All right, so here is our payload. Same as last time where we were passing in",
    "start": "2639280",
    "end": "2646570"
  },
  {
    "text": "a variety of inputs to the model and then, so the inputs\nare the text, right?",
    "start": "2646570",
    "end": "2653109"
  },
  {
    "text": "The language that we want\nthe model to interact with and then we're sending it\nother hyperparameters as well.",
    "start": "2653110",
    "end": "2658869"
  },
  {
    "text": "And so the hyperparameters\nare of course ways to optimize and fine tune, so to speak,\nthe output of the model,",
    "start": "2658870",
    "end": "2668407"
  },
  {
    "text": "and so let's give this a shot. So we're sending in \"what\nis the purpose of life?\"",
    "start": "2668407",
    "end": "2675100"
  },
  {
    "text": "And then Falcon instruct 40B says, \"As an AI language model, I don't have a personal belief system",
    "start": "2675100",
    "end": "2681940"
  },
  {
    "text": "or a purpose in life.\" And so that's Falcon's response",
    "start": "2681940",
    "end": "2687940"
  },
  {
    "text": "and then same as last time, we have a variety of prompts.",
    "start": "2687940",
    "end": "2692950"
  },
  {
    "text": "This one is sort of an\nopen-ended question. Tell me about SageMaker including a number of\nhyperparameters here.",
    "start": "2692950",
    "end": "2700540"
  },
  {
    "text": "We invoke the endpoint again, using Boto3, so\nclient.invoke_endpoint,",
    "start": "2700540",
    "end": "2705643"
  },
  {
    "text": "and then we get this response back. And so the notebook continues\nwith a number of others.",
    "start": "2706540",
    "end": "2711912"
  },
  {
    "text": "I defined a little function here, it's just called query_endpoints, and so this takes as a default\nhandle your endpoint name.",
    "start": "2713110",
    "end": "2721650"
  },
  {
    "text": "So actually I have this hard coded here, but you can also just pass the variable that you created above.",
    "start": "2721990",
    "end": "2729549"
  },
  {
    "text": "But in any case, this is nice 'cause it serves as the\ndefault in that function and then when you're\ninvoking the function,",
    "start": "2729550",
    "end": "2735430"
  },
  {
    "text": "you can just send the payload. And so the payload remember,",
    "start": "2735430",
    "end": "2740530"
  },
  {
    "text": "it's nice to have the payload be both the prompt and the hyperparameters",
    "start": "2740530",
    "end": "2748450"
  },
  {
    "text": "because then you can modify the parameters when you're modifying the prompt and that's pretty useful to do.",
    "start": "2748450",
    "end": "2754300"
  },
  {
    "text": "So you can write this to\ntake the whole payload and then this is just using\nthat same function we saw.",
    "start": "2754300",
    "end": "2762310"
  },
  {
    "text": "So we're invoking the endpoint and we're dumping this payload into JSON,",
    "start": "2762310",
    "end": "2769393"
  },
  {
    "text": "specifying that it's a\ncontent type application/json and then we get this response,",
    "start": "2770620",
    "end": "2777970"
  },
  {
    "text": "and then we're gonna read\nit, decoding it in utf8, load that back into JSON,",
    "start": "2777970",
    "end": "2783130"
  },
  {
    "text": "and then return it and then we'll use this a couple times. So the payload here,",
    "start": "2783130",
    "end": "2789167"
  },
  {
    "text": "\"Building a website can be\ndone in 10 simple steps\" which is then of course,\nprompting the model to elaborate",
    "start": "2789167",
    "end": "2796210"
  },
  {
    "text": "and to explain those 10 steps. And so we can look at this",
    "start": "2796210",
    "end": "2802610"
  },
  {
    "text": "and so here we go, the 10 steps. Choose our domain name and our\nhosting provider and so on.",
    "start": "2804790",
    "end": "2810403"
  },
  {
    "text": "And then of course, the\ntranslation, sentiment analysis, question answering, recipe\ngeneration, summarization,",
    "start": "2811780",
    "end": "2820440"
  },
  {
    "text": "and on we go. And so now we're coming\nto the new section. So first, we're going to\ninstall some more packages.",
    "start": "2822820",
    "end": "2832080"
  },
  {
    "text": "So transformers, datasets,\nevaluate, and so on. And then this abls-py",
    "start": "2832090",
    "end": "2838900"
  },
  {
    "text": "which is useful for the rouge metric and then we'll load the\ndatasets from Hugging Face.",
    "start": "2838900",
    "end": "2847320"
  },
  {
    "text": "and then the datasets\nare rendered as follows. So three splits, so we have\nthe train, validation, and test",
    "start": "2847390",
    "end": "2855880"
  },
  {
    "text": "and then each of these\nsplits have an article. They have highlights from that article,",
    "start": "2855880",
    "end": "2862000"
  },
  {
    "text": "and then of course, the\ntotal number of rows. So about 287,000 articles\nin the training set,",
    "start": "2862000",
    "end": "2868860"
  },
  {
    "text": "about 13,000 in the validation, and then 11 in the task. We're just gonna use\nthe training set here.",
    "start": "2870460",
    "end": "2876609"
  },
  {
    "text": "The loop we're gonna run through is quite small for the\npurposes of the notebook, but you're welcome to of course,",
    "start": "2876610",
    "end": "2882970"
  },
  {
    "text": "extend this as you're interested. So just to give you a couple examples",
    "start": "2882970",
    "end": "2889990"
  },
  {
    "text": "of what the dataset looks like, we're gonna send in this article_is. So just the month and\nthen point to an article",
    "start": "2889990",
    "end": "2898440"
  },
  {
    "text": "and here we go. And so we have this, some article.",
    "start": "2899620",
    "end": "2907740"
  },
  {
    "text": "Seems to be about Miami and let's see if we can\nget a summary from this.",
    "start": "2909970",
    "end": "2915400"
  },
  {
    "text": "So it's nice to have a character cutoff. This is to limit the likelihood",
    "start": "2915400",
    "end": "2922960"
  },
  {
    "text": "that your endpoint is\ngoing to run out of memory. So when you're hosting the model yourself, you get to deal with the\nout of memory challenges",
    "start": "2922960",
    "end": "2931690"
  },
  {
    "text": "and so we will use a character\ncutoff here of 2,000.",
    "start": "2931690",
    "end": "2936690"
  },
  {
    "text": "So this just grabs the first\n2,000 characters of the article and then tokenizes it to get closer",
    "start": "2936880",
    "end": "2944080"
  },
  {
    "text": "to that 1024 token cutoff\nlimit for the model.",
    "start": "2944080",
    "end": "2949080"
  },
  {
    "text": "Actually, because the\nCNN dataset here is not,",
    "start": "2949180",
    "end": "2953203"
  },
  {
    "text": "it doesn't seem to be a\nformal summary linguistically. It's truly sort of articles,",
    "start": "2955150",
    "end": "2962800"
  },
  {
    "text": "it's news articles and then\nit's highlights from those that are useful in this context.",
    "start": "2962800",
    "end": "2969670"
  },
  {
    "text": "So I found it helpful to\nmodify the instructions. So instead of using the word\nsummarize as the instruction,",
    "start": "2969670",
    "end": "2978190"
  },
  {
    "text": "I just used the word highlights and then I seem to get summaries",
    "start": "2978190",
    "end": "2983680"
  },
  {
    "text": "that were much closer\nto the label dataset. And so here we'll send this\narticle to the endpoint.",
    "start": "2983680",
    "end": "2991770"
  },
  {
    "text": "I am pretty sure you can eke\nout some performance gains just with modifying some\nof the hyperparameters.",
    "start": "2994870",
    "end": "3001260"
  },
  {
    "text": "I'll leave that up to you, but I did find it helpful to have the max new token set to 66",
    "start": "3001260",
    "end": "3008670"
  },
  {
    "text": "just to keep the predictions shorter.",
    "start": "3008670",
    "end": "3013670"
  },
  {
    "text": "So we have this empty rocket launcher. Goodness gracious, did I do this correct?",
    "start": "3013950",
    "end": "3022130"
  },
  {
    "text": "Oh, great, okay, sorry. That must have been a previous one. Okay, let me make sure\nI'm defining this right.",
    "start": "3022890",
    "end": "3029370"
  },
  {
    "text": "So article one, yes. Okay, we're in Miami. Now let's send this up one more time here.",
    "start": "3029370",
    "end": "3037020"
  },
  {
    "text": "See if we can point to the right article. That must have been from a previous.",
    "start": "3037020",
    "end": "3043230"
  },
  {
    "text": "Here we go, great. Okay, so this judge",
    "start": "3043230",
    "end": "3048130"
  },
  {
    "text": "and okay, looks like it's\ncertainly related to this one.",
    "start": "3049590",
    "end": "3054590"
  },
  {
    "text": "And then the dataset again is labeled, so it comes with a pre-written\nhighlight of the article,",
    "start": "3056220",
    "end": "3064160"
  },
  {
    "text": "and so this is the label for this article",
    "start": "3067380",
    "end": "3072380"
  },
  {
    "text": "and let's see if we can\ncompute rouge on this. So now we're going to use the\nHugging Face evaluate library",
    "start": "3072870",
    "end": "3079200"
  },
  {
    "text": "and we're gonna load this rouge score. Rouge is very sensitive\nto the lengths of the text",
    "start": "3079200",
    "end": "3087140"
  },
  {
    "text": "and it needs the two lengths\nto be identical actually. So this little function\njust equalizes those two",
    "start": "3087690",
    "end": "3095280"
  },
  {
    "text": "so that you have identical in length basically predictions and labels.",
    "start": "3095280",
    "end": "3101280"
  },
  {
    "text": "And so that's this function\nthat just equalizes them",
    "start": "3101280",
    "end": "3106120"
  },
  {
    "text": "and then so we'll use that function to then equalize the\npredictions in the label",
    "start": "3107520",
    "end": "3113730"
  },
  {
    "text": "and then we'll send those\nto this rouge.compute, and then we'll extract.",
    "start": "3113730",
    "end": "3120060"
  },
  {
    "text": "This gives you about four\nor five different metrics and we'll pull out this rouge1\nto see them equalize there",
    "start": "3120060",
    "end": "3128480"
  },
  {
    "text": "and see what we get. So this is again, equalized to one,",
    "start": "3128670",
    "end": "3133890"
  },
  {
    "text": "so that's about a 2% rouge. Rouge is a tough metric to optimize for",
    "start": "3133890",
    "end": "3141860"
  },
  {
    "text": "because again, it's really\nfocused on the strings being exactly the same\nwhen we know in real life",
    "start": "3141870",
    "end": "3149760"
  },
  {
    "text": "that summaries can carry the same meaning,",
    "start": "3149760",
    "end": "3154760"
  },
  {
    "text": "but be phrased differently. So there's a bit of a challenge\nwith the metric there,",
    "start": "3155430",
    "end": "3161010"
  },
  {
    "text": "but nonetheless let's proceed. So this little function for you",
    "start": "3161010",
    "end": "3167670"
  },
  {
    "text": "should be a nice way to\nloop through your dataset and compare the performance of the LLM",
    "start": "3167670",
    "end": "3176170"
  },
  {
    "text": "with your ground truth in\na summarization use case so you can easily see how well the model",
    "start": "3177240",
    "end": "3183450"
  },
  {
    "text": "is summarizing your data. So this is just a number of samples, indicator you can change,",
    "start": "3183450",
    "end": "3190020"
  },
  {
    "text": "and then you're gonna loop through that. So we're gonna go up to 10 and then as we go up to 10,",
    "start": "3190020",
    "end": "3195240"
  },
  {
    "text": "we're gonna point to the article again, coming from that dataset object set to the character cutoff.",
    "start": "3195240",
    "end": "3201900"
  },
  {
    "text": "We're gonna point to the\nlabel just as we did before which is the highlights for that article",
    "start": "3201900",
    "end": "3208560"
  },
  {
    "text": "and then we're gonna send that to the LLM. So we're putting that\ninto the payload here.",
    "start": "3208560",
    "end": "3216510"
  },
  {
    "text": "So again, we're sending in the article asking the LLM to define the highlights.",
    "start": "3216510",
    "end": "3222513"
  },
  {
    "text": "Changing some parameters here. The length of the highlights\ncan change somewhat.",
    "start": "3223740",
    "end": "3229019"
  },
  {
    "text": "So in a future version it, it might be helpful to modify this based on the length of the highlights.",
    "start": "3229020",
    "end": "3236339"
  },
  {
    "text": "And then we'll query the endpoint. So we take this payload, send it to the endpoint,\nget our predictions,",
    "start": "3236340",
    "end": "3243660"
  },
  {
    "text": "send both the predictions and\nthe label to this equalizer. So they're equalized, then we put them",
    "start": "3243660",
    "end": "3251040"
  },
  {
    "text": "into the rouge metrics\nand then rouge.compute, and we'll get the results.",
    "start": "3251040",
    "end": "3257820"
  },
  {
    "text": "From those, we'll extract that rouge1 and then this just writes those\nto a little text file here",
    "start": "3257820",
    "end": "3265940"
  },
  {
    "text": "that tells you the\narticle_id, the predictions, and then the rouge metric.",
    "start": "3266640",
    "end": "3271800"
  },
  {
    "text": "And so of course, it's handy to write that to disc as soon as possible",
    "start": "3271800",
    "end": "3276960"
  },
  {
    "text": "because then you save the results. And so we can see those here",
    "start": "3276960",
    "end": "3282299"
  },
  {
    "text": "and so we have a number of\narticles going up to nine,",
    "start": "3282300",
    "end": "3287300"
  },
  {
    "text": "and they do indeed have the highlights and they also have the rouge.",
    "start": "3287670",
    "end": "3295440"
  },
  {
    "text": "And so in this notebook, again, we explored the rouge metric",
    "start": "3295440",
    "end": "3301470"
  },
  {
    "text": "in order to evaluate our foundation model and see how well it can\nperform summarization.",
    "start": "3301470",
    "end": "3308460"
  },
  {
    "text": "So I hope you enjoyed it. In the next video, we're gonna learn more about interacting with pre-trained foundation models.",
    "start": "3308460",
    "end": "3315240"
  },
  {
    "text": "So that will include prompt engineering and then fine tuning in particular, so I will see you there.",
    "start": "3315240",
    "end": "3320913"
  }
]