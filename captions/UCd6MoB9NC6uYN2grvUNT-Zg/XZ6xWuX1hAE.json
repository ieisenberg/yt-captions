[
  {
    "text": "everyone that works so thanks for all for coming this is uh building your own web analytics",
    "start": "439",
    "end": "7040"
  },
  {
    "text": "service so uh you know we're going to go it's a 200 level track so we're not going to go too deep into anything but",
    "start": "7040",
    "end": "12880"
  },
  {
    "text": "we're going to sort of cover a whole bunch of Technologies we use that uh you know have allowed us to build this out",
    "start": "12880",
    "end": "17920"
  },
  {
    "text": "and uh you know there should be lots of room for questions at the end so uh let's get",
    "start": "17920",
    "end": "23640"
  },
  {
    "text": "started so first of all who am I so my name is Jonathan keer I'm Keebler on",
    "start": "23640",
    "end": "28760"
  },
  {
    "text": "Twitter um I've worked with a lot of media companies over the years and um you know the latest one was uh CTV it",
    "start": "28760",
    "end": "35640"
  },
  {
    "text": "was the largest Media company in Canada and we sort of we built a lot of video players for their news sites and so",
    "start": "35640",
    "end": "41520"
  },
  {
    "text": "scale and and availability and things like that were always sort of at the Forefront um after that I went on in",
    "start": "41520",
    "end": "48399"
  },
  {
    "text": "2008 to to found scribble live with my co-founder Michael Damonte and uh you",
    "start": "48399",
    "end": "53559"
  },
  {
    "text": "know scribble live we bootstrap from the ground up we were using you know our own credit cards and things so we had to",
    "start": "53559",
    "end": "59320"
  },
  {
    "text": "find ways to do things cheap but yet work with really large media companies",
    "start": "59320",
    "end": "64559"
  },
  {
    "text": "so what is scribble live so scribble live is the leading provider of realtime engagement Management Solutions so we",
    "start": "64559",
    "end": "71000"
  },
  {
    "text": "enable real-time publication and syndication of digital content for customers all over the world so usually",
    "start": "71000",
    "end": "76479"
  },
  {
    "text": "that takes a form of live blogs and interactive real-time experiences that they use to You Know cover news in real",
    "start": "76479",
    "end": "82320"
  },
  {
    "text": "time or or engage with an audience q&as all sorts of stuff we work with a lot of Brands too and so just to give you an",
    "start": "82320",
    "end": "89479"
  },
  {
    "text": "idea of some some of our customers um you know CNN is a really big customer you guys probably know down here quite a",
    "start": "89479",
    "end": "95000"
  },
  {
    "text": "bit uh ESPN um we work with customers in 14 different languages all around the",
    "start": "95000",
    "end": "100159"
  },
  {
    "text": "world to power these sort of experiences so obviously scale is huge for us",
    "start": "100159",
    "end": "105560"
  },
  {
    "text": "because if something big happens in the world it's not just one of our customers covering it it's probably all of them",
    "start": "105560",
    "end": "110719"
  },
  {
    "text": "that are interested or a big subset of that so you know we Amazon has been a great product for us and um you know",
    "start": "110719",
    "end": "118000"
  },
  {
    "text": "they've they've asked us today to talk about um a pattern that's worked really well and and allowed us to build out this analytics platform we",
    "start": "118000",
    "end": "124840"
  },
  {
    "text": "use so that's what we're going to do today we're going to build your very own analytics service um sounds a little",
    "start": "124840",
    "end": "130879"
  },
  {
    "text": "crazy but I swear you can do it um we're going to use some really cool buzzwords so we're going to use uh node.js on",
    "start": "130879",
    "end": "136920"
  },
  {
    "text": "Amazon ec2 for our web servers we're going to use Dynamo DB as the database and we're going to use Hadoop running on",
    "start": "136920",
    "end": "143040"
  },
  {
    "text": "elastic map reduce to do the rollups of data so to sort of extract uh more information out of the data we're",
    "start": "143040",
    "end": "149239"
  },
  {
    "text": "collecting so why would we do this why would we go and build this stuff so scribble live",
    "start": "149239",
    "end": "155800"
  },
  {
    "text": "tracks what we call engagement minutes across our customer sites and what what an engagement minute is is one minute of",
    "start": "155800",
    "end": "162239"
  },
  {
    "text": "a user watching a web page so it's how many eyeballs you have on your events so for us you know for and for our",
    "start": "162239",
    "end": "168440"
  },
  {
    "text": "customers understanding how many people are watching their events um if what they're doing is successful is a really",
    "start": "168440",
    "end": "174720"
  },
  {
    "text": "key thing for them to to understand and so to give you a sense of scale we do about 2.5 billion of these a month um",
    "start": "174720",
    "end": "181840"
  },
  {
    "text": "we've seen 120 million in an hour so it's very spiky because it's driven by how interesting the news is what people",
    "start": "181840",
    "end": "188680"
  },
  {
    "text": "you know if people are asleep uh you know if news is a story that's continuing over days and days it's something that's been really important",
    "start": "188680",
    "end": "194760"
  },
  {
    "text": "for us to track and what we found was the big analytics providers couldn't do it they either didn't have the features",
    "start": "194760",
    "end": "200840"
  },
  {
    "text": "to do it in real time the way we needed or they were too inaccurate you know not to name any names but some of them were",
    "start": "200840",
    "end": "207280"
  },
  {
    "text": "Under reporting by as much as 30% because they just don't see users the same way way we do we we see people you",
    "start": "207280",
    "end": "214200"
  },
  {
    "text": "know they might be sitting on that page for half an hour an hour watching something um so you know looking at just",
    "start": "214200",
    "end": "219480"
  },
  {
    "text": "page views and things like that isn't very helpful so what we're going to do is",
    "start": "219480",
    "end": "224920"
  },
  {
    "text": "we're going to go through our what we've done here um it's a very basic stack at a at a high level the the visitor",
    "start": "224920",
    "end": "230879"
  },
  {
    "text": "traffic comes in they're hitting in elastic our elastic load balancing then they're being distributed to a whole",
    "start": "230879",
    "end": "236360"
  },
  {
    "text": "bunch of node.js servers and then those node.js servers are writing back to Dynamo DB and then Dynamo DB is having",
    "start": "236360",
    "end": "243519"
  },
  {
    "text": "its data extracted cooked by Hadoop and put back into Dynamo so um we're going",
    "start": "243519",
    "end": "248560"
  },
  {
    "text": "to go through every step of this and uh see what we did and you know a big part of this too",
    "start": "248560",
    "end": "255280"
  },
  {
    "text": "is just showing you a pattern in some products that have worked really well for us you know this even if you're not building an analytics platform hopefully",
    "start": "255280",
    "end": "261720"
  },
  {
    "text": "you can take away from this session you know some of the the practices that we found have been really",
    "start": "261720",
    "end": "266800"
  },
  {
    "text": "successful so let's start by looking at the data structure ructure in Dynamo DB so for us we have tables Broken Out by",
    "start": "266800",
    "end": "274360"
  },
  {
    "text": "minute hour day and month and the minute data is the stuff that's being written",
    "start": "274360",
    "end": "279400"
  },
  {
    "text": "to live so those node.js servers that are that are in our stack there they're all continually writing back that",
    "start": "279400",
    "end": "285520"
  },
  {
    "text": "minute-by-minute data into Dynamo and Dynamo has been a great product for that um the structure in Dynamo um anyone who",
    "start": "285520",
    "end": "293720"
  },
  {
    "text": "who's familiar with it who uses Dynamo DB here today so yeah pretty big number I'd say",
    "start": "293720",
    "end": "299479"
  },
  {
    "text": "about a third of you um it it has two big Concepts first is a hash so this is",
    "start": "299479",
    "end": "304759"
  },
  {
    "text": "sort of an ID um for your data and so for us this is a page ID can be anything",
    "start": "304759",
    "end": "309840"
  },
  {
    "text": "it could be a URL for this example it doesn't matter and the other thing we do is um we use a range that is the time so",
    "start": "309840",
    "end": "317160"
  },
  {
    "text": "um with Dynamo DB you can use these ranges to extract um data um for a certain time period so we're storing the",
    "start": "317160",
    "end": "324919"
  },
  {
    "text": "the minute uh the minutes in there or the hours or the days and then the other",
    "start": "324919",
    "end": "330000"
  },
  {
    "text": "thing in inside each of these items that we're storing in Dynamo we're just storing a counter the number of hits um",
    "start": "330000",
    "end": "335880"
  },
  {
    "text": "I'm sort of simplifying this a bit but you could store anything in there you could have hits or unique hits or a whole bunch of data um the great thing",
    "start": "335880",
    "end": "342639"
  },
  {
    "text": "about Dynamo is it's uh you know whatever Json you want essentially you can jam in",
    "start": "342639",
    "end": "348160"
  },
  {
    "text": "there so looking at our last uh load balancing this is uh you know we use a custom Ami uh Amazon machine image for",
    "start": "348160",
    "end": "356240"
  },
  {
    "text": "on ec2 so what it does is when you spin up the EC we've used Linux startup scripts to download the source for our",
    "start": "356240",
    "end": "363240"
  },
  {
    "text": "application from sub subversion and then launch it with node.js so for us doing a",
    "start": "363240",
    "end": "368400"
  },
  {
    "text": "roll out is as easy as doing a rolling um reboot of these servers or turning on",
    "start": "368400",
    "end": "373680"
  },
  {
    "text": "totally new servers and then sliding them into production um we use one load balancer",
    "start": "373680",
    "end": "380520"
  },
  {
    "text": "in multiple zones so we're you know we have those noj server sitting in lots of different zones and we use cookies on",
    "start": "380520",
    "end": "386520"
  },
  {
    "text": "that load balancer to keep um per user to keep people sort of in the same zone",
    "start": "386520",
    "end": "391800"
  },
  {
    "text": "for uh D duplicating the data we also use Auto scaling on the elbs which is",
    "start": "391800",
    "end": "397280"
  },
  {
    "text": "you know we've really just gotten into it recently it's a great feature we have these rules that basically if the CPU",
    "start": "397280",
    "end": "402880"
  },
  {
    "text": "goes above 50% on our nodejs servers or the network in is sort of around the 50",
    "start": "402880",
    "end": "408319"
  },
  {
    "text": "million bytes per minute Mark that triggers new servers coming online so we can be assured that say it's the middle",
    "start": "408319",
    "end": "414479"
  },
  {
    "text": "of the night there's a big breaking news story somewhere um our load our noj servers are going to scale up to handle",
    "start": "414479",
    "end": "420560"
  },
  {
    "text": "that however big they need to get so what are the noj servers doing um",
    "start": "420560",
    "end": "428199"
  },
  {
    "text": "basically they're they're accepting a a request that has an ID of the item and a",
    "start": "428199",
    "end": "433440"
  },
  {
    "text": "unique user ID and we're we're doing this at the JavaScript layer so you can imagine there's there's code on in the",
    "start": "433440",
    "end": "438840"
  },
  {
    "text": "browser that's you know running an Ajax call that's saying okay I'm user Jonathan um I'm I just saw page one two",
    "start": "438840",
    "end": "446800"
  },
  {
    "text": "3 4 and it sends that back to our servers on the other side what we keep is a is a dictionary of that's based on",
    "start": "446800",
    "end": "454879"
  },
  {
    "text": "the minute and the ID combination so um you can see in my example here in that",
    "start": "454879",
    "end": "461080"
  },
  {
    "text": "array we've got a particular time so January 1st 2014 at 123 a.m. and we're",
    "start": "461080",
    "end": "467039"
  },
  {
    "text": "saying that we got one more hit to item ABCD so it's it's just a way for us to",
    "start": "467039",
    "end": "473919"
  },
  {
    "text": "start aggregating the data down at the nodejs level so we're not saying um we",
    "start": "473919",
    "end": "479080"
  },
  {
    "text": "have to every single request back to Dynamo we're sort of aggregating it within the minute within the node to",
    "start": "479080",
    "end": "485319"
  },
  {
    "text": "help make data processing easier later on we also have an array of what users",
    "start": "485319",
    "end": "490360"
  },
  {
    "text": "have already been counted for that minute so if the same user makes a request twice or something weird they",
    "start": "490360",
    "end": "495960"
  },
  {
    "text": "they reload the page or something like that um we're D duplicating by data by saying we've already seen that person",
    "start": "495960",
    "end": "501759"
  },
  {
    "text": "don't count them again and then at the end of that minute we're writing the data back to Dynamo so the whole minute",
    "start": "501759",
    "end": "507120"
  },
  {
    "text": "goes on we're sort of counting Counting counting it JS and at the end of that minute boom we write it back to",
    "start": "507120",
    "end": "514080"
  },
  {
    "text": "Dynamo so one of the best practices we found really quickly was you don't want to write your data back in a loop like",
    "start": "514080",
    "end": "520760"
  },
  {
    "text": "if you are if you have a loop that says I've reached this minute I'm going to just write everything back as fast as I",
    "start": "520760",
    "end": "525959"
  },
  {
    "text": "can what happens is the throughput in Dynamo spikes really really high for a few seconds because it's so good at",
    "start": "525959",
    "end": "531839"
  },
  {
    "text": "taking all those requests and then nothing happens for the rest of the minute so you can even look at your",
    "start": "531839",
    "end": "538079"
  },
  {
    "text": "throughput graphs in Dynamo DB and you can't even tell what's going on because you seem to be getting a lot of Errors",
    "start": "538079",
    "end": "543560"
  },
  {
    "text": "writing data but looking at your graphs it looks like it's well below whatever throughput limit you set so at the end",
    "start": "543560",
    "end": "550440"
  },
  {
    "text": "of the day you have to keep increasing that throughput limit up and up and up and which costs you more and more money",
    "start": "550440",
    "end": "555760"
  },
  {
    "text": "and you don't know why it's going on so what we figured out is you really have to stagger your rights to Dynamo over a",
    "start": "555760",
    "end": "561360"
  },
  {
    "text": "minute so we look at how many rights have to happen so say really easy example we have to do 60 rights within",
    "start": "561360",
    "end": "568240"
  },
  {
    "text": "that minute um so we figure do the math and we say okay that's about one right per minute per second sorry and uh we we",
    "start": "568240",
    "end": "576440"
  },
  {
    "text": "just write at that rate and so it keeps our our throughput to Dynamo really nice and level we can see it in our graphs",
    "start": "576440",
    "end": "582640"
  },
  {
    "text": "and we've actually saved thousands of dollars a month just doing that practice it it really makes it that much more",
    "start": "582640",
    "end": "590279"
  },
  {
    "text": "efficient so here's what our calls to Dynamo DB look like so it has an update function and we're just saying add x to",
    "start": "590279",
    "end": "598000"
  },
  {
    "text": "the the minute ID combination so if if I'm one of the nodejs servers and and",
    "start": "598000",
    "end": "603560"
  },
  {
    "text": "I've just completed my my uh minute um for page 1 two 3 4 um I'm just going to",
    "start": "603560",
    "end": "610240"
  },
  {
    "text": "write back to Dynamo and say you know add 10 to that that hit counter that you're running so because Dynamo has",
    "start": "610240",
    "end": "617040"
  },
  {
    "text": "Atomic Edition you can really have an unlimited number of these node.js servers writing back at exactly the same",
    "start": "617040",
    "end": "623959"
  },
  {
    "text": "time and you're going to get the correct number at the end of it you're not going to lose counts because there's concurrency happening so that Atomic",
    "start": "623959",
    "end": "631000"
  },
  {
    "text": "right has been a great feature of Dynamo we love using so then we've got we've got this",
    "start": "631000",
    "end": "637880"
  },
  {
    "text": "table of minute-by-minute data so you can imagine for event 1 2 3 4 that we've got every single minute how many people",
    "start": "637880",
    "end": "643920"
  },
  {
    "text": "were're looking at it the next step is to go from the minute to our hourly data",
    "start": "643920",
    "end": "649079"
  },
  {
    "text": "so we do that by rounding down to the nearest hour here's the equation we use and summing the number of hits at each",
    "start": "649079",
    "end": "655800"
  },
  {
    "text": "for each minute of that hour and I'll show you how to do that in a sec um you something we do to to Really make",
    "start": "655800",
    "end": "662760"
  },
  {
    "text": "this um run a lot faster is we only look at the last 24 hours so we've sort of this is a judgment call we made it takes",
    "start": "662760",
    "end": "669240"
  },
  {
    "text": "about six hours to run through our our data so we only look at the last day to sort of save some time and we do the",
    "start": "669240",
    "end": "675720"
  },
  {
    "text": "exact same thing to go from hourly to daily and daily to monthly so it's always you know round all the data down",
    "start": "675720",
    "end": "680800"
  },
  {
    "text": "to the closest hour sum it all up put it back in Dynamo so on Hadoop we're using Hive",
    "start": "680800",
    "end": "687959"
  },
  {
    "text": "scripts to do that and here's an example of what we're doing um you can see at the bottom there where we're doing the",
    "start": "687959",
    "end": "693600"
  },
  {
    "text": "group by item so the item id as well as the rounding operation and then in the",
    "start": "693600",
    "end": "699680"
  },
  {
    "text": "select you can see we're we're summing that up so anybody who's who's looked at",
    "start": "699680",
    "end": "705000"
  },
  {
    "text": "uh SQL query language or something um you know Hive is very much the same syntax it's really easy to get going",
    "start": "705000",
    "end": "710959"
  },
  {
    "text": "with which is at the end of the day why we started using it it's it's very straightforward and you don't have to",
    "start": "710959",
    "end": "716120"
  },
  {
    "text": "get into Java or anything like that so to set up our elastic map reduce um",
    "start": "716120",
    "end": "722760"
  },
  {
    "text": "you know anybody who's played around with it a bit has probably seen this screen um so we create a new job flow um",
    "start": "722760",
    "end": "729560"
  },
  {
    "text": "by saying run your own application and we choose Hive program so um we're saying set up a hive uh a hive Hadoop",
    "start": "729560",
    "end": "737360"
  },
  {
    "text": "cluster for us and Amazon makes this it's we use console for all this stuff it's it's so straightforward you just",
    "start": "737360",
    "end": "743160"
  },
  {
    "text": "click the buttons and you're in in a little while it takes about 15 minutes to spin up um you know when you're setting it up",
    "start": "743160",
    "end": "750839"
  },
  {
    "text": "we choose an interactive Hive session anyone who's familiar with EMR um you",
    "start": "750839",
    "end": "755920"
  },
  {
    "text": "know there's there's a couple ways you can use it some people um really want to just have it execute a job and then be",
    "start": "755920",
    "end": "761959"
  },
  {
    "text": "done so they they sort of there's startup scripts and ways that you can get elastic map produced running your",
    "start": "761959",
    "end": "767600"
  },
  {
    "text": "job right from the get-go and and uh you don't have to deal with anything for us we want this job continually running 24",
    "start": "767600",
    "end": "774920"
  },
  {
    "text": "hours a day 7 days a week forever so the way we approached it is we wrote A A Cron job that runs that Cycles every 15",
    "start": "774920",
    "end": "783000"
  },
  {
    "text": "minutes to check if the The Hive job is complete so it's if if it is complete so",
    "start": "783000",
    "end": "788360"
  },
  {
    "text": "if there's if it's done doing its work um it downloads the newest Hive script",
    "start": "788360",
    "end": "793519"
  },
  {
    "text": "and then runs it again so that way you know say we have we find a way to optimize the script that's running or we",
    "start": "793519",
    "end": "799440"
  },
  {
    "text": "want it to do a couple more things we want to go from monthly to yearly for example um it's as simple as putting our",
    "start": "799440",
    "end": "806560"
  },
  {
    "text": "um our new script into our deployment bucket and waiting for the last job to run and then the new jobs will kick in",
    "start": "806560",
    "end": "813480"
  },
  {
    "text": "so um for us it's been a great way to to to run Hive um you can even the",
    "start": "813480",
    "end": "818680"
  },
  {
    "text": "advantage of running an interactive session like this too is that you can log into the machines at any time and",
    "start": "818680",
    "end": "823800"
  },
  {
    "text": "and see how your jobs are progressing um run your own jobs if if you think you know you might have a new way to sum up",
    "start": "823800",
    "end": "829920"
  },
  {
    "text": "this data even faster it's been a great tool for us and then we're using Amazon cloudwatch to see if the jobs are taking",
    "start": "829920",
    "end": "836800"
  },
  {
    "text": "too long so you know sometimes these job jobs can um you know start taking hours",
    "start": "836800",
    "end": "842440"
  },
  {
    "text": "or or sorry more than hours days and so cloudwatch lets us track if you know the",
    "start": "842440",
    "end": "848120"
  },
  {
    "text": "the throughput level is is a problem or if there's you know the job hasn't run for a few hours something's crashed it",
    "start": "848120",
    "end": "854759"
  },
  {
    "text": "and it so Cloud watch is how we really keep track that these things are healthy that they're running properly and that",
    "start": "854759",
    "end": "861320"
  },
  {
    "text": "uh you know our data is getting crunched so uh here's an example of one",
    "start": "861320",
    "end": "867600"
  },
  {
    "text": "of these cron jobs anyone who's familiar with uh bash should should find it",
    "start": "867600",
    "end": "872839"
  },
  {
    "text": "pretty straightforward we're really just um looking at hadoop's Job list and and seeing is there a job running right now",
    "start": "872839",
    "end": "879519"
  },
  {
    "text": "and if it's if uh there isn't then we down at the last line there we're we're",
    "start": "879519",
    "end": "884920"
  },
  {
    "text": "downloading the new rollup script and then starting it executing um for us it's it's been a great pattern to store",
    "start": "884920",
    "end": "892320"
  },
  {
    "text": "um our our scripts especially when we're talking about our Linux instances to store scripts out on S3 and and have",
    "start": "892320",
    "end": "899399"
  },
  {
    "text": "this sort of pattern where machines turn on they download your script from S3 and then spin it up um a lot of people do",
    "start": "899399",
    "end": "905880"
  },
  {
    "text": "another method where with an Amazon machine image they actually bake the script into the image and it it turns on",
    "start": "905880",
    "end": "912079"
  },
  {
    "text": "with the right script already on that that works really well too but for us you know we we really don't want to be",
    "start": "912079",
    "end": "917839"
  },
  {
    "text": "in the practice of turning machines on and off constantly it's it's easier for us to just reboot",
    "start": "917839",
    "end": "923920"
  },
  {
    "text": "them so then looking at our application API so at this point the language really",
    "start": "924199",
    "end": "929600"
  },
  {
    "text": "doesn't matter for us we're we're a net shop which most people find pretty funny because we're a startup but but um you",
    "start": "929600",
    "end": "936680"
  },
  {
    "text": "know to get the data out of Dynamo it's as simple as using their query command um with the hash ID which we talked",
    "start": "936680",
    "end": "942959"
  },
  {
    "text": "about before that's sort of the this is Page ID 1 12 3 4 or the URL or whatever you're tracking on and then AR range so",
    "start": "942959",
    "end": "949920"
  },
  {
    "text": "we were able to say give me all the data between you know 4 p.m. and 5:00 p.m. um",
    "start": "949920",
    "end": "955319"
  },
  {
    "text": "return it all to me and then I'm going to cook it on the am on the appli side um one thing we found is that",
    "start": "955319",
    "end": "964519"
  },
  {
    "text": "because we're running these map reduced jobs um they can take a while to run and for our customers they need the data",
    "start": "964519",
    "end": "971279"
  },
  {
    "text": "immediately it's not good enough to say you know wait wait a day you'll have your analytics on your event we need to",
    "start": "971279",
    "end": "976600"
  },
  {
    "text": "know right now so what we do is we actually say someone asked for you know two days worth of data so we know that",
    "start": "976600",
    "end": "983880"
  },
  {
    "text": "we're going to have the data from yesterday already cooked but we don't know about today that job has hasn't",
    "start": "983880",
    "end": "989160"
  },
  {
    "text": "finished complete that hasn't completed yet so what we do is we actually say give me the hourly data from yesterday",
    "start": "989160",
    "end": "996279"
  },
  {
    "text": "and then for today give me all the minute-by-minute data so you can imagine this gets really easy if it's the",
    "start": "996279",
    "end": "1001759"
  },
  {
    "text": "beginning of the day at the end of the day it's a bit more data but at the end of the day you end up with you know a",
    "start": "1001759",
    "end": "1008319"
  },
  {
    "text": "manageable number of data points in your application code and then with those data points it's uh we we sort of do the",
    "start": "1008319",
    "end": "1016160"
  },
  {
    "text": "same things that we do with the map reduce we roll up that data in the application itself so what we're able to",
    "start": "1016160",
    "end": "1022440"
  },
  {
    "text": "do is produce um API results to our customers that have the latest minute up",
    "start": "1022440",
    "end": "1029160"
  },
  {
    "text": "to the latest minute um and uh you know they're able to get that 24/7 a day",
    "start": "1029160",
    "end": "1034400"
  },
  {
    "text": "without waiting for the map ruce job to to to finish",
    "start": "1034400",
    "end": "1039319"
  },
  {
    "text": "up so here's here's some graphs everyone loves graphs um this is this is what our",
    "start": "1039679",
    "end": "1044880"
  },
  {
    "text": "performance looks like um from Network in so this was a couple weeks ago um and what we found is you can see",
    "start": "1044880",
    "end": "1052320"
  },
  {
    "text": "about Midway through something started happening there was a a big news event happening um so you can see our traffic",
    "start": "1052320",
    "end": "1058480"
  },
  {
    "text": "just growing bit by bit you know with the servers that were already online and at some point they hit that magic",
    "start": "1058480",
    "end": "1064440"
  },
  {
    "text": "threshold where it's time to turn on more servers and that's really where autoscale um kicks in so you can see",
    "start": "1064440",
    "end": "1070559"
  },
  {
    "text": "we've got it set to turn on new servers uh two new servers so um it's really easy to say you know turn on two more",
    "start": "1070559",
    "end": "1076360"
  },
  {
    "text": "servers or you know maybe you've got traffic that's scale so quickly you need to turn on 10 more at a time but in this",
    "start": "1076360",
    "end": "1082240"
  },
  {
    "text": "case you can see those two new servers a green one and a blue one come up to to uh help their brothers deal with this",
    "start": "1082240",
    "end": "1089720"
  },
  {
    "text": "traffic looking at the same time period um on the CPU utilization you can see",
    "start": "1089720",
    "end": "1094880"
  },
  {
    "text": "what was happening the CPUs were getting large higher and higher and higher um on the existing nodes and at some point",
    "start": "1094880",
    "end": "1101880"
  },
  {
    "text": "node would have just crapped out and uh you know we would have started losing these analytics so you can see the two",
    "start": "1101880",
    "end": "1109000"
  },
  {
    "text": "servers came online at about uh 12:45 you can see it takes about 5",
    "start": "1109000",
    "end": "1115440"
  },
  {
    "text": "minutes for them to really come online and to download the scripts and and get going but then immediately after they",
    "start": "1115440",
    "end": "1122240"
  },
  {
    "text": "get into the elastic load balcer they start receiving that traffic and really helping um all of their their brothers",
    "start": "1122240",
    "end": "1129080"
  },
  {
    "text": "deal with this traffic Spike so you know we've been using this for about eight months now um you know",
    "start": "1129080",
    "end": "1137440"
  },
  {
    "text": "it's it's something we started using Dynamo DB really as uh for an MVP you",
    "start": "1137440",
    "end": "1143840"
  },
  {
    "text": "know we like to stay agile do a lot of continuous integration that sort of stuff and Dynamo came out right as we",
    "start": "1143840",
    "end": "1149919"
  },
  {
    "text": "were starting this product and we were sort of like H why don't we try you know it's a key value store we don't have to",
    "start": "1149919",
    "end": "1156280"
  },
  {
    "text": "you know deal with setting up Cassandra or something else like that and you know it was amazing how quickly it made it",
    "start": "1156280",
    "end": "1163000"
  },
  {
    "text": "into our production environment because you the moment we start thinking about production and doing production traffic",
    "start": "1163000",
    "end": "1170280"
  },
  {
    "text": "tests it was really just a matter of increasing our throughput on Dynamo DB and it scales that easily so um you know",
    "start": "1170280",
    "end": "1178559"
  },
  {
    "text": "the amount of hours that it saved for for us over the uh over those months of",
    "start": "1178559",
    "end": "1183799"
  },
  {
    "text": "you know hours we would have spent configuring key value shares or learning Cassandra and then having to scale that",
    "start": "1183799",
    "end": "1191360"
  },
  {
    "text": "up um so that we could use it in production have really been significant and like I said too some of the tricks",
    "start": "1191360",
    "end": "1197840"
  },
  {
    "text": "that we've learned along the way such as um you know how limiting the throughput",
    "start": "1197840",
    "end": "1204159"
  },
  {
    "text": "limiting your own rate of throughput to Dynamo have allowed us to go from you know spends of tens of thousands of",
    "start": "1204159",
    "end": "1211000"
  },
  {
    "text": "dollars a month down to the S the single thousands and in some cases per table actually down into the hundreds so",
    "start": "1211000",
    "end": "1217799"
  },
  {
    "text": "there's a lot that you can get to know about Dynamo that will allow you to save money and for us our our Amazon team has",
    "start": "1217799",
    "end": "1224320"
  },
  {
    "text": "really been uh you know a great resource for that as well um any you that aren't under the NDA with Amazon right now it's",
    "start": "1224320",
    "end": "1231400"
  },
  {
    "text": "worth reaching out to your account manager because what they can actually do is get you in touch with the people that can actually tell you a bit more",
    "start": "1231400",
    "end": "1238000"
  },
  {
    "text": "about how Dynamo Works behind the scenes and sort of give you tricks to to Really scale your your your use of Dynamo down",
    "start": "1238000",
    "end": "1245919"
  },
  {
    "text": "down down because you understand a bit more of how it works so you know it's been uh it's been a great partnership",
    "start": "1245919",
    "end": "1251799"
  },
  {
    "text": "for us and uh I blew through those slides really really quick so we have lots of times",
    "start": "1251799",
    "end": "1257600"
  },
  {
    "text": "for questions um does anyone have any questions about what we've",
    "start": "1257600",
    "end": "1262679"
  },
  {
    "text": "done",
    "start": "1262679",
    "end": "1265679"
  },
  {
    "text": "yeah well so the question is um you know we've got traffic that scales up and",
    "start": "1276640",
    "end": "1281799"
  },
  {
    "text": "down really quick um how do we decide how many workers to have in our elastic map produce well for us",
    "start": "1281799",
    "end": "1289279"
  },
  {
    "text": "traffic is really uh targeted traffic so if there's a really big story you can",
    "start": "1289279",
    "end": "1294600"
  },
  {
    "text": "imagine that's a lot of people that are interested in that story but it's not a lot of different stories that are going",
    "start": "1294600",
    "end": "1300240"
  },
  {
    "text": "to be hit at the same time like you can imagine the the traffic to one story might go up by a th",
    "start": "1300240",
    "end": "1306360"
  },
  {
    "text": "10,000% but the number of Stories being written like different stories by different customers might only go up by",
    "start": "1306360",
    "end": "1312480"
  },
  {
    "text": "100 200% so um we actually don't need to turn on any more workers because the the",
    "start": "1312480",
    "end": "1318960"
  },
  {
    "text": "aggregation is all being done at the node.js level so those nodejs servers you know whether they count one hit per",
    "start": "1318960",
    "end": "1326200"
  },
  {
    "text": "minute on an event or whether they count a 100 hits per minute on an event the same amount of writing to Dynamo is",
    "start": "1326200",
    "end": "1332320"
  },
  {
    "text": "happening and as such the same amount of um cooking by Amazon map produce happens",
    "start": "1332320",
    "end": "1337679"
  },
  {
    "text": "so we really don't have to scale them up dynamically if if it was if we did have the problem you were talking about the",
    "start": "1337679",
    "end": "1343880"
  },
  {
    "text": "way we'd probably approach it would just be to spin up new elastic map produce instan",
    "start": "1343880",
    "end": "1349240"
  },
  {
    "text": "every every time",
    "start": "1349240",
    "end": "1354000"
  },
  {
    "text": "yeah well we actually and we've aisc some of this so the question is if we're rolling up the data to hours and days",
    "start": "1358919",
    "end": "1365120"
  },
  {
    "text": "and hours and days and months and everything how do we keep track of uniques for us we actually use the same",
    "start": "1365120",
    "end": "1371039"
  },
  {
    "text": "pattern for uniques as we use for hits and this example we we end up counting the number of uniques per node server so",
    "start": "1371039",
    "end": "1378760"
  },
  {
    "text": "if uh you know we get a total count of uniques but we don't actually understand",
    "start": "1378760",
    "end": "1383840"
  },
  {
    "text": "who the uniques are and then we write that that that uh piece of data right back into the uh node Dynamo DB the same",
    "start": "1383840",
    "end": "1391559"
  },
  {
    "text": "time we do the hits so if you think about what our what our item looks like at Dynamo there's a whole bunch of",
    "start": "1391559",
    "end": "1397640"
  },
  {
    "text": "different uh values in there the first is just how many eyeballs do I have a second would be how many unique eyeballs",
    "start": "1397640",
    "end": "1404039"
  },
  {
    "text": "do you have we actually go as far as having how many page views and how many all these other things as well so",
    "start": "1404039",
    "end": "1409840"
  },
  {
    "text": "there's really you know we've sort of simplified it here but there really is no limit to the amount of data that you can store uh in a in a Dynamo item up to",
    "start": "1409840",
    "end": "1417360"
  },
  {
    "text": "the you know I think there's one Meg cap on there yeah",
    "start": "1417360",
    "end": "1424520"
  },
  {
    "text": "yeah so great question so the question is um in our example we're just sort of worrying about totals we don't really",
    "start": "1440679",
    "end": "1447240"
  },
  {
    "text": "care about where they're coming from or anything like that that's actually a problem we solved in the last few months after after we wrote These slides and we",
    "start": "1447240",
    "end": "1454720"
  },
  {
    "text": "we really use the same pattern um so you know in this example the the ID the item",
    "start": "1454720",
    "end": "1461559"
  },
  {
    "text": "ID that we're sending back to Dynamo was just an EV an event ID for us so we know it's this event going on um the patter",
    "start": "1461559",
    "end": "1468559"
  },
  {
    "text": "we used was basically do the same thing per refer so for event 1 23 4 um this is",
    "start": "1468559",
    "end": "1474480"
  },
  {
    "text": "the amount of traffic that's coming from Google or this is the amount of traffic that's coming from Yahoo and we we wrote",
    "start": "1474480",
    "end": "1480320"
  },
  {
    "text": "that back to Dynamo on a minute-by-minute basis the exact same way so you know what the end result is",
    "start": "1480320",
    "end": "1486720"
  },
  {
    "text": "that you end up with a map reduce that is much much bigger because you've got all this data and you know we We R",
    "start": "1486720",
    "end": "1493320"
  },
  {
    "text": "definitely ran into challenges with customers that would put us for example on every page of their site you know that's just a huge amount of data that",
    "start": "1493320",
    "end": "1499960"
  },
  {
    "text": "you have to crunch through but of course with elastic map reduce the the solution to most of that is oh instead of three",
    "start": "1499960",
    "end": "1506360"
  },
  {
    "text": "workers we're just going to put in 20 and you know it just works so um we use very much the same pattern for",
    "start": "1506360",
    "end": "1514320"
  },
  {
    "text": "that",
    "start": "1514320",
    "end": "1517320"
  },
  {
    "text": "yeah so elastic uh if you use elastic map reduce and in particular if you use",
    "start": "1523640",
    "end": "1529080"
  },
  {
    "text": "uh Hadoop And Hive right in there um they've got a a connection to Dynamo",
    "start": "1529080",
    "end": "1534279"
  },
  {
    "text": "already it's actually baked into the Hadoop solution so if you look at you know the hive script that we were",
    "start": "1534279",
    "end": "1540720"
  },
  {
    "text": "using you're able to right there you're able to code against Dynamo as if it",
    "start": "1540720",
    "end": "1547240"
  },
  {
    "text": "were a SQL table so you can see here where you know this is exactly like SQL query language um but this this metrics",
    "start": "1547240",
    "end": "1555279"
  },
  {
    "text": "hourly table that we're talking about is actually a Dynamo DB table so the same",
    "start": "1555279",
    "end": "1561399"
  },
  {
    "text": "way the select is able to pull information out of Dynamo um you can do an insert back into Dynamo so in this",
    "start": "1561399",
    "end": "1568559"
  },
  {
    "text": "case we're doing a bulk insert you know the equivalent of a bulk insert for SQL but you could do it on a Case by case",
    "start": "1568559",
    "end": "1574559"
  },
  {
    "text": "basis as well you could do an you know cook data up somehow and then do insert operations it really is um you know the",
    "start": "1574559",
    "end": "1582000"
  },
  {
    "text": "same sort of syntax to connect to Dynamo and you get that and that for us that was one of the main benefits to using",
    "start": "1582000",
    "end": "1588799"
  },
  {
    "text": "um elastic map ruce versus our own Hadoop installation that you get all that for free you don't even have to",
    "start": "1588799",
    "end": "1593880"
  },
  {
    "text": "think about it um I think there was examples in their white papers on how to how to use Hive with Dynamo DB and it's",
    "start": "1593880",
    "end": "1600080"
  },
  {
    "text": "really that straightforward yeah",
    "start": "1600080",
    "end": "1606158"
  },
  {
    "text": "quite yeah",
    "start": "1612039",
    "end": "1616039"
  },
  {
    "text": "so the question is um you know we've got we're we're waiting till the end of the minute to to grab all the to add up the",
    "start": "1622919",
    "end": "1630240"
  },
  {
    "text": "data on the node.js instances uh what happens if we lose one basically by",
    "start": "1630240",
    "end": "1635279"
  },
  {
    "text": "surprise um this has been a big problem for us with auto scale actually because",
    "start": "1635279",
    "end": "1640640"
  },
  {
    "text": "Auto scale what we found is that it kills those servers really really quick and and we've played around with you",
    "start": "1640640",
    "end": "1646200"
  },
  {
    "text": "know run runtime levels and things like that to try to um you know get that data off before the machine shuts down but",
    "start": "1646200",
    "end": "1653399"
  },
  {
    "text": "really we we haven't found a way to do that if if anyone is",
    "start": "1653399",
    "end": "1659480"
  },
  {
    "text": "yeah yeah I think that would be a good pattern but so the question his comment",
    "start": "1667080",
    "end": "1672720"
  },
  {
    "text": "is what if we uh you know Ed a q or something for us you'd still have to",
    "start": "1672720",
    "end": "1677840"
  },
  {
    "text": "write it to the que so for us you know writing it to Dynamo or writing it to the que it was about the same amount of",
    "start": "1677840",
    "end": "1683600"
  },
  {
    "text": "latency which is why we were like you know let's just write it to Dynamo directly the the other you would have to",
    "start": "1683600",
    "end": "1689440"
  },
  {
    "text": "get to a pattern like that though if you actually cared about who the people were if you wanted to record every single",
    "start": "1689440",
    "end": "1694840"
  },
  {
    "text": "unique hit and get that put somewhere you definitely have to do it that way for us you know we're sort of more",
    "start": "1694840",
    "end": "1701039"
  },
  {
    "text": "talking about big numbers of people big timelines for us getting the data",
    "start": "1701039",
    "end": "1706600"
  },
  {
    "text": "quickly is way more important than um being able to do analysis of that level on it and so that that's a trade-off",
    "start": "1706600",
    "end": "1712880"
  },
  {
    "text": "that we had to make",
    "start": "1712880",
    "end": "1717760"
  },
  {
    "text": "yeah sorry are they",
    "start": "1725080",
    "end": "1728679"
  },
  {
    "text": "what yeah so the question is um are our rollup are our range queries for the rollups time consuming um",
    "start": "1731120",
    "end": "1739159"
  },
  {
    "text": "when we're talking about dat data for a day you know it's still only 24 hours times 60 minutes it's not a large amount",
    "start": "1739159",
    "end": "1745880"
  },
  {
    "text": "of data and um also we're we're talking about mostly numbers so we can grab this",
    "start": "1745880",
    "end": "1751000"
  },
  {
    "text": "data back really efficiently from Dynamo the query uh command actually has a they have a great um option where you can",
    "start": "1751000",
    "end": "1757919"
  },
  {
    "text": "return only certain information from from the uh in the resolve so for example we we can return say you we're",
    "start": "1757919",
    "end": "1764919"
  },
  {
    "text": "only interested in um that engagement minute number I told you we actually have engagement minutes and Page views",
    "start": "1764919",
    "end": "1770559"
  },
  {
    "text": "and everything else but say we only care about that engagement minute um we can say Dynamo only return me the data",
    "start": "1770559",
    "end": "1776840"
  },
  {
    "text": "points in this range and only return me the time and the uh the engagement",
    "start": "1776840",
    "end": "1782840"
  },
  {
    "text": "minute value for that time so you end up you know you're talking about um what",
    "start": "1782840",
    "end": "1788480"
  },
  {
    "text": "3,600 ins like it it's not it's not a really big dat return result and so",
    "start": "1788480",
    "end": "1794240"
  },
  {
    "text": "we're we're able to to uh you know get that back in a very very timely manner like I think it's it's well less than a",
    "start": "1794240",
    "end": "1800519"
  },
  {
    "text": "second to get all that D that data out of",
    "start": "1800519",
    "end": "1805279"
  },
  {
    "text": "Dynamo no we actually store the rows are the items in Dynamo are actually the um",
    "start": "1810840",
    "end": "1817240"
  },
  {
    "text": "item id combined with the time so for one item it doesn't have all the different times in it there's actually a",
    "start": "1817240",
    "end": "1824039"
  },
  {
    "text": "data point for the item time combination so there's a say say we're we're page X",
    "start": "1824039",
    "end": "1830519"
  },
  {
    "text": "um you know we have page X at 1:00 then we have page X at 101 then we have page X at 102 and so um the data you can grow",
    "start": "1830519",
    "end": "1838960"
  },
  {
    "text": "it as quickly as you need and also more importantly you can do those atomic ads very efficiently on all those data",
    "start": "1838960",
    "end": "1844000"
  },
  {
    "text": "points and something else that you know we didn't touch on was um say nodejs",
    "start": "1844000",
    "end": "1849960"
  },
  {
    "text": "servers all those nodejs servers running out there they'll never have clocks that are synced up 100% so you know having",
    "start": "1849960",
    "end": "1857240"
  },
  {
    "text": "the data like that your last minute of data is going to be continually written to over the course of you know maybe a",
    "start": "1857240",
    "end": "1862919"
  },
  {
    "text": "couple minutes just based on what how the clocks are ticking in each of those nodejs servers but when it all averages",
    "start": "1862919",
    "end": "1868559"
  },
  {
    "text": "out it ends up being a really smooth data graph that we're able to extract from it",
    "start": "1868559",
    "end": "1875720"
  },
  {
    "text": "yeah sure so the question is on each nodejs server where are we where are we saving those results",
    "start": "1884279",
    "end": "1890200"
  },
  {
    "text": "temporarily so for us the beauty of nodejs is that it can really take a beating like it if you know if our",
    "start": "1890200",
    "end": "1896440"
  },
  {
    "text": "traffic is scaling really suddenly it can just take that pounding and and so",
    "start": "1896440",
    "end": "1901480"
  },
  {
    "text": "with uh what we do is we actually store it in memory so we're using um the array",
    "start": "1901480",
    "end": "1907200"
  },
  {
    "text": "construct in node.js in JavaScript same thing um you know I think it is technically an array but you know you",
    "start": "1907200",
    "end": "1913399"
  },
  {
    "text": "can sort of use it like a dictionary and so we just use those to hold the data the we're we're using uh M1 medium",
    "start": "1913399",
    "end": "1920720"
  },
  {
    "text": "instances so there's plenty of memory to store a minute maybe two minutes of data in in memory if we ever hit a memory",
    "start": "1920720",
    "end": "1927720"
  },
  {
    "text": "constraint we'd probably just upgrade to the M1 larges and and increase our our limit quite a",
    "start": "1927720",
    "end": "1934559"
  },
  {
    "text": "bit Yeah so s s what's the",
    "start": "1938159",
    "end": "1942600"
  },
  {
    "text": "question it's just a ticking of the clock so if if it's if we're starting at 1:00 for that from 1:00 until 10:1 um",
    "start": "1943760",
    "end": "1953639"
  },
  {
    "text": "every single request comes in gets sort of rooted into one of those arrays and then there's another there's an",
    "start": "1953639",
    "end": "1959399"
  },
  {
    "text": "asynchronous process in nodejs that's that's ticking every minute and when it reaches that minute marker it says it's",
    "start": "1959399",
    "end": "1965519"
  },
  {
    "text": "101 that's when it starts the process of writing the data back but because at that point it actually knows I have to",
    "start": "1965519",
    "end": "1972159"
  },
  {
    "text": "do this many rights I have to do 60 rights I have 60 seconds before the next time I have to do this so I'm going to",
    "start": "1972159",
    "end": "1978919"
  },
  {
    "text": "you know do one a second and then that's a loop that we're sort of putting a timeout in",
    "start": "1978919",
    "end": "1984000"
  },
  {
    "text": "essentially yeah",
    "start": "1984000",
    "end": "1990559"
  },
  {
    "text": "sorry that's true so if no goes down um so the question is what what if no goes",
    "start": "1992440",
    "end": "1997760"
  },
  {
    "text": "down so if no goes down we're going to lose whatever the minute we're in right because um it won't be taken out",
    "start": "1997760",
    "end": "2004120"
  },
  {
    "text": "immediately because the elastic load balancers we run a health check that checks um I think it checks every 10 seconds",
    "start": "2004120",
    "end": "2010360"
  },
  {
    "text": "and if it fails three of those it's out so there is a possibility that we might",
    "start": "2010360",
    "end": "2015960"
  },
  {
    "text": "lose a minute of data the way we deal with that is by having more instances so",
    "start": "2015960",
    "end": "2021120"
  },
  {
    "text": "you know we could probably run all of everything we do on two of the largest uh instance types and you know put one",
    "start": "2021120",
    "end": "2028559"
  },
  {
    "text": "in each availability Zone and away we go but if one of those went down you we'd have 50% of our traffic first of all",
    "start": "2028559",
    "end": "2035440"
  },
  {
    "text": "that would smack the other instance which would be a they probably couldn't deal with that and we lose 50% of the",
    "start": "2035440",
    "end": "2041159"
  },
  {
    "text": "data so instead our strategy is to use really small instances and have a whole bunch of them so if we have 10 instances",
    "start": "2041159",
    "end": "2047679"
  },
  {
    "text": "and one of those goes down for some reason we're only losing nine10 of the data and that that one tenth of the load",
    "start": "2047679",
    "end": "2054679"
  },
  {
    "text": "gets spread across nine other machines so it's a much more efficient way for us to to operate the same rule applies for",
    "start": "2054679",
    "end": "2061040"
  },
  {
    "text": "availability zones you know if if we have two availability zones and one whole Zone goes out which is pretty rare",
    "start": "2061040",
    "end": "2066760"
  },
  {
    "text": "these days um that would mean all that traffic would have to reroot to one availability Zone that could be a",
    "start": "2066760",
    "end": "2072118"
  },
  {
    "text": "problem instead it's a better strategy to use as many availability zones as you can and at that point it's really just a",
    "start": "2072119",
    "end": "2078118"
  },
  {
    "text": "cost benefit analysis how much how many instances can you afford to have online to to deal with your",
    "start": "2078119",
    "end": "2085839"
  },
  {
    "text": "traffic yeah",
    "start": "2085879",
    "end": "2089878"
  },
  {
    "text": "yeah so the question is um for a hive script how are we deciding you know how we look at the next hour so if you look",
    "start": "2099400",
    "end": "2107119"
  },
  {
    "text": "at the wear Clause here we've got our where our time is greater than um that algorith that uh formula there um what",
    "start": "2107119",
    "end": "2114839"
  },
  {
    "text": "it's doing is it's actually saying go back to midnight on whatever day you are then go back 24 hours behind that point",
    "start": "2114839",
    "end": "2122599"
  },
  {
    "text": "and so you know what that means is that you know you're covering at least 24",
    "start": "2122599",
    "end": "2127680"
  },
  {
    "text": "24hour block at worst case scenario you're going to be um covering a 47h hour 59 minute block um then the same",
    "start": "2127680",
    "end": "2137119"
  },
  {
    "text": "way as in SQL this is going to do a roll up on an hourly by hourly basis so if you look by the group by Clause there um",
    "start": "2137119",
    "end": "2143760"
  },
  {
    "text": "we're actually rounding down to the hour so each data point that Hive encounters it's going to roll down to its own hour",
    "start": "2143760",
    "end": "2150160"
  },
  {
    "text": "and that's where the map reduce is happening so the the mapping is it taking all that data and going down to",
    "start": "2150160",
    "end": "2156440"
  },
  {
    "text": "the hour and then the reducing is um you know sorry it's sorry the mapping is",
    "start": "2156440",
    "end": "2162119"
  },
  {
    "text": "getting the hour per item and then the reducing is summing up those um hours",
    "start": "2162119",
    "end": "2168720"
  },
  {
    "text": "for each of the items so this algorith is actually saying say you're running it at 4:00 in the morning it's actually",
    "start": "2168720",
    "end": "2175520"
  },
  {
    "text": "coming up with a new total for 1:00 a.m. for 2: a.m. for 3:00 a.m. and for 4:00 a.m. as far as you've gone along so far",
    "start": "2175520",
    "end": "2182680"
  },
  {
    "text": "so that way we don't have to generate these dynamically another pattern would be though to actually generate this",
    "start": "2182680",
    "end": "2188880"
  },
  {
    "text": "dynamically um you know write it somewhere using a Cron job or something and then run it in Hive that would be",
    "start": "2188880",
    "end": "2194760"
  },
  {
    "text": "another strategy probably",
    "start": "2194760",
    "end": "2200520"
  },
  {
    "text": "yeah sorry the amount",
    "start": "2205560",
    "end": "2209599"
  },
  {
    "text": "of sure so um our Dynamo the question is how big are our Dynamo DB tables um we",
    "start": "2214359",
    "end": "2221880"
  },
  {
    "text": "have some tables that have billions of of points of data in them um you know terabytes of of data um our throughputs",
    "start": "2221880",
    "end": "2230079"
  },
  {
    "text": "I'm looking at Mike because he actually built this what our throughputs a couple hundred right",
    "start": "2230079",
    "end": "2235119"
  },
  {
    "text": "now yeah 100 to 200 so and really we we were in the thousands we were in the",
    "start": "2235119",
    "end": "2241560"
  },
  {
    "text": "tens of thousands of throughput we got it to come down to the hundreds by employing the strategies that we used",
    "start": "2241560",
    "end": "2247240"
  },
  {
    "text": "here as well as if you start getting huge amounts of data what happens is the map",
    "start": "2247240",
    "end": "2253560"
  },
  {
    "text": "ruce jobs start taking a long time so one pattern that you know this is a bonus pattern for everyone who's stuck",
    "start": "2253560",
    "end": "2259920"
  },
  {
    "text": "around um one pattern that we used was to actually put the minute-by-minute data into month blocks so you know we we",
    "start": "2259920",
    "end": "2268480"
  },
  {
    "text": "would have the minute data for November and then the minute data for October so",
    "start": "2268480",
    "end": "2274119"
  },
  {
    "text": "that really makes the map reduce jobs run a lot faster as well and so you know that's how we dealt with",
    "start": "2274119",
    "end": "2279880"
  },
  {
    "text": "it essentially what we found though is the the amount of data that you're",
    "start": "2279880",
    "end": "2285880"
  },
  {
    "text": "writing to Dynamo doesn't really affect anything it doesn't really matter how much data you're putting in there but",
    "start": "2285880",
    "end": "2291319"
  },
  {
    "text": "you have to make sure you're you're writing it evenly and that you're uh you know extracting it evenly like if",
    "start": "2291319",
    "end": "2298160"
  },
  {
    "text": "there's one record that you're really really pounding like everybody's going to that one record and you're hitting it you know tens of thousands of times a",
    "start": "2298160",
    "end": "2304920"
  },
  {
    "text": "second that's when the throughput of of Dynamo is going to start to be a",
    "start": "2304920",
    "end": "2310560"
  },
  {
    "text": "problem",
    "start": "2310560",
    "end": "2313560"
  },
  {
    "text": "yeah yes so the question is um when we write it back do we do it for reporting purposes so yeah that's exactly it if if",
    "start": "2320839",
    "end": "2328160"
  },
  {
    "text": "our users are looking at a day worth of data um it would take a long time for us to pull to graph all the",
    "start": "2328160",
    "end": "2334520"
  },
  {
    "text": "minute-by-minute data for that entire day and the graph would be really like jittery um so by having it stored in",
    "start": "2334520",
    "end": "2341640"
  },
  {
    "text": "those those blocks we're able to make nice graphs very very quickly because it's really just as easy as pulling the",
    "start": "2341640",
    "end": "2347560"
  },
  {
    "text": "information out of Dynamo so if I want to give you an hourly graph for the past 24 hours that's actually only four data",
    "start": "2347560",
    "end": "2353839"
  },
  {
    "text": "points that I have to get out of Dynamo so Dynamo will do that in in milliseconds and then you can draw the",
    "start": "2353839",
    "end": "2359359"
  },
  {
    "text": "graph and however long it takes the browser to draw so um it really saves us a lot of time on the rendering side on",
    "start": "2359359",
    "end": "2365480"
  },
  {
    "text": "the visualization side as as well as the the storage and Dynamo there's just not much data that we have to put in and out",
    "start": "2365480",
    "end": "2372319"
  },
  {
    "text": "the the map reduce is really doing the the bulk of the data crunching and understanding what that data is and then",
    "start": "2372319",
    "end": "2377960"
  },
  {
    "text": "we're just using the cooked up version of that other",
    "start": "2377960",
    "end": "2384920"
  },
  {
    "text": "questions yeah one in the back",
    "start": "2384920",
    "end": "2389440"
  },
  {
    "text": "so so the question is tying into Dynamo through a hi script like this um what",
    "start": "2398839",
    "end": "2404920"
  },
  {
    "text": "throughput does that affect is that a different throughput or is that the throughput on your table the the simple",
    "start": "2404920",
    "end": "2410040"
  },
  {
    "text": "answer is it's the same throughput um when you set up Hive I think by default it will use half your throughput that",
    "start": "2410040",
    "end": "2416839"
  },
  {
    "text": "you've made available so if you made a 100 available Hive will automatically use up to 50% of that at which point it",
    "start": "2416839",
    "end": "2422960"
  },
  {
    "text": "will start throttling itself so that is something that you have to think about um it it's especially important when",
    "start": "2422960",
    "end": "2430160"
  },
  {
    "text": "you're looking at you know extracting large amounts of that minute-by-minute data to do the map ruce if you're doing",
    "start": "2430160",
    "end": "2435599"
  },
  {
    "text": "that that could actually be a huge amount of your throughput that you're spending just doing the map Produce job",
    "start": "2435599",
    "end": "2440839"
  },
  {
    "text": "so um you know one way is to let your jobs take a little longer by reducing the amount of through throughput that",
    "start": "2440839",
    "end": "2447280"
  },
  {
    "text": "your map produce that your hive jobs can run um another way to do it is to just do it in off peak times so if if an",
    "start": "2447280",
    "end": "2454760"
  },
  {
    "text": "event if for us you know we're really pounding Dynamo DB there's a lot of requests coming in that might not be the",
    "start": "2454760",
    "end": "2460200"
  },
  {
    "text": "best time to start a huge map Produce job whereas if we wait to the middle of the night wherever the the country is",
    "start": "2460200",
    "end": "2465920"
  },
  {
    "text": "that's doing that traffic we might have almost 99% of that throughput left unused so you know there there's some",
    "start": "2465920",
    "end": "2472800"
  },
  {
    "text": "ways to get around it but I think it's going to depend on an implementation",
    "start": "2472800",
    "end": "2478640"
  },
  {
    "text": "basis so how do I deal with geography",
    "start": "2482160",
    "end": "2486960"
  },
  {
    "text": "yeah so the question is how do we deal with Geographic um availability for something like",
    "start": "2494119",
    "end": "2501680"
  },
  {
    "text": "this yeah so so how do we deal with you know right just having all our servers in USCS so um it's true right now we we",
    "start": "2504920",
    "end": "2513040"
  },
  {
    "text": "run out of one uh region multiple availability zones d DB is tied to one",
    "start": "2513040",
    "end": "2518599"
  },
  {
    "text": "availability zone so I think this is where if you wanted to to crack that nut you'd have to get into the pattern that",
    "start": "2518599",
    "end": "2524720"
  },
  {
    "text": "the gentleman beside you was sort of talking about where you're using a queue to process that information you're storing it somewhere else um the way I'd",
    "start": "2524720",
    "end": "2532040"
  },
  {
    "text": "probably look at it would be to write it twice into two different regions you know and actually get full duplicates of",
    "start": "2532040",
    "end": "2538280"
  },
  {
    "text": "of the data but that's for us like I said having this data um you know it's",
    "start": "2538280",
    "end": "2544280"
  },
  {
    "text": "important to us but it doesn't we're not a bank it's if it's not 100% accurate",
    "start": "2544280",
    "end": "2549359"
  },
  {
    "text": "we're happy with 99.99 so um we're only working in one region right",
    "start": "2549359",
    "end": "2556000"
  },
  {
    "text": "now all right well thank you so much for attending um you know thanks for the questions and uh if if you have any",
    "start": "2557119",
    "end": "2564359"
  },
  {
    "text": "other questions I'll be roaming the Halls I'll probably be up here for a while so come on up and say hi so thank you very much everyone",
    "start": "2564359",
    "end": "2572400"
  }
]