[
  {
    "start": "0",
    "end": "33000"
  },
  {
    "text": "[Music]",
    "start": "0",
    "end": "2250"
  },
  {
    "text": "hi",
    "start": "6000",
    "end": "6960"
  },
  {
    "text": "my name is jinyu ding and i'm a product",
    "start": "6960",
    "end": "9200"
  },
  {
    "text": "manager of a-league's database migration",
    "start": "9200",
    "end": "11280"
  },
  {
    "text": "service",
    "start": "11280",
    "end": "12719"
  },
  {
    "text": "welcome to another video of this series",
    "start": "12719",
    "end": "14799"
  },
  {
    "text": "of videos on building s3 data lake using",
    "start": "14799",
    "end": "17359"
  },
  {
    "text": "aws dms",
    "start": "17359",
    "end": "19920"
  },
  {
    "text": "in the last video suvendo discussed how",
    "start": "19920",
    "end": "22880"
  },
  {
    "text": "to configure dns in a detailed",
    "start": "22880",
    "end": "24880"
  },
  {
    "text": "demonstration",
    "start": "24880",
    "end": "26560"
  },
  {
    "text": "in this video we will go through the dns",
    "start": "26560",
    "end": "28880"
  },
  {
    "text": "best practices",
    "start": "28880",
    "end": "30880"
  },
  {
    "text": "let's get started",
    "start": "30880",
    "end": "32800"
  },
  {
    "text": "as we dive deeper to discuss using dms",
    "start": "32800",
    "end": "35760"
  },
  {
    "start": "33000",
    "end": "126000"
  },
  {
    "text": "to build data lakes",
    "start": "35760",
    "end": "37520"
  },
  {
    "text": "we want to talk about the best practices",
    "start": "37520",
    "end": "39840"
  },
  {
    "text": "to set up configuration parameters and",
    "start": "39840",
    "end": "42239"
  },
  {
    "text": "optimize dms tasks to better ingest data",
    "start": "42239",
    "end": "45440"
  },
  {
    "text": "into s3 data lakes",
    "start": "45440",
    "end": "47920"
  },
  {
    "text": "in this video we are going to cover the",
    "start": "47920",
    "end": "50079"
  },
  {
    "text": "following agenda items",
    "start": "50079",
    "end": "52800"
  },
  {
    "text": "we will start with choosing the right",
    "start": "52800",
    "end": "54719"
  },
  {
    "text": "replication instance type",
    "start": "54719",
    "end": "57120"
  },
  {
    "text": "then we will talk about s3 file formats",
    "start": "57120",
    "end": "60559"
  },
  {
    "text": "what are the supported data formats and",
    "start": "60559",
    "end": "62719"
  },
  {
    "text": "how to configure them for data lake",
    "start": "62719",
    "end": "64799"
  },
  {
    "text": "building",
    "start": "64799",
    "end": "66400"
  },
  {
    "text": "then we will talk about cdc transaction",
    "start": "66400",
    "end": "68799"
  },
  {
    "text": "order",
    "start": "68799",
    "end": "69840"
  },
  {
    "text": "how does that help with data lake",
    "start": "69840",
    "end": "71439"
  },
  {
    "text": "building and what are the different",
    "start": "71439",
    "end": "73439"
  },
  {
    "text": "options available",
    "start": "73439",
    "end": "76080"
  },
  {
    "text": "then we will talk about date-based",
    "start": "76080",
    "end": "78080"
  },
  {
    "text": "folder",
    "start": "78080",
    "end": "79280"
  },
  {
    "text": "this is a very useful way to optimize",
    "start": "79280",
    "end": "81600"
  },
  {
    "text": "the output files generated by a cdc so",
    "start": "81600",
    "end": "84880"
  },
  {
    "text": "that they can be consumed by downstream",
    "start": "84880",
    "end": "86799"
  },
  {
    "text": "applications",
    "start": "86799",
    "end": "88880"
  },
  {
    "text": "we will follow by data transformation as",
    "start": "88880",
    "end": "91280"
  },
  {
    "text": "in how can we transform the data before",
    "start": "91280",
    "end": "93759"
  },
  {
    "text": "it goes into s3 and what are the common",
    "start": "93759",
    "end": "96479"
  },
  {
    "text": "transformations we have seen customers",
    "start": "96479",
    "end": "98479"
  },
  {
    "text": "using to build data lakes",
    "start": "98479",
    "end": "102079"
  },
  {
    "text": "the final agenda item is s3 target data",
    "start": "102079",
    "end": "104960"
  },
  {
    "text": "validations",
    "start": "104960",
    "end": "106720"
  },
  {
    "text": "how can we validate the data in the data",
    "start": "106720",
    "end": "108799"
  },
  {
    "text": "lake against the source to make sure",
    "start": "108799",
    "end": "111119"
  },
  {
    "text": "that they are consistent",
    "start": "111119",
    "end": "113439"
  },
  {
    "text": "at the time of this video dms data",
    "start": "113439",
    "end": "115759"
  },
  {
    "text": "validation does not support s3 as a",
    "start": "115759",
    "end": "118240"
  },
  {
    "text": "target",
    "start": "118240",
    "end": "119520"
  },
  {
    "text": "we are going to share a typical approach",
    "start": "119520",
    "end": "121759"
  },
  {
    "text": "on how to validate data between sources",
    "start": "121759",
    "end": "124640"
  },
  {
    "text": "and s3 target",
    "start": "124640",
    "end": "127200"
  },
  {
    "start": "126000",
    "end": "173000"
  },
  {
    "text": "it is important to choose the right",
    "start": "127200",
    "end": "129440"
  },
  {
    "text": "replication instance class for any dms",
    "start": "129440",
    "end": "132239"
  },
  {
    "text": "tasks",
    "start": "132239",
    "end": "134000"
  },
  {
    "text": "aws dms currently supports the t2 t3 c4",
    "start": "134000",
    "end": "139200"
  },
  {
    "text": "c5 r4 and r5 amazon ec2 instance classes",
    "start": "139200",
    "end": "145040"
  },
  {
    "text": "you should use t2 and t3 instance",
    "start": "145040",
    "end": "147680"
  },
  {
    "text": "classes for development and testing",
    "start": "147680",
    "end": "150080"
  },
  {
    "text": "workload",
    "start": "150080",
    "end": "151360"
  },
  {
    "text": "but avoid them for production workload",
    "start": "151360",
    "end": "154959"
  },
  {
    "text": "c4 and c5 instance classes are compute",
    "start": "154959",
    "end": "158160"
  },
  {
    "text": "intensive",
    "start": "158160",
    "end": "159280"
  },
  {
    "text": "and are good choices for heterogeneous",
    "start": "159280",
    "end": "161519"
  },
  {
    "text": "migrations and replications",
    "start": "161519",
    "end": "165120"
  },
  {
    "text": "r4 and r5 instance classes are memory",
    "start": "165120",
    "end": "168000"
  },
  {
    "text": "optimized",
    "start": "168000",
    "end": "169200"
  },
  {
    "text": "and are suitable for heavy workloads",
    "start": "169200",
    "end": "171519"
  },
  {
    "text": "with high throughputs",
    "start": "171519",
    "end": "173519"
  },
  {
    "start": "173000",
    "end": "245000"
  },
  {
    "text": "now let's talk about how does dms work",
    "start": "173519",
    "end": "176480"
  },
  {
    "text": "with s3 target in general",
    "start": "176480",
    "end": "179920"
  },
  {
    "text": "when dms runs for the first time it will",
    "start": "179920",
    "end": "182640"
  },
  {
    "text": "create an initial bulk dump file of all",
    "start": "182640",
    "end": "185360"
  },
  {
    "text": "records in the database with the format",
    "start": "185360",
    "end": "187920"
  },
  {
    "text": "shown in the slide as follows files",
    "start": "187920",
    "end": "191280"
  },
  {
    "text": "which will have your specified working",
    "start": "191280",
    "end": "193120"
  },
  {
    "text": "name schema name and then table name",
    "start": "193120",
    "end": "197840"
  },
  {
    "text": "default",
    "start": "197840",
    "end": "198959"
  },
  {
    "text": "all followed files will be in the csv",
    "start": "198959",
    "end": "201440"
  },
  {
    "text": "format",
    "start": "201440",
    "end": "203280"
  },
  {
    "text": "there are other file formats available",
    "start": "203280",
    "end": "205519"
  },
  {
    "text": "like parquet which we'll discuss in next",
    "start": "205519",
    "end": "207920"
  },
  {
    "text": "slide",
    "start": "207920",
    "end": "210159"
  },
  {
    "text": "once full load is complete dms will",
    "start": "210159",
    "end": "213040"
  },
  {
    "text": "start generating cdc files for the table",
    "start": "213040",
    "end": "216239"
  },
  {
    "text": "which has a similar schema with the",
    "start": "216239",
    "end": "218239"
  },
  {
    "text": "addition of the type of operation",
    "start": "218239",
    "end": "220400"
  },
  {
    "text": "performed",
    "start": "220400",
    "end": "221920"
  },
  {
    "text": "i is for insert u for update and d for",
    "start": "221920",
    "end": "225200"
  },
  {
    "text": "delete",
    "start": "225200",
    "end": "227440"
  },
  {
    "text": "it's a common pattern to have one job",
    "start": "227440",
    "end": "229680"
  },
  {
    "text": "running that performs the bulk plus cdc",
    "start": "229680",
    "end": "232560"
  },
  {
    "text": "operation",
    "start": "232560",
    "end": "233920"
  },
  {
    "text": "and another that does a less frequent",
    "start": "233920",
    "end": "236159"
  },
  {
    "text": "bulk dump each time as a way to create",
    "start": "236159",
    "end": "239040"
  },
  {
    "text": "checkpoints and re-synchronize the data",
    "start": "239040",
    "end": "242080"
  },
  {
    "text": "if something goes wrong with downstream",
    "start": "242080",
    "end": "244319"
  },
  {
    "text": "processing",
    "start": "244319",
    "end": "246080"
  },
  {
    "start": "245000",
    "end": "393000"
  },
  {
    "text": "let's now talk about s3 file formats",
    "start": "246080",
    "end": "249680"
  },
  {
    "text": "the default file format is csv",
    "start": "249680",
    "end": "252560"
  },
  {
    "text": "however this is a configurable behavior",
    "start": "252560",
    "end": "255280"
  },
  {
    "text": "controlled by extra connection",
    "start": "255280",
    "end": "256799"
  },
  {
    "text": "attributes",
    "start": "256799",
    "end": "257528"
  },
  {
    "text": "[Music]",
    "start": "257529",
    "end": "259199"
  },
  {
    "text": "extra connection attributes or eca are",
    "start": "259199",
    "end": "262240"
  },
  {
    "text": "settings specific to source and target",
    "start": "262240",
    "end": "264320"
  },
  {
    "text": "endpoints to help customers customize",
    "start": "264320",
    "end": "267120"
  },
  {
    "text": "how dms interacts with their endpoints",
    "start": "267120",
    "end": "271199"
  },
  {
    "text": "every single source and target endpoints",
    "start": "271199",
    "end": "273280"
  },
  {
    "text": "has its own eca",
    "start": "273280",
    "end": "275199"
  },
  {
    "text": "which details can be found in the dms",
    "start": "275199",
    "end": "277520"
  },
  {
    "text": "public documentation",
    "start": "277520",
    "end": "280400"
  },
  {
    "text": "for data lake use cases",
    "start": "280400",
    "end": "282400"
  },
  {
    "text": "the most commonly recommended file",
    "start": "282400",
    "end": "284000"
  },
  {
    "text": "format is parquet",
    "start": "284000",
    "end": "285919"
  },
  {
    "text": "since it's a columnar format best",
    "start": "285919",
    "end": "288000"
  },
  {
    "text": "optimized for storage and data",
    "start": "288000",
    "end": "290160"
  },
  {
    "text": "processing",
    "start": "290160",
    "end": "292400"
  },
  {
    "text": "parquet has the ability to store file",
    "start": "292400",
    "end": "295040"
  },
  {
    "text": "schema as well as the option to store",
    "start": "295040",
    "end": "297680"
  },
  {
    "text": "column level statistics like minimum",
    "start": "297680",
    "end": "300479"
  },
  {
    "text": "maximum",
    "start": "300479",
    "end": "301600"
  },
  {
    "text": "count of nulls and distinct values",
    "start": "301600",
    "end": "304530"
  },
  {
    "text": "[Music]",
    "start": "304530",
    "end": "305759"
  },
  {
    "text": "another benefit of using parking format",
    "start": "305759",
    "end": "308639"
  },
  {
    "text": "is its ability to have higher",
    "start": "308639",
    "end": "310320"
  },
  {
    "text": "compression ratio as compared to csv",
    "start": "310320",
    "end": "312960"
  },
  {
    "text": "format",
    "start": "312960",
    "end": "315360"
  },
  {
    "text": "in order to set this data format for dms",
    "start": "315520",
    "end": "318560"
  },
  {
    "text": "you can set that in target endpoint",
    "start": "318560",
    "end": "320720"
  },
  {
    "text": "using eca data format",
    "start": "320720",
    "end": "323039"
  },
  {
    "text": "and set the value to park it",
    "start": "323039",
    "end": "326080"
  },
  {
    "text": "there are other eca's relevant to s3",
    "start": "326080",
    "end": "328479"
  },
  {
    "text": "file formats as well",
    "start": "328479",
    "end": "330639"
  },
  {
    "text": "for example eca dig page size limit",
    "start": "330639",
    "end": "334240"
  },
  {
    "text": "sets the maximum allowed size in bytes",
    "start": "334240",
    "end": "337520"
  },
  {
    "text": "for a dictionary page in a proxy file",
    "start": "337520",
    "end": "341680"
  },
  {
    "text": "if a dictionary page exceeds its value",
    "start": "341680",
    "end": "344240"
  },
  {
    "text": "that page uses plain coding",
    "start": "344240",
    "end": "348000"
  },
  {
    "text": "eca enabled statistics",
    "start": "348000",
    "end": "350560"
  },
  {
    "text": "enables statistics about the project",
    "start": "350560",
    "end": "352720"
  },
  {
    "text": "file pages and row groups",
    "start": "352720",
    "end": "355440"
  },
  {
    "text": "this allows column level statistics to",
    "start": "355440",
    "end": "357919"
  },
  {
    "text": "be stored as metadata with parque files",
    "start": "357919",
    "end": "362080"
  },
  {
    "text": "this is particularly helpful when you",
    "start": "362080",
    "end": "364160"
  },
  {
    "text": "use spark for data processing due to",
    "start": "364160",
    "end": "366639"
  },
  {
    "text": "optimizations such as predicate push",
    "start": "366639",
    "end": "368880"
  },
  {
    "text": "down",
    "start": "368880",
    "end": "369290"
  },
  {
    "text": "[Music]",
    "start": "369290",
    "end": "371360"
  },
  {
    "text": "the last eca is parking timestamp in",
    "start": "371360",
    "end": "374160"
  },
  {
    "text": "millisecond",
    "start": "374160",
    "end": "376080"
  },
  {
    "text": "by default dms use microseconds for",
    "start": "376080",
    "end": "378960"
  },
  {
    "text": "precision",
    "start": "378960",
    "end": "380639"
  },
  {
    "text": "by using this parameter dms write",
    "start": "380639",
    "end": "383280"
  },
  {
    "text": "timestamp in milliseconds and that's",
    "start": "383280",
    "end": "385840"
  },
  {
    "text": "what aws glue and amazon athena can",
    "start": "385840",
    "end": "388400"
  },
  {
    "text": "handle",
    "start": "388400",
    "end": "389600"
  },
  {
    "text": "otherwise it will cause potential data",
    "start": "389600",
    "end": "391919"
  },
  {
    "text": "issues",
    "start": "391919",
    "end": "393759"
  },
  {
    "start": "393000",
    "end": "461000"
  },
  {
    "text": "now let's look into cdc transaction",
    "start": "393759",
    "end": "395840"
  },
  {
    "text": "order",
    "start": "395840",
    "end": "397199"
  },
  {
    "text": "by default new cdc data for each table",
    "start": "397199",
    "end": "400800"
  },
  {
    "text": "is stored in the prefix of that table",
    "start": "400800",
    "end": "404400"
  },
  {
    "text": "what i mean by that is as new data comes",
    "start": "404400",
    "end": "406960"
  },
  {
    "text": "in for the table",
    "start": "406960",
    "end": "408560"
  },
  {
    "text": "data will be stored in your s3 bucket",
    "start": "408560",
    "end": "411199"
  },
  {
    "text": "slash schema name slash table name",
    "start": "411199",
    "end": "414960"
  },
  {
    "text": "however many times when building data",
    "start": "414960",
    "end": "417680"
  },
  {
    "text": "lakes there are downstream etl processes",
    "start": "417680",
    "end": "420880"
  },
  {
    "text": "which need to have a point in time view",
    "start": "420880",
    "end": "422880"
  },
  {
    "text": "of tables and preserve the order of",
    "start": "422880",
    "end": "425360"
  },
  {
    "text": "transactions as they happen on source",
    "start": "425360",
    "end": "427599"
  },
  {
    "text": "databases",
    "start": "427599",
    "end": "429599"
  },
  {
    "text": "within dms you can set eca preserve",
    "start": "429599",
    "end": "432960"
  },
  {
    "text": "transactions to chew",
    "start": "432960",
    "end": "435199"
  },
  {
    "text": "and then dms will start to record",
    "start": "435199",
    "end": "437360"
  },
  {
    "text": "transactions from all tables together in",
    "start": "437360",
    "end": "440080"
  },
  {
    "text": "such an order as they happen on source",
    "start": "440080",
    "end": "442319"
  },
  {
    "text": "databases",
    "start": "442319",
    "end": "444880"
  },
  {
    "text": "one limitation is that the changes are",
    "start": "444880",
    "end": "447280"
  },
  {
    "text": "only supported in csv format",
    "start": "447280",
    "end": "450880"
  },
  {
    "text": "next level etl job will have to consume",
    "start": "450880",
    "end": "453520"
  },
  {
    "text": "this data and create a merged version of",
    "start": "453520",
    "end": "456639"
  },
  {
    "text": "downstream tables to build a pointing",
    "start": "456639",
    "end": "458960"
  },
  {
    "text": "time view of source tables",
    "start": "458960",
    "end": "461680"
  },
  {
    "start": "461000",
    "end": "533000"
  },
  {
    "text": "as we just learned",
    "start": "461680",
    "end": "463199"
  },
  {
    "text": "all new cdc data will be stored in the",
    "start": "463199",
    "end": "465520"
  },
  {
    "text": "table level prefix",
    "start": "465520",
    "end": "468160"
  },
  {
    "text": "let's say you have a use case where you",
    "start": "468160",
    "end": "470720"
  },
  {
    "text": "want to preserve transaction orders",
    "start": "470720",
    "end": "473440"
  },
  {
    "text": "then all data of each individual table",
    "start": "473440",
    "end": "476000"
  },
  {
    "text": "will be stored in a single prefix",
    "start": "476000",
    "end": "478639"
  },
  {
    "text": "which is bucket name",
    "start": "478639",
    "end": "480560"
  },
  {
    "text": "schema name slash table name",
    "start": "480560",
    "end": "483759"
  },
  {
    "text": "there will be no partitions at the table",
    "start": "483759",
    "end": "485680"
  },
  {
    "text": "level",
    "start": "485680",
    "end": "487759"
  },
  {
    "text": "for accessing tables with huge data",
    "start": "487759",
    "end": "489840"
  },
  {
    "text": "volume this will cause performance",
    "start": "489840",
    "end": "492160"
  },
  {
    "text": "challenges since etl jobs will have to",
    "start": "492160",
    "end": "494879"
  },
  {
    "text": "process or scan all data at once",
    "start": "494879",
    "end": "498720"
  },
  {
    "text": "a better approach is to partition data",
    "start": "498720",
    "end": "501840"
  },
  {
    "text": "and it's usually based on transaction",
    "start": "501840",
    "end": "503840"
  },
  {
    "text": "committee",
    "start": "503840",
    "end": "506720"
  },
  {
    "text": "using date-based folder partitioning you",
    "start": "506720",
    "end": "509120"
  },
  {
    "text": "can write data from a single source",
    "start": "509120",
    "end": "510960"
  },
  {
    "text": "table to a time hierarchy folder",
    "start": "510960",
    "end": "513200"
  },
  {
    "text": "structure in a s3 bucket",
    "start": "513200",
    "end": "516080"
  },
  {
    "text": "by partitioning folders you can better",
    "start": "516080",
    "end": "518320"
  },
  {
    "text": "manage your s3 objects and limit the",
    "start": "518320",
    "end": "520719"
  },
  {
    "text": "size of each s3 folder",
    "start": "520719",
    "end": "524080"
  },
  {
    "text": "in order to configure this you enable",
    "start": "524080",
    "end": "526399"
  },
  {
    "text": "eca date partition enabled and date",
    "start": "526399",
    "end": "529760"
  },
  {
    "text": "partition sequence",
    "start": "529760",
    "end": "531360"
  },
  {
    "text": "as shown in the",
    "start": "531360",
    "end": "532839"
  },
  {
    "text": "slide let's continue on with data",
    "start": "532839",
    "end": "535680"
  },
  {
    "start": "533000",
    "end": "602000"
  },
  {
    "text": "transformations",
    "start": "535680",
    "end": "537360"
  },
  {
    "text": "with dms you can use operations and",
    "start": "537360",
    "end": "540240"
  },
  {
    "text": "transformations to enhance the data as",
    "start": "540240",
    "end": "542959"
  },
  {
    "text": "part of the replication process",
    "start": "542959",
    "end": "545519"
  },
  {
    "text": "for example there is an inconsistency",
    "start": "545519",
    "end": "547920"
  },
  {
    "text": "between cdc files and full load files",
    "start": "547920",
    "end": "550959"
  },
  {
    "text": "as cdc files has an additional column",
    "start": "550959",
    "end": "553600"
  },
  {
    "text": "for operation code i u and d",
    "start": "553600",
    "end": "557839"
  },
  {
    "text": "to make both files consistent you can",
    "start": "557839",
    "end": "560240"
  },
  {
    "text": "add an additional column operation code",
    "start": "560240",
    "end": "563040"
  },
  {
    "text": "with value i in front of all followed",
    "start": "563040",
    "end": "565760"
  },
  {
    "text": "rows",
    "start": "565760",
    "end": "567519"
  },
  {
    "text": "this will make it easier for downstream",
    "start": "567519",
    "end": "569360"
  },
  {
    "text": "etl jobs",
    "start": "569360",
    "end": "572080"
  },
  {
    "text": "with data transformation you can filter",
    "start": "572080",
    "end": "574399"
  },
  {
    "text": "your data selection to only load new",
    "start": "574399",
    "end": "576800"
  },
  {
    "text": "data into s3 target",
    "start": "576800",
    "end": "579360"
  },
  {
    "text": "you can use expressions to flag records",
    "start": "579360",
    "end": "582080"
  },
  {
    "text": "on target tables as inserted updated or",
    "start": "582080",
    "end": "585200"
  },
  {
    "text": "deleted at the source",
    "start": "585200",
    "end": "587760"
  },
  {
    "text": "you can also rename a column or add a",
    "start": "587760",
    "end": "590160"
  },
  {
    "text": "new column with default values as part",
    "start": "590160",
    "end": "592640"
  },
  {
    "text": "of the data ingestion",
    "start": "592640",
    "end": "595680"
  },
  {
    "text": "data transformation brings more",
    "start": "595680",
    "end": "597680"
  },
  {
    "text": "flexibility in data processing to make",
    "start": "597680",
    "end": "600480"
  },
  {
    "text": "downstream processing easier",
    "start": "600480",
    "end": "603360"
  },
  {
    "start": "602000",
    "end": "664000"
  },
  {
    "text": "lastly",
    "start": "603360",
    "end": "604399"
  },
  {
    "text": "let's talk about data validation",
    "start": "604399",
    "end": "607440"
  },
  {
    "text": "as part of any replication it is",
    "start": "607440",
    "end": "609680"
  },
  {
    "text": "important to validate or compare the",
    "start": "609680",
    "end": "612160"
  },
  {
    "text": "data between source and target",
    "start": "612160",
    "end": "615839"
  },
  {
    "text": "at the time of this video dms has the",
    "start": "615839",
    "end": "618480"
  },
  {
    "text": "option to validate data between",
    "start": "618480",
    "end": "620560"
  },
  {
    "text": "relational databases but not with s3 as",
    "start": "620560",
    "end": "623680"
  },
  {
    "text": "a target",
    "start": "623680",
    "end": "626000"
  },
  {
    "text": "to validate your s3 target data yourself",
    "start": "626000",
    "end": "628880"
  },
  {
    "text": "one approach is to perform some etl or",
    "start": "628880",
    "end": "632079"
  },
  {
    "text": "use something like apache hoodie tables",
    "start": "632079",
    "end": "634399"
  },
  {
    "text": "to create a merge table using cdc",
    "start": "634399",
    "end": "637040"
  },
  {
    "text": "incremental files",
    "start": "637040",
    "end": "639839"
  },
  {
    "text": "this table will be brought available in",
    "start": "639839",
    "end": "642079"
  },
  {
    "text": "glue catalog",
    "start": "642079",
    "end": "643680"
  },
  {
    "text": "from there",
    "start": "643680",
    "end": "644720"
  },
  {
    "text": "data in s3 can be accessed by athena",
    "start": "644720",
    "end": "648640"
  },
  {
    "text": "and you can use athena federated query",
    "start": "648640",
    "end": "651200"
  },
  {
    "text": "to join source data with s3 data and",
    "start": "651200",
    "end": "654320"
  },
  {
    "text": "perform validation",
    "start": "654320",
    "end": "657279"
  },
  {
    "text": "there are other customized ways to",
    "start": "657279",
    "end": "659360"
  },
  {
    "text": "validate s3 data",
    "start": "659360",
    "end": "661120"
  },
  {
    "text": "but this architecture is very commonly",
    "start": "661120",
    "end": "663440"
  },
  {
    "text": "used",
    "start": "663440",
    "end": "664800"
  },
  {
    "start": "664000",
    "end": "687000"
  },
  {
    "text": "thank you very much for watching",
    "start": "664800",
    "end": "667040"
  },
  {
    "text": "in this video we learned about best",
    "start": "667040",
    "end": "669279"
  },
  {
    "text": "practices using dms to build data lakes",
    "start": "669279",
    "end": "673120"
  },
  {
    "text": "in the next video suvendu will talk",
    "start": "673120",
    "end": "675519"
  },
  {
    "text": "about the best practices specific to s3",
    "start": "675519",
    "end": "678330"
  },
  {
    "text": "[Music]",
    "start": "678330",
    "end": "679760"
  },
  {
    "text": "we'll see you in the next video",
    "start": "679760",
    "end": "684040"
  }
]