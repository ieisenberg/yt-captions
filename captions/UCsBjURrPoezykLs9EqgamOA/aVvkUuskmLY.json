[
  {
    "text": "when Mark Zuckerberg isn't wake surfing",
    "start": "80",
    "end": "1920"
  },
  {
    "text": "wearing a tuxedo and a puka shell",
    "start": "1920",
    "end": "3679"
  },
  {
    "text": "necklace at his Lake Taho Mansion",
    "start": "3679",
    "end": "5400"
  },
  {
    "text": "crushing K's yellow bellies and waving",
    "start": "5400",
    "end": "7120"
  },
  {
    "text": "the American flag he clocks into work",
    "start": "7120",
    "end": "8880"
  },
  {
    "text": "with the sunburn to battle Google and",
    "start": "8880",
    "end": "10519"
  },
  {
    "text": "open AI for artificial intelligence",
    "start": "10519",
    "end": "12639"
  },
  {
    "text": "Supremacy yesterday meta released its",
    "start": "12639",
    "end": "14920"
  },
  {
    "text": "biggest and baddest large language model",
    "start": "14920",
    "end": "16720"
  },
  {
    "text": "ever which also happens to be free and",
    "start": "16720",
    "end": "18920"
  },
  {
    "text": "arguably open source it took months to",
    "start": "18920",
    "end": "21039"
  },
  {
    "text": "train on 16,000 Nvidia h100 gpus which",
    "start": "21039",
    "end": "24760"
  },
  {
    "text": "likely cost hundreds of millions of",
    "start": "24760",
    "end": "26199"
  },
  {
    "text": "dollars and used enough electricity to",
    "start": "26199",
    "end": "28119"
  },
  {
    "text": "power a small country but the end result",
    "start": "28119",
    "end": "30199"
  },
  {
    "text": "is a massive 405 billion parameter model",
    "start": "30199",
    "end": "33079"
  },
  {
    "text": "with a 128,000 token contacts length",
    "start": "33079",
    "end": "35760"
  },
  {
    "text": "which according to benchmarks is mostly",
    "start": "35760",
    "end": "37600"
  },
  {
    "text": "Superior to open ai's GPT 40 and even",
    "start": "37600",
    "end": "40520"
  },
  {
    "text": "beats Claude 3.5 Sonet on some key",
    "start": "40520",
    "end": "42879"
  },
  {
    "text": "benchmarks but benchmarks lie and the",
    "start": "42879",
    "end": "45000"
  },
  {
    "text": "only way to find out if a new model is",
    "start": "45000",
    "end": "46520"
  },
  {
    "text": "any good is to Vibe with it in today's",
    "start": "46520",
    "end": "48640"
  },
  {
    "text": "video we'll try out llama 3.1 heavy and",
    "start": "48640",
    "end": "51120"
  },
  {
    "text": "find out if it actually doesn't suck",
    "start": "51120",
    "end": "52680"
  },
  {
    "text": "like most meta products it is July 24th",
    "start": "52680",
    "end": "55079"
  },
  {
    "text": "2024 and you're watching the code report",
    "start": "55079",
    "end": "57440"
  },
  {
    "text": "AI hype has died down a lot recently and",
    "start": "57440",
    "end": "59480"
  },
  {
    "text": "it's been almost a week since I've",
    "start": "59480",
    "end": "60680"
  },
  {
    "text": "mentioned it in a video which I'm",
    "start": "60680",
    "end": "62000"
  },
  {
    "text": "extremely proud of but llama 3.1 is a",
    "start": "62000",
    "end": "64478"
  },
  {
    "text": "model that cannot be ignored it comes in",
    "start": "64479",
    "end": "66280"
  },
  {
    "text": "three sizes 8B 70b and 405b where B",
    "start": "66280",
    "end": "70400"
  },
  {
    "text": "refers to billions of parameters or the",
    "start": "70400",
    "end": "72520"
  },
  {
    "text": "variables that the model can use to make",
    "start": "72520",
    "end": "74159"
  },
  {
    "text": "predictions in general more parameters",
    "start": "74159",
    "end": "76200"
  },
  {
    "text": "can capture more complex patterns but",
    "start": "76200",
    "end": "78240"
  },
  {
    "text": "more parameters doesn't always mean that",
    "start": "78240",
    "end": "80000"
  },
  {
    "text": "the model is better GPT 4 has been",
    "start": "80000",
    "end": "82000"
  },
  {
    "text": "rumored to have over 1 trillion",
    "start": "82000",
    "end": "83439"
  },
  {
    "text": "parameters but we don't really know the",
    "start": "83439",
    "end": "84799"
  },
  {
    "text": "true numbers from companies like open Ai",
    "start": "84799",
    "end": "86640"
  },
  {
    "text": "and anthropic the cool thing about llama",
    "start": "86640",
    "end": "88640"
  },
  {
    "text": "is that it's open source well kind of",
    "start": "88640",
    "end": "90600"
  },
  {
    "text": "you can make money off of it as long as",
    "start": "90600",
    "end": "92159"
  },
  {
    "text": "your app doesn't have 700 million",
    "start": "92159",
    "end": "93759"
  },
  {
    "text": "monthly active users in which case you",
    "start": "93759",
    "end": "95600"
  },
  {
    "text": "need to request a license from meta",
    "start": "95600",
    "end": "97240"
  },
  {
    "text": "what's not open source is the training",
    "start": "97240",
    "end": "98680"
  },
  {
    "text": "data which might include your blog your",
    "start": "98680",
    "end": "100439"
  },
  {
    "text": "GitHub repos all your Facebook posts",
    "start": "100439",
    "end": "102320"
  },
  {
    "text": "from 2006 and maybe even your WhatsApp",
    "start": "102320",
    "end": "104600"
  },
  {
    "text": "messages what's interesting is that we",
    "start": "104600",
    "end": "106280"
  },
  {
    "text": "can take a look at the actual code used",
    "start": "106280",
    "end": "107799"
  },
  {
    "text": "to train this model which is only 300",
    "start": "107799",
    "end": "109840"
  },
  {
    "text": "lines of python and pytorch along with a",
    "start": "109840",
    "end": "112119"
  },
  {
    "text": "library called Fair scale to distribute",
    "start": "112119",
    "end": "113920"
  },
  {
    "text": "training across multiple gpus it's a",
    "start": "113920",
    "end": "116039"
  },
  {
    "text": "relatively simple decoder only",
    "start": "116039",
    "end": "117759"
  },
  {
    "text": "Transformer as opposed to the mixture of",
    "start": "117759",
    "end": "119920"
  },
  {
    "text": "experts approach used in a lot of other",
    "start": "119920",
    "end": "121439"
  },
  {
    "text": "big models like its biggest open source",
    "start": "121439",
    "end": "123360"
  },
  {
    "text": "rival mixol most importantly though the",
    "start": "123360",
    "end": "125560"
  },
  {
    "text": "model weights are open and that's a huge",
    "start": "125560",
    "end": "127360"
  },
  {
    "text": "win for developers building AI powered",
    "start": "127360",
    "end": "129360"
  },
  {
    "text": "apps and now you don't have to pay a",
    "start": "129360",
    "end": "130720"
  },
  {
    "text": "bunch of money to use the gbt 4 API and",
    "start": "130720",
    "end": "133239"
  },
  {
    "text": "instead can self-host your own model and",
    "start": "133239",
    "end": "134959"
  },
  {
    "text": "pay a cloud provider a bunch of money to",
    "start": "134959",
    "end": "136599"
  },
  {
    "text": "rent some gpus the big model would not",
    "start": "136599",
    "end": "138560"
  },
  {
    "text": "be cheap to self host I used olama to",
    "start": "138560",
    "end": "140840"
  },
  {
    "text": "download it and use it locally but the",
    "start": "140840",
    "end": "142640"
  },
  {
    "text": "weights weigh 230 GB and even with an",
    "start": "142640",
    "end": "145200"
  },
  {
    "text": "RTX 490 I wasn't able to ride this llama",
    "start": "145200",
    "end": "148280"
  },
  {
    "text": "the good news though is that you can try",
    "start": "148280",
    "end": "149560"
  },
  {
    "text": "it for free on platforms like meta or",
    "start": "149560",
    "end": "152360"
  },
  {
    "text": "platforms like Gro or nvidia's",
    "start": "152360",
    "end": "154160"
  },
  {
    "text": "Playground now the initial feedback from",
    "start": "154160",
    "end": "156120"
  },
  {
    "text": "random weirdos on the Internet is that",
    "start": "156120",
    "end": "157920"
  },
  {
    "text": "big llama is somewhat disappointing",
    "start": "157920",
    "end": "159640"
  },
  {
    "text": "while the smaller llamas are quite",
    "start": "159640",
    "end": "160959"
  },
  {
    "text": "impressive but the real power of llama",
    "start": "160959",
    "end": "162959"
  },
  {
    "text": "is that it can be fine-tuned with custom",
    "start": "162959",
    "end": "164640"
  },
  {
    "text": "data and in the near future will have",
    "start": "164640",
    "end": "166400"
  },
  {
    "text": "some amazing uncensored fine-tune models",
    "start": "166400",
    "end": "168680"
  },
  {
    "text": "like dolphin my favorite test for new",
    "start": "168680",
    "end": "170480"
  },
  {
    "text": "llms is to ask it to build a spelt 5 web",
    "start": "170480",
    "end": "173440"
  },
  {
    "text": "application with runes which is a new",
    "start": "173440",
    "end": "175640"
  },
  {
    "text": "yet to be released feature the only",
    "start": "175640",
    "end": "177360"
  },
  {
    "text": "model I've seen do this correctly in a",
    "start": "177360",
    "end": "178760"
  },
  {
    "text": "single shot is CL 3.5 son it and llama",
    "start": "178760",
    "end": "181599"
  },
  {
    "text": "405b failed pretty miserably and seems",
    "start": "181599",
    "end": "183920"
  },
  {
    "text": "to have no awareness of this feature",
    "start": "183920",
    "end": "185720"
  },
  {
    "text": "overall though it is pretty decent en",
    "start": "185720",
    "end": "187319"
  },
  {
    "text": "coding but still clearly behind Claude I",
    "start": "187319",
    "end": "189560"
  },
  {
    "text": "also had it do some creative writing and",
    "start": "189560",
    "end": "191200"
  },
  {
    "text": "poetry and overall it's pretty good just",
    "start": "191200",
    "end": "193480"
  },
  {
    "text": "not the best I've ever seen if we take a",
    "start": "193480",
    "end": "195159"
  },
  {
    "text": "minute to reflect though what's crazy is",
    "start": "195159",
    "end": "197200"
  },
  {
    "text": "that we have multiple different",
    "start": "197200",
    "end": "198400"
  },
  {
    "text": "companies that have trained massive",
    "start": "198400",
    "end": "199959"
  },
  {
    "text": "models with massive computers and",
    "start": "199959",
    "end": "201760"
  },
  {
    "text": "they're all plateauing at the same level",
    "start": "201760",
    "end": "203440"
  },
  {
    "text": "of capability open AI was the first to",
    "start": "203440",
    "end": "205480"
  },
  {
    "text": "make a huge leap from gpt3 to GPT 4 but",
    "start": "205480",
    "end": "208680"
  },
  {
    "text": "since then it's only been small",
    "start": "208680",
    "end": "210120"
  },
  {
    "text": "incremental gains last year Sam ultman",
    "start": "210120",
    "end": "212439"
  },
  {
    "text": "practically begged the government to",
    "start": "212439",
    "end": "213760"
  },
  {
    "text": "regulate AI to protect Humanity but a",
    "start": "213760",
    "end": "215920"
  },
  {
    "text": "year later we still haven't seen the",
    "start": "215920",
    "end": "217640"
  },
  {
    "text": "apocalyptic Skynet human extinction",
    "start": "217640",
    "end": "219640"
  },
  {
    "text": "event that they promised us I mean AI",
    "start": "219640",
    "end": "221599"
  },
  {
    "text": "still hasn't even replaced programmers",
    "start": "221599",
    "end": "223360"
  },
  {
    "text": "it's like that time airplanes went from",
    "start": "223360",
    "end": "225040"
  },
  {
    "text": "propellers to jet engines but the",
    "start": "225040",
    "end": "226840"
  },
  {
    "text": "advancement to light speed engines never",
    "start": "226840",
    "end": "228560"
  },
  {
    "text": "happened when talking about llms",
    "start": "228560",
    "end": "230680"
  },
  {
    "text": "artificial super intelligence is still",
    "start": "230680",
    "end": "232599"
  },
  {
    "text": "nowhere in sight except in the",
    "start": "232599",
    "end": "234000"
  },
  {
    "text": "imagination of the Silicon Valley Mafia",
    "start": "234000",
    "end": "236439"
  },
  {
    "text": "it feels wrong to say this but meta is",
    "start": "236439",
    "end": "238519"
  },
  {
    "text": "really the only big tech company keeping",
    "start": "238519",
    "end": "240159"
  },
  {
    "text": "it real in the AI space I'm sure there's",
    "start": "240159",
    "end": "242000"
  },
  {
    "text": "an evil ulterior motive hidden in there",
    "start": "242000",
    "end": "243760"
  },
  {
    "text": "somewhere but llama is one small step",
    "start": "243760",
    "end": "245760"
  },
  {
    "text": "for man one giant leap for Zuckerberg's",
    "start": "245760",
    "end": "248079"
  },
  {
    "text": "Redemption Arc this has been the code",
    "start": "248079",
    "end": "249599"
  },
  {
    "text": "report thanks for watching and I will",
    "start": "249599",
    "end": "251360"
  },
  {
    "text": "see you in the next one",
    "start": "251360",
    "end": "254519"
  }
]