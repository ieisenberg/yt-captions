[
  {
    "text": "Cuda a parallel Computing platform that",
    "start": "280",
    "end": "2760"
  },
  {
    "text": "allows you to use your GPU for more than",
    "start": "2760",
    "end": "5080"
  },
  {
    "text": "just playing video games compute unified",
    "start": "5080",
    "end": "7319"
  },
  {
    "text": "device architecture was developed by",
    "start": "7319",
    "end": "9200"
  },
  {
    "text": "Nvidia in 2007 based on the prior work",
    "start": "9200",
    "end": "11719"
  },
  {
    "text": "of Ian buck and John Nichols since then",
    "start": "11719",
    "end": "14080"
  },
  {
    "text": "Cuda has revolutionized the World by",
    "start": "14080",
    "end": "16000"
  },
  {
    "text": "allowing humans to compute large blocks",
    "start": "16000",
    "end": "17800"
  },
  {
    "text": "of data in parallel which is unlock the",
    "start": "17800",
    "end": "19840"
  },
  {
    "text": "true potential of the deep neural",
    "start": "19840",
    "end": "21359"
  },
  {
    "text": "networks behind artificial intelligence",
    "start": "21359",
    "end": "23359"
  },
  {
    "text": "the graphics Processing Unit or GPU is",
    "start": "23359",
    "end": "25840"
  },
  {
    "text": "historically used for what the name",
    "start": "25840",
    "end": "27320"
  },
  {
    "text": "implies to compute Graphics when you",
    "start": "27320",
    "end": "29320"
  },
  {
    "text": "play a game in 1080p at 60 FPS you've",
    "start": "29320",
    "end": "31960"
  },
  {
    "text": "got over 2 million pixels on the screen",
    "start": "31960",
    "end": "33920"
  },
  {
    "text": "that may need to be recalculated after",
    "start": "33920",
    "end": "35559"
  },
  {
    "text": "every frame which requires Hardware that",
    "start": "35559",
    "end": "37320"
  },
  {
    "text": "can do a lot of matrix multiplication",
    "start": "37320",
    "end": "39360"
  },
  {
    "text": "and Vector transformations in parallel",
    "start": "39360",
    "end": "41360"
  },
  {
    "text": "and I mean a lot modern gpus are",
    "start": "41360",
    "end": "43200"
  },
  {
    "text": "measured in teraflops or how many",
    "start": "43200",
    "end": "45039"
  },
  {
    "text": "trillions of floating Point operations",
    "start": "45039",
    "end": "46840"
  },
  {
    "text": "can it handle per second unlike modern",
    "start": "46840",
    "end": "48680"
  },
  {
    "text": "CPUs like the Intel I9 which has 24",
    "start": "48680",
    "end": "51399"
  },
  {
    "text": "cores a modern GPU like the RTX 490 has",
    "start": "51399",
    "end": "54920"
  },
  {
    "text": "over 16,000 cores a CPU is designed to",
    "start": "54920",
    "end": "57840"
  },
  {
    "text": "be versatile while a GPU is designed to",
    "start": "57840",
    "end": "60120"
  },
  {
    "text": "go really fast in parallel Cuda allows",
    "start": "60120",
    "end": "62199"
  },
  {
    "text": "developers to tap into the gpu's power",
    "start": "62199",
    "end": "64439"
  },
  {
    "text": "and data scientists all around the world",
    "start": "64439",
    "end": "66040"
  },
  {
    "text": "are using at this very moment trying to",
    "start": "66040",
    "end": "67960"
  },
  {
    "text": "train the most powerful machine learning",
    "start": "67960",
    "end": "69520"
  },
  {
    "text": "models it works like this you write a",
    "start": "69520",
    "end": "71479"
  },
  {
    "text": "function called a Cuda kernel that runs",
    "start": "71479",
    "end": "73560"
  },
  {
    "text": "on the GPU you then copy some data from",
    "start": "73560",
    "end": "75680"
  },
  {
    "text": "your main Ram over to the gpu's memory",
    "start": "75680",
    "end": "78119"
  },
  {
    "text": "then the CPU will tell the GPU to",
    "start": "78119",
    "end": "80200"
  },
  {
    "text": "execute that function or kernel in",
    "start": "80200",
    "end": "82000"
  },
  {
    "text": "parallel the code is executed in a block",
    "start": "82000",
    "end": "84240"
  },
  {
    "text": "which itself organizes threads into a",
    "start": "84240",
    "end": "86040"
  },
  {
    "text": "multi-dimensional grid then the final",
    "start": "86040",
    "end": "87920"
  },
  {
    "text": "result from the GPU is copied back to",
    "start": "87920",
    "end": "90040"
  },
  {
    "text": "the main memory a piece of cake let's go",
    "start": "90040",
    "end": "91960"
  },
  {
    "text": "ahead and build a Cuda application right",
    "start": "91960",
    "end": "93759"
  },
  {
    "text": "now first you'll need an Nvidia GPU then",
    "start": "93759",
    "end": "96119"
  },
  {
    "text": "install the Cuda toolkit Cuda includes",
    "start": "96119",
    "end": "98240"
  },
  {
    "text": "device drivers a runtime compilers and",
    "start": "98240",
    "end": "100560"
  },
  {
    "text": "Dev tools but the actual code is most",
    "start": "100560",
    "end": "102479"
  },
  {
    "text": "often written in C++ as I'm doing here",
    "start": "102479",
    "end": "104920"
  },
  {
    "text": "in Visual Studio first we use the global",
    "start": "104920",
    "end": "107079"
  },
  {
    "text": "specifier to define a function or Cuda",
    "start": "107079",
    "end": "109320"
  },
  {
    "text": "kernel that runs on the actual GPU this",
    "start": "109320",
    "end": "111840"
  },
  {
    "text": "function adds two vectors or arrays",
    "start": "111840",
    "end": "114040"
  },
  {
    "text": "together it takes pointer arguments A",
    "start": "114040",
    "end": "115880"
  },
  {
    "text": "and B which are the two vectors to be",
    "start": "115880",
    "end": "117560"
  },
  {
    "text": "added together and pointer C for the",
    "start": "117560",
    "end": "119479"
  },
  {
    "text": "result C equals a plus b but because",
    "start": "119479",
    "end": "122000"
  },
  {
    "text": "hypothetically we're doing billions of",
    "start": "122000",
    "end": "123560"
  },
  {
    "text": "operations in parallel we need to",
    "start": "123560",
    "end": "125240"
  },
  {
    "text": "calculate the global index of the thread",
    "start": "125240",
    "end": "127200"
  },
  {
    "text": "in the block that we're working on from",
    "start": "127200",
    "end": "128840"
  },
  {
    "text": "there we can use managed which tells",
    "start": "128840",
    "end": "130679"
  },
  {
    "text": "Cuda this data can be accessed from both",
    "start": "130679",
    "end": "132680"
  },
  {
    "text": "the host CPU and the device GPU without",
    "start": "132680",
    "end": "135480"
  },
  {
    "text": "the need to manually copy data between",
    "start": "135480",
    "end": "137440"
  },
  {
    "text": "them and now we can write a main",
    "start": "137440",
    "end": "138760"
  },
  {
    "text": "function for the CPU that runs the Cuda",
    "start": "138760",
    "end": "140879"
  },
  {
    "text": "kernel we use a for Loop to initialize",
    "start": "140879",
    "end": "142760"
  },
  {
    "text": "our arrays with data then from there we",
    "start": "142760",
    "end": "144879"
  },
  {
    "text": "pass this data to the ad function to run",
    "start": "144879",
    "end": "147040"
  },
  {
    "text": "it on the GPU but you might be wondering",
    "start": "147040",
    "end": "149160"
  },
  {
    "text": "what these weird triple brackets are",
    "start": "149160",
    "end": "150840"
  },
  {
    "text": "they allow us to configure the Cuda",
    "start": "150840",
    "end": "152319"
  },
  {
    "text": "kernel launch to control how many blocks",
    "start": "152319",
    "end": "154480"
  },
  {
    "text": "and how many threads per block are used",
    "start": "154480",
    "end": "156160"
  },
  {
    "text": "to run this code in parallel and that's",
    "start": "156160",
    "end": "157599"
  },
  {
    "text": "crucial for optimizing multi-dimensional",
    "start": "157599",
    "end": "159640"
  },
  {
    "text": "data structures like tensors used in",
    "start": "159640",
    "end": "161519"
  },
  {
    "text": "deep learning from there Cuda device",
    "start": "161519",
    "end": "163319"
  },
  {
    "text": "synchronize will pause the execution of",
    "start": "163319",
    "end": "165319"
  },
  {
    "text": "this code and wait for it to complete on",
    "start": "165319",
    "end": "167120"
  },
  {
    "text": "the GPU when it finishes and copies the",
    "start": "167120",
    "end": "169480"
  },
  {
    "text": "data back to the host machine we can",
    "start": "169480",
    "end": "171040"
  },
  {
    "text": "then use the result and print it to the",
    "start": "171040",
    "end": "172680"
  },
  {
    "text": "standard output now let's execute this",
    "start": "172680",
    "end": "174599"
  },
  {
    "text": "code with Auda compiler by clicking the",
    "start": "174599",
    "end": "176480"
  },
  {
    "text": "play button congratulations you just ran",
    "start": "176480",
    "end": "178560"
  },
  {
    "text": "256 threads in parallel on your GPU but",
    "start": "178560",
    "end": "181560"
  },
  {
    "text": "if you want to go beyond nvidia's GTC",
    "start": "181560",
    "end": "183599"
  },
  {
    "text": "conference is coming up in a few weeks",
    "start": "183599",
    "end": "185280"
  },
  {
    "text": "it's free to attend virtually featuring",
    "start": "185280",
    "end": "187000"
  },
  {
    "text": "talks about building massive parallel",
    "start": "187000",
    "end": "188879"
  },
  {
    "text": "systems with Cuda thanks for watching",
    "start": "188879",
    "end": "190840"
  },
  {
    "text": "and I will see you in the next one",
    "start": "190840",
    "end": "194400"
  }
]