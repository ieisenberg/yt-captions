[
  {
    "start": "0",
    "end": "160000"
  },
  {
    "text": "so i read this article around a month ago it's titled h.a proxy reaches",
    "start": "80",
    "end": "8400"
  },
  {
    "text": "2 million requests per second and i took a while to",
    "start": "8400",
    "end": "16240"
  },
  {
    "text": "process that article i actually read it and understand it a month later picking this article up",
    "start": "16240",
    "end": "22800"
  },
  {
    "text": "again and reading it again led me to many other questions",
    "start": "22800",
    "end": "30080"
  },
  {
    "text": "right which i attempt to answer so i thought maybe this worth talking about it in the",
    "start": "30080",
    "end": "36960"
  },
  {
    "text": "podcast and sharing my thoughts on this because first of all what does two",
    "start": "36960",
    "end": "44559"
  },
  {
    "text": "million requests really mean right it's like what machine can produce",
    "start": "44559",
    "end": "53760"
  },
  {
    "text": "two million requests first in a benchmark environment it's like that's a lot",
    "start": "53760",
    "end": "59120"
  },
  {
    "text": "right and how does that affect the networking stack and how does the operating system handle",
    "start": "59120",
    "end": "66479"
  },
  {
    "text": "such a large quantity and is this the largest that a proxy or or a server can handle",
    "start": "66479",
    "end": "73920"
  },
  {
    "text": "two million requests per second right just imagine how much that is right",
    "start": "73920",
    "end": "80880"
  },
  {
    "text": "obviously as a summary this is the best case scenario",
    "start": "80880",
    "end": "87600"
  },
  {
    "text": "right that we're demonstrating i'm going to read the article and then detail and talk",
    "start": "87600",
    "end": "93360"
  },
  {
    "text": "about the details about this right but that's what i'm going to talk about in this video",
    "start": "93360",
    "end": "98640"
  },
  {
    "text": "i'm going to i'm gonna illustrate what's the meaning of 2 million requests per",
    "start": "98640",
    "end": "103680"
  },
  {
    "text": "second specifically this is http what kind of hardware did the haproxy team use to",
    "start": "103680",
    "end": "112240"
  },
  {
    "text": "demonstrate that and uh what's exactly a request contained here initially of course what",
    "start": "112240",
    "end": "118159"
  },
  {
    "text": "does it have does it have tls there's how many connections and all these questions we'll attempt to answer them",
    "start": "118159",
    "end": "124560"
  },
  {
    "text": "as we progress through the episode if you're interested stay tuned welcome to the backend engineering show",
    "start": "124560",
    "end": "130720"
  },
  {
    "text": "with your host hussein nasser and this is the uh the show where we talk",
    "start": "130720",
    "end": "137120"
  },
  {
    "text": "about proxies back-end technologies databases security",
    "start": "137120",
    "end": "144000"
  },
  {
    "text": "uh backend frameworks web servers all kind of back in the",
    "start": "144000",
    "end": "151200"
  },
  {
    "text": "stuff right so if you like this stuff make sure to subscribe and check out the podcast",
    "start": "151200",
    "end": "156480"
  },
  {
    "text": "how about we jump into it guys so what i want to do here is take few minutes to summarize the article",
    "start": "156480",
    "end": "162319"
  },
  {
    "start": "160000",
    "end": "715000"
  },
  {
    "text": "so for that for people who are just interested to know what is what this thing is and then i'm going to",
    "start": "162319",
    "end": "168160"
  },
  {
    "text": "jump into the screen and then actually read blurbs of the article and explain in details",
    "start": "168160",
    "end": "175280"
  },
  {
    "text": "and give my thoughts about each portion because i'm i'm just blown away about",
    "start": "175280",
    "end": "181200"
  },
  {
    "text": "the things i learned about from this article things that you really don't think about you just take for",
    "start": "181200",
    "end": "187519"
  },
  {
    "text": "granted or just that maybe just me like the number of packets",
    "start": "187519",
    "end": "192560"
  },
  {
    "text": "per per per per second right we always take a think of a request",
    "start": "192560",
    "end": "197840"
  },
  {
    "text": "really it's just a request but if you translate that into into networking",
    "start": "197840",
    "end": "205519"
  },
  {
    "text": "speak that goes into the osi model and eventually end up into the layer two",
    "start": "205519",
    "end": "212000"
  },
  {
    "text": "which is a frame and that has a bunch of bytes and there is some sort of an amplification effect so if you send",
    "start": "212000",
    "end": "219920"
  },
  {
    "text": "at layer seven one byte of request what do you call an http request",
    "start": "219920",
    "end": "227120"
  },
  {
    "text": "right it's a very small request i don't think it you can send it one byte request it's a get request to get slash maybe that's the",
    "start": "227120",
    "end": "233920"
  },
  {
    "text": "smallest request that you can send it get amplified as you get down the stack",
    "start": "233920",
    "end": "241840"
  },
  {
    "text": "because the tcp stack adds its own header the ip stack adds its own header the frames",
    "start": "241840",
    "end": "248720"
  },
  {
    "text": "at the mac layer two layer adds its own header and it sums up i think it says as it was",
    "start": "248720",
    "end": "254720"
  },
  {
    "text": "64 bytes minimum the cost of doing business right so think about that 64 byte",
    "start": "254720",
    "end": "262880"
  },
  {
    "text": "leaves your network and who who really sends it out the operating system right and and",
    "start": "262880",
    "end": "270720"
  },
  {
    "text": "how does the pipe that is leaving your machine sending i'm talking about just sending",
    "start": "270720",
    "end": "276400"
  },
  {
    "text": "that request how does that pipe how much bandwidth can handle and that's",
    "start": "276400",
    "end": "282000"
  },
  {
    "text": "all comes down to what your isp allows right and that's across the internet but",
    "start": "282000",
    "end": "289360"
  },
  {
    "text": "if within a network itself what's your network card allows as an output input right uh",
    "start": "289360",
    "end": "296400"
  },
  {
    "text": "i mean my machine here is what i think uh i'm around one gigabit per second",
    "start": "296400",
    "end": "303919"
  },
  {
    "text": "some network card allowed 10 gigabit per second and there are networking configurations",
    "start": "303919",
    "end": "311199"
  },
  {
    "text": "even in the cloud you can configure to be 100 gigabit per second close uh",
    "start": "311199",
    "end": "318800"
  },
  {
    "text": "servers can take advantage of this high bandwidth so if i say 100 gigabits per second",
    "start": "318800",
    "end": "326880"
  },
  {
    "text": "right it made me think how many packets are those how many packets can 100 gigabits per",
    "start": "326880",
    "end": "333600"
  },
  {
    "text": "second really achieve and as a result",
    "start": "333600",
    "end": "338639"
  },
  {
    "text": "how many packets does not directly translate to how many requests right because the request can have many",
    "start": "338639",
    "end": "344960"
  },
  {
    "text": "packets so all of this is explained in in a very nice detail really",
    "start": "344960",
    "end": "351199"
  },
  {
    "text": "aj proxy community really did a good job explaining all",
    "start": "351199",
    "end": "356400"
  },
  {
    "text": "that but i really really love it but",
    "start": "356400",
    "end": "361919"
  },
  {
    "text": "let's summarize what what what the hi proxy team did here they said here's what i'm going to do",
    "start": "361919",
    "end": "368400"
  },
  {
    "text": "i'm going to assume don't assume they actually did that a",
    "start": "368400",
    "end": "373600"
  },
  {
    "text": "request initiative request is equal one packet so they made that request so tiny",
    "start": "373600",
    "end": "381440"
  },
  {
    "text": "uh such that it fits in a single packet and i believe what they mean by packet here ip",
    "start": "381440",
    "end": "387280"
  },
  {
    "text": "packets right and the response that comes back from",
    "start": "387280",
    "end": "393120"
  },
  {
    "text": "the server is also fits into one packet with a few bytes or not",
    "start": "393120",
    "end": "398960"
  },
  {
    "text": "it's just an empty response and you might say i'm saying what kind of test is this that real life scenarios is never",
    "start": "398960",
    "end": "406400"
  },
  {
    "text": "one one packet right at a quest we send all those nasty cookies and then headers",
    "start": "406400",
    "end": "411919"
  },
  {
    "text": "and and then if it's a post request we send all this body information and we upload",
    "start": "411919",
    "end": "416960"
  },
  {
    "text": "files and stuff so it's easily seven eight ten packets",
    "start": "416960",
    "end": "422479"
  },
  {
    "text": "right per request this is demonstrating the best case scenario which is not",
    "start": "422479",
    "end": "428319"
  },
  {
    "text": "wrong it's just what they are telling you it's like okay this is what you can take maximum benefits",
    "start": "428319",
    "end": "433680"
  },
  {
    "text": "and i kind of like that so okay very clear right another block they will not tell",
    "start": "433680",
    "end": "439280"
  },
  {
    "text": "you this kind of details this is really beautiful details so our request is equal to one packet",
    "start": "439280",
    "end": "446000"
  },
  {
    "text": "all right and now if you think about it",
    "start": "446000",
    "end": "451039"
  },
  {
    "text": "they have to demonstrate h a proxy as a proxy they place three machines",
    "start": "451039",
    "end": "458639"
  },
  {
    "text": "right there's the back end which is a server that is specific benchmark they teamed up by the",
    "start": "458639",
    "end": "464400"
  },
  {
    "text": "way with the nginx uh community and team to build this",
    "start": "464400",
    "end": "470160"
  },
  {
    "text": "tuning benchmarking tool that's called dv march i believe db bench i believe and they built up",
    "start": "470160",
    "end": "477280"
  },
  {
    "text": "there's a beautiful benchmarking tool that since allows you to send a lot of requests",
    "start": "477280",
    "end": "484160"
  },
  {
    "text": "allows you to spin up multiple servers on the same port obviously right you just might say can",
    "start": "484160",
    "end": "491520"
  },
  {
    "text": "how can you spin up multiple servers on the same port that just isn't that just a port conflict no linux",
    "start": "491520",
    "end": "498800"
  },
  {
    "text": "allows it right linux does the it's on load balancing if you enable this",
    "start": "498800",
    "end": "503919"
  },
  {
    "text": "shared object reuse port you can listen on multiple processes on the same port and",
    "start": "503919",
    "end": "510160"
  },
  {
    "text": "the kernel will load balance packets that comes to that port across",
    "start": "510160",
    "end": "516320"
  },
  {
    "text": "these processes send packets let's be very clever careful there you can't",
    "start": "516320",
    "end": "521680"
  },
  {
    "text": "back balance any package right otherwise as a packet coming for a single",
    "start": "521680",
    "end": "528399"
  },
  {
    "text": "connection might go to another process and it will will get confused this is stateful protocol right",
    "start": "528399",
    "end": "533440"
  },
  {
    "text": "tcp all right so that's what they did so they brought it a machine that is so badass a 64",
    "start": "533440",
    "end": "540320"
  },
  {
    "text": "core or machine from amazon right and they brought three instances",
    "start": "540320",
    "end": "546800"
  },
  {
    "text": "of that one is the backend server one is the proxy and one is the client that producing",
    "start": "546800",
    "end": "552399"
  },
  {
    "text": "these two million requests and they managed to do it there's a lot",
    "start": "552399",
    "end": "557760"
  },
  {
    "text": "of configuration details a lot guys so much configuration details there",
    "start": "557760",
    "end": "562880"
  },
  {
    "text": "they want to to achieve that because think about it like how can you send",
    "start": "562880",
    "end": "571120"
  },
  {
    "text": "two million requests per second from a client you have to spin up concurrent",
    "start": "571120",
    "end": "577680"
  },
  {
    "text": "processes that allow you to push this kind of data out right from your",
    "start": "577680",
    "end": "583519"
  },
  {
    "text": "machine first and the networking interrupt at the proxy level under the server level",
    "start": "583519",
    "end": "590160"
  },
  {
    "text": "should be able to receive that and they achieved a fantastic latency",
    "start": "590160",
    "end": "596720"
  },
  {
    "text": "around last time i checked i don't have the article right now but it's 500 microseconds no millisecond",
    "start": "596720",
    "end": "603920"
  },
  {
    "text": "microsecond so half a millisecond right and let's at speak i think 99.9 latency",
    "start": "603920",
    "end": "610480"
  },
  {
    "text": "it was 64 milliseconds so it's a little bit you can see the delay but that's in",
    "start": "610480",
    "end": "616720"
  },
  {
    "text": "proxy to uh proxy 2.3 in the newer version they slash this",
    "start": "616720",
    "end": "622320"
  },
  {
    "text": "down to two milliseconds or whatever or six milliseconds so overall",
    "start": "622320",
    "end": "627600"
  },
  {
    "text": "it's really really nice i got by by the way guys i'm not sponsored of a chair proxy or anything i just love",
    "start": "627600",
    "end": "634079"
  },
  {
    "text": "this article and i absolutely will comment on it right just take it as a face value it's really",
    "start": "634079",
    "end": "641600"
  },
  {
    "text": "beautiful stuff here so two million",
    "start": "641600",
    "end": "647519"
  },
  {
    "text": "requests per second right they won't throw different alterations to achieve to get to the point where they can send",
    "start": "647519",
    "end": "655120"
  },
  {
    "text": "two million request http request per second right and then you might say",
    "start": "655120",
    "end": "660880"
  },
  {
    "text": "i'm saying is that a single tcp connection what does that mean right what does it mean http request",
    "start": "660880",
    "end": "666240"
  },
  {
    "text": "right that's the same question i had first okay how many connections are we opening",
    "start": "666240",
    "end": "671600"
  },
  {
    "text": "they detailed that as well it's around thousand connections only thousand connection between the client",
    "start": "671600",
    "end": "677279"
  },
  {
    "text": "and the proxy and and then concurrently multiplexing these requests",
    "start": "677279",
    "end": "685360"
  },
  {
    "text": "in those tcp connections right and i had so many other questions as",
    "start": "685360",
    "end": "690640"
  },
  {
    "text": "well okay what if we use hdb2 i believe we're going to get a drop when we use http 2",
    "start": "690640",
    "end": "696240"
  },
  {
    "text": "personally if we multiplex these requests but that's in general that's what they have",
    "start": "696240",
    "end": "701440"
  },
  {
    "text": "they managed to do for the details how about we actually go to the blog article",
    "start": "701440",
    "end": "707600"
  },
  {
    "text": "and start piecing it one piece by piece and then go into the details right",
    "start": "707600",
    "end": "714079"
  },
  {
    "text": "let's jump into it all right how about we jump into the article guys",
    "start": "714079",
    "end": "719360"
  },
  {
    "start": "715000",
    "end": "1260000"
  },
  {
    "text": "aha proxy forwards focus on the word forwards over two million",
    "start": "719360",
    "end": "725920"
  },
  {
    "text": "http requests request per second on a single arm-based aws graviton 2",
    "start": "725920",
    "end": "732800"
  },
  {
    "text": "instance written by willy tarrou april 8 2021 so it's almost last month",
    "start": "732800",
    "end": "740240"
  },
  {
    "text": "right so let's read the how they started doing that a few weeks ago while i was working",
    "start": "740240",
    "end": "747839"
  },
  {
    "text": "on an h.a proxy issue related to thread locking contention i found myself running some",
    "start": "747839",
    "end": "753120"
  },
  {
    "text": "tests on a server with an eight core 16 thread intel xion processor that we have in the lab",
    "start": "753120",
    "end": "760800"
  },
  {
    "text": "although my intention wasn't to do benchmark the proxy i've observed hip proxy reached 1.03",
    "start": "760800",
    "end": "768160"
  },
  {
    "text": "million hdb requests per second so they reached 1 million hdb requests",
    "start": "768160",
    "end": "773279"
  },
  {
    "text": "per second while he was debugging unrelated issue on an eight core",
    "start": "773279",
    "end": "778639"
  },
  {
    "text": "machine that's impressive right one million request if you can serve one",
    "start": "778639",
    "end": "785200"
  },
  {
    "text": "million requests right and per second that is fantastic obviously",
    "start": "785200",
    "end": "793519"
  },
  {
    "text": "these are tiny requests that takes quick responses but most production",
    "start": "793519",
    "end": "798720"
  },
  {
    "text": "request responses take more than that right if you're creating a database so you have to minimize the",
    "start": "798720",
    "end": "804160"
  },
  {
    "text": "latency as as much as possible right very very critical these are they",
    "start": "804160",
    "end": "809920"
  },
  {
    "text": "they measure the overhead of the proxy itself right and and",
    "start": "809920",
    "end": "816160"
  },
  {
    "text": "i forgot to mention they also mentioned the direct latency here through from the client to the direct server as",
    "start": "816160",
    "end": "821920"
  },
  {
    "text": "well i suddenly recorded a lot of times that i told people around me",
    "start": "821920",
    "end": "827040"
  },
  {
    "text": "the day we cross one million requests per second barrier are right about it so i have stand by my",
    "start": "827040",
    "end": "833600"
  },
  {
    "text": "promise so what is writing about it i wanted to see how that would scale on more cores",
    "start": "833600",
    "end": "841360"
  },
  {
    "text": "so on eighth chord we got a one million do we get on on 64. do we get more do we get eight",
    "start": "841360",
    "end": "848560"
  },
  {
    "text": "times one could we get eight million obviously i hope i wish it can it works like that",
    "start": "848560",
    "end": "854160"
  },
  {
    "text": "but fortunately no there is so much stuff guys that the operating system",
    "start": "854160",
    "end": "860480"
  },
  {
    "text": "does that kind of hinders this process right because all these packets",
    "start": "860480",
    "end": "866000"
  },
  {
    "text": "that are arrived you know the operating system has to sniff them have to parse them has to convert them from the",
    "start": "866000",
    "end": "874079"
  },
  {
    "text": "single layer to the data layer to the tcp stack and then deliver them to the app so that is latency and need it needs",
    "start": "874079",
    "end": "881199"
  },
  {
    "text": "machine it needs cores to achieve that thing obviously right",
    "start": "881199",
    "end": "886639"
  },
  {
    "text": "so they're going to explain in a minute that even though they got they got 64 grafton",
    "start": "886639",
    "end": "892000"
  },
  {
    "text": "graviton core they could not use purely all the 64 cores",
    "start": "892000",
    "end": "897519"
  },
  {
    "text": "for hda proxy it's impossible because you need free cores to i don't know to have like few cores",
    "start": "897519",
    "end": "905199"
  },
  {
    "text": "to to for the network interrupts right to act to listen to these packets that comes like you need the",
    "start": "905199",
    "end": "911600"
  },
  {
    "text": "operating system to give it something right and you need other uh cores for",
    "start": "911600",
    "end": "918160"
  },
  {
    "text": "like ssh because you're gonna eventually get an ssh into the thing you need to look at your results right so here's a",
    "start": "918160",
    "end": "924480"
  },
  {
    "text": "decision you need some other course dedicated to other stuff as well right",
    "start": "924480",
    "end": "929519"
  },
  {
    "text": "right so these 64 cores to give you an idea their design each core uses its own l2 cache and",
    "start": "929519",
    "end": "937120"
  },
  {
    "text": "there is a single l3 cache shared by all the cores so much beautiful a single cache 64. so",
    "start": "937120",
    "end": "945680"
  },
  {
    "text": "if i hit this core right and i cache it on my l3 and i hit the same almost very similar",
    "start": "945680",
    "end": "953279"
  },
  {
    "text": "content or data on that other core i can benefit from",
    "start": "953279",
    "end": "958639"
  },
  {
    "text": "the cache powerful stuff right",
    "start": "958639",
    "end": "963600"
  },
  {
    "text": "let's continue i have been extremely impressed by them he's talking about the the machine",
    "start": "964000",
    "end": "970079"
  },
  {
    "text": "itself here going through then and then going through his steps like okay how do you if you want to compile hip proxy to",
    "start": "970079",
    "end": "975839"
  },
  {
    "text": "enable arm here's how you do it i'm not really interested in that but syn synposes yes you've read the",
    "start": "975839",
    "end": "983440"
  },
  {
    "text": "title of the block right ha proxy version 2.3 when tested on arm based aws graviton",
    "start": "983440",
    "end": "989040"
  },
  {
    "text": "instances reaches 2.04 million requests per second per second",
    "start": "989040",
    "end": "997040"
  },
  {
    "text": "guys i don't know i don't know if nobody is as impressed as much as i am but per second in a second",
    "start": "997040",
    "end": "1005440"
  },
  {
    "text": "two million request has been processed we're talking request and response we're talking end",
    "start": "1005440",
    "end": "1012639"
  },
  {
    "text": "to end here guys end to end two million requests in a given second",
    "start": "1012639",
    "end": "1019040"
  },
  {
    "text": "process like that proxy 2.4 which is still under",
    "start": "1019759",
    "end": "1026160"
  },
  {
    "text": "development surpasses that reaching 2.07 and 2.08 it's not that great it's like i",
    "start": "1026160",
    "end": "1032880"
  },
  {
    "text": "mean how much is that another another 70 000.",
    "start": "1032880",
    "end": "1038558"
  },
  {
    "text": "another another forty thousand request uh well he do better i'm joking obviously",
    "start": "1038559",
    "end": "1047438"
  },
  {
    "text": "two point two million is a lot i mean an additional forty million is not a game but 2.4 did fix a major",
    "start": "1047439",
    "end": "1054640"
  },
  {
    "text": "bug looks like it the latency went the the 99 percentile",
    "start": "1054640",
    "end": "1060160"
  },
  {
    "text": "latency went severely down when 2.4 they fixed it right they they had in two point three a",
    "start": "1060160",
    "end": "1067440"
  },
  {
    "text": "j proxy they had like a 68 99.99 per latency",
    "start": "1067440",
    "end": "1073360"
  },
  {
    "text": "and they dropped that down to six millisecond",
    "start": "1073360",
    "end": "1078559"
  },
  {
    "text": "great obviously all right so let's look through the graph here",
    "start": "1078559",
    "end": "1085440"
  },
  {
    "text": "and for people listening in the podcast i don't know how you listen in the podcast at the podcast on the podcast",
    "start": "1085440",
    "end": "1092000"
  },
  {
    "text": "i don't know i don't know how to speak english we're looking at a graph here with four lines",
    "start": "1092000",
    "end": "1099679"
  },
  {
    "text": "there is the line that shows us the direct request per second and it",
    "start": "1099679",
    "end": "1108080"
  },
  {
    "text": "shows us also the latency and we talk about direct there's the client there's the proxy and there's the",
    "start": "1108080",
    "end": "1113280"
  },
  {
    "text": "backend server they for for to test the direct latency",
    "start": "1113280",
    "end": "1118559"
  },
  {
    "text": "they connected the client directly to the server that's obviously faster it's going to be faster there is no hi box there is no proxy in the middle",
    "start": "1118559",
    "end": "1126000"
  },
  {
    "text": "taking some precious milliseconds from you or micro seconds right there's there is",
    "start": "1126000",
    "end": "1131039"
  },
  {
    "text": "no middle man to take this hit if you go directly it's",
    "start": "1131039",
    "end": "1136080"
  },
  {
    "text": "always faster they show you how much the latency and we're looking at around 180 get this",
    "start": "1136080",
    "end": "1144480"
  },
  {
    "text": "micro second not millisecond mic 180 micro second a round trip a request and response",
    "start": "1144480",
    "end": "1152240"
  },
  {
    "text": "directly 180. so a request is taking an average of 180 right and",
    "start": "1152240",
    "end": "1157760"
  },
  {
    "text": "they're sending around seven 750 000 requests per second",
    "start": "1157760",
    "end": "1163440"
  },
  {
    "text": "and what you might say jose that's very low that's intentionally low because they",
    "start": "1163440",
    "end": "1169200"
  },
  {
    "text": "dedicated few cores on the client machine for this direct connection to test these",
    "start": "1169200",
    "end": "1177039"
  },
  {
    "text": "numbers right because they they want to do some sort of a pivot and test like uh what do they call this they",
    "start": "1177039",
    "end": "1184400"
  },
  {
    "text": "they want a fixed variable to fit test against direct versus the proxy so we're looking at 180 around 750",
    "start": "1184400",
    "end": "1191520"
  },
  {
    "text": "requests but that's very low because it's not compared to them 2 million because they didn't give us as much core as the",
    "start": "1191520",
    "end": "1197600"
  },
  {
    "text": "proxy let's take a look at the proxy the blue purple line here the proxy",
    "start": "1197600",
    "end": "1202880"
  },
  {
    "text": "looking at the latency of the proxy here and we're looking at 600 microseconds not that bad it's",
    "start": "1202880",
    "end": "1209679"
  },
  {
    "text": "almost half a millisecond right half a millisecond for our complete round trip from decline",
    "start": "1209679",
    "end": "1216559"
  },
  {
    "text": "through a cha proxy and we're talking about obviously a layer seven proxy here right not layer",
    "start": "1216559",
    "end": "1222559"
  },
  {
    "text": "four all the way to the back end coming back to the client 600 microsecond fantastic",
    "start": "1222559",
    "end": "1230960"
  },
  {
    "text": "and then that gives you two point around two million requests per second",
    "start": "1230960",
    "end": "1238559"
  },
  {
    "text": "sweet right so this is good and there's a fourth line here we're",
    "start": "1238559",
    "end": "1243919"
  },
  {
    "text": "looking at the connection concurrent number of connections and we're looking at a thousand a",
    "start": "1243919",
    "end": "1251200"
  },
  {
    "text": "hundred one hundred one thousand one hundred fifty tcp connections right and this is the connection between the client and",
    "start": "1251200",
    "end": "1257200"
  },
  {
    "text": "the proxy because at the end of the day you can't just send requests nearly in this is not udb right",
    "start": "1257200",
    "end": "1262559"
  },
  {
    "start": "1260000",
    "end": "1680000"
  },
  {
    "text": "you have to establish a connection and funnel the request inside the tcp connection and they had",
    "start": "1262559",
    "end": "1269600"
  },
  {
    "text": "1 000 only 1 000 connections to work with right which is to me it's very few right",
    "start": "1269600",
    "end": "1276640"
  },
  {
    "text": "usually when you receive two million this is one one point i don't know if they uh uh if",
    "start": "1276640",
    "end": "1284000"
  },
  {
    "text": "a proxy team mentioned this i didn't read it but when you get two million requests per",
    "start": "1284000",
    "end": "1289840"
  },
  {
    "text": "second from direct clients i'm talking if h a proxy if he proxy is an ingress that is facing",
    "start": "1289840",
    "end": "1298000"
  },
  {
    "text": "the ward the internet it's not gonna get a thousand connection if it's receiving two million requests",
    "start": "1298000",
    "end": "1304799"
  },
  {
    "text": "it's impossible if you're getting two million requests you're gonna get it from almost distinct connections you're gonna get it",
    "start": "1304799",
    "end": "1312640"
  },
  {
    "text": "from all over the world so you're gonna get around 2 million connections",
    "start": "1312640",
    "end": "1319440"
  },
  {
    "text": "that would be nice to test how would he proxy achieve act like an ingress in that case",
    "start": "1319440",
    "end": "1327280"
  },
  {
    "text": "because that is a completely different ballgame first of all can you",
    "start": "1327280",
    "end": "1333120"
  },
  {
    "text": "even open 2 million tcp connection how much ram would you need to open 2",
    "start": "1333120",
    "end": "1339039"
  },
  {
    "text": "million tcp connection now whatsapp did it what's up reach what 2 million 3 million tcp connections",
    "start": "1339039",
    "end": "1346400"
  },
  {
    "text": "right i talked about that three million tcp connection that that's that's the definition of concurrency",
    "start": "1346400",
    "end": "1353520"
  },
  {
    "text": "because you have end users hungry connecting to you this is a back end",
    "start": "1353520",
    "end": "1361600"
  },
  {
    "text": "metric here where you are using a little bit of a few thousand requests a thousand",
    "start": "1361600",
    "end": "1367360"
  },
  {
    "text": "specific action and then funnel tcp connection to request through these connections multiplex them",
    "start": "1367360",
    "end": "1372720"
  },
  {
    "text": "essentially right it's already truly multiplexing right it's still",
    "start": "1372720",
    "end": "1377840"
  },
  {
    "text": "serializing but you get the idea does that mean this is useless no there",
    "start": "1377840",
    "end": "1384080"
  },
  {
    "text": "are use cases that i can think of what do you have fewer clients",
    "start": "1384080",
    "end": "1390720"
  },
  {
    "text": "to ha proxy and the actual front end",
    "start": "1390720",
    "end": "1399600"
  },
  {
    "text": "is directly facing clients actual user client from all around the",
    "start": "1399840",
    "end": "1405440"
  },
  {
    "text": "world you can do a lot of tricks but obviously i'm not going to recommend you putting 2 million tcp connections like all of",
    "start": "1405440",
    "end": "1412080"
  },
  {
    "text": "the you you need to load balance them right you need to have many ha proxy fleets obviously people",
    "start": "1412080",
    "end": "1419520"
  },
  {
    "text": "from asia in singapore and japan should hit a different server",
    "start": "1419520",
    "end": "1425760"
  },
  {
    "text": "than people in north america then people in europe you should have different servers",
    "start": "1425760",
    "end": "1431760"
  },
  {
    "text": "so you shouldn't really and you you will get to that to a million if i don't know if some influencer in the uk decided to",
    "start": "1431760",
    "end": "1438960"
  },
  {
    "text": "tweet about your service and all of bill two million people in the uk just decided to hit you",
    "start": "1438960",
    "end": "1445039"
  },
  {
    "text": "you're gonna hit that two million in that case but regardless you start to distribute as much as possible",
    "start": "1445039",
    "end": "1450559"
  },
  {
    "text": "at the end of the day you funnel down on the back end with to to fewer sets of front-end",
    "start": "1450559",
    "end": "1458480"
  },
  {
    "text": "interfaces services nodes which then funnels back to h.a proxy or any app proxy that you use",
    "start": "1458480",
    "end": "1464799"
  },
  {
    "text": "which then lowers the number of tcp connections low tcp connections they call happy life",
    "start": "1464799",
    "end": "1470080"
  },
  {
    "text": "right okay i talked about a lot about this let's move on here they mentioned the",
    "start": "1470080",
    "end": "1476320"
  },
  {
    "text": "the new project that's called dp bench has been launched in collaboration with",
    "start": "1476320",
    "end": "1482320"
  },
  {
    "text": "few members of nginx team guys check this project out i read about it fantastic right the thoughts that",
    "start": "1482320",
    "end": "1489279"
  },
  {
    "text": "they thought about all of the stuff that i'm talking about here they thought about all the things and obviously more stuff that i even don't",
    "start": "1489279",
    "end": "1496080"
  },
  {
    "text": "know about right benchmarking proxy is this dedicated for benchmarking brooks it's",
    "start": "1496080",
    "end": "1501200"
  },
  {
    "text": "it's so it's so cool that engine x is working on the horizon despite them being competitors huh i i",
    "start": "1501200",
    "end": "1508000"
  },
  {
    "text": "kind of love that i like it it's like it's like a it's like an all-star soccer game right kind of a thing like a world cup",
    "start": "1508000",
    "end": "1515840"
  },
  {
    "text": "like all the elite come together like this cute methodology all right so you talk",
    "start": "1515840",
    "end": "1523840"
  },
  {
    "text": "about in this benchmark we're testing http requests per second and then end to",
    "start": "1523840",
    "end": "1528960"
  },
  {
    "text": "end request latency at the 99.99 which i talked about here",
    "start": "1528960",
    "end": "1535039"
  },
  {
    "text": "here to talk about the instance size we used an aws c6 gn",
    "start": "1535679",
    "end": "1540720"
  },
  {
    "text": "16x large virtual machine instances with r which are with r which are 64",
    "start": "1540720",
    "end": "1547600"
  },
  {
    "text": "core machines from the c6 gn series guys about either by the way i",
    "start": "1547600",
    "end": "1554799"
  },
  {
    "text": "know and nothing about machines and hardware guys so these numbers mean nothing to me maybe to you if you're a",
    "start": "1554799",
    "end": "1560400"
  },
  {
    "text": "hardware engineer or if you're like into pcs and building pcs this you're probably",
    "start": "1560400",
    "end": "1566080"
  },
  {
    "text": "like freaking out right now that's like oh wow but to me these numbers don't mean",
    "start": "1566080",
    "end": "1571360"
  },
  {
    "text": "anything so probably it's obviously a great machine",
    "start": "1571360",
    "end": "1577600"
  },
  {
    "text": "with access to 100 g gigabit per second network bandwidth very",
    "start": "1577600",
    "end": "1583600"
  },
  {
    "text": "critical here because i'm going to talk about this 100 gigabit per second",
    "start": "1583600",
    "end": "1588640"
  },
  {
    "text": "they're computed optimized they they're compute optimized using graphiton",
    "start": "1588640",
    "end": "1593760"
  },
  {
    "text": "processors and our built in application require high network bandwidth the instance",
    "start": "1593760",
    "end": "1600080"
  },
  {
    "text": "size is used for the client proxy and server so three instances",
    "start": "1600080",
    "end": "1605440"
  },
  {
    "text": "i first started with a 16 core then a 32 core so they started moving from 16",
    "start": "1605440",
    "end": "1612240"
  },
  {
    "text": "core to 33 core and instead thinking in instance thinking this could be have been enough a direct test",
    "start": "1612240",
    "end": "1619760"
  },
  {
    "text": "from the client to server without hi proxy between right so they tested first",
    "start": "1619760",
    "end": "1625600"
  },
  {
    "text": "from the client to the server let's see okay what's the maximum can we get because let's let's forget about actual",
    "start": "1625600",
    "end": "1630799"
  },
  {
    "text": "proxy directly from the client to the server in the same network 100 beautiful 100 gigabit",
    "start": "1630799",
    "end": "1636480"
  },
  {
    "text": "per second how much can we get so they did a test right they didn't just they don't",
    "start": "1636480",
    "end": "1642799"
  },
  {
    "text": "they're not testing requests yet they're testing packets per second it's like i talked about that in the summary what does that",
    "start": "1642799",
    "end": "1648480"
  },
  {
    "text": "mean because how much packet can i",
    "start": "1648480",
    "end": "1652799"
  },
  {
    "text": "eject from my client how fast can i eject packets from my",
    "start": "1653679",
    "end": "1659679"
  },
  {
    "text": "client to my server right and that's another problem",
    "start": "1659679",
    "end": "1665520"
  },
  {
    "text": "eight only he's disappointed will is disappointed with an 800k packet per second",
    "start": "1665520",
    "end": "1673679"
  },
  {
    "text": "i'm going to say willy's will is spoiled 800k he's not happy with an 800k i'll i'll",
    "start": "1673679",
    "end": "1679200"
  },
  {
    "text": "kill for 800k packets per second but yeah",
    "start": "1679200",
    "end": "1684399"
  },
  {
    "start": "1680000",
    "end": "2100000"
  },
  {
    "text": "that's that's essentially low and and you might say why law hussain 800k 800",
    "start": "1684399",
    "end": "1691279"
  },
  {
    "text": "thousands almost a million packets per second right so",
    "start": "1691279",
    "end": "1697279"
  },
  {
    "text": "let's talk about packets per seconds guys here and then let's read the first and then can talk about packets per second",
    "start": "1697279",
    "end": "1702799"
  },
  {
    "text": "i then decided to go for the full on one 64 cores on a dedicated host",
    "start": "1702799",
    "end": "1710000"
  },
  {
    "text": "so they no is that does that mean it's not virtualized i guess that's me that's what it means",
    "start": "1710000",
    "end": "1717360"
  },
  {
    "text": "this time i got my 4.15 million packets per second right so he got",
    "start": "1718399",
    "end": "1725360"
  },
  {
    "text": "four million packets per second sending from the client to the server i guess",
    "start": "1725360",
    "end": "1730480"
  },
  {
    "text": "there is a benchmark that does that i just i just think how many packets per second i'm talking i think it means",
    "start": "1730480",
    "end": "1736559"
  },
  {
    "text": "ip packets your low level ip packets right four million packets per second right in",
    "start": "1736559",
    "end": "1742480"
  },
  {
    "text": "the same network and thought it would be sufficient since i really only need two million",
    "start": "1742480",
    "end": "1750559"
  },
  {
    "text": "packets to achieve one million requests per second accounting for one packet for the",
    "start": "1750559",
    "end": "1756480"
  },
  {
    "text": "request and one for the response that that's what i mentioned in the summary the http request that he's making is",
    "start": "1756480",
    "end": "1764399"
  },
  {
    "text": "essentially fits tucks nicely within a single pipe which is almost never the case",
    "start": "1764399",
    "end": "1772080"
  },
  {
    "text": "in in a real world scenario right that's why he's describing the best case scenario",
    "start": "1772080",
    "end": "1777200"
  },
  {
    "text": "and that's not wrong it's just something you have to understand so if you in your case you might not get",
    "start": "1777200",
    "end": "1784240"
  },
  {
    "text": "two million you're gonna get 800 or 1.5 million",
    "start": "1784240",
    "end": "1789679"
  },
  {
    "text": "depends on the really in the loading car something that's fine that's great too one million that's amazing let's talk",
    "start": "1789679",
    "end": "1797279"
  },
  {
    "text": "about this a little bit so let's let's open this uh let me open the",
    "start": "1797279",
    "end": "1802559"
  },
  {
    "text": "the calculator a little bit here and do some math let's open the calculator can we make this bigger",
    "start": "1802559",
    "end": "1808159"
  },
  {
    "text": "can this makes this bigger this is as big as it gets all right how big is a packet really a packet",
    "start": "1808159",
    "end": "1815760"
  },
  {
    "text": "last time i checked adds 20 bytes an empty packet in the tcp an empty no",
    "start": "1815760",
    "end": "1822320"
  },
  {
    "text": "data adds 20 byte header in the",
    "start": "1822320",
    "end": "1827600"
  },
  {
    "text": "let me get the numbers right correct because i actually strip them from wikipedia an empty",
    "start": "1827600",
    "end": "1835279"
  },
  {
    "text": "packet right an mp mttcp packet no data at all",
    "start": "1835279",
    "end": "1841279"
  },
  {
    "text": "you get an amplified byte of 20 bytes on the tcp stack which is layer",
    "start": "1841279",
    "end": "1848320"
  },
  {
    "text": "4 and then you add another 20 bytes on the on the ip frame and the ip packet on the",
    "start": "1848320",
    "end": "1855520"
  },
  {
    "text": "iplayer layer 3. so that's 40 and then another 24 on the frame on layer 2. so that's total",
    "start": "1855520",
    "end": "1864080"
  },
  {
    "text": "64 bytes with no data let's be",
    "start": "1864080",
    "end": "1871279"
  },
  {
    "text": "conservative and say my application http is 64 bit obviously",
    "start": "1871279",
    "end": "1877600"
  },
  {
    "text": "it's less 64 byte so that means a single request is 128",
    "start": "1877600",
    "end": "1886480"
  },
  {
    "text": "bytes so sorry a single packet is 128 byte okay how much packets",
    "start": "1886480",
    "end": "1895279"
  },
  {
    "text": "could i send on a hundred gigabit theoretically speaking",
    "start": "1895279",
    "end": "1902640"
  },
  {
    "text": "let's do some math a hundred gigabit make sure this is gigabit so you have to",
    "start": "1902640",
    "end": "1908559"
  },
  {
    "text": "divide it by eight right to make a byte 100 gigabit network",
    "start": "1908559",
    "end": "1914080"
  },
  {
    "text": "is 12.5 gigabyte right so you can set 12.5 gigabytes",
    "start": "1914080",
    "end": "1922799"
  },
  {
    "text": "in a single second on this beautiful network that he that he has that really has 12.5 gigabyte right",
    "start": "1922799",
    "end": "1931360"
  },
  {
    "text": "now you multiply this by one twenty thousand four to get",
    "start": "1931360",
    "end": "1938640"
  },
  {
    "text": "one twenty eight thousand megabyte and you multiply it by another one two",
    "start": "1938640",
    "end": "1944960"
  },
  {
    "text": "thousand four you get blah blah blah blah blah",
    "start": "1944960",
    "end": "1950480"
  },
  {
    "text": "what is this number this is 13 million kilobyte and then you multiply by another 1024",
    "start": "1950480",
    "end": "1957600"
  },
  {
    "text": "and you get this much bytes this is 13 billion bytes",
    "start": "1957600",
    "end": "1965080"
  },
  {
    "text": "421 million uh and 772",
    "start": "1965080",
    "end": "1971200"
  },
  {
    "text": "000 bytes you're going to divide that this is the number of bugs that you can",
    "start": "1971200",
    "end": "1976559"
  },
  {
    "text": "eject from your machine if you have a 100 gigabit network you divide that how do i know how many packets i divided",
    "start": "1976559",
    "end": "1983760"
  },
  {
    "text": "by 128 and you're gonna get a whopping 100",
    "start": "1983760",
    "end": "1989440"
  },
  {
    "text": "million packets 100 million packets this is all theoretical based on math",
    "start": "1989440",
    "end": "1996799"
  },
  {
    "text": "right if you have a 128 package size right which is a 64 byte",
    "start": "1996799",
    "end": "2004480"
  },
  {
    "text": "http request you can get up to 100 million let's just simple",
    "start": "2004480",
    "end": "2009919"
  },
  {
    "text": "100 million packets this machine is not even close",
    "start": "2009919",
    "end": "2016320"
  },
  {
    "text": "to how many packets you can get this machine gave you four million so this is not just a",
    "start": "2016320",
    "end": "2021919"
  },
  {
    "text": "machine the operating system cannot possibly output more than",
    "start": "2021919",
    "end": "2027440"
  },
  {
    "text": "four million packets per second so we still have so much to go you don't need more than 100",
    "start": "2027440",
    "end": "2035600"
  },
  {
    "text": "to me in my opinion 100 gigabits it can give you 100 million if you are efficient enough",
    "start": "2035600",
    "end": "2042320"
  },
  {
    "text": "but it looks like today with the latest stack we can output up to four million packets",
    "start": "2042320",
    "end": "2048320"
  },
  {
    "text": "per second where theoretically speaking we",
    "start": "2048320",
    "end": "2054800"
  },
  {
    "text": "can reach 100 million packets per second again this is all my math i might have done something wrong but",
    "start": "2054800",
    "end": "2062638"
  },
  {
    "text": "right and then obviously this is very hard to achieve because there is latency there is there is processes eating through this",
    "start": "2062639",
    "end": "2071280"
  },
  {
    "text": "from the operating system to the network side to the hardware to the motherboard to",
    "start": "2071280",
    "end": "2076320"
  },
  {
    "text": "to the operating system stack to the application stack to all other stuff to the cpu so if we can",
    "start": "2076320",
    "end": "2083760"
  },
  {
    "text": "reach four million which what he did on a 64 course because you need cpu",
    "start": "2083760",
    "end": "2089679"
  },
  {
    "text": "to push this thing out right you need core you need to compute to push it out right",
    "start": "2089679",
    "end": "2097760"
  },
  {
    "text": "yeah so you reach 4 million packets which translates to 2 million requests here's one of the",
    "start": "2097760",
    "end": "2104000"
  },
  {
    "start": "2100000",
    "end": "2400000"
  },
  {
    "text": "things that i absolutely didn't know anything about interrupt affinity apparently who knew",
    "start": "2104000",
    "end": "2111119"
  },
  {
    "text": "but to to enable networking to send data outside of your operating system to to",
    "start": "2111119",
    "end": "2117839"
  },
  {
    "text": "to the word you you kind of need some operating system help and you need",
    "start": "2117839",
    "end": "2123599"
  },
  {
    "text": "cores to do that you need course yeah it's not free duh obviously i didn't think about it",
    "start": "2123599",
    "end": "2129520"
  },
  {
    "text": "but it's so fascinating to read through this i'm gonna let you read through this but i'm gonna just read part of this",
    "start": "2129520",
    "end": "2134880"
  },
  {
    "text": "it took me a while to figure out how to completely stabilize the platform because while virtualized there are still 32 interrupts assigned",
    "start": "2134880",
    "end": "2142800"
  },
  {
    "text": "to the network queues there is a network queues apparently right delivered to 32q so there's a network",
    "start": "2142800",
    "end": "2148000"
  },
  {
    "text": "queue going to the 32 core machine cpus and only it can work only with 32",
    "start": "2148000",
    "end": "2154000"
  },
  {
    "text": "cores right so this could possibly explain the lower performance with a lower number of",
    "start": "2154000",
    "end": "2160960"
  },
  {
    "text": "cores the lower number of course the lower the performance because there's there's bottlenecks right 32 cues can",
    "start": "2160960",
    "end": "2168160"
  },
  {
    "text": "sound a lot to some reader but speaking about 100 gigabit per second networking here and",
    "start": "2168160",
    "end": "2173680"
  },
  {
    "text": "millions of packets per second at such packet rates you definitely do not want",
    "start": "2173680",
    "end": "2179520"
  },
  {
    "text": "any user land processes to hit one of these core during the middle of your test right",
    "start": "2179520",
    "end": "2187280"
  },
  {
    "text": "because you don't want these cpus are busy these beautiful cores are busy serving your",
    "start": "2187280",
    "end": "2193359"
  },
  {
    "text": "network latency they're just sending out data if you are if you're executing your stinking node.js express",
    "start": "2193359",
    "end": "2200640"
  },
  {
    "text": "and then taking precious time from these scores that this is going to suffer so he's",
    "start": "2200640",
    "end": "2207359"
  },
  {
    "text": "going through it's like how did he solve this he kind of dedicated eventually he said okay",
    "start": "2207359",
    "end": "2212960"
  },
  {
    "text": "i'm gonna dedicate 16 cues to 32 cores right two uh uh",
    "start": "2212960",
    "end": "2220800"
  },
  {
    "text": "two interrupt per core essentially that's what he did and then go and showing you how to do that essentially",
    "start": "2220800",
    "end": "2227200"
  },
  {
    "text": "and then going through the uh the network topology this is very",
    "start": "2227200",
    "end": "2233440"
  },
  {
    "text": "interesting so let's talk about 64 core how did",
    "start": "2233440",
    "end": "2239760"
  },
  {
    "text": "woolie divided this 64 chord he reserved chords from 48 to 64 to 63",
    "start": "2239760",
    "end": "2247680"
  },
  {
    "text": "to the network interrupts so you have what 16 cores dedicated just",
    "start": "2247680",
    "end": "2254240"
  },
  {
    "text": "for networking stuff because the operating system need to push these packets out and calls",
    "start": "2254240",
    "end": "2259280"
  },
  {
    "text": "from 2 to 47 are the cores dedicated for the actual",
    "start": "2259280",
    "end": "2267119"
  },
  {
    "text": "running process which is this is hi proxy in their proxy machine this is the server which is whatever the",
    "start": "2267119",
    "end": "2272880"
  },
  {
    "text": "server they're using and uh the the the the client process that",
    "start": "2272880",
    "end": "2280880"
  },
  {
    "text": "spits out all this content in the client machine itself right very interesting stuff and cpu",
    "start": "2280880",
    "end": "2288000"
  },
  {
    "text": "number one is used for the uh for the reference right they're",
    "start": "2288000",
    "end": "2293440"
  },
  {
    "text": "hitting the direct connection between the client and the server that's this is the controller thing is they they want to",
    "start": "2293440",
    "end": "2299040"
  },
  {
    "text": "connect from the client directly to the server to control this behavior all right so that's that's",
    "start": "2299040",
    "end": "2305839"
  },
  {
    "text": "how they broke things up and cl core zero is the reserve for ssh so connect through machines and look at",
    "start": "2305839",
    "end": "2311280"
  },
  {
    "text": "them you you need to give the ssh some memory right some some cores to work the cpu is important",
    "start": "2311280",
    "end": "2317599"
  },
  {
    "text": "apparently so obviously the thing the first thing to do here if you have",
    "start": "2317599",
    "end": "2323200"
  },
  {
    "text": "46 cores to work with as nha proxy what do you do 46 scores on the back end so we're",
    "start": "2323200",
    "end": "2329040"
  },
  {
    "text": "looking at the back end the server there's this thing called http term which is their http server",
    "start": "2329040",
    "end": "2334640"
  },
  {
    "text": "that is in the back end the first thing i do is well you need to max out how many",
    "start": "2334640",
    "end": "2340480"
  },
  {
    "text": "connections can you achieve how many file descriptors update the user limit using you limit",
    "start": "2340480",
    "end": "2347359"
  },
  {
    "text": "dash n hundred thousand as much as you can and then spin up using this task set",
    "start": "2347359",
    "end": "2354720"
  },
  {
    "text": "spin up this much http instances right through course 2 to 47",
    "start": "2354720",
    "end": "2362079"
  },
  {
    "text": "right because that's where the course we're going to work with on port 800 8 000. obviously",
    "start": "2362079",
    "end": "2368800"
  },
  {
    "text": "since we're listening on the same port definitely you have to configure uh this shared object port or use to",
    "start": "2368800",
    "end": "2375200"
  },
  {
    "text": "listen on on the same port right and then all of these machine all these processes are dedicated for this course",
    "start": "2375200",
    "end": "2381839"
  },
  {
    "text": "beautiful stuff and they do the same thing with hp proxy again from two to forty two",
    "start": "2381839",
    "end": "2387200"
  },
  {
    "text": "to four from core two to forty seven that's dedicated for proxy and they're",
    "start": "2387200",
    "end": "2392320"
  },
  {
    "text": "running and showing the code here they do the same thing for the proxy they do the same thing for the h1 loader",
    "start": "2392320",
    "end": "2397599"
  },
  {
    "text": "which is the client that spits out this much connections right and then the client here they specify how many tcp connections to",
    "start": "2397599",
    "end": "2405040"
  },
  {
    "start": "2400000",
    "end": "2570000"
  },
  {
    "text": "connect to 1150 is the maximum current current connections here and",
    "start": "2405040",
    "end": "2412720"
  },
  {
    "text": "we talk about also the the controller process which they call",
    "start": "2412720",
    "end": "2417920"
  },
  {
    "text": "the reference connecting directly from the client to the back end immediately directly",
    "start": "2417920",
    "end": "2424079"
  },
  {
    "text": "and that is using 128 connections they're opening 2 128 connections",
    "start": "2424079",
    "end": "2429280"
  },
  {
    "text": "and they're using one single core they give it only a poor thing only one core",
    "start": "2429280",
    "end": "2434720"
  },
  {
    "text": "and it did achieve around what 200 milli microseconds that was look amazing now",
    "start": "2434720",
    "end": "2440240"
  },
  {
    "text": "we're going through the result hd proxy 2.3 let's look at 2.3 and then and look 2.4 and then in the video and",
    "start": "2440240",
    "end": "2447280"
  },
  {
    "text": "also a little bit long video guys but i'm just fascinated by this fa",
    "start": "2447280",
    "end": "2452680"
  },
  {
    "text": "senated absolutely fascinated this is the same graph that we looked at",
    "start": "2452680",
    "end": "2460400"
  },
  {
    "text": "we're looking at 100 1150 concave connection",
    "start": "2460400",
    "end": "2465440"
  },
  {
    "text": "between the client and the ha proxy and we're looking at 600",
    "start": "2465440",
    "end": "2470480"
  },
  {
    "text": "latency micro uh microsecond latency right and around 2 million requests per second",
    "start": "2470480",
    "end": "2478640"
  },
  {
    "text": "and the direct connection between the one the second is obviously peaking at 120 concurrent connection and",
    "start": "2478640",
    "end": "2485520"
  },
  {
    "text": "120 micro seconds wow right and then they show you another graph which is the",
    "start": "2485520",
    "end": "2492839"
  },
  {
    "text": "99.9 latency for 2.3 can you give take a",
    "start": "2492839",
    "end": "2498560"
  },
  {
    "text": "guess it's a little bit yikesy on 2.3",
    "start": "2498560",
    "end": "2503599"
  },
  {
    "text": "it's 68 milliseconds",
    "start": "2503599",
    "end": "2508720"
  },
  {
    "text": "that's uh it's quite a lot right for for something in in the network itself",
    "start": "2508720",
    "end": "2515599"
  },
  {
    "text": "but i'm fine with it to be 168. but look at that that's the maximum 16",
    "start": "2515599",
    "end": "2521359"
  },
  {
    "text": "that means 99.9999 and requests are less than 68 milliseconds that's what it means",
    "start": "2521359",
    "end": "2527920"
  },
  {
    "text": "all right but h.a proxy team says you know what that's not acceptable because the direct is one one",
    "start": "2527920",
    "end": "2533920"
  },
  {
    "text": "millisecond one millisecond that's the direct 99.99",
    "start": "2533920",
    "end": "2540640"
  },
  {
    "text": "latency wow right so they worked on 2.4 they fixed a lot of did their magic and they dropped the",
    "start": "2540640",
    "end": "2549119"
  },
  {
    "text": "latency from 68 down to 1.8 which is just",
    "start": "2549119",
    "end": "2555839"
  },
  {
    "text": "fantastic they fixed so much stuff i'm not going to go through the details i'm going to reference the article below",
    "start": "2555839",
    "end": "2560960"
  },
  {
    "text": "guys if you want to read more about it but they essentially took full advantage of",
    "start": "2560960",
    "end": "2567359"
  },
  {
    "text": "that all right let's talk about tls the above that we discussed was",
    "start": "2567359",
    "end": "2573280"
  },
  {
    "start": "2570000",
    "end": "2824000"
  },
  {
    "text": "unencrypted http so the first question says okay how does this behave in tls guys and",
    "start": "2573280",
    "end": "2580160"
  },
  {
    "text": "when i realized like it says it's not going to have any effect right why because tls",
    "start": "2580160",
    "end": "2587680"
  },
  {
    "text": "is a one-time cost and based on what they are doing here the thousand connection",
    "start": "2587680",
    "end": "2593040"
  },
  {
    "text": "you're gonna pay the price of tls 1.2 or 1.3 right it's just a one-time price you're",
    "start": "2593040",
    "end": "2599920"
  },
  {
    "text": "gonna establish the tv connection once they are open you're done hr proxy is happy now you can funnel all",
    "start": "2599920",
    "end": "2607040"
  },
  {
    "text": "the requests you want the only cost that was the tls handshake is the expensive thing",
    "start": "2607040",
    "end": "2613920"
  },
  {
    "text": "right especially if you're using depending on the cipher the the the",
    "start": "2613920",
    "end": "2619680"
  },
  {
    "text": "whether you're using a diffie helmet how many bits in the filming exchange and whether you're using",
    "start": "2619680",
    "end": "2625599"
  },
  {
    "text": "elliptic curve diffie hellman how many bits there what's the bit size",
    "start": "2625599",
    "end": "2631599"
  },
  {
    "text": "in the certificate right there the private public key of the civic are using rsa how much bits are",
    "start": "2631599",
    "end": "2638640"
  },
  {
    "text": "there it's like this is 4 000 bits that's obviously slow right",
    "start": "2638640",
    "end": "2643839"
  },
  {
    "text": "and and then that's what they're using they're using 2 000 rsa but and i i had a conversation with william",
    "start": "2643839",
    "end": "2650160"
  },
  {
    "text": "and and i said like oh guys would if you use the lower if you use the certificate with such a",
    "start": "2650160",
    "end": "2658560"
  },
  {
    "text": "an elliptic curve dsa like the digitally signed uh forgot the algorithm",
    "start": "2658560",
    "end": "2663680"
  },
  {
    "text": "which uses less bit essentially using elliptic curve and instead of rsa so you have fewer bits",
    "start": "2663680",
    "end": "2670640"
  },
  {
    "text": "right instead of higher number of bits so that's obviously faster right dealing with the lower number of bits",
    "start": "2670640",
    "end": "2676880"
  },
  {
    "text": "and they said it doesn't matter the same doesn't matter because this is like a one-time cost you're going to hit it and",
    "start": "2676880",
    "end": "2682000"
  },
  {
    "text": "then it's going to swallow away it's getting gone right yeah you're going to hit it in the beginning",
    "start": "2682000",
    "end": "2687920"
  },
  {
    "text": "just the establishment of the connection but once you have the connection you're encrypted the extra cost that i",
    "start": "2687920",
    "end": "2694560"
  },
  {
    "text": "can think of is in the algorithm the the the symmetric algorithm itself which",
    "start": "2694560",
    "end": "2702400"
  },
  {
    "text": "is this i see having an impact right but it looks like we didn't get",
    "start": "2702400",
    "end": "2710079"
  },
  {
    "text": "much we had like around 600 microsoft it's almost the same right it's the same thing tls terminated",
    "start": "2710079",
    "end": "2716400"
  },
  {
    "text": "all that jazz the number of requests dipped a little bit right",
    "start": "2716400",
    "end": "2721599"
  },
  {
    "text": "from 2.07 to 2.01",
    "start": "2721599",
    "end": "2727359"
  },
  {
    "text": "so it dipped around 0.07 whatever 70 requests 70 000 requests not much",
    "start": "2727359",
    "end": "2735680"
  },
  {
    "text": "so as is really good and i mean you can use charger which i believe it's faster charger is faster",
    "start": "2735680",
    "end": "2742800"
  },
  {
    "text": "than he is they claim using a little bit lower number of bits",
    "start": "2742800",
    "end": "2747839"
  },
  {
    "text": "that's what i heard i don't know if that's true but yeah i think uh church is a little bit popular with the",
    "start": "2747839",
    "end": "2753200"
  },
  {
    "text": "with kids these days and they show here the obviously the latency",
    "start": "2753200",
    "end": "2758880"
  },
  {
    "text": "we're seeing the latency a little bit pumping up with the with tls up to six milliseconds that's",
    "start": "2758880",
    "end": "2764720"
  },
  {
    "text": "that's kind of understandable because we're using tl with terminated dls so there's additional overhead",
    "start": "2764720",
    "end": "2770079"
  },
  {
    "text": "i'll take it i'll take the six millisecond that's nothing so going through analysis here and all",
    "start": "2770079",
    "end": "2775520"
  },
  {
    "text": "the details but this is fantastic fantastic fantastic",
    "start": "2775520",
    "end": "2781680"
  },
  {
    "text": "article really i really uh uh suggest you guys read through this",
    "start": "2781680",
    "end": "2787280"
  },
  {
    "text": "i learned so much through reading just through this it took me a month just to process the",
    "start": "2787280",
    "end": "2792640"
  },
  {
    "text": "amount of information there and i'm pretty sure i missed a lot of stuff so thank you to the jproxy team",
    "start": "2792640",
    "end": "2799280"
  },
  {
    "text": "this fantastic article sharing their knowledge sharing their title pace knowledge",
    "start": "2799280",
    "end": "2804640"
  },
  {
    "text": "knowledge keep sharing knowledge guys this is beautiful stuff guys if you enjoy this kind of things click",
    "start": "2804640",
    "end": "2811760"
  },
  {
    "text": "on the this video and watch it i think you're gonna love it this is talking about actual hd proxy how to set it up and all",
    "start": "2811760",
    "end": "2818640"
  },
  {
    "text": "that jazz all right guys i'm going to see you on the next one you guys stay awesome goodbye",
    "start": "2818640",
    "end": "2825280"
  }
]