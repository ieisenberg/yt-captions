[
  {
    "start": "0",
    "end": "148000"
  },
  {
    "text": "hi everyone I'm really excited to be",
    "start": "5870",
    "end": "8820"
  },
  {
    "text": "here today at the race i'ma connect",
    "start": "8820",
    "end": "11160"
  },
  {
    "text": "event to give a talk on demystified",
    "start": "11160",
    "end": "13830"
  },
  {
    "text": "neural architecture research in theory",
    "start": "13830",
    "end": "15990"
  },
  {
    "text": "and practice so I want to just give a",
    "start": "15990",
    "end": "20310"
  },
  {
    "text": "brief introduction about myself before",
    "start": "20310",
    "end": "23070"
  },
  {
    "text": "we get started so my name is Liam I",
    "start": "23070",
    "end": "26550"
  },
  {
    "text": "recently graduated with a PhD in machine",
    "start": "26550",
    "end": "30059"
  },
  {
    "text": "learning from CMU and since then I",
    "start": "30059",
    "end": "32130"
  },
  {
    "text": "joined determined as a machine learning",
    "start": "32130",
    "end": "33989"
  },
  {
    "text": "engineer and this talk today will be",
    "start": "33989",
    "end": "37170"
  },
  {
    "text": "sharing some of the insights that I've",
    "start": "37170",
    "end": "40140"
  },
  {
    "text": "gained about your architecture to search",
    "start": "40140",
    "end": "42180"
  },
  {
    "text": "both in sort of the research that I've",
    "start": "42180",
    "end": "45120"
  },
  {
    "text": "done and also more recently at determine",
    "start": "45120",
    "end": "48020"
  },
  {
    "text": "let's get started",
    "start": "48020",
    "end": "51090"
  },
  {
    "text": "so whenever I train a neural network say",
    "start": "51090",
    "end": "57239"
  },
  {
    "text": "I'm training a network for image",
    "start": "57239",
    "end": "58770"
  },
  {
    "text": "classification I was always find myself",
    "start": "58770",
    "end": "61100"
  },
  {
    "text": "wondering about certain questions like",
    "start": "61100",
    "end": "64018"
  },
  {
    "text": "how should I design this network what",
    "start": "64019",
    "end": "65970"
  },
  {
    "text": "activation function or pooling method",
    "start": "65970",
    "end": "67829"
  },
  {
    "text": "should I use where should I put a",
    "start": "67829",
    "end": "70320"
  },
  {
    "text": "skipped connect should I just use some",
    "start": "70320",
    "end": "73620"
  },
  {
    "text": "pre-canned architectures like resna or",
    "start": "73620",
    "end": "76229"
  },
  {
    "text": "vgg or can i actually find more",
    "start": "76229",
    "end": "79740"
  },
  {
    "text": "specialized architectures that would",
    "start": "79740",
    "end": "81180"
  },
  {
    "text": "actually perform better for my problem",
    "start": "81180",
    "end": "84080"
  },
  {
    "text": "these kinds of questions are kind of the",
    "start": "84080",
    "end": "87120"
  },
  {
    "text": "exact questions that we want to be able",
    "start": "87120",
    "end": "88950"
  },
  {
    "text": "to answer with recent work in this field",
    "start": "88950",
    "end": "92700"
  },
  {
    "text": "of machine learning called neural",
    "start": "92700",
    "end": "93960"
  },
  {
    "text": "architecture search and we've seen a lot",
    "start": "93960",
    "end": "96630"
  },
  {
    "text": "of interest in your art architecture",
    "start": "96630",
    "end": "98790"
  },
  {
    "text": "search from some of the largest tech",
    "start": "98790",
    "end": "101130"
  },
  {
    "text": "companies who have published papers in",
    "start": "101130",
    "end": "104670"
  },
  {
    "text": "the area and also offer nice solutions",
    "start": "104670",
    "end": "108360"
  },
  {
    "text": "in some of their cloud platforms and in",
    "start": "108360",
    "end": "112439"
  },
  {
    "text": "the previous talk Richard shared with us",
    "start": "112439",
    "end": "115830"
  },
  {
    "text": "how to do hyper parameter optimization",
    "start": "115830",
    "end": "117740"
  },
  {
    "text": "with Ray tune and we all know that hyper",
    "start": "117740",
    "end": "120930"
  },
  {
    "text": "parameter optimization is a large part",
    "start": "120930",
    "end": "123000"
  },
  {
    "text": "of auto ml but where does neural",
    "start": "123000",
    "end": "124890"
  },
  {
    "text": "architecture search fit into this",
    "start": "124890",
    "end": "126479"
  },
  {
    "text": "picture",
    "start": "126479",
    "end": "128149"
  },
  {
    "text": "so the stated goal of nafse is to",
    "start": "128149",
    "end": "131189"
  },
  {
    "text": "automate architecture design but in",
    "start": "131189",
    "end": "134340"
  },
  {
    "text": "reality involves searching through",
    "start": "134340",
    "end": "136760"
  },
  {
    "text": "basically a large",
    "start": "136760",
    "end": "138350"
  },
  {
    "text": "space of possible architectures so from",
    "start": "138350",
    "end": "142460"
  },
  {
    "text": "our view NASA's a special case of hyper",
    "start": "142460",
    "end": "145190"
  },
  {
    "text": "parameter optimization and the first",
    "start": "145190",
    "end": "149090"
  },
  {
    "start": "148000",
    "end": "148000"
  },
  {
    "text": "generation of NAS methods were able to",
    "start": "149090",
    "end": "151700"
  },
  {
    "text": "achieve state-of-the-art results on two",
    "start": "151700",
    "end": "154220"
  },
  {
    "text": "common machine learning tasks image",
    "start": "154220",
    "end": "156290"
  },
  {
    "text": "classification and language modeling but",
    "start": "156290",
    "end": "160100"
  },
  {
    "text": "they did so that tremendous compute cost",
    "start": "160100",
    "end": "161960"
  },
  {
    "text": "so for the result on so far 10 it",
    "start": "161960",
    "end": "164720"
  },
  {
    "text": "required over 3,000 GPU days in order to",
    "start": "164720",
    "end": "167420"
  },
  {
    "text": "find a good architecture and for the",
    "start": "167420",
    "end": "169610"
  },
  {
    "text": "language modeling tasks on penn treebank",
    "start": "169610",
    "end": "171260"
  },
  {
    "text": "it required over 10,000 cpu days and it",
    "start": "171260",
    "end": "175280"
  },
  {
    "text": "was very expensive to get these results",
    "start": "175280",
    "end": "177230"
  },
  {
    "text": "because it required training thousands",
    "start": "177230",
    "end": "179570"
  },
  {
    "text": "of architectures from the search base",
    "start": "179570",
    "end": "181660"
  },
  {
    "text": "before finding a good architecture more",
    "start": "181660",
    "end": "185810"
  },
  {
    "text": "recently we've seen weight sharing and",
    "start": "185810",
    "end": "187910"
  },
  {
    "start": "186000",
    "end": "186000"
  },
  {
    "text": "emerge as in a more efficient method for",
    "start": "187910",
    "end": "190730"
  },
  {
    "text": "NOS and the way that weight sharing",
    "start": "190730",
    "end": "194540"
  },
  {
    "text": "works is you basically have a single",
    "start": "194540",
    "end": "197900"
  },
  {
    "text": "weight sharing network that you can use",
    "start": "197900",
    "end": "199670"
  },
  {
    "text": "to evaluate all possible architectures",
    "start": "199670",
    "end": "202430"
  },
  {
    "text": "in your search base hence the search",
    "start": "202430",
    "end": "204950"
  },
  {
    "text": "cost is reduced to that of training a",
    "start": "204950",
    "end": "207170"
  },
  {
    "text": "single network so let's pull back the",
    "start": "207170",
    "end": "210650"
  },
  {
    "text": "hood a little bit and see what is",
    "start": "210650",
    "end": "212030"
  },
  {
    "text": "happening in weight sharing so the way",
    "start": "212030",
    "end": "215600"
  },
  {
    "text": "that we Shane works is it's a single",
    "start": "215600",
    "end": "218180"
  },
  {
    "text": "weight training network that includes",
    "start": "218180",
    "end": "219770"
  },
  {
    "text": "all possible architecture choices so",
    "start": "219770",
    "end": "226790"
  },
  {
    "text": "here I'm the gif on the right is",
    "start": "226790",
    "end": "228950"
  },
  {
    "text": "basically demonstrating that for the",
    "start": "228950",
    "end": "231890"
  },
  {
    "text": "search space that you could consider",
    "start": "231890",
    "end": "233510"
  },
  {
    "text": "there are four possible operations that",
    "start": "233510",
    "end": "235520"
  },
  {
    "text": "you can use for every single layer",
    "start": "235520",
    "end": "238430"
  },
  {
    "text": "so here it's con 3 by 3 comma 5 by 5",
    "start": "238430",
    "end": "241310"
  },
  {
    "text": "pooling or skip connect and if you were",
    "start": "241310",
    "end": "245930"
  },
  {
    "text": "considering a single architecture you",
    "start": "245930",
    "end": "247670"
  },
  {
    "text": "would pick one of these operations for",
    "start": "247670",
    "end": "250010"
  },
  {
    "text": "every single layer but with the weight",
    "start": "250010",
    "end": "252110"
  },
  {
    "text": "training network includes all operations",
    "start": "252110",
    "end": "254860"
  },
  {
    "text": "all possible operations at each layer",
    "start": "254860",
    "end": "257540"
  },
  {
    "text": "and there are these architecture",
    "start": "257540",
    "end": "259340"
  },
  {
    "text": "parameters alpha that basically are the",
    "start": "259340",
    "end": "262160"
  },
  {
    "text": "parameters that we learn in order to",
    "start": "262160",
    "end": "264350"
  },
  {
    "text": "find a good architecture within the",
    "start": "264350",
    "end": "266990"
  },
  {
    "text": "super within this weight sharing Network",
    "start": "266990",
    "end": "271310"
  },
  {
    "text": "so by including all the possible choices",
    "start": "271310",
    "end": "274520"
  },
  {
    "text": "and all the possible operations in each",
    "start": "274520",
    "end": "276500"
  },
  {
    "text": "layer the weight sharing network also",
    "start": "276500",
    "end": "278300"
  },
  {
    "text": "includes all possible architectures as",
    "start": "278300",
    "end": "280580"
  },
  {
    "text": "sub networks and in this way training",
    "start": "280580",
    "end": "283850"
  },
  {
    "text": "just the weight sharing network we're",
    "start": "283850",
    "end": "285650"
  },
  {
    "text": "able to use the architecture parameters",
    "start": "285650",
    "end": "287510"
  },
  {
    "text": "to get a signal about what a good",
    "start": "287510",
    "end": "290930"
  },
  {
    "text": "architecture is from the search space so",
    "start": "290930",
    "end": "295790"
  },
  {
    "text": "you might be thinking this is",
    "start": "295790",
    "end": "297380"
  },
  {
    "text": "hocus-pocus how does this even work",
    "start": "297380",
    "end": "299470"
  },
  {
    "text": "won't the sub networks from the way",
    "start": "299470",
    "end": "301940"
  },
  {
    "text": "sharing network interfere with each",
    "start": "301940",
    "end": "303170"
  },
  {
    "text": "other and you might have also heard that",
    "start": "303170",
    "end": "306170"
  },
  {
    "text": "it's a it's crazy that way sharing works",
    "start": "306170",
    "end": "308690"
  },
  {
    "text": "because the weight sharing performance",
    "start": "308690",
    "end": "311360"
  },
  {
    "text": "of stand-alone architectures has low",
    "start": "311360",
    "end": "314360"
  },
  {
    "text": "correlation with the ground truth",
    "start": "314360",
    "end": "315830"
  },
  {
    "text": "performance when you train those",
    "start": "315830",
    "end": "317180"
  },
  {
    "text": "architectures from scratch so in this",
    "start": "317180",
    "end": "320600"
  },
  {
    "text": "talk I'm going to address or I'm going",
    "start": "320600",
    "end": "323690"
  },
  {
    "text": "to attempt to sort of demystify wage",
    "start": "323690",
    "end": "326900"
  },
  {
    "text": "hanging through the lens of optimization",
    "start": "326900",
    "end": "329500"
  },
  {
    "text": "and in the second part we're gonna put",
    "start": "329500",
    "end": "332960"
  },
  {
    "text": "our practitioner hats on and I'm going",
    "start": "332960",
    "end": "335510"
  },
  {
    "text": "to sort of talk about how we can apply",
    "start": "335510",
    "end": "337790"
  },
  {
    "text": "nos and practice with determined which",
    "start": "337790",
    "end": "340490"
  },
  {
    "text": "is an open source deep learning training",
    "start": "340490",
    "end": "342200"
  },
  {
    "text": "platform so for the first part",
    "start": "342200",
    "end": "347140"
  },
  {
    "text": "understanding wage hanging through the",
    "start": "347140",
    "end": "348860"
  },
  {
    "text": "lens of optimization some of these some",
    "start": "348860",
    "end": "352340"
  },
  {
    "text": "of the key questions that we had where",
    "start": "352340",
    "end": "354440"
  },
  {
    "text": "are these way train approaches",
    "start": "354440",
    "end": "356150"
  },
  {
    "text": "guaranteed to converge and in this talk",
    "start": "356150",
    "end": "359030"
  },
  {
    "text": "I won't have time to go into the details",
    "start": "359030",
    "end": "360830"
  },
  {
    "text": "of our work but I will give I will try",
    "start": "360830",
    "end": "364070"
  },
  {
    "text": "to give some high-level intuition for",
    "start": "364070",
    "end": "366050"
  },
  {
    "text": "our conclusions here so the answer to",
    "start": "366050",
    "end": "369620"
  },
  {
    "text": "this first question is that yes these",
    "start": "369620",
    "end": "372190"
  },
  {
    "text": "approaches are guaranteed to converge",
    "start": "372190",
    "end": "374210"
  },
  {
    "text": "and we are able to analyze the",
    "start": "374210",
    "end": "377030"
  },
  {
    "text": "convergence traded through the framework",
    "start": "377030",
    "end": "379040"
  },
  {
    "text": "of mirror descent second question we had",
    "start": "379040",
    "end": "383060"
  },
  {
    "text": "was how can we design faster and more",
    "start": "383060",
    "end": "385070"
  },
  {
    "text": "accurate nos methods and we show that",
    "start": "385070",
    "end": "388520"
  },
  {
    "text": "we're able to take a geometry aware",
    "start": "388520",
    "end": "390560"
  },
  {
    "text": "approach to design a new algorithm",
    "start": "390560",
    "end": "392540"
  },
  {
    "text": "called Gaia that has provably faster",
    "start": "392540",
    "end": "395540"
  },
  {
    "text": "convergence rates than standard nos",
    "start": "395540",
    "end": "398660"
  },
  {
    "text": "algorithms like darts and finally does",
    "start": "398660",
    "end": "403190"
  },
  {
    "text": "taking this geometry aware",
    "start": "403190",
    "end": "405050"
  },
  {
    "text": "coach improve performance on actual nas",
    "start": "405050",
    "end": "407509"
  },
  {
    "text": "benchmarks so let's see for the dart",
    "start": "407509",
    "end": "411770"
  },
  {
    "start": "410000",
    "end": "410000"
  },
  {
    "text": "search base on the safar 10 data set we",
    "start": "411770",
    "end": "415370"
  },
  {
    "text": "see that our gaya approach is able to",
    "start": "415370",
    "end": "418250"
  },
  {
    "text": "achieve the best result on this search",
    "start": "418250",
    "end": "420319"
  },
  {
    "text": "face and the results highlighted in",
    "start": "420319",
    "end": "424190"
  },
  {
    "text": "orange are sort of algorithms standard",
    "start": "424190",
    "end": "427580"
  },
  {
    "text": "hyper primary optimization algorithms",
    "start": "427580",
    "end": "429379"
  },
  {
    "text": "while all the other algorithms are sort",
    "start": "429379",
    "end": "432860"
  },
  {
    "text": "of not specific algorithms also for the",
    "start": "432860",
    "end": "437780"
  },
  {
    "start": "437000",
    "end": "437000"
  },
  {
    "text": "dart search based on image net here the",
    "start": "437780",
    "end": "441830"
  },
  {
    "text": "first two rows show basically the",
    "start": "441830",
    "end": "443750"
  },
  {
    "text": "performance of the first generation of",
    "start": "443750",
    "end": "445729"
  },
  {
    "text": "NAS methods that used full training in",
    "start": "445729",
    "end": "448009"
  },
  {
    "text": "order to find a good architecture and",
    "start": "448009",
    "end": "450280"
  },
  {
    "text": "recently we've seen that weight sharing",
    "start": "450280",
    "end": "453830"
  },
  {
    "text": "nas methods have been able to match the",
    "start": "453830",
    "end": "456050"
  },
  {
    "text": "performance of the best sort of first",
    "start": "456050",
    "end": "458090"
  },
  {
    "text": "generation full training methods so here",
    "start": "458090",
    "end": "461210"
  },
  {
    "text": "we can see that PC darts was able to",
    "start": "461210",
    "end": "463849"
  },
  {
    "text": "recently match ameba net on image net",
    "start": "463849",
    "end": "466789"
  },
  {
    "text": "and with gaya were able to improve upon",
    "start": "466789",
    "end": "470659"
  },
  {
    "text": "the PC darts results even more and reach",
    "start": "470659",
    "end": "473930"
  },
  {
    "text": "state-of-the-art on image net for this",
    "start": "473930",
    "end": "476090"
  },
  {
    "text": "benchmark and this search space we also",
    "start": "476090",
    "end": "481069"
  },
  {
    "start": "480000",
    "end": "480000"
  },
  {
    "text": "looked at the nas bench 201 benchmarks",
    "start": "481069",
    "end": "485180"
  },
  {
    "text": "where in this benchmark the results for",
    "start": "485180",
    "end": "489380"
  },
  {
    "text": "three different data sets are provided",
    "start": "489380",
    "end": "491840"
  },
  {
    "text": "and the authors studied the performance",
    "start": "491840",
    "end": "494659"
  },
  {
    "text": "of standard hyper parameter optimization",
    "start": "494659",
    "end": "496250"
  },
  {
    "text": "methods like random surgery enforcement",
    "start": "496250",
    "end": "498680"
  },
  {
    "text": "and learning and Bayesian optimization",
    "start": "498680",
    "end": "500990"
  },
  {
    "text": "hyper band and also compared so recent",
    "start": "500990",
    "end": "505279"
  },
  {
    "text": "weight sharing methods to standard high",
    "start": "505279",
    "end": "506990"
  },
  {
    "text": "parameter optimization methods and what",
    "start": "506990",
    "end": "510020"
  },
  {
    "text": "they found was kind of surprising which",
    "start": "510020",
    "end": "511969"
  },
  {
    "text": "is that the standard hyper parameter",
    "start": "511969",
    "end": "513860"
  },
  {
    "text": "optimization methods actually",
    "start": "513860",
    "end": "515120"
  },
  {
    "text": "outperformed the weight training methods",
    "start": "515120",
    "end": "519190"
  },
  {
    "text": "when we evaluated gaya on these three",
    "start": "519190",
    "end": "522320"
  },
  {
    "text": "data sets we surprisingly saw that we",
    "start": "522320",
    "end": "524810"
  },
  {
    "text": "were able to beat not only all the",
    "start": "524810",
    "end": "526490"
  },
  {
    "text": "weight sharing methods but also the",
    "start": "526490",
    "end": "529519"
  },
  {
    "text": "standard hyper Kroeger optimization",
    "start": "529519",
    "end": "531470"
  },
  {
    "text": "methods and approach the optimum",
    "start": "531470",
    "end": "533470"
  },
  {
    "text": "architectures in each of these three",
    "start": "533470",
    "end": "535550"
  },
  {
    "text": "data sets",
    "start": "535550",
    "end": "538060"
  },
  {
    "text": "so what is what is Gaea doing and why",
    "start": "538209",
    "end": "542089"
  },
  {
    "start": "539000",
    "end": "539000"
  },
  {
    "text": "does it work so well in practice so in",
    "start": "542089",
    "end": "545329"
  },
  {
    "text": "order to understand that we have to",
    "start": "545329",
    "end": "546649"
  },
  {
    "text": "first understand the weight sharing",
    "start": "546649",
    "end": "548449"
  },
  {
    "text": "optimization problem and so if we start",
    "start": "548449",
    "end": "552829"
  },
  {
    "text": "with the original optimization problem",
    "start": "552829",
    "end": "554720"
  },
  {
    "text": "saw by first generation mass methods the",
    "start": "554720",
    "end": "557600"
  },
  {
    "text": "objectives that they saw is basically",
    "start": "557600",
    "end": "559579"
  },
  {
    "text": "this by level objective where we want to",
    "start": "559579",
    "end": "562429"
  },
  {
    "text": "find the architecture that has the",
    "start": "562429",
    "end": "564619"
  },
  {
    "text": "lowest loss from the search space right",
    "start": "564619",
    "end": "569059"
  },
  {
    "text": "so here the objective is just saying",
    "start": "569059",
    "end": "571749"
  },
  {
    "text": "configure a certain model m according to",
    "start": "571749",
    "end": "574939"
  },
  {
    "text": "an architecture alpha and then find the",
    "start": "574939",
    "end": "578420"
  },
  {
    "text": "weights that minimizes this loss and",
    "start": "578420",
    "end": "581089"
  },
  {
    "text": "then choose the architecture that has",
    "start": "581089",
    "end": "583490"
  },
  {
    "text": "the lowest loss so here just as a",
    "start": "583490",
    "end": "586879"
  },
  {
    "text": "reminder these first generation methods",
    "start": "586879",
    "end": "589699"
  },
  {
    "text": "train many architectures individually",
    "start": "589699",
    "end": "592059"
  },
  {
    "text": "and adapted the architecture",
    "start": "592059",
    "end": "594949"
  },
  {
    "text": "distribution after receiving some",
    "start": "594949",
    "end": "597259"
  },
  {
    "text": "signals about the performance of",
    "start": "597259",
    "end": "598819"
  },
  {
    "text": "different architectures and would",
    "start": "598819",
    "end": "600829"
  },
  {
    "text": "finally return the best architecture in",
    "start": "600829",
    "end": "603350"
  },
  {
    "text": "contrast the weight sharing optimization",
    "start": "603350",
    "end": "605959"
  },
  {
    "text": "problem is a single objective where we",
    "start": "605959",
    "end": "609860"
  },
  {
    "text": "have a weight sharing network M that is",
    "start": "609860",
    "end": "611779"
  },
  {
    "text": "parametrized by both the weights of the",
    "start": "611779",
    "end": "614240"
  },
  {
    "text": "network as well as architecture",
    "start": "614240",
    "end": "616429"
  },
  {
    "text": "parameters theta and the architecture",
    "start": "616429",
    "end": "619850"
  },
  {
    "text": "parameters theta is a continuous",
    "start": "619850",
    "end": "621290"
  },
  {
    "text": "relaxation of the original architecture",
    "start": "621290",
    "end": "624319"
  },
  {
    "text": "space a and the weight training methods",
    "start": "624319",
    "end": "628699"
  },
  {
    "text": "optimize both W and data simultaneously",
    "start": "628699",
    "end": "631420"
  },
  {
    "text": "using gradients so when you look at this",
    "start": "631420",
    "end": "635629"
  },
  {
    "text": "new optimization objective it looks very",
    "start": "635629",
    "end": "638540"
  },
  {
    "text": "similar to the objectives that we work",
    "start": "638540",
    "end": "640730"
  },
  {
    "text": "with in standard neural network training",
    "start": "640730",
    "end": "642499"
  },
  {
    "text": "where we're able to apply gradients in",
    "start": "642499",
    "end": "644809"
  },
  {
    "text": "order to update the parameters so",
    "start": "644809",
    "end": "649309"
  },
  {
    "text": "exactly so theta is learned is a learn",
    "start": "649309",
    "end": "652069"
  },
  {
    "text": "parameter like W is the weight gradient",
    "start": "652069",
    "end": "656899"
  },
  {
    "text": "based weight sharing methods work is",
    "start": "656899",
    "end": "658670"
  },
  {
    "text": "that they optimize both W and theta",
    "start": "658670",
    "end": "662240"
  },
  {
    "text": "alternately so first we fix the",
    "start": "662240",
    "end": "665089"
  },
  {
    "text": "architecture parameters theta and then",
    "start": "665089",
    "end": "666980"
  },
  {
    "text": "we update W and then we fix the",
    "start": "666980",
    "end": "670399"
  },
  {
    "text": "architect",
    "start": "670399",
    "end": "671089"
  },
  {
    "text": "the network parameters W and update",
    "start": "671089",
    "end": "673399"
  },
  {
    "text": "theta so in the case of the darts",
    "start": "673399",
    "end": "675799"
  },
  {
    "text": "algorithm the update rule used for the",
    "start": "675799",
    "end": "679279"
  },
  {
    "text": "architecture each parameter theta is",
    "start": "679279",
    "end": "680899"
  },
  {
    "text": "based on the atom optimizer and our",
    "start": "680899",
    "end": "684949"
  },
  {
    "start": "684000",
    "end": "684000"
  },
  {
    "text": "geometry aware exponentiated algorithm",
    "start": "684949",
    "end": "687259"
  },
  {
    "text": "Gaea is a simple modification of the",
    "start": "687259",
    "end": "689809"
  },
  {
    "text": "darts algorithm instead of 2 using Adam",
    "start": "689809",
    "end": "693379"
  },
  {
    "text": "to optimize the architecture parameters",
    "start": "693379",
    "end": "695299"
  },
  {
    "text": "theta we use the exponentiated gradient",
    "start": "695299",
    "end": "698569"
  },
  {
    "text": "update instead and this simple",
    "start": "698569",
    "end": "701659"
  },
  {
    "text": "modification as you've seen leads to",
    "start": "701659",
    "end": "704139"
  },
  {
    "text": "significantly better results in practice",
    "start": "704139",
    "end": "706959"
  },
  {
    "text": "and the insight for using exponentiated",
    "start": "706959",
    "end": "711229"
  },
  {
    "start": "708000",
    "end": "708000"
  },
  {
    "text": "gradient comes from our convergence",
    "start": "711229",
    "end": "712849"
  },
  {
    "text": "analysis and in our paper we analyzed",
    "start": "712849",
    "end": "716779"
  },
  {
    "text": "the convergence rates for different",
    "start": "716779",
    "end": "718959"
  },
  {
    "text": "gradient based weight sharing methods",
    "start": "718959",
    "end": "720859"
  },
  {
    "text": "and show that these convergence rates",
    "start": "720859",
    "end": "724219"
  },
  {
    "text": "depend on the number of edges in the",
    "start": "724219",
    "end": "726139"
  },
  {
    "text": "search base and the number of operations",
    "start": "726139",
    "end": "729729"
  },
  {
    "text": "so the the takeaway is that our",
    "start": "729729",
    "end": "734199"
  },
  {
    "text": "algorithm Gaea has has basically less",
    "start": "734199",
    "end": "739929"
  },
  {
    "text": "dependency on the or weaker dependency",
    "start": "739929",
    "end": "744019"
  },
  {
    "text": "on the dimension of the search base so",
    "start": "744019",
    "end": "747619"
  },
  {
    "text": "in this case we see that darts has",
    "start": "747619",
    "end": "751129"
  },
  {
    "text": "convergence rate ye times o where as",
    "start": "751129",
    "end": "754339"
  },
  {
    "text": "Gaia has convergence rate e log o for",
    "start": "754339",
    "end": "758149"
  },
  {
    "text": "the dart search base and even though",
    "start": "758149",
    "end": "760669"
  },
  {
    "text": "this gain is modest we are able to see",
    "start": "760669",
    "end": "764559"
  },
  {
    "text": "significant performance improvements so",
    "start": "764559",
    "end": "769459"
  },
  {
    "start": "769000",
    "end": "769000"
  },
  {
    "text": "do we actually observe faster",
    "start": "769459",
    "end": "771169"
  },
  {
    "text": "convergence and practice given what the",
    "start": "771169",
    "end": "773539"
  },
  {
    "text": "theory suggests and in these two charts",
    "start": "773539",
    "end": "776629"
  },
  {
    "text": "I'm showing the the entropy for two",
    "start": "776629",
    "end": "780259"
  },
  {
    "text": "search spaces comparing both darts and",
    "start": "780259",
    "end": "783769"
  },
  {
    "text": "Gaea you can see that the entropy does",
    "start": "783769",
    "end": "788119"
  },
  {
    "text": "decrease faster for Gaea suggesting",
    "start": "788119",
    "end": "790279"
  },
  {
    "text": "faster convergence so now that we've",
    "start": "790279",
    "end": "796009"
  },
  {
    "text": "sort of taken a deep dive into the",
    "start": "796009",
    "end": "798409"
  },
  {
    "text": "optimization problem associated with",
    "start": "798409",
    "end": "800509"
  },
  {
    "text": "gradient based weight sharing some of",
    "start": "800509",
    "end": "802249"
  },
  {
    "text": "you might be wondering how do",
    "start": "802249",
    "end": "804010"
  },
  {
    "text": "I actually apply NOS in practice so in",
    "start": "804010",
    "end": "807970"
  },
  {
    "text": "this next section I'll talk about how",
    "start": "807970",
    "end": "809470"
  },
  {
    "text": "you can get started with nos with the",
    "start": "809470",
    "end": "812500"
  },
  {
    "text": "with our open source platform determined",
    "start": "812500",
    "end": "814840"
  },
  {
    "text": "and I'll cover each of these three",
    "start": "814840",
    "end": "817180"
  },
  {
    "text": "points in more detail next so if you go",
    "start": "817180",
    "end": "822430"
  },
  {
    "start": "821000",
    "end": "821000"
  },
  {
    "text": "back to the first set of experiments",
    "start": "822430",
    "end": "823870"
  },
  {
    "text": "that I showed one observation is that",
    "start": "823870",
    "end": "826720"
  },
  {
    "text": "Asha which is an early stopping based",
    "start": "826720",
    "end": "829360"
  },
  {
    "text": "hyper parameter optimization algorithm",
    "start": "829360",
    "end": "831670"
  },
  {
    "text": "is a very strong baseline for nos and",
    "start": "831670",
    "end": "835260"
  },
  {
    "text": "the surprising thing is that Asha works",
    "start": "835260",
    "end": "838180"
  },
  {
    "text": "out of the box and no training was",
    "start": "838180",
    "end": "840250"
  },
  {
    "text": "required for the algorithm in contrast",
    "start": "840250",
    "end": "843490"
  },
  {
    "text": "weight training methods often require a",
    "start": "843490",
    "end": "845380"
  },
  {
    "text": "significant amount of tuning and and",
    "start": "845380",
    "end": "848190"
  },
  {
    "text": "actually behind the scenes there's a lot",
    "start": "848190",
    "end": "850630"
  },
  {
    "start": "849000",
    "end": "849000"
  },
  {
    "text": "of computation going on so what are the",
    "start": "850630",
    "end": "853750"
  },
  {
    "text": "drawbacks with weight training methods",
    "start": "853750",
    "end": "856390"
  },
  {
    "text": "so the first drawback is that the weight",
    "start": "856390",
    "end": "858790"
  },
  {
    "text": "train network is sensitive to",
    "start": "858790",
    "end": "860160"
  },
  {
    "text": "initialization and as the results",
    "start": "860160",
    "end": "862420"
  },
  {
    "text": "multiple search runs are required to",
    "start": "862420",
    "end": "864610"
  },
  {
    "text": "find a good architecture so we don't",
    "start": "864610",
    "end": "867430"
  },
  {
    "text": "train just a single weight training",
    "start": "867430",
    "end": "869380"
  },
  {
    "text": "network in order to find a good",
    "start": "869380",
    "end": "870880"
  },
  {
    "text": "architecture we wind up having to repeat",
    "start": "870880",
    "end": "873280"
  },
  {
    "text": "this process four times in order to be",
    "start": "873280",
    "end": "875740"
  },
  {
    "text": "more robust to the weight initialization",
    "start": "875740",
    "end": "877720"
  },
  {
    "text": "of the weight training Network a second",
    "start": "877720",
    "end": "881320"
  },
  {
    "text": "drawback is that the shared wastes do",
    "start": "881320",
    "end": "882880"
  },
  {
    "text": "not quantify the standalone performance",
    "start": "882880",
    "end": "884950"
  },
  {
    "text": "of the architectures so the",
    "start": "884950",
    "end": "887020"
  },
  {
    "text": "architectures the resulting",
    "start": "887020",
    "end": "889060"
  },
  {
    "text": "architectures that the wage sharing",
    "start": "889060",
    "end": "890500"
  },
  {
    "text": "network finds need to be retrained from",
    "start": "890500",
    "end": "893230"
  },
  {
    "text": "scratch in order to provide the final",
    "start": "893230",
    "end": "896580"
  },
  {
    "text": "final results so in the case of the dart",
    "start": "896580",
    "end": "900520"
  },
  {
    "text": "search space the retraining process",
    "start": "900520",
    "end": "902860"
  },
  {
    "text": "required 600 epochs and this is done for",
    "start": "902860",
    "end": "906250"
  },
  {
    "text": "each of the four final architectures",
    "start": "906250",
    "end": "907990"
  },
  {
    "text": "that were found by the way Shane network",
    "start": "907990",
    "end": "911100"
  },
  {
    "text": "so there is a lot of computation going",
    "start": "911100",
    "end": "913750"
  },
  {
    "text": "on the third drawback is that CID in the",
    "start": "913750",
    "end": "918310"
  },
  {
    "start": "916000",
    "end": "916000"
  },
  {
    "text": "art nas methods are sensitive to",
    "start": "918310",
    "end": "920050"
  },
  {
    "text": "internal hyper pembers and therefore",
    "start": "920050",
    "end": "922630"
  },
  {
    "text": "they're not robust and and are fairly",
    "start": "922630",
    "end": "925960"
  },
  {
    "text": "difficult to apply to new problems so if",
    "start": "925960",
    "end": "929050"
  },
  {
    "text": "we go back to the NAS bench 201 results",
    "start": "929050",
    "end": "931360"
  },
  {
    "text": "we saw there that weight sharing",
    "start": "931360",
    "end": "933700"
  },
  {
    "text": "actually underperformed random search",
    "start": "933700",
    "end": "935830"
  },
  {
    "text": "which is",
    "start": "935830",
    "end": "937130"
  },
  {
    "text": "and very baseline standard baseline",
    "start": "937130",
    "end": "940250"
  },
  {
    "text": "hyper parameter optimization method and",
    "start": "940250",
    "end": "943000"
  },
  {
    "text": "this suggests that weight sharing is not",
    "start": "943000",
    "end": "946100"
  },
  {
    "text": "ready for primetime just yet",
    "start": "946100",
    "end": "948530"
  },
  {
    "text": "and right now I think the easiest way",
    "start": "948530",
    "end": "952250"
  },
  {
    "text": "and most robust way to get started with",
    "start": "952250",
    "end": "954920"
  },
  {
    "text": "nas is basically to run an early",
    "start": "954920",
    "end": "957980"
  },
  {
    "text": "stopping method like Asha on the nos",
    "start": "957980",
    "end": "960830"
  },
  {
    "text": "problem so ah she is an algorithm that",
    "start": "960830",
    "end": "964910"
  },
  {
    "text": "is available within determined and you",
    "start": "964910",
    "end": "966800"
  },
  {
    "text": "can go and try it out on the dart search",
    "start": "966800",
    "end": "969410"
  },
  {
    "text": "face and these two results here we're",
    "start": "969410",
    "end": "973970"
  },
  {
    "text": "comparing a few different hyper",
    "start": "973970",
    "end": "975860"
  },
  {
    "text": "Cranberry optimization algorithms on the",
    "start": "975860",
    "end": "978740"
  },
  {
    "text": "two search base is considered by darts",
    "start": "978740",
    "end": "981910"
  },
  {
    "text": "so the CNN and the RN and search face",
    "start": "981910",
    "end": "984680"
  },
  {
    "text": "and you see that Asha outperforms two",
    "start": "984680",
    "end": "988220"
  },
  {
    "text": "other early stopping hyper cranberry",
    "start": "988220",
    "end": "990290"
  },
  {
    "text": "optimization methods so you can go and",
    "start": "990290",
    "end": "993140"
  },
  {
    "text": "try this out yourself if you want the",
    "start": "993140",
    "end": "996740"
  },
  {
    "start": "996000",
    "end": "996000"
  },
  {
    "text": "next observation is that reproducibility",
    "start": "996740",
    "end": "998780"
  },
  {
    "text": "is still a challenge for nos so if we",
    "start": "998780",
    "end": "1002290"
  },
  {
    "text": "look at the results for the penn",
    "start": "1002290",
    "end": "1003790"
  },
  {
    "text": "treebank data set using the dart search",
    "start": "1003790",
    "end": "1006730"
  },
  {
    "text": "base we see that there are kind of",
    "start": "1006730",
    "end": "1010900"
  },
  {
    "text": "different signals coming from different",
    "start": "1010900",
    "end": "1012850"
  },
  {
    "text": "papers so the first one first",
    "start": "1012850",
    "end": "1016000"
  },
  {
    "text": "observation is that there are two random",
    "start": "1016000",
    "end": "1018040"
  },
  {
    "text": "search base lines provided by the e nós",
    "start": "1018040",
    "end": "1020170"
  },
  {
    "text": "paper and the darts paper present",
    "start": "1020170",
    "end": "1022120"
  },
  {
    "text": "different results second observation is",
    "start": "1022120",
    "end": "1024880"
  },
  {
    "text": "that when we try to reproduce",
    "start": "1024880",
    "end": "1027100"
  },
  {
    "text": "enos we actually got a significantly",
    "start": "1027100",
    "end": "1029438"
  },
  {
    "text": "worse result than the original number",
    "start": "1029439",
    "end": "1031810"
  },
  {
    "text": "published in the paper and the fact that",
    "start": "1031810",
    "end": "1035500"
  },
  {
    "text": "reproducibility is hard for nos is not",
    "start": "1035500",
    "end": "1037569"
  },
  {
    "text": "challenging or is not surprising because",
    "start": "1037569",
    "end": "1040230"
  },
  {
    "text": "nos algorithms as you've seen before",
    "start": "1040230",
    "end": "1043120"
  },
  {
    "text": "require a lot of tuning they're",
    "start": "1043120",
    "end": "1044890"
  },
  {
    "text": "sensitive to random seeds and there are",
    "start": "1044890",
    "end": "1047230"
  },
  {
    "text": "also a lot of compute environment",
    "start": "1047230",
    "end": "1048820"
  },
  {
    "text": "factors to track like GPU type CUDA",
    "start": "1048820",
    "end": "1051550"
  },
  {
    "text": "version deep learning framework Python",
    "start": "1051550",
    "end": "1053380"
  },
  {
    "text": "version and the list just goes on so how",
    "start": "1053380",
    "end": "1056980"
  },
  {
    "text": "can we use determined or a similar deep",
    "start": "1056980",
    "end": "1059620"
  },
  {
    "start": "1057000",
    "end": "1057000"
  },
  {
    "text": "learning platform to improve the",
    "start": "1059620",
    "end": "1061990"
  },
  {
    "text": "reproducibility of nos so some questions",
    "start": "1061990",
    "end": "1066190"
  },
  {
    "text": "that I found myself encountering",
    "start": "1066190",
    "end": "1068680"
  },
  {
    "text": "I was doing nice research where things",
    "start": "1068680",
    "end": "1070720"
  },
  {
    "text": "like which tensorflow and CUDA version",
    "start": "1070720",
    "end": "1072970"
  },
  {
    "text": "did I use to run these experiments which",
    "start": "1072970",
    "end": "1075730"
  },
  {
    "text": "version of the code did I used to get",
    "start": "1075730",
    "end": "1078580"
  },
  {
    "text": "these results and which checkpoint",
    "start": "1078580",
    "end": "1081780"
  },
  {
    "text": "actually gave me the best result that I",
    "start": "1081780",
    "end": "1084700"
  },
  {
    "text": "found from a bunch of different runs",
    "start": "1084700",
    "end": "1088090"
  },
  {
    "text": "so before determine my experiment",
    "start": "1088090",
    "end": "1091150"
  },
  {
    "text": "tracking was done in my file system and",
    "start": "1091150",
    "end": "1093280"
  },
  {
    "text": "that was somewhat of a nightmare",
    "start": "1093280",
    "end": "1096250"
  },
  {
    "text": "and with determined you're able to track",
    "start": "1096250",
    "end": "1099010"
  },
  {
    "text": "these things a lot easier with features",
    "start": "1099010",
    "end": "1103090"
  },
  {
    "text": "such as containerized training where",
    "start": "1103090",
    "end": "1104950"
  },
  {
    "text": "your experiments are run within a docker",
    "start": "1104950",
    "end": "1107260"
  },
  {
    "text": "image we also offer experiment tracking",
    "start": "1107260",
    "end": "1110260"
  },
  {
    "text": "where the exact code that is run is",
    "start": "1110260",
    "end": "1112570"
  },
  {
    "text": "saved and associated with each",
    "start": "1112570",
    "end": "1114790"
  },
  {
    "text": "experiment and also checkpoint tracking",
    "start": "1114790",
    "end": "1118110"
  },
  {
    "text": "with different determined allows us to",
    "start": "1118110",
    "end": "1120940"
  },
  {
    "text": "associate checkpoints with each of our",
    "start": "1120940",
    "end": "1122740"
  },
  {
    "text": "experiments as well so with with a",
    "start": "1122740",
    "end": "1126220"
  },
  {
    "text": "platform like determine you're able to",
    "start": "1126220",
    "end": "1128290"
  },
  {
    "text": "reproduce your experiments with a click",
    "start": "1128290",
    "end": "1130120"
  },
  {
    "text": "of a button the third observation about",
    "start": "1130120",
    "end": "1134110"
  },
  {
    "start": "1132000",
    "end": "1132000"
  },
  {
    "text": "nos and practice is that many nas",
    "start": "1134110",
    "end": "1136330"
  },
  {
    "text": "methods require training the resulting",
    "start": "1136330",
    "end": "1138370"
  },
  {
    "text": "architectures from scratch and this can",
    "start": "1138370",
    "end": "1141460"
  },
  {
    "text": "be very expensive and time-consuming so",
    "start": "1141460",
    "end": "1144190"
  },
  {
    "text": "if we go back to the image net results",
    "start": "1144190",
    "end": "1146020"
  },
  {
    "text": "that we showed in the beginning the gaya",
    "start": "1146020",
    "end": "1148830"
  },
  {
    "text": "architecture that we found required",
    "start": "1148830",
    "end": "1152470"
  },
  {
    "text": "training on age and video v 100 GPUs for",
    "start": "1152470",
    "end": "1155770"
  },
  {
    "text": "around 70 hours which costs $1500 and",
    "start": "1155770",
    "end": "1159670"
  },
  {
    "text": "required writing native PI torch data",
    "start": "1159670",
    "end": "1162310"
  },
  {
    "text": "parallel code without determined and",
    "start": "1162310",
    "end": "1164730"
  },
  {
    "text": "with determined we're able to make it",
    "start": "1164730",
    "end": "1167230"
  },
  {
    "text": "much easier to do distributor training",
    "start": "1167230",
    "end": "1169390"
  },
  {
    "text": "and we're also able to reduce the cost",
    "start": "1169390",
    "end": "1172600"
  },
  {
    "text": "from endlessly with preemptable",
    "start": "1172600",
    "end": "1174910"
  },
  {
    "text": "instances so if we look at the single",
    "start": "1174910",
    "end": "1179440"
  },
  {
    "text": "node result for determine on preemptable",
    "start": "1179440",
    "end": "1183040"
  },
  {
    "text": "instances the cost is reduced to just",
    "start": "1183040",
    "end": "1184990"
  },
  {
    "text": "500 dollars determine also makes it easy",
    "start": "1184990",
    "end": "1188170"
  },
  {
    "text": "to distribute training across multiple",
    "start": "1188170",
    "end": "1190930"
  },
  {
    "text": "nodes so you can train on 16 Nvidia K 80",
    "start": "1190930",
    "end": "1194890"
  },
  {
    "text": "GPUs and get about the same performance",
    "start": "1194890",
    "end": "1197380"
  },
  {
    "text": "but only use $250",
    "start": "1197380",
    "end": "1201890"
  },
  {
    "text": "and that also means that you can use to",
    "start": "1201890",
    "end": "1203840"
  },
  {
    "text": "distribute training and better GPUs to",
    "start": "1203840",
    "end": "1206150"
  },
  {
    "text": "speed up the data parallel training so",
    "start": "1206150",
    "end": "1211250"
  },
  {
    "text": "that's it for my talk and if you're",
    "start": "1211250",
    "end": "1214040"
  },
  {
    "text": "interested in any other things that I",
    "start": "1214040",
    "end": "1215900"
  },
  {
    "text": "brought up in the slides please check",
    "start": "1215900",
    "end": "1219020"
  },
  {
    "text": "out our papers for more details I'd like",
    "start": "1219020",
    "end": "1222050"
  },
  {
    "text": "to especially thank my collaborators on",
    "start": "1222050",
    "end": "1224240"
  },
  {
    "text": "each of these papers and yeah feel free",
    "start": "1224240",
    "end": "1229130"
  },
  {
    "text": "to take determent for a test run it's an",
    "start": "1229130",
    "end": "1231800"
  },
  {
    "text": "open source project so you can just",
    "start": "1231800",
    "end": "1233270"
  },
  {
    "text": "download the code from github and spin",
    "start": "1233270",
    "end": "1237320"
  },
  {
    "text": "up your own cluster and start trying",
    "start": "1237320",
    "end": "1239570"
  },
  {
    "text": "myself thank you",
    "start": "1239570",
    "end": "1243460"
  }
]