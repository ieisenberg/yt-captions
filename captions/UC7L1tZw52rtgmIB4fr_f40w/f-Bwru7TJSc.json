[
  {
    "text": "um so so hey everyone uh I'm Mark I I work on pythorch I'm an applied AI engineer there",
    "start": "7379",
    "end": "13200"
  },
  {
    "text": "um and thank you for joining this talk I'm going to be talking to you about like a tool we built to make large-scale training a bit easier by using a new",
    "start": "13200",
    "end": "19500"
  },
  {
    "text": "product we have called torch X and NRA which I which you're hopefully all familiar with",
    "start": "19500",
    "end": "24900"
  },
  {
    "text": "so so quickly about me like I uh like pytos open source so I spend the",
    "start": "24900",
    "end": "31140"
  },
  {
    "text": "majority of my time on GitHub like contributing to like by torch like torch surf or model inferencing towards data",
    "start": "31140",
    "end": "36660"
  },
  {
    "text": "for data loading torch X for infrastructure management the subject of this talk and examples and a bunch of",
    "start": "36660",
    "end": "42840"
  },
  {
    "text": "miscellaneous stuff so for the most part like what I really worry about is do people have a good time outside of meta",
    "start": "42840",
    "end": "50280"
  },
  {
    "text": "deploying models to production and historically this has been a pain point because at meta this is an easy problem",
    "start": "50280",
    "end": "56460"
  },
  {
    "text": "you you write a Json file and you you click submit and that's how you train models but outside but outside of this",
    "start": "56460",
    "end": "62820"
  },
  {
    "text": "bizarre world people like you know need to write scripts they need to set up their infrastructure they need to set up",
    "start": "62820",
    "end": "67860"
  },
  {
    "text": "their data and this has historically been hard so at a high level this is really what",
    "start": "67860",
    "end": "74520"
  },
  {
    "text": "door checks is like door checks is a way to run an application by an application",
    "start": "74520",
    "end": "79619"
  },
  {
    "text": "I mean like you're like you know your model.py on different schedulers so like",
    "start": "79619",
    "end": "85080"
  },
  {
    "text": "if you can imagine your infrastructure as something like maybe I don't know maybe maybe it's like a bare metal machine that's like your Local Host",
    "start": "85080",
    "end": "91259"
  },
  {
    "text": "maybe it's slurm uh maybe it's kubernetes maybe it's Rey which is the",
    "start": "91259",
    "end": "96780"
  },
  {
    "text": "subject of the stock right um and so at a high level here like this is a this is a really simple like Hello",
    "start": "96780",
    "end": "103200"
  },
  {
    "text": "World application it's like print something to console so my app is just like literally a printello world uh but",
    "start": "103200",
    "end": "109259"
  },
  {
    "text": "but the benefit of this is that like in in usually in a lot of applications people may have like different scheduling Stacks it's hard to get",
    "start": "109259",
    "end": "116340"
  },
  {
    "text": "people to agree on which scheduling stack to use or just like throw away their code and even for us at meta like",
    "start": "116340",
    "end": "121920"
  },
  {
    "text": "there isn't really a charter to tell people you should use the scheduler and so having this kind of optionality is",
    "start": "121920",
    "end": "127200"
  },
  {
    "text": "like really really helpful so the the the the the heart of the problem",
    "start": "127200",
    "end": "133560"
  },
  {
    "text": "is this or really the the root of the problem is that you know research feels good research because you have like lots",
    "start": "133560",
    "end": "139319"
  },
  {
    "text": "of pre-trained models uh projects have a due date as in like when you're done you're done like you just like you say",
    "start": "139319",
    "end": "144360"
  },
  {
    "text": "forget everything you wrote you move on to the next like cool thing uh there's not too many abstractions or at least by",
    "start": "144360",
    "end": "150300"
  },
  {
    "text": "abstractions I mean like human created abstractions and tools you need to learn about like the domain is complex as in",
    "start": "150300",
    "end": "156540"
  },
  {
    "text": "like the problems you're wrestling with are hard but it's really more property of the problem like if anything like",
    "start": "156540",
    "end": "162120"
  },
  {
    "text": "what you're wrestling with is like I don't know the foundations of the universe are not necessarily like other people uh production is very different like",
    "start": "162120",
    "end": "168480"
  },
  {
    "text": "production is well you have lots of options if if you happen to be successful that somewhat feels worse",
    "start": "168480",
    "end": "174360"
  },
  {
    "text": "because now you actually have to maintain and you're like subject to your like bad past decisions you need to go",
    "start": "174360",
    "end": "179580"
  },
  {
    "text": "learn like OS operating systems you know uh Hardware Cuda optimization libraries",
    "start": "179580",
    "end": "185300"
  },
  {
    "text": "uh you're always debating like should I have created like my own solution should I use a library should I use a service",
    "start": "185300",
    "end": "192620"
  },
  {
    "text": "the majority of abstractions are unintuitive you're just like spending the majority of your time like Googling and pipelining stuff together and if",
    "start": "192620",
    "end": "199200"
  },
  {
    "text": "anything you're instead of wrestling with the universe you're wrestling with sales peoples and PMs um",
    "start": "199200",
    "end": "204720"
  },
  {
    "text": "and so and so the problem here is like you're like the the the the the there is",
    "start": "204720",
    "end": "210480"
  },
  {
    "text": "like a benefit to open source in that like because everything is there for you you just like go do the thing that you",
    "start": "210480",
    "end": "216480"
  },
  {
    "text": "need instead of like like asking someone to do it which is always going to be slower because they may not like share your same uh priorities",
    "start": "216480",
    "end": "223980"
  },
  {
    "text": "so there's this like proverb that I that I see is super popular which is like bad workers like always blame their tools",
    "start": "223980",
    "end": "230159"
  },
  {
    "text": "like I I really hate this proverb because I feel like you know good workers improve their tools and so I",
    "start": "230159",
    "end": "236099"
  },
  {
    "text": "think historically and Ops people have been complaining like that stuff is hard I don't like using this",
    "start": "236099",
    "end": "242220"
  },
  {
    "text": "um doesn't have to be this way so the key problems like you know we",
    "start": "242220",
    "end": "247319"
  },
  {
    "text": "were looking at for this project were uh people were telling us like you know we don't like setting up an infrastructure",
    "start": "247319",
    "end": "253379"
  },
  {
    "text": "like I just have my model.py I want to run it and now like I need to go learn like you know how to set up an infra I",
    "start": "253379",
    "end": "259139"
  },
  {
    "text": "don't know how to turn things off uh submitting jobs against that infrastructure is like non-intuitive",
    "start": "259139",
    "end": "264419"
  },
  {
    "text": "because they're not necessarily built with like machine learning in mind you may want to get like logs or a job",
    "start": "264419",
    "end": "270000"
  },
  {
    "text": "status and if your logs are distributed over multiple machines how do you even do that and even like how do you go from",
    "start": "270000",
    "end": "275759"
  },
  {
    "text": "like your model.py to like an actual leak an end-to-end application",
    "start": "275759",
    "end": "281240"
  },
  {
    "text": "so I think that the reason why open source works so well is that like the users of",
    "start": "281280",
    "end": "286919"
  },
  {
    "text": "a product are fundamentally more aware of its limitations than the maintainers so often like usability issues like it's",
    "start": "286919",
    "end": "294840"
  },
  {
    "text": "it's my end users that understand them a lot better and it's meritocratic like a lot of times like I'll get feature",
    "start": "294840",
    "end": "299880"
  },
  {
    "text": "requests from completely Anonymous people that make like totally like valid points where I was like in a more traditional prioritization structure",
    "start": "299880",
    "end": "306000"
  },
  {
    "text": "like I don't know you need to be like a VP or something and influence the roadmap uh you control your own destiny",
    "start": "306000",
    "end": "311220"
  },
  {
    "text": "you add your own features you add your own helper functions and for me personally like what I feel is like the",
    "start": "311220",
    "end": "316500"
  },
  {
    "text": "benefit of having infrastructure tools be open source is that like they ensure it's long-term survival as in the best",
    "start": "316500",
    "end": "323100"
  },
  {
    "text": "open source tools aren't dependent on a single company so even if that company like implodes for whatever reason the",
    "start": "323100",
    "end": "330360"
  },
  {
    "text": "community still exists and the project still evolves and that's like the sign of a healthy open source project",
    "start": "330360",
    "end": "336300"
  },
  {
    "text": "so back to torch X right so with torch X like uh after like talking of like a",
    "start": "336300",
    "end": "341340"
  },
  {
    "text": "bunch of users like the verbs we came up with to sort of describe what the majority of our users wanted to do with this like there was a like a torch X run",
    "start": "341340",
    "end": "348300"
  },
  {
    "text": "which is you have your model.py you run a script and then you get a job ID back and then you may want to describe this",
    "start": "348300",
    "end": "355380"
  },
  {
    "text": "job as in what was the model.py which Docker image was this running on uh how",
    "start": "355380",
    "end": "361380"
  },
  {
    "text": "many resources are associated with this machine when was the job kicked off uh the job status you know that the job",
    "start": "361380",
    "end": "367020"
  },
  {
    "text": "fail did it succeed did it like did it set fault and then the logs like when you're actually trying to debug things",
    "start": "367020",
    "end": "373139"
  },
  {
    "text": "from multiple machines like actually like getting getting a grip as to like what actually happened",
    "start": "373139",
    "end": "379280"
  },
  {
    "text": "um so I do want to acknowledge like there's sort of like a lot of people that help like contribute to the story",
    "start": "379620",
    "end": "385440"
  },
  {
    "text": "um so like you know Jean Tristan Alex like Colin Gita Cuke Diamond among Jiao",
    "start": "385440",
    "end": "390660"
  },
  {
    "text": "jewels and Richard like all helped like tremendously um so let's sort of get to the meat of",
    "start": "390660",
    "end": "395699"
  },
  {
    "text": "how this this actually works",
    "start": "395699",
    "end": "398840"
  },
  {
    "text": "so the the example like we're going to cover is building a distributed training application from scratch and so uh if",
    "start": "400800",
    "end": "407039"
  },
  {
    "text": "you don't know how Python's distributed works this is great I'm going to give you a quick crash course as to like what does it mean to make a job distributed",
    "start": "407039",
    "end": "414840"
  },
  {
    "text": "um so the first part the first part of the problem was setting up the infrastructure right and so Ray has this",
    "start": "414840",
    "end": "421080"
  },
  {
    "text": "like really really nice feature called Ray up where you can give it like a like a yaml file for a cluster so for example",
    "start": "421080",
    "end": "426479"
  },
  {
    "text": "here this is using like an M5 like instance on AWS which are like cost effective CPUs it's using a deep",
    "start": "426479",
    "end": "433080"
  },
  {
    "text": "learning like like Army which has like a bunch of deep learning libraries pre-installed and it's using spot instances which are cheaper uh",
    "start": "433080",
    "end": "439860"
  },
  {
    "text": "personally like I found this to be like a far easier experience than like using ES CTL they're fairly similar but pretty",
    "start": "439860",
    "end": "446400"
  },
  {
    "text": "much like after typing in AWS configure giving my credentials Ray up like the thing just works and now now I can have",
    "start": "446400",
    "end": "452460"
  },
  {
    "text": "like a cluster with like hundreds of machines and not not worry too much about maintaining it ah the next the next part is just",
    "start": "452460",
    "end": "459120"
  },
  {
    "text": "installing torch X so you want to install a torch hooks you want to install right like nothing too surprising uh but when it comes to actually",
    "start": "459120",
    "end": "465060"
  },
  {
    "text": "submitting a job this is really what the high level structure looks like so you're saying like tour checks to run so run a job dash s on a scheduler in this",
    "start": "465060",
    "end": "472319"
  },
  {
    "text": "case array with a configuration of the dashboard address like where you're submitting a job the working directory",
    "start": "472319",
    "end": "478500"
  },
  {
    "text": "which is the files like the models like everything that you want to pass in your requirements.txt which is a list of",
    "start": "478500",
    "end": "484800"
  },
  {
    "text": "dependencies you want to run since we don't want to hard code those and we're also passing in this interesting argument called this this dot DDP which",
    "start": "484800",
    "end": "492360"
  },
  {
    "text": "is basically saying oh make this a DDP job with Dash j a single replica and two",
    "start": "492360",
    "end": "498479"
  },
  {
    "text": "workers so here we're running like a distributed job with with two nodes and the actual script you'd give would",
    "start": "498479",
    "end": "504599"
  },
  {
    "text": "be like your train.py so in this case where like this example is using like torch Rec which is like a new domain",
    "start": "504599",
    "end": "509639"
  },
  {
    "text": "Library we have for recommendation systems but at a high level this works like regardless of the kind of script you're running",
    "start": "509639",
    "end": "516740"
  },
  {
    "text": "um typically like you know we've seen users like want to configure their jobs in like one of two ways like sometimes",
    "start": "516779",
    "end": "522300"
  },
  {
    "text": "like people like the CLI experience other times people like the like python configuration I personally like the",
    "start": "522300",
    "end": "528240"
  },
  {
    "text": "python configuration because I can like set it up and then other people can inherit it from it and then like add",
    "start": "528240",
    "end": "534240"
  },
  {
    "text": "more stuff to it but these essentially mean the same thing the only real addition here like to the to the",
    "start": "534240",
    "end": "540180"
  },
  {
    "text": "previous CLI Flags is this like number a number of resources where you can constrain a certain job to have a",
    "start": "540180",
    "end": "545640"
  },
  {
    "text": "certain number of CPU nodes a certain number of GPU nodes a certain like amount of ram which when you combine it",
    "start": "545640",
    "end": "550740"
  },
  {
    "text": "with Rey up is like a good way of like having like resource isolation per job without necessarily even like",
    "start": "550740",
    "end": "558060"
  },
  {
    "text": "being you know too aware of like anything else like it's just it's it's pretty much just a flag and you can get this stuff working",
    "start": "558060",
    "end": "564860"
  },
  {
    "text": "um so this is the crash course that Fighters distributed and I realize that this is a long example uh but there's",
    "start": "565080",
    "end": "570360"
  },
  {
    "text": "like really two details that I want you to pay attention to here the first is that you know when you open up the",
    "start": "570360",
    "end": "575760"
  },
  {
    "text": "python distributed docs you may see the stern these terms like Rank and World size and I think these are just like terrible terms but they're they're",
    "start": "575760",
    "end": "582060"
  },
  {
    "text": "pretty much like borrowed from MPI which is like the Legacy Way of of doing distributed jobs a rank basically means",
    "start": "582060",
    "end": "588480"
  },
  {
    "text": "like my ID like which of the nodes in like a distributed jobs let's say there's 10 nodes so that's the world",
    "start": "588480",
    "end": "594180"
  },
  {
    "text": "size is 10 and then if my rank is five that means I'm the fifth node among those ten nodes right so this is pretty",
    "start": "594180",
    "end": "600839"
  },
  {
    "text": "much how a lot of distributed training Legacy Works which is you each each node has its own like Rank and World size",
    "start": "600839",
    "end": "607440"
  },
  {
    "text": "which you set up with environment variables so you would need to spin up let's say 10 machines SSH into the",
    "start": "607440",
    "end": "613440"
  },
  {
    "text": "machine set up those environment variables make sure the nodes can talk to each other and then you can start",
    "start": "613440",
    "end": "619320"
  },
  {
    "text": "training something this is obviously a pain and if you go to like I don't know like the python is distributed like like",
    "start": "619320",
    "end": "624720"
  },
  {
    "text": "help dogs a lot of people are like like the majority of issues are like setup issues people just like set up their",
    "start": "624720",
    "end": "630240"
  },
  {
    "text": "infrastructure wrong they don't have like a good way of like shipping their infrastructure to us so I can debug it",
    "start": "630240",
    "end": "635700"
  },
  {
    "text": "like I can't SSH into it and help them um so that's why like these setup scripts are really really important so",
    "start": "635700",
    "end": "640800"
  },
  {
    "text": "there's a couple of options here like torch run but but tort checks like is basically like a wrapper on top of that that makes this even easier so great so",
    "start": "640800",
    "end": "648420"
  },
  {
    "text": "let's say so now each node set up its own uh rank in a world size you want to also set up like a way for like nodes to",
    "start": "648420",
    "end": "655320"
  },
  {
    "text": "talk to each other so there's a couple of like communication protocols for this there's like glue there's like nickel for like",
    "start": "655320",
    "end": "662519"
  },
  {
    "text": "Nvidia gpus uh they're all like basically controlled by like a single like parameter flux used to exchange the",
    "start": "662519",
    "end": "668279"
  },
  {
    "text": "string and can experiment with like different like communication protocols and the important detail here is the",
    "start": "668279",
    "end": "673560"
  },
  {
    "text": "when you look at that this dot all reduce that dot this dot all reduce parentheses T what we're effectively",
    "start": "673560",
    "end": "680459"
  },
  {
    "text": "doing here is we're trying to get all the nodes to agree to what the world size is but the same thing would apply",
    "start": "680459",
    "end": "686760"
  },
  {
    "text": "if you wanted to agree to like let's say your gradients instead of like reducing over like an integer you would do like a",
    "start": "686760",
    "end": "693180"
  },
  {
    "text": "reduction over a tensor but the same idea applies and if you understand this you understand how distributed training",
    "start": "693180",
    "end": "698820"
  },
  {
    "text": "Works in pi torch so we we like so all this time like I've",
    "start": "698820",
    "end": "705120"
  },
  {
    "text": "been talking about torchek's run and once you say torch X run you would get like this like long string of a job idea",
    "start": "705120",
    "end": "711300"
  },
  {
    "text": "the array slash store checks with an IP erase them you know we're working on making this like shorter",
    "start": "711300",
    "end": "716700"
  },
  {
    "text": "um but at a high level like once you have this URL like that's basically the unique like ID that you can use to then",
    "start": "716700",
    "end": "722100"
  },
  {
    "text": "like describe the job get its status and then also later get its logs",
    "start": "722100",
    "end": "727399"
  },
  {
    "text": "um so the log is in this case will look something like this right so each process or like each each node will",
    "start": "728579",
    "end": "736560"
  },
  {
    "text": "basically print you know whatever you had in your job and here it's going to say well we initialized like a process",
    "start": "736560",
    "end": "741600"
  },
  {
    "text": "group so like a group of processes I need to communicate with each other so in distributed training to analysis called gank scheduling you're scheduling",
    "start": "741600",
    "end": "747899"
  },
  {
    "text": "a bunch of jobs that need to like collaborate with each other and then you can see here there's two different like command actors that are each saying that",
    "start": "747899",
    "end": "754079"
  },
  {
    "text": "each have their own process ID and then once they agree on the world size you can see that like in both of the logs",
    "start": "754079",
    "end": "759959"
  },
  {
    "text": "this is great because the alternative to this before was I need to actually SSH into like a given a given node and then",
    "start": "759959",
    "end": "767339"
  },
  {
    "text": "like figure out like what happened here and so I would literally before I discovered door checks I would have like",
    "start": "767339",
    "end": "772620"
  },
  {
    "text": "four terminals open and bshing into each one with like tail.f just just kind of not not a great experience",
    "start": "772620",
    "end": "780620"
  },
  {
    "text": "um so let me briefly talk about like how it works right like at a high level the way it works is we you have a configuration of like the kind of job",
    "start": "780720",
    "end": "786959"
  },
  {
    "text": "you want to have as a python object uh so we call this a component and this component is serializable and so given",
    "start": "786959",
    "end": "794760"
  },
  {
    "text": "this component definition we use like a new project called The Raid job API to",
    "start": "794760",
    "end": "799860"
  },
  {
    "text": "submit jobs against the cluster and then this the rate job API would run",
    "start": "799860",
    "end": "805200"
  },
  {
    "text": "a driver script which I will describe in a second which if you if you used rate before it's sort of like it's like sort of like your main function and this this",
    "start": "805200",
    "end": "812399"
  },
  {
    "text": "main function and this main function takes care of like your resource requirements uh your like in the number",
    "start": "812399",
    "end": "818760"
  },
  {
    "text": "of replicas you have like the environment variables that I just talked about with the rank and World size and then each of these processes are actors",
    "start": "818760",
    "end": "825600"
  },
  {
    "text": "and rate in Ray terminology would do like array get to get back these logs so let me just so this is really the",
    "start": "825600",
    "end": "831060"
  },
  {
    "text": "picture here um so oh sorry the the torch actually very scheduling here would essentially",
    "start": "831060",
    "end": "837240"
  },
  {
    "text": "take a torch X component use the driver's script and then you can use this for submitting a job getting job",
    "start": "837240",
    "end": "842820"
  },
  {
    "text": "status and the exact same architecture applies regardless of the kind of scheduler you're using so if you happen",
    "start": "842820",
    "end": "849000"
  },
  {
    "text": "to be using kubernetes or slurm it's sort of the same thing but there are some real benefits as to why I think you",
    "start": "849000",
    "end": "854160"
  },
  {
    "text": "should probably consider the ray one which I'll talk about in a second um so yeah like if you want to serialize",
    "start": "854160",
    "end": "860700"
  },
  {
    "text": "stuff like python data classes are great uh essentially they give you like a great idea of like what is like what",
    "start": "860700",
    "end": "867240"
  },
  {
    "text": "what is what is the input parameter of something here so in this case we're calling it array actor which uh",
    "start": "867240",
    "end": "872459"
  },
  {
    "text": "unfortunately torch X also uses the actual terminology but it means a different thing from Ray so by actor and",
    "start": "872459",
    "end": "880139"
  },
  {
    "text": "torch X we really mean like a worker or like a replica which has like a name it has a certain command uh and then the",
    "start": "880139",
    "end": "887639"
  },
  {
    "text": "environment variables that I talked about here which would be again rank in World size are the most important ones and then some resource requirements like",
    "start": "887639",
    "end": "894660"
  },
  {
    "text": "number of CPUs and number of gpus so under the hood this uses like the the",
    "start": "894660",
    "end": "900120"
  },
  {
    "text": "rate job SDK so the rate job SDK here will take like a given like job ID the",
    "start": "900120",
    "end": "905339"
  },
  {
    "text": "entry point is basically what are you running when this job is first being executed so this is the ray driver",
    "start": "905339",
    "end": "910440"
  },
  {
    "text": "script and then the runtime environments which are again you know those those two famous like environment variables with",
    "start": "910440",
    "end": "915660"
  },
  {
    "text": "the rank and World size um I personally found like because like",
    "start": "915660",
    "end": "923699"
  },
  {
    "text": "in torch X like we have to be familiar with so many schedulers like I just want to give you like some idea of like how",
    "start": "923699",
    "end": "929579"
  },
  {
    "text": "easy it was to implement some stuff like for example let's say I want to get like the job status and I want to wait for",
    "start": "929579",
    "end": "935160"
  },
  {
    "text": "like a job to finish uh well in Ray like I just have like this get job status function that I give it a job ID and",
    "start": "935160",
    "end": "942120"
  },
  {
    "text": "then I can just like Pull It in a for loot in a while loop and then when it succeeds or fails I'm like great it",
    "start": "942120",
    "end": "947639"
  },
  {
    "text": "finished and and that was about it so I don't have to like wrestle with like reading kubernetes docs or anything I",
    "start": "947639",
    "end": "954000"
  },
  {
    "text": "can just like write python code I can look at the resource code and I can build stuff like build stuff on top of",
    "start": "954000",
    "end": "959220"
  },
  {
    "text": "it and this felt really good to me um the red driver script essentially",
    "start": "959220",
    "end": "964620"
  },
  {
    "text": "uses an abstraction called the command actor which is basically saying I want you to run this job with some extra",
    "start": "964620",
    "end": "970560"
  },
  {
    "text": "State uh by extra State I mean like a master address and an a port for example",
    "start": "970560",
    "end": "976019"
  },
  {
    "text": "because when you're running a distributed job they all need to be like all the nodes need to be aware of what the what the master node is",
    "start": "976019",
    "end": "983579"
  },
  {
    "text": "the actual Ray driver script here looks pretty simple so you'll see this like this is the like if you pay attention",
    "start": "983579",
    "end": "989880"
  },
  {
    "text": "here near the bottom at the if I equals zero zero so this is called like the so",
    "start": "989880",
    "end": "995519"
  },
  {
    "text": "you can see here the programming model is the single process multiple data because this is the same program that's",
    "start": "995519",
    "end": "1000680"
  },
  {
    "text": "running on all the nodes but the way we get different behavior is that we do an if condition on the port right like",
    "start": "1000680",
    "end": "1007040"
  },
  {
    "text": "sorry on on the rank so if your rank 0 that means like this is this is this is the master node and doesn't need to",
    "start": "1007040",
    "end": "1012980"
  },
  {
    "text": "communicate to anyone else but people need to communicate to it and then once once you've set up all",
    "start": "1012980",
    "end": "1019339"
  },
  {
    "text": "your command actors you basically have set up your District like you've set up the script that you want to run on all the nodes all the nodes are aware of",
    "start": "1019339",
    "end": "1025938"
  },
  {
    "text": "what the master port and master address is and all the nodes are aware of what their Rank and and the world sizes",
    "start": "1025939",
    "end": "1032860"
  },
  {
    "text": "um and then you know once you do that like things are really easy it's basically you launch like these",
    "start": "1034520",
    "end": "1039980"
  },
  {
    "text": "different reactors and to get the result from each actor you like run array.get",
    "start": "1039980",
    "end": "1045860"
  },
  {
    "text": "so I just want to be really clear here that this is not like a toy example that",
    "start": "1045860",
    "end": "1051679"
  },
  {
    "text": "I use to simplify the code like this is actually what the ray driver looks like",
    "start": "1051679",
    "end": "1056960"
  },
  {
    "text": "um so you just basically check well like you know are there any active workers if so like wait for the results and if",
    "start": "1056960",
    "end": "1062720"
  },
  {
    "text": "there's a result like just get it and dereference it so super easy",
    "start": "1062720",
    "end": "1068440"
  },
  {
    "text": "um so this has been kind of really interesting for us like at meta because like internally like I said we don't",
    "start": "1068480",
    "end": "1074900"
  },
  {
    "text": "necessarily use like the same kinds of schedulers that people use internally like you can really think of meta as",
    "start": "1074900",
    "end": "1080059"
  },
  {
    "text": "having like our own like Bizarro Cloud that's different from what people use in open source but this is great for us",
    "start": "1080059",
    "end": "1085640"
  },
  {
    "text": "because like if if there's something that works for us and we want other people to use it we can now literally",
    "start": "1085640",
    "end": "1090799"
  },
  {
    "text": "just like exchange a schedule or parameter so like same thing so like you're like let's say you haven't even",
    "start": "1090799",
    "end": "1095840"
  },
  {
    "text": "picked an infrastructure and you want to be like well let me try this Ray thing let me try this kubernetes thing well",
    "start": "1095840",
    "end": "1101000"
  },
  {
    "text": "you just change the flag it's not like you have to go like read a book and you know just like learn how the configuration works every time because",
    "start": "1101000",
    "end": "1106880"
  },
  {
    "text": "the the the the verbs we're trying to have are verbs that I would hope apply to people that are more like machine",
    "start": "1106880",
    "end": "1113780"
  },
  {
    "text": "learning practitioners instead of people that like love doing infrastructure like I got into infrastructure to make my",
    "start": "1113780",
    "end": "1119419"
  },
  {
    "text": "life easy not to learn infrastructure because I just like do I just like training models so as an example to this like was was",
    "start": "1119419",
    "end": "1127160"
  },
  {
    "text": "Star Trek right so torch wreck is kind of unique among like our domain libraries so torch Trek is our",
    "start": "1127160",
    "end": "1132500"
  },
  {
    "text": "recommendation systems library and the way a lot of recommender systems work is that like they aggregate like dense",
    "start": "1132500",
    "end": "1138620"
  },
  {
    "text": "features and also like sparse features like let's say who do you know like you know like who who do you who else do you",
    "start": "1138620",
    "end": "1144679"
  },
  {
    "text": "know like that's like relatively sparse data and running these is kind of tricky in",
    "start": "1144679",
    "end": "1150320"
  },
  {
    "text": "open source because like you need like very large embedding tables you need those to be sharded in some way so now",
    "start": "1150320",
    "end": "1156620"
  },
  {
    "text": "all of a sudden like you can't just like run it's not easy to run a hello world example but again with with the torchek",
    "start": "1156620",
    "end": "1163220"
  },
  {
    "text": "scheduler this was really easy and it made it really easy for us to get like people outside of meta to try out like",
    "start": "1163220",
    "end": "1169160"
  },
  {
    "text": "torch Rec because it's like oh hey you use Ray great like just change change this flag and you can like run this code",
    "start": "1169160",
    "end": "1174500"
  },
  {
    "text": "now um so this this is this is important because like even though like I showed",
    "start": "1174500",
    "end": "1180260"
  },
  {
    "text": "you like a simple hello world example like which was the hello world the distributed training like Thor track is a lot more complex like torch Rec has",
    "start": "1180260",
    "end": "1186919"
  },
  {
    "text": "like sharding policies for like a model it has like a C plus plus library for",
    "start": "1186919",
    "end": "1192200"
  },
  {
    "text": "inference API it has like a central like metrics module but that sort of doesn't matter like as long as you can launch it",
    "start": "1192200",
    "end": "1198980"
  },
  {
    "text": "on any scheduler that means you can run it on all of the schedulers and so uh",
    "start": "1198980",
    "end": "1204320"
  },
  {
    "text": "we're we're hoping here to create a story where people can experiment with different schedulers and not sort of create like these like top-down mandates",
    "start": "1204320",
    "end": "1210740"
  },
  {
    "text": "in a company around what scheduler you should be using um so yeah I mean the so so this is like",
    "start": "1210740",
    "end": "1217400"
  },
  {
    "text": "a Blog announcement that we worked on where like the case study for us was really like a proof point that you can",
    "start": "1217400",
    "end": "1222919"
  },
  {
    "text": "launch jobs in a scheduler agnostic manner in particular the example like in this in in this blog here like ran from",
    "start": "1222919",
    "end": "1229640"
  },
  {
    "text": "a notebook so from like within a collab notebook I like Rand Ray up I set up the",
    "start": "1229640",
    "end": "1234980"
  },
  {
    "text": "distributed jobs I ran the IRA I created a whole cluster and then I looked at the results like all from a notebook and",
    "start": "1234980",
    "end": "1241400"
  },
  {
    "text": "this was just like great for me because I felt like hey I'm just like in this like familiar experience but I have like this like Giga cluster uh at my disposal",
    "start": "1241400",
    "end": "1248900"
  },
  {
    "text": "I guess as long as you have the AWS credits for it uh but but but it just works right",
    "start": "1248900",
    "end": "1254960"
  },
  {
    "text": "so uh I I hope like the key takeaway here is that you know you don't really have to be a scientist and an infra",
    "start": "1254960",
    "end": "1262640"
  },
  {
    "text": "engineer to train large models unless you really want to like if you're interested in that like you know by all means don't let me stop you uh but but",
    "start": "1262640",
    "end": "1269600"
  },
  {
    "text": "there are like better tools to sort of make this to make this easier for you and not force you to learn about like everything to just run a job",
    "start": "1269600",
    "end": "1277400"
  },
  {
    "text": "um so we talked about like distributed training but there's sort of like I said like any any torch X app can run now on",
    "start": "1277400",
    "end": "1284539"
  },
  {
    "text": "Ray so this includes like stuff like serving a model with torch serve it includes like elastic job launching",
    "start": "1284539",
    "end": "1290539"
  },
  {
    "text": "which let's say you're running a distributed job and someone like trips over one of the nodes your whole job isn't going to collapse you know which",
    "start": "1290539",
    "end": "1296720"
  },
  {
    "text": "is what it would do uh you can like set up hyper parameter optimization there's metric logging even works with Python's",
    "start": "1296720",
    "end": "1303020"
  },
  {
    "text": "lightning if if that's like what you're more familiar with and yeah like I said thank you you know",
    "start": "1303020",
    "end": "1308840"
  },
  {
    "text": "give us a star like like I said open source is nice like just if there's anything you'd like us to build feel",
    "start": "1308840",
    "end": "1314780"
  },
  {
    "text": "free to open up a rant as a GitHub issue um and I would say like already the",
    "start": "1314780",
    "end": "1320000"
  },
  {
    "text": "community for me has been a good sign of uh like people have already started to improve the torch x-ray scheduler in",
    "start": "1320000",
    "end": "1325820"
  },
  {
    "text": "ways that I didn't think was possible and so that's why that's why I'm in open source um yeah like I said throw checks you",
    "start": "1325820",
    "end": "1332480"
  },
  {
    "text": "know it's a great way to run like scheduler agnostic jobs and okay so not I I've I've been talking",
    "start": "1332480",
    "end": "1339919"
  },
  {
    "text": "a lot about schedule agnosticity but like why should you use the race scheduler right and so I think the big",
    "start": "1339919",
    "end": "1345799"
  },
  {
    "text": "benefit here is that one like the it's all python so you don't need to Google anything you can just like look at the",
    "start": "1345799",
    "end": "1351919"
  },
  {
    "text": "source code and figure out what's going on it also means that I as I was like debugging distributed jobs I got to use",
    "start": "1351919",
    "end": "1357679"
  },
  {
    "text": "like the rate debugger which is I think is like fantastic to figure out like I just like set pdb across multiple nodes",
    "start": "1357679",
    "end": "1363559"
  },
  {
    "text": "and figure out what's going on as I was debugging my driver script uh and it was also easy to extend like we actually",
    "start": "1363559",
    "end": "1368780"
  },
  {
    "text": "have like a person an open source that I've never met that's like working really hard to add like elastic training",
    "start": "1368780",
    "end": "1374960"
  },
  {
    "text": "support on the scheduler like I've never talked to them but you know they just like went ahead and did it and I think",
    "start": "1374960",
    "end": "1380000"
  },
  {
    "text": "this is sort of like really the benefit of having like a library that's all python because if if you're if that's what your ml scientists are aware of",
    "start": "1380000",
    "end": "1386960"
  },
  {
    "text": "it's very easy for them to contribute to the infrastructure so you know why should you use the torch",
    "start": "1386960",
    "end": "1393620"
  },
  {
    "text": "x-ray scheduler like I think the the three words would be like it's pythonic it's like debuggable and it's scalable",
    "start": "1393620",
    "end": "1398980"
  },
  {
    "text": "and if anything I believe like you know my personal opinion is that these are probably the same reasons like why",
    "start": "1398980",
    "end": "1404780"
  },
  {
    "text": "pytharch was successful uh and so I hope like you know I can make uh you know like I hope I can make like",
    "start": "1404780",
    "end": "1411260"
  },
  {
    "text": "infrastructure management a bit less painful for you so uh thank you so much",
    "start": "1411260",
    "end": "1416620"
  },
  {
    "text": "um I will say like before we take questions if any of this was interesting to you and you know you're looking for a",
    "start": "1419480",
    "end": "1425659"
  },
  {
    "text": "job and you know you you don't want to go to an office like feel free to Ping us we're hiring people based on your",
    "start": "1425659",
    "end": "1431360"
  },
  {
    "text": "Scana those are I guess like for tax reasons but full-time remote is great like a lot of our team is distributed so",
    "start": "1431360",
    "end": "1436520"
  },
  {
    "text": "feel free to just talk to me afterwards thank you",
    "start": "1436520",
    "end": "1440860"
  },
  {
    "text": "okay",
    "start": "1442100",
    "end": "1444220"
  },
  {
    "text": "all right uh thanks for a talk this is my first time hearing about torch X it seems super promising uh so I come from",
    "start": "1452179",
    "end": "1457520"
  },
  {
    "text": "reinforced learning research world and my lab we have our own internal HPC",
    "start": "1457520",
    "end": "1462620"
  },
  {
    "text": "system so we just learn now using rayon's learn has not been the",
    "start": "1462620",
    "end": "1467720"
  },
  {
    "text": "best experience in in my in my personal experience experience and so this torch X make something like submitted launcher",
    "start": "1467720",
    "end": "1474500"
  },
  {
    "text": "for example um obsolete um which part does it make obsolete sorry",
    "start": "1474500",
    "end": "1480320"
  },
  {
    "text": "so the the other uh slurm launcher submitted yeah yeah so like I just want",
    "start": "1480320",
    "end": "1485480"
  },
  {
    "text": "to understand like you know where does torch X Stand you kind of like the ecosystem of libraries okay yeah that's",
    "start": "1485480",
    "end": "1490820"
  },
  {
    "text": "a good question actually so so internally like even like even within meta there's a lot of people that use slurm it's just that they don't have the",
    "start": "1490820",
    "end": "1497780"
  },
  {
    "text": "best experience with it because they need to manage the cluster and then they'll often have like these super long like CLI arguments",
    "start": "1497780",
    "end": "1504440"
  },
  {
    "text": "um so for us like I mean again it's more of a language thing but if you do happen to use both array and slurm like within",
    "start": "1504440",
    "end": "1512299"
  },
  {
    "text": "within your work then you can use array as a as a you can use Ray as a scheduler",
    "start": "1512299",
    "end": "1518900"
  },
  {
    "text": "and you can have the same script and you just like change the flag and now you can run it on your slurm scheduler so it",
    "start": "1518900",
    "end": "1524240"
  },
  {
    "text": "gives you like the flexibility instead of like I guess like trying to run slurm Andre or or vice versa uh you can just",
    "start": "1524240",
    "end": "1531500"
  },
  {
    "text": "like whatever like basically like Whoever has the capacity available just launch it so it's like there's going to be a pool of available compute that",
    "start": "1531500",
    "end": "1538279"
  },
  {
    "text": "you're sharing like with your colleagues in a lab uh and this gives you the flexibility to do so without like changing anything in your scripts",
    "start": "1538279",
    "end": "1544820"
  },
  {
    "text": "actually not a single word yeah and then how well does it integrate with something like a Hydra for config management or pytorch lightning because",
    "start": "1544820",
    "end": "1551840"
  },
  {
    "text": "they have their own kind of abstracted away uh assess them you know",
    "start": "1551840",
    "end": "1557480"
  },
  {
    "text": "um yeah I'm I'm not personally sure what we like the the story there but pretty much like as long as you can map the",
    "start": "1557480",
    "end": "1563720"
  },
  {
    "text": "configurations together uh I think it's sort of like easy to do it uh I I think",
    "start": "1563720",
    "end": "1569000"
  },
  {
    "text": "there was like a GitHub issue open maybe Kerman here knows like about like about Hydra uh but you know by all means like",
    "start": "1569000",
    "end": "1575539"
  },
  {
    "text": "if it's important to you then I think the team can quickly Build It Up thanks",
    "start": "1575539",
    "end": "1581380"
  },
  {
    "text": "I think we do support basic usage of Hydra or lightning from torch X it's essentially like torch X will launch it",
    "start": "1584779",
    "end": "1590240"
  },
  {
    "text": "as long as it's essentially living within the python layer yeah yeah lightning for sure like",
    "start": "1590240",
    "end": "1595520"
  },
  {
    "text": "lightning we have we have a full example called classy Vision there like as our hello world example so yeah it's it's a",
    "start": "1595520",
    "end": "1601159"
  },
  {
    "text": "great one to look at",
    "start": "1601159",
    "end": "1603760"
  },
  {
    "text": "hi it's a great talk thanks can you talk about elastic training are you doing",
    "start": "1612440",
    "end": "1618320"
  },
  {
    "text": "elastic training in torch X um yeah so in in general the answer to that is is yes uh we are like like it",
    "start": "1618320",
    "end": "1626960"
  },
  {
    "text": "all all of the schedules work with elastic training except Ray like that's the one where the community started",
    "start": "1626960",
    "end": "1632900"
  },
  {
    "text": "contributing it so I like I basically just more took of a like sidekick role",
    "start": "1632900",
    "end": "1638000"
  },
  {
    "text": "and let them do it but but essentially actually like torch X was like the the creator of torch X is the same creator",
    "start": "1638000",
    "end": "1644539"
  },
  {
    "text": "of search elastic just to be clear uh and so like a lot of the ideas are similar and just because like from from",
    "start": "1644539",
    "end": "1650779"
  },
  {
    "text": "our observation it's just like really difficult to deal with non-elastic training Beyond a certain number of node",
    "start": "1650779",
    "end": "1656900"
  },
  {
    "text": "count it's just like it feels like a must uh so like if you look at for example our classy Vision examples and",
    "start": "1656900",
    "end": "1662779"
  },
  {
    "text": "kubernetes or slurm like those those all currently have support for elastic training",
    "start": "1662779",
    "end": "1668200"
  },
  {
    "text": "hi uh thanks again for the talk so I'm trying to learn more about the scope of the torch X So based on the presentation",
    "start": "1668240",
    "end": "1675740"
  },
  {
    "text": "it looks like it's uh like okay or concentration of two orchestrate a single distributed task and given",
    "start": "1675740",
    "end": "1682640"
  },
  {
    "text": "specific scheduler right so um so is that the case like for example uh you beat this is between the",
    "start": "1682640",
    "end": "1689299"
  },
  {
    "text": "scheduler and the user application and basically do some kind of management about that one",
    "start": "1689299",
    "end": "1694460"
  },
  {
    "text": "um so do you have any uh so basic what's the next for the project so I like is",
    "start": "1694460",
    "end": "1699559"
  },
  {
    "text": "there going to be some kind of experiment concept drawing or like some kind of overflow you chain multiple job",
    "start": "1699559",
    "end": "1704900"
  },
  {
    "text": "together task dependency those are something uh because like right now it's still we're seeing a single digital job",
    "start": "1704900",
    "end": "1711559"
  },
  {
    "text": "right I think of this real training job so yeah what's next and what's creating scope of that",
    "start": "1711559",
    "end": "1717080"
  },
  {
    "text": "um yeah so like here I mostly talked about like the single train.py but for us like work like chaining workflows is",
    "start": "1717080",
    "end": "1724039"
  },
  {
    "text": "obviously sort of the the way people would actually use this as in you train",
    "start": "1724039",
    "end": "1729559"
  },
  {
    "text": "a model you pull up the artifacts you want to serve it like this is all possible like today I just didn't have",
    "start": "1729559",
    "end": "1737299"
  },
  {
    "text": "like too many too many slides about this but fundamentally you can think of all of these steps as being like components",
    "start": "1737299",
    "end": "1742340"
  },
  {
    "text": "and you can chain them together like and and that works just fine um so in general like though like like",
    "start": "1742340",
    "end": "1748760"
  },
  {
    "text": "the question around the scope I find interesting because uh you're right like depending on how view it like the scope",
    "start": "1748760",
    "end": "1754640"
  },
  {
    "text": "is sort of like a bit a bit nebulous right it's large um and and there are lots of alternatives to to do similar things I",
    "start": "1754640",
    "end": "1761600"
  },
  {
    "text": "think from from our observation is that like like if I'm talking to people is that if people have already invested in",
    "start": "1761600",
    "end": "1768559"
  },
  {
    "text": "ecosystems where they really deeply understand how to change workflows and existing Solutions uh we actually feel",
    "start": "1768559",
    "end": "1774740"
  },
  {
    "text": "like great you know just just use whatever makes you comfortable but a good chunk of our users have been giving",
    "start": "1774740",
    "end": "1780260"
  },
  {
    "text": "us the feedback that like okay I have my train.py how do I make it distributed I have my train.py how do I chain it into",
    "start": "1780260",
    "end": "1786980"
  },
  {
    "text": "workflows so again we we want more people to you know go to prod and this",
    "start": "1786980",
    "end": "1792860"
  },
  {
    "text": "storage checks ended up like being like a good answer for us to get users",
    "start": "1792860",
    "end": "1797919"
  },
  {
    "text": "will be able to provide the notion",
    "start": "1803260",
    "end": "1810398"
  },
  {
    "text": "so uh yeah repeating what I was stated before so for uh if you want to like run",
    "start": "1815480",
    "end": "1821600"
  },
  {
    "text": "multiple components there's a notion of Pipelines uh first thing second thing is for the",
    "start": "1821600",
    "end": "1827840"
  },
  {
    "text": "road mapping we are considering of adding additional features such as tracking your experiments and artifacts",
    "start": "1827840",
    "end": "1836200"
  },
  {
    "text": "all right well thank you everyone for joining really appreciate it um yeah if you have any more questions",
    "start": "1844760",
    "end": "1850279"
  },
  {
    "text": "feel free to ping me in if you're looking for a job full-time remote work on work on pie torch and Ray please please come talk to me uh thank you",
    "start": "1850279",
    "end": "1858760"
  }
]