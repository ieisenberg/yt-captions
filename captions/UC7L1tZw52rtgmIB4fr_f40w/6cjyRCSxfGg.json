[
  {
    "text": "hey guys uh thanks thanks for coming to our presentation we are going to present radio observability 2.0 on how to debug",
    "start": "3540",
    "end": "10500"
  },
  {
    "text": "your rare application with new observability tooling so before we start let us quickly",
    "start": "10500",
    "end": "16320"
  },
  {
    "text": "introduce ourselves I'm sangbin I'm a software engineer at any scale I have been working on the record team and",
    "start": "16320",
    "end": "22080"
  },
  {
    "text": "recently also worked on on observability features I'm out I'm Alan I work on the",
    "start": "22080",
    "end": "28920"
  },
  {
    "text": "experience team I work on observability features and recently I've been working on the public endpoints product",
    "start": "28920",
    "end": "34739"
  },
  {
    "text": "I'm a software engineer in any skill experience team I have been working on",
    "start": "34739",
    "end": "39840"
  },
  {
    "text": "observability and any scale platform okay so let's um dive into our agenda",
    "start": "39840",
    "end": "46500"
  },
  {
    "text": "so first we're going to talk about how how the array or observability looked like about last year",
    "start": "46500",
    "end": "52800"
  },
  {
    "text": "and we are going to Showcase all the new features we developed over the year using the ray job Ray job is a offline",
    "start": "52800",
    "end": "60120"
  },
  {
    "text": "workload basically something like training or on battery inference",
    "start": "60120",
    "end": "66080"
  },
  {
    "text": "how to develop Reserve application and for the missing part um that our",
    "start": "66680",
    "end": "72479"
  },
  {
    "text": "dashboard cannot solve Charles gonna explain how to solve this um product how to support the product",
    "start": "72479",
    "end": "78299"
  },
  {
    "text": "production use cases uh using any scale as an example",
    "start": "78299",
    "end": "83840"
  },
  {
    "text": "so as you many of you here already know debugging distributed system is very difficult why there are three more than",
    "start": "84000",
    "end": "91259"
  },
  {
    "text": "three reasons but three main reasons um firstly there are a lot of concurrent tasks um running in the cluster",
    "start": "91259",
    "end": "97799"
  },
  {
    "text": "and of course there are a lot of different failures from application to Hardware failures and it's also a very large scale which",
    "start": "97799",
    "end": "104340"
  },
  {
    "text": "means whenever something fails it's going to be so much more difficult to find a problem and solve the problem",
    "start": "104340",
    "end": "111600"
  },
  {
    "text": "which basically gives us conclusion um the good observability is a key feature for distributed systems and of course",
    "start": "111600",
    "end": "118500"
  },
  {
    "text": "foray which which the goal is to simplify distributed systems",
    "start": "118500",
    "end": "124380"
  },
  {
    "text": "so let's see how radio observability looked like when we have rate 2.0 like about last year about the same time we",
    "start": "124380",
    "end": "130739"
  },
  {
    "text": "had another presentation last year um we had array dashboard that shows the snapshot of the hardware resources or",
    "start": "130739",
    "end": "138120"
  },
  {
    "text": "the list of like machines and also processes we had the log viewer which allows you",
    "start": "138120",
    "end": "143640"
  },
  {
    "text": "to access all different logs in the cluster and we had a race State API it was a new",
    "start": "143640",
    "end": "149400"
  },
  {
    "text": "tool at the time which allows you to access um task actor um or any other way resources through CLI",
    "start": "149400",
    "end": "157020"
  },
  {
    "text": "and we had other tools such as debugger or um pre-built metrics or like",
    "start": "157020",
    "end": "162599"
  },
  {
    "text": "integration to Prometheus and things like that however although all these tools exist",
    "start": "162599",
    "end": "168420"
  },
  {
    "text": "they are all designed for very domain specific purpose and we found many users who had really hard time um debugging",
    "start": "168420",
    "end": "174599"
  },
  {
    "text": "end-to-end problem because there's uh because of lacking UI UI and you have to like Jump Around different places to",
    "start": "174599",
    "end": "181319"
  },
  {
    "text": "debug the problem it's our ux designer um Huawei is like",
    "start": "181319",
    "end": "186780"
  },
  {
    "text": "right there um he yeah he did a lot of visual research like talked to tens of users",
    "start": "186780",
    "end": "192060"
  },
  {
    "text": "and we basically figured out that it's very extremely critical to make the ray",
    "start": "192060",
    "end": "197459"
  },
  {
    "text": "dashboard as an all-in-one observability tool that provides the seamless UI experience at scale and that's why we",
    "start": "197459",
    "end": "204300"
  },
  {
    "text": "built over the year and let me try another demo using this fine tuning",
    "start": "204300",
    "end": "209340"
  },
  {
    "text": "workload that we took from the ray website um so we are going to find in the stable",
    "start": "209340",
    "end": "215400"
  },
  {
    "text": "division model or using a technique called dreambooth and the goal is very simple we just give",
    "start": "215400",
    "end": "221040"
  },
  {
    "text": "the image of this dog on the left side and we're going to run fine tuning and the goal is to basically generate",
    "start": "221040",
    "end": "226500"
  },
  {
    "text": "image of dogs using stable diffusion and there's gonna be some failure and we're gonna",
    "start": "226500",
    "end": "231840"
  },
  {
    "text": "um debug using new features so the workload or you know glance is that we first just read images very",
    "start": "231840",
    "end": "238260"
  },
  {
    "text": "simple and transfer the images and we're going to run the trainer um four of them on GPU and all these",
    "start": "238260",
    "end": "244500"
  },
  {
    "text": "images are gonna be on fine tuned and we are going to generate of course the model and I'll create images",
    "start": "244500",
    "end": "251720"
  },
  {
    "text": "so here we are going to demonstrate on some of the new features we built a lot of new features so we didn't have enough",
    "start": "253920",
    "end": "259979"
  },
  {
    "text": "time to demonstrate everything so we're going to specifically focus on how to",
    "start": "259979",
    "end": "265020"
  },
  {
    "text": "debug out of memory error using the metrics page and also the memory profiler",
    "start": "265020",
    "end": "270919"
  },
  {
    "text": "okay for the demo we're gonna use the any scale workspace as on environment so any scale workspace is a development",
    "start": "271380",
    "end": "277860"
  },
  {
    "text": "environment that helps you to take the cloud resources and also gives you may",
    "start": "277860",
    "end": "283139"
  },
  {
    "text": "make it easy to run Ray by providing like EFS um not Jupiter notebook and things like",
    "start": "283139",
    "end": "288840"
  },
  {
    "text": "that so this is our script um it's pretty simple we first get the",
    "start": "288840",
    "end": "293940"
  },
  {
    "text": "data it's gonna use rate data under the hood and we're gonna use the torch trainer with four gpus a train function",
    "start": "293940",
    "end": "300660"
  },
  {
    "text": "I'm not gonna dive deep into how how to do fine tuning because that's not the goal of the talk but if you're interested you can go to the example",
    "start": "300660",
    "end": "306960"
  },
  {
    "text": "gallery and find exactly the same code so let's run this workload",
    "start": "306960",
    "end": "314479"
  },
  {
    "text": "okay the workload is running now and we are going to the dashboard so this is the newest dashboard you can find from",
    "start": "316080",
    "end": "322139"
  },
  {
    "text": "Ray 2.7 when you get into the dashboard you can first see the cluster utilization which shows you on the",
    "start": "322139",
    "end": "328320"
  },
  {
    "text": "utilization of all different Hardware resources and a list of jobs that are running in the cluster and you can see",
    "start": "328320",
    "end": "334199"
  },
  {
    "text": "our train.pi it's a fine tuning workload is running and it shows other high level information such as how many nodes are",
    "start": "334199",
    "end": "340620"
  },
  {
    "text": "there in the cluster the output of race status how many demands are there in the cluster and also the list of events on",
    "start": "340620",
    "end": "348000"
  },
  {
    "text": "auto scalar events application events and Etc other noticeable Improvement we made is",
    "start": "348000",
    "end": "353759"
  },
  {
    "text": "we have a tighter integration to raise data API which means all the different resources in Ray like the jobs actors",
    "start": "353759",
    "end": "360360"
  },
  {
    "text": "task um their metadata in the state is accessible from the dashboard and as you can see you can see the entry",
    "start": "360360",
    "end": "366060"
  },
  {
    "text": "point on the job is actually running there are like some tasks running for like one minute and 50 seconds there are",
    "start": "366060",
    "end": "371940"
  },
  {
    "text": "like other features like actions you can actually obtain like a stack Trace flame graph and things like that you can try",
    "start": "371940",
    "end": "377100"
  },
  {
    "text": "later so you can go to more detail by clicking the ID and this shows you the progress",
    "start": "377100",
    "end": "383400"
  },
  {
    "text": "bar which shows all the tasks actors created from desktop you can actually see some of tasks has been failing you",
    "start": "383400",
    "end": "390419"
  },
  {
    "text": "can see on the right side there was red and you can go to the actual task and you can set your error message from the",
    "start": "390419",
    "end": "396780"
  },
  {
    "text": "dashboard as you can see from here it failed by out of memory um and there are a lot of map patches",
    "start": "396780",
    "end": "403380"
  },
  {
    "text": "running concurrently that uses about eight gigabytes of memory so let's further debug let's go to the metrics",
    "start": "403380",
    "end": "408539"
  },
  {
    "text": "page it shows all the pre-built metrics such as like what kind of task is running",
    "start": "408539",
    "end": "414000"
  },
  {
    "text": "um what tasks have been failed by this out of memory error so you can see there's a split single block has been",
    "start": "414000",
    "end": "419460"
  },
  {
    "text": "failing or it shows many other very useful information such as what's the logical",
    "start": "419460",
    "end": "424919"
  },
  {
    "text": "allocation on of the cluster like resource allocation of the cluster how many placement groups are in the cluster",
    "start": "424919",
    "end": "431180"
  },
  {
    "text": "or for the most important feature today is it's showing you the hardware",
    "start": "431180",
    "end": "436860"
  },
  {
    "text": "resource visualization so you can see the memory usage has been very high so in order to further debug",
    "start": "436860",
    "end": "443280"
  },
  {
    "text": "um and sexually go to the graph on the page the previous page was powered by grafana",
    "start": "443280",
    "end": "449280"
  },
  {
    "text": "and you can go to this memory or usage by component and from here you can see there has been a lot of train workers so",
    "start": "449280",
    "end": "456419"
  },
  {
    "text": "we have four train workers using about like four to five gigabyte of memory and there was also map patches using a 60",
    "start": "456419",
    "end": "462419"
  },
  {
    "text": "gigabyte of memory and this is Snapshot so there might be like even more up and down around this time and that's that's why",
    "start": "462419",
    "end": "468840"
  },
  {
    "text": "there was a lot of memory error so to further debug um we're going to first debug why train",
    "start": "468840",
    "end": "475199"
  },
  {
    "text": "worker uses a lot of memory so in order to do that let's go to the script and we're gonna use this open source tool",
    "start": "475199",
    "end": "481680"
  },
  {
    "text": "called memory so memory is the um open source tool developed by Bloomberg and it's very good they generate track all",
    "start": "481680",
    "end": "489900"
  },
  {
    "text": "the python allocation and then gives you a flame graph or other sort of reports so you can just simply call this tracker",
    "start": "489900",
    "end": "495660"
  },
  {
    "text": "enter in the beginning of the train function and we're going to store the result into the log folder which means",
    "start": "495660",
    "end": "501599"
  },
  {
    "text": "that since we provide the log viewer you're able to access all these profile for filing file inside of our dashboard",
    "start": "501599",
    "end": "508860"
  },
  {
    "text": "so you can see workload is running again and let's go to the log page um",
    "start": "508860",
    "end": "514140"
  },
  {
    "text": "the typical work uh workflow here is you can download all these profiling files to your local machine and just run the",
    "start": "514140",
    "end": "520440"
  },
  {
    "text": "memory command there's a command called memory flame graph but since we're already in workspace you you can just",
    "start": "520440",
    "end": "526140"
  },
  {
    "text": "run directly in the terminal so I'm just going to run memory flame graph and the path to the memory profiling file",
    "start": "526140",
    "end": "534720"
  },
  {
    "text": "and this gives you um the older allocation information here and as you can see most of memory usage",
    "start": "534720",
    "end": "541260"
  },
  {
    "text": "has been just used by load models so you just loaded models and they had to use some RAM and that's the um basically",
    "start": "541260",
    "end": "548220"
  },
  {
    "text": "primary usage and you can you can see there was no memory leak or something inefficiency or something you can",
    "start": "548220",
    "end": "553980"
  },
  {
    "text": "optimize from um from train workers so here as a",
    "start": "553980",
    "end": "559200"
  },
  {
    "text": "solution what we can do alternatively is since we know map patches has been using a lot of memory we can simply um",
    "start": "559200",
    "end": "565320"
  },
  {
    "text": "you know schedule less of them at the same time by reducing the parallelism or another Simple Solutions you can use",
    "start": "565320",
    "end": "571800"
  },
  {
    "text": "like same number of gpus but a motion that has more RAM here okay so let's recap so we demonstrated",
    "start": "571800",
    "end": "578220"
  },
  {
    "text": "some of the new dashboard Improvement we made after Ray 2.0 for the last one year",
    "start": "578220",
    "end": "583560"
  },
  {
    "text": "and also we demonstrated how to use some of new features to debug on some simple out of memory errors",
    "start": "583560",
    "end": "590100"
  },
  {
    "text": "okay so from here Ellen is going to talk about the race verb observability",
    "start": "590100",
    "end": "596060"
  },
  {
    "text": "thank you sang so my my section will be talking about some of the observability we built for our AI libraries in this",
    "start": "596519",
    "end": "604560"
  },
  {
    "text": "case I'm focusing on reserve so first just to give you some context of what is Ray serve so serve is a AI library for a",
    "start": "604560",
    "end": "612000"
  },
  {
    "text": "building model serving apis it's built on top of Ray so that means",
    "start": "612000",
    "end": "617160"
  },
  {
    "text": "they can easily scale to many many machines and because it also supports race",
    "start": "617160",
    "end": "622620"
  },
  {
    "text": "Primitives it's uh like tasks and actors it's super simple to create complex or",
    "start": "622620",
    "end": "627720"
  },
  {
    "text": "flexible architectures like many model serving or model composition",
    "start": "627720",
    "end": "633180"
  },
  {
    "text": "so um I'm also going to introduce two main components about racer just to give you more context of the demo the first",
    "start": "633180",
    "end": "640320"
  },
  {
    "text": "is a serve deployment this is more or less a python class that you can Define that will implement the behavior you",
    "start": "640320",
    "end": "647100"
  },
  {
    "text": "want for handling a request the second concept is a replica this is",
    "start": "647100",
    "end": "652980"
  },
  {
    "text": "an instantiation of your deployment it's a unit of scaling for Reserve to create copies of your deployment and it can add",
    "start": "652980",
    "end": "660420"
  },
  {
    "text": "and remove copies to handle an increase or decrease of traffic",
    "start": "660420",
    "end": "665640"
  },
  {
    "text": "so why is observability important for race serve well the first thing I mentioned was that this application is",
    "start": "665640",
    "end": "672899"
  },
  {
    "text": "serving apis to your users or to other services so it's very important that your application is running at all times",
    "start": "672899",
    "end": "679260"
  },
  {
    "text": "and to do that you really need observability into understanding the health of your application",
    "start": "679260",
    "end": "686240"
  },
  {
    "text": "the second thing I mentioned was like services are often or Services often run for long periods of time so having time",
    "start": "686519",
    "end": "692760"
  },
  {
    "text": "series data and metrics can help you understand historical Trends and can also help you diagnose passive failures",
    "start": "692760",
    "end": "700880"
  },
  {
    "text": "um and I mentioned that server enables complex architectures such as multimodal",
    "start": "702120",
    "end": "707640"
  },
  {
    "text": "serving so Azure application gets more complex having specialized tools can be very useful in understanding how your",
    "start": "707640",
    "end": "714600"
  },
  {
    "text": "application works which will help make the development and the debugging process just much more smooth",
    "start": "714600",
    "end": "722579"
  },
  {
    "text": "so to demo some of some of these observability features we will be downloading a large language model",
    "start": "722579",
    "end": "727740"
  },
  {
    "text": "serving application on top of Reserve we'll be using an existing code base which is rate llm this is previously",
    "start": "727740",
    "end": "735060"
  },
  {
    "text": "known as Avery but this is an open source project that we've created to make it super simple to deploy open",
    "start": "735060",
    "end": "741600"
  },
  {
    "text": "source large language models it's built on top of Ray serve so it has a bunch of the features I mentioned in",
    "start": "741600",
    "end": "747839"
  },
  {
    "text": "the past like multi-model serving and auto scaling and it comes out of the box with support",
    "start": "747839",
    "end": "753779"
  },
  {
    "text": "for all the most popular open source LMS like llama 2 and falcon",
    "start": "753779",
    "end": "759360"
  },
  {
    "text": "so over the course of the demo we'll be doing an overview of some of the server observability features we'll debug a GPU",
    "start": "759360",
    "end": "766860"
  },
  {
    "text": "resource availability issue we'll observe some Auto scaling and finally",
    "start": "766860",
    "end": "771959"
  },
  {
    "text": "we'll look at metrics and custom metrics",
    "start": "771959",
    "end": "776240"
  },
  {
    "text": "so here I have any scale workspace with my Ray LM code checked out",
    "start": "779700",
    "end": "785880"
  },
  {
    "text": "um the first thing I'm going to do is is run my serve application do server run and give it to serve yaml so this served",
    "start": "785880",
    "end": "793079"
  },
  {
    "text": "yamo is a configuration file that just tells serve the shape of application I",
    "start": "793079",
    "end": "798480"
  },
  {
    "text": "want to run in this case I have three deployments I have the router deployment and I have one deployment each for the",
    "start": "798480",
    "end": "805260"
  },
  {
    "text": "large language models I'm trying to serve llama27b llama 213b",
    "start": "805260",
    "end": "810300"
  },
  {
    "text": "and where the ray LM code base comes in is to serve these large language models",
    "start": "810300",
    "end": "815639"
  },
  {
    "text": "I just have to provide it these model yamls you can see I linked to this",
    "start": "815639",
    "end": "821120"
  },
  {
    "text": "llama27b model right here and with just that one line I have the open source model serving",
    "start": "821120",
    "end": "828300"
  },
  {
    "text": "so we just quickly look at what this yaml file is it just has some information that helps sort of",
    "start": "828300",
    "end": "833459"
  },
  {
    "text": "understand how to run this model things like where to download the model weights from how to scale how much request a",
    "start": "833459",
    "end": "839820"
  },
  {
    "text": "model can handle What GPU to use just various configurations like that",
    "start": "839820",
    "end": "846180"
  },
  {
    "text": "so I've been running the application so let's see how how it's doing so I opened",
    "start": "846180",
    "end": "851399"
  },
  {
    "text": "array dashboard and go to serve page so this is one of the new pages we built for the ray dashboard targeted towards",
    "start": "851399",
    "end": "857279"
  },
  {
    "text": "Ray serve this page is designed to At a Glance give you an understanding of the",
    "start": "857279",
    "end": "862560"
  },
  {
    "text": "health of your serve application so at the top you see a high level summary of a couple of the sort of components we",
    "start": "862560",
    "end": "869100"
  },
  {
    "text": "have a couple of system components like the controller and the HTTP proxy but there's also some information about your",
    "start": "869100",
    "end": "874320"
  },
  {
    "text": "own application um and here we can see that two of our",
    "start": "874320",
    "end": "879720"
  },
  {
    "text": "deployments are still deploying so as we scroll down we see that there are three deployments this is what we've",
    "start": "879720",
    "end": "885779"
  },
  {
    "text": "configured from our our yaml file and we can see that the llama27b and the Llama",
    "start": "885779",
    "end": "892680"
  },
  {
    "text": "213p it's taking a while to to to to to to deploy about a minute and a half so let's try to figure out what's wrong",
    "start": "892680",
    "end": "900360"
  },
  {
    "text": "um so I'm going to go to the overview page which will let me use the events",
    "start": "900360",
    "end": "906540"
  },
  {
    "text": "component so this component is used by Ray to give information to users that",
    "start": "906540",
    "end": "912600"
  },
  {
    "text": "might be important so in this case the auto scalar component is giving us information that is trying to launch a",
    "start": "912600",
    "end": "917779"
  },
  {
    "text": "p4d24x large instance if we scroll a little bit further down we can see that this isn't the first attempt and the",
    "start": "917779",
    "end": "924720"
  },
  {
    "text": "previous attempt it actually errored and you can see there's this event right here that there's a insufficient",
    "start": "924720",
    "end": "931560"
  },
  {
    "text": "instance capacity error we do not have a sufficient p4d24x large capacity",
    "start": "931560",
    "end": "938600"
  },
  {
    "text": "so we'll scroll up and see the resource status to kind of understand why we're",
    "start": "938600",
    "end": "943800"
  },
  {
    "text": "trying to launch that instance we can see the status here is that we're looking for a100 GPU so this is actually",
    "start": "943800",
    "end": "949860"
  },
  {
    "text": "a super common problem with deploying large language models is it's very hard to get a100 gpus and",
    "start": "949860",
    "end": "956579"
  },
  {
    "text": "um and the way we're going to fix this is we're just going to actually change this to use A10 GPU instead we do know",
    "start": "956579",
    "end": "963300"
  },
  {
    "text": "that llama27b can run just fine with the A10 GPU",
    "start": "963300",
    "end": "969199"
  },
  {
    "text": "so we'll restart the Serv application and give it a few moments",
    "start": "970380",
    "end": "977600"
  },
  {
    "text": "okay so let's see how it's doing we'll go back to the ray dashboard and we'll go to the serve page",
    "start": "982380",
    "end": "987480"
  },
  {
    "text": "we can see right here that the deployments have indeed restarted the duration has reset to zero and in just a",
    "start": "987480",
    "end": "994320"
  },
  {
    "text": "few moments we should see the llama27b model start running",
    "start": "994320",
    "end": "999720"
  },
  {
    "text": "Okay cool so it's working and we'll go ahead and send some test traffic just to",
    "start": "999720",
    "end": "1006380"
  },
  {
    "text": "verify it works so I'm just going to run a simple script I have here it's just querying my local service and asking",
    "start": "1006380",
    "end": "1013399"
  },
  {
    "text": "llama2 a question what are the top tourist attractions in San Francisco",
    "start": "1013399",
    "end": "1019420"
  },
  {
    "text": "and here we can see that Ray LM does support streaming so it's streaming back some response from llama27b and it's",
    "start": "1024199",
    "end": "1031160"
  },
  {
    "text": "giving us quite reasonable answers to this question so things are looking good so we're going to start sending more",
    "start": "1031160",
    "end": "1038000"
  },
  {
    "text": "traffic to this service so we can see some Auto scaling so back at the rate dashboard I click",
    "start": "1038000",
    "end": "1044120"
  },
  {
    "text": "into the the detail page and we can see more information about this serve deployment",
    "start": "1044120",
    "end": "1049700"
  },
  {
    "text": "here we can see a list of the deployments and number of replicas we have one replica right now but pretty soon we should see a couple more get",
    "start": "1049700",
    "end": "1057200"
  },
  {
    "text": "added and yeah so now we're up to three so the way this works is uh as these replicas receive traffic they may start",
    "start": "1057200",
    "end": "1063440"
  },
  {
    "text": "queuing if the traffic can't get handled and if race serve sees enough requests being queued they can decide to add more",
    "start": "1063440",
    "end": "1069980"
  },
  {
    "text": "replicas to handle the traffic so it's added two more replicas and they're now",
    "start": "1069980",
    "end": "1075440"
  },
  {
    "text": "running so things are looking good so we've actually we'll jump forward a",
    "start": "1075440",
    "end": "1080900"
  },
  {
    "text": "couple of hours so this is three hours later we've been sending traffic and if we scroll down here we can see these",
    "start": "1080900",
    "end": "1086360"
  },
  {
    "text": "metrics so important part of the health of your application is not just whether your application is running but if it's",
    "start": "1086360",
    "end": "1092780"
  },
  {
    "text": "actually handling the traffic correctly so we have metrics right here to give you some good information of uh how it's",
    "start": "1092780",
    "end": "1099679"
  },
  {
    "text": "How how it's doing so here these are all a bunch of metrics",
    "start": "1099679",
    "end": "1105020"
  },
  {
    "text": "that come out of the box supported for every race serve application these are General metrics that we think are useful",
    "start": "1105020",
    "end": "1110240"
  },
  {
    "text": "to understand the health of your application things like QPS error QPS latencies even some Hardware level",
    "start": "1110240",
    "end": "1117020"
  },
  {
    "text": "metrics like CPU usage and node network but as an application developer it's",
    "start": "1117020",
    "end": "1122360"
  },
  {
    "text": "actually pretty important that you want to look at business logic metrics as well so we do have a support with your",
    "start": "1122360",
    "end": "1129320"
  },
  {
    "text": "custom metrics where you can create custom metrics using raycourse metrics library and with grafana you can easily",
    "start": "1129320",
    "end": "1136220"
  },
  {
    "text": "visualize those metrics so because we are using a pre-existing code base the ray LM project there is some custom",
    "start": "1136220",
    "end": "1142940"
  },
  {
    "text": "metrics that come out of the box and we'll just visualize those metrics super quickly here",
    "start": "1142940",
    "end": "1147980"
  },
  {
    "text": "so what I'm going to do is do a graph for the amount of tokens per second",
    "start": "1147980",
    "end": "1154039"
  },
  {
    "text": "being generated per model and if I just add the query here and set up the",
    "start": "1154039",
    "end": "1159919"
  },
  {
    "text": "dashboard we can see quite easily we've added a custom chart right here that can",
    "start": "1159919",
    "end": "1165020"
  },
  {
    "text": "give us understanding of whether our application is running correctly or not",
    "start": "1165020",
    "end": "1170419"
  },
  {
    "text": "cool so um to give a quick recap",
    "start": "1170419",
    "end": "1176500"
  },
  {
    "text": "so some things I showed you I talked about race serve it's an AI model serving",
    "start": "1177020",
    "end": "1182840"
  },
  {
    "text": "library and it scales easily with increase the traffic",
    "start": "1182840",
    "end": "1188000"
  },
  {
    "text": "I also showed you Ray LOM which is an open source project that makes it super simple to deploy open source llms",
    "start": "1188000",
    "end": "1196280"
  },
  {
    "text": "I've showed you some built-in observability features so you can easily understand the health of your service",
    "start": "1196280",
    "end": "1201740"
  },
  {
    "text": "and you can also use it for debugging application or infra level issues",
    "start": "1201740",
    "end": "1207500"
  },
  {
    "text": "and finally with custom metrics you can easily Define what healthy means for your own application",
    "start": "1207500",
    "end": "1213260"
  },
  {
    "text": "uh thank yous our transition to child who's going to talk more about what it takes to serve in production",
    "start": "1213260",
    "end": "1220780"
  },
  {
    "text": "okay thanks Ellen okay so now we have already talked about how you will use three dashboard to",
    "start": "1222140",
    "end": "1230000"
  },
  {
    "text": "develop understand and debug your machine learning application so in this",
    "start": "1230000",
    "end": "1235220"
  },
  {
    "text": "section we're going to talk about what's missing for observability in production use case",
    "start": "1235220",
    "end": "1242080"
  },
  {
    "text": "if you introduce three challenges here in your production use case and for each of them we will talk about the",
    "start": "1242720",
    "end": "1249380"
  },
  {
    "text": "plugability that we offers to help you tackle those challenge and we will use the example of how any",
    "start": "1249380",
    "end": "1256460"
  },
  {
    "text": "SQL platform address those problems so when you want to shift your workloads",
    "start": "1256460",
    "end": "1263480"
  },
  {
    "text": "to production use case there are at least three challenges you need to care about persistency locks and Matrix",
    "start": "1263480",
    "end": "1270799"
  },
  {
    "text": "discoverability and cluster management we will talk about them one by one",
    "start": "1270799",
    "end": "1277039"
  },
  {
    "text": "the first one is persistency so let's imagine a scenario here let's",
    "start": "1277039",
    "end": "1282440"
  },
  {
    "text": "say at the end of the day you'll submit a ray train job to train your test generation model you know it will take",
    "start": "1282440",
    "end": "1289640"
  },
  {
    "text": "several hours to finish so you want to check the status next morning however in the middle of it your hand",
    "start": "1289640",
    "end": "1297440"
  },
  {
    "text": "knows cute so the next morning you found that your logs and metrics are lost so read",
    "start": "1297440",
    "end": "1303799"
  },
  {
    "text": "dashboard don't have the persistency for the logs and matrix by default when your head node is terminated",
    "start": "1303799",
    "end": "1311679"
  },
  {
    "text": "so that's why logs and metrics should be accessible after your handles is",
    "start": "1312320",
    "end": "1317539"
  },
  {
    "text": "terminated and you can discover from this diagram react for three different data logs",
    "start": "1317539",
    "end": "1324860"
  },
  {
    "text": "metrics events so you can easily set in some config and store in your own",
    "start": "1324860",
    "end": "1330620"
  },
  {
    "text": "persistent storage like you can use Amazon RDS or S3",
    "start": "1330620",
    "end": "1337159"
  },
  {
    "text": "and let's jump into detail of how any field platform achieve this so in the",
    "start": "1337159",
    "end": "1342919"
  },
  {
    "text": "left side you can see the blue box represent the rig cluster the green box in the right represents the any scale",
    "start": "1342919",
    "end": "1349520"
  },
  {
    "text": "platform and the red box in the bottom represents our external users so the",
    "start": "1349520",
    "end": "1355100"
  },
  {
    "text": "re-cluster will write the logs into the file system any skill demon process",
    "start": "1355100",
    "end": "1360140"
  },
  {
    "text": "belong to the nfql platform will read the logs and push them to S3",
    "start": "1360140",
    "end": "1365720"
  },
  {
    "text": "so for the external users they can use SDK or API to query and download the",
    "start": "1365720",
    "end": "1372140"
  },
  {
    "text": "logs by directly reading from i3 okay so Second Challenge is logs and",
    "start": "1372140",
    "end": "1379940"
  },
  {
    "text": "Metric discoverability so you should allow some shoes from third-party providers to support querying and",
    "start": "1379940",
    "end": "1387559"
  },
  {
    "text": "alerting to have a better understanding of your persistent logs and metrics so",
    "start": "1387559",
    "end": "1393460"
  },
  {
    "text": "re-exports logs and metrics and you can easily read them into the third party",
    "start": "1393460",
    "end": "1398539"
  },
  {
    "text": "tools like grafana datadog and Promises to achieve the functionality like",
    "start": "1398539",
    "end": "1404780"
  },
  {
    "text": "querying and alerting",
    "start": "1404780",
    "end": "1408700"
  },
  {
    "text": "and let's have a look of how any skill platform achieved this so you can see from this big box here",
    "start": "1409820",
    "end": "1416360"
  },
  {
    "text": "the recluster will serve the API to collect and generate the metrics",
    "start": "1416360",
    "end": "1422360"
  },
  {
    "text": "they also produce the grafana dashboard configuration and any skill demand",
    "start": "1422360",
    "end": "1427400"
  },
  {
    "text": "process will reach those data and push them into our any scale system",
    "start": "1427400",
    "end": "1432440"
  },
  {
    "text": "the premises will collect the data and transform into time series data",
    "start": "1432440",
    "end": "1437900"
  },
  {
    "text": "and the storage is backed by disk the grafana will fetch those data and",
    "start": "1437900",
    "end": "1443539"
  },
  {
    "text": "realize them available in the UI for the user they also set the alert strategy by",
    "start": "1443539",
    "end": "1449900"
  },
  {
    "text": "pager Duty and lastly let's talk about the cluster",
    "start": "1449900",
    "end": "1455240"
  },
  {
    "text": "management why so important so when you want to run your workload on",
    "start": "1455240",
    "end": "1460820"
  },
  {
    "text": "your cluster there are several things you need to care about first is instance management you need to",
    "start": "1460820",
    "end": "1468080"
  },
  {
    "text": "grab the cheapest resources in the red availability Zone regions from the red",
    "start": "1468080",
    "end": "1474080"
  },
  {
    "text": "cloud provider you may also need to deal with the spot instance to save money",
    "start": "1474080",
    "end": "1481100"
  },
  {
    "text": "and second is dependency and deployment so you need the environment's",
    "start": "1481100",
    "end": "1487940"
  },
  {
    "text": "preparation on each instance to installing the dependencies and your code",
    "start": "1487940",
    "end": "1495039"
  },
  {
    "text": "and as you can see those events or say information May correlated with each",
    "start": "1495380",
    "end": "1500900"
  },
  {
    "text": "other for example if you are using a spot instance and at some points this",
    "start": "1500900",
    "end": "1506419"
  },
  {
    "text": "instance is unavailable will leading to some application failures of your rate",
    "start": "1506419",
    "end": "1511520"
  },
  {
    "text": "job like the task failure so that's why we need a all-in-one wheel that",
    "start": "1511520",
    "end": "1518080"
  },
  {
    "text": "consolidates all the cluster related events",
    "start": "1518080",
    "end": "1523059"
  },
  {
    "text": "and you can see the re-export logs and events and you can easily to integrate",
    "start": "1523159",
    "end": "1528320"
  },
  {
    "text": "into your own system you may also have some customized events like related to your own business logic",
    "start": "1528320",
    "end": "1534679"
  },
  {
    "text": "and you need a all-in-one wheel to understand them and debug your application",
    "start": "1534679",
    "end": "1541778"
  },
  {
    "text": "and now let's have a recap of our presentation today so first we'll show the improvements in",
    "start": "1542480",
    "end": "1548000"
  },
  {
    "text": "read dashboard and end-to-end user experience in debugging a job and",
    "start": "1548000",
    "end": "1553340"
  },
  {
    "text": "Reserve applications second we showed how production grid",
    "start": "1553340",
    "end": "1558799"
  },
  {
    "text": "availability could be built by consuming the data from Ray",
    "start": "1558799",
    "end": "1564880"
  },
  {
    "text": "and one more thing here so we are excited to announce that we're",
    "start": "1564980",
    "end": "1570440"
  },
  {
    "text": "going to hold a tech office hour to answer the question from you related to",
    "start": "1570440",
    "end": "1575900"
  },
  {
    "text": "the obserability so the time is on Tuesday 2 30 PM by weekly you can feel",
    "start": "1575900",
    "end": "1581720"
  },
  {
    "text": "free to scan the QR code here to join the slack Channel and that's all of our topic today and",
    "start": "1581720",
    "end": "1589340"
  },
  {
    "text": "thank everyone for coming we do have several minutes for the QA here I will hand over back to Sam",
    "start": "1589340",
    "end": "1596299"
  },
  {
    "text": "okay [Applause]",
    "start": "1596299",
    "end": "1605549"
  },
  {
    "text": "thank you my question would be art is uh integrated",
    "start": "1611120",
    "end": "1617080"
  },
  {
    "text": "points pluggable so if I have my own Enterprise way to dealing with logs",
    "start": "1617080",
    "end": "1624559"
  },
  {
    "text": "events and so on can I write my own custom plugin to send the events to Kafka or send logs to",
    "start": "1624559",
    "end": "1633020"
  },
  {
    "text": "Splunk how does it work sure yeah okay so yes these integration",
    "start": "1633020",
    "end": "1640940"
  },
  {
    "text": "points are pluggable uh they uh I think they all have schemas at this point or",
    "start": "1640940",
    "end": "1647860"
  },
  {
    "text": "the API will either be for logs for example be a file based API so",
    "start": "1647860",
    "end": "1653059"
  },
  {
    "text": "um knowing the right directories to sort of save those files you can push those those logs into whatever third-party",
    "start": "1653059",
    "end": "1659419"
  },
  {
    "text": "integration you want and for things like events we do have like a Proto buff that defines the structure of those events",
    "start": "1659419",
    "end": "1664760"
  },
  {
    "text": "and you can import those and do whatever you want",
    "start": "1664760",
    "end": "1669580"
  },
  {
    "text": "hi I would like to know do these tools also work with on-prem machines and the",
    "start": "1672320",
    "end": "1678799"
  },
  {
    "text": "instances managed and self-hosted or",
    "start": "1678799",
    "end": "1684260"
  },
  {
    "text": "it's one or the other or managed Services can also watch your on-prem",
    "start": "1684260",
    "end": "1689720"
  },
  {
    "text": "services okay yeah sure so the um so the raid",
    "start": "1689720",
    "end": "1695960"
  },
  {
    "text": "dashboard works on all clusters if you run it locally on your laptop or you run it on an on-prem machine or use a cloud",
    "start": "1695960",
    "end": "1702320"
  },
  {
    "text": "provider it should work for some of the Integrations like Prometheus or grafana for example it will absolutely work but",
    "start": "1702320",
    "end": "1709940"
  },
  {
    "text": "you kind of have to set that up yourself so if you have it in the cloud you need to run those services",
    "start": "1709940",
    "end": "1716679"
  },
  {
    "text": "any other questions we can take one more",
    "start": "1720440",
    "end": "1724480"
  },
  {
    "text": "thanks I've had a great talk my question is does this support automatically deleting",
    "start": "1729679",
    "end": "1735980"
  },
  {
    "text": "all logs so can you repeat yeah the old I'll say if I've been running it for a long time",
    "start": "1735980",
    "end": "1742820"
  },
  {
    "text": "I have been collecting a lot of whole logs does it have support to automatically",
    "start": "1742820",
    "end": "1748159"
  },
  {
    "text": "deleting them um that's deleting like there's some log",
    "start": "1748159",
    "end": "1754220"
  },
  {
    "text": "rotation support right oh so so Ray has some like built-in logo rotation for some of the components but",
    "start": "1754220",
    "end": "1761000"
  },
  {
    "text": "not everything is automatically deleted so while we were doing at any scale is we deployed a log rotate like a Linux",
    "start": "1761000",
    "end": "1768020"
  },
  {
    "text": "command and then we are um there's an option called like copy trinket which is basically",
    "start": "1768020",
    "end": "1773299"
  },
  {
    "text": "um rewrite like a create a new file and um start writing the new log into the",
    "start": "1773299",
    "end": "1778880"
  },
  {
    "text": "new file so we haven't written that to our official documentation but we plan to write it very soon so you're probably",
    "start": "1778880",
    "end": "1784880"
  },
  {
    "text": "gonna be able to use the same config and delay all the files so you can avoid disk",
    "start": "1784880",
    "end": "1790700"
  },
  {
    "text": "um yeah thanks yeah",
    "start": "1790700",
    "end": "1795100"
  },
  {
    "text": "all right that's the talk thank you everyone another hand for uh our fine presenters",
    "start": "1795980",
    "end": "1801140"
  },
  {
    "text": "[Applause]",
    "start": "1801140",
    "end": "1805899"
  }
]