[
  {
    "text": "all right good afternoon and welcome everyone my name is Abdullah and today",
    "start": "2960",
    "end": "8440"
  },
  {
    "text": "we have a talk on with how Spotify users Ray on Google kubernetes engine to run distributed llm training I will be",
    "start": "8440",
    "end": "16000"
  },
  {
    "text": "presenting with uh Brandon from Google cloud and we'll be talking about gke and",
    "start": "16000",
    "end": "21640"
  },
  {
    "text": "how Spotify leverages it for distributed training so over to you Brandon excellent thank you Abdullah and",
    "start": "21640",
    "end": "28480"
  },
  {
    "text": "thank you everyone for being here uh so as ad mentioned my name is Brandon Royal uh I am a product manager on the gke",
    "start": "28480",
    "end": "35200"
  },
  {
    "text": "team focused on AIML workloads so uh really pleased to be uh here with the Spotify team um sharing some insights",
    "start": "35200",
    "end": "42039"
  },
  {
    "text": "around you know building large distributed platforms using Ray and Google K and's engine together um so we",
    "start": "42039",
    "end": "48039"
  },
  {
    "text": "will um sort of focus on two areas so we'll talk a little bit sort of in general terms first um addressing some",
    "start": "48039",
    "end": "53719"
  },
  {
    "text": "of the core challenges in distributed AI infrastructure um and what the Google kubernetes engine team is specifically",
    "start": "53719",
    "end": "59559"
  },
  {
    "text": "doing uh Investments that we're making in order to solve some of those challenges and then I'm going to pass it back over to Abdullah here shortly um",
    "start": "59559",
    "end": "66119"
  },
  {
    "text": "he's going to show the really exciting stuff which is um how real customers like Spotify are implementing a lot of",
    "start": "66119",
    "end": "71560"
  },
  {
    "text": "these features and capabilities in order to serve the needs of their ml users internally um so this is probably",
    "start": "71560",
    "end": "79439"
  },
  {
    "text": "familiar to a lot of you folks um has anyone has anyone ever run out of a",
    "start": "79439",
    "end": "84799"
  },
  {
    "text": "GPU yes okay um has anyone had an out of memory issue with a GP GPU yes right so",
    "start": "84799",
    "end": "91759"
  },
  {
    "text": "a lot a lot of like consistent challenges right and and Abdullah will talk about some of those challenges that they've seen on the Spotify side um but",
    "start": "91759",
    "end": "98320"
  },
  {
    "text": "you know we know GPU scarcity is a a Perpetual unfortunately a bit of an evergreen problem uh within our customer",
    "start": "98320",
    "end": "105600"
  },
  {
    "text": "base um and we know that a lot of these large scale distributed um GP or excuse",
    "start": "105600",
    "end": "110719"
  },
  {
    "text": "me distributed models um they require hundreds thousands tens of thousands of",
    "start": "110719",
    "end": "115799"
  },
  {
    "text": "gpus in some very large cases uh and and um you know ultimately we need to efficiently orchestrate workloads across",
    "start": "115799",
    "end": "122399"
  },
  {
    "text": "all of those uh and then we also recognize that unfortunately or fortunately depending on which side",
    "start": "122399",
    "end": "128119"
  },
  {
    "text": "you're on uh Nvidia gpus and accelerators are quite expensive so we",
    "start": "128119",
    "end": "133440"
  },
  {
    "text": "need to make sure that we are U getting every penny out of those accelerators uh to deliver value ultimately to you so um",
    "start": "133440",
    "end": "140840"
  },
  {
    "text": "these are sort of challenges that we've seen across the board there are many others um and we really believe that you",
    "start": "140840",
    "end": "146400"
  },
  {
    "text": "know kubernetes and Ray it's a sort of a match made in heaven it's really the perfect relationship uh to help address",
    "start": "146400",
    "end": "152760"
  },
  {
    "text": "some of those challenges um both in terms of all the things you've heard through the course of the last couple of days around what Rey is doing U but also",
    "start": "152760",
    "end": "160040"
  },
  {
    "text": "because well kubernetes is about you know 10 years old a little more than 10 years old um and is quite mature when it",
    "start": "160040",
    "end": "165480"
  },
  {
    "text": "comes to large scale orchestration of complex and stateful workloads um so it really is the perfect uh perfect fit so",
    "start": "165480",
    "end": "172720"
  },
  {
    "text": "as the kubernetes team our job is to really sort of make that relationship work uh and make that relationship um",
    "start": "172720",
    "end": "179159"
  },
  {
    "text": "sort of last and and serve the needs of our customers um and so U one of the ways in",
    "start": "179159",
    "end": "184319"
  },
  {
    "text": "which we're primarily doing that is to uh integrate features of kubra directly into Google KUB engine and so these are",
    "start": "184319",
    "end": "191720"
  },
  {
    "text": "optimizations both in sort of the Upstream kubra project and I'm joined by some of my Engineers uh my colleagues",
    "start": "191720",
    "end": "197560"
  },
  {
    "text": "that are doing a lot of Upstream work directly in kubra um but also sort of bringing the kubra operational",
    "start": "197560",
    "end": "203040"
  },
  {
    "text": "experience to the kubernetes U cluster and controlled plane um so with uh Ray",
    "start": "203040",
    "end": "209239"
  },
  {
    "text": "on Google in his engine uh we introduced a new add-on a couple of months ago and essentially with one configuration you",
    "start": "209239",
    "end": "215720"
  },
  {
    "text": "can get the kubra operator deployed directly into your cluster um and that provides some sort of nice low hanging",
    "start": "215720",
    "end": "221640"
  },
  {
    "text": "fruit benefits things like you know monitoring and logging and observability additional scalability additional",
    "start": "221640",
    "end": "226680"
  },
  {
    "text": "security um so so this is just sort of easy nice to have things that we're adding on top of the great work that's",
    "start": "226680",
    "end": "232599"
  },
  {
    "text": "already happening within the open source Community um but sort of in broader terms right that's sort of the the easy",
    "start": "232599",
    "end": "239120"
  },
  {
    "text": "stuff right that's the low hanging fruit just to make Ray easier to use within kubernetes the challenges that we're",
    "start": "239120",
    "end": "244879"
  },
  {
    "text": "trying to address are sort of bigger challenges especially when you think about large scale ml distributed compute",
    "start": "244879",
    "end": "251000"
  },
  {
    "text": "um you know we care a lot about helping customers to solve scale and performance",
    "start": "251000",
    "end": "256199"
  },
  {
    "text": "and that means um you know improving cluster scaling in terms of horizontal workload scaling and node scaling uh but",
    "start": "256199",
    "end": "262680"
  },
  {
    "text": "also um you know helping with price performance uh for both distributed training as well as for inference um on",
    "start": "262680",
    "end": "268919"
  },
  {
    "text": "the obtainability side we we recognize this is a big challenge right uh and so this is an area that we're investing",
    "start": "268919",
    "end": "274199"
  },
  {
    "text": "quite heavily across Google Cloud to help with this obtainability challenge we recognize that not every organization",
    "start": "274199",
    "end": "280160"
  },
  {
    "text": "can go and get a large reservation sort of pay for all of their gpus in advance for a full year they need some level of",
    "start": "280160",
    "end": "287360"
  },
  {
    "text": "dynamic allocation of those GPU resources um and so it's our job on the",
    "start": "287360",
    "end": "292400"
  },
  {
    "text": "kubernetes side and the kubernetes engine team to make that interface work so as you're building your ray cluster",
    "start": "292400",
    "end": "299240"
  },
  {
    "text": "or your expanding your ray cluster how do you ensure that those Dynamic resources that are allocated via gcp um",
    "start": "299240",
    "end": "305199"
  },
  {
    "text": "are made available to those those jobs so we want to make it easy for job execution we want to make it easy for",
    "start": "305199",
    "end": "310759"
  },
  {
    "text": "those Ray apis uh or excuse me kubernetes apis to express the needs of those um those resources uh and then the",
    "start": "310759",
    "end": "317080"
  },
  {
    "text": "last is densification so in a lot of cases some some cases you might have sort of dynamic allocation of resources",
    "start": "317080",
    "end": "323800"
  },
  {
    "text": "where you want gpus when you need them and when you don't need them anymore you want to spend them down but in other cases you want to densify your workloads",
    "start": "323800",
    "end": "331039"
  },
  {
    "text": "in such a way that you are maximizing the the utilization of your GPU resources across the board and we have",
    "start": "331039",
    "end": "336840"
  },
  {
    "text": "some methodologies that we use internally we'll talk a little bit about uh but in general we're talking about sort of Bin packing for Ray jobs and Ray",
    "start": "336840",
    "end": "343840"
  },
  {
    "text": "clusters within your your cluster and making that as easy as possible and so one of the patterns that you'll hear",
    "start": "343840",
    "end": "349039"
  },
  {
    "text": "from Abdu is this sort of idea of ephemeral clusters and ephemeral array jobs and that's one of the ways in which",
    "start": "349039",
    "end": "354319"
  },
  {
    "text": "we can start to do that combined with queuing and prioritization so um as a distributed",
    "start": "354319",
    "end": "361160"
  },
  {
    "text": "platform of course like our job on the kubernetes side is to make the infrastructure easy and to make it highly scalable highly performant and so",
    "start": "361160",
    "end": "368280"
  },
  {
    "text": "there's a lot of things that we're doing in general to sort of make that um ml experience great for our customers um",
    "start": "368280",
    "end": "374160"
  },
  {
    "text": "and so you may be familiar obviously you're familiar with gpus uh but we're doing a whole lot of work of course with",
    "start": "374160",
    "end": "379199"
  },
  {
    "text": "tpus as well and so Ray is natively integrated into tpus it's been supported",
    "start": "379199",
    "end": "384759"
  },
  {
    "text": "for some time and so that gives you the flexibility you want to use gpus you can also use now tpus um especially if you",
    "start": "384759",
    "end": "391360"
  },
  {
    "text": "use Frameworks like Jacks um on the networking side we recognize we heard a lot about sort of nickel uh last week or",
    "start": "391360",
    "end": "398680"
  },
  {
    "text": "saw last week yesterday it's been a long week um and and so we want to make that",
    "start": "398680",
    "end": "403720"
  },
  {
    "text": "GPU to GPU high bandwidth low latency per uh performance um as as good as",
    "start": "403720",
    "end": "409039"
  },
  {
    "text": "possible right and so we're doing some specific or we made some specific investments in the the latest generation",
    "start": "409039",
    "end": "414599"
  },
  {
    "text": "of h100s to significantly improve that latency and performance and then of course we're doing things on theore side",
    "start": "414599",
    "end": "419960"
  },
  {
    "text": "as well and so we won't talk a lot about that stuff but I'll just kind of give you a high Lev view um these are things that we care deeply about um really it's",
    "start": "419960",
    "end": "427080"
  },
  {
    "text": "just based on your uh your needs is as customers um to sort of provide the best solution for",
    "start": "427080",
    "end": "432840"
  },
  {
    "text": "that um now on the scaling front we mentioned this sort of challenge of horizontal scaling and how do we",
    "start": "432840",
    "end": "438960"
  },
  {
    "text": "ultimately solve this we now have base images within Ray within P torch",
    "start": "438960",
    "end": "444160"
  },
  {
    "text": "anything basically with a Cuda Library that's going to be tens of gigabytes and so um if those of you who are familiar",
    "start": "444160",
    "end": "450680"
  },
  {
    "text": "with um oci images and and sort of how layers work this whole system of",
    "start": "450680",
    "end": "455879"
  },
  {
    "text": "shipping around containers really wasn't designed for tens of gigabytes and so the system actually ends up being",
    "start": "455879",
    "end": "462440"
  },
  {
    "text": "incredibly slow we've done some things recently in the last couple of years um things like image streaming so you can",
    "start": "462440",
    "end": "468800"
  },
  {
    "text": "sort of lazy load those layers together but still you're sort of bound by the laws of physics we have to move bits",
    "start": "468800",
    "end": "474039"
  },
  {
    "text": "around from one node to another and so one of the ways in which we're trying to solve that to increase the performance",
    "start": "474039",
    "end": "480479"
  },
  {
    "text": "of cold starts is um what we call secondary boot dis or container image pre-loading so you can imagine a",
    "start": "480479",
    "end": "486759"
  },
  {
    "text": "scenario where if you have a standard set of Base images that you're using across your ray clusters you would",
    "start": "486759",
    "end": "492080"
  },
  {
    "text": "essentially pre-bake those into your node pool so every time a new node stands up that node pool is essentially",
    "start": "492080",
    "end": "498440"
  },
  {
    "text": "pre-provisioned or that image essentially is pre-provisioned in a known location so container D can go and",
    "start": "498440",
    "end": "504479"
  },
  {
    "text": "basically pull it directly from the cache so it it significantly reduces the overhead of cold starts and we've seen",
    "start": "504479",
    "end": "511199"
  },
  {
    "text": "great examples of customers who have seen you know upwards of 29x improvements in starter performance with",
    "start": "511199",
    "end": "517039"
  },
  {
    "text": "things like you know 15 to 20 gigabyte or 16 gigabyte container images so that's um one way in which",
    "start": "517039",
    "end": "524000"
  },
  {
    "text": "we're we're helping to sort of improve performance and you know we've seen that pretty consistently across the a number",
    "start": "524000",
    "end": "529480"
  },
  {
    "text": "of talks this week but another way is on the the data side and so one of the ways",
    "start": "529480",
    "end": "535399"
  },
  {
    "text": "in which um kubernetes generally expresses connectivity to backend uh you",
    "start": "535399",
    "end": "540600"
  },
  {
    "text": "know data system data storage systems is via a CSI driver and so you can consume a bunch of different um storage backends",
    "start": "540600",
    "end": "547240"
  },
  {
    "text": "um one of the ways in which um sort of in talking to our customers we see a data centricity around uh GCS or or sort",
    "start": "547240",
    "end": "554640"
  },
  {
    "text": "of object-based storage so we want to make it as easy as possible for data scientists who are used to using",
    "start": "554640",
    "end": "559680"
  },
  {
    "text": "object-based storage to consume that data within their GK cluster and do it in such a way that they got Optimal",
    "start": "559680",
    "end": "565079"
  },
  {
    "text": "Performance both in terms of caching and parallelism and so uh We've introduced what's called uh GCS fuse or Google",
    "start": "565079",
    "end": "572360"
  },
  {
    "text": "Cloud Storage fuse and essentially what that does is it fuses sort of object storage into a file like storage",
    "start": "572360",
    "end": "578560"
  },
  {
    "text": "semantic uh and that gives you significant import performance improvements and things like parallelism",
    "start": "578560",
    "end": "584079"
  },
  {
    "text": "uh and caching so you can actually utilize those local ssds that are sort of inherent in a lot of their GPU",
    "start": "584079",
    "end": "590160"
  },
  {
    "text": "deployments you got these gpus with very underutilized local ssds so we're taking advantage of those local ssds by",
    "start": "590160",
    "end": "596160"
  },
  {
    "text": "pre-caching that information um and we've seen um or that data and that we've seen significant improvements um",
    "start": "596160",
    "end": "602000"
  },
  {
    "text": "in that if you have you know multi multi-epoch training runs as an example we're sort of caching that so the",
    "start": "602000",
    "end": "607880"
  },
  {
    "text": "secondary and and uh you know two to n uh epochs are going to be significantly more performant than the",
    "start": "607880",
    "end": "614079"
  },
  {
    "text": "previous um on the obtainability side so one challenge again we've seen is I want",
    "start": "614079",
    "end": "619880"
  },
  {
    "text": "this GPU I want it now um and I get a stock out error right and we monitor that by the way and so we realize this",
    "start": "619880",
    "end": "626240"
  },
  {
    "text": "is a pain point that our customers experience they tell us about it unfortunately just about every day so",
    "start": "626240",
    "end": "631600"
  },
  {
    "text": "this is a uh the idea of GPU scarcity unfortunately is not going away uh but",
    "start": "631600",
    "end": "636760"
  },
  {
    "text": "really what we want to do is provide sort of better apis and better experiences so we can help optimize gpus",
    "start": "636760",
    "end": "642839"
  },
  {
    "text": "for your use case essentially make them available as soon as they're available directly to you and there's a couple",
    "start": "642839",
    "end": "648360"
  },
  {
    "text": "ways in which we are doing that uh one is what's called a compute class um so there's actually a new paradigm within",
    "start": "648360",
    "end": "654680"
  },
  {
    "text": "kubernetes so you can start to express priorities so you know my highest priority is going to be uh you know",
    "start": "654680",
    "end": "661399"
  },
  {
    "text": "whatever an h100 with a spot instance if that's not available use my reservation if that's not available use this right",
    "start": "661399",
    "end": "667680"
  },
  {
    "text": "and and we're continuing to extend this across single region multi- region uh kind of use case so this is one semantic",
    "start": "667680",
    "end": "673880"
  },
  {
    "text": "that is being built into uh into gke and into kubernetes uh to help with some of this problem so we'll sort of prioritize",
    "start": "673880",
    "end": "680279"
  },
  {
    "text": "if it's not available we'll move down to the next one uh so on and so forth um another um important semantic that we'll",
    "start": "680279",
    "end": "687600"
  },
  {
    "text": "talk about is sort of queuing and this idea of well I want to put something in a queue and as soon as resources are",
    "start": "687600",
    "end": "693320"
  },
  {
    "text": "available I want to execute against that and the mechanism that we're do that we're using in order to to CU is project",
    "start": "693320",
    "end": "700200"
  },
  {
    "text": "that we call Q we're really good at names um but this is actually an upstream uh kubernetes project uh and so",
    "start": "700200",
    "end": "708160"
  },
  {
    "text": "this is actually a uh a kubernetes Centric uh queuing system so it's kubernetes native and that it uses",
    "start": "708160",
    "end": "713800"
  },
  {
    "text": "kubernetes based objects but it adds this sort of idea of on the exact or on",
    "start": "713800",
    "end": "719440"
  },
  {
    "text": "the specific cluster you can specify a local queue and a cluster level queue and you can provide um sort of",
    "start": "719440",
    "end": "724720"
  },
  {
    "text": "governance and control and prioritization around how those jobs are executed against that queue and that's",
    "start": "724720",
    "end": "731279"
  },
  {
    "text": "relevant in this case because now we want to start to cue up those ephemeral jobs those ephemeral Ray clusters and",
    "start": "731279",
    "end": "738000"
  },
  {
    "text": "ensure that they're sort of Bin packed if you will on the cluster itself um so for that densification problem that I",
    "start": "738000",
    "end": "743920"
  },
  {
    "text": "was talking about earlier um as customers move more and more to ephemeral deployments of of Ray jobs and",
    "start": "743920",
    "end": "749920"
  },
  {
    "text": "Ray clusters they can now start to take advantage of that queuing so as soon as a resource pool is available for for",
    "start": "749920",
    "end": "756160"
  },
  {
    "text": "execution against that particular job U it can be then scheduled and executed and we can see sort of the densification",
    "start": "756160",
    "end": "762120"
  },
  {
    "text": "a very sort of crude and simple example of how we can start to share resources across teams uh and introduce additional",
    "start": "762120",
    "end": "767920"
  },
  {
    "text": "fungibility for resource allocation um now back to the the",
    "start": "767920",
    "end": "773600"
  },
  {
    "text": "challenge of um obtainability we know that again the prioritization uh",
    "start": "773600",
    "end": "779399"
  },
  {
    "text": "semantic doesn't work for for everyone that's a great semantic that might work in some cases but in other cases you might want to say I want a certain",
    "start": "779399",
    "end": "786279"
  },
  {
    "text": "resource at a certain time and I want it so you know up to a certain number of days so I want you know n number of",
    "start": "786279",
    "end": "792560"
  },
  {
    "text": "h100s available for seven days as soon as they're available um and the way in which we're doing that is what's called",
    "start": "792560",
    "end": "798320"
  },
  {
    "text": "Dynamic workload scheduling and again this is a kubernetes uh API that we're",
    "start": "798320",
    "end": "803760"
  },
  {
    "text": "exposing actually across gcp and this really allows you to say you know again I'm going to request the resources that",
    "start": "803760",
    "end": "809320"
  },
  {
    "text": "I need um and with Q the um project I was mentioning a second ago we can now",
    "start": "809320",
    "end": "814360"
  },
  {
    "text": "then uh dynamically uh schedule those resources as soon as those resources are available or schedule the job I should",
    "start": "814360",
    "end": "820720"
  },
  {
    "text": "say as soon as those resources are available um and you can do that in what's called Flex mode so as soon as",
    "start": "820720",
    "end": "826480"
  },
  {
    "text": "they're available I'm just going to schedule it um which we can basically take you know we can deploy them in in",
    "start": "826480",
    "end": "831959"
  },
  {
    "text": "you know minutes and hours in some cases um or you can also do it in what's called calendar mode so in some cases you have a specific training run you",
    "start": "831959",
    "end": "838519"
  },
  {
    "text": "want it to start at a specific time um and um and you can leverage it that way as well so um hopefully that gives you a",
    "start": "838519",
    "end": "845639"
  },
  {
    "text": "little bit of a preview as far as you know some of the things that we're doing on the product side to help the kubra project uh and help our customers with",
    "start": "845639",
    "end": "852680"
  },
  {
    "text": "um building out distributed ml platforms I'm going to hand it over to abdulla to talk a little bit about how Spotify is",
    "start": "852680",
    "end": "858160"
  },
  {
    "text": "taken the approach of building our",
    "start": "858160",
    "end": "861360"
  },
  {
    "text": "platforms thank you Brandon so well my name is Abdullah I I'm a machine learning engineer at Spotify and today",
    "start": "863800",
    "end": "870360"
  },
  {
    "text": "we'll be talking about as customers of gke we'll be talking about how Spotify build its platform U using the offering",
    "start": "870360",
    "end": "876600"
  },
  {
    "text": "from GK um and some of the features that Brandon talked about how we're currently using them as",
    "start": "876600",
    "end": "883199"
  },
  {
    "text": "Spotify so just a quick overview of spotify's machine learning platform we have we call our machine learning",
    "start": "883199",
    "end": "889399"
  },
  {
    "text": "platform hrx uh which essentially aims to facilitate and I'm a practitioner from through every stage that they go",
    "start": "889399",
    "end": "896639"
  },
  {
    "text": "through during a machine learning de life cycle so from features to workflows to well providing compute um and model",
    "start": "896639",
    "end": "903680"
  },
  {
    "text": "serving Solutions what we will be focusing on today is the compute layer that's really um our solution which is",
    "start": "903680",
    "end": "910240"
  },
  {
    "text": "based which is built on gke and that's how we allow our users to provision or",
    "start": "910240",
    "end": "916079"
  },
  {
    "text": "create Ray clusters and submit Ray jobs and that's where we use a lot of offerings from",
    "start": "916079",
    "end": "921399"
  },
  {
    "text": "gke moving on and just you know I'll be giving for a few minutes I'll be giving some context so we understand how the",
    "start": "921399",
    "end": "927320"
  },
  {
    "text": "platform works so when it comes to the compute layer and when it comes to a user trying to create a ray cluster um",
    "start": "927320",
    "end": "934720"
  },
  {
    "text": "if you guys remember the keynote from yesterday uh Robert talked about complexity complexity is the enemy so we",
    "start": "934720",
    "end": "941199"
  },
  {
    "text": "do understand a lot of the stuff that actually happens on the infrastructure level it's too low level and that's not",
    "start": "941199",
    "end": "947480"
  },
  {
    "text": "something that if you're a research scientist or ANL practitioner that you would want to spend too much time",
    "start": "947480",
    "end": "952759"
  },
  {
    "text": "thinking about our solution to that is what we call Hendrick confin fig it is our abstraction over the lowlevel",
    "start": "952759",
    "end": "959920"
  },
  {
    "text": "constructs that we deal with at the infr level so as an example um when it comes to a ray",
    "start": "959920",
    "end": "965959"
  },
  {
    "text": "cluster cubra recognizes a custom resource called Ray cluster uh which is",
    "start": "965959",
    "end": "971440"
  },
  {
    "text": "a yaml file what we instead allow our users to is to keep and maintain a set of yamu files which are an abstraction",
    "start": "971440",
    "end": "979079"
  },
  {
    "text": "over the actual Ray cluster object and so they can go in as as much detail as",
    "start": "979079",
    "end": "984519"
  },
  {
    "text": "they want to but that's a nice substraction something that if you know a regular user who doesn't need to think",
    "start": "984519",
    "end": "990199"
  },
  {
    "text": "about too much granularity can interact with and build R clusters so they submit",
    "start": "990199",
    "end": "995639"
  },
  {
    "text": "a yamu file we take care of building a ray cluster custom resource applying the features that we leverage from gke and",
    "start": "995639",
    "end": "1002959"
  },
  {
    "text": "creating a ray cluster accordingly moving on again very quickly",
    "start": "1002959",
    "end": "1008560"
  },
  {
    "text": "high level overview of the user facing side of machine learning platform at spottify which you can see on the left",
    "start": "1008560",
    "end": "1015120"
  },
  {
    "text": "in green and on the right is our infrastructure that we maintain on gke so we have stuff like um you can see Q",
    "start": "1015120",
    "end": "1022639"
  },
  {
    "text": "which I'll be talking about in a while you can see a lot of stuff that we that actually happens on the infrastructure level but we aim to provide good",
    "start": "1022639",
    "end": "1029520"
  },
  {
    "text": "abstractions to our users and try to bring down the",
    "start": "1029520",
    "end": "1034520"
  },
  {
    "text": "complexity all right so now to the problem what exactly are we trying to address and something that we actually",
    "start": "1034880",
    "end": "1040798"
  },
  {
    "text": "saw uh when team started using or showing interest in gen use cases was",
    "start": "1040799",
    "end": "1046038"
  },
  {
    "text": "mostly around gpus so a lot of times you know you're trying to run a program um",
    "start": "1046039",
    "end": "1051240"
  },
  {
    "text": "with the GPU that you have access to is not something that is fulfilling the computer requirements so we actually had",
    "start": "1051240",
    "end": "1057679"
  },
  {
    "text": "to rethink um the gpus we have and how do we optimize um optimize the gpus so",
    "start": "1057679",
    "end": "1063280"
  },
  {
    "text": "we're not wasting anything sometimes you know you may have a GPU but it's not something it's not u a GPU that actually",
    "start": "1063280",
    "end": "1069840"
  },
  {
    "text": "fulfills your need so training time could take long a lot of problems and",
    "start": "1069840",
    "end": "1074960"
  },
  {
    "text": "based on that if you do go to a bigger GPU that also comes with a huge cost so",
    "start": "1074960",
    "end": "1080840"
  },
  {
    "text": "as a platform team we also had to think about how can we have access to expensive gpus at the same time make",
    "start": "1080840",
    "end": "1087120"
  },
  {
    "text": "sure that they're utilized well and we're not they're not sitting idle we're not wasting money on the",
    "start": "1087120",
    "end": "1092799"
  },
  {
    "text": "gpus and so the problem at hand really is the lower utilization and over",
    "start": "1092799",
    "end": "1099240"
  },
  {
    "text": "provisioning uh of gpus so we want our teams we want our tents to use gpus in a",
    "start": "1099240",
    "end": "1104799"
  },
  {
    "text": "way that everyone gets a fair share and the gpus that we actually have access to are not being underutilized and that's",
    "start": "1104799",
    "end": "1112440"
  },
  {
    "text": "something that we will that's a problem we will talk about and we will be focusing on three key aspects uh or",
    "start": "1112440",
    "end": "1118320"
  },
  {
    "text": "three key three key aspects that we leverage from gke to resolve that problem uh network optimizations storage",
    "start": "1118320",
    "end": "1125919"
  },
  {
    "text": "optimizations and scheduling optimizations again quick context we uh",
    "start": "1125919",
    "end": "1132760"
  },
  {
    "text": "the machines we use from gke are A3 high you can see the specs you can also go to the gcp uh",
    "start": "1132760",
    "end": "1139400"
  },
  {
    "text": "documentation read it more on it but just to give you context on what Spotify uses currently we are on H3 high but we",
    "start": "1139400",
    "end": "1146960"
  },
  {
    "text": "are in future there is a possibility that we would consider using A3 Mega and so to talk about the features",
    "start": "1146960",
    "end": "1154200"
  },
  {
    "text": "that we leverage from gke to resolve the issue that I was talking about earlier so gke comes with high bandwidth",
    "start": "1154200",
    "end": "1161320"
  },
  {
    "text": "interconnect that has been very useful as Spotify um so again concept talking",
    "start": "1161320",
    "end": "1166720"
  },
  {
    "text": "about the concepts there's internode where versus intranode communication so usually for the workloads which require",
    "start": "1166720",
    "end": "1173159"
  },
  {
    "text": "distributed GPU training where multiple gpus need to be in sync need to share data it's really essential that the",
    "start": "1173159",
    "end": "1179640"
  },
  {
    "text": "network on which they're sharing data and they're communicating is one fast and two reliable and so gk's offering",
    "start": "1179640",
    "end": "1186520"
  },
  {
    "text": "has been really helpful you can see the specs um when it comes to intranode um",
    "start": "1186520",
    "end": "1192440"
  },
  {
    "text": "it's a th000 g 1,000 gbits per second and when it comes to intranode it's 200",
    "start": "1192440",
    "end": "1198679"
  },
  {
    "text": "per neck 200 GB GB per second per neck so four necks that's 800 altogether",
    "start": "1198679",
    "end": "1204280"
  },
  {
    "text": "th000 g u Gaby per per second so communic so the point being that the",
    "start": "1204280",
    "end": "1210280"
  },
  {
    "text": "communication protocol that we or the networking that we use from gke has been really helpful in making sure that the",
    "start": "1210280",
    "end": "1216760"
  },
  {
    "text": "network is quick gpus that need to sync together are not are communicating reliably and quickly so we make sure",
    "start": "1216760",
    "end": "1223159"
  },
  {
    "text": "that there's no workload that is incomplete or there there's no bottle like there's no GPU that is sitting or",
    "start": "1223159",
    "end": "1229720"
  },
  {
    "text": "or to make sure that we're actually utilizing gpus to the best of their",
    "start": "1229720",
    "end": "1235120"
  },
  {
    "text": "capacity another optimization another feature that we leverage from gke is the nickel fast sockhead um so really",
    "start": "1235120",
    "end": "1242679"
  },
  {
    "text": "essentially it's you know nickel being invidious Collective communication library and it again it really also it's",
    "start": "1242679",
    "end": "1249360"
  },
  {
    "text": "a it's a transport layer plug-in to improve uh nickel Collective communication performance on Google Cloud um what you can see on the right",
    "start": "1249360",
    "end": "1256360"
  },
  {
    "text": "is a quick snippet so I would keep making this point repeatedly which is complexity is the enemy so on the right",
    "start": "1256360",
    "end": "1262640"
  },
  {
    "text": "you can see a quick terraform snippet which is essentially how we we leverage",
    "start": "1262640",
    "end": "1267799"
  },
  {
    "text": "nickel fast socket on gke it's really just two lines for our note pools a user",
    "start": "1267799",
    "end": "1272960"
  },
  {
    "text": "doesn't have to think about it as long as it's enabled and available on gge a user can simply create a cluster using",
    "start": "1272960",
    "end": "1279760"
  },
  {
    "text": "our config layer and this is enabled for them again in workloads that require distributed training distributed",
    "start": "1279760",
    "end": "1285840"
  },
  {
    "text": "fine-tuning it has been a key to uh really optimize the network and make sure that that the network is fast and",
    "start": "1285840",
    "end": "1292200"
  },
  {
    "text": "is reliable and with A3 High we also have the ability to leverage compact",
    "start": "1292200",
    "end": "1298520"
  },
  {
    "text": "placement from gke which essentially um it makes sure that the nodes the VMS",
    "start": "1298520",
    "end": "1304320"
  },
  {
    "text": "that we're using VMS with gpus are physically placed closer to each other and so this can help in performance and",
    "start": "1304320",
    "end": "1309880"
  },
  {
    "text": "also reducing the network latency among the VMS that need to communicate with each other similar idea you can see on",
    "start": "1309880",
    "end": "1317159"
  },
  {
    "text": "the right is quite terraform snippet again this is something that we have to do as a platform team and as long as",
    "start": "1317159",
    "end": "1323120"
  },
  {
    "text": "we're leveraging it from gke a user doesn't have to go into too much details and they can simply create a cluster and",
    "start": "1323120",
    "end": "1330320"
  },
  {
    "text": "have access to to the features that we leverage from gke moving on so the the first three",
    "start": "1330320",
    "end": "1337720"
  },
  {
    "text": "features we talked about were on the network optimizations that we're leveraging from gke um we also do uh use",
    "start": "1337720",
    "end": "1345080"
  },
  {
    "text": "features that are targeted on the storage optimization and one of of them as Brandon also pointed out earlier is",
    "start": "1345080",
    "end": "1351200"
  },
  {
    "text": "GCS fuse so really in use cases where someone is um train or fine tuning a",
    "start": "1351200",
    "end": "1356720"
  },
  {
    "text": "distri well distributed fine-tuning a large language model um we often see a",
    "start": "1356720",
    "end": "1362039"
  },
  {
    "text": "case where a workload is running for a long time but for some reason the rate cluster goes down and then the GPU power",
    "start": "1362039",
    "end": "1368159"
  },
  {
    "text": "that you used for fine tuning or for that job goes to waste so we really needed a solution to process or to",
    "start": "1368159",
    "end": "1373799"
  },
  {
    "text": "checkpoint or to allow users to checkpoint their models so if a rate cluster goes down it can pick up from",
    "start": "1373799",
    "end": "1380159"
  },
  {
    "text": "where it got interrupted and so GCS fuse has been a great solution it allows rate clusters to access a GCS bucket as a",
    "start": "1380159",
    "end": "1387240"
  },
  {
    "text": "local storage local file system and we have teams that use it to do model checkpointing sometimes even to store",
    "start": "1387240",
    "end": "1393799"
  },
  {
    "text": "data that is not directly related to checkpointing but it helps in cases where there's long running rate clusters",
    "start": "1393799",
    "end": "1399880"
  },
  {
    "text": "that need access to um or long running jobs that need access to some data so",
    "start": "1399880",
    "end": "1405720"
  },
  {
    "text": "just to make sure that if there's some compute that is spent on that job it doesn't go to waste and this storage",
    "start": "1405720",
    "end": "1412520"
  },
  {
    "text": "optimization has been pretty tremendous in our use case this is to show how",
    "start": "1412520",
    "end": "1417919"
  },
  {
    "text": "again as a user um the config layer that we provide again a lot a lot of stuff",
    "start": "1417919",
    "end": "1423120"
  },
  {
    "text": "goes on under the hood for the infrastructure but if you're a user using it all you have to deal with is",
    "start": "1423120",
    "end": "1429679"
  },
  {
    "text": "simply a config file and enabling GCS fuse in it so if you're user and you",
    "start": "1429679",
    "end": "1435120"
  },
  {
    "text": "provided in your config file on the Hendrick side on our platform site we would take care of creating a ray",
    "start": "1435120",
    "end": "1440440"
  },
  {
    "text": "cluster that has access to a GCS bucket and it can treat it as a local file",
    "start": "1440440",
    "end": "1446320"
  },
  {
    "text": "system and finally so we talked about two optimizations so far we talked about Network optimization and we talked about",
    "start": "1447240",
    "end": "1453480"
  },
  {
    "text": "storage optimization from gke the final optimization that we'll be talking about today is the scheduling optimization so",
    "start": "1453480",
    "end": "1460520"
  },
  {
    "text": "Brandon talked about um workload densing problem and also um GPU provisioning uh",
    "start": "1460520",
    "end": "1468200"
  },
  {
    "text": "with DWS and Q so we really the problem that we ran into is Spotify had limited",
    "start": "1468200",
    "end": "1473760"
  },
  {
    "text": "access to gpus but a lot more teams trying to use it so with limited gpus you sometimes have teams fighting over",
    "start": "1473760",
    "end": "1480279"
  },
  {
    "text": "those resources so as a platform team really had to make sure or understand what can we do to one ensure fair share",
    "start": "1480279",
    "end": "1487720"
  },
  {
    "text": "usage two understand the priority of different jobs being submitted and also make sure that um no team is able to",
    "start": "1487720",
    "end": "1495080"
  },
  {
    "text": "hold onto resources without using them also solution to this problem was to use",
    "start": "1495080",
    "end": "1500799"
  },
  {
    "text": "uh q and DWS so U just to show you a quick rundown on how it works at Spotify",
    "start": "1500799",
    "end": "1509279"
  },
  {
    "text": "is a user begins with again simply there's a yaml file that corresponds to a job and in the job if it if they want",
    "start": "1509279",
    "end": "1516360"
  },
  {
    "text": "to enable cute provisioning which is our term or the the term we use to really",
    "start": "1516360",
    "end": "1521600"
  },
  {
    "text": "really indicate that a job has to be should be executed when the resources required by it are available so a user",
    "start": "1521600",
    "end": "1529919"
  },
  {
    "text": "enables Cube provisioning if they want to use it by setting that option to True submit the yaml file and on the Hendrick",
    "start": "1529919",
    "end": "1536080"
  },
  {
    "text": "side what we do is we create a ray job which uh again another construct another custom resource at the cube Ray level",
    "start": "1536080",
    "end": "1543440"
  },
  {
    "text": "and we submit it to our GK cluster in the username space now there's a local queue in the username space where uh",
    "start": "1543440",
    "end": "1549960"
  },
  {
    "text": "team members sharing that names space could submit the job to Q also has a cluster queue which is a global queue um",
    "start": "1549960",
    "end": "1556679"
  },
  {
    "text": "it's not a namespaced object and that's really when Q um Q can make sure that",
    "start": "1556679",
    "end": "1561840"
  },
  {
    "text": "the resource is required by the ray job so Ray job creates a ray cluster for those of you are familiar with it it",
    "start": "1561840",
    "end": "1567320"
  },
  {
    "text": "creates a ray cluster runs the job on it and then deletes the ray cluster so Q",
    "start": "1567320",
    "end": "1573120"
  },
  {
    "text": "makes sure that the resources that are required by that Ray cluster once they're available it admits the job and",
    "start": "1573120",
    "end": "1579000"
  },
  {
    "text": "then we use DWS to work on GPU obtainability to really um dynamically",
    "start": "1579000",
    "end": "1585640"
  },
  {
    "text": "get access to gpus once the workload is admitted so this has been a great offering from gke we have seen um users",
    "start": "1585640",
    "end": "1592039"
  },
  {
    "text": "queuing their jobs a lot more often and this really helps it goes back to the problem that we talked about uh",
    "start": "1592039",
    "end": "1597120"
  },
  {
    "text": "underutilization and over provisioning of gpus it really helps that problem and makes sure that we the gpus that we have",
    "start": "1597120",
    "end": "1604200"
  },
  {
    "text": "uh the reserved instances we're utilizing them to the best of their capacity and we're not we don't have",
    "start": "1604200",
    "end": "1610200"
  },
  {
    "text": "resources that we are paying for and no one's using them so that's how it works",
    "start": "1610200",
    "end": "1616279"
  },
  {
    "text": "at Spotify again I'll Ty to two concepts two things that we talked about I'll try to bring them I'll try to bring the",
    "start": "1616279",
    "end": "1622440"
  },
  {
    "text": "points together from GK side we use a lot of optimizations three categories are talked about network optimizations",
    "start": "1622440",
    "end": "1629120"
  },
  {
    "text": "storage optimizations with GCS PS scheduling optimizations with q and DWS",
    "start": "1629120",
    "end": "1634360"
  },
  {
    "text": "on the Hendrick side on the platform side the aim of the message we want to deliver is you really have to think",
    "start": "1634360",
    "end": "1639480"
  },
  {
    "text": "about the complexity of these features and also how do you how do you provide that feature to the users such that they",
    "start": "1639480",
    "end": "1646320"
  },
  {
    "text": "don't have to think too much about it so we have these great features that we use solving our problems and we spend a lot",
    "start": "1646320",
    "end": "1652240"
  },
  {
    "text": "of effort on making sure that we are uh lowering the complexity so as a user you",
    "start": "1652240",
    "end": "1657399"
  },
  {
    "text": "can really focus on the workload that you have and not think about the resources and the infrastructure level",
    "start": "1657399",
    "end": "1663519"
  },
  {
    "text": "um struggles all in all it has been a very smooth experience working with gke",
    "start": "1663519",
    "end": "1668559"
  },
  {
    "text": "um you know we always appreciate the response time and the openness to work",
    "start": "1668559",
    "end": "1674080"
  },
  {
    "text": "with us and so yeah this was in a nutshell Spotify platform we talked",
    "start": "1674080",
    "end": "1679559"
  },
  {
    "text": "about some quick problems but if you are interested in other problems that we're running into I will be here feel free to",
    "start": "1679559",
    "end": "1684840"
  },
  {
    "text": "reach me um and happy to chat about [Applause]",
    "start": "1684840",
    "end": "1692700"
  },
  {
    "text": "it so I think we got about two minutes for additional questions hopefully more of the",
    "start": "1693720",
    "end": "1699039"
  },
  {
    "text": "questions are directed to Abdullah because you know he's actually building this stuff in the real world",
    "start": "1699039",
    "end": "1705360"
  },
  {
    "text": "question which is run",
    "start": "1705360",
    "end": "1712919"
  },
  {
    "text": "running problem so just to repeat the question so that question is about use of GCS",
    "start": "1713640",
    "end": "1719960"
  },
  {
    "text": "fuse and posix compliance and sort of the challenges that they've seen in terms yeah so it's a recent offering we",
    "start": "1719960",
    "end": "1725760"
  },
  {
    "text": "haven't seen too many issues with it we actually had so before GCS feuded we had an internal solution uh on for storage",
    "start": "1725760",
    "end": "1732559"
  },
  {
    "text": "but recently switched to GCS so I guess it's yet to be seen if there's are there are issues in the future so far using it",
    "start": "1732559",
    "end": "1738720"
  },
  {
    "text": "it's been a smooth experience we don't um yeah no",
    "start": "1738720",
    "end": "1744000"
  },
  {
    "text": "issue are there any more questions",
    "start": "1744200",
    "end": "1750120"
  },
  {
    "text": "hi um so question is performance issues with GCS fuse have we looked",
    "start": "1754559",
    "end": "1761000"
  },
  {
    "text": "into hyp disc ml uh we have not yet uh",
    "start": "1761000",
    "end": "1766159"
  },
  {
    "text": "it's still uh well J is a very fast evolving field so everything happens too",
    "start": "1766159",
    "end": "1771320"
  },
  {
    "text": "quickly so far we have not but it's something uh if there are issues that we see our users facing we're open to",
    "start": "1771320",
    "end": "1778600"
  },
  {
    "text": "looking into it but we don't currently use it yeah hyper dis ml is a relatively new uh offering within gcp so um but I'd",
    "start": "1778600",
    "end": "1785080"
  },
  {
    "text": "be happy to answer any questions specific to HTML and um yeah it's it's a",
    "start": "1785080",
    "end": "1790720"
  },
  {
    "text": "you know different storage solution obviously um in general we see customers still gravitating towards GCS as sort of",
    "start": "1790720",
    "end": "1796399"
  },
  {
    "text": "a central repository and then hydrating the data from GCS into HTML but it's it's block storage solution different",
    "start": "1796399",
    "end": "1801960"
  },
  {
    "text": "than object storage um so there's some additional complexity there um uh that you get in exchange for the",
    "start": "1801960",
    "end": "1809440"
  },
  {
    "text": "performance I'll be taking more questions any other",
    "start": "1810600",
    "end": "1814720"
  },
  {
    "text": "questions when you're teams",
    "start": "1816320",
    "end": "1821360"
  },
  {
    "text": "right the question is if our teams use fractional GPU or whole GPU to overcome the problem of GPU uh utilization",
    "start": "1830799",
    "end": "1838080"
  },
  {
    "text": "currently this use whole gpus um but fractional GPU if there's a problem in",
    "start": "1838080",
    "end": "1843480"
  },
  {
    "text": "future something that is on our road map I think any other",
    "start": "1843480",
    "end": "1851600"
  },
  {
    "text": "questions excellent thank you so much awesome thank you [Applause]",
    "start": "1851600",
    "end": "1861540"
  }
]