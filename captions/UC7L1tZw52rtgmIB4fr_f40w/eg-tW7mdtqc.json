[
  {
    "text": "thanks for joining me the topic for",
    "start": "4880",
    "end": "9960"
  },
  {
    "text": "today's talk is Enterprise vector-based information retrieval and prompt engineering at scale using open source",
    "start": "9960",
    "end": "15059"
  },
  {
    "text": "Technologies on AWS I'm just glad to be doing this in person so I don't have to worry about my cat",
    "start": "15059",
    "end": "21000"
  },
  {
    "text": "jumping on the keyboard uh but before I get into the talk I want to",
    "start": "21000",
    "end": "28859"
  },
  {
    "text": "do a quick show of hands um who how many people in the room have",
    "start": "28859",
    "end": "34980"
  },
  {
    "text": "actually heard of retrieval augmented generation okay so quite a decent amount and then",
    "start": "34980",
    "end": "40860"
  },
  {
    "text": "how many of you have actually like played with it like done some actual work with it and kind of get what rag is",
    "start": "40860",
    "end": "50600"
  },
  {
    "text": "okay fair enough great so uh yeah we do have a packed agenda",
    "start": "50640",
    "end": "57420"
  },
  {
    "text": "today and I will be discussing the challenges and opportunities for the",
    "start": "57420",
    "end": "62520"
  },
  {
    "text": "Enterprise when it comes to generative AI and then deep dive into vectorized",
    "start": "62520",
    "end": "68220"
  },
  {
    "text": "information retrieval using the rag architecture retrieval augmented generation architecture",
    "start": "68220",
    "end": "74700"
  },
  {
    "text": "and then I will describe the role of of Rey in addressing these the scaling",
    "start": "74700",
    "end": "80880"
  },
  {
    "text": "challenges that are encountered in the rag architecture um stick around till the end because I",
    "start": "80880",
    "end": "87060"
  },
  {
    "text": "will end with a demo so um",
    "start": "87060",
    "end": "92580"
  },
  {
    "text": "when we are building these uh rag applications or chat Bots Enterprise chat Bots or agents achieving quality",
    "start": "92580",
    "end": "100740"
  },
  {
    "text": "user experience is is being able to provide very high quality results in the",
    "start": "100740",
    "end": "105900"
  },
  {
    "text": "least amount of time possible regardless of the size of the user base so what you'll see is that when you're",
    "start": "105900",
    "end": "112740"
  },
  {
    "text": "building these applications what it comes down to is like this this causal interplay of scale latency and",
    "start": "112740",
    "end": "119100"
  },
  {
    "text": "accuracy and when you're building these applications you're trying to find like",
    "start": "119100",
    "end": "124200"
  },
  {
    "text": "an Optimum uh or an Optimum uh you know what I'm calling a global Optimum",
    "start": "124200",
    "end": "129539"
  },
  {
    "text": "because I'm taking this graphic from from a gradient descent example but",
    "start": "129539",
    "end": "134760"
  },
  {
    "text": "you're trying to balance these factors out um and you know you have to you know pay each of these different",
    "start": "134760",
    "end": "142140"
  },
  {
    "text": "dimensions attention in order to get optimal user experience",
    "start": "142140",
    "end": "147900"
  },
  {
    "text": "so there are many challenges when you're dealing with these large language models large language models are the core of",
    "start": "147900",
    "end": "153599"
  },
  {
    "text": "what's sitting at the gender of AI applications um",
    "start": "153599",
    "end": "158640"
  },
  {
    "text": "they're only a subset of the challenges that I introduce here which are relevant to rag in particular",
    "start": "158640",
    "end": "165540"
  },
  {
    "text": "um so number one and the most important one is is hallucinations so likely",
    "start": "165540",
    "end": "171300"
  },
  {
    "text": "you're already aware of hallucinations and large language models but basically the idea is these llms are trained on",
    "start": "171300",
    "end": "178980"
  },
  {
    "text": "very large Corpus of data and they perform really well near human",
    "start": "178980",
    "end": "185340"
  },
  {
    "text": "level accuracy if you ask them questions that are contained in the training data",
    "start": "185340",
    "end": "190560"
  },
  {
    "text": "set when you ask these llms questions that lack context from private data",
    "start": "190560",
    "end": "197099"
  },
  {
    "text": "let's say Enterprise data or something that it's not been trained on that's when it starts to provide you know give",
    "start": "197099",
    "end": "202500"
  },
  {
    "text": "these these spurious kind of results that are known as hallucinations and",
    "start": "202500",
    "end": "208260"
  },
  {
    "text": "there's an example there of chat GPT talking about a New York",
    "start": "208260",
    "end": "213480"
  },
  {
    "text": "Times article that does not exist um and then also like",
    "start": "213480",
    "end": "219540"
  },
  {
    "text": "um sometimes llms may may be subject to not being able to tell fact from fiction",
    "start": "219540",
    "end": "224640"
  },
  {
    "text": "so taking a movie storyline and actually thinking that happened in real life when it's just basically a fictional plot",
    "start": "224640",
    "end": "231900"
  },
  {
    "text": "right so uh these are these these are real challenges with um",
    "start": "231900",
    "end": "237420"
  },
  {
    "text": "llms and what's interesting is so you know how llms are trained on these their",
    "start": "237420",
    "end": "243900"
  },
  {
    "text": "billions of parameters um and you know it can be you know few hundred million to over 100 billion",
    "start": "243900",
    "end": "251040"
  },
  {
    "text": "parameters and generally the idea is that the more parameters the model is",
    "start": "251040",
    "end": "256320"
  },
  {
    "text": "built on the more accurate it will be on known tasks um",
    "start": "256320",
    "end": "261959"
  },
  {
    "text": "what's interesting is that the bigger the model gets the worse the hallucinations get so if the model is",
    "start": "261959",
    "end": "269820"
  },
  {
    "text": "bigger in size it's supposed to be more accurate but if it if you ask it questions on data that it's not being trained on the hallucination it'll",
    "start": "269820",
    "end": "276840"
  },
  {
    "text": "hallucinate more than a than a smaller model um so there are these academic studies that",
    "start": "276840",
    "end": "283800"
  },
  {
    "text": "have actually Quantified these hallucinations so we're seeing these uh u-shaped and inverse u-scaling Trends",
    "start": "283800",
    "end": "292560"
  },
  {
    "text": "um where uh you know these models are basically less reliable at or their",
    "start": "292560",
    "end": "297900"
  },
  {
    "text": "scaling trends that are less reliable at predicting the behavior of large-scale models than than previously understood",
    "start": "297900",
    "end": "305460"
  },
  {
    "text": "um so behind every successful machine learning model there's data it's an old saying right and gen AI is no different",
    "start": "305460",
    "end": "314000"
  },
  {
    "text": "so Enterprise generative AI applications need access to many different data",
    "start": "314000",
    "end": "320340"
  },
  {
    "text": "sources including you know the internet or even just Enterprise data which is",
    "start": "320340",
    "end": "325440"
  },
  {
    "text": "sitting inside your you know VPC or inside your Cloud environment",
    "start": "325440",
    "end": "330740"
  },
  {
    "text": "the data that is going to be fed to the model as context can get quite large and",
    "start": "330740",
    "end": "336479"
  },
  {
    "text": "quite fast and so there should be a mechanism to even scale what we call the embedding",
    "start": "336479",
    "end": "342479"
  },
  {
    "text": "process so basically we are taking all this this Corpus of data we're converting those into Vector embeddings that we're going to store into like a",
    "start": "342479",
    "end": "349979"
  },
  {
    "text": "vector database and in the typical rag architecture so that that can actually get uh really massive as you start to",
    "start": "349979",
    "end": "358080"
  },
  {
    "text": "scale up the number of data sources also when you start exposing these",
    "start": "358080",
    "end": "364080"
  },
  {
    "text": "applications to a larger user base you will need to account for the query",
    "start": "364080",
    "end": "370380"
  },
  {
    "text": "per second throughput that uh your application is providing",
    "start": "370380",
    "end": "376698"
  },
  {
    "text": "so every llm's Behavior varies slightly depending on the purpose that it's",
    "start": "377880",
    "end": "384419"
  },
  {
    "text": "serving in this talk I will be specifically talking about this very popular pre-trained llm from meta called",
    "start": "384419",
    "end": "391500"
  },
  {
    "text": "Lama 2. um so there's some considerations here so I",
    "start": "391500",
    "end": "397500"
  },
  {
    "text": "mentioned the inverse scaling law uh you know obviously that becomes very",
    "start": "397500",
    "end": "402720"
  },
  {
    "text": "apparent if you're actually monitoring the accuracy and perplexity of the output from the llm and then there's",
    "start": "402720",
    "end": "411539"
  },
  {
    "text": "also the ordering bias so what you see is uh this is kind of the inverse of the",
    "start": "411539",
    "end": "416639"
  },
  {
    "text": "inverse skating law where you see smaller models when they retrieve",
    "start": "416639",
    "end": "422940"
  },
  {
    "text": "information from Vector databases let's say they retrieve like the three top results they tend to prefer the the you know",
    "start": "422940",
    "end": "431220"
  },
  {
    "text": "based on the ranked order of the results they tend to show or you know prioritize",
    "start": "431220",
    "end": "436560"
  },
  {
    "text": "those at the output as the output of the end user which may not be always correct",
    "start": "436560",
    "end": "442319"
  },
  {
    "text": "um and then you know in terms of the infrastructure",
    "start": "442319",
    "end": "447360"
  },
  {
    "text": "itself there are some obviously these llms need a very you know they need",
    "start": "447360",
    "end": "453000"
  },
  {
    "text": "large infrastructure they need machines with very many gpus um and uh you know there are some",
    "start": "453000",
    "end": "459360"
  },
  {
    "text": "recommendations that I've mentioned here these are actually from hugging face they can be uh not easy to find so for",
    "start": "459360",
    "end": "466860"
  },
  {
    "text": "example if you are dealing with a 7 billion parameter model that can sit on",
    "start": "466860",
    "end": "472139"
  },
  {
    "text": "a medium-sized uh Nvidia a10g uh",
    "start": "472139",
    "end": "477319"
  },
  {
    "text": "instance but as you start to scale up you'll see that you know you'll need",
    "start": "477319",
    "end": "483000"
  },
  {
    "text": "many more machines many more instances which can be not that easy to find we",
    "start": "483000",
    "end": "488580"
  },
  {
    "text": "had AWS or are also starting to we actually released the llama 2 support",
    "start": "488580",
    "end": "497780"
  },
  {
    "text": "on our AWS purpose built processors for",
    "start": "497780",
    "end": "503400"
  },
  {
    "text": "generative AI we just had to talk about that we will also mention that in the in",
    "start": "503400",
    "end": "508740"
  },
  {
    "text": "the keynote tomorrow um but here I'm talking about Nvidia",
    "start": "508740",
    "end": "514200"
  },
  {
    "text": "instances specifically also on AWS",
    "start": "514200",
    "end": "519300"
  },
  {
    "text": "um and then a Kent so the the output from the 70 billion parameter lamma 2",
    "start": "519300",
    "end": "525060"
  },
  {
    "text": "model is excellent it's uh you know if we take it into context of gpt4 that everybody knows about",
    "start": "525060",
    "end": "531480"
  },
  {
    "text": "um it can you know reach similar uh near human factuality levels",
    "start": "531480",
    "end": "537839"
  },
  {
    "text": "um and there's a graphic that shows uh you know how good llama to 70 billion parameter model actually performs in",
    "start": "537839",
    "end": "544620"
  },
  {
    "text": "comparison to uh to gpd4 so you can see it's actually pretty close or even better in some cases and all of these",
    "start": "544620",
    "end": "552120"
  },
  {
    "text": "what I've mentioned are um they are not considering quantization",
    "start": "552120",
    "end": "558120"
  },
  {
    "text": "which I will talk about in detail in the next few slides",
    "start": "558120",
    "end": "563820"
  },
  {
    "text": "this is um in French 2 is is what we uh we just",
    "start": "563820",
    "end": "570000"
  },
  {
    "text": "released the race support for inferentia twos if you are building these llm based workloads you can now use Rey for",
    "start": "570000",
    "end": "577380"
  },
  {
    "text": "scaling these workloads on AWS purpose-built Hardware hardware for uh",
    "start": "577380",
    "end": "583320"
  },
  {
    "text": "specifically for serving uh there's there's inferential for serving and then for model training there's trainium",
    "start": "583320",
    "end": "589019"
  },
  {
    "text": "there's still work going on and we will uh you know we plan to get re-working uh",
    "start": "589019",
    "end": "595320"
  },
  {
    "text": "for training model training Andre training as well but this is specifically for serving you can see",
    "start": "595320",
    "end": "600779"
  },
  {
    "text": "that compared to the a10g processors that I just talked about which was one of the right recommendations for serving",
    "start": "600779",
    "end": "607080"
  },
  {
    "text": "llama2 models and hugging face inf2 actually uh perform uh better",
    "start": "607080",
    "end": "612600"
  },
  {
    "text": "and provide better latency and throughput compared to the G5",
    "start": "612600",
    "end": "618080"
  },
  {
    "text": "instance types which are backed by Nvidia 10g processors",
    "start": "618080",
    "end": "624920"
  },
  {
    "text": "um and so yeah quantization is an important technique when we're talking about",
    "start": "626160",
    "end": "631680"
  },
  {
    "text": "scaling um so there are there are really two types of quantization techniques there",
    "start": "631680",
    "end": "637200"
  },
  {
    "text": "is uh um you know there's the one that is used when you're actually training these",
    "start": "637200",
    "end": "642480"
  },
  {
    "text": "pre-trained models and then there's a post training quantization technique uh the post training one is more applicable",
    "start": "642480",
    "end": "648180"
  },
  {
    "text": "in the in the rag um architecture you can see here this is an actual graphic of what I'm going to",
    "start": "648180",
    "end": "653760"
  },
  {
    "text": "show you in the demo uh when I took a 13 billion parameter model and I loaded it",
    "start": "653760",
    "end": "659160"
  },
  {
    "text": "onto a single g5.8x large instance without quantization uh the",
    "start": "659160",
    "end": "665459"
  },
  {
    "text": "um uh yeah the GPU utilization was 94 when I did add the quantization uh it",
    "start": "665459",
    "end": "672899"
  },
  {
    "text": "went down to 47 the you know in short basically quantization reduces your",
    "start": "672899",
    "end": "678420"
  },
  {
    "text": "memory requirement and also uses less power for your",
    "start": "678420",
    "end": "684120"
  },
  {
    "text": "for your application and then I'm not going to go too deep into this but uh",
    "start": "684120",
    "end": "689339"
  },
  {
    "text": "essentially since everybody already knows about drag but this is a pretty standard rag",
    "start": "689339",
    "end": "695040"
  },
  {
    "text": "architecture we're also starting to see evolution of these autonomous and semi-autonomous agents within that are",
    "start": "695040",
    "end": "701880"
  },
  {
    "text": "powering different uh parts of the rag architecture uh and then in order to actually improve",
    "start": "701880",
    "end": "708300"
  },
  {
    "text": "the user experience and keep it conversational and improve the response time uh we need the ability for the",
    "start": "708300",
    "end": "714839"
  },
  {
    "text": "model to keep short-term memory of the interactions with the user so that's where it's important to introduce uh",
    "start": "714839",
    "end": "723240"
  },
  {
    "text": "cache so outside of just having the long-term storage for Vector databases you also want to Cache these results so",
    "start": "723240",
    "end": "730320"
  },
  {
    "text": "when a user actually queries uh you know if if there's a cache hit you'll see that there is I'll show you actually",
    "start": "730320",
    "end": "736260"
  },
  {
    "text": "there's the the latency is greatly reduced",
    "start": "736260",
    "end": "742320"
  },
  {
    "text": "and then as you were scaling up so you want to scale up independently the",
    "start": "742320",
    "end": "747959"
  },
  {
    "text": "embedding process like I mentioned earlier you might actually land up embedding a large Corpus of documents",
    "start": "747959",
    "end": "753140"
  },
  {
    "text": "and then you want to scale up the inferencing as well so as you start surfacing these applications to a larger",
    "start": "753140",
    "end": "760980"
  },
  {
    "text": "user base you want to be able to scale up both so Ray provides a perfect API or perfect solution for this so there's a",
    "start": "760980",
    "end": "767279"
  },
  {
    "text": "ray core that will paralyze these tasks and then there is Ray serve which is a",
    "start": "767279",
    "end": "772680"
  },
  {
    "text": "sorry Ray core which would parallelize the embedding process and then there's a separate Library Reserve uh which will",
    "start": "772680",
    "end": "780180"
  },
  {
    "text": "which helps in building online inference apis and uh you know you can uh you can",
    "start": "780180",
    "end": "786000"
  },
  {
    "text": "scale up just the serving layer using record or Reserve as well",
    "start": "786000",
    "end": "791480"
  },
  {
    "text": "um so the demo that I actually built is including uh llama2 it's built on llama2 the orchestrator that I'm using in this",
    "start": "792720",
    "end": "799200"
  },
  {
    "text": "case from if you see that image the llm orchestrator is using Lang chain uh Ray",
    "start": "799200",
    "end": "804420"
  },
  {
    "text": "for scaling and then I'm using the pine cone Vector database uh for um",
    "start": "804420",
    "end": "810120"
  },
  {
    "text": "for the inferencing so okay good",
    "start": "810120",
    "end": "817079"
  },
  {
    "text": "so um yeah so talking about Ray for a little bit um",
    "start": "817079",
    "end": "822779"
  },
  {
    "text": "so it's really easy to go from like a single node into like a cluster mode",
    "start": "822779",
    "end": "828440"
  },
  {
    "text": "using Ray so let's say if you have like a python function or you know a couple of python functions like that",
    "start": "828440",
    "end": "835519"
  },
  {
    "text": "in order to let's say you want to distribute this number one you need a array cluster so you can run this Ray",
    "start": "835519",
    "end": "842220"
  },
  {
    "text": "cluster on many different infrastructure types you can have the Stray cluster running on ec2 you can have it running",
    "start": "842220",
    "end": "848459"
  },
  {
    "text": "on kubernetes on eks on AWS or on any scale obviously uh so basically what",
    "start": "848459",
    "end": "855420"
  },
  {
    "text": "you'll do is you will add this this decorator radar remote and then you will",
    "start": "855420",
    "end": "860880"
  },
  {
    "text": "add an extension to your actual function to call that decorator right",
    "start": "860880",
    "end": "866100"
  },
  {
    "text": "it's really that simple and then there there are also you know you can provide at a granular level like how Ray is",
    "start": "866100",
    "end": "872639"
  },
  {
    "text": "different from spark is that you can gracefully uh provide you know the number of gpus you want to use for that",
    "start": "872639",
    "end": "878700"
  },
  {
    "text": "process number of CPUs you want to use and you don't have to just think about scaling from machine to machine you can",
    "start": "878700",
    "end": "884579"
  },
  {
    "text": "actually think about scaling at a processor level at the number of cores you want to consume for a particular process",
    "start": "884579",
    "end": "891120"
  },
  {
    "text": "so and then you can also like there's purpose-built Ray libraries for the kind",
    "start": "891120",
    "end": "897300"
  },
  {
    "text": "of accelerators that you're using so if you're using for example a100s you can provide that to Ray and you will get you",
    "start": "897300",
    "end": "902639"
  },
  {
    "text": "know optimizations there we also recently announced and we will actually have that announcement officially in",
    "start": "902639",
    "end": "908940"
  },
  {
    "text": "tomorrow's keynote where Ray will automatically also recognize the trainium and",
    "start": "908940",
    "end": "915620"
  },
  {
    "text": "infrentia accelerator types like I said before trainium and in French are AWS",
    "start": "915620",
    "end": "921120"
  },
  {
    "text": "purpose built processors for um building generative AI applications and",
    "start": "921120",
    "end": "927959"
  },
  {
    "text": "hosting these large language models so training and serving these large language models",
    "start": "927959",
    "end": "933920"
  },
  {
    "text": "so just some numbers here um so if I if you don't do for example any",
    "start": "934860",
    "end": "941699"
  },
  {
    "text": "any caching or quantization running on a single core G5 dot 8X large instance",
    "start": "941699",
    "end": "950160"
  },
  {
    "text": "which has 32 Gigs available GPU memory uh if you take a basic system prompt",
    "start": "950160",
    "end": "957120"
  },
  {
    "text": "with a basic query such as you know give me some more information about Ray you know we are seeing GPU usage of 94 and",
    "start": "957120",
    "end": "965760"
  },
  {
    "text": "RAM usage of 20 Gigabytes and takes about 26 seconds to run that query",
    "start": "965760",
    "end": "972120"
  },
  {
    "text": "that same query if you quantize the model runs in a fraction of a time you're",
    "start": "972120",
    "end": "977339"
  },
  {
    "text": "talking four seconds comparatively and the GPU usage also goes down by 50",
    "start": "977339",
    "end": "983360"
  },
  {
    "text": "and the RAM usage goes down by less than 50 percent now what's interesting is uh you know",
    "start": "983360",
    "end": "989040"
  },
  {
    "text": "when you're actually building this Enterprise chat bot you would want to not always hit the endpoints and add",
    "start": "989040",
    "end": "996839"
  },
  {
    "text": "latency to your serving layer so that's where I mentioned the caching layer so",
    "start": "996839",
    "end": "1002899"
  },
  {
    "text": "you would want to have results being surfaced from the cache and you'll see uh dramatic Improvement in latency there",
    "start": "1002899",
    "end": "1010639"
  },
  {
    "text": "so instead of four seconds to answer that query you know if you just pull it from the cache it's subsequent latency",
    "start": "1010639",
    "end": "1015920"
  },
  {
    "text": "right um similarly as uh you know if you",
    "start": "1015920",
    "end": "1021740"
  },
  {
    "text": "provide a a deeper query So when you say depth one so um when the model is",
    "start": "1021740",
    "end": "1027380"
  },
  {
    "text": "actually having to go or the vector database is actually having to go and find a child web page of the parent web",
    "start": "1027380",
    "end": "1034220"
  },
  {
    "text": "page so depth is greater than one so you start seeing the latency go up a little bit more and then when you start doing",
    "start": "1034220",
    "end": "1041720"
  },
  {
    "text": "actual prompt engineering right so you you're prompt tuning and you're providing different kinds of prompts such as you want your model to write uh",
    "start": "1041720",
    "end": "1051020"
  },
  {
    "text": "the code um so that's when you know you really start",
    "start": "1051020",
    "end": "1057440"
  },
  {
    "text": "seeing the latency go up and that's uh um also because the model is trying to",
    "start": "1057440",
    "end": "1064640"
  },
  {
    "text": "now meet its maximum token limit so in the previous queries which are simpler",
    "start": "1064640",
    "end": "1069860"
  },
  {
    "text": "you get you know results which are like 128 or less number of tokens or the",
    "start": "1069860",
    "end": "1075440"
  },
  {
    "text": "words uh the token size or the maximum token limit in this case that I'm using",
    "start": "1075440",
    "end": "1080960"
  },
  {
    "text": "is 512 tokens so when you get a response the model will max out after producing",
    "start": "1080960",
    "end": "1086620"
  },
  {
    "text": "512 words in the response so that's where it's actually maxing out when",
    "start": "1086620",
    "end": "1091940"
  },
  {
    "text": "you're providing these more difficult uh more elegant or more complex prompts",
    "start": "1091940",
    "end": "1099080"
  },
  {
    "text": "um but you also see like a scaling Trend so the model actually gives you better",
    "start": "1099080",
    "end": "1105860"
  },
  {
    "text": "throughput uh when you ask for these more complex queries than just returning",
    "start": "1105860",
    "end": "1113539"
  },
  {
    "text": "simple results um so this is uh you know you can see",
    "start": "1113539",
    "end": "1119960"
  },
  {
    "text": "the the highest throughput that that I was able to get before optimization which is not using",
    "start": "1119960",
    "end": "1127460"
  },
  {
    "text": "caching not using quantization was about 1.6 tokens per second which is uh which",
    "start": "1127460",
    "end": "1133400"
  },
  {
    "text": "is similar to the speech of an average American an average American uh uh you",
    "start": "1133400",
    "end": "1139280"
  },
  {
    "text": "know the rate of speech is about two words per second um",
    "start": "1139280",
    "end": "1144620"
  },
  {
    "text": "and then if I if I did add quantization that's when it went up to 25 tokens per second right which is still in the in",
    "start": "1144620",
    "end": "1151760"
  },
  {
    "text": "kind of the human um realm uh but but much higher and",
    "start": "1151760",
    "end": "1157940"
  },
  {
    "text": "like we have to consider these factors when we are building these applications for the end",
    "start": "1157940",
    "end": "1163280"
  },
  {
    "text": "users and then with purpose-built Hardware like like I mentioned before we are",
    "start": "1163280",
    "end": "1169100"
  },
  {
    "text": "seeing uh you know what I call para human response rates which is like we're completely blowing uh you know into",
    "start": "1169100",
    "end": "1175039"
  },
  {
    "text": "hundreds or even thousands of tokens per second range those are based on studies I haven't personally tested that but",
    "start": "1175039",
    "end": "1180919"
  },
  {
    "text": "it's just interesting to to see that that kind of advancement in evolution is actually happening",
    "start": "1180919",
    "end": "1187840"
  },
  {
    "text": "um okay so I'm going to go into a demo of this um so this will be on GitHub you can",
    "start": "1188299",
    "end": "1194720"
  },
  {
    "text": "find this on my GitHub repo um and you you know it's it's purposeful so",
    "start": "1194720",
    "end": "1200000"
  },
  {
    "text": "you if you have a ray cluster running you can just run through this demo yourself so essentially uh what I have here is I",
    "start": "1200000",
    "end": "1208100"
  },
  {
    "text": "have a Pinecone database running in the in the back end um",
    "start": "1208100",
    "end": "1213679"
  },
  {
    "text": "so let me show you the architecture again of the application real quick",
    "start": "1213679",
    "end": "1221080"
  },
  {
    "text": "yeah so there are basically two parallel processes here one is you actually build",
    "start": "1226340",
    "end": "1231440"
  },
  {
    "text": "the Enterprise knowledge repository so think like if you are a car company and",
    "start": "1231440",
    "end": "1236660"
  },
  {
    "text": "you want to have very specific information about your car fed to the models if your end customers asks you",
    "start": "1236660",
    "end": "1243200"
  },
  {
    "text": "these these questions it doesn't have to go through you know the model does not start to hallucinate and provides",
    "start": "1243200",
    "end": "1249140"
  },
  {
    "text": "accurate responses so that's where you go through through the actual hydration of the vector database itself",
    "start": "1249140",
    "end": "1254360"
  },
  {
    "text": "uh in my case I'm actually going to take data from a a URL directly and without",
    "start": "1254360",
    "end": "1260660"
  },
  {
    "text": "having to download it I'll just create the embeddings and upload them directly onto the vector database and then you",
    "start": "1260660",
    "end": "1267200"
  },
  {
    "text": "have a parallel pipeline where you know which is the actual the user experience right which is the application that you",
    "start": "1267200",
    "end": "1273679"
  },
  {
    "text": "surface to the end user and that's where you take these embeddings from the vector database the model you know uses",
    "start": "1273679",
    "end": "1281419"
  },
  {
    "text": "these embeddings and crafts a response which is then provided to the end user",
    "start": "1281419",
    "end": "1287360"
  },
  {
    "text": "there's a cached lookup that is introduced in the middle to improve again to improve the latency if the",
    "start": "1287360",
    "end": "1294620"
  },
  {
    "text": "query has not been asked to that model before it will go to the embedding process and",
    "start": "1294620",
    "end": "1300799"
  },
  {
    "text": "that queries response will be drafted by the model and sent to the cache storage right and then the next time you ask",
    "start": "1300799",
    "end": "1307700"
  },
  {
    "text": "that query you know there's a cache hit so you get that query directly or the response from",
    "start": "1307700",
    "end": "1314360"
  },
  {
    "text": "that query directly through the cache now there's they're going to be significant you know work being put into",
    "start": "1314360",
    "end": "1321500"
  },
  {
    "text": "this caching layer as well for example right now it has to be the exact query for the",
    "start": "1321500",
    "end": "1327860"
  },
  {
    "text": "cache hit to actually happen but in the future you will see that like how the",
    "start": "1327860",
    "end": "1333559"
  },
  {
    "text": "vector database is actually calculated similarity indexes um between you know between the Corpus",
    "start": "1333559",
    "end": "1340159"
  },
  {
    "text": "and the query you will see that there'll be a similarity score that will be calculated between queries so if a query",
    "start": "1340159",
    "end": "1346460"
  },
  {
    "text": "matches previous queries even though they are not worded exactly the same you'll be able to get a successful",
    "start": "1346460",
    "end": "1351620"
  },
  {
    "text": "cachet",
    "start": "1351620",
    "end": "1354039"
  },
  {
    "text": "um see I just have a Jupiter notebook here uh in order to run this demo there are a few different requirements that",
    "start": "1356659",
    "end": "1362780"
  },
  {
    "text": "you need to have you need to have the hugging face the library uh the Transformers Library I'm downloading",
    "start": "1362780",
    "end": "1368900"
  },
  {
    "text": "llama2 or using llama2 directly from hugging phase there is a bits and bytes for the the",
    "start": "1368900",
    "end": "1376100"
  },
  {
    "text": "quantization of the model itself since I'm using Pinecone uh there is you know I need the Pinecone client and obviously",
    "start": "1376100",
    "end": "1382220"
  },
  {
    "text": "Lang chain which is the orchestration engine here so first things first like they we",
    "start": "1382220",
    "end": "1388640"
  },
  {
    "text": "create the pine cone embeddings um",
    "start": "1388640",
    "end": "1394220"
  },
  {
    "text": "so here you know I basically initialize the Pinecone client I have some Pinecone",
    "start": "1394220",
    "end": "1401000"
  },
  {
    "text": "related credentials oh I'm sorry I'm not actually sharing",
    "start": "1401000",
    "end": "1408039"
  },
  {
    "text": "okay um let's scale back up so yeah the requirements uh file so I have the you",
    "start": "1413600",
    "end": "1420380"
  },
  {
    "text": "know obviously the hugging face Library where I get uh the Llama 2 model from uh the",
    "start": "1420380",
    "end": "1426320"
  },
  {
    "text": "Transformers Library uh the Lang chain which is the orchestrator here",
    "start": "1426320",
    "end": "1431720"
  },
  {
    "text": "um and then there is bits and bytes which I use for quantization",
    "start": "1431720",
    "end": "1437139"
  },
  {
    "text": "and the first notebook that I'll walk through is basically you know hydrating the vector database with certain",
    "start": "1437179",
    "end": "1442720"
  },
  {
    "text": "information like which we want to query from the from the model itself",
    "start": "1442720",
    "end": "1448059"
  },
  {
    "text": "so initially I I initialize the the pine",
    "start": "1448059",
    "end": "1453080"
  },
  {
    "text": "cone client I need some credentials here such as the index name of the pine cone database",
    "start": "1453080",
    "end": "1460700"
  },
  {
    "text": "and some other credentials in the API key then I initialized Ray I'm going to be",
    "start": "1460700",
    "end": "1467240"
  },
  {
    "text": "using a ray cluster here my array cluster is actually it's Bill it's backed by uh Nvidia 10g inst you know",
    "start": "1467240",
    "end": "1476620"
  },
  {
    "text": "processors it's the g5.8x large that I'm using you can see the GPU here this is",
    "start": "1476620",
    "end": "1482720"
  },
  {
    "text": "the ray um dashboard you can see the GPU and you can see the breakdown of of all the",
    "start": "1482720",
    "end": "1488240"
  },
  {
    "text": "processes here as well along with the error logs and the stack Trace uh so pretty nifty tool",
    "start": "1488240",
    "end": "1494360"
  },
  {
    "text": "um and then I provide the URL so here I can you know I can give a you",
    "start": "1494360",
    "end": "1501620"
  },
  {
    "text": "know sagemaker or any online documentation uh that you want the model",
    "start": "1501620",
    "end": "1507260"
  },
  {
    "text": "to not necessarily train on but to basically look up when it's providing the responses",
    "start": "1507260",
    "end": "1514000"
  },
  {
    "text": "and then when I do the recursive URL loader I can provide the max depth so that is like how deep inside the web",
    "start": "1514000",
    "end": "1521539"
  },
  {
    "text": "page do you want uh the vector database to go to create these embeddings",
    "start": "1521539",
    "end": "1528320"
  },
  {
    "text": "and then instead of having to download all this data which uh you know which is pretty common you can actually now",
    "start": "1528320",
    "end": "1534620"
  },
  {
    "text": "upload the data directly into the vector database using this uh",
    "start": "1534620",
    "end": "1541520"
  },
  {
    "text": "recursive URL loader and that's a exactly what I'm doing here",
    "start": "1541520",
    "end": "1547299"
  },
  {
    "text": "I use the hugging face mini LM model which it's actually a very small size model and works pretty effectively",
    "start": "1547299",
    "end": "1554720"
  },
  {
    "text": "um and it takes about eight seconds to to load this data into into my Vector",
    "start": "1554720",
    "end": "1561679"
  },
  {
    "text": "database uh and uh basically I want to parallelize this process right so as as",
    "start": "1561679",
    "end": "1567740"
  },
  {
    "text": "uh my the amount of data that I want the model to look up",
    "start": "1567740",
    "end": "1572960"
  },
  {
    "text": "um you you know as that starts to grow you want to be able to gracefully paralyze this process and that's where",
    "start": "1572960",
    "end": "1578600"
  },
  {
    "text": "Ray comes into play and like I mentioned uh you don't have to you know with like",
    "start": "1578600",
    "end": "1583700"
  },
  {
    "text": "I come from the world of Big Data like spark and Hadoop the big challenge of you know Big Data was that it you know",
    "start": "1583700",
    "end": "1590659"
  },
  {
    "text": "there was this property of over subscription of cluster resources so with Ray Ray essentially solves that",
    "start": "1590659",
    "end": "1595760"
  },
  {
    "text": "problem because you can granularly describe exactly how much resources your individual processes should be using so",
    "start": "1595760",
    "end": "1602179"
  },
  {
    "text": "in this case for example since I just have this one repository I don't need to",
    "start": "1602179",
    "end": "1607220"
  },
  {
    "text": "use one entire GPU I can only say I just use one tenth of the GPU and that way if I were to use let's say",
    "start": "1607220",
    "end": "1614600"
  },
  {
    "text": "an entire GPU I could paralyze uh you know 10 different processes on a single",
    "start": "1614600",
    "end": "1619820"
  },
  {
    "text": "GPU as well and that's exactly what I'm doing here this is a function that I create similarly to what I walked over",
    "start": "1619820",
    "end": "1625520"
  },
  {
    "text": "through earlier I initialized the pine cone client um and you know this is where I actually",
    "start": "1625520",
    "end": "1632000"
  },
  {
    "text": "drop the embeddings into Pinecone inside this function and I mean it's a typical python",
    "start": "1632000",
    "end": "1638900"
  },
  {
    "text": "function there's no change in the function the only thing that I'm doing is adding this array remote decorator and then finally I uh you know I say how",
    "start": "1638900",
    "end": "1647120"
  },
  {
    "text": "many shards I want so in this case I want to paralyze let's say I had uh you",
    "start": "1647120",
    "end": "1652220"
  },
  {
    "text": "know eight different repositories right I had Ray sagemaker Bedrock all kinds of stuff and I wanted to paralyze that",
    "start": "1652220",
    "end": "1657860"
  },
  {
    "text": "process instantaneously uh I can you know save the number of shards and then",
    "start": "1657860",
    "end": "1663880"
  },
  {
    "text": "uh you know I have this function that splits the embeddings into these shards",
    "start": "1663880",
    "end": "1673760"
  },
  {
    "text": "and then when I call this function process Shard and I just instead of like",
    "start": "1673760",
    "end": "1678980"
  },
  {
    "text": "a typical native pythonian call where I would just say process Shard shard's eye and embeddings I just say process short",
    "start": "1678980",
    "end": "1686299"
  },
  {
    "text": "dot remote and pass those arguments and that's it",
    "start": "1686299",
    "end": "1691580"
  },
  {
    "text": "and that's that's when it's going to start running through the ray cluster I'm not going to run through this in the interest of time basically we have about",
    "start": "1691580",
    "end": "1697760"
  },
  {
    "text": "a minute and a half left but you will start to see these uh you know these uh",
    "start": "1697760",
    "end": "1703580"
  },
  {
    "text": "different threads hydrate when I would run through that function and I'm able to parallelize this and I'm",
    "start": "1703580",
    "end": "1709580"
  },
  {
    "text": "actually able to view the amount of resources that are being consumed by the cluster as well",
    "start": "1709580",
    "end": "1714860"
  },
  {
    "text": "uh once this is done ah you know the next step is to actually",
    "start": "1714860",
    "end": "1720500"
  },
  {
    "text": "um query the results so um you know let's say",
    "start": "1720500",
    "end": "1728500"
  },
  {
    "text": "um so there's some issue here but uh yeah I want to point to in this case I want to",
    "start": "1729140",
    "end": "1735799"
  },
  {
    "text": "uh you know I'm using race serve so the model is actually loaded to the",
    "start": "1735799",
    "end": "1741260"
  },
  {
    "text": "um the ray cluster and and racer you know with what Ray does it paralyzes",
    "start": "1741260",
    "end": "1747140"
  },
  {
    "text": "your requests um so I can I can ask these queries now specific to Ray from from the model",
    "start": "1747140",
    "end": "1754700"
  },
  {
    "text": "which and this is data that wasn't actually trained on so I talked about uh",
    "start": "1754700",
    "end": "1760580"
  },
  {
    "text": "you know how the latency went down when I quantized",
    "start": "1760580",
    "end": "1765740"
  },
  {
    "text": "the model I showed you that graphic it'll go further down if I if this this query was cached but here you can see",
    "start": "1765740",
    "end": "1772520"
  },
  {
    "text": "like I'm asking a more complex question rather than how does Ray paralyze computations across instances I'm asking",
    "start": "1772520",
    "end": "1777980"
  },
  {
    "text": "uh what is an act reporting strategy in rate gives a response in 13 seconds then",
    "start": "1777980",
    "end": "1783559"
  },
  {
    "text": "I ask then I pass a prompt explain in detail the actor building strategy in Ray and that's where it gives me a more",
    "start": "1783559",
    "end": "1790039"
  },
  {
    "text": "detailed breakdown response and then I say act like a programmer and",
    "start": "1790039",
    "end": "1795860"
  },
  {
    "text": "break down various steps right so I have a function X that calculates a square of the input and can you parallelize that",
    "start": "1795860",
    "end": "1802460"
  },
  {
    "text": "function using gray and give me the code right and that's that's exactly what it does it breaks down the code I could",
    "start": "1802460",
    "end": "1808580"
  },
  {
    "text": "even say hey provide me that code in JavaScript or in Python where he has apis for both and it would it would give",
    "start": "1808580",
    "end": "1815299"
  },
  {
    "text": "me the right results and I mentioned the token size you can see the tokens the max token size here",
    "start": "1815299",
    "end": "1822260"
  },
  {
    "text": "is lower and you're getting a response and let's say 13 13 seconds this is where it's actually going through the",
    "start": "1822260",
    "end": "1828500"
  },
  {
    "text": "entire Max token size of 512 tokens and uh you know that's where",
    "start": "1828500",
    "end": "1834260"
  },
  {
    "text": "you know you see you actually start seeing at scale slightly better so okay uh we're up on time but uh yeah uh",
    "start": "1834260",
    "end": "1842720"
  },
  {
    "text": "thanks so much and uh if you have questions just uh grab me on the way out",
    "start": "1842720",
    "end": "1848919"
  }
]