[
  {
    "text": "hi everyone today we're going to be talking about how we modernized uh doordash model serving platform with",
    "start": "3120",
    "end": "10019"
  },
  {
    "text": "racer so for introductions my name is Cornell I'm a Staff engineer at doordash I'm a machine learning platform team",
    "start": "10019",
    "end": "17160"
  },
  {
    "text": "previously I worked on work that quora between 2012 and 2020 on also on the",
    "start": "17160",
    "end": "24060"
  },
  {
    "text": "machine learning platform and I'd like to also introduce Sid who's going to be presenting along with me I'll say a few",
    "start": "24060",
    "end": "31859"
  },
  {
    "text": "words yeah Sid worked at Amazon between 2016 and 2021 and he'll be presenting second",
    "start": "31859",
    "end": "40320"
  },
  {
    "text": "half of this so today the agenda is going to be about one one key thing is how we",
    "start": "40320",
    "end": "47520"
  },
  {
    "text": "launched open source llms within a day at doordash so the talk is going to be",
    "start": "47520",
    "end": "54620"
  },
  {
    "text": "structured in three different sections the first section is going to be about",
    "start": "54620",
    "end": "60360"
  },
  {
    "text": "how we use machine learning at doordash and talk about what our scale is what an",
    "start": "60360",
    "end": "65820"
  },
  {
    "text": "order life cycle looks like I'm going to introduce our online machine learning",
    "start": "65820",
    "end": "71119"
  },
  {
    "text": "platform our first generation we call it civil and next I'm going to talk about",
    "start": "71119",
    "end": "76560"
  },
  {
    "text": "flexible predictions why we needed flexibility why we chose",
    "start": "76560",
    "end": "82259"
  },
  {
    "text": "racer and our journey with racer and finally Sid is going to talk about the outcomes",
    "start": "82259",
    "end": "89939"
  },
  {
    "text": "so what did we achieve by using great serve what were some of our use cases",
    "start": "89939",
    "end": "95100"
  },
  {
    "text": "and what are some of the future work so let's Jump Right In so at doordash our mission is to grow",
    "start": "95100",
    "end": "101400"
  },
  {
    "text": "and Empower a local economies there's many different ways to do that but today",
    "start": "101400",
    "end": "106799"
  },
  {
    "text": "we're going to focus about on how we use machine learning to empower the product",
    "start": "106799",
    "end": "113579"
  },
  {
    "text": "itself so as you may be aware doordash consists of",
    "start": "113579",
    "end": "118799"
  },
  {
    "text": "many different verticals the one you might be most familiar with is the food delivery and pickup aspect so customers",
    "start": "118799",
    "end": "127259"
  },
  {
    "text": "can order lunch dinner breakfast or the other",
    "start": "127259",
    "end": "132480"
  },
  {
    "text": "things we can order also order is a convenience and grocery so if you want to use a want to order household",
    "start": "132480",
    "end": "138599"
  },
  {
    "text": "supplies we also provide that and then finally another aspect of doordash is",
    "start": "138599",
    "end": "145200"
  },
  {
    "text": "the subscription aspect so we have something called dash pass which is a great Subscription Service provides lots",
    "start": "145200",
    "end": "152400"
  },
  {
    "text": "of savings and other benefits so doordash is a tree sorry three-sided",
    "start": "152400",
    "end": "159239"
  },
  {
    "text": "Marketplace um you can think of it as uh there are consumers people ordering food there's",
    "start": "159239",
    "end": "166200"
  },
  {
    "text": "Merchants who provide those services and then Dashers who do deliveries so it",
    "start": "166200",
    "end": "173340"
  },
  {
    "text": "also has a flywheel effect where the more consumers we have the more sales we",
    "start": "173340",
    "end": "180540"
  },
  {
    "text": "can generate to those Merchants the more Merchants we have the more the Dashers",
    "start": "180540",
    "end": "185940"
  },
  {
    "text": "have an economic opportunity to make a living by doing deliveries as well as",
    "start": "185940",
    "end": "192239"
  },
  {
    "text": "the more Dashers we have delivering items",
    "start": "192239",
    "end": "197280"
  },
  {
    "text": "the more the better experience we can provide for our consumers including faster deliveries",
    "start": "197280",
    "end": "203879"
  },
  {
    "text": "so this just keeps repeating and getting better over time so in terms of doordash",
    "start": "203879",
    "end": "211940"
  },
  {
    "text": "numbers we have over 32 million consumers per month we have over 15",
    "start": "211940",
    "end": "220739"
  },
  {
    "text": "million stress members 2 million dashes per month and we're also category",
    "start": "220739",
    "end": "227040"
  },
  {
    "text": "leaders in food convenience and pickup but for us in terms of the machine learning side of things we have more",
    "start": "227040",
    "end": "234900"
  },
  {
    "text": "than 6 million predictions per second for our production systems",
    "start": "234900",
    "end": "240420"
  },
  {
    "text": "so next I'm going to talk about how we use machine learning at doordash so",
    "start": "240420",
    "end": "246959"
  },
  {
    "text": "I'm going to do this through the life cycle of a restaurant order so the first step is",
    "start": "246959",
    "end": "253799"
  },
  {
    "text": "going to be a customer creates an order and they pick their the food they want",
    "start": "253799",
    "end": "258840"
  },
  {
    "text": "the restaurant they want and then they do a checkout one important thing I personally like to",
    "start": "258840",
    "end": "265080"
  },
  {
    "text": "look at the checkout is how long will it take to get the delivery so that's a very important aspect",
    "start": "265080",
    "end": "272280"
  },
  {
    "text": "um next I'm going to talk about what happens behind the scenes when an order is placed",
    "start": "272280",
    "end": "277860"
  },
  {
    "text": "dispatch to the Dashers and the restaurants and then last maybe the most important step is how the order actually",
    "start": "277860",
    "end": "284400"
  },
  {
    "text": "gets delivered so first up is if you're looking to order some dinner",
    "start": "284400",
    "end": "291120"
  },
  {
    "text": "maybe you're looking at various restaurants on the door that feed so we",
    "start": "291120",
    "end": "297360"
  },
  {
    "text": "show you recommendations these are stores based on your past purchasing history or preferences this is powered",
    "start": "297360",
    "end": "304620"
  },
  {
    "text": "by Machine learning all of our recommendation systems are powered by our machine learning platform",
    "start": "304620",
    "end": "311220"
  },
  {
    "text": "uh but in some cases you may not find what you like in the home page so you might want to search for a specific type",
    "start": "311220",
    "end": "318419"
  },
  {
    "text": "of food or restaurant and there's a search feature for that we also power",
    "start": "318419",
    "end": "323820"
  },
  {
    "text": "that with machine learning so next maybe you place some items in",
    "start": "323820",
    "end": "329039"
  },
  {
    "text": "your checkout card you would like to see how",
    "start": "329039",
    "end": "334500"
  },
  {
    "text": "long the delivery will take so you might say in this case it's between 33 and 43",
    "start": "334500",
    "end": "339720"
  },
  {
    "text": "minutes that's powered by a ETA model it's a very important machine learning model we have a doordash",
    "start": "339720",
    "end": "345919"
  },
  {
    "text": "also we check whether just verify that the payment information",
    "start": "345919",
    "end": "351539"
  },
  {
    "text": "is valid for for this checkout and that's also has some ml aspects",
    "start": "351539",
    "end": "358320"
  },
  {
    "text": "so next is dispatching the order so once we know what the order is going to",
    "start": "358320",
    "end": "364560"
  },
  {
    "text": "look like we send that information to the merchant and they start preparing",
    "start": "364560",
    "end": "369600"
  },
  {
    "text": "the food and we also want to schedule a Dasher to pick up that food so we also",
    "start": "369600",
    "end": "376440"
  },
  {
    "text": "use machine learning models to predict how long it will take to prepare the",
    "start": "376440",
    "end": "381900"
  },
  {
    "text": "food as well as you know for the Dasher to arrive at that restaurant",
    "start": "381900",
    "end": "387979"
  },
  {
    "text": "based on like you know weather conditions supply and demand and other things interestingly there are some",
    "start": "387979",
    "end": "393600"
  },
  {
    "text": "restaurants where we actually dispatch the dashes first because they just make the food so fast",
    "start": "393600",
    "end": "399419"
  },
  {
    "text": "um so next is uh once the Dasher picked up the order we need to the final step is",
    "start": "399419",
    "end": "407639"
  },
  {
    "text": "to deliver the food at the customer's doorstep so the introduce Doorstep Delivery and with that the Kima",
    "start": "407639",
    "end": "414900"
  },
  {
    "text": "challenge where people were not able to always find their food um and so we introduced this where the",
    "start": "414900",
    "end": "421800"
  },
  {
    "text": "Dasher will take a photo and upload it to the customer and it we also got that",
    "start": "421800",
    "end": "426960"
  },
  {
    "text": "photo so one important thing we did is we launched a deep learning model that",
    "start": "426960",
    "end": "432479"
  },
  {
    "text": "detects whether this doorstep image that you can see barely on the right hand side whether",
    "start": "432479",
    "end": "439620"
  },
  {
    "text": "whether it is follow that the Dasher took is is a valid uh drop off fee estimated by looking and seeing if it",
    "start": "439620",
    "end": "446520"
  },
  {
    "text": "looks like a door is there food and so on so we use a deep learning image recognition models for this",
    "start": "446520",
    "end": "453539"
  },
  {
    "text": "so next I'm going to introduce how we do online ml predictions and doordash our",
    "start": "453539",
    "end": "460380"
  },
  {
    "text": "first generation machine learning platform is based something we call C bill",
    "start": "460380",
    "end": "465479"
  },
  {
    "text": "so Sybil is a kotlin microservice we deployed it in kubernetes it's um",
    "start": "465479",
    "end": "471900"
  },
  {
    "text": "highly available it's optimized for high throughput low latency use cases home",
    "start": "471900",
    "end": "477300"
  },
  {
    "text": "page feed search those are some of the ones and it takes a grpc request",
    "start": "477300",
    "end": "482720"
  },
  {
    "text": "it has the models cached from the model store so those are going to be live GBM",
    "start": "482720",
    "end": "488340"
  },
  {
    "text": "and pytorch models in RQ so I'll talk about this in more detail it fetches the",
    "start": "488340",
    "end": "493620"
  },
  {
    "text": "features required for making a prediction we typically have like a feature store with redis but there's",
    "start": "493620",
    "end": "499440"
  },
  {
    "text": "other versions as well and then finally like evaluates that model and locks the",
    "start": "499440",
    "end": "504840"
  },
  {
    "text": "resulting predictions in a prediction store we use snowflake for that and also",
    "start": "504840",
    "end": "510360"
  },
  {
    "text": "Returns the response to a client so diving a little bit deeper into civil",
    "start": "510360",
    "end": "516380"
  },
  {
    "text": "the model types that we support here are primarily restricted to light GBM by",
    "start": "516380",
    "end": "521880"
  },
  {
    "text": "torch and composite so composite is just a way to you know create a a graph of live GB and",
    "start": "521880",
    "end": "529800"
  },
  {
    "text": "pytors and some other basic mathematical computations so for example we're able to support",
    "start": "529800",
    "end": "537060"
  },
  {
    "text": "numerical category core lists and embedding features and the requests will also contain some other information",
    "start": "537060",
    "end": "543120"
  },
  {
    "text": "whether it's an experiment model may need to do Shadow or anything about logging and it will output the",
    "start": "543120",
    "end": "549839"
  },
  {
    "text": "predictive values those predicted values are typically numerical Boolean or list",
    "start": "549839",
    "end": "555180"
  },
  {
    "text": "so as you may notice this list covers a lot of what traditional machine learning",
    "start": "555180",
    "end": "562399"
  },
  {
    "text": "needed over the last many years and we found this to be really great Foundation",
    "start": "562399",
    "end": "568860"
  },
  {
    "text": "it works really well for you know high throughput low latency requirements",
    "start": "568860",
    "end": "574800"
  },
  {
    "text": "and however over time we need to expand",
    "start": "574800",
    "end": "581040"
  },
  {
    "text": "because our customer came to us and said we need to use different libraries for",
    "start": "581040",
    "end": "586320"
  },
  {
    "text": "example we need to use a hugging face or tensorflow or Fast Tax and we need to",
    "start": "586320",
    "end": "591420"
  },
  {
    "text": "also use different mediums so we need to use maybe images or we need to do",
    "start": "591420",
    "end": "596459"
  },
  {
    "text": "predictions on text generate text classify text and so on so we need to go",
    "start": "596459",
    "end": "601620"
  },
  {
    "text": "back to our drawing board and come up with a a different approach for this and that's where ourgill was started so",
    "start": "601620",
    "end": "609899"
  },
  {
    "text": "Argyle tries to solve all of these problems so",
    "start": "609899",
    "end": "615000"
  },
  {
    "text": "um we also noticed that onboarding was a little bit time consuming and impacted",
    "start": "615000",
    "end": "620820"
  },
  {
    "text": "our velocity so it took a lot of time for people to onboard new types of mediums so like let's say if you want to",
    "start": "620820",
    "end": "628019"
  },
  {
    "text": "do image prediction it required a lot of changing around for",
    "start": "628019",
    "end": "633480"
  },
  {
    "text": "the existing architecture and also we found that the platform wasn't very open",
    "start": "633480",
    "end": "639019"
  },
  {
    "text": "because it was a kotlin microservice so our machine learning Engineers data",
    "start": "639019",
    "end": "644519"
  },
  {
    "text": "scientists had trouble contributing to this foundation and it was difficult to",
    "start": "644519",
    "end": "650100"
  },
  {
    "text": "partner with these teams and make their platform more open so with these requirements we had three",
    "start": "650100",
    "end": "659459"
  },
  {
    "text": "main goals for arguele which is we wanted to make it a generalized production platform to be able to host",
    "start": "659459",
    "end": "665880"
  },
  {
    "text": "any type of model we also wanted to provide flexibility so that we can also do predictions on",
    "start": "665880",
    "end": "671279"
  },
  {
    "text": "image text and so on voice as well not just you know numerical list and Boolean",
    "start": "671279",
    "end": "677880"
  },
  {
    "text": "features we also wanted to open it up so that machine learning Engineers can",
    "start": "677880",
    "end": "682920"
  },
  {
    "text": "customize the model prediction environment so if they want to use a particular Library they're free to do",
    "start": "682920",
    "end": "689220"
  },
  {
    "text": "that so next city is going to be talking about argue in depth",
    "start": "689220",
    "end": "696500"
  },
  {
    "text": "yeah okay can you can anybody hear me yeah okay cool yeah thank you Cornell for providing more insight into like you",
    "start": "699000",
    "end": "705660"
  },
  {
    "text": "know how ml predictions worked in uh dodash uh what was civil and how we reached a point where we had to look",
    "start": "705660",
    "end": "712140"
  },
  {
    "text": "Beyond uh what we are doing with Sybil so we come to the juncture where we want to make a little more flexible for our",
    "start": "712140",
    "end": "718800"
  },
  {
    "text": "users any users here are all the data scientists and machine learning engineers in doordash so I'll provide some more depth into our journey how it",
    "start": "718800",
    "end": "726000"
  },
  {
    "text": "started and how we built our Guild uh so yeah the first thing when we",
    "start": "726000",
    "end": "731579"
  },
  {
    "text": "started looking uh like you know we had defined our ethos we want to build something which is flexible self-serve can support any models and if you go out",
    "start": "731579",
    "end": "739019"
  },
  {
    "text": "and look for choices there are way too many options uh like you know we looked at Bento ml Cube flow serving which is K",
    "start": "739019",
    "end": "746459"
  },
  {
    "text": "serving now uh Triton uh and many others including racer so again went back to",
    "start": "746459",
    "end": "753240"
  },
  {
    "text": "our customers potential customers who we can pilot with we ask them like you know what they would like to see from a generalized prediction serving platform",
    "start": "753240",
    "end": "759899"
  },
  {
    "text": "collected some requirements we added some more requirement from the engineering standpoint and we came up with evaluation framework which had top",
    "start": "759899",
    "end": "767399"
  },
  {
    "text": "five criterias where like you know I'm going to look at functionality uh when",
    "start": "767399",
    "end": "772500"
  },
  {
    "text": "when I say functionality is like a lot of users want to do like you know things like they want to do image CV use",
    "start": "772500",
    "end": "779160"
  },
  {
    "text": "cases NLP use cases somebody was like you know we want GPU serving and somebody's like you know I want to host",
    "start": "779160",
    "end": "785700"
  },
  {
    "text": "two models which are totally different architecture Frameworks uh did I say NLP",
    "start": "785700",
    "end": "791760"
  },
  {
    "text": "yikes so and then the next thing was",
    "start": "791760",
    "end": "797100"
  },
  {
    "text": "flexibility uh when it comes to flexibility they want to like you know",
    "start": "797100",
    "end": "802200"
  },
  {
    "text": "do pre-processing before actually making a prediction and after prediction they want to make post processing on the",
    "start": "802200",
    "end": "808139"
  },
  {
    "text": "predictions they also want to like you know have some certain libraries that they want which would be very hard to",
    "start": "808139",
    "end": "813600"
  },
  {
    "text": "onboard to if we were if we do not have something uh like you know a generalized prediction serving platform",
    "start": "813600",
    "end": "819360"
  },
  {
    "text": "from the engineering point of view we wanted to look at how easy it is to integrate with existing dodash infra we",
    "start": "819360",
    "end": "826560"
  },
  {
    "text": "have certain uh like you know the foundational uh engineering uh pillars",
    "start": "826560",
    "end": "833579"
  },
  {
    "text": "like traffic compute I want to make sure that whatever we adopt uh like you know integration is possible and it's it's",
    "start": "833579",
    "end": "839820"
  },
  {
    "text": "easy to integrate with them uh we were looking for performances as well we compared we were looking at",
    "start": "839820",
    "end": "845639"
  },
  {
    "text": "benchmarks as benchmarks of already existing Frameworks and uh like you know took took one use case and saw like you",
    "start": "845639",
    "end": "853560"
  },
  {
    "text": "know what we see uh across different Frameworks and at the end Community Support matters a lot uh whenever you",
    "start": "853560",
    "end": "858959"
  },
  {
    "text": "are onboarding something new the documentation matters like you know the support on the slack Channel matters uh",
    "start": "858959",
    "end": "864540"
  },
  {
    "text": "how quickly we get the response from our from from our questions all of these things matters and looking at everything uh don't want",
    "start": "864540",
    "end": "871620"
  },
  {
    "text": "to say like one framework is better than the other but we realized this what fits the bill is the most important thing and",
    "start": "871620",
    "end": "877800"
  },
  {
    "text": "for us it was a race of",
    "start": "877800",
    "end": "881360"
  },
  {
    "text": "uh yeah next I'll talk about why we chose racer and uh",
    "start": "883680",
    "end": "888959"
  },
  {
    "text": "the first thing is flexibility it's incredibly flexible uh we can you can",
    "start": "888959",
    "end": "894300"
  },
  {
    "text": "onboard and install libraries of your choices with ease uh keep in mind that we are building this platform where a",
    "start": "894300",
    "end": "901380"
  },
  {
    "text": "lot of like you know contributions are going to come from folks who are not like you know focusing on day-to-day",
    "start": "901380",
    "end": "907560"
  },
  {
    "text": "software engineering they are data scientists they are focusing on writing ml models uh raceav allows us to be",
    "start": "907560",
    "end": "913920"
  },
  {
    "text": "really flexible with what model architecture we choose and also something that I found was not",
    "start": "913920",
    "end": "920820"
  },
  {
    "text": "everywhere but you for a single use case or an application that you come up with you can have multiple models within the",
    "start": "920820",
    "end": "926760"
  },
  {
    "text": "same application within racer which was really important for some use cases where you know you're again for pre-processing you might be calling one",
    "start": "926760",
    "end": "933660"
  },
  {
    "text": "model and like for actual predictions you might be calling some other models so that that's that's key",
    "start": "933660",
    "end": "941000"
  },
  {
    "text": "uh self-serve again as as we mentioned we want to build a platform which is uh like you know Self Serve so we host a",
    "start": "942300",
    "end": "948779"
  },
  {
    "text": "party but you uh like you know bring your own boost so",
    "start": "948779",
    "end": "954000"
  },
  {
    "text": "allows you to write your subscripts in Python one of the very powerful feature I feel is you can test your scripts",
    "start": "954000",
    "end": "960959"
  },
  {
    "text": "locally on your laptop which provides a very easy playground for anybody who wants to test out something",
    "start": "960959",
    "end": "966360"
  },
  {
    "text": "and again active community support from Ray I think we have been chatting with them on slack channels they are really",
    "start": "966360",
    "end": "971880"
  },
  {
    "text": "quick in responses their documentation they also take feedback really well they are key updating their documents with",
    "start": "971880",
    "end": "977579"
  },
  {
    "text": "what we provide from in feedback",
    "start": "977579",
    "end": "981620"
  },
  {
    "text": "uh distributed compute capabilities of Ray I think one of the easiest Frameworks where we",
    "start": "984380",
    "end": "991019"
  },
  {
    "text": "can write few lines of code and make multi GPU work for your application",
    "start": "991019",
    "end": "996180"
  },
  {
    "text": "uh you we can have multiple as I said multiple models on the same application you can divide them across the different",
    "start": "996180",
    "end": "1003019"
  },
  {
    "text": "parts of kubernetes are you on on the same pod and still make the resource utilization work for you and also the",
    "start": "1003019",
    "end": "1009560"
  },
  {
    "text": "auto scaling uh resource capabilities of racer comes in handy",
    "start": "1009560",
    "end": "1015820"
  },
  {
    "text": "and one of the things that I'd say is is one of very key advantages for us like",
    "start": "1016459",
    "end": "1021740"
  },
  {
    "text": "around the same time our model training team was also looking into Ray for model training uh but if you adopt Ray for",
    "start": "1021740",
    "end": "1028880"
  },
  {
    "text": "training and serving it allows us a unique opportunity where we can provide an ecosystem to all our users to not",
    "start": "1028880",
    "end": "1035660"
  },
  {
    "text": "only just train your models but also serve them and it also helps with uh like you know education point of view",
    "start": "1035660",
    "end": "1041418"
  },
  {
    "text": "like once you are used to Ray and you are the one writing your subscripts along with writing models it makes",
    "start": "1041419",
    "end": "1047959"
  },
  {
    "text": "adoption easier",
    "start": "1047959",
    "end": "1050799"
  },
  {
    "text": "now I would like to go through one of the argil app example how it looks like",
    "start": "1053240",
    "end": "1058640"
  },
  {
    "text": "in dodash uh so this is we generally every application will have",
    "start": "1058640",
    "end": "1064760"
  },
  {
    "text": "two deployments and I said deployments is uh two classes that are serving different purposes so one is uh like you",
    "start": "1064760",
    "end": "1071240"
  },
  {
    "text": "know a grpc Ingress endpoint which starts the grpc server on the array cluster",
    "start": "1071240",
    "end": "1076760"
  },
  {
    "text": "and if you see it requires resources does not require a GPU and for the next it's the grbc server",
    "start": "1076760",
    "end": "1084740"
  },
  {
    "text": "has an entry point for request which eventually makes a model inference and that is a separate deployment this this",
    "start": "1084740",
    "end": "1090380"
  },
  {
    "text": "example is just like you know uh productionizing Falcon 7B model within dodash where like you know we",
    "start": "1090380",
    "end": "1097700"
  },
  {
    "text": "need a GPU so you can Define your resources as per your use case and and both of these will be deployed on the",
    "start": "1097700",
    "end": "1103580"
  },
  {
    "text": "same part which is very interesting the another thing that we want for",
    "start": "1103580",
    "end": "1109940"
  },
  {
    "text": "deployment is Argo CD config so along with the defining your serving",
    "start": "1109940",
    "end": "1115280"
  },
  {
    "text": "logic you need to define a config which tells us like you know what kind of resources you would need",
    "start": "1115280",
    "end": "1121340"
  },
  {
    "text": "for your head what kind of resources you will need for worker resources do you want to use GPU for your application what are the dependencies that you would",
    "start": "1121340",
    "end": "1128059"
  },
  {
    "text": "need and working directory is something where like you know we build and package the whole application and",
    "start": "1128059",
    "end": "1133400"
  },
  {
    "text": "store it this is generally an arguile deployment",
    "start": "1133400",
    "end": "1138860"
  },
  {
    "text": "workflow would look like so we need two things from our users we need like you know a server script and an Argo CD",
    "start": "1138860",
    "end": "1145100"
  },
  {
    "text": "config the sample that I just showed there is an Argyle repo where you can check in these two things our CI CD",
    "start": "1145100",
    "end": "1151100"
  },
  {
    "text": "workflow will kick in will build your app and store it in our config store the Argo CD config will is in sync with",
    "start": "1151100",
    "end": "1158600"
  },
  {
    "text": "Argo CD deployment that is set up within do Dash it will take if if there is an update or",
    "start": "1158600",
    "end": "1165500"
  },
  {
    "text": "a new creation it will create an application specific for that use case and through that we handle the",
    "start": "1165500",
    "end": "1171320"
  },
  {
    "text": "deployment of Ray cluster we create Ray cluster through that we'll talk more about it later in the talk and once array cluster is up the server",
    "start": "1171320",
    "end": "1177919"
  },
  {
    "text": "deployment will be deployed and it will load the model and the subscript",
    "start": "1177919",
    "end": "1183159"
  },
  {
    "text": "this is in general the architecture within Ado Dash it looks like so if you",
    "start": "1184880",
    "end": "1190160"
  },
  {
    "text": "see the green boxes are what we have built which is like infra and yellow",
    "start": "1190160",
    "end": "1196100"
  },
  {
    "text": "team Yellow Boxes are what we expect from our users so we expect like you know you provide us what your dependent",
    "start": "1196100",
    "end": "1201200"
  },
  {
    "text": "libraries are about your subscript is we'll package everything and we'll make",
    "start": "1201200",
    "end": "1206720"
  },
  {
    "text": "sure that like you know rest of the things are managed for you we built a argual client most of our dodash is a",
    "start": "1206720",
    "end": "1211940"
  },
  {
    "text": "kotlin first uh company and we all our mostly client services are in kotlin so we built our client for easy adoption",
    "start": "1211940",
    "end": "1218240"
  },
  {
    "text": "where like users although the search scripts everything is in Python but while integrating you have an Argyle",
    "start": "1218240",
    "end": "1223640"
  },
  {
    "text": "client which is a kotlin library that hits the grbc endpoint for your rbl service for",
    "start": "1223640",
    "end": "1229400"
  },
  {
    "text": "each use case this is how it will look like we we have a use case isolation with Argyle where like each use case has",
    "start": "1229400",
    "end": "1236179"
  },
  {
    "text": "its own service endpoint and that was done so that like you know not we don't run into uh resource",
    "start": "1236179",
    "end": "1243380"
  },
  {
    "text": "starvation problems if multiple use cases are running on the same gray cluster",
    "start": "1243380",
    "end": "1248740"
  },
  {
    "text": "uh now I would like to talk a bit about how we made that happen like how did like you know race have started",
    "start": "1250640",
    "end": "1256880"
  },
  {
    "text": "happening within dodash and what were the challenges we faced so the first step was to okay now like",
    "start": "1256880",
    "end": "1263000"
  },
  {
    "text": "you know I want to set up a ray cluster in kubernetes we have a internally in-house managed kubernetes clusters",
    "start": "1263000",
    "end": "1269720"
  },
  {
    "text": "that are run by our compute team now I want to go to them and tell them that hey I want to set up this array cluster like how do we start from there so a lot",
    "start": "1269720",
    "end": "1277160"
  },
  {
    "text": "of collaboration happened here with again modeled uh training team because we were building infra for both trading",
    "start": "1277160",
    "end": "1283520"
  },
  {
    "text": "and serving and compute team to Define like you know okay the bottom uh box is where we are",
    "start": "1283520",
    "end": "1291559"
  },
  {
    "text": "and now we need to like create array cluster on top so we",
    "start": "1291559",
    "end": "1296659"
  },
  {
    "text": "started in integrating with cubery that's where kubray comes in which allows us to like you know which acts as",
    "start": "1296659",
    "end": "1302840"
  },
  {
    "text": "a control plane between the ray cluster and the kubernetes layer within do Dash we ran into Cub multiple challenges but",
    "start": "1302840",
    "end": "1310460"
  },
  {
    "text": "the tough one that I would like to highlight is like GPU accessibility like you know a lot of driver Cuda driver",
    "start": "1310460",
    "end": "1316100"
  },
  {
    "text": "like you know incompatibilities so we work with closely with Nvidia team to figure out like you know what would work",
    "start": "1316100",
    "end": "1322340"
  },
  {
    "text": "uh like you know to have gpus within the kubernetes cluster and how Ray can",
    "start": "1322340",
    "end": "1327799"
  },
  {
    "text": "access it so shout out to like both our compute team",
    "start": "1327799",
    "end": "1333020"
  },
  {
    "text": "and the model development team the next step is like okay now regular is ready you have deployed it but how do I deploy",
    "start": "1333020",
    "end": "1339559"
  },
  {
    "text": "my how do I deploy my serve changes on top of it so for that I think uh",
    "start": "1339559",
    "end": "1346700"
  },
  {
    "text": "the workflow basically looks like this a developer or like you know whoever wants to build and serve application on top of",
    "start": "1346700",
    "end": "1353419"
  },
  {
    "text": "phrase of using Argyle they commit a change again so we were using Helm",
    "start": "1353419",
    "end": "1358940"
  },
  {
    "text": "charts that were provided for Ray team that works closely with kubray all the values that you need for your",
    "start": "1358940",
    "end": "1365059"
  },
  {
    "text": "application as we discussed in the rocd config they are passed to that",
    "start": "1365059",
    "end": "1370960"
  },
  {
    "text": "oh sorry the hand charge and that are that is deployed to the cluster using Argo CD",
    "start": "1370960",
    "end": "1379520"
  },
  {
    "text": "the last step is figuring out traffic like now you have deployed your changes but you need to make sure that the service is accessible within do Dash",
    "start": "1379520",
    "end": "1386960"
  },
  {
    "text": "Network so that like you know clients can integrate with it as well like you know we interacted with our traffic team",
    "start": "1386960",
    "end": "1392480"
  },
  {
    "text": "and we integrate uh like to decide like how do we integrate with them we realize there are actually gaps between uh like",
    "start": "1392480",
    "end": "1399620"
  },
  {
    "text": "you know what is possible with the current uh Cube Ray and uh like you know",
    "start": "1399620",
    "end": "1404720"
  },
  {
    "text": "and what what changes we need to make so that services are accessible so for that to enable like you know load",
    "start": "1404720",
    "end": "1412039"
  },
  {
    "text": "balance the traffic across pods within dodash we had to make couple of Open Source contribution to qbre to fill",
    "start": "1412039",
    "end": "1417799"
  },
  {
    "text": "those gaps the two changes that we had to make was like how can we make all worker pod",
    "start": "1417799",
    "end": "1422960"
  },
  {
    "text": "accessible behind a server endpoint and like how do we make head node accessible for let's say submitting any",
    "start": "1422960",
    "end": "1429260"
  },
  {
    "text": "jobs integration with observability was pretty straightforward because of the support of Prometheus that Dre provides",
    "start": "1429260",
    "end": "1437500"
  },
  {
    "text": "I would like to talk a bit about outcome after we have been like you know able to uh",
    "start": "1437539",
    "end": "1443840"
  },
  {
    "text": "achieve all those steps like you can now set up brake luster you can deploy your changes and like you know make your service accessible so one of the key",
    "start": "1443840",
    "end": "1451580"
  },
  {
    "text": "things that we were able to achieve is velocity now you before generalized prediction platform we were able if you",
    "start": "1451580",
    "end": "1457760"
  },
  {
    "text": "have to take an idea to production it was a matter of weeks but now it's a matter of couple of days",
    "start": "1457760",
    "end": "1463039"
  },
  {
    "text": "local laptop testing makes everything really fast adoption I think one thing that we focused a lot on is the focus on",
    "start": "1463039",
    "end": "1470299"
  },
  {
    "text": "flexibility in sales are paid off lot of users can bring their ideas and productionize them very easily",
    "start": "1470299",
    "end": "1475539"
  },
  {
    "text": "and one more thing that we did is if we feel there is some repetitive overheads for use case for onboarding we try to",
    "start": "1475539",
    "end": "1481580"
  },
  {
    "text": "include them in our libraries and we have article library that you can just onboard to and make your life easy",
    "start": "1481580",
    "end": "1487460"
  },
  {
    "text": "from the performance point of view I think enable GPU serving was like the big task and the power of gpus where he",
    "start": "1487460",
    "end": "1493940"
  },
  {
    "text": "was able to like you know make us see like 10 to 20 x gains in performance some of the use cases that",
    "start": "1493940",
    "end": "1500120"
  },
  {
    "text": "were running on CBS before use cases that we had targeted and we are targeting and like a lot of use a",
    "start": "1500120",
    "end": "1507980"
  },
  {
    "text": "lot of workflows that used to run and Offline that used to like you know check the quality of images or like you know",
    "start": "1507980",
    "end": "1513320"
  },
  {
    "text": "menu types or detect an item in a menu by most of the multimedia that is",
    "start": "1513320",
    "end": "1518480"
  },
  {
    "text": "submitted by our like you know users or Dashers those can be taken to online and",
    "start": "1518480",
    "end": "1523760"
  },
  {
    "text": "all those workflows that you should run once a day can be done real-time inference",
    "start": "1523760",
    "end": "1529179"
  },
  {
    "text": "we were not really focusing on llm but that's a very big uh like you know Boon",
    "start": "1533620",
    "end": "1539419"
  },
  {
    "text": "for us that we were focusing on reserve that makes llam inference really uh easy for us we can launch models within the",
    "start": "1539419",
    "end": "1546679"
  },
  {
    "text": "matter of day uh like you know we opened endpoints where you can access",
    "start": "1546679",
    "end": "1552080"
  },
  {
    "text": "open source llm models you can do model Exploration with like monitoring API reductions we're targeting use cases",
    "start": "1552080",
    "end": "1558080"
  },
  {
    "text": "like support Channel chatbots and voice a assistance for the future work we are focusing on",
    "start": "1558080",
    "end": "1564140"
  },
  {
    "text": "performance uh like how do we now we have reached at this point like how the",
    "start": "1564140",
    "end": "1569179"
  },
  {
    "text": "next step is like how can we make things run faster uh fault tolerance uh and unite something that we always",
    "start": "1569179",
    "end": "1577279"
  },
  {
    "text": "wanted to do is Unified training and serving platform where you know the end goal is like doing once you've trained",
    "start": "1577279",
    "end": "1582919"
  },
  {
    "text": "the model do one click and it is available behind a serving endpoint add on to self-serve capabilities we are",
    "start": "1582919",
    "end": "1588440"
  },
  {
    "text": "always looking for making life easier for our audience uh streaming for llm use case is",
    "start": "1588440",
    "end": "1593480"
  },
  {
    "text": "something that we are also looking at uh yeah before iPhone is the talk we are",
    "start": "1593480",
    "end": "1598940"
  },
  {
    "text": "hiring uh feel free to which I want to do Dash engineering and if you are interested in the work that we have been",
    "start": "1598940",
    "end": "1604520"
  },
  {
    "text": "doing in ml platform please reach out to me Cornell or any of us and thank you",
    "start": "1604520",
    "end": "1610740"
  },
  {
    "text": "[Applause]",
    "start": "1610740",
    "end": "1616100"
  },
  {
    "text": "about three minutes for any questions",
    "start": "1616100",
    "end": "1619840"
  },
  {
    "text": "thank you I have two questions okay um do you do anything to make the ray",
    "start": "1622460",
    "end": "1628279"
  },
  {
    "text": "clusters fault tolerance for race serving and then the second question",
    "start": "1628279",
    "end": "1633559"
  },
  {
    "text": "um is is the platform ideally mentioned is multi-tenant and how do you handle",
    "start": "1633559",
    "end": "1639140"
  },
  {
    "text": "authentication users if that's the case uh yeah I'll repeat the question the",
    "start": "1639140",
    "end": "1645140"
  },
  {
    "text": "first question is how do we make uh Ray cluster fault tolerant so one of the things some of the guidelines that",
    "start": "1645140",
    "end": "1651260"
  },
  {
    "text": "already is provided by Ray we had a good conversation with them so one of the thing is like keep head node as low on",
    "start": "1651260",
    "end": "1658220"
  },
  {
    "text": "utilization as possible like you know we do not actually serve any traffic on head node uh and that is something that",
    "start": "1658220",
    "end": "1664760"
  },
  {
    "text": "I would recommend everybody to do uh and apart from that or the second point was like",
    "start": "1664760",
    "end": "1671120"
  },
  {
    "text": "authentication around it so I think that is something that since we built internally uh we were able to integrate",
    "start": "1671120",
    "end": "1678080"
  },
  {
    "text": "with most of the system that odas has already built and we can take advantage of all those things by default so we",
    "start": "1678080",
    "end": "1683779"
  },
  {
    "text": "didn't really have to think of those things because we are we took open source array deployed on the Clusters that are in-house like you know most of",
    "start": "1683779",
    "end": "1690080"
  },
  {
    "text": "the integration with dodash in like you know foundational pillars as I said about those that was really important to",
    "start": "1690080",
    "end": "1696740"
  },
  {
    "text": "us so that we don't have to like you know really focus on that specifically",
    "start": "1696740",
    "end": "1702158"
  },
  {
    "text": "um I had the one question which was basically do you see yourself migrating",
    "start": "1703940",
    "end": "1709159"
  },
  {
    "text": "the workloads that are on Sybil onto our goal or however we pronounce it",
    "start": "1709159",
    "end": "1715520"
  },
  {
    "text": "um and then I guess like the second question is I guess around like you know",
    "start": "1715520",
    "end": "1720620"
  },
  {
    "text": "performance like you know for auto scaling or like cold starts like have",
    "start": "1720620",
    "end": "1726020"
  },
  {
    "text": "you seen you know kind of issues with that or you know anything of that sort I",
    "start": "1726020",
    "end": "1732740"
  },
  {
    "text": "want to do that yeah so I think eventually we'll migrate most use cases at least I think there's",
    "start": "1732740",
    "end": "1740659"
  },
  {
    "text": "still potentially room for uh civil since it has such a focus on optimization and performance",
    "start": "1740659",
    "end": "1747980"
  },
  {
    "text": "historically that we've invested a lot into that so we're racer shines is the flexibility",
    "start": "1747980",
    "end": "1756620"
  },
  {
    "text": "um and sorry what was the second question",
    "start": "1756620",
    "end": "1761500"
  },
  {
    "text": "oh startup yeah in general we've been pretty happy with",
    "start": "1763640",
    "end": "1770120"
  },
  {
    "text": "reliability and fault tolerance and we haven't seen any like cold start we do deployments using Argo CD in",
    "start": "1770120",
    "end": "1777919"
  },
  {
    "text": "kubernetes hey uh thanks for the talk I have a",
    "start": "1777919",
    "end": "1784100"
  },
  {
    "text": "couple of questions so first of all is there serve script and the dependencies built into the image the of the ray",
    "start": "1784100",
    "end": "1790640"
  },
  {
    "text": "cluster node and second how do you do upgrades like do you do rolling restarts or do you do a like red green",
    "start": "1790640",
    "end": "1796880"
  },
  {
    "text": "deployments if like the serving script is updated or the dependencies are updated",
    "start": "1796880",
    "end": "1803480"
  },
  {
    "text": "yeah so the surf script will have uh kind of the decorators on like what pip packages need to be installed that's a",
    "start": "1803480",
    "end": "1809899"
  },
  {
    "text": "racer uh standard thing so we we take advantage of that uh works really well for us",
    "start": "1809899",
    "end": "1815360"
  },
  {
    "text": "in terms of like upgrading the models uh we we typically like do an In-Place",
    "start": "1815360",
    "end": "1821059"
  },
  {
    "text": "upgrade for now but we'll have to think about better upgrade strategies in the future for like switching the models",
    "start": "1821059",
    "end": "1827179"
  },
  {
    "text": "versioning and everything uh that's all the time we have uh please",
    "start": "1827179",
    "end": "1832760"
  },
  {
    "text": "follow up offline if you have any more questions um thank you again thank you",
    "start": "1832760",
    "end": "1839919"
  }
]