[
  {
    "text": "hi",
    "start": "2800",
    "end": "3360"
  },
  {
    "text": "my name is simon moe i'm a software",
    "start": "3360",
    "end": "5200"
  },
  {
    "text": "engineer at uniskill",
    "start": "5200",
    "end": "6399"
  },
  {
    "text": "and today i'll be talking about research",
    "start": "6399",
    "end": "8559"
  },
  {
    "text": "which is a web framework",
    "start": "8559",
    "end": "9920"
  },
  {
    "text": "surveying machine learning application",
    "start": "9920",
    "end": "13200"
  },
  {
    "text": "let's start with what is model serving",
    "start": "13200",
    "end": "15120"
  },
  {
    "text": "or prediction survey",
    "start": "15120",
    "end": "16400"
  },
  {
    "text": "in particular where does it fit into the",
    "start": "16400",
    "end": "18640"
  },
  {
    "text": "machine learning lifecycle",
    "start": "18640",
    "end": "20720"
  },
  {
    "text": "to build and deploy machine learning",
    "start": "20720",
    "end": "22320"
  },
  {
    "text": "powered application we roughly have",
    "start": "22320",
    "end": "24800"
  },
  {
    "text": "these three major steps you develop the",
    "start": "24800",
    "end": "27840"
  },
  {
    "text": "model",
    "start": "27840",
    "end": "28560"
  },
  {
    "text": "train the model and perform inference on",
    "start": "28560",
    "end": "31039"
  },
  {
    "text": "the model",
    "start": "31039",
    "end": "32238"
  },
  {
    "text": "model development is where we collect",
    "start": "32239",
    "end": "34239"
  },
  {
    "text": "the data analyze the data",
    "start": "34239",
    "end": "35760"
  },
  {
    "text": "and interactively improve the modeling",
    "start": "35760",
    "end": "38719"
  },
  {
    "text": "once we have the model",
    "start": "38719",
    "end": "40160"
  },
  {
    "text": "is pushed into a training pipeline that",
    "start": "40160",
    "end": "42719"
  },
  {
    "text": "learns from live production data",
    "start": "42719",
    "end": "45120"
  },
  {
    "text": "and what happens then after we have",
    "start": "45120",
    "end": "46879"
  },
  {
    "text": "several trend and validated models",
    "start": "46879",
    "end": "49440"
  },
  {
    "text": "these models need to be put inside",
    "start": "49440",
    "end": "51600"
  },
  {
    "text": "prediction services",
    "start": "51600",
    "end": "53199"
  },
  {
    "text": "suitable for any user application",
    "start": "53199",
    "end": "57039"
  },
  {
    "text": "the last stage is what we care about",
    "start": "57600",
    "end": "59600"
  },
  {
    "text": "when we talk about",
    "start": "59600",
    "end": "60719"
  },
  {
    "text": "survey machine learning model and",
    "start": "60719",
    "end": "62239"
  },
  {
    "text": "deployed to production",
    "start": "62239",
    "end": "64799"
  },
  {
    "text": "this talk will be broken down in three",
    "start": "64799",
    "end": "66479"
  },
  {
    "text": "parts we'll discuss",
    "start": "66479",
    "end": "68080"
  },
  {
    "text": "the requirements of prediction serving",
    "start": "68080",
    "end": "70560"
  },
  {
    "text": "we'll have discussed two common",
    "start": "70560",
    "end": "72640"
  },
  {
    "text": "approaches",
    "start": "72640",
    "end": "73520"
  },
  {
    "text": "of how the industry is solving the model",
    "start": "73520",
    "end": "75680"
  },
  {
    "text": "serving problem today",
    "start": "75680",
    "end": "77040"
  },
  {
    "text": "and then we'll introduce research let's",
    "start": "77040",
    "end": "80159"
  },
  {
    "text": "start with the requirements of model",
    "start": "80159",
    "end": "81759"
  },
  {
    "text": "survey",
    "start": "81759",
    "end": "84159"
  },
  {
    "text": "we'll talk about six requirements they",
    "start": "84320",
    "end": "86880"
  },
  {
    "text": "are deliver predictions",
    "start": "86880",
    "end": "88720"
  },
  {
    "text": "being framework agnostic handle",
    "start": "88720",
    "end": "91040"
  },
  {
    "text": "arbitrary business logic",
    "start": "91040",
    "end": "92720"
  },
  {
    "text": "as well as scaling to match load bashing",
    "start": "92720",
    "end": "95759"
  },
  {
    "text": "to improve protection and knowing",
    "start": "95759",
    "end": "97600"
  },
  {
    "text": "uh to improve their throughput and",
    "start": "97600",
    "end": "99360"
  },
  {
    "text": "knowing the cost and being able to",
    "start": "99360",
    "end": "101360"
  },
  {
    "text": "handle highly concurrent usage",
    "start": "101360",
    "end": "103600"
  },
  {
    "text": "while delivering low latency",
    "start": "103600",
    "end": "106720"
  },
  {
    "text": "let's discuss them one by one well first",
    "start": "106720",
    "end": "110079"
  },
  {
    "text": "of all",
    "start": "110079",
    "end": "111439"
  },
  {
    "text": "the model server and hd delivery",
    "start": "111439",
    "end": "113040"
  },
  {
    "text": "protection given an",
    "start": "113040",
    "end": "114640"
  },
  {
    "text": "input right here i have a cat it should",
    "start": "114640",
    "end": "116719"
  },
  {
    "text": "run through the machine learning model",
    "start": "116719",
    "end": "118479"
  },
  {
    "text": "and outputs the prediction",
    "start": "118479",
    "end": "119920"
  },
  {
    "text": "as account but in many cases we have",
    "start": "119920",
    "end": "122640"
  },
  {
    "text": "more than just one",
    "start": "122640",
    "end": "123680"
  },
  {
    "text": "type of model model servers need to be",
    "start": "123680",
    "end": "126880"
  },
  {
    "text": "framework agnostic",
    "start": "126880",
    "end": "128319"
  },
  {
    "text": "it should be able to handle different",
    "start": "128319",
    "end": "130080"
  },
  {
    "text": "frameworks different algorithms",
    "start": "130080",
    "end": "131840"
  },
  {
    "text": "or different architecture this is",
    "start": "131840",
    "end": "133920"
  },
  {
    "text": "because different frameworks each have",
    "start": "133920",
    "end": "135680"
  },
  {
    "text": "their pros and cons and specialties",
    "start": "135680",
    "end": "138000"
  },
  {
    "text": "to solve a real world problem you",
    "start": "138000",
    "end": "140160"
  },
  {
    "text": "typically just grab the best to avoid",
    "start": "140160",
    "end": "142239"
  },
  {
    "text": "out there and be practical",
    "start": "142239",
    "end": "146000"
  },
  {
    "text": "there are also requirements for handling",
    "start": "146319",
    "end": "148480"
  },
  {
    "text": "pre-processing post-processing",
    "start": "148480",
    "end": "150160"
  },
  {
    "text": "and arbitrary business logic for the",
    "start": "150160",
    "end": "151760"
  },
  {
    "text": "models typically our model accepts",
    "start": "151760",
    "end": "154000"
  },
  {
    "text": "neuromic cancers and alphanumeric",
    "start": "154000",
    "end": "155599"
  },
  {
    "text": "tensors",
    "start": "155599",
    "end": "156480"
  },
  {
    "text": "but this is not what the end user wants",
    "start": "156480",
    "end": "159440"
  },
  {
    "text": "you can let the user",
    "start": "159440",
    "end": "160640"
  },
  {
    "text": "type in numeric array and be able to",
    "start": "160640",
    "end": "162400"
  },
  {
    "text": "parse the probability density scores",
    "start": "162400",
    "end": "165120"
  },
  {
    "text": "that's why we need to handle",
    "start": "165120",
    "end": "166400"
  },
  {
    "text": "pre-processing and post-processing",
    "start": "166400",
    "end": "168800"
  },
  {
    "text": "furthermore there are a lot of a lot",
    "start": "168800",
    "end": "171519"
  },
  {
    "text": "more than just model inference",
    "start": "171519",
    "end": "173599"
  },
  {
    "text": "when you are building a practical",
    "start": "173599",
    "end": "174959"
  },
  {
    "text": "machine learning application",
    "start": "174959",
    "end": "176800"
  },
  {
    "text": "you often need to send network requests",
    "start": "176800",
    "end": "179200"
  },
  {
    "text": "authorize",
    "start": "179200",
    "end": "180239"
  },
  {
    "text": "inputs or pin databases this is why",
    "start": "180239",
    "end": "183280"
  },
  {
    "text": "arbitrary business logic is also",
    "start": "183280",
    "end": "185120"
  },
  {
    "text": "critical",
    "start": "185120",
    "end": "187519"
  },
  {
    "text": "another key characteristic of machine",
    "start": "188239",
    "end": "190319"
  },
  {
    "text": "learning models",
    "start": "190319",
    "end": "191280"
  },
  {
    "text": "is that they are quite slow and resource",
    "start": "191280",
    "end": "193519"
  },
  {
    "text": "intensive",
    "start": "193519",
    "end": "194720"
  },
  {
    "text": "we don't want user to wait for the",
    "start": "194720",
    "end": "196159"
  },
  {
    "text": "request to pile up",
    "start": "196159",
    "end": "197760"
  },
  {
    "text": "model servers should be able to",
    "start": "197760",
    "end": "199040"
  },
  {
    "text": "transparently scale the model",
    "start": "199040",
    "end": "201040"
  },
  {
    "text": "to multiple replicas and no balance",
    "start": "201040",
    "end": "203840"
  },
  {
    "text": "among",
    "start": "203840",
    "end": "204239"
  },
  {
    "text": "them so requests can be parallelized",
    "start": "204239",
    "end": "207840"
  },
  {
    "text": "despite being resource intensive and",
    "start": "207840",
    "end": "209680"
  },
  {
    "text": "often often times slow",
    "start": "209680",
    "end": "211599"
  },
  {
    "text": "machine learning models can actually",
    "start": "211599",
    "end": "213360"
  },
  {
    "text": "utilize hardware parallelism like",
    "start": "213360",
    "end": "215599"
  },
  {
    "text": "vectorized instruction",
    "start": "215599",
    "end": "216959"
  },
  {
    "text": "or multi-threading to efficiently",
    "start": "216959",
    "end": "218879"
  },
  {
    "text": "perform inference on a batch of requests",
    "start": "218879",
    "end": "221680"
  },
  {
    "text": "many times performing inference on a",
    "start": "221680",
    "end": "223440"
  },
  {
    "text": "batch of say four images",
    "start": "223440",
    "end": "225440"
  },
  {
    "text": "only come with a very small cost",
    "start": "225440",
    "end": "227760"
  },
  {
    "text": "compared to just one",
    "start": "227760",
    "end": "228959"
  },
  {
    "text": "image batching is a simple technique to",
    "start": "228959",
    "end": "231680"
  },
  {
    "text": "improve utilization and throughput",
    "start": "231680",
    "end": "233840"
  },
  {
    "text": "of the models and model servers should",
    "start": "233840",
    "end": "236319"
  },
  {
    "text": "make it possible",
    "start": "236319",
    "end": "239040"
  },
  {
    "text": "and lastly machinery application today",
    "start": "239040",
    "end": "241439"
  },
  {
    "text": "requires real-time response while",
    "start": "241439",
    "end": "243200"
  },
  {
    "text": "dealing with",
    "start": "243200",
    "end": "243920"
  },
  {
    "text": "massive incoming traffic this means we",
    "start": "243920",
    "end": "246560"
  },
  {
    "text": "want our",
    "start": "246560",
    "end": "247280"
  },
  {
    "text": "models to handle a lot of requests and",
    "start": "247280",
    "end": "249760"
  },
  {
    "text": "be performing enough to deliver",
    "start": "249760",
    "end": "251200"
  },
  {
    "text": "millisecond level responses to each",
    "start": "251200",
    "end": "253200"
  },
  {
    "text": "individual users",
    "start": "253200",
    "end": "256159"
  },
  {
    "text": "putting this together will have a basic",
    "start": "256720",
    "end": "258560"
  },
  {
    "text": "set of requirements",
    "start": "258560",
    "end": "260000"
  },
  {
    "text": "now let's take a look at how people",
    "start": "260000",
    "end": "261600"
  },
  {
    "text": "deploy models in production today",
    "start": "261600",
    "end": "264240"
  },
  {
    "text": "we are going to look at two common",
    "start": "264240",
    "end": "266840"
  },
  {
    "text": "approaches",
    "start": "266840",
    "end": "268560"
  },
  {
    "text": "one's approach is to embed model",
    "start": "268560",
    "end": "270880"
  },
  {
    "text": "evaluation in a traditional web server",
    "start": "270880",
    "end": "273840"
  },
  {
    "text": "and the other is to offload this",
    "start": "273840",
    "end": "276400"
  },
  {
    "text": "production",
    "start": "276400",
    "end": "277120"
  },
  {
    "text": "to an external service",
    "start": "277120",
    "end": "280320"
  },
  {
    "text": "let's start with the first approach the",
    "start": "280320",
    "end": "282960"
  },
  {
    "text": "first approach typically",
    "start": "282960",
    "end": "284320"
  },
  {
    "text": "wraps the model inside a traditional web",
    "start": "284320",
    "end": "286400"
  },
  {
    "text": "server here i'll use",
    "start": "286400",
    "end": "288080"
  },
  {
    "text": "flask a popular python web framework as",
    "start": "288080",
    "end": "290560"
  },
  {
    "text": "an example",
    "start": "290560",
    "end": "291919"
  },
  {
    "text": "when http requests come in the file",
    "start": "291919",
    "end": "294400"
  },
  {
    "text": "server can dispatch the request to",
    "start": "294400",
    "end": "296000"
  },
  {
    "text": "different api endpoints we can make one",
    "start": "296000",
    "end": "298720"
  },
  {
    "text": "of the endpoint handler",
    "start": "298720",
    "end": "300080"
  },
  {
    "text": "a model evaluation logic",
    "start": "300080",
    "end": "303439"
  },
  {
    "text": "let's say we have reached slash api",
    "start": "304080",
    "end": "307759"
  },
  {
    "text": "image predicting point the input will be",
    "start": "307759",
    "end": "310320"
  },
  {
    "text": "transformed and",
    "start": "310320",
    "end": "311759"
  },
  {
    "text": "evaluated in a transfer model and then",
    "start": "311759",
    "end": "315120"
  },
  {
    "text": "we can have the output result captured",
    "start": "315120",
    "end": "317360"
  },
  {
    "text": "inside the return response",
    "start": "317360",
    "end": "319759"
  },
  {
    "text": "and pretty much that's it by embedding",
    "start": "319759",
    "end": "322960"
  },
  {
    "text": "your model inside a web server this",
    "start": "322960",
    "end": "325600"
  },
  {
    "text": "approach is definitely simple",
    "start": "325600",
    "end": "327759"
  },
  {
    "text": "and easy to build as well as easy to",
    "start": "327759",
    "end": "329759"
  },
  {
    "text": "understand",
    "start": "329759",
    "end": "330960"
  },
  {
    "text": "it gives the developer end-to-end",
    "start": "330960",
    "end": "332880"
  },
  {
    "text": "control over how the request is handled",
    "start": "332880",
    "end": "335039"
  },
  {
    "text": "and how the model is served if you're",
    "start": "335039",
    "end": "337440"
  },
  {
    "text": "just doing simple demo or proof of",
    "start": "337440",
    "end": "339199"
  },
  {
    "text": "concept",
    "start": "339199",
    "end": "340000"
  },
  {
    "text": "this approach is perfect because it gets",
    "start": "340000",
    "end": "342000"
  },
  {
    "text": "a job down",
    "start": "342000",
    "end": "343520"
  },
  {
    "text": "however a big downside of this approach",
    "start": "343520",
    "end": "345840"
  },
  {
    "text": "happens whenever you want to scale more",
    "start": "345840",
    "end": "347680"
  },
  {
    "text": "than just",
    "start": "347680",
    "end": "348160"
  },
  {
    "text": "one career at a time or have multiple",
    "start": "348160",
    "end": "350320"
  },
  {
    "text": "models to be served at the same time",
    "start": "350320",
    "end": "352960"
  },
  {
    "text": "concurrency and scalability are the big",
    "start": "352960",
    "end": "355600"
  },
  {
    "text": "drawback of this",
    "start": "355600",
    "end": "356720"
  },
  {
    "text": "approach why because we cannot use the",
    "start": "356720",
    "end": "359759"
  },
  {
    "text": "technique in web serving",
    "start": "359759",
    "end": "361280"
  },
  {
    "text": "to handle concurrency and scalability",
    "start": "361280",
    "end": "364479"
  },
  {
    "text": "after all web serving are not model",
    "start": "364479",
    "end": "367280"
  },
  {
    "text": "serving",
    "start": "367280",
    "end": "369680"
  },
  {
    "text": "on the surface web serving and model",
    "start": "370319",
    "end": "372080"
  },
  {
    "text": "serving looks very similar",
    "start": "372080",
    "end": "373520"
  },
  {
    "text": "here we have two code blocks on the left",
    "start": "373520",
    "end": "376319"
  },
  {
    "text": "we are showing a web serving",
    "start": "376319",
    "end": "377919"
  },
  {
    "text": "handler talking to databases on the",
    "start": "377919",
    "end": "380240"
  },
  {
    "text": "right we're showing a model serving",
    "start": "380240",
    "end": "381600"
  },
  {
    "text": "handler",
    "start": "381600",
    "end": "382720"
  },
  {
    "text": "the model is embedded inside the web",
    "start": "382720",
    "end": "384880"
  },
  {
    "text": "server that",
    "start": "384880",
    "end": "385919"
  },
  {
    "text": "and then we perform inference inside it",
    "start": "385919",
    "end": "389520"
  },
  {
    "text": "the two blocks looks very similar and",
    "start": "389520",
    "end": "392160"
  },
  {
    "text": "have",
    "start": "392160",
    "end": "392560"
  },
  {
    "text": "almost identical structures however what",
    "start": "392560",
    "end": "395600"
  },
  {
    "text": "happens under the hood",
    "start": "395600",
    "end": "397199"
  },
  {
    "text": "is different in the traditional web",
    "start": "397199",
    "end": "400639"
  },
  {
    "text": "server case",
    "start": "400639",
    "end": "401520"
  },
  {
    "text": "our server are typically network bounded",
    "start": "401520",
    "end": "404000"
  },
  {
    "text": "it can easily handle",
    "start": "404000",
    "end": "405280"
  },
  {
    "text": "thousands or tens of thousand requests",
    "start": "405280",
    "end": "407120"
  },
  {
    "text": "per second and typically utilize a pool",
    "start": "407120",
    "end": "409919"
  },
  {
    "text": "of network",
    "start": "409919",
    "end": "410720"
  },
  {
    "text": "uh connection to the database for",
    "start": "410720",
    "end": "412639"
  },
  {
    "text": "concurrency",
    "start": "412639",
    "end": "414240"
  },
  {
    "text": "model survey on the attitude is quite",
    "start": "414240",
    "end": "416240"
  },
  {
    "text": "different it is compute and memory",
    "start": "416240",
    "end": "418639"
  },
  {
    "text": "bonded",
    "start": "418639",
    "end": "419440"
  },
  {
    "text": "and the heavy machine learning model has",
    "start": "419440",
    "end": "421440"
  },
  {
    "text": "throughput of 10",
    "start": "421440",
    "end": "422639"
  },
  {
    "text": "to 100 queries per second and more",
    "start": "422639",
    "end": "425759"
  },
  {
    "text": "importantly",
    "start": "425759",
    "end": "426479"
  },
  {
    "text": "you can only run one computation at a",
    "start": "426479",
    "end": "428400"
  },
  {
    "text": "time by talking to a single global",
    "start": "428400",
    "end": "430560"
  },
  {
    "text": "variable",
    "start": "430560",
    "end": "431440"
  },
  {
    "text": "of the loaded model machine learning",
    "start": "431440",
    "end": "433759"
  },
  {
    "text": "frameworks are typically not stress safe",
    "start": "433759",
    "end": "437840"
  },
  {
    "text": "model serving is also different from web",
    "start": "439039",
    "end": "440800"
  },
  {
    "text": "serving when we are talking about",
    "start": "440800",
    "end": "442639"
  },
  {
    "text": "the from the scalability perspective on",
    "start": "442639",
    "end": "445520"
  },
  {
    "text": "the left hand side",
    "start": "445520",
    "end": "446639"
  },
  {
    "text": "we are showing a typical way to scale",
    "start": "446639",
    "end": "448560"
  },
  {
    "text": "web server because the code is",
    "start": "448560",
    "end": "451039"
  },
  {
    "text": "lightweight",
    "start": "451039",
    "end": "451680"
  },
  {
    "text": "you can often replicate the serving",
    "start": "451680",
    "end": "453520"
  },
  {
    "text": "process to many machine",
    "start": "453520",
    "end": "455120"
  },
  {
    "text": "and any query can go to any of the",
    "start": "455120",
    "end": "456960"
  },
  {
    "text": "server the processes are homogeneous",
    "start": "456960",
    "end": "460240"
  },
  {
    "text": "however you cannot do the same for",
    "start": "460240",
    "end": "461840"
  },
  {
    "text": "machine learning models",
    "start": "461840",
    "end": "463360"
  },
  {
    "text": "because it is impossible to pack",
    "start": "463360",
    "end": "466400"
  },
  {
    "text": "all of the many many machinery models in",
    "start": "466400",
    "end": "468800"
  },
  {
    "text": "the same process",
    "start": "468800",
    "end": "470240"
  },
  {
    "text": "models are big and resource intensive",
    "start": "470240",
    "end": "472879"
  },
  {
    "text": "therefore scaling machine learning model",
    "start": "472879",
    "end": "474879"
  },
  {
    "text": "needs to",
    "start": "474879",
    "end": "475440"
  },
  {
    "text": "have fine grained replication of",
    "start": "475440",
    "end": "477759"
  },
  {
    "text": "individual",
    "start": "477759",
    "end": "478879"
  },
  {
    "text": "model processes the serving processes",
    "start": "478879",
    "end": "481520"
  },
  {
    "text": "are therefore",
    "start": "481520",
    "end": "482479"
  },
  {
    "text": "heterogeneous that's on the right hand",
    "start": "482479",
    "end": "484800"
  },
  {
    "text": "side queries cannot go to",
    "start": "484800",
    "end": "486720"
  },
  {
    "text": "any of the work server because you don't",
    "start": "486720",
    "end": "488960"
  },
  {
    "text": "have all the models",
    "start": "488960",
    "end": "490639"
  },
  {
    "text": "in any of them",
    "start": "490639",
    "end": "493840"
  },
  {
    "text": "let's look at the requirement again the",
    "start": "494000",
    "end": "496240"
  },
  {
    "text": "web server approach does great in the",
    "start": "496240",
    "end": "498080"
  },
  {
    "text": "first three requirements",
    "start": "498080",
    "end": "499360"
  },
  {
    "text": "it delivers prediction it is framework",
    "start": "499360",
    "end": "501759"
  },
  {
    "text": "agnostic",
    "start": "501759",
    "end": "502560"
  },
  {
    "text": "and allows arbitrary business logic it",
    "start": "502560",
    "end": "505520"
  },
  {
    "text": "performs poorly in scaling",
    "start": "505520",
    "end": "507360"
  },
  {
    "text": "it doesn't have batching and cannot",
    "start": "507360",
    "end": "509360"
  },
  {
    "text": "handle highly concurrent workload",
    "start": "509360",
    "end": "511199"
  },
  {
    "text": "because after all",
    "start": "511199",
    "end": "512479"
  },
  {
    "text": "model serving is not web serving",
    "start": "512479",
    "end": "516640"
  },
  {
    "text": "it is with these limitations the second",
    "start": "516640",
    "end": "518719"
  },
  {
    "text": "approach is also",
    "start": "518719",
    "end": "519680"
  },
  {
    "text": "often preferred when handling more than",
    "start": "519680",
    "end": "522080"
  },
  {
    "text": "just one request",
    "start": "522080",
    "end": "524720"
  },
  {
    "text": "let's take a look at this offload",
    "start": "524720",
    "end": "527040"
  },
  {
    "text": "inference to an external service",
    "start": "527040",
    "end": "528399"
  },
  {
    "text": "approach",
    "start": "528399",
    "end": "529519"
  },
  {
    "text": "using our previous flash server example",
    "start": "529519",
    "end": "533120"
  },
  {
    "text": "for the slash api image predict endpoint",
    "start": "533120",
    "end": "536000"
  },
  {
    "text": "when you receive the request",
    "start": "536000",
    "end": "537680"
  },
  {
    "text": "instead of passes through tensorflow it",
    "start": "537680",
    "end": "540000"
  },
  {
    "text": "will now forward the request to some",
    "start": "540000",
    "end": "541519"
  },
  {
    "text": "sort of external service",
    "start": "541519",
    "end": "543040"
  },
  {
    "text": "here we're going to represent it with",
    "start": "543040",
    "end": "544880"
  },
  {
    "text": "the cloud",
    "start": "544880",
    "end": "546240"
  },
  {
    "text": "so example of these services includes",
    "start": "546240",
    "end": "548320"
  },
  {
    "text": "tensorflow serving from google",
    "start": "548320",
    "end": "550399"
  },
  {
    "text": "onyx runtime from facebook and microsoft",
    "start": "550399",
    "end": "553440"
  },
  {
    "text": "tends to rt from nvidia as well as",
    "start": "553440",
    "end": "556000"
  },
  {
    "text": "platform offering like adaba stage maker",
    "start": "556000",
    "end": "560160"
  },
  {
    "text": "these kind of specialized cistern",
    "start": "560160",
    "end": "561920"
  },
  {
    "text": "systems turn the model serving problem",
    "start": "561920",
    "end": "564000"
  },
  {
    "text": "into something similar to web serving",
    "start": "564000",
    "end": "566240"
  },
  {
    "text": "once you push your model to the",
    "start": "566240",
    "end": "568000"
  },
  {
    "text": "specialized system you don't need to",
    "start": "568000",
    "end": "569839"
  },
  {
    "text": "worry about being compute and memory",
    "start": "569839",
    "end": "571839"
  },
  {
    "text": "bonded",
    "start": "571839",
    "end": "572480"
  },
  {
    "text": "you're just issuing your network",
    "start": "572480",
    "end": "574000"
  },
  {
    "text": "requests to these systems",
    "start": "574000",
    "end": "576000"
  },
  {
    "text": "your concurrency is now bounded by the",
    "start": "576000",
    "end": "577680"
  },
  {
    "text": "external systems",
    "start": "577680",
    "end": "579200"
  },
  {
    "text": "but these can typically achieve at least",
    "start": "579200",
    "end": "581120"
  },
  {
    "text": "a thousand crews per second",
    "start": "581120",
    "end": "583120"
  },
  {
    "text": "and lastly instead of a single global",
    "start": "583120",
    "end": "585040"
  },
  {
    "text": "variable for the loaded model",
    "start": "585040",
    "end": "586560"
  },
  {
    "text": "we now have the same pool of network",
    "start": "586560",
    "end": "588839"
  },
  {
    "text": "connections",
    "start": "588839",
    "end": "590160"
  },
  {
    "text": "talking to the model server just like",
    "start": "590160",
    "end": "592000"
  },
  {
    "text": "how we have the pool talking to the",
    "start": "592000",
    "end": "593440"
  },
  {
    "text": "database",
    "start": "593440",
    "end": "595920"
  },
  {
    "text": "in summary the external service approach",
    "start": "596320",
    "end": "598959"
  },
  {
    "text": "encourages",
    "start": "598959",
    "end": "599839"
  },
  {
    "text": "separation of concern but you need to",
    "start": "599839",
    "end": "602399"
  },
  {
    "text": "manage the different",
    "start": "602399",
    "end": "603680"
  },
  {
    "text": "service separately you also get",
    "start": "603680",
    "end": "606079"
  },
  {
    "text": "concurrency and scalability",
    "start": "606079",
    "end": "607680"
  },
  {
    "text": "scalability out of the box that's great",
    "start": "607680",
    "end": "610640"
  },
  {
    "text": "however there's a lot of added",
    "start": "610640",
    "end": "612160"
  },
  {
    "text": "complexity",
    "start": "612160",
    "end": "614800"
  },
  {
    "text": "in our original web server example when",
    "start": "614800",
    "end": "617440"
  },
  {
    "text": "request comments for http",
    "start": "617440",
    "end": "619360"
  },
  {
    "text": "there are many steps before it reaches",
    "start": "619360",
    "end": "621200"
  },
  {
    "text": "the model for the core",
    "start": "621200",
    "end": "622399"
  },
  {
    "text": "interface and many steps after the",
    "start": "622399",
    "end": "624800"
  },
  {
    "text": "inference",
    "start": "624800",
    "end": "626480"
  },
  {
    "text": "when http requests come in it needs to",
    "start": "626480",
    "end": "629680"
  },
  {
    "text": "be checked and validated",
    "start": "629680",
    "end": "631360"
  },
  {
    "text": "and parts transformed with custom",
    "start": "631360",
    "end": "633760"
  },
  {
    "text": "business logic",
    "start": "633760",
    "end": "634560"
  },
  {
    "text": "specific to the workflow and then",
    "start": "634560",
    "end": "637360"
  },
  {
    "text": "transform",
    "start": "637360",
    "end": "638000"
  },
  {
    "text": "the input it will be run by the model",
    "start": "638000",
    "end": "641600"
  },
  {
    "text": "then we have the model influence stage",
    "start": "641600",
    "end": "643680"
  },
  {
    "text": "but after that the model output",
    "start": "643680",
    "end": "646000"
  },
  {
    "text": "still needs to be transformed and apply",
    "start": "646000",
    "end": "648160"
  },
  {
    "text": "custom business logic and",
    "start": "648160",
    "end": "649600"
  },
  {
    "text": "finally the result goes out as http",
    "start": "649600",
    "end": "652560"
  },
  {
    "text": "response",
    "start": "652560",
    "end": "655040"
  },
  {
    "text": "what happened when we have external",
    "start": "655040",
    "end": "656959"
  },
  {
    "text": "service",
    "start": "656959",
    "end": "658240"
  },
  {
    "text": "originally the entire stack step just",
    "start": "658240",
    "end": "660320"
  },
  {
    "text": "happens in the web server on the left",
    "start": "660320",
    "end": "663120"
  },
  {
    "text": "but when you bring in external service",
    "start": "663120",
    "end": "664959"
  },
  {
    "text": "and offload prediction",
    "start": "664959",
    "end": "666640"
  },
  {
    "text": "to it only the inference is shipped to",
    "start": "666640",
    "end": "669600"
  },
  {
    "text": "external services",
    "start": "669600",
    "end": "671200"
  },
  {
    "text": "the input and output span across the web",
    "start": "671200",
    "end": "674720"
  },
  {
    "text": "server",
    "start": "674720",
    "end": "675360"
  },
  {
    "text": "and external service",
    "start": "675360",
    "end": "678399"
  },
  {
    "text": "in essence i will characterize this",
    "start": "678399",
    "end": "680079"
  },
  {
    "text": "external service as tensioning tensor",
    "start": "680079",
    "end": "682079"
  },
  {
    "text": "out",
    "start": "682079",
    "end": "682560"
  },
  {
    "text": "their interface is pretty much limited",
    "start": "682560",
    "end": "685040"
  },
  {
    "text": "to just numeric tensor",
    "start": "685040",
    "end": "686399"
  },
  {
    "text": "or some varying of it what does this",
    "start": "686399",
    "end": "688959"
  },
  {
    "text": "mean",
    "start": "688959",
    "end": "689519"
  },
  {
    "text": "it means to run a model influencing your",
    "start": "689519",
    "end": "691600"
  },
  {
    "text": "ml application",
    "start": "691600",
    "end": "692800"
  },
  {
    "text": "you need to learn to deploy and manage",
    "start": "692800",
    "end": "694959"
  },
  {
    "text": "model in a separate system",
    "start": "694959",
    "end": "696880"
  },
  {
    "text": "figure out how to wrangle the implant",
    "start": "696880",
    "end": "698560"
  },
  {
    "text": "output correctly and send a networked",
    "start": "698560",
    "end": "700720"
  },
  {
    "text": "request to the right endpoint",
    "start": "700720",
    "end": "703120"
  },
  {
    "text": "more importantly transformation and",
    "start": "703120",
    "end": "704959"
  },
  {
    "text": "model performance are oftentimes tightly",
    "start": "704959",
    "end": "707279"
  },
  {
    "text": "copied",
    "start": "707279",
    "end": "708240"
  },
  {
    "text": "if you update your model but didn't",
    "start": "708240",
    "end": "710160"
  },
  {
    "text": "update say the feature normalization",
    "start": "710160",
    "end": "712240"
  },
  {
    "text": "step for input transformation",
    "start": "712240",
    "end": "714160"
  },
  {
    "text": "then the feature and model ways stays",
    "start": "714160",
    "end": "716560"
  },
  {
    "text": "out of sync",
    "start": "716560",
    "end": "717839"
  },
  {
    "text": "then the accuracy can definitely go down",
    "start": "717839",
    "end": "719920"
  },
  {
    "text": "drastically",
    "start": "719920",
    "end": "722160"
  },
  {
    "text": "this brings a lot of complexity and",
    "start": "722160",
    "end": "725360"
  },
  {
    "text": "lack flexibility it's way harder",
    "start": "725360",
    "end": "728560"
  },
  {
    "text": "to update either of this component",
    "start": "728560",
    "end": "731120"
  },
  {
    "text": "because they are so tightly coupled by",
    "start": "731120",
    "end": "733120"
  },
  {
    "text": "the tensor interface",
    "start": "733120",
    "end": "736160"
  },
  {
    "text": "so looking back at our requirement how",
    "start": "737279",
    "end": "739440"
  },
  {
    "text": "did this approach perform",
    "start": "739440",
    "end": "743040"
  },
  {
    "text": "well the specialized system did well in",
    "start": "743040",
    "end": "745680"
  },
  {
    "text": "delivery predictions",
    "start": "745680",
    "end": "747120"
  },
  {
    "text": "scaling badging and concurrency",
    "start": "747120",
    "end": "749279"
  },
  {
    "text": "requirements",
    "start": "749279",
    "end": "750720"
  },
  {
    "text": "some of the systems are framework",
    "start": "750720",
    "end": "752320"
  },
  {
    "text": "agnostic some of the internal model",
    "start": "752320",
    "end": "755040"
  },
  {
    "text": "trending framework and then compile it",
    "start": "755040",
    "end": "757279"
  },
  {
    "text": "to its own framework",
    "start": "757279",
    "end": "758480"
  },
  {
    "text": "so the requirements may vary case by",
    "start": "758480",
    "end": "761519"
  },
  {
    "text": "case by case but most importantly the",
    "start": "761519",
    "end": "765200"
  },
  {
    "text": "lack of business logic",
    "start": "765200",
    "end": "766959"
  },
  {
    "text": "uh the custom pre-processing and",
    "start": "766959",
    "end": "768959"
  },
  {
    "text": "post-processing",
    "start": "768959",
    "end": "770079"
  },
  {
    "text": "and the end-to-end control of api http",
    "start": "770079",
    "end": "773600"
  },
  {
    "text": "api",
    "start": "773600",
    "end": "774560"
  },
  {
    "text": "is lacking it is not met by the",
    "start": "774560",
    "end": "776639"
  },
  {
    "text": "specialized systems",
    "start": "776639",
    "end": "779600"
  },
  {
    "text": "now let me tell you about research",
    "start": "780079",
    "end": "783519"
  },
  {
    "text": "reserve is a web framework for machine",
    "start": "783519",
    "end": "785920"
  },
  {
    "text": "learning serving",
    "start": "785920",
    "end": "787440"
  },
  {
    "text": "let me break it down reserve is a web",
    "start": "787440",
    "end": "790800"
  },
  {
    "text": "framework",
    "start": "790800",
    "end": "791760"
  },
  {
    "text": "this means we give you end-to-end",
    "start": "791760",
    "end": "793440"
  },
  {
    "text": "control over the http api",
    "start": "793440",
    "end": "796000"
  },
  {
    "text": "your input is just a simple flask",
    "start": "796000",
    "end": "798399"
  },
  {
    "text": "request object",
    "start": "798399",
    "end": "800959"
  },
  {
    "text": "reserve is a web framework built for",
    "start": "800959",
    "end": "803440"
  },
  {
    "text": "machine",
    "start": "803440",
    "end": "804000"
  },
  {
    "text": "learning serving this means you can",
    "start": "804000",
    "end": "806560"
  },
  {
    "text": "define your api",
    "start": "806560",
    "end": "808160"
  },
  {
    "text": "to pre-load the model warm up the model",
    "start": "808160",
    "end": "811040"
  },
  {
    "text": "as well as",
    "start": "811040",
    "end": "811839"
  },
  {
    "text": "arbitrary pre-process and",
    "start": "811839",
    "end": "813440"
  },
  {
    "text": "post-processing steps along the way",
    "start": "813440",
    "end": "817200"
  },
  {
    "text": "these two combined is what we call the",
    "start": "817200",
    "end": "819040"
  },
  {
    "text": "servo api",
    "start": "819040",
    "end": "820320"
  },
  {
    "text": "you just need to define a functional",
    "start": "820320",
    "end": "822000"
  },
  {
    "text": "class that set the single argument",
    "start": "822000",
    "end": "824320"
  },
  {
    "text": "the web request with",
    "start": "824320",
    "end": "827600"
  },
  {
    "text": "the serverable api reserve allows you to",
    "start": "827600",
    "end": "830079"
  },
  {
    "text": "put arbitrary python function",
    "start": "830079",
    "end": "832160"
  },
  {
    "text": "or classes and grant you http request",
    "start": "832160",
    "end": "835839"
  },
  {
    "text": "entry and control over the http request",
    "start": "835839",
    "end": "837920"
  },
  {
    "text": "api",
    "start": "837920",
    "end": "840320"
  },
  {
    "text": "look let's look closely at the arbitrary",
    "start": "840639",
    "end": "843040"
  },
  {
    "text": "python function and class component",
    "start": "843040",
    "end": "845760"
  },
  {
    "text": "because ray is a distributed python",
    "start": "845760",
    "end": "847920"
  },
  {
    "text": "framework and serve runs on top of gray",
    "start": "847920",
    "end": "851440"
  },
  {
    "text": "this means you can run arbitrary rig",
    "start": "851440",
    "end": "853440"
  },
  {
    "text": "program inside observable",
    "start": "853440",
    "end": "856000"
  },
  {
    "text": "what does this mean this means you can",
    "start": "856000",
    "end": "858240"
  },
  {
    "text": "define apis",
    "start": "858240",
    "end": "859519"
  },
  {
    "text": "that's backed by many work notes in",
    "start": "859519",
    "end": "861760"
  },
  {
    "text": "which you can perform",
    "start": "861760",
    "end": "863600"
  },
  {
    "text": "massive fan-out online learning or",
    "start": "863600",
    "end": "866880"
  },
  {
    "text": "personalization and for concrete use use",
    "start": "866880",
    "end": "869839"
  },
  {
    "text": "case",
    "start": "869839",
    "end": "870480"
  },
  {
    "text": "one example is how seldom alibi used",
    "start": "870480",
    "end": "873760"
  },
  {
    "text": "a ray and reserve to scale model",
    "start": "873760",
    "end": "875760"
  },
  {
    "text": "explainability",
    "start": "875760",
    "end": "877279"
  },
  {
    "text": "you can go check out the talk in race",
    "start": "877279",
    "end": "879680"
  },
  {
    "text": "summit",
    "start": "879680",
    "end": "882079"
  },
  {
    "text": "the serverable api make it possible for",
    "start": "883199",
    "end": "885040"
  },
  {
    "text": "serv to satisfy the first three",
    "start": "885040",
    "end": "886720"
  },
  {
    "text": "requirements",
    "start": "886720",
    "end": "887600"
  },
  {
    "text": "but what about the scouting and bashing",
    "start": "887600",
    "end": "889839"
  },
  {
    "text": "part",
    "start": "889839",
    "end": "891199"
  },
  {
    "text": "well for that reserve introduced the",
    "start": "891199",
    "end": "893600"
  },
  {
    "text": "concept called backend",
    "start": "893600",
    "end": "895279"
  },
  {
    "text": "a backend in some version code",
    "start": "895279",
    "end": "897920"
  },
  {
    "text": "implementation of the serverable",
    "start": "897920",
    "end": "899760"
  },
  {
    "text": "you give it a name say right here in",
    "start": "899760",
    "end": "901600"
  },
  {
    "text": "this example it's my model v1",
    "start": "901600",
    "end": "904000"
  },
  {
    "text": "in the code sample and then you can pass",
    "start": "904000",
    "end": "906480"
  },
  {
    "text": "any configuration",
    "start": "906480",
    "end": "908240"
  },
  {
    "text": "for example in the configuration you can",
    "start": "908240",
    "end": "910160"
  },
  {
    "text": "specify the number of",
    "start": "910160",
    "end": "911760"
  },
  {
    "text": "replicas this enables her to replicate",
    "start": "911760",
    "end": "914800"
  },
  {
    "text": "your models and low balance amounting",
    "start": "914800",
    "end": "918720"
  },
  {
    "text": "this is how reserves make it possible to",
    "start": "918720",
    "end": "920800"
  },
  {
    "text": "scale your machine learning service",
    "start": "920800",
    "end": "922480"
  },
  {
    "text": "to match incoming higher load",
    "start": "922480",
    "end": "926079"
  },
  {
    "text": "you can also enable batching by",
    "start": "926079",
    "end": "928880"
  },
  {
    "text": "accepting",
    "start": "928880",
    "end": "929600"
  },
  {
    "text": "a list of requests instead of single one",
    "start": "929600",
    "end": "932560"
  },
  {
    "text": "you can configure the maximum batch size",
    "start": "932560",
    "end": "934880"
  },
  {
    "text": "and reserve will perform bashing for you",
    "start": "934880",
    "end": "939040"
  },
  {
    "text": "you can do even more with back-end",
    "start": "939040",
    "end": "940800"
  },
  {
    "text": "configuration you can pass in resource",
    "start": "940800",
    "end": "943199"
  },
  {
    "text": "requirements like",
    "start": "943199",
    "end": "944399"
  },
  {
    "text": "number of cpus or gpus this enable low",
    "start": "944399",
    "end": "947279"
  },
  {
    "text": "latency model response time using custom",
    "start": "947279",
    "end": "949600"
  },
  {
    "text": "hardware",
    "start": "949600",
    "end": "950880"
  },
  {
    "text": "and you can put in a high number of",
    "start": "950880",
    "end": "953920"
  },
  {
    "text": "replicas",
    "start": "953920",
    "end": "955839"
  },
  {
    "text": "given the hardware available server",
    "start": "955839",
    "end": "958160"
  },
  {
    "text": "actually transparently scale out",
    "start": "958160",
    "end": "959920"
  },
  {
    "text": "to many many worker nodes",
    "start": "959920",
    "end": "963440"
  },
  {
    "text": "where the concept of backend which is",
    "start": "963440",
    "end": "965839"
  },
  {
    "text": "versioned",
    "start": "965839",
    "end": "966480"
  },
  {
    "text": "serverable implementation and instead of",
    "start": "966480",
    "end": "968560"
  },
  {
    "text": "configuration",
    "start": "968560",
    "end": "969839"
  },
  {
    "text": "reserve help your model service to scale",
    "start": "969839",
    "end": "972959"
  },
  {
    "text": "batch and be highly performant",
    "start": "972959",
    "end": "976079"
  },
  {
    "text": "this is how reserves satisfy satisfy all",
    "start": "976079",
    "end": "978639"
  },
  {
    "text": "the requirements",
    "start": "978639",
    "end": "979680"
  },
  {
    "text": "for model serving with these features",
    "start": "979680",
    "end": "983600"
  },
  {
    "text": "reserve help you to productionize a",
    "start": "983600",
    "end": "986079"
  },
  {
    "text": "machine learning",
    "start": "986079",
    "end": "987199"
  },
  {
    "text": "model in just a few lines of code but",
    "start": "987199",
    "end": "990320"
  },
  {
    "text": "can we do even better",
    "start": "990320",
    "end": "992880"
  },
  {
    "text": "we have one core observation here that",
    "start": "992880",
    "end": "995440"
  },
  {
    "text": "is no",
    "start": "995440",
    "end": "996079"
  },
  {
    "text": "model is an island in real world machine",
    "start": "996079",
    "end": "998880"
  },
  {
    "text": "learning application",
    "start": "998880",
    "end": "1000240"
  },
  {
    "text": "model doesn't exist in isolation you",
    "start": "1000240",
    "end": "1002959"
  },
  {
    "text": "cannot build a machine learning app with",
    "start": "1002959",
    "end": "1004959"
  },
  {
    "text": "just a single model",
    "start": "1004959",
    "end": "1007759"
  },
  {
    "text": "for example there are typically chance",
    "start": "1007759",
    "end": "1009920"
  },
  {
    "text": "of operators that involve pre-processing",
    "start": "1009920",
    "end": "1012000"
  },
  {
    "text": "and post-processing for",
    "start": "1012000",
    "end": "1013360"
  },
  {
    "text": "means through the ml pipeline",
    "start": "1013360",
    "end": "1016800"
  },
  {
    "text": "there are there can be say feature",
    "start": "1016800",
    "end": "1018560"
  },
  {
    "text": "normalization and scaling before the",
    "start": "1018560",
    "end": "1020480"
  },
  {
    "text": "model",
    "start": "1020480",
    "end": "1020959"
  },
  {
    "text": "and scaling it back after the model or",
    "start": "1020959",
    "end": "1023839"
  },
  {
    "text": "in recommendation system you have",
    "start": "1023839",
    "end": "1025360"
  },
  {
    "text": "something like a chain of",
    "start": "1025360",
    "end": "1026640"
  },
  {
    "text": "embedding lookup k nearest neighbor",
    "start": "1026640",
    "end": "1028959"
  },
  {
    "text": "search and then",
    "start": "1028959",
    "end": "1030000"
  },
  {
    "text": "one or more ranking model to finalize",
    "start": "1030000",
    "end": "1032959"
  },
  {
    "text": "the output",
    "start": "1032959",
    "end": "1035520"
  },
  {
    "text": "model doesn't exist in isolation not",
    "start": "1036240",
    "end": "1038079"
  },
  {
    "text": "just because you work with other models",
    "start": "1038079",
    "end": "1040079"
  },
  {
    "text": "it's also because there are model",
    "start": "1040079",
    "end": "1041360"
  },
  {
    "text": "updates over time there are cases where",
    "start": "1041360",
    "end": "1044000"
  },
  {
    "text": "you need to split the",
    "start": "1044000",
    "end": "1045360"
  },
  {
    "text": "request to different version of the",
    "start": "1045360",
    "end": "1047038"
  },
  {
    "text": "model to perform",
    "start": "1047039",
    "end": "1048400"
  },
  {
    "text": "say a b testing for example we want",
    "start": "1048400",
    "end": "1051039"
  },
  {
    "text": "percent of the traffic to",
    "start": "1051039",
    "end": "1052000"
  },
  {
    "text": "one of the model and twenty percent to",
    "start": "1052000",
    "end": "1053520"
  },
  {
    "text": "the other there are also deployment",
    "start": "1053520",
    "end": "1055760"
  },
  {
    "text": "patterns like canary deployment",
    "start": "1055760",
    "end": "1057840"
  },
  {
    "text": "or shadow testing in which you send a",
    "start": "1057840",
    "end": "1060000"
  },
  {
    "text": "fraction of the traffic to your model",
    "start": "1060000",
    "end": "1061840"
  },
  {
    "text": "just to test it out",
    "start": "1061840",
    "end": "1063200"
  },
  {
    "text": "and in case something horribly bad",
    "start": "1063200",
    "end": "1065280"
  },
  {
    "text": "happened you can roll it back easily",
    "start": "1065280",
    "end": "1067200"
  },
  {
    "text": "without disrupting your users",
    "start": "1067200",
    "end": "1070720"
  },
  {
    "text": "to for this restaurant introduced a",
    "start": "1071360",
    "end": "1073200"
  },
  {
    "text": "concept called endpoint",
    "start": "1073200",
    "end": "1074880"
  },
  {
    "text": "endpoints are just like the endpoint you",
    "start": "1074880",
    "end": "1076640"
  },
  {
    "text": "know in microservices or network proxies",
    "start": "1076640",
    "end": "1079280"
  },
  {
    "text": "there are some named reachable address",
    "start": "1079280",
    "end": "1081440"
  },
  {
    "text": "that correspond to one or more backends",
    "start": "1081440",
    "end": "1085039"
  },
  {
    "text": "endpoints are http reachable you can",
    "start": "1085039",
    "end": "1087760"
  },
  {
    "text": "specify the http routes or methods",
    "start": "1087760",
    "end": "1090799"
  },
  {
    "text": "once deployed you can use any http",
    "start": "1090799",
    "end": "1093679"
  },
  {
    "text": "client",
    "start": "1093679",
    "end": "1094000"
  },
  {
    "text": "library to pin the endpoints",
    "start": "1094000",
    "end": "1097440"
  },
  {
    "text": "endpoints also correspond to one or more",
    "start": "1097440",
    "end": "1099440"
  },
  {
    "text": "backends in this example",
    "start": "1099440",
    "end": "1101280"
  },
  {
    "text": "we have two backends called my backend",
    "start": "1101280",
    "end": "1103440"
  },
  {
    "text": "v0 and v1",
    "start": "1103440",
    "end": "1105440"
  },
  {
    "text": "we set the default back into v0 and then",
    "start": "1105440",
    "end": "1108000"
  },
  {
    "text": "set the traffic policy to route 70",
    "start": "1108000",
    "end": "1110480"
  },
  {
    "text": "of the traffic to v0 at only 30 to v1",
    "start": "1110480",
    "end": "1114240"
  },
  {
    "text": "now reserve will randomly load balance",
    "start": "1114240",
    "end": "1116799"
  },
  {
    "text": "between the two",
    "start": "1116799",
    "end": "1118080"
  },
  {
    "text": "given specified weights and you can",
    "start": "1118080",
    "end": "1121039"
  },
  {
    "text": "dynamically",
    "start": "1121039",
    "end": "1121840"
  },
  {
    "text": "update these awaits over time without",
    "start": "1121840",
    "end": "1125280"
  },
  {
    "text": "any downtime for a service",
    "start": "1125280",
    "end": "1128640"
  },
  {
    "text": "this is how reserve is the concept of",
    "start": "1128799",
    "end": "1130960"
  },
  {
    "text": "employees to make it possible to deploy",
    "start": "1130960",
    "end": "1133679"
  },
  {
    "text": "real operational deployment patterns",
    "start": "1133679",
    "end": "1137679"
  },
  {
    "text": "but there's more endpoints are not just",
    "start": "1137679",
    "end": "1140400"
  },
  {
    "text": "http",
    "start": "1140400",
    "end": "1141280"
  },
  {
    "text": "reachable they're also python reachable",
    "start": "1141280",
    "end": "1143679"
  },
  {
    "text": "you can define your service as usual",
    "start": "1143679",
    "end": "1145919"
  },
  {
    "text": "the request object will be a server",
    "start": "1145919",
    "end": "1147919"
  },
  {
    "text": "request that has identical api",
    "start": "1147919",
    "end": "1150160"
  },
  {
    "text": "as a flash request when you deploy the",
    "start": "1150160",
    "end": "1153120"
  },
  {
    "text": "model",
    "start": "1153120",
    "end": "1153840"
  },
  {
    "text": "and get what we call a handle to the",
    "start": "1153840",
    "end": "1156000"
  },
  {
    "text": "endpoint you can issue python request",
    "start": "1156000",
    "end": "1158640"
  },
  {
    "text": "with python data type directly to the",
    "start": "1158640",
    "end": "1161280"
  },
  {
    "text": "backend",
    "start": "1161280",
    "end": "1162160"
  },
  {
    "text": "you can pass in any python object",
    "start": "1162160",
    "end": "1164960"
  },
  {
    "text": "including say",
    "start": "1164960",
    "end": "1165840"
  },
  {
    "text": "mp array or other big numeric values",
    "start": "1165840",
    "end": "1170320"
  },
  {
    "text": "it will not go through the http server",
    "start": "1170320",
    "end": "1173760"
  },
  {
    "text": "this is a big deal being python",
    "start": "1173760",
    "end": "1175919"
  },
  {
    "text": "reachable means we can call other",
    "start": "1175919",
    "end": "1177679"
  },
  {
    "text": "endpoints within our backends",
    "start": "1177679",
    "end": "1179679"
  },
  {
    "text": "enable us to have arbitrary composed",
    "start": "1179679",
    "end": "1182640"
  },
  {
    "text": "model",
    "start": "1182640",
    "end": "1184480"
  },
  {
    "text": "here's an example of model composition",
    "start": "1184480",
    "end": "1187200"
  },
  {
    "text": "in compose serverable definition",
    "start": "1187200",
    "end": "1189280"
  },
  {
    "text": "we get the handle to endpoints one and",
    "start": "1189280",
    "end": "1191760"
  },
  {
    "text": "two",
    "start": "1191760",
    "end": "1192400"
  },
  {
    "text": "send the request to both of them at the",
    "start": "1192400",
    "end": "1194080"
  },
  {
    "text": "same time and wait for both of the",
    "start": "1194080",
    "end": "1196240"
  },
  {
    "text": "results to come back",
    "start": "1196240",
    "end": "1197440"
  },
  {
    "text": "and lastly sort of just combine them and",
    "start": "1197440",
    "end": "1199520"
  },
  {
    "text": "return them as a single response",
    "start": "1199520",
    "end": "1201840"
  },
  {
    "text": "you can deploy this composed backend as",
    "start": "1201840",
    "end": "1204559"
  },
  {
    "text": "a normal serve back end",
    "start": "1204559",
    "end": "1206159"
  },
  {
    "text": "and then request talking to it will",
    "start": "1206159",
    "end": "1208080"
  },
  {
    "text": "entrance top to other two",
    "start": "1208080",
    "end": "1210000"
  },
  {
    "text": "uh to other endpoints what does this",
    "start": "1210000",
    "end": "1213520"
  },
  {
    "text": "mean",
    "start": "1213520",
    "end": "1213840"
  },
  {
    "text": "this means you can have arbitrary",
    "start": "1213840",
    "end": "1215760"
  },
  {
    "text": "pipeline like the chaining operator i've",
    "start": "1215760",
    "end": "1217919"
  },
  {
    "text": "shown before",
    "start": "1217919",
    "end": "1218960"
  },
  {
    "text": "arbitrary fanning out or ensemble where",
    "start": "1218960",
    "end": "1221600"
  },
  {
    "text": "you gather result from multiple model",
    "start": "1221600",
    "end": "1223600"
  },
  {
    "text": "and average them or cascade where you",
    "start": "1223600",
    "end": "1226559"
  },
  {
    "text": "would conditionally select which model",
    "start": "1226559",
    "end": "1228720"
  },
  {
    "text": "we are using based on the model accuracy",
    "start": "1228720",
    "end": "1230880"
  },
  {
    "text": "or model latency",
    "start": "1230880",
    "end": "1234159"
  },
  {
    "text": "this is how being python reachable",
    "start": "1234159",
    "end": "1235919"
  },
  {
    "text": "allows you to easily compose models",
    "start": "1235919",
    "end": "1237919"
  },
  {
    "text": "together",
    "start": "1237919",
    "end": "1240240"
  },
  {
    "text": "and one last thing about being python",
    "start": "1240240",
    "end": "1242240"
  },
  {
    "text": "reachable endpoints",
    "start": "1242240",
    "end": "1243600"
  },
  {
    "text": "is that you can use the same code even",
    "start": "1243600",
    "end": "1245919"
  },
  {
    "text": "the same backend",
    "start": "1245919",
    "end": "1247120"
  },
  {
    "text": "for both online interactive serving and",
    "start": "1247120",
    "end": "1249840"
  },
  {
    "text": "offline batch influence",
    "start": "1249840",
    "end": "1251520"
  },
  {
    "text": "in the sample here we're just reading",
    "start": "1251520",
    "end": "1253120"
  },
  {
    "text": "our records from data lake",
    "start": "1253120",
    "end": "1254559"
  },
  {
    "text": "and issuing requests directly to the",
    "start": "1254559",
    "end": "1256720"
  },
  {
    "text": "model",
    "start": "1256720",
    "end": "1258720"
  },
  {
    "text": "this is a big deal when your machine",
    "start": "1258720",
    "end": "1260640"
  },
  {
    "text": "learning application is not ready for",
    "start": "1260640",
    "end": "1262159"
  },
  {
    "text": "real-time usage",
    "start": "1262159",
    "end": "1263200"
  },
  {
    "text": "yet that's fine serv can help you get",
    "start": "1263200",
    "end": "1265679"
  },
  {
    "text": "prepared",
    "start": "1265679",
    "end": "1266480"
  },
  {
    "text": "you can utilize all the amazing features",
    "start": "1266480",
    "end": "1269120"
  },
  {
    "text": "from serve like",
    "start": "1269120",
    "end": "1270080"
  },
  {
    "text": "scouting on batching not",
    "start": "1270080",
    "end": "1273280"
  },
  {
    "text": "but not deploy it for http",
    "start": "1273280",
    "end": "1276400"
  },
  {
    "text": "you can use serve as a worker pool for",
    "start": "1276400",
    "end": "1278640"
  },
  {
    "text": "parallel scoring",
    "start": "1278640",
    "end": "1279919"
  },
  {
    "text": "or mixing real-time serving with small",
    "start": "1279919",
    "end": "1281919"
  },
  {
    "text": "batch streaming",
    "start": "1281919",
    "end": "1283120"
  },
  {
    "text": "we have production users who are not",
    "start": "1283120",
    "end": "1285280"
  },
  {
    "text": "ready for real-time interactive usage",
    "start": "1285280",
    "end": "1287280"
  },
  {
    "text": "yet",
    "start": "1287280",
    "end": "1287760"
  },
  {
    "text": "for both of these use cases",
    "start": "1287760",
    "end": "1291679"
  },
  {
    "text": "this is how endpoint concepts help you",
    "start": "1291679",
    "end": "1293600"
  },
  {
    "text": "with offline batch influence as well",
    "start": "1293600",
    "end": "1296080"
  },
  {
    "text": "these are all scenes reserve enables and",
    "start": "1296080",
    "end": "1298880"
  },
  {
    "text": "how it helps you to productionize your",
    "start": "1298880",
    "end": "1300640"
  },
  {
    "text": "machine learning models",
    "start": "1300640",
    "end": "1304320"
  },
  {
    "text": "so research we just apply some package",
    "start": "1304320",
    "end": "1306400"
  },
  {
    "text": "it can run on your",
    "start": "1306400",
    "end": "1307760"
  },
  {
    "text": "laptop for development and quick demo",
    "start": "1307760",
    "end": "1310960"
  },
  {
    "text": "you can deploy it to on-premise server",
    "start": "1310960",
    "end": "1313200"
  },
  {
    "text": "you can have template for you to run it",
    "start": "1313200",
    "end": "1315360"
  },
  {
    "text": "on",
    "start": "1315360",
    "end": "1315840"
  },
  {
    "text": "kubernetes as well as all the major",
    "start": "1315840",
    "end": "1318799"
  },
  {
    "text": "public cloud",
    "start": "1318799",
    "end": "1321039"
  },
  {
    "text": "because reserve is just a library built",
    "start": "1321039",
    "end": "1323440"
  },
  {
    "text": "on pop-up ray",
    "start": "1323440",
    "end": "1324640"
  },
  {
    "text": "we got a lot of features out of the box",
    "start": "1324640",
    "end": "1326799"
  },
  {
    "text": "like the cluster launcher that deploys",
    "start": "1326799",
    "end": "1328640"
  },
  {
    "text": "program to all public cloud",
    "start": "1328640",
    "end": "1330480"
  },
  {
    "text": "we also got monitoring autoscaling and",
    "start": "1330480",
    "end": "1332960"
  },
  {
    "text": "fault tolerance",
    "start": "1332960",
    "end": "1334320"
  },
  {
    "text": "easily as developer you can also build",
    "start": "1334320",
    "end": "1337600"
  },
  {
    "text": "your special purpose frameworks that",
    "start": "1337600",
    "end": "1339200"
  },
  {
    "text": "focus on the api",
    "start": "1339200",
    "end": "1340480"
  },
  {
    "text": "and amazing features while led ray takes",
    "start": "1340480",
    "end": "1343280"
  },
  {
    "text": "care of this tricky part",
    "start": "1343280",
    "end": "1345039"
  },
  {
    "text": "about distributed programming",
    "start": "1345039",
    "end": "1348480"
  },
  {
    "text": "you can try it out try out reserve today",
    "start": "1348559",
    "end": "1350880"
  },
  {
    "text": "by just pip install and import serve",
    "start": "1350880",
    "end": "1353520"
  },
  {
    "text": "and we have a very active community",
    "start": "1353520",
    "end": "1356240"
  },
  {
    "text": "discussing serve in the race slack",
    "start": "1356240",
    "end": "1358559"
  },
  {
    "text": "you can check out our extensive",
    "start": "1358559",
    "end": "1360080"
  },
  {
    "text": "documentation as well as",
    "start": "1360080",
    "end": "1363039"
  },
  {
    "text": "come to our office hour tell us about",
    "start": "1363039",
    "end": "1364720"
  },
  {
    "text": "your use case we",
    "start": "1364720",
    "end": "1366159"
  },
  {
    "text": "welcome your feedback i'm very excited",
    "start": "1366159",
    "end": "1368159"
  },
  {
    "text": "to work with you",
    "start": "1368159",
    "end": "1369280"
  },
  {
    "text": "to solve your machine learning serving",
    "start": "1369280",
    "end": "1370960"
  },
  {
    "text": "challenge",
    "start": "1370960",
    "end": "1372720"
  },
  {
    "text": "and finally let me leave you with the",
    "start": "1372720",
    "end": "1374720"
  },
  {
    "text": "slides about how",
    "start": "1374720",
    "end": "1376240"
  },
  {
    "text": "reserve can help you productionize",
    "start": "1376240",
    "end": "1378320"
  },
  {
    "text": "machine learning models",
    "start": "1378320",
    "end": "1380000"
  },
  {
    "text": "and i'm happy to take your questions",
    "start": "1380000",
    "end": "1384480"
  }
]