[
  {
    "text": "[Music]",
    "start": "170",
    "end": "14559"
  },
  {
    "text": "thank you very much uh for having me",
    "start": "14559",
    "end": "16080"
  },
  {
    "text": "here in",
    "start": "16080",
    "end": "16480"
  },
  {
    "text": "ray summit very excited to be here and",
    "start": "16480",
    "end": "18080"
  },
  {
    "text": "talk about um",
    "start": "18080",
    "end": "19520"
  },
  {
    "text": "software 2.0 uh and what it needs",
    "start": "19520",
    "end": "23199"
  },
  {
    "text": "and here we're going to introduce about",
    "start": "23199",
    "end": "24720"
  },
  {
    "text": "data 2.0 and how we are",
    "start": "24720",
    "end": "26560"
  },
  {
    "text": "thinking about storing and managing",
    "start": "26560",
    "end": "28880"
  },
  {
    "text": "unstructured data sets for",
    "start": "28880",
    "end": "30800"
  },
  {
    "text": "deep learning at active loop before",
    "start": "30800",
    "end": "34320"
  },
  {
    "text": "starting the main of the conversation a",
    "start": "34320",
    "end": "37280"
  },
  {
    "text": "lot",
    "start": "37280",
    "end": "37600"
  },
  {
    "text": "to get into details what i was doing",
    "start": "37600",
    "end": "39040"
  },
  {
    "text": "before um i was doing a phd at princeton",
    "start": "39040",
    "end": "41440"
  },
  {
    "text": "university where i was in a neuroscience",
    "start": "41440",
    "end": "43200"
  },
  {
    "text": "lab advised by sebastian saying",
    "start": "43200",
    "end": "46079"
  },
  {
    "text": "and the problem that we were working on",
    "start": "46079",
    "end": "48000"
  },
  {
    "text": "we were taking a mass brain cutting into",
    "start": "48000",
    "end": "49840"
  },
  {
    "text": "very thin slices",
    "start": "49840",
    "end": "51039"
  },
  {
    "text": "imaging each slice and then applying",
    "start": "51039",
    "end": "54399"
  },
  {
    "text": "machine learning and deep learning at",
    "start": "54399",
    "end": "55760"
  },
  {
    "text": "scale to transform those",
    "start": "55760",
    "end": "57039"
  },
  {
    "text": "images into segmented neurons and then",
    "start": "57039",
    "end": "60399"
  },
  {
    "text": "find the connections to build the",
    "start": "60399",
    "end": "61520"
  },
  {
    "text": "connectome or the graph of the",
    "start": "61520",
    "end": "62879"
  },
  {
    "text": "connections",
    "start": "62879",
    "end": "65040"
  },
  {
    "text": "each slice image size was hundred",
    "start": "65040",
    "end": "67040"
  },
  {
    "text": "thousand by hundred thousand pixels",
    "start": "67040",
    "end": "68960"
  },
  {
    "text": "and we had twenty thousand of those",
    "start": "68960",
    "end": "70799"
  },
  {
    "text": "slices as well so it was around a better",
    "start": "70799",
    "end": "72479"
  },
  {
    "text": "by scale dataset",
    "start": "72479",
    "end": "73520"
  },
  {
    "text": "and if you apply a deep learning",
    "start": "73520",
    "end": "74720"
  },
  {
    "text": "pipeline on petabyte scale dataset on a",
    "start": "74720",
    "end": "76799"
  },
  {
    "text": "bl",
    "start": "76799",
    "end": "77680"
  },
  {
    "text": "adiables it becomes very expensive",
    "start": "77680",
    "end": "81360"
  },
  {
    "text": "and our goal was to reduce this cost to",
    "start": "81360",
    "end": "83759"
  },
  {
    "text": "200",
    "start": "83759",
    "end": "84560"
  },
  {
    "text": "or like two or three times cheaper by",
    "start": "84560",
    "end": "86799"
  },
  {
    "text": "rethinking",
    "start": "86799",
    "end": "87840"
  },
  {
    "text": "how the data should be stored how it",
    "start": "87840",
    "end": "89680"
  },
  {
    "text": "should be streamed from the object",
    "start": "89680",
    "end": "90960"
  },
  {
    "text": "storage to the computing machines",
    "start": "90960",
    "end": "92640"
  },
  {
    "text": "should we use cpus and gpus what kind of",
    "start": "92640",
    "end": "94880"
  },
  {
    "text": "models to use",
    "start": "94880",
    "end": "95759"
  },
  {
    "text": "and how to scale all this processing on",
    "start": "95759",
    "end": "97680"
  },
  {
    "text": "like hundred or thousand machines",
    "start": "97680",
    "end": "100479"
  },
  {
    "text": "on the cloud at the same time that's how",
    "start": "100479",
    "end": "103840"
  },
  {
    "text": "we got into a y combinator",
    "start": "103840",
    "end": "105439"
  },
  {
    "text": "then raised our seed fund and we have",
    "start": "105439",
    "end": "106880"
  },
  {
    "text": "been working with early customers to",
    "start": "106880",
    "end": "108399"
  },
  {
    "text": "help them to be more efficient in terms",
    "start": "108399",
    "end": "109840"
  },
  {
    "text": "of large-scale machine learning",
    "start": "109840",
    "end": "110960"
  },
  {
    "text": "applications",
    "start": "110960",
    "end": "112000"
  },
  {
    "text": "one of the customers we worked with was",
    "start": "112000",
    "end": "113920"
  },
  {
    "text": "in legal tech",
    "start": "113920",
    "end": "115360"
  },
  {
    "text": "space they had 80 million text documents",
    "start": "115360",
    "end": "117600"
  },
  {
    "text": "and their problem was to train an",
    "start": "117600",
    "end": "119040"
  },
  {
    "text": "embedding model to um be able to very",
    "start": "119040",
    "end": "122719"
  },
  {
    "text": "efficiently search the data",
    "start": "122719",
    "end": "124399"
  },
  {
    "text": "and they were spending like two months",
    "start": "124399",
    "end": "126079"
  },
  {
    "text": "to train a single model",
    "start": "126079",
    "end": "128160"
  },
  {
    "text": "on gpu and we also work with another",
    "start": "128160",
    "end": "132080"
  },
  {
    "text": "architect company",
    "start": "132080",
    "end": "133360"
  },
  {
    "text": "that had airplanes flying over the",
    "start": "133360",
    "end": "134879"
  },
  {
    "text": "fields in illinois they were",
    "start": "134879",
    "end": "136879"
  },
  {
    "text": "collecting unstructured data sets of",
    "start": "136879",
    "end": "140720"
  },
  {
    "text": "aerial images of agricultural fields",
    "start": "140720",
    "end": "144000"
  },
  {
    "text": "and the problem was to provide insights",
    "start": "144000",
    "end": "145680"
  },
  {
    "text": "to the farmers where",
    "start": "145680",
    "end": "147520"
  },
  {
    "text": "there is a disease in the field or dry",
    "start": "147520",
    "end": "149360"
  },
  {
    "text": "down area and we cooperated within",
    "start": "149360",
    "end": "151440"
  },
  {
    "text": "culinary to help them to",
    "start": "151440",
    "end": "152879"
  },
  {
    "text": "plug the unsecured datasets to the",
    "start": "152879",
    "end": "155280"
  },
  {
    "text": "machine learning models they can train a",
    "start": "155280",
    "end": "156720"
  },
  {
    "text": "lot of deep learning models to do crop",
    "start": "156720",
    "end": "158239"
  },
  {
    "text": "classification or",
    "start": "158239",
    "end": "160000"
  },
  {
    "text": "identifying them segmenting the pixels",
    "start": "160000",
    "end": "163200"
  },
  {
    "text": "for",
    "start": "163200",
    "end": "163599"
  },
  {
    "text": "dry down areas and and so on to provide",
    "start": "163599",
    "end": "165760"
  },
  {
    "text": "us in insights",
    "start": "165760",
    "end": "167200"
  },
  {
    "text": "and what we learned so far is that the",
    "start": "167200",
    "end": "169599"
  },
  {
    "text": "problem is actually not in the training",
    "start": "169599",
    "end": "171840"
  },
  {
    "text": "or the deployment of those models it's",
    "start": "171840",
    "end": "174000"
  },
  {
    "text": "actually in the data",
    "start": "174000",
    "end": "175280"
  },
  {
    "text": "how people store unstructured data sets",
    "start": "175280",
    "end": "178080"
  },
  {
    "text": "we use a lot of files",
    "start": "178080",
    "end": "179120"
  },
  {
    "text": "images etc and it's a lot of time",
    "start": "179120",
    "end": "183040"
  },
  {
    "text": "consuming essentially even today",
    "start": "183040",
    "end": "185280"
  },
  {
    "text": "to process this data and extract",
    "start": "185280",
    "end": "187840"
  },
  {
    "text": "valuable insights",
    "start": "187840",
    "end": "189680"
  },
  {
    "text": "for the businesses it's still data",
    "start": "189680",
    "end": "191280"
  },
  {
    "text": "scientists spend 80 percent of their",
    "start": "191280",
    "end": "192800"
  },
  {
    "text": "time",
    "start": "192800",
    "end": "193440"
  },
  {
    "text": "on preparing data and",
    "start": "193440",
    "end": "196560"
  },
  {
    "text": "what we try to look for is like even if",
    "start": "196560",
    "end": "198400"
  },
  {
    "text": "we you try to go and google search like",
    "start": "198400",
    "end": "200560"
  },
  {
    "text": "can you give me a database for images",
    "start": "200560",
    "end": "202640"
  },
  {
    "text": "what you find out is that all these",
    "start": "202640",
    "end": "205519"
  },
  {
    "text": "tools",
    "start": "205519",
    "end": "206159"
  },
  {
    "text": "like they all suggest like store still",
    "start": "206159",
    "end": "208159"
  },
  {
    "text": "store all your data",
    "start": "208159",
    "end": "209360"
  },
  {
    "text": "in a mysql or tablet database and have",
    "start": "209360",
    "end": "212720"
  },
  {
    "text": "additional column that points to the",
    "start": "212720",
    "end": "214239"
  },
  {
    "text": "files",
    "start": "214239",
    "end": "215120"
  },
  {
    "text": "but if you want to do a machine learning",
    "start": "215120",
    "end": "217040"
  },
  {
    "text": "on top it's pretty inefficient",
    "start": "217040",
    "end": "218799"
  },
  {
    "text": "and there's no any database data",
    "start": "218799",
    "end": "220400"
  },
  {
    "text": "warehouse data lake for especially",
    "start": "220400",
    "end": "222000"
  },
  {
    "text": "specialized for unstructured data",
    "start": "222000",
    "end": "223760"
  },
  {
    "text": "so that it's very efficient to do",
    "start": "223760",
    "end": "224959"
  },
  {
    "text": "machine learning on top and that's what",
    "start": "224959",
    "end": "227280"
  },
  {
    "text": "we decided to focus on so what we are",
    "start": "227280",
    "end": "230640"
  },
  {
    "text": "working on",
    "start": "230640",
    "end": "231360"
  },
  {
    "text": "is essentially building an industry",
    "start": "231360",
    "end": "233360"
  },
  {
    "text": "standard for storing unstructured data",
    "start": "233360",
    "end": "234879"
  },
  {
    "text": "sets",
    "start": "234879",
    "end": "235519"
  },
  {
    "text": "and then streaming them to software 2.0",
    "start": "235519",
    "end": "237760"
  },
  {
    "text": "or machine learning purposes",
    "start": "237760",
    "end": "240239"
  },
  {
    "text": "and here what we did introduce basically",
    "start": "240239",
    "end": "242480"
  },
  {
    "text": "a new standard for",
    "start": "242480",
    "end": "243439"
  },
  {
    "text": "storing and streaming those data sets",
    "start": "243439",
    "end": "244959"
  },
  {
    "text": "it's you can think of it as like",
    "start": "244959",
    "end": "246319"
  },
  {
    "text": "serverless numpy arrays",
    "start": "246319",
    "end": "248000"
  },
  {
    "text": "um those are like you can store the data",
    "start": "248000",
    "end": "251120"
  },
  {
    "text": "and stream the data in unified arrays on",
    "start": "251120",
    "end": "253439"
  },
  {
    "text": "the cloud",
    "start": "253439",
    "end": "254159"
  },
  {
    "text": "it can scale to any machine and at any",
    "start": "254159",
    "end": "257120"
  },
  {
    "text": "scale",
    "start": "257120",
    "end": "258160"
  },
  {
    "text": "and be accessible for each of those",
    "start": "258160",
    "end": "261040"
  },
  {
    "text": "machines as if the data was local to the",
    "start": "261040",
    "end": "262639"
  },
  {
    "text": "machine now get into details what does",
    "start": "262639",
    "end": "264080"
  },
  {
    "text": "it mean",
    "start": "264080",
    "end": "264800"
  },
  {
    "text": "literally but this is what we do so",
    "start": "264800",
    "end": "267680"
  },
  {
    "text": "let's say you have million images",
    "start": "267680",
    "end": "269280"
  },
  {
    "text": "they start sitting on your file system",
    "start": "269280",
    "end": "271040"
  },
  {
    "text": "or they are sitting on an object storage",
    "start": "271040",
    "end": "272960"
  },
  {
    "text": "and you want to be able to very",
    "start": "272960",
    "end": "274080"
  },
  {
    "text": "efficiently stream them to machine",
    "start": "274080",
    "end": "275680"
  },
  {
    "text": "learning",
    "start": "275680",
    "end": "276320"
  },
  {
    "text": "frameworks and trainer model and you do",
    "start": "276320",
    "end": "278320"
  },
  {
    "text": "have to do this all this heavy lifting",
    "start": "278320",
    "end": "280400"
  },
  {
    "text": "of taking this data writing like",
    "start": "280400",
    "end": "282880"
  },
  {
    "text": "hundreds of lines of code",
    "start": "282880",
    "end": "284240"
  },
  {
    "text": "boilerplate code to read this and then",
    "start": "284240",
    "end": "286400"
  },
  {
    "text": "plug this into tensorflow or pytorch",
    "start": "286400",
    "end": "288960"
  },
  {
    "text": "just to start training a single model",
    "start": "288960",
    "end": "290479"
  },
  {
    "text": "but what if you want to do distributed",
    "start": "290479",
    "end": "292000"
  },
  {
    "text": "training or what if you want to do",
    "start": "292000",
    "end": "293520"
  },
  {
    "text": "like run a lot of different versions of",
    "start": "293520",
    "end": "295280"
  },
  {
    "text": "the data set and have",
    "start": "295280",
    "end": "296800"
  },
  {
    "text": "be able to do and what we do is that we",
    "start": "296800",
    "end": "298560"
  },
  {
    "text": "take all your different images",
    "start": "298560",
    "end": "300479"
  },
  {
    "text": "transform them into tensors and then",
    "start": "300479",
    "end": "303039"
  },
  {
    "text": "very efficiently store this on",
    "start": "303039",
    "end": "305360"
  },
  {
    "text": "your specified cloud location could be",
    "start": "305360",
    "end": "307440"
  },
  {
    "text": "an astro google cloud",
    "start": "307440",
    "end": "308800"
  },
  {
    "text": "such that when you stream the data from",
    "start": "308800",
    "end": "310960"
  },
  {
    "text": "that location it becomes very",
    "start": "310960",
    "end": "312639"
  },
  {
    "text": "inefficient",
    "start": "312639",
    "end": "313759"
  },
  {
    "text": "um to take this data set to stream to",
    "start": "313759",
    "end": "316639"
  },
  {
    "text": "the gpu",
    "start": "316639",
    "end": "317360"
  },
  {
    "text": "and for the gpu the data is actually",
    "start": "317360",
    "end": "319120"
  },
  {
    "text": "feels the same the throughput that",
    "start": "319120",
    "end": "320800"
  },
  {
    "text": "we can achieve maximum throughput",
    "start": "320800",
    "end": "322320"
  },
  {
    "text": "there's not any much difference",
    "start": "322320",
    "end": "324320"
  },
  {
    "text": "so if we get into the machine learning",
    "start": "324320",
    "end": "327280"
  },
  {
    "text": "um",
    "start": "327280",
    "end": "327680"
  },
  {
    "text": "life cycle we have this collection the",
    "start": "327680",
    "end": "329840"
  },
  {
    "text": "audit of data and then you",
    "start": "329840",
    "end": "331520"
  },
  {
    "text": "the data scientists are machine learning",
    "start": "331520",
    "end": "332880"
  },
  {
    "text": "engineers they spend a lot of time on",
    "start": "332880",
    "end": "334000"
  },
  {
    "text": "data running",
    "start": "334000",
    "end": "335120"
  },
  {
    "text": "and then they have to train this again",
    "start": "335120",
    "end": "336479"
  },
  {
    "text": "of course once you start doing it build",
    "start": "336479",
    "end": "339440"
  },
  {
    "text": "your first data pipelines and it becomes",
    "start": "339440",
    "end": "341120"
  },
  {
    "text": "much less time and then you start",
    "start": "341120",
    "end": "342320"
  },
  {
    "text": "tweaking it",
    "start": "342320",
    "end": "343199"
  },
  {
    "text": "and so on but for every use case you",
    "start": "343199",
    "end": "344720"
  },
  {
    "text": "have to spend spend a week or two weeks",
    "start": "344720",
    "end": "347199"
  },
  {
    "text": "just to set up the",
    "start": "347199",
    "end": "348320"
  },
  {
    "text": "whole data pipeline to train your model",
    "start": "348320",
    "end": "350639"
  },
  {
    "text": "and with active loop and hub and we are",
    "start": "350639",
    "end": "352800"
  },
  {
    "text": "like taking this",
    "start": "352800",
    "end": "353919"
  },
  {
    "text": "chunk of code and then saying that hey",
    "start": "353919",
    "end": "356000"
  },
  {
    "text": "instead machine learning engineers data",
    "start": "356000",
    "end": "358080"
  },
  {
    "text": "scientists they can actually focus on",
    "start": "358080",
    "end": "359680"
  },
  {
    "text": "the most important part of extracting",
    "start": "359680",
    "end": "361600"
  },
  {
    "text": "value",
    "start": "361600",
    "end": "362400"
  },
  {
    "text": "or insights from the data itself instead",
    "start": "362400",
    "end": "364560"
  },
  {
    "text": "of doing this data janitorial work of",
    "start": "364560",
    "end": "367360"
  },
  {
    "text": "hey we should set up the pipelines and",
    "start": "367360",
    "end": "369199"
  },
  {
    "text": "so on and what we did with",
    "start": "369199",
    "end": "371120"
  },
  {
    "text": "uh waymo we did the calibration",
    "start": "371120",
    "end": "372800"
  },
  {
    "text": "basically they had a data set",
    "start": "372800",
    "end": "374319"
  },
  {
    "text": "two terabytes has a thousand tab record",
    "start": "374319",
    "end": "377520"
  },
  {
    "text": "files each of them like kind of combined",
    "start": "377520",
    "end": "379039"
  },
  {
    "text": "into zip file stored on google cloud",
    "start": "379039",
    "end": "381120"
  },
  {
    "text": "storage location and for an average data",
    "start": "381120",
    "end": "383199"
  },
  {
    "text": "scientist it spends like at least two",
    "start": "383199",
    "end": "385199"
  },
  {
    "text": "days just to download the data",
    "start": "385199",
    "end": "386960"
  },
  {
    "text": "onto those files understand their api",
    "start": "386960",
    "end": "389280"
  },
  {
    "text": "file all the data and then access the",
    "start": "389280",
    "end": "390800"
  },
  {
    "text": "particular slides of the data in average",
    "start": "390800",
    "end": "393280"
  },
  {
    "text": "and what we did for them is that we",
    "start": "393280",
    "end": "395039"
  },
  {
    "text": "transform their data into our format",
    "start": "395039",
    "end": "397199"
  },
  {
    "text": "and store it back on their google cloud",
    "start": "397199",
    "end": "398800"
  },
  {
    "text": "search so they have the full control",
    "start": "398800",
    "end": "400800"
  },
  {
    "text": "and data center is knocking with just",
    "start": "400800",
    "end": "402720"
  },
  {
    "text": "single people so how they can",
    "start": "402720",
    "end": "404400"
  },
  {
    "text": "literally access the data and start",
    "start": "404400",
    "end": "406479"
  },
  {
    "text": "working with it while it's getting",
    "start": "406479",
    "end": "408000"
  },
  {
    "text": "streamed to their machine and there's no",
    "start": "408000",
    "end": "410400"
  },
  {
    "text": "any need for",
    "start": "410400",
    "end": "411360"
  },
  {
    "text": "having all these different layouts of",
    "start": "411360",
    "end": "413360"
  },
  {
    "text": "the data with",
    "start": "413360",
    "end": "414400"
  },
  {
    "text": "compression and tier records files and",
    "start": "414400",
    "end": "416479"
  },
  {
    "text": "then in the every course you have",
    "start": "416479",
    "end": "417759"
  },
  {
    "text": "protobuf files you have",
    "start": "417759",
    "end": "419280"
  },
  {
    "text": "uh then you have the each like image in",
    "start": "419280",
    "end": "422080"
  },
  {
    "text": "your pack format",
    "start": "422080",
    "end": "423919"
  },
  {
    "text": "laid down in bytes and so on so we",
    "start": "423919",
    "end": "426400"
  },
  {
    "text": "simplify all this more specifically we",
    "start": "426400",
    "end": "428400"
  },
  {
    "text": "focus on those problems",
    "start": "428400",
    "end": "430160"
  },
  {
    "text": "where um you see that like essentially",
    "start": "430160",
    "end": "433520"
  },
  {
    "text": "when you're trying to train or deploy a",
    "start": "433520",
    "end": "435840"
  },
  {
    "text": "machining model or you want to do a",
    "start": "435840",
    "end": "437280"
  },
  {
    "text": "compute",
    "start": "437280",
    "end": "437919"
  },
  {
    "text": "you want to make sure that data is local",
    "start": "437919",
    "end": "440400"
  },
  {
    "text": "so you want to make sure that you don't",
    "start": "440400",
    "end": "441919"
  },
  {
    "text": "spend a lot of time on transferring",
    "start": "441919",
    "end": "443759"
  },
  {
    "text": "data from one location to another",
    "start": "443759",
    "end": "445360"
  },
  {
    "text": "location you want to make sure that",
    "start": "445360",
    "end": "447520"
  },
  {
    "text": "you own the local folder structure",
    "start": "447520",
    "end": "449199"
  },
  {
    "text": "dependency let's say you train a",
    "start": "449199",
    "end": "450639"
  },
  {
    "text": "image that only your local file system",
    "start": "450639",
    "end": "452639"
  },
  {
    "text": "and then you want to train it on like",
    "start": "452639",
    "end": "454080"
  },
  {
    "text": "on larger scale you have to copy all the",
    "start": "454080",
    "end": "456080"
  },
  {
    "text": "same kind of",
    "start": "456080",
    "end": "457199"
  },
  {
    "text": "structures otherwise you have to change",
    "start": "457199",
    "end": "458880"
  },
  {
    "text": "the code so you want to be able",
    "start": "458880",
    "end": "460639"
  },
  {
    "text": "easily without changing any line of code",
    "start": "460639",
    "end": "463039"
  },
  {
    "text": "in the",
    "start": "463039",
    "end": "463680"
  },
  {
    "text": "way you read the data to people to scale",
    "start": "463680",
    "end": "465919"
  },
  {
    "text": "to a",
    "start": "465919",
    "end": "466720"
  },
  {
    "text": "cluster and we'll get into details like",
    "start": "466720",
    "end": "468479"
  },
  {
    "text": "how you can do it actually with",
    "start": "468479",
    "end": "470319"
  },
  {
    "text": "ray at any scale and then the another",
    "start": "470319",
    "end": "473280"
  },
  {
    "text": "question",
    "start": "473280",
    "end": "473840"
  },
  {
    "text": "very very relevant to this topic as well",
    "start": "473840",
    "end": "476560"
  },
  {
    "text": "is building this",
    "start": "476560",
    "end": "477440"
  },
  {
    "text": "processing pipelines where you have to",
    "start": "477440",
    "end": "480000"
  },
  {
    "text": "take care of",
    "start": "480000",
    "end": "481120"
  },
  {
    "text": "okay you have a data set you have to",
    "start": "481120",
    "end": "482800"
  },
  {
    "text": "apply a lot of transformations",
    "start": "482800",
    "end": "484639"
  },
  {
    "text": "how what is the best way to run radar",
    "start": "484639",
    "end": "487039"
  },
  {
    "text": "transformations keep track of them",
    "start": "487039",
    "end": "489039"
  },
  {
    "text": "version control and then plug this into",
    "start": "489039",
    "end": "490720"
  },
  {
    "text": "machine learning outlets",
    "start": "490720",
    "end": "493199"
  },
  {
    "text": "if you're working with a team you have",
    "start": "493199",
    "end": "494879"
  },
  {
    "text": "to make sure that the data is accessible",
    "start": "494879",
    "end": "496479"
  },
  {
    "text": "to your team members",
    "start": "496479",
    "end": "497680"
  },
  {
    "text": "and they can easily plug into or combine",
    "start": "497680",
    "end": "499840"
  },
  {
    "text": "their data sets together to",
    "start": "499840",
    "end": "501360"
  },
  {
    "text": "train a model let's say you just want to",
    "start": "501360",
    "end": "503599"
  },
  {
    "text": "access a specific data set",
    "start": "503599",
    "end": "505199"
  },
  {
    "text": "and then or a part of the data set",
    "start": "505199",
    "end": "507280"
  },
  {
    "text": "itself you can you want you don't want",
    "start": "507280",
    "end": "509120"
  },
  {
    "text": "to unzip all the files or you don't want",
    "start": "509120",
    "end": "510720"
  },
  {
    "text": "to just leave read",
    "start": "510720",
    "end": "511919"
  },
  {
    "text": "linearly alt here record files just to",
    "start": "511919",
    "end": "514080"
  },
  {
    "text": "get a piece of it",
    "start": "514080",
    "end": "515839"
  },
  {
    "text": "and of course there's like the issues",
    "start": "515839",
    "end": "517279"
  },
  {
    "text": "with in-memory management",
    "start": "517279",
    "end": "519599"
  },
  {
    "text": "and visualization we do also provide a",
    "start": "519599",
    "end": "522640"
  },
  {
    "text": "visualization tool that helps you to",
    "start": "522640",
    "end": "524159"
  },
  {
    "text": "instantly",
    "start": "524159",
    "end": "524880"
  },
  {
    "text": "see the data and the 2.0 visualization",
    "start": "524880",
    "end": "527519"
  },
  {
    "text": "is soon coming that gonna",
    "start": "527519",
    "end": "529120"
  },
  {
    "text": "have mind-blowing visualization uh here",
    "start": "529120",
    "end": "532000"
  },
  {
    "text": "you can see that you can easily see all",
    "start": "532000",
    "end": "533600"
  },
  {
    "text": "the data in the image formats that",
    "start": "533600",
    "end": "535440"
  },
  {
    "text": "we do prevent and more importantly",
    "start": "535440",
    "end": "538640"
  },
  {
    "text": "uh from the developer perspective",
    "start": "538640",
    "end": "540800"
  },
  {
    "text": "there's no need for writing these four",
    "start": "540800",
    "end": "542560"
  },
  {
    "text": "lines of code",
    "start": "542560",
    "end": "543600"
  },
  {
    "text": "just reading here uh facial name is data",
    "start": "543600",
    "end": "546320"
  },
  {
    "text": "set like fascia and this is like a very",
    "start": "546320",
    "end": "548080"
  },
  {
    "text": "small like 50 megabyte",
    "start": "548080",
    "end": "549600"
  },
  {
    "text": "data set uh and you have to write all",
    "start": "549600",
    "end": "551839"
  },
  {
    "text": "this boilerplate code just to get the",
    "start": "551839",
    "end": "553440"
  },
  {
    "text": "images and labels to stream",
    "start": "553440",
    "end": "554959"
  },
  {
    "text": "to the training process and we want to",
    "start": "554959",
    "end": "556800"
  },
  {
    "text": "automate this whole part",
    "start": "556800",
    "end": "558240"
  },
  {
    "text": "where data scientists don't write more",
    "start": "558240",
    "end": "560560"
  },
  {
    "text": "than that besides just simplicity or",
    "start": "560560",
    "end": "563040"
  },
  {
    "text": "easy of use we also focus on the",
    "start": "563040",
    "end": "565519"
  },
  {
    "text": "performance",
    "start": "565519",
    "end": "566560"
  },
  {
    "text": "we did compare us with the tensorflow",
    "start": "566560",
    "end": "568399"
  },
  {
    "text": "datasets uh plus ignite server",
    "start": "568399",
    "end": "570560"
  },
  {
    "text": "so what tensorflow dataset does is like",
    "start": "570560",
    "end": "573040"
  },
  {
    "text": "it's a",
    "start": "573040",
    "end": "574160"
  },
  {
    "text": "way to load the data sets then you have",
    "start": "574160",
    "end": "576160"
  },
  {
    "text": "an ignite which keeps the data set in",
    "start": "576160",
    "end": "577839"
  },
  {
    "text": "memory in a cluster",
    "start": "577839",
    "end": "579120"
  },
  {
    "text": "so they tried to run the full length two",
    "start": "579120",
    "end": "580720"
  },
  {
    "text": "benchmarks where they have ignite",
    "start": "580720",
    "end": "583040"
  },
  {
    "text": "running um on the same machine in memory",
    "start": "583040",
    "end": "586000"
  },
  {
    "text": "and then you can achieve like",
    "start": "586000",
    "end": "587360"
  },
  {
    "text": "875 megabytes per second to tensorflow",
    "start": "587360",
    "end": "590880"
  },
  {
    "text": "to the gpu",
    "start": "590880",
    "end": "592399"
  },
  {
    "text": "and what you can do else you can take",
    "start": "592399",
    "end": "594160"
  },
  {
    "text": "the ignite and then put it on a remote",
    "start": "594160",
    "end": "595760"
  },
  {
    "text": "cluster",
    "start": "595760",
    "end": "596399"
  },
  {
    "text": "where the machine doesn't own the",
    "start": "596399",
    "end": "598080"
  },
  {
    "text": "dataset internally but it can stream",
    "start": "598080",
    "end": "599760"
  },
  {
    "text": "over the network to the",
    "start": "599760",
    "end": "601040"
  },
  {
    "text": "gpu train and you achieve 800 uh with",
    "start": "601040",
    "end": "604160"
  },
  {
    "text": "hub",
    "start": "604160",
    "end": "605360"
  },
  {
    "text": "and we what we can achieve is that we",
    "start": "605360",
    "end": "607120"
  },
  {
    "text": "can achieve",
    "start": "607120",
    "end": "608600"
  },
  {
    "text": "875 megabyte per second",
    "start": "608600",
    "end": "611680"
  },
  {
    "text": "network utilization to the gpu as if the",
    "start": "611680",
    "end": "614720"
  },
  {
    "text": "data was local to the machine",
    "start": "614720",
    "end": "616320"
  },
  {
    "text": "but without any in-memory database and",
    "start": "616320",
    "end": "618720"
  },
  {
    "text": "with 20 times",
    "start": "618720",
    "end": "619680"
  },
  {
    "text": "cheaper cost as you would expect",
    "start": "619680",
    "end": "622959"
  },
  {
    "text": "because there is no data memory stored",
    "start": "622959",
    "end": "624959"
  },
  {
    "text": "and you can actually scale this to",
    "start": "624959",
    "end": "626160"
  },
  {
    "text": "multiple machines at the same time",
    "start": "626160",
    "end": "628160"
  },
  {
    "text": "and then stream this data to each of",
    "start": "628160",
    "end": "629440"
  },
  {
    "text": "those machines",
    "start": "629440",
    "end": "631279"
  },
  {
    "text": "just to train deploy or maybe run some",
    "start": "631279",
    "end": "634399"
  },
  {
    "text": "query on top of them",
    "start": "634399",
    "end": "636320"
  },
  {
    "text": "and here you go most importantly what we",
    "start": "636320",
    "end": "638240"
  },
  {
    "text": "are excited to share is that",
    "start": "638240",
    "end": "640000"
  },
  {
    "text": "we also did an integration with um ray",
    "start": "640000",
    "end": "642560"
  },
  {
    "text": "uh what you are able to do",
    "start": "642560",
    "end": "643839"
  },
  {
    "text": "not only able to store your data sets",
    "start": "643839",
    "end": "646560"
  },
  {
    "text": "locally but",
    "start": "646560",
    "end": "647920"
  },
  {
    "text": "and as if it's stored locally but you",
    "start": "647920",
    "end": "650480"
  },
  {
    "text": "can also",
    "start": "650480",
    "end": "651360"
  },
  {
    "text": "essentially take a transformation or",
    "start": "651360",
    "end": "653680"
  },
  {
    "text": "piece of",
    "start": "653680",
    "end": "654720"
  },
  {
    "text": "custom arbitrary code you want to apply",
    "start": "654720",
    "end": "656959"
  },
  {
    "text": "and then scale this to a cluster",
    "start": "656959",
    "end": "658880"
  },
  {
    "text": "so ray does a very good job in taking a",
    "start": "658880",
    "end": "661920"
  },
  {
    "text": "custom lambda functions or could be like",
    "start": "661920",
    "end": "664720"
  },
  {
    "text": "transformations",
    "start": "664720",
    "end": "666000"
  },
  {
    "text": "uh written how far for",
    "start": "666000",
    "end": "669600"
  },
  {
    "text": "hub essentially we take that translated",
    "start": "669600",
    "end": "672320"
  },
  {
    "text": "into our format the way we store the",
    "start": "672320",
    "end": "673839"
  },
  {
    "text": "data it's very efficient",
    "start": "673839",
    "end": "675200"
  },
  {
    "text": "to run the computations and then we",
    "start": "675200",
    "end": "677519"
  },
  {
    "text": "scale it to the cluster",
    "start": "677519",
    "end": "678959"
  },
  {
    "text": "run the computation and bring back",
    "start": "678959",
    "end": "680160"
  },
  {
    "text": "another dataset and",
    "start": "680160",
    "end": "682000"
  },
  {
    "text": "you can do this with very high",
    "start": "682000",
    "end": "683839"
  },
  {
    "text": "utilization of both cpus and gpus",
    "start": "683839",
    "end": "686240"
  },
  {
    "text": "and we did a benchmarking as well just",
    "start": "686240",
    "end": "688640"
  },
  {
    "text": "to compare the benefits of",
    "start": "688640",
    "end": "690560"
  },
  {
    "text": "having um ray plus how versus beam",
    "start": "690560",
    "end": "693600"
  },
  {
    "text": "about to be we had a use case where one",
    "start": "693600",
    "end": "696399"
  },
  {
    "text": "of our customers",
    "start": "696399",
    "end": "697760"
  },
  {
    "text": "had a single job running on apache beam",
    "start": "697760",
    "end": "700079"
  },
  {
    "text": "and did a fall",
    "start": "700079",
    "end": "700880"
  },
  {
    "text": "to have um have been spending like 500",
    "start": "700880",
    "end": "703519"
  },
  {
    "text": "cp hours",
    "start": "703519",
    "end": "704399"
  },
  {
    "text": "to process a data set transform a data",
    "start": "704399",
    "end": "706399"
  },
  {
    "text": "set into a tensorflow datasets",
    "start": "706399",
    "end": "708560"
  },
  {
    "text": "um and end up actually the data itself",
    "start": "708560",
    "end": "711200"
  },
  {
    "text": "end up with low gp usage",
    "start": "711200",
    "end": "712959"
  },
  {
    "text": "and the cost per round was around 29",
    "start": "712959",
    "end": "715040"
  },
  {
    "text": "dollars per um",
    "start": "715040",
    "end": "716480"
  },
  {
    "text": "period um the amount of the data was",
    "start": "716480",
    "end": "718800"
  },
  {
    "text": "getting i think around",
    "start": "718800",
    "end": "719680"
  },
  {
    "text": "three terabytes what we did with them is",
    "start": "719680",
    "end": "721920"
  },
  {
    "text": "that we took hub",
    "start": "721920",
    "end": "723680"
  },
  {
    "text": "uh we took ray as well we write a",
    "start": "723680",
    "end": "725440"
  },
  {
    "text": "connector to their data set originally",
    "start": "725440",
    "end": "727680"
  },
  {
    "text": "and then scale this to a cluster of aws",
    "start": "727680",
    "end": "730720"
  },
  {
    "text": "machines",
    "start": "730720",
    "end": "731920"
  },
  {
    "text": "and what we achieved is like around five",
    "start": "731920",
    "end": "734480"
  },
  {
    "text": "times better performance",
    "start": "734480",
    "end": "736240"
  },
  {
    "text": "and better cost as well and doing the",
    "start": "736240",
    "end": "738480"
  },
  {
    "text": "same amount of job with 100 cpu hours",
    "start": "738480",
    "end": "741200"
  },
  {
    "text": "with and like the data set that's ended",
    "start": "741200",
    "end": "744240"
  },
  {
    "text": "up",
    "start": "744240",
    "end": "745279"
  },
  {
    "text": "being output of this machine can have",
    "start": "745279",
    "end": "748000"
  },
  {
    "text": "been like very highly utilized with the",
    "start": "748000",
    "end": "749680"
  },
  {
    "text": "gpu",
    "start": "749680",
    "end": "750399"
  },
  {
    "text": "to train the model uh because of the",
    "start": "750399",
    "end": "752560"
  },
  {
    "text": "format and the layout over there",
    "start": "752560",
    "end": "754959"
  },
  {
    "text": "and very excited about this like kind of",
    "start": "754959",
    "end": "757279"
  },
  {
    "text": "partnership here",
    "start": "757279",
    "end": "759120"
  },
  {
    "text": "and this this technology itself can be",
    "start": "759120",
    "end": "761360"
  },
  {
    "text": "used for",
    "start": "761360",
    "end": "762160"
  },
  {
    "text": "a lot of use cases that we have seen so",
    "start": "762160",
    "end": "764160"
  },
  {
    "text": "far we have worked with the",
    "start": "764160",
    "end": "766079"
  },
  {
    "text": "our partners in toner they have this",
    "start": "766079",
    "end": "768240"
  },
  {
    "text": "better by scale data sets",
    "start": "768240",
    "end": "770399"
  },
  {
    "text": "we work with a legal tech company we",
    "start": "770399",
    "end": "772160"
  },
  {
    "text": "also work with another customer of ours",
    "start": "772160",
    "end": "774800"
  },
  {
    "text": "that help them to optimize the data",
    "start": "774800",
    "end": "777200"
  },
  {
    "text": "processing",
    "start": "777200",
    "end": "778079"
  },
  {
    "text": "for their team of 50 data scientists",
    "start": "778079",
    "end": "780240"
  },
  {
    "text": "working together",
    "start": "780240",
    "end": "782240"
  },
  {
    "text": "and we also seen our open source tool",
    "start": "782240",
    "end": "785360"
  },
  {
    "text": "growing and building a computer vision",
    "start": "785360",
    "end": "789519"
  },
  {
    "text": "basically and",
    "start": "789519",
    "end": "790800"
  },
  {
    "text": "not only a community but where we would",
    "start": "790800",
    "end": "793760"
  },
  {
    "text": "love to welcome and want to join us and",
    "start": "793760",
    "end": "795440"
  },
  {
    "text": "help us building data 2.0",
    "start": "795440",
    "end": "797839"
  },
  {
    "text": "which is a new way for um unstructured",
    "start": "797839",
    "end": "800160"
  },
  {
    "text": "data sets and then streaming this to",
    "start": "800160",
    "end": "801839"
  },
  {
    "text": "machine learning platforms and very well",
    "start": "801839",
    "end": "803440"
  },
  {
    "text": "integrated with ray to run all those",
    "start": "803440",
    "end": "805760"
  },
  {
    "text": "computations at scale",
    "start": "805760",
    "end": "807600"
  },
  {
    "text": "uh we essentially just",
    "start": "807600",
    "end": "810880"
  },
  {
    "text": "couple notes that what i would love to",
    "start": "810880",
    "end": "813680"
  },
  {
    "text": "give a takeaway",
    "start": "813680",
    "end": "814959"
  },
  {
    "text": "with you it's like it really takes a lot",
    "start": "814959",
    "end": "816720"
  },
  {
    "text": "of time to connect to uncertainty data",
    "start": "816720",
    "end": "818880"
  },
  {
    "text": "and train machine learning models there",
    "start": "818880",
    "end": "820959"
  },
  {
    "text": "isn't actually widely adopted tools for",
    "start": "820959",
    "end": "822959"
  },
  {
    "text": "storing your processing uncertainty that",
    "start": "822959",
    "end": "824399"
  },
  {
    "text": "you have a lot of databases data",
    "start": "824399",
    "end": "825600"
  },
  {
    "text": "warehouse it",
    "start": "825600",
    "end": "826240"
  },
  {
    "text": "likes they're not very much very well",
    "start": "826240",
    "end": "828639"
  },
  {
    "text": "suited for deep learning applications",
    "start": "828639",
    "end": "831120"
  },
  {
    "text": "and with data 2.0 we want to structure",
    "start": "831120",
    "end": "834079"
  },
  {
    "text": "all your unstructured data sets",
    "start": "834079",
    "end": "835600"
  },
  {
    "text": "in a very simple and unified way so love",
    "start": "835600",
    "end": "838000"
  },
  {
    "text": "to if you can just run pip install hub",
    "start": "838000",
    "end": "839920"
  },
  {
    "text": "or switch to data 2.0",
    "start": "839920",
    "end": "842079"
  },
  {
    "text": "and join our community as well",
    "start": "842079",
    "end": "845839"
  },
  {
    "text": "let me know if you have any questions",
    "start": "845839",
    "end": "847279"
  },
  {
    "text": "feel free to reach me out",
    "start": "847279",
    "end": "850240"
  },
  {
    "text": "or over the same email or you also have",
    "start": "850240",
    "end": "852320"
  },
  {
    "text": "my personal email",
    "start": "852320",
    "end": "853360"
  },
  {
    "text": "and twitter account",
    "start": "853360",
    "end": "856720"
  },
  {
    "text": "feel free to contact us at any point and",
    "start": "856720",
    "end": "859440"
  },
  {
    "text": "we'll love to get into details and see",
    "start": "859440",
    "end": "861040"
  },
  {
    "text": "where we can be helpful",
    "start": "861040",
    "end": "866959"
  }
]