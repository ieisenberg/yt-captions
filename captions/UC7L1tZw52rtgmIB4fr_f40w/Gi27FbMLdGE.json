[
  {
    "text": "it's your time hello everybody uh thank you for being here so today we're going to uh talk how to leverage r and v to",
    "start": "2879",
    "end": "11400"
  },
  {
    "text": "for the Gen B prediction at Uber uh this presentation prepared by B Jun that's",
    "start": "11400",
    "end": "17480"
  },
  {
    "text": "myself um Bing and not withas we are all um software engineer at Uber mangelo",
    "start": "17480",
    "end": "25279"
  },
  {
    "text": "mangelo is Uber's unified ml platform so provide end to end uh machine learning",
    "start": "25279",
    "end": "31039"
  },
  {
    "text": "application support from like um data preparation F engineering model training",
    "start": "31039",
    "end": "37200"
  },
  {
    "text": "evaluation model deployment for experiment and production like prediction monitoring R everything so in",
    "start": "37200",
    "end": "44399"
  },
  {
    "text": "the past a few years actually ma have been have been evolving from like a",
    "start": "44399",
    "end": "50000"
  },
  {
    "text": "boost model to the deep deep learning model since last year um since the Gen",
    "start": "50000",
    "end": "55680"
  },
  {
    "text": "technology emergence uh maon have been actively develop veling the tools and",
    "start": "55680",
    "end": "61559"
  },
  {
    "text": "pipeline to support gen based application so turn into a gen",
    "start": "61559",
    "end": "67000"
  },
  {
    "text": "platform so uh we have a bunch of tools like LX is for example is a lunch based",
    "start": "67000",
    "end": "73360"
  },
  {
    "text": "tool to actually help the user to easily develop um a GI based application we",
    "start": "73360",
    "end": "79799"
  },
  {
    "text": "have the GI Gateway Bally provide a uni unified interface to user to Calla the",
    "start": "79799",
    "end": "86040"
  },
  {
    "text": "like models like third party El model for open eye Germany or it's a self Hol",
    "start": "86040",
    "end": "92479"
  },
  {
    "text": "model we also have the Safeguard and Pi detection um basically for the um",
    "start": "92479",
    "end": "98560"
  },
  {
    "text": "security and privacy we have a um M Studio provide um UI basically for",
    "start": "98560",
    "end": "106880"
  },
  {
    "text": "people oh uh to select the mod they want to use prefer the DAT like for R application uh",
    "start": "106880",
    "end": "114320"
  },
  {
    "text": "prompt engineering and model fan tuning monitoring for the performance safety",
    "start": "114320",
    "end": "120439"
  },
  {
    "text": "cost their so uh today we talk about batch prediction so batch prediction is U",
    "start": "120439",
    "end": "127640"
  },
  {
    "text": "basically use cases you if you don't need a real time uh application you don't need a realtime",
    "start": "127640",
    "end": "134840"
  },
  {
    "text": "response if you need that you probably need to deploy it for in the point you can get instant response but for patch",
    "start": "134840",
    "end": "141920"
  },
  {
    "text": "prediction usually you have a um like a data site you have a model either raw model fanun model so you run inference",
    "start": "141920",
    "end": "150040"
  },
  {
    "text": "get result in put into um database or somewhere so for our our use um we we um",
    "start": "150040",
    "end": "157720"
  },
  {
    "text": "we we use it for like offline scoring jobs typically this type of job we",
    "start": "157720",
    "end": "163200"
  },
  {
    "text": "procise like 1 million to 1 billion uh rows it's like oh we call a prom for Tech generation cases this case usually",
    "start": "163200",
    "end": "170640"
  },
  {
    "text": "take like 1,000 to 5,000 input tokens output token us a couple hundred to like",
    "start": "170640",
    "end": "176959"
  },
  {
    "text": "1,000 range that's a typical job we need to handle though so basically 1 million like like like maybe several several",
    "start": "176959",
    "end": "184480"
  },
  {
    "text": "million 50 million that's uh we want to finish in Daily that's basically a problem we trying to resolve",
    "start": "184480",
    "end": "190440"
  },
  {
    "text": "here so give example like how how how we use it and why we need it for example",
    "start": "190440",
    "end": "196400"
  },
  {
    "text": "like for user store tagging for example we Uber we got Uber get a dealer data",
    "start": "196400",
    "end": "202040"
  },
  {
    "text": "like you have a store for example restaurant we can M to taget this what kind of restaurant is this is like",
    "start": "202040",
    "end": "208120"
  },
  {
    "text": "American food Asian food of stuff we want do the uh do the search we we can U",
    "start": "208120",
    "end": "214120"
  },
  {
    "text": "Pull the search result for a search evaluation and get the quality of searching like we have a document you",
    "start": "214120",
    "end": "220400"
  },
  {
    "text": "want to do like document summarization uh characterization uh like labeling like",
    "start": "220400",
    "end": "226840"
  },
  {
    "text": "for document we can do doc injection get embedding and and uh saving Vector DB so",
    "start": "226840",
    "end": "233959"
  },
  {
    "text": "later we can for retrieval so basically that's a user cases so just to give us some comparison the the par prediction",
    "start": "233959",
    "end": "242280"
  },
  {
    "text": "om for compared to traditional machine learning is um quite some difference is",
    "start": "242280",
    "end": "247599"
  },
  {
    "text": "a for model size is U is much larger for example our um traditional machine",
    "start": "247599",
    "end": "252760"
  },
  {
    "text": "learning typically is in couple megabytes but for larg langage model goes to like 10 GB like 1,000 GB for",
    "start": "252760",
    "end": "259479"
  },
  {
    "text": "example the Llama 3.45 GB even use ip8 you got 800gb the L device uh typically",
    "start": "259479",
    "end": "266440"
  },
  {
    "text": "we control our um lot of model like ETA model model is less than 10 Mond lency",
    "start": "266440",
    "end": "272199"
  },
  {
    "text": "but for lar model it typically can go to like seconds depends on how many tokens you want it can do tens of seconds",
    "start": "272199",
    "end": "279039"
  },
  {
    "text": "throughput used to you can handle like 100 to thousand prediction per seconds",
    "start": "279039",
    "end": "284680"
  },
  {
    "text": "and for logic model you can only get a couple a couple prediction per second",
    "start": "284680",
    "end": "289960"
  },
  {
    "text": "for serving um the you can use to use a CPU or use one GPU with no not a lot of",
    "start": "289960",
    "end": "296360"
  },
  {
    "text": "memory but for the lar G model typically need like multiple gpus and you need a",
    "start": "296360",
    "end": "302919"
  },
  {
    "text": "like a lot of memories to able to host the model in general it's it's a the L",
    "start": "302919",
    "end": "310199"
  },
  {
    "text": "model back prediction was a pretty pretty costly compared to the traditional model like example for the",
    "start": "310199",
    "end": "316199"
  },
  {
    "text": "make sure 8 * 7B model uh you're going to need like a two like 00 h80 GB memory",
    "start": "316199",
    "end": "323160"
  },
  {
    "text": "to host it uh if like one second let see if you don't have a good batching mechanism let's say you process like a m",
    "start": "323160",
    "end": "330759"
  },
  {
    "text": "request per second throughput or typical job we handle like 50 million prompt you",
    "start": "330759",
    "end": "336160"
  },
  {
    "text": "going to this many GPR so it's a it's a prohibitive expensive and you cannot",
    "start": "336160",
    "end": "341759"
  },
  {
    "text": "finish basically on time certainly you need a distributed system to handle it",
    "start": "341759",
    "end": "347400"
  },
  {
    "text": "so um to highlight of the challenges so why is the we mentioned the large large large model size basically you're going",
    "start": "347400",
    "end": "354120"
  },
  {
    "text": "to put some resource uh pressure one is to save them another is you download the",
    "start": "354120",
    "end": "359800"
  },
  {
    "text": "model so put some Network bandas issue because when you uh distribute system",
    "start": "359800",
    "end": "365440"
  },
  {
    "text": "you download them C uh concurrently so basically need a band otherwise it going",
    "start": "365440",
    "end": "371000"
  },
  {
    "text": "take a long time to download some uh is high lency and low low throughput",
    "start": "371000",
    "end": "376240"
  },
  {
    "text": "basically need to algorithm to optimize it you need like effective batching to",
    "start": "376240",
    "end": "381599"
  },
  {
    "text": "increase the throughput and then you have a try to get a good GP utilization toal cost the serving certainly in this",
    "start": "381599",
    "end": "388680"
  },
  {
    "text": "distributed system otherwi you cannot finish basic on time so you need a GPU",
    "start": "388680",
    "end": "393759"
  },
  {
    "text": "serving um you need to B node node level attration you also need in know kind of",
    "start": "393759",
    "end": "400880"
  },
  {
    "text": "transfer paralyzation to man to manage like mod than one GPU so in general to",
    "start": "400880",
    "end": "406360"
  },
  {
    "text": "basically you want a batching system to be scalable performance reliable so scalable so you can easily do the hard",
    "start": "406360",
    "end": "414000"
  },
  {
    "text": "scale to handle the large job then your performance so basic you get a good GP utilization low cost effective batching",
    "start": "414000",
    "end": "421440"
  },
  {
    "text": "reliable so basically you don't actually uh get a job lost you know in the long",
    "start": "421440",
    "end": "427599"
  },
  {
    "text": "hours so here really basically is a the good tool to help us here you can help",
    "start": "427599",
    "end": "433120"
  },
  {
    "text": "you easily scale the job from like one too many uh you have a good GPU support",
    "start": "433120",
    "end": "439000"
  },
  {
    "text": "to do effective uh GPU management either um node level or something or is a ml",
    "start": "439000",
    "end": "445680"
  },
  {
    "text": "tune so you have the good grity and flexibilities you can user contr show function this is pretty helpful for the",
    "start": "445680",
    "end": "452160"
  },
  {
    "text": "large language model because a lot of the implementation is not so standardized so um it's also if you um",
    "start": "452160",
    "end": "459080"
  },
  {
    "text": "you cluster is on the kubernetes kubernetes is is pretty really is working pretty well on it for um our",
    "start": "459080",
    "end": "466840"
  },
  {
    "text": "system we are using kuber um we can use uh we have our um unified API and the",
    "start": "466840",
    "end": "473800"
  },
  {
    "text": "job controller basic to control how we um if we start manage a cluster so we",
    "start": "473800",
    "end": "479680"
  },
  {
    "text": "can start to R and on it and we can run the prediction so for GPU we have a100",
    "start": "479680",
    "end": "485759"
  },
  {
    "text": "we have h100 h100 each node we have 8 GPU then each have 8GB memory then",
    "start": "485759",
    "end": "492199"
  },
  {
    "text": "connected by m link so to get a a good fast",
    "start": "492199",
    "end": "497800"
  },
  {
    "text": "communication so how do we control the like a reach out for um so typical flow",
    "start": "497800",
    "end": "504080"
  },
  {
    "text": "is you have like uh you start to Define your reob spe using yamama or something",
    "start": "504080",
    "end": "510039"
  },
  {
    "text": "then you can um basically Define a re job then you create a cluster like",
    "start": "510039",
    "end": "516360"
  },
  {
    "text": "control like how many um how many notes you need like how many uh gpus or CPUs on it so once the cluster is ready you",
    "start": "516360",
    "end": "523560"
  },
  {
    "text": "can submit a re job uh this can be done with a we have the API basic W re R API",
    "start": "523560",
    "end": "531160"
  },
  {
    "text": "to manage the re job then can Vis re job to complete and get relock and result",
    "start": "531160",
    "end": "536440"
  },
  {
    "text": "then you can short down the the cluster so the the reob submission you can submit like a a command to the rehad",
    "start": "536440",
    "end": "544240"
  },
  {
    "text": "then basically you have a script located on the rehad then this script should be",
    "start": "544240",
    "end": "549399"
  },
  {
    "text": "capable to actually able to manage the job underway to basically expand skill your job from head to worker then you",
    "start": "549399",
    "end": "556600"
  },
  {
    "text": "can specify where to pull the data while to get model you specify the number of worker and gpus number inment how to",
    "start": "556600",
    "end": "564680"
  },
  {
    "text": "manage result in this script as the way so that's how typically how we handle the re",
    "start": "564680",
    "end": "570240"
  },
  {
    "text": "job so the r basically provides some interactive Dev environment you can use",
    "start": "570240",
    "end": "576079"
  },
  {
    "text": "a jupit notebook to actually um do the code development and testing you can",
    "start": "576079",
    "end": "582040"
  },
  {
    "text": "also submit the job manually to do like testing before you are putting the peline so you also have the dashboard to",
    "start": "582040",
    "end": "589240"
  },
  {
    "text": "access a log monitoring like the GPU matric",
    "start": "589240",
    "end": "594519"
  },
  {
    "text": "everything uh here is basically uh straightforward um implement ation",
    "start": "594519",
    "end": "600000"
  },
  {
    "text": "basically typically like for you you have like online prediction you have something set up for inference to one to",
    "start": "600000",
    "end": "606560"
  },
  {
    "text": "one node so you can use R to actually expand into motive node this is a",
    "start": "606560",
    "end": "612399"
  },
  {
    "text": "straightforward um implementation so good s is often you need the online",
    "start": "612399",
    "end": "617720"
  },
  {
    "text": "offline consistency so basically you spend through this you for the offline you got the same same result as your",
    "start": "617720",
    "end": "624160"
  },
  {
    "text": "your um offline got the same as the online so basically each of the",
    "start": "624160",
    "end": "629880"
  },
  {
    "text": "um Rew worker or re had you're going to um handles the like working like a",
    "start": "629880",
    "end": "636160"
  },
  {
    "text": "independent node to get the model get the input and get the output so you probably need a leverage rate to do some",
    "start": "636160",
    "end": "643279"
  },
  {
    "text": "partition then um result aggregation but every actor like acting like individual",
    "start": "643279",
    "end": "649440"
  },
  {
    "text": "like a single node just like the online to for the online offline",
    "start": "649440",
    "end": "654959"
  },
  {
    "text": "consistency so um so here we can show the some uh quick how we can implement",
    "start": "654959",
    "end": "661120"
  },
  {
    "text": "this you can set up like actor pool and uh basically like uh like a number",
    "start": "661120",
    "end": "667440"
  },
  {
    "text": "actors you need and number GPU you need then you can Define the actor call for prediction job so Bic say you want to",
    "start": "667440",
    "end": "673920"
  },
  {
    "text": "call um like what you going to run for each of the actor this one is something like a scoring you're going to Define it",
    "start": "673920",
    "end": "680959"
  },
  {
    "text": "then you can trigger it then you can pull the result to the to the rehad you can also",
    "start": "680959",
    "end": "686760"
  },
  {
    "text": "choose to upload the result in each of the the node you want to this are basically a straightforward the the good",
    "start": "686760",
    "end": "693760"
  },
  {
    "text": "thing is is I mentioned is online offline consistency Bic you just suspend your uh single single node prediction to",
    "start": "693760",
    "end": "701120"
  },
  {
    "text": "many and every node doing what is what doing for like like",
    "start": "701120",
    "end": "707040"
  },
  {
    "text": "online um so prediction on each of the uh each of the actor basically you you",
    "start": "707040",
    "end": "712880"
  },
  {
    "text": "handle the your your own way to model download dat download you you can do",
    "start": "712880",
    "end": "718120"
  },
  {
    "text": "some sharing or not Shar sharing uh you you run your individual inference server for examp vom this basically we use it",
    "start": "718120",
    "end": "724959"
  },
  {
    "text": "for Tex generation then we can also use a Trent server like for handle safal like",
    "start": "724959",
    "end": "731920"
  },
  {
    "text": "embeding uh the inference server uh they provide a different type of API for",
    "start": "731920",
    "end": "738000"
  },
  {
    "text": "example uh we provide a nice like U um inise API for the batch prediction so",
    "start": "738000",
    "end": "745399"
  },
  {
    "text": "basically you can work like a function call like a batch prediction batch in batch out you get it's very easy for",
    "start": "745399",
    "end": "751760"
  },
  {
    "text": "integration but a lot of server um nowadays they provide like a separate",
    "start": "751760",
    "end": "757399"
  },
  {
    "text": "process either as a separate process or is you can use a side car in a container then use the HTTP or JPC client to make",
    "start": "757399",
    "end": "764760"
  },
  {
    "text": "the prediction either uh synchronized or async so the way you run you start a",
    "start": "764760",
    "end": "770279"
  },
  {
    "text": "server start a client you the client send a request and get result back you can sh down the the server um for triton",
    "start": "770279",
    "end": "777959"
  },
  {
    "text": "we use in this way for offline also basically uh Mayim make this prise for",
    "start": "777959",
    "end": "783360"
  },
  {
    "text": "for online um V also provides SAR API but since they have those um convenient API",
    "start": "783360",
    "end": "791199"
  },
  {
    "text": "so we use it this way uh another thing to mention the GPU par paradism because you need to handle",
    "start": "791199",
    "end": "797880"
  },
  {
    "text": "multi GPU there's a many open source Solutions can handle this nicely for",
    "start": "797880",
    "end": "803360"
  },
  {
    "text": "like a deep speed deep speed usually people use it for training but you can also use for inference but inference",
    "start": "803360",
    "end": "808880"
  },
  {
    "text": "some point is a easier case for for training then you can use NPI M prise uh",
    "start": "808880",
    "end": "815040"
  },
  {
    "text": "this tens tlm actually use this way another way is using Ray using Ray",
    "start": "815040",
    "end": "820360"
  },
  {
    "text": "actually um the V actually have a way to use Ray to manage the M GPU on the",
    "start": "820360",
    "end": "827800"
  },
  {
    "text": "Node uh the next we going to um mostly talk about how how we uh leverage V and",
    "start": "827800",
    "end": "834880"
  },
  {
    "text": "the rate to actually Implement it's a basic efficient uh batch prediction also",
    "start": "834880",
    "end": "840800"
  },
  {
    "text": "save a lot of code uh for V basically is you probably already familiar is in the",
    "start": "840800",
    "end": "845839"
  },
  {
    "text": "past presentation is a efficient scalable am batch batch prediction inference it part the high throughput is",
    "start": "845839",
    "end": "853440"
  },
  {
    "text": "effective batching KV cash is has a page pred page attention for the",
    "start": "853440",
    "end": "859839"
  },
  {
    "text": "memory to efficient map the memory to reduce memory fragmentation uh is",
    "start": "859839",
    "end": "865440"
  },
  {
    "text": "scalable is handle the mot GPU inference pretty well it's a pretty easy and convenient and it has a v Spectrum model",
    "start": "865440",
    "end": "873440"
  },
  {
    "text": "support so um basically these are I showing the API basically is a simple uh",
    "start": "873440",
    "end": "879959"
  },
  {
    "text": "inise batch prediction API then uh it's very easy to load model you can download",
    "start": "879959",
    "end": "886160"
  },
  {
    "text": "the the the model vas from hugen face and just point the pass to it then you",
    "start": "886160",
    "end": "891759"
  },
  {
    "text": "can load the model and for the tensor parallel is have a simple configuration",
    "start": "891759",
    "end": "897320"
  },
  {
    "text": "just specify the tensor parallel size and tell how many GPU you need to use going to find GPU and load the model uh",
    "start": "897320",
    "end": "904240"
  },
  {
    "text": "it pretty efficient is for memory wise then you can uh use this simple API to",
    "start": "904240",
    "end": "910600"
  },
  {
    "text": "uh for example the text generation so um so for the batch",
    "start": "910600",
    "end": "916880"
  },
  {
    "text": "prediction um the r data actually is is pretty convenient to use uh it has a",
    "start": "916880",
    "end": "922440"
  },
  {
    "text": "streaming execution with read data and they have concurrent reading and prediction writing so you can use map",
    "start": "922440",
    "end": "929880"
  },
  {
    "text": "batch the API basically um you um you can pass a",
    "start": "929880",
    "end": "937199"
  },
  {
    "text": "VM as a like a predictor running on each reactor then can pass the like the",
    "start": "937199",
    "end": "943319"
  },
  {
    "text": "configuration model pass the how many GPU you need and those Tech generation",
    "start": "943319",
    "end": "949279"
  },
  {
    "text": "parameter like temperature top P Max token as the argument you can pass through then going to pass through this",
    "start": "949279",
    "end": "955440"
  },
  {
    "text": "predictor each reactor then can use this concurrent concurrency to control like",
    "start": "955440",
    "end": "960680"
  },
  {
    "text": "how many actors you need for example you run 10 or 20 um B based on your job size",
    "start": "960680",
    "end": "967680"
  },
  {
    "text": "uh the batch size you can specify when the streaming is going to basically split uh your your your data into chunks",
    "start": "967680",
    "end": "974839"
  },
  {
    "text": "so pass of the reactor they also let you to uh set a resource um res can to to",
    "start": "974839",
    "end": "982240"
  },
  {
    "text": "actually set your scheduling policy so you want to use um strier pack I think",
    "start": "982240",
    "end": "988560"
  },
  {
    "text": "this consumption V and prefer so basically you can pack your actors on",
    "start": "988560",
    "end": "994639"
  },
  {
    "text": "each of those no that you you actually uh specify it um so here we shows like",
    "start": "994639",
    "end": "1003000"
  },
  {
    "text": "uh we we may be quite um um optimize implementation because compared to the",
    "start": "1003000",
    "end": "1009800"
  },
  {
    "text": "original vcation are optimized for B prediction so instead of um in the pr",
    "start": "1009800",
    "end": "1016519"
  },
  {
    "text": "implementation we actually claim the node with the necessary resources for",
    "start": "1016519",
    "end": "1022040"
  },
  {
    "text": "example we need two GPU we claim two GPU on each node but in here the case we we",
    "start": "1022040",
    "end": "1027640"
  },
  {
    "text": "basically uh claim the whole node example if you have four GPU claim all",
    "start": "1027640",
    "end": "1033280"
  },
  {
    "text": "the four of them if you have eight eight of them so the reason we do this to avoid the GPU fragmentation because a",
    "start": "1033280",
    "end": "1041558"
  },
  {
    "text": "lot of times the B prediction you share the the pool with the training pool for training a lot of times they want the",
    "start": "1041559",
    "end": "1048038"
  },
  {
    "text": "the whole node because they need more GPU and more resources if we use a badge prediction",
    "start": "1048039",
    "end": "1053760"
  },
  {
    "text": "we claim the partial of GPU Department won't able to use the rest of resource for training so uh this way we CL we",
    "start": "1053760",
    "end": "1061240"
  },
  {
    "text": "claim the whole node then but we split into a different group with reactors to",
    "start": "1061240",
    "end": "1066880"
  },
  {
    "text": "claim like the number of gpus Need For example is a make sure in the 2 GPU we just put 2 GPU on each of the the actor",
    "start": "1066880",
    "end": "1074679"
  },
  {
    "text": "this pretty easy to specify and ear you can set up the tensor paralyze how many gpus you need going to actually",
    "start": "1074679",
    "end": "1081679"
  },
  {
    "text": "locate the number of GPU for you for the data downloading because PR implantation",
    "start": "1081679",
    "end": "1088000"
  },
  {
    "text": "we download the data for each of the reactor but here we um download only",
    "start": "1088000",
    "end": "1093280"
  },
  {
    "text": "three had and use a streaming to actually stream to each of the the the",
    "start": "1093280",
    "end": "1100440"
  },
  {
    "text": "reactor um for the for the for the model uh basically um",
    "start": "1100440",
    "end": "1108919"
  },
  {
    "text": "we we used to download the model for um every to um um every um uh actor so each",
    "start": "1108919",
    "end": "1117799"
  },
  {
    "text": "got each copy but since I mentioned for the model since a large size so um one",
    "start": "1117799",
    "end": "1123400"
  },
  {
    "text": "is take resources another um another is uh is also um when you download for",
    "start": "1123400",
    "end": "1131720"
  },
  {
    "text": "example you have many of them you can download concurrently you're going to post like Network pressure to to to your",
    "start": "1131720",
    "end": "1138400"
  },
  {
    "text": "system so basically here we we we download the model basically in",
    "start": "1138400",
    "end": "1144520"
  },
  {
    "text": "um on the same not we download one copy so if you have hpu uh you're going to",
    "start": "1144520",
    "end": "1149840"
  },
  {
    "text": "share all all those model one copy so it you can reduce your um you um the model",
    "start": "1149840",
    "end": "1156480"
  },
  {
    "text": "download actually um pressure so the way it works the D download to reker",
    "start": "1156480",
    "end": "1162400"
  },
  {
    "text": "streaming into the to each of the actors then you can aggregate to the hat and",
    "start": "1162400",
    "end": "1167840"
  },
  {
    "text": "upload to H DFS so that's basically this implementation basic compared to um this",
    "start": "1167840",
    "end": "1175440"
  },
  {
    "text": "um code implementation that's how it works so here we show some uh the The Benchmark",
    "start": "1175440",
    "end": "1184000"
  },
  {
    "text": "uh benchmarker result so um um this is basically shows the batch",
    "start": "1184000",
    "end": "1190559"
  },
  {
    "text": "size and the um and the number of tokens of the throughput wi throughput um so",
    "start": "1190559",
    "end": "1196720"
  },
  {
    "text": "you see with a larger bch size you can see the the war throughput can increase",
    "start": "1196720",
    "end": "1202360"
  },
  {
    "text": "but in January flight and somewhere about like like uh B size like 64 um for",
    "start": "1202360",
    "end": "1209320"
  },
  {
    "text": "the Mi sure 8 time 7 B model U we have 50 input we can get somewhere about uh",
    "start": "1209320",
    "end": "1215919"
  },
  {
    "text": "500 tokens per second per GPU so our typical typical job we need to handle",
    "start": "1215919",
    "end": "1222360"
  },
  {
    "text": "like 50 million EUR and the um the output token usually won't be this much",
    "start": "1222360",
    "end": "1229799"
  },
  {
    "text": "100 or less so um for this job we found we we can use somewhere",
    "start": "1229799",
    "end": "1235840"
  },
  {
    "text": "about uh 80 uh h100 so we can finish the job in lesser day so typically we this",
    "start": "1235840",
    "end": "1243360"
  },
  {
    "text": "meaning we are able to um to proide to support our application for a daily job",
    "start": "1243360",
    "end": "1249679"
  },
  {
    "text": "that's the original problem we trying to tackle so we can expand this to to more",
    "start": "1249679",
    "end": "1255600"
  },
  {
    "text": "but we only couple hundred GPU we we we can use so we really uh don't have to don't use all of them we can spend also",
    "start": "1255600",
    "end": "1262840"
  },
  {
    "text": "we we found a you can actually it's easier to actually you can split your",
    "start": "1262840",
    "end": "1268000"
  },
  {
    "text": "data into smaller chunk so um you use our transtion pipeline to U handle in",
    "start": "1268000",
    "end": "1274279"
  },
  {
    "text": "badge rather than um basically do a do do a large super large job because super",
    "start": "1274279",
    "end": "1281840"
  },
  {
    "text": "large job you can have a run longer time you have run into um higher chance to",
    "start": "1281840",
    "end": "1287360"
  },
  {
    "text": "fail because you disted if you have a certain field rate on one of the the actors you have so many um and with the",
    "start": "1287360",
    "end": "1294600"
  },
  {
    "text": "time you you get a accumulate a good chance to um to actually fail so so next",
    "start": "1294600",
    "end": "1300640"
  },
  {
    "text": "we going to um need to imp implant like a better like for tolerance so that's",
    "start": "1300640",
    "end": "1307720"
  },
  {
    "text": "something we have an employment that's something we're going to do next so we can make the job more",
    "start": "1307720",
    "end": "1313520"
  },
  {
    "text": "robust so basically summarize what you're doing the challenge basically",
    "start": "1313520",
    "end": "1319480"
  },
  {
    "text": "uh we we um you you're going to have the resource pressure so you handle because",
    "start": "1319480",
    "end": "1324880"
  },
  {
    "text": "super larger size everything uh either the the saving uh model saving uh",
    "start": "1324880",
    "end": "1330159"
  },
  {
    "text": "pressure also the the band base pressure the the lower putut basically um you you",
    "start": "1330159",
    "end": "1335919"
  },
  {
    "text": "basically need to have a like effective batching you need to um basically lower your cost so the GPU adjust rtion um you",
    "start": "1335919",
    "end": "1344120"
  },
  {
    "text": "need to handle basically nodewise you can also handle basically um you know um",
    "start": "1344120",
    "end": "1350520"
  },
  {
    "text": "you you also a lot of time you handle the online offline consistency so I can talk little little more here b um",
    "start": "1350520",
    "end": "1358679"
  },
  {
    "text": "because a lot of our application we have boths when the online is on we we want",
    "start": "1358679",
    "end": "1365200"
  },
  {
    "text": "to the offline to be on so we trying to make some uh consistent but but some of",
    "start": "1365200",
    "end": "1370799"
  },
  {
    "text": "the job basically we only have offline so we can optimize offline um but if you",
    "start": "1370799",
    "end": "1376640"
  },
  {
    "text": "have online B you Prett you probably need to go a treat off like your efficiency and the online offline",
    "start": "1376640",
    "end": "1383960"
  },
  {
    "text": "consistent you don't want to one the result consistency or another is you're able to uh easily enable the offline if",
    "start": "1383960",
    "end": "1392520"
  },
  {
    "text": "you have the online po already so we use original application to scale to many so",
    "start": "1392520",
    "end": "1397840"
  },
  {
    "text": "we can quickly got offline application uh ready but for those I mean uh basically",
    "start": "1397840",
    "end": "1405240"
  },
  {
    "text": "the r and v basically Mi a batch with ition job easier face you use Ray you",
    "start": "1405240",
    "end": "1412080"
  },
  {
    "text": "can help you easily scale your job you use V you can uh you have a easy and",
    "start": "1412080",
    "end": "1418360"
  },
  {
    "text": "efficient batch prediction applications yeah that's uh pretty much",
    "start": "1418360",
    "end": "1423799"
  },
  {
    "text": "it so I can answer some questions or you can reach out to us if you have anything to discuss",
    "start": "1423799",
    "end": "1430080"
  },
  {
    "text": "yeah uh if",
    "start": "1430080",
    "end": "1433600"
  },
  {
    "text": "anyone hello um I I probably missed the reason but why why you want to use re",
    "start": "1435720",
    "end": "1441480"
  },
  {
    "text": "had node to download the data in of re cuz it might crash the hand not easily",
    "start": "1441480",
    "end": "1447440"
  },
  {
    "text": "right um You mean here yeah yeah here oh this uh I think this mod read",
    "start": "1447440",
    "end": "1454640"
  },
  {
    "text": "databased you just you only need because in the earlier we download",
    "start": "1454640",
    "end": "1460320"
  },
  {
    "text": "the we download the data for each of the the actors um we in this impl mation you",
    "start": "1460320",
    "end": "1469080"
  },
  {
    "text": "download from rehad and the your data can do the streaming to each Vector",
    "start": "1469080",
    "end": "1474480"
  },
  {
    "text": "these are more like uh the R datation so we can we can maybe try",
    "start": "1474480",
    "end": "1481120"
  },
  {
    "text": "other way but so far it works fine you mean so as I mention the um here so you",
    "start": "1481120",
    "end": "1487200"
  },
  {
    "text": "can uh you can have a super large job I mean um to to download a lot of data and",
    "start": "1487200",
    "end": "1494399"
  },
  {
    "text": "process long hour but you can also try to split into uh smaller trunk so",
    "start": "1494399",
    "end": "1500279"
  },
  {
    "text": "basically you have one field you lost a few hours job most most then you can",
    "start": "1500279",
    "end": "1506120"
  },
  {
    "text": "basically maybe redo it so this way you don't lose everything so the risk so far",
    "start": "1506120",
    "end": "1511240"
  },
  {
    "text": "it looks fine for us yeah I think it's a concern but so far it's still okay yeah",
    "start": "1511240",
    "end": "1517039"
  },
  {
    "text": "and maybe you need to learn more about the ray data that's another Library Ray data in the head note yeah uh yeah you",
    "start": "1517039",
    "end": "1525279"
  },
  {
    "text": "need to install the r yeah it's a it's a have the Red Data library right so I",
    "start": "1525279",
    "end": "1531520"
  },
  {
    "text": "don't know if you covered this in your talk but for your badge predictions are you sharing prompts or there all different prompts for each row share",
    "start": "1531520",
    "end": "1539679"
  },
  {
    "text": "prompt yeah uh not sure what you mean the share prompt so you can have the",
    "start": "1539679",
    "end": "1546240"
  },
  {
    "text": "same prompt but different input for to do the predictions oh no we we do not",
    "start": "1546240",
    "end": "1551919"
  },
  {
    "text": "usually a um prompt engineer is down before uh coming to the prediction so",
    "start": "1551919",
    "end": "1558640"
  },
  {
    "text": "that's a good idea but yeah right now we're not doing that yet yeah yeah okay and um maybe this is a more VM question",
    "start": "1558640",
    "end": "1565880"
  },
  {
    "text": "you said you do a uh pack strategy strict pack mhm so just curious what the",
    "start": "1565880",
    "end": "1573320"
  },
  {
    "text": "other options are and enough with the so I think this something with with",
    "start": "1573320",
    "end": "1579159"
  },
  {
    "text": "developer the v um developer I think we have them here they can answer maybe",
    "start": "1579159",
    "end": "1584440"
  },
  {
    "text": "basically after I think well we uh 0.26 uh they kind of ask to a strict",
    "start": "1584440",
    "end": "1593240"
  },
  {
    "text": "pack to make it work so I think recently the newest because my reason I think is",
    "start": "1593240",
    "end": "1599159"
  },
  {
    "text": "the user Ray uh underneath to manage M GPU I think I talk with them they're saying this fing to use a multi per size",
    "start": "1599159",
    "end": "1606640"
  },
  {
    "text": "to handle it they don't have this problem anymore uh but that's kind of one of the reason we use a strict",
    "start": "1606640",
    "end": "1613360"
  },
  {
    "text": "pack yeah also like the strict pack should like uh make sure the process",
    "start": "1613360",
    "end": "1620159"
  },
  {
    "text": "group does not spend across noes so if you have t parallel then inside one not",
    "start": "1620159",
    "end": "1625919"
  },
  {
    "text": "the communication is more efficient um thank you for the",
    "start": "1625919",
    "end": "1633120"
  },
  {
    "text": "presentation um my question about uh you mentioned that you used a cluster with",
    "start": "1633120",
    "end": "1639960"
  },
  {
    "text": "80 um gpus um do you have a number of uh",
    "start": "1639960",
    "end": "1647679"
  },
  {
    "text": "reasonable reasonable size of cluster for this kind of methods like for eight it's not",
    "start": "1647679",
    "end": "1654640"
  },
  {
    "text": "enough foration or we don't have that exact number because the h100 is a pretty uh",
    "start": "1654640",
    "end": "1662360"
  },
  {
    "text": "expensive resources it's we use a lot of for the training so we can use only use",
    "start": "1662360",
    "end": "1668159"
  },
  {
    "text": "a part of the resource for the scoring so we cannot use all of them also uh we",
    "start": "1668159",
    "end": "1674720"
  },
  {
    "text": "the the the goal is for us to finish we we want to do it daily so finish in a",
    "start": "1674720",
    "end": "1680480"
  },
  {
    "text": "day 10 to 20 hours and the the amount of J to handle about 50 million we just",
    "start": "1680480",
    "end": "1687120"
  },
  {
    "text": "found 80 GPU is enough for us we should able to spend we we just didn't actually",
    "start": "1687120",
    "end": "1693480"
  },
  {
    "text": "test but basically refocus they're saying the for there they do like a lot of more it's no problem but for us we",
    "start": "1693480",
    "end": "1700799"
  },
  {
    "text": "don't have so many GPU for us to test and we don't really need it for our job",
    "start": "1700799",
    "end": "1706240"
  },
  {
    "text": "but I think you can do a couple hundred it should be okay I mean that if I have a cluster with eight uh h100 gpus is it",
    "start": "1706240",
    "end": "1714880"
  },
  {
    "text": "reasonable to use the same approaches uh you presented uh depends I mean for this",
    "start": "1714880",
    "end": "1722919"
  },
  {
    "text": "approach we can claim the whole node 8 GPU um but um that's also if um some",
    "start": "1722919",
    "end": "1731640"
  },
  {
    "text": "cases I mentioned sometime we want online offline consistency depends on",
    "start": "1731640",
    "end": "1736720"
  },
  {
    "text": "how the people imp their model their model like how many gpus you need we",
    "start": "1736720",
    "end": "1742279"
  },
  {
    "text": "sometime we want to mimic what they people doing on a single no for online",
    "start": "1742279",
    "end": "1747559"
  },
  {
    "text": "we not going to claim all the GPU resources so this way probably not the",
    "start": "1747559",
    "end": "1753279"
  },
  {
    "text": "most efficient way to uh use the resource get the best but sometimes the mod the the job size is not so large we",
    "start": "1753279",
    "end": "1761320"
  },
  {
    "text": "just want to easily bring out the batch prediction um so um we don't have to",
    "start": "1761320",
    "end": "1766600"
  },
  {
    "text": "claim all the resources just do the same as online so but we found if this real",
    "start": "1766600",
    "end": "1771960"
  },
  {
    "text": "this a real big job in optimize we probably optimize for the batch this one",
    "start": "1771960",
    "end": "1778360"
  },
  {
    "text": "of the challenges hard to optimize for both for the online single prediction and the batch prediction so yeah",
    "start": "1778360",
    "end": "1787399"
  },
  {
    "text": "oh oh sorry my timer any other questions uh sorry I had",
    "start": "1787399",
    "end": "1793120"
  },
  {
    "text": "just two quick questions uh so for the first one you mentioned that there is some issue with collocating",
    "start": "1793120",
    "end": "1798880"
  },
  {
    "text": "the uh inferencing with the training together on the same GPU uh could you kind of elaborate on that I I thought",
    "start": "1798880",
    "end": "1804880"
  },
  {
    "text": "you have like know results allocation to provide this isolations so what's the missing part um so",
    "start": "1804880",
    "end": "1813039"
  },
  {
    "text": "basically for training for example like uh uh we 1100 we have four 1100 on it",
    "start": "1813039",
    "end": "1819679"
  },
  {
    "text": "often they do like for example they want F tun 7tb model they need the the whole",
    "start": "1819679",
    "end": "1825279"
  },
  {
    "text": "full GPU the whole node so if we do the um scoring we use like one or two of GP",
    "start": "1825279",
    "end": "1833240"
  },
  {
    "text": "from that node we leave like two to three of the rest of them but when they",
    "start": "1833240",
    "end": "1838399"
  },
  {
    "text": "the the training job they trying to claim full gpus they cannot find a node to do that um that that's a",
    "start": "1838399",
    "end": "1845000"
  },
  {
    "text": "fragmentation I mentioned but uh recently we just uh we also trying to um",
    "start": "1845000",
    "end": "1851240"
  },
  {
    "text": "do the schedule a different way for example the bean pack we can p pack our scaring scaring job Clan GPU we trying",
    "start": "1851240",
    "end": "1859679"
  },
  {
    "text": "to um pack them to on the same node um then you basically still um although you",
    "start": "1859679",
    "end": "1865960"
  },
  {
    "text": "not claiming all the gpus but can still able to fully utilize one GPU not expand",
    "start": "1865960",
    "end": "1871080"
  },
  {
    "text": "to many of the nodes uh that's another way to uh solve the problem yeah thank",
    "start": "1871080",
    "end": "1876720"
  },
  {
    "text": "you and the second quick question is um the batch size experiment that you guys did um so I we saw that it's saturated",
    "start": "1876720",
    "end": "1883399"
  },
  {
    "text": "on the larger bad size was there any other things that you found with like having a larger B size is there any",
    "start": "1883399",
    "end": "1889480"
  },
  {
    "text": "downside with that or any other things that you guys tuned for for the workload um for the because the mixture model",
    "start": "1889480",
    "end": "1897039"
  },
  {
    "text": "this size is we often use for fine tuning for this one we don't see a",
    "start": "1897039",
    "end": "1903240"
  },
  {
    "text": "problem um but I think if you go to for example the the Lama Tre they have a",
    "start": "1903240",
    "end": "1909519"
  },
  {
    "text": "pretty long uh tax window so basically batch size longer you're going to you",
    "start": "1909519",
    "end": "1915760"
  },
  {
    "text": "may not have enough memory to hi it so you probably have to tune your batch",
    "start": "1915760",
    "end": "1920799"
  },
  {
    "text": "size down like for the K cach like for 44 5B model example you already uh use 8",
    "start": "1920799",
    "end": "1928320"
  },
  {
    "text": "GPU to host it you're going to have lat memory of KV cach if you increase the batch size you can probably run into",
    "start": "1928320",
    "end": "1935159"
  },
  {
    "text": "problem you need to basically reduce your badge size that's one thing I can",
    "start": "1935159",
    "end": "1941279"
  },
  {
    "text": "think hopefully answer your question okay if there's no more questions thanks",
    "start": "1942279",
    "end": "1948159"
  },
  {
    "text": "for the talk all right thank you thanks for coming everybody",
    "start": "1948159",
    "end": "1953639"
  }
]