[
  {
    "text": "yeah hi hi good evening thanks for all coming for um our meet up so we're gonna",
    "start": "240",
    "end": "6160"
  },
  {
    "text": "start with John who's scaling multil pipelines who plans and plans to be and",
    "start": "6160",
    "end": "11360"
  },
  {
    "text": "then Richard was gonna talk about LLM so we're in for a treat yep thank you so",
    "start": "11360",
    "end": "16720"
  },
  {
    "text": "much Danielle um hi uh my name is John Shay um and uh sorry I'm not late but uh",
    "start": "16720",
    "end": "25439"
  },
  {
    "text": "you have me um let me give a little background about myself and then I'll just jump in so",
    "start": "25439",
    "end": "32840"
  },
  {
    "text": "um if you a while back I used to work at a",
    "start": "32840",
    "end": "38320"
  },
  {
    "text": "company called Cloudera and I worked on a system called HBase hbs is this giant key value store um and two of the",
    "start": "38320",
    "end": "44559"
  },
  {
    "text": "features that I worked on that were super interesting to me although not kind of the star of the show snapshots",
    "start": "44559",
    "end": "50719"
  },
  {
    "text": "and blob storage in this massive key value store the way these things were built the way the things were added they",
    "start": "50719",
    "end": "56079"
  },
  {
    "text": "were kind of secondass citizens um and most workloads weren't really ready to handle those kinds of things but the",
    "start": "56079",
    "end": "62719"
  },
  {
    "text": "world is different today um with LLMs with you know foundational models these",
    "start": "62719",
    "end": "68799"
  },
  {
    "text": "things that used to be really hard to deal with have become really important and really valuable and unlocked and I'm",
    "start": "68799",
    "end": "75280"
  },
  {
    "text": "going to talk about why NADB exists and how this is uh you know one of the keys that will help unlock this information",
    "start": "75280",
    "end": "81520"
  },
  {
    "text": "and data so NASAD is a data platform for",
    "start": "81520",
    "end": "88560"
  },
  {
    "text": "processing multimodal data for AI workloads and for um you know lots of",
    "start": "88560",
    "end": "94240"
  },
  {
    "text": "use cases like search and similarity search and find you know full text",
    "start": "94240",
    "end": "99920"
  },
  {
    "text": "search as well as doing analytic queries um and it's trusted by some of the leading you AI groups in the world right",
    "start": "99920",
    "end": "106960"
  },
  {
    "text": "so companies like data bricks and bite dance you know tik tok fame use the lance format and use lance behind the",
    "start": "106960",
    "end": "113159"
  },
  {
    "text": "scenes uh it's used to help do curation and select the raw data used to build",
    "start": "113159",
    "end": "119200"
  },
  {
    "text": "foundational models so for midjourney for image generation for runway for the videos that they generate for character",
    "start": "119200",
    "end": "125439"
  },
  {
    "text": "AI for the virtual friends that they create other companies as well and oh yeah it does a lot of vector database",
    "start": "125439",
    "end": "131599"
  },
  {
    "text": "stuff too which is really good for building applications so Hex uses it to do the SQL code generation and the BI",
    "start": "131599",
    "end": "138959"
  },
  {
    "text": "visualization generation tuby does it for video search harvey uses it to",
    "start": "138959",
    "end": "144319"
  },
  {
    "text": "search AI sorry search legal documents and give you kind of rag responses let me just ask a quick question right here",
    "start": "144319",
    "end": "150800"
  },
  {
    "text": "how many of you guys are familiar with rag and what that is it's just about everyone okay so I'm not going to go",
    "start": "150800",
    "end": "156640"
  },
  {
    "text": "into the details of what that is but it's clearly one of the most important kind of use cases and patterns for LLMs",
    "start": "156640",
    "end": "163760"
  },
  {
    "text": "and this kind of data today so all of these companies face what I",
    "start": "163760",
    "end": "170000"
  },
  {
    "text": "call the multimodal data challenge right so in the big data days it was all about",
    "start": "170000",
    "end": "175760"
  },
  {
    "text": "logs and JSON and structured and semiructured data but today again it's this multimodal data images PDFs videos",
    "start": "175760",
    "end": "184080"
  },
  {
    "text": "audio sensor data links to web pages all these things are now unlocked by what AI",
    "start": "184080",
    "end": "190239"
  },
  {
    "text": "and LMS can provide it makes them something that's usable not just by people who have PhDs or teams or",
    "start": "190239",
    "end": "197280"
  },
  {
    "text": "companies that can hire teams of PhDs to do this we got a resume from a kid in high school and he was building LLM",
    "start": "197280",
    "end": "204239"
  },
  {
    "text": "applications right so like a high schooler kid could very motivated one but like it's way easier today for",
    "start": "204239",
    "end": "210239"
  },
  {
    "text": "people to use this kind of data which used to be you know kind of considered impossible",
    "start": "210239",
    "end": "216159"
  },
  {
    "text": "um and the thing about this data you mentioned blob storage or whatever it's been secondass and the traditional data",
    "start": "216159",
    "end": "223840"
  },
  {
    "text": "stacks and the traditional data links just weren't optimized for working with this kind of data it made it super",
    "start": "223840",
    "end": "229920"
  },
  {
    "text": "expensive to experiment it made it really expensive to uh you know try different things made it really",
    "start": "229920",
    "end": "236239"
  },
  {
    "text": "expensive to do iteration um you lack the ability to do versioning and it made it really slow to do versioning and",
    "start": "236239",
    "end": "242799"
  },
  {
    "text": "experimentation and that's what you really need today because these LLM based systems and these modelbased systems they're they're not like dbt",
    "start": "242799",
    "end": "249519"
  },
  {
    "text": "they're not like an loop they're not perfect you can't check them mechanically all the time you have to kind of go in and vibe check it and and",
    "start": "249519",
    "end": "255599"
  },
  {
    "text": "do these kinds of things so it's different because there's a lot more iteration that has to go in to making a high quality product or it really is",
    "start": "255599",
    "end": "263280"
  },
  {
    "text": "kind of trying to shove a square peg in this round hole right like analytics is all nice and round but this stuff is",
    "start": "263280",
    "end": "269840"
  },
  {
    "text": "slightly different shape and so the key idea is these AI",
    "start": "269840",
    "end": "275280"
  },
  {
    "text": "workloads they're not like online analytical proc uh analytical processing it's it's different use cases it's",
    "start": "275280",
    "end": "281680"
  },
  {
    "text": "different problems we're trying to solve you might come in with an off-the-shelf model and try tweaking your prompts and",
    "start": "281680",
    "end": "287680"
  },
  {
    "text": "trying to get some output to come out you'll do you're doing there an eval right you're putting that kind of first",
    "start": "287680",
    "end": "293919"
  },
  {
    "text": "version of the prompt out there and serving it and using it for your application uh you're collecting information about that getting feedback",
    "start": "293919",
    "end": "300960"
  },
  {
    "text": "and then um looking at that to try to find out what you can do to improve it you have some hypothesis you're saying",
    "start": "300960",
    "end": "306160"
  },
  {
    "text": "\"Oh maybe if I chunk things differently or maybe if I added more context or whatever.\" So you go back and you",
    "start": "306160",
    "end": "312080"
  },
  {
    "text": "explore the data and see if there's these patterns present and then you can you know update your prompt or update",
    "start": "312080",
    "end": "317680"
  },
  {
    "text": "your model or fine-tune it or whatever um so that you can do new evals get a better result push it out to production",
    "start": "317680",
    "end": "323199"
  },
  {
    "text": "again right so this the cycle happens in all these different places where you're using AI to do",
    "start": "323199",
    "end": "331199"
  },
  {
    "text": "processing so I'm going to take a step back so how many of you guys are parents okay not so many of you guys",
    "start": "331560",
    "end": "338800"
  },
  {
    "text": "well I'm a parent i've got two kids and I get about 30 emails a week from my school aboutund 150 WhatsApp chat",
    "start": "338800",
    "end": "346400"
  },
  {
    "text": "messages um from the parents group also there's attachments and web links and",
    "start": "346400",
    "end": "352080"
  },
  {
    "text": "sometimes even text messages that I'm supposed to track right and being a techie playing with this new technology",
    "start": "352080",
    "end": "358240"
  },
  {
    "text": "i wanted something to help me and my my family uh and I didn't want to spend the 5 to 10 hours a week that some parents",
    "start": "358240",
    "end": "364000"
  },
  {
    "text": "do reading all this stuff so what did I do i built a little app and it ends up being basically a rag pipe right and",
    "start": "364000",
    "end": "371280"
  },
  {
    "text": "what I'm doing is I'm just pulling in emails images videos all these things that have been shared by the group and",
    "start": "371280",
    "end": "378319"
  },
  {
    "text": "putting them into storage i'm either doing some traditional text extraction um or I'm using an LLM to do caption",
    "start": "378319",
    "end": "385280"
  },
  {
    "text": "generation so that I can put it into text and just use it in the rest of the pipeline and then from there the",
    "start": "385280",
    "end": "391120"
  },
  {
    "text": "traditional rag pipeline things I chunk it up into uh you know bite-sized chunks",
    "start": "391120",
    "end": "396560"
  },
  {
    "text": "i use a model to embed them and put them into a vector database and then that's kind of the backend side and the front",
    "start": "396560",
    "end": "402479"
  },
  {
    "text": "end side can take queries hey when is the field trip for my first grader ask",
    "start": "402479",
    "end": "407759"
  },
  {
    "text": "the question do the vector lookup put it into the prompt get an answer back right",
    "start": "407759",
    "end": "412960"
  },
  {
    "text": "so the one thing that is never going to change is that raw data that came in so I collect that raw data but everything",
    "start": "412960",
    "end": "419840"
  },
  {
    "text": "else could be changing the prompt it could be changing the model it could be changing the chunking all these things are kind of up in the air and could",
    "start": "419840",
    "end": "426759"
  },
  {
    "text": "change so I built this and like whoa I should be done and I can make a million dollars then right the problem is that",
    "start": "426759",
    "end": "432720"
  },
  {
    "text": "these things aren't perfect right so about 60% of the answers were good and about 40% of the answers may have been",
    "start": "432720",
    "end": "438800"
  },
  {
    "text": "hallucinations not quite so good so what do you do you science this right what do",
    "start": "438800",
    "end": "444560"
  },
  {
    "text": "you do you go and you build an evaluation suite so I kind of added some things on the side i used Braggus which",
    "start": "444560",
    "end": "451199"
  },
  {
    "text": "is a um LM is a judge eval platform you know I collected the questions and",
    "start": "451199",
    "end": "456400"
  },
  {
    "text": "answers so I can go look at what was generating the answers that were coming back to the users i also have something that I call",
    "start": "456400",
    "end": "463599"
  },
  {
    "text": "custom eval reading the logs and seeing what is in there you need to go and guide check",
    "start": "463599",
    "end": "470880"
  },
  {
    "text": "this stuff right because at the end of the day human judgment is really important here and what that led to was",
    "start": "470880",
    "end": "476560"
  },
  {
    "text": "a bunch of refinements and more things that I added to the main pipeline i added hide hypothetical document",
    "start": "476560",
    "end": "482879"
  },
  {
    "text": "embedding i added reranking oh and then I redid the whole chunking mechanism so I could add more data metadata and have",
    "start": "482879",
    "end": "489280"
  },
  {
    "text": "more context right so after all this work got to about 85 90% what I would consider to be good answers um that's",
    "start": "489280",
    "end": "496960"
  },
  {
    "text": "great right like I um made a lot of progress on the business side nobody was",
    "start": "496960",
    "end": "502000"
  },
  {
    "text": "there weren't that many people asking questions and I had all this data so I had to find out something else to build",
    "start": "502000",
    "end": "507280"
  },
  {
    "text": "right um and what I ended up doing is I added yet another pipeline right added",
    "start": "507280",
    "end": "513000"
  },
  {
    "text": "summarization summarize the email summarize the PDFs um I found that a lot of these things had things relevant to",
    "start": "513000",
    "end": "519120"
  },
  {
    "text": "fifth graders but I have a first grader don't care i need to add tagging and classifications so I can filter things out um and then I added other things",
    "start": "519120",
    "end": "526160"
  },
  {
    "text": "like searches and vector indexes of the summaries that when people came into the app and were looking for citations and",
    "start": "526160",
    "end": "532080"
  },
  {
    "text": "looking for where these things came from they could get answer and go back to the original data right um and along the way",
    "start": "532080",
    "end": "539120"
  },
  {
    "text": "prompts LMS these are all also things that are things that I could change right should I try Sonnet should I try",
    "start": "539120",
    "end": "545920"
  },
  {
    "text": "uh GPT40 should I try Llama should I try you know which model should I use and",
    "start": "545920",
    "end": "552240"
  },
  {
    "text": "all these different tweaks that I could choose there um oh yeah and then since I had",
    "start": "552240",
    "end": "558880"
  },
  {
    "text": "the evaluations and since I did these vibe checks I could say which ones were good and if I really wanted to I could",
    "start": "558880",
    "end": "564480"
  },
  {
    "text": "use those as an examplars throw that back in as my training set for some fine-tuning on these models so here I",
    "start": "564480",
    "end": "570000"
  },
  {
    "text": "would probably focus on the rag query parts right so all I need to all I really want to say here is that that",
    "start": "570000",
    "end": "576560"
  },
  {
    "text": "simple pipeline got pretty complicated there's lots of dependencies in here there are LLM jobs that generate data",
    "start": "576560",
    "end": "583920"
  },
  {
    "text": "that then feed other LLM processing and and other kinds of things and just makes it super complicated especially if I",
    "start": "583920",
    "end": "590320"
  },
  {
    "text": "want to make an experiment all the way at the beginning of the pipeline like it's going to impact all these other things and I have to rerun everything or",
    "start": "590320",
    "end": "596959"
  },
  {
    "text": "maybe I don't maybe I only have to rerun some how do you keep track of all that it becomes a mess",
    "start": "596959",
    "end": "603040"
  },
  {
    "text": "um this is just logical infrastructure right this is how data and processes are",
    "start": "603040",
    "end": "608480"
  },
  {
    "text": "happening uh there's a decision here also about what kind of physical infrastructure I'm going to use to run",
    "start": "608480",
    "end": "614320"
  },
  {
    "text": "this right so here is a traditional data lake",
    "start": "614320",
    "end": "619920"
  },
  {
    "text": "solution right so I land the data in my data lake raw files or k I do my vector",
    "start": "619920",
    "end": "626720"
  },
  {
    "text": "embeddings and throw them into a vector database I um you know collect information and put them maybe in an",
    "start": "626720",
    "end": "632160"
  },
  {
    "text": "operation database like history of the questions and those kinds of things i have some process to look at that and do",
    "start": "632160",
    "end": "637839"
  },
  {
    "text": "some curation i'm searching and you know in this picture I' I've been a big data guy for a long time but there like five",
    "start": "637839",
    "end": "644079"
  },
  {
    "text": "or six systems in here that I'm mostly familiar with but like as a single man show like I don't want to have to run",
    "start": "644079",
    "end": "649440"
  },
  {
    "text": "all this infrastructure i want something simpler something more straightforward um and you know really what it is is",
    "start": "649440",
    "end": "655760"
  },
  {
    "text": "that again the multimodal workloads just is not what this stuff is built for",
    "start": "655760",
    "end": "663600"
  },
  {
    "text": "so this is where Lance DVD entered right and their basic idea is that this is a",
    "start": "664320",
    "end": "670240"
  },
  {
    "text": "unified foundational model for dealing with multi-modal data it's developer",
    "start": "670240",
    "end": "676000"
  },
  {
    "text": "friendly it's open source i love open source um it supports Python TypeScript",
    "start": "676000",
    "end": "682079"
  },
  {
    "text": "and the Java variants it's built on Apache um arrow so it has a lot of",
    "start": "682079",
    "end": "687360"
  },
  {
    "text": "interoperability with other data tools like duct DB and um other systems out there it's kind of purpose-built to",
    "start": "687360",
    "end": "693839"
  },
  {
    "text": "handle storage uh of images videos you can even store your prompts in there you",
    "start": "693839",
    "end": "699040"
  },
  {
    "text": "can also store your embeddings in there so you can all put this information into one table one database effectively and",
    "start": "699040",
    "end": "706320"
  },
  {
    "text": "it you know supports all of the major search types both vector searches different kinds of neighbor algorithms",
    "start": "706320",
    "end": "713040"
  },
  {
    "text": "and um you know full text search like the M25 it also ran great on my laptop but",
    "start": "713040",
    "end": "720320"
  },
  {
    "text": "it's also been great running at hypers scale right some of the biggest users of Lance run at around 60 pabytes of data",
    "start": "720320",
    "end": "727680"
  },
  {
    "text": "right so um and with the proper engineering it can run at very high",
    "start": "727680",
    "end": "732720"
  },
  {
    "text": "queries per second um and it benefits from you know modern cloud design",
    "start": "732720",
    "end": "738320"
  },
  {
    "text": "compute and storage separation um being able to directly work against object store which is the most efficient cost",
    "start": "738320",
    "end": "743920"
  },
  {
    "text": "efficient place to store your",
    "start": "743920",
    "end": "746880"
  },
  {
    "text": "data all of this is kind of built on some fundamental foundational work based",
    "start": "750040",
    "end": "755519"
  },
  {
    "text": "on the lance columner format and there's a lot of details in here a lot of features i'm not going to cover it i'm going to talk about the one most",
    "start": "755519",
    "end": "761440"
  },
  {
    "text": "important feature or the problem that I was showing you earlier um and how we kind of solve that",
    "start": "761440",
    "end": "767480"
  },
  {
    "text": "problem and that feature is zero cost schema evolution if you want to add a",
    "start": "767480",
    "end": "773279"
  },
  {
    "text": "new feature you just add a column and we make it extremely cheap for you to add a",
    "start": "773279",
    "end": "778720"
  },
  {
    "text": "new column to an existing table sounds very basic right very simple but this is",
    "start": "778720",
    "end": "784240"
  },
  {
    "text": "actually fundamentally really important because if I just want to add one embedding column to this 10 pabyte data",
    "start": "784240",
    "end": "790240"
  },
  {
    "text": "set I do not want to have to write a second copy of that 10 pabyte data set i want to leave that where it is and I",
    "start": "790240",
    "end": "796639"
  },
  {
    "text": "just want to drop that column in right next to it and have that new set of data but have that nice transactional",
    "start": "796639",
    "end": "802160"
  },
  {
    "text": "behavior of a database and that's what plants DB kind of gives you um and there's these operations that make it",
    "start": "802160",
    "end": "807760"
  },
  {
    "text": "very easy for you to add features to your data set and serve them in your",
    "start": "807760",
    "end": "813519"
  },
  {
    "text": "application or use them for exploratory data analysis or to uh you know come up",
    "start": "813519",
    "end": "819920"
  },
  {
    "text": "with new uses or derive new data from so let me give a more",
    "start": "819920",
    "end": "826160"
  },
  {
    "text": "straightforward example a little bit less complicated than the rag example this is uh similar to what some of the customers that I've been working with",
    "start": "826160",
    "end": "832079"
  },
  {
    "text": "are trying to do this customer has a billion images in the database um all in",
    "start": "832079",
    "end": "837279"
  },
  {
    "text": "lance format which makes it really nice because then they can do experiments trying different models to generate",
    "start": "837279",
    "end": "843680"
  },
  {
    "text": "captions for them right so you could try a deepsee you could try a Gemma you could try um you know different",
    "start": "843680",
    "end": "849199"
  },
  {
    "text": "techniques for image embedding um and all they have to do is drop a new column",
    "start": "849199",
    "end": "855639"
  },
  {
    "text": "in run uh the job in Ray um using these models on their GPU machines um you know",
    "start": "855639",
    "end": "863040"
  },
  {
    "text": "maybe do it against the sample look at that and then decide to expand to the rest right so it's just adding another",
    "start": "863040",
    "end": "869399"
  },
  {
    "text": "column the other nice thing about this is you can use traditional uh processing also pillow is just a image processing",
    "start": "869399",
    "end": "875519"
  },
  {
    "text": "library and you can do things like rescale images to a particular size thumbnails say for surveying right um or",
    "start": "875519",
    "end": "881440"
  },
  {
    "text": "capturing width and height information this allows you to when you're doing analysis do a similarity search like",
    "start": "881440",
    "end": "889440"
  },
  {
    "text": "maybe I'm going to drop an image and say find something similar to this it allows you to do full text search but it also",
    "start": "889440",
    "end": "894959"
  },
  {
    "text": "allows you to add filters for images that are less than 128 bytes",
    "start": "894959",
    "end": "902480"
  },
  {
    "text": "128 pixels tall or something like that and you can add these other filters based on what traditionally would be",
    "start": "902480",
    "end": "907839"
  },
  {
    "text": "analytical um filters okay so let me dive in a little",
    "start": "907839",
    "end": "912880"
  },
  {
    "text": "bit more oh there's one other thing in here um there are dependencies right so like you know this captioning model and",
    "start": "912880",
    "end": "918480"
  },
  {
    "text": "then use another model to generate classifications or tags right so like there's derived data and there's data",
    "start": "918480",
    "end": "924160"
  },
  {
    "text": "derived off of that so how do you manage that DAG right and these are things that are are going to become more and more",
    "start": "924160",
    "end": "929360"
  },
  {
    "text": "important over time as more data gets processed in this kind of",
    "start": "929360",
    "end": "934399"
  },
  {
    "text": "way so this is where Ray comes in right so Lance DB makes it really easy to add",
    "start": "934600",
    "end": "941240"
  },
  {
    "text": "features ray handles the processing problem and makes it able to scale right",
    "start": "941240",
    "end": "946320"
  },
  {
    "text": "so if you use Lance DB's and Lance's zero cost schema evolution adding a",
    "start": "946320",
    "end": "952000"
  },
  {
    "text": "feature is really just adding a column kicking off array job or array data job",
    "start": "952000",
    "end": "957199"
  },
  {
    "text": "to do the processing and then committing that the merge insert into the database",
    "start": "957199",
    "end": "962480"
  },
  {
    "text": "and then it becomes available to all your users for real-time serving for new experiments for building even more",
    "start": "962480",
    "end": "969120"
  },
  {
    "text": "derived data ray and and uh and and Lance are",
    "start": "969120",
    "end": "974480"
  },
  {
    "text": "really good pair right so Lance has taken care of a lot of the storage problems and a lot of the right",
    "start": "974480",
    "end": "979759"
  },
  {
    "text": "amplification problems and the things kind of at that level and Ray handles the distribution of code it handles the",
    "start": "979759",
    "end": "985839"
  },
  {
    "text": "plumbing for batching and dealing with partitions it handles memory and GPU scheduling and handles retry back",
    "start": "985839",
    "end": "992480"
  },
  {
    "text": "pressure and a bunch of the orchestration to do the processing so a good onetwo",
    "start": "992480",
    "end": "998759"
  },
  {
    "text": "punch so let me just show you an example uh you know a while ago maybe six months",
    "start": "998759",
    "end": "1004320"
  },
  {
    "text": "ago or so we added a integration between lance uh and ray data right so this is",
    "start": "1004320",
    "end": "1010480"
  },
  {
    "text": "an example code of using ray code uh this is the ray data library to read a lance format file just like you read",
    "start": "1010480",
    "end": "1016959"
  },
  {
    "text": "parquet or read from some other database  whatever right it's it's just this easy to read data in and start",
    "start": "1016959",
    "end": "1023839"
  },
  {
    "text": "processing it then you can just define a function",
    "start": "1023839",
    "end": "1029760"
  },
  {
    "text": "so here's a fake embedding function I just generate a bunch of randoms right but normally you'd throw it through some",
    "start": "1029760",
    "end": "1035760"
  },
  {
    "text": "embedding algorithm that would generate um an array that represented that information right um and I just feed it",
    "start": "1035760",
    "end": "1042319"
  },
  {
    "text": "to a radiated map function and then materialize it as a lance table um you",
    "start": "1042319",
    "end": "1048400"
  },
  {
    "text": "can see here um this is the function that does the transform um I take the",
    "start": "1048400",
    "end": "1054480"
  },
  {
    "text": "data I transform it I put it into the map so it applies it to every single row",
    "start": "1054480",
    "end": "1059919"
  },
  {
    "text": "and then I write it at Lance into the cloud simple there's like 20 lines of",
    "start": "1059919",
    "end": "1065360"
  },
  {
    "text": "code so far and then I can do a merge which commits it atomically into the big data",
    "start": "1065360",
    "end": "1072720"
  },
  {
    "text": "set so this is all you really need to do to just add another column that has you know the information processed from the",
    "start": "1072720",
    "end": "1078720"
  },
  {
    "text": "ray world and uh again it's super cheap it's just the metadata operation and the copy",
    "start": "1078720",
    "end": "1085039"
  },
  {
    "text": "of the new data in so that's kind of the gist of my talk",
    "start": "1085039",
    "end": "1091200"
  },
  {
    "text": "you know basically plans and ray are like kind of a match made for each other",
    "start": "1091200",
    "end": "1096640"
  },
  {
    "text": "um modern AI pipelines have this multimodal data challenge um and it really impacts an AI engineer's",
    "start": "1096640",
    "end": "1103120"
  },
  {
    "text": "productivity their ability to scale experiment uh and and reproduce results",
    "start": "1103120",
    "end": "1108720"
  },
  {
    "text": "uh Lance DB is one half of this equation it tackles the multimodal storage problem and it does it kind of from this",
    "start": "1108720",
    "end": "1115600"
  },
  {
    "text": "fundamental level and uh in in this particular use case the zerocost schema evolution is like that key feature that",
    "start": "1115600",
    "end": "1122320"
  },
  {
    "text": "unlocks um making it efficient ray Ray data tackle the",
    "start": "1122320",
    "end": "1128000"
  },
  {
    "text": "multimodal data processing challenges right so and um you know together with any scale and lens DB we're basically",
    "start": "1128000",
    "end": "1134720"
  },
  {
    "text": "working handinand glove to help you you know as an AI engineer get your work done more efficiently more quickly and",
    "start": "1134720",
    "end": "1140720"
  },
  {
    "text": "more economically oh yeah and there's that square peg fitting into the square",
    "start": "1140720",
    "end": "1145919"
  },
  {
    "text": "holder last time okay show",
    "start": "1145919",
    "end": "1152360"
  },
  {
    "text": "anyway all right so that that's all I had so I'm I'm here for questions i probably have about five 10 minutes um",
    "start": "1152360",
    "end": "1158960"
  },
  {
    "text": "for whatever questions you may have um it looks like one of my QR codes didn't work uh big announcement is that LanceD",
    "start": "1158960",
    "end": "1166000"
  },
  {
    "text": "cloud a version of Lance DD that just you sign up with uh your Google account and a credit card is available for",
    "start": "1166000",
    "end": "1172160"
  },
  {
    "text": "public beta as of about a week ago um yeah so I'm open for questions",
    "start": "1172160",
    "end": "1180679"
  },
  {
    "text": "guys seem awfully quiet all right there you go",
    "start": "1188559",
    "end": "1193799"
  },
  {
    "text": "so uh the rows don't have to be exactly in the same order like we don't rewrite or reorder the data the way the write",
    "start": "1202080",
    "end": "1208559"
  },
  {
    "text": "path works it actually kind of writes it in in random order um but um you know",
    "start": "1208559",
    "end": "1214080"
  },
  {
    "text": "the the files have the data aligned so that when it's doing the read it can say",
    "start": "1214080",
    "end": "1219200"
  },
  {
    "text": "oh when you've written all these values out it's aligned with the other files that are present and makes it really",
    "start": "1219200",
    "end": "1225679"
  },
  {
    "text": "efficient to do the read like you know one or two um IOPS to get to the actual",
    "start": "1225679",
    "end": "1231120"
  },
  {
    "text": "data oh yeah we we we we were on we were in",
    "start": "1231120",
    "end": "1238720"
  },
  {
    "text": "New Orleans last week and we found Lance Crackers it's like oh man we need that for marketing",
    "start": "1238720",
    "end": "1245960"
  },
  {
    "text": "yep",
    "start": "1251760",
    "end": "1254760"
  },
  {
    "text": "yeah so let me give you the short super short answer parket is optimized for",
    "start": "1260000",
    "end": "1266880"
  },
  {
    "text": "analytical workloads so you know data warehouse type workloads um if you add a column to parquet you have to rewrite",
    "start": "1266880",
    "end": "1273200"
  },
  {
    "text": "all your parquet data when you add a column in lance format you're only doing",
    "start": "1273200",
    "end": "1278720"
  },
  {
    "text": "a metadata operation and writing that data once so uh you know if you have 10",
    "start": "1278720",
    "end": "1284480"
  },
  {
    "text": "pabytes of images in a parket table and you add one bite in a new column per row",
    "start": "1284480",
    "end": "1292960"
  },
  {
    "text": "you end up rewriting that 10 pabytes potentially that's really bad",
    "start": "1292960",
    "end": "1299960"
  },
  {
    "text": "yeah yeah so the question is like the benefit is easily adding new columns yeah yeah",
    "start": "1309039",
    "end": "1316520"
  },
  {
    "text": "yeah so the the thing about that picture is that there are five or six different",
    "start": "1327440",
    "end": "1332760"
  },
  {
    "text": "systems four of which are like four different storage systems potentially in play there right so you'll have your",
    "start": "1332760",
    "end": "1339120"
  },
  {
    "text": "data lake storing in an object store and you might have big query or snowflake or",
    "start": "1339120",
    "end": "1344400"
  },
  {
    "text": "something to do you know analytic queries right and you would have a copy of some of that data living in some",
    "start": "1344400",
    "end": "1349919"
  },
  {
    "text": "other database you want to do vector search fine go pull up we go pull up uh postgress with you know the vector",
    "start": "1349919",
    "end": "1356480"
  },
  {
    "text": "extensions now you have another database that has another copy of some of that data in some other system okay now you",
    "start": "1356480",
    "end": "1362880"
  },
  {
    "text": "want to do full text search all right pull up elastic search and have that next to that and add that to so you get",
    "start": "1362880",
    "end": "1369039"
  },
  {
    "text": "this operational problem where you have multiple fairly large potentially complex systems that you have to operate",
    "start": "1369039",
    "end": "1375120"
  },
  {
    "text": "and manage in order to get your full application working and you know with Lance if I have a small scale data set I",
    "start": "1375120",
    "end": "1382400"
  },
  {
    "text": "can just copy that along to my laptop and do it all on my laptop right I don't have to have five or six different distributed system running in the back I",
    "start": "1382400",
    "end": "1388880"
  },
  {
    "text": "just have all that data in a directory in cloud steamer and I can use the the WAN client and get access to all of that",
    "start": "1388880",
    "end": "1396400"
  },
  {
    "text": "so it's just operationally much simpler",
    "start": "1396400",
    "end": "1400760"
  },
  {
    "text": "yeah that's basically it there a lot of different systems and yeah some of them could be combined but you're still gonna have multiple systems and you don't get",
    "start": "1421360",
    "end": "1427919"
  },
  {
    "text": "one system that kind of does it all like I come from the Hadoop background they're like 30 zoo animals and like",
    "start": "1427919",
    "end": "1433360"
  },
  {
    "text": "that's probably about 28 too many right um and you know modern data stack has",
    "start": "1433360",
    "end": "1439200"
  },
  {
    "text": "the same problem right except now you have 10 different categories and you have five different choices that you can",
    "start": "1439200",
    "end": "1444240"
  },
  {
    "text": "make in each of them also way too complicated right can we can we simplify this problem distill it down to a",
    "start": "1444240",
    "end": "1449760"
  },
  {
    "text": "processing system and a storage system and and you know have that kind of in a more unified way hey let other",
    "start": "1449760",
    "end": "1455520"
  },
  {
    "text": "applications go query that data too right um and and you know there definitely will be cases where you're",
    "start": "1455520",
    "end": "1461760"
  },
  {
    "text": "going to want other systems you're going to want a click house if you're going to do real time like you know dashboarding and stuff but but that's kind of uh you",
    "start": "1461760",
    "end": "1469279"
  },
  {
    "text": "know derivative data and and not necessarily the source of truth data",
    "start": "1469279",
    "end": "1475000"
  },
  {
    "text": "it's not necessarily going to be better but it's going to be within striking distance right so the one of the the",
    "start": "1477520",
    "end": "1482559"
  },
  {
    "text": "secondary goals of Lance is to have good enough analytical query performance right so depending on the kinds of data",
    "start": "1482559",
    "end": "1489840"
  },
  {
    "text": "that you're processing um if you're storing multimodal data in parquet like Lance DB is going to beat the pants off",
    "start": "1489840",
    "end": "1495520"
  },
  {
    "text": "of it if you're just doing analytical workloads you might be better off with just having you know an analytical",
    "start": "1495520",
    "end": "1501120"
  },
  {
    "text": "database or data warehouse if that's all you have right but like if you want to be able to do all of these things on the",
    "start": "1501120",
    "end": "1506640"
  },
  {
    "text": "same storage system then um right now Lance seems like the only game in town for that",
    "start": "1506640",
    "end": "1514679"
  },
  {
    "text": "yeah so in like the iceberg ecosystem right you have iceberg the table format",
    "start": "1530640",
    "end": "1536159"
  },
  {
    "text": "you have parquet file format and then you'll have like um photon or um trino",
    "start": "1536159",
    "end": "1545120"
  },
  {
    "text": "or impala as your query engine right so there's kind of three different levels they all have different names and some of them you have different choices you",
    "start": "1545120",
    "end": "1551279"
  },
  {
    "text": "could choose or instead of parquet in lance right now there's the lance file",
    "start": "1551279",
    "end": "1557039"
  },
  {
    "text": "format which is kind of analogous to parquet there's the lance table format which is kind of analogous to iceberg",
    "start": "1557039",
    "end": "1564640"
  },
  {
    "text": "and there's lance db which is the processing system the thing that runs the queries the things that does the lookups and that kind of stuff so for",
    "start": "1564640",
    "end": "1571279"
  },
  {
    "text": "better or for worse they're all named lance right now but they kind of cover all three of these different levels um",
    "start": "1571279",
    "end": "1576640"
  },
  {
    "text": "and uh um yeah like you know It's uh and there's going to be kind of more",
    "start": "1576640",
    "end": "1582640"
  },
  {
    "text": "compatibility between these systems that we're going to try to build out",
    "start": "1582640",
    "end": "1587720"
  },
  {
    "text": "my second question",
    "start": "1592960",
    "end": "1596440"
  },
  {
    "text": "yeah so you know this is all kind of the lower levels so the first question is is there compression yes there is um the",
    "start": "1605039",
    "end": "1610799"
  },
  {
    "text": "second question is do I need a catalog or something higher level so right now there is no you know um it's called a",
    "start": "1610799",
    "end": "1616960"
  },
  {
    "text": "governance data governance data catalog there's no data get a governance data catalog on top of lance today there is",
    "start": "1616960",
    "end": "1624799"
  },
  {
    "text": "um you know uh if you use the enterprise version there is something that's kind of similar to the hive meta store um",
    "start": "1624799",
    "end": "1630880"
  },
  {
    "text": "that's in the works or kind of there so that you can look up things um and uh you know go through a service instead of",
    "start": "1630880",
    "end": "1636799"
  },
  {
    "text": "having this embedded um so there there is some of that and there's gonna be a lot more work on that soon",
    "start": "1636799",
    "end": "1644120"
  },
  {
    "text": "i need to double check on that answer i don't know completely but I I think that something it's either the hyenasur or something similar serves that function",
    "start": "1645679",
    "end": "1654600"
  },
  {
    "text": "question",
    "start": "1655200",
    "end": "1658200"
  },
  {
    "text": "yes",
    "start": "1673919",
    "end": "1676919"
  },
  {
    "text": "um so the ask is a comparison between like the iceberg stack tools uh versus",
    "start": "1684760",
    "end": "1692080"
  },
  {
    "text": "um what the the LAN stack is and I I think there's a world where both of these coexists right like that",
    "start": "1692080",
    "end": "1698240"
  },
  {
    "text": "analytical data stack is great from that structured data and that semi-structured data and right now Lance like we're",
    "start": "1698240",
    "end": "1705520"
  },
  {
    "text": "focusing multimodal data that is just not really handled very well over there",
    "start": "1705520",
    "end": "1712440"
  },
  {
    "text": "so um if you wanted to do these large blobs in the analytic you know analytic",
    "start": "1716960",
    "end": "1723760"
  },
  {
    "text": "uh sorry in the um iceberg model right you have kind of two options you can",
    "start": "1723760",
    "end": "1729360"
  },
  {
    "text": "stick the big files off in a corner somewhere and have pointers into it and now you have this consistency problem that you have to maintain because",
    "start": "1729360",
    "end": "1735039"
  },
  {
    "text": "they're two different storage systems and the metadata service like HMS doesn't really know about the files you",
    "start": "1735039",
    "end": "1740399"
  },
  {
    "text": "have to kind of go through this extra layer to get there in the Lance world um it's just embedded into these lance",
    "start": "1740399",
    "end": "1746640"
  },
  {
    "text": "formatted files right and you know it's it's very efficient like one or two IOPS to get to the blob of data or get to the",
    "start": "1746640",
    "end": "1754960"
  },
  {
    "text": "compressed columner data that you need to do more analytical queries right so",
    "start": "1754960",
    "end": "1760480"
  },
  {
    "text": "um by putting all of the data multimodal data along with the scalar data you can",
    "start": "1760480",
    "end": "1766799"
  },
  {
    "text": "have transactional guarantees between these kinds of things you can version them you can try different uh let's say",
    "start": "1766799",
    "end": "1773840"
  },
  {
    "text": "LLM paths or different prompts store the data in go back in time and snapshot",
    "start": "1773840",
    "end": "1779760"
  },
  {
    "text": "back the entire state the raw and the previous version of the result that was generated from the LLMs um and in the",
    "start": "1779760",
    "end": "1786880"
  },
  {
    "text": "other world uh you would have to a different management problem there right like you'd have to potentially coordinate between three or four",
    "start": "1786880",
    "end": "1792799"
  },
  {
    "text": "different storage systems to get the full text search the um vector index and",
    "start": "1792799",
    "end": "1799520"
  },
  {
    "text": "the scalar indexes all in sync again and in Lance you can do that all in one place",
    "start": "1799520",
    "end": "1807159"
  },
  {
    "text": "sure",
    "start": "1808720",
    "end": "1811720"
  },
  {
    "text": "so all this goodness is great but there's a price that we pay right we're",
    "start": "1821720",
    "end": "1827279"
  },
  {
    "text": "not great at lots of transactional inputs coming into the system right we're good at bulk imports of data so if",
    "start": "1827279",
    "end": "1834559"
  },
  {
    "text": "you want streaming data and you want to process like one event at a time Lance",
    "start": "1834559",
    "end": "1839840"
  },
  {
    "text": "is not the best choice for you use something else or take that stream of data and batch it and turn it into a",
    "start": "1839840",
    "end": "1846159"
  },
  {
    "text": "batch operation and then then we might have a better story there but like the trade-off we're making for all these",
    "start": "1846159",
    "end": "1851520"
  },
  {
    "text": "different kinds of uh querying capabilities and efficiency with the um",
    "start": "1851520",
    "end": "1857200"
  },
  {
    "text": "you know incremental writes and column stuff is that we're not going to be great at lots of small transactional",
    "start": "1857200",
    "end": "1864279"
  },
  {
    "text": "rights and maybe someday we'll tackle that but that's not a primary concern for this kind of multimodal data",
    "start": "1864279",
    "end": "1872399"
  },
  {
    "text": "all right I think I'm a little bit past my time but um I'm happy to ask more questions later you know offline i want",
    "start": "1873520",
    "end": "1879520"
  },
  {
    "text": "to make sure that Richard has time to give his talk as well but thank you very much i hope this was helpful for you all",
    "start": "1879520",
    "end": "1887640"
  },
  {
    "text": "hi so just doing the sound real",
    "start": "1940320",
    "end": "1947919"
  },
  {
    "text": "quick okay can everyone hear me um so my name is Richard um I'll be talking about",
    "start": "1949480",
    "end": "1956320"
  },
  {
    "text": "some of the new APIs that we've been introducing in the Ray uh ecosystem recently especially around LM",
    "start": "1956320",
    "end": "1963720"
  },
  {
    "text": "inference so a little bit about me to kick things off i'm a product manager here at any scale i previously was doing",
    "start": "1963720",
    "end": "1970720"
  },
  {
    "text": "my PhD um at the UC Berkeley Rise Lab now the Skyab um where I was working",
    "start": "1970720",
    "end": "1976159"
  },
  {
    "text": "with Yan and Joey um and uh since then",
    "start": "1976159",
    "end": "1981200"
  },
  {
    "text": "I've sort of joined any scale um and I've been working on Ray for the last three or seven years um and so today",
    "start": "1981200",
    "end": "1988000"
  },
  {
    "text": "I'll be talking a little bit about um the top like the topic of serving LLMs",
    "start": "1988000",
    "end": "1994080"
  },
  {
    "text": "with especially open source LMS with Ry and so we'll start with like why is it",
    "start": "1994080",
    "end": "1999200"
  },
  {
    "text": "hard to do this uh especially with the open source whatever is available in open source today i'll talk then a",
    "start": "1999200",
    "end": "2004720"
  },
  {
    "text": "little bit about uh two modes of inferencing one is like batch offline inferencing and how you can do that",
    "start": "2004720",
    "end": "2010559"
  },
  {
    "text": "really easily with some of the new APIs that we've introduc in introduced with ray data lm and then online inferencing",
    "start": "2010559",
    "end": "2018000"
  },
  {
    "text": "which is also a very common use case and you can do that really easily with race serve so um to sort of set a stage um we",
    "start": "2018000",
    "end": "2027279"
  },
  {
    "text": "all know that like open source models are getting better and better and especially with Nepc coming out um this",
    "start": "2027279",
    "end": "2033360"
  },
  {
    "text": "past year we've sort of seen like the frontier um the top frontier of the open source or like model intelligence sort",
    "start": "2033360",
    "end": "2040240"
  },
  {
    "text": "of being open sourced now um in order to take advantage of that like a lot of users",
    "start": "2040240",
    "end": "2046159"
  },
  {
    "text": "often want to integrate this into existing workflows whether it be sort of um uh LS specific workflows like rag or",
    "start": "2046159",
    "end": "2054240"
  },
  {
    "text": "just like common day-to-day data applications or like within sort of userf facing serving applications as",
    "start": "2054240",
    "end": "2062040"
  },
  {
    "text": "well um but it's actually not really easy to do this and I'll sort of explain why um and I think what's actually",
    "start": "2062040",
    "end": "2070398"
  },
  {
    "text": "really important is to distinguish between like the two different type of common inference pipelines that we see",
    "start": "2070399",
    "end": "2076560"
  },
  {
    "text": "one being the batch online batch LM use case where we have offline we care about",
    "start": "2076560",
    "end": "2082240"
  },
  {
    "text": "high throughput we are typically doing this within existing data pipeline and the second one is online LM infringer",
    "start": "2082240",
    "end": "2089118"
  },
  {
    "text": "which is about low latency inference we care about the user response time uh and we care about sort of doing this in a",
    "start": "2089119",
    "end": "2095358"
  },
  {
    "text": "serving pipeline along with other business logic in most cases there are problems that exist today and there are",
    "start": "2095359",
    "end": "2101760"
  },
  {
    "text": "problems that we're trying to solve with some of the new APIs we've introduced So today I'll start off with talking",
    "start": "2101760",
    "end": "2108240"
  },
  {
    "text": "about batch LM inference and specifically this is the uh offline high throughput use",
    "start": "2108240",
    "end": "2113480"
  },
  {
    "text": "cases um so with LLMs it's actually quite easy to do um batch inference",
    "start": "2113480",
    "end": "2119760"
  },
  {
    "text": "especially with existing uh inference engine if you're running on a single replica um DLM master provide like",
    "start": "2119760",
    "end": "2126640"
  },
  {
    "text": "really simple APIs to sort of call and allow you to to execute high throughput",
    "start": "2126640",
    "end": "2132800"
  },
  {
    "text": "execution um high throughput sort of inference on a single GPU or like on a single model however as you s want to",
    "start": "2132800",
    "end": "2140720"
  },
  {
    "text": "start scaling this out and you saturate the ability to do this on a single node um and you want to sort of be able to",
    "start": "2140720",
    "end": "2145920"
  },
  {
    "text": "handle multiple replicas this becomes much harder because that is beyond the scope of BLM and it's beyond the scope",
    "start": "2145920",
    "end": "2151680"
  },
  {
    "text": "of S3 so you need to start figuring out how to load balance now um oftent times like what",
    "start": "2151680",
    "end": "2158560"
  },
  {
    "text": "you might look towards is like being able to do load balancing with an existing data processing engine such as",
    "start": "2158560",
    "end": "2163680"
  },
  {
    "text": "like Spark or Trader or or Flink and um the problem with that is that like you",
    "start": "2163680",
    "end": "2169839"
  },
  {
    "text": "actually in in Spark for example like you have to sort of deploy on two different sort of systems you have spark",
    "start": "2169839",
    "end": "2176079"
  },
  {
    "text": "which handles your data processing and you have a separate online inference engine or online inference inferencing",
    "start": "2176079",
    "end": "2182560"
  },
  {
    "text": "service um which complicates your deployment process so you actually want to sort of simplify the deployment",
    "start": "2182560",
    "end": "2188240"
  },
  {
    "text": "process by integrating that all in one application um existing data processing systems unfortunately have the",
    "start": "2188240",
    "end": "2194800"
  },
  {
    "text": "limitation of not being able to support that because GPU support and um supporting large models and having very",
    "start": "2194800",
    "end": "2201520"
  },
  {
    "text": "very large uh functions where the um you know the the size of the GDF can be",
    "start": "2201520",
    "end": "2206880"
  },
  {
    "text": "gigabytes uh it's not something that has was something that was present in the previous big data era and therefore sort",
    "start": "2206880",
    "end": "2213760"
  },
  {
    "text": "of break some of the existing assumptions on uh these big data processing systems",
    "start": "2213760",
    "end": "2218960"
  },
  {
    "text": "so um this is where ray data comes in and I'll talk a little bit about who ray data is for those who who don't know um",
    "start": "2218960",
    "end": "2226560"
  },
  {
    "text": "so ray data is a sort of a a scalable fall tolerant set of APIs for uh um for",
    "start": "2226560",
    "end": "2233119"
  },
  {
    "text": "data processing it's fundamentally a data processing engine built for uh for GPU this like GPU era and being able to",
    "start": "2233119",
    "end": "2240160"
  },
  {
    "text": "do inferencing it provides first class support for GPU scheduling it provides actors uh to avoid heavy startup times",
    "start": "2240160",
    "end": "2247280"
  },
  {
    "text": "especially because sometimes starting up the model can take a long time on GPUs and also it integrates very closely with",
    "start": "2247280",
    "end": "2253680"
  },
  {
    "text": "the rest of the uh machine learning and data ecosystem such as like pyro numpy bandas and also pietorrch in order to",
    "start": "2253680",
    "end": "2260640"
  },
  {
    "text": "sort of tie the workflows together and finally there is the capability to sort of stream data through all the operators",
    "start": "2260640",
    "end": "2268000"
  },
  {
    "text": "for like a very very efficient sort of mixed CPU and GPU execution so all this sort of lines up",
    "start": "2268000",
    "end": "2274560"
  },
  {
    "text": "really well to sort of sit within like a a pipeline that does um inferencing especially for LLMs and so then the",
    "start": "2274560",
    "end": "2281440"
  },
  {
    "text": "question is like okay well if this is already this already exists and it provides all the scaling capabilities",
    "start": "2281440",
    "end": "2287040"
  },
  {
    "text": "that I need the load balancing capabilities I need and also has a GPU sort of uh support then why is it hard",
    "start": "2287040",
    "end": "2293280"
  },
  {
    "text": "or what is buzzer that that we are announcing um so the key problem is that like to rate data it's actually kind of",
    "start": "2293280",
    "end": "2300240"
  },
  {
    "text": "difficult to do high performance sort of execution especially for batch LM inference um and the reason why is",
    "start": "2300240",
    "end": "2307760"
  },
  {
    "text": "because there's actually a lot of blue play they have to write in order to make this really efficient um there's the need to sort of support continuous",
    "start": "2307760",
    "end": "2314160"
  },
  {
    "text": "batching so you want to sort of continuously feed um the GPU very you know constantly for uh as you sort of",
    "start": "2314160",
    "end": "2320960"
  },
  {
    "text": "receive a ton of data uh you might want to sort of actually tinker around with like the actual architecture of the LM",
    "start": "2320960",
    "end": "2327359"
  },
  {
    "text": "engine to actually improve throughput and when you have large models being",
    "start": "2327359",
    "end": "2332480"
  },
  {
    "text": "able to do handle like a multi-node model within an existing data pipeline is actually something that requires",
    "start": "2332480",
    "end": "2338800"
  },
  {
    "text": "quite a bit of uh internal ray knowledge in order to get working and so what we're actually",
    "start": "2338800",
    "end": "2345280"
  },
  {
    "text": "introducing today is a a set of APIs that actually make this really simple for you and this is what we call ray",
    "start": "2345280",
    "end": "2350720"
  },
  {
    "text": "data lm it directly integrates with the ray data pipelines it has first class support for continuous batching um it",
    "start": "2350720",
    "end": "2358000"
  },
  {
    "text": "sort of has all like the nice tricks around like disagregating tokenization so that you can actually improve",
    "start": "2358000",
    "end": "2363760"
  },
  {
    "text": "throughput very effectively and has obviously with ray data like and other data existing systems like you have the",
    "start": "2363760",
    "end": "2370000"
  },
  {
    "text": "scaling and the balancing capabilities that come underneath uh with with just the properties of the API and and model",
    "start": "2370000",
    "end": "2377200"
  },
  {
    "text": "parallelism becomes actually something that's supported first class i'll talk through a couple examples of what this looks like in terms of the code",
    "start": "2377200",
    "end": "2385680"
  },
  {
    "text": "so um to start we actually define a configuration that that sort of",
    "start": "2385680",
    "end": "2390960"
  },
  {
    "text": "specifies like the model that you're trying to inference with and and also like you know custom VLM arguments um",
    "start": "2390960",
    "end": "2398480"
  },
  {
    "text": "that that you might want to configure your engine with so just looking walking through this code more specifically you",
    "start": "2398480",
    "end": "2404720"
  },
  {
    "text": "specify a model on the very top with the model source API you can configure the",
    "start": "2404720",
    "end": "2410160"
  },
  {
    "text": "VM engine that will run for replica including sort of tensor parallelism or pipeline parallelism and ray will",
    "start": "2410160",
    "end": "2416960"
  },
  {
    "text": "automatically take care of this underneath the hood um you can also configure the data parallelism scaling",
    "start": "2416960",
    "end": "2422720"
  },
  {
    "text": "so that's is the number of replicas and you can configure to autoscale and then also finally you can",
    "start": "2422720",
    "end": "2429359"
  },
  {
    "text": "configure the batching that happens uh for continuous batching uh within each",
    "start": "2429359",
    "end": "2435960"
  },
  {
    "text": "replica um so if we sort of simplify this configuration how do we sort of plug this into the existing data",
    "start": "2435960",
    "end": "2442320"
  },
  {
    "text": "pipeline right so you basically can build out this processor object which takes in this config and provides you",
    "start": "2442320",
    "end": "2449760"
  },
  {
    "text": "abilities to specify um standard sort of LM um uh sort of prompts such as the",
    "start": "2449760",
    "end": "2456240"
  },
  {
    "text": "system prompt and exactly what you want it to do when you're receiving like the data entry and then you can also specify",
    "start": "2456240",
    "end": "2463440"
  },
  {
    "text": "some post-processing to to sort of process exactly what happens after the",
    "start": "2463440",
    "end": "2468560"
  },
  {
    "text": "LM has generated text and all this will flow fluently with the existing ray data API so you",
    "start": "2468560",
    "end": "2475280"
  },
  {
    "text": "can do something where you read from parquet or read from S3 or you provide like a list of text uh uh text for ray",
    "start": "2475280",
    "end": "2483119"
  },
  {
    "text": "data to sort of read and then you can just hand the data set to this processor which will automatically um run map",
    "start": "2483119",
    "end": "2490000"
  },
  {
    "text": "batches and like all the other lazy uh ray data operators so that this can get",
    "start": "2490000",
    "end": "2495280"
  },
  {
    "text": "executed in a very efficient fashion so this is basically what we've sort of",
    "start": "2495280",
    "end": "2501040"
  },
  {
    "text": "introduced and as you noted um a lot of the boiler plate that comes with needing",
    "start": "2501040",
    "end": "2506319"
  },
  {
    "text": "to figure out how to orchestrate um multiple actors in order to do model",
    "start": "2506319",
    "end": "2511920"
  },
  {
    "text": "parallelism or like how do we sort of configure VLM and and specify the VLM",
    "start": "2511920",
    "end": "2517040"
  },
  {
    "text": "engine all that sort of goes away and we have this like nice API to sort of clean up and and simplify the deployment",
    "start": "2517040",
    "end": "2523280"
  },
  {
    "text": "process here um and again just to recap Ray Data LM has out of the box support for a lot of",
    "start": "2523280",
    "end": "2530000"
  },
  {
    "text": "the performance improvements that are needed um it allows you to load balance really effectively across cluster and",
    "start": "2530000",
    "end": "2536000"
  },
  {
    "text": "for more complex models and it sort of will scale with the size of the model as",
    "start": "2536000",
    "end": "2542680"
  },
  {
    "text": "well so we've talked a little bit about batch inference and now I'm going to talk a little bit more about online",
    "start": "2542680",
    "end": "2549760"
  },
  {
    "text": "inference and so this is the situation where you care about latency you want low latency inference you want the",
    "start": "2549760",
    "end": "2555440"
  },
  {
    "text": "inference to respond very quickly and you want to be able to do this within existing serving",
    "start": "2555440",
    "end": "2561160"
  },
  {
    "text": "pipelines so u as as we sort of talked about in the previous slides um in the",
    "start": "2561160",
    "end": "2566520"
  },
  {
    "text": "batchm use case um DLM and other inference engines are really good for single model inference and that still",
    "start": "2566520",
    "end": "2573200"
  },
  {
    "text": "holds true for online inference as well however um obviously as you sort of",
    "start": "2573200",
    "end": "2578640"
  },
  {
    "text": "start scaling out your deployment and you get get more users you want to sort of be able to handle data parallelism",
    "start": "2578640",
    "end": "2584800"
  },
  {
    "text": "and that's something that's out of script for BLM now in particular for online inference you also care about um",
    "start": "2584800",
    "end": "2591359"
  },
  {
    "text": "there's an emerging need to be able to sort of have multiple LMS talk to each other and this is what is sort of deemed",
    "start": "2591359",
    "end": "2597520"
  },
  {
    "text": "as agentic serving right um you have LMS that sort of call other LLMs through through tool use and these LMS can call",
    "start": "2597520",
    "end": "2604240"
  },
  {
    "text": "other LMS or call their sort of um servers like MCP servers or something like that and this sort of like like",
    "start": "2604240",
    "end": "2610960"
  },
  {
    "text": "multi- aent sort of situation gets very very complex and becomes very heavy on the sort of deployment logic imagine",
    "start": "2610960",
    "end": "2617680"
  },
  {
    "text": "trying to do this with YAML it's going to be a nightmare and so um so here's where I",
    "start": "2617680",
    "end": "2623760"
  },
  {
    "text": "want to sort of bring in Ray Serve think that Rayerf actually solves a lot of the problems for you but again there's sort",
    "start": "2623760",
    "end": "2629520"
  },
  {
    "text": "of going to be some limitations um with racer for what um what it can do today",
    "start": "2629520",
    "end": "2635040"
  },
  {
    "text": "and why we need a new set of APIs so Rayerf is a um flexible scalable",
    "start": "2635040",
    "end": "2642480"
  },
  {
    "text": "uh sort of compute layer for uh online inferencing it provides a Python native",
    "start": "2642480",
    "end": "2647920"
  },
  {
    "text": "API so that you can mix business logic and machine learning models very quickly um it provides first class support for",
    "start": "2647920",
    "end": "2655040"
  },
  {
    "text": "you to to write these like multimodel inferencing pipelines just as you would",
    "start": "2655040",
    "end": "2660079"
  },
  {
    "text": "write standard Python and it provides also ability to scale really flexibly",
    "start": "2660079",
    "end": "2665119"
  },
  {
    "text": "and and uh specify fine grain resource allocations so that we can reduce",
    "start": "2665119",
    "end": "2670280"
  },
  {
    "text": "costs um so uh with race serve I think one of the big things is that like",
    "start": "2670280",
    "end": "2676400"
  },
  {
    "text": "obviously we can solve a lot of problems around autoscaling low bouncing all these standard production requirements",
    "start": "2676400",
    "end": "2682240"
  },
  {
    "text": "and in addition to that we sort of are able to support these like multimodel training pipelines this kind of looks",
    "start": "2682240",
    "end": "2689520"
  },
  {
    "text": "like the right solution but there's some limitations here right and this is where like real world production requirements",
    "start": "2689520",
    "end": "2695520"
  },
  {
    "text": "sort of come in especially around LMS um oftentimes with LMS like you're going to",
    "start": "2695520",
    "end": "2701119"
  },
  {
    "text": "have a lot of users that require uh sort of fine-tunes and fine tunes can be sort of varied across um across your sort of",
    "start": "2701119",
    "end": "2708640"
  },
  {
    "text": "cluster and across your use user base and so you're going to need to manage or fine tunes very effectively and this is",
    "start": "2708640",
    "end": "2714560"
  },
  {
    "text": "where we call what we call Laura management um in addition to that like with um oftent times like we like uh",
    "start": "2714560",
    "end": "2722000"
  },
  {
    "text": "deploying LLMs requires like a open AI compatible API that's something that BLM comes out of the box with but if you",
    "start": "2722000",
    "end": "2728000"
  },
  {
    "text": "were to use ray serve this is something that you have to implement by yourself and then finally similar to ray data",
    "start": "2728000",
    "end": "2734480"
  },
  {
    "text": "orchestrating your model parallelism where you have multiple autoscaling ray uh autoscaling VLMs each of which have",
    "start": "2734480",
    "end": "2742079"
  },
  {
    "text": "their own sort of set of actors to take care of or set of model shorts to take care becomes very very complex for",
    "start": "2742079",
    "end": "2748960"
  },
  {
    "text": "example it's actually quite hard to be able to do this with like deepsek which requires you to have like 16 different",
    "start": "2748960",
    "end": "2754880"
  },
  {
    "text": "sort of shards per replica and and and scaling that sort of linearly as you",
    "start": "2754880",
    "end": "2760480"
  },
  {
    "text": "have demand come in now there's ways to do this but it's actually just it's just a lot of play and you have to do this by",
    "start": "2760480",
    "end": "2767640"
  },
  {
    "text": "yourself so um here's where we're introducing Racer LM it provides native",
    "start": "2767640",
    "end": "2773760"
  },
  {
    "text": "APIs for you to have out of the box support for serving LMS on top of Ray serve um it's going to have an open",
    "start": "2773760",
    "end": "2780319"
  },
  {
    "text": "compatible API that can sit across multiple models it's going to have",
    "start": "2780319",
    "end": "2785359"
  },
  {
    "text": "multiplexing and management out of the box you can configure a model parallel",
    "start": "2785359",
    "end": "2790960"
  },
  {
    "text": "parallelism simply uh very easily just by using the standard BLM APIs and the",
    "start": "2790960",
    "end": "2796560"
  },
  {
    "text": "scaling will happen and scaling and sort of orchestration will happen uh for you underneath the hood uh there's a router",
    "start": "2796560",
    "end": "2802960"
  },
  {
    "text": "that comes with that allows you to route across multiple models and then obviously with rays um it already comes",
    "start": "2802960",
    "end": "2809520"
  },
  {
    "text": "with like the ability to do scaling very effectively autoscaling balancing to so",
    "start": "2809520",
    "end": "2814920"
  },
  {
    "text": "forth that's not walk through a couple sort of API examples so that we make this a little bit more",
    "start": "2814920",
    "end": "2821800"
  },
  {
    "text": "concrete um so here's like a similar sort of config object that we have with",
    "start": "2821800",
    "end": "2827359"
  },
  {
    "text": "uh with um with race LLM and I'll walk through this really quickly right so you",
    "start": "2827359",
    "end": "2832480"
  },
  {
    "text": "have a place where you specify the models this is very similar to what we saw earlier with rad you have a place where you can",
    "start": "2832480",
    "end": "2839200"
  },
  {
    "text": "configure VLM this is a little bit shorter but what I just wanted to show is that this is where you would configure your tensor parallelism for",
    "start": "2839200",
    "end": "2845280"
  },
  {
    "text": "example or your your pipeline parallelism and this can sort of span across nodes as",
    "start": "2845280",
    "end": "2850680"
  },
  {
    "text": "well and finally you configure your scaling right you have all scaling configured by minmax so on so forth and",
    "start": "2850680",
    "end": "2857920"
  },
  {
    "text": "what's like the failure recovery um uh situation and um with that there's a",
    "start": "2857920",
    "end": "2865200"
  },
  {
    "text": "couple ways to take this configuration and deploy it um there's a simple way",
    "start": "2865200",
    "end": "2870560"
  },
  {
    "text": "where you can uh provide like the builder API and you can pass in the configuration and you can pass this this",
    "start": "2870560",
    "end": "2876880"
  },
  {
    "text": "all into serve run and this will run this locally this is kind of similar to what you would do with fast",
    "start": "2876880",
    "end": "2882680"
  },
  {
    "text": "API um and the other way to do this is you have something that's a little bit more racer native um where people are",
    "start": "2882680",
    "end": "2890000"
  },
  {
    "text": "working with deployments and are are familiar with the bind concept and you can sort of work with um these LM",
    "start": "2890000",
    "end": "2896800"
  },
  {
    "text": "configs as deployment objects which is a little bit more native to what race other racer programs might",
    "start": "2896800",
    "end": "2904680"
  },
  {
    "text": "use now um some more examples here are like how do you sort of make sure you",
    "start": "2904680",
    "end": "2910480"
  },
  {
    "text": "can support Dora right um and so this is actually simple configuration as well where you can specify like a directory",
    "start": "2910480",
    "end": "2917119"
  },
  {
    "text": "read that holds thousands of luras and um and specify that like you want to enable Laura support and from here then",
    "start": "2917119",
    "end": "2924640"
  },
  {
    "text": "you can sort of automatically have your server manage ton of different Laura um",
    "start": "2924640",
    "end": "2929920"
  },
  {
    "text": "Laura adapters and be able to sort of autoscale and you know scale down and",
    "start": "2929920",
    "end": "2935040"
  },
  {
    "text": "not lose track of what's going on um and then on the AP on the the client side you'd be able to query and specify",
    "start": "2935040",
    "end": "2940880"
  },
  {
    "text": "exactly which for you want and then the racer will sort of route that for",
    "start": "2940880",
    "end": "2946520"
  },
  {
    "text": "you um and then there's also the other example where similar to OpenAI um where",
    "start": "2946520",
    "end": "2952240"
  },
  {
    "text": "they have multiple models of different sort of resolutions uh you may want to serve like different size models within",
    "start": "2952240",
    "end": "2957760"
  },
  {
    "text": "the same cluster and for different use cases right and so this is actually very simple to do in the new um the uh LM",
    "start": "2957760",
    "end": "2965760"
  },
  {
    "text": "APIs with Ray Serve where you just specify multiple configs and be able to pass that into like the builder API and",
    "start": "2965760",
    "end": "2972480"
  },
  {
    "text": "and run that now what I don't show within these examples is the way you would do this in",
    "start": "2972480",
    "end": "2978559"
  },
  {
    "text": "production with like the uh YAML configurations but there are examples of and and sort of like out of blocks uh",
    "start": "2978559",
    "end": "2985200"
  },
  {
    "text": "sort of uh boiler plates that you can take on the rate to surf documentation",
    "start": "2985200",
    "end": "2991119"
  },
  {
    "text": "um so that's a little bit re um of what we sort of introduced here um and just",
    "start": "2991119",
    "end": "2996960"
  },
  {
    "text": "to recap I've talked a little bit about why LM serving can be kind of difficult with some of the open LMS uh open source",
    "start": "2996960",
    "end": "3004240"
  },
  {
    "text": "LMS today and I've talked about two two set of APIs that we've introduced um",
    "start": "3004240",
    "end": "3009280"
  },
  {
    "text": "with the most recent Ray release one is the batch LM inference APIs which come",
    "start": "3009280",
    "end": "3014640"
  },
  {
    "text": "with come with ray data LM and then the other one is the online LM inference APIs that come with RER um in in the",
    "start": "3014640",
    "end": "3022640"
  },
  {
    "text": "near future we'll be sort of expanding this road map with a lot of sort of new features including the ability to",
    "start": "3022640",
    "end": "3028640"
  },
  {
    "text": "support Deep Seekum in a very scalable effect effective and low latency way uh in addition to having better support for",
    "start": "3028640",
    "end": "3035200"
  },
  {
    "text": "different accelerators and TPUs um so yeah that's a little bit that's basically um my talk and would love to",
    "start": "3035200",
    "end": "3042400"
  },
  {
    "text": "sort of take any questions if you",
    "start": "3042400",
    "end": "3045680"
  },
  {
    "text": "have yeah",
    "start": "3052680",
    "end": "3056680"
  },
  {
    "text": "that's a great question so right now in the current rate of release BLM is the primary thing that we support we also",
    "start": "3063040",
    "end": "3069520"
  },
  {
    "text": "support sort of adapters for any open AI compatible API but that requires you to sort of manage it out of band right like",
    "start": "3069520",
    "end": "3076079"
  },
  {
    "text": "you'd have to sort of being a separate service we are current there is a open PR to support everything as well and",
    "start": "3076079",
    "end": "3081680"
  },
  {
    "text": "we're trying to get that merge as soon as",
    "start": "3081680",
    "end": "3085200"
  },
  {
    "text": "possible any other questions",
    "start": "3087480",
    "end": "3092040"
  },
  {
    "text": "yeah sure",
    "start": "3092880",
    "end": "3096200"
  },
  {
    "text": "yeah for sure so basically in other data processing systems oftentimes you have like uh processing units or like",
    "start": "3106160",
    "end": "3113520"
  },
  {
    "text": "processing functions and these get invoked sort of uh with the life cycle of like the the time at which it gets",
    "start": "3113520",
    "end": "3120480"
  },
  {
    "text": "processed right so so basically it's like it runs once and it disappears and",
    "start": "3120480",
    "end": "3125920"
  },
  {
    "text": "then if you have like a stage then you can imagine using that but you have to like load a GPU like a GPU model in and",
    "start": "3125920",
    "end": "3132720"
  },
  {
    "text": "the model is 600 gigabytes right so like basically every time like it like this function runs you have to wait you know",
    "start": "3132720",
    "end": "3139280"
  },
  {
    "text": "two minutes three minutes or five minutes or 10 minutes depending on how fast your model loading is um for that",
    "start": "3139280",
    "end": "3144319"
  },
  {
    "text": "GPU to start up for you to download the model weights and then do the processing and then throw it away and what reactors",
    "start": "3144319",
    "end": "3151680"
  },
  {
    "text": "allow you to do is sort of spin up and have this like constant set of operators where like you hold the model weights",
    "start": "3151680",
    "end": "3158880"
  },
  {
    "text": "hot in the GPU and then the rest of the data processing sort of can can happen",
    "start": "3158880",
    "end": "3164079"
  },
  {
    "text": "at the same time and so basically what ends up happening is throughout the entire life cycle of the data processing",
    "start": "3164079",
    "end": "3170319"
  },
  {
    "text": "um like the GPUs are able to m be maintained at high utilization instead of like the other situation where you",
    "start": "3170319",
    "end": "3177440"
  },
  {
    "text": "might have to like like continue like loading and uh and deleting like the GPU cache",
    "start": "3177440",
    "end": "3185319"
  },
  {
    "text": "derek only available",
    "start": "3189920",
    "end": "3196240"
  },
  {
    "text": "no so all these APIs are open source",
    "start": "3196240",
    "end": "3201240"
  },
  {
    "text": "ai inference worker",
    "start": "3217440",
    "end": "3221200"
  },
  {
    "text": "yeah that's a good question so right now like um right now I don't believe we use",
    "start": "3228319",
    "end": "3234160"
  },
  {
    "text": "so we definitely don't take a dependency on envoy we just implement our own set of like routers and like with simple",
    "start": "3234160",
    "end": "3240480"
  },
  {
    "text": "Python classes since you can do that very easily with reserve um what we are working on is making it so that it's a",
    "start": "3240480",
    "end": "3247200"
  },
  {
    "text": "little bit more aware of exactly what prompts are coming in so that we can do like prompt uh prompt aware prefix",
    "start": "3247200",
    "end": "3252960"
  },
  {
    "text": "routing and stuff like that um now with regards to autoscaling right now I",
    "start": "3252960",
    "end": "3258000"
  },
  {
    "text": "believe we just look at the sort of number of incoming requests and then sort of scale off of that if like the queue is growing uh in like an unbounded",
    "start": "3258000",
    "end": "3264800"
  },
  {
    "text": "fashion then then we start sort of increasing the number of models that are are",
    "start": "3264800",
    "end": "3270440"
  },
  {
    "text": "live yeah any other questions",
    "start": "3270440",
    "end": "3277720"
  },
  {
    "text": "ibrahim",
    "start": "3277720",
    "end": "3280720"
  },
  {
    "text": "yeah for example",
    "start": "3290280",
    "end": "3295240"
  },
  {
    "text": "yeah so um with for example for prefield disagregation right like what will",
    "start": "3316400",
    "end": "3321760"
  },
  {
    "text": "happen is likely that we will have at least the first quarter support will be",
    "start": "3321760",
    "end": "3326800"
  },
  {
    "text": "like be able to serve prefill and decode as separate like separate actors or",
    "start": "3326800",
    "end": "3332559"
  },
  {
    "text": "actor groups and serve that as a single replica and the communication will go through nickel or something that is a",
    "start": "3332559",
    "end": "3339040"
  },
  {
    "text": "little bit more out of band um in the future we will we are looking at using like compile graphs and GPU objects",
    "start": "3339040",
    "end": "3345119"
  },
  {
    "text": "which are new advanced concepts in ray core to be able to manage the communication and memory management a",
    "start": "3345119",
    "end": "3351359"
  },
  {
    "text": "little bit more native within the ray",
    "start": "3351359",
    "end": "3354798"
  },
  {
    "text": "system any other questions cool thanks everyone",
    "start": "3358680",
    "end": "3367799"
  }
]