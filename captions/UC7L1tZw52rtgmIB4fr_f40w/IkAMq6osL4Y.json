[
  {
    "start": "0",
    "end": "219000"
  },
  {
    "text": "okay i'm gonna start then um my name is korsh um and i",
    "start": "160",
    "end": "5279"
  },
  {
    "text": "joined any scale about a month ago uh working on rli before that i was a phd student at",
    "start": "5279",
    "end": "12240"
  },
  {
    "text": "berkeley uc berkeley working on rl um in context of robotics and design optimization",
    "start": "12240",
    "end": "18800"
  },
  {
    "text": "um so i'd be happy to essentially guide you know provide some",
    "start": "18800",
    "end": "24480"
  },
  {
    "text": "some guidelines to how we approach certain problems and align with what the questions that you guys have asked",
    "start": "24480",
    "end": "31840"
  },
  {
    "text": "yeah federico do you want to go next yeah so i am a",
    "start": "31840",
    "end": "37040"
  },
  {
    "text": "master student in machine learning at kth uh in stockholm",
    "start": "37040",
    "end": "43040"
  },
  {
    "text": "and i've been interested in interested in reinforcement learning for",
    "start": "43040",
    "end": "48559"
  },
  {
    "text": "the past two years so and in particular",
    "start": "48559",
    "end": "54879"
  },
  {
    "text": "in this last period i've been interested in multi-agent reinforcement learning",
    "start": "54879",
    "end": "60239"
  },
  {
    "text": "and i am using uh rla um yeah let me start so yeah hi",
    "start": "60239",
    "end": "66080"
  },
  {
    "text": "everyone uh great it's great to see you here my name is sven i'm the team lead of the reinforcement team at any scale",
    "start": "66080",
    "end": "73119"
  },
  {
    "text": "my team korosh is a part of it uh and he's probably new you haven't met him yet maybe but",
    "start": "73119",
    "end": "79200"
  },
  {
    "text": "we're responsible for developing maintaining our lip um which you're using i think i know most",
    "start": "79200",
    "end": "86080"
  },
  {
    "text": "of you except for federico i think you're and sebastian also i i'm excited to",
    "start": "86080",
    "end": "92479"
  },
  {
    "text": "hear what you guys are doing uh and yeah let's let's hear everyone else what uh what you do with our lip also um",
    "start": "92479",
    "end": "97840"
  },
  {
    "text": "always super welcome to to hear feedback from you especially um uh constructive feedback that's that's",
    "start": "97840",
    "end": "104240"
  },
  {
    "text": "uh like more on the like what we should change and what's what's what's terrible about",
    "start": "104240",
    "end": "109280"
  },
  {
    "text": "um we're always happy to hear that and to um to add that to our list of things to do",
    "start": "109280",
    "end": "114320"
  },
  {
    "text": "um because that's that's how our lip gets better over time and uh yeah this sounds like a really",
    "start": "114320",
    "end": "119439"
  },
  {
    "text": "interesting problem for the cookie we have been thinking about this especially corrosion has been thinking about this for the last couple weeks like a lot so this is it sounds like fun",
    "start": "119439",
    "end": "126640"
  },
  {
    "text": "yeah let's go around the entire room and then come back",
    "start": "126640",
    "end": "131680"
  },
  {
    "text": "sebastian sure um hey guys so i'm a data science",
    "start": "131680",
    "end": "137040"
  },
  {
    "text": "intern over at altair and i've been working with reinforcement learning since march uh so my project pretty much",
    "start": "137040",
    "end": "143599"
  },
  {
    "text": "involves implementing an agent to drive a",
    "start": "143599",
    "end": "148720"
  },
  {
    "text": "sailboat in a simulator and for that we've been using sac so we have a pretty big continuous uh",
    "start": "148720",
    "end": "155440"
  },
  {
    "text": "observation space and then eventually we're as we're learning about the sac agent and you",
    "start": "155440",
    "end": "161280"
  },
  {
    "text": "know how to optimize and what now we're adding more uh both controls to it so it's been it's been challenging but pretty fun",
    "start": "161280",
    "end": "167680"
  },
  {
    "text": "journey so yeah we're excited to learn more about uh reinforcement learning",
    "start": "167680",
    "end": "172879"
  },
  {
    "text": "yeah uh i'm the maintainer of gem and petting zoo and a handful of other of the large uh rl environment libraries um",
    "start": "172879",
    "end": "180640"
  },
  {
    "text": "recently i took over the mini grid and many world libraries that a lot of people use",
    "start": "180640",
    "end": "186480"
  },
  {
    "text": "um yeah i'm here to follow up on the past discussions of petting zoo",
    "start": "186480",
    "end": "191920"
  },
  {
    "text": "in rl lib and the breaking api changes that we've been doing in gym hi",
    "start": "191920",
    "end": "199040"
  },
  {
    "text": "i am the creator of the neural mmo platform a large-scale multi-agent simulator for training on decades of",
    "start": "199040",
    "end": "206400"
  },
  {
    "text": "simulated experience and i'm also here about the petting zoo integration today",
    "start": "206400",
    "end": "211519"
  },
  {
    "text": "though i might uh bother spending about a couple of neural mmo things if their time with integration on that",
    "start": "211519",
    "end": "218080"
  },
  {
    "text": "happy road i can get during directly to my question which is about multi-agents",
    "start": "218080",
    "end": "224799"
  },
  {
    "start": "219000",
    "end": "320000"
  },
  {
    "text": "and basically i've been reading some literature and i've seen a lot of",
    "start": "224799",
    "end": "229920"
  },
  {
    "text": "methods that in which there is an information exchange between agents",
    "start": "229920",
    "end": "237120"
  },
  {
    "text": "which is learned so um the the problem is we want each agent to",
    "start": "237120",
    "end": "244159"
  },
  {
    "text": "be aware of the other agents in the environment and a native way of doing this would be",
    "start": "244159",
    "end": "251439"
  },
  {
    "text": "to just hand crafting some features to pass to",
    "start": "251439",
    "end": "256799"
  },
  {
    "text": "to the agent that represent information about the other agents but an interesting approach is that each agent",
    "start": "256799",
    "end": "264400"
  },
  {
    "text": "generates some what we ca we could call a message and uh",
    "start": "264400",
    "end": "271120"
  },
  {
    "text": "this message is passed to the other agents and this procedure is uh",
    "start": "271120",
    "end": "276800"
  },
  {
    "text": "differentiable and therefore the agent learns uh how to produce a message",
    "start": "276800",
    "end": "283360"
  },
  {
    "text": "and so my more generally my question is how is it",
    "start": "283360",
    "end": "288560"
  },
  {
    "text": "possible to do this in our lib and if yes how and a bit more specifically i'm",
    "start": "288560",
    "end": "294639"
  },
  {
    "text": "interested in a setting where each agent has a quite different",
    "start": "294639",
    "end": "299919"
  },
  {
    "text": "observation space which is different in size and meaning and therefore",
    "start": "299919",
    "end": "305120"
  },
  {
    "text": "the model that generates the message can't be shared",
    "start": "305120",
    "end": "311840"
  },
  {
    "text": "across all the agents but each agent has needs to have its own model that produces a",
    "start": "311919",
    "end": "317600"
  },
  {
    "text": "message for the other agents um yeah just to summarize the question i think the",
    "start": "317600",
    "end": "323120"
  },
  {
    "start": "320000",
    "end": "415000"
  },
  {
    "text": "question is more about like how do we share information um between agents and between like",
    "start": "323120",
    "end": "329600"
  },
  {
    "text": "models within our lib context um so this question is like about",
    "start": "329600",
    "end": "334960"
  },
  {
    "text": "essentially has multiple facets i guess stress to it um when we think about like",
    "start": "334960",
    "end": "340479"
  },
  {
    "text": "sharing information um it can be categorized in different ways one is that in multi-agent context",
    "start": "340479",
    "end": "347360"
  },
  {
    "text": "the agents can i mean does that is it possible in early to share raw observation space of different agents",
    "start": "347360",
    "end": "354400"
  },
  {
    "text": "between each other right so for example agent one for computing its value function it needs to know",
    "start": "354400",
    "end": "361199"
  },
  {
    "text": "it needs to condition its model on the observation of other agents um and that's actually possible",
    "start": "361199",
    "end": "367680"
  },
  {
    "text": "like and it's very straightforward um and we have like a very good example of this called centralized critic",
    "start": "367680",
    "end": "374240"
  },
  {
    "text": "um in one of the examples things and i can share it to you but like i guess i want to clarify that",
    "start": "374240",
    "end": "381120"
  },
  {
    "text": "this is this is just like a non-differentiable uh information sharing where you just have to augment",
    "start": "381120",
    "end": "387840"
  },
  {
    "text": "current agents um states with the other agent's observations and like maybe actions and that's um done through",
    "start": "387840",
    "end": "396000"
  },
  {
    "text": "through some hooks that we have already in the early in the multi-agent api for the rla",
    "start": "396000",
    "end": "401360"
  },
  {
    "text": "and i'm going to show you how that's actually done and then we get back to",
    "start": "401360",
    "end": "406479"
  },
  {
    "text": "the more i guess the challenging case where you want to have like a differentiable um information sharing between agents",
    "start": "406479",
    "end": "414160"
  },
  {
    "text": "so um this is my work space so um this is called like critic um",
    "start": "414160",
    "end": "421199"
  },
  {
    "start": "415000",
    "end": "672000"
  },
  {
    "text": "centralized predict it's in under our ellipse example centralized",
    "start": "421199",
    "end": "427520"
  },
  {
    "text": "critics there are two versions of this there are two ways of doing this and centralized critic two is",
    "start": "427520",
    "end": "434880"
  },
  {
    "text": "i guess the one that we have provided like built-in apis for um so basically",
    "start": "434880",
    "end": "441440"
  },
  {
    "text": "in your constructor of essentially your config files",
    "start": "441440",
    "end": "446639"
  },
  {
    "text": "uh when you specify these multi-agent things um you know policies uh very like",
    "start": "446639",
    "end": "452080"
  },
  {
    "text": "there's a policy map and there's a falsity mapping function uh we also accept this observation function",
    "start": "452080",
    "end": "458240"
  },
  {
    "text": "where you basically pass in a function which takes in",
    "start": "458240",
    "end": "463520"
  },
  {
    "text": "the as as input it takes the all the agent observations and then you can",
    "start": "463520",
    "end": "468800"
  },
  {
    "text": "essentially augment each agent's observation with the other ones and then pass it to their individual policies",
    "start": "468800",
    "end": "475199"
  },
  {
    "text": "um the way that it works is that um basically you for for this to work you",
    "start": "475199",
    "end": "480400"
  },
  {
    "text": "have to essentially provide the output actions observation space",
    "start": "480400",
    "end": "485919"
  },
  {
    "text": "that you would have you would see after this function is applied so for example here um i mean we have",
    "start": "485919",
    "end": "493360"
  },
  {
    "text": "two policies and we have like a centralized critic observer which essentially is augmenting",
    "start": "493360",
    "end": "499680"
  },
  {
    "text": "these observations for each agent and the way that it works is that you get these agent observations",
    "start": "499680",
    "end": "505440"
  },
  {
    "text": "and then you basically construct this new observation which is just a nested dictionary of okay what is my own",
    "start": "505440",
    "end": "511520"
  },
  {
    "text": "observation what is my opponent's observation what is my opponent action right",
    "start": "511520",
    "end": "517039"
  },
  {
    "text": "and this gets passed to the policy so you should assume that your policy will get this observation and it's going to",
    "start": "517039",
    "end": "523279"
  },
  {
    "text": "be the new observation space and in order to kind of like uh inform your policy that this is going",
    "start": "523279",
    "end": "530399"
  },
  {
    "text": "to be the observation space when you kind of pass you passing the cons you know the parameters to this",
    "start": "530399",
    "end": "536480"
  },
  {
    "text": "constructors you should provide um the correct observation space not the",
    "start": "536480",
    "end": "541680"
  },
  {
    "text": "original observation space but actually the augmented one right so here we",
    "start": "541680",
    "end": "546720"
  },
  {
    "text": "basically construct this observation space as a dictionary of you know it's discrete six for one observation a",
    "start": "546720",
    "end": "552720"
  },
  {
    "text": "discrete six for opponent observation and discrete two for you know opponent's action",
    "start": "552720",
    "end": "558240"
  },
  {
    "text": "and this is kind of like once you get to the forward pass of your model you know",
    "start": "558240",
    "end": "564800"
  },
  {
    "text": "whichever it is like it can be a ppo model you know anything right you will get this augmented observation instead",
    "start": "564800",
    "end": "572560"
  },
  {
    "text": "of um just an individual observation from the original space",
    "start": "572560",
    "end": "578320"
  },
  {
    "text": "so i'm going to make a pause here in case there's any questions here and federico if this kind of answers some",
    "start": "578320",
    "end": "584880"
  },
  {
    "text": "part of your questions yes yes the the part about sharing the",
    "start": "584880",
    "end": "590640"
  },
  {
    "text": "observation yes yes so um yeah i'm sorry i have a question actually sorry i have a question the",
    "start": "590640",
    "end": "596720"
  },
  {
    "text": "opponent action like how do you uh is this the opponent's previous action",
    "start": "596720",
    "end": "601920"
  },
  {
    "text": "uh so opponent i guess in agent this is just as a place tour right now but like",
    "start": "601920",
    "end": "607360"
  },
  {
    "text": "opponent age and observation also includes it's a sample batch of essentially what you get from the agent",
    "start": "607360",
    "end": "613279"
  },
  {
    "text": "so it includes not only like observation but also like i don't know actions uh",
    "start": "613279",
    "end": "619279"
  },
  {
    "text": "advantages i guess even right so the post-process uh agent observation that you can essentially pass into",
    "start": "619279",
    "end": "626240"
  },
  {
    "text": "the other agent as well okay okay okay so this okay yeah yeah so",
    "start": "626240",
    "end": "631279"
  },
  {
    "text": "there's more for the training sorry i'm not super familiar with this example either so this is more for the training",
    "start": "631279",
    "end": "637760"
  },
  {
    "text": "uh for preparing the train detection it is also used for",
    "start": "637760",
    "end": "643120"
  },
  {
    "text": "during inference right like so basically you don't have the thing is that with this flow you cannot",
    "start": "643120",
    "end": "650000"
  },
  {
    "text": "have independent observation spaces for inference and training both of them",
    "start": "650000",
    "end": "655600"
  },
  {
    "text": "should be coordinated with each other you can ignore i mean part of it like for uh you know during",
    "start": "655600",
    "end": "661839"
  },
  {
    "text": "action prediction you can just ignore observation space that you have but you will get it anyways in there max makes",
    "start": "661839",
    "end": "668320"
  },
  {
    "text": "sense okay okay yeah thanks thanks yes sir now here comes the challenging part",
    "start": "668320",
    "end": "675440"
  },
  {
    "start": "672000",
    "end": "1681000"
  },
  {
    "text": "right sharing information between observe between agents that also need to share gradients share",
    "start": "675440",
    "end": "682399"
  },
  {
    "text": "essentially neural network uh architectures between each other and that is something that is not",
    "start": "682399",
    "end": "689440"
  },
  {
    "text": "uh currently like um elegantly built into our live",
    "start": "689440",
    "end": "694720"
  },
  {
    "text": "and we are thinking about changing it down the line and we have a couple of like design uh explorations that we're",
    "start": "694720",
    "end": "700399"
  },
  {
    "text": "doing here um to just make it consistent with what we've been doing in the past",
    "start": "700399",
    "end": "705440"
  },
  {
    "text": "and also enable users to do this kind of stuff and but like",
    "start": "705440",
    "end": "710720"
  },
  {
    "text": "i can cover like for now i can cover what i think the hard way to do it right",
    "start": "710720",
    "end": "716639"
  },
  {
    "text": "now is in our leap and then uh what are its limitation and how we are actually moving forward with like i guess",
    "start": "716639",
    "end": "723519"
  },
  {
    "text": "changing that that part um so uh basically",
    "start": "723519",
    "end": "729120"
  },
  {
    "text": "whenever we want to share like differentiable stuff between agents",
    "start": "729120",
    "end": "734240"
  },
  {
    "text": "uh we have already some examples in the i guess in the algorithm folder it's like",
    "start": "734240",
    "end": "739519"
  },
  {
    "text": "uh this is more like uh referred to as you know a centralized training",
    "start": "739519",
    "end": "745519"
  },
  {
    "text": "decentralized execution where during training you may want to also share like models neural networks and",
    "start": "745519",
    "end": "752160"
  },
  {
    "text": "um maybe or like a mixture of like a mixer for q functions and that basically takes",
    "start": "752160",
    "end": "759680"
  },
  {
    "text": "a collection of tensors from you know all the agents and perform some like update on them like",
    "start": "759680",
    "end": "765920"
  },
  {
    "text": "uses them in some way and then you do some back propagation right do that that happens during training but during",
    "start": "765920",
    "end": "773040"
  },
  {
    "text": "execution you essentially treat each agent as an independent entity and like run with it right",
    "start": "773040",
    "end": "779120"
  },
  {
    "text": "um so we have two examples um it's called maddpg multi-agent ddbg and also",
    "start": "779120",
    "end": "785760"
  },
  {
    "text": "um what was the other one um yeah",
    "start": "785760",
    "end": "791519"
  },
  {
    "text": "so qmix is basically this thing where you have n agents they provide a q function",
    "start": "791519",
    "end": "796959"
  },
  {
    "text": "value at the output and then you have a non-linear mixture over their q function",
    "start": "796959",
    "end": "802480"
  },
  {
    "text": "values to basically get the um you know the environment's q function",
    "start": "802480",
    "end": "807600"
  },
  {
    "text": "the q function over all the agents jointly together um",
    "start": "807600",
    "end": "812720"
  },
  {
    "text": "so for these kind of algorithms uh what i mean in a nutshell what happens",
    "start": "812720",
    "end": "818800"
  },
  {
    "text": "is that you essentially need to create some sort of wrapper around your multi-agent",
    "start": "818800",
    "end": "824639"
  },
  {
    "text": "environment to treat it to make it look like a single agent environment",
    "start": "824639",
    "end": "829839"
  },
  {
    "text": "and build your policies in a way that has information about like everything",
    "start": "829839",
    "end": "835040"
  },
  {
    "text": "right all the neural network parameters so the difference between this and the other one that the previous version that",
    "start": "835040",
    "end": "841680"
  },
  {
    "text": "i said is that you can use existing for example ppo model ppo policy",
    "start": "841680",
    "end": "848399"
  },
  {
    "text": "just to like you know create any agents of them and then run in independent uh local",
    "start": "848399",
    "end": "856079"
  },
  {
    "text": "observation through each and then don't share anything between them so that's that's what we talked about i",
    "start": "856079",
    "end": "862240"
  },
  {
    "text": "guess in the previous approach but here because you want to share these gradients you have to kind of",
    "start": "862240",
    "end": "869519"
  },
  {
    "text": "encapsulate all of these neural networks under the same model the same policy so",
    "start": "869519",
    "end": "874639"
  },
  {
    "text": "you have access to all of these at the same time and to kind of like so you don't have",
    "start": "874639",
    "end": "880160"
  },
  {
    "text": "any dictionary of like any policy map you have a default policy that controls that um",
    "start": "880160",
    "end": "886560"
  },
  {
    "text": "entire thing entire uni and then um basically once your",
    "start": "886560",
    "end": "891600"
  },
  {
    "text": "environment is kind of wrapped in a way that it looks like a single agent then you can just like okay treat it as a",
    "start": "891600",
    "end": "898160"
  },
  {
    "text": "single agent trainer so you pass in like a number of agents i think into the",
    "start": "898160",
    "end": "903440"
  },
  {
    "text": "model inside the model constructor you create like a collector collection of",
    "start": "903440",
    "end": "908959"
  },
  {
    "text": "models and also like a shared you know shared component and then like you know it's basically pi",
    "start": "908959",
    "end": "916720"
  },
  {
    "text": "torch from then on right like you get the observation it's basically looks like a single agent observation",
    "start": "916720",
    "end": "923120"
  },
  {
    "text": "um but it's like nested so you can actually index through it very easily for to get the correspond like each",
    "start": "923120",
    "end": "929600"
  },
  {
    "text": "agent's corresponding observation and then run it through the corresponding neural network",
    "start": "929600",
    "end": "935519"
  },
  {
    "text": "um any any question about like the high level approach on this and i can show you an",
    "start": "935519",
    "end": "940720"
  },
  {
    "text": "example of how it's done no no i i fully get okay yeah it's",
    "start": "940720",
    "end": "946880"
  },
  {
    "text": "actually expected yeah so one of the problems one of the limitations is that of our ellipse for",
    "start": "946880",
    "end": "954240"
  },
  {
    "text": "qmix algorithm and m80 phd um [Music] two yeah to my knowledge are only capable of",
    "start": "954240",
    "end": "961440"
  },
  {
    "text": "handling these um homogeneous um observation spaces for the single agents so you uh what you mentioned before",
    "start": "961440",
    "end": "967120"
  },
  {
    "text": "puerto rico where you need different observation spaces for the individual agents uh that's currently not",
    "start": "967120",
    "end": "973279"
  },
  {
    "text": "not covered by the qmix but you could of course construct your own uh algo that's that's similar to qmix or",
    "start": "973279",
    "end": "979279"
  },
  {
    "text": "that uses this this shared policy that's course just described and that does have um different",
    "start": "979279",
    "end": "984639"
  },
  {
    "text": "different preprocessors or different yeah well it does a lot of different observation spaces yeah yeah",
    "start": "984639",
    "end": "990320"
  },
  {
    "text": "so in this example i have actually shown your use case where you have a home heterogeneous observation and not",
    "start": "990320",
    "end": "998399"
  },
  {
    "text": "just for actions but uh for input observation but also on actions maybe agent a",
    "start": "998399",
    "end": "1004000"
  },
  {
    "text": "is like three-dimensional input two-dimensional output and b is like two-dimensional input three-dimensional",
    "start": "1004000",
    "end": "1010079"
  },
  {
    "text": "output right and how do we do this and they share like a media-like encoder right um",
    "start": "1010079",
    "end": "1016320"
  },
  {
    "text": "they have like input encoders that are different agent-specific they share some i guess let's say transformer",
    "start": "1016320",
    "end": "1023600"
  },
  {
    "text": "or anything really right several layers and then they also have like independent output heads at the end right how do we",
    "start": "1023600",
    "end": "1030400"
  },
  {
    "text": "do this here um so you still have you're seeing my screen",
    "start": "1030400",
    "end": "1036558"
  },
  {
    "text": "right yeah um so just to give you an idea this is just like a custom um multi-agent class",
    "start": "1036559",
    "end": "1044480"
  },
  {
    "text": "that i wrote it doesn't do anything it's just a placeholder um just to just for",
    "start": "1044480",
    "end": "1049679"
  },
  {
    "text": "illustration you have like action space observation space which is just this dictionary of things um i mean this can",
    "start": "1049679",
    "end": "1056000"
  },
  {
    "text": "be like i guess heterogeneous here and you also have like a heterogeneous",
    "start": "1056000",
    "end": "1061360"
  },
  {
    "text": "observation space right and then um i guess these are standard um",
    "start": "1061360",
    "end": "1068640"
  },
  {
    "text": "attributes that you have to set for like these two um for like multi-agent environments it's documented as part of",
    "start": "1068640",
    "end": "1074640"
  },
  {
    "text": "the hourly but like it's it's very like this this is an environment like very consistent with multi-agent early um documentation",
    "start": "1074640",
    "end": "1082080"
  },
  {
    "text": "right so you basically take an action dictionary in your step function and then um you construct these multi-agent",
    "start": "1082080",
    "end": "1089679"
  },
  {
    "text": "dictionaries which is like a observation reward and like done",
    "start": "1089679",
    "end": "1094799"
  },
  {
    "text": "and now to kind of wrap this into a single agent looking environment right",
    "start": "1094799",
    "end": "1101280"
  },
  {
    "text": "um what we do is like we essentially um augment",
    "start": "1101280",
    "end": "1107280"
  },
  {
    "text": "the basically you create this custom environment there and then you basically have to uh",
    "start": "1107280",
    "end": "1114799"
  },
  {
    "text": "augment this observation space and action space to look like a single agent right",
    "start": "1114799",
    "end": "1120000"
  },
  {
    "text": "so what happens here is that you go through for example each um you know each agent id here um",
    "start": "1120000",
    "end": "1126720"
  },
  {
    "text": "and then you basically construct what i did was construct this thing so",
    "start": "1126720",
    "end": "1131840"
  },
  {
    "text": "basically it looks like something like this you have",
    "start": "1131840",
    "end": "1137840"
  },
  {
    "text": "you know um essentially like for example for policy a it's a dictionary first of all where",
    "start": "1138080",
    "end": "1143919"
  },
  {
    "text": "keys are policy and calls to b and then for a it's going to be like agent",
    "start": "1143919",
    "end": "1149760"
  },
  {
    "text": "you know opposite space of a and then neighbors",
    "start": "1149760",
    "end": "1157440"
  },
  {
    "text": "um which is basically a dictionary where for agents the other agent you have like",
    "start": "1158320",
    "end": "1163600"
  },
  {
    "text": "a b keyboard right and then what was my observation space",
    "start": "1163600",
    "end": "1170000"
  },
  {
    "text": "right if you had a c agent you could like extend there as well right",
    "start": "1170000",
    "end": "1176880"
  },
  {
    "text": "and same thing for b right so so this opposite space that comes out is essentially this nested structure",
    "start": "1177440",
    "end": "1184799"
  },
  {
    "text": "um and then your action space is going to be just like the inherited version because it",
    "start": "1184799",
    "end": "1190640"
  },
  {
    "text": "was like homogeneous out of town um now okay so",
    "start": "1190640",
    "end": "1197039"
  },
  {
    "text": "when you want to take a step in the environment you basically have to um",
    "start": "1197039",
    "end": "1202080"
  },
  {
    "text": "take this action dictionary construct a single you know like the original",
    "start": "1202080",
    "end": "1208240"
  },
  {
    "text": "um action space that you would expect in a multi-agent environment run it through the environment and then",
    "start": "1208240",
    "end": "1214400"
  },
  {
    "text": "augment your observations right to look like this is space",
    "start": "1214400",
    "end": "1219440"
  },
  {
    "text": "right so in a single observation here you would see both a and b",
    "start": "1219440",
    "end": "1224720"
  },
  {
    "text": "right whereas in the previous version you would have only seen like you know a and b like",
    "start": "1224720",
    "end": "1231440"
  },
  {
    "text": "that then the model would expect to see one agent at a time here you would get two of them at the same time right",
    "start": "1231440",
    "end": "1238640"
  },
  {
    "text": "now any question about like how we wrap the environment in a single legend one",
    "start": "1238640",
    "end": "1245520"
  },
  {
    "text": "no please stop crystal clear okay awesome now it comes to the model right so which in",
    "start": "1245520",
    "end": "1252240"
  },
  {
    "text": "this model which is going to be a torch model v2 which is basically the base model for all our elite models right now",
    "start": "1252240",
    "end": "1259600"
  },
  {
    "text": "um you're gonna have to construct all of the neural networks",
    "start": "1259600",
    "end": "1264640"
  },
  {
    "text": "that we would need inside this so there's not gonna be like any higher level",
    "start": "1264640",
    "end": "1270000"
  },
  {
    "text": "executor which will create like copies of this for each agent you have to kind of like write them here yourself right",
    "start": "1270000",
    "end": "1278080"
  },
  {
    "text": "um so basically i'm gonna come back to these like configurations that i'm doing but like you have like a dictionary of",
    "start": "1278080",
    "end": "1285120"
  },
  {
    "text": "encoders right where it's basically a mapping of keywords to like their neural network which is right",
    "start": "1285120",
    "end": "1291200"
  },
  {
    "text": "now is just a linear layer from the kind of like the agent's observation",
    "start": "1291200",
    "end": "1296720"
  },
  {
    "text": "space to a latent dimension right so for a it's going to be 3 2 1 28",
    "start": "1296720",
    "end": "1302640"
  },
  {
    "text": "for b it's going to be 2 to 120 right so this is going to be a dictionary of encoders you have like a shared",
    "start": "1302640",
    "end": "1310320"
  },
  {
    "text": "several layers right and then for trunk you have again independent uh linear relations from 128",
    "start": "1310320",
    "end": "1317120"
  },
  {
    "text": "to 2 to 3 right and then you construct this neural network uh in a module dict further",
    "start": "1317120",
    "end": "1323679"
  },
  {
    "text": "right now um the observation space that you get here is like a flattened",
    "start": "1323679",
    "end": "1330880"
  },
  {
    "text": "multi-agent observation space to in order to kind of like go back to the original structure you have to kind",
    "start": "1330880",
    "end": "1337600"
  },
  {
    "text": "of call its original space right and then construct your",
    "start": "1337600",
    "end": "1343280"
  },
  {
    "text": "neural network sizes and dimensions based on this like um you know parsed version of the original space and that's",
    "start": "1343280",
    "end": "1350559"
  },
  {
    "text": "how we do it here so you basically the observation space here is going to get flat 10 so if you have like 3 2",
    "start": "1350559",
    "end": "1357280"
  },
  {
    "text": "2 3 it's going to be like 10 dimensional right for each age gen um and then you basically have to",
    "start": "1357280",
    "end": "1364799"
  },
  {
    "text": "par like reconstruct back the original thing right um so in a nested dictionary and that's how",
    "start": "1364799",
    "end": "1371600"
  },
  {
    "text": "you do it like we have this we retrieved the original space how it was and then we use that to construct our neural",
    "start": "1371600",
    "end": "1377679"
  },
  {
    "text": "networks right and now during forward pass basically what you need to do is like loop through",
    "start": "1377679",
    "end": "1384240"
  },
  {
    "text": "each agent right um this happened usually before like in in the previous story it happened",
    "start": "1384240",
    "end": "1390000"
  },
  {
    "text": "outside of this model but now it has to come inside right uh you look through the agents you",
    "start": "1390000",
    "end": "1395280"
  },
  {
    "text": "encode each agent specifically with its own encoder right you get the agent estates you get",
    "start": "1395280",
    "end": "1402400"
  },
  {
    "text": "the neighbor states the same way right you uh do some pooling for example mean",
    "start": "1402400",
    "end": "1409200"
  },
  {
    "text": "pooling average following um you concatenate both and then you pass them through the shared encode you know stuff",
    "start": "1409200",
    "end": "1415280"
  },
  {
    "text": "and then uh use the trunk to kind of construct this and you have your like",
    "start": "1415280",
    "end": "1420480"
  },
  {
    "text": "list of agent actions right and for value function you may do something similar i just didn't do any",
    "start": "1420480",
    "end": "1427440"
  },
  {
    "text": "do it here um but then what you pass out is just the agent actions and the updated state",
    "start": "1427440",
    "end": "1432960"
  },
  {
    "text": "if this was like a recurrent and neural network right um and that's it so when it comes to",
    "start": "1432960",
    "end": "1441120"
  },
  {
    "text": "um the configuration setup you don't have any multi-agent stuff",
    "start": "1441120",
    "end": "1446240"
  },
  {
    "text": "anymore it's a single agent from the point of view of the rlip and then you pass in like a model",
    "start": "1446240",
    "end": "1452640"
  },
  {
    "text": "dictionary which kind of constructs your original model so you have a custom model and maybe a config file config",
    "start": "1452640",
    "end": "1458559"
  },
  {
    "text": "dictionary that passes like these um new things that we defined like latent dimension and things like that",
    "start": "1458559",
    "end": "1464799"
  },
  {
    "text": "um yeah so that's pretty much how you would do it today uh it's a bit hacky because",
    "start": "1464799",
    "end": "1471279"
  },
  {
    "text": "i mean you're masking the fact that this is a multi-agent thing but just treating as a",
    "start": "1471279",
    "end": "1476480"
  },
  {
    "text": "single agent and you have like if you um it's not modular so if i want to",
    "start": "1476480",
    "end": "1482559"
  },
  {
    "text": "reuse ppo um i cannot do it i have to go ahead and create like a new tor you know torch v2",
    "start": "1482559",
    "end": "1489840"
  },
  {
    "text": "model which is different than like ppo um now what we are doing i guess moving forward",
    "start": "1489840",
    "end": "1496400"
  },
  {
    "text": "is like how do we consolidate these things how do we essentially share these modules in a",
    "start": "1496400",
    "end": "1502080"
  },
  {
    "text": "more general way so we can actually use existing code for example for ppo and then all we need to do is like passing",
    "start": "1502080",
    "end": "1508880"
  },
  {
    "text": "these like shared modules in there and then they should have like all access to the same thing when they when you're inside these",
    "start": "1508880",
    "end": "1515919"
  },
  {
    "text": "like models so can i ask a question real quick i think i missed it in your environment wrapper",
    "start": "1515919",
    "end": "1522799"
  },
  {
    "text": "how are you tracking the rewards of each of the individual agents oh so i didn't augment the reward",
    "start": "1522799",
    "end": "1530159"
  },
  {
    "text": "here but uh basically what you can do is like passing a dictionary",
    "start": "1530159",
    "end": "1536640"
  },
  {
    "text": "of rewards out and they will show up in the sample agent as they they are right",
    "start": "1536640",
    "end": "1543120"
  },
  {
    "text": "like basically um i think that's that's the case am i",
    "start": "1543120",
    "end": "1548640"
  },
  {
    "text": "right silent or do you have any opinion on that i haven't actually looked into that yet but",
    "start": "1548640",
    "end": "1553840"
  },
  {
    "text": "i think the structures that you preserve here are going to get passed to the samples uh seven year mute yeah yeah",
    "start": "1553840",
    "end": "1560400"
  },
  {
    "text": "sorry sorry i'm trying to think yeah i think you don't have to change anything because you",
    "start": "1560400",
    "end": "1566640"
  },
  {
    "text": "uh this is your reward dictionary already because nf is already a multi-agent",
    "start": "1566640",
    "end": "1572240"
  },
  {
    "text": "a self-done",
    "start": "1572240",
    "end": "1574799"
  },
  {
    "text": "let's see [Music]",
    "start": "1577440",
    "end": "1582569"
  },
  {
    "text": "i mean i can't put a breakpoint in the forward function yeah i mean the whole problem is still a multi-agent one right so you still have a dictionary",
    "start": "1582720",
    "end": "1589200"
  },
  {
    "text": "of agent keys um that's not the point the point is that your your model now handles these",
    "start": "1589200",
    "end": "1594720"
  },
  {
    "text": "different um nested things where you have agent a and then has a neighboring",
    "start": "1594720",
    "end": "1600480"
  },
  {
    "text": "and that goes through agent ace encoder and stuff but for the rewards it shouldn't matter right um",
    "start": "1600480",
    "end": "1605760"
  },
  {
    "text": "yeah so because this has i ran this and didn't didn't give any errors i think",
    "start": "1605760",
    "end": "1612000"
  },
  {
    "text": "you still have like this reward is uh the same structure that is preserved like that it has it's a nested",
    "start": "1612000",
    "end": "1619279"
  },
  {
    "text": "one it will get transferred to a sample bag and",
    "start": "1619279",
    "end": "1624480"
  },
  {
    "text": "then yeah",
    "start": "1624480",
    "end": "1630720"
  },
  {
    "text": "uh yeah so your loss function will have to deal with this with this multi-agent-ness so similar to how q-mix",
    "start": "1631360",
    "end": "1637039"
  },
  {
    "text": "rhino does it's uh which is kind of like what does it do",
    "start": "1637039",
    "end": "1642080"
  },
  {
    "text": "like a sum or something um",
    "start": "1642080",
    "end": "1645360"
  },
  {
    "text": "thanks whoa this was like i guess uh",
    "start": "1647840",
    "end": "1654559"
  },
  {
    "text": "one of the hardest questions that we've been asked in office hours that's why it took like a bit of time",
    "start": "1654559",
    "end": "1660080"
  },
  {
    "text": "but yeah hopefully it will be helpful for a lot of folks yes thank you very much the explanation",
    "start": "1660080",
    "end": "1666720"
  },
  {
    "text": "was very very clear now i yeah i know what to do yeah uh we will probably like also",
    "start": "1666720",
    "end": "1674000"
  },
  {
    "text": "attach these files to the discussion um i'll hand it off to christian",
    "start": "1674000",
    "end": "1680720"
  },
  {
    "text": "but maybe maybe of course give like a quick overview of what's what will change or what we think should change or",
    "start": "1680720",
    "end": "1686159"
  },
  {
    "start": "1681000",
    "end": "2196000"
  },
  {
    "text": "what how how we can make this better and less complicated um yeah to do this",
    "start": "1686159",
    "end": "1691600"
  },
  {
    "text": "yeah so the way that we are thinking about doing this is that um",
    "start": "1691600",
    "end": "1696799"
  },
  {
    "text": "so we're gonna have this like right now uh policy is the owner of everything and",
    "start": "1696799",
    "end": "1703520"
  },
  {
    "text": "under that you have a torch model v2 but like turkish model v2 is not",
    "start": "1703520",
    "end": "1709200"
  },
  {
    "text": "um independently doing anything so you cannot really take out this model v2 and then run it through the environment and",
    "start": "1709200",
    "end": "1715600"
  },
  {
    "text": "do in france with it right you have to like reload load the entire policy to be able",
    "start": "1715600",
    "end": "1722480"
  },
  {
    "text": "to run that right so that's number one thing we want to change we want to essentially have a thin wrapper around",
    "start": "1722480",
    "end": "1728640"
  },
  {
    "text": "all these neural networks which is um i guess self-contained and it can be run",
    "start": "1728640",
    "end": "1733919"
  },
  {
    "text": "through through like during inference right and then what this policy will now be",
    "start": "1733919",
    "end": "1739520"
  },
  {
    "text": "called like unit trainer which is essentially not a policy anymore your policy is like your at that model right",
    "start": "1739520",
    "end": "1746000"
  },
  {
    "text": "that gets run on the environment and this what this unit trainer does is that um during trade like it it it does more",
    "start": "1746000",
    "end": "1752960"
  },
  {
    "text": "stuff than just like running the sampling right it computes the loss function and it is like that right",
    "start": "1752960",
    "end": "1759200"
  },
  {
    "text": "and um for these kind of problems where you want to share parameters what you need",
    "start": "1759200",
    "end": "1765039"
  },
  {
    "text": "to do is that this this in this like unit train here you would construct these like shared components",
    "start": "1765039",
    "end": "1772480"
  },
  {
    "text": "outside of the model right outside of the policy model and then pass pointers or handles um",
    "start": "1772480",
    "end": "1780080"
  },
  {
    "text": "into to inside of these like models so that these models will have access to",
    "start": "1780080",
    "end": "1785279"
  },
  {
    "text": "the same object um during training and then when you pass for example like i don't know",
    "start": "1785279",
    "end": "1790880"
  },
  {
    "text": "tensor through that the gradient like and then you back propagate the gradient gets accumulated",
    "start": "1790880",
    "end": "1796559"
  },
  {
    "text": "on the same um set of parameters instead of like copies of things and then you",
    "start": "1796559",
    "end": "1802720"
  },
  {
    "text": "have to like do some syncing between them um this design basically tells like yeah",
    "start": "1802720",
    "end": "1809200"
  },
  {
    "text": "um tries to isolate these shared models from whatever the policies but also contain",
    "start": "1809200",
    "end": "1815919"
  },
  {
    "text": "them when you want to run them together um during france so that's kind of the high level it hasn't been polished yet",
    "start": "1815919",
    "end": "1821919"
  },
  {
    "text": "but i guess once it's a design a very clean design document comes out we can",
    "start": "1821919",
    "end": "1827919"
  },
  {
    "text": "share it between folks to collect feedback yeah sven how hard would it be to just add a",
    "start": "1827919",
    "end": "1835200"
  },
  {
    "text": "feature that allows you to mark a portion of the observation space or say you know one gym.spaces object of the",
    "start": "1835200",
    "end": "1842480"
  },
  {
    "text": "observation spaces is differentiable like assuming that it is filled in by the neural net",
    "start": "1842480",
    "end": "1849840"
  },
  {
    "text": "you mean like in the sample batch right that you get well what i was saying is like let's say that you have an action in",
    "start": "1849840",
    "end": "1856799"
  },
  {
    "text": "your uh your action space that is communicate and that outputs uh a box you know a vector",
    "start": "1856799",
    "end": "1863120"
  },
  {
    "text": "and then that vector then the environment takes that or an environment wrapper takes that and that becomes part",
    "start": "1863120",
    "end": "1868960"
  },
  {
    "text": "of the observation space so then either any agent nearby if you want it locally or any agent globally that observes that",
    "start": "1868960",
    "end": "1875600"
  },
  {
    "text": "agent can observe the communication from the last time step and then you would not need to make any",
    "start": "1875600",
    "end": "1881679"
  },
  {
    "text": "modifications whatsoever to the network side because all that's happened is essentially you've marked an observation",
    "start": "1881679",
    "end": "1887679"
  },
  {
    "text": "as differentiable and it would be handled statefully the same way as an lstm state is",
    "start": "1887679",
    "end": "1893840"
  },
  {
    "text": "but the thing is like under the hood these are running on um different actors",
    "start": "1894399",
    "end": "1899519"
  },
  {
    "text": "which have essentially a copy of these things right they live on different processes during inference and isn't",
    "start": "1899519",
    "end": "1906399"
  },
  {
    "text": "this the same as the lstm state though no the lcm state is not",
    "start": "1906399",
    "end": "1912320"
  },
  {
    "text": "the ls attack the sample batch is not doesn't contain the graph anymore it's just a snapshot of the",
    "start": "1912320",
    "end": "1918960"
  },
  {
    "text": "of the output of the last output um oh if you had let's see if you had a series",
    "start": "1918960",
    "end": "1926640"
  },
  {
    "text": "of lstm states um",
    "start": "1926640",
    "end": "1932640"
  },
  {
    "text": "then you could run those through an lcm and then you could backdrop through the entire sequence right then you would have like a static",
    "start": "1932720",
    "end": "1938960"
  },
  {
    "text": "um back propagation through time kind of thing but if you just treat it the same as the",
    "start": "1938960",
    "end": "1944799"
  },
  {
    "text": "state then and you like you do maintain that series i'm just wondering if there's a way to cheat this into the the",
    "start": "1944799",
    "end": "1950720"
  },
  {
    "text": "same mechanism you use for the lstm and not require really you know for your own storage to cheat that into the same",
    "start": "1950720",
    "end": "1956320"
  },
  {
    "text": "mechanism uh of your sample your trajectory processor and then not even",
    "start": "1956320",
    "end": "1961360"
  },
  {
    "text": "have to have any user side modifications for at least a large fraction of communication applications",
    "start": "1961360",
    "end": "1968799"
  },
  {
    "text": "but how does arlit know which um like where this so so you let's say you",
    "start": "1970640",
    "end": "1975679"
  },
  {
    "text": "have one component in your observation that's that comes from another model um",
    "start": "1975679",
    "end": "1981039"
  },
  {
    "text": "because it already passed through some some original observation environment through some preprocessor or some encoder um it",
    "start": "1981039",
    "end": "1987600"
  },
  {
    "text": "would be associated with the agent that it comes from so if your observation space contains like agent one colon a",
    "start": "1987600",
    "end": "1994640"
  },
  {
    "text": "bunch of stuff and the ops and the communication from agent one no comma agent two uh all the stuff for agent two comma the",
    "start": "1994640",
    "end": "2001679"
  },
  {
    "text": "communication for agent two then you know it's associated with that specific agent",
    "start": "2001679",
    "end": "2006960"
  },
  {
    "text": "basically what i'm proposing here is a way to add um you'd have to be able to explicitly mark a section of the",
    "start": "2006960",
    "end": "2012000"
  },
  {
    "text": "observation as corresponding to a specific agent and then naturally that gives you the",
    "start": "2012000",
    "end": "2017440"
  },
  {
    "text": "ability to know where the communication uh you know which agent to link that",
    "start": "2017440",
    "end": "2022720"
  },
  {
    "text": "communication vector to this will be possible",
    "start": "2022720",
    "end": "2028720"
  },
  {
    "text": "but it doesn't preserve the gradients right like because because of the problem i said right like",
    "start": "2028720",
    "end": "2035840"
  },
  {
    "text": "once they get into the um sampling side right they live on a",
    "start": "2035840",
    "end": "2041440"
  },
  {
    "text": "different process they live on a different process yeah that's why we convert them to number",
    "start": "2041440",
    "end": "2047679"
  },
  {
    "text": "five right so we can try we don't preserve the graph right now i say",
    "start": "2047679",
    "end": "2053679"
  },
  {
    "text": "you don't preserve the the graph for lstms either though you just maintain a sequence of everything right correct",
    "start": "2053919",
    "end": "2059919"
  },
  {
    "text": "correct so that's what i was trying to see if there's a way to do that given that all the agents come from the same environment",
    "start": "2059919",
    "end": "2066398"
  },
  {
    "text": "we don't that's one of our models always require do you require the state output to capture that you just store that but",
    "start": "2066399",
    "end": "2071679"
  },
  {
    "text": "it's not it's not we don't keep the graph we just keep all these yeah so you have this and later",
    "start": "2071679",
    "end": "2078960"
  },
  {
    "text": "you have the state out of communication the states and the policy match in that case right and lstm is designed",
    "start": "2078960",
    "end": "2085919"
  },
  {
    "text": "to work that way in this case you'd have states from other agents policies that the learner doesn't have access to",
    "start": "2085919",
    "end": "2092398"
  },
  {
    "text": "when it's doing the gradient update yeah so",
    "start": "2092399",
    "end": "2097359"
  },
  {
    "text": "yeah hmm i guess i was sort of thinking of it the flip side so like from the perspective",
    "start": "2099520",
    "end": "2105839"
  },
  {
    "text": "of the agent communicating it needs the upstream gradient from the agent that has observed that communication",
    "start": "2105839",
    "end": "2112560"
  },
  {
    "text": "we can probably do this during um training when you want to like",
    "start": "2112560",
    "end": "2117599"
  },
  {
    "text": "loop through it against one by one you can augment the second agent's observation with the",
    "start": "2117599",
    "end": "2123839"
  },
  {
    "text": "tensors that agent one is or has already completed or something like that and then",
    "start": "2123839",
    "end": "2130400"
  },
  {
    "text": "i mean but this this pro there should be like a very complicated protocol that that needs to be yeah i see i see why",
    "start": "2130400",
    "end": "2136560"
  },
  {
    "text": "that's complex i was just wondering because if that worked then you wouldn't have to modify user side code in order",
    "start": "2136560",
    "end": "2141680"
  },
  {
    "text": "to do fancy communication right yeah it's it's already an advanced usage",
    "start": "2141680",
    "end": "2148160"
  },
  {
    "text": "so i don't think hey i mean today's today's advanced usage just tomorrow's every case right",
    "start": "2148160",
    "end": "2154720"
  },
  {
    "text": "yeah yeah that's true this is something i'm looking to add natively in my framework without uh without",
    "start": "2154720",
    "end": "2160320"
  },
  {
    "text": "differentiability but you know differentiability would be awesome for whatever good it does what joseph",
    "start": "2160320",
    "end": "2166240"
  },
  {
    "text": "said about today's advanced cases tomorrow standard that's like profoundly true like when you ask rls limitations i think the",
    "start": "2166240",
    "end": "2173200"
  },
  {
    "text": "biggest one is if you want to do you know the next new thing which you know like if you like you know like i can fix",
    "start": "2173200",
    "end": "2178400"
  },
  {
    "text": "the transformers you can do instantly doing whatever the next new thing is in rl that because it's so interlocked and",
    "start": "2178400",
    "end": "2184320"
  },
  {
    "text": "like things like cleaner l is really hard as i'm sure you're aware and this means that people using it inherently",
    "start": "2184320",
    "end": "2189920"
  },
  {
    "text": "can't use state-of-the-art things which is often where the people who want to use distributed rl i mean advanced tools",
    "start": "2189920",
    "end": "2195440"
  },
  {
    "text": "will be the most the petting zoo discussion maybe because we have two folks here in gym",
    "start": "2195440",
    "end": "2201280"
  },
  {
    "start": "2196000",
    "end": "2467000"
  },
  {
    "text": "yeah yeah go ahead so i i just have some do so we have we have rohan on our team interning for the next uh couple months",
    "start": "2201280",
    "end": "2207920"
  },
  {
    "text": "and um the double robust um",
    "start": "2207920",
    "end": "2213200"
  },
  {
    "text": "implementation but once he's done with that uh and he's always has a prs in review so it's not much left uh he will",
    "start": "2213200",
    "end": "2218960"
  },
  {
    "text": "go full time on on petting zoo making sure that our adapter works that all the bugs are fixed that are currently in there",
    "start": "2218960",
    "end": "2224880"
  },
  {
    "text": "um and then we start adding test cases finally to uh to our lib that use newer mmos um",
    "start": "2224880",
    "end": "2230800"
  },
  {
    "text": "great that's that's the quick update on our end um so we're finally getting to do that um and i've already sent him a list",
    "start": "2230800",
    "end": "2237680"
  },
  {
    "text": "of how how exactly to best do that as well",
    "start": "2237680",
    "end": "2243000"
  },
  {
    "text": "that we should pay attention to and should do definitely so i i do have a few more things i was",
    "start": "2245520",
    "end": "2252400"
  },
  {
    "text": "waiting for you so so my thing with petting zoo is so what we discussed last time what we were",
    "start": "2252400",
    "end": "2258240"
  },
  {
    "text": "trying to advocate for was that you know if pretty much all the other major or multi-agent rl not pretty much",
    "start": "2258240",
    "end": "2265200"
  },
  {
    "text": "literally all the major multitasking rl libraries except rl use pending to is their internal api and most of the",
    "start": "2265200",
    "end": "2270640"
  },
  {
    "text": "multi-asian environments use it now um then it would be beneficial to",
    "start": "2270640",
    "end": "2275920"
  },
  {
    "text": "petting zoo and rl in the community and say development costs switch from your custom multi-agent api to petting use",
    "start": "2275920",
    "end": "2281839"
  },
  {
    "text": "because they're equivalent so that's the thing that we were previously discussed and that you guys said that you'd want to look into at",
    "start": "2281839",
    "end": "2287599"
  },
  {
    "text": "some point uh yeah i'm afraid we still don't have a",
    "start": "2287599",
    "end": "2292800"
  },
  {
    "text": "definite answer on that um we well as a first step um",
    "start": "2292800",
    "end": "2298800"
  },
  {
    "text": "and maybe we will do that but maybe we'll just keep making sure that our adapter works 100 with all the different",
    "start": "2298800",
    "end": "2305760"
  },
  {
    "text": "petting zoo uh api use cases and um corner cases um",
    "start": "2305760",
    "end": "2312480"
  },
  {
    "text": "and then it will be just as if we support petting zoo fully right it's similar to how we how we can claim that",
    "start": "2312480",
    "end": "2318079"
  },
  {
    "text": "we fully support gym api even though under the hood our lib is not operating on the gym api it's using i didn't know",
    "start": "2318079",
    "end": "2323839"
  },
  {
    "text": "that you weren't operating on the under the hood now okay we don't don't we we it's the same it's",
    "start": "2323839",
    "end": "2330079"
  },
  {
    "text": "it on the outside it looks like you can you can give us a gym environment and it perfectly works we vectorize and do all the stuff but um under the hood we don't",
    "start": "2330079",
    "end": "2336960"
  },
  {
    "text": "use gym anymore we basically convert it automatically into what we call a base end and then operate on that um",
    "start": "2336960",
    "end": "2343680"
  },
  {
    "text": "the reason being several reasons one is vectorization the other one is multi-agent the third one is um",
    "start": "2343680",
    "end": "2349040"
  },
  {
    "text": "asynchronousness we need to be able to have external environments and allow the environment to pull us and as a a little",
    "start": "2349040",
    "end": "2355520"
  },
  {
    "text": "bit of service in home um so as a first step to to uh we",
    "start": "2355520",
    "end": "2360720"
  },
  {
    "text": "acknowledge that we have bucks on the paddington side our example is not complete and we don't cover all the",
    "start": "2360720",
    "end": "2365839"
  },
  {
    "text": "cases and we want to fix that first um to make you guys happy and to um",
    "start": "2365839",
    "end": "2371760"
  },
  {
    "text": "to fully be able to claim yes you can you can give us a pending and it will work okay i'll work right now we cannot",
    "start": "2371760",
    "end": "2377200"
  },
  {
    "text": "make that claim unfortunately and that's that's that's too bad and we're trying to fix that right now um and then we can still decide say later uh is does it",
    "start": "2377200",
    "end": "2384160"
  },
  {
    "text": "really make sense should we should we get rid of our internal multi-agent and api",
    "start": "2384160",
    "end": "2389839"
  },
  {
    "text": "so yeah it's just nice because the test cover i was just going to say the test coverage is so wide like the space of",
    "start": "2390560",
    "end": "2395839"
  },
  {
    "text": "multi-agent environments that people are using i mean we're only scratching the surface and already we're doing things",
    "start": "2395839",
    "end": "2401280"
  },
  {
    "text": "that are far far outside of the the norm of most academic rl research so like",
    "start": "2401280",
    "end": "2406720"
  },
  {
    "text": "getting that that wrapper working so that you natively support it i think that that middle layer it's very",
    "start": "2406720",
    "end": "2412800"
  },
  {
    "text": "difficult to not have bugs there right i mean the the middle layer should just handle the",
    "start": "2412800",
    "end": "2418720"
  },
  {
    "text": "interface with this right and you can essentially i i don't think that that test is that",
    "start": "2418720",
    "end": "2425599"
  },
  {
    "text": "should be covered that scope is too complicated like too broad right because",
    "start": "2425599",
    "end": "2431839"
  },
  {
    "text": "you can just just sweep all the possible interfaces and things like that like think about logically what are the pos edge cases of",
    "start": "2431839",
    "end": "2440079"
  },
  {
    "text": "this communication that needs to happen and then cover that interface interaction right because under the hood",
    "start": "2440079",
    "end": "2446319"
  },
  {
    "text": "you're still running i guess heading to right yeah i mean in a perfect world right the",
    "start": "2446319",
    "end": "2452880"
  },
  {
    "text": "hope is that you're able to any bugs that arise as a result of the api get upstreamed to petting zoo to be",
    "start": "2452880",
    "end": "2460000"
  },
  {
    "text": "fixed and like there is not the question of whether this is a bug in the rl lib integration or in the petting zoo api",
    "start": "2460000",
    "end": "2467119"
  },
  {
    "start": "2467000",
    "end": "3037000"
  },
  {
    "text": "yeah so the other thing i was going to talk about was i want to talk about jim and",
    "start": "2467119",
    "end": "2473040"
  },
  {
    "text": "the python 3.6 heavily so for jim i know you guys have been complaining a bunch about there being breaking api changes",
    "start": "2473040",
    "end": "2478640"
  },
  {
    "text": "in gym in the next week roughly we're going to release the last",
    "start": "2478640",
    "end": "2483680"
  },
  {
    "text": "um release the gym that has any new braking api changes we're going to do bug fix",
    "start": "2483680",
    "end": "2489359"
  },
  {
    "text": "some bug pick stuff um and we're and some changes to environments",
    "start": "2489359",
    "end": "2495680"
  },
  {
    "text": "and we're going to release the change where all the breaking changes become like on by default and permanent but like all the new stuff to the core",
    "start": "2495680",
    "end": "2502160"
  },
  {
    "text": "api is going to be done you know you released in uh off by default manner at least in um",
    "start": "2502160",
    "end": "2509599"
  },
  {
    "text": "in like a week that that'll be 25 or",
    "start": "2509599",
    "end": "2515119"
  },
  {
    "text": "uh i think it'll be twenty point uh zero point twenty five yeah okay okay cool and then yeah so so once",
    "start": "2515119",
    "end": "2521599"
  },
  {
    "text": "so there's still going to be some of course environment changes going on but that's easy to deal with um and then the rappers jim's rappers",
    "start": "2521599",
    "end": "2529200"
  },
  {
    "text": "are going to essentially be redone in full um when that's going to happen isn't",
    "start": "2529200",
    "end": "2535280"
  },
  {
    "text": "entirely known and then at some point after there's like a sort of stable release",
    "start": "2535280",
    "end": "2540960"
  },
  {
    "text": "we're going to go and completely redo all the vector api stuff to have it natively do good vectorization with",
    "start": "2540960",
    "end": "2548319"
  },
  {
    "text": "asynchronous support with like within pool which lets you use all the cpu cores and like all the stuff you would",
    "start": "2548319",
    "end": "2554800"
  },
  {
    "text": "want at the natively do fracturization do that with a good api just we want to get the base api and everything else",
    "start": "2554800",
    "end": "2560720"
  },
  {
    "text": "stable and all the environment stable for you that for what hopefully obvious reasons",
    "start": "2560720",
    "end": "2566640"
  },
  {
    "text": "um yeah then in terms of",
    "start": "2566640",
    "end": "2572240"
  },
  {
    "text": "then in terms of petting zoo stuff what we've been doing is uh after the petting zoo",
    "start": "2572240",
    "end": "2578720"
  },
  {
    "text": "after making change of major gyms api we've just been making them into petting zoo so there's gonna be another release",
    "start": "2578720",
    "end": "2584400"
  },
  {
    "text": "again this week that'll have all the breaking changes from the last batch to jim finally released an out to",
    "start": "2584400",
    "end": "2590880"
  },
  {
    "text": "petting zoo and super suit um and then we'll be making the breaking changes from this release to truncation",
    "start": "2590880",
    "end": "2596640"
  },
  {
    "text": "and rendering the petting zoo and um at in some point in your uh future or in",
    "start": "2596640",
    "end": "2603760"
  },
  {
    "text": "the next few months more likely the other thing going on is that in the make function of gem when you take the first",
    "start": "2603760",
    "end": "2610480"
  },
  {
    "text": "step in an environment we apply the imp checker to it now because none of the environments for jim followed the api",
    "start": "2610480",
    "end": "2617280"
  },
  {
    "text": "especially with the changes but even before they still didn't ever and this way hopefully you'll have a",
    "start": "2617280",
    "end": "2623280"
  },
  {
    "text": "much higher level of api compliance going on um [Music]",
    "start": "2623280",
    "end": "2628400"
  },
  {
    "text": "another thing of relevance too i know you support procter and gym three we have a maintained fork of proctogen that",
    "start": "2628400",
    "end": "2633760"
  },
  {
    "text": "hopefully will be linked to by on the official repo soon they're calling just proc gen 2 lack of better",
    "start": "2633760",
    "end": "2639040"
  },
  {
    "text": "name but just procton except it's kind of documentation website it's going to support the actual gen api it's going to have bug fixes it's going to be",
    "start": "2639040",
    "end": "2645359"
  },
  {
    "text": "available on pi pi basic stuff like that um whatever good that will do you and",
    "start": "2645359",
    "end": "2651760"
  },
  {
    "text": "the final thing i wanted to mention was uh talk about platform 3.6 so a while ago so so whenever you guys try",
    "start": "2651760",
    "end": "2658560"
  },
  {
    "text": "to upgrade versions of gym and petting zoo zoo drop 3.6 um",
    "start": "2658560",
    "end": "2664160"
  },
  {
    "text": "you guys um complained a bunch because you had to",
    "start": "2664160",
    "end": "2669440"
  },
  {
    "text": "maintain 3.6 due to whatever the reasons are now another problem the reason we talked about 3.6 is",
    "start": "2669440",
    "end": "2675200"
  },
  {
    "text": "quite literally every single library gym depends on um doesn't support 3.6 anymore",
    "start": "2675200",
    "end": "2681200"
  },
  {
    "text": "and there's various types and stuff into that which started 3.7 that we've been using",
    "start": "2681200",
    "end": "2686800"
  },
  {
    "text": "and um so there's been that",
    "start": "2686800",
    "end": "2692800"
  },
  {
    "text": "and you know pi 3.6 isn't even secure updates anymore there's you know python 3.6 you know bugs and after exploitation",
    "start": "2692800",
    "end": "2699119"
  },
  {
    "text": "right now they're never going to be patched you can google this on the cve lists but apparently guys still had to have",
    "start": "2699119",
    "end": "2704400"
  },
  {
    "text": "three point six so we went to in the minute in the last gen release we went and actually re-added three point six support temporarily just for rl",
    "start": "2704400",
    "end": "2711920"
  },
  {
    "text": "um they're gonna have a depending it's looking like we aren't going to be able to do that um for the most part and the reason that",
    "start": "2711920",
    "end": "2718960"
  },
  {
    "text": "we probably aren't going to be able to to to keep doing the 3.6 support is that jax is support 3.6 and we're",
    "start": "2718960",
    "end": "2726000"
  },
  {
    "text": "having a bunch of four api integrations going on with jax so that environments can be harder accelerating run on gpus",
    "start": "2726000",
    "end": "2731520"
  },
  {
    "text": "if you see that before so i don't know how you want to handle this but like python 3.6 has been you",
    "start": "2731520",
    "end": "2737839"
  },
  {
    "text": "know not supported by the other major libraries for a very long time it's it has active exploitations a while",
    "start": "2737839",
    "end": "2744079"
  },
  {
    "text": "that'll have to be fixed and like if we're going to continue we're doing harder solar environments like it's not up to us anymore to stop supporting 3.6",
    "start": "2744079",
    "end": "2752880"
  },
  {
    "text": "yeah i think the reason why we're still sticking to 3.6 it's like more like a ray upstream thing uh",
    "start": "2754240",
    "end": "2759440"
  },
  {
    "text": "yeah that's what i was told we have moved all our tests already to 3.7 um i don't think we can test probably on",
    "start": "2759440",
    "end": "2765839"
  },
  {
    "text": "3.6 anymore um our lip stuff but ray still officially supports 3.6 so that's",
    "start": "2765839",
    "end": "2772000"
  },
  {
    "text": "that's an issue there um okay those are outlets",
    "start": "2772000",
    "end": "2778640"
  },
  {
    "text": "even if base rate does",
    "start": "2778640",
    "end": "2781680"
  },
  {
    "text": "yeah because it's installed together right um",
    "start": "2784960",
    "end": "2790318"
  },
  {
    "text": "that's yeah i wish i can ask whether it's i think it's policy that we should all be on the same",
    "start": "2794480",
    "end": "2802079"
  },
  {
    "text": "unfortunate just because there's a lot of development happening on all the open source stuff in our uh nrl right now and it's just like 3.6",
    "start": "2802079",
    "end": "2809200"
  },
  {
    "text": "is just being treated as end of life dropped right so i mean dropped a year ago",
    "start": "2809200",
    "end": "2815520"
  },
  {
    "text": "yeah yeah yeah it makes sense and i understand that like you know",
    "start": "2815520",
    "end": "2820960"
  },
  {
    "text": "industries and partners potentially have uh have this requirement it takes a long time to get it out i'm just wondering",
    "start": "2820960",
    "end": "2827359"
  },
  {
    "text": "you know like if you take a look at that list if there are people actually doing rl",
    "start": "2827359",
    "end": "2832640"
  },
  {
    "text": "oh jeez if there are people actually doing rl on 3.6 or if it's just rayon",
    "start": "2832640",
    "end": "2838160"
  },
  {
    "text": "3.6 right because like if it's really if it's just rayon 3.6 it makes it so much easier for",
    "start": "2838160",
    "end": "2844079"
  },
  {
    "text": "everyone involved to just drop it from rl lab at the least i'll i'll ask again",
    "start": "2844079",
    "end": "2850640"
  },
  {
    "text": "yeah because like because like like we're only working on this like like",
    "start": "2850640",
    "end": "2855839"
  },
  {
    "text": "with with the newest stuff with jax like this isn't even up to us anymore you know",
    "start": "2855839",
    "end": "2861200"
  },
  {
    "text": "yeah it's like at this point it like supporting this stuff it even if you continue supporting 3.6 it's really",
    "start": "2864559",
    "end": "2871599"
  },
  {
    "text": "going to be nominal um because you're going to be missing all the new features right like like",
    "start": "2871599",
    "end": "2876640"
  },
  {
    "text": "jordan says it's not even up to us so you're going to be having like old style gym without all the new stuff",
    "start": "2876640",
    "end": "2883760"
  },
  {
    "text": "that you actually want to use if you want to get stuff done yeah yeah",
    "start": "2883760",
    "end": "2889040"
  },
  {
    "text": "yeah makes sense i will try to closely align with with the gym updates and the great to make sure that they're flowing",
    "start": "2889040",
    "end": "2894400"
  },
  {
    "text": "then let me just ask one more thing um so jim is this is going to start",
    "start": "2894400",
    "end": "2901359"
  },
  {
    "text": "being released soon but over the next two to six months because the estimated",
    "start": "2901359",
    "end": "2906640"
  },
  {
    "text": "time is hard all the environments in gym are going to be hardware accelerated by default now",
    "start": "2906640",
    "end": "2912000"
  },
  {
    "text": "there's like like they'll run a gpus or tps or whatever by default so my question for this and and for this",
    "start": "2912000",
    "end": "2919200"
  },
  {
    "text": "is uh in you know this is a hundred to a few thousands faster than in environments normally are right",
    "start": "2919200",
    "end": "2927119"
  },
  {
    "text": "so my question is that like for a lot of libraries you have to do semi-custom code to have them um actually",
    "start": "2927520",
    "end": "2936319"
  },
  {
    "text": "you know support this you have to do something custom code actually have the environments um",
    "start": "2936319",
    "end": "2943520"
  },
  {
    "text": "uh sorry to have the learning codes of um you know functions harder accelerated environments with the proper performance improvements so i'm just going to say",
    "start": "2943520",
    "end": "2950160"
  },
  {
    "text": "does rl have this and if not is adding that on your roadmap uh it's on our roadmap to support",
    "start": "2950160",
    "end": "2956960"
  },
  {
    "text": "environments uh producing data on the gpu tpu and then",
    "start": "2956960",
    "end": "2962240"
  },
  {
    "text": "without moving it to somewhere else directly for learning that's all uh it's actually very close we're",
    "start": "2962240",
    "end": "2968079"
  },
  {
    "text": "actually very close to um some initial",
    "start": "2968079",
    "end": "2973520"
  },
  {
    "text": "exploration as to how much it will accelerate um do you guys have any other questions straight about jim or pettings or",
    "start": "2973520",
    "end": "2979520"
  },
  {
    "text": "anything else um",
    "start": "2979520",
    "end": "2984839"
  },
  {
    "text": "nope nice that's right actually yeah thanks thanks for all the updates and uh information that was great",
    "start": "2985119",
    "end": "2990880"
  },
  {
    "text": "all right thank you guys so much and i'll see you later joseph can you call me for like two minutes all right we lost the two other people",
    "start": "2990880",
    "end": "2997359"
  },
  {
    "text": "who had questions uh yeah uh it's okay um",
    "start": "2997359",
    "end": "3003920"
  },
  {
    "text": "uh yeah many anything anything else otherwise we can just continue this discussion awesome",
    "start": "3003920",
    "end": "3009280"
  },
  {
    "text": "thanks so much this is great yeah does manny have any questions or no yeah many",
    "start": "3009280",
    "end": "3014640"
  },
  {
    "text": "anything else no i i don't have anything else i just made some popcorn and was watching [Laughter]",
    "start": "3014640",
    "end": "3021359"
  },
  {
    "text": "yeah great thanks thanks [Music]",
    "start": "3021359",
    "end": "3026640"
  },
  {
    "text": "all right hey then thanks sarah for your time and hard work i'll see you in two weeks",
    "start": "3026640",
    "end": "3034520"
  }
]