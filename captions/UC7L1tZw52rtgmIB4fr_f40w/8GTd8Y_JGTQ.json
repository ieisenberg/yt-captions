[
  {
    "text": "hi my name is edward oakes and today i have the pleasure of introducing you to any scale",
    "start": "160",
    "end": "5200"
  },
  {
    "text": "as jan mentioned earlier any scale makes it easy to build and run distributed applications in this demo we're going to build an",
    "start": "5200",
    "end": "11200"
  },
  {
    "text": "end-to-end online learning system in a matter of minutes specifically we're going to be building a movie recommendation system",
    "start": "11200",
    "end": "19199"
  },
  {
    "text": "so here's the ui for the application that we want to build you can imagine this being the front end for a movie streaming service the any scale cinema",
    "start": "19840",
    "end": "27199"
  },
  {
    "text": "the idea is really simple we just want to be able to recommend new movies to users based on movies that they've liked",
    "start": "27199",
    "end": "32558"
  },
  {
    "text": "in the past this sounds like a simple idea but in practice it can be really hard to do end",
    "start": "32559",
    "end": "37680"
  },
  {
    "text": "to end let's dive into what it will take to build this today and then i'll show you how much easier it is using any scale in",
    "start": "37680",
    "end": "44480"
  },
  {
    "text": "grey so first we need to build the api that will actually return recommendations to",
    "start": "44480",
    "end": "49840"
  },
  {
    "text": "users the first thing that we need is just a web server for this we could use something simple like flask",
    "start": "49840",
    "end": "57039"
  },
  {
    "text": "then we also need to forward these requests to a number of different models to actually get the recommendations we",
    "start": "57039",
    "end": "62879"
  },
  {
    "text": "want to present to users if we want to customize this to each user based on their preferences we'll also need to build a custom service that",
    "start": "62879",
    "end": "69200"
  },
  {
    "text": "stores per user state and finally over time we'll want to",
    "start": "69200",
    "end": "74400"
  },
  {
    "text": "improve our models as new user data is coming in so we'll need to deploy yet another custom",
    "start": "74400",
    "end": "79680"
  },
  {
    "text": "service that will fetch new user data and deploy new versions of models",
    "start": "79680",
    "end": "85119"
  },
  {
    "text": "so this is starting to sound really complicated but unfortunately serving is only one piece of the puzzle",
    "start": "85119",
    "end": "91200"
  },
  {
    "text": "in addition we'll likely need to do some kind of basic parallel python processing in order to actually build our models so for this we might do",
    "start": "91200",
    "end": "98320"
  },
  {
    "text": "some nlp with spacey or image processing with pillow and to scale these up we would use joblib or maybe",
    "start": "98320",
    "end": "104720"
  },
  {
    "text": "even spark or desk then we need to actually train our",
    "start": "104720",
    "end": "109840"
  },
  {
    "text": "models so for this we could use pi torture tensorflow or maybe scikit-learn and to scale this up and do a hyper",
    "start": "109840",
    "end": "116399"
  },
  {
    "text": "parameter search we'd also need to integrate with hyperopt or optuma in addition each of these tools and",
    "start": "116399",
    "end": "122799"
  },
  {
    "text": "libraries comes with a different way to build and deploy your application in many cases we also need to run them",
    "start": "122799",
    "end": "128959"
  },
  {
    "text": "or we would want to have the flexibility to run them across different platforms or cloud providers",
    "start": "128959",
    "end": "134239"
  },
  {
    "text": "to doing all of this end-to-end is obviously incredibly complicated for a team let alone one individual",
    "start": "134239",
    "end": "141760"
  },
  {
    "text": "today i'm going to show you how ray and any scale's universal computing platform enable us to focus only on our",
    "start": "141760",
    "end": "147200"
  },
  {
    "text": "application logic and build this end-to-end ai application in just a few short minutes",
    "start": "147200",
    "end": "153599"
  },
  {
    "text": "we're going to leverage ray and its native libraries to do a lot of the heavy lifting so first we'll do parallel python",
    "start": "154080",
    "end": "160400"
  },
  {
    "text": "processing and we'll scale this up using the core ray api",
    "start": "160400",
    "end": "165599"
  },
  {
    "text": "then we'll perform a hyper parameter search to do ml training using a native ray library called raytune",
    "start": "165680",
    "end": "173360"
  },
  {
    "text": "finally we'll deploy the models we developed to production using rayserv and on top of this we'll also be",
    "start": "173360",
    "end": "180159"
  },
  {
    "text": "able to easily deploy the custom services that we need just by writing simple python classes",
    "start": "180159",
    "end": "185200"
  },
  {
    "text": "and deploying them as reactors and finally because any scale is",
    "start": "185200",
    "end": "191040"
  },
  {
    "text": "multi-cloud native and provides a seamless local development experience we'll be able to write and test our code",
    "start": "191040",
    "end": "197040"
  },
  {
    "text": "locally scale up our pre-processing and ml training workloads on gcp and then deploy to aws",
    "start": "197040",
    "end": "204000"
  },
  {
    "text": "and do it all from my laptop note that any scale and ray are really just providing the infrastructure here",
    "start": "204000",
    "end": "209920"
  },
  {
    "text": "and we'll still be using all of our favorite libraries to build each individual piece",
    "start": "209920",
    "end": "215120"
  },
  {
    "text": "and in addition while we're focused on ai in this demo this is really just one example of a general distributed",
    "start": "215120",
    "end": "220400"
  },
  {
    "text": "application that any skill and ray enable you to build so let's get started with building this",
    "start": "220400",
    "end": "228000"
  },
  {
    "text": "before we actually dive into the details let's see what we currently have here we have the any scale cinema ui and",
    "start": "228000",
    "end": "235439"
  },
  {
    "text": "you can see we're just recommending random recommendations based on popular movies and if i click on these we save",
    "start": "235439",
    "end": "242000"
  },
  {
    "text": "them as liked by the user but we're not actually providing any recommendations that'll be the goal for the demo so note",
    "start": "242000",
    "end": "249280"
  },
  {
    "text": "that the api for this is running on rey serve inside of any scale session here i have",
    "start": "249280",
    "end": "254640"
  },
  {
    "text": "the any scale ui open you can see that we're in a project called movie recommendations and a project is just how you can",
    "start": "254640",
    "end": "261040"
  },
  {
    "text": "organize and share your work inside this project we currently have one active session",
    "start": "261040",
    "end": "266560"
  },
  {
    "text": "it's running on aws and it's called serve session so this is serving our production api",
    "start": "266560",
    "end": "273440"
  },
  {
    "text": "if we take a look in here we can see that a session is really nothing more than a ray cluster with a bunch of nice",
    "start": "273680",
    "end": "279919"
  },
  {
    "text": "features out of the box so we can easily view logs we can see the commands we've run on the session",
    "start": "279919",
    "end": "285919"
  },
  {
    "text": "and we also get jupiter for interactive development easy access to the ray dashboard",
    "start": "285919",
    "end": "292000"
  },
  {
    "text": "tensorboard for ml training and an integration with grafana for production monitor",
    "start": "292000",
    "end": "298960"
  },
  {
    "text": "so now that we've oriented ourselves let's get started with developing our first model",
    "start": "299280",
    "end": "306160"
  },
  {
    "text": "so we'll begin by using ray to scale up a parallel python workload and use the output to build one of our models",
    "start": "307600",
    "end": "314720"
  },
  {
    "text": "instead of building a fancy machine learning model to begin with we're going to start off by recommending movies just based on the similarity of their cover",
    "start": "314720",
    "end": "321360"
  },
  {
    "text": "images i'm really not sure how effective this is going to be but it should be easy to build and for some reason my manager",
    "start": "321360",
    "end": "327520"
  },
  {
    "text": "really wants to do it so let's get to work we'll implement this by taking a movie that the user",
    "start": "327520",
    "end": "333600"
  },
  {
    "text": "liked and then performing a k nearest neighbors search to find movie covers with a similar color palette and then",
    "start": "333600",
    "end": "339520"
  },
  {
    "text": "recommending those movies to the user we'll use a facebook research library",
    "start": "339520",
    "end": "344880"
  },
  {
    "text": "called faiss to perform this k nearest neighbor search",
    "start": "344880",
    "end": "350160"
  },
  {
    "text": "in order to actually do this though we're going to need to do some pre-processing first so we need to extract color palettes to",
    "start": "350560",
    "end": "357280"
  },
  {
    "text": "make the search feasible here you can see an example on the left we have the movie cover for star wars",
    "start": "357280",
    "end": "362800"
  },
  {
    "text": "return of the jedi and then on the right we have the extracted color palette which will make it easy to efficiently search over",
    "start": "362800",
    "end": "370560"
  },
  {
    "text": "so first let me give you a sneak peek of what we're about to do first we're going to extract these color palettes using a simple script on",
    "start": "371520",
    "end": "378080"
  },
  {
    "text": "one process on my laptop then we'll change just a few lines of code and parallelize it across the cores of my",
    "start": "378080",
    "end": "384400"
  },
  {
    "text": "laptop using ray and finally we'll start in any scale session not need to change the code at",
    "start": "384400",
    "end": "390720"
  },
  {
    "text": "all and scale this up to hundreds of cores in the cloud so let's get started",
    "start": "390720",
    "end": "396800"
  },
  {
    "text": "a data scientist on our team was nice enough to give us this palettes.pi script",
    "start": "398639",
    "end": "403680"
  },
  {
    "text": "which will extract the color palettes for all of our images if we give this a run we can see that it",
    "start": "403680",
    "end": "409759"
  },
  {
    "text": "seems to be working but unfortunately it's also very slow you can see our progress bar is not really moving at all and we're only able",
    "start": "409759",
    "end": "415840"
  },
  {
    "text": "to do about one image per second so thankfully ray makes it really easy to paralyze existing code",
    "start": "415840",
    "end": "421919"
  },
  {
    "text": "like this so let's see what we can do up above i have the source code for palettes.pi",
    "start": "421919",
    "end": "428800"
  },
  {
    "text": "let's take a look to understand what's happening first so here we're just getting the ids of",
    "start": "428800",
    "end": "434479"
  },
  {
    "text": "all of the images in our data set then most of the core logic is happening in this get palette function",
    "start": "434479",
    "end": "441120"
  },
  {
    "text": "you can see we're using an off-the-shelf python library called color thief we're just passing in our image and then",
    "start": "441120",
    "end": "448000"
  },
  {
    "text": "calling get palette to get the palette that we need then we just call this function across",
    "start": "448000",
    "end": "453360"
  },
  {
    "text": "all of the images in our data set and once we've finished we write the output palettes to a file",
    "start": "453360",
    "end": "459360"
  },
  {
    "text": "seems simple enough so let's parallelize it using ray we only need to change a few lines of",
    "start": "459360",
    "end": "464960"
  },
  {
    "text": "code here first we import ray and connect to the array runtime then we use ray.remote to convert this",
    "start": "464960",
    "end": "472240"
  },
  {
    "text": "function to a remote task definition and then finally when we invoke the",
    "start": "472240",
    "end": "477599"
  },
  {
    "text": "function we're going to call dot remote as well to spin this off in parallel and because",
    "start": "477599",
    "end": "483039"
  },
  {
    "text": "these are running in parallel now we need to actually get all of the results at the output using ray.get",
    "start": "483039",
    "end": "489759"
  },
  {
    "text": "we're also going to use this progress bar utility just to get that nice progress bar we had before",
    "start": "489759",
    "end": "495520"
  },
  {
    "text": "and those should be all of the changes that we need to paralyze it across the core of my laptop let's try running it again",
    "start": "496240",
    "end": "503360"
  },
  {
    "text": "so just like that you can see that we're now running significantly faster we can get something like eight images per second because i have eight",
    "start": "505360",
    "end": "511360"
  },
  {
    "text": "cores on my laptop but it's still going to take quite a while to get through all of these images",
    "start": "511360",
    "end": "516800"
  },
  {
    "text": "so let's try using any scale to scale this up to the cloud if you recall from before we currently",
    "start": "516800",
    "end": "522880"
  },
  {
    "text": "only have a single session running our production serving session and that was running in aws",
    "start": "522880",
    "end": "530480"
  },
  {
    "text": "but any scale is multi-cloud native so it's really easy to run even multiple sessions in the same project",
    "start": "530480",
    "end": "536000"
  },
  {
    "text": "on multiple clouds here you can see that we currently have gcp and aws configured and given that i have",
    "start": "536000",
    "end": "542160"
  },
  {
    "text": "some gcp credits why don't we run this parallel processing workload in gcp",
    "start": "542160",
    "end": "548399"
  },
  {
    "text": "to do that all we need to do is run any scale up give it the name of the cloud we want to run on and then give it a name",
    "start": "548399",
    "end": "555680"
  },
  {
    "text": "let's call it dev session so we can do some development",
    "start": "555680",
    "end": "559920"
  },
  {
    "text": "this should take just a few seconds to initialize and in the meantime we can also configure any scale auto sync",
    "start": "560959",
    "end": "567920"
  },
  {
    "text": "so auto sync lets us keep the code on our laptop up to date in the session so that when we want to run our code we don't need to",
    "start": "567920",
    "end": "574399"
  },
  {
    "text": "worry about manually pushing it so all we need to do is run any scale auto sync and give it the name of our",
    "start": "574399",
    "end": "580000"
  },
  {
    "text": "session and this will copy over the files from our laptop to the session make sure that the files stay up to date",
    "start": "580000",
    "end": "586320"
  },
  {
    "text": "and we'll also copy down any outputs from our script back down to my laptop",
    "start": "586320",
    "end": "592240"
  },
  {
    "text": "now that the session looks like it's finished starting up let's take a look at it",
    "start": "592720",
    "end": "598399"
  },
  {
    "text": "so here we can see we now have the search session running on aws and in addition we have this new dev session we just started running on gcp",
    "start": "600080",
    "end": "608240"
  },
  {
    "text": "if we take a closer look at the dev session we can open up the ray dashboard to see what's currently going on in this",
    "start": "608240",
    "end": "614240"
  },
  {
    "text": "cluster here the ray dashboard is showing us the information about all of the hosts in",
    "start": "614240",
    "end": "619920"
  },
  {
    "text": "the cluster we can see that we currently only have one machine and that machine has four cores and",
    "start": "619920",
    "end": "626320"
  },
  {
    "text": "there isn't too much happening on it right now if we had gpus we could also see information about gpus and their",
    "start": "626320",
    "end": "632320"
  },
  {
    "text": "utilization here so let's try running our script on this new session that we just created",
    "start": "632320",
    "end": "639120"
  },
  {
    "text": "to do that all we need to do is use any scale exec give it the name of our dev session and",
    "start": "639120",
    "end": "644720"
  },
  {
    "text": "then run python palettes dot pi because the code is auto synced up",
    "start": "644720",
    "end": "650240"
  },
  {
    "text": "that's all it takes and in a second we should see this start to run",
    "start": "650240",
    "end": "656720"
  },
  {
    "text": "so at first this shouldn't be much faster than it was running locally because we actually only have that",
    "start": "658000",
    "end": "664640"
  },
  {
    "text": "single host so you can see we can still only do like eight iterations per second but the great thing about any skill is",
    "start": "664640",
    "end": "671040"
  },
  {
    "text": "that it's gonna automatically recognize we don't have enough resources to run all these tasks and spin up new",
    "start": "671040",
    "end": "676399"
  },
  {
    "text": "worker notes for us so here you can see we now have six hosts and we're still scaling up",
    "start": "676399",
    "end": "683120"
  },
  {
    "text": "and each one of these hosts once it joined automatically started working on these tasks and the cpu is being fully",
    "start": "683120",
    "end": "688800"
  },
  {
    "text": "utilized so with all of this parallelism it shouldn't take too long to get through all of our images",
    "start": "688800",
    "end": "695440"
  },
  {
    "text": "we're now upwards of 10 hosts and total of 180 cores in the cluster and if we take a look at our progress",
    "start": "695440",
    "end": "701600"
  },
  {
    "text": "bar we can see it's going much faster so we're able to do hundreds of images per second because now we have",
    "start": "701600",
    "end": "707360"
  },
  {
    "text": "hundreds of cores in our cluster so just like that we got through all of the images in our data set",
    "start": "707360",
    "end": "713519"
  },
  {
    "text": "auto sync should have also synced down the result to my laptop so here we can see this is just all of",
    "start": "713519",
    "end": "719519"
  },
  {
    "text": "the raw color palettes that we just pre-processed so all that's left to do is to upload",
    "start": "719519",
    "end": "725440"
  },
  {
    "text": "these to the database and we should be done with our free process",
    "start": "725440",
    "end": "731760"
  },
  {
    "text": "okay and just like that we finished so to recap we started with a simple",
    "start": "731760",
    "end": "737120"
  },
  {
    "text": "script that was provided by a data scientist and ran it on our laptop on one core that was too slow so we used ray with",
    "start": "737120",
    "end": "743279"
  },
  {
    "text": "just a few lines of code change to parallelize it across the cores of my laptop then we didn't change any of the code",
    "start": "743279",
    "end": "750240"
  },
  {
    "text": "and just started a new any scale session from the cli and were able to scale this up to hundreds of cores in the cloud",
    "start": "750240",
    "end": "759839"
  },
  {
    "text": "so now that we've finished pre-processing the images we should be ready to build and deploy the color-based model",
    "start": "760639",
    "end": "765920"
  },
  {
    "text": "however before we get to that one of the ml researchers on our team suggested that we should also try a deep learning",
    "start": "765920",
    "end": "771519"
  },
  {
    "text": "model their suggestion is to use nlp to recommend movies based on their plot",
    "start": "771519",
    "end": "777680"
  },
  {
    "text": "descriptions so first we'll take a movie that the user liked and then we'll search for movies with a similar plot",
    "start": "777680",
    "end": "784639"
  },
  {
    "text": "well i guess in this case it's it's pretty easy because all these movies basically have the same plot but you get the idea so we'll implement",
    "start": "784639",
    "end": "792480"
  },
  {
    "text": "this this search using the same fa iss library and and also using a hugging face",
    "start": "792480",
    "end": "799440"
  },
  {
    "text": "implementation of bert which is a popular nlp deep learning model",
    "start": "799440",
    "end": "804560"
  },
  {
    "text": "we're also going to add another step to this model which is to rank the results of this search using a simple logistic",
    "start": "804560",
    "end": "810240"
  },
  {
    "text": "regression model and only presenting the highest ranked movies to the user we can evolve this ranking model over",
    "start": "810240",
    "end": "816560"
  },
  {
    "text": "time to improve our recommendations so let's get started with implementing this",
    "start": "816560",
    "end": "822480"
  },
  {
    "text": "so here i'm back in my local editor so we could just use an off-the-shelf",
    "start": "824959",
    "end": "830000"
  },
  {
    "text": "pre-trained implementation of bert but to make this really effective we want to fine-tune it on some",
    "start": "830000",
    "end": "835199"
  },
  {
    "text": "movie plot descriptions that i have here in my local editor so in order to do this we're going to",
    "start": "835199",
    "end": "841199"
  },
  {
    "text": "leverage a native ray library ray tune so tune makes it really easy to run hyper parameter searches and because",
    "start": "841199",
    "end": "847360"
  },
  {
    "text": "it's built on ray we can easily develop the code on my laptop and then scale it up to a cluster",
    "start": "847360",
    "end": "853920"
  },
  {
    "text": "so let's take a look at how the script works so first you can see we're just using an off-the-shelf version of",
    "start": "853920",
    "end": "859440"
  },
  {
    "text": "bert from hugging face then we define our hyper parameter search so we need to",
    "start": "859440",
    "end": "865760"
  },
  {
    "text": "define the parameters we actually want to search over here we also need to specify a stopping",
    "start": "865760",
    "end": "871120"
  },
  {
    "text": "condition or when to stop tuning the model in reality you probably want to run this for a very long time to get the best",
    "start": "871120",
    "end": "876880"
  },
  {
    "text": "model but let's just run it for 20 iterations for the sake of the demo and another thing that's really great",
    "start": "876880",
    "end": "882160"
  },
  {
    "text": "about tune is it comes with a lot of great ecosystem integration so for example if we want to use weights and biases to",
    "start": "882160",
    "end": "889040"
  },
  {
    "text": "log the results of our hyperparameter tuning we can easily do that just by adding a few lines in the config",
    "start": "889040",
    "end": "894399"
  },
  {
    "text": "we just need to set a a project name for weights and biases and just like that tune should log the",
    "start": "894399",
    "end": "900480"
  },
  {
    "text": "results automatically and finally because this is a pretty heavyweight deep learning model",
    "start": "900480",
    "end": "907440"
  },
  {
    "text": "we're definitely going to want to train it on gpus to do that we just need to change this one line of code so that",
    "start": "907440",
    "end": "913120"
  },
  {
    "text": "we're not only using one cpu per trial but also one gpu",
    "start": "913120",
    "end": "919360"
  },
  {
    "text": "so we should be ready to run this let's just run it directly on our existing dev session so we're going",
    "start": "919600",
    "end": "926240"
  },
  {
    "text": "to use any scale exact once again and just run this tuned script as that starts up",
    "start": "926240",
    "end": "934880"
  },
  {
    "text": "let's take a look at what's happening in our session so here we can see that because we haven't been using the",
    "start": "934880",
    "end": "940000"
  },
  {
    "text": "resources the any scale platform has automatically scaled down back to just this single host",
    "start": "940000",
    "end": "945920"
  },
  {
    "text": "but because we need gpus for these trials any skills should automatically start adding gpu nodes to the customer",
    "start": "945920",
    "end": "954000"
  },
  {
    "text": "so now we can see that we're having new nodes get started up and you can see from these two columns that these have gpus available",
    "start": "954000",
    "end": "960240"
  },
  {
    "text": "it looks like they're tesla t4s so after a few seconds",
    "start": "960240",
    "end": "967120"
  },
  {
    "text": "it should take a few seconds to load the models into memory but then we'll see that the training will start to run",
    "start": "967120",
    "end": "973199"
  },
  {
    "text": "so you can see that now we have it running on a few of the gpus and the rest should follow",
    "start": "973199",
    "end": "978720"
  },
  {
    "text": "so just like that with that one line of code in that simple tune script we were able to scale this up to a",
    "start": "978720",
    "end": "984880"
  },
  {
    "text": "cluster of gpus while this is running let's check out",
    "start": "984880",
    "end": "990320"
  },
  {
    "text": "weights and biases to monitor its progress so here we can find the link that was printed for",
    "start": "990320",
    "end": "997839"
  },
  {
    "text": "our weights and biases project and here we can see a bunch of stats",
    "start": "997839",
    "end": "1002959"
  },
  {
    "text": "about the trials that we're running so as you can see the as the number of iterations goes up it looks like the",
    "start": "1002959",
    "end": "1008720"
  },
  {
    "text": "loss is going down which means that the training is working awesome",
    "start": "1008720",
    "end": "1014240"
  },
  {
    "text": "so back in the dashboard it looks like the training is pretty much finished",
    "start": "1014240",
    "end": "1019360"
  },
  {
    "text": "because we specified that pretty early stopping condition and just like that we have finished",
    "start": "1019360",
    "end": "1026079"
  },
  {
    "text": "fine-tuning the model and it should be uploaded so that we can download it later when we're serving",
    "start": "1026079",
    "end": "1033199"
  },
  {
    "text": "so let's recap what we've been able to do thus far so we started by just creating a blank",
    "start": "1035439",
    "end": "1041120"
  },
  {
    "text": "dev session in any scale using any scale up then we used any scale exec and ran our",
    "start": "1041120",
    "end": "1046640"
  },
  {
    "text": "image processing workload and seamlessly scaled up to hundreds of cpus",
    "start": "1046640",
    "end": "1052640"
  },
  {
    "text": "while we were coding up our hyper parameter tuning script the session automatically scaled down back to nothing",
    "start": "1052640",
    "end": "1058799"
  },
  {
    "text": "and then just by running any scale exec with this hyperparameter tuning script that said we needed gpus we scaled up to",
    "start": "1058799",
    "end": "1066480"
  },
  {
    "text": "a cluster of gpus in the exact same dev session that we were already using",
    "start": "1066480",
    "end": "1072240"
  },
  {
    "text": "so now that we've finished our pre-processing in our ml training we should be ready to start working on",
    "start": "1074160",
    "end": "1079760"
  },
  {
    "text": "actually deploying our models to production so first we need to deploy the two",
    "start": "1079760",
    "end": "1086240"
  },
  {
    "text": "models that we just finished doing the pre-processing and training for to do this we're going to use another native write library called race search",
    "start": "1086240",
    "end": "1095840"
  },
  {
    "text": "so back in my editor i have another simple python script open here so here you can see we're just defining",
    "start": "1095840",
    "end": "1101840"
  },
  {
    "text": "a regular python class called plot recommender which is going to implement the nlp plot recommendations that we talked about",
    "start": "1101840",
    "end": "1107760"
  },
  {
    "text": "earlier so in the constructor you can see that we're first loading in our",
    "start": "1107760",
    "end": "1113360"
  },
  {
    "text": "fine-tuned burt embeddings and building an index out of them using the facebook research library",
    "start": "1113360",
    "end": "1118480"
  },
  {
    "text": "faiss then we're loading in our logistic regression model that will be used to rank the",
    "start": "1118480",
    "end": "1124880"
  },
  {
    "text": "movies so let's define the logic that happens when a request comes in",
    "start": "1124880",
    "end": "1130480"
  },
  {
    "text": "so first we're going to get movies with similar plots using our plot index so we just need to search over it and",
    "start": "1130480",
    "end": "1136080"
  },
  {
    "text": "give it the request then we're going to use our logistic regression model to rank the movies",
    "start": "1136080",
    "end": "1142880"
  },
  {
    "text": "that were output from our search and that should be all it takes so we just need to implement this really",
    "start": "1142880",
    "end": "1149039"
  },
  {
    "text": "simple python class and now we can deploy this model to actually deploy it we just need a few",
    "start": "1149039",
    "end": "1155600"
  },
  {
    "text": "more lines of code so we need to tell serve to create a backend with this class that we just made",
    "start": "1155600",
    "end": "1161360"
  },
  {
    "text": "and then we expose that back end at a specified http route so all this means is that when we send a",
    "start": "1161360",
    "end": "1167679"
  },
  {
    "text": "request to this route the class that we wrote above is going to handle it and return a response",
    "start": "1167679",
    "end": "1173919"
  },
  {
    "text": "so one of the great things about ray is that we can easily test our application locally and then",
    "start": "1173919",
    "end": "1179679"
  },
  {
    "text": "run the exact same cl uh code when we're actually running in a cluster so let's try testing this out locally",
    "start": "1179679",
    "end": "1185919"
  },
  {
    "text": "so first we just need to set up race serv on our local laptop it should take just a second and then we",
    "start": "1185919",
    "end": "1192480"
  },
  {
    "text": "can just run the script that we just defined deploy plot so this will connect to the",
    "start": "1192480",
    "end": "1198799"
  },
  {
    "text": "local ray cluster that we just started and it looks like it finished the point we can test this out with a really",
    "start": "1198799",
    "end": "1204880"
  },
  {
    "text": "simple curl command so here i'm just sending requests to that endpoint that we specified above",
    "start": "1204880",
    "end": "1210640"
  },
  {
    "text": "and i'm giving it an id of a movie that i know exists in the data set",
    "start": "1210640",
    "end": "1215760"
  },
  {
    "text": "as you can see it looks like it's working as a response we got a bunch of other movies that should have similar plots",
    "start": "1215760",
    "end": "1222159"
  },
  {
    "text": "this isn't too meaningful without actually seeing the you know the movie descriptions but we'll be able to see that in the ui",
    "start": "1222159",
    "end": "1227679"
  },
  {
    "text": "soon so in addition to this deploy plot script we have a really similar one for",
    "start": "1227679",
    "end": "1233120"
  },
  {
    "text": "deploying our color based model but we can skip over looking at the code for that because it looks very similar",
    "start": "1233120",
    "end": "1240720"
  },
  {
    "text": "so now that we have these two models ready to go we also want to be able to dynamically",
    "start": "1241200",
    "end": "1246640"
  },
  {
    "text": "select them based on user preferences so thankfully race serve allows us to flexibly compose models",
    "start": "1246640",
    "end": "1252240"
  },
  {
    "text": "using whatever business logic that we want so the goal is to build a third ensemble model that's going to store",
    "start": "1252240",
    "end": "1258320"
  },
  {
    "text": "preferences in another ray actor the per user state on the diagram and then the model will",
    "start": "1258320",
    "end": "1264000"
  },
  {
    "text": "use those preferences to dynamically select between recommendations from our two models the movie cover model and",
    "start": "1264000",
    "end": "1270159"
  },
  {
    "text": "the plot-based model so this sounds pretty complicated but once again it's pretty easy to express",
    "start": "1270159",
    "end": "1275679"
  },
  {
    "text": "in python code here i have another racer script",
    "start": "1275679",
    "end": "1282400"
  },
  {
    "text": "so once again we're defining an ordinary python class this one's called the compose model so the first thing we need to do is use",
    "start": "1282400",
    "end": "1289280"
  },
  {
    "text": "a little bit more advanced of eraser feature and get a handle to the two underlying models",
    "start": "1289280",
    "end": "1295679"
  },
  {
    "text": "what this means is that we can forward requests to those two underlying models which are each",
    "start": "1295679",
    "end": "1301280"
  },
  {
    "text": "scalable services running in different processes on the cluster",
    "start": "1301280",
    "end": "1306158"
  },
  {
    "text": "so the first thing that we're going to do when a request comes in is we're going to forward the request to",
    "start": "1307200",
    "end": "1312240"
  },
  {
    "text": "those two models and get their recommendations then we'll get the",
    "start": "1312240",
    "end": "1317919"
  },
  {
    "text": "current model weights for this user from that user state actor we'll then use those weights to select",
    "start": "1317919",
    "end": "1324080"
  },
  {
    "text": "which results we want to send to the user and then finally we're going to update the model weights based on this click",
    "start": "1324080",
    "end": "1330640"
  },
  {
    "text": "and record these recommendations so that we can use them later so that should be all",
    "start": "1330640",
    "end": "1338080"
  },
  {
    "text": "that it takes now that we've defined our two individual models and this composed model that",
    "start": "1338080",
    "end": "1343120"
  },
  {
    "text": "dynamically selects between them we're ready to deploy in reality you would probably want to",
    "start": "1343120",
    "end": "1348320"
  },
  {
    "text": "hand this off to an ops team or do a canary deployment or something like that but for the sake of the demo let's go straight to production",
    "start": "1348320",
    "end": "1355600"
  },
  {
    "text": "to do that we can just use our any scale exec command once more but this time we're going to point it at the serve session which is running our",
    "start": "1355600",
    "end": "1361760"
  },
  {
    "text": "production serve cluster in aws so all we need to do is run the three",
    "start": "1361760",
    "end": "1366799"
  },
  {
    "text": "scripts that we just defined so we're going to apply the color model and then the plot model",
    "start": "1366799",
    "end": "1372640"
  },
  {
    "text": "and then finally the ensemble bottle",
    "start": "1372640",
    "end": "1376559"
  },
  {
    "text": "so if we run that it should take just a few seconds for those to deploy and we'll be ready to go",
    "start": "1378880",
    "end": "1384880"
  },
  {
    "text": "so it looks like it's just about finished so let's head back to our ui",
    "start": "1386799",
    "end": "1392480"
  },
  {
    "text": "so now if i click on a movie we can see that we're starting to get recommendations from our two models",
    "start": "1394000",
    "end": "1399360"
  },
  {
    "text": "so i clicked on a yellow movie here and you can see that the color based model as indicated by this this text at the bottom gave us",
    "start": "1399360",
    "end": "1406880"
  },
  {
    "text": "uh this yellow mickey mouse movie another yellow movie here and then some more with a similar color palette",
    "start": "1406880",
    "end": "1412880"
  },
  {
    "text": "we are also getting plot based recommendations so if i click on spiderman you can see we get",
    "start": "1412880",
    "end": "1417919"
  },
  {
    "text": "plot-based recommendations for superhero movies like captain america if i click on that we get even more of",
    "start": "1417919",
    "end": "1423679"
  },
  {
    "text": "those and we're also dynamically changing the distribution of how many recommendations from each model that we see",
    "start": "1423679",
    "end": "1430400"
  },
  {
    "text": "so i clicked on a few plot-based recommendations and we're starting to get more of those",
    "start": "1430400",
    "end": "1436159"
  },
  {
    "text": "we can also shift the distribution the other way by starting to click on a few color-based recommendations",
    "start": "1436159",
    "end": "1444240"
  },
  {
    "text": "okay so it looks like this is working and now we have this these personalized recommendations so let's take a look behind the scenes",
    "start": "1444240",
    "end": "1449840"
  },
  {
    "text": "and how this is working so here i'm in the u in the ui for our",
    "start": "1449840",
    "end": "1454880"
  },
  {
    "text": "serve session so while the dashboard that we were looking at earlier is really great for viewing the current state of the cluster",
    "start": "1454880",
    "end": "1461039"
  },
  {
    "text": "and debugging your applications there's also an out-of-the-box integration with grafana for production monitoring",
    "start": "1461039",
    "end": "1467520"
  },
  {
    "text": "so let's check that out so here we can see a mixture of system",
    "start": "1467520",
    "end": "1472720"
  },
  {
    "text": "level ray stats for example in this box on the bottom right we have the total number of resources available and",
    "start": "1472720",
    "end": "1479039"
  },
  {
    "text": "allocated as well as application specific metrics that are omitted using raise",
    "start": "1479039",
    "end": "1485760"
  },
  {
    "text": "application metrics api on the left so you can see that initially we all of the requests are going to our",
    "start": "1485760",
    "end": "1491520"
  },
  {
    "text": "random backend that was already deployed but once we deployed we started",
    "start": "1491520",
    "end": "1498000"
  },
  {
    "text": "having more and more recommendations going to the three models that we just developed and we did this with zero downtime in",
    "start": "1498000",
    "end": "1504880"
  },
  {
    "text": "addition we're also exporting a a model click-through rate to show how well our models are performing",
    "start": "1504880",
    "end": "1512240"
  },
  {
    "text": "and here you can see that the color based model that my manager wanted me to build for some reason is hovering at about 40 percent and the",
    "start": "1512240",
    "end": "1519039"
  },
  {
    "text": "plot based model is doing better at about 60 or so and by the way the reason you're seeing so much traffic",
    "start": "1519039",
    "end": "1524960"
  },
  {
    "text": "here is because we have some just some background synthetic traffic that's emulating users",
    "start": "1524960",
    "end": "1531840"
  },
  {
    "text": "okay so this looks like it's working but we can definitely improve the this plot-based model now that we're",
    "start": "1532320",
    "end": "1538720"
  },
  {
    "text": "getting more information about our users preferences so we want to do is deploy another",
    "start": "1538720",
    "end": "1545679"
  },
  {
    "text": "service on the raycluster which will be the periodic retrainer what this will do is read in data from the user's clicks",
    "start": "1545679",
    "end": "1553200"
  },
  {
    "text": "from the per user state actor and then it's going to retrain our logistic regression ranking model",
    "start": "1553200",
    "end": "1558960"
  },
  {
    "text": "and redeploy it to production race serve supports updating back ends",
    "start": "1558960",
    "end": "1564080"
  },
  {
    "text": "with zero downtime and in this case we'll also want to do incremental rollout before fully switching to the",
    "start": "1564080",
    "end": "1569200"
  },
  {
    "text": "new model so let's see how we can do that",
    "start": "1569200",
    "end": "1573600"
  },
  {
    "text": "back to my local editor you can see that we have this periodic retrainer class that has all the logic that we need",
    "start": "1575120",
    "end": "1583120"
  },
  {
    "text": "so the main loop that we need to do all of this online learning is shown here",
    "start": "1583120",
    "end": "1589520"
  },
  {
    "text": "so basically we're just going to periodically check if we should retrain the model and this is just going to check to see",
    "start": "1589520",
    "end": "1594720"
  },
  {
    "text": "if we have enough new data points if we should retrain the model then what we're going to do is fetch the new data",
    "start": "1594720",
    "end": "1601279"
  },
  {
    "text": "retrain that scikit-learn logistic regression model and then we're going to redeploy so",
    "start": "1601279",
    "end": "1607520"
  },
  {
    "text": "here what we're doing is using the ray serv set traffic api to make sure that 90 of our",
    "start": "1607520",
    "end": "1614080"
  },
  {
    "text": "traffic is sent to the old model and only 10 is sent to the new model of course once we see that the model is",
    "start": "1614080",
    "end": "1620559"
  },
  {
    "text": "working well we could change this and shift the distribution to send more traffic to our new model",
    "start": "1620559",
    "end": "1628080"
  },
  {
    "text": "so let's make the changes we need to deploy this as a long-lived service on our ray cluster once again we just need to import ray",
    "start": "1628080",
    "end": "1634320"
  },
  {
    "text": "and connect to the runtime and then we just add ray.remote to make this an actor definition",
    "start": "1634320",
    "end": "1641840"
  },
  {
    "text": "then we just need to instantiate this actor so here i'm going to use a few options",
    "start": "1642960",
    "end": "1648159"
  },
  {
    "text": "for my actor so we want this to be a detached actor to make sure that it will be a long live",
    "start": "1648159",
    "end": "1655200"
  },
  {
    "text": "service on the cluster then we're also going to give it a name so that we can refer to it later and then we just need to instantiate it",
    "start": "1655200",
    "end": "1662320"
  },
  {
    "text": "on the cluster and finally we're just going to run this main loop that has the online learning",
    "start": "1662320",
    "end": "1668320"
  },
  {
    "text": "logic so just those few lines are all we need to do to take this python class and make",
    "start": "1668320",
    "end": "1673679"
  },
  {
    "text": "it a long-lived service on our ray cluster we can deploy this to production in the same way that we did with the serve",
    "start": "1673679",
    "end": "1679760"
  },
  {
    "text": "scripts just by using any scale exec and running pythonretrain.com",
    "start": "1679760",
    "end": "1686000"
  },
  {
    "text": "so that should just take a second to deploy and in the meantime we can",
    "start": "1686960",
    "end": "1692159"
  },
  {
    "text": "head over to the dashboard for our serve cluster so here we can see we have a bunch of",
    "start": "1692159",
    "end": "1698240"
  },
  {
    "text": "services for our different models and we also have this retrainer actor now running on the cluster",
    "start": "1698240",
    "end": "1704480"
  },
  {
    "text": "we take a look at the logs for the retrainer we can see that it already got enough new samples to retrain the model",
    "start": "1704480",
    "end": "1710720"
  },
  {
    "text": "so it learned from 515 new samples it trained the new model and got an",
    "start": "1710720",
    "end": "1715760"
  },
  {
    "text": "accuracy of about 87 and then we deployed this new back end",
    "start": "1715760",
    "end": "1721360"
  },
  {
    "text": "as an a b test onto our cluster if we take a look back at grafana",
    "start": "1721360",
    "end": "1727360"
  },
  {
    "text": "we can see that while before we had quest going to our random back end in red and then our other three models",
    "start": "1727360",
    "end": "1734640"
  },
  {
    "text": "in orange now we have this teal line at the bottom that represents the traffic going to our new plot model",
    "start": "1734640",
    "end": "1742080"
  },
  {
    "text": "you can see that that's starting to ramp up and the traffic to the old plot model in orange is starting to go down a",
    "start": "1742080",
    "end": "1748000"
  },
  {
    "text": "little bit so we've successfully retrained and redeployed this model and it's now serving a portion of our production",
    "start": "1748000",
    "end": "1754000"
  },
  {
    "text": "traffic",
    "start": "1754000",
    "end": "1756320"
  },
  {
    "text": "so let's take a look back at what we built together so looking at our original diagram we've been able to",
    "start": "1759120",
    "end": "1764320"
  },
  {
    "text": "successfully build all of these pieces of an end-to-end recommendation system in just a few minutes i hope this demo",
    "start": "1764320",
    "end": "1770559"
  },
  {
    "text": "has given you a sense of the power of rey and any scale so to recap we wrote our application",
    "start": "1770559",
    "end": "1776559"
  },
  {
    "text": "from the comfort of our laptop using existing python libraries like hugging face transformers scikit-learn",
    "start": "1776559",
    "end": "1782640"
  },
  {
    "text": "and fa iss then we were able to run an image processing workload on hundreds of",
    "start": "1782640",
    "end": "1788559"
  },
  {
    "text": "cpus and fine-tune our deep learning bert model",
    "start": "1788559",
    "end": "1793600"
  },
  {
    "text": "on a cluster of gpus and we did this on the same session without ever worrying about machines",
    "start": "1793600",
    "end": "1800159"
  },
  {
    "text": "and finally we are able to seamlessly switch between building and testing our code locally",
    "start": "1800159",
    "end": "1805679"
  },
  {
    "text": "and then scaling it up on gcp and then deploying our resulting models on aws and we really didn't even have to",
    "start": "1805679",
    "end": "1812000"
  },
  {
    "text": "think about which cloud we were running on it was just one changed flag in the cli so all of this",
    "start": "1812000",
    "end": "1817760"
  },
  {
    "text": "was possible because any scale in ray allowed us to focus on our core business logic in python not configuring our",
    "start": "1817760",
    "end": "1823520"
  },
  {
    "text": "clusters or worrying about the infrastructure while here we were focused on building an ai application this is really just",
    "start": "1823520",
    "end": "1829919"
  },
  {
    "text": "one example of a general distributed application that we could build using ray and any scale i hope this has given you a",
    "start": "1829919",
    "end": "1836159"
  },
  {
    "text": "glimpse into our vision for any skill to be a true universal platform for distributed applications",
    "start": "1836159",
    "end": "1844159"
  },
  {
    "text": "so we're so excited to see what you're going to build with it and we invite you to sign up for the private beta on our website at anyscale.com",
    "start": "1844320",
    "end": "1850799"
  },
  {
    "text": "thanks so much for listening",
    "start": "1850799",
    "end": "1854880"
  }
]