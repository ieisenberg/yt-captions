[
  {
    "start": "0",
    "end": "91000"
  },
  {
    "text": "[Music]",
    "start": "50",
    "end": "7120"
  },
  {
    "text": "um we're kicking off today with an",
    "start": "7120",
    "end": "9120"
  },
  {
    "text": "opening keynote from sergey levine",
    "start": "9120",
    "end": "11040"
  },
  {
    "text": "someone who probably needs no",
    "start": "11040",
    "end": "13200"
  },
  {
    "text": "introduction in the rl community he's an",
    "start": "13200",
    "end": "15599"
  },
  {
    "text": "associate professor in the department of",
    "start": "15599",
    "end": "17520"
  },
  {
    "text": "electrical engineering and computer",
    "start": "17520",
    "end": "19119"
  },
  {
    "text": "sciences at berkeley where his research",
    "start": "19119",
    "end": "21520"
  },
  {
    "text": "focuses on machine learning for decision",
    "start": "21520",
    "end": "23439"
  },
  {
    "text": "making and control",
    "start": "23439",
    "end": "25199"
  },
  {
    "text": "with an emphasis on deep learning and",
    "start": "25199",
    "end": "26960"
  },
  {
    "text": "reinforcement learning algorithms and",
    "start": "26960",
    "end": "28720"
  },
  {
    "text": "he's interested in exploring how",
    "start": "28720",
    "end": "30800"
  },
  {
    "text": "learning can be used to acquire complex",
    "start": "30800",
    "end": "33680"
  },
  {
    "text": "behavioral skills in order to endow",
    "start": "33680",
    "end": "36000"
  },
  {
    "text": "machines with greater autonomy and",
    "start": "36000",
    "end": "37760"
  },
  {
    "text": "intelligence",
    "start": "37760",
    "end": "39280"
  },
  {
    "text": "he's published works in many of the top",
    "start": "39280",
    "end": "41600"
  },
  {
    "text": "academic journals in the ml and ai space",
    "start": "41600",
    "end": "44000"
  },
  {
    "text": "and he's also routinely featured in many",
    "start": "44000",
    "end": "46800"
  },
  {
    "text": "top mainstream publications including",
    "start": "46800",
    "end": "48719"
  },
  {
    "text": "the new york times the bbc the mit tech",
    "start": "48719",
    "end": "51280"
  },
  {
    "text": "review and bloomberg business",
    "start": "51280",
    "end": "53840"
  },
  {
    "text": "today he'll be talking about the",
    "start": "53840",
    "end": "55520"
  },
  {
    "text": "emerging field of offline rl which",
    "start": "55520",
    "end": "58160"
  },
  {
    "text": "allows one to leverage large previously",
    "start": "58160",
    "end": "60320"
  },
  {
    "text": "collected data sets to build better rl",
    "start": "60320",
    "end": "62960"
  },
  {
    "text": "agents",
    "start": "62960",
    "end": "64000"
  },
  {
    "text": "and he'll also be covering the technical",
    "start": "64000",
    "end": "66080"
  },
  {
    "text": "foundations of offline rl which is great",
    "start": "66080",
    "end": "68240"
  },
  {
    "text": "because i don't know much about it",
    "start": "68240",
    "end": "70159"
  },
  {
    "text": "and discussing recent algorithm advances",
    "start": "70159",
    "end": "72799"
  },
  {
    "text": "uh and present um uh he'll also be",
    "start": "72799",
    "end": "75920"
  },
  {
    "text": "presenting several applications um so",
    "start": "75920",
    "end": "78159"
  },
  {
    "text": "this sounds super exciting uh welcome",
    "start": "78159",
    "end": "80640"
  },
  {
    "text": "and uh we're excited to have you here",
    "start": "80640",
    "end": "83040"
  },
  {
    "text": "too",
    "start": "83040",
    "end": "85200"
  },
  {
    "text": "all right thank you for the introduction",
    "start": "85200",
    "end": "87360"
  },
  {
    "text": "um",
    "start": "87360",
    "end": "88400"
  },
  {
    "text": "all right i'll start with a kind of a",
    "start": "88400",
    "end": "90400"
  },
  {
    "text": "big big picture question",
    "start": "90400",
    "end": "92880"
  },
  {
    "start": "91000",
    "end": "91000"
  },
  {
    "text": "what makes modern machine learning work",
    "start": "92880",
    "end": "95360"
  },
  {
    "text": "at a very high level uh we could say",
    "start": "95360",
    "end": "97200"
  },
  {
    "text": "that a lot of what makes modern machine",
    "start": "97200",
    "end": "98880"
  },
  {
    "text": "learning work is the ability to utilize",
    "start": "98880",
    "end": "101439"
  },
  {
    "text": "very large data sets and combine them",
    "start": "101439",
    "end": "103680"
  },
  {
    "text": "with very large high-capacity models you",
    "start": "103680",
    "end": "105520"
  },
  {
    "text": "know transformer models big resonant",
    "start": "105520",
    "end": "107200"
  },
  {
    "text": "models that sort of thing",
    "start": "107200",
    "end": "108799"
  },
  {
    "text": "and you know there's a lot more of",
    "start": "108799",
    "end": "110640"
  },
  {
    "text": "course that goes into it a lot more uh",
    "start": "110640",
    "end": "112320"
  },
  {
    "text": "technical stuff that you have to know to",
    "start": "112320",
    "end": "113520"
  },
  {
    "text": "get this right but with these two",
    "start": "113520",
    "end": "115680"
  },
  {
    "text": "ingredients and a bit of care you can",
    "start": "115680",
    "end": "118000"
  },
  {
    "text": "get very good results on image",
    "start": "118000",
    "end": "119280"
  },
  {
    "text": "recognition translating text recognizing",
    "start": "119280",
    "end": "121920"
  },
  {
    "text": "speech all sorts of applications that",
    "start": "121920",
    "end": "123920"
  },
  {
    "text": "are basically kind of perceptual",
    "start": "123920",
    "end": "125360"
  },
  {
    "text": "recognition tasks",
    "start": "125360",
    "end": "128080"
  },
  {
    "start": "127000",
    "end": "127000"
  },
  {
    "text": "now a lot of these domains",
    "start": "128080",
    "end": "130399"
  },
  {
    "text": "involve us making some assumptions",
    "start": "130399",
    "end": "132879"
  },
  {
    "text": "that make them easy that make them",
    "start": "132879",
    "end": "134800"
  },
  {
    "text": "manageable with standard supervised",
    "start": "134800",
    "end": "136480"
  },
  {
    "text": "learning methods",
    "start": "136480",
    "end": "137920"
  },
  {
    "text": "for example we might assume that our",
    "start": "137920",
    "end": "139360"
  },
  {
    "text": "data points are independent of one",
    "start": "139360",
    "end": "140720"
  },
  {
    "text": "another",
    "start": "140720",
    "end": "141599"
  },
  {
    "text": "that the output for one input doesn't",
    "start": "141599",
    "end": "143440"
  },
  {
    "text": "influence another one",
    "start": "143440",
    "end": "146080"
  },
  {
    "text": "and that the ground truth labels are",
    "start": "146080",
    "end": "147760"
  },
  {
    "text": "provided to us at training time these",
    "start": "147760",
    "end": "149440"
  },
  {
    "text": "are kind of standard assumptions for",
    "start": "149440",
    "end": "151200"
  },
  {
    "text": "supervised learning methods",
    "start": "151200",
    "end": "153040"
  },
  {
    "text": "now many domains overtly define these",
    "start": "153040",
    "end": "155360"
  },
  {
    "text": "assumptions so if you're controlling an",
    "start": "155360",
    "end": "157280"
  },
  {
    "text": "autonomous vehicle of course your",
    "start": "157280",
    "end": "158800"
  },
  {
    "text": "outputs do influence future inputs if",
    "start": "158800",
    "end": "160800"
  },
  {
    "text": "you're controlling a robotic arm the",
    "start": "160800",
    "end": "163040"
  },
  {
    "text": "data points are not independent uh if",
    "start": "163040",
    "end": "165040"
  },
  {
    "text": "you're doing inventory management to",
    "start": "165040",
    "end": "166400"
  },
  {
    "text": "maximize profits you might not have",
    "start": "166400",
    "end": "168400"
  },
  {
    "text": "ground truth labels that tell you how to",
    "start": "168400",
    "end": "170080"
  },
  {
    "text": "adjust stock how to adjust inventory",
    "start": "170080",
    "end": "172000"
  },
  {
    "text": "levels you need to figure out",
    "start": "172000",
    "end": "174000"
  },
  {
    "text": "how to do it if you're trading stocks",
    "start": "174000",
    "end": "176160"
  },
  {
    "text": "again same deal so a lot of these",
    "start": "176160",
    "end": "178159"
  },
  {
    "text": "domains these are",
    "start": "178159",
    "end": "179519"
  },
  {
    "text": "control centric domains that don't fit",
    "start": "179519",
    "end": "181200"
  },
  {
    "text": "into these assumptions and that's we all",
    "start": "181200",
    "end": "183040"
  },
  {
    "text": "know that so decision-making problems",
    "start": "183040",
    "end": "185040"
  },
  {
    "text": "often don't satisfy these assumptions",
    "start": "185040",
    "end": "186640"
  },
  {
    "text": "because current actions influence future",
    "start": "186640",
    "end": "188560"
  },
  {
    "text": "observations the goal is to maximize",
    "start": "188560",
    "end": "191120"
  },
  {
    "text": "some utility metric like a reward",
    "start": "191120",
    "end": "192720"
  },
  {
    "text": "instead of just to match a ground truth",
    "start": "192720",
    "end": "194480"
  },
  {
    "text": "output and optimal actions are not",
    "start": "194480",
    "end": "196400"
  },
  {
    "text": "provided okay that's all fairly basic",
    "start": "196400",
    "end": "198640"
  },
  {
    "text": "stuff however",
    "start": "198640",
    "end": "201440"
  },
  {
    "text": "i would say these are not actually just",
    "start": "201440",
    "end": "203200"
  },
  {
    "text": "issues in control",
    "start": "203200",
    "end": "204959"
  },
  {
    "text": "in many cases i would say even in most",
    "start": "204959",
    "end": "206640"
  },
  {
    "text": "cases real world deployment of machine",
    "start": "206640",
    "end": "209599"
  },
  {
    "text": "learning systems",
    "start": "209599",
    "end": "210959"
  },
  {
    "text": "even the supervised learning systems",
    "start": "210959",
    "end": "212400"
  },
  {
    "text": "listed above has these kinds of feedback",
    "start": "212400",
    "end": "214720"
  },
  {
    "text": "issues",
    "start": "214720",
    "end": "215920"
  },
  {
    "text": "here's an example let's say that you're",
    "start": "215920",
    "end": "217760"
  },
  {
    "text": "building an app to",
    "start": "217760",
    "end": "220000"
  },
  {
    "text": "predict traffic let's say your google",
    "start": "220000",
    "end": "222080"
  },
  {
    "text": "maps or something of that sort",
    "start": "222080",
    "end": "224400"
  },
  {
    "text": "well you might think that that's a",
    "start": "224400",
    "end": "225680"
  },
  {
    "text": "supervised learning problem because you",
    "start": "225680",
    "end": "227280"
  },
  {
    "text": "get training data of traffic and you",
    "start": "227280",
    "end": "229040"
  },
  {
    "text": "learn to predict traffic based on",
    "start": "229040",
    "end": "230319"
  },
  {
    "text": "previous features",
    "start": "230319",
    "end": "231760"
  },
  {
    "text": "but then people are going to use your",
    "start": "231760",
    "end": "233040"
  },
  {
    "text": "app to make decisions and they're going",
    "start": "233040",
    "end": "234400"
  },
  {
    "text": "to choose where to drive or not drive",
    "start": "234400",
    "end": "236640"
  },
  {
    "text": "depending on whether you predict there's",
    "start": "236640",
    "end": "237840"
  },
  {
    "text": "traffic there so now there's a feedback",
    "start": "237840",
    "end": "239599"
  },
  {
    "text": "problem",
    "start": "239599",
    "end": "240799"
  },
  {
    "text": "your",
    "start": "240799",
    "end": "241840"
  },
  {
    "text": "outputs which you think of as",
    "start": "241840",
    "end": "243360"
  },
  {
    "text": "predictions but they're really actions",
    "start": "243360",
    "end": "245120"
  },
  {
    "text": "influence your future inputs because",
    "start": "245120",
    "end": "246319"
  },
  {
    "text": "they'll determine where google drive and",
    "start": "246319",
    "end": "248319"
  },
  {
    "text": "if your app doesn't take that into",
    "start": "248319",
    "end": "249519"
  },
  {
    "text": "account then it'll make mistakes",
    "start": "249519",
    "end": "251920"
  },
  {
    "text": "in fact i would say that most",
    "start": "251920",
    "end": "253120"
  },
  {
    "text": "deployments of machine learning systems",
    "start": "253120",
    "end": "254560"
  },
  {
    "text": "in the real world have these kinds of",
    "start": "254560",
    "end": "256239"
  },
  {
    "text": "feedback issues we just usually sweep",
    "start": "256239",
    "end": "258400"
  },
  {
    "text": "them under the rug if we're doing",
    "start": "258400",
    "end": "259519"
  },
  {
    "text": "supervised learning",
    "start": "259519",
    "end": "261680"
  },
  {
    "text": "so if ultimately machine learning is",
    "start": "261680",
    "end": "263280"
  },
  {
    "text": "always about making a decision of some",
    "start": "263280",
    "end": "264800"
  },
  {
    "text": "sort",
    "start": "264800",
    "end": "265919"
  },
  {
    "text": "why don't we treat every machine",
    "start": "265919",
    "end": "267280"
  },
  {
    "text": "learning problem like a reinforcement",
    "start": "267280",
    "end": "269040"
  },
  {
    "text": "learning problem",
    "start": "269040",
    "end": "271759"
  },
  {
    "start": "271000",
    "end": "271000"
  },
  {
    "text": "well the issue is that it's darn",
    "start": "272479",
    "end": "274560"
  },
  {
    "text": "inconvenient to do that",
    "start": "274560",
    "end": "276080"
  },
  {
    "text": "and that's because reinforcement",
    "start": "276080",
    "end": "277280"
  },
  {
    "text": "learning is really two different things",
    "start": "277280",
    "end": "279600"
  },
  {
    "text": "first it's a framework for learning",
    "start": "279600",
    "end": "281120"
  },
  {
    "text": "based decision making",
    "start": "281120",
    "end": "282800"
  },
  {
    "text": "it takes us from a paradigm where we",
    "start": "282800",
    "end": "284880"
  },
  {
    "text": "think about models that map inputs",
    "start": "284880",
    "end": "287520"
  },
  {
    "text": "x to labels y",
    "start": "287520",
    "end": "289600"
  },
  {
    "text": "into one where we think about models",
    "start": "289600",
    "end": "291040"
  },
  {
    "text": "that map",
    "start": "291040",
    "end": "292000"
  },
  {
    "text": "states or observations to actions to",
    "start": "292000",
    "end": "294240"
  },
  {
    "text": "decisions and those decisions that have",
    "start": "294240",
    "end": "296160"
  },
  {
    "text": "consequences they affect future states",
    "start": "296160",
    "end": "298560"
  },
  {
    "text": "and we have to reason about long-term",
    "start": "298560",
    "end": "300240"
  },
  {
    "text": "utility maximization",
    "start": "300240",
    "end": "302880"
  },
  {
    "text": "and as i",
    "start": "302880",
    "end": "304160"
  },
  {
    "text": "try to argue on the previous slide",
    "start": "304160",
    "end": "306479"
  },
  {
    "text": "most deployments of ml systems actually",
    "start": "306479",
    "end": "309360"
  },
  {
    "text": "look like this",
    "start": "309360",
    "end": "311120"
  },
  {
    "text": "but reinforcement learning is also",
    "start": "311120",
    "end": "312400"
  },
  {
    "text": "something else it's a framework for",
    "start": "312400",
    "end": "314160"
  },
  {
    "text": "active online learning for control",
    "start": "314160",
    "end": "318240"
  },
  {
    "text": "so this is the classic picture from the",
    "start": "318240",
    "end": "319919"
  },
  {
    "text": "um the sutton embardo textbook",
    "start": "319919",
    "end": "322160"
  },
  {
    "text": "that describes reinforcement as an",
    "start": "322160",
    "end": "324000"
  },
  {
    "text": "active online process where an agent",
    "start": "324000",
    "end": "325680"
  },
  {
    "text": "interacts with the environment collects",
    "start": "325680",
    "end": "327520"
  },
  {
    "text": "experience updates its behavior and so",
    "start": "327520",
    "end": "329440"
  },
  {
    "text": "on um you can think of this visually",
    "start": "329440",
    "end": "331360"
  },
  {
    "text": "like this so you've got your little",
    "start": "331360",
    "end": "333120"
  },
  {
    "text": "agent your robot it interacts with the",
    "start": "333120",
    "end": "334800"
  },
  {
    "text": "world gets a little bit of data use that",
    "start": "334800",
    "end": "337120"
  },
  {
    "text": "little bit of data to update its policy",
    "start": "337120",
    "end": "339280"
  },
  {
    "text": "or value function and then interact with",
    "start": "339280",
    "end": "341280"
  },
  {
    "text": "the world some more throws out that data",
    "start": "341280",
    "end": "343039"
  },
  {
    "text": "get some more data and repeats this many",
    "start": "343039",
    "end": "344400"
  },
  {
    "text": "many times",
    "start": "344400",
    "end": "345680"
  },
  {
    "text": "and that's very appealing to a lot of",
    "start": "345680",
    "end": "347199"
  },
  {
    "text": "people because it kind of looks a little",
    "start": "347199",
    "end": "349280"
  },
  {
    "text": "bit like the way you might imagine that",
    "start": "349280",
    "end": "350560"
  },
  {
    "text": "a person or an animal might learn",
    "start": "350560",
    "end": "353280"
  },
  {
    "text": "but it can be very difficult to deal",
    "start": "353280",
    "end": "355199"
  },
  {
    "text": "with in complex high-stakes environments",
    "start": "355199",
    "end": "357440"
  },
  {
    "text": "like inventory management or stock",
    "start": "357440",
    "end": "358960"
  },
  {
    "text": "trading",
    "start": "358960",
    "end": "360319"
  },
  {
    "text": "so",
    "start": "360319",
    "end": "361039"
  },
  {
    "text": "number one the framework for",
    "start": "361039",
    "end": "362160"
  },
  {
    "text": "learning-based decision making",
    "start": "362160",
    "end": "364319"
  },
  {
    "text": "almost all world learning problems look",
    "start": "364319",
    "end": "366080"
  },
  {
    "text": "like this so we would really like to",
    "start": "366080",
    "end": "367759"
  },
  {
    "text": "have something that can handle uh the",
    "start": "367759",
    "end": "369919"
  },
  {
    "text": "proper the the challenges of decision",
    "start": "369919",
    "end": "371759"
  },
  {
    "text": "making",
    "start": "371759",
    "end": "373039"
  },
  {
    "text": "but for number two",
    "start": "373039",
    "end": "374720"
  },
  {
    "text": "almost all real-world learning problems",
    "start": "374720",
    "end": "376800"
  },
  {
    "text": "actually make it very difficult to do",
    "start": "376800",
    "end": "378240"
  },
  {
    "text": "this they make it very difficult to do",
    "start": "378240",
    "end": "379600"
  },
  {
    "text": "active online learning when for example",
    "start": "379600",
    "end": "382160"
  },
  {
    "text": "deploying an untrained policy to do",
    "start": "382160",
    "end": "383680"
  },
  {
    "text": "inventory management for your",
    "start": "383680",
    "end": "385360"
  },
  {
    "text": "multinational corporation can result in",
    "start": "385360",
    "end": "387440"
  },
  {
    "text": "you losing billions of dollars in just",
    "start": "387440",
    "end": "389120"
  },
  {
    "text": "the first hour so you don't want to do",
    "start": "389120",
    "end": "390880"
  },
  {
    "text": "active online learning for autonomous",
    "start": "390880",
    "end": "392400"
  },
  {
    "text": "driving you don't want to do it for",
    "start": "392400",
    "end": "393440"
  },
  {
    "text": "stock trading you don't want to do it",
    "start": "393440",
    "end": "394880"
  },
  {
    "text": "for inventory management at least not",
    "start": "394880",
    "end": "396080"
  },
  {
    "text": "from scratch",
    "start": "396080",
    "end": "397759"
  },
  {
    "text": "so",
    "start": "397759",
    "end": "398800"
  },
  {
    "text": "basically what this talk is about is how",
    "start": "398800",
    "end": "400160"
  },
  {
    "text": "you can get one without having to do two",
    "start": "400160",
    "end": "404639"
  },
  {
    "start": "404000",
    "end": "404000"
  },
  {
    "text": "okay now as an aside here some of you",
    "start": "404960",
    "end": "406560"
  },
  {
    "text": "might be wondering okay maybe i'm making",
    "start": "406560",
    "end": "408000"
  },
  {
    "text": "too big of a deal out of this right like",
    "start": "408000",
    "end": "409520"
  },
  {
    "text": "doesn't rl kind of work like",
    "start": "409520",
    "end": "411599"
  },
  {
    "text": "aren't there all these cool results that",
    "start": "411599",
    "end": "413039"
  },
  {
    "text": "use rl",
    "start": "413039",
    "end": "414560"
  },
  {
    "text": "well let's look at some of these things",
    "start": "414560",
    "end": "416080"
  },
  {
    "text": "so here i have some pictures on the left",
    "start": "416080",
    "end": "418240"
  },
  {
    "text": "um",
    "start": "418240",
    "end": "419120"
  },
  {
    "text": "some of these are things you might have",
    "start": "419120",
    "end": "420400"
  },
  {
    "text": "heard about some of them less so",
    "start": "420400",
    "end": "422000"
  },
  {
    "text": "depending on your application area but",
    "start": "422000",
    "end": "424000"
  },
  {
    "text": "you know we've seen rl work really well",
    "start": "424000",
    "end": "425759"
  },
  {
    "text": "in domains from playing video games to",
    "start": "425759",
    "end": "427840"
  },
  {
    "text": "controlling robotic systems to beating",
    "start": "427840",
    "end": "429840"
  },
  {
    "text": "the world champion and go",
    "start": "429840",
    "end": "431840"
  },
  {
    "text": "and that's very impressive",
    "start": "431840",
    "end": "434160"
  },
  {
    "text": "on the other side though supervised",
    "start": "434160",
    "end": "436240"
  },
  {
    "text": "learning systems have been very",
    "start": "436240",
    "end": "437840"
  },
  {
    "text": "successful in",
    "start": "437840",
    "end": "439759"
  },
  {
    "text": "settings that we would characterize as",
    "start": "439759",
    "end": "441440"
  },
  {
    "text": "open world settings",
    "start": "441440",
    "end": "443919"
  },
  {
    "text": "recognizing the class of any image on",
    "start": "443919",
    "end": "446560"
  },
  {
    "text": "the internet translating any text",
    "start": "446560",
    "end": "448639"
  },
  {
    "text": "recognizing any speech",
    "start": "448639",
    "end": "450479"
  },
  {
    "text": "and there's a big gap between the stuff",
    "start": "450479",
    "end": "452240"
  },
  {
    "text": "on the left and the stuff on the right",
    "start": "452240",
    "end": "454639"
  },
  {
    "text": "the gap is not so much a gap in terms of",
    "start": "454639",
    "end": "456240"
  },
  {
    "text": "task difficulty it's a gap of task",
    "start": "456240",
    "end": "458639"
  },
  {
    "text": "diversity",
    "start": "458639",
    "end": "460000"
  },
  {
    "text": "it's the difference between closed world",
    "start": "460000",
    "end": "461360"
  },
  {
    "text": "and open world environments the bot",
    "start": "461360",
    "end": "463280"
  },
  {
    "text": "that's playing go needs to be very good",
    "start": "463280",
    "end": "465360"
  },
  {
    "text": "at playing go but it doesn't need to",
    "start": "465360",
    "end": "466479"
  },
  {
    "text": "worry about someone spilling coffee on",
    "start": "466479",
    "end": "468319"
  },
  {
    "text": "the go board and having to figure out",
    "start": "468319",
    "end": "469759"
  },
  {
    "text": "how to deal with that unpredictable",
    "start": "469759",
    "end": "470879"
  },
  {
    "text": "situation",
    "start": "470879",
    "end": "472160"
  },
  {
    "text": "in open world environments these kinds",
    "start": "472160",
    "end": "474080"
  },
  {
    "text": "of methods are much harder to use and",
    "start": "474080",
    "end": "476319"
  },
  {
    "text": "it's pretty clear to see why if you",
    "start": "476319",
    "end": "477759"
  },
  {
    "text": "imagine that you need an imagenet scale",
    "start": "477759",
    "end": "479840"
  },
  {
    "text": "data set to learn to generalize to your",
    "start": "479840",
    "end": "481759"
  },
  {
    "text": "open world environment and you have to",
    "start": "481759",
    "end": "483919"
  },
  {
    "text": "recollect the data each time you update",
    "start": "483919",
    "end": "485759"
  },
  {
    "text": "your model then we're really talking",
    "start": "485759",
    "end": "487199"
  },
  {
    "text": "about recollecting an imagenet scale",
    "start": "487199",
    "end": "488960"
  },
  {
    "text": "data set each time you update your model",
    "start": "488960",
    "end": "491440"
  },
  {
    "text": "and that's of course never going to work",
    "start": "491440",
    "end": "494800"
  },
  {
    "start": "496000",
    "end": "496000"
  },
  {
    "text": "all right",
    "start": "497120",
    "end": "498479"
  },
  {
    "text": "so how can we make rl look more like",
    "start": "498479",
    "end": "501520"
  },
  {
    "text": "supervised learning",
    "start": "501520",
    "end": "502960"
  },
  {
    "text": "in the sense that we still wanted to be",
    "start": "502960",
    "end": "504479"
  },
  {
    "text": "able to handle all the requirements of a",
    "start": "504479",
    "end": "506560"
  },
  {
    "text": "decision-making system but we don't want",
    "start": "506560",
    "end": "507919"
  },
  {
    "text": "to require this online active data",
    "start": "507919",
    "end": "509599"
  },
  {
    "text": "collection we want to be able to do rl",
    "start": "509599",
    "end": "512080"
  },
  {
    "text": "off of",
    "start": "512080",
    "end": "513039"
  },
  {
    "text": "previously collected data sets just like",
    "start": "513039",
    "end": "514880"
  },
  {
    "text": "we do supervised learning using you know",
    "start": "514880",
    "end": "516640"
  },
  {
    "text": "imagenet or large textual datasets",
    "start": "516640",
    "end": "521240"
  },
  {
    "text": "all right so",
    "start": "521440",
    "end": "523200"
  },
  {
    "text": "these are the two the two things from",
    "start": "523200",
    "end": "524640"
  },
  {
    "text": "before i just swapped their order so now",
    "start": "524640",
    "end": "526399"
  },
  {
    "text": "the online part is on the left",
    "start": "526399",
    "end": "529040"
  },
  {
    "text": "and classically we think of rl as",
    "start": "529040",
    "end": "531120"
  },
  {
    "text": "onpolicyrl so you collect some data",
    "start": "531120",
    "end": "534000"
  },
  {
    "text": "update your your policy and then repeat",
    "start": "534000",
    "end": "536640"
  },
  {
    "text": "and what we're going to try to do is",
    "start": "536640",
    "end": "538480"
  },
  {
    "text": "offline rl so offline rl basically uh",
    "start": "538480",
    "end": "541920"
  },
  {
    "text": "deals with a setting where some other",
    "start": "541920",
    "end": "543920"
  },
  {
    "text": "policy that you have no control over pi",
    "start": "543920",
    "end": "545760"
  },
  {
    "text": "beta has collected data before and that",
    "start": "545760",
    "end": "548160"
  },
  {
    "text": "data goes into a buffer that we're going",
    "start": "548160",
    "end": "549680"
  },
  {
    "text": "to denote d",
    "start": "549680",
    "end": "551200"
  },
  {
    "text": "and that buffer is going to be used to",
    "start": "551200",
    "end": "553120"
  },
  {
    "text": "learn the best policy you can get",
    "start": "553120",
    "end": "555120"
  },
  {
    "text": "and that policy has to be learned using",
    "start": "555120",
    "end": "556800"
  },
  {
    "text": "only d without any additional",
    "start": "556800",
    "end": "558080"
  },
  {
    "text": "interaction with the environment and",
    "start": "558080",
    "end": "559600"
  },
  {
    "text": "once you've learned it you deployed in",
    "start": "559600",
    "end": "560880"
  },
  {
    "text": "the in the environment of course in",
    "start": "560880",
    "end": "562560"
  },
  {
    "text": "reality you might go and fine tune it",
    "start": "562560",
    "end": "564000"
  },
  {
    "text": "further",
    "start": "564000",
    "end": "564880"
  },
  {
    "text": "but the main focus of today's talk is",
    "start": "564880",
    "end": "566320"
  },
  {
    "text": "how to get that fully offline stage",
    "start": "566320",
    "end": "568080"
  },
  {
    "text": "that's usually the hard part once you",
    "start": "568080",
    "end": "569519"
  },
  {
    "text": "can train a decent policy off the data",
    "start": "569519",
    "end": "571680"
  },
  {
    "text": "set d then you can deploy it and fine",
    "start": "571680",
    "end": "573839"
  },
  {
    "text": "tune it just like with any other rl",
    "start": "573839",
    "end": "575360"
  },
  {
    "text": "method and that's not particularly",
    "start": "575360",
    "end": "576720"
  },
  {
    "text": "difficult",
    "start": "576720",
    "end": "578560"
  },
  {
    "start": "578000",
    "end": "578000"
  },
  {
    "text": "all right so the offline workflow then",
    "start": "578560",
    "end": "580959"
  },
  {
    "text": "would look something like this step one",
    "start": "580959",
    "end": "582880"
  },
  {
    "text": "would be to collect a data set using any",
    "start": "582880",
    "end": "584800"
  },
  {
    "text": "policy or mixture of policies so that's",
    "start": "584800",
    "end": "587040"
  },
  {
    "text": "pi beta and typically the algorithm does",
    "start": "587040",
    "end": "589680"
  },
  {
    "text": "not assume that has control over what pi",
    "start": "589680",
    "end": "591279"
  },
  {
    "text": "beta is pi beta is basically the source",
    "start": "591279",
    "end": "593120"
  },
  {
    "text": "of data and it could be anything it",
    "start": "593120",
    "end": "595120"
  },
  {
    "text": "could be humans performing the task it",
    "start": "595120",
    "end": "597440"
  },
  {
    "text": "could be an existing system performing",
    "start": "597440",
    "end": "599120"
  },
  {
    "text": "the task it could be random behaviors or",
    "start": "599120",
    "end": "601760"
  },
  {
    "text": "it could be any combination of the above",
    "start": "601760",
    "end": "604399"
  },
  {
    "text": "so it's okay to just mix things whatever",
    "start": "604399",
    "end": "607120"
  },
  {
    "text": "process you can get to collect data",
    "start": "607120",
    "end": "609120"
  },
  {
    "text": "that's what you've got",
    "start": "609120",
    "end": "610560"
  },
  {
    "text": "and this is only done once so in the",
    "start": "610560",
    "end": "612399"
  },
  {
    "text": "same way that like the image and data",
    "start": "612399",
    "end": "613760"
  },
  {
    "text": "set was only collected once and then",
    "start": "613760",
    "end": "615760"
  },
  {
    "text": "it's used by everybody step one should",
    "start": "615760",
    "end": "618079"
  },
  {
    "text": "be done once you can go out and collect",
    "start": "618079",
    "end": "619920"
  },
  {
    "text": "more data if you want more data later",
    "start": "619920",
    "end": "621200"
  },
  {
    "text": "but you just append it to the data you",
    "start": "621200",
    "end": "622560"
  },
  {
    "text": "already have",
    "start": "622560",
    "end": "624160"
  },
  {
    "text": "step two is run offline rl on this data",
    "start": "624160",
    "end": "627279"
  },
  {
    "text": "set to learn a policy and you can think",
    "start": "627279",
    "end": "629920"
  },
  {
    "text": "of this as sort of squeezing out the",
    "start": "629920",
    "end": "631279"
  },
  {
    "text": "best policy you can get out of the data",
    "start": "631279",
    "end": "634000"
  },
  {
    "text": "step three is to then deploy this policy",
    "start": "634000",
    "end": "636240"
  },
  {
    "text": "in the real world and if you're not",
    "start": "636240",
    "end": "637920"
  },
  {
    "text": "happy with how well it does you can",
    "start": "637920",
    "end": "639839"
  },
  {
    "text": "modify your algorithm or adjust its",
    "start": "639839",
    "end": "641440"
  },
  {
    "text": "hyper parameters and go back to step two",
    "start": "641440",
    "end": "644000"
  },
  {
    "text": "reusing the same data",
    "start": "644000",
    "end": "647200"
  },
  {
    "text": "so this kind of paradigm can then be",
    "start": "647200",
    "end": "649279"
  },
  {
    "text": "used for settings where online active",
    "start": "649279",
    "end": "651040"
  },
  {
    "text": "interaction is impractical for example",
    "start": "651040",
    "end": "653279"
  },
  {
    "text": "where",
    "start": "653279",
    "end": "654560"
  },
  {
    "text": "the stakes are really high where",
    "start": "654560",
    "end": "656720"
  },
  {
    "text": "untrained policy should not be allowed",
    "start": "656720",
    "end": "658399"
  },
  {
    "text": "to explore willy-nilly",
    "start": "658399",
    "end": "660560"
  },
  {
    "text": "or",
    "start": "660560",
    "end": "661440"
  },
  {
    "text": "settings where it's just very costly to",
    "start": "661440",
    "end": "663680"
  },
  {
    "text": "do that online interaction things like",
    "start": "663680",
    "end": "665360"
  },
  {
    "text": "inventory management writing drug",
    "start": "665360",
    "end": "667360"
  },
  {
    "text": "prescriptions scheduling the course of",
    "start": "667360",
    "end": "669120"
  },
  {
    "text": "scientific experiments managing power",
    "start": "669120",
    "end": "671120"
  },
  {
    "text": "grids or controlling robots",
    "start": "671120",
    "end": "674720"
  },
  {
    "start": "674000",
    "end": "674000"
  },
  {
    "text": "all right so why is offline",
    "start": "674720",
    "end": "676560"
  },
  {
    "text": "reinforcement learning difficult like",
    "start": "676560",
    "end": "678000"
  },
  {
    "text": "why can't we just use regular",
    "start": "678000",
    "end": "679680"
  },
  {
    "text": "off-the-shelf rl methods that we already",
    "start": "679680",
    "end": "681839"
  },
  {
    "text": "know work in the online regime",
    "start": "681839",
    "end": "684720"
  },
  {
    "text": "well",
    "start": "684720",
    "end": "685440"
  },
  {
    "text": "i'm going to talk about much more",
    "start": "685440",
    "end": "686480"
  },
  {
    "text": "technical reasons later in the talk but",
    "start": "686480",
    "end": "688240"
  },
  {
    "text": "intuitively the fundamental problem is",
    "start": "688240",
    "end": "690320"
  },
  {
    "text": "that offline rl requires resolving",
    "start": "690320",
    "end": "692560"
  },
  {
    "text": "counterfactual queries",
    "start": "692560",
    "end": "694480"
  },
  {
    "text": "so let's imagine that we're trying to do",
    "start": "694480",
    "end": "696160"
  },
  {
    "text": "offline rl for autonomous driving and we",
    "start": "696160",
    "end": "698800"
  },
  {
    "text": "have trading data where humans drive",
    "start": "698800",
    "end": "701440"
  },
  {
    "text": "cars on roads and humans are not perfect",
    "start": "701440",
    "end": "703519"
  },
  {
    "text": "drivers but they're pretty good so maybe",
    "start": "703519",
    "end": "705519"
  },
  {
    "text": "humans are not optimal they're not going",
    "start": "705519",
    "end": "706959"
  },
  {
    "text": "to change lanes in the most efficient",
    "start": "706959",
    "end": "709120"
  },
  {
    "text": "and safest way",
    "start": "709120",
    "end": "710639"
  },
  {
    "text": "but they're not going to do silly things",
    "start": "710639",
    "end": "711920"
  },
  {
    "text": "like just drive off the road so",
    "start": "711920",
    "end": "713360"
  },
  {
    "text": "generally if they're on a straightaway",
    "start": "713360",
    "end": "716079"
  },
  {
    "text": "they're going to stay straight",
    "start": "716079",
    "end": "719040"
  },
  {
    "text": "during offline reinforcement learning",
    "start": "719519",
    "end": "721360"
  },
  {
    "text": "the policy needs to figure out",
    "start": "721360",
    "end": "724240"
  },
  {
    "text": "you've never seen somebody just",
    "start": "724240",
    "end": "726079"
  },
  {
    "text": "turn left and swerve off the road before",
    "start": "726079",
    "end": "728240"
  },
  {
    "text": "is that a good idea or not",
    "start": "728240",
    "end": "731440"
  },
  {
    "text": "that kind of seems like a silly question",
    "start": "731440",
    "end": "733040"
  },
  {
    "text": "it's like of course it's a bad idea but",
    "start": "733040",
    "end": "734560"
  },
  {
    "text": "the policy has never seen it like you",
    "start": "734560",
    "end": "736160"
  },
  {
    "text": "know it's a bad idea to swerve off the",
    "start": "736160",
    "end": "737440"
  },
  {
    "text": "road because someone told you like yeah",
    "start": "737440",
    "end": "738639"
  },
  {
    "text": "if you crash your car like bad stuff",
    "start": "738639",
    "end": "740000"
  },
  {
    "text": "happens",
    "start": "740000",
    "end": "741040"
  },
  {
    "text": "no one told the policy this all it ever",
    "start": "741040",
    "end": "742880"
  },
  {
    "text": "saw is humans driving cars",
    "start": "742880",
    "end": "744800"
  },
  {
    "text": "in normal reasonable ways",
    "start": "744800",
    "end": "746959"
  },
  {
    "text": "so how do we know if this is good or bad",
    "start": "746959",
    "end": "748639"
  },
  {
    "text": "if we didn't see it in the data",
    "start": "748639",
    "end": "750959"
  },
  {
    "text": "online rl algorithms don't have to",
    "start": "750959",
    "end": "752399"
  },
  {
    "text": "handle this because they can simply try",
    "start": "752399",
    "end": "754240"
  },
  {
    "text": "this action and see what happens so an",
    "start": "754240",
    "end": "756320"
  },
  {
    "text": "online our algorithm will actually crash",
    "start": "756320",
    "end": "757920"
  },
  {
    "text": "the car and from that it will learn that",
    "start": "757920",
    "end": "759680"
  },
  {
    "text": "that's a bad idea",
    "start": "759680",
    "end": "761600"
  },
  {
    "text": "offline rl methods have to somehow",
    "start": "761600",
    "end": "763040"
  },
  {
    "text": "account for these unseen or out of",
    "start": "763040",
    "end": "765120"
  },
  {
    "text": "distribution actions ideally in a safe",
    "start": "765120",
    "end": "767680"
  },
  {
    "text": "way",
    "start": "767680",
    "end": "769680"
  },
  {
    "text": "but at the same time they still have to",
    "start": "769680",
    "end": "770959"
  },
  {
    "text": "make use of generalization they still",
    "start": "770959",
    "end": "772320"
  },
  {
    "text": "have to come up with behaviors that are",
    "start": "772320",
    "end": "774079"
  },
  {
    "text": "better than the",
    "start": "774079",
    "end": "777040"
  },
  {
    "text": "same",
    "start": "779440",
    "end": "782440"
  },
  {
    "text": "so you have to allow them to generalize",
    "start": "786800",
    "end": "788320"
  },
  {
    "text": "but the same things might happen so",
    "start": "788320",
    "end": "789839"
  },
  {
    "text": "that's kind of a delicate trade-off",
    "start": "789839",
    "end": "792560"
  },
  {
    "start": "791000",
    "end": "791000"
  },
  {
    "text": "all right so",
    "start": "792560",
    "end": "794079"
  },
  {
    "text": "what do we expect offline rl methods to",
    "start": "794079",
    "end": "796000"
  },
  {
    "text": "do like if we can't allow them to go out",
    "start": "796000",
    "end": "797680"
  },
  {
    "text": "of distribution um",
    "start": "797680",
    "end": "799519"
  },
  {
    "text": "how do they",
    "start": "799519",
    "end": "800720"
  },
  {
    "text": "how are they going to help",
    "start": "800720",
    "end": "802880"
  },
  {
    "text": "well",
    "start": "802880",
    "end": "803680"
  },
  {
    "text": "so one bit of potentially bad intuition",
    "start": "803680",
    "end": "806560"
  },
  {
    "text": "is",
    "start": "806560",
    "end": "807519"
  },
  {
    "text": "that",
    "start": "807519",
    "end": "808800"
  },
  {
    "text": "it's like imitation learning okay so",
    "start": "808800",
    "end": "811440"
  },
  {
    "text": "you've got your data and what you're",
    "start": "811440",
    "end": "813360"
  },
  {
    "text": "going to do is you're going to try to",
    "start": "813360",
    "end": "814880"
  },
  {
    "text": "copy that data to get a policy pi theta",
    "start": "814880",
    "end": "817519"
  },
  {
    "text": "that",
    "start": "817519",
    "end": "818240"
  },
  {
    "text": "does just what the data did",
    "start": "818240",
    "end": "821360"
  },
  {
    "text": "i think that's bad intuition because",
    "start": "822399",
    "end": "823600"
  },
  {
    "text": "offline rl can do a lot more than that",
    "start": "823600",
    "end": "826160"
  },
  {
    "text": "that said it can actually be proven that",
    "start": "826160",
    "end": "828560"
  },
  {
    "text": "under some circumstances even in this",
    "start": "828560",
    "end": "830320"
  },
  {
    "text": "setting offline url is actually better",
    "start": "830320",
    "end": "831680"
  },
  {
    "text": "than imitation and we have a paper on",
    "start": "831680",
    "end": "833040"
  },
  {
    "text": "that called should i run offline rl or",
    "start": "833040",
    "end": "834880"
  },
  {
    "text": "behavior cloning uh that's in iclear2022",
    "start": "834880",
    "end": "837839"
  },
  {
    "text": "but",
    "start": "837839",
    "end": "838560"
  },
  {
    "text": "i think the much better intuition the",
    "start": "838560",
    "end": "840480"
  },
  {
    "text": "kind of setting where we really expect",
    "start": "840480",
    "end": "842079"
  },
  {
    "text": "to see a lot of benefit from offline rl",
    "start": "842079",
    "end": "843920"
  },
  {
    "text": "is a setting where you have highly",
    "start": "843920",
    "end": "845199"
  },
  {
    "text": "sub-optimal data and you can kind of",
    "start": "845199",
    "end": "847040"
  },
  {
    "text": "think of the policy as pulling out the",
    "start": "847040",
    "end": "849120"
  },
  {
    "text": "best thing",
    "start": "849120",
    "end": "850720"
  },
  {
    "text": "that you can do by combining parts of",
    "start": "850720",
    "end": "852720"
  },
  {
    "text": "the data",
    "start": "852720",
    "end": "854880"
  },
  {
    "text": "so",
    "start": "854880",
    "end": "855680"
  },
  {
    "text": "one kind of very simple intuitive",
    "start": "855680",
    "end": "857920"
  },
  {
    "text": "example of this is this stitching",
    "start": "857920",
    "end": "859920"
  },
  {
    "text": "example if you've seen data of going",
    "start": "859920",
    "end": "862160"
  },
  {
    "text": "from a to b and you've seen data from of",
    "start": "862160",
    "end": "864160"
  },
  {
    "text": "going from b to c you should be able to",
    "start": "864160",
    "end": "866000"
  },
  {
    "text": "figure out that you can go from a to c",
    "start": "866000",
    "end": "867440"
  },
  {
    "text": "so that's an inference you can make",
    "start": "867440",
    "end": "868480"
  },
  {
    "text": "using the data and offline nrl methods",
    "start": "868480",
    "end": "869920"
  },
  {
    "text": "should be able to do that",
    "start": "869920",
    "end": "871279"
  },
  {
    "text": "but it actually goes further than that",
    "start": "871279",
    "end": "872560"
  },
  {
    "text": "that's simply the clearest example you",
    "start": "872560",
    "end": "874639"
  },
  {
    "text": "can imagine a setting where this",
    "start": "874639",
    "end": "876560"
  },
  {
    "text": "stitching behavior happens all the time",
    "start": "876560",
    "end": "878639"
  },
  {
    "text": "at the kind of the micro level so",
    "start": "878639",
    "end": "880959"
  },
  {
    "text": "imagine for instance that you have data",
    "start": "880959",
    "end": "882399"
  },
  {
    "text": "from humans trading stocks and each of",
    "start": "882399",
    "end": "884720"
  },
  {
    "text": "the stock traders is good in different",
    "start": "884720",
    "end": "886639"
  },
  {
    "text": "circumstances and bad in other",
    "start": "886639",
    "end": "887920"
  },
  {
    "text": "circumstances so none of them are",
    "start": "887920",
    "end": "889040"
  },
  {
    "text": "perfect none of them are good everywhere",
    "start": "889040",
    "end": "891040"
  },
  {
    "text": "well you could imagine an offline rl",
    "start": "891040",
    "end": "892800"
  },
  {
    "text": "method that sort of figures out how to",
    "start": "892800",
    "end": "894000"
  },
  {
    "text": "be the best of the best human in each",
    "start": "894000",
    "end": "896959"
  },
  {
    "text": "situation",
    "start": "896959",
    "end": "898720"
  },
  {
    "text": "but overall that would be a lot better",
    "start": "898720",
    "end": "900079"
  },
  {
    "text": "than any one human could be by",
    "start": "900079",
    "end": "901360"
  },
  {
    "text": "themselves so if you sort of combine the",
    "start": "901360",
    "end": "903600"
  },
  {
    "text": "best parts of the data everywhere at",
    "start": "903600",
    "end": "905199"
  },
  {
    "text": "this micro scale you can actually get a",
    "start": "905199",
    "end": "907199"
  },
  {
    "text": "behavior that is a lot better",
    "start": "907199",
    "end": "909279"
  },
  {
    "text": "than",
    "start": "909279",
    "end": "910079"
  },
  {
    "text": "for example",
    "start": "910079",
    "end": "911279"
  },
  {
    "text": "the um",
    "start": "911279",
    "end": "912560"
  },
  {
    "text": "the best person that was responsible for",
    "start": "912560",
    "end": "914800"
  },
  {
    "text": "collecting the data",
    "start": "914800",
    "end": "917199"
  },
  {
    "text": "so if we have algorithms that properly",
    "start": "917199",
    "end": "918720"
  },
  {
    "text": "perform dynamic programming which i'll",
    "start": "918720",
    "end": "920079"
  },
  {
    "text": "discuss we can take this idea much",
    "start": "920079",
    "end": "921760"
  },
  {
    "text": "further and get near optimal policies",
    "start": "921760",
    "end": "923440"
  },
  {
    "text": "even from highly suboptimal uh",
    "start": "923440",
    "end": "925120"
  },
  {
    "text": "behavioral policies even from highly",
    "start": "925120",
    "end": "926480"
  },
  {
    "text": "sub-optimal data",
    "start": "926480",
    "end": "928560"
  },
  {
    "start": "928000",
    "end": "928000"
  },
  {
    "text": "here is an example from a paper that we",
    "start": "928560",
    "end": "931199"
  },
  {
    "text": "that we wrote a couple years back called",
    "start": "931199",
    "end": "932800"
  },
  {
    "text": "cog this is",
    "start": "932800",
    "end": "934160"
  },
  {
    "text": "led by avi singh that illustrates one of",
    "start": "934160",
    "end": "936399"
  },
  {
    "text": "the things you can do with offline rl",
    "start": "936399",
    "end": "937920"
  },
  {
    "text": "and this is a robotics example so let's",
    "start": "937920",
    "end": "940160"
  },
  {
    "text": "say that you have this task where you",
    "start": "940160",
    "end": "941360"
  },
  {
    "text": "have to pick up a ball from a drawer",
    "start": "941360",
    "end": "944240"
  },
  {
    "text": "and you have some data of the robot",
    "start": "944240",
    "end": "946160"
  },
  {
    "text": "picking up the ball from the drawer and",
    "start": "946160",
    "end": "947519"
  },
  {
    "text": "you can use it to learn this task but",
    "start": "947519",
    "end": "949279"
  },
  {
    "text": "then at test time the drawer will be",
    "start": "949279",
    "end": "950639"
  },
  {
    "text": "closed",
    "start": "950639",
    "end": "951600"
  },
  {
    "text": "okay so it's kind of like a common sense",
    "start": "951600",
    "end": "953199"
  },
  {
    "text": "example a person would have common sense",
    "start": "953199",
    "end": "954959"
  },
  {
    "text": "they would figure out that like hey my",
    "start": "954959",
    "end": "956480"
  },
  {
    "text": "job is to pick up this green ball the",
    "start": "956480",
    "end": "958800"
  },
  {
    "text": "drawer is closed what i do well you go",
    "start": "958800",
    "end": "960639"
  },
  {
    "text": "and open it but of course the robot",
    "start": "960639",
    "end": "962320"
  },
  {
    "text": "can't figure that out because it's never",
    "start": "962320",
    "end": "963440"
  },
  {
    "text": "seen a closed drawer before it has no",
    "start": "963440",
    "end": "965360"
  },
  {
    "text": "idea what to do with this so it would",
    "start": "965360",
    "end": "967040"
  },
  {
    "text": "fail but now imagine that",
    "start": "967040",
    "end": "969680"
  },
  {
    "text": "just like a you know a person has prior",
    "start": "969680",
    "end": "972000"
  },
  {
    "text": "experience of all these other things",
    "start": "972000",
    "end": "973519"
  },
  {
    "text": "from their lifetime let's say the robot",
    "start": "973519",
    "end": "975199"
  },
  {
    "text": "also has a prior data set of a whole",
    "start": "975199",
    "end": "977279"
  },
  {
    "text": "bunch of other things that could be done",
    "start": "977279",
    "end": "978560"
  },
  {
    "text": "in this environment now none of these",
    "start": "978560",
    "end": "980800"
  },
  {
    "text": "things actually involve performing the",
    "start": "980800",
    "end": "982399"
  },
  {
    "text": "desired task none of these things",
    "start": "982399",
    "end": "983920"
  },
  {
    "text": "involve picking up the ball",
    "start": "983920",
    "end": "985839"
  },
  {
    "text": "but they involve other behaviors like",
    "start": "985839",
    "end": "987680"
  },
  {
    "text": "opening drawers closing drawers moving",
    "start": "987680",
    "end": "989839"
  },
  {
    "text": "objects around that sort of thing",
    "start": "989839",
    "end": "993120"
  },
  {
    "text": "if we run offline rl and we simply",
    "start": "993120",
    "end": "995360"
  },
  {
    "text": "combine the prior data and the task data",
    "start": "995360",
    "end": "998079"
  },
  {
    "text": "we simply put them into the same buffer",
    "start": "998079",
    "end": "1000079"
  },
  {
    "text": "assigning a reward of zero to all the",
    "start": "1000079",
    "end": "1001600"
  },
  {
    "text": "prior data the offline rl algorithm",
    "start": "1001600",
    "end": "1003680"
  },
  {
    "text": "should be able to figure out that the",
    "start": "1003680",
    "end": "1005279"
  },
  {
    "text": "state where the drawer is open has a",
    "start": "1005279",
    "end": "1007120"
  },
  {
    "text": "high value because you can pick up the",
    "start": "1007120",
    "end": "1008800"
  },
  {
    "text": "ball",
    "start": "1008800",
    "end": "1009680"
  },
  {
    "text": "therefore any state in the prior data",
    "start": "1009680",
    "end": "1012079"
  },
  {
    "text": "that resulted in the drawer being open",
    "start": "1012079",
    "end": "1014000"
  },
  {
    "text": "also has a high value and therefore",
    "start": "1014000",
    "end": "1015920"
  },
  {
    "text": "anything the robot did leading up to",
    "start": "1015920",
    "end": "1017360"
  },
  {
    "text": "that state also has a high value even",
    "start": "1017360",
    "end": "1019759"
  },
  {
    "text": "though it has never done those things",
    "start": "1019759",
    "end": "1020959"
  },
  {
    "text": "together it's never opened the drawer",
    "start": "1020959",
    "end": "1022160"
  },
  {
    "text": "and then picked up the ball but it can",
    "start": "1022160",
    "end": "1023759"
  },
  {
    "text": "figure out with dynamic programming that",
    "start": "1023759",
    "end": "1025360"
  },
  {
    "text": "these things relate to one another",
    "start": "1025360",
    "end": "1028240"
  },
  {
    "text": "so that means that if you run offline rl",
    "start": "1028240",
    "end": "1030319"
  },
  {
    "text": "on this combining the prior date in the",
    "start": "1030319",
    "end": "1031678"
  },
  {
    "text": "task theta you can pick up the ball from",
    "start": "1031679",
    "end": "1033120"
  },
  {
    "text": "an open drawer but you can also open the",
    "start": "1033120",
    "end": "1035520"
  },
  {
    "text": "drawer and pick up the ball if it starts",
    "start": "1035520",
    "end": "1037038"
  },
  {
    "text": "off closed",
    "start": "1037039",
    "end": "1038400"
  },
  {
    "text": "and you can also",
    "start": "1038400",
    "end": "1040240"
  },
  {
    "text": "close the top drawer",
    "start": "1040240",
    "end": "1042079"
  },
  {
    "text": "open the bottom drawer and then pick up",
    "start": "1042079",
    "end": "1043678"
  },
  {
    "text": "the ball",
    "start": "1043679",
    "end": "1044798"
  },
  {
    "text": "and if there's an object in the way you",
    "start": "1044799",
    "end": "1046480"
  },
  {
    "text": "can lift the object put it away and then",
    "start": "1046480",
    "end": "1049360"
  },
  {
    "text": "open the bottom drawer and then pick up",
    "start": "1049360",
    "end": "1050720"
  },
  {
    "text": "the ball so you're stringing together",
    "start": "1050720",
    "end": "1051919"
  },
  {
    "text": "all these behaviors even though you've",
    "start": "1051919",
    "end": "1053200"
  },
  {
    "text": "never seen them perform together",
    "start": "1053200",
    "end": "1056640"
  },
  {
    "start": "1056000",
    "end": "1056000"
  },
  {
    "text": "all right let's talk about some",
    "start": "1056640",
    "end": "1058240"
  },
  {
    "text": "technical materials",
    "start": "1058240",
    "end": "1060160"
  },
  {
    "text": "so before i explain some offline rl",
    "start": "1060160",
    "end": "1062720"
  },
  {
    "text": "methods let me just give a quick primer",
    "start": "1062720",
    "end": "1064880"
  },
  {
    "text": "on off policy rl so those of you that",
    "start": "1064880",
    "end": "1066720"
  },
  {
    "text": "are familiar with q learning will",
    "start": "1066720",
    "end": "1068480"
  },
  {
    "text": "basically know all this but just to get",
    "start": "1068480",
    "end": "1070160"
  },
  {
    "text": "us all on the same page",
    "start": "1070160",
    "end": "1071679"
  },
  {
    "text": "in reinforcement learning you have an",
    "start": "1071679",
    "end": "1073200"
  },
  {
    "text": "agent that takes actions we denote those",
    "start": "1073200",
    "end": "1075200"
  },
  {
    "text": "with a and the world responds with",
    "start": "1075200",
    "end": "1077200"
  },
  {
    "text": "states s and the resulting reward r of s",
    "start": "1077200",
    "end": "1080559"
  },
  {
    "text": "comma a",
    "start": "1080559",
    "end": "1081760"
  },
  {
    "text": "the goal in reinforcement learning is to",
    "start": "1081760",
    "end": "1083840"
  },
  {
    "text": "select a policy pi of a given s that",
    "start": "1083840",
    "end": "1086559"
  },
  {
    "text": "maximizes the expected cumulative reward",
    "start": "1086559",
    "end": "1089840"
  },
  {
    "text": "so the total reward that the agent will",
    "start": "1089840",
    "end": "1091440"
  },
  {
    "text": "get if it runs that policy for an entire",
    "start": "1091440",
    "end": "1093919"
  },
  {
    "text": "episode",
    "start": "1093919",
    "end": "1096000"
  },
  {
    "text": "the queue function is a very useful",
    "start": "1096000",
    "end": "1097679"
  },
  {
    "text": "object it depends on the policy and the",
    "start": "1097679",
    "end": "1100000"
  },
  {
    "text": "queue function quantifies what is the",
    "start": "1100000",
    "end": "1102240"
  },
  {
    "text": "total reward you will get if you take",
    "start": "1102240",
    "end": "1104320"
  },
  {
    "text": "the action 80 in state st and then",
    "start": "1104320",
    "end": "1106960"
  },
  {
    "text": "follow the policy pi thereafter",
    "start": "1106960",
    "end": "1109919"
  },
  {
    "text": "so if you can learn the q function for a",
    "start": "1109919",
    "end": "1112080"
  },
  {
    "text": "particular policy pi",
    "start": "1112080",
    "end": "1113760"
  },
  {
    "text": "then you can recover a new policy that",
    "start": "1113760",
    "end": "1116000"
  },
  {
    "text": "is as good or better usually better by",
    "start": "1116000",
    "end": "1118799"
  },
  {
    "text": "taking an action with probability one if",
    "start": "1118799",
    "end": "1120799"
  },
  {
    "text": "it is the arg max of q pi",
    "start": "1120799",
    "end": "1123120"
  },
  {
    "text": "and this is the basis of policy",
    "start": "1123120",
    "end": "1124559"
  },
  {
    "text": "iteration",
    "start": "1124559",
    "end": "1126160"
  },
  {
    "text": "you can also skip the middleman and try",
    "start": "1126160",
    "end": "1128160"
  },
  {
    "text": "to directly learn the optimal q function",
    "start": "1128160",
    "end": "1130559"
  },
  {
    "text": "so that its argmax policy is the optimal",
    "start": "1130559",
    "end": "1132400"
  },
  {
    "text": "policy and you can do that by minimizing",
    "start": "1132400",
    "end": "1134640"
  },
  {
    "text": "the difference between the left hand",
    "start": "1134640",
    "end": "1136080"
  },
  {
    "text": "side and right hand side of the bellman",
    "start": "1136080",
    "end": "1137440"
  },
  {
    "text": "equation so if you ensure that qsa is",
    "start": "1137440",
    "end": "1140640"
  },
  {
    "text": "equal to rsa plus the max over the next",
    "start": "1140640",
    "end": "1143039"
  },
  {
    "text": "q value at all states and actions then",
    "start": "1143039",
    "end": "1145360"
  },
  {
    "text": "the corresponding rmax policy will be",
    "start": "1145360",
    "end": "1147600"
  },
  {
    "text": "the optimal policy and that's the basis",
    "start": "1147600",
    "end": "1149520"
  },
  {
    "text": "of q learning so if you can enforce this",
    "start": "1149520",
    "end": "1151520"
  },
  {
    "text": "equation at all states you get the",
    "start": "1151520",
    "end": "1152880"
  },
  {
    "text": "optimal q function",
    "start": "1152880",
    "end": "1154400"
  },
  {
    "text": "so what we typically do for q learning",
    "start": "1154400",
    "end": "1156160"
  },
  {
    "text": "is we subtract the right hand side of",
    "start": "1156160",
    "end": "1158000"
  },
  {
    "text": "this equation from the left hand side",
    "start": "1158000",
    "end": "1159760"
  },
  {
    "text": "and minimize the difference at sampled",
    "start": "1159760",
    "end": "1162080"
  },
  {
    "text": "states and actions",
    "start": "1162080",
    "end": "1163440"
  },
  {
    "text": "so here s i a i and s i prime are",
    "start": "1163440",
    "end": "1166240"
  },
  {
    "text": "samples from a data set from a replay",
    "start": "1166240",
    "end": "1168640"
  },
  {
    "text": "buffer",
    "start": "1168640",
    "end": "1170320"
  },
  {
    "text": "okay so in principle this can now be",
    "start": "1170320",
    "end": "1172720"
  },
  {
    "text": "used as the basis for an off policy rl",
    "start": "1172720",
    "end": "1175200"
  },
  {
    "text": "algorithm also the basis for offline rl",
    "start": "1175200",
    "end": "1177679"
  },
  {
    "text": "algorithm you could simply take log data",
    "start": "1177679",
    "end": "1179760"
  },
  {
    "text": "without collecting any additional data",
    "start": "1179760",
    "end": "1180960"
  },
  {
    "text": "and try to minimize the left-hand side",
    "start": "1180960",
    "end": "1182559"
  },
  {
    "text": "the difference between the left-hand",
    "start": "1182559",
    "end": "1183520"
  },
  {
    "text": "side and right-hand side of this",
    "start": "1183520",
    "end": "1184480"
  },
  {
    "text": "equation",
    "start": "1184480",
    "end": "1185600"
  },
  {
    "text": "and that would be a viable way to get",
    "start": "1185600",
    "end": "1187039"
  },
  {
    "text": "started but by itself this isn't quite",
    "start": "1187039",
    "end": "1189760"
  },
  {
    "text": "going to work",
    "start": "1189760",
    "end": "1191280"
  },
  {
    "text": "okay what's the problem",
    "start": "1191280",
    "end": "1192960"
  },
  {
    "start": "1192000",
    "end": "1192000"
  },
  {
    "text": "so",
    "start": "1192960",
    "end": "1194640"
  },
  {
    "text": "we tried uh to basically run this kind",
    "start": "1194640",
    "end": "1197280"
  },
  {
    "text": "of naive procedure a few years back",
    "start": "1197280",
    "end": "1200400"
  },
  {
    "text": "and to see what would happen",
    "start": "1200400",
    "end": "1202080"
  },
  {
    "text": "and we used the half cheetah benchmark",
    "start": "1202080",
    "end": "1205200"
  },
  {
    "text": "and what we saw is that performance",
    "start": "1205200",
    "end": "1206960"
  },
  {
    "text": "starts to go up a little bit but then it",
    "start": "1206960",
    "end": "1208240"
  },
  {
    "text": "quickly drops",
    "start": "1208240",
    "end": "1209600"
  },
  {
    "text": "and it looks a lot like kind of",
    "start": "1209600",
    "end": "1211039"
  },
  {
    "text": "overfitting right that's kind of what",
    "start": "1211039",
    "end": "1212480"
  },
  {
    "text": "you'd expect if you had really",
    "start": "1212480",
    "end": "1213360"
  },
  {
    "text": "catastrophic overfitting so we tried to",
    "start": "1213360",
    "end": "1215600"
  },
  {
    "text": "vary the data set size",
    "start": "1215600",
    "end": "1217280"
  },
  {
    "text": "so here blue shows a data set of a",
    "start": "1217280",
    "end": "1219520"
  },
  {
    "text": "thousand data points red is one million",
    "start": "1219520",
    "end": "1222000"
  },
  {
    "text": "you can see that there's basically no",
    "start": "1222000",
    "end": "1223120"
  },
  {
    "text": "difference you don't you don't become",
    "start": "1223120",
    "end": "1224159"
  },
  {
    "text": "better as you add more data which is",
    "start": "1224159",
    "end": "1225919"
  },
  {
    "text": "strange like you would expect if it was",
    "start": "1225919",
    "end": "1227280"
  },
  {
    "text": "overfitting that this would fix it",
    "start": "1227280",
    "end": "1229520"
  },
  {
    "text": "um",
    "start": "1229520",
    "end": "1230400"
  },
  {
    "text": "so then we try to see how well does the",
    "start": "1230400",
    "end": "1232400"
  },
  {
    "text": "q function thinks think it's doing so if",
    "start": "1232400",
    "end": "1234320"
  },
  {
    "text": "we actually query the q function itself",
    "start": "1234320",
    "end": "1236960"
  },
  {
    "text": "for the estimated values the y axis here",
    "start": "1236960",
    "end": "1239120"
  },
  {
    "text": "is a log scale it thinks it's going to",
    "start": "1239120",
    "end": "1240640"
  },
  {
    "text": "get 10 to the seventh power",
    "start": "1240640",
    "end": "1242720"
  },
  {
    "text": "whoa so it thinks it's going to do",
    "start": "1242720",
    "end": "1244080"
  },
  {
    "text": "really well but ends up doing terribly",
    "start": "1244080",
    "end": "1246559"
  },
  {
    "text": "so we're getting massive overestimation",
    "start": "1246559",
    "end": "1250240"
  },
  {
    "start": "1248000",
    "end": "1248000"
  },
  {
    "text": "why is that happening",
    "start": "1250559",
    "end": "1252000"
  },
  {
    "text": "well the issue basically comes down to",
    "start": "1252000",
    "end": "1253919"
  },
  {
    "text": "something called distributional shift so",
    "start": "1253919",
    "end": "1255919"
  },
  {
    "text": "if we take this bellman backup and we",
    "start": "1255919",
    "end": "1257520"
  },
  {
    "text": "rewrite it in a slightly different way",
    "start": "1257520",
    "end": "1260320"
  },
  {
    "text": "we can write it as the expected value of",
    "start": "1260320",
    "end": "1262320"
  },
  {
    "text": "the target q function under some",
    "start": "1262320",
    "end": "1264240"
  },
  {
    "text": "distribution pi nu and pi nu is going to",
    "start": "1264240",
    "end": "1266159"
  },
  {
    "text": "be that r max distribution",
    "start": "1266159",
    "end": "1269520"
  },
  {
    "text": "what is our objective for training uh",
    "start": "1269840",
    "end": "1271919"
  },
  {
    "text": "the q function",
    "start": "1271919",
    "end": "1273679"
  },
  {
    "text": "well the objective is to minimize the",
    "start": "1273679",
    "end": "1275200"
  },
  {
    "text": "difference between the left hand side",
    "start": "1275200",
    "end": "1276559"
  },
  {
    "text": "and right hand side but under a training",
    "start": "1276559",
    "end": "1278559"
  },
  {
    "text": "distribution pi beta",
    "start": "1278559",
    "end": "1282158"
  },
  {
    "text": "so we would expect when we do this that",
    "start": "1282400",
    "end": "1284320"
  },
  {
    "text": "we would have high accuracy under pi",
    "start": "1284320",
    "end": "1286080"
  },
  {
    "text": "beta",
    "start": "1286080",
    "end": "1286960"
  },
  {
    "text": "but of course pi nu and pi beta are not",
    "start": "1286960",
    "end": "1288799"
  },
  {
    "text": "equal",
    "start": "1288799",
    "end": "1290320"
  },
  {
    "text": "the whole point is to find a pi nu that",
    "start": "1290320",
    "end": "1291679"
  },
  {
    "text": "is better than pi beta so we're",
    "start": "1291679",
    "end": "1293120"
  },
  {
    "text": "experiencing distributional shift",
    "start": "1293120",
    "end": "1295120"
  },
  {
    "text": "even worse pi new is selected to",
    "start": "1295120",
    "end": "1297200"
  },
  {
    "text": "maximize the expected value of q",
    "start": "1297200",
    "end": "1300960"
  },
  {
    "text": "that's a lot like what you do if you",
    "start": "1300960",
    "end": "1302320"
  },
  {
    "text": "want to get an adversarial example",
    "start": "1302320",
    "end": "1304159"
  },
  {
    "text": "essentially pi new is trying to find an",
    "start": "1304159",
    "end": "1305919"
  },
  {
    "text": "action that will shrink the q function",
    "start": "1305919",
    "end": "1307600"
  },
  {
    "text": "into outputting a large number",
    "start": "1307600",
    "end": "1309760"
  },
  {
    "text": "and we know from the study of",
    "start": "1309760",
    "end": "1311200"
  },
  {
    "text": "adversarial examples that if you want to",
    "start": "1311200",
    "end": "1312559"
  },
  {
    "text": "trick a q function or any neural net",
    "start": "1312559",
    "end": "1315120"
  },
  {
    "text": "it's not that hard to do you'll find",
    "start": "1315120",
    "end": "1316320"
  },
  {
    "text": "that adversarial example that tricks it",
    "start": "1316320",
    "end": "1318080"
  },
  {
    "text": "into outputting the value you want in",
    "start": "1318080",
    "end": "1319280"
  },
  {
    "text": "this case and are only an erroneously",
    "start": "1319280",
    "end": "1321280"
  },
  {
    "text": "high value so that's why we see this",
    "start": "1321280",
    "end": "1323039"
  },
  {
    "text": "massive overestimation is due to",
    "start": "1323039",
    "end": "1324400"
  },
  {
    "text": "distributional shift we have to somehow",
    "start": "1324400",
    "end": "1326320"
  },
  {
    "text": "address this if we're going to get",
    "start": "1326320",
    "end": "1327679"
  },
  {
    "text": "effective offline rl methods",
    "start": "1327679",
    "end": "1331279"
  },
  {
    "text": "so",
    "start": "1331760",
    "end": "1332400"
  },
  {
    "text": "one of the methods that we've developed",
    "start": "1332400",
    "end": "1333600"
  },
  {
    "text": "for doing this is called conservative q",
    "start": "1333600",
    "end": "1335120"
  },
  {
    "text": "learning",
    "start": "1335120",
    "end": "1336080"
  },
  {
    "text": "so the aiming is in conservative q",
    "start": "1336080",
    "end": "1337919"
  },
  {
    "text": "learning is to directly repair these",
    "start": "1337919",
    "end": "1339760"
  },
  {
    "text": "overestimated q values",
    "start": "1339760",
    "end": "1341760"
  },
  {
    "text": "but let's imagine that this green curve",
    "start": "1341760",
    "end": "1343919"
  },
  {
    "text": "is the true function and the blue curve",
    "start": "1343919",
    "end": "1346000"
  },
  {
    "text": "is your fit",
    "start": "1346000",
    "end": "1347200"
  },
  {
    "text": "so",
    "start": "1347200",
    "end": "1347919"
  },
  {
    "text": "the the blue curve is a pretty good fit",
    "start": "1347919",
    "end": "1349679"
  },
  {
    "text": "to the green curve",
    "start": "1349679",
    "end": "1351440"
  },
  {
    "text": "but if you try to maximize the blue",
    "start": "1351440",
    "end": "1353440"
  },
  {
    "text": "curve if you try to maximize the q",
    "start": "1353440",
    "end": "1354799"
  },
  {
    "text": "function you'll find exactly the place",
    "start": "1354799",
    "end": "1356480"
  },
  {
    "text": "where it makes the largest error in the",
    "start": "1356480",
    "end": "1357919"
  },
  {
    "text": "positive direction",
    "start": "1357919",
    "end": "1360480"
  },
  {
    "text": "so the idea in conservative queue",
    "start": "1360480",
    "end": "1362320"
  },
  {
    "text": "learning is to directly repair these",
    "start": "1362320",
    "end": "1364000"
  },
  {
    "text": "mistakes",
    "start": "1364000",
    "end": "1365440"
  },
  {
    "text": "so what we're going to do",
    "start": "1365440",
    "end": "1367120"
  },
  {
    "text": "is we're going to have an objective",
    "start": "1367120",
    "end": "1370320"
  },
  {
    "text": "where we've got our regular development",
    "start": "1370320",
    "end": "1372799"
  },
  {
    "text": "error minimization just like before",
    "start": "1372799",
    "end": "1374880"
  },
  {
    "text": "and then we have a regularizer and what",
    "start": "1374880",
    "end": "1376880"
  },
  {
    "text": "this regularizer tries to do is it tries",
    "start": "1376880",
    "end": "1379120"
  },
  {
    "text": "to find a distribution mu",
    "start": "1379120",
    "end": "1381120"
  },
  {
    "text": "that maximizes q values",
    "start": "1381120",
    "end": "1383360"
  },
  {
    "text": "and it tries to minimize the q values",
    "start": "1383360",
    "end": "1385039"
  },
  {
    "text": "under mu it's a little bit like a gan in",
    "start": "1385039",
    "end": "1386880"
  },
  {
    "text": "that sense so what that's going to do is",
    "start": "1386880",
    "end": "1389760"
  },
  {
    "text": "mu will identify those peaks the largest",
    "start": "1389760",
    "end": "1392480"
  },
  {
    "text": "peaks",
    "start": "1392480",
    "end": "1393440"
  },
  {
    "text": "and then the q function optimization",
    "start": "1393440",
    "end": "1395600"
  },
  {
    "text": "will push them down",
    "start": "1395600",
    "end": "1397360"
  },
  {
    "text": "now of course this won't result in",
    "start": "1397360",
    "end": "1399360"
  },
  {
    "text": "getting the right q value at those out",
    "start": "1399360",
    "end": "1401440"
  },
  {
    "text": "of distribution actions because you",
    "start": "1401440",
    "end": "1402799"
  },
  {
    "text": "don't know what the right q value is but",
    "start": "1402799",
    "end": "1404480"
  },
  {
    "text": "it will result in those out of",
    "start": "1404480",
    "end": "1405760"
  },
  {
    "text": "distribution actions not having the",
    "start": "1405760",
    "end": "1407280"
  },
  {
    "text": "largest q value so then hopefully one of",
    "start": "1407280",
    "end": "1409440"
  },
  {
    "text": "those actions that is actually uh",
    "start": "1409440",
    "end": "1411120"
  },
  {
    "text": "supervised will be the better one or",
    "start": "1411120",
    "end": "1413440"
  },
  {
    "text": "something similar to it",
    "start": "1413440",
    "end": "1415280"
  },
  {
    "text": "in fact it's actually possible to prove",
    "start": "1415280",
    "end": "1417200"
  },
  {
    "text": "that conservative queue learning will",
    "start": "1417200",
    "end": "1418799"
  },
  {
    "text": "result in cuba functions that lower",
    "start": "1418799",
    "end": "1420880"
  },
  {
    "text": "bound the true q value",
    "start": "1420880",
    "end": "1423919"
  },
  {
    "start": "1424000",
    "end": "1424000"
  },
  {
    "text": "now in reality we use a better lower",
    "start": "1424559",
    "end": "1426159"
  },
  {
    "text": "bound where we also push up on samples",
    "start": "1426159",
    "end": "1428480"
  },
  {
    "text": "from the data and this might seem",
    "start": "1428480",
    "end": "1429919"
  },
  {
    "text": "counterintuitive because if we're trying",
    "start": "1429919",
    "end": "1431279"
  },
  {
    "text": "to avoid overestimation why would we",
    "start": "1431279",
    "end": "1432880"
  },
  {
    "text": "push up on samples from the data but",
    "start": "1432880",
    "end": "1434880"
  },
  {
    "text": "imagine that all of your good actions",
    "start": "1434880",
    "end": "1436799"
  },
  {
    "text": "are actually close to the data set then",
    "start": "1436799",
    "end": "1438480"
  },
  {
    "text": "these two terms will essentially cancel",
    "start": "1438480",
    "end": "1439919"
  },
  {
    "text": "out so if good actions are out of",
    "start": "1439919",
    "end": "1441520"
  },
  {
    "text": "distribution the first term pushes them",
    "start": "1441520",
    "end": "1442960"
  },
  {
    "text": "down and the second term pushes up on",
    "start": "1442960",
    "end": "1444400"
  },
  {
    "text": "something else",
    "start": "1444400",
    "end": "1445520"
  },
  {
    "text": "and then all the good actions become in",
    "start": "1445520",
    "end": "1446880"
  },
  {
    "text": "distribution then these two terms",
    "start": "1446880",
    "end": "1448159"
  },
  {
    "text": "basically balance out",
    "start": "1448159",
    "end": "1449760"
  },
  {
    "text": "and now it's not a point wise guarantee",
    "start": "1449760",
    "end": "1451279"
  },
  {
    "text": "anymore but you're still guaranteed to",
    "start": "1451279",
    "end": "1452480"
  },
  {
    "text": "lower down the value under your learned",
    "start": "1452480",
    "end": "1454080"
  },
  {
    "text": "policy which is all you really care",
    "start": "1454080",
    "end": "1455600"
  },
  {
    "text": "about",
    "start": "1455600",
    "end": "1456960"
  },
  {
    "start": "1456000",
    "end": "1456000"
  },
  {
    "text": "um the bound actually holds in practice",
    "start": "1456960",
    "end": "1458640"
  },
  {
    "text": "so you can actually measure it with",
    "start": "1458640",
    "end": "1459760"
  },
  {
    "text": "monte carlo estimation and find that",
    "start": "1459760",
    "end": "1462400"
  },
  {
    "text": "conservative q learning always results",
    "start": "1462400",
    "end": "1464480"
  },
  {
    "text": "in underestimation and there's less",
    "start": "1464480",
    "end": "1466400"
  },
  {
    "text": "underestimation if you use the version",
    "start": "1466400",
    "end": "1467840"
  },
  {
    "text": "with both terms",
    "start": "1467840",
    "end": "1469200"
  },
  {
    "text": "so that's the cqlh variant here",
    "start": "1469200",
    "end": "1471840"
  },
  {
    "text": "and it performs pretty well in practice",
    "start": "1471840",
    "end": "1473679"
  },
  {
    "text": "so these are older results at this point",
    "start": "1473679",
    "end": "1475679"
  },
  {
    "text": "they're a few years old but across a",
    "start": "1475679",
    "end": "1477360"
  },
  {
    "text": "range of benchmark tasks cql tends to",
    "start": "1477360",
    "end": "1479840"
  },
  {
    "text": "attain very good results now since then",
    "start": "1479840",
    "end": "1481279"
  },
  {
    "text": "better methods have come out so back",
    "start": "1481279",
    "end": "1483279"
  },
  {
    "text": "when this first came out it was a lot",
    "start": "1483279",
    "end": "1484480"
  },
  {
    "text": "better",
    "start": "1484480",
    "end": "1486559"
  },
  {
    "text": "but uh for the methods that came out",
    "start": "1486559",
    "end": "1488159"
  },
  {
    "start": "1487000",
    "end": "1487000"
  },
  {
    "text": "since then there's been quite a bit of",
    "start": "1488159",
    "end": "1489679"
  },
  {
    "text": "work on methods that extend cqm there's",
    "start": "1489679",
    "end": "1491440"
  },
  {
    "text": "been a lot of growth in offline rl",
    "start": "1491440",
    "end": "1493279"
  },
  {
    "text": "methods and cql has been very widely",
    "start": "1493279",
    "end": "1495279"
  },
  {
    "text": "used so these are just a few papers that",
    "start": "1495279",
    "end": "1496799"
  },
  {
    "text": "have come out that show that cql",
    "start": "1496799",
    "end": "1498559"
  },
  {
    "text": "actually still works pretty well",
    "start": "1498559",
    "end": "1500400"
  },
  {
    "text": "on new tasks and you can extend it in",
    "start": "1500400",
    "end": "1502080"
  },
  {
    "text": "various ways with things like data",
    "start": "1502080",
    "end": "1503360"
  },
  {
    "text": "augmentation and get even better results",
    "start": "1503360",
    "end": "1507278"
  },
  {
    "start": "1506000",
    "end": "1506000"
  },
  {
    "text": "but the thing that i want to conclude on",
    "start": "1507520",
    "end": "1509760"
  },
  {
    "text": "is",
    "start": "1509760",
    "end": "1510559"
  },
  {
    "text": "a little bit of a discussion of",
    "start": "1510559",
    "end": "1512480"
  },
  {
    "text": "applications that perhaps illustrate",
    "start": "1512480",
    "end": "1514080"
  },
  {
    "text": "some of the power of offline rl",
    "start": "1514080",
    "end": "1516400"
  },
  {
    "text": "so",
    "start": "1516400",
    "end": "1517919"
  },
  {
    "text": "i",
    "start": "1517919",
    "end": "1518640"
  },
  {
    "text": "in my research i do a lot of work on",
    "start": "1518640",
    "end": "1520240"
  },
  {
    "text": "applications of rl to things like",
    "start": "1520240",
    "end": "1521600"
  },
  {
    "text": "robotics and the process for doing that",
    "start": "1521600",
    "end": "1523600"
  },
  {
    "text": "with online rl has always been quite",
    "start": "1523600",
    "end": "1524960"
  },
  {
    "text": "painful",
    "start": "1524960",
    "end": "1525919"
  },
  {
    "text": "so typically you first have to",
    "start": "1525919",
    "end": "1527120"
  },
  {
    "text": "instrument the task so that you can run",
    "start": "1527120",
    "end": "1528720"
  },
  {
    "text": "rl at safety mechanisms autonomous",
    "start": "1528720",
    "end": "1530480"
  },
  {
    "text": "collection rewards etc then you wait a",
    "start": "1530480",
    "end": "1532720"
  },
  {
    "text": "really long time for online rl to run",
    "start": "1532720",
    "end": "1535120"
  },
  {
    "text": "and then typically it doesn't work on",
    "start": "1535120",
    "end": "1536480"
  },
  {
    "text": "the first try so you change the",
    "start": "1536480",
    "end": "1537679"
  },
  {
    "text": "algorithm in some small way and then you",
    "start": "1537679",
    "end": "1539279"
  },
  {
    "text": "wait a really long time again and once",
    "start": "1539279",
    "end": "1541600"
  },
  {
    "text": "you're done with this even if you're",
    "start": "1541600",
    "end": "1542640"
  },
  {
    "text": "successful you have to throw it all in",
    "start": "1542640",
    "end": "1543919"
  },
  {
    "text": "the garbage and start over for the next",
    "start": "1543919",
    "end": "1545360"
  },
  {
    "text": "task that you want to learn so that",
    "start": "1545360",
    "end": "1546799"
  },
  {
    "text": "becomes a very onerous process if you're",
    "start": "1546799",
    "end": "1548320"
  },
  {
    "text": "actually doing rl in the real world",
    "start": "1548320",
    "end": "1550799"
  },
  {
    "text": "the offline rl process is to first",
    "start": "1550799",
    "end": "1552720"
  },
  {
    "text": "collect your initial data set and train",
    "start": "1552720",
    "end": "1554640"
  },
  {
    "text": "a policy on that data set now again it",
    "start": "1554640",
    "end": "1556720"
  },
  {
    "text": "doesn't always work so then you change",
    "start": "1556720",
    "end": "1558320"
  },
  {
    "text": "the algorithm in some small way but you",
    "start": "1558320",
    "end": "1560000"
  },
  {
    "text": "don't have to recollect the data you",
    "start": "1560000",
    "end": "1560960"
  },
  {
    "text": "just rerun the training process the same",
    "start": "1560960",
    "end": "1562720"
  },
  {
    "text": "way that you would with supervised",
    "start": "1562720",
    "end": "1563840"
  },
  {
    "text": "learning and if you do want to collect",
    "start": "1563840",
    "end": "1565679"
  },
  {
    "text": "more data you simply append it to the",
    "start": "1565679",
    "end": "1566880"
  },
  {
    "text": "datasets you don't recollect it all from",
    "start": "1566880",
    "end": "1568400"
  },
  {
    "text": "scratch",
    "start": "1568400",
    "end": "1569600"
  },
  {
    "text": "and when you're done with this you can",
    "start": "1569600",
    "end": "1571039"
  },
  {
    "text": "keep the data set and use it again for",
    "start": "1571039",
    "end": "1572320"
  },
  {
    "text": "the next project which is also really",
    "start": "1572320",
    "end": "1573679"
  },
  {
    "text": "powerful",
    "start": "1573679",
    "end": "1574880"
  },
  {
    "text": "um so i'll tell you about a couple of",
    "start": "1574880",
    "end": "1576720"
  },
  {
    "start": "1575000",
    "end": "1575000"
  },
  {
    "text": "settings where we've used this",
    "start": "1576720",
    "end": "1578720"
  },
  {
    "text": "power to make it a lot easier for us to",
    "start": "1578720",
    "end": "1580640"
  },
  {
    "text": "make research progress",
    "start": "1580640",
    "end": "1582480"
  },
  {
    "text": "so we've been doing a lot of research at",
    "start": "1582480",
    "end": "1584159"
  },
  {
    "text": "google on large-scale robotic",
    "start": "1584159",
    "end": "1585760"
  },
  {
    "text": "reinforcement learning using real-world",
    "start": "1585760",
    "end": "1587520"
  },
  {
    "text": "data",
    "start": "1587520",
    "end": "1588480"
  },
  {
    "text": "so most recently we had this project",
    "start": "1588480",
    "end": "1590320"
  },
  {
    "text": "called mt opt where collected a huge",
    "start": "1590320",
    "end": "1591919"
  },
  {
    "text": "data set of multi-task behaviors 12",
    "start": "1591919",
    "end": "1594320"
  },
  {
    "text": "different tasks thousands of objects",
    "start": "1594320",
    "end": "1595919"
  },
  {
    "text": "months of data collection and then we",
    "start": "1595919",
    "end": "1597760"
  },
  {
    "text": "had a new hypothesis we thought well can",
    "start": "1597760",
    "end": "1599679"
  },
  {
    "text": "we learn these tasks without rewards",
    "start": "1599679",
    "end": "1601279"
  },
  {
    "text": "using goal conditioned rl now this talk",
    "start": "1601279",
    "end": "1603520"
  },
  {
    "text": "is not about goal condition rl i just",
    "start": "1603520",
    "end": "1604960"
  },
  {
    "text": "want to make the point that we could",
    "start": "1604960",
    "end": "1606400"
  },
  {
    "text": "answer this hypothesis with offline",
    "start": "1606400",
    "end": "1608320"
  },
  {
    "text": "reinforcement learning without any data",
    "start": "1608320",
    "end": "1609919"
  },
  {
    "text": "collection so we wanted to see okay can",
    "start": "1609919",
    "end": "1611520"
  },
  {
    "text": "we if we give the robot a gold image can",
    "start": "1611520",
    "end": "1613440"
  },
  {
    "text": "it learn to reach that gold image",
    "start": "1613440",
    "end": "1614720"
  },
  {
    "text": "without any hand designed rewards so we",
    "start": "1614720",
    "end": "1617440"
  },
  {
    "text": "reused the same data that we'd used for",
    "start": "1617440",
    "end": "1619600"
  },
  {
    "text": "these other projects and then just",
    "start": "1619600",
    "end": "1621120"
  },
  {
    "text": "change the algorithm to a variant of cql",
    "start": "1621120",
    "end": "1623440"
  },
  {
    "text": "with gold images and we could",
    "start": "1623440",
    "end": "1625440"
  },
  {
    "start": "1625000",
    "end": "1625000"
  },
  {
    "text": "immediately actually get policies that",
    "start": "1625440",
    "end": "1627039"
  },
  {
    "text": "could succeed at the task to resolve our",
    "start": "1627039",
    "end": "1628559"
  },
  {
    "text": "hypothesis",
    "start": "1628559",
    "end": "1630159"
  },
  {
    "text": "so",
    "start": "1630159",
    "end": "1631200"
  },
  {
    "text": "in this method there is no reward",
    "start": "1631200",
    "end": "1632480"
  },
  {
    "text": "function tests are defined entirely",
    "start": "1632480",
    "end": "1633840"
  },
  {
    "text": "using goal images it uses a conservative",
    "start": "1633840",
    "end": "1635840"
  },
  {
    "text": "offline rl method similar to cql and",
    "start": "1635840",
    "end": "1637600"
  },
  {
    "text": "works very well as an unsupervised",
    "start": "1637600",
    "end": "1639200"
  },
  {
    "text": "pre-training objective you can think of",
    "start": "1639200",
    "end": "1640799"
  },
  {
    "text": "it almost like kind of a bert for",
    "start": "1640799",
    "end": "1642240"
  },
  {
    "text": "robotics you train the goal condition q",
    "start": "1642240",
    "end": "1644159"
  },
  {
    "text": "function with unsupervised offline rl",
    "start": "1644159",
    "end": "1646399"
  },
  {
    "text": "and then you can fine-tune it with a",
    "start": "1646399",
    "end": "1647440"
  },
  {
    "text": "task reward much more quickly",
    "start": "1647440",
    "end": "1650000"
  },
  {
    "text": "so the point is that we could resolve",
    "start": "1650000",
    "end": "1651520"
  },
  {
    "text": "this without any new data collection",
    "start": "1651520",
    "end": "1653039"
  },
  {
    "text": "just using the data from these other",
    "start": "1653039",
    "end": "1654240"
  },
  {
    "start": "1654000",
    "end": "1654000"
  },
  {
    "text": "projects another example in early 2020",
    "start": "1654240",
    "end": "1656880"
  },
  {
    "text": "my student gregory khan collected 40",
    "start": "1656880",
    "end": "1658880"
  },
  {
    "text": "hours of a ground robot think of it as",
    "start": "1658880",
    "end": "1661039"
  },
  {
    "text": "kind of a delivery robot driving around",
    "start": "1661039",
    "end": "1662880"
  },
  {
    "text": "different environments in late 2020 drew",
    "start": "1662880",
    "end": "1665520"
  },
  {
    "text": "shaw could use offline methods to",
    "start": "1665520",
    "end": "1669520"
  },
  {
    "text": "use the same data set to train a robot",
    "start": "1669520",
    "end": "1671440"
  },
  {
    "text": "that could deliver to target locations",
    "start": "1671440",
    "end": "1673760"
  },
  {
    "text": "and in 2021 extend this method further",
    "start": "1673760",
    "end": "1676000"
  },
  {
    "text": "to get a robot that can search through",
    "start": "1676000",
    "end": "1677200"
  },
  {
    "text": "novel environments to find objects that",
    "start": "1677200",
    "end": "1679120"
  },
  {
    "text": "hasn't seen before again using all the",
    "start": "1679120",
    "end": "1681039"
  },
  {
    "text": "same data sets",
    "start": "1681039",
    "end": "1682960"
  },
  {
    "text": "now there are many other topics in",
    "start": "1682960",
    "end": "1684240"
  },
  {
    "text": "offline rl that i don't have time to",
    "start": "1684240",
    "end": "1685840"
  },
  {
    "text": "cover",
    "start": "1685840",
    "end": "1686720"
  },
  {
    "text": "model based uh learning is a really good",
    "start": "1686720",
    "end": "1688559"
  },
  {
    "text": "fit for offline rl so generally you can",
    "start": "1688559",
    "end": "1690480"
  },
  {
    "text": "do some really cool stuff there",
    "start": "1690480",
    "end": "1692399"
  },
  {
    "text": "a lot of ideas in cql actually transfer",
    "start": "1692399",
    "end": "1694159"
  },
  {
    "text": "to the model based setting we can also",
    "start": "1694159",
    "end": "1695679"
  },
  {
    "text": "use huge models like transformer models",
    "start": "1695679",
    "end": "1697600"
  },
  {
    "text": "those work great for offline model based",
    "start": "1697600",
    "end": "1699440"
  },
  {
    "text": "rl so for that you can check out the",
    "start": "1699440",
    "end": "1700880"
  },
  {
    "text": "paper called on the structure",
    "start": "1700880",
    "end": "1702320"
  },
  {
    "text": "transformer multicast offline rl there's",
    "start": "1702320",
    "end": "1704880"
  },
  {
    "text": "really interesting stuff you can do",
    "start": "1704880",
    "end": "1705919"
  },
  {
    "text": "there incorporating data from other",
    "start": "1705919",
    "end": "1707200"
  },
  {
    "text": "tasks and many more",
    "start": "1707200",
    "end": "1709360"
  },
  {
    "start": "1709000",
    "end": "1709000"
  },
  {
    "text": "but the main takeaways",
    "start": "1709360",
    "end": "1711039"
  },
  {
    "text": "i want to leave you with is that there's",
    "start": "1711039",
    "end": "1712880"
  },
  {
    "text": "still a considerable gap between current",
    "start": "1712880",
    "end": "1715440"
  },
  {
    "text": "offline rl method and kind of the dream",
    "start": "1715440",
    "end": "1717360"
  },
  {
    "text": "of offline rl that i presented so if",
    "start": "1717360",
    "end": "1719279"
  },
  {
    "text": "you're interested in doing more work in",
    "start": "1719279",
    "end": "1720399"
  },
  {
    "text": "this area there's a lot of work to be",
    "start": "1720399",
    "end": "1721679"
  },
  {
    "text": "done developing workflows for offline rl",
    "start": "1721679",
    "end": "1724080"
  },
  {
    "text": "in supervised learning we have this",
    "start": "1724080",
    "end": "1725279"
  },
  {
    "text": "training test split what's the",
    "start": "1725279",
    "end": "1726640"
  },
  {
    "text": "equivalent of that in offline rl as a",
    "start": "1726640",
    "end": "1728559"
  },
  {
    "text": "starting point we have a paper that",
    "start": "1728559",
    "end": "1730080"
  },
  {
    "text": "provides some ideas but a lot more work",
    "start": "1730080",
    "end": "1731600"
  },
  {
    "text": "is needed statistical guarantees",
    "start": "1731600",
    "end": "1733919"
  },
  {
    "text": "guarantees about distributional shift",
    "start": "1733919",
    "end": "1735200"
  },
  {
    "text": "guarantees about safety as well as",
    "start": "1735200",
    "end": "1737279"
  },
  {
    "text": "scalable methods and large scale",
    "start": "1737279",
    "end": "1738559"
  },
  {
    "text": "applications i talked a lot about",
    "start": "1738559",
    "end": "1739840"
  },
  {
    "text": "applications to robotics but i think",
    "start": "1739840",
    "end": "1741760"
  },
  {
    "text": "these methods can be applied very nicely",
    "start": "1741760",
    "end": "1743760"
  },
  {
    "text": "to lots of other domains from inventory",
    "start": "1743760",
    "end": "1745840"
  },
  {
    "text": "management to power grids to dialog",
    "start": "1745840",
    "end": "1747520"
  },
  {
    "text": "systems i think exploring applications",
    "start": "1747520",
    "end": "1749360"
  },
  {
    "text": "of scalable systems is a really",
    "start": "1749360",
    "end": "1750960"
  },
  {
    "text": "promising future direction",
    "start": "1750960",
    "end": "1753600"
  },
  {
    "text": "all right so i'd like to thank of course",
    "start": "1753600",
    "end": "1755039"
  },
  {
    "text": "the students that carried out much of",
    "start": "1755039",
    "end": "1756720"
  },
  {
    "text": "the research that i discussed and i'd",
    "start": "1756720",
    "end": "1758240"
  },
  {
    "text": "like to thank you for listening",
    "start": "1758240",
    "end": "1761960"
  }
]