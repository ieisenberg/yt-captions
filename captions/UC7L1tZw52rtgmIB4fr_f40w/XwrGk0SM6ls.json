[
  {
    "start": "0",
    "end": "41000"
  },
  {
    "text": "uh hey everyone thanks for coming uh today we're going to talk about how uh we're using Ray to scale ml at Reddit uh",
    "start": "3120",
    "end": "11240"
  },
  {
    "text": "my name's uh Garrett I'm a principal engineer at Reddit where I drive the technical direction for our ml platform",
    "start": "11240",
    "end": "19400"
  },
  {
    "text": "uh which is called Gazette uh and ml infrastructure more broadly uh we'll spend most of our time",
    "start": "19400",
    "end": "26519"
  },
  {
    "text": "today on the why and how for for training uh that's a bit more mature at",
    "start": "26519",
    "end": "31599"
  },
  {
    "text": "redddit and then we'll spend a little bit of time uh with some of our work uh with Ray around serving uh we got a lot",
    "start": "31599",
    "end": "38480"
  },
  {
    "text": "to cover uh so let's get into it uh starting with the why so uh at",
    "start": "38480",
    "end": "45559"
  },
  {
    "start": "41000",
    "end": "483000"
  },
  {
    "text": "Reddit we made the Strategic decision to move away from our Cube flow based",
    "start": "45559",
    "end": "51280"
  },
  {
    "text": "platform and start building on top of Ray uh we did this for a few reasons uh",
    "start": "51280",
    "end": "57320"
  },
  {
    "text": "to start Ray is uh what I'll call framework first and Cube flow is",
    "start": "57320",
    "end": "62960"
  },
  {
    "text": "orchestration first we're going to dig more into this in a minute uh second but just as important",
    "start": "62960",
    "end": "69720"
  },
  {
    "text": "uh we believe that we can build a much much better developer experience for our users on top of Ray versus Cube flow uh",
    "start": "69720",
    "end": "77119"
  },
  {
    "text": "we're going to dig in more uh there as well uh and beyond that between the",
    "start": "77119",
    "end": "82560"
  },
  {
    "text": "active development team and how we've seen race scale for a lot of other companies we had a very high degree of",
    "start": "82560",
    "end": "89360"
  },
  {
    "text": "confidence uh in investing in it so qlow and Ray are both technologies",
    "start": "89360",
    "end": "96560"
  },
  {
    "text": "that are commonly used as a foundation for ML uh training platforms but they're",
    "start": "96560",
    "end": "101720"
  },
  {
    "text": "fundamentally oriented a bit different the way we see it Cube flow is primarily",
    "start": "101720",
    "end": "107240"
  },
  {
    "text": "an orchestration layer on top of kubernetes as a compute layer uh where then you can layer a distributed compute",
    "start": "107240",
    "end": "114680"
  },
  {
    "text": "framework on top of it where Ray is primarily a distributed compute",
    "start": "114680",
    "end": "119880"
  },
  {
    "text": "framework uh on top of a compute layer uh which can be kubernetes if you want it to be uh and that can be used in the",
    "start": "119880",
    "end": "127280"
  },
  {
    "text": "context of an external orchestration layer uh so what are the implications of",
    "start": "127280",
    "end": "132480"
  },
  {
    "text": "this uh what we found is that when scaling our ml training platform on Cube",
    "start": "132480",
    "end": "137519"
  },
  {
    "text": "flow uh a technology that is primarily a workflow orchestration system we ran into some issues uh specifically uh",
    "start": "137519",
    "end": "146040"
  },
  {
    "text": "dealing with orchestration of Orchestra uh orchestrators uh less than ideal abstractions for sharing code uh an",
    "start": "146040",
    "end": "154239"
  },
  {
    "text": "additional operational burden around uh distributed compute um so airflow is a Reddit",
    "start": "154239",
    "end": "162800"
  },
  {
    "text": "primary workflow orchestration engine for data analytics and ml when the the Primitive of the ml",
    "start": "162800",
    "end": "170599"
  },
  {
    "text": "training platform is also a pipeline orchestrator you end up with workflows",
    "start": "170599",
    "end": "175959"
  },
  {
    "text": "that span multiple orchestration systems uh and in the worst case you get",
    "start": "175959",
    "end": "181159"
  },
  {
    "text": "something uh that looks like this uh introducing uh a lot of incidental",
    "start": "181159",
    "end": "186560"
  },
  {
    "text": "complexity around orchestrating between uh two orchestrators uh this is not",
    "start": "186560",
    "end": "192799"
  },
  {
    "text": "great and we can Implement best practices to ensure that these two",
    "start": "192799",
    "end": "199120"
  },
  {
    "text": "orchestration systems interact in saner ways uh like this um for",
    "start": "199120",
    "end": "205080"
  },
  {
    "text": "example but it's it's very hard to enforce this mechanically and when uh",
    "start": "205080",
    "end": "211360"
  },
  {
    "text": "building a self-service platform supporting uh dozens of teams and a whole lot of users uh if you can't",
    "start": "211360",
    "end": "218280"
  },
  {
    "text": "enforce something mechanically it it effectively becomes uh unenforcable so",
    "start": "218280",
    "end": "223400"
  },
  {
    "text": "it's really hard to make people do things uh in this",
    "start": "223400",
    "end": "229599"
  },
  {
    "text": "way uh so moving on to Ray our our primitive isn't a pipeline it's just a job uh this makes integration into",
    "start": "229599",
    "end": "237920"
  },
  {
    "text": "orchestration systems like airflow uh significantly less complex and and a",
    "start": "237920",
    "end": "243239"
  },
  {
    "text": "lot easier to reason about moving on to Shared code",
    "start": "243239",
    "end": "251480"
  },
  {
    "text": "abstractions uh the Primitive and Cube flow again is a pipeline uh and that pipeline orchestrates containerized",
    "start": "251480",
    "end": "258199"
  },
  {
    "text": "components so code is shared at the component level in order to build a component on a",
    "start": "258199",
    "end": "265520"
  },
  {
    "text": "qflow pipelines V1 which is what reddit's training platform uh was built on a user has to implement the component",
    "start": "265520",
    "end": "274000"
  },
  {
    "text": "logic uh with argument parsing uh they need to write a yaml configuration and Implement a Docker",
    "start": "274000",
    "end": "281160"
  },
  {
    "text": "file this ends up being a a lot of code uh almost 150 lines of code for very",
    "start": "281160",
    "end": "287680"
  },
  {
    "text": "very simple components like date maath uh and if it's hard to do simple things",
    "start": "287680",
    "end": "293199"
  },
  {
    "text": "like users just aren't going to do them uh with Ray uh shared code is is",
    "start": "293199",
    "end": "299479"
  },
  {
    "text": "just code um admittedly I think hlow got a little bit better uh at this in V2 but",
    "start": "299479",
    "end": "306680"
  },
  {
    "text": "we can ignore that for now because it makes this point much less uh",
    "start": "306680",
    "end": "311880"
  },
  {
    "text": "dramatic so as mlet Reddit matures distributed compute for training",
    "start": "313440",
    "end": "319800"
  },
  {
    "text": "hyperparameter tuning and batch inference become much more important the qf flow ecosystem has",
    "start": "319800",
    "end": "326440"
  },
  {
    "text": "components like the training operator and C that you can layer on top of Q",
    "start": "326440",
    "end": "331680"
  },
  {
    "text": "flow pipelines to enable distributed compute but this just means more infrastructure components more",
    "start": "331680",
    "end": "338720"
  },
  {
    "text": "configuration uh and more not great abstractions that the platform team has to build around uh but this is Ray bread",
    "start": "338720",
    "end": "346039"
  },
  {
    "text": "and butter uh with Ray we just get this for free this is what it does the other main reason we decid to",
    "start": "346039",
    "end": "353400"
  },
  {
    "text": "build on Ray uh was that we believe it provides a much stronger foundation for",
    "start": "353400",
    "end": "359039"
  },
  {
    "text": "building a great developer experience for ML uh what do we think is a great",
    "start": "359039",
    "end": "364520"
  },
  {
    "text": "developer experience well it's one that feels local users primarily work on writing their job code with uh minimal",
    "start": "364520",
    "end": "372160"
  },
  {
    "text": "or no Docker builds pushes and pulls minimal configuration uh they should be able to",
    "start": "372160",
    "end": "378400"
  },
  {
    "text": "just write some code and run it against the resources that that job",
    "start": "378400",
    "end": "383840"
  },
  {
    "text": "requires this was our developer loop with Cube flow at a a very high level",
    "start": "383840",
    "end": "390520"
  },
  {
    "text": "uh users write their component and pipeline code uh implementing the required Docker files and component",
    "start": "390520",
    "end": "397960"
  },
  {
    "text": "configurations uh when the user's ready to run their job all the component images are built and",
    "start": "397960",
    "end": "405160"
  },
  {
    "text": "pushed and then for each component in the pipeline you know uh resources are",
    "start": "405160",
    "end": "410360"
  },
  {
    "text": "provisioned in kubernetes images for that component are pulled and the",
    "start": "410360",
    "end": "415479"
  },
  {
    "text": "component is run and that runs repeatedly for the duration for the the structure of your",
    "start": "415479",
    "end": "422080"
  },
  {
    "text": "pipeline if there's a bug in the code or the user is on to the next iteration of their experimentation they come back to",
    "start": "422080",
    "end": "429199"
  },
  {
    "text": "the beginning uh when you build your ml platform on top of a system that is primarily a container orchestrator it's",
    "start": "429199",
    "end": "437080"
  },
  {
    "text": "it's very very hard to avoid containers uh during development uh with Ray our developer",
    "start": "437080",
    "end": "444280"
  },
  {
    "text": "loop again at a very high level looks something like this uh a user uh provision array cluster uh at which time",
    "start": "444280",
    "end": "452800"
  },
  {
    "text": "resources are provisioned using a pre-built Docker image the user develops their job code",
    "start": "452800",
    "end": "460080"
  },
  {
    "text": "uh writing a minimal amount of configuration for their job they submit their job to their Ray cluster using",
    "start": "460080",
    "end": "466479"
  },
  {
    "text": "Dynamic runtime environments that install their job's dependency at runtime and the job runs if there's a",
    "start": "466479",
    "end": "473240"
  },
  {
    "text": "bug in the code or the user is onto the next iteration of their experimentation",
    "start": "473240",
    "end": "478319"
  },
  {
    "text": "they simply come back to their job code make changes and",
    "start": "478319",
    "end": "483560"
  },
  {
    "start": "483000",
    "end": "831000"
  },
  {
    "text": "submit all right so that's the why let's move on to the",
    "start": "483560",
    "end": "490199"
  },
  {
    "text": "how at Reddit we use open source Ray on kubernetes with",
    "start": "490199",
    "end": "495520"
  },
  {
    "text": "cubay we have a non-production and production gke cluster where we run rray",
    "start": "495520",
    "end": "501280"
  },
  {
    "text": "workloads those clusters are managed by reddit's uh core infrastructure",
    "start": "501280",
    "end": "506960"
  },
  {
    "text": "platform when a team onboards onto our training platform they provision what we",
    "start": "506960",
    "end": "512360"
  },
  {
    "text": "refer to as their array environment this is effectively uh a bag of",
    "start": "512360",
    "end": "517919"
  },
  {
    "text": "infrastructure uh some gcp stuff like projects big query uh data sets bucket",
    "start": "517919",
    "end": "524240"
  },
  {
    "text": "service accounts some kubernetes stuff uh like name spaces and services account",
    "start": "524240",
    "end": "529399"
  },
  {
    "text": "uh service accounts um and some configuration uh for reddits",
    "start": "529399",
    "end": "535480"
  },
  {
    "text": "internet we've also created uh an abstraction that we call array node",
    "start": "535480",
    "end": "540600"
  },
  {
    "text": "class which can kind of be thought of as a bundle of resources we actually don't let users right now specify uh resources",
    "start": "540600",
    "end": "548920"
  },
  {
    "text": "directly for their worker node but instead they choose uh one of our provided Ray node classes and a number",
    "start": "548920",
    "end": "555600"
  },
  {
    "text": "of workers uh that they they want to run we found that this approach uh helps a",
    "start": "555600",
    "end": "561320"
  },
  {
    "text": "lot with like utilization and efficiency especially when uh your users aren't",
    "start": "561320",
    "end": "567519"
  },
  {
    "text": "super comfortable thinking about you know resource mixes and things like",
    "start": "567519",
    "end": "573440"
  },
  {
    "text": "that we call our main development and experimentation product Ray",
    "start": "573640",
    "end": "579200"
  },
  {
    "text": "workspaces users can provision and manage their Ray workspace using uh a",
    "start": "579200",
    "end": "584600"
  },
  {
    "text": "CLI tool that we've built uh array workspace is effectively",
    "start": "584600",
    "end": "590480"
  },
  {
    "text": "a semilong running Ray cluster with a persistent uh volume Mount where users",
    "start": "590480",
    "end": "597040"
  },
  {
    "text": "can access Jupiter lab running on the head node via reddits internet uh or",
    "start": "597040",
    "end": "602320"
  },
  {
    "text": "they can submit uh remote job executions from their local",
    "start": "602320",
    "end": "607399"
  },
  {
    "text": "machine I say it's semi- long running because we we Leverage The Ray Auto scaler to spin uh worker nodes down",
    "start": "607399",
    "end": "614600"
  },
  {
    "text": "automatically after after a period of inactivity with workspaces we deploy Ray",
    "start": "614600",
    "end": "621600"
  },
  {
    "text": "clusters with pre-built uh base Docker images uh and like I was talking about before we leverage Dynamic runtime",
    "start": "621600",
    "end": "628399"
  },
  {
    "text": "environments to avoid Docker on the development Loop and in term of debugging users can",
    "start": "628399",
    "end": "635040"
  },
  {
    "text": "use uh the ray dashboard via the Internet uh and we also uh let users",
    "start": "635040",
    "end": "641760"
  },
  {
    "text": "leverage debug tools like the ray debugger and test timeline C profile uh",
    "start": "641760",
    "end": "647320"
  },
  {
    "text": "and the tensor board",
    "start": "647320",
    "end": "650480"
  },
  {
    "text": "profiler this is a quick screen grab of our Ray workspace CLI tooling",
    "start": "652519",
    "end": "659639"
  },
  {
    "text": "uh we provide basic commands for cluster management uh users can create clusters",
    "start": "659639",
    "end": "665000"
  },
  {
    "text": "delete them update them uh they can get the state of the cluster by describing it uh as well we also have a few utility",
    "start": "665000",
    "end": "673079"
  },
  {
    "text": "commands these commands uh are generally for development tests that require",
    "start": "673079",
    "end": "678120"
  },
  {
    "text": "executing commands on the head node of the ray cluster uh so we use this to essentially abstracts uh access to the",
    "start": "678120",
    "end": "686200"
  },
  {
    "text": "Head node that's running on kubernetes",
    "start": "686200",
    "end": "690320"
  },
  {
    "text": "we can uh zoom into the ray workspace create command uh this is relatively",
    "start": "692760",
    "end": "698320"
  },
  {
    "text": "simple most notably users configure their Ray and python versions uh we let",
    "start": "698320",
    "end": "703680"
  },
  {
    "text": "them have some flexibility on their head node resources uh and they can choose a",
    "start": "703680",
    "end": "709040"
  },
  {
    "text": "worker node class and a worker node count there's some other stuff",
    "start": "709040",
    "end": "714440"
  },
  {
    "text": "too and users can also use the CLI to view the available rate node classes and",
    "start": "714440",
    "end": "720480"
  },
  {
    "text": "their Associated resources we Center node classes around the type and number of GPU as this is",
    "start": "720480",
    "end": "727639"
  },
  {
    "text": "generally the main consideration for uh the majority of our users uh and then",
    "start": "727639",
    "end": "733199"
  },
  {
    "text": "which with each class we we provide you know what we feel is an appropriate amount of CPU and RAM to to go with a",
    "start": "733199",
    "end": "740920"
  },
  {
    "text": "job that would require those GPU",
    "start": "740920",
    "end": "744920"
  },
  {
    "text": "resources uh okay so when it comes to users job code uh we spent some time",
    "start": "747560",
    "end": "753639"
  },
  {
    "text": "defining what we call the ray job spec uh the spec dictates an opinionated uh",
    "start": "753639",
    "end": "760959"
  },
  {
    "text": "Reddit shaped Ray job that workloads must conform to to be able to be",
    "start": "760959",
    "end": "766120"
  },
  {
    "text": "successfully submitted for execution on the platform the spec is relatively simple",
    "start": "766120",
    "end": "772680"
  },
  {
    "text": "uh it just requires that a job has a python script uh that has the entry",
    "start": "772680",
    "end": "778040"
  },
  {
    "text": "point for the job uh a p project toml and poetry lock file",
    "start": "778040",
    "end": "783959"
  },
  {
    "text": "specifying the jobs environment we use uh poetry for dependency",
    "start": "783959",
    "end": "789160"
  },
  {
    "text": "management and then it requires three very basic configuration files which",
    "start": "789160",
    "end": "794360"
  },
  {
    "text": "specify how to build the image for the job uh how to provision the underlying",
    "start": "794360",
    "end": "799680"
  },
  {
    "text": "Ray cluster for the job and how to actually uh execute the",
    "start": "799680",
    "end": "805199"
  },
  {
    "text": "job and while while I've talked a lot about avoiding exposing Docker to our",
    "start": "805199",
    "end": "811160"
  },
  {
    "text": "users at all costs uh we do provide a few Escape hatches that let users bring",
    "start": "811160",
    "end": "817240"
  },
  {
    "text": "a custom Docker file as long as it um uses one of our supported base images uh",
    "start": "817240",
    "end": "824360"
  },
  {
    "text": "and they would use this for use cases that require things like additional system dependencies and and things like",
    "start": "824360",
    "end": "832160"
  },
  {
    "start": "831000",
    "end": "951000"
  },
  {
    "text": "that so earlier we were talking about shared code with Ray we formalized a few",
    "start": "832160",
    "end": "839519"
  },
  {
    "text": "different concepts around sharing code um so array project at Reddit is",
    "start": "839519",
    "end": "846079"
  },
  {
    "text": "the primary form factor for array workload uh array project consists of",
    "start": "846079",
    "end": "852079"
  },
  {
    "text": "one or more array jobs so the concept of a project library is that it is shared",
    "start": "852079",
    "end": "858639"
  },
  {
    "text": "code at the project level that is used by all of the jobs in the project uh or",
    "start": "858639",
    "end": "863880"
  },
  {
    "text": "some of the jobs in the project this code is treated as local modules uh so",
    "start": "863880",
    "end": "870360"
  },
  {
    "text": "this is not versioned and packaged and published this is treated as local modules and when we execute a job it",
    "start": "870360",
    "end": "876440"
  },
  {
    "text": "always executes from the context of the project that it's in this gives users uh",
    "start": "876440",
    "end": "882120"
  },
  {
    "text": "a lot of flexibility for just kind of iterating on these libraries without kind of these build version publish",
    "start": "882120",
    "end": "889600"
  },
  {
    "text": "Cycles uh shared libraries are code that shared across different Ray projects uh",
    "start": "889600",
    "end": "895600"
  },
  {
    "text": "these are version packaged and published as private python packages and jobs",
    "start": "895600",
    "end": "900639"
  },
  {
    "text": "install these via poetry with other external libraries uh the ml platform",
    "start": "900639",
    "end": "905720"
  },
  {
    "text": "team provides CI abstractions that make it very very easy for users to publish",
    "start": "905720",
    "end": "911279"
  },
  {
    "text": "and manage these types of libraries and finally we have the uh",
    "start": "911279",
    "end": "916480"
  },
  {
    "text": "Gazette Ray SDK which is uh an SDK that the ml platform team owns which provides",
    "start": "916480",
    "end": "923360"
  },
  {
    "text": "uh platform level Integrations and functionality so this would be things like uh functions to determine if a job",
    "start": "923360",
    "end": "930600"
  },
  {
    "text": "is executing in the nonpr or the prod environment um and again things like",
    "start": "930600",
    "end": "935920"
  },
  {
    "text": "this uh make it very easy to kind of just develop your job once and run it in non-prod or run it in prod uh without",
    "start": "935920",
    "end": "943360"
  },
  {
    "text": "code changes because you can just uh write control flow Logic for you know",
    "start": "943360",
    "end": "948759"
  },
  {
    "text": "accessing different things in different environments okay one of the most",
    "start": "948759",
    "end": "955519"
  },
  {
    "start": "951000",
    "end": "1043000"
  },
  {
    "text": "important things probably the most important thing for enabling High development velocity in ml is",
    "start": "955519",
    "end": "962079"
  },
  {
    "text": "smoothly Bridging the Gap between development and production uh Reddit ml platform team",
    "start": "962079",
    "end": "968279"
  },
  {
    "text": "provides CLI tooling that helps us do exactly this uh by making making it very",
    "start": "968279",
    "end": "974440"
  },
  {
    "text": "very easy for users to develop their job in the constraints of the ray job spec",
    "start": "974440",
    "end": "982240"
  },
  {
    "text": "so when they're writing their code it is being written in the way that it needs to be for it to run in production",
    "start": "982240",
    "end": "989680"
  },
  {
    "text": "and we do this via a a ray job submit wrapper command if you will and",
    "start": "989680",
    "end": "995040"
  },
  {
    "text": "effectively what this does is uh the user runs it on their local machine pointing to their job uh we start and",
    "start": "995040",
    "end": "1001759"
  },
  {
    "text": "manage a background process proxying the remote raay cluster uh we parse the job",
    "start": "1001759",
    "end": "1007680"
  },
  {
    "text": "dependencies and build a dynamic runtime environment uh from that we parse the",
    "start": "1007680",
    "end": "1013000"
  },
  {
    "text": "entry points arguments and environment variables from the config and inject those into the the dynamic run time",
    "start": "1013000",
    "end": "1019560"
  },
  {
    "text": "environment uh and finally we build uh a ray job submit command and and execute",
    "start": "1019560",
    "end": "1024880"
  },
  {
    "text": "the command against the users's uh Ray workor space cluster so effectively we",
    "start": "1024880",
    "end": "1030798"
  },
  {
    "text": "are executing the job in a development context using literally the same source",
    "start": "1030799",
    "end": "1036678"
  },
  {
    "text": "of Truth for the dependency the job that we would effectively run in in",
    "start": "1036679",
    "end": "1043678"
  },
  {
    "start": "1043000",
    "end": "1112000"
  },
  {
    "text": "production and this brings us to production uh so in production we absolutely do want Docker it is a",
    "start": "1043919",
    "end": "1050360"
  },
  {
    "text": "requirement so users version build and release job images via git tags uh in",
    "start": "1050360",
    "end": "1056480"
  },
  {
    "text": "our Ray jobs monor repo where the ml platform team provides cicd abstractions",
    "start": "1056480",
    "end": "1061640"
  },
  {
    "text": "for building and Publishing job images we have a control plan for scheduling uh Ray job executions which",
    "start": "1061640",
    "end": "1069600"
  },
  {
    "text": "consist of a lightweight API server and a custom uh Gazette Ray job kubernetes",
    "start": "1069600",
    "end": "1076880"
  },
  {
    "text": "controller uh the controller allows us to orchestrate the cuay ray job with",
    "start": "1076880",
    "end": "1082919"
  },
  {
    "text": "other required infrastructure components um such as like Prometheus uh pod",
    "start": "1082919",
    "end": "1089400"
  },
  {
    "text": "monitors for uh ingesting metrics from that job into reddits observability",
    "start": "1089400",
    "end": "1096280"
  },
  {
    "text": "system uh and I mentioned earlier that airflow is reddits deao workflow orchestration system and we opted to",
    "start": "1096280",
    "end": "1104080"
  },
  {
    "text": "meet uh our users where they are with a custom airflow operator for for uh",
    "start": "1104080",
    "end": "1109960"
  },
  {
    "text": "executing Ray jobs so putting that all together uh you get something that looks roughly like",
    "start": "1109960",
    "end": "1116559"
  },
  {
    "start": "1112000",
    "end": "1163000"
  },
  {
    "text": "this uh a user comes to the ray job monor repo and tags uh a",
    "start": "1116559",
    "end": "1121919"
  },
  {
    "text": "release uh we build an image with the jobs environment baked into it and",
    "start": "1121919",
    "end": "1127320"
  },
  {
    "text": "published that along with publishing the config uh when a user triggers an",
    "start": "1127320",
    "end": "1132960"
  },
  {
    "text": "execution in airflow um the airflow operator talks to the API server in our",
    "start": "1132960",
    "end": "1140200"
  },
  {
    "text": "control plane and then the API server creates a Gazette Ray job uh where the controller",
    "start": "1140200",
    "end": "1146360"
  },
  {
    "text": "manager orchestrates the deployment of the the job with the associated uh other",
    "start": "1146360",
    "end": "1152120"
  },
  {
    "text": "you know infrastructure related resources uh and then the operator can pull the API server to sync the status",
    "start": "1152120",
    "end": "1159000"
  },
  {
    "text": "of the job with uh the airf flow task status digging into the airflow operator",
    "start": "1159000",
    "end": "1166520"
  },
  {
    "text": "real quick the API for users is extremely ex simple uh users simply specify their job's project name and",
    "start": "1166520",
    "end": "1172480"
  },
  {
    "text": "version we support till day version uh ranges so that users can make minor and",
    "start": "1172480",
    "end": "1178760"
  },
  {
    "text": "Patch releases to their job without needing to re-release their airflow dags um and the config is versioned and",
    "start": "1178760",
    "end": "1186200"
  },
  {
    "text": "published with the job but we also allow um some Dynamic overrides for for the",
    "start": "1186200",
    "end": "1191360"
  },
  {
    "text": "job in cluster config uh so what happened when users moved to Ray uh tldr a lot of good stuff",
    "start": "1191360",
    "end": "1200320"
  },
  {
    "start": "1193000",
    "end": "1243000"
  },
  {
    "text": "so for our large rexus models moving to distributed training on Ray reduced",
    "start": "1200320",
    "end": "1205760"
  },
  {
    "text": "training Time by almost in order of magnitude at significantly lower costs",
    "start": "1205760",
    "end": "1211120"
  },
  {
    "text": "uh we the improved developer experience enabled our safety team to train you know between five and 10 more models per",
    "start": "1211120",
    "end": "1218000"
  },
  {
    "text": "month and one of our ml research Focus teams to fine-tune hundreds of llms in",
    "start": "1218000",
    "end": "1224000"
  },
  {
    "text": "just a few days and unlocking llm fine tuning on right also created a lot of value for us",
    "start": "1224000",
    "end": "1230559"
  },
  {
    "text": "because what we found is we're able to fine-tune much smaller uh open-source",
    "start": "1230559",
    "end": "1235880"
  },
  {
    "text": "models and get better performance than the out of thebox large thirdparty Foundation",
    "start": "1235880",
    "end": "1243200"
  },
  {
    "text": "models all right well we've been well we've seen really great results but we have a lot of work to do um our big",
    "start": "1244200",
    "end": "1251000"
  },
  {
    "text": "priorities are improve multi- tendency uh we need to be able to kind of create virtual quotas of our shared GPU pools",
    "start": "1251000",
    "end": "1259039"
  },
  {
    "text": "uh we want to invest in job queuing with priority preemption and gang scheduling ultimately this will improve scheduling",
    "start": "1259039",
    "end": "1265840"
  },
  {
    "text": "under resource constraint and also uh help us with utilization uh and we also want to",
    "start": "1265840",
    "end": "1271480"
  },
  {
    "text": "invest in improvements around heterogeneous clusters right now users can only spin up one uh worker node",
    "start": "1271480",
    "end": "1278480"
  },
  {
    "text": "group and so enabling more worker node groups just creates more flexibility in the shapes of clusters that they can",
    "start": "1278480",
    "end": "1285640"
  },
  {
    "text": "build all right cool let's quickly talk about our experience with RAC serve uh this is going to be much quicker this is",
    "start": "1285640",
    "end": "1292159"
  },
  {
    "text": "much more nent at Reddit um so Reddit has a very mature inference system but we were seeing uh",
    "start": "1292159",
    "end": "1299279"
  },
  {
    "text": "ml growing in both scaling complexity when it came to serving models uh more",
    "start": "1299279",
    "end": "1304360"
  },
  {
    "text": "models are being trained at Reddit that require GPU to serve its scale uh we're seeing use cases with much more complex",
    "start": "1304360",
    "end": "1311760"
  },
  {
    "text": "model Handler logic uh use cases involving multimedia content understanding uh and of course uh",
    "start": "1311760",
    "end": "1318720"
  },
  {
    "text": "there's lolms we explored racer for a few Greenfield use cases and we found it was",
    "start": "1318720",
    "end": "1325880"
  },
  {
    "start": "1320000",
    "end": "1359000"
  },
  {
    "text": "a very good fit for a lot of reasons um we have a few asynchronous ml serving",
    "start": "1325880",
    "end": "1331919"
  },
  {
    "text": "workloads and race serve makes unlocking higher throughput with Dynamic request batching extremely",
    "start": "1331919",
    "end": "1338360"
  },
  {
    "text": "simple uh it's also extremely simple to express more complex model serving logic",
    "start": "1338360",
    "end": "1344480"
  },
  {
    "text": "uh with model composition and um finally we are leaning into VM for serving and so we",
    "start": "1344480",
    "end": "1351600"
  },
  {
    "text": "found that using VM in the context of race serve uh enabled us to get even",
    "start": "1351600",
    "end": "1357080"
  },
  {
    "text": "better performance we did explore cubra for serving uh which we are using heavily",
    "start": "1357080",
    "end": "1364480"
  },
  {
    "start": "1359000",
    "end": "1384000"
  },
  {
    "text": "for training but we actually decided this wasn't necessary for our current level of maturity we don't really have",
    "start": "1364480",
    "end": "1370360"
  },
  {
    "text": "like multi- Noe distributed serving use cases but this is something we want to look into um and recent improvements in",
    "start": "1370360",
    "end": "1377640"
  },
  {
    "text": "racer has made it a lot more reliable on cubra uh so it's definitely trending in",
    "start": "1377640",
    "end": "1382880"
  },
  {
    "text": "a very good direction all right now let's look at some examples of how we're using it we",
    "start": "1382880",
    "end": "1389640"
  },
  {
    "text": "have a light development flow um we make it very easy to develop new Ray services",
    "start": "1389640",
    "end": "1394880"
  },
  {
    "text": "our ml serving team provides a VM based service wrapper uh and then ml can",
    "start": "1394880",
    "end": "1400600"
  },
  {
    "text": "simply Define their model Handler with custom argu Uh custom arguments and they",
    "start": "1400600",
    "end": "1406159"
  },
  {
    "text": "can actually develop that Handler intera L against the same Ray work spaces that",
    "start": "1406159",
    "end": "1411520"
  },
  {
    "text": "we use for um the dev environment for training and then uh new model S can",
    "start": "1411520",
    "end": "1418200"
  },
  {
    "text": "easily be deployed just with uh simple config changes all model servers leverage",
    "start": "1418200",
    "end": "1425600"
  },
  {
    "start": "1422000",
    "end": "1442000"
  },
  {
    "text": "tensor parallel uh inference across multiple gpus on a single node Ray makes",
    "start": "1425600",
    "end": "1431840"
  },
  {
    "text": "it incredibly simple to do this with VM by just configuring placement group",
    "start": "1431840",
    "end": "1437919"
  },
  {
    "text": "bundles uh inside of the ray the ray serve deployment and like I mentioned we also",
    "start": "1437919",
    "end": "1444240"
  },
  {
    "start": "1442000",
    "end": "1471000"
  },
  {
    "text": "make heavy use of dynamic batching for asynchronous serving use cases uh this is a a simple example of us using it in",
    "start": "1444240",
    "end": "1452600"
  },
  {
    "text": "the context of one of our text classification pipelines uh you simply Define the async batch Handler uh it",
    "start": "1452600",
    "end": "1459640"
  },
  {
    "text": "needs to take in and return a list of your models input and output uh and then",
    "start": "1459640",
    "end": "1464960"
  },
  {
    "text": "simply you configure the max batch size and and weight time and",
    "start": "1464960",
    "end": "1472399"
  },
  {
    "start": "1471000",
    "end": "1512000"
  },
  {
    "text": "go but probably one of the most appealing aspects of racer is just how easy it is to express complex model",
    "start": "1472600",
    "end": "1479360"
  },
  {
    "text": "server uh Handler logic uh here we have an example of a",
    "start": "1479360",
    "end": "1484440"
  },
  {
    "text": "race serve deployment at Reddit that performs inference over a few different models that are sharded by subreddit and",
    "start": "1484440",
    "end": "1492000"
  },
  {
    "text": "the expressive Handler interface allows us to Simply maintain a dictionary of Reserve applications and an index of",
    "start": "1492000",
    "end": "1499480"
  },
  {
    "text": "subreddit shards and when performing inference we can simply look up A Shard for a given subreddit get the model uh",
    "start": "1499480",
    "end": "1506440"
  },
  {
    "text": "for that Shard and make a remote call uh to that model to get the",
    "start": "1506440",
    "end": "1512480"
  },
  {
    "start": "1512000",
    "end": "1545000"
  },
  {
    "text": "results one of our main use cases of racer is also to host a number of",
    "start": "1512960",
    "end": "1518240"
  },
  {
    "text": "internal deployments of open- source llms for uh applied ML and researchers to experiment with uh the team provides",
    "start": "1518240",
    "end": "1526640"
  },
  {
    "text": "an llm playground UI for quick and iterative exploration uh as well as API",
    "start": "1526640",
    "end": "1531880"
  },
  {
    "text": "access for testing applications built on top of llms uh and beyond that the serving team",
    "start": "1531880",
    "end": "1538320"
  },
  {
    "text": "has also made it very easy to spin up dedicated uh self-hosted llms for",
    "start": "1538320",
    "end": "1543399"
  },
  {
    "text": "specific production use cases here's a quick screen grab of the",
    "start": "1543399",
    "end": "1548840"
  },
  {
    "start": "1545000",
    "end": "1564000"
  },
  {
    "text": "LL llm playground UI a user can simply uh experiment uh by selecting an llm",
    "start": "1548840",
    "end": "1557760"
  },
  {
    "text": "typing their prompt putting in their their tuning generation parameters and",
    "start": "1557760",
    "end": "1564600"
  },
  {
    "start": "1564000",
    "end": "1617000"
  },
  {
    "text": "generate so much like with training we've seen some very positive results in our early uh use of RAC serve uh moving",
    "start": "1564919",
    "end": "1572640"
  },
  {
    "text": "some of our larger asynchronous text classification use cases uh to RAC serve",
    "start": "1572640",
    "end": "1578159"
  },
  {
    "text": "with gpus and dynamic batching increased uh throughput by almost 10x at 10x less",
    "start": "1578159",
    "end": "1584760"
  },
  {
    "text": "cost uh we can do much more for much less uh our racer infrastructure enabled",
    "start": "1584760",
    "end": "1590760"
  },
  {
    "text": "us to host uh large complex media understanding models inhouse uh which ended us saving us hundreds of thousands",
    "start": "1590760",
    "end": "1597840"
  },
  {
    "text": "of dollars per year uh and finally during our most recent snooze week",
    "start": "1597840",
    "end": "1603799"
  },
  {
    "text": "hackathon uh the llm playground API saw about 400,000 uh requests um which is",
    "start": "1603799",
    "end": "1610600"
  },
  {
    "text": "just a good signal for how easy we've made it for folks to experiment with open source self-hosted llms",
    "start": "1610600",
    "end": "1618640"
  },
  {
    "start": "1617000",
    "end": "2127000"
  },
  {
    "text": "and that's it thank you so much for coming and uh Shameless plug Reddit is hiring for our ml platform team and",
    "start": "1618640",
    "end": "1625880"
  },
  {
    "text": "other ml teams that Reddit so if you liked what you heard uh feel free to reach out to me uh or apply thank",
    "start": "1625880",
    "end": "1634640"
  },
  {
    "text": "you thanks for that great talk about training and serving at Reddit uh for",
    "start": "1636640",
    "end": "1642039"
  },
  {
    "text": "questions please come to the middle aisle and ask them at the mic thank you thank you so much",
    "start": "1642039",
    "end": "1649200"
  },
  {
    "text": "hi thank you for your presentation so I have a question regarding the cluster management specifically so you mentioned",
    "start": "1649200",
    "end": "1655200"
  },
  {
    "text": "redit runs semi long running cluster right but then in the presentation in the r job specs it seems like you also",
    "start": "1655200",
    "end": "1662720"
  },
  {
    "text": "need to specify the cluster configuration how does it TI to reusing the cluster across multiple jobs yeah so",
    "start": "1662720",
    "end": "1669720"
  },
  {
    "text": "that's a good question so in the development environments we kind of have a oneto one mapping where you like",
    "start": "1669720",
    "end": "1675200"
  },
  {
    "text": "request your resources you have that and you can use that until you're inactive on it in production we're running",
    "start": "1675200",
    "end": "1682840"
  },
  {
    "text": "completely ephemeral aray clusters so for every job that's executed we're spinning up uh array cluster",
    "start": "1682840",
    "end": "1689279"
  },
  {
    "text": "specifically for that job and it's torn down at the end so it's really the split between Dev and prod uh we are going to",
    "start": "1689279",
    "end": "1695960"
  },
  {
    "text": "be looking into as we go into queuing and prioritization and stuff we're probably going to bring some of that",
    "start": "1695960",
    "end": "1702600"
  },
  {
    "text": "prod workflow into the dev path when you just want to like submit your job and have it run whenever there's enough",
    "start": "1702600",
    "end": "1708760"
  },
  {
    "text": "resources for it to run yep um would you consider open",
    "start": "1708760",
    "end": "1714679"
  },
  {
    "text": "sourcing The Raid job orchestrator um I would say it's not at",
    "start": "1714679",
    "end": "1721200"
  },
  {
    "text": "the top of mine right now but I do think Reddit as a company is a proponent of",
    "start": "1721200",
    "end": "1726640"
  },
  {
    "text": "using and contributing to open source so uh open sourcing parts of our platform is something we've talked about but",
    "start": "1726640",
    "end": "1732679"
  },
  {
    "text": "realistically it would probably not be in the next uh one or two years",
    "start": "1732679",
    "end": "1739720"
  },
  {
    "text": "hey thanks for an awesome talk you mentioned you had the Jupiter lab running on a ray head node does that",
    "start": "1739720",
    "end": "1745640"
  },
  {
    "text": "mean you're using a Jupiter notebook to spin up another gray work worker group",
    "start": "1745640",
    "end": "1751399"
  },
  {
    "text": "to then process a job that you schedule no so um the basically Jupiter is",
    "start": "1751399",
    "end": "1757640"
  },
  {
    "text": "running on the head node and so it you can execute uh remote raid tests from that",
    "start": "1757640",
    "end": "1765480"
  },
  {
    "text": "Jupiter uh notebook in kind of like the the not as ideal way of submitting Ray",
    "start": "1765480",
    "end": "1772720"
  },
  {
    "text": "tests which is through the ray job API this is kind of more direct communication uh with the ray cluster so",
    "start": "1772720",
    "end": "1780440"
  },
  {
    "text": "you are basically doing interactive development in the ray cluster where you are submitting test to um so you're not",
    "start": "1780440",
    "end": "1787200"
  },
  {
    "text": "spitting up new workers um if you had a scaling group configured and you needed to get more resources in that cluster to",
    "start": "1787200",
    "end": "1794440"
  },
  {
    "text": "execute that task it would scale up to do that but there is no kind of cluster",
    "start": "1794440",
    "end": "1800240"
  },
  {
    "text": "Management in the Jupiter layer that's just for like interactive development gotcha so did you have to end up",
    "start": "1800240",
    "end": "1806480"
  },
  {
    "text": "splitting the notebook usable kind of Ray cluster and whatever you use for",
    "start": "1806480",
    "end": "1811919"
  },
  {
    "text": "your more of your long running or heavier workloads to preserve node CL",
    "start": "1811919",
    "end": "1817440"
  },
  {
    "text": "classes uh effectively yes the Jupiter can only be accessed in Dev and it only",
    "start": "1817440",
    "end": "1822559"
  },
  {
    "text": "runs on the head node and we prevent any other test from being scheduled on the head node",
    "start": "1822559",
    "end": "1828440"
  },
  {
    "text": "hey uh really great talk loved everything uh when you mentioned the the",
    "start": "1828440",
    "end": "1834480"
  },
  {
    "text": "race serve with Dynamic batching I wonder like what type of throughput uh you can achieve with that uh like in",
    "start": "1834480",
    "end": "1841000"
  },
  {
    "text": "like queries per day or something like that so I think it would depend on the",
    "start": "1841000",
    "end": "1846960"
  },
  {
    "text": "use case and the scale I think we're we're getting like hundreds of",
    "start": "1846960",
    "end": "1852080"
  },
  {
    "text": "inferences per second over like one GPU and I don't even think it's very well saturated so I I do think you can you",
    "start": "1852080",
    "end": "1860880"
  },
  {
    "text": "can get very good throughput and the more you can use tensor parallel uh",
    "start": "1860880",
    "end": "1866399"
  },
  {
    "text": "inference or extend to multi Noe inference uh or just have more workers and scale your throughput up that way",
    "start": "1866399",
    "end": "1872840"
  },
  {
    "text": "there are ways to get this pretty high so did you have something uh that had",
    "start": "1872840",
    "end": "1878519"
  },
  {
    "text": "that did batching before you you switched to Dynamic batching so something like a Flink or something like",
    "start": "1878519",
    "end": "1884480"
  },
  {
    "text": "that that did batching so we um so no we introduced Dynamic batching when we",
    "start": "1884480",
    "end": "1890519"
  },
  {
    "text": "introduced Ray we have other model servers running in Triton where we're likely going to experiment in Dynamic",
    "start": "1890519",
    "end": "1897399"
  },
  {
    "text": "batching but we don't really currently have data comparing uh two Frameworks thank you",
    "start": "1897399",
    "end": "1904720"
  },
  {
    "text": "yep how do you figure out what kinds of options you provide in a CLI tool like Gazette for like engineers at Reddit so",
    "start": "1904720",
    "end": "1912919"
  },
  {
    "text": "you mean like what flags and what to expose and like how they provision their clusters yeah like which flags to expose",
    "start": "1912919",
    "end": "1919200"
  },
  {
    "text": "which ones you hire yeah I think we've tried to just take a very minimal",
    "start": "1919200",
    "end": "1924360"
  },
  {
    "text": "approach and say what's the least amount of things we can Expose and then as we've added new features things have",
    "start": "1924360",
    "end": "1930519"
  },
  {
    "text": "layered on that so things like the ability to run a cluster in debug mode so you can leverage some of the debug",
    "start": "1930519",
    "end": "1937440"
  },
  {
    "text": "tooling um we don't want to do that by default but we want to let users uh do that other than that I think it's really",
    "start": "1937440",
    "end": "1944639"
  },
  {
    "text": "about thinking about what like what you want to constrain your users so what the",
    "start": "1944639",
    "end": "1949799"
  },
  {
    "text": "platform dictates versus what what kind of axes you want to provide flexibility",
    "start": "1949799",
    "end": "1955039"
  },
  {
    "text": "for your users okay um so yeah apologize in advance if",
    "start": "1955039",
    "end": "1963000"
  },
  {
    "text": "I missed somewhere with the question uh so regarding the Jupiter notebook and the fact that you can call remote for",
    "start": "1963000",
    "end": "1969360"
  },
  {
    "text": "the developer experience um have you had this need or the developers need to run",
    "start": "1969360",
    "end": "1974760"
  },
  {
    "text": "the code that they submit in a debugger so they could like step through it see the variables so yeah I would say",
    "start": "1974760",
    "end": "1982919"
  },
  {
    "text": "um debugging distributed workloads on Ray has probably been the the sharpest",
    "start": "1982919",
    "end": "1988760"
  },
  {
    "text": "change for our user base um when you go distribute it in any context it it's a",
    "start": "1988760",
    "end": "1995240"
  },
  {
    "text": "lot harder to reason about so that is a problem we're solving um I think we've",
    "start": "1995240",
    "end": "2000440"
  },
  {
    "text": "made some progress in exposing the tools that Ray provides and and taking more of",
    "start": "2000440",
    "end": "2006200"
  },
  {
    "text": "an education route and how folks can just learn more about using those uh but",
    "start": "2006200",
    "end": "2011240"
  },
  {
    "text": "it's definitely not a silver bullet we have a lot of requests to we don't currently allow gpus in our head node uh",
    "start": "2011240",
    "end": "2018600"
  },
  {
    "text": "but we do have a lot of requests to enable gpus in the head node so that folks can kind of do some quick",
    "start": "2018600",
    "end": "2023960"
  },
  {
    "text": "iteration locally and then kind of transition to distributed mode um it's something we actively still need to",
    "start": "2023960",
    "end": "2030760"
  },
  {
    "text": "figure out but my understanding is a lot of teams using Ray this is kind of a sharp edge for them I see I guess is it",
    "start": "2030760",
    "end": "2037519"
  },
  {
    "text": "the same about let's say uh you know follow the symbols as I'm typing I want to autoc comp something you get in an",
    "start": "2037519",
    "end": "2044519"
  },
  {
    "text": "editor yeah yeah exactly and I do think Ray has been the team has been improving",
    "start": "2044519",
    "end": "2049599"
  },
  {
    "text": "the the debugging tooling as well okay um and another question I had so",
    "start": "2049599",
    "end": "2054800"
  },
  {
    "text": "regarding the race serve that you were showing like you were storing the handles and depending on the input",
    "start": "2054800",
    "end": "2060480"
  },
  {
    "text": "submitted um have you well in a similar context we ran into a issue with load",
    "start": "2060480",
    "end": "2066240"
  },
  {
    "text": "balancing like uh r didn't really expose how it if there were like multiple workers behind a",
    "start": "2066240",
    "end": "2072280"
  },
  {
    "text": "certain API to um did youan any issue like you that load balancing was uh over",
    "start": "2072280",
    "end": "2079919"
  },
  {
    "text": "subscribing to one worker and like not so the API getting high P99 or something",
    "start": "2079919",
    "end": "2085839"
  },
  {
    "text": "like that or the um so I don't believe load balancing has been one of the",
    "start": "2085839",
    "end": "2091560"
  },
  {
    "text": "issues using the VM server in the context of Ray to kind of andc",
    "start": "2091560",
    "end": "2096679"
  },
  {
    "text": "configuring the placement groups has been pretty successful for us but we we have I would say we are still very nent",
    "start": "2096679",
    "end": "2102920"
  },
  {
    "text": "in our use and our use cases are relatively lows scale Evergreen use cases so I expect it's an issue we will",
    "start": "2102920",
    "end": "2110079"
  },
  {
    "text": "probably run into as well okay thank you uh what's the time situation okay",
    "start": "2110079",
    "end": "2115640"
  },
  {
    "text": "yeah yeah I'm I'll uh I'll come off over to the side if you want to come chat with me uh feel free thank you all for",
    "start": "2115640",
    "end": "2122240"
  },
  {
    "text": "coming appreciate it",
    "start": "2122240",
    "end": "2126040"
  }
]