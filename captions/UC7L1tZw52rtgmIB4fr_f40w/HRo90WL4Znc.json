[
  {
    "text": "so hi everyone my name is Hal and I'm currently a postdoc researcher at UC",
    "start": "3980",
    "end": "9720"
  },
  {
    "text": "Berkeley Skylab working with John stoica and welcome to the resale 2022 and welcome to my talk and in this talk I'm",
    "start": "9720",
    "end": "16800"
  },
  {
    "text": "going to tell you about a very very new system that we built at UC Berkeley on top of",
    "start": "16800",
    "end": "21840"
  },
  {
    "text": "Ray of course called Alpha that can automate the model parallel training and serving a large neural networks like",
    "start": "21840",
    "end": "28380"
  },
  {
    "text": "gbt3 and the project is a collaboration between UC Berkeley any skill and many",
    "start": "28380",
    "end": "33840"
  },
  {
    "text": "other institutes and before I dive into the detail I want to First share two links so the first link of the AI is our",
    "start": "33840",
    "end": "40620"
  },
  {
    "text": "project page and the second link opt.orpa.ai is a free online service",
    "start": "40620",
    "end": "45780"
  },
  {
    "text": "that you might be interested in so it provides a free unlimited gpt3 like tax generation service powered by Alpha",
    "start": "45780",
    "end": "52440"
  },
  {
    "text": "Henry and as far as we know this is the first free gpp3 text Journey service in",
    "start": "52440",
    "end": "58440"
  },
  {
    "text": "public because if you use open air service level charge for each token generated and the please visit this link and give",
    "start": "58440",
    "end": "65580"
  },
  {
    "text": "it a try and I guarantee you will find some interesting results okay and let's let's let us begin with some",
    "start": "65580",
    "end": "72659"
  },
  {
    "text": "background on large models so property has every of us here already know a lot",
    "start": "72659",
    "end": "77700"
  },
  {
    "text": "of recent ones in deep learning are enabled by large models and since 2018",
    "start": "77700",
    "end": "84540"
  },
  {
    "text": "the largest model size started to increase more than 10 times every year and two years ago open AI released the",
    "start": "84540",
    "end": "91439"
  },
  {
    "text": "famous gigantic gpd3 model with 175 billion parameters and this model is recognized that as one of the most",
    "start": "91439",
    "end": "98040"
  },
  {
    "text": "important machine learning breaks through and today the benefit of scanning model",
    "start": "98040",
    "end": "103740"
  },
  {
    "text": "size does not diminish and people are still trying to train larger and larger models and meanwhile logic models are",
    "start": "103740",
    "end": "109979"
  },
  {
    "text": "also made more and more accessible to the General Public so three months ago meta open source a",
    "start": "109979",
    "end": "115979"
  },
  {
    "text": "protein language model called opt with 175 billion parameters and making large",
    "start": "115979",
    "end": "121200"
  },
  {
    "text": "language models at this size publicly available for the first time and the bottom a month ago the big science",
    "start": "121200",
    "end": "127500"
  },
  {
    "text": "initiative open source another language model called Bloom with 170. 6 billion",
    "start": "127500",
    "end": "133260"
  },
  {
    "text": "parameters and another large language model in open source so as a machine learning developer how",
    "start": "133260",
    "end": "139440"
  },
  {
    "text": "can we Embrace large models while larger models can provide a lot of benefits it also poses a lot of challenges in",
    "start": "139440",
    "end": "146160"
  },
  {
    "text": "Computing and systems so to see the core challenge let me use a simple example so",
    "start": "146160",
    "end": "151260"
  },
  {
    "text": "many of today's machine learning programs often assume the model such as this bird can fit into the memory of a",
    "start": "151260",
    "end": "157440"
  },
  {
    "text": "single device like a GPU and with this assumption all the computation is conducted on a single device using",
    "start": "157440",
    "end": "164280"
  },
  {
    "text": "Frameworks like tensorflow and package without any problem however when the model becomes large",
    "start": "164280",
    "end": "170340"
  },
  {
    "text": "it's hard to fit the model or even a single layer of this model into its limited device memory",
    "start": "170340",
    "end": "175980"
  },
  {
    "text": "so how can we train and service big models right and the only possible way is to partition and parallelize the",
    "start": "175980",
    "end": "182099"
  },
  {
    "text": "speaker models on more devices however these steps requires a big amount of engineering effort that is",
    "start": "182099",
    "end": "188340"
  },
  {
    "text": "specific to both the model definition and the cluster system we design on top of array to",
    "start": "188340",
    "end": "196500"
  },
  {
    "text": "specifically facilitate the training and serving of such big models and hopefully after this talk you could be convinced",
    "start": "196500",
    "end": "202980"
  },
  {
    "text": "that Opera can help you better Embrace these big models in your research and development",
    "start": "202980",
    "end": "208019"
  },
  {
    "text": "so to introduce our par let me begin with the computational pattern of deep learning and first we have a bachelor",
    "start": "208019",
    "end": "214680"
  },
  {
    "text": "input images and we fit input into a deep learning model under the computation involves a forward propagation to the to compute the",
    "start": "214680",
    "end": "222060"
  },
  {
    "text": "predictions and then a backward propagation to compute the ingredients and update the model and now if we want to scale the",
    "start": "222060",
    "end": "228540"
  },
  {
    "text": "screening pattern we Face two problems so first what if the input data is very large for example we want to train our",
    "start": "228540",
    "end": "235319"
  },
  {
    "text": "minions of images and second what if the model or the model layers are very large for example",
    "start": "235319",
    "end": "241200"
  },
  {
    "text": "we want to train a model with opinions of parameters so the difficulty of these two problems",
    "start": "241200",
    "end": "246480"
  },
  {
    "text": "are quite different so the first problem is easy and has been studied pretty well it is called",
    "start": "246480",
    "end": "252659"
  },
  {
    "text": "the data parallelism and we can partition the input data and replicate the model so here are two gpus and we can",
    "start": "252659",
    "end": "258900"
  },
  {
    "text": "replicate the model on two gpus and then feed the different input patches to different gpus to paralyze this",
    "start": "258900",
    "end": "264840"
  },
  {
    "text": "computation if you are if you are currently user this pattern is actually addressed pretty well by the new AI runtime where",
    "start": "264840",
    "end": "273120"
  },
  {
    "text": "you can find autonomous interfaces in red to paralyze this pattern but the second problem is much harder",
    "start": "273120",
    "end": "279660"
  },
  {
    "text": "if the model is very large the size of the model can be much larger than the device memory capacity and as soon as",
    "start": "279660",
    "end": "285960"
  },
  {
    "text": "this figure we need at least thousands of gpus to hold this model so we cannot",
    "start": "285960",
    "end": "291000"
  },
  {
    "text": "afford to replicate the model needless to see that how also how to store a lot of intermediate results so we also have",
    "start": "291000",
    "end": "297840"
  },
  {
    "text": "to partition the model so however unlike the data set which are just a sequence of images or sentences",
    "start": "297840",
    "end": "304560"
  },
  {
    "text": "the model is complete a complicated computational graph and how to partition this model is land trivial",
    "start": "304560",
    "end": "310259"
  },
  {
    "text": "so now let's take a look at the computational graph and explain why it is so difficult so the graph begins with an input X and",
    "start": "310259",
    "end": "317639"
  },
  {
    "text": "we then have a lot of Matrix multiplications convolutions element-wise activations and the reductions Etc and the figure only",
    "start": "317639",
    "end": "324539"
  },
  {
    "text": "Shields the forward propagation of the graph but the real graph will be more complicated with the background propagation and the greeting updates",
    "start": "324539",
    "end": "331500"
  },
  {
    "text": "so how can we partition a graph onto two devices here device one and device two so the most straightforward way is to",
    "start": "331500",
    "end": "338580"
  },
  {
    "text": "cut the graph in the middle and put them on two devices this is called the interoperator partisan and the cut is actually not too",
    "start": "338580",
    "end": "345600"
  },
  {
    "text": "bad because the communication between between the cut is actually quite small",
    "start": "345600",
    "end": "350900"
  },
  {
    "text": "however due to the data dependency the second device cannot always be busy it",
    "start": "350900",
    "end": "356100"
  },
  {
    "text": "has to wait for the first device to generate this in its input so the second",
    "start": "356100",
    "end": "361259"
  },
  {
    "text": "device will be either at some time and although this can be elevated by using pipelining it still results in device",
    "start": "361259",
    "end": "367139"
  },
  {
    "text": "idle time due to the dependency between forward and backward propagation now if we want all devices to or to be",
    "start": "367139",
    "end": "374039"
  },
  {
    "text": "always busy what should we do so instead of partitioning at the graph level we can partition each operator",
    "start": "374039",
    "end": "379740"
  },
  {
    "text": "analyze different devices work on different regions of the same operator as this figure is used and this is",
    "start": "379740",
    "end": "385440"
  },
  {
    "text": "called the intraopular partisome an interoper partition makes all devices busy but it leads to more frequent",
    "start": "385440",
    "end": "391680"
  },
  {
    "text": "communication because devices have to think after one operator if the next operator cannot preserve the previous",
    "start": "391680",
    "end": "397259"
  },
  {
    "text": "partitioning pattern so in summary this page shows two basic patterns of partitioning a graph which",
    "start": "397259",
    "end": "403380"
  },
  {
    "text": "is also which is also multipartism as I just mentioned um there are trade-offs between these",
    "start": "403380",
    "end": "409319"
  },
  {
    "text": "two methods so interoperative parallelism requires less communication but has more device idle time but the",
    "start": "409319",
    "end": "414900"
  },
  {
    "text": "intraopularism requires more communication but let's do a central time",
    "start": "414900",
    "end": "420000"
  },
  {
    "text": "so under this classification there are more variants so first for interoperable particum there are multiple possible",
    "start": "420000",
    "end": "426419"
  },
  {
    "text": "strategies to partition operator for example for Matrix we can partition along rows we can also partition along",
    "start": "426419",
    "end": "432960"
  },
  {
    "text": "columns or we can even replicate it and when connecting this node in a graph different partitioning strategies can",
    "start": "432960",
    "end": "439440"
  },
  {
    "text": "lead to different communication and computational costs and this is a very difficult combinatorial problem",
    "start": "439440",
    "end": "446840"
  },
  {
    "text": "we can increase device resolution by pipelining the computation or multiple input batches as shown in this figure we",
    "start": "447199",
    "end": "454380"
  },
  {
    "text": "can put three layers on three devices and then we can keep all the devices busy by letting them work on different",
    "start": "454380",
    "end": "459900"
  },
  {
    "text": "input batches so the intra operator and interoper parisum can also be combined as shown",
    "start": "459900",
    "end": "465960"
  },
  {
    "text": "this figure right on the right so we first cut the graph in the middle into two parts for the interoperative",
    "start": "465960",
    "end": "472380"
  },
  {
    "text": "parallelism and then for each subgraph we do interoper product we inside each subgraph so for a mixture of expert",
    "start": "472380",
    "end": "479099"
  },
  {
    "text": "model developed by Google we can get the best performance only when we combine these two parallelisms as shown in this",
    "start": "479099",
    "end": "484740"
  },
  {
    "text": "figure so in summary all these choices form a very complex space with many trade-offs",
    "start": "484740",
    "end": "490680"
  },
  {
    "text": "and given a model of interest it's very hard to efficiently search for good strategy in its entire space",
    "start": "490680",
    "end": "497520"
  },
  {
    "text": "so however this is still not the most tricky part so the next tricky part is how to map the partitioning to actual",
    "start": "497520",
    "end": "503940"
  },
  {
    "text": "devices so after partitioning graph we have to map the partition into devices so here's",
    "start": "503940",
    "end": "509580"
  },
  {
    "text": "one GPU it is definitely not enough for training large models right so I get a machine with 4 gpus and more machines",
    "start": "509580",
    "end": "516899"
  },
  {
    "text": "with more gpus so here comes the tricky part so the network used to connect these",
    "start": "516899",
    "end": "522240"
  },
  {
    "text": "machines choose a two level hierarchy where we have fast fast connections for gpus inside the node such as the",
    "start": "522240",
    "end": "528240"
  },
  {
    "text": "unwilling technology and slower connections across different nodes such as a like Infinity band or ethernet and",
    "start": "528240",
    "end": "535019"
  },
  {
    "text": "the challenge here is how to map the graph partitioning to the two level hierarchy right so as you can see the problem is",
    "start": "535019",
    "end": "542040"
  },
  {
    "text": "changing and very important and in the past few years people have developed various systems to solve this problem in",
    "start": "542040",
    "end": "548399"
  },
  {
    "text": "order to train larger model and I will list a few here each of the project here below is developed by a good team of",
    "start": "548399",
    "end": "555300"
  },
  {
    "text": "researchers and engineers and each technique also typically enables a breakthrough on the machine learning",
    "start": "555300",
    "end": "560580"
  },
  {
    "text": "side so to classify them I draw three circles so red circle for intra-operative",
    "start": "560580",
    "end": "565800"
  },
  {
    "text": "parallelism and the yellow circle for interoper terrorism and the blue circle for whether the technique can be applied",
    "start": "565800",
    "end": "572580"
  },
  {
    "text": "to an arbitrary model automatically so first people design specialized strategies for specific models and",
    "start": "572580",
    "end": "579180"
  },
  {
    "text": "media's mechatron system is a good example it designs a specific model product strategy for parallelizing the",
    "start": "579180",
    "end": "585660"
  },
  {
    "text": "Transformer language model like gbd3 and later they improve it by adding",
    "start": "585660",
    "end": "590880"
  },
  {
    "text": "interoper parallelism for better scalability and there are also a lot of other systems trying to address this",
    "start": "590880",
    "end": "596040"
  },
  {
    "text": "problem however the user cannot support both types or partisans or cannot",
    "start": "596040",
    "end": "601080"
  },
  {
    "text": "automatically applied to a customized model and in contrast Opa is the first system",
    "start": "601080",
    "end": "606480"
  },
  {
    "text": "that can solve both times types of parallelisms automatically and next I will spin a few minutes to",
    "start": "606480",
    "end": "612839"
  },
  {
    "text": "tell you more of an opera so in the upper project our goal is to",
    "start": "612839",
    "end": "619140"
  },
  {
    "text": "build an unified compiler that can automatically find and ask you the best strategy with both inter and intrabular",
    "start": "619140",
    "end": "625800"
  },
  {
    "text": "particle for large deep learning models so Uber has a very simple API so Opa",
    "start": "625800",
    "end": "631860"
  },
  {
    "text": "provides a python declarator at operator parallelize and you can put a Speculator on top of your python function deep",
    "start": "631860",
    "end": "637740"
  },
  {
    "text": "learning training function and when this function is being called for the first time it triggers compilation and then",
    "start": "637740",
    "end": "642959"
  },
  {
    "text": "the function will be paralyzed and run distributedly on your cluster so under the hood this simple API is",
    "start": "642959",
    "end": "649500"
  },
  {
    "text": "made possible by several Innovations of this upper project to deal with the complicated search",
    "start": "649500",
    "end": "654959"
  },
  {
    "text": "space of multipartism techniques we organize them as a two level hierarchical space within design",
    "start": "654959",
    "end": "661079"
  },
  {
    "text": "optimization algorithms to derive effective parallel implants at each level and finally we Implement an",
    "start": "661079",
    "end": "667320"
  },
  {
    "text": "efficient compiler to generate the plan under very high performance runtime to execute the plan on top of Ray",
    "start": "667320",
    "end": "674660"
  },
  {
    "text": "the entire model parallel strategy space is a combination of all inter and interpreter parallel strategy but we",
    "start": "676700",
    "end": "683940"
  },
  {
    "text": "want to find the best performed one formulas bigger and complex space and existing",
    "start": "683940",
    "end": "689459"
  },
  {
    "text": "systems fail to do this because they do this either by human experts or they do they do this for one specific model",
    "start": "689459",
    "end": "695519"
  },
  {
    "text": "Activity three so in oppa the key to make this problem solvable is to decouple and reorganize this search",
    "start": "695519",
    "end": "702480"
  },
  {
    "text": "space so most basically we first search for the interopter parallel plan at the",
    "start": "702480",
    "end": "708480"
  },
  {
    "text": "first level and then at the next level we derive the best intra operator plan conditional and interpolo plan",
    "start": "708480",
    "end": "715980"
  },
  {
    "text": "so we designed the Opera compiler based on this 30 space decomposition and the input of the compiler is a computational",
    "start": "715980",
    "end": "722279"
  },
  {
    "text": "graph and a cluster specification and we designed three compilation passes to do this optimization",
    "start": "722279",
    "end": "727920"
  },
  {
    "text": "so the interoper pass finds the best interoperative parallelism strategy with the dynamic programming algorithm and",
    "start": "727920",
    "end": "734220"
  },
  {
    "text": "the inter interpreter path finds the best in trouble pattern strategy with an integer linear programming algorithm and",
    "start": "734220",
    "end": "740519"
  },
  {
    "text": "this optimization is also hierarchical which means that the higher level interoperative paths were called Lower",
    "start": "740519",
    "end": "746519"
  },
  {
    "text": "Level interpreter paths multiple times and mixed make decisions based on the feedback from the second pass",
    "start": "746519",
    "end": "752579"
  },
  {
    "text": "and finally the runtime orchestrating paths will realize this parallel parallel plan and actually executed the",
    "start": "752579",
    "end": "759360"
  },
  {
    "text": "strategy on recluster so let me walk you through an example unsure what each pass does",
    "start": "759360",
    "end": "765899"
  },
  {
    "text": "so for the interoper pass with the given conditional graph we need to partition a graph into",
    "start": "765899",
    "end": "771360"
  },
  {
    "text": "multiple subgraphs which we call stages stages to form a pipeline and there are",
    "start": "771360",
    "end": "776459"
  },
  {
    "text": "different ways to partition a graph and we need to select the best one of them and here assume our algorithm will pick",
    "start": "776459",
    "end": "782820"
  },
  {
    "text": "the first one as the best one so which partitional graph into four stages here shown in four different",
    "start": "782820",
    "end": "788760"
  },
  {
    "text": "colors we still need to assign each pipeline stage to settle devices to execute the",
    "start": "788760",
    "end": "794040"
  },
  {
    "text": "stage as the slides use so in Opera we abstract all the computational device such as gpus in a",
    "start": "794040",
    "end": "800700"
  },
  {
    "text": "real cluster as a 2d device mesh and we assume that devices along each mesh Dimension has the same communication",
    "start": "800700",
    "end": "807480"
  },
  {
    "text": "property so for example for TPU for typical GPU cluster we can set the one",
    "start": "807480",
    "end": "813180"
  },
  {
    "text": "dimension to be all the nodes and other communication along this Dimension will go through slower Crossing ethernet and",
    "start": "813180",
    "end": "820019"
  },
  {
    "text": "we can set another dimension to be all the gpus inside a node and here the communication will go through faster",
    "start": "820019",
    "end": "825360"
  },
  {
    "text": "connections like a link to a subset of devices from the entire",
    "start": "825360",
    "end": "831839"
  },
  {
    "text": "cluster by picking the best subset of devices for each stage we call each subset of device a sub mesh of the",
    "start": "831839",
    "end": "838620"
  },
  {
    "text": "original cluster so following these procedures here we have two problems first how to partition",
    "start": "838620",
    "end": "845519"
  },
  {
    "text": "a computer graph into multiple stages right and second how to assign the partition stages to sub meshes from the",
    "start": "845519",
    "end": "852000"
  },
  {
    "text": "cluster and in Alpha we find The Joint of these two problems can be nicely formulated as a dynamic programming",
    "start": "852000",
    "end": "858240"
  },
  {
    "text": "algorithm to minimize the total pipeline execution latency and more details about this algorithm can be found in our",
    "start": "858240",
    "end": "864540"
  },
  {
    "text": "project page and with this interval path we obtain a lot of stage and South magic",
    "start": "864540",
    "end": "869579"
  },
  {
    "text": "pairs after partitioning and mapping and let's now focus on the interoper",
    "start": "869579",
    "end": "874860"
  },
  {
    "text": "path so for a single stage under some matchable devices from The Interpreter path the goal here is to parallelize the",
    "start": "874860",
    "end": "881639"
  },
  {
    "text": "stage on the devices in a sub mesh using the best intraopular parallel strategy possible and in all part we find that",
    "start": "881639",
    "end": "888420"
  },
  {
    "text": "this problem can be formulated as an integer linear programming problem so specifically the choices of different",
    "start": "888420",
    "end": "895440"
  },
  {
    "text": "parallel strategies for each operator in the competition graph can be formulated as a decision fact Vector in the lp and",
    "start": "895440",
    "end": "902519"
  },
  {
    "text": "the optimal parallel strategy should minimize this LP objective which is the sum of the computational cost of each",
    "start": "902519",
    "end": "908699"
  },
  {
    "text": "parallelized operator and the communication cost between different parallelized operators again more",
    "start": "908699",
    "end": "914519"
  },
  {
    "text": "details can be found in our project documentation so with this internal interpreter path",
    "start": "914519",
    "end": "920160"
  },
  {
    "text": "we can transform an original single node computer graph to this fertilized pipeline stages and with no existing",
    "start": "920160",
    "end": "927000"
  },
  {
    "text": "framework supporting executing such a complex parallel plan we also design a runtime to efficiently execute this",
    "start": "927000",
    "end": "933480"
  },
  {
    "text": "parallel plan so specifically we compile each stage to an executable with static instructions and this is suitable as in",
    "start": "933480",
    "end": "940920"
  },
  {
    "text": "sent to the corresponding some matches and the Opera runtime orchestrate intra orbitalism within a mesh and the",
    "start": "940920",
    "end": "948000"
  },
  {
    "text": "interoperable across multiple device meshes and we're also Implement various optimizations for cross-match",
    "start": "948000",
    "end": "954180"
  },
  {
    "text": "communication in the runtime with more details in our code base so we input as a compiler stack and this",
    "start": "954180",
    "end": "961920"
  },
  {
    "text": "slides gives you an overview of the architecture of how opera is implemented and how it is built on top of Ray so",
    "start": "961920",
    "end": "968160"
  },
  {
    "text": "given the code of the model um we extract its computational computational graph in the format of a",
    "start": "968160",
    "end": "974699"
  },
  {
    "text": "gxpr and intermediate orientation used in the framework Jacks and the interval",
    "start": "974699",
    "end": "980459"
  },
  {
    "text": "path is performed on this Jack's PR to slice the graph into stages and each stage is then lowered to",
    "start": "980459",
    "end": "986880"
  },
  {
    "text": "another intermediate limitation called hul developed by Google SLE compiler and we perform the intro LP path on the",
    "start": "986880",
    "end": "993959"
  },
  {
    "text": "lower level hro repetition and finally Opera compiles distributions into executables and send them to device",
    "start": "993959",
    "end": "1000980"
  },
  {
    "text": "matches and workers which are implemented using array and as a lot of reactors",
    "start": "1000980",
    "end": "1007459"
  },
  {
    "text": "that's a brief introduction about the technology behind Opera and more details can be found in our paper and our",
    "start": "1007459",
    "end": "1013160"
  },
  {
    "text": "project page now let's move on to talk about use cases the current Opera project offers",
    "start": "1013160",
    "end": "1019519"
  },
  {
    "text": "so you know current Opera project we offer three main use cases that are readily available twin serve and fine",
    "start": "1019519",
    "end": "1025699"
  },
  {
    "text": "tune but all for big models that are existing systems or Solutions cannot handle well due to the difficult",
    "start": "1025699",
    "end": "1031520"
  },
  {
    "text": "problems I mentioned in the at the start of this presentation so I'm treating other country models",
    "start": "1031520",
    "end": "1037100"
  },
  {
    "text": "larger models like gpd3 with state-of-the-art performance and serving open server existing between large",
    "start": "1037100",
    "end": "1042918"
  },
  {
    "text": "models like Metals OBT with 175 billion parameters with minimized cost an online",
    "start": "1042919",
    "end": "1048620"
  },
  {
    "text": "tuning oppa provides a lot of flexibility which means that by using oppa you might have more space to",
    "start": "1048620",
    "end": "1054500"
  },
  {
    "text": "customize your model architecture and all power will generate a flexible parallel strategies for your specific model architecture and cluster",
    "start": "1054500",
    "end": "1062539"
  },
  {
    "text": "so we started developing Opera from training large models so our single declarator interface is made very simple",
    "start": "1062539",
    "end": "1069559"
  },
  {
    "text": "and the backend is highly optimized on several typical training workloads of bigger models",
    "start": "1069559",
    "end": "1075260"
  },
  {
    "text": "such as a training big Transformer models like gbd3 or training big mixture",
    "start": "1075260",
    "end": "1080419"
  },
  {
    "text": "of expert models like Google's G-sharp Moe or big computer vision models such",
    "start": "1080419",
    "end": "1085520"
  },
  {
    "text": "as well rest net and all you need to do is to first start array and then append",
    "start": "1085520",
    "end": "1091520"
  },
  {
    "text": "a single line on top of your twin step function and Opera will trigger the compilation and the distributed training",
    "start": "1091520",
    "end": "1096559"
  },
  {
    "text": "workload on the recluster automatically with high performance so in training people care a lot about",
    "start": "1096559",
    "end": "1102559"
  },
  {
    "text": "the training performance so let's talk about training performance We compare Opa with previous works on three widely",
    "start": "1102559",
    "end": "1108559"
  },
  {
    "text": "used models so for GPT the standard Transformer model we test for models up",
    "start": "1108559",
    "end": "1113720"
  },
  {
    "text": "to 39 billion parameters and the Transformer is a extensively studied",
    "start": "1113720",
    "end": "1118880"
  },
  {
    "text": "model and we can match the performance of the best existing expert design framework actually we find that almost identical",
    "start": "1118880",
    "end": "1125660"
  },
  {
    "text": "parallel strategy as the as the best manual system Moe is a Transformer with additional",
    "start": "1125660",
    "end": "1133160"
  },
  {
    "text": "mixture of expert layers and we test for models up to 70 billion parameters and we see that we can also perform the best",
    "start": "1133160",
    "end": "1139580"
  },
  {
    "text": "manual Baseline on gpus by up to 8 times faster on AWS cluster",
    "start": "1139580",
    "end": "1145160"
  },
  {
    "text": "and for well resonant which is the significantly different different model compared to Transformers and there's no",
    "start": "1145160",
    "end": "1150860"
  },
  {
    "text": "existing manual model part of ready for it and we show that we can generalize to to this model without menu plans where",
    "start": "1150860",
    "end": "1157700"
  },
  {
    "text": "other systems will would fail so I'm serving Alba provides the first",
    "start": "1157700",
    "end": "1163460"
  },
  {
    "text": "public unlimited and free tax generation for the reasonably open sourced large language model opt with one 75 opinion",
    "start": "1163460",
    "end": "1170539"
  },
  {
    "text": "parameters and you can try it immediately now at this link https opt Ai and here's a screenshot so basically",
    "start": "1170539",
    "end": "1178280"
  },
  {
    "text": "you can type the prompts and our serving cluster will use several gpus to write inference for your requests and we",
    "start": "1178280",
    "end": "1185059"
  },
  {
    "text": "allows this service to a few University mailing list about two weeks ago and it wouldn't wear immediately on Twitter and",
    "start": "1185059",
    "end": "1191660"
  },
  {
    "text": "a lot of users have expressed their interest and I'm serving upper has three unique",
    "start": "1191660",
    "end": "1197720"
  },
  {
    "text": "advantages so first all-pass serving is designed for serving large models like",
    "start": "1197720",
    "end": "1202760"
  },
  {
    "text": "gpd3 While most existing serving systems are limited to some small models that must fade on one single GPU or TPU",
    "start": "1202760",
    "end": "1210080"
  },
  {
    "text": "and second oppa can use commodity Hardware to serve large the largest models with Opera you can serve OBT 175",
    "start": "1210080",
    "end": "1217940"
  },
  {
    "text": "billion using your in-house CPU cluster while many existing systems have a higher requirement on leading the latest",
    "start": "1217940",
    "end": "1224120"
  },
  {
    "text": "generation of 800 GPU with a 80 gig memory and high bandwidth like",
    "start": "1224120",
    "end": "1229520"
  },
  {
    "text": "connections like Infinity band which are very expensive and hard to get but over",
    "start": "1229520",
    "end": "1234919"
  },
  {
    "text": "helps eliminate this constraint and third since Opera is a compiler it also means that it's very flexible it",
    "start": "1234919",
    "end": "1241340"
  },
  {
    "text": "will generate the optimal parallel strategy depending on your available hardware and the model you want to serve",
    "start": "1241340",
    "end": "1248059"
  },
  {
    "text": "with these three advantages putting other words oppa makes large models more accessible to everyone and we provide a",
    "start": "1248059",
    "end": "1255020"
  },
  {
    "text": "step-by-step tutorial on how to set up your own GPS relax service in the link below and feel free to follow this",
    "start": "1255020",
    "end": "1261440"
  },
  {
    "text": "tutorial if we want to do this in your cluster and training a bigger model is very",
    "start": "1261440",
    "end": "1267020"
  },
  {
    "text": "expensive and it's estimated that training a big model costs several million dollars if you do it with the",
    "start": "1267020",
    "end": "1272240"
  },
  {
    "text": "gpus instance on public Cloud this is not affordable for small companies or",
    "start": "1272240",
    "end": "1277580"
  },
  {
    "text": "research groups so fortunately the community loves open science and there are some groups that use the available",
    "start": "1277580",
    "end": "1283400"
  },
  {
    "text": "resources they can they have to trim big models and then give Open Access of this protein of wish to everyone",
    "start": "1283400",
    "end": "1289700"
  },
  {
    "text": "so this page issues of your public available big models so the first model Bloom was released by big science just",
    "start": "1289700",
    "end": "1295880"
  },
  {
    "text": "one month ago it used to train on 50 line languages and it's the largest public available language model",
    "start": "1295880",
    "end": "1301340"
  },
  {
    "text": "currently and meta metas opd1 some people is another another large model released by",
    "start": "1301340",
    "end": "1307100"
  },
  {
    "text": "meta AI several months ago and there's another model called the GPT new x20b is a smaller model released",
    "start": "1307100",
    "end": "1313580"
  },
  {
    "text": "earlier this year and the the last feature we are actually developing in alpha is to allow users to",
    "start": "1313580",
    "end": "1320179"
  },
  {
    "text": "modify this existing between large models with customized components and support fine tune in these models with",
    "start": "1320179",
    "end": "1326419"
  },
  {
    "text": "flexible particle strategies and the op actually supports fine-tuning all these models and again Uber has a ready to",
    "start": "1326419",
    "end": "1334039"
  },
  {
    "text": "available tutorial and cool examples on how to fine-tune these models please check it out",
    "start": "1334039",
    "end": "1339140"
  },
  {
    "text": "so with that I want to wrap up my talk with a quick summary so we presented the upper project for automatic model",
    "start": "1339140",
    "end": "1345679"
  },
  {
    "text": "parallel training with both interoperator and interoper partition an opera matches or outperforms specializes",
    "start": "1345679",
    "end": "1352280"
  },
  {
    "text": "systems and internalize to new models and opa provides use case for big models",
    "start": "1352280",
    "end": "1357500"
  },
  {
    "text": "and you can train serve or fine-tune big models on Oppa without worrying about how to fertilize your competition",
    "start": "1357500",
    "end": "1362960"
  },
  {
    "text": "anymore so you can try about me immediately using our unlimited generating service please do not hesitate to start up a",
    "start": "1362960",
    "end": "1369679"
  },
  {
    "text": "project on GitHub to support our development and thanks everyone for attending my talk and I'm happy to take",
    "start": "1369679",
    "end": "1374720"
  },
  {
    "text": "a few questions [Applause]",
    "start": "1374720",
    "end": "1381020"
  },
  {
    "text": "hi uh very interesting design choices here so I have two questions first is",
    "start": "1381020",
    "end": "1387140"
  },
  {
    "text": "something I'm not able to wrap my head around it's which all operators can be",
    "start": "1387140",
    "end": "1393159"
  },
  {
    "text": "parallelized using this interoperator parallelism do what characteristics or they need",
    "start": "1393159",
    "end": "1399799"
  },
  {
    "text": "and the second is now that you have this big optimization problem are you able to",
    "start": "1399799",
    "end": "1405679"
  },
  {
    "text": "guarantee like load balance across this party uh different gpus that you used to",
    "start": "1405679",
    "end": "1412700"
  },
  {
    "text": "serve yeah so the first problem is which kind of operator emergency learning programs need to be parallelized right",
    "start": "1412700",
    "end": "1418460"
  },
  {
    "text": "so typically I think the a lot of like Matrix modification operators need to be paralyzed but like a mission can appear",
    "start": "1418460",
    "end": "1425600"
  },
  {
    "text": "can appear in many different forms such as a 2d 3D 40 or come to the Commerce",
    "start": "1425600",
    "end": "1431120"
  },
  {
    "text": "ready or com 1D and all these operators when they're when with their input are very large that can all fit on a single",
    "start": "1431120",
    "end": "1437299"
  },
  {
    "text": "GPU you have to consider how to paralyze them and this happens for models like lgbt3 basically each Matrix modification",
    "start": "1437299",
    "end": "1444620"
  },
  {
    "text": "like those in attentions like those in MLPs need to be parallelized because they cannot be fit into one single GPU",
    "start": "1444620",
    "end": "1451340"
  },
  {
    "text": "so your second second question is can we guarantee load balance or say optimality when we perform this kind of compilation",
    "start": "1451340",
    "end": "1458000"
  },
  {
    "text": "path uh the strong answer is we can cannot but we are the best compared to other systems which we can provide",
    "start": "1458000",
    "end": "1464720"
  },
  {
    "text": "limited guarantee but others can also cannot provide any guarantee yeah cool because this problem is so",
    "start": "1464720",
    "end": "1470179"
  },
  {
    "text": "difficult that I don't think these are you can guarantee optimality yeah yeah cool yeah hi",
    "start": "1470179",
    "end": "1478460"
  },
  {
    "text": "it was a really good talk but I have two questions as well uh first one is",
    "start": "1478460",
    "end": "1484039"
  },
  {
    "text": "um why do you have to do IOP versus something like smt solving uh for the optimization problem",
    "start": "1484039",
    "end": "1490640"
  },
  {
    "text": "um I think both are doable because we just we choose IOP because um uh iops are pretty well developed community and",
    "start": "1490640",
    "end": "1496940"
  },
  {
    "text": "you can find a lot of free servers which can run efficiently yeah okay yeah and the second one is for the cost",
    "start": "1496940",
    "end": "1502880"
  },
  {
    "text": "estimation are you doing it using like a heuristic or are you doing it using",
    "start": "1502880",
    "end": "1508100"
  },
  {
    "text": "empirical measurements um we are using a combined method so the matter has two components the first",
    "start": "1508100",
    "end": "1514100"
  },
  {
    "text": "component is a cost model for example Alpha Beta model where uh for example the communication time is Alpha which is",
    "start": "1514100",
    "end": "1521000"
  },
  {
    "text": "a latency plus the size of message divided by Beta which is the communicating boundaries but this model",
    "start": "1521000",
    "end": "1526760"
  },
  {
    "text": "cannot always run accurately so we also have a second component which will run perfectly before actually you perform",
    "start": "1526760",
    "end": "1533419"
  },
  {
    "text": "this application now the profanity will run that model given strategy on the cluster for a few iterations and because",
    "start": "1533419",
    "end": "1538760"
  },
  {
    "text": "I'm a real estimation and then combine it together we can get a very good enough cost model yeah for that first",
    "start": "1538760",
    "end": "1545299"
  },
  {
    "text": "part are you using a aromatic intensity yeah okay yeah okay yeah cool",
    "start": "1545299",
    "end": "1552100"
  },
  {
    "text": "hi thanks for the talk uh so one question on the serving part I wonder if the model partitioning uh that you are",
    "start": "1552620",
    "end": "1560299"
  },
  {
    "text": "doing can do actually for any arbitrary checkpoints or this is like a specific to this model and you are partitioning",
    "start": "1560299",
    "end": "1567260"
  },
  {
    "text": "it based on the known architecture of opt or that's a really good question so um without all if you use any other",
    "start": "1567260",
    "end": "1573500"
  },
  {
    "text": "existing systems uh the way that they handle this partition is they will ask a team of expert to design a specific",
    "start": "1573500",
    "end": "1579559"
  },
  {
    "text": "partitioning for that speed model but the difference of opa is Alpha is a compiler it will look at the mod",
    "start": "1579559",
    "end": "1585320"
  },
  {
    "text": "architecture and the Run of your optimizations are not very figured out so basically whatever your model input",
    "start": "1585320",
    "end": "1590840"
  },
  {
    "text": "is like model definitely is of our implementation and get the best strategy Parton is right and the second part of",
    "start": "1590840",
    "end": "1597080"
  },
  {
    "text": "the question is about how you are actually optimizing the uh you know batch inferencing uh kind of how you're",
    "start": "1597080",
    "end": "1604220"
  },
  {
    "text": "optimizing about that because um I think you are saying I think you're asking something about how to increase the",
    "start": "1604220",
    "end": "1609980"
  },
  {
    "text": "throughput of the serving system basically when you are sharding your model among different gpus and so it",
    "start": "1609980",
    "end": "1618320"
  },
  {
    "text": "seems that you have to have kind of pipelining to kind of utilize you know all the gpus so how are you handling it",
    "start": "1618320",
    "end": "1626179"
  },
  {
    "text": "for the serving part um for Serene I think the pipeline pipeline is much easier than training",
    "start": "1626179",
    "end": "1631460"
  },
  {
    "text": "because in survey you can think of that as a influence streaming and what you need to do is just pipeline in the",
    "start": "1631460",
    "end": "1637940"
  },
  {
    "text": "incoming request following the pipeline parties and you don't have to like address some hard questions such as a",
    "start": "1637940",
    "end": "1644299"
  },
  {
    "text": "backward propagation dependency which is much easier so basically what we do is we just apply our algorithm for training",
    "start": "1644299",
    "end": "1650059"
  },
  {
    "text": "but we dominated at a hard part which is a back propagation part and that one naturally works yeah I see thank you",
    "start": "1650059",
    "end": "1657200"
  },
  {
    "text": "cool",
    "start": "1657200",
    "end": "1659620"
  },
  {
    "text": "but uh hi so thanks for the wonderful talks so I have several questions so the",
    "start": "1664940",
    "end": "1671000"
  },
  {
    "text": "first one is for the integer programming uh program uh um are you sure you got the optimal",
    "start": "1671000",
    "end": "1677419"
  },
  {
    "text": "backer Solutions or it's like a sub Optimal Solutions uh you know our problem formulation is optimal but like",
    "start": "1677419",
    "end": "1684020"
  },
  {
    "text": "I said we reduce the problem a lot so I cannot guarantee that it's optimal is original problem setting Yeah okay or or",
    "start": "1684020",
    "end": "1692000"
  },
  {
    "text": "one of my story is that did you try using the r lab to formulating a reinforcementing problem and the two so",
    "start": "1692000",
    "end": "1698960"
  },
  {
    "text": "like it's how to deploy the mesh into good question so we considered that but",
    "start": "1698960",
    "end": "1704600"
  },
  {
    "text": "um is not so efficient especially when you when your model is so big that of so",
    "start": "1704600",
    "end": "1711260"
  },
  {
    "text": "many operators on the other hand uh I don't think in practice people like that because they like Optimal optimality",
    "start": "1711260",
    "end": "1717740"
  },
  {
    "text": "they want to like determine this answer so they can debug the system if you apply Richmond learning okay every time",
    "start": "1717740",
    "end": "1723559"
  },
  {
    "text": "your strategy is different return but that's over with a different strategy and people can feel hard to debug their",
    "start": "1723559",
    "end": "1729200"
  },
  {
    "text": "program they probably would not use like to use your system anymore yeah that's our personal opinion yeah thank you so",
    "start": "1729200",
    "end": "1736100"
  },
  {
    "text": "NASA is a quite open question so how do you see up like a compared with Google's password okay oh that's a really good",
    "start": "1736100",
    "end": "1743600"
  },
  {
    "text": "question um we are three person almost a three-person team and Google has a big team but for now I think",
    "start": "1743600",
    "end": "1749559"
  },
  {
    "text": "I think Opera has a few advantages first the OPAC supports GPU pretty well and I",
    "start": "1749559",
    "end": "1755240"
  },
  {
    "text": "don't think Google care about GPU and they mainly focus on TPU right second is a uh I think we are trying to",
    "start": "1755240",
    "end": "1762200"
  },
  {
    "text": "build on top of the ray ecosystem and the ray already has a lot of users and the people who like like to use the grip",
    "start": "1762200",
    "end": "1768080"
  },
  {
    "text": "will also fill up a smooth experience because what you need to do is just decorate your code and start with",
    "start": "1768080",
    "end": "1774020"
  },
  {
    "text": "cluster uh in terms of organization um I feel at least theoretical words I think Opera can provide more automatic",
    "start": "1774020",
    "end": "1780799"
  },
  {
    "text": "guarantee compared to password because password does not actually perform this kind of automatic Organization for parallel strategies and other does help",
    "start": "1780799",
    "end": "1788179"
  },
  {
    "text": "some optimality guarantee and like a theory yeah cool I have a quick question about kind",
    "start": "1788179",
    "end": "1795320"
  },
  {
    "text": "of road map and possible future Integrations um you know Google has this plan to kind of split xla into its own repo open xla",
    "start": "1795320",
    "end": "1802520"
  },
  {
    "text": "yeah do you think Alpha could become just a powerful set of xla passes or yeah that's what is happening so we have",
    "start": "1802520",
    "end": "1809059"
  },
  {
    "text": "a contributor who is actually inside the SLE team and the wow our optimal impacts",
    "start": "1809059",
    "end": "1814100"
  },
  {
    "text": "that the lp pass is being ported into XLE and I think you know what you will say soon in the public XL report yeah",
    "start": "1814100",
    "end": "1819799"
  },
  {
    "text": "okay cool So eventually all of the alpha technology could just be a set of xla passes that's a possible future I don't",
    "start": "1819799",
    "end": "1826520"
  },
  {
    "text": "think so because um only the IOP part will be the introoper will be xre",
    "start": "1826520",
    "end": "1832700"
  },
  {
    "text": "because the Excel team and Google only cares about TPU and TPO has very fast connections there's no slow connections",
    "start": "1832700",
    "end": "1838820"
  },
  {
    "text": "and in the other side I don't think people would prefer pipeline parties or interpreter partisan but in our like",
    "start": "1838820",
    "end": "1845120"
  },
  {
    "text": "outside of Google I think people are still relying on a lot of GPU classes right not GPU classes and people care about this internal preparationism and",
    "start": "1845120",
    "end": "1851360"
  },
  {
    "text": "the other part is what offers for this sort of GPU classers yeah yeah cool",
    "start": "1851360",
    "end": "1856520"
  },
  {
    "text": "yeah I think we are at the end and I'm happy to take more questions offline yeah thank you",
    "start": "1856520",
    "end": "1862240"
  },
  {
    "text": "[Applause]",
    "start": "1862240",
    "end": "1865690"
  }
]