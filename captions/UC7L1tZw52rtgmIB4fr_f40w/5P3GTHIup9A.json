[
  {
    "text": "hi everyone thanks for coming out to uh",
    "start": "3360",
    "end": "5759"
  },
  {
    "text": "this lightning talk today uh so I'm",
    "start": "5759",
    "end": "8160"
  },
  {
    "text": "Travis Adair I'm the CTO of predabase",
    "start": "8160",
    "end": "10559"
  },
  {
    "text": "and today I'm going to tell you about",
    "start": "10559",
    "end": "12360"
  },
  {
    "text": "some optimizations that we have that",
    "start": "12360",
    "end": "14700"
  },
  {
    "text": "enable 50x faster fine-tuning and 10",
    "start": "14700",
    "end": "17279"
  },
  {
    "text": "lines of yam with little yandere so very",
    "start": "17279",
    "end": "19800"
  },
  {
    "text": "sensationalist title like what are we",
    "start": "19800",
    "end": "21900"
  },
  {
    "text": "really trying to convey here the main",
    "start": "21900",
    "end": "24300"
  },
  {
    "text": "point is that with our declarative",
    "start": "24300",
    "end": "26640"
  },
  {
    "text": "specification that we provide in Ludwig",
    "start": "26640",
    "end": "29099"
  },
  {
    "text": "we are able to provide a lot of really",
    "start": "29099",
    "end": "31800"
  },
  {
    "text": "tight optimizations under the hood that",
    "start": "31800",
    "end": "34559"
  },
  {
    "text": "enable you to solve very task specific",
    "start": "34559",
    "end": "36300"
  },
  {
    "text": "use cases with llms in a very performant",
    "start": "36300",
    "end": "38940"
  },
  {
    "text": "way that would otherwise require a lot",
    "start": "38940",
    "end": "40800"
  },
  {
    "text": "of bespoke code to get for yourself and",
    "start": "40800",
    "end": "45059"
  },
  {
    "text": "so just to kind of motivate this quickly",
    "start": "45059",
    "end": "48120"
  },
  {
    "text": "um I think most of you here probably are",
    "start": "48120",
    "end": "49680"
  },
  {
    "text": "aware of this but you know",
    "start": "49680",
    "end": "52100"
  },
  {
    "text": "operationalizing training llms today",
    "start": "52100",
    "end": "54539"
  },
  {
    "text": "does require a lot of Highly specialized",
    "start": "54539",
    "end": "57360"
  },
  {
    "text": "infrastructure at multiple levels of the",
    "start": "57360",
    "end": "59640"
  },
  {
    "text": "stack here we're talking about different",
    "start": "59640",
    "end": "61680"
  },
  {
    "text": "capabilities right like obviously you",
    "start": "61680",
    "end": "63960"
  },
  {
    "text": "start with hosted llms for inference but",
    "start": "63960",
    "end": "67200"
  },
  {
    "text": "then you have retrieval augmented",
    "start": "67200",
    "end": "68460"
  },
  {
    "text": "generation distributed fine tuning all",
    "start": "68460",
    "end": "71040"
  },
  {
    "text": "the way down to pre-training but this",
    "start": "71040",
    "end": "72900"
  },
  {
    "text": "also comes into play in terms of the",
    "start": "72900",
    "end": "74520"
  },
  {
    "text": "physical stack of Hardware software and",
    "start": "74520",
    "end": "77159"
  },
  {
    "text": "the optimizations that you layer on top",
    "start": "77159",
    "end": "78960"
  },
  {
    "text": "so no matter how you look at it getting",
    "start": "78960",
    "end": "81000"
  },
  {
    "text": "the best performance for a use case is a",
    "start": "81000",
    "end": "84659"
  },
  {
    "text": "challenge with lens",
    "start": "84659",
    "end": "86580"
  },
  {
    "text": "and so the way that we try to Wrangle",
    "start": "86580",
    "end": "88500"
  },
  {
    "text": "this with predabase you know there's an",
    "start": "88500",
    "end": "90600"
  },
  {
    "text": "open source project that we developed",
    "start": "90600",
    "end": "92100"
  },
  {
    "text": "called Ludwig this came out of uber AI a",
    "start": "92100",
    "end": "95220"
  },
  {
    "text": "few years back and we since founded the",
    "start": "95220",
    "end": "97259"
  },
  {
    "text": "company around this concept and the idea",
    "start": "97259",
    "end": "100560"
  },
  {
    "text": "behind Ludwig is to try to Wrangle this",
    "start": "100560",
    "end": "103740"
  },
  {
    "text": "complexity of how to figure out what's",
    "start": "103740",
    "end": "105659"
  },
  {
    "text": "the right set of optimizations or",
    "start": "105659",
    "end": "109020"
  },
  {
    "text": "techniques or modeling architectures",
    "start": "109020",
    "end": "110520"
  },
  {
    "text": "that I need to use in order to solve a",
    "start": "110520",
    "end": "113040"
  },
  {
    "text": "specific task that the user has and so",
    "start": "113040",
    "end": "115680"
  },
  {
    "text": "what we mean by task is if you for",
    "start": "115680",
    "end": "117479"
  },
  {
    "text": "example you look at the configuration on",
    "start": "117479",
    "end": "119579"
  },
  {
    "text": "the left you see that the user has some",
    "start": "119579",
    "end": "121920"
  },
  {
    "text": "input features like a sentence an image",
    "start": "121920",
    "end": "125460"
  },
  {
    "text": "and then they have some output features",
    "start": "125460",
    "end": "127380"
  },
  {
    "text": "like a description that they want to",
    "start": "127380",
    "end": "128880"
  },
  {
    "text": "generate and so this is fundamentally a",
    "start": "128880",
    "end": "131640"
  },
  {
    "text": "you know text generation problem that",
    "start": "131640",
    "end": "133319"
  },
  {
    "text": "they're solving for and so Ludwig allows",
    "start": "133319",
    "end": "136620"
  },
  {
    "text": "you to you know do this with like a",
    "start": "136620",
    "end": "138480"
  },
  {
    "text": "llama 7 billion model just with a single",
    "start": "138480",
    "end": "140700"
  },
  {
    "text": "config here on the on the left",
    "start": "140700",
    "end": "143520"
  },
  {
    "text": "but you know we try to provide multiple",
    "start": "143520",
    "end": "145980"
  },
  {
    "text": "levels of control down the stack as well",
    "start": "145980",
    "end": "148200"
  },
  {
    "text": "so if you want to say well I want to use",
    "start": "148200",
    "end": "150420"
  },
  {
    "text": "a specific Optimizer or learning rate or",
    "start": "150420",
    "end": "153000"
  },
  {
    "text": "whatever those parameters are fully",
    "start": "153000",
    "end": "155520"
  },
  {
    "text": "available to you here in the",
    "start": "155520",
    "end": "158099"
  },
  {
    "text": "configuration and then if you want to go",
    "start": "158099",
    "end": "160020"
  },
  {
    "text": "further and do hyper parameter",
    "start": "160020",
    "end": "161400"
  },
  {
    "text": "optimization over multiple different",
    "start": "161400",
    "end": "163680"
  },
  {
    "text": "large language models that's all",
    "start": "163680",
    "end": "165959"
  },
  {
    "text": "supported declaratively as well",
    "start": "165959",
    "end": "168660"
  },
  {
    "text": "and so you know a few techniques to call",
    "start": "168660",
    "end": "170819"
  },
  {
    "text": "out here that we have integrated into",
    "start": "170819",
    "end": "172680"
  },
  {
    "text": "the system natively now low rank",
    "start": "172680",
    "end": "174900"
  },
  {
    "text": "adaptation is one that's very popular",
    "start": "174900",
    "end": "176519"
  },
  {
    "text": "for doing fine tuning without having to",
    "start": "176519",
    "end": "180540"
  },
  {
    "text": "um you know fine tune all the weights of",
    "start": "180540",
    "end": "183060"
  },
  {
    "text": "the models simultaneously and this is",
    "start": "183060",
    "end": "185580"
  },
  {
    "text": "something we enable with a single",
    "start": "185580",
    "end": "186900"
  },
  {
    "text": "parameter and in the config you just say",
    "start": "186900",
    "end": "188700"
  },
  {
    "text": "adapter equals Laura and you're good to",
    "start": "188700",
    "end": "190500"
  },
  {
    "text": "go",
    "start": "190500",
    "end": "191340"
  },
  {
    "text": "and then similarly if you want to do",
    "start": "191340",
    "end": "193739"
  },
  {
    "text": "quantization quantization aware training",
    "start": "193739",
    "end": "196080"
  },
  {
    "text": "with Q Laura for example you know all",
    "start": "196080",
    "end": "198360"
  },
  {
    "text": "you need to do is say quantization bits",
    "start": "198360",
    "end": "200340"
  },
  {
    "text": "equals four and suddenly you're able to",
    "start": "200340",
    "end": "202379"
  },
  {
    "text": "do this you know quite complex training",
    "start": "202379",
    "end": "205379"
  },
  {
    "text": "process under the hood that does you",
    "start": "205379",
    "end": "206940"
  },
  {
    "text": "know mixed Precision training Etc",
    "start": "206940",
    "end": "209000"
  },
  {
    "text": "without having to think about the",
    "start": "209000",
    "end": "211080"
  },
  {
    "text": "implementation details",
    "start": "211080",
    "end": "213720"
  },
  {
    "text": "and running Ludwig is very",
    "start": "213720",
    "end": "215280"
  },
  {
    "text": "straightforward as well we have a python",
    "start": "215280",
    "end": "216780"
  },
  {
    "text": "API we also have a command line utility",
    "start": "216780",
    "end": "218819"
  },
  {
    "text": "that you can use and so here's an",
    "start": "218819",
    "end": "220799"
  },
  {
    "text": "example of how you would run Ludwig",
    "start": "220799",
    "end": "222659"
  },
  {
    "text": "locally just by saying Ludwig train you",
    "start": "222659",
    "end": "225060"
  },
  {
    "text": "know here's my config here's my data set",
    "start": "225060",
    "end": "226799"
  },
  {
    "text": "and then running on Ray which is the",
    "start": "226799",
    "end": "229200"
  },
  {
    "text": "next part that we'll talk about is also",
    "start": "229200",
    "end": "231239"
  },
  {
    "text": "just as easy as you know creating array",
    "start": "231239",
    "end": "232980"
  },
  {
    "text": "cluster and then submitting that",
    "start": "232980",
    "end": "235980"
  },
  {
    "text": "training command to the ray cluster to",
    "start": "235980",
    "end": "237840"
  },
  {
    "text": "be executed there",
    "start": "237840",
    "end": "240260"
  },
  {
    "text": "and then under the hood we will figure",
    "start": "240260",
    "end": "242700"
  },
  {
    "text": "out you know that you're running on a",
    "start": "242700",
    "end": "244500"
  },
  {
    "text": "ray cluster and we should distribute the",
    "start": "244500",
    "end": "246060"
  },
  {
    "text": "job across multiple workers Etc",
    "start": "246060",
    "end": "249000"
  },
  {
    "text": "the architecture for ludagon Ray is",
    "start": "249000",
    "end": "251299"
  },
  {
    "text": "quite uh interesting under the hood so",
    "start": "251299",
    "end": "254519"
  },
  {
    "text": "we have these different stages of the",
    "start": "254519",
    "end": "256079"
  },
  {
    "text": "Ludwig pipeline that exists you know",
    "start": "256079",
    "end": "258120"
  },
  {
    "text": "pre-processing training model evaluation",
    "start": "258120",
    "end": "260280"
  },
  {
    "text": "and under the hood those all get",
    "start": "260280",
    "end": "262500"
  },
  {
    "text": "submitted as distributed jobs to Rey",
    "start": "262500",
    "end": "265680"
  },
  {
    "text": "so here we have for example dask on Ray",
    "start": "265680",
    "end": "268440"
  },
  {
    "text": "that's distributed pandas that's",
    "start": "268440",
    "end": "270780"
  },
  {
    "text": "processing very large data sets with Ray",
    "start": "270780",
    "end": "273000"
  },
  {
    "text": "uses the ray Object Store as kind of a",
    "start": "273000",
    "end": "275699"
  },
  {
    "text": "data broker between the different stages",
    "start": "275699",
    "end": "277680"
  },
  {
    "text": "we have a distributed training process",
    "start": "277680",
    "end": "279840"
  },
  {
    "text": "that does distributed python across",
    "start": "279840",
    "end": "281940"
  },
  {
    "text": "multiple different workers and then",
    "start": "281940",
    "end": "283680"
  },
  {
    "text": "finally batch evaluation at the end and",
    "start": "283680",
    "end": "286020"
  },
  {
    "text": "so all this is an abstraction that we",
    "start": "286020",
    "end": "288120"
  },
  {
    "text": "built on top of Ray so you don't have to",
    "start": "288120",
    "end": "290220"
  },
  {
    "text": "think about you know the actual actors",
    "start": "290220",
    "end": "293220"
  },
  {
    "text": "that are being used under the hood it's",
    "start": "293220",
    "end": "294540"
  },
  {
    "text": "something you just get for free with the",
    "start": "294540",
    "end": "295919"
  },
  {
    "text": "architecture",
    "start": "295919",
    "end": "297780"
  },
  {
    "text": "and so taking this back to the problem",
    "start": "297780",
    "end": "299699"
  },
  {
    "text": "of llm fine-tuning I want to talk a",
    "start": "299699",
    "end": "302759"
  },
  {
    "text": "little bit about this aspect of",
    "start": "302759",
    "end": "304500"
  },
  {
    "text": "flexibility and how declarative can help",
    "start": "304500",
    "end": "306300"
  },
  {
    "text": "you solve it",
    "start": "306300",
    "end": "308880"
  },
  {
    "text": "and so one thing I want to talk about is",
    "start": "308880",
    "end": "310620"
  },
  {
    "text": "this notion of you know is there only",
    "start": "310620",
    "end": "312540"
  },
  {
    "text": "one right way to do fine-tuning to begin",
    "start": "312540",
    "end": "314460"
  },
  {
    "text": "with",
    "start": "314460",
    "end": "315300"
  },
  {
    "text": "so for example when most people think",
    "start": "315300",
    "end": "317940"
  },
  {
    "text": "about fine-tuning a model they think of",
    "start": "317940",
    "end": "319440"
  },
  {
    "text": "this kind of instruction fine-tuning",
    "start": "319440",
    "end": "321120"
  },
  {
    "text": "based workload where you have the you",
    "start": "321120",
    "end": "323580"
  },
  {
    "text": "know LM encoders we call it you also",
    "start": "323580",
    "end": "326280"
  },
  {
    "text": "call it the backbone of the model that",
    "start": "326280",
    "end": "328320"
  },
  {
    "text": "generates the hidden State and then you",
    "start": "328320",
    "end": "330120"
  },
  {
    "text": "have something like an LM head on top of",
    "start": "330120",
    "end": "331919"
  },
  {
    "text": "that that generates the logits for what",
    "start": "331919",
    "end": "334139"
  },
  {
    "text": "the next token prediction is going to be",
    "start": "334139",
    "end": "335699"
  },
  {
    "text": "and then you fine-tune this to try to",
    "start": "335699",
    "end": "337620"
  },
  {
    "text": "get the model to generate text that",
    "start": "337620",
    "end": "339360"
  },
  {
    "text": "better matches your desired output right",
    "start": "339360",
    "end": "342180"
  },
  {
    "text": "but that's actually not the only thing",
    "start": "342180",
    "end": "344100"
  },
  {
    "text": "that you want to do when you're",
    "start": "344100",
    "end": "345600"
  },
  {
    "text": "fine-tuning oftentimes you have a",
    "start": "345600",
    "end": "347100"
  },
  {
    "text": "classification task like sentiment",
    "start": "347100",
    "end": "348900"
  },
  {
    "text": "analysis you know predict positive",
    "start": "348900",
    "end": "350520"
  },
  {
    "text": "neutral negative binary classification",
    "start": "350520",
    "end": "352680"
  },
  {
    "text": "regression things like that in which",
    "start": "352680",
    "end": "355020"
  },
  {
    "text": "case generating text is actually quite a",
    "start": "355020",
    "end": "357060"
  },
  {
    "text": "lot of additional overhead for you and",
    "start": "357060",
    "end": "359280"
  },
  {
    "text": "what you'd probably rather do is just",
    "start": "359280",
    "end": "360960"
  },
  {
    "text": "attach like a multi-layer perceptron",
    "start": "360960",
    "end": "363060"
  },
  {
    "text": "that does the classification as a head",
    "start": "363060",
    "end": "365340"
  },
  {
    "text": "on top and then fine tune that right",
    "start": "365340",
    "end": "368699"
  },
  {
    "text": "and so specifically you can even go as",
    "start": "368699",
    "end": "370800"
  },
  {
    "text": "far as freezing the weights of the llm",
    "start": "370800",
    "end": "373979"
  },
  {
    "text": "backbone itself and only training this",
    "start": "373979",
    "end": "375840"
  },
  {
    "text": "classification decoder to really speed",
    "start": "375840",
    "end": "378060"
  },
  {
    "text": "up the process to enable you to get to a",
    "start": "378060",
    "end": "381479"
  },
  {
    "text": "classification model without having to",
    "start": "381479",
    "end": "383280"
  },
  {
    "text": "do this large fine-tuning process over",
    "start": "383280",
    "end": "385560"
  },
  {
    "text": "these billions of parameters",
    "start": "385560",
    "end": "388759"
  },
  {
    "text": "and so in the next slides I'll talk",
    "start": "388800",
    "end": "390479"
  },
  {
    "text": "about each of these in turn and when you",
    "start": "390479",
    "end": "392639"
  },
  {
    "text": "want to use one and what optimizations",
    "start": "392639",
    "end": "394199"
  },
  {
    "text": "we provide for each of these scenarios",
    "start": "394199",
    "end": "398720"
  },
  {
    "text": "so and the way that you do this in",
    "start": "399060",
    "end": "400979"
  },
  {
    "text": "Ludwig is very simple as well you know",
    "start": "400979",
    "end": "402960"
  },
  {
    "text": "to do this in Python you'd have to write",
    "start": "402960",
    "end": "404759"
  },
  {
    "text": "quite a lot of bespoke code to",
    "start": "404759",
    "end": "406319"
  },
  {
    "text": "transition like how do I add the LM head",
    "start": "406319",
    "end": "408600"
  },
  {
    "text": "and then change my fine tuning process",
    "start": "408600",
    "end": "410280"
  },
  {
    "text": "and cache the weights and whatever we",
    "start": "410280",
    "end": "412860"
  },
  {
    "text": "make it super simple for you we just say",
    "start": "412860",
    "end": "414720"
  },
  {
    "text": "you know if you want to do instruction",
    "start": "414720",
    "end": "416819"
  },
  {
    "text": "fine-tuning text to text",
    "start": "416819",
    "end": "419100"
  },
  {
    "text": "use the LM model type you know it's a",
    "start": "419100",
    "end": "421740"
  },
  {
    "text": "text input text output if you want to do",
    "start": "421740",
    "end": "423900"
  },
  {
    "text": "a classification problem with a large",
    "start": "423900",
    "end": "425460"
  },
  {
    "text": "language model you have a text input",
    "start": "425460",
    "end": "427500"
  },
  {
    "text": "that uses this encoder as the large",
    "start": "427500",
    "end": "430259"
  },
  {
    "text": "language model and then outputs a",
    "start": "430259",
    "end": "432240"
  },
  {
    "text": "categorical feature in the case of",
    "start": "432240",
    "end": "433680"
  },
  {
    "text": "multi-class classification and then if",
    "start": "433680",
    "end": "435900"
  },
  {
    "text": "you don't want to fine tune the weights",
    "start": "435900",
    "end": "437520"
  },
  {
    "text": "of the large language model you just say",
    "start": "437520",
    "end": "439440"
  },
  {
    "text": "trainable equals false and you get all",
    "start": "439440",
    "end": "441479"
  },
  {
    "text": "the benefits for free",
    "start": "441479",
    "end": "443580"
  },
  {
    "text": "so first let's talk about text",
    "start": "443580",
    "end": "445139"
  },
  {
    "text": "generation so this is what most people",
    "start": "445139",
    "end": "446699"
  },
  {
    "text": "think of when they think of fine-tuning",
    "start": "446699",
    "end": "448259"
  },
  {
    "text": "an llm",
    "start": "448259",
    "end": "449639"
  },
  {
    "text": "and essentially you know all you do is",
    "start": "449639",
    "end": "452160"
  },
  {
    "text": "again specify you know longitude 7",
    "start": "452160",
    "end": "454259"
  },
  {
    "text": "billions of the model trainer top type",
    "start": "454259",
    "end": "455940"
  },
  {
    "text": "is fine tuning you have a text output",
    "start": "455940",
    "end": "457680"
  },
  {
    "text": "and under the hood what we do in Ludwig",
    "start": "457680",
    "end": "460199"
  },
  {
    "text": "is we use deep speed to distribute this",
    "start": "460199",
    "end": "462720"
  },
  {
    "text": "training job across multiple nodes you",
    "start": "462720",
    "end": "464940"
  },
  {
    "text": "know this is something that I think a",
    "start": "464940",
    "end": "466139"
  },
  {
    "text": "lot of people have heard about is you",
    "start": "466139",
    "end": "467940"
  },
  {
    "text": "know this technology that enables this",
    "start": "467940",
    "end": "469680"
  },
  {
    "text": "model Plus data parallelism",
    "start": "469680",
    "end": "471960"
  },
  {
    "text": "um but and you can configure it fully in",
    "start": "471960",
    "end": "474660"
  },
  {
    "text": "uh Ludwig on Ray by configuring this",
    "start": "474660",
    "end": "476580"
  },
  {
    "text": "back end section of the config that",
    "start": "476580",
    "end": "478620"
  },
  {
    "text": "makes it very easy to specify",
    "start": "478620",
    "end": "480780"
  },
  {
    "text": "what type of offload you want to using",
    "start": "480780",
    "end": "482699"
  },
  {
    "text": "what type of precision you want to train",
    "start": "482699",
    "end": "484259"
  },
  {
    "text": "with but one particular optimization",
    "start": "484259",
    "end": "486780"
  },
  {
    "text": "that we've added on top of this in",
    "start": "486780",
    "end": "488400"
  },
  {
    "text": "Ludwig that we think is very useful is",
    "start": "488400",
    "end": "491220"
  },
  {
    "text": "this idea of zero copy model loading",
    "start": "491220",
    "end": "495259"
  },
  {
    "text": "and this is a specific optimization that",
    "start": "495660",
    "end": "498419"
  },
  {
    "text": "allows you to reduce the memory pressure",
    "start": "498419",
    "end": "501000"
  },
  {
    "text": "when loading the model weights so",
    "start": "501000",
    "end": "503340"
  },
  {
    "text": "initially if you're going to do deep",
    "start": "503340",
    "end": "504780"
  },
  {
    "text": "speed the command line utility or",
    "start": "504780",
    "end": "506400"
  },
  {
    "text": "something like that you end up loading",
    "start": "506400",
    "end": "508259"
  },
  {
    "text": "the model into host memory on everyone",
    "start": "508259",
    "end": "510539"
  },
  {
    "text": "the worker processes before moving some",
    "start": "510539",
    "end": "513300"
  },
  {
    "text": "subset of the weights to GPU memory and",
    "start": "513300",
    "end": "515580"
  },
  {
    "text": "that ends up causing a very huge amount",
    "start": "515580",
    "end": "517260"
  },
  {
    "text": "of memory overhead right because you're",
    "start": "517260",
    "end": "518700"
  },
  {
    "text": "multiplying the number of copies of the",
    "start": "518700",
    "end": "520320"
  },
  {
    "text": "model by the number of workers what we",
    "start": "520320",
    "end": "522479"
  },
  {
    "text": "do in Ludwig instead is we load the",
    "start": "522479",
    "end": "524339"
  },
  {
    "text": "model once into host memory on the",
    "start": "524339",
    "end": "526260"
  },
  {
    "text": "driver process and then put in the right",
    "start": "526260",
    "end": "528480"
  },
  {
    "text": "Object Store which allows us to then",
    "start": "528480",
    "end": "530640"
  },
  {
    "text": "zero copy read those weights into the",
    "start": "530640",
    "end": "533339"
  },
  {
    "text": "GPU without having to make additional",
    "start": "533339",
    "end": "535019"
  },
  {
    "text": "copies so this can dramatically reduce",
    "start": "535019",
    "end": "536700"
  },
  {
    "text": "the chance of having an out of memory",
    "start": "536700",
    "end": "538320"
  },
  {
    "text": "error when training a model with deep",
    "start": "538320",
    "end": "540240"
  },
  {
    "text": "speed",
    "start": "540240",
    "end": "542100"
  },
  {
    "text": "so that's one optimization we do on full",
    "start": "542100",
    "end": "544320"
  },
  {
    "text": "fine tuning now let's talk about this",
    "start": "544320",
    "end": "545880"
  },
  {
    "text": "idea of fine tuning for classification",
    "start": "545880",
    "end": "548279"
  },
  {
    "text": "which is where this 50x speed up comes",
    "start": "548279",
    "end": "550440"
  },
  {
    "text": "from and specifically I want to talk",
    "start": "550440",
    "end": "552180"
  },
  {
    "text": "about optimization we've added called",
    "start": "552180",
    "end": "553860"
  },
  {
    "text": "cached encoder embeddings",
    "start": "553860",
    "end": "555899"
  },
  {
    "text": "and the way this works under the hood is",
    "start": "555899",
    "end": "558540"
  },
  {
    "text": "it's something that's really made",
    "start": "558540",
    "end": "559620"
  },
  {
    "text": "possible by Ray in particular",
    "start": "559620",
    "end": "561959"
  },
  {
    "text": "and when you say bird encoder here you",
    "start": "561959",
    "end": "563760"
  },
  {
    "text": "can imagine this to be any text",
    "start": "563760",
    "end": "565500"
  },
  {
    "text": "generation model like Llama Or GPT Etc",
    "start": "565500",
    "end": "569160"
  },
  {
    "text": "and essentially what we do is during the",
    "start": "569160",
    "end": "572100"
  },
  {
    "text": "pre-processing stage where we're",
    "start": "572100",
    "end": "573540"
  },
  {
    "text": "preparing the data for fine tuning we",
    "start": "573540",
    "end": "576060"
  },
  {
    "text": "actually run a forward pass over the",
    "start": "576060",
    "end": "577980"
  },
  {
    "text": "data using the model itself like the",
    "start": "577980",
    "end": "581100"
  },
  {
    "text": "Llama model Etc to generate those hidden",
    "start": "581100",
    "end": "583200"
  },
  {
    "text": "State embeddings and then we write that",
    "start": "583200",
    "end": "585600"
  },
  {
    "text": "out to a process data set",
    "start": "585600",
    "end": "588600"
  },
  {
    "text": "statically and then when we load the",
    "start": "588600",
    "end": "590880"
  },
  {
    "text": "model for fine tuning we actually",
    "start": "590880",
    "end": "592620"
  },
  {
    "text": "replace that llama Burt model in the",
    "start": "592620",
    "end": "597180"
  },
  {
    "text": "model architecture with this kind of",
    "start": "597180",
    "end": "599100"
  },
  {
    "text": "pass-through module called this skip",
    "start": "599100",
    "end": "602160"
  },
  {
    "text": "encoder shim",
    "start": "602160",
    "end": "604080"
  },
  {
    "text": "and then we simply fine tune the rest of",
    "start": "604080",
    "end": "606240"
  },
  {
    "text": "the model",
    "start": "606240",
    "end": "607100"
  },
  {
    "text": "only the multi-layer perceptron pieces",
    "start": "607100",
    "end": "609779"
  },
  {
    "text": "by basically just inserting the outputs",
    "start": "609779",
    "end": "612360"
  },
  {
    "text": "directly in there without having to do",
    "start": "612360",
    "end": "613980"
  },
  {
    "text": "the forward pass so effectively what you",
    "start": "613980",
    "end": "616800"
  },
  {
    "text": "can think of is happening here is that",
    "start": "616800",
    "end": "618720"
  },
  {
    "text": "we pay the cost of a single forward pass",
    "start": "618720",
    "end": "621000"
  },
  {
    "text": "to Cache the the hidden State embeddings",
    "start": "621000",
    "end": "623640"
  },
  {
    "text": "once write it out to disk",
    "start": "623640",
    "end": "626160"
  },
  {
    "text": "and then during each Epoch of training",
    "start": "626160",
    "end": "628200"
  },
  {
    "text": "we only ever load those weights from",
    "start": "628200",
    "end": "630660"
  },
  {
    "text": "disk and then we can do multiple epochs",
    "start": "630660",
    "end": "632519"
  },
  {
    "text": "very quickly and only ever fine-tune",
    "start": "632519",
    "end": "635459"
  },
  {
    "text": "slash do additional forward backward",
    "start": "635459",
    "end": "637080"
  },
  {
    "text": "passes on this classification head on",
    "start": "637080",
    "end": "639660"
  },
  {
    "text": "top",
    "start": "639660",
    "end": "640620"
  },
  {
    "text": "and what does this buy you it actually",
    "start": "640620",
    "end": "642660"
  },
  {
    "text": "buys you quite a lot in terms of the",
    "start": "642660",
    "end": "644279"
  },
  {
    "text": "throughput of training",
    "start": "644279",
    "end": "647120"
  },
  {
    "text": "and so if we look at for example",
    "start": "647339",
    "end": "650100"
  },
  {
    "text": "what happens if you train the full",
    "start": "650100",
    "end": "652200"
  },
  {
    "text": "fine-tuning version which is what we",
    "start": "652200",
    "end": "653700"
  },
  {
    "text": "showed at the beginning you know the",
    "start": "653700",
    "end": "655680"
  },
  {
    "text": "training duration in minutes",
    "start": "655680",
    "end": "657440"
  },
  {
    "text": "in this case was 52 using just all the",
    "start": "657440",
    "end": "661440"
  },
  {
    "text": "defaults if we use automatic mix",
    "start": "661440",
    "end": "663120"
  },
  {
    "text": "Precision it goes down to 30",
    "start": "663120",
    "end": "665399"
  },
  {
    "text": "and then if we do it with the fixed",
    "start": "665399",
    "end": "667260"
  },
  {
    "text": "weight so this is where we're not",
    "start": "667260",
    "end": "668579"
  },
  {
    "text": "fine-tuning the weights of the the large",
    "start": "668579",
    "end": "670500"
  },
  {
    "text": "language model itself we can get that",
    "start": "670500",
    "end": "672660"
  },
  {
    "text": "down to 21 minutes",
    "start": "672660",
    "end": "674760"
  },
  {
    "text": "um so this effectively is like basically",
    "start": "674760",
    "end": "676680"
  },
  {
    "text": "the cost of the forward pass on the",
    "start": "676680",
    "end": "678480"
  },
  {
    "text": "large language model plus the cost of",
    "start": "678480",
    "end": "680399"
  },
  {
    "text": "the Ford and backward pass on the on the",
    "start": "680399",
    "end": "682320"
  },
  {
    "text": "classification head",
    "start": "682320",
    "end": "684480"
  },
  {
    "text": "and then you see if we do this cache",
    "start": "684480",
    "end": "686279"
  },
  {
    "text": "encoder embeddings thing we get this",
    "start": "686279",
    "end": "688200"
  },
  {
    "text": "down to like 30 seconds right which is a",
    "start": "688200",
    "end": "690300"
  },
  {
    "text": "pretty crazy uh speed up in practice and",
    "start": "690300",
    "end": "693240"
  },
  {
    "text": "that's all because we're essentially not",
    "start": "693240",
    "end": "694740"
  },
  {
    "text": "paying the cost of running inference on",
    "start": "694740",
    "end": "696779"
  },
  {
    "text": "the large language model outside of that",
    "start": "696779",
    "end": "698820"
  },
  {
    "text": "very first initial time when we generate",
    "start": "698820",
    "end": "701279"
  },
  {
    "text": "the embedding cache",
    "start": "701279",
    "end": "703860"
  },
  {
    "text": "and that's it that's all I had to talk",
    "start": "703860",
    "end": "705959"
  },
  {
    "text": "about today with about a minute to spare",
    "start": "705959",
    "end": "708240"
  },
  {
    "text": "so please check out Ludwig if you're",
    "start": "708240",
    "end": "710640"
  },
  {
    "text": "interested in learning more about this",
    "start": "710640",
    "end": "712019"
  },
  {
    "text": "and interested in doing fine tuning for",
    "start": "712019",
    "end": "713820"
  },
  {
    "text": "your own tasks we were pretty popular",
    "start": "713820",
    "end": "715860"
  },
  {
    "text": "project got 9 700 plus stars on GitHub",
    "start": "715860",
    "end": "718640"
  },
  {
    "text": "140 plus contributors and if you're also",
    "start": "718640",
    "end": "722760"
  },
  {
    "text": "interested in a managed solution to",
    "start": "722760",
    "end": "725160"
  },
  {
    "text": "doing",
    "start": "725160",
    "end": "726180"
  },
  {
    "text": "um fine-tuning and serving of large",
    "start": "726180",
    "end": "727860"
  },
  {
    "text": "language models and other task specific",
    "start": "727860",
    "end": "729600"
  },
  {
    "text": "specialized models definitely check out",
    "start": "729600",
    "end": "731640"
  },
  {
    "text": "predabase this is a low code platform",
    "start": "731640",
    "end": "733800"
  },
  {
    "text": "that enables you to do all of this",
    "start": "733800",
    "end": "736260"
  },
  {
    "text": "um with just a few lines of code so",
    "start": "736260",
    "end": "738839"
  },
  {
    "text": "thank you very much and uh go ahead and",
    "start": "738839",
    "end": "740579"
  },
  {
    "text": "hand it over to the next person",
    "start": "740579",
    "end": "743480"
  }
]