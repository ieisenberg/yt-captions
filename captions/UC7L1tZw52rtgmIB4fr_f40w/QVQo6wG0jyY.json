[
  {
    "text": "so uh good afternoon everyone Thanks for for coming to this session so today we",
    "start": "5180",
    "end": "10559"
  },
  {
    "text": "are going to uh Pablo and I we're going to talk about the heterogeneous training cluster which is really on Netflix so",
    "start": "10559",
    "end": "16740"
  },
  {
    "text": "both Pablo and me are from a machine learning platform group in Netflix",
    "start": "16740",
    "end": "21840"
  },
  {
    "text": "so uh this is actually the uh the home page of Netflix so if you are an",
    "start": "21840",
    "end": "27420"
  },
  {
    "text": "excellent member you probably already found out that the the home page is",
    "start": "27420",
    "end": "32940"
  },
  {
    "text": "highly personalized for each profile so our our overall goal is try to enable",
    "start": "32940",
    "end": "39480"
  },
  {
    "text": "our users to find the increasing contents with the Netflix home page and",
    "start": "39480",
    "end": "45000"
  },
  {
    "text": "the personalization actually is the core pillows because it will enable the user",
    "start": "45000",
    "end": "50760"
  },
  {
    "text": "to have a different view of the of the content which actually adapts to the",
    "start": "50760",
    "end": "56100"
  },
  {
    "text": "interest and even like get the extended interest over times so in short in Netflix everything on the",
    "start": "56100",
    "end": "63120"
  },
  {
    "text": "homepage actually is the recommendation so uh it actually could start from the how",
    "start": "63120",
    "end": "69119"
  },
  {
    "text": "do we construct the page for you so if you see the selection and the placement of the row types actually are highly",
    "start": "69119",
    "end": "76920"
  },
  {
    "text": "personalized so they are like tens to thousands of different rows uh you know",
    "start": "76920",
    "end": "82560"
  },
  {
    "text": "like uh database basically so if you like say you are a super fan of the",
    "start": "82560",
    "end": "87600"
  },
  {
    "text": "document documentary like kind of movies so you may see that kind of rows will",
    "start": "87600",
    "end": "93479"
  },
  {
    "text": "show up at the top so when it comes to uh each different",
    "start": "93479",
    "end": "99240"
  },
  {
    "text": "rows when it comes to which titles we want to recommend so uh this the the we",
    "start": "99240",
    "end": "105240"
  },
  {
    "text": "have the video ranking algorithm to sort out uh the different videos in hero according to your test",
    "start": "105240",
    "end": "112560"
  },
  {
    "text": "yeah the last even for the same videos for the same movies so we have a set of",
    "start": "112560",
    "end": "118140"
  },
  {
    "text": "the artworks which is also customized for your test so you can like uh choose",
    "start": "118140",
    "end": "123479"
  },
  {
    "text": "whatever like according to your Long View histories so uh let's uh talk about like the Ray",
    "start": "123479",
    "end": "131640"
  },
  {
    "text": "and so uh the first I want to give an overview of the some performance Organization for the model Training",
    "start": "131640",
    "end": "138000"
  },
  {
    "text": "Systems so this will help you to see a glance at a glance of why we use Ray so",
    "start": "138000",
    "end": "144239"
  },
  {
    "text": "our mainly talk about from a different aspects of the model training system like the computation communication also",
    "start": "144239",
    "end": "152280"
  },
  {
    "text": "like the file system IO uh first let's step into the training",
    "start": "152280",
    "end": "158520"
  },
  {
    "text": "workload characterizations so we have different workloads the first one is most heavily used is all recommendation",
    "start": "158520",
    "end": "165180"
  },
  {
    "text": "user case so it's more the model is more like a DRM or like the different white",
    "start": "165180",
    "end": "170280"
  },
  {
    "text": "mod architecture which is featured by the spots like embedding tables and uh",
    "start": "170280",
    "end": "176840"
  },
  {
    "text": "concatenate with some dense features and then plug into the uh the multiple prescription MLP so normally for this",
    "start": "176840",
    "end": "184560"
  },
  {
    "text": "kind of models we have a very large data set but but actually we have a really",
    "start": "184560",
    "end": "189720"
  },
  {
    "text": "visible model training time because it's more like a memory bounded so the the main lookup is super fast actually so",
    "start": "189720",
    "end": "195659"
  },
  {
    "text": "and normally in order to reduce the uh the training time so we we normally use",
    "start": "195659",
    "end": "201720"
  },
  {
    "text": "the multiple GPU distribute training and and it turns out like the gradient",
    "start": "201720",
    "end": "206879"
  },
  {
    "text": "Communications are pop out at the bottlenecks from time to time for different kind of training models",
    "start": "206879",
    "end": "213000"
  },
  {
    "text": "another big category is the media Mia use the case so our internal researchers",
    "start": "213000",
    "end": "219200"
  },
  {
    "text": "use the multiple models with constructive Learners to learn the",
    "start": "219200",
    "end": "224340"
  },
  {
    "text": "semantic implementation of our content so so the the data source is already very diverged we have the text uh images",
    "start": "224340",
    "end": "231659"
  },
  {
    "text": "audios videos and Etc and also because we always",
    "start": "231659",
    "end": "236720"
  },
  {
    "text": "try to divide the long format videos into like uh different samples so so",
    "start": "236720",
    "end": "244200"
  },
  {
    "text": "this will result in a very very large data set basically for all the like the asset that we have and again we also",
    "start": "244200",
    "end": "251340"
  },
  {
    "text": "have to use the multiple GP training and with some distributed data parallelism to speed up the training so the last is",
    "start": "251340",
    "end": "257579"
  },
  {
    "text": "more like an emerging use case is the large language model for tuning is a",
    "start": "257579",
    "end": "263220"
  },
  {
    "text": "small data set but it had a very long model experience time and also sometimes",
    "start": "263220",
    "end": "269160"
  },
  {
    "text": "it's very hard to feed some models into the uh in the single GPU memories it's especially like the 70 uh billion like",
    "start": "269160",
    "end": "276300"
  },
  {
    "text": "you know the Llama two models and also the computation is really contained intensives due to some Transformer based",
    "start": "276300",
    "end": "283080"
  },
  {
    "text": "architectures value using this architecture so in terms of like computation so we",
    "start": "283080",
    "end": "292380"
  },
  {
    "text": "use the custom operators for efficient to improve the computation efficiency as",
    "start": "292380",
    "end": "298860"
  },
  {
    "text": "you know it's always very hard to compose some detailed operations uh in a very efficient way I think this can be",
    "start": "298860",
    "end": "305820"
  },
  {
    "text": "like formed you know like the very high frame of framework overhead if you have a lot of the tiny Ops to compose a very",
    "start": "305820",
    "end": "313080"
  },
  {
    "text": "complex complicated operations and also for each different or small operators it",
    "start": "313080",
    "end": "319380"
  },
  {
    "text": "always involves a lot of memory allocation deallocation due to the in the model training runtime and also",
    "start": "319380",
    "end": "325560"
  },
  {
    "text": "another opportunity is like refuse or a series of haters sometimes for better performance uh like this sometimes is",
    "start": "325560",
    "end": "333180"
  },
  {
    "text": "not supported by the framework like the pathology and the tensorflow yeah a very",
    "start": "333180",
    "end": "339060"
  },
  {
    "text": "interesting example like I I did is like we did is like we have a basically have two embedded tables and then we have",
    "start": "339060",
    "end": "345720"
  },
  {
    "text": "look up some entries from this embedded look up table and then do the third product to it's more like a reduction",
    "start": "345720",
    "end": "351360"
  },
  {
    "text": "and when we've used all this operation into one single or we can get like two",
    "start": "351360",
    "end": "356460"
  },
  {
    "text": "to three x speed up of the training time yeah uh the second item is like we always adopt state-of-the-art operator",
    "start": "356460",
    "end": "363120"
  },
  {
    "text": "from OSS no matter whether it's from petals or tensorflow for example we",
    "start": "363120",
    "end": "368520"
  },
  {
    "text": "adopt the like the table batch embedding which actually is used in the in meta like for the production uh which to",
    "start": "368520",
    "end": "374759"
  },
  {
    "text": "replace the native embedding bag of petals and otherwise it's foremost which is also a pretty populous in the pathway",
    "start": "374759",
    "end": "382319"
  },
  {
    "text": "Community because we have uh a lot of like Legacy models still in tensorflow so we still try to develop some",
    "start": "382319",
    "end": "389100"
  },
  {
    "text": "tensorflow operators from this you know the core loss based implementation of The Flash attention to speed up our",
    "start": "389100",
    "end": "395220"
  },
  {
    "text": "tensorflow models so this will result in much less memory and faster speed which",
    "start": "395220",
    "end": "400919"
  },
  {
    "text": "allows a much longer sequence yeah the second one is the GPU communication so as you know like the",
    "start": "400919",
    "end": "407940"
  },
  {
    "text": "the test weight uh gradient like oil reduce is uh actually it gives very",
    "start": "407940",
    "end": "413039"
  },
  {
    "text": "great performance thanks to the organization of the like intensive law patterns however for the Sparks way like",
    "start": "413039",
    "end": "419039"
  },
  {
    "text": "the embedding table so this which is what widely used in the recommendation user case so this really",
    "start": "419039",
    "end": "425160"
  },
  {
    "text": "have very very different performance according to your different like model size batch size so uh we have different",
    "start": "425160",
    "end": "432720"
  },
  {
    "text": "solutions for example the first one um is is actually native tensorflow it",
    "start": "432720",
    "end": "437759"
  },
  {
    "text": "tries to gather all the activity indices try to duplicate the uh the same index",
    "start": "437759",
    "end": "443340"
  },
  {
    "text": "and do the significant sum to further gradient computations this actually could be the bottleneck if the patch",
    "start": "443340",
    "end": "449460"
  },
  {
    "text": "size is large because there's so many like indexes in there for different gpus you have to gather them together and",
    "start": "449460",
    "end": "455639"
  },
  {
    "text": "then do the update and another solution uh is like if the uh the table is not",
    "start": "455639",
    "end": "462720"
  },
  {
    "text": "that is not that large actually we can use the uh the test method to do the",
    "start": "462720",
    "end": "469620"
  },
  {
    "text": "table uh or reduce uh this is also supported by Hardware support I think the last one is most",
    "start": "469620",
    "end": "475020"
  },
  {
    "text": "probably for the light table like the sharing this can also be used for the small embedding table either you don't",
    "start": "475020",
    "end": "481139"
  },
  {
    "text": "need to shut the table basically so actually we did some experiments so as",
    "start": "481139",
    "end": "486900"
  },
  {
    "text": "long as the table size is not that large actually the dense gradient Community gives right a very good performance",
    "start": "486900",
    "end": "493259"
  },
  {
    "text": "comparing with the sparse gradient communication so the next one is uh where to the",
    "start": "493259",
    "end": "499139"
  },
  {
    "text": "storage the IELTS system where to store this data set so we have multiple",
    "start": "499139",
    "end": "504240"
  },
  {
    "text": "choices of i o system for the data set so the most interesting one is we can",
    "start": "504240",
    "end": "509280"
  },
  {
    "text": "download the dataset to local SSD disk which is available in the like the P40 instance on AWS but this definitely has",
    "start": "509280",
    "end": "516959"
  },
  {
    "text": "a sales limitation and also it is not easy to support like multiple node",
    "start": "516959",
    "end": "522899"
  },
  {
    "text": "because you need to do some shoveling of the data after each epochs another solution is the S3 streaming however",
    "start": "522899",
    "end": "530279"
  },
  {
    "text": "this has some performance super limitations so finally we are seek for the solution",
    "start": "530279",
    "end": "536459"
  },
  {
    "text": "which is called FS for last year as a as a data set cache so it is a global file",
    "start": "536459",
    "end": "543480"
  },
  {
    "text": "system so it definitely could support the distribute training and traveling as long as this instance is mounted into",
    "start": "543480",
    "end": "551459"
  },
  {
    "text": "different nodes of your training system also it has no storage disability as long as you can apply for the unlimited",
    "start": "551459",
    "end": "557820"
  },
  {
    "text": "space from the AWS another thing is the higher hour support so according to our statistics our like performance",
    "start": "557820",
    "end": "564720"
  },
  {
    "text": "basically for each P for now like eight p a 100 GPU we need around two to five gigabytes per second",
    "start": "564720",
    "end": "571140"
  },
  {
    "text": "to saturate this at GPU this is really a challenge for a lot of like file system so at the last due to the high cost of",
    "start": "571140",
    "end": "578399"
  },
  {
    "text": "the FSX file system so we actually use a feature from AWS which is the we can",
    "start": "578399",
    "end": "584399"
  },
  {
    "text": "store the data set in S3 and then use this as more like a cache we can automatically sync the data when the",
    "start": "584399",
    "end": "591120"
  },
  {
    "text": "training started when the training is finished we release the uh the data this data space the storage space for other",
    "start": "591120",
    "end": "597899"
  },
  {
    "text": "training jobs yeah and then we have the data in the in",
    "start": "597899",
    "end": "603600"
  },
  {
    "text": "the disk so how do you move the data from the disk to GPU memory this can be another challenge so uh so normally like",
    "start": "603600",
    "end": "611519"
  },
  {
    "text": "if you see like a lot of talking about like data pipeline so we have batching we have like shoveling we have some",
    "start": "611519",
    "end": "618600"
  },
  {
    "text": "preposition Logics in the from the data set from the disk to the GPU memory it's more like a last stage of the whole",
    "start": "618600",
    "end": "624779"
  },
  {
    "text": "training system so according to our some workload uh 30 to 40 of the CPU are",
    "start": "624779",
    "end": "632339"
  },
  {
    "text": "needed for data loading preposition to saturate a single GPU but as you know like uh there we have eight GPU in one",
    "start": "632339",
    "end": "641220"
  },
  {
    "text": "box but only have 96 CPUs so basically you we only can saturate about like uh",
    "start": "641220",
    "end": "648660"
  },
  {
    "text": "half of the gpus in the P4 box so I think the solution is like we try",
    "start": "648660",
    "end": "654300"
  },
  {
    "text": "to offload the data loader to remote CPU basically it's out of a box so this kind",
    "start": "654300",
    "end": "660420"
  },
  {
    "text": "of is more like a horizontal scaling of the input pad line to resolve the IR bottleneck and also this decarbox the",
    "start": "660420",
    "end": "666720"
  },
  {
    "text": "data loading from the GPU training box and also in some self connect coordination and a dynamic balancing for",
    "start": "666720",
    "end": "673740"
  },
  {
    "text": "multiple gpus this is really a great fit of the array can help us I think probably will give more details how this",
    "start": "673740",
    "end": "680220"
  },
  {
    "text": "is implemented so I think at the rough sum we really need more CPUs to prepare",
    "start": "680220",
    "end": "686399"
  },
  {
    "text": "the data for GPU to move the data to Inner GPU memory so the last one is the one interesting",
    "start": "686399",
    "end": "693600"
  },
  {
    "text": "problems we encounter in our Productions basically if we use some tensorflow",
    "start": "693600",
    "end": "699300"
  },
  {
    "text": "native like Auto tuning support So of the of the data pipelines so we",
    "start": "699300",
    "end": "705000"
  },
  {
    "text": "frequently encounter some a lot of crashes and even out of memory so we have a stunning indent copy I actually",
    "start": "705000",
    "end": "710640"
  },
  {
    "text": "did some research in this area so basically we use the reinforcement learning to distribute the CPU resource",
    "start": "710640",
    "end": "718079"
  },
  {
    "text": "in the uh in the training pipeline basically it's more like we employ an agent try to adjust the the basically",
    "start": "718079",
    "end": "726360"
  },
  {
    "text": "the CPU resources in the intraining pipeline and then we can get some feedback from the uh like the",
    "start": "726360",
    "end": "731940"
  },
  {
    "text": "performance bottlenecks feedback from the uh from the training system and then try to try to tune this kind of pipeline",
    "start": "731940",
    "end": "739440"
  },
  {
    "text": "to get the highest throughput so if you want to know more data you can check this paper we published in brexit this",
    "start": "739440",
    "end": "746100"
  },
  {
    "text": "year yeah uh I think the next part I will hand it over to my colleague Pablo so he",
    "start": "746100",
    "end": "752700"
  },
  {
    "text": "will talk about more like detailed implementation thanks",
    "start": "752700",
    "end": "757459"
  },
  {
    "text": "yes so the paper before is actually being presented right now in Singapore in rexis at the same time so the",
    "start": "759540",
    "end": "766800"
  },
  {
    "text": "coincidence um so uh we do the the injection of of",
    "start": "766800",
    "end": "774660"
  },
  {
    "text": "data from the file system we need a lot of workers in CPU in order to get more threats to consume faster from from from",
    "start": "774660",
    "end": "780600"
  },
  {
    "text": "the network and we use this TF data experimental service provided by tensorflow to the 19 onwards that has a",
    "start": "780600",
    "end": "789720"
  },
  {
    "text": "dispatcher and worker servers the dispatcher is the one that coordinates between the workers and the workers are",
    "start": "789720",
    "end": "795300"
  },
  {
    "text": "the ones that actually execute the TF data pipeline in in every processor",
    "start": "795300",
    "end": "800959"
  },
  {
    "text": "these are some advantages in in making you go scale horizontal by just adding",
    "start": "800959",
    "end": "807540"
  },
  {
    "text": "more workers and and have the dispatch server coordinate between the different batches that comes from the workers to",
    "start": "807540",
    "end": "814200"
  },
  {
    "text": "to have a on with only only one semantics for example",
    "start": "814200",
    "end": "819600"
  },
  {
    "text": "um what we have done is we wrap the whole idea of launching a",
    "start": "819600",
    "end": "825720"
  },
  {
    "text": "dispatcher and worker as a something called the data service so when we instantiate the data service we let Ray",
    "start": "825720",
    "end": "833579"
  },
  {
    "text": "deploy the workers and the dispatcher in different nodes so we can capture more",
    "start": "833579",
    "end": "839279"
  },
  {
    "text": "threads to read the data from there are various ways we can use this",
    "start": "839279",
    "end": "845100"
  },
  {
    "text": "the first one is just create local workers that means the dispatcher is deployed in the same node where the the",
    "start": "845100",
    "end": "852120"
  },
  {
    "text": "tensorflow worker is deployed and the data workers as well so if you if you",
    "start": "852120",
    "end": "857279"
  },
  {
    "text": "have remaining CPUs idle in the same box where you have the gpus you can deploy them there",
    "start": "857279",
    "end": "863040"
  },
  {
    "text": "is an option to do this remotely just have the dispatcher in the same machine",
    "start": "863040",
    "end": "868440"
  },
  {
    "text": "what the gpus are but the data workers living in an extra CPU box where you can use more threads",
    "start": "868440",
    "end": "874800"
  },
  {
    "text": "and there's the combination of of both like having local workers and and remote",
    "start": "874800",
    "end": "879959"
  },
  {
    "text": "workers all coordinated with the dispatcher we are we're going to describe a little",
    "start": "879959",
    "end": "886560"
  },
  {
    "text": "bit about the cluster infrastructure and how we deploy these clusters in the company uh as follows we will describe",
    "start": "886560",
    "end": "894380"
  },
  {
    "text": "the cluster management how we manage the resources and how we integrate all of",
    "start": "894380",
    "end": "901440"
  },
  {
    "text": "this with Ray and in the end how we we practically use the the training data",
    "start": "901440",
    "end": "906959"
  },
  {
    "text": "storage and management um we call them the manta ray cluster uh",
    "start": "906959",
    "end": "914880"
  },
  {
    "text": "coating and just Wheels I'm very bad at naming but I compensate that I've been very good at caching validation",
    "start": "914880",
    "end": "923300"
  },
  {
    "text": "we have a managed infrastructure correct so we have a centralized place where we",
    "start": "924480",
    "end": "929760"
  },
  {
    "text": "deploy the Clusters and we use the application called spinaker which is open source and this is the the central",
    "start": "929760",
    "end": "936180"
  },
  {
    "text": "point where we deploy all applications on Netflix we use an immutable infrastructure that means we bake a Amis",
    "start": "936180",
    "end": "944279"
  },
  {
    "text": "and we deploy a complete machines already immutable and we use a virtual",
    "start": "944279",
    "end": "950579"
  },
  {
    "text": "machine isolation instead of Docker or container isolation or cluster deployments have a recipe to",
    "start": "950579",
    "end": "957779"
  },
  {
    "text": "put all the software based software in the machine already such as the Cuda",
    "start": "957779",
    "end": "962820"
  },
  {
    "text": "kudyan and nickel libraries the python version and the ray version that we use and all the file system drivers to",
    "start": "962820",
    "end": "969540"
  },
  {
    "text": "connect to to external providers we also have a notion of multi-tenant",
    "start": "969540",
    "end": "974940"
  },
  {
    "text": "and multi-tenant in the sense that we have a single cluster that runs all the jobs at the same time so we have all our",
    "start": "974940",
    "end": "983100"
  },
  {
    "text": "jobs lives in our monorepo that that has the the great feature that we can fix",
    "start": "983100",
    "end": "990000"
  },
  {
    "text": "the version of python the version of Ray and also the version of the entire set of dependencies so they are immutable",
    "start": "990000",
    "end": "996420"
  },
  {
    "text": "for for as long as we have deployed all the workflows um and we we run hundreds of jobs per",
    "start": "996420",
    "end": "1004339"
  },
  {
    "text": "day which have a duration between one and ten hours",
    "start": "1004339",
    "end": "1009519"
  },
  {
    "text": "these clusters are also heterogeneous meaning that we have a head node and",
    "start": "1009519",
    "end": "1015019"
  },
  {
    "text": "then we have resource pools and our resource pool is a group of machines",
    "start": "1015019",
    "end": "1020120"
  },
  {
    "text": "that can Auto scale automatically so we have CPU instances that can be used for",
    "start": "1020120",
    "end": "1025339"
  },
  {
    "text": "for consuming these workers that read from the file system we have a Nvidia 800 gpus that are meant",
    "start": "1025339",
    "end": "1032418"
  },
  {
    "text": "to be used in a single GPU case Nvidia say 100 that are meant to be used in a",
    "start": "1032419",
    "end": "1037760"
  },
  {
    "text": "multi-node or multi-gpu setting and all of the Clusters have a EFS which is an",
    "start": "1037760",
    "end": "1043100"
  },
  {
    "text": "elastic NF NFS file system to store metadata checkpoint slog and output of of the",
    "start": "1043100",
    "end": "1050299"
  },
  {
    "text": "trainings and we have this FSX for luster which my colleague described as a",
    "start": "1050299",
    "end": "1055340"
  },
  {
    "text": "high throughput file system for reading data in a picture it looks like this we have",
    "start": "1055340",
    "end": "1061880"
  },
  {
    "text": "a Spinnaker lets you organize in in Auto scaling groups because we we deploy",
    "start": "1061880",
    "end": "1068900"
  },
  {
    "text": "everything here in AWS so the worker pools are a uniform on the on the",
    "start": "1068900",
    "end": "1075020"
  },
  {
    "text": "machine type so we have a worker pool for a hundred can you you see the dots represent that they are Auto scalable",
    "start": "1075020",
    "end": "1081620"
  },
  {
    "text": "you can we don't know how to scale because we have a fixed number of machines but they have the capability of",
    "start": "1081620",
    "end": "1087200"
  },
  {
    "text": "Auto scaling if we wish so um we can Auto scale also the the 800",
    "start": "1087200",
    "end": "1092600"
  },
  {
    "text": "A10 and the CPU only workers all of the Clusters all of the nodes in",
    "start": "1092600",
    "end": "1099740"
  },
  {
    "text": "the cluster share this EFS file system for writing data and the FSX file system",
    "start": "1099740",
    "end": "1105559"
  },
  {
    "text": "for for reading the data speed on the lifetime of resources we decided",
    "start": "1105559",
    "end": "1113299"
  },
  {
    "text": "on a strategy of deploying durable clusters instead of ephemeral clusters there are clusters mean that we set up",
    "start": "1113299",
    "end": "1120020"
  },
  {
    "text": "the a cluster for for for at the beginning of of the trainings and when the cluster",
    "start": "1120020",
    "end": "1127100"
  },
  {
    "text": "remain up and ready to receive all jobs and then all the daily jobs that come to the cluster execute one after the other",
    "start": "1127100",
    "end": "1133580"
  },
  {
    "text": "until the 24-hour cycle finishes um in order to have the training job",
    "start": "1133580",
    "end": "1139700"
  },
  {
    "text": "fresh we need to retrain them over and over at a 24 hour cycle these clusters are maintained independently and they",
    "start": "1139700",
    "end": "1146780"
  },
  {
    "text": "are stable and we reconfigure and upgrade them only at a different Cadence",
    "start": "1146780",
    "end": "1151880"
  },
  {
    "text": "slower than than we configure the jobs so this is typically a weekly or even monthly",
    "start": "1151880",
    "end": "1159220"
  },
  {
    "text": "um this approach has the advantage that or the training jobs come to the platform and train and live and and you",
    "start": "1159260",
    "end": "1165980"
  },
  {
    "text": "don't have to wait for that from time to launch the cluster and be be sure that it's ready for you to deploy your jobs",
    "start": "1165980",
    "end": "1171940"
  },
  {
    "text": "uh also it's quicker for deployment and it it has a simpler version Matrix",
    "start": "1171940",
    "end": "1178340"
  },
  {
    "text": "because we support only one version of python only one version of Ray and only one version of the set of dependencies",
    "start": "1178340",
    "end": "1184720"
  },
  {
    "text": "and it has the reduced maintenance for the team on the resource sharing we there are",
    "start": "1184720",
    "end": "1192980"
  },
  {
    "text": "several models that people are applying on on clusters there are clusters that share nothing that means you have a",
    "start": "1192980",
    "end": "1199160"
  },
  {
    "text": "launch script that deploys the cluster and then one cluster one job so one job needs a cluster and",
    "start": "1199160",
    "end": "1205100"
  },
  {
    "text": "and then there's also the model for sharing all so you have a unified big cluster and all the jobs share it we",
    "start": "1205100",
    "end": "1213500"
  },
  {
    "text": "opted for the for the mid Middle Ground which is share by team we have different teams get the use of different clusters",
    "start": "1213500",
    "end": "1221240"
  },
  {
    "text": "that we deploy for them for them and every team uses that cluster for a longer period of",
    "start": "1221240",
    "end": "1228080"
  },
  {
    "text": "time we use spinach that we said in order to deploy these clusters and",
    "start": "1228080",
    "end": "1235640"
  },
  {
    "text": "um having the share by team has some",
    "start": "1235640",
    "end": "1241520"
  },
  {
    "text": "advantages like like we say all the all the members of that Team all the workflows that run from the same team",
    "start": "1241520",
    "end": "1247400"
  },
  {
    "text": "usually have the same dependencies so running one after the other uh it's it's",
    "start": "1247400",
    "end": "1252919"
  },
  {
    "text": "not a problem but it has the drawback that if you have two disjoint teams if one team has the",
    "start": "1252919",
    "end": "1259220"
  },
  {
    "text": "resources available the other team cannot use those resources and we're working on on a central scheduler and a",
    "start": "1259220",
    "end": "1266179"
  },
  {
    "text": "queue and my colleague he's in the audience and we we expect to have this",
    "start": "1266179",
    "end": "1273380"
  },
  {
    "text": "Central schedule to be a router who which will move the jobs to different clusters where when their sources are",
    "start": "1273380",
    "end": "1280160"
  },
  {
    "text": "available um another thing that we do is",
    "start": "1280160",
    "end": "1285320"
  },
  {
    "text": "um we we try to reduce the problem of uh specifying resources to run or jobs",
    "start": "1285320",
    "end": "1292240"
  },
  {
    "text": "instead of a combining all the dimensions so my job needs this much run this many CPUs this",
    "start": "1292240",
    "end": "1298880"
  },
  {
    "text": "many gpus we only specify gpus and we say we have a pool of single GPU usage",
    "start": "1298880",
    "end": "1304280"
  },
  {
    "text": "and another Pool for agpu usage and you have the constraint that when you launch",
    "start": "1304280",
    "end": "1309380"
  },
  {
    "text": "a job you can only specify how many gpus you need everything else is determined by by a predefined by by the cluster",
    "start": "1309380",
    "end": "1316159"
  },
  {
    "text": "management people so the jobs can only that can run in this cluster can be either one GPU eight",
    "start": "1316159",
    "end": "1322760"
  },
  {
    "text": "gpus or multiples of 8 16 24 Etc these terms are n-dimensional problem",
    "start": "1322760",
    "end": "1329720"
  },
  {
    "text": "into a one-dimensional problem so it's tractable for us to to to do a scheduling",
    "start": "1329720",
    "end": "1336559"
  },
  {
    "text": "um this this simplify a resource scheduling it's also",
    "start": "1336559",
    "end": "1343340"
  },
  {
    "text": "a reduced a toll on cognitive toll on on the researchers who need to launch jobs",
    "start": "1343340",
    "end": "1349400"
  },
  {
    "text": "because they instead of remembering how much RAM do I need to allocate or how much how many CPUs do I need for this",
    "start": "1349400",
    "end": "1355760"
  },
  {
    "text": "particular job they only allocate by how many gpus of a certain type I need and",
    "start": "1355760",
    "end": "1361280"
  },
  {
    "text": "everything else is given by default remember that every instance in in AWS",
    "start": "1361280",
    "end": "1366559"
  },
  {
    "text": "has different CPU to memory radio another example is that the the in the",
    "start": "1366559",
    "end": "1372320"
  },
  {
    "text": "case of a 800 the P4 instances have only 8 gpus and they are meant to be used at",
    "start": "1372320",
    "end": "1378980"
  },
  {
    "text": "once the agpus at once because they are linked with the NV link which is a high bandwidth communication between gpus so",
    "start": "1378980",
    "end": "1386360"
  },
  {
    "text": "we meant to be use these machines by eight gpus at a time in the case of the CPU only we also use one dimension which",
    "start": "1386360",
    "end": "1394640"
  },
  {
    "text": "is the number of CPUs that you need instead of also specifying the Ram or the network",
    "start": "1394640",
    "end": "1401380"
  },
  {
    "text": "um here we describe a little bit in words",
    "start": "1402320",
    "end": "1407419"
  },
  {
    "text": "but I will switch to a picture um",
    "start": "1407419",
    "end": "1412299"
  },
  {
    "text": "as my colleague described we use FSX the the file system with a backup on S3 that",
    "start": "1412700",
    "end": "1419299"
  },
  {
    "text": "means we have all the pipelines that produce data in a company that are in different",
    "start": "1419299",
    "end": "1424520"
  },
  {
    "text": "clusters in a different environment with this different scheduler produce a materialize the data that lives in S3",
    "start": "1424520",
    "end": "1430760"
  },
  {
    "text": "and we we ask them to toss the data into a certain bucket and this bucket is",
    "start": "1430760",
    "end": "1437000"
  },
  {
    "text": "synchronized with FSX so for the training cluster view when you deploy a job you have the data",
    "start": "1437000",
    "end": "1443480"
  },
  {
    "text": "immediately available in a in a in a posix location in a file system slash FSX slash something so we decouple the",
    "start": "1443480",
    "end": "1451520"
  },
  {
    "text": "the data producing cluster there was the data producing jobs with the training jobs by having this instance",
    "start": "1451520",
    "end": "1458360"
  },
  {
    "text": "synchronization between FSX and NSA",
    "start": "1458360",
    "end": "1463299"
  },
  {
    "text": "future work as I said we are where we have a mixture of client mode to access",
    "start": "1463940",
    "end": "1470539"
  },
  {
    "text": "the cluster for interactive jobs and submission uh jobs we will try to go on",
    "start": "1470539",
    "end": "1476000"
  },
  {
    "text": "job submission only um we are deploying currently a job",
    "start": "1476000",
    "end": "1481700"
  },
  {
    "text": "schedule that will be act as a central entry point for all the Clusters it will also act as a router or as a queue with",
    "start": "1481700",
    "end": "1489980"
  },
  {
    "text": "plugable policies so we can then figure out how to maximize the use of the machines",
    "start": "1489980",
    "end": "1495620"
  },
  {
    "text": "and we are also exploring a batch inference using the actor pool in the",
    "start": "1495620",
    "end": "1500900"
  },
  {
    "text": "array data set pattern and to do for the use case of evaluation or evaluation metrics",
    "start": "1500900",
    "end": "1507620"
  },
  {
    "text": "um and I think that's all we want to give a special thanks to sisang and Matthew",
    "start": "1507620",
    "end": "1514280"
  },
  {
    "text": "Deng richardlio who is very kindly help us on the",
    "start": "1514280",
    "end": "1519679"
  },
  {
    "text": "height if you're watching us on the on on all the channels in support",
    "start": "1519679",
    "end": "1525140"
  },
  {
    "text": "thank you and also for the people who are in the training platform here as well and from Netflix we are sitting in",
    "start": "1525140",
    "end": "1531020"
  },
  {
    "text": "the areas good",
    "start": "1531020",
    "end": "1537820"
  },
  {
    "text": "two we have four minutes uh yeah four or five minutes for question",
    "start": "1538820",
    "end": "1545799"
  },
  {
    "text": "hi uh great talk so I have a question about the Frameworks so first of all for the TF data service so that's something",
    "start": "1549740",
    "end": "1556640"
  },
  {
    "text": "I want to like compare with the rate it has it right why do you pick like the TF",
    "start": "1556640",
    "end": "1562039"
  },
  {
    "text": "data service what's the advantage it over the related because I in my opinion if you just want to orchestrate the",
    "start": "1562039",
    "end": "1568279"
  },
  {
    "text": "remote worker right local worker remote worker the red data sets can also do the similar scene that's the first question",
    "start": "1568279",
    "end": "1574279"
  },
  {
    "text": "the second question in uh I'm also wondering like in the beginning image about S3 streaming has a performance",
    "start": "1574279",
    "end": "1579799"
  },
  {
    "text": "issue do you have any number like how can you give at least a little bit more like context about that like what the",
    "start": "1579799",
    "end": "1586039"
  },
  {
    "text": "performance issue You observe and until you do and which drives you to use some kind of uh like a cache using like file",
    "start": "1586039",
    "end": "1593059"
  },
  {
    "text": "system a very long question",
    "start": "1593059",
    "end": "1598179"
  },
  {
    "text": "the first one is about why we prefer this versus we don't have a preference",
    "start": "1598179",
    "end": "1603679"
  },
  {
    "text": "we just use this because we had we haven't explored rate of it fully in",
    "start": "1603679",
    "end": "1609860"
  },
  {
    "text": "this we choose because it's tensorflow specific and all the training jobs are needed are tensorflow specific but",
    "start": "1609860",
    "end": "1616760"
  },
  {
    "text": "coming to your question Ray data is a very good option as well you can you can use rate data set to fetch the data and",
    "start": "1616760",
    "end": "1622700"
  },
  {
    "text": "share the blocks in a no copy fashion with the arrow in Flight using memory so",
    "start": "1622700",
    "end": "1629240"
  },
  {
    "text": "it's a very very good option interestingly we can probably use Ray data sets",
    "start": "1629240",
    "end": "1636440"
  },
  {
    "text": "together with FSX or totally percent of FSX and use rate of assets only",
    "start": "1636440",
    "end": "1642170"
  },
  {
    "text": "[Music] but it's something that can be used but we are not currently using",
    "start": "1642170",
    "end": "1648980"
  },
  {
    "text": "we are not using radial asset in that fashion and you ask one more at the end but S3",
    "start": "1648980",
    "end": "1657320"
  },
  {
    "text": "so I can I can answer it yeah yeah I think like initially like uh I think S3",
    "start": "1657320",
    "end": "1662960"
  },
  {
    "text": "is good enough for like for like T4 it's low in gpus for this high-end GPU like",
    "start": "1662960",
    "end": "1668539"
  },
  {
    "text": "we really need a lot of like CPUs maybe even uh is more like a trade-off of the CPU",
    "start": "1668539",
    "end": "1676159"
  },
  {
    "text": "resource basically so let's say the FSX because I also put uh is high we may not",
    "start": "1676159",
    "end": "1682700"
  },
  {
    "text": "need to manage CPU a lot of data but if you plug in S3 then you in order to get",
    "start": "1682700",
    "end": "1688039"
  },
  {
    "text": "like to like I just mentioned like two to five gigabytes per second it's really a big challenge for S3 file system maybe",
    "start": "1688039",
    "end": "1694039"
  },
  {
    "text": "you need like 100 CPU to load the data to get this High support yeah I think for the like the first",
    "start": "1694039",
    "end": "1700760"
  },
  {
    "text": "creating the model I once into that is like I think when we started this project the web data is still not developed yeah I think it is yeah we can",
    "start": "1700760",
    "end": "1707840"
  },
  {
    "text": "definitely impress that to see yeah which one is more suitable basically yeah also the PF data set it might work",
    "start": "1707840",
    "end": "1715580"
  },
  {
    "text": "for all the all the Frameworks like uh by torch for example this one is only",
    "start": "1715580",
    "end": "1720799"
  },
  {
    "text": "tensor for specific",
    "start": "1720799",
    "end": "1723760"
  },
  {
    "text": "hi uh thank you for the great talk um I have a question on Resource Management uh so you mentioned you",
    "start": "1726679",
    "end": "1733400"
  },
  {
    "text": "um allow teams to specify resources on the GPU dimension",
    "start": "1733400",
    "end": "1738980"
  },
  {
    "text": "so what would be your strategy when a user wants to use Mig enable instances",
    "start": "1738980",
    "end": "1745340"
  },
  {
    "text": "or GPU sharing strategies and it's about the GPU sharing basically",
    "start": "1745340",
    "end": "1752000"
  },
  {
    "text": "yeah the feature in Nvidia GPU is called Mig",
    "start": "1752000",
    "end": "1757179"
  },
  {
    "text": "multi-instance GPU GPU sharing if user wants to consume",
    "start": "1757179",
    "end": "1763220"
  },
  {
    "text": "that then what would be your strategy yeah so we currently don't don't don't",
    "start": "1763220",
    "end": "1768460"
  },
  {
    "text": "share gpus right so you we don't we don't let you specify half of a GPU correct but uh but if you want to use",
    "start": "1768460",
    "end": "1776440"
  },
  {
    "text": "the utility that the Nvidia provides with Mig you can partition the machine",
    "start": "1776440",
    "end": "1781460"
  },
  {
    "text": "in physical or virtual gpus but for Ray you can read them as physical so let's",
    "start": "1781460",
    "end": "1788539"
  },
  {
    "text": "say you partition all the gpus by half so you have double the number of gpus this strategy will work the same you can",
    "start": "1788539",
    "end": "1795140"
  },
  {
    "text": "deploy a pool of machines that are partitioned already beforehand and then",
    "start": "1795140",
    "end": "1800240"
  },
  {
    "text": "you can schedule jobs on them yeah I think like question right actually in a GPU in a GPU Based training I didn't see",
    "start": "1800240",
    "end": "1806419"
  },
  {
    "text": "like uh people are trying to use the virtualizations basically because we have many strategies to get the gpua",
    "start": "1806419",
    "end": "1813260"
  },
  {
    "text": "utilized for example we can increase the batch size right we can also like to do to like use uh like the motor partisan",
    "start": "1813260",
    "end": "1821000"
  },
  {
    "text": "if you I mean that is because the motor is too large so basically we have different ways to saturate if you during",
    "start": "1821000",
    "end": "1827659"
  },
  {
    "text": "the during the training so I I didn't see a lot of examples about like try to I think the infant size an inference",
    "start": "1827659",
    "end": "1834380"
  },
  {
    "text": "yeah there's still a lot of like this kind of improved utilizations by like like doing the virtualization to speed",
    "start": "1834380",
    "end": "1841520"
  },
  {
    "text": "the GPU for multiple like usage thank you",
    "start": "1841520",
    "end": "1847360"
  },
  {
    "text": "that's uh sorry all the time we had for today folks uh thank you all and thank you again to Netflix [Applause]",
    "start": "1847580",
    "end": "1857730"
  }
]