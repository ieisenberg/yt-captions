[
  {
    "text": "hi thanks so much guys it's great to be here and uh it's great honor to to speak at race Summit so we are left in AI who",
    "start": "5540",
    "end": "14219"
  },
  {
    "text": "is basically uh aiming to provide lunch ready AI modules for you guys to build applications as easy as possible with AI",
    "start": "14219",
    "end": "21060"
  },
  {
    "text": "capabilities we've got a bunch of Open Source contributions and from our um from our co-founders and family members",
    "start": "21060",
    "end": "28619"
  },
  {
    "text": "in the in the community and if you heard about you know pytorch Cafe tensorflow Onyx Jupiter Labs kubernetes etcd we",
    "start": "28619",
    "end": "37620"
  },
  {
    "text": "might have created or contributed to those kind of libraries um first of all let's address the",
    "start": "37620",
    "end": "43140"
  },
  {
    "text": "problem the the elephant in the room which is like is an API is enough for you already right there's import open Ai",
    "start": "43140",
    "end": "49379"
  },
  {
    "text": "and it seems to be that everything is really great it's just like one chat away um I'm not going to be answering this",
    "start": "49379",
    "end": "55440"
  },
  {
    "text": "question directly but I'll just like show you some data and if we take a look at the statistics we were really proud",
    "start": "55440",
    "end": "63000"
  },
  {
    "text": "of you a few years ago when we're basically doing pythology and was one of the fastest growing open source projects",
    "start": "63000",
    "end": "68520"
  },
  {
    "text": "if you look at the start history which isn't a good metric but maybe the one of",
    "start": "68520",
    "end": "74340"
  },
  {
    "text": "the best metrics you can see that there are a lot of Open Source projects that are related to models applications and",
    "start": "74340",
    "end": "79920"
  },
  {
    "text": "things like that that have been growing in some ways slightly or much faster than pytorch hugging phase has been one",
    "start": "79920",
    "end": "86340"
  },
  {
    "text": "of the examples and argument face right now has tons of modules and and recently Facebook or better open source llama",
    "start": "86340",
    "end": "93180"
  },
  {
    "text": "which allows people to basically have an open source chat capability and also code generation things like that and if",
    "start": "93180",
    "end": "98759"
  },
  {
    "text": "you look at the star history is growing really fast the fact that people care to care a lot about domain specific models",
    "start": "98759",
    "end": "106560"
  },
  {
    "text": "customization with these models being able to access those models and use those models in an open source way and",
    "start": "106560",
    "end": "112680"
  },
  {
    "text": "being able to apply apply to their own Fields whether it be multimodal applications or they need Enterprise",
    "start": "112680",
    "end": "118320"
  },
  {
    "text": "level security or basically having to ship those models on the edge of things like that there's really a market or a",
    "start": "118320",
    "end": "126060"
  },
  {
    "text": "need Beyond apis to ship their own models launch their own models and optimize on top of that",
    "start": "126060",
    "end": "132360"
  },
  {
    "text": "so if you think about it um if that one isn't really convincing enough let me show you once and imagine your CFO and",
    "start": "132360",
    "end": "138900"
  },
  {
    "text": "you're in a learning Hall one of our one of our accounting members are usually who are sitting back there basically",
    "start": "138900",
    "end": "144300"
  },
  {
    "text": "made a cute demo and showing the difference between a open source API model which is strategy BT and you know",
    "start": "144300",
    "end": "150420"
  },
  {
    "text": "like the uh the uh the the fine-tuned model that we did based on top of llama and we basically fed Apple's earning",
    "start": "150420",
    "end": "158160"
  },
  {
    "text": "call to train that model and well actually we basically like gave all the gave the model A lot of the the ironical results",
    "start": "158160",
    "end": "164340"
  },
  {
    "text": "from the from the earlier CFOs and basically we asked the model to then behave as if it were a CFO and this is",
    "start": "164340",
    "end": "172379"
  },
  {
    "text": "the result that we're getting which is kind of cute we basically ask the bot to answer the potential for the growth in",
    "start": "172379",
    "end": "178680"
  },
  {
    "text": "the number of Apple devices per iPhone things like that um charitivity gave you know like a kind",
    "start": "178680",
    "end": "184140"
  },
  {
    "text": "of reasonable kind of answer but if you look at the model that's basically fine-tuned it kind of really looks like",
    "start": "184140",
    "end": "190920"
  },
  {
    "text": "a CFO right customized models actually go a long way to give you much higher",
    "start": "190920",
    "end": "196260"
  },
  {
    "text": "quality and also kind of like domain specific kind of behaviors you can kind of imagine that a CFO would be basically",
    "start": "196260",
    "end": "202440"
  },
  {
    "text": "saying we're really happy and that's a great question and also little words in a company q a kind of session if you're",
    "start": "202440",
    "end": "208739"
  },
  {
    "text": "thinking about applications such as industry and things like that these kind of models fine-tuned on your own data",
    "start": "208739",
    "end": "214019"
  },
  {
    "text": "and your applications could go quite a long way and that's why there's quite a lot of Need for model customizations and",
    "start": "214019",
    "end": "220140"
  },
  {
    "text": "things like that now the tricky thing is that shipping is going to model is kind of tricky right",
    "start": "220140",
    "end": "226080"
  },
  {
    "text": "um if you think about it over the last 10 or maybe 15 years we've gone through a lot of trouble trying to get familiar",
    "start": "226080",
    "end": "233040"
  },
  {
    "text": "with the software and Hardware Stacks we kind of started with Nvidia Cuda and things like that we've gone through",
    "start": "233040",
    "end": "238500"
  },
  {
    "text": "multiple rounds of AI framework sentence like that and even today we're still hearing a lot of new and interesting but",
    "start": "238500",
    "end": "245640"
  },
  {
    "text": "sometimes hard to learn kind of abstractions we've probably heard about tensor RT 1030 has a new model a new",
    "start": "245640",
    "end": "251700"
  },
  {
    "text": "version called trt llm you might have heard about a new language called Mojo coming up and things like that they're all great but what if I were a data",
    "start": "251700",
    "end": "259019"
  },
  {
    "text": "scientist what if I just want to run those kind of modules and I don't really want to go through five years of PhD on",
    "start": "259019",
    "end": "264960"
  },
  {
    "text": "a grad study to be able to start getting my hands onto those models what is wrong with AI software stack right if you look",
    "start": "264960",
    "end": "271560"
  },
  {
    "text": "at the history of computing you kind of feel that you know there's a lot of applications that are actually not as",
    "start": "271560",
    "end": "277199"
  },
  {
    "text": "not as difficult as AI if you think about databases there's a SQL and you don't need to like",
    "start": "277199",
    "end": "283560"
  },
  {
    "text": "if you use the database you never worry about the underlying you know like the machines and things like that whether the load is working well or things like",
    "start": "283560",
    "end": "289680"
  },
  {
    "text": "that you write a SQL which is a very nice the abstraction language and it works if you want to build a web app",
    "start": "289680",
    "end": "295740"
  },
  {
    "text": "it's kind of easy because you basically write a program in Python JavaScript npm or whatever kind of like boundary is",
    "start": "295740",
    "end": "300960"
  },
  {
    "text": "like and that your mindset is it runs inside one machine a simple machine right a CPU machine and",
    "start": "300960",
    "end": "307620"
  },
  {
    "text": "then you give it to a platform like Versa or some other website websites that host those pages and then you run",
    "start": "307620",
    "end": "314280"
  },
  {
    "text": "you run happy as a clan you don't need to worry about the scaling instance like that because abstraction is easy",
    "start": "314280",
    "end": "320220"
  },
  {
    "text": "AI today is kind of a tricky because I don't really know how to call it like you basically have to go on to hugging phase sometimes the model works",
    "start": "320220",
    "end": "326400"
  },
  {
    "text": "sometimes the model doesn't work sometimes you need to download models from GitHub they have a Jupiter notebook",
    "start": "326400",
    "end": "331919"
  },
  {
    "text": "that runs things and it tells you explicitly in that you get a notebook if this doesn't work try those other",
    "start": "331919",
    "end": "338039"
  },
  {
    "text": "approaches try and uninstall that module that dependency and then pull it up again and see if it works those kind of",
    "start": "338039",
    "end": "344280"
  },
  {
    "text": "things are kind of tricky and this is basically exactly what we have been trying to address there is a big market",
    "start": "344280",
    "end": "350460"
  },
  {
    "text": "supply and demand is pretty big right now if you look at hugging face there are something like 6K models last",
    "start": "350460",
    "end": "358320"
  },
  {
    "text": "year and this year I checked yesterday there are about",
    "start": "358320",
    "end": "363840"
  },
  {
    "text": "300 well actually I think I think I think I think I was actually like a 10 times smaller last year there was about",
    "start": "363840",
    "end": "369240"
  },
  {
    "text": "60k sorry about that and this year when I checked yesterday there was about 334k models up there a huge amount of",
    "start": "369240",
    "end": "377100"
  },
  {
    "text": "modules and the demand is also kind of pretty big in the sense that not only AI Engineers are also kind of a lot of you",
    "start": "377100",
    "end": "383220"
  },
  {
    "text": "know like data scientists application Builders and folks who want to get access to those models GPT models is one",
    "start": "383220",
    "end": "389960"
  },
  {
    "text": "prominent example of why different kind of people without AI backgrounds once you get access to those models right all",
    "start": "389960",
    "end": "396780"
  },
  {
    "text": "right so um so we kind of wanted to basically like write an open source Library that helps people to get access to those",
    "start": "396780",
    "end": "403620"
  },
  {
    "text": "models and some of our key design parameters what kind of things we want to have in this in this Library we kind",
    "start": "403620",
    "end": "411000"
  },
  {
    "text": "of used our experiences back in the in our days when we when we were AI developers to design The Guiding",
    "start": "411000",
    "end": "417780"
  },
  {
    "text": "principles one right now python is probably the de facto standard for all those AI models and model building",
    "start": "417780",
    "end": "423720"
  },
  {
    "text": "application building right so it has to be python if we don't really want it to be like too difficult and it has to be",
    "start": "423720",
    "end": "429419"
  },
  {
    "text": "user friendly in the sense that we don't really want people to go through all those kind of AI software stack to be",
    "start": "429419",
    "end": "435479"
  },
  {
    "text": "able to run a single model it should be as easy as running the simple python function",
    "start": "435479",
    "end": "440639"
  },
  {
    "text": "um it has to be flexible in the sense that you know like it doesn't only run in a Jupiter notebook it has to be",
    "start": "440639",
    "end": "447000"
  },
  {
    "text": "seamlessly converted into a web service so that on the other side if there are JavaScript Engineers their web web",
    "start": "447000",
    "end": "453180"
  },
  {
    "text": "application Builders who want to use those modules they can use it as if it were a standard open API call",
    "start": "453180",
    "end": "459180"
  },
  {
    "text": "the last one might be might be a little bit controversial I think we don't really want docker today there's a lot of uh you know",
    "start": "459180",
    "end": "466860"
  },
  {
    "text": "platforms that are built on Docker and we actually use Docker under the hood as well but there's one Cavite we don't",
    "start": "466860",
    "end": "472979"
  },
  {
    "text": "really want users to directly interface with Docker right and I'm kind of wondering and if if I want to if you",
    "start": "472979",
    "end": "480000"
  },
  {
    "text": "don't mind doing a show of hands how many people have deployed or run AI algorithms like in pytorch",
    "start": "480000",
    "end": "487620"
  },
  {
    "text": "not too many or a few how many of you have built Docker containers before",
    "start": "487620",
    "end": "493560"
  },
  {
    "text": "Ah that's a good amount how many of you have run AI algorithms and at the same",
    "start": "493560",
    "end": "499560"
  },
  {
    "text": "time build Docker with that and for those of you who have done those",
    "start": "499560",
    "end": "506699"
  },
  {
    "text": "kind of things how much you like the experience I'm expecting the answer to the last",
    "start": "506699",
    "end": "512940"
  },
  {
    "text": "question to be no because I mean Doc is great for infrastructure Builders right if I want to basically ship some module",
    "start": "512940",
    "end": "518279"
  },
  {
    "text": "over the ship some kind of software like a database or like big data and things like that is great for AI I was like I've been using python",
    "start": "518279",
    "end": "525540"
  },
  {
    "text": "for such a long time and in fact the reason we kind of went from the first generation AI Frameworks like Cafe",
    "start": "525540",
    "end": "531720"
  },
  {
    "text": "thiano torch 2 today's Pi torch is really because we love python right we want to stay in the python environment",
    "start": "531720",
    "end": "537720"
  },
  {
    "text": "and you know why don't we just run things in Python that's how guiding principle and we recently actually as of",
    "start": "537720",
    "end": "544380"
  },
  {
    "text": "two weeks ago we open sourced a library called leptinessdk by designing a key",
    "start": "544380",
    "end": "550440"
  },
  {
    "text": "abstraction term called Photon as a standard abstraction to build AI applications we can do some code later",
    "start": "550440",
    "end": "556740"
  },
  {
    "text": "and then the basic idea is that you just need to basically specify what kind of things you want to run and then we pull",
    "start": "556740",
    "end": "563279"
  },
  {
    "text": "up a model for you in a pythonic environment on the other side you could use python or you could also use uh you",
    "start": "563279",
    "end": "570300"
  },
  {
    "text": "know like JavaScript and things like that to call into it now I've been bullshiting for quite some time it's about 10 minutes and uh one",
    "start": "570300",
    "end": "577800"
  },
  {
    "text": "thing I like about the ray conference is that there's so many people showing code I mean trust me I've been in China for",
    "start": "577800",
    "end": "583320"
  },
  {
    "text": "the last four years and I don't see too much people showing code and the love showing code in a in a in a developer",
    "start": "583320",
    "end": "589380"
  },
  {
    "text": "conference so forgive me I want to basically like show you some code that could basically um give you a quick idea",
    "start": "589380",
    "end": "595500"
  },
  {
    "text": "about what leptin Photon and things like look like right okay so it's it's light coding if",
    "start": "595500",
    "end": "602279"
  },
  {
    "text": "something goes down I apologize in advance uh essentially basically just want to import stuff let's first of all",
    "start": "602279",
    "end": "607740"
  },
  {
    "text": "pull up a gpg2 module on my local box right now uh the reason I don't really",
    "start": "607740",
    "end": "612839"
  },
  {
    "text": "do llama instance like that is that my Pro laptop doesn't really have it have enough memory gvg2 is good enough so",
    "start": "612839",
    "end": "618839"
  },
  {
    "text": "usually what you do is after you install lepton you can basically say laptop Photon run you give it a module type",
    "start": "618839",
    "end": "624959"
  },
  {
    "text": "called hug and face 2pd2 and it should run a local service that allows you to access it now since we're in a notebook",
    "start": "624959",
    "end": "632100"
  },
  {
    "text": "we basically want to like also continue running the the block so I just launched it in a sub process which is the",
    "start": "632100",
    "end": "637800"
  },
  {
    "text": "equivalent of running things in a console right so basically what it does is it creates a python class that",
    "start": "637800",
    "end": "645360"
  },
  {
    "text": "downloads the models from hug and face whereas cache so hopefully it's fast and then it basically launches a service",
    "start": "645360",
    "end": "651480"
  },
  {
    "text": "it's a standard open API service so that you know like if you want to basically use your favorite of an API",
    "start": "651480",
    "end": "657959"
  },
  {
    "text": "client in npm or things like that there are links and stuff like that you can do it now of course in this case we also",
    "start": "657959",
    "end": "665100"
  },
  {
    "text": "provide a client that allows you to run those kind of things as if you're just calling local functions right so um we",
    "start": "665100",
    "end": "671519"
  },
  {
    "text": "create a client and we basically ask it what kind of methods what kind of like pythonic methods does This Server expose",
    "start": "671519",
    "end": "679920"
  },
  {
    "text": "so let's connect to it and it tells us that there is a run function now when",
    "start": "679920",
    "end": "685620"
  },
  {
    "text": "you use web services there's one thing that's like you've always needed you always need to basically look at documentations and things like that so",
    "start": "685620",
    "end": "691200"
  },
  {
    "text": "we embed this documentation for you there is a method called run and we can basically say hey let me let me know",
    "start": "691200",
    "end": "696899"
  },
  {
    "text": "what kind of parameters this run accepts right so obviously help client.run it automatically generates the input in a",
    "start": "696899",
    "end": "703560"
  },
  {
    "text": "pythonic fashion it basically says it runs something the input schema is it it requires you to input these kind of",
    "start": "703560",
    "end": "709980"
  },
  {
    "text": "things kind of similar to a standard you know like a input set you need to do for llms and it gives you an example input",
    "start": "709980",
    "end": "716700"
  },
  {
    "text": "basically saying I enjoy working with my computer talk and things like that now with that hopefully you will be able",
    "start": "716700",
    "end": "721740"
  },
  {
    "text": "to construct a function that actually calls that then so let's say result equals to client.run and I'll just give",
    "start": "721740",
    "end": "727560"
  },
  {
    "text": "it a prompt right everything is in Python and it gives you a string which is the result Let's ignore the spreads",
    "start": "727560",
    "end": "734220"
  },
  {
    "text": "and it's the internal log and then basically it says it returns I love working fine talk and he's really liked",
    "start": "734220",
    "end": "739620"
  },
  {
    "text": "us for about five months blah blah blah blah blah gvg2 isn't really a good module but it shows you the basic idea about how to",
    "start": "739620",
    "end": "746220"
  },
  {
    "text": "actually launch that model right well shut this off and basically say go to",
    "start": "746220",
    "end": "751500"
  },
  {
    "text": "the next steps imagine you are a model producer you're like I have a bunch of python code now one of the common challenge",
    "start": "751500",
    "end": "759120"
  },
  {
    "text": "that we often face is how to make it accessible to my colleagues right uh we",
    "start": "759120",
    "end": "764700"
  },
  {
    "text": "usually what we do as well yeah here's a python if it's a notebook just run that notebook but often on the other side my",
    "start": "764700",
    "end": "770880"
  },
  {
    "text": "colleague is trying to build some applications and things like that and he was like whoa could you give me an API right so in this case let me show you a",
    "start": "770880",
    "end": "778019"
  },
  {
    "text": "simple example being able to do that uh I don't really want to like like show two complex codes so I basically wrote",
    "start": "778019",
    "end": "783959"
  },
  {
    "text": "up a simple example about mandelbrot kind of like generating a man over art image right it's a simple python",
    "start": "783959",
    "end": "791959"
  },
  {
    "text": "example you basically say we write a class we write all those kind of things as if they are standard python classes",
    "start": "791959",
    "end": "798600"
  },
  {
    "text": "you can basically Define a function with that returns a string you can Define the general function which is also python",
    "start": "798600",
    "end": "803760"
  },
  {
    "text": "function it accepts a bunch of parameters and things like that and it runs a bunch of torch computations and",
    "start": "803760",
    "end": "810180"
  },
  {
    "text": "then it returns AR AAA towards 10 now we want to basically",
    "start": "810180",
    "end": "816839"
  },
  {
    "text": "be compatible with both Python and also JavaScript so basically we convert that torch tensor to a list but simple as",
    "start": "816839",
    "end": "823320"
  },
  {
    "text": "that so that's basically just a python class right we're also kind of like basically Define a helper function to",
    "start": "823320",
    "end": "828720"
  },
  {
    "text": "show the images let's show this one and now we are at the debugging phase",
    "start": "828720",
    "end": "833820"
  },
  {
    "text": "we're not debugging you don't really want to debug a web service right you want to basically run things in Python and use all those pdb and things like",
    "start": "833820",
    "end": "840360"
  },
  {
    "text": "that to do it that class can be instantiated as an instance and then you can basically use standard python",
    "start": "840360",
    "end": "846860"
  },
  {
    "text": "methods to call it and get the results in this case an image so we run it and",
    "start": "846860",
    "end": "852120"
  },
  {
    "text": "it shows you that image looks pretty good if there were bugs and things like that it standard python exceptions pop",
    "start": "852120",
    "end": "859500"
  },
  {
    "text": "up allowing you to change the code and things like that now you're like well now I can basically",
    "start": "859500",
    "end": "864959"
  },
  {
    "text": "run it as a as a as a service a local service in a separate process not in this process a separate process we can",
    "start": "864959",
    "end": "871200"
  },
  {
    "text": "save the above code in the in the dot Pi file I've just did it offline and we can basically say python listen.pi what what",
    "start": "871200",
    "end": "879240"
  },
  {
    "text": "happens is that code eventually the pi the photon wrapper would allow you to convert that python class to",
    "start": "879240",
    "end": "885540"
  },
  {
    "text": "instantiation of that python class into a web service that service will expose two functions",
    "start": "885540",
    "end": "891779"
  },
  {
    "text": "one is alive as we just designed and another function that's called generate",
    "start": "891779",
    "end": "897420"
  },
  {
    "text": "you can then basically use the same client to basically call it as if see exactly",
    "start": "897420",
    "end": "903360"
  },
  {
    "text": "the same python python python code and it still runs under the hood what happens is basically it is using you",
    "start": "903360",
    "end": "910199"
  },
  {
    "text": "know HTTP to talk to the server and I get results back",
    "start": "910199",
    "end": "915660"
  },
  {
    "text": "now uh that's the result exactly the same and things like that now you might ask us lucky this is because",
    "start": "915660",
    "end": "921779"
  },
  {
    "text": "I'm on the same machine why do I need to uh to have it in a separate process right if you think about it a lot of the",
    "start": "921779",
    "end": "928199"
  },
  {
    "text": "AI applications today is like hugely um you know fractured in the sense that it requires different dependencies and",
    "start": "928199",
    "end": "934500"
  },
  {
    "text": "things like that and it's really hard for you to put two models in one environment with different dependencies",
    "start": "934500",
    "end": "940440"
  },
  {
    "text": "and things like that right now if you're able to basically just convert a model into a service you can basically say I",
    "start": "940440",
    "end": "948060"
  },
  {
    "text": "have my conduct environment that has like pie touched this version and tensorflow that version and things like that I'll just run it",
    "start": "948060",
    "end": "954120"
  },
  {
    "text": "and the other side is like I don't need to install anything I just want to call it with numpy or things like that it",
    "start": "954120",
    "end": "959459"
  },
  {
    "text": "gives you a very clean separation of environments so that launching and stuff like that gets much easier now for",
    "start": "959459",
    "end": "965399"
  },
  {
    "text": "example if I launch like a separate python process here right and I can basically just like say",
    "start": "965399",
    "end": "971040"
  },
  {
    "text": "I call this function and it gives you exactly the same results it gives you a good separation between the service side",
    "start": "971040",
    "end": "976800"
  },
  {
    "text": "and the client side um you can then basically you know launch these contents on the cloud and",
    "start": "976800",
    "end": "982680"
  },
  {
    "text": "we have a service platform allowing allowing you to do that let me terminate that um and you can basically say you run",
    "start": "982680",
    "end": "989699"
  },
  {
    "text": "this Photon remotely it automatically automatically uploads this into the server and it runs this stuff now under",
    "start": "989699",
    "end": "996180"
  },
  {
    "text": "the hood we use no container technology and stuff like that but when you are compiling that thing well one simple way",
    "start": "996180",
    "end": "1002480"
  },
  {
    "text": "is basically saying we will compile that simple function that simple python file which is about you know like four",
    "start": "1002480",
    "end": "1008240"
  },
  {
    "text": "kilobytes into a Docker container but that's a Nova kill right why would I",
    "start": "1008240",
    "end": "1013820"
  },
  {
    "text": "want to wrap a 20 gigabyte Docker container just to ship one four kilobyte file instead we use cloud pick one",
    "start": "1013820",
    "end": "1020180"
  },
  {
    "text": "things like that basically pickle the absolute necessities and then ship it to the cloud so that you know like you just",
    "start": "1020180",
    "end": "1025400"
  },
  {
    "text": "need to uh run it and then you get the results out exactly as we had before",
    "start": "1025400",
    "end": "1032860"
  },
  {
    "text": "um it's a remote so it takes a bit of time but usually you know like if you're running you know like really heavy",
    "start": "1033199",
    "end": "1038418"
  },
  {
    "text": "computations and things like that the 10 to 20 milliseconds above head doesn't really hurt too much",
    "start": "1038419",
    "end": "1044600"
  },
  {
    "text": "you can basically run other kind of interesting models such as stable diffusion and things like that and the",
    "start": "1044600",
    "end": "1050120"
  },
  {
    "text": "good thing is now I don't really have a local machine right I have the left and SDK it free ships with an sdxl module",
    "start": "1050120",
    "end": "1056480"
  },
  {
    "text": "but then I want to run it remotely and the good thing is I don't really need a GPU locally but I can still call those",
    "start": "1056480",
    "end": "1063200"
  },
  {
    "text": "kind of services remote as if I'm just running things locally so if I want to debug my you know image generation pipelines and things like that I have a",
    "start": "1063200",
    "end": "1070039"
  },
  {
    "text": "remote GPU for me to naturally use now for the sake of time because pulling up",
    "start": "1070039",
    "end": "1075200"
  },
  {
    "text": "gpus is a little bit slower I already have one service running called sdxr and then I can basically",
    "start": "1075200",
    "end": "1082520"
  },
  {
    "text": "connect to my service kind of similar to what I did before and I will be able to",
    "start": "1082520",
    "end": "1089419"
  },
  {
    "text": "just you know call it it takes some time about five seconds or so to do so and then we will be able to get the results",
    "start": "1089419",
    "end": "1095720"
  },
  {
    "text": "back and show the results and things like that while we wait for the results come back I'll basically tell you one",
    "start": "1095720",
    "end": "1101960"
  },
  {
    "text": "thing if you are seeing you know like gamma 2 modules you know other kind of like so in Japanese there's a model",
    "start": "1101960",
    "end": "1107360"
  },
  {
    "text": "called Elisa in Chinese there's a model called by Trend and things like that pulling up those models is as easy as",
    "start": "1107360",
    "end": "1113660"
  },
  {
    "text": "basically just changing that string that hugging phase string I basically say hey I want to pull up that model you will need to have if you're running things",
    "start": "1113660",
    "end": "1119960"
  },
  {
    "text": "locally you will need to have a beefy GPU to do so but it's as simple as that",
    "start": "1119960",
    "end": "1125840"
  },
  {
    "text": "that result is ready come back and if you plot it it's it looks reasonable",
    "start": "1125840",
    "end": "1131480"
  },
  {
    "text": "good the idea is that you'll be able to just launch a bunch of AI models and things like that in a kind of pythonic",
    "start": "1131480",
    "end": "1137480"
  },
  {
    "text": "way but still enjoying the flexibility of using remote gpus and stuff like that this is basically what we kind of do a",
    "start": "1137480",
    "end": "1144260"
  },
  {
    "text": "quick preview about what kind of things we have right and then on top of that we have quite a lot of things that we could",
    "start": "1144260",
    "end": "1149539"
  },
  {
    "text": "optimize for for example we can uh we can pull up really fast stable diffusion modules we can pull up really fast llama",
    "start": "1149539",
    "end": "1156500"
  },
  {
    "text": "modules and things like that for the sake of time I'm probably not going to be jumping into details about that but",
    "start": "1156500",
    "end": "1161660"
  },
  {
    "text": "there's quite a lot of optimization tricks that's needed in running today's models if you run llama models for",
    "start": "1161660",
    "end": "1168140"
  },
  {
    "text": "example you will need like things like Dynamic batch and Page attention and things like that there are awesome libraries such as vlm for you to do so",
    "start": "1168140",
    "end": "1175280"
  },
  {
    "text": "right we kind of integrate all these kind of things into our service so that you know like you can basically find",
    "start": "1175280",
    "end": "1180799"
  },
  {
    "text": "something up to 19 times speed 90 times speed up when you're running typical models such as long two engines like",
    "start": "1180799",
    "end": "1186860"
  },
  {
    "text": "that there are a lot of Future Works that we could do with like which is like to make it more more faster more concurrent more scale and things like",
    "start": "1186860",
    "end": "1193340"
  },
  {
    "text": "that but that's story for another day now coming to the question the elephant",
    "start": "1193340",
    "end": "1198740"
  },
  {
    "text": "in the room management info is tricky right that's why uh there you see quite a lot of interesting projects coming up",
    "start": "1198740",
    "end": "1204500"
  },
  {
    "text": "in the infrastructure space and and the reason we are basically exploring together with Rey is that we want to",
    "start": "1204500",
    "end": "1210080"
  },
  {
    "text": "give people a lot of flexibility in managing resources and still enjoying the simple",
    "start": "1210080",
    "end": "1216620"
  },
  {
    "text": "the the simpleness of launching models running models running area and stuff like that Ray is pretty great in that",
    "start": "1216620",
    "end": "1223880"
  },
  {
    "text": "sense and let me show you uh uh well before we before I show the actual code let me kind of give you a basic idea in",
    "start": "1223880",
    "end": "1231500"
  },
  {
    "text": "Python in Lambda SDK you will be able to basically pull up models such as Lama models and things like that in the local",
    "start": "1231500",
    "end": "1237799"
  },
  {
    "text": "pythonic environment now Ray gives you a decoration or basically it gives you a way to basically wrap that thin that",
    "start": "1237799",
    "end": "1244760"
  },
  {
    "text": "model into either a remote run or remote service right and then basically in that",
    "start": "1244760",
    "end": "1252080"
  },
  {
    "text": "way all those kind of infrastructure pieces how to actually manage the cluster how do you actually manage the replica the number of replicas in the",
    "start": "1252080",
    "end": "1258559"
  },
  {
    "text": "service settings like that gets very nicely separated as an application developer will be able to just simply",
    "start": "1258559",
    "end": "1264620"
  },
  {
    "text": "focus on our applications and as infrastructure Ray takes care of a lot of these concerns",
    "start": "1264620",
    "end": "1269840"
  },
  {
    "text": "I'm going to be using the um any skills platform to show you uh how",
    "start": "1269840",
    "end": "1275780"
  },
  {
    "text": "do we how we do that so basically install something that was a tiny bug that that requires us to pinp Identity",
    "start": "1275780",
    "end": "1281960"
  },
  {
    "text": "to 1.10 let's do that and then we'll basically initialize array or basically",
    "start": "1281960",
    "end": "1287059"
  },
  {
    "text": "import Ray and as normal right now A minimal example what if we just like one",
    "start": "1287059",
    "end": "1292340"
  },
  {
    "text": "want to write a very very simple Photon right um you basically have a class that's a",
    "start": "1292340",
    "end": "1298340"
  },
  {
    "text": "photon class in leptin and then you basically say I want to basically decorate this class using very remote",
    "start": "1298340",
    "end": "1304760"
  },
  {
    "text": "and if it becomes a class that you can run it becomes a very remote object that actor that you can run in the red",
    "start": "1304760",
    "end": "1311299"
  },
  {
    "text": "cluster when you run since everything gets very nicely connected you get the",
    "start": "1311299",
    "end": "1316580"
  },
  {
    "text": "result as you are you'll be expecting right now let's say we want to run something more complex in this case let",
    "start": "1316580",
    "end": "1322760"
  },
  {
    "text": "me show you something that's that that's that's a common model used in the in AI which is called clip uh it embeds images",
    "start": "1322760",
    "end": "1329720"
  },
  {
    "text": "and video and text at the same time so now usually you write a python class right the interesting thing about this",
    "start": "1329720",
    "end": "1336500"
  },
  {
    "text": "class is we'll be able to expose multiple methods multiple endpoints so in this case let me scroll down the",
    "start": "1336500",
    "end": "1341840"
  },
  {
    "text": "unusual the useless code and what it does is it embeds it gives you a",
    "start": "1341840",
    "end": "1347000"
  },
  {
    "text": "function called embed a function called embed text a function called embed image and it's basically a long class that",
    "start": "1347000",
    "end": "1353360"
  },
  {
    "text": "does these constants in Python right now we call that thin and then we",
    "start": "1353360",
    "end": "1358700"
  },
  {
    "text": "instantiate two things one is basically we can run things locally by basically saying we instantiate that class or we",
    "start": "1358700",
    "end": "1364640"
  },
  {
    "text": "can you say we used very remote to decorate it it becomes a a remote actor",
    "start": "1364640",
    "end": "1370640"
  },
  {
    "text": "and then we can run these two things at the same time and compare the results",
    "start": "1370640",
    "end": "1376700"
  },
  {
    "text": "Ray is going to be doing scheduling and things like that to run the inside background and uh is the clustered dead",
    "start": "1376700",
    "end": "1384320"
  },
  {
    "text": "um yeah it works so if you compare the results numerical results they are about the same right you know that they are",
    "start": "1384320",
    "end": "1389900"
  },
  {
    "text": "running since in two different environments mainly because the the final bits are usually different that's",
    "start": "1389900",
    "end": "1395059"
  },
  {
    "text": "a common pattern in float computations numerical computations but if you look at it they're about the same results you",
    "start": "1395059",
    "end": "1401360"
  },
  {
    "text": "can basically embed a cat image or things like that as well it gives you a lot of flexibility interesting that",
    "start": "1401360",
    "end": "1407720"
  },
  {
    "text": "image is not showing up probably because of network issues but the invading result is is basically exactly the same",
    "start": "1407720",
    "end": "1413059"
  },
  {
    "text": "The Good the the good thing is basically once you write a class you can run things or debug thems locally and you",
    "start": "1413059",
    "end": "1419360"
  },
  {
    "text": "can use Ray existing infrastructure to run all those kind of things remotely as well right now I want to deploy things",
    "start": "1419360",
    "end": "1425659"
  },
  {
    "text": "it's kind of similar as well Reserve has a way to basically offload or basically like to to reroute Ingress",
    "start": "1425659",
    "end": "1433280"
  },
  {
    "text": "to a faster API application which is the underlying underlying server that we use for leptin",
    "start": "1433280",
    "end": "1440240"
  },
  {
    "text": "AI as well so you basically say you write an adapter you instantiate that class and then you",
    "start": "1440240",
    "end": "1446720"
  },
  {
    "text": "ask Ray serve to reroute all those all those traffic into that application that",
    "start": "1446720",
    "end": "1453140"
  },
  {
    "text": "we use right um I'm probably going to be running out of time so I'm not going to be diving too deep into the code but let's run",
    "start": "1453140",
    "end": "1458659"
  },
  {
    "text": "that and then we can basically say racer.run it binds to a service address in this",
    "start": "1458659",
    "end": "1464900"
  },
  {
    "text": "case it's local and we will basically just call it using kind of like HTTP",
    "start": "1464900",
    "end": "1470600"
  },
  {
    "text": "requests and things like that right so it gives you the ray version and kind of",
    "start": "1470600",
    "end": "1477500"
  },
  {
    "text": "similar it gives you the local results and it gives you the remote results now",
    "start": "1477500",
    "end": "1482539"
  },
  {
    "text": "remember we just talked about the fact that leptin air gives you quite a lot of different modules to very easily pull up",
    "start": "1482539",
    "end": "1488600"
  },
  {
    "text": "let's say you know like if you want to create a model called where's model strain",
    "start": "1488600",
    "end": "1493640"
  },
  {
    "text": "um oh yeah so you basically create a general generic rate adapter that that takes in a model string and then",
    "start": "1493640",
    "end": "1500299"
  },
  {
    "text": "instantiate that model now for the sake of speed I'm still going to be using hf2pd2 but you can plug in llama2 and",
    "start": "1500299",
    "end": "1507500"
  },
  {
    "text": "you'll request the GPU and it runs the same as well Razer pulls up test service",
    "start": "1507500",
    "end": "1513020"
  },
  {
    "text": "and then when you basically tries to call it you try to call it ah it takes",
    "start": "1513020",
    "end": "1518059"
  },
  {
    "text": "some time and also mainly because it's going to be long downloading the uh the hug based models and things like that",
    "start": "1518059",
    "end": "1523760"
  },
  {
    "text": "let's wait for half a minute um once it starts up then you will be able to just call it similar to you know",
    "start": "1523760",
    "end": "1530720"
  },
  {
    "text": "like uh the way you call every kind of single race serving function and things like that",
    "start": "1530720",
    "end": "1535900"
  },
  {
    "text": "you can also use clients that gives you like interest in uh documentations and",
    "start": "1535900",
    "end": "1541760"
  },
  {
    "text": "things like that to continue keeping in a python environment for your",
    "start": "1541760",
    "end": "1547460"
  },
  {
    "text": "development and stuff like that so you can just call it as if it were a python function and you get exactly the same result that",
    "start": "1547460",
    "end": "1554659"
  },
  {
    "text": "you will be getting with the HTTP service a sentence like that now imagine this is a very simple gpd2 kind of",
    "start": "1554659",
    "end": "1561620"
  },
  {
    "text": "module for the local time imagine if you can basically use the same number of lines of code to pull up llama to pull",
    "start": "1561620",
    "end": "1567559"
  },
  {
    "text": "up organs like your favorite modules on hugging face that would hopefully be much more convenient for you to run",
    "start": "1567559",
    "end": "1573740"
  },
  {
    "text": "applications with all those AI models that are out there okay so let's come back to the",
    "start": "1573740",
    "end": "1579380"
  },
  {
    "text": "conclusions because I'm probably running out of time um we really want to create an open",
    "start": "1579380",
    "end": "1585679"
  },
  {
    "text": "source kind of solution for us to access to to to to allow us access to production ready models with minimal",
    "start": "1585679",
    "end": "1592700"
  },
  {
    "text": "code right one seems to be pythonic we want you to not worry too much about infrastructure pieces and things like",
    "start": "1592700",
    "end": "1598400"
  },
  {
    "text": "that on and you will be able to just like enjoy the plethora of Open Source",
    "start": "1598400",
    "end": "1604340"
  },
  {
    "text": "models out there we of course do a bunch of optimizations and things like that so you can kind of stay sure that the launch models are",
    "start": "1604340",
    "end": "1611600"
  },
  {
    "text": "high performance high throughput and things like that if you're a ray user already then",
    "start": "1611600",
    "end": "1617299"
  },
  {
    "text": "using leptin AI is as simple as adding one dependency in your right environment",
    "start": "1617299",
    "end": "1623299"
  },
  {
    "text": "the thing we really love about Ray is that it gives you a very simple again",
    "start": "1623299",
    "end": "1628340"
  },
  {
    "text": "pythonic way to define what kind of software packages python packages that you need right so you basically do",
    "start": "1628340",
    "end": "1634400"
  },
  {
    "text": "runtime environment add in one dependency over there you initialize the environment and you'll be able to run",
    "start": "1634400",
    "end": "1639919"
  },
  {
    "text": "all those kind of models with ease on the left well actually on the on on the",
    "start": "1639919",
    "end": "1645200"
  },
  {
    "text": "right these are a bunch of kind of in-like demos that we kind of put up with like QR code Generations code llama",
    "start": "1645200",
    "end": "1651440"
  },
  {
    "text": "and things like that if you go to leptin.ai homepage you'll be able to check that out and there's also a",
    "start": "1651440",
    "end": "1656600"
  },
  {
    "text": "whisper mode when things like that just for fun and I will really our goal is there really to basically provide apis",
    "start": "1656600",
    "end": "1661940"
  },
  {
    "text": "our goal is basically to help you to get access to those kind of models as easy as possible",
    "start": "1661940",
    "end": "1668360"
  },
  {
    "text": "uh so the whole code base is open and we are going to be putting up the array",
    "start": "1668360",
    "end": "1674659"
  },
  {
    "text": "integration examples as well um for our platform we're in beta just a Shameless advertisement so if you're",
    "start": "1674659",
    "end": "1680059"
  },
  {
    "text": "interested in basically using a serverless platform that runs AI models and things like that um feel free to contact us",
    "start": "1680059",
    "end": "1686840"
  },
  {
    "text": "um that's our home page and uh and then and then we're in contact will help you to run am more easily thank you",
    "start": "1686840",
    "end": "1693450"
  },
  {
    "text": "[Applause]",
    "start": "1693450",
    "end": "1699649"
  },
  {
    "text": "great I think we have time for just uh one question yep",
    "start": "1702380",
    "end": "1707900"
  },
  {
    "text": "anyone has a yes sir",
    "start": "1707900",
    "end": "1711700"
  },
  {
    "text": "uh thank you for the talk so uh you demoed two ways of basically building",
    "start": "1715520",
    "end": "1721520"
  },
  {
    "text": "RPC Services right one one is using the laptop AI itself and the photon thing",
    "start": "1721520",
    "end": "1727400"
  },
  {
    "text": "the other is using Ray serve right so can you give some guidance like when should one use a versus B right yeah so",
    "start": "1727400",
    "end": "1735740"
  },
  {
    "text": "basically they are kind of complementary in the sense that when you write a photon class",
    "start": "1735740",
    "end": "1740779"
  },
  {
    "text": "um it really enables you to basically do local debugging and things like that really easily right you can basically just run it in process as if it were",
    "start": "1740779",
    "end": "1747799"
  },
  {
    "text": "just like python class or you can basically invoke a launch function that didn't show and it gives you a local",
    "start": "1747799",
    "end": "1753700"
  },
  {
    "text": "HTTP service now the question then becomes how to actually put it remote right how do I",
    "start": "1753700",
    "end": "1759020"
  },
  {
    "text": "put it on a GPU server and things like that which is basically where Ray remote comes in already serve comes in Ray",
    "start": "1759020",
    "end": "1764419"
  },
  {
    "text": "basically says I'll take that class and I'll ship the code onto a remote machine on that cluster",
    "start": "1764419",
    "end": "1771799"
  },
  {
    "text": "and then we will be able to select call that thing remotely now under the hood for in our platform we basically like",
    "start": "1771799",
    "end": "1778279"
  },
  {
    "text": "provide those kind of capabilities in a serverless way but you know like if you are basically having a cluster and that",
    "start": "1778279",
    "end": "1783740"
  },
  {
    "text": "kind of resource orchestration becomes really easy and also there's the difference between very remote and Ray",
    "start": "1783740",
    "end": "1789320"
  },
  {
    "text": "serve right I think remote is really good for like batch compute and things like that and serve is you know if you",
    "start": "1789320",
    "end": "1794779"
  },
  {
    "text": "want to put things online for others call into and things like that that's serve so",
    "start": "1794779",
    "end": "1800380"
  },
  {
    "text": "cool cool well thank you so much all right thanks so much",
    "start": "1800779",
    "end": "1806140"
  },
  {
    "text": "[Applause]",
    "start": "1806180",
    "end": "1809599"
  }
]