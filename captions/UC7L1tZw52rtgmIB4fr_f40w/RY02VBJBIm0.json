[
  {
    "text": "well good evening my name is Jose welcome to the Bay Area in-person Meetup",
    "start": "986000",
    "end": "992820"
  },
  {
    "text": "this has been a while since we had a official rate area made up uh how many brilliant were welcome delighted to all",
    "start": "992820",
    "end": "999720"
  },
  {
    "text": "of you to actually hear here how many of you actually have used Ray before heard of Ray before okay where you can stay",
    "start": "999720",
    "end": "1005720"
  },
  {
    "text": "please well thanks for coming on the sort of um moderating the the Meetup today and",
    "start": "1005720",
    "end": "1013160"
  },
  {
    "text": "uh we have actually a a good agenda today we have about three talks I'm gonna go very briefly about the upcoming",
    "start": "1013160",
    "end": "1019839"
  },
  {
    "text": "announcements what's actually coming and what's actually happening in the rare community and I will have three talks",
    "start": "1019839",
    "end": "1025819"
  },
  {
    "text": "one from now those of you who actually are working in the machine learning you",
    "start": "1025819",
    "end": "1031280"
  },
  {
    "text": "know that batch inference is really Central it's it's one of the very common workloads and today with the size of the",
    "start": "1031280",
    "end": "1038900"
  },
  {
    "text": "models increasing and the size of the compute you actually need to do batch entrance at scale so mortgage is going",
    "start": "1038900",
    "end": "1044839"
  },
  {
    "text": "to talk about what are some of the challenges that you actually had to deal with when you're doing batch in printed",
    "start": "1044839",
    "end": "1050600"
  },
  {
    "text": "scale what are the existing Solutions available for you today and how does ray fit into the ground scheme of things and",
    "start": "1050600",
    "end": "1057740"
  },
  {
    "text": "probably demonstrate that probably it is one of the in an opinionated way the best solution available for you to do",
    "start": "1057740",
    "end": "1063380"
  },
  {
    "text": "batch in prints and massive scale and then very much similar to what mogas is",
    "start": "1063380",
    "end": "1068660"
  },
  {
    "text": "going to talk about we're going to have Eric Liang is also one of the committee around Ray and is going to talk about",
    "start": "1068660",
    "end": "1073700"
  },
  {
    "text": "how you can actually use Ray data to saturate both your CPUs and gpus and",
    "start": "1073700",
    "end": "1078980"
  },
  {
    "text": "today when you're doing batch inferences especially when you're dealing with images I mean you're dealing with sound",
    "start": "1078980",
    "end": "1084679"
  },
  {
    "text": "and video recordings you tend to use both your heterogeneous cluster you have your CPUs and you have a gpus and you",
    "start": "1084679",
    "end": "1091100"
  },
  {
    "text": "want to make sure that you optimize and saturate them when they actually are working together so you're going to hear",
    "start": "1091100",
    "end": "1098000"
  },
  {
    "text": "about how how you can actually use radar to stream the data to do your parallel execution and these two talks are very",
    "start": "1098000",
    "end": "1105080"
  },
  {
    "text": "actually related and then we're going to hear from array developer array user from Pinterest and a lot of companies",
    "start": "1105080",
    "end": "1112100"
  },
  {
    "text": "today are actually sort of building machine learning platform on top of Ray Pinterest is one of them doordash it is",
    "start": "1112100",
    "end": "1118940"
  },
  {
    "text": "another one Uber has actually done the same thing Spotify is actually done the same thing so we've got all these",
    "start": "1118940",
    "end": "1124220"
  },
  {
    "text": "different companies who are actually building these their own infrastructure on top of red to expedite the",
    "start": "1124220",
    "end": "1131360"
  },
  {
    "text": "development in Ray so I think that's going to be this three great talks for",
    "start": "1131360",
    "end": "1136760"
  },
  {
    "text": "you to actually have a good news going to be there for us a couple of announcements just about a week ago or maybe last a week ago we",
    "start": "1136760",
    "end": "1144200"
  },
  {
    "text": "actually announced our 2.5 release on Ray now Rachel was announced last year at",
    "start": "1144200",
    "end": "1150020"
  },
  {
    "text": "the summit and there were actually three words that that the the that uh robot",
    "start": "1150020",
    "end": "1155299"
  },
  {
    "text": "said what rate to dollar is going to be about it's going to be simple it's going to be steady and it's going to be swift",
    "start": "1155299",
    "end": "1160640"
  },
  {
    "text": "or performant and since its release we have steadfastly committed ourselves",
    "start": "1160640",
    "end": "1166340"
  },
  {
    "text": "with every incremental release to make sure that we actually aim towards our goal make it simple make it scalable",
    "start": "1166340",
    "end": "1172460"
  },
  {
    "text": "make it easy to use and this particular release has sort of you know keep on going to that particular pillar of our",
    "start": "1172460",
    "end": "1179720"
  },
  {
    "text": "objectives and you can now do distributed training with very train when you're actually building large",
    "start": "1179720",
    "end": "1184880"
  },
  {
    "text": "particular models if the model doesn't actually fit into your one worker you can actually shot the model across and",
    "start": "1184880",
    "end": "1191480"
  },
  {
    "text": "when you didn't check pointing there is a need for you to actually aggregate all the checkpoints on one central place and",
    "start": "1191480",
    "end": "1197660"
  },
  {
    "text": "then load it up to somewhere where you actually want to store it it turns out that that's not the optimal solution because you can either run out of memory",
    "start": "1197660",
    "end": "1203840"
  },
  {
    "text": "because you've got all these checkpoints coming in across and you can also increase the the bandwidth on the network so the solution is to actually",
    "start": "1203840",
    "end": "1210260"
  },
  {
    "text": "do distributed checkpointing where each and every model worker will upload and and store the checkpoint in a local",
    "start": "1210260",
    "end": "1216679"
  },
  {
    "text": "storage likewise well you don't put in a racer it has the ability now to serve",
    "start": "1216679",
    "end": "1222860"
  },
  {
    "text": "llm model that scale and one of the things when you actually do inference journal and a model you actually notice",
    "start": "1222860",
    "end": "1228140"
  },
  {
    "text": "to see that when you type in a prompt and you send it in an inference it might take five seconds it might take 30",
    "start": "1228140",
    "end": "1234559"
  },
  {
    "text": "seconds it is just waiting waiting there for for the for the prompt response to come in well one way to actually give",
    "start": "1234559",
    "end": "1240799"
  },
  {
    "text": "you a better user experience is to have yours your your your your your tokens or",
    "start": "1240799",
    "end": "1245900"
  },
  {
    "text": "the next word being streamed back to you so one of the support we have actually added in rare server is to do streaming",
    "start": "1245900",
    "end": "1251539"
  },
  {
    "text": "response so now you actually get the streaming group points right away as the l m is actually Computing what's the",
    "start": "1251539",
    "end": "1257120"
  },
  {
    "text": "next generated word so you actually get the feedback something really similar what you see in charge GPD or one of the",
    "start": "1257120",
    "end": "1262580"
  },
  {
    "text": "advanced proprietary model so you get this good user experience so that's one of the things that you can actually do",
    "start": "1262580",
    "end": "1267860"
  },
  {
    "text": "with red serve and then one of the things we actually added was in the earlier version of the release you",
    "start": "1267860",
    "end": "1273620"
  },
  {
    "text": "couldn't do um a multi RL engine training on multi-gp on Multi",
    "start": "1273620",
    "end": "1280160"
  },
  {
    "text": "machines so that has been sort of added and that sort of gives you a good advantage of it when you're doing your",
    "start": "1280160",
    "end": "1285440"
  },
  {
    "text": "reimburse your reinforcement learning and finally we're going to hear about today what are some of the performance",
    "start": "1285440",
    "end": "1291260"
  },
  {
    "text": "enhancements you can actually do on Ray at a batch infrons at scale so that's in 2.5 we have a Blog out there you can",
    "start": "1291260",
    "end": "1297799"
  },
  {
    "text": "actually uh talk about that blog now talk about llm a couple of months ago we",
    "start": "1297799",
    "end": "1303260"
  },
  {
    "text": "actually released this thing called every which is really an open source way of exploring what the models are out",
    "start": "1303260",
    "end": "1309980"
  },
  {
    "text": "there so if you actually if you're new with a little lamb or you want to figure out what are some of the open source models",
    "start": "1309980",
    "end": "1315799"
  },
  {
    "text": "would it look like how long would it actually take me to do an inference we have this Explorer browser which is a",
    "start": "1315799",
    "end": "1321500"
  },
  {
    "text": "front-end that gives you the ability to actually choose a particular model that you actually want so you can choose any",
    "start": "1321500",
    "end": "1326780"
  },
  {
    "text": "one of the llm models that you want open source models which are hosted on on hugging face",
    "start": "1326780",
    "end": "1331820"
  },
  {
    "text": "you provide a particular prompt to it whatever prompt you actually want and then you do the inference and it",
    "start": "1331820",
    "end": "1337940"
  },
  {
    "text": "actually gives you the three prompts based on the respective models and you",
    "start": "1337940",
    "end": "1343039"
  },
  {
    "text": "cannot compare which was actually the best model which was the worst one how long it actually took and how much it is actually cost so if you're browsing",
    "start": "1343039",
    "end": "1349700"
  },
  {
    "text": "through uh open source models you want to figure out whether your prompt or your particular use case",
    "start": "1349700",
    "end": "1355580"
  },
  {
    "text": "um choose a particular model that we actually have and and you'll be able to do that currently it's open source we also have the ability to host you on",
    "start": "1355580",
    "end": "1362600"
  },
  {
    "text": "particular models that's down down the line you can actually find out more information about that we'll go to",
    "start": "1362600",
    "end": "1367760"
  },
  {
    "text": "everya.com so talking about llms we have another Meetup here tomorrow a house",
    "start": "1367760",
    "end": "1373520"
  },
  {
    "text": "full of great lineable speakers we can have a co-founder and co-creator of Ray",
    "start": "1373520",
    "end": "1379760"
  },
  {
    "text": "is going to be here Harrison case who's known for Lang chain this is going to be here as well Michael tricoat is the",
    "start": "1379760",
    "end": "1386179"
  },
  {
    "text": "founder and CEO the CTO of air byte and it's going to talk about how they actually use private data to host on",
    "start": "1386179",
    "end": "1393380"
  },
  {
    "text": "their on their bike workflow and then the creator of the kuna from UC Berkeley",
    "start": "1393380",
    "end": "1398600"
  },
  {
    "text": "Jiang is going to be here and moderated by Charles fry who is one of the co-founder of of full stack so good meet",
    "start": "1398600",
    "end": "1405919"
  },
  {
    "text": "up tomorrow hope to see you guys here I think it's house full but there's always a room for more if you're interested",
    "start": "1405919",
    "end": "1412039"
  },
  {
    "text": "anybody been to Ray Summit okay maybe this is the first time is is",
    "start": "1412039",
    "end": "1417260"
  },
  {
    "text": "is a good way uh we have something coming up uh in September this is early",
    "start": "1417260",
    "end": "1423080"
  },
  {
    "text": "registration so if you're interested to find out how people are actually using Rey globally uh what are some of the",
    "start": "1423080",
    "end": "1429380"
  },
  {
    "text": "interesting use cases they're solving you know it's great to actually have that we had this Great summit last year",
    "start": "1429380",
    "end": "1434900"
  },
  {
    "text": "was a huge success we had people from all around the world you expect to have twice as many people twice as many",
    "start": "1434900",
    "end": "1440179"
  },
  {
    "text": "training sessions so please do um feel free to to join us in a couple of months to find out everything about",
    "start": "1440179",
    "end": "1446360"
  },
  {
    "text": "Ray and we have a line of community keynote speakers from these luminary",
    "start": "1446360",
    "end": "1452539"
  },
  {
    "text": "companies who are going to share about not only how they're actually using great the company but also what is the the future of distributed computing uh",
    "start": "1452539",
    "end": "1459799"
  },
  {
    "text": "great line of speakers so a good investment for you to know about where Ray is actually heading in the future",
    "start": "1459799",
    "end": "1467720"
  },
  {
    "text": "two books on Ray if you're interested to find out um they're available on Amazon this particular book is written by the",
    "start": "1467720",
    "end": "1473900"
  },
  {
    "text": "commuters over here and that one is written by uh Holman Corral she's one of the early advocate of Open Source and so",
    "start": "1473900",
    "end": "1480620"
  },
  {
    "text": "she's one of the authors as well they're all available feel free to get them if you want to have more about Rey",
    "start": "1480620",
    "end": "1488659"
  },
  {
    "text": "and then finally we hold these meetups on regular basis both online as well as as well as uh in person there are",
    "start": "1488659",
    "end": "1495980"
  },
  {
    "text": "various ways for us for you to guys to join us you can subscribe to any one of the channels I'm normally on discus",
    "start": "1495980",
    "end": "1502340"
  },
  {
    "text": "quite a bit answering questions we have two new followers and if you want to speak and you want to share about your",
    "start": "1502340",
    "end": "1509419"
  },
  {
    "text": "year Journey please get in touch with me and we'll definitely help you as one of the speakers over here now",
    "start": "1509419",
    "end": "1516799"
  },
  {
    "text": "I'm obliged to put this on front normally when we actually have this particular meter if we actually have a code of contact we have we are obliged",
    "start": "1516799",
    "end": "1524059"
  },
  {
    "text": "to to show that I'm not going to read all that for you you can probably just scan it um uh and this is this is nothing but",
    "start": "1524059",
    "end": "1530840"
  },
  {
    "text": "more than making sure that there's another harassment thing for us so welcome and now I'm gonna have a first",
    "start": "1530840",
    "end": "1537919"
  },
  {
    "text": "Speaker emote who is a software engineer here in the scale for the longest time he's one of the commuters of Ray he's",
    "start": "1537919",
    "end": "1543500"
  },
  {
    "text": "been here for for a long time and he's got his Footprints all over the GitHub so um",
    "start": "1543500",
    "end": "1549200"
  },
  {
    "text": "welcome to ammo [Applause]",
    "start": "1549200",
    "end": "1558669"
  },
  {
    "text": "thank you hey everyone thanks for coming today so",
    "start": "1566299",
    "end": "1573140"
  },
  {
    "text": "I'll be giving the first talk um and basically just talking to me setting up the problem that we're really one of",
    "start": "1573140",
    "end": "1579740"
  },
  {
    "text": "the poems that we're trying to solve here and it's uh this particular workload offline batch inference and in",
    "start": "1579740",
    "end": "1584779"
  },
  {
    "text": "particular we're going to be taking a look at like a few different approaches and comparing how Rey fares against spark against uh sagemaker uh batch",
    "start": "1584779",
    "end": "1592760"
  },
  {
    "text": "transform so a little bit about me as Jules mentioned uh I'm one of the developers here on the right team I've",
    "start": "1592760",
    "end": "1597799"
  },
  {
    "text": "been developed working on developing array for uh three years now um yeah I've been working on its entire",
    "start": "1597799",
    "end": "1604340"
  },
  {
    "text": "time in a few different areas such as train like model training and then now focusing on like offline batch inference",
    "start": "1604340",
    "end": "1611840"
  },
  {
    "text": "so just an overview we're going to first go over like what is Bachelor information why does it matter um some of the challenges that people",
    "start": "1611840",
    "end": "1618500"
  },
  {
    "text": "face when actually trying to productionize batch inference and scale this out to large data sets uh we're going to take a look exploring",
    "start": "1618500",
    "end": "1624980"
  },
  {
    "text": "the current solution space for this problem and compare three solutions in particular sagemaker batch transform",
    "start": "1624980",
    "end": "1631640"
  },
  {
    "text": "Apache spark and Ray data sets so first like what is offline batch",
    "start": "1631640",
    "end": "1637039"
  },
  {
    "text": "inference and uh overall it's like pretty pretty simple workbook right like you have some",
    "start": "1637039",
    "end": "1643039"
  },
  {
    "text": "pre-trained model and you want to get predictions from the model in some data set and uh this differs from online",
    "start": "1643039",
    "end": "1648559"
  },
  {
    "text": "inference in that requests are not coming in live you have your input data sets already predefined it's already",
    "start": "1648559",
    "end": "1654740"
  },
  {
    "text": "available um and you have it already available for you to do it for Nissan all right so",
    "start": "1654740",
    "end": "1660260"
  },
  {
    "text": "when you want to do some skill across like hundreds of gigabytes or terabyte level size data sets",
    "start": "1660260",
    "end": "1666020"
  },
  {
    "text": "um this essentially looks like a very simple batch processing problem right you basically map your pre-trained model",
    "start": "1666020",
    "end": "1671120"
  },
  {
    "text": "over this big data set to get your predictions and this kind of workload is especially relevant today especially uh",
    "start": "1671120",
    "end": "1677779"
  },
  {
    "text": "with the rise of like Foundation models and being able to do zero shot inference so without even needing to train models",
    "start": "1677779",
    "end": "1684980"
  },
  {
    "text": "you can actually get very high quality prediction from these pre-trained models or even just like generating embeddings",
    "start": "1684980",
    "end": "1690799"
  },
  {
    "text": "which is common uh which is actually very useful for like out building LM applications again generating embeddings",
    "start": "1690799",
    "end": "1697760"
  },
  {
    "text": "is a batch inference yeah so what are some of the challenges with doing this a scale uh I think first one",
    "start": "1697760",
    "end": "1704120"
  },
  {
    "text": "is very common is how do you actually manage your compute infrastructure um right so you have gigabytes or",
    "start": "1704120",
    "end": "1711080"
  },
  {
    "text": "terabytes of data you need to scale out this computation the smash processing okay so what do you do you know you need",
    "start": "1711080",
    "end": "1716480"
  },
  {
    "text": "to create multiple machines these machines need to have like CPUs and gpus and you want to manage these",
    "start": "1716480",
    "end": "1722299"
  },
  {
    "text": "and in particular you may also want to manage heterogeneous compute compute infrastructure and what this means is",
    "start": "1722299",
    "end": "1727760"
  },
  {
    "text": "that you may want to Cluster where not every node of net cluster is the same type um so this could be useful for certain",
    "start": "1727760",
    "end": "1734059"
  },
  {
    "text": "types of workloads particularly ones that are very memory constrained um so you don't want to be using expensive GPU if the bottleneck in your",
    "start": "1734059",
    "end": "1741679"
  },
  {
    "text": "workload is memory like CPU memory you don't want to be using expensive GPU instances to address that problem you'd",
    "start": "1741679",
    "end": "1747080"
  },
  {
    "text": "rather use uh just the GPU instances that you want and the remainder of the instances can be pure CPU on a much",
    "start": "1747080",
    "end": "1753679"
  },
  {
    "text": "cheaper CPU instances to address that problem the Second Challenge is okay now that",
    "start": "1753679",
    "end": "1759559"
  },
  {
    "text": "you have this cluster you have all these CPS and gpus how do you actually write your software to utilize all the resources in the cluster and I mean this",
    "start": "1759559",
    "end": "1766279"
  },
  {
    "text": "is like primarily the challenge here is how do you keep your GPU saturated right that means this is the most expensive",
    "start": "1766279",
    "end": "1771500"
  },
  {
    "text": "part that you're paying for uh and if you're running your batch inference jobs at high frequency or a very large scale",
    "start": "1771500",
    "end": "1777620"
  },
  {
    "text": "you want to make sure they run quickly they run cost effectively and to do so you need to make sure they're they're",
    "start": "1777620",
    "end": "1782840"
  },
  {
    "text": "resource in your cluster is fully utilized and you're not basically paying for anything that that's not being used",
    "start": "1782840",
    "end": "1789679"
  },
  {
    "text": "third challenge is how do you facilitate this data trans this the transfer of data from your storage whether it's at",
    "start": "1789679",
    "end": "1796640"
  },
  {
    "text": "cloud storage or disk to CPU RAM for pre-processing and then to GPU RAM for",
    "start": "1796640",
    "end": "1802279"
  },
  {
    "text": "model inference and finally the fourth challenge is okay how can you actually develop very easily",
    "start": "1802279",
    "end": "1809240"
  },
  {
    "text": "right so how can you build these pipelines in a way that you can iterate quickly develop quickly and",
    "start": "1809240",
    "end": "1814520"
  },
  {
    "text": "um there's not a large gap between what you're doing during development and what you actually put into production at the end so these are the four main",
    "start": "1814520",
    "end": "1820580"
  },
  {
    "text": "challenges that what we've seen that people run into with batch inference um yeah and if anyone else has anything",
    "start": "1820580",
    "end": "1826820"
  },
  {
    "text": "to um also be happy to chat afterwards as well but from what we've seen these",
    "start": "1826820",
    "end": "1832039"
  },
  {
    "text": "are like four primary challenges so what does the industry recommend as",
    "start": "1832039",
    "end": "1837380"
  },
  {
    "text": "the solution to this right so like what are the common problems or those are the common problems that we saw what is",
    "start": "1837380",
    "end": "1843380"
  },
  {
    "text": "generally the common solutions that people recommend and I think like this tweet this Twitter thread in particular pretty much sums it up right so this is",
    "start": "1843380",
    "end": "1850100"
  },
  {
    "text": "Ben Hammer he's a CE he's a creator of kaggle and he's just posted hey I want to you know spin up a cluster of GPU",
    "start": "1850100",
    "end": "1855799"
  },
  {
    "text": "instances to do some bash inference job supposed to be very easy um what's like the best approach right",
    "start": "1855799",
    "end": "1862100"
  },
  {
    "text": "and you can see there's a quite a larger variety of responses so one guy very",
    "start": "1862100",
    "end": "1867500"
  },
  {
    "text": "confidently just says AWS batch um and you can also try Moto Labs or",
    "start": "1867500",
    "end": "1873140"
  },
  {
    "text": "lightning AI apparently handles this well um you want to do it yourself this guy",
    "start": "1873140",
    "end": "1879140"
  },
  {
    "text": "says or use kubef flow with AWS Spa instances or Azure batch compute or this",
    "start": "1879140",
    "end": "1884779"
  },
  {
    "text": "one guy even says hey let's use Ray serve um overall quite like it's a pretty all over the place right there's not real",
    "start": "1884779",
    "end": "1890899"
  },
  {
    "text": "any real solution that addresses these challenges well and sort of what people",
    "start": "1890899",
    "end": "1896779"
  },
  {
    "text": "have found out is like you may do this all yourself you may use like this mishmash of like different solutions",
    "start": "1896779",
    "end": "1902779"
  },
  {
    "text": "each people has their own have their own thing that they want to use and we can categorize these Solutions",
    "start": "1902779",
    "end": "1909440"
  },
  {
    "text": "into like three different uh categories the first one is just general batch services so I'm thinking about like AWS",
    "start": "1909440",
    "end": "1915500"
  },
  {
    "text": "badge gcp batch or Azure batch um and these handle challenge number one right they provide an easy way to uh",
    "start": "1915500",
    "end": "1922700"
  },
  {
    "text": "manage your infrastructure to spin up clusters create nodes um they don't necessarily change like",
    "start": "1922700",
    "end": "1928159"
  },
  {
    "text": "allow heterogeneous clusters so I think AWA for AWS batch all the nodes need to be in the same instance type and they",
    "start": "1928159",
    "end": "1934880"
  },
  {
    "text": "don't really handle challenges two three or four right like you can create these clusters you can submit jobs but you as",
    "start": "1934880",
    "end": "1941240"
  },
  {
    "text": "the user need to write your software in a way that can fully utilize all these resources can actually read data from",
    "start": "1941240",
    "end": "1947480"
  },
  {
    "text": "Storage you can actually transfer data from CPU Ram to GPU RAM and development",
    "start": "1947480",
    "end": "1952700"
  },
  {
    "text": "is also not very easy with these Solutions um there's other ones like Moto Labs which does make the development very",
    "start": "1952700",
    "end": "1958640"
  },
  {
    "text": "easy and provides some like basic Primitives for uh parallelization but again it's not really tailored for this",
    "start": "1958640",
    "end": "1964820"
  },
  {
    "text": "particular use case of batch inference the second category that we see people",
    "start": "1964820",
    "end": "1970580"
  },
  {
    "text": "use is actually like online inference Solutions right so solutions that are meant for being uh being deployed as",
    "start": "1970580",
    "end": "1976820"
  },
  {
    "text": "like online services uh and they just just use that for the batch case as well right and it's like overall pretty",
    "start": "1976820",
    "end": "1982580"
  },
  {
    "text": "simple uh uh you know this might be a straightforward way if uh you know because you already have your online",
    "start": "1982580",
    "end": "1988220"
  },
  {
    "text": "inference stuff okay let's just like use this for batch inference as well but um there's some some issues here and it's",
    "start": "1988220",
    "end": "1993860"
  },
  {
    "text": "primarily because it adds like a bunch of unnecessary complexities for the offline case right like you have to",
    "start": "1993860",
    "end": "1999440"
  },
  {
    "text": "start an HTTP server you're sending requests over the network a lot of additional overhead that you don't",
    "start": "1999440",
    "end": "2005140"
  },
  {
    "text": "really need for what should be a simple like map job essentially right and all this makes it very hard to saturate your",
    "start": "2005140",
    "end": "2010960"
  },
  {
    "text": "gpus and your cluster um because of all this overhead and in fact like Bento ml which is one of the",
    "start": "2010960",
    "end": "2016779"
  },
  {
    "text": "common like online sort of like popular online serving libraries in an open source that's like integrated with spark for this particular reason",
    "start": "2016779",
    "end": "2024159"
  },
  {
    "text": "and the third approach what we see is actually works is really tailored made",
    "start": "2024159",
    "end": "2029380"
  },
  {
    "text": "for this workload is just distributed Data Systems right so as we talked about you know if you have a pre-trained model",
    "start": "2029380",
    "end": "2034720"
  },
  {
    "text": "you have a bunch of data okay let's just map that model over the data that's really you know for the most part what",
    "start": "2034720",
    "end": "2040240"
  },
  {
    "text": "the problem is right so here we're talking about like spark or rate data sets um and these these Frameworks have",
    "start": "2040240",
    "end": "2047620"
  },
  {
    "text": "support for scaling across clusters you can you know they have support for like partitioning your data and batching them",
    "start": "2047620",
    "end": "2053440"
  },
  {
    "text": "uh as needed and they also have connectors to variety of different data sources can be they can read from cloud",
    "start": "2053440",
    "end": "2059919"
  },
  {
    "text": "storage and things like that right so this category of solutions really is what's most ideal for the batch",
    "start": "2059919",
    "end": "2066220"
  },
  {
    "text": "inference workload um as opposed to like the first two",
    "start": "2066220",
    "end": "2071700"
  },
  {
    "text": "so now let's take a deep dive into like 3D in particular um stage maker bash transform Apache",
    "start": "2071859",
    "end": "2077320"
  },
  {
    "text": "spark and Ray data sets and we're gonna actually so what we did is we actually benchmarked all three of these on an",
    "start": "2077320",
    "end": "2082960"
  },
  {
    "text": "image classification so very simple image classification uh Benchmark um it's going to read um yeah we can",
    "start": "2082960",
    "end": "2089260"
  },
  {
    "text": "just see the Benchmark here we have a pre-trained resnet 50 model uh and we just want to do inference on imagenet",
    "start": "2089260",
    "end": "2094540"
  },
  {
    "text": "data and essentially it's three steps right read images from S3 we have some very basic pre-processing that's",
    "start": "2094540",
    "end": "2100119"
  },
  {
    "text": "happening on CPU uh such as like resizing images cropping images normalizing images and then finally we",
    "start": "2100119",
    "end": "2106060"
  },
  {
    "text": "have model inference on GPU so you want to create a data Pipeline with these three steps using those three approaches",
    "start": "2106060",
    "end": "2111280"
  },
  {
    "text": "sagemaker spark and Ray data sets and we do this on both 10 gigabytes and 300",
    "start": "2111280",
    "end": "2116619"
  },
  {
    "text": "gigabytes so here are the results from 10 gigabytes um",
    "start": "2116619",
    "end": "2122680"
  },
  {
    "text": "yeah so we tried a few different spark configurations uh as well as like sagemaker batch transform and array and",
    "start": "2122680",
    "end": "2128619"
  },
  {
    "text": "we see that from this actually Ray Ray data sets uh outperform sagemaker quite",
    "start": "2128619",
    "end": "2134079"
  },
  {
    "text": "a bit and then also it's roughly like two times the throughput um compared against like the best spark",
    "start": "2134079",
    "end": "2139300"
  },
  {
    "text": "configuration so I think the first question probably you're wondering is like why well you know why is sagemaker batch transformed so low",
    "start": "2139300",
    "end": "2145839"
  },
  {
    "text": "um right it's only like 18 19 images per second um so the thing with stagemaker batch",
    "start": "2145839",
    "end": "2151900"
  },
  {
    "text": "transform um is that it fell into the category number two that we talked about before it's like an online inference solution",
    "start": "2151900",
    "end": "2158560"
  },
  {
    "text": "right so it's you basically using an architecture that's meant for online surf serving and like trying to shoehorn",
    "start": "2158560",
    "end": "2163660"
  },
  {
    "text": "that into this workload that doesn't really uh do well in so it actually starts an HTTP server it deploys your",
    "start": "2163660",
    "end": "2169480"
  },
  {
    "text": "model as an endpoint each image is sent as a request to the server and it has uh",
    "start": "2169480",
    "end": "2174940"
  },
  {
    "text": "some limitations like kind of it's not able to batch data across multiple files and there's like a maximum payload size",
    "start": "2174940",
    "end": "2181300"
  },
  {
    "text": "of 100 megabytes and overall like this really cannot saturate your gpus right you're not able to",
    "start": "2181300",
    "end": "2187660"
  },
  {
    "text": "batch enough data or movement of data through your through your pipeline to fully like utilize all your gpus and",
    "start": "2187660",
    "end": "2194680"
  },
  {
    "text": "therefore your throughput overall is very low and you know out to that that you have like pretty you know overall at",
    "start": "2194680",
    "end": "2200200"
  },
  {
    "text": "least when I was doing it from the developer ux to be quiet not not as easy and debugging was quite difficult um you",
    "start": "2200200",
    "end": "2206380"
  },
  {
    "text": "have to start a cluster every time you want to run the job so it overall made the iterations pretty slow",
    "start": "2206380",
    "end": "2213040"
  },
  {
    "text": "so that was for stage maker um now let's take a look at like so what's the difference between Ray and",
    "start": "2213040",
    "end": "2218680"
  },
  {
    "text": "Spark here um so yeah so let's compare ready data",
    "start": "2218680",
    "end": "2223900"
  },
  {
    "text": "assessment spark I think the biggest difference here in performance is that um we want to be able to utilize all the",
    "start": "2223900",
    "end": "2229180"
  },
  {
    "text": "resource in our cluster and what's unique about this particular workload is that it's a hybrid workload consisting of both CPU pre-processing and GPU",
    "start": "2229180",
    "end": "2236560"
  },
  {
    "text": "inference right so you have computation that requires these two different sets of hardware and you need to make sure at",
    "start": "2236560",
    "end": "2242740"
  },
  {
    "text": "any given point of time both the CPUs and your cluster and your GPS and cluster are going to be fully utilized and this is where one of the",
    "start": "2242740",
    "end": "2249579"
  },
  {
    "text": "advantages of rate datasets comes in is that it has basically a very native scheduling support for gpus and I think",
    "start": "2249579",
    "end": "2256839"
  },
  {
    "text": "this uh GIF kind of demonstrates this um so with spark you generally you have",
    "start": "2256839",
    "end": "2264160"
  },
  {
    "text": "more CPUs in your cluster than gpus and but what it does is because it doesn't really uh respect the GPU scheduling",
    "start": "2264160",
    "end": "2270880"
  },
  {
    "text": "it'll end up like fusing all of the different operations together so it'll run both the pre-processing and the",
    "start": "2270880",
    "end": "2276099"
  },
  {
    "text": "inference as part of the same task therefore you're not fully utilizing your CPUs however with Ray it can independently",
    "start": "2276099",
    "end": "2283119"
  },
  {
    "text": "scale these two steps because they're on different they have different resource requirements so even though we have less gpus than",
    "start": "2283119",
    "end": "2290320"
  },
  {
    "text": "CPUs in our cluster we can still scale the pre-processing independently from the inferencing step",
    "start": "2290320",
    "end": "2296380"
  },
  {
    "text": "and I think the second difference here is that array data sets has a streaming uh streaming execution support well for",
    "start": "2296380",
    "end": "2302619"
  },
  {
    "text": "spark it is a bulk execution model and I think in Eric's talk we'll be going over",
    "start": "2302619",
    "end": "2307720"
  },
  {
    "text": "more of the details here but as you can see for spark um we don't start executing partitions",
    "start": "2307720",
    "end": "2315099"
  },
  {
    "text": "three and four until one and two are fully finished and we get the predictions for those oh for Ray data",
    "start": "2315099",
    "end": "2321160"
  },
  {
    "text": "sets we can actually stream all this data through and take advantage of the ray Object Store but in order to offer",
    "start": "2321160",
    "end": "2327579"
  },
  {
    "text": "data in between these steps so while we're doing inferencing on gpus for partitions one and two partitions three",
    "start": "2327579",
    "end": "2335020"
  },
  {
    "text": "and four can then we can do pre-processing on partitions three and four so fully pipeline all the data",
    "start": "2335020",
    "end": "2340060"
  },
  {
    "text": "through and this is actually one of the I mean this is the primary reason for why we're seeing such a big performance difference with the great data sets and",
    "start": "2340060",
    "end": "2345940"
  },
  {
    "text": "Spark is the ability to support the streaming execution model and also the",
    "start": "2345940",
    "end": "2351400"
  },
  {
    "text": "ability to scale your CPU and GPU stages independently I think some other uh differences here",
    "start": "2351400",
    "end": "2358240"
  },
  {
    "text": "is that uh you know as we talked about for Challenge number three we want efficient data transfer and for this",
    "start": "2358240",
    "end": "2363640"
  },
  {
    "text": "particular workload again like one one unique property for these type of deep learning workloads is that you want to work with large you have this actually",
    "start": "2363640",
    "end": "2369880"
  },
  {
    "text": "have large multi-dimensional tensors right so each image is um uh quite a quite large size generally",
    "start": "2369880",
    "end": "2376300"
  },
  {
    "text": "compared to tabular data or if you're working with like audio data or video data gets even larger",
    "start": "2376300",
    "end": "2381940"
  },
  {
    "text": "um so with the right data sets you have fully it's fully numpy and Pyro based there's no pandas overhead so that's one",
    "start": "2381940",
    "end": "2390040"
  },
  {
    "text": "of the differences here and then also rated assets is fully like python native right so there's no jvm to pyro overhead",
    "start": "2390040",
    "end": "2395680"
  },
  {
    "text": "as well and then finally the with regards to developer experience",
    "start": "2395680",
    "end": "2400839"
  },
  {
    "text": "um overall it's like roughly the same between Ray and Spark but one of the differences here is that Ray is python first so a lot of your you know Common",
    "start": "2400839",
    "end": "2407859"
  },
  {
    "text": "python debugging tools you can leverage that with Ray and overall makes for a better like easier debugging better",
    "start": "2407859",
    "end": "2413920"
  },
  {
    "text": "stack traces um just overall smoother experience cool okay so that was like 10 gigabyte",
    "start": "2413920",
    "end": "2420099"
  },
  {
    "text": "between range spark uh let's see how this works with 300 gigabytes and I'm going to try to show this live",
    "start": "2420099",
    "end": "2426700"
  },
  {
    "text": "so I have here so I have your spark notebook and what",
    "start": "2426700",
    "end": "2432640"
  },
  {
    "text": "it's going to do so let me just run this whole thing",
    "start": "2432640",
    "end": "2436560"
  },
  {
    "text": "yep yep so let me just get started with running it and we can go over how this exactly looks like so again it's the",
    "start": "2438940",
    "end": "2444280"
  },
  {
    "text": "three steps that we talked about you're going to read images from S3 pre-process those images on CPU and then do model",
    "start": "2444280",
    "end": "2449980"
  },
  {
    "text": "inference and GPU so we have about 300 gigabytes of images stored in S3 and parquet file so we're going to read",
    "start": "2449980",
    "end": "2456160"
  },
  {
    "text": "those um apply some basic pre-processing that we talked about so these are all like Pi",
    "start": "2456160",
    "end": "2461980"
  },
  {
    "text": "torch pre-processing functions very standard um and then do our inferencing so what",
    "start": "2461980",
    "end": "2468099"
  },
  {
    "text": "we had here is we just have a pre-trained resonant model we're going to broadcast this to all these smart executors",
    "start": "2468099",
    "end": "2474760"
  },
  {
    "text": "and then we're going to have a predict function here I'm just going to take like the next batch uh converted into Pi",
    "start": "2474760",
    "end": "2481960"
  },
  {
    "text": "torch tensor do some small manipulation here pre-process it and then pass",
    "start": "2481960",
    "end": "2487780"
  },
  {
    "text": "through to the model to get predictions so this guy's gonna run and let me start",
    "start": "2487780",
    "end": "2494500"
  },
  {
    "text": "kicking off the array side as well so I'm going to run this here this is like so this is any scale uh platform that we",
    "start": "2494500",
    "end": "2500680"
  },
  {
    "text": "have but this both both of these like benchmarks are can be fully done on open source and oh the benchmarks are fully",
    "start": "2500680",
    "end": "2507280"
  },
  {
    "text": "released as well on GitHub um so let me just run this kick this off",
    "start": "2507280",
    "end": "2513520"
  },
  {
    "text": "and as you can see the code again just looks exactly the same except here we're using Ray data sets instead of spark so",
    "start": "2513520",
    "end": "2520000"
  },
  {
    "text": "we have our pre-trained resonant model that we get from pytorch we have a pre-process function here",
    "start": "2520000",
    "end": "2525820"
  },
  {
    "text": "and then we have a uh our inferencing logic is here and we do this in a class rather than a function so that we can",
    "start": "2525820",
    "end": "2533200"
  },
  {
    "text": "only we only have to initialize the model just once and we can reuse that same model across different batches rather than having to initialize a model",
    "start": "2533200",
    "end": "2539980"
  },
  {
    "text": "every time for each batch then we just map that over our data set um and we'll get the predictions so both",
    "start": "2539980",
    "end": "2547420"
  },
  {
    "text": "of these are running right now and let's see how long these will take I think in the",
    "start": "2547420",
    "end": "2553240"
  },
  {
    "text": "meantime if uh maybe it's a good point to stop here if anyone has any questions or",
    "start": "2553240",
    "end": "2558480"
  },
  {
    "text": "is",
    "start": "2561640",
    "end": "2563640"
  },
  {
    "text": "so the question is what's the time split between what's the inferencing and yeah",
    "start": "2569619",
    "end": "2574900"
  },
  {
    "text": "foreign",
    "start": "2574900",
    "end": "2577900"
  },
  {
    "text": "[Music]",
    "start": "2581780",
    "end": "2584929"
  },
  {
    "text": "but I think most of the time is coming from the inferencing call for both cases yeah",
    "start": "2590700",
    "end": "2598079"
  },
  {
    "text": "yeah any other questions",
    "start": "2599260",
    "end": "2605039"
  },
  {
    "text": "yeah yeah so uh in the previous",
    "start": "2608079",
    "end": "2615280"
  },
  {
    "text": "animation which you showed there was like a ray buffer which was trying to accelerate the I mean the difference",
    "start": "2615280",
    "end": "2621700"
  },
  {
    "text": "comes because you're using the buffer in my opinion apart from the fact that you're also finally starting work in CPU even though",
    "start": "2621700",
    "end": "2629020"
  },
  {
    "text": "this inference going on a GPU but uh what is this additional resource with Ray buffer and do you have like back",
    "start": "2629020",
    "end": "2634720"
  },
  {
    "text": "pressure when there is lot of items in the buffer itself yeah that's a great question so I think that will be covered",
    "start": "2634720",
    "end": "2641020"
  },
  {
    "text": "more in the next talk about the details about how the execution model works but there is a back pressure if we try to buffer if you buffer too much data",
    "start": "2641020",
    "end": "2648280"
  },
  {
    "text": "um so we'll limit the parallelism of like Upstream operators if that's the case",
    "start": "2648280",
    "end": "2653338"
  },
  {
    "text": "[Music] no no so yeah portion of the memory of",
    "start": "2655010",
    "end": "2660160"
  },
  {
    "text": "each node is used for the shared Object Store so it's not an additional resource that's required",
    "start": "2660160",
    "end": "2665760"
  },
  {
    "text": "yeah cool so I think the uh yep so these are almost done Ray one is still going",
    "start": "2665859",
    "end": "2671380"
  },
  {
    "text": "and the spark one still going here um",
    "start": "2671380",
    "end": "2676380"
  },
  {
    "text": "and we should see they should finish in a few seconds",
    "start": "2676960",
    "end": "2683519"
  },
  {
    "text": "yep so we get an overall throughput of like uh 2800 images per second with the",
    "start": "2686859",
    "end": "2692560"
  },
  {
    "text": "right data sets and with the spark one this looks like it's about halfway through so",
    "start": "2692560",
    "end": "2698800"
  },
  {
    "text": "um yeah let's revisit this one once it's ready and we can see how long it takes",
    "start": "2698800",
    "end": "2703859"
  },
  {
    "text": "cool so that was 300 gigabytes and the next question you're probably asking us so how does this scale Beyond this right",
    "start": "2706180",
    "end": "2712060"
  },
  {
    "text": "for very large data sizes um and we've also tried this up to up to 10 terabytes of data",
    "start": "2712060",
    "end": "2718119"
  },
  {
    "text": "um so here's a 40 GPU cluster running the same exact workload and we're seeing a throughput of uh close to 11 and a",
    "start": "2718119",
    "end": "2725140"
  },
  {
    "text": "half thousand images per second and overall the GPU utilization is over 90 throughout the entire uh run so we're",
    "start": "2725140",
    "end": "2732819"
  },
  {
    "text": "getting very good GPU saturation with the right data sets and good throughput even though we're scaling up to 10",
    "start": "2732819",
    "end": "2739420"
  },
  {
    "text": "terabytes of data and 40 GPU cluster so just to summarize uh you know we see",
    "start": "2739420",
    "end": "2746500"
  },
  {
    "text": "the rate data outperformed sagemaker and Spark for offline batch inference and um overall that uh you know using these",
    "start": "2746500",
    "end": "2754000"
  },
  {
    "text": "types of like data processing Frameworks are better for dispatch inference workload than uh online inference",
    "start": "2754000",
    "end": "2760060"
  },
  {
    "text": "Solutions or kind of rolls with something about yourself and ready data sets meets all Forge calendars right so",
    "start": "2760060",
    "end": "2766540"
  },
  {
    "text": "um with array you can easily launch clusters across any cloud provider it can they'll scale across this entire",
    "start": "2766540",
    "end": "2772480"
  },
  {
    "text": "cluster it abstracts the way the computer infrastructure management you can stream data easily from cloud",
    "start": "2772480",
    "end": "2778839"
  },
  {
    "text": "storage to CPU to GPU utilizing all the cluster resources it has native support for",
    "start": "2778839",
    "end": "2785500"
  },
  {
    "text": "multi-dimensional tensors you get like zero copy exchange rather than having to do go through pandas overhead or jvm",
    "start": "2785500",
    "end": "2791560"
  },
  {
    "text": "overhead as well and then finally it's fully python native so it makes it very easy to develop and debug",
    "start": "2791560",
    "end": "2799318"
  },
  {
    "text": "so let me just see if it's still going yeah still going okay foreign",
    "start": "2799359",
    "end": "2805540"
  },
  {
    "text": "so what's next if you are doing batch inference uh you have some batch inference workload that's running on",
    "start": "2805540",
    "end": "2810760"
  },
  {
    "text": "large data sets um I'd love to talk to you so please come find me after after all the talks",
    "start": "2810760",
    "end": "2816040"
  },
  {
    "text": "you can also check out the rate documentation here expansion prints.io and how to get started or um if you if",
    "start": "2816040",
    "end": "2821859"
  },
  {
    "text": "you want to get in touch with us you can scan this QR code fill out a form and we can reach out we will reach out to you",
    "start": "2821859",
    "end": "2827200"
  },
  {
    "text": "to to help you guys get started with ready um yeah so we believe we have the most performant uh solution out there in open",
    "start": "2827200",
    "end": "2834220"
  },
  {
    "text": "source for offline batch inference and we'd love to work with you to to make that a reality for your guys's workload",
    "start": "2834220",
    "end": "2841119"
  },
  {
    "text": "and we do have a published blog that actually has all the code in there so if you actually want to reproduce this how",
    "start": "2841119",
    "end": "2846940"
  },
  {
    "text": "you can actually do that so we were quite transparent in terms of what's available and this is not hand waving",
    "start": "2846940",
    "end": "2853000"
  },
  {
    "text": "this is really experiment that we have conducted any questions",
    "start": "2853000",
    "end": "2858180"
  },
  {
    "text": "sorry um talk to me I think do we actually",
    "start": "2864040",
    "end": "2870220"
  },
  {
    "text": "have a Blog link over there no I don't I'll edit the slides okay yeah we'll we'll all I'm going to do is I'm going",
    "start": "2870220",
    "end": "2875980"
  },
  {
    "text": "to share those lines on the race I'm on the on the community the great meetups if you have joined us you will actually",
    "start": "2875980",
    "end": "2882700"
  },
  {
    "text": "get it so if you're part of the member you should you'll get all the recording as well as the the slides as well",
    "start": "2882700",
    "end": "2888220"
  },
  {
    "text": "anybody else we're gonna give a big hand to a Moog we'll have Eric come in next",
    "start": "2888220",
    "end": "2895960"
  },
  {
    "text": "and like I said we're going to hang around here if you have any questions let us know um",
    "start": "2895960",
    "end": "2901800"
  },
  {
    "text": "to show your batch in France goes smoothly",
    "start": "2902980",
    "end": "2907140"
  },
  {
    "text": "okay",
    "start": "2913780",
    "end": "2916780"
  },
  {
    "text": "should I join the zoom yeah",
    "start": "2920200",
    "end": "2924180"
  },
  {
    "text": "then fight somewhere find it if you send him anybody foreign",
    "start": "2931859",
    "end": "2939000"
  },
  {
    "text": "foreign",
    "start": "2964119",
    "end": "2966299"
  },
  {
    "text": "[Music]",
    "start": "2992350",
    "end": "2995409"
  },
  {
    "text": "hi everyone uh so um in my talk I'm going to kind of",
    "start": "3014000",
    "end": "3019319"
  },
  {
    "text": "go over Ray data and this new feature of radar called uh Ray data streaming that",
    "start": "3019319",
    "end": "3024960"
  },
  {
    "text": "kind of powered amongst talk and I'm going to cover a little bit of the same material but I'm hoping from a different",
    "start": "3024960",
    "end": "3030420"
  },
  {
    "text": "angle especially for those of you wondering okay this data streaming is great how does it actually work is it you know a fundamentally different you",
    "start": "3030420",
    "end": "3037079"
  },
  {
    "text": "know and so on so uh this talk is going to cover ml",
    "start": "3037079",
    "end": "3042420"
  },
  {
    "text": "both ml inference workloads so that I'm talked about and and distributed training which is another popular use case for Ray data and um I'm gonna tell",
    "start": "3042420",
    "end": "3049980"
  },
  {
    "text": "you about this new Ray data streaming thing because I just recently released actually rate 2.4",
    "start": "3049980",
    "end": "3055140"
  },
  {
    "text": "um and how it kind of improves both of these workloads um I'm going to walk through a bunch of examples and then we'll uh hopefully",
    "start": "3055140",
    "end": "3061380"
  },
  {
    "text": "have time to go over the the back end so about a bit about me I'm the teal for",
    "start": "3061380",
    "end": "3066720"
  },
  {
    "text": "rate and open source any skill my primary focus these days is on Ray data and other machine learning workloads",
    "start": "3066720",
    "end": "3074040"
  },
  {
    "text": "um previously I worked in a lot of system stuff in Berkeley and other companies",
    "start": "3074040",
    "end": "3079460"
  },
  {
    "text": "um yeah so let's dive into ML workloads and data so at a very uh high level",
    "start": "3080700",
    "end": "3086040"
  },
  {
    "text": "I I think like a kind of data processing comes into ml workloads kind of in the kind of a pipeline of three steps first",
    "start": "3086040",
    "end": "3092520"
  },
  {
    "text": "you have the kind of problem just organizing your data in the first place um and in in kind of the industry",
    "start": "3092520",
    "end": "3098040"
  },
  {
    "text": "companies is known as the ETL step where you just take raw data and you're organizing it into a bunch of tables",
    "start": "3098040",
    "end": "3103559"
  },
  {
    "text": "maybe your data warehouse and so on um you need to do this before you're even ready to get ready for Admiral",
    "start": "3103559",
    "end": "3108660"
  },
  {
    "text": "training then you have the next step of like how do I get my data into my model in the right format where you do pre-processing",
    "start": "3108660",
    "end": "3115380"
  },
  {
    "text": "and then you have the actual um you know training or inference where we're doing modeling work",
    "start": "3115380",
    "end": "3121680"
  },
  {
    "text": "and just to make this concrete the first step um involves stuff like ingesting data training tables",
    "start": "3121680",
    "end": "3128460"
  },
  {
    "text": "um pre-processing will it can involve things like resizing images decoding videos data augmentation scaling data",
    "start": "3128460",
    "end": "3135119"
  },
  {
    "text": "and the last step is where you're using your machine learning Frameworks like pytorch hugging face and so on",
    "start": "3135119",
    "end": "3142980"
  },
  {
    "text": "and just to kind of generalize here typically you see like ETL and pre-processing take place on CPU so",
    "start": "3142980",
    "end": "3148859"
  },
  {
    "text": "running on CPU nodes or maybe a CPU heavy cluster and then uh in a training inference where you really see people",
    "start": "3148859",
    "end": "3154619"
  },
  {
    "text": "leverage gpus very often for models and uh we're going to focus really on the last two steps here",
    "start": "3154619",
    "end": "3161160"
  },
  {
    "text": "um and this is kind of the usual scope of metal team is also kind of where raid data is kind of intended to shine",
    "start": "3161160",
    "end": "3167819"
  },
  {
    "text": "so our vision for how Rey fits into the ecosystem space is kind of segmenting",
    "start": "3167819",
    "end": "3172920"
  },
  {
    "text": "this by to utl and this last mile like pre-processing and training it for instance so for ETL we you know we kind of don't",
    "start": "3172920",
    "end": "3180000"
  },
  {
    "text": "want to reinvent the wheel there's like a really mature industry out there with spark your data warehouse SQL systems",
    "start": "3180000",
    "end": "3185160"
  },
  {
    "text": "and so on so um we think that that that's fine that's great",
    "start": "3185160",
    "end": "3190859"
  },
  {
    "text": "um where where we think uh the ecosystem could be simplified with Rey is is in",
    "start": "3190859",
    "end": "3197280"
  },
  {
    "text": "this pre-processing and training step where the ecosystem is much more fragmented and the needs are much more diverse so our the kind of goal of raid",
    "start": "3197280",
    "end": "3204720"
  },
  {
    "text": "data is to allow you to write any kind of pre-processing or um and training inference job kind of in a single array",
    "start": "3204720",
    "end": "3211440"
  },
  {
    "text": "program kind of no matter if you're training on one laptop or a big cluster it could kind of should all just work in",
    "start": "3211440",
    "end": "3217140"
  },
  {
    "text": "the same way um so a bit about raid data uh uh I",
    "start": "3217140",
    "end": "3224579"
  },
  {
    "text": "think in the previous talk we mentioned Ray data a lot it's a it's one of the libraries uh of Rey uh in Ray we have",
    "start": "3224579",
    "end": "3232260"
  },
  {
    "text": "this Ray air we sound like a suite of libraries like a toolkit of libraries for ML workloads alongside a training a",
    "start": "3232260",
    "end": "3238980"
  },
  {
    "text": "train uh distributed uh typewriter tuning serving and of course reinforcement learning and these",
    "start": "3238980",
    "end": "3245220"
  },
  {
    "text": "libraries are all in Python they're they're built on top of Ray core which is a lower level uh kind of general",
    "start": "3245220",
    "end": "3251760"
  },
  {
    "text": "purpose parallelization framework so that allows these libraries to easily scale",
    "start": "3251760",
    "end": "3257480"
  },
  {
    "text": "um and there's a little bit of History here where um uh radius is actually a relatively",
    "start": "3258540",
    "end": "3264599"
  },
  {
    "text": "recent Edition a recent recent Library added to Ray like when Rey was initially created there were actually no libraries",
    "start": "3264599",
    "end": "3270000"
  },
  {
    "text": "it was just a very low level parallelization framework but it's kind of we work with more ml Engineers on ML",
    "start": "3270000",
    "end": "3276839"
  },
  {
    "text": "teams we kind of realized like these higher level libraries uh kind of would simplify a lot of user needs so we",
    "start": "3276839",
    "end": "3282240"
  },
  {
    "text": "probably start adding more starting kind of more on the research that I would reinforce some learning we're kind of graduating to like more production use",
    "start": "3282240",
    "end": "3288900"
  },
  {
    "text": "cases like serving tuning um and an hour rate data Ray data is about two years old at this point",
    "start": "3288900",
    "end": "3296240"
  },
  {
    "text": "and at a very high level you can think of raid data as stocking the problem like distributed data sets for machine",
    "start": "3296240",
    "end": "3302280"
  },
  {
    "text": "learning um so a data data data set is kind of a reference kind of object that can that",
    "start": "3302280",
    "end": "3308640"
  },
  {
    "text": "you can use to work with data that um Can spam nodes in a cluster",
    "start": "3308640",
    "end": "3315200"
  },
  {
    "text": "and it integrates well with the kind of industry standards like uh you can read your data from store from S3 written out",
    "start": "3315720",
    "end": "3321900"
  },
  {
    "text": "from spark and parquet format other common formats uh internally Ray data",
    "start": "3321900",
    "end": "3327900"
  },
  {
    "text": "uses Arrow to to represent data so that makes it really easy to feed it to ml Frameworks such as pytorch",
    "start": "3327900",
    "end": "3335040"
  },
  {
    "text": "um uh you know exe boost and so on and uh there's a lot of apis I'm not",
    "start": "3335040",
    "end": "3341700"
  },
  {
    "text": "going to go over the apis but a couple of the kind of high-level things that you should be aware of for this talk is",
    "start": "3341700",
    "end": "3347099"
  },
  {
    "text": "you can read uh kind of read data from storage using Ray data read for example you can read from a cloud bucket or you",
    "start": "3347099",
    "end": "3353760"
  },
  {
    "text": "can read files from your laptop um you can transform data in batch you can",
    "start": "3353760",
    "end": "3359760"
  },
  {
    "text": "pass a function like a python UDF function to transform data and you can consume data in various ways",
    "start": "3359760",
    "end": "3366359"
  },
  {
    "text": "um I think the consuming part is actually where rate is kind of interesting and not only can you write data out to cloud",
    "start": "3366359",
    "end": "3373740"
  },
  {
    "text": "storage but actually with Rada you can get an iterator A distributed iterator of the data and that is a actually a",
    "start": "3373740",
    "end": "3379200"
  },
  {
    "text": "high performance iterator you can use for beating data into distributor training",
    "start": "3379200",
    "end": "3384859"
  },
  {
    "text": "okay uh streaming so streaming is a new feature um what what did radio do before",
    "start": "3386880",
    "end": "3392760"
  },
  {
    "text": "streaming so previously for applying pre-processing radiator just used bulk execution",
    "start": "3392760",
    "end": "3399000"
  },
  {
    "text": "um and what is bulk execution uh if you're not familiar with kind of the data Frameworks it's kind of like the naive straightforward way of data",
    "start": "3399000",
    "end": "3405900"
  },
  {
    "text": "processing so basically if you have your data Pipeline and Ray data we would load",
    "start": "3405900",
    "end": "3411300"
  },
  {
    "text": "the data to memory apply all our Transformations one step at a time um and if you happen to run out of",
    "start": "3411300",
    "end": "3417180"
  },
  {
    "text": "memory oops uh Ray will spill the data to disk it'll slow down the execution but it'll still continue",
    "start": "3417180",
    "end": "3424140"
  },
  {
    "text": "um yeah and and this work kind of pretty fine for reasonable size data",
    "start": "3424140",
    "end": "3429180"
  },
  {
    "text": "bits for example machines these days have a lot of memory so if it's 100 gigabyte data set for ammo training it'll work just fine I'll just load in",
    "start": "3429180",
    "end": "3435480"
  },
  {
    "text": "memory but once you scale beyond that it just kind of doesn't work um",
    "start": "3435480",
    "end": "3441619"
  },
  {
    "text": "um yeah and this is a very similar Sparks execution model uh in the kind of academic literature is old synchronous",
    "start": "3441660",
    "end": "3448079"
  },
  {
    "text": "Barrel execution model so what we kind of realized is that for",
    "start": "3448079",
    "end": "3453300"
  },
  {
    "text": "larger scale use cases really we needed to implement first class streaming execution and uh this actually became uh",
    "start": "3453300",
    "end": "3460319"
  },
  {
    "text": "was implemented it became the default execution strategy rate 2.4 it's uh it didn't it's not an API change it's",
    "start": "3460319",
    "end": "3465839"
  },
  {
    "text": "purely a back-end change and the idea is uh kind of instead of executing operators one at a time in bulk you",
    "start": "3465839",
    "end": "3472260"
  },
  {
    "text": "build a pipeline of these operators so there's just read pre-processing and so on and you kind of stream data through",
    "start": "3472260",
    "end": "3477599"
  },
  {
    "text": "these operators and this this the goal is like reduce your memory usage um kind of stream the memory be very",
    "start": "3477599",
    "end": "3483960"
  },
  {
    "text": "efficient now you avoid slowing down for larger data sets foreign so what kind of a motivating",
    "start": "3483960",
    "end": "3490800"
  },
  {
    "text": "example is uh where pre-processing is like a big bottleneck is a kind of video",
    "start": "3490800",
    "end": "3496640"
  },
  {
    "text": "inference so imagine you're trying to do you have a model that works on video frames and you're reading video data",
    "start": "3496640",
    "end": "3503400"
  },
  {
    "text": "decoding that video applying your model and Bachelor for instance saving the results right so when users try to do this they often",
    "start": "3503400",
    "end": "3511680"
  },
  {
    "text": "run to several problems um first uh the decoding can be pretty expensive so you you might want to use a",
    "start": "3511680",
    "end": "3519480"
  },
  {
    "text": "heterogeneous cluster you have CPU nodes additional GPU nodes so that's where array data can come in",
    "start": "3519480",
    "end": "3526559"
  },
  {
    "text": "and another challenge is you have this type of blow up like you decode the video frame you say you read like 10",
    "start": "3526559",
    "end": "3531960"
  },
  {
    "text": "gigabytes of video you decode that video it kind of blows up to like you know like 100 like 100 gigabytes or more of",
    "start": "3531960",
    "end": "3537900"
  },
  {
    "text": "it of frames and you really don't want that data to end up like spill to disk because you just write you write up",
    "start": "3537900",
    "end": "3543240"
  },
  {
    "text": "writing to this reading from disk and it's super slow right so you wanted the data to be streamed if you remember",
    "start": "3543240",
    "end": "3548520"
  },
  {
    "text": "um and that that's kind of the problem that we we data streaming solves the video video inference is kind of an",
    "start": "3548520",
    "end": "3553799"
  },
  {
    "text": "extreme example of this and uh I used the first example but at",
    "start": "3553799",
    "end": "3561000"
  },
  {
    "text": "the train it also applies to training and that's see how we could switch from inference to training all you're doing",
    "start": "3561000",
    "end": "3566880"
  },
  {
    "text": "for training really is instead of sending it to the model the stream to model and then saving it to disk you're",
    "start": "3566880",
    "end": "3572880"
  },
  {
    "text": "splitting the stream up into however many training workers you have sending it to those workers for for doing their",
    "start": "3572880",
    "end": "3578220"
  },
  {
    "text": "model of forward pass so that's that's really the only difference the the decode part is entirely the same so it",
    "start": "3578220",
    "end": "3583500"
  },
  {
    "text": "really did a streaming also can power distributed training for like Pi torch tensorflow and so on",
    "start": "3583500",
    "end": "3591440"
  },
  {
    "text": "okay um so I want to give a high level overview of",
    "start": "3592200",
    "end": "3598500"
  },
  {
    "text": "uh wide streaming when would you use streaming the benefits and then we'll dive into examples to make it more",
    "start": "3598500",
    "end": "3603540"
  },
  {
    "text": "concrete but so let's start with a very simple uh CPU only pipeline like say",
    "start": "3603540",
    "end": "3608700"
  },
  {
    "text": "you're just you're not even using gpus you're just using some simple CPU model um with a bulk execution strategy it's",
    "start": "3608700",
    "end": "3615900"
  },
  {
    "text": "actually pretty good if you're not working with like gpus you won't do any spilling to the disk",
    "start": "3615900",
    "end": "3621420"
  },
  {
    "text": "it's memory optimal and it's fine it's good for inference there's actually no reason to prefer",
    "start": "3621420",
    "end": "3626520"
  },
  {
    "text": "um streaming in this case for for these simple pipelines but it's actually kind of bad for training because you cannot",
    "start": "3626520",
    "end": "3632220"
  },
  {
    "text": "kind of incrementally access your results you kind of have to actually the whole thing if it doesn't depend in memory it's kind of bad",
    "start": "3632220",
    "end": "3638280"
  },
  {
    "text": "um so streaming is um does even for the very simplest pipelines uh help kind of",
    "start": "3638280",
    "end": "3645059"
  },
  {
    "text": "address some needs there for distribution um for heterogeneous pipelines uh such",
    "start": "3645059",
    "end": "3652200"
  },
  {
    "text": "as the one in most showed where you have GPU and CPU stages it's a streaming starts showing even more advantages so",
    "start": "3652200",
    "end": "3659099"
  },
  {
    "text": "bulk execution as we stopped before is kind of inefficient uh High overhead whereas streaming is is still basically",
    "start": "3659099",
    "end": "3666839"
  },
  {
    "text": "just just as good right and uh we we actually see uh see both of these These",
    "start": "3666839",
    "end": "3672720"
  },
  {
    "text": "are pretty common in Dental space um obviously if you have a second one you might see more value From Radiator",
    "start": "3672720",
    "end": "3679079"
  },
  {
    "text": "streaming than the first but it's kind of beautiful",
    "start": "3679079",
    "end": "3683058"
  },
  {
    "text": "so next I just want to walk through concrete examples so that's that's let's revisit our simple batch inference job",
    "start": "3684359",
    "end": "3690180"
  },
  {
    "text": "let's say the model is just on CPUs no gpus the data flow is I read some data",
    "start": "3690180",
    "end": "3695220"
  },
  {
    "text": "pre-process it do inference the model ends in code and Ray it'll look like this so I",
    "start": "3695220",
    "end": "3702119"
  },
  {
    "text": "import Ray let's say we Define our model entry processor Define our UDF so the UDF is a batch of",
    "start": "3702119",
    "end": "3708900"
  },
  {
    "text": "and Ray data UDS are batch of the numpy Rays and they return batches in the Empire race",
    "start": "3708900",
    "end": "3715140"
  },
  {
    "text": "and uh from my pipeline just radiator re-parking map batches the",
    "start": "3715140",
    "end": "3720180"
  },
  {
    "text": "pre-processing function the model function and the right so that implements that that for that for uh for",
    "start": "3720180",
    "end": "3726180"
  },
  {
    "text": "operation outline you saw before um and you might think okay why are",
    "start": "3726180",
    "end": "3732000"
  },
  {
    "text": "there four why is it a single stage pipeline why am I calling single stage five languages four things is because they run on CPU so they're pretty",
    "start": "3732000",
    "end": "3738119"
  },
  {
    "text": "standard optimization is uh as you said fuse them into one task um so it's just one task running",
    "start": "3738119",
    "end": "3744960"
  },
  {
    "text": "um so if you bulk execute this uh I guess you have like three partial data you run that stage",
    "start": "3744960",
    "end": "3750780"
  },
  {
    "text": "you get all your outputs and then uh pretty straightforward um so yeah you see um",
    "start": "3750780",
    "end": "3758280"
  },
  {
    "text": "the memory is just fine there's no disc spilling going on um uh the only thing is it's not not great",
    "start": "3758280",
    "end": "3764400"
  },
  {
    "text": "for distributed training the data is too big for for streaming",
    "start": "3764400",
    "end": "3770280"
  },
  {
    "text": "um the execution is we create the operator and then we and then we can uh",
    "start": "3770280",
    "end": "3775799"
  },
  {
    "text": "kind of stream data through it so here you kind of see it's like one block at a time in principle the operator could be",
    "start": "3775799",
    "end": "3781799"
  },
  {
    "text": "parallel could have internal parallelism could take a lot of time um and the output is one at a time and the",
    "start": "3781799",
    "end": "3789000"
  },
  {
    "text": "operator can Implement things like back pressure um and like an incremental returning results which makes it uh suitable for",
    "start": "3789000",
    "end": "3794520"
  },
  {
    "text": "for Distributing now let's look at the the more complex",
    "start": "3794520",
    "end": "3800339"
  },
  {
    "text": "case the the heterogeneous pipeline so the only difference is pipeline is the imprints operator is placed on a GPU",
    "start": "3800339",
    "end": "3808020"
  },
  {
    "text": "and how we express that in Rey as as you saw before is we have an array we have a",
    "start": "3808020",
    "end": "3813180"
  },
  {
    "text": "class so the Constructor is where you create the model as a safer model and then we have a call function that you",
    "start": "3813180",
    "end": "3819480"
  },
  {
    "text": "can use to repeatedly call that model the model is on a GPU and it's cached and then the only difference in the",
    "start": "3819480",
    "end": "3826079"
  },
  {
    "text": "pipeline is that we map batches with that class instead of the function um with Ray you can say num gpus want to",
    "start": "3826079",
    "end": "3832500"
  },
  {
    "text": "set the resources to that and we can set the number of replicas of that factor so we set up say size equals 10 for 10. for",
    "start": "3832500",
    "end": "3839280"
  },
  {
    "text": "10 gpus or 10 replicas so this creates a",
    "start": "3839280",
    "end": "3845460"
  },
  {
    "text": "it's like a three-stage pipeline here so we have the initial i o and pre-processing on CPUs and the GPU operator and then the the",
    "start": "3845460",
    "end": "3852420"
  },
  {
    "text": "output operator and and the reason we want separate stages here is so you can independently scale",
    "start": "3852420",
    "end": "3858540"
  },
  {
    "text": "these things right so you might want to place these on CPU machines are different from the GPU machines like so",
    "start": "3858540",
    "end": "3863940"
  },
  {
    "text": "you just have one GPU machine and have 10 CPU machines",
    "start": "3863940",
    "end": "3868400"
  },
  {
    "text": "so um and Visually the the bulk execution here looks like this you actually this",
    "start": "3869460",
    "end": "3875760"
  },
  {
    "text": "the data now we have intermediate data so it could be spilled to disk the cluster the second stage reads that data",
    "start": "3875760",
    "end": "3882059"
  },
  {
    "text": "back writes it to Cluster storage and their storage state which read that data back before we're ready ready to wherever",
    "start": "3882059",
    "end": "3888240"
  },
  {
    "text": "wherever it's going um so so as you see there's this writing to storage disability so we can slow down",
    "start": "3888240",
    "end": "3895140"
  },
  {
    "text": "for inference that that this is like the main reason why spark was slower workload before and of course it's not",
    "start": "3895140",
    "end": "3901380"
  },
  {
    "text": "great and uh so of course with streaming you don't have this problem every operator",
    "start": "3901380",
    "end": "3907920"
  },
  {
    "text": "will Implement its back pressure to reduce memory usage every operator here can run say one at a time so the first",
    "start": "3907920",
    "end": "3914040"
  },
  {
    "text": "when's the first operator is done with the block it sends it to the second operator and the first opera can start working on the next partition",
    "start": "3914040",
    "end": "3919559"
  },
  {
    "text": "and so on so um every operator runs one for more things at a time and then eventually",
    "start": "3919559",
    "end": "3926339"
  },
  {
    "text": "finish and without without exceeding the memory limits of the cluster or anything like that yeah",
    "start": "3926339",
    "end": "3933119"
  },
  {
    "text": "so uh no real downsides for this for these ml workloads",
    "start": "3933119",
    "end": "3939078"
  },
  {
    "text": "um and yeah just to summarize the comparison to other systems out there um for open source systems in particular",
    "start": "3940980",
    "end": "3947760"
  },
  {
    "text": "uh Ray data stream is more memory efficient than kind of more ETL oriented data Frameworks please have our",
    "start": "3947760",
    "end": "3953640"
  },
  {
    "text": "workloads uh it supports heterogeneous clusters because Ray supports heterogeneous clusters and um and this",
    "start": "3953640",
    "end": "3959760"
  },
  {
    "text": "is really because of the execution model and the which is better for it better",
    "start": "3959760",
    "end": "3965160"
  },
  {
    "text": "for inference than the better fit for training as well um another interesting thing is ml",
    "start": "3965160",
    "end": "3970440"
  },
  {
    "text": "address libraries uh TF data torch data pedestorm kind of solve a similar problem for distributed training",
    "start": "3970440",
    "end": "3977220"
  },
  {
    "text": "um the the main limitations we've seen is that they're they're not good for scaling out and they're kind of a",
    "start": "3977220",
    "end": "3982260"
  },
  {
    "text": "separate Paradigm right it's like it's not like the same data pipeline for reference training you have to kind of write separate work with separate",
    "start": "3982260",
    "end": "3987780"
  },
  {
    "text": "systems that that's one reason we see people like why don't I choose Ray data compared to these other systems",
    "start": "3987780",
    "end": "3994559"
  },
  {
    "text": "um yes it's actually kind of interesting I think in like meta and Google internally have stuff that's more similar to Ray",
    "start": "3994559",
    "end": "4000079"
  },
  {
    "text": "data but not open source",
    "start": "4000079",
    "end": "4003460"
  },
  {
    "text": "um I think I have time to do this example so um",
    "start": "4005359",
    "end": "4010520"
  },
  {
    "text": "here's another example of a four stage Pipeline and I'm going to use the example to point out some features that",
    "start": "4010520",
    "end": "4015920"
  },
  {
    "text": "Ray did um so uh as I'm going to write four edfs so here's the video that you have to",
    "start": "4015920",
    "end": "4021799"
  },
  {
    "text": "decode the frames I guess it's very similar to what you've seen before here's an annotation actor",
    "start": "4021799",
    "end": "4029059"
  },
  {
    "text": "uh here here's a frame classifier actor so so kind of the high level idea of this pipeline is",
    "start": "4029059",
    "end": "4035359"
  },
  {
    "text": "um you have two models not just one so you after you decode your frames you have one model that annotates your",
    "start": "4035359",
    "end": "4041000"
  },
  {
    "text": "frames with additional metadata and then you have a second model that does the classification there's a kind of a chain of models",
    "start": "4041000",
    "end": "4047839"
  },
  {
    "text": "um and then you finally have the right the final the right operator so when you put this together",
    "start": "4047839",
    "end": "4053240"
  },
  {
    "text": "um I think there are a couple of things features to note from Ray data first is",
    "start": "4053240",
    "end": "4058760"
  },
  {
    "text": "the the read binary so we have good support for binary data formats not just part A and so on",
    "start": "4058760",
    "end": "4065059"
  },
  {
    "text": "um so uh in general like unstructured data is is",
    "start": "4065059",
    "end": "4071000"
  },
  {
    "text": "handled well in great data the the second thing to know about is that",
    "start": "4071000",
    "end": "4076339"
  },
  {
    "text": "um this decode frames you'll see has this interesting thing number CPU is four so",
    "start": "4076339",
    "end": "4081740"
  },
  {
    "text": "this allows you to kind of customize how Ray allocates uh resources to your",
    "start": "4081740",
    "end": "4087079"
  },
  {
    "text": "operators so here we're saying this decode frame is really expensive um I'm gonna allocate a four seat views",
    "start": "4087079",
    "end": "4093500"
  },
  {
    "text": "Lots on a note instead of one so you can limit the parallelism um uh kind of kind of customize how it's",
    "start": "4093500",
    "end": "4099020"
  },
  {
    "text": "scheduled and uh yeah here we have actual strategy with",
    "start": "4099020",
    "end": "4105140"
  },
  {
    "text": "size fives and an actual pool uh this is on CPUs actually you can have CPU actors and here we're using GPU actor and we're",
    "start": "4105140",
    "end": "4112100"
  },
  {
    "text": "setting a batch size so Ray data will also like make sure like the right batch size is reached for 4G views which is",
    "start": "4112100",
    "end": "4118699"
  },
  {
    "text": "important um and then we have the the output right so you can really customize your uh your",
    "start": "4118699",
    "end": "4124278"
  },
  {
    "text": "pipelines for example it's very common for like models to use much more memory than you might anticipate so you might",
    "start": "4124279",
    "end": "4129920"
  },
  {
    "text": "want to increase the CPUs for the model to prevent it from crashing things like that",
    "start": "4129920",
    "end": "4136359"
  },
  {
    "text": "um and yeah this is kind of a complex pipeline instead of every every transforms its own operator the first",
    "start": "4137239",
    "end": "4143838"
  },
  {
    "text": "one's running on CPU tasks second one running CPU actors third one GPU actors and the fourth one on CPU tests so like",
    "start": "4143839",
    "end": "4150859"
  },
  {
    "text": "Ray data will kind of stitch together this Pipeline and run it in the cluster so you run it",
    "start": "4150859",
    "end": "4157520"
  },
  {
    "text": "um you'll see you'll see some log messages showing the dag that's actually created and there's a progress bar",
    "start": "4157520",
    "end": "4163238"
  },
  {
    "text": "showing resource limits so um like this is actually part of the back pressure so like these are this is",
    "start": "4163239",
    "end": "4170298"
  },
  {
    "text": "all like the limits that Ray data will try to keep the pipeline under",
    "start": "4170299",
    "end": "4175400"
  },
  {
    "text": "they run it then we can uh look at the outputs to sell",
    "start": "4175400",
    "end": "4182060"
  },
  {
    "text": "and uh if you haven't seen Ray dashboard there's also uh observability tools if",
    "start": "4182060",
    "end": "4187460"
  },
  {
    "text": "you go to the metrics page to see what's going on in the cluster so uh when I ran this pipeline these were the graphs I",
    "start": "4187460",
    "end": "4193338"
  },
  {
    "text": "got um so I think this is like a very small cluster maybe like five machines or something like that and you can see",
    "start": "4193339",
    "end": "4199400"
  },
  {
    "text": "here's one graph of active tasks by name so map batches you can see um I guess the",
    "start": "4199400",
    "end": "4205880"
  },
  {
    "text": "most active task was this Frame classifier actor there were six tasks active or 16 tasks active on average the",
    "start": "4205880",
    "end": "4213140"
  },
  {
    "text": "whole time then the second one was the frame annotator you can also see the state of the actors",
    "start": "4213140",
    "end": "4219260"
  },
  {
    "text": "you see like after the actors were created and um they kind of stay busy the whole time the pipeline was running",
    "start": "4219260",
    "end": "4226159"
  },
  {
    "text": "uh and you can see the network usage of the cluster um",
    "start": "4226159",
    "end": "4231980"
  },
  {
    "text": "I I guess here there are a lot of nodes sending sending data between between the operators",
    "start": "4231980",
    "end": "4238600"
  },
  {
    "text": "okay and uh in order to the back end um",
    "start": "4240500",
    "end": "4246199"
  },
  {
    "text": "so Ray data is built on top of Ray it it's there's no special sauce it's just using Ray apis so it uses Ray tasks and",
    "start": "4246199",
    "end": "4254000"
  },
  {
    "text": "actors for execution operators and by default uses tasks but for gpus and",
    "start": "4254000",
    "end": "4259159"
  },
  {
    "text": "stuff you can use actors the intermediate data is stored in the very Object Store so",
    "start": "4259159",
    "end": "4266120"
  },
  {
    "text": "um and there's a back pressure to kind of enable efficient streaming without spilling to disk this is kind of a kind",
    "start": "4266120",
    "end": "4272300"
  },
  {
    "text": "of a subtle point because like if you don't have back pressure there's kind of no point in streaming because it's just it's just bulk execution it's the same",
    "start": "4272300",
    "end": "4277880"
  },
  {
    "text": "thing so the main main difference is actually exact question",
    "start": "4277880",
    "end": "4283420"
  },
  {
    "text": "um and this is kind of uh interesting like why did we build on rate core",
    "start": "4284480",
    "end": "4290780"
  },
  {
    "text": "um there's kind of a lot of stuff we just get out of the box array like for free um so we got we have a heterogeneous",
    "start": "4290780",
    "end": "4297199"
  },
  {
    "text": "cluster support and we get automatic fault tolerance actually um because we just use rate tasks and",
    "start": "4297199",
    "end": "4302420"
  },
  {
    "text": "actors we inherit raise the lineage-based Reconstruction which means that you're running a large batch",
    "start": "4302420",
    "end": "4308239"
  },
  {
    "text": "inference job and something crashes we do some objects we're able to just resubmit the the tasks resubmit the",
    "start": "4308239",
    "end": "4313699"
  },
  {
    "text": "actors and uh it should survive those scalers",
    "start": "4313699",
    "end": "4318820"
  },
  {
    "text": "um and of course the object store uh is provides things like spilling uh if",
    "start": "4319219",
    "end": "4324380"
  },
  {
    "text": "there is for some reason unexpectedly High memory usage like some UDF returns some enormous data array that you know",
    "start": "4324380",
    "end": "4331040"
  },
  {
    "text": "has to be spilled then it will just slow down and still crashing right um and uh there's a lot of other things",
    "start": "4331040",
    "end": "4336800"
  },
  {
    "text": "Object Store can also do shuffles um",
    "start": "4336800",
    "end": "4342400"
  },
  {
    "text": "yeah and uh and uh the little optimizations from from Rey uh",
    "start": "4342620",
    "end": "4349580"
  },
  {
    "text": "um like for data locality also kind of automatically apply phone",
    "start": "4349580",
    "end": "4355659"
  },
  {
    "text": "so scalability we're still early in the scalability test for for Ray data streaming because it's a new component",
    "start": "4356000",
    "end": "4362060"
  },
  {
    "text": "but here's one example of a 20 terabyte array data set that we ran on 500 machines and uh we so we try to sample",
    "start": "4362060",
    "end": "4369500"
  },
  {
    "text": "some simple like one two four three four stage pipelines and kind of just check the throughput and you can see the",
    "start": "4369500",
    "end": "4375020"
  },
  {
    "text": "throughput is actually fairly good um uh so you could do more than one terabyte a second on on the simplest",
    "start": "4375020",
    "end": "4382040"
  },
  {
    "text": "pipeline like there's very little data movement for these pipelines as you need to send more data across the network uh",
    "start": "4382040",
    "end": "4387380"
  },
  {
    "text": "kind of the throat slows down yeah you see it still works",
    "start": "4387380",
    "end": "4392079"
  },
  {
    "text": "um and the final feature I want to talk about what about distributive training so I mentioned before you can use these",
    "start": "4393560",
    "end": "4399980"
  },
  {
    "text": "same pipelines for inference and for training and what does that actually mean uh more code basically uh you're",
    "start": "4399980",
    "end": "4406280"
  },
  {
    "text": "just swapping the latter half of the the operator so like say I have that read pipeline I'm reading some data I'm",
    "start": "4406280",
    "end": "4412640"
  },
  {
    "text": "applying some pre-processing and then sending it to my model for inference with training the first two things stay",
    "start": "4412640",
    "end": "4419000"
  },
  {
    "text": "the same so you have that read data source map batches the thing the only thing I changed is I'm going to add this",
    "start": "4419000",
    "end": "4424520"
  },
  {
    "text": "split call so I'm going to add an operator to split it into you know a number of subset uh data data sub",
    "start": "4424520",
    "end": "4431840"
  },
  {
    "text": "streams and then I can kind of give it to my rate actors to to consume in",
    "start": "4431840",
    "end": "4436880"
  },
  {
    "text": "parallel uh this is something that we're trying to",
    "start": "4436880",
    "end": "4441980"
  },
  {
    "text": "incorporate with raytrain as well to make it a little bit easier to use",
    "start": "4441980",
    "end": "4446260"
  },
  {
    "text": "we also did an experiment of streaming for training for training with Ray data um so this is a example where we used uh",
    "start": "4447679",
    "end": "4455780"
  },
  {
    "text": "just GPU nodes and then GPU notes plus CPU nodes and this is actually not even this is uh this pipeline I think I",
    "start": "4455780",
    "end": "4462860"
  },
  {
    "text": "should say it's it's just reading some images from S3 and applying a pre-processing operation before sending it to to uh to the",
    "start": "4462860",
    "end": "4469940"
  },
  {
    "text": "trainer and you can see that adding CPU notes actually adds the speeds up the training significantly like",
    "start": "4469940",
    "end": "4476120"
  },
  {
    "text": "um the the training throughput is is much higher if you use CPU notes because additional like network network uh and",
    "start": "4476120",
    "end": "4483800"
  },
  {
    "text": "uh TPU from those notes okay so I think that's that's pretty",
    "start": "4483800",
    "end": "4489260"
  },
  {
    "text": "much all I had kind of in tldr's rate data scaled both batch inference and training workloads um it's kind of",
    "start": "4489260",
    "end": "4495620"
  },
  {
    "text": "inherently more efficient computation model than both processing and the apis lets you kind of compose these things uh",
    "start": "4495620",
    "end": "4502040"
  },
  {
    "text": "simply so uh we have the docs available in docs.rata IO um coming next is we're",
    "start": "4502040",
    "end": "4509000"
  },
  {
    "text": "improving Ray train integration and 2006 and we're trying to harden it to work more robustly at larger scales for",
    "start": "4509000",
    "end": "4514699"
  },
  {
    "text": "example hundreds of nodes and very large numbers of the input files itself and uh",
    "start": "4514699",
    "end": "4520100"
  },
  {
    "text": "yeah if you're interested in working on these though we're happy to kind of collaborate thank you Eric",
    "start": "4520100",
    "end": "4526400"
  },
  {
    "text": "um quick question what does an operator comprise can you",
    "start": "4526400",
    "end": "4531739"
  },
  {
    "text": "go back to a code and say is the operator the function the user-defined function that you provide to the map",
    "start": "4531739",
    "end": "4537440"
  },
  {
    "text": "batches or the operator is something that that radiator fuses together into one operator yeah",
    "start": "4537440",
    "end": "4545239"
  },
  {
    "text": "that's a good question um I I think it's yeah it's a little subtle Point what is an operator is this an",
    "start": "4545239",
    "end": "4551900"
  },
  {
    "text": "operator is this an operator uh the terminology I think we have is this is a logical operator and this is a physical",
    "start": "4551900",
    "end": "4557300"
  },
  {
    "text": "operator um so like when you define a API call",
    "start": "4557300",
    "end": "4562340"
  },
  {
    "text": "that creates a logical operator and then finally the ray data will optimize and create these physical operators that",
    "start": "4562340",
    "end": "4568340"
  },
  {
    "text": "might involve used multiple fuselogical operations so the concept of operator is actually something that the red ray data does",
    "start": "4568340",
    "end": "4575060"
  },
  {
    "text": "behind the scenes it's not that you defined as an operator you define the the stages of the things you want to do",
    "start": "4575060",
    "end": "4581239"
  },
  {
    "text": "and the radio data when it's actually parsing the code fuses them into a single operator that actually makes",
    "start": "4581239",
    "end": "4586340"
  },
  {
    "text": "sense to execute that's right like as a user you write Transformations on the data source and the the term operator is",
    "start": "4586340",
    "end": "4593900"
  },
  {
    "text": "in actually in the API it comes out in kind of like the log messages and the status reporting it's been more of an",
    "start": "4593900",
    "end": "4599480"
  },
  {
    "text": "implementation detail yeah I couldn't I couldn't help but to see the analogy and maybe I'm wrong over here but when you",
    "start": "4599480",
    "end": "4606140"
  },
  {
    "text": "use the word like staging because you know if you recall obviously you come from the spark background the the way",
    "start": "4606140",
    "end": "4612380"
  },
  {
    "text": "the spark takes the job converts into a stages and stages that converts into your task is that very similar in in how",
    "start": "4612380",
    "end": "4619880"
  },
  {
    "text": "you're actually parsing the code to to to divide them into stages and then from",
    "start": "4619880",
    "end": "4624920"
  },
  {
    "text": "the stages you're actually executing things in parallel yeah it's pretty similar um actually I would say uh",
    "start": "4624920",
    "end": "4630860"
  },
  {
    "text": "before before uh uh before the streaming back end it was a it's like pretty much the same",
    "start": "4630860",
    "end": "4636260"
  },
  {
    "text": "execution model yeah folks it's a bulk synchronous execution okay any questions I'm going to open up to the crowd",
    "start": "4636260",
    "end": "4643460"
  },
  {
    "text": "yeah there you go there's the ray data also supports some kind of an SQL translation like these",
    "start": "4643460",
    "end": "4650540"
  },
  {
    "text": "operators and the stages kind of the thing yeah so this race uh data Sports SQL so we're we don't have any first",
    "start": "4650540",
    "end": "4657679"
  },
  {
    "text": "party support for the SQL um there are actually some Community projects for running sq1 array so I know",
    "start": "4657679",
    "end": "4663860"
  },
  {
    "text": "of a lot of experimental like data frame libraries on array for running SQL and you can also run actually spark Andre",
    "start": "4663860",
    "end": "4669020"
  },
  {
    "text": "actually I think some companies actually run that in production um uh like spark Andre to kind of like",
    "start": "4669020",
    "end": "4675440"
  },
  {
    "text": "you know if either infrastructure there is an independent Community project called Ray SQL",
    "start": "4675440",
    "end": "4682219"
  },
  {
    "text": "which underneath actually uses Ray data right any other questions",
    "start": "4682219",
    "end": "4689260"
  },
  {
    "text": "I had a question regarding the heterogeneous pipeline just trying to understand it better so when so essentially when you have a",
    "start": "4690860",
    "end": "4699199"
  },
  {
    "text": "task that gets completed on a CPU are is that are those CPUs on the same node uh as",
    "start": "4699199",
    "end": "4706760"
  },
  {
    "text": "the node with gpus or our deceptive nodes because then communication buttons",
    "start": "4706760",
    "end": "4711920"
  },
  {
    "text": "become very different and performance can vary yeah so the question is if you have like the CPU state in the GPU stage",
    "start": "4711920",
    "end": "4718340"
  },
  {
    "text": "like what happens if they're on different nodes or can they be on different nodes um so the answer is yeah they can be on different nodes for",
    "start": "4718340",
    "end": "4724100"
  },
  {
    "text": "example you have one GPU node then you might make sense for rate data as a schedule is on different nodes and and",
    "start": "4724100",
    "end": "4730340"
  },
  {
    "text": "Ray data has this optimization to for locality optimization where it will try to put them on the same node but if if",
    "start": "4730340",
    "end": "4737060"
  },
  {
    "text": "if it would have like if there's not enough resource it will like you know put them on other nodes and there might",
    "start": "4737060",
    "end": "4743120"
  },
  {
    "text": "be network transfer incurred to to transfer the data between nodes",
    "start": "4743120",
    "end": "4748880"
  },
  {
    "text": "yeah you need one few examples you say so I can you can satisfy how many gpus you",
    "start": "4748880",
    "end": "4755120"
  },
  {
    "text": "use to run inference operator and I'm wearing can you also specify like a GPU type if you have like a a100 or 100 and",
    "start": "4755120",
    "end": "4763400"
  },
  {
    "text": "then you want to say uh which one you want to use",
    "start": "4763400",
    "end": "4768620"
  },
  {
    "text": "yeah a good question so let's say you have a cluster with two different GPU types um so Ray",
    "start": "4768620",
    "end": "4774440"
  },
  {
    "text": "has this notion of like uh resource resource custom resources so you could",
    "start": "4774440",
    "end": "4779540"
  },
  {
    "text": "like you you could like kind of tag nodes with like custom resources and then you can add that to a constraint I",
    "start": "4779540",
    "end": "4785300"
  },
  {
    "text": "think have probably some example of that um",
    "start": "4785300",
    "end": "4791300"
  },
  {
    "text": "like here like here we're saying let me see people is four but you can also add like resources equals like custom tags",
    "start": "4791300",
    "end": "4797900"
  },
  {
    "text": "on a node so you can make sure it runs on a V100 note so uh yeah",
    "start": "4797900",
    "end": "4803860"
  },
  {
    "text": "yeah custom research is a lot is a way to provide an affinity that this particular task is going to be scheduled",
    "start": "4803960",
    "end": "4809300"
  },
  {
    "text": "on that particular GPU or a CPU any other questions",
    "start": "4809300",
    "end": "4815500"
  },
  {
    "text": "thank you this work reminds me uh no House Park",
    "start": "4818060",
    "end": "4823900"
  },
  {
    "text": "outperformed uh Hadoop you know when you when you avoid you know writing on the",
    "start": "4823900",
    "end": "4829280"
  },
  {
    "text": "disk anyway but when you compare the one stage two three and four stages",
    "start": "4829280",
    "end": "4835520"
  },
  {
    "text": "it's not clear if you are just using CPUs gpus you are mixing so",
    "start": "4835520",
    "end": "4840920"
  },
  {
    "text": "oh yeah good question so for like kind of the stress tests we did that's that's a question right this one um these are",
    "start": "4840920",
    "end": "4847760"
  },
  {
    "text": "very simple stages so these are all actually all CPU actor stages um and it was run on a CPU cluster",
    "start": "4847760",
    "end": "4853219"
  },
  {
    "text": "because it's like a 500 CPU CPU CPU node machine um so what this is doing is actually",
    "start": "4853219",
    "end": "4859400"
  },
  {
    "text": "it's testing the scheduler it's testing a data plan there's no there's no gpus involved so there is a lot of communication",
    "start": "4859400",
    "end": "4865820"
  },
  {
    "text": "that's why you know losing the performance um so one is yeah there is there is more",
    "start": "4865820",
    "end": "4872900"
  },
  {
    "text": "some more Network overhead like so rate data has this like locality optimization we try to avoid data movement but as you",
    "start": "4872900",
    "end": "4878960"
  },
  {
    "text": "have more stage there will be some more data movement and also the number of tasks you have to dispatch is just very",
    "start": "4878960",
    "end": "4884480"
  },
  {
    "text": "high at the scale so the the scheduling the actual control group starts to becoming a bottleneck at this at the",
    "start": "4884480",
    "end": "4890239"
  },
  {
    "text": "scale so each stage is executed on one machine",
    "start": "4890239",
    "end": "4896000"
  },
  {
    "text": "or a different machine or oh so uh yeah I should give more details about this so I imagine like um this like three stage",
    "start": "4896000",
    "end": "4903260"
  },
  {
    "text": "pipeline um like every stage will have like there'll be three operators and every",
    "start": "4903260",
    "end": "4908360"
  },
  {
    "text": "operator will have I think it was like 200 actors or something like that um so wait so it had",
    "start": "4908360",
    "end": "4914179"
  },
  {
    "text": "up to 200 current tasks per stage um and these these actors are spread across the cluster yeah",
    "start": "4914179",
    "end": "4921820"
  },
  {
    "text": "because I think you can have better results if you",
    "start": "4924040",
    "end": "4930820"
  },
  {
    "text": "some tuning you can you know have better throughput anyway yeah I think I agree with you",
    "start": "4931159",
    "end": "4937100"
  },
  {
    "text": "like um so this is stressing the upper boundaries of our scheduler and we we haven't bothered really optimizing the",
    "start": "4937100",
    "end": "4943460"
  },
  {
    "text": "scheduler it's like a you know it's like a event Looper and then Michael yeah my intuition is you can do better absolutely yeah I think so if you have a",
    "start": "4943460",
    "end": "4950780"
  },
  {
    "text": "use case or need it yeah enthusiasm but we are hiring by the way",
    "start": "4950780",
    "end": "4956199"
  },
  {
    "text": "any other questions Eric",
    "start": "4956199",
    "end": "4962659"
  },
  {
    "text": "and last but not least we actually have uh array user case from uh Pinterest and",
    "start": "4966860",
    "end": "4973280"
  },
  {
    "text": "he's going to show us how they're actually using rain to build the internal platform interactively to to to expedite their",
    "start": "4973280",
    "end": "4980300"
  },
  {
    "text": "developer experience so please welcome suan",
    "start": "4980300",
    "end": "4985380"
  },
  {
    "text": "[Applause] let's give it a try so",
    "start": "4985380",
    "end": "4992780"
  },
  {
    "text": "um",
    "start": "4997820",
    "end": "4999820"
  },
  {
    "text": "other good permissions or something yeah",
    "start": "5015100",
    "end": "5020040"
  },
  {
    "text": "thank you",
    "start": "5037300",
    "end": "5039960"
  },
  {
    "text": "all the hooks here we come through okay",
    "start": "5050620",
    "end": "5055420"
  },
  {
    "text": "I should be there",
    "start": "5057600",
    "end": "5062699"
  },
  {
    "text": "foreign",
    "start": "5065800",
    "end": "5068040"
  },
  {
    "text": "oh all right cool",
    "start": "5096280",
    "end": "5101760"
  },
  {
    "text": "um hello everyone I'm say one and I'm from the machine learning data platform",
    "start": "5101980",
    "end": "5107260"
  },
  {
    "text": "team at Pinterest thanks so much for having me here today",
    "start": "5107260",
    "end": "5112560"
  },
  {
    "text": "I'll be talking about improving ml Dev velocity with Rey at",
    "start": "5112560",
    "end": "5118780"
  },
  {
    "text": "Pinterest so hopefully this is uh something that you find interesting this",
    "start": "5118780",
    "end": "5124659"
  },
  {
    "text": "today's talk is going to be a little bit more higher level I'm not going to have a deep dive into the performance",
    "start": "5124659",
    "end": "5131739"
  },
  {
    "text": "benchmarks and how we're squeezing the most out of the hardware using Ray but we're going to take a slightly different",
    "start": "5131739",
    "end": "5138159"
  },
  {
    "text": "angle and talk about machine learning engineer velocity and how we think about making",
    "start": "5138159",
    "end": "5145120"
  },
  {
    "text": "machine learning Engineers move faster so as we go through this talk and help you you know put yourself in as an ml",
    "start": "5145120",
    "end": "5151420"
  },
  {
    "text": "engineer shoes working in a web scale recommendations company and see uh and",
    "start": "5151420",
    "end": "5157000"
  },
  {
    "text": "really follow through on how you might be able to develop models faster all right so I'm gonna start by giving a",
    "start": "5157000",
    "end": "5164739"
  },
  {
    "text": "very brief overview of machine learning at Pinterest what kind of problems we solve at what scale what kind of metrics",
    "start": "5164739",
    "end": "5172239"
  },
  {
    "text": "we care about then I'm going to talk about one of the most popular developer interfaces in",
    "start": "5172239",
    "end": "5179139"
  },
  {
    "text": "machine learning which is workflows and then we're going to talk about an",
    "start": "5179139",
    "end": "5184540"
  },
  {
    "text": "interesting limitation that workflows have which we call the scale first and learn",
    "start": "5184540",
    "end": "5189880"
  },
  {
    "text": "last problem and then we're gonna go into how we are using Rey how we're",
    "start": "5189880",
    "end": "5196659"
  },
  {
    "text": "thinking about using Ray going forward the Pinterest to switch this around to learn fast and",
    "start": "5196659",
    "end": "5203320"
  },
  {
    "text": "scale fast and then I'll be sharing some asks for the the ray Community all right",
    "start": "5203320",
    "end": "5209980"
  },
  {
    "text": "so let's get started first of all machine learning at Pinterest",
    "start": "5209980",
    "end": "5216540"
  },
  {
    "text": "in case some of you may not be too well versed with uh what Pinterest is it's a",
    "start": "5216820",
    "end": "5222639"
  },
  {
    "text": "it's a visual inspiration engine uh people come to Pinterest and our mission is to bring everyone the",
    "start": "5222639",
    "end": "5229000"
  },
  {
    "text": "inspiration to create the life they love people come to Pinterest with various intentions",
    "start": "5229000",
    "end": "5234239"
  },
  {
    "text": "uh yes or some of our painters might be looking to you know revamp their house",
    "start": "5234239",
    "end": "5240219"
  },
  {
    "text": "interior or you know look for new fashion look for new recipes and they're",
    "start": "5240219",
    "end": "5245560"
  },
  {
    "text": "trying to do stuff in real life and look for Inspirations online and that's where Pinterest comes in",
    "start": "5245560",
    "end": "5253260"
  },
  {
    "text": "so at its core it's a big big recommendation engine right and this recommendation is engine",
    "start": "5253840",
    "end": "5261400"
  },
  {
    "text": "is powered by a variety of machine learning applications when you first open the Pinterest",
    "start": "5261400",
    "end": "5267219"
  },
  {
    "text": "application you will be shown the home feed that's where our very first recommendations show up you like some of",
    "start": "5267219",
    "end": "5274420"
  },
  {
    "text": "the pins which is what we call our content you click on them and it's going to show take you to a variety of other",
    "start": "5274420",
    "end": "5281020"
  },
  {
    "text": "related pins uh you can also search for Pins using",
    "start": "5281020",
    "end": "5286239"
  },
  {
    "text": "text you can perform image based search and between various recommendation types",
    "start": "5286239",
    "end": "5292480"
  },
  {
    "text": "you will add will be interleaved where the in an ideal form the ads are so",
    "start": "5292480",
    "end": "5297820"
  },
  {
    "text": "relevant we know exactly what you're looking for that it's hard to distinguish them from organic content",
    "start": "5297820",
    "end": "5303219"
  },
  {
    "text": "right and a lot of these product features and surfaces under the hood are also then further powered by other types",
    "start": "5303219",
    "end": "5310780"
  },
  {
    "text": "of machine learning applications we have image understanding content understanding LM is a big thing",
    "start": "5310780",
    "end": "5317980"
  },
  {
    "text": "right we have a lot of good kind of a good llns and maybe visual models under",
    "start": "5317980",
    "end": "5323620"
  },
  {
    "text": "the hood extracting content and summarizing them under the hood we also",
    "start": "5323620",
    "end": "5329020"
  },
  {
    "text": "use them for uh unhealthy content detection right these are all very important machine learning applications",
    "start": "5329020",
    "end": "5335920"
  },
  {
    "text": "the power this recommendation engine called Pinterest",
    "start": "5335920",
    "end": "5342659"
  },
  {
    "text": "so and it's uh and we do this at a very large scale and that's where interesting problems start popping up right and we",
    "start": "5343600",
    "end": "5350980"
  },
  {
    "text": "have 450 million monthly active users exploring Corpus of over 100 billion",
    "start": "5350980",
    "end": "5356860"
  },
  {
    "text": "pins generating billions of events every day a variety of use machine learning use",
    "start": "5356860",
    "end": "5363280"
  },
  {
    "text": "cases in terms of how the problems are formulated there's a recommendation and ads problems and within those there's",
    "start": "5363280",
    "end": "5370179"
  },
  {
    "text": "retrieval ranking and blending machine learning problems and briefly mentioned representation learning got to",
    "start": "5370179",
    "end": "5377080"
  },
  {
    "text": "understand the content to understand the user and really help them do the stuff they love",
    "start": "5377080",
    "end": "5382540"
  },
  {
    "text": "and these are as I showed scattered across all sorts of different product",
    "start": "5382540",
    "end": "5387580"
  },
  {
    "text": "surfaces that I just mentioned and this is powered by a centralized",
    "start": "5387580",
    "end": "5392739"
  },
  {
    "text": "machine learning platform what our goal at a higher level is twofold first of all we want to build",
    "start": "5392739",
    "end": "5400120"
  },
  {
    "text": "foundational components with new capabilities and these are things like feature store",
    "start": "5400120",
    "end": "5406060"
  },
  {
    "text": "training from Chip platform inference services but we also really care about",
    "start": "5406060",
    "end": "5412440"
  },
  {
    "text": "our machine learning engineers industry and Industry people talk about",
    "start": "5412440",
    "end": "5417520"
  },
  {
    "text": "it it's so hard to get gpus gpus are so expensive but if you look at it from a business perspective machine learning",
    "start": "5417520",
    "end": "5424120"
  },
  {
    "text": "Engineers are more expensive than gpus and we really care about helping them move faster this is the resource that we",
    "start": "5424120",
    "end": "5430780"
  },
  {
    "text": "want to optimize for and that's going to be the center of my talk today on how we are thinking about improving machine",
    "start": "5430780",
    "end": "5437679"
  },
  {
    "text": "learning developer velocity you know from a you know rough very high level point of view we break it down",
    "start": "5437679",
    "end": "5444520"
  },
  {
    "text": "into two metrics calendar days taken to perform a certain task and then the active development time",
    "start": "5444520",
    "end": "5451320"
  },
  {
    "text": "taken to perform certain machine learning tasks all right so as you go",
    "start": "5451320",
    "end": "5456880"
  },
  {
    "text": "through the talk please keep in mind the angle they were approaching from is optimizing machine learning engineers",
    "start": "5456880",
    "end": "5464580"
  },
  {
    "text": "all right so now let's uh go into the the main body of the talk",
    "start": "5464980",
    "end": "5471520"
  },
  {
    "text": "machine learning pipelines and the scale first and learn last problem",
    "start": "5471520",
    "end": "5477460"
  },
  {
    "text": "so you know empowering web scale machine learning is is a very hard thing and and",
    "start": "5477460",
    "end": "5484179"
  },
  {
    "text": "by now I'm sure a lot of you have seen this diagram before it does a really good job in in demonstrating",
    "start": "5484179",
    "end": "5491920"
  },
  {
    "text": "that writing machine learning modeling code is a small part of the entire machine learning life cycle right you",
    "start": "5491920",
    "end": "5499300"
  },
  {
    "text": "gotta collect the data somehow you've got to verify them extract machine understandable features and then you got",
    "start": "5499300",
    "end": "5506500"
  },
  {
    "text": "to analyze them resource uh you've got to manage resources manage the pipelines and you have the",
    "start": "5506500",
    "end": "5513520"
  },
  {
    "text": "serving infrastructure and so forth It's really really tough right and I particularly like this thing because it",
    "start": "5513520",
    "end": "5520780"
  },
  {
    "text": "makes such a clear case to our business leaders and why they should be signing our ml platform paychecks right this",
    "start": "5520780",
    "end": "5526900"
  },
  {
    "text": "really shows how complicated this space is and why we should be investing it to",
    "start": "5526900",
    "end": "5532480"
  },
  {
    "text": "help machine learning Engineers move faster and when you have a lot of these steps",
    "start": "5532480",
    "end": "5538120"
  },
  {
    "text": "right it's it's really natural the first thing Engineers think about is okay you got to do a bunch of stuff why don't we",
    "start": "5538120",
    "end": "5544659"
  },
  {
    "text": "connect them as workflows right so then in this kind of model we have ml platform teams jumping into the",
    "start": "5544659",
    "end": "5551380"
  },
  {
    "text": "scene building some foundational components feature store data sets hyper Camera",
    "start": "5551380",
    "end": "5557440"
  },
  {
    "text": "Tuning training platform model store visualization inference Solutions",
    "start": "5557440",
    "end": "5562900"
  },
  {
    "text": "and each of these foundational components end up exposing apis or even",
    "start": "5562900",
    "end": "5568120"
  },
  {
    "text": "jobs that run on popular execution runtimes right they can run in spark",
    "start": "5568120",
    "end": "5573340"
  },
  {
    "text": "they can run on Pi torch they can run an array and this is the very first layer of operation for our mission learning",
    "start": "5573340",
    "end": "5580000"
  },
  {
    "text": "platform teams we build the components the components are programmable right once you have stuff that's programmable",
    "start": "5580000",
    "end": "5586960"
  },
  {
    "text": "you've got to figure out how to orchestrate and glue them together and then we talked about how are you",
    "start": "5586960",
    "end": "5592239"
  },
  {
    "text": "going to do that and most companies use something similar to airflow or some",
    "start": "5592239",
    "end": "5598239"
  },
  {
    "text": "some variants of their workflow systems where you can say hey I got a perform this step that's that that step in that",
    "start": "5598239",
    "end": "5605139"
  },
  {
    "text": "order and I want to be given some visualization and some operational tooling right",
    "start": "5605139",
    "end": "5612960"
  },
  {
    "text": "realistic view of how a lot of machine learning pipelines are structured at",
    "start": "5613480",
    "end": "5619239"
  },
  {
    "text": "Pinterest once you start gluing these things together and Eric actually earlier touch",
    "start": "5619239",
    "end": "5624699"
  },
  {
    "text": "upon this and some a little bit more of a higher level review where many ml",
    "start": "5624699",
    "end": "5629800"
  },
  {
    "text": "application teams have a summer fairly static workflow that's continuously producing data sets in our case to",
    "start": "5629800",
    "end": "5636040"
  },
  {
    "text": "recommendation heavy so we need to join stuff that we served a lot of features",
    "start": "5636040",
    "end": "5641380"
  },
  {
    "text": "and the stuff that users did on our application you join these two and then",
    "start": "5641380",
    "end": "5646659"
  },
  {
    "text": "now we have a training data set for some large-scale supervised learning and uh and then we move on to the actual",
    "start": "5646659",
    "end": "5654340"
  },
  {
    "text": "training workflow and this is we call what we call the inner loop of the ml life cycle because this is where machine",
    "start": "5654340",
    "end": "5660820"
  },
  {
    "text": "learning Engineers iterate this is where they they interact with the most and start bringing business value",
    "start": "5660820",
    "end": "5667420"
  },
  {
    "text": "and this is also fairly realistic at Pinterest where one of the very first things you do is try out new features",
    "start": "5667420",
    "end": "5674080"
  },
  {
    "text": "right it's not there in the training data set somebody's publishing a new feature I'm gonna try it out",
    "start": "5674080",
    "end": "5679360"
  },
  {
    "text": "and then okay I enriched my data set with a few features I'm going to run some new sampling strategies I'm going",
    "start": "5679360",
    "end": "5685600"
  },
  {
    "text": "to run some new split strategies and then you apply some reward algorithms to assign weights and that you understand",
    "start": "5685600",
    "end": "5691900"
  },
  {
    "text": "the feature stats then you got a train you got to evaluate run some feature",
    "start": "5691900",
    "end": "5697360"
  },
  {
    "text": "importance analysis and then you promote the model to some model store right and",
    "start": "5697360",
    "end": "5702460"
  },
  {
    "text": "this that part is the inner loop and that part is what we're interested in optimizing right and when we think about",
    "start": "5702460",
    "end": "5710199"
  },
  {
    "text": "optimizing workflows the very first thing we think about isn't really about okay how can I make individual things",
    "start": "5710199",
    "end": "5717340"
  },
  {
    "text": "faster the very first thing we think about is are people doing the same stuff over and",
    "start": "5717340",
    "end": "5723520"
  },
  {
    "text": "over again right then just like how we refactor code there's rule of three you",
    "start": "5723520",
    "end": "5729340"
  },
  {
    "text": "find yourself doing the same stuff three times you're going to refactor pull it out into a function and then make sure",
    "start": "5729340",
    "end": "5734440"
  },
  {
    "text": "that you don't do it again same principle applies to ml Ops from platform teams where we instead of",
    "start": "5734440",
    "end": "5741400"
  },
  {
    "text": "refactoring code we templatize the workflows so that if we know that people",
    "start": "5741400",
    "end": "5746920"
  },
  {
    "text": "are going to be following a certain structure to train models and solve some business problems we're just going to",
    "start": "5746920",
    "end": "5753760"
  },
  {
    "text": "give them configurations somebody's written it why should anybody else write the same thing over and over",
    "start": "5753760",
    "end": "5758860"
  },
  {
    "text": "again if I've written this workflow I want to try learning right 0.1 and this my colleague wants to try 0.2 we should",
    "start": "5758860",
    "end": "5766480"
  },
  {
    "text": "be using the same stuff and just plug in 0.1 and 0.2 right and that's the principle behind it",
    "start": "5766480",
    "end": "5772179"
  },
  {
    "text": "and this kind of workflow templating strategy really improves a ml developer velocity",
    "start": "5772179",
    "end": "5778360"
  },
  {
    "text": "and bring further values right and there's a lot of different steps in this Pipeline and it's reasonable to think",
    "start": "5778360",
    "end": "5785199"
  },
  {
    "text": "that every step is producing some form of data variance right we don't want people to be doing the same running the",
    "start": "5785199",
    "end": "5792340"
  },
  {
    "text": "same stuff over again it's not just about writing code you've got to be able to Cache the data and if it's no longer",
    "start": "5792340",
    "end": "5799239"
  },
  {
    "text": "important you got to be able to garbage collect it you've got to make sure that these intermediate data sets are discoverable you want to be able to",
    "start": "5799239",
    "end": "5806500"
  },
  {
    "text": "de-duplicate them linked from lineage tracking the one of the more important thing on",
    "start": "5806500",
    "end": "5812679"
  },
  {
    "text": "top of developer velocity is reproducibility in machine learning right when we template templatize",
    "start": "5812679",
    "end": "5818800"
  },
  {
    "text": "workflows like this a model is now then defined by it's some function of shuffling configs into some template",
    "start": "5818800",
    "end": "5825219"
  },
  {
    "text": "right and this becomes highly reproducible you don't have to figure out all which data set did you use oh",
    "start": "5825219",
    "end": "5831699"
  },
  {
    "text": "wait how come I run the same thing I'm not getting the same result nobody needs to worry about that anymore because all",
    "start": "5831699",
    "end": "5838420"
  },
  {
    "text": "of your experiments hypothesis and validation is defined by a tuple of configurations and templates",
    "start": "5838420",
    "end": "5844780"
  },
  {
    "text": "right and it really helps that this configs are declarative it's like you Google lab and you have some kind of",
    "start": "5844780",
    "end": "5850780"
  },
  {
    "text": "experimentation plan and you can reason about what's going to happen right",
    "start": "5850780",
    "end": "5856678"
  },
  {
    "text": "so at Pinterest uh we built some lightweight workflow wrapper uh so we",
    "start": "5858100",
    "end": "5864159"
  },
  {
    "text": "call it the ml workflow templatization system it's called easy flow here's some",
    "start": "5864159",
    "end": "5870699"
  },
  {
    "text": "realistic screenshots and the code examples that's code example somewhat",
    "start": "5870699",
    "end": "5875800"
  },
  {
    "text": "fake basically we give the ml Engineers or platform teams some tool to write some",
    "start": "5875800",
    "end": "5882880"
  },
  {
    "text": "Python program and in a Python program they're saying hey this function is a template I'm",
    "start": "5882880",
    "end": "5888639"
  },
  {
    "text": "going to define a bunch of configurations and what the system does it just renders some UI form Fields so",
    "start": "5888639",
    "end": "5895239"
  },
  {
    "text": "that once somebody has written and done the job of did the job of writing the pipeline and templatize it anybody can",
    "start": "5895239",
    "end": "5902380"
  },
  {
    "text": "go and just plug in some configurations and then gets value out of it right and also it as you can see here it",
    "start": "5902380",
    "end": "5910060"
  },
  {
    "text": "performs some deterministic lineage tracking and does a few fancy stuff out of it right if two ml Engineers are",
    "start": "5910060",
    "end": "5916780"
  },
  {
    "text": "kicking off the same stuff and they're evaluating the exact same sequence of lineage of data and jobs it's going to",
    "start": "5916780",
    "end": "5923739"
  },
  {
    "text": "perform some one-time synchronization it's going to lock it so that hey somebody else is doing the same thing I'm not going to let you do the same",
    "start": "5923739",
    "end": "5929500"
  },
  {
    "text": "thing let's wait a little bit or they can also detect the jobs and data sets",
    "start": "5929500",
    "end": "5934540"
  },
  {
    "text": "automate the data list and management as I mentioned once you start tracking the lineage things are even more",
    "start": "5934540",
    "end": "5940780"
  },
  {
    "text": "reproducible than before all right and this is also one thing that the",
    "start": "5940780",
    "end": "5946540"
  },
  {
    "text": "templating approach that we particularly like from Mo platform teams is that it",
    "start": "5946540",
    "end": "5952360"
  },
  {
    "text": "doesn't have to be ml platform or infra teams going ahead and building this Plumbing work right anybody can go and",
    "start": "5952360",
    "end": "5959560"
  },
  {
    "text": "write domain-specific templates so anybody can do if they have time and one of the plumbing work go do the plumbing",
    "start": "5959560",
    "end": "5966520"
  },
  {
    "text": "work and you and you come up with this kind of the templates you can release it",
    "start": "5966520",
    "end": "5972280"
  },
  {
    "text": "to see what if the community likes it or not and then you can get some feedback and give them further democratized",
    "start": "5972280",
    "end": "5978460"
  },
  {
    "text": "machine learning in the company so let's see now I want you all to think",
    "start": "5978460",
    "end": "5985120"
  },
  {
    "text": "about uh you put yourself in ml Engineers shoes in this kind of world or such a an approach exists",
    "start": "5985120",
    "end": "5992020"
  },
  {
    "text": "let's talk about one case here where you're trying to train a new student model using a different teacher model in",
    "start": "5992020",
    "end": "5999159"
  },
  {
    "text": "production there's a model student model trained running but it was trained on some other teacher model I want to plug",
    "start": "5999159",
    "end": "6004620"
  },
  {
    "text": "in a different teacher model and then here's what then engineer needs to do and it's only going to take",
    "start": "6004620",
    "end": "6009980"
  },
  {
    "text": "maximum a few hours first of all you know you just gotta",
    "start": "6009980",
    "end": "6015239"
  },
  {
    "text": "find the template that you need go to the UI there's a drop down search for some template and then you can",
    "start": "6015239",
    "end": "6021960"
  },
  {
    "text": "conclude the configuration from the model you're trying to beat you clone that thing and then you plug",
    "start": "6021960",
    "end": "6028380"
  },
  {
    "text": "in a different different teacher model and maybe you want to try some other configuration right and then you change",
    "start": "6028380",
    "end": "6034920"
  },
  {
    "text": "a few things click save the workflow gets scheduled and it starts running periodically refreshing the model and",
    "start": "6034920",
    "end": "6041760"
  },
  {
    "text": "then you you like it if you like how the model is performing and then you can go and take it to an online a b experiment",
    "start": "6041760",
    "end": "6049560"
  },
  {
    "text": "and here in this workflow or in this developer flow just need a few hours maybe of maybe a",
    "start": "6049560",
    "end": "6055800"
  },
  {
    "text": "few minutes or maybe a few hours if you want to make some minor changes to your training script but that's all it takes",
    "start": "6055800",
    "end": "6061500"
  },
  {
    "text": "to push a model all right and with this kind of approach we're",
    "start": "6061500",
    "end": "6068699"
  },
  {
    "text": "juggling hundreds of petabytes of training data of course our machine learning applications we're running 1500",
    "start": "6068699",
    "end": "6076080"
  },
  {
    "text": "workflows every day spanning across 3 000 training jobs every single day right and ultimately",
    "start": "6076080",
    "end": "6082500"
  },
  {
    "text": "this is because for if if there is a template you can just go and try out new ideas and that",
    "start": "6082500",
    "end": "6089400"
  },
  {
    "text": "means we are improving machine learning developer velocity what that means is people can train more",
    "start": "6089400",
    "end": "6095280"
  },
  {
    "text": "models try out more ideas and then you can see more improvements and it's going to be good for the",
    "start": "6095280",
    "end": "6100739"
  },
  {
    "text": "business",
    "start": "6100739",
    "end": "6102920"
  },
  {
    "text": "so now let's talk about a different case this time",
    "start": "6106739",
    "end": "6111780"
  },
  {
    "text": "where I'm I'm a machine learning engineer and I want to try a new waiting algorithm",
    "start": "6111780",
    "end": "6117900"
  },
  {
    "text": "right it's it's an algorithm that nobody's tried before and I want to try it I have an intuition it's going to work",
    "start": "6117900",
    "end": "6123719"
  },
  {
    "text": "and this one this specific case takes a lot",
    "start": "6123719",
    "end": "6129000"
  },
  {
    "text": "longer and we're still operating with this whole templating engine and infrastructure and everything there let's see nobody has done it before so I",
    "start": "6129000",
    "end": "6137280"
  },
  {
    "text": "still gotta go write a job it's going to be a big data job and oftentimes I have",
    "start": "6137280",
    "end": "6142320"
  },
  {
    "text": "to write it in Scholars Park and Scala although I love it very much it is very",
    "start": "6142320",
    "end": "6147420"
  },
  {
    "text": "hard to learn and pick up over time right and people say oh you know can I just write Pi Sparks or you can't what",
    "start": "6147420",
    "end": "6154020"
  },
  {
    "text": "if you're going to be doing a lot of Road heavy operations you don't want to do it in Python you maybe want to do it in jvm",
    "start": "6154020",
    "end": "6160320"
  },
  {
    "text": "and then this although we hate to admit it it is one of the most effective forms",
    "start": "6160320",
    "end": "6165780"
  },
  {
    "text": "of development you just kick it off and kill it kick it off and kill it and until it works right it's just part of",
    "start": "6165780",
    "end": "6171060"
  },
  {
    "text": "uh development you've got to test it unit test integration test and you run it in testing environment run some data",
    "start": "6171060",
    "end": "6178320"
  },
  {
    "text": "validation and then not okay you're satisfied with the job you wrote You've got to go and integrate that with the",
    "start": "6178320",
    "end": "6184139"
  },
  {
    "text": "workflow template write some new airflow operator you know if else logic branch and you",
    "start": "6184139",
    "end": "6191219"
  },
  {
    "text": "got a tunnel through a new configuration parameter to the template and then you need tested integration testing and then",
    "start": "6191219",
    "end": "6197520"
  },
  {
    "text": "you land the code obviously it took a lot of time for us to get there you probably had a bunch of",
    "start": "6197520",
    "end": "6203940"
  },
  {
    "text": "meetings in between which further slowed you down all right then once this thing lands go through cicd now we can finally",
    "start": "6203940",
    "end": "6212000"
  },
  {
    "text": "go to the templating engine and kick off the workflow and test the idea that you",
    "start": "6212000",
    "end": "6217500"
  },
  {
    "text": "wanted to try right so something's not right here obviously there are different approaches",
    "start": "6217500",
    "end": "6223500"
  },
  {
    "text": "you can just manually glue things together you can write a bunch of C spark SQL queries and write some data",
    "start": "6223500",
    "end": "6229199"
  },
  {
    "text": "data test zero here data test one here and then juggle them manually maintain a",
    "start": "6229199",
    "end": "6234300"
  },
  {
    "text": "spreadsheet what did I do here what did I do there and you can actually lose reproducibility and iteration speed why",
    "start": "6234300",
    "end": "6242280"
  },
  {
    "text": "are we losing iteration speed because you're not end up just going to want to try one reward algorithm right you're",
    "start": "6242280",
    "end": "6247679"
  },
  {
    "text": "going to have an intuition that there's this sort of direction is going to work well and it's a space of algorithms",
    "start": "6247679",
    "end": "6253560"
  },
  {
    "text": "you're not just going to want to try one so you're actually losing both but actually a super popular approach is",
    "start": "6253560",
    "end": "6260520"
  },
  {
    "text": "to file a ticket to ml platform tell them what you want to do and you're going to revisit the idea of quarter",
    "start": "6260520",
    "end": "6266820"
  },
  {
    "text": "later or next half we're going to have more meetings and ml platform teams gonna have to reroute resources and",
    "start": "6266820",
    "end": "6273719"
  },
  {
    "text": "improving stuff in different parts of the stack so these are some Alternatives and",
    "start": "6273719",
    "end": "6279420"
  },
  {
    "text": "that's why ultimately we just have to go through all of these steps",
    "start": "6279420",
    "end": "6285199"
  },
  {
    "text": "and this is what we call the scale first and learn last problem right so what's",
    "start": "6286139",
    "end": "6292920"
  },
  {
    "text": "happening here it's just a classic challenge with templating whenever you template stuff machine learning especially is something",
    "start": "6292920",
    "end": "6300119"
  },
  {
    "text": "that changes super fast and templates inherently reflect our understanding of the past",
    "start": "6300119",
    "end": "6306300"
  },
  {
    "text": "sure there will be really intuitive Engineers who can foresee the future and",
    "start": "6306300",
    "end": "6311820"
  },
  {
    "text": "see okay I'm gonna I know I'm gonna try a bunch of these things so they can create some templates for it but ultimately it's a refactoring process",
    "start": "6311820",
    "end": "6319139"
  },
  {
    "text": "and refactoring is done on existing code",
    "start": "6319139",
    "end": "6323360"
  },
  {
    "text": "reducibility at the cost of developer velocity and as you saw in the two cases",
    "start": "6324960",
    "end": "6330420"
  },
  {
    "text": "there's over a hundred times difference in developer velocity forward doing different things but still it's a we",
    "start": "6330420",
    "end": "6338159"
  },
  {
    "text": "cannot argue that the engineers are moving 100 times lower right so we really want to solve this problem",
    "start": "6338159",
    "end": "6343980"
  },
  {
    "text": "as I said ml Engineers are more expensive than gpus right so and we want to look at where the bottlenecks were",
    "start": "6343980",
    "end": "6350520"
  },
  {
    "text": "this engineer was juggling through so many different languages airflow in Python and you go with spark and you",
    "start": "6350520",
    "end": "6357719"
  },
  {
    "text": "learn Scala maybe you want to you know leave it with some spark sequel stuff here and there and then you also have to",
    "start": "6357719",
    "end": "6363420"
  },
  {
    "text": "learn a lot of runtimes and Frameworks airflow spark pie torch and the in-house",
    "start": "6363420",
    "end": "6369659"
  },
  {
    "text": "libraries tend to be the worst out of these and you go through code reviews wrestle",
    "start": "6369659",
    "end": "6376320"
  },
  {
    "text": "with the build system meetings and most importantly development is not Interactive",
    "start": "6376320",
    "end": "6382739"
  },
  {
    "text": "kick off kill kickoff kill is that's how a lot of people develop and that we need",
    "start": "6382739",
    "end": "6388739"
  },
  {
    "text": "to fully Embrace somehow right and these are some of the bottlenecks that we identify",
    "start": "6388739",
    "end": "6393840"
  },
  {
    "text": "the reality then is that Renaissance ml Engineers who are so good at doing everything they're good plumbers they",
    "start": "6393840",
    "end": "6400440"
  },
  {
    "text": "know how to model they have intuition these people can move fast but most of us are not Renaissance CML",
    "start": "6400440",
    "end": "6407880"
  },
  {
    "text": "engineers and we are all going to be moving very slow and that's bad for the business",
    "start": "6407880",
    "end": "6414320"
  },
  {
    "text": "so then let's talk about how we are thinking about using relay and how we've",
    "start": "6414780",
    "end": "6420360"
  },
  {
    "text": "been how uh chasing the road of uh solving learn fast and skill fast",
    "start": "6420360",
    "end": "6427320"
  },
  {
    "text": "problem right we're trying to switch it so looked at the bottlenecks we convert",
    "start": "6427320",
    "end": "6433619"
  },
  {
    "text": "bottlenecks into requirements that's an easy way to set our roadmap so we want the developer interface to be",
    "start": "6433619",
    "end": "6439260"
  },
  {
    "text": "interactive I keep emphasizing running and fixing it is an inherent part of development we",
    "start": "6439260",
    "end": "6445260"
  },
  {
    "text": "have to embrace it we want less languages less Frameworks and we want shorter simpler pipelines right",
    "start": "6445260",
    "end": "6452340"
  },
  {
    "text": "so then now let's uh translate that to the user experience",
    "start": "6452340",
    "end": "6457800"
  },
  {
    "text": "North Star we want to shoot for is some email engineer can express data",
    "start": "6457800",
    "end": "6463199"
  },
  {
    "text": "processing training inference evaluation analysis all in one language and one",
    "start": "6463199",
    "end": "6468659"
  },
  {
    "text": "framework which is then testable interactively and productionizable as workflow",
    "start": "6468659",
    "end": "6475020"
  },
  {
    "text": "templates this is the North Star we set for ourselves and we were looking around",
    "start": "6475020",
    "end": "6480600"
  },
  {
    "text": "very hard and then uh and Terrain uh and the idea then is that using Rey",
    "start": "6480600",
    "end": "6488400"
  },
  {
    "text": "and I I want you to quickly remind yourselves of what uh what Eric was talking about earlier and some of the",
    "start": "6488400",
    "end": "6494820"
  },
  {
    "text": "diagrams right the idea is to express the ml office cycle life cycle is Ray pipelines",
    "start": "6494820",
    "end": "6503239"
  },
  {
    "text": "instead of airflow workflows right instead of airflow you want to do radius",
    "start": "6503239",
    "end": "6509040"
  },
  {
    "text": "and pipelines instead of writing spark jobs SQL jobs and Etc we want to write",
    "start": "6509040",
    "end": "6514440"
  },
  {
    "text": "great tasks and actors and um there's and that's not it and",
    "start": "6514440",
    "end": "6520080"
  },
  {
    "text": "that's not all of it there's a lot of really good stuff that comes out of it now Eric talked about stream processing",
    "start": "6520080",
    "end": "6526199"
  },
  {
    "text": "earlier it greatly improves developer velocity think about let's say we have three jobs so we are clever and we",
    "start": "6526199",
    "end": "6532139"
  },
  {
    "text": "merged it into two jobs we still have to process billions of rows hundreds of terabytes of data finish one job and",
    "start": "6532139",
    "end": "6539820"
  },
  {
    "text": "then move on to the next and then move on to the next and then finally our model is going to see if it likes it or",
    "start": "6539820",
    "end": "6545460"
  },
  {
    "text": "not right by converting into streaming going from that ideation to actually letting",
    "start": "6545460",
    "end": "6551760"
  },
  {
    "text": "your model taste a new form of data is super fast and that's exactly what we want to save developer time",
    "start": "6551760",
    "end": "6558480"
  },
  {
    "text": "also we uh talked Eric talked about being able to manage heterogeneous pool",
    "start": "6558480",
    "end": "6564239"
  },
  {
    "text": "of resources it's really important that when something doesn't work and it's dying out of memory we can just blindly",
    "start": "6564239",
    "end": "6571320"
  },
  {
    "text": "throw money at in a horizontally scale it because we don't want machine learning Engineers waiting around to figure stuff out they are more expensive",
    "start": "6571320",
    "end": "6577619"
  },
  {
    "text": "than gpus all right then we can also if everything is in the training everything can be",
    "start": "6577619",
    "end": "6583920"
  },
  {
    "text": "orchestrated interactively it means we can run hyper parameter tuning right there can there's a really big space of",
    "start": "6583920",
    "end": "6591000"
  },
  {
    "text": "down sampling algorithms out there really big space of reward algorithms and can we just come up with some",
    "start": "6591000",
    "end": "6597000"
  },
  {
    "text": "intuition identify a few and run a hyper Camera Tuning on them that that's exactly also going to be allow us to",
    "start": "6597000",
    "end": "6603420"
  },
  {
    "text": "help uh improve machine learning developer velocity and uh kick it off and kill it this can",
    "start": "6603420",
    "end": "6610860"
  },
  {
    "text": "also then be supported by connecting Jupiter so we tend to think about",
    "start": "6610860",
    "end": "6616500"
  },
  {
    "text": "Jupiter kernels running in some physical resources but what Ray allows us is that Jupiter just becomes an entry point",
    "start": "6616500",
    "end": "6623880"
  },
  {
    "text": "that can connect to a larger pool of resources and that's also some good stuff that Ray enables us to do",
    "start": "6623880",
    "end": "6633500"
  },
  {
    "text": "then we have to uh we want to challenge our idea right is it really realistic to",
    "start": "6633719",
    "end": "6638760"
  },
  {
    "text": "replace workflows with Ray probably not because workflows are not only about scheduling and just gluing",
    "start": "6638760",
    "end": "6645360"
  },
  {
    "text": "jobs together they allow us to Cache intermediate data sets",
    "start": "6645360",
    "end": "6650760"
  },
  {
    "text": "make them available for reuse you can reproduce things and we can retry things easily",
    "start": "6650760",
    "end": "6657540"
  },
  {
    "text": "ml Engineers will ask hey if I put it on Reddit as a pipeline and somebody has",
    "start": "6657540",
    "end": "6662639"
  },
  {
    "text": "done it again before done it before am I going to be doing the same stuff over and over again now if one step stage of the pipeline",
    "start": "6662639",
    "end": "6670139"
  },
  {
    "text": "fails do I have to start all over again or will the ray application require some",
    "start": "6670139",
    "end": "6675420"
  },
  {
    "text": "kind of a Super Key to tap into all forms of data sets out there these are all really important and valid questions",
    "start": "6675420",
    "end": "6683040"
  },
  {
    "text": "but most importantly as developers we have to think okay do I go and enrich Ray data as a pipeline with workflow",
    "start": "6683040",
    "end": "6689460"
  },
  {
    "text": "like functionalities and reinvent a bunch of Wheels right",
    "start": "6689460",
    "end": "6695420"
  },
  {
    "text": "so we're we've been thinking about this problem and uh we think that it is a solvable thing which is there is a way",
    "start": "6695639",
    "end": "6703139"
  },
  {
    "text": "to get the best of both worlds right and the idea is that",
    "start": "6703139",
    "end": "6710040"
  },
  {
    "text": "you know from Eric's presentation we looked at stage like tasks and operators and actors and whatnot right and once",
    "start": "6710040",
    "end": "6717480"
  },
  {
    "text": "the and these are individually defined reusable building blocks and if we actually uh and once we have a",
    "start": "6717480",
    "end": "6725400"
  },
  {
    "text": "bag of these golden blocks stitching them together or chain of them or splitting them apart is not that hard of",
    "start": "6725400",
    "end": "6731940"
  },
  {
    "text": "a problem right we can stitch them together and run it on a single array application as one streaming pipeline or",
    "start": "6731940",
    "end": "6738600"
  },
  {
    "text": "we can be a little bit more clever and say okay these ones I want to stick together into one-ray job those two I",
    "start": "6738600",
    "end": "6744060"
  },
  {
    "text": "want to stitch together this one I want to be in a in a separate job and then now then what that is is just going to",
    "start": "6744060",
    "end": "6750659"
  },
  {
    "text": "be a work training workflow template right either we run it on rate or we orchestrate using airflow",
    "start": "6750659",
    "end": "6757619"
  },
  {
    "text": "and it's it really works out nicely because uh we platform teams don't want",
    "start": "6757619",
    "end": "6762840"
  },
  {
    "text": "to do a lot of work we want to delegate to ml engineers and then we can maintain",
    "start": "6762840",
    "end": "6768060"
  },
  {
    "text": "the shared pool of sdks where ml Engineers are contributing to when somebody has done it it should be",
    "start": "6768060",
    "end": "6774119"
  },
  {
    "text": "discoverable by all the hundreds of ml Engineers out there",
    "start": "6774119",
    "end": "6779178"
  },
  {
    "text": "now the Second Challenge when we think about replacing batch pipelines is is it",
    "start": "6780239",
    "end": "6786480"
  },
  {
    "text": "really realistic to replace badge jobs with radio data set pipelines or stream execution and I want to point you to One",
    "start": "6786480",
    "end": "6793739"
  },
  {
    "text": "Step the first step of our inner loop it's a very relevant problem it's called",
    "start": "6793739",
    "end": "6800159"
  },
  {
    "text": "the feature enrichment or backfill problem they said it's very common one of the",
    "start": "6800159",
    "end": "6806460"
  },
  {
    "text": "most interacted knobs in ml is to try out new feature so first thing we're",
    "start": "6806460",
    "end": "6811679"
  },
  {
    "text": "going to try and it's such a fundamental part of machine learning development in a Pinterest where you train on months",
    "start": "6811679",
    "end": "6818280"
  },
  {
    "text": "and months of data from and that's logged over time just to try one feature you don't want",
    "start": "6818280",
    "end": "6824940"
  },
  {
    "text": "to wait around for 100 days until it logs and fills up your training data and now you can see if it works or not if it",
    "start": "6824940",
    "end": "6831480"
  },
  {
    "text": "works great if it doesn't work well how you're going to clean it up you wasted a",
    "start": "6831480",
    "end": "6836580"
  },
  {
    "text": "lot of time right in the future back filler enrichment is inherently a joint problem we have",
    "start": "6836580",
    "end": "6844080"
  },
  {
    "text": "logged training data set hundreds of terabytes coming in and we also have features from feature store which isn't",
    "start": "6844080",
    "end": "6851280"
  },
  {
    "text": "going to be as large as training data sets but it's still pretty large and then the question is okay can Ray do",
    "start": "6851280",
    "end": "6859500"
  },
  {
    "text": "this is it is it feasible for the ray streaming executor to do this kind of joins",
    "start": "6859500",
    "end": "6866600"
  },
  {
    "text": "and turns out that it is and the idea is to perform feature back",
    "start": "6866820",
    "end": "6873659"
  },
  {
    "text": "filling through storage partition join and this is the one that's commonly known as bucket joins",
    "start": "6873659",
    "end": "6879840"
  },
  {
    "text": "and uh we're going to be using iceberg to partition the data into a few",
    "start": "6879840",
    "end": "6885119"
  },
  {
    "text": "deterministic buckets right and you can think of this pre-bucketing",
    "start": "6885119",
    "end": "6890400"
  },
  {
    "text": "of data into as if we are leaking Shuffle stage all the way to the storage",
    "start": "6890400",
    "end": "6895440"
  },
  {
    "text": "layer so that we're pre-shuffling the data since we already know what the join key is going to be",
    "start": "6895440",
    "end": "6901560"
  },
  {
    "text": "and we can just we can skip Shuffle and perform streaming joins and we can benefit from the streaming executor that",
    "start": "6901560",
    "end": "6907619"
  },
  {
    "text": "they talked about from a very high level here's how it's going to work we're gonna write a custom",
    "start": "6907619",
    "end": "6914219"
  },
  {
    "text": "data source it's a nice abstraction in relate data it allows us to spawn whatever read tasks that we want to",
    "start": "6914219",
    "end": "6920880"
  },
  {
    "text": "respond and each read task will be assigned a bucket ID and training data",
    "start": "6920880",
    "end": "6927000"
  },
  {
    "text": "set and feature store are going to share the same bucketing scheme and then we can say that the re-task",
    "start": "6927000",
    "end": "6932580"
  },
  {
    "text": "isn't going to perform any form of shuffling where he's good at shuffling but we're not going to do it because we want everything to be in a stream",
    "start": "6932580",
    "end": "6937980"
  },
  {
    "text": "streaming fashion so re-task is going to go through the buckets load files that belong to the",
    "start": "6937980",
    "end": "6943440"
  },
  {
    "text": "same Bucket from left hand side and right hand side performing memory join yielded to the downstream of the",
    "start": "6943440",
    "end": "6949860"
  },
  {
    "text": "pipeline just gonna keep doing it right and this is how we are going to be",
    "start": "6949860",
    "end": "6954960"
  },
  {
    "text": "performing joints in Ray streaming there's no magic way to skip Shuffle if we don't start leaking stuff into the",
    "start": "6954960",
    "end": "6962159"
  },
  {
    "text": "storage layer what we are going to be making this happen",
    "start": "6962159",
    "end": "6967199"
  },
  {
    "text": "so with these two things in mind here's what the new user experience is going to",
    "start": "6967199",
    "end": "6972360"
  },
  {
    "text": "be for stuff that nobody has tried before let's go back to the new reward algorithm new waiting algorithm right",
    "start": "6972360",
    "end": "6979560"
  },
  {
    "text": "I'm going to connect as an ml engineer I'm going to connect to Jupiter and then connect to a rate cluster",
    "start": "6979560",
    "end": "6986340"
  },
  {
    "text": "I'm going to express whatever new algorithm is in the rate task connector right I'm gonna and interactively train",
    "start": "6986340",
    "end": "6992820"
  },
  {
    "text": "experimental models evaluate them and test my hypothesis as I said space of algorithms is massive",
    "start": "6992820",
    "end": "6999780"
  },
  {
    "text": "with intuition I should be able to interactively trim it down and and read away stuff that's not going to work",
    "start": "6999780",
    "end": "7006320"
  },
  {
    "text": "then I'm going to commit the code or this whatever new task connector that I created and the platform should be able",
    "start": "7006320",
    "end": "7012800"
  },
  {
    "text": "to figure out how this is going to be integrated Whirlpool template as I said once you have task connectors it's not",
    "start": "7012800",
    "end": "7019100"
  },
  {
    "text": "that hard to figure out if I'm going to compose one job out of it or multiple jobs out of it right",
    "start": "7019100",
    "end": "7025400"
  },
  {
    "text": "and then you can now then just go kick off workflows using the templates now this is the North Star user experience",
    "start": "7025400",
    "end": "7032119"
  },
  {
    "text": "we're aiming for and we hope that the velocity variance will be greatly",
    "start": "7032119",
    "end": "7037460"
  },
  {
    "text": "reduced from the 100x that we saw before so hopefully something that's a lot less than 10.",
    "start": "7037460",
    "end": "7044960"
  },
  {
    "text": "and all of this is from having one language interactive development and all the good stuff we talked about ready uh",
    "start": "7044960",
    "end": "7051980"
  },
  {
    "text": "earlier so our progress with adopting Ray and some future work uh we rolled it out to",
    "start": "7051980",
    "end": "7059960"
  },
  {
    "text": "one large-scale ranking use case this use case is doing a couple things",
    "start": "7059960",
    "end": "7065659"
  },
  {
    "text": "leveraging Radio Data set pipelines right so we're performing match side join",
    "start": "7065659",
    "end": "7070820"
  },
  {
    "text": "uh but this isn't about for for future backfilling just yet we're going to be working on it later but this is for",
    "start": "7070820",
    "end": "7077060"
  },
  {
    "text": "filtering out events that happened that were generated from Bad actors on our applications",
    "start": "7077060",
    "end": "7082699"
  },
  {
    "text": "we don't want the models to learn bad behaviors from them and we also uh were doing uh on demand",
    "start": "7082699",
    "end": "7089540"
  },
  {
    "text": "down sampling down sampling is a very good it's like building a curriculum to teach your model right you want to",
    "start": "7089540",
    "end": "7095780"
  },
  {
    "text": "emphasize certain things and Hive certain things away and these things should be doable on demand just by",
    "start": "7095780",
    "end": "7101599"
  },
  {
    "text": "writing some python code and it was it's been very helpful because yes doing more stuff in the",
    "start": "7101599",
    "end": "7107719"
  },
  {
    "text": "trainer will slow down the input that's just how it's going to work but once we start observing slowdowns and training",
    "start": "7107719",
    "end": "7114380"
  },
  {
    "text": "through but we can add one machine a few more CPUs in a horizontally scale the",
    "start": "7114380",
    "end": "7119719"
  },
  {
    "text": "data processing portion of our training pipelines right then our plan is to expand to five or more use cases in H2",
    "start": "7119719",
    "end": "7128900"
  },
  {
    "text": "future work exciting stuff that I talked about earlier all Fallen here right I",
    "start": "7128900",
    "end": "7134540"
  },
  {
    "text": "think we've proved that it's going to work and we're going to aggressively chase this down on demand feature",
    "start": "7134540",
    "end": "7140060"
  },
  {
    "text": "backfill we can actually perform a variety of applications that might require joins some Downstream reward",
    "start": "7140060",
    "end": "7146599"
  },
  {
    "text": "calculation user sequence experiments and then we also will be investing in",
    "start": "7146599",
    "end": "7151820"
  },
  {
    "text": "the data set pipeline to workflow translation they are homomorphic",
    "start": "7151820",
    "end": "7158139"
  },
  {
    "text": "so summarize really allows us to solve the skill first learn Last Problem flip",
    "start": "7158420",
    "end": "7165560"
  },
  {
    "text": "it to learn fast and scale fast we can express most of the big data",
    "start": "7165560",
    "end": "7170960"
  },
  {
    "text": "processing in Ray and connect them as Ray streaming Pipelines",
    "start": "7170960",
    "end": "7176300"
  },
  {
    "text": "the good stuff I talked about and then ml Engineers will be given one tool they can focus on writing tasks",
    "start": "7176300",
    "end": "7183139"
  },
  {
    "text": "actors and uh yep so this is a summary of",
    "start": "7183139",
    "end": "7189619"
  },
  {
    "text": "radio Pinterest and now I just wanted to end it with some some suggestions and ask for the community there's a lot of",
    "start": "7189619",
    "end": "7196159"
  },
  {
    "text": "creative ideas at Pinterest that's going around but I just picked a few uh our Engineers will be giving a talk at the",
    "start": "7196159",
    "end": "7202760"
  },
  {
    "text": "race Summit later this year I'm sure by then we'll have even more good suggestions right",
    "start": "7202760",
    "end": "7209659"
  },
  {
    "text": "so yep all the good stuff too many of them",
    "start": "7209659",
    "end": "7215420"
  },
  {
    "text": "but uh here's some very high level suggestions that could make the data set pipelines even more powerful",
    "start": "7215420",
    "end": "7224020"
  },
  {
    "text": "uh first of all might be a little bit more contentious uh but we believe that",
    "start": "7224179",
    "end": "7230719"
  },
  {
    "text": "in order for us to leverage rate data set pipelines more for what was",
    "start": "7230719",
    "end": "7236480"
  },
  {
    "text": "traditionally ETL workload we really need to invest in data frame-centric apis",
    "start": "7236480",
    "end": "7242659"
  },
  {
    "text": "and we want to hide fully hide if possible Right Concepts like blocks and tasks",
    "start": "7242659",
    "end": "7249260"
  },
  {
    "text": "away from the end users because once you start working with data frames you really shouldn't be or thinking about",
    "start": "7249260",
    "end": "7255739"
  },
  {
    "text": "this and we want it to be as declarative as possible Right and can we have some kind of distributed",
    "start": "7255739",
    "end": "7262460"
  },
  {
    "text": "data frame abstraction I've been starting to see this in some other open source project recently I",
    "start": "7262460",
    "end": "7269239"
  },
  {
    "text": "came across this project called Daft right they try to have this distributed data frame abstraction ready DPS another",
    "start": "7269239",
    "end": "7276739"
  },
  {
    "text": "open source project I was looking at which tries to layer Pi spark apis and run it on on Ray but uh",
    "start": "7276739",
    "end": "7286159"
  },
  {
    "text": "we'd love to see something mature something that's endorsed by any scale before we adopt something",
    "start": "7286159",
    "end": "7292880"
  },
  {
    "text": "and also I talked about iceberg and storage partition join yes we're going to prove it we've proved that it's it",
    "start": "7292880",
    "end": "7299179"
  },
  {
    "text": "works and we're going to scale it out but what if we was formally uh supported",
    "start": "7299179",
    "end": "7304280"
  },
  {
    "text": "API in the the array data stack",
    "start": "7304280",
    "end": "7310460"
  },
  {
    "text": "okay who also points on user experience right once we start looking into using",
    "start": "7310460",
    "end": "7317000"
  },
  {
    "text": "Ray for more pipelining there's got to be some dedicated UI for pipelining right Eric showed you some of the like",
    "start": "7317000",
    "end": "7325400"
  },
  {
    "text": "the number of tests and how they scale and and work over time what you know I",
    "start": "7325400",
    "end": "7331159"
  },
  {
    "text": "hate to bring in spark here but it shows there's steps and what's Happening Here how much did it process",
    "start": "7331159",
    "end": "7338020"
  },
  {
    "text": "and what the throughputs were now can if once we start visualizing this can we automatically detect bottlenecks and",
    "start": "7338020",
    "end": "7344540"
  },
  {
    "text": "flag it there can there be some kind of like a doctor elephant like interface right or experience and I I",
    "start": "7344540",
    "end": "7353000"
  },
  {
    "text": "hope this one's simple but I think this is actually going to be a killer feature can we just inspect some samples on",
    "start": "7353000",
    "end": "7360500"
  },
  {
    "text": "demand and see if my pipeline is doing the correct thing and these things would be would make the",
    "start": "7360500",
    "end": "7366199"
  },
  {
    "text": "developer experience a lot better another thing from my team is can we have replayable UI logs yes we're biased",
    "start": "7366199",
    "end": "7373460"
  },
  {
    "text": "by spark but the spark you can have the spark history you can spin up the UI server later and replay it and see what",
    "start": "7373460",
    "end": "7380420"
  },
  {
    "text": "happened over time maybe if we had that something similar for Rey that would",
    "start": "7380420",
    "end": "7385699"
  },
  {
    "text": "make debugging a lot easier there can be a lot of Creative Solutions here when we",
    "start": "7385699",
    "end": "7390800"
  },
  {
    "text": "think about how to help ml Engineers save their time in the",
    "start": "7390800",
    "end": "7395900"
  },
  {
    "text": "debugging space especially but yeah so that's some of the suggestions or asks we have so that's it",
    "start": "7395900",
    "end": "7404119"
  },
  {
    "text": "thanks so much and we can open up for our q a thank you very much for the very",
    "start": "7404119",
    "end": "7409159"
  },
  {
    "text": "well thought out pipeline stages of the problems what are",
    "start": "7409159",
    "end": "7414619"
  },
  {
    "text": "we trying to do how Ray can actually fit in and so very valuable insights in what are some of the things that we can",
    "start": "7414619",
    "end": "7420320"
  },
  {
    "text": "actually improve to make sure that it addresses some of the questions they actually raised for the ray community",
    "start": "7420320",
    "end": "7426679"
  },
  {
    "text": "and I'm sure our developers over here including myself so we're going to make sure that we pay eight to it as we as we",
    "start": "7426679",
    "end": "7434420"
  },
  {
    "text": "constantly talk to you on daily or weekly basis so I'll open up the floor if you have any questions for um",
    "start": "7434420",
    "end": "7442540"
  },
  {
    "text": "beautiful things to do with how many use Pinterest every day I and I do I mean I go for design and stuff but",
    "start": "7442580",
    "end": "7449840"
  },
  {
    "text": "any questions for uh C1 yeah do you want to catch this",
    "start": "7449840",
    "end": "7458380"
  },
  {
    "text": "so I'm curious uh I don't know I just I want to know what implementation of",
    "start": "7458480",
    "end": "7464000"
  },
  {
    "text": "everything like of you know the experiments you run of the jobs you've run I just want to know",
    "start": "7464000",
    "end": "7470659"
  },
  {
    "text": "what's the eventual user uh and state that is beneficial",
    "start": "7470659",
    "end": "7477280"
  },
  {
    "text": "um let's see",
    "start": "7477980",
    "end": "7483860"
  },
  {
    "text": "yep the end state is something like this where I Envision a bag of tasks in our",
    "start": "7483860",
    "end": "7492320"
  },
  {
    "text": "code base where there's uh the apis for the tests are automatically generated",
    "start": "7492320",
    "end": "7498980"
  },
  {
    "text": "but and the gluing of them is treated as a pure infrastructure problem and I can declare the sequence of tasks",
    "start": "7498980",
    "end": "7506119"
  },
  {
    "text": "to get something done um is that what you're looking for okay so you're just processing images right",
    "start": "7506119",
    "end": "7512360"
  },
  {
    "text": "oh no actually um oh yeah maybe I could go a little bit deeper into the variety of tasks",
    "start": "7512360",
    "end": "7518960"
  },
  {
    "text": "um so that this kind of pipeline view that I showed here isn't is primarily",
    "start": "7518960",
    "end": "7525560"
  },
  {
    "text": "with tabular data so these are actions that users took on",
    "start": "7525560",
    "end": "7531980"
  },
  {
    "text": "Pinterest that help that gives us valuable signals on which contents are better than the other ones",
    "start": "7531980",
    "end": "7539300"
  },
  {
    "text": "so they are tabular data and let me give a little bit a very high level problem formulation",
    "start": "7539300",
    "end": "7544940"
  },
  {
    "text": "every ml application is going to be a little bit different but the problem for formulation is going to be given a user",
    "start": "7544940",
    "end": "7551719"
  },
  {
    "text": "U with a bunch of features when I show this pin with a bunch of",
    "start": "7551719",
    "end": "7557659"
  },
  {
    "text": "features what is the likelihood of Engagement and you're gonna be you're going to have",
    "start": "7557659",
    "end": "7564679"
  },
  {
    "text": "a bunch of user features you're going to have a bunch of content features and then in a supervised training setup you",
    "start": "7564679",
    "end": "7570679"
  },
  {
    "text": "will have did this person click this or not hide it like it save it and those are some of",
    "start": "7570679",
    "end": "7577639"
  },
  {
    "text": "and that's what I meant by supervised learning training uh data set earlier",
    "start": "7577639",
    "end": "7585158"
  },
  {
    "text": "we do have a lot of image understanding um ml applications as well they're not",
    "start": "7587000",
    "end": "7593900"
  },
  {
    "text": "going to quite fit well with this uh sample structure that I shared here",
    "start": "7593900",
    "end": "7599900"
  },
  {
    "text": "today you can consider that I'm primarily talking about tabular data which are logs of user activities on",
    "start": "7599900",
    "end": "7607340"
  },
  {
    "text": "Pinterest C1 can you go back to the SDK slide",
    "start": "7607340",
    "end": "7614020"
  },
  {
    "text": "so this is the ml platform SKS x that you were actually developing in-house correct",
    "start": "7620739",
    "end": "7626659"
  },
  {
    "text": "oh yeah so yes we're not here yet no I know this is the normal that's that's that's that's that's your North Star and",
    "start": "7626659",
    "end": "7633199"
  },
  {
    "text": "I think when you mentioned about that that you wanted Ray data set to be more apis to be more declarative retelling it",
    "start": "7633199",
    "end": "7640040"
  },
  {
    "text": "what needed to do I'm assuming that what you're what you're implying is that these political",
    "start": "7640040",
    "end": "7645260"
  },
  {
    "text": "sdks will be declarative in terms of here's a sample task and that will be",
    "start": "7645260",
    "end": "7650420"
  },
  {
    "text": "converted in this equivalent array map batches task which will go ahead and",
    "start": "7650420",
    "end": "7657260"
  },
  {
    "text": "either do the stages so you don't have to worry about the stages and all that really should take care of it but the",
    "start": "7657260",
    "end": "7662659"
  },
  {
    "text": "high level this is where you actually want the declared apis to be translated into range stages and and tasks you",
    "start": "7662659",
    "end": "7669139"
  },
  {
    "text": "don't have to worry about it or email engineer who is more expensive than GPU doesn't it worry about that yeah so yeah",
    "start": "7669139",
    "end": "7675500"
  },
  {
    "text": "if you crack open each individual task or actual implementation it's going to be",
    "start": "7675500",
    "end": "7681080"
  },
  {
    "text": "imperative but the way users ask us to glue these things together can be declared",
    "start": "7681080",
    "end": "7688460"
  },
  {
    "text": "towards the reaction when internally when you say well you don't we don't have to use Scala you don't need to to",
    "start": "7688460",
    "end": "7694040"
  },
  {
    "text": "use um SQL you don't have to use any land all these languages all you're going to do is in Python with the related or they",
    "start": "7694040",
    "end": "7701599"
  },
  {
    "text": "are saying oh my God why should I just invest only in one language this is a great question and nobody disagrees that",
    "start": "7701599",
    "end": "7709460"
  },
  {
    "text": "when you come to them say hey we're going to help you only work with python nobody's going to see don't do that",
    "start": "7709460",
    "end": "7714920"
  },
  {
    "text": "right there will be people who are saying but what are we trading off and",
    "start": "7714920",
    "end": "7720020"
  },
  {
    "text": "that's the more important question we will be trading things off right",
    "start": "7720020",
    "end": "7725360"
  },
  {
    "text": "so and that's why I was asking for more of a distributed data frame API because",
    "start": "7725360",
    "end": "7730960"
  },
  {
    "text": "when people author tasks and actors they are exposed to system level",
    "start": "7730960",
    "end": "7737060"
  },
  {
    "text": "understanding so we are let's say today somehow we wanted",
    "start": "7737060",
    "end": "7743119"
  },
  {
    "text": "to make a magical transition to Pi spark to let's say this kind of World We are giving users a lot of knobs right and",
    "start": "7743119",
    "end": "7751040"
  },
  {
    "text": "that is a trade-off that we would be making today so we're waiting to see how",
    "start": "7751040",
    "end": "7756679"
  },
  {
    "text": "Ray evolves how Ray data evolves it's it's a really exciting space any questions",
    "start": "7756679",
    "end": "7763719"
  },
  {
    "text": "and three-dimensional data sorry uh",
    "start": "7767080",
    "end": "7772340"
  },
  {
    "text": "gltf trial formats yet like are you guys envisioning interest world where",
    "start": "7772340",
    "end": "7777820"
  },
  {
    "text": "three-dimensional assets are a new Marketplace or a new inspiration engine",
    "start": "7777820",
    "end": "7783820"
  },
  {
    "text": "um that I am not I don't think I can comment um",
    "start": "7784340",
    "end": "7789679"
  },
  {
    "text": "mostly because I don't know I'm sure there's a lot of creative stuff going on uh in the ml Engineers brains anybody",
    "start": "7789679",
    "end": "7797780"
  },
  {
    "text": "wants to take a large question going once oh there you go",
    "start": "7797780",
    "end": "7805420"
  },
  {
    "text": "so if I understood correctly you're estimating a 10x productivity increase",
    "start": "7805940",
    "end": "7813340"
  },
  {
    "text": "yep that's okay so if you have you thought about taking",
    "start": "7813340",
    "end": "7819560"
  },
  {
    "text": "that productivity increase by the number of engineers in the cost of Engineers so that when the executives come and say",
    "start": "7819560",
    "end": "7825500"
  },
  {
    "text": "what are you guys doing the CFO says what are you doing with all this money that I give to you you can say well I",
    "start": "7825500",
    "end": "7831199"
  },
  {
    "text": "just saved you x amount of money it's a thought oh yeah definitely it's",
    "start": "7831199",
    "end": "7836960"
  },
  {
    "text": "going on my performance review patch we'll give a big hand to someone from Pinterest thanks a lot for sharing us",
    "start": "7836960",
    "end": "7845300"
  },
  {
    "text": "your initial journeyman Ray and thanks a lot for showing us giving us food back and and sharing with the community as",
    "start": "7845300",
    "end": "7851239"
  },
  {
    "text": "well I want to thank a Moog and Eric for giving us an insightful to talk about Ray data and my friends Eric and",
    "start": "7851239",
    "end": "7858440"
  },
  {
    "text": "um Diana she's they're all gone and thank you all for coming and tomorrow we have another meet up so if you like oh",
    "start": "7858440",
    "end": "7864800"
  },
  {
    "text": "HQ headquarters business come by tomorrow to find out about how you can actually scale and put llm applications",
    "start": "7864800",
    "end": "7871760"
  },
  {
    "text": "in production we have a CEO and a co-creator of Ray is going to be here",
    "start": "7871760",
    "end": "7877159"
  },
  {
    "text": "um the creator of LinkedIn is going to be here the creator of vikuna from Berkeley is going to be here so you're going to be able to exciting panel and",
    "start": "7877159",
    "end": "7884420"
  },
  {
    "text": "quick lightning talks to find out how it fits into this gun chemo scheme of",
    "start": "7884420",
    "end": "7889520"
  },
  {
    "text": "things a bit called llm and foundational models so hope to see you tomorrow and thanks a lot and safe drive home again",
    "start": "7889520",
    "end": "7895400"
  },
  {
    "text": "cheers",
    "start": "7895400",
    "end": "7897580"
  }
]