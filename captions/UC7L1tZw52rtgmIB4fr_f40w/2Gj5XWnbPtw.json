[
  {
    "text": "yeah okay thank you so um thanks",
    "start": "4440",
    "end": "7160"
  },
  {
    "text": "everyone uh so I'll give a talk on uh",
    "start": "7160",
    "end": "9960"
  },
  {
    "text": "how we used Ray for our llm based",
    "start": "9960",
    "end": "13240"
  },
  {
    "text": "recommender system at Jo and uh I'm",
    "start": "13240",
    "end": "16720"
  },
  {
    "text": "denish uh I work for vers Innovation",
    "start": "16720",
    "end": "20000"
  },
  {
    "text": "which is an Indian startup uh in the",
    "start": "20000",
    "end": "22920"
  },
  {
    "text": "short video recommendation space and our",
    "start": "22920",
    "end": "25760"
  },
  {
    "text": "app is called Jos so I'll get started uh",
    "start": "25760",
    "end": "29279"
  },
  {
    "text": "so uh first let's go with introductions",
    "start": "29279",
    "end": "32439"
  },
  {
    "text": "so uh yeah Jos is one of India's leading",
    "start": "32439",
    "end": "35040"
  },
  {
    "text": "short video platforms and we have",
    "start": "35040",
    "end": "37480"
  },
  {
    "text": "content in English and 11 other local",
    "start": "37480",
    "end": "40920"
  },
  {
    "text": "languages um so the app looks somewhat",
    "start": "40920",
    "end": "44079"
  },
  {
    "text": "like uh the figure on the right",
    "start": "44079",
    "end": "46120"
  },
  {
    "text": "basically it's it's very similar to Tik",
    "start": "46120",
    "end": "47840"
  },
  {
    "text": "Tok or uh insta res or YouTube shots and",
    "start": "47840",
    "end": "52160"
  },
  {
    "text": "you basically just swipe and uh watch",
    "start": "52160",
    "end": "54559"
  },
  {
    "text": "recomend recommendations that are served",
    "start": "54559",
    "end": "57680"
  },
  {
    "text": "to you and uh obvious obviously the",
    "start": "57680",
    "end": "60320"
  },
  {
    "text": "recommendation stack uh Powers the core",
    "start": "60320",
    "end": "63120"
  },
  {
    "text": "product experience and is very important",
    "start": "63120",
    "end": "66000"
  },
  {
    "text": "and uh I'll go into more details about",
    "start": "66000",
    "end": "68920"
  },
  {
    "text": "the rexus stack at uh",
    "start": "68920",
    "end": "72080"
  },
  {
    "text": "J so yeah so uh we'll first talk about",
    "start": "72080",
    "end": "75400"
  },
  {
    "text": "the ml training infrastructure J uh it's",
    "start": "75400",
    "end": "78880"
  },
  {
    "text": "basically uh we have uh this sort of",
    "start": "78880",
    "end": "82320"
  },
  {
    "text": "like uh yaml based configurators for",
    "start": "82320",
    "end": "85079"
  },
  {
    "text": "very common stuff that you would use in",
    "start": "85079",
    "end": "87320"
  },
  {
    "text": "the training workflow so we use antic",
    "start": "87320",
    "end": "90400"
  },
  {
    "text": "and Hydra for our",
    "start": "90400",
    "end": "92600"
  },
  {
    "text": "configurations and uh basically lot of",
    "start": "92600",
    "end": "96079"
  },
  {
    "text": "the uh components in your training",
    "start": "96079",
    "end": "98119"
  },
  {
    "text": "workflow like the trainer data",
    "start": "98119",
    "end": "100920"
  },
  {
    "text": "loader um uh maybe",
    "start": "100920",
    "end": "104040"
  },
  {
    "text": "evaluation uh uh uh configurations Etc",
    "start": "104040",
    "end": "107799"
  },
  {
    "text": "like exporting data loaders like all of",
    "start": "107799",
    "end": "110520"
  },
  {
    "text": "this uh would probably be shared among",
    "start": "110520",
    "end": "113079"
  },
  {
    "text": "all models so we have uh like uh ways to",
    "start": "113079",
    "end": "116360"
  },
  {
    "text": "populate the defaults for them and uh",
    "start": "116360",
    "end": "120159"
  },
  {
    "text": "the model part alone uh we can actually",
    "start": "120159",
    "end": "122399"
  },
  {
    "text": "specify uh we can just point to code",
    "start": "122399",
    "end": "124840"
  },
  {
    "text": "basically pych code and so we can we",
    "start": "124840",
    "end": "127600"
  },
  {
    "text": "have uh uh flexibility to support all",
    "start": "127600",
    "end": "129959"
  },
  {
    "text": "kinds of models as I'll talk about",
    "start": "129959",
    "end": "132239"
  },
  {
    "text": "later and uh we also have support for uh",
    "start": "132239",
    "end": "135400"
  },
  {
    "text": "a bunch of different kinds of features",
    "start": "135400",
    "end": "137680"
  },
  {
    "text": "to again support all of these models uh",
    "start": "137680",
    "end": "140400"
  },
  {
    "text": "like things like continuous categorical",
    "start": "140400",
    "end": "142640"
  },
  {
    "text": "features uh tensor features uh for like",
    "start": "142640",
    "end": "146080"
  },
  {
    "text": "embeddings and uh basically history",
    "start": "146080",
    "end": "148800"
  },
  {
    "text": "features which are basically some",
    "start": "148800",
    "end": "149879"
  },
  {
    "text": "special class of tensor features with",
    "start": "149879",
    "end": "152120"
  },
  {
    "text": "sort of like un uh uh sort of like",
    "start": "152120",
    "end": "155120"
  },
  {
    "text": "ragged kind of features so we support",
    "start": "155120",
    "end": "157400"
  },
  {
    "text": "all of this and uh all of this can be",
    "start": "157400",
    "end": "160120"
  },
  {
    "text": "configured via yl so that we can like uh",
    "start": "160120",
    "end": "162760"
  },
  {
    "text": "Change Model uh architectures pretty",
    "start": "162760",
    "end": "165159"
  },
  {
    "text": "quickly and uh do hyper parameter",
    "start": "165159",
    "end": "167400"
  },
  {
    "text": "optimizations and all of this is powered",
    "start": "167400",
    "end": "169879"
  },
  {
    "text": "by Ray so this is the uh training",
    "start": "169879",
    "end": "173239"
  },
  {
    "text": "infrastructure that we have set up um",
    "start": "173239",
    "end": "177000"
  },
  {
    "text": "and yeah so if you can uh look at the",
    "start": "177000",
    "end": "179319"
  },
  {
    "text": "infra",
    "start": "179319",
    "end": "180360"
  },
  {
    "text": "basically uh everything uses uh Ray uh",
    "start": "180360",
    "end": "183560"
  },
  {
    "text": "at least Ray core like uh uh like um so",
    "start": "183560",
    "end": "187440"
  },
  {
    "text": "this is the basic workflow that we have",
    "start": "187440",
    "end": "189280"
  },
  {
    "text": "for offline jobs uh online jobs",
    "start": "189280",
    "end": "192080"
  },
  {
    "text": "basically the same thing except we don't",
    "start": "192080",
    "end": "193560"
  },
  {
    "text": "have the batch predict stage uh for like",
    "start": "193560",
    "end": "195760"
  },
  {
    "text": "online models and uh yeah we use Ray DP",
    "start": "195760",
    "end": "198879"
  },
  {
    "text": "for data prep uh Ray core or and uh py",
    "start": "198879",
    "end": "203319"
  },
  {
    "text": "torch multiprocessing uh or day data we",
    "start": "203319",
    "end": "206360"
  },
  {
    "text": "have both options and then we use Ray",
    "start": "206360",
    "end": "208680"
  },
  {
    "text": "train for our trainer",
    "start": "208680",
    "end": "210519"
  },
  {
    "text": "and for batch predict we basically just",
    "start": "210519",
    "end": "212280"
  },
  {
    "text": "use rore as is and for cluster",
    "start": "212280",
    "end": "214720"
  },
  {
    "text": "management we've been using kubet so",
    "start": "214720",
    "end": "217120"
  },
  {
    "text": "this is the overall picture that for our",
    "start": "217120",
    "end": "219959"
  },
  {
    "text": "training",
    "start": "219959",
    "end": "221000"
  },
  {
    "text": "infrastructure and uh yeah so basically",
    "start": "221000",
    "end": "224080"
  },
  {
    "text": "just what I said uh we use uh lot of the",
    "start": "224080",
    "end": "228599"
  },
  {
    "text": "components from aray for uh setting up",
    "start": "228599",
    "end": "230879"
  },
  {
    "text": "our infrastructure for training uh jobs",
    "start": "230879",
    "end": "234439"
  },
  {
    "text": "and uh for inference we use nvdia",
    "start": "234439",
    "end": "237760"
  },
  {
    "text": "Tron so uh yeah this is basically the uh",
    "start": "237760",
    "end": "241280"
  },
  {
    "text": "setup of our uh",
    "start": "241280",
    "end": "244680"
  },
  {
    "text": "infrastructure uh so uh yeah the kind of",
    "start": "244680",
    "end": "247480"
  },
  {
    "text": "models that we support are uh basically",
    "start": "247480",
    "end": "250439"
  },
  {
    "text": "uh like more regular models like",
    "start": "250439",
    "end": "252400"
  },
  {
    "text": "classification regression point-wise",
    "start": "252400",
    "end": "254319"
  },
  {
    "text": "models uh retrieval models like for uh",
    "start": "254319",
    "end": "257400"
  },
  {
    "text": "recommendation systems they are kind of",
    "start": "257400",
    "end": "259120"
  },
  {
    "text": "important for uh fetching the uh",
    "start": "259120",
    "end": "261840"
  },
  {
    "text": "relevant content uh uh before you pass",
    "start": "261840",
    "end": "264840"
  },
  {
    "text": "it into a more heavy ranker so we we we",
    "start": "264840",
    "end": "267919"
  },
  {
    "text": "where we are able to train retrieval",
    "start": "267919",
    "end": "269240"
  },
  {
    "text": "models",
    "start": "269240",
    "end": "270120"
  },
  {
    "text": "and we also train multitask models which",
    "start": "270120",
    "end": "272080"
  },
  {
    "text": "are basically like a combination of uh",
    "start": "272080",
    "end": "274880"
  },
  {
    "text": "ranker models and retriever models",
    "start": "274880",
    "end": "277160"
  },
  {
    "text": "because we don't want to sort of like uh",
    "start": "277160",
    "end": "278919"
  },
  {
    "text": "spend too much effort on maintaining uh",
    "start": "278919",
    "end": "281600"
  },
  {
    "text": "or keep like maintaining multiple models",
    "start": "281600",
    "end": "283520"
  },
  {
    "text": "and so we sort of like uh train both of",
    "start": "283520",
    "end": "286560"
  },
  {
    "text": "these models hand in hand and uh yeah we",
    "start": "286560",
    "end": "289479"
  },
  {
    "text": "have support for that we also have been",
    "start": "289479",
    "end": "292080"
  },
  {
    "text": "working on recently working on uh",
    "start": "292080",
    "end": "294639"
  },
  {
    "text": "blending or diversification models using",
    "start": "294639",
    "end": "297800"
  },
  {
    "text": "session level data and uh so this is",
    "start": "297800",
    "end": "301199"
  },
  {
    "text": "sort of like using uh basically uh uh",
    "start": "301199",
    "end": "305080"
  },
  {
    "text": "data which is grouped into sessions so",
    "start": "305080",
    "end": "308120"
  },
  {
    "text": "even this is supported by our training",
    "start": "308120",
    "end": "309680"
  },
  {
    "text": "infrastructure and similarly if we use a",
    "start": "309680",
    "end": "311680"
  },
  {
    "text": "different Group by Clause like basically",
    "start": "311680",
    "end": "313600"
  },
  {
    "text": "use user ID as our group group by Clause",
    "start": "313600",
    "end": "316520"
  },
  {
    "text": "we get like user level models which are",
    "start": "316520",
    "end": "318720"
  },
  {
    "text": "like llm like models because they sort",
    "start": "318720",
    "end": "320560"
  },
  {
    "text": "of like predict the next engagement of",
    "start": "320560",
    "end": "323120"
  },
  {
    "text": "the user uh and that'll be the focus of",
    "start": "323120",
    "end": "325880"
  },
  {
    "text": "this talk like how how did we go about",
    "start": "325880",
    "end": "328160"
  },
  {
    "text": "implementing that and uh what were the",
    "start": "328160",
    "end": "330280"
  },
  {
    "text": "learnings from uh uh the llm like models",
    "start": "330280",
    "end": "334680"
  },
  {
    "text": "for recommender systems and uh yeah so",
    "start": "334680",
    "end": "338039"
  },
  {
    "text": "all of this was implemented in using our",
    "start": "338039",
    "end": "340199"
  },
  {
    "text": "uh training framework which uh leverages",
    "start": "340199",
    "end": "342880"
  },
  {
    "text": "very heavily and uh yeah I'll talk about",
    "start": "342880",
    "end": "346000"
  },
  {
    "text": "the llm uh style models in more detail",
    "start": "346000",
    "end": "350199"
  },
  {
    "text": "uh for the rest of the",
    "start": "350199",
    "end": "351600"
  },
  {
    "text": "talk um yeah so uh maybe first I'll just",
    "start": "351600",
    "end": "355720"
  },
  {
    "text": "talk about like how uh uh recommended",
    "start": "355720",
    "end": "359639"
  },
  {
    "text": "systems are typically uh built and what",
    "start": "359639",
    "end": "362520"
  },
  {
    "text": "are the issues and why maybe use llm",
    "start": "362520",
    "end": "365280"
  },
  {
    "text": "like models for recommended systems and",
    "start": "365280",
    "end": "368599"
  },
  {
    "text": "yeah so basically uh recommended systems",
    "start": "368599",
    "end": "371479"
  },
  {
    "text": "are very heavy on feature engineering",
    "start": "371479",
    "end": "374880"
  },
  {
    "text": "you have many kind uh different feature",
    "start": "374880",
    "end": "376880"
  },
  {
    "text": "groups so what are feature groups",
    "start": "376880",
    "end": "378759"
  },
  {
    "text": "feature groups are sort of like uh",
    "start": "378759",
    "end": "380680"
  },
  {
    "text": "basically groups of features that you uh",
    "start": "380680",
    "end": "382880"
  },
  {
    "text": "typically uh uh construct with uh one uh",
    "start": "382880",
    "end": "386960"
  },
  {
    "text": "uh query right like or one uh in one",
    "start": "386960",
    "end": "390360"
  },
  {
    "text": "data uh ETL job so normally you group",
    "start": "390360",
    "end": "394080"
  },
  {
    "text": "them by user you group them by item you",
    "start": "394080",
    "end": "396599"
  },
  {
    "text": "group them by context and also do a lot",
    "start": "396599",
    "end": "398680"
  },
  {
    "text": "of crosses like okay this user ordered",
    "start": "398680",
    "end": "401440"
  },
  {
    "text": "from this or watched this video sort of",
    "start": "401440",
    "end": "404120"
  },
  {
    "text": "like those kind of uh crosses you would",
    "start": "404120",
    "end": "406960"
  },
  {
    "text": "do also you might do it for item and",
    "start": "406960",
    "end": "409919"
  },
  {
    "text": "context that's also a popular cross so",
    "start": "409919",
    "end": "412120"
  },
  {
    "text": "you have a bunch of these crosses and",
    "start": "412120",
    "end": "414919"
  },
  {
    "text": "you also do uh sort of like groupings",
    "start": "414919",
    "end": "417080"
  },
  {
    "text": "over different time windows so basically",
    "start": "417080",
    "end": "419120"
  },
  {
    "text": "you have a lot of features right so uh a",
    "start": "419120",
    "end": "421599"
  },
  {
    "text": "lot of them could be redundant because",
    "start": "421599",
    "end": "424160"
  },
  {
    "text": "uh people normally add one feature group",
    "start": "424160",
    "end": "426039"
  },
  {
    "text": "at a time because that will be sort of",
    "start": "426039",
    "end": "427400"
  },
  {
    "text": "like the uh uh uh uh that's what you uh",
    "start": "427400",
    "end": "432319"
  },
  {
    "text": "sort of like build in your uh ETL jobs",
    "start": "432319",
    "end": "435120"
  },
  {
    "text": "and you get one new feature group maybe",
    "start": "435120",
    "end": "437199"
  },
  {
    "text": "a lot of features in that feature group",
    "start": "437199",
    "end": "439080"
  },
  {
    "text": "are not relevant but still the feature",
    "start": "439080",
    "end": "441080"
  },
  {
    "text": "group as a whole uh sort of like",
    "start": "441080",
    "end": "442599"
  },
  {
    "text": "improves your performance so you don't",
    "start": "442599",
    "end": "444960"
  },
  {
    "text": "you just add the whole feature group or",
    "start": "444960",
    "end": "446680"
  },
  {
    "text": "maybe your some of your past feature",
    "start": "446680",
    "end": "448639"
  },
  {
    "text": "groups are kind of irrelevant given your",
    "start": "448639",
    "end": "451240"
  },
  {
    "text": "new uh feature group but you just don't",
    "start": "451240",
    "end": "454000"
  },
  {
    "text": "spend time on those ablation studies and",
    "start": "454000",
    "end": "456360"
  },
  {
    "text": "uh like prune out the excessive features",
    "start": "456360",
    "end": "459199"
  },
  {
    "text": "because uh yeah you're uh yeah you",
    "start": "459199",
    "end": "462000"
  },
  {
    "text": "basically may not have the uh time or uh",
    "start": "462000",
    "end": "465360"
  },
  {
    "text": "the resources to train so many models to",
    "start": "465360",
    "end": "467240"
  },
  {
    "text": "do those ablations so basically at the",
    "start": "467240",
    "end": "469560"
  },
  {
    "text": "end of the day you have too many",
    "start": "469560",
    "end": "471759"
  },
  {
    "text": "features right like even in uh like J",
    "start": "471759",
    "end": "474159"
  },
  {
    "text": "like we have like around 500 600",
    "start": "474159",
    "end": "476639"
  },
  {
    "text": "features and I'm sure like uh maybe 50%",
    "start": "476639",
    "end": "479879"
  },
  {
    "text": "of those features are uh not relevant",
    "start": "479879",
    "end": "482400"
  },
  {
    "text": "now given the new features that we've",
    "start": "482400",
    "end": "484240"
  },
  {
    "text": "added so yeah so these are some of the",
    "start": "484240",
    "end": "487280"
  },
  {
    "text": "issues with traditional feature",
    "start": "487280",
    "end": "488840"
  },
  {
    "text": "engineering and uh when I look at the",
    "start": "488840",
    "end": "491720"
  },
  {
    "text": "llm world what we see is basically uh",
    "start": "491720",
    "end": "494840"
  },
  {
    "text": "there's very little feature engineering",
    "start": "494840",
    "end": "496319"
  },
  {
    "text": "that's done um basically people just put",
    "start": "496319",
    "end": "500240"
  },
  {
    "text": "tokens in and take tokens out right like",
    "start": "500240",
    "end": "502039"
  },
  {
    "text": "that's the simple API and uh you use it",
    "start": "502039",
    "end": "504800"
  },
  {
    "text": "for training and when you use you use it",
    "start": "504800",
    "end": "507240"
  },
  {
    "text": "also for prediction so uh",
    "start": "507240",
    "end": "509759"
  },
  {
    "text": "how can we use this kind of simple uh uh",
    "start": "509759",
    "end": "514000"
  },
  {
    "text": "sort of like API uh for feature",
    "start": "514000",
    "end": "516320"
  },
  {
    "text": "engineering for uh rexus models so",
    "start": "516320",
    "end": "519518"
  },
  {
    "text": "that's what uh uh we looked at and uh",
    "start": "519519",
    "end": "523080"
  },
  {
    "text": "one of the things that uh sort of like",
    "start": "523080",
    "end": "525320"
  },
  {
    "text": "stood out was some prior work from uh",
    "start": "525320",
    "end": "528120"
  },
  {
    "text": "Pinterest which is uh uh sort of like",
    "start": "528120",
    "end": "531160"
  },
  {
    "text": "which inspired our work here with with",
    "start": "531160",
    "end": "534080"
  },
  {
    "text": "this rexus llm basically it was uh this",
    "start": "534080",
    "end": "537040"
  },
  {
    "text": "paper uh from last year called Pinner",
    "start": "537040",
    "end": "540079"
  },
  {
    "text": "forer uh and the basic idea is to",
    "start": "540079",
    "end": "542959"
  },
  {
    "text": "essentially take a long user sequence",
    "start": "542959",
    "end": "546200"
  },
  {
    "text": "which sort of like summarizes the",
    "start": "546200",
    "end": "547600"
  },
  {
    "text": "overall history of the user and uh uh",
    "start": "547600",
    "end": "551240"
  },
  {
    "text": "basically try to predict future actions",
    "start": "551240",
    "end": "553240"
  },
  {
    "text": "of the user so uh that was the uh idea",
    "start": "553240",
    "end": "556880"
  },
  {
    "text": "behind pin forer and we basically",
    "start": "556880",
    "end": "559760"
  },
  {
    "text": "thought like okay we could Implement",
    "start": "559760",
    "end": "561360"
  },
  {
    "text": "something similar in our framework and",
    "start": "561360",
    "end": "564399"
  },
  {
    "text": "yeah so uh that was uh one of the uh",
    "start": "564399",
    "end": "568200"
  },
  {
    "text": "motivations for this work so uh yeah",
    "start": "568200",
    "end": "571200"
  },
  {
    "text": "I'll talk about some of our learnings uh",
    "start": "571200",
    "end": "573519"
  },
  {
    "text": "maybe this is a little bit uh uh kind of",
    "start": "573519",
    "end": "576440"
  },
  {
    "text": "like uh different from pin forer so I",
    "start": "576440",
    "end": "578760"
  },
  {
    "text": "wanted to talk about some of the",
    "start": "578760",
    "end": "580200"
  },
  {
    "text": "learnings from this uh uh from this imp",
    "start": "580200",
    "end": "583320"
  },
  {
    "text": "when we implemented the pin forer like",
    "start": "583320",
    "end": "585440"
  },
  {
    "text": "model so yeah so this is the basically",
    "start": "585440",
    "end": "587839"
  },
  {
    "text": "the high level AR model architecture",
    "start": "587839",
    "end": "590200"
  },
  {
    "text": "basic idea is you just take a list of",
    "start": "590200",
    "end": "592880"
  },
  {
    "text": "you just take the sequence of user",
    "start": "592880",
    "end": "594399"
  },
  {
    "text": "actions from the past and you try to",
    "start": "594399",
    "end": "596760"
  },
  {
    "text": "predict future uh user action",
    "start": "596760",
    "end": "599680"
  },
  {
    "text": "the main difference from say uh llm",
    "start": "599680",
    "end": "602839"
  },
  {
    "text": "model is llm model only has one of those",
    "start": "602839",
    "end": "605279"
  },
  {
    "text": "heads so I have four heads there so",
    "start": "605279",
    "end": "607680"
  },
  {
    "text": "which are basically four predictions so",
    "start": "607680",
    "end": "610079"
  },
  {
    "text": "in this in a recommendation system you",
    "start": "610079",
    "end": "611800"
  },
  {
    "text": "want to be diverse so you want to",
    "start": "611800",
    "end": "613480"
  },
  {
    "text": "predict not just the next token but",
    "start": "613480",
    "end": "615480"
  },
  {
    "text": "maybe some future tokens also so we have",
    "start": "615480",
    "end": "618200"
  },
  {
    "text": "like delays like uh 21 41 81 which means",
    "start": "618200",
    "end": "622040"
  },
  {
    "text": "like 21 interactions into the future",
    "start": "622040",
    "end": "624160"
  },
  {
    "text": "what would the user watch uh and so",
    "start": "624160",
    "end": "627079"
  },
  {
    "text": "forth as opposed to just having the",
    "start": "627079",
    "end": "629279"
  },
  {
    "text": "delay equal to one which is basically",
    "start": "629279",
    "end": "631279"
  },
  {
    "text": "the next token right what is the next",
    "start": "631279",
    "end": "633399"
  },
  {
    "text": "video that the user will watch so that",
    "start": "633399",
    "end": "636000"
  },
  {
    "text": "that's the basic",
    "start": "636000",
    "end": "637519"
  },
  {
    "text": "architecture and uh another difference",
    "start": "637519",
    "end": "640240"
  },
  {
    "text": "is basically you don't have the um",
    "start": "640240",
    "end": "643560"
  },
  {
    "text": "language modeling head like in a",
    "start": "643560",
    "end": "646000"
  },
  {
    "text": "traditional sense because your",
    "start": "646000",
    "end": "647639"
  },
  {
    "text": "vocabulary size is unlimited here",
    "start": "647639",
    "end": "650320"
  },
  {
    "text": "because you have a huge Corpus of items",
    "start": "650320",
    "end": "653079"
  },
  {
    "text": "so you basically just take the item",
    "start": "653079",
    "end": "655720"
  },
  {
    "text": "embedding which is basically same as uh",
    "start": "655720",
    "end": "658639"
  },
  {
    "text": "like uh",
    "start": "658639",
    "end": "660040"
  },
  {
    "text": "what you would call as your input",
    "start": "660040",
    "end": "662560"
  },
  {
    "text": "embedding right this is like kind of",
    "start": "662560",
    "end": "663920"
  },
  {
    "text": "like weight tying in uh llm models so",
    "start": "663920",
    "end": "667120"
  },
  {
    "text": "You' use the same idea uh and sort of",
    "start": "667120",
    "end": "670160"
  },
  {
    "text": "like uh project compare the uh",
    "start": "670160",
    "end": "673079"
  },
  {
    "text": "predictions with the input weights",
    "start": "673079",
    "end": "675040"
  },
  {
    "text": "itself or a mapping of the input weights",
    "start": "675040",
    "end": "677880"
  },
  {
    "text": "so that was the that's another",
    "start": "677880",
    "end": "679760"
  },
  {
    "text": "difference from a traditional llm",
    "start": "679760",
    "end": "681800"
  },
  {
    "text": "otherwise this is very similar to a",
    "start": "681800",
    "end": "683200"
  },
  {
    "text": "traditional MLM the whatever I put in",
    "start": "683200",
    "end": "686040"
  },
  {
    "text": "the middle like causal Transformer",
    "start": "686040",
    "end": "687480"
  },
  {
    "text": "encoder or uh decod only model that's",
    "start": "687480",
    "end": "690360"
  },
  {
    "text": "what is used so yeah this is the high",
    "start": "690360",
    "end": "692720"
  },
  {
    "text": "level",
    "start": "692720",
    "end": "693680"
  },
  {
    "text": "architecture and uh some of the things",
    "start": "693680",
    "end": "695680"
  },
  {
    "text": "that are kind of like uh different from",
    "start": "695680",
    "end": "698240"
  },
  {
    "text": "L llms is like I said uh there are no",
    "start": "698240",
    "end": "701279"
  },
  {
    "text": "latent embeddings like no nn. embedding",
    "start": "701279",
    "end": "703600"
  },
  {
    "text": "modules because uh pre-training these",
    "start": "703600",
    "end": "706320"
  },
  {
    "text": "llms is very expensive so you don't want",
    "start": "706320",
    "end": "708120"
  },
  {
    "text": "to learn like uh ID embeddings uh uh as",
    "start": "708120",
    "end": "712600"
  },
  {
    "text": "part of the model because your IDs will",
    "start": "712600",
    "end": "714880"
  },
  {
    "text": "keep changing uh as you and you can't",
    "start": "714880",
    "end": "718200"
  },
  {
    "text": "like uh keep refining this model it's",
    "start": "718200",
    "end": "720320"
  },
  {
    "text": "just too expensive to do that so you",
    "start": "720320",
    "end": "722320"
  },
  {
    "text": "just want a fixed model so what you do",
    "start": "722320",
    "end": "725000"
  },
  {
    "text": "is you basically uh use content",
    "start": "725000",
    "end": "727519"
  },
  {
    "text": "embeddings so which are sort of like",
    "start": "727519",
    "end": "729440"
  },
  {
    "text": "maybe in our case it's sort of like uh",
    "start": "729440",
    "end": "731680"
  },
  {
    "text": "video embeddings uh uh or it could be",
    "start": "731680",
    "end": "734639"
  },
  {
    "text": "text embeddings whatever that uh your",
    "start": "734639",
    "end": "737079"
  },
  {
    "text": "content is for your rexis uh you would",
    "start": "737079",
    "end": "740120"
  },
  {
    "text": "use those embeddings we also noticed",
    "start": "740120",
    "end": "742279"
  },
  {
    "text": "that like uh like linear layers or uh",
    "start": "742279",
    "end": "745680"
  },
  {
    "text": "MLPs like what was uh presented in Piner",
    "start": "745680",
    "end": "749240"
  },
  {
    "text": "forer were not uh as good a",
    "start": "749240",
    "end": "752360"
  },
  {
    "text": "transformation uh as lsh mapping which",
    "start": "752360",
    "end": "755320"
  },
  {
    "text": "is something new that we came up with so",
    "start": "755320",
    "end": "757839"
  },
  {
    "text": "we were able to like convert some 32",
    "start": "757839",
    "end": "759800"
  },
  {
    "text": "dimensional embeddings into 512",
    "start": "759800",
    "end": "762199"
  },
  {
    "text": "dimensional Transformer input more",
    "start": "762199",
    "end": "764120"
  },
  {
    "text": "efficiently are more uh sort of like u",
    "start": "764120",
    "end": "767160"
  },
  {
    "text": "in a more performant way using",
    "start": "767160",
    "end": "769760"
  },
  {
    "text": "lsh uh inspired mappings and uh yeah the",
    "start": "769760",
    "end": "774040"
  },
  {
    "text": "basic idea is like learning functions",
    "start": "774040",
    "end": "776519"
  },
  {
    "text": "through MLPs is much harder than sort of",
    "start": "776519",
    "end": "778920"
  },
  {
    "text": "of like memorizing outcomes through MLPs",
    "start": "778920",
    "end": "782079"
  },
  {
    "text": "and yeah so that was another uh change",
    "start": "782079",
    "end": "784680"
  },
  {
    "text": "from pin forer so yeah uh that uh next",
    "start": "784680",
    "end": "789320"
  },
  {
    "text": "thing that I talk about is basically the",
    "start": "789320",
    "end": "790920"
  },
  {
    "text": "loss function that we used uh as I said",
    "start": "790920",
    "end": "793279"
  },
  {
    "text": "a little bit earlier the vocabulary of",
    "start": "793279",
    "end": "794800"
  },
  {
    "text": "items is very large because you normally",
    "start": "794800",
    "end": "797000"
  },
  {
    "text": "recommend a large number of uh items in",
    "start": "797000",
    "end": "799800"
  },
  {
    "text": "a recommended system much larger than",
    "start": "799800",
    "end": "801760"
  },
  {
    "text": "what you do in an llm so uh you cannot",
    "start": "801760",
    "end": "804920"
  },
  {
    "text": "use uh the regular causal llm loss",
    "start": "804920",
    "end": "808279"
  },
  {
    "text": "language model loss which sort of needs",
    "start": "808279",
    "end": "810959"
  },
  {
    "text": "uh all the tokens to be present in the",
    "start": "810959",
    "end": "813720"
  },
  {
    "text": "loss formulation like you would use a",
    "start": "813720",
    "end": "816000"
  },
  {
    "text": "linear layer that outputs basically as",
    "start": "816000",
    "end": "818279"
  },
  {
    "text": "many Logics as the number of tokens",
    "start": "818279",
    "end": "820959"
  },
  {
    "text": "there are so that's just not feasible so",
    "start": "820959",
    "end": "823240"
  },
  {
    "text": "what we do is we uh sort of like borrow",
    "start": "823240",
    "end": "825320"
  },
  {
    "text": "from retrieval uh kind of uh models",
    "start": "825320",
    "end": "829040"
  },
  {
    "text": "basically embedding models where they",
    "start": "829040",
    "end": "831560"
  },
  {
    "text": "use a concept that's called inbatch",
    "start": "831560",
    "end": "833600"
  },
  {
    "text": "negatives but we've sort of like adapted",
    "start": "833600",
    "end": "835839"
  },
  {
    "text": "it to a sequence model so essentially",
    "start": "835839",
    "end": "838240"
  },
  {
    "text": "the figure in in the bottom shows what",
    "start": "838240",
    "end": "839959"
  },
  {
    "text": "we do you just think of uh so the",
    "start": "839959",
    "end": "842320"
  },
  {
    "text": "horizontal axis is the batch axis and",
    "start": "842320",
    "end": "844920"
  },
  {
    "text": "the uh uh sorry horizontal axis is the",
    "start": "844920",
    "end": "847480"
  },
  {
    "text": "time axis and the vertical axis is the",
    "start": "847480",
    "end": "849120"
  },
  {
    "text": "batch axis so this is same as a regular",
    "start": "849120",
    "end": "851279"
  },
  {
    "text": "llm except that uh instead of using",
    "start": "851279",
    "end": "854160"
  },
  {
    "text": "tokens to represent uh each of those uh",
    "start": "854160",
    "end": "857360"
  },
  {
    "text": "blocks we use embeddings directly in the",
    "start": "857360",
    "end": "859880"
  },
  {
    "text": "input and in the output and just uh",
    "start": "859880",
    "end": "863040"
  },
  {
    "text": "compare the how similar the embeddings",
    "start": "863040",
    "end": "864880"
  },
  {
    "text": "are and uh use uh the same cross entropy",
    "start": "864880",
    "end": "868560"
  },
  {
    "text": "l",
    "start": "868560",
    "end": "869880"
  },
  {
    "text": "uh but instead of uh doing it on just",
    "start": "869880",
    "end": "872000"
  },
  {
    "text": "one row at a time we would sort of like",
    "start": "872000",
    "end": "874320"
  },
  {
    "text": "do it uh across rows as well so uh",
    "start": "874320",
    "end": "877959"
  },
  {
    "text": "basically uh for the query which is in",
    "start": "877959",
    "end": "880399"
  },
  {
    "text": "blue all the items to the right",
    "start": "880399",
    "end": "882519"
  },
  {
    "text": "basically the future items can be",
    "start": "882519",
    "end": "885120"
  },
  {
    "text": "positive uh candidate items and all the",
    "start": "885120",
    "end": "888480"
  },
  {
    "text": "items in red all of them uh not just one",
    "start": "888480",
    "end": "890839"
  },
  {
    "text": "all of them can be uh negative",
    "start": "890839",
    "end": "892880"
  },
  {
    "text": "candidates basically other",
    "start": "892880",
    "end": "894600"
  },
  {
    "text": "users uh uh videos that they have",
    "start": "894600",
    "end": "897519"
  },
  {
    "text": "engaged with so uh so this is basically",
    "start": "897519",
    "end": "900199"
  },
  {
    "text": "the loss function and we can also",
    "start": "900199",
    "end": "902279"
  },
  {
    "text": "predict like some outcomes that the",
    "start": "902279",
    "end": "904040"
  },
  {
    "text": "users uh did to those videos right like",
    "start": "904040",
    "end": "906759"
  },
  {
    "text": "whether they liked the video or they",
    "start": "906759",
    "end": "908079"
  },
  {
    "text": "shared the video and so forth uh also in",
    "start": "908079",
    "end": "910639"
  },
  {
    "text": "a causal manner using more uh simpler",
    "start": "910639",
    "end": "913279"
  },
  {
    "text": "loss functions but we didn't actually do",
    "start": "913279",
    "end": "915480"
  },
  {
    "text": "that yet so yeah this is basically the",
    "start": "915480",
    "end": "917800"
  },
  {
    "text": "high level uh overview of the uh loss",
    "start": "917800",
    "end": "920720"
  },
  {
    "text": "function of the model that was used for",
    "start": "920720",
    "end": "922240"
  },
  {
    "text": "this model and uh we were also able to",
    "start": "922240",
    "end": "925399"
  },
  {
    "text": "use Ray train to train these models uh",
    "start": "925399",
    "end": "928880"
  },
  {
    "text": "pretty we we were basically this this",
    "start": "928880",
    "end": "931480"
  },
  {
    "text": "didn't have a lot of I/O this was like a",
    "start": "931480",
    "end": "933240"
  },
  {
    "text": "very uh GPU heavy job U basically",
    "start": "933240",
    "end": "937959"
  },
  {
    "text": "because uh most of the work for uh the",
    "start": "937959",
    "end": "941079"
  },
  {
    "text": "preparing the data set was done through",
    "start": "941079",
    "end": "943240"
  },
  {
    "text": "a spark job and basically uh and these",
    "start": "943240",
    "end": "946600"
  },
  {
    "text": "models were pretty heavy so uh yeah it",
    "start": "946600",
    "end": "949440"
  },
  {
    "text": "was able to fully utilize the",
    "start": "949440",
    "end": "951279"
  },
  {
    "text": "GPU and in in our case basically uh we",
    "start": "951279",
    "end": "955000"
  },
  {
    "text": "didn't play around too much with the",
    "start": "955000",
    "end": "956160"
  },
  {
    "text": "batch size so uh because we just uh",
    "start": "956160",
    "end": "958959"
  },
  {
    "text": "normally just double the batch sizes and",
    "start": "958959",
    "end": "961000"
  },
  {
    "text": "uh this double of the current batch size",
    "start": "961000",
    "end": "963199"
  },
  {
    "text": "would have hit the GPU memory limit so",
    "start": "963199",
    "end": "964880"
  },
  {
    "text": "you'd see some Gap there but otherwise",
    "start": "964880",
    "end": "967319"
  },
  {
    "text": "like you can see that GPU utilization is",
    "start": "967319",
    "end": "969240"
  },
  {
    "text": "very high and this was just done on one",
    "start": "969240",
    "end": "972000"
  },
  {
    "text": "uh uh a100 instance with four",
    "start": "972000",
    "end": "974839"
  },
  {
    "text": "gpus and yeah so we were able to",
    "start": "974839",
    "end": "977279"
  },
  {
    "text": "saturate the uh GPU using our uh infra",
    "start": "977279",
    "end": "982399"
  },
  {
    "text": "yeah so now in terms of like Model",
    "start": "982399",
    "end": "984399"
  },
  {
    "text": "results like basically Pinner former",
    "start": "984399",
    "end": "986480"
  },
  {
    "text": "like model is the one in yellow and we",
    "start": "986480",
    "end": "989040"
  },
  {
    "text": "were able to do much better than that",
    "start": "989040",
    "end": "990720"
  },
  {
    "text": "using the lsh uh kind of mapping uh that",
    "start": "990720",
    "end": "994440"
  },
  {
    "text": "I was talking about so uh basically uh",
    "start": "994440",
    "end": "998319"
  },
  {
    "text": "it looks like even if you have like uh",
    "start": "998319",
    "end": "1001399"
  },
  {
    "text": "uh content embeddings as input to your",
    "start": "1001399",
    "end": "1003160"
  },
  {
    "text": "model uh in those cases uh it's better",
    "start": "1003160",
    "end": "1006040"
  },
  {
    "text": "to normally use uh an embedding layer as",
    "start": "1006040",
    "end": "1009079"
  },
  {
    "text": "the input rather than using a linear",
    "start": "1009079",
    "end": "1011399"
  },
  {
    "text": "layer and that's what I sort of like got",
    "start": "1011399",
    "end": "1013399"
  },
  {
    "text": "out of this experiment uh and how you",
    "start": "1013399",
    "end": "1016040"
  },
  {
    "text": "would use an embedding layer for uh for",
    "start": "1016040",
    "end": "1019480"
  },
  {
    "text": "vectors is uh the how you is where LS",
    "start": "1019480",
    "end": "1022240"
  },
  {
    "text": "comes in uh so yeah and uh some other",
    "start": "1022240",
    "end": "1026520"
  },
  {
    "text": "results were basically um instead of",
    "start": "1026520",
    "end": "1029600"
  },
  {
    "text": "making one prediction using the full",
    "start": "1029600",
    "end": "1031360"
  },
  {
    "text": "power of the uh Transformer model like",
    "start": "1031360",
    "end": "1034240"
  },
  {
    "text": "as you would normally do with an llm we",
    "start": "1034240",
    "end": "1036199"
  },
  {
    "text": "are making four predictions so does that",
    "start": "1036199",
    "end": "1038760"
  },
  {
    "text": "hurt the accuracy of your uh uh model uh",
    "start": "1038760",
    "end": "1042798"
  },
  {
    "text": "it turns out that yes it does like a",
    "start": "1042799",
    "end": "1044798"
  },
  {
    "text": "little bit um you lose some small",
    "start": "1044799",
    "end": "1047038"
  },
  {
    "text": "accuracy uh as compared to a sing",
    "start": "1047039",
    "end": "1049360"
  },
  {
    "text": "predicting the next token Alone um that",
    "start": "1049360",
    "end": "1052720"
  },
  {
    "text": "was uh that's the difference between the",
    "start": "1052720",
    "end": "1054520"
  },
  {
    "text": "first two lines uh uh and then uh the",
    "start": "1054520",
    "end": "1058000"
  },
  {
    "text": "remaining lines are basically future",
    "start": "1058000",
    "end": "1059400"
  },
  {
    "text": "predictions they are definitely harder",
    "start": "1059400",
    "end": "1060960"
  },
  {
    "text": "to make because you're trying to like",
    "start": "1060960",
    "end": "1062720"
  },
  {
    "text": "predict the fusers action into the",
    "start": "1062720",
    "end": "1064960"
  },
  {
    "text": "future like maybe one day later what",
    "start": "1064960",
    "end": "1066880"
  },
  {
    "text": "will the user watch given the context",
    "start": "1066880",
    "end": "1069840"
  },
  {
    "text": "given a context of what the user has",
    "start": "1069840",
    "end": "1071320"
  },
  {
    "text": "watched so far right like that's a",
    "start": "1071320",
    "end": "1072559"
  },
  {
    "text": "harder problem so naturally it's less",
    "start": "1072559",
    "end": "1074960"
  },
  {
    "text": "accurate and but uh the bottom line is",
    "start": "1074960",
    "end": "1077840"
  },
  {
    "text": "we don't lose Lo too much accuracy by",
    "start": "1077840",
    "end": "1079480"
  },
  {
    "text": "predicting those future tokens and our",
    "start": "1079480",
    "end": "1081880"
  },
  {
    "text": "embedding has more diversity as a result",
    "start": "1081880",
    "end": "1084400"
  },
  {
    "text": "of having predict like having like uh",
    "start": "1084400",
    "end": "1087720"
  },
  {
    "text": "learned what the users future",
    "start": "1087720",
    "end": "1089240"
  },
  {
    "text": "preferences are also so yeah this is uh",
    "start": "1089240",
    "end": "1092760"
  },
  {
    "text": "uh one of the results and then yeah I",
    "start": "1092760",
    "end": "1095400"
  },
  {
    "text": "also have some visual results like",
    "start": "1095400",
    "end": "1096679"
  },
  {
    "text": "basically um this is like from one user",
    "start": "1096679",
    "end": "1099559"
  },
  {
    "text": "their history basically has the video on",
    "start": "1099559",
    "end": "1102240"
  },
  {
    "text": "the left as the next engaged video and",
    "start": "1102240",
    "end": "1104919"
  },
  {
    "text": "the videos on the right except maybe the",
    "start": "1104919",
    "end": "1107000"
  },
  {
    "text": "last video are kind of relevant like",
    "start": "1107000",
    "end": "1108640"
  },
  {
    "text": "they're related to Udu um one of the",
    "start": "1108640",
    "end": "1111760"
  },
  {
    "text": "languages in India so you can see that",
    "start": "1111760",
    "end": "1114159"
  },
  {
    "text": "the model learns to predict the future",
    "start": "1114159",
    "end": "1116760"
  },
  {
    "text": "engagement pretty well and this is one",
    "start": "1116760",
    "end": "1118440"
  },
  {
    "text": "user with like full history like very",
    "start": "1118440",
    "end": "1120120"
  },
  {
    "text": "active user our batch size was actually",
    "start": "1120120",
    "end": "1122799"
  },
  {
    "text": "768 so that's what the 768 history",
    "start": "1122799",
    "end": "1126039"
  },
  {
    "text": "refers to and uh when we tried it with I",
    "start": "1126039",
    "end": "1129200"
  },
  {
    "text": "looked at another user of course one of",
    "start": "1129200",
    "end": "1130960"
  },
  {
    "text": "the better results uh was uh we looked",
    "start": "1130960",
    "end": "1134240"
  },
  {
    "text": "at a user with less history so 100 100",
    "start": "1134240",
    "end": "1136960"
  },
  {
    "text": "odd uh history and this was super",
    "start": "1136960",
    "end": "1139480"
  },
  {
    "text": "relevant like uh like three out of the I",
    "start": "1139480",
    "end": "1142520"
  },
  {
    "text": "think five predictions were the same God",
    "start": "1142520",
    "end": "1145200"
  },
  {
    "text": "uh Hindu god and the languages were uh",
    "start": "1145200",
    "end": "1148039"
  },
  {
    "text": "uh correct like uh in matches uh the",
    "start": "1148039",
    "end": "1150520"
  },
  {
    "text": "language in the captions in the video",
    "start": "1150520",
    "end": "1153360"
  },
  {
    "text": "that the user actually watched so",
    "start": "1153360",
    "end": "1155559"
  },
  {
    "text": "basically these were some spot checks",
    "start": "1155559",
    "end": "1156880"
  },
  {
    "text": "that we did and we were pretty happy",
    "start": "1156880",
    "end": "1158600"
  },
  {
    "text": "with the results and uh yeah so U uh we",
    "start": "1158600",
    "end": "1163039"
  },
  {
    "text": "decided to sort of like productionize",
    "start": "1163039",
    "end": "1164640"
  },
  {
    "text": "this uh and we are in that path now uh",
    "start": "1164640",
    "end": "1167200"
  },
  {
    "text": "We've not fully productionize it uh but",
    "start": "1167200",
    "end": "1170000"
  },
  {
    "text": "uh we did do some ablations and uh uh",
    "start": "1170000",
    "end": "1173520"
  },
  {
    "text": "like we Tred to see what will be the",
    "start": "1173520",
    "end": "1175320"
  },
  {
    "text": "gains when if we incorporate this into",
    "start": "1175320",
    "end": "1178039"
  },
  {
    "text": "our uh actual models right into our",
    "start": "1178039",
    "end": "1180400"
  },
  {
    "text": "production models so there were some",
    "start": "1180400",
    "end": "1181919"
  },
  {
    "text": "design decisions that we had to take and",
    "start": "1181919",
    "end": "1184000"
  },
  {
    "text": "I'll just walk you through those uh so",
    "start": "1184000",
    "end": "1186559"
  },
  {
    "text": "basically uh high level like this is our",
    "start": "1186559",
    "end": "1188440"
  },
  {
    "text": "ranker and retrieval model it's a single",
    "start": "1188440",
    "end": "1190400"
  },
  {
    "text": "model as I said like when you train it",
    "start": "1190400",
    "end": "1192559"
  },
  {
    "text": "and then you can like extract the",
    "start": "1192559",
    "end": "1193799"
  },
  {
    "text": "retrieval component uh and the ranker",
    "start": "1193799",
    "end": "1196400"
  },
  {
    "text": "components into like uh save them in",
    "start": "1196400",
    "end": "1198760"
  },
  {
    "text": "separate models and use them for your uh",
    "start": "1198760",
    "end": "1201760"
  },
  {
    "text": "retrieval and ranking uh scenarios um so",
    "start": "1201760",
    "end": "1205240"
  },
  {
    "text": "basically uh high level basically you",
    "start": "1205240",
    "end": "1207360"
  },
  {
    "text": "have a user component item component and",
    "start": "1207360",
    "end": "1209960"
  },
  {
    "text": "a cross component which looks at",
    "start": "1209960",
    "end": "1211320"
  },
  {
    "text": "everything the cross component",
    "start": "1211320",
    "end": "1212799"
  },
  {
    "text": "corresponds to the ranker the user and",
    "start": "1212799",
    "end": "1214880"
  },
  {
    "text": "item component sort of like emit",
    "start": "1214880",
    "end": "1216880"
  },
  {
    "text": "embeddings in addition to other features",
    "start": "1216880",
    "end": "1219240"
  },
  {
    "text": "and the embeddings are uh sort of like",
    "start": "1219240",
    "end": "1221120"
  },
  {
    "text": "compared uh to give you the retrieval",
    "start": "1221120",
    "end": "1223559"
  },
  {
    "text": "model or the lightweight ranker model",
    "start": "1223559",
    "end": "1226559"
  },
  {
    "text": "and uh in this situ scenario we we are",
    "start": "1226559",
    "end": "1229760"
  },
  {
    "text": "going to use the uh uh embeddings that",
    "start": "1229760",
    "end": "1232720"
  },
  {
    "text": "are output from like uh basically this",
    "start": "1232720",
    "end": "1235919"
  },
  {
    "text": "uh uh maybe I'll just go back a bit uh",
    "start": "1235919",
    "end": "1238559"
  },
  {
    "text": "sorry so basically the uh embeddings",
    "start": "1238559",
    "end": "1241480"
  },
  {
    "text": "that are just before the MLPs here uh",
    "start": "1241480",
    "end": "1244559"
  },
  {
    "text": "will be the output of the Transformer",
    "start": "1244559",
    "end": "1246919"
  },
  {
    "text": "will be used as sort of like embeddings",
    "start": "1246919",
    "end": "1249360"
  },
  {
    "text": "in the uh user Tower uh sorry uh user",
    "start": "1249360",
    "end": "1253240"
  },
  {
    "text": "Tower here so so that like uh they sort",
    "start": "1253240",
    "end": "1256480"
  },
  {
    "text": "of like tell you more about the user uh",
    "start": "1256480",
    "end": "1259880"
  },
  {
    "text": "it's a long-term history yeah so that",
    "start": "1259880",
    "end": "1261679"
  },
  {
    "text": "was the idea and um basically uh yeah so",
    "start": "1261679",
    "end": "1266520"
  },
  {
    "text": "uh the reason why we are doing it that",
    "start": "1266520",
    "end": "1268919"
  },
  {
    "text": "way is because uh yeah we do get a",
    "start": "1268919",
    "end": "1271200"
  },
  {
    "text": "powerful llm like model from",
    "start": "1271200",
    "end": "1273320"
  },
  {
    "text": "pre-training this uh content model uh",
    "start": "1273320",
    "end": "1276840"
  },
  {
    "text": "using this user",
    "start": "1276840",
    "end": "1278240"
  },
  {
    "text": "sequences uh but like uh it requires a",
    "start": "1278240",
    "end": "1280880"
  },
  {
    "text": "lot of uh data right you need to have",
    "start": "1280880",
    "end": "1283440"
  },
  {
    "text": "768 diam 768 32 dimensional embeddings",
    "start": "1283440",
    "end": "1287360"
  },
  {
    "text": "per user per request uh so uh if you",
    "start": "1287360",
    "end": "1291480"
  },
  {
    "text": "were to run that model real time and uh",
    "start": "1291480",
    "end": "1294400"
  },
  {
    "text": "there' be like very high latencies uh",
    "start": "1294400",
    "end": "1296760"
  },
  {
    "text": "low throughput Etc like the usual llm",
    "start": "1296760",
    "end": "1299120"
  },
  {
    "text": "problems if you want to try to uh run",
    "start": "1299120",
    "end": "1301960"
  },
  {
    "text": "this model real time so uh we we just",
    "start": "1301960",
    "end": "1304960"
  },
  {
    "text": "decided that yeah that's not possible",
    "start": "1304960",
    "end": "1306799"
  },
  {
    "text": "very similar to what Pinterest did and",
    "start": "1306799",
    "end": "1309400"
  },
  {
    "text": "what we decided to do was to just batch",
    "start": "1309400",
    "end": "1311320"
  },
  {
    "text": "predict that uh hidden layer the fight",
    "start": "1311320",
    "end": "1313559"
  },
  {
    "text": "12 dimensional embedding offline for all",
    "start": "1313559",
    "end": "1316320"
  },
  {
    "text": "daily active users and uh use that",
    "start": "1316320",
    "end": "1320520"
  },
  {
    "text": "embedding as a feature in our ranker and",
    "start": "1320520",
    "end": "1323200"
  },
  {
    "text": "retrieval models which actually also",
    "start": "1323200",
    "end": "1325600"
  },
  {
    "text": "have access to as I showed uh reason",
    "start": "1325600",
    "end": "1328279"
  },
  {
    "text": "history so that uh basically like one or",
    "start": "1328279",
    "end": "1331320"
  },
  {
    "text": "two day uh uh full history for the user",
    "start": "1331320",
    "end": "1334960"
  },
  {
    "text": "so that's more uh like uh so that like",
    "start": "1334960",
    "end": "1337039"
  },
  {
    "text": "we have information about the long uh",
    "start": "1337039",
    "end": "1340360"
  },
  {
    "text": "history and the recent history so uh",
    "start": "1340360",
    "end": "1343200"
  },
  {
    "text": "that was the decision that we made and",
    "start": "1343200",
    "end": "1346200"
  },
  {
    "text": "we are basically running some ablations",
    "start": "1346200",
    "end": "1348400"
  },
  {
    "text": "to see if uh we can remove unnecessary",
    "start": "1348400",
    "end": "1350400"
  },
  {
    "text": "feature groups uh like what I said",
    "start": "1350400",
    "end": "1352640"
  },
  {
    "text": "earlier like because these models are",
    "start": "1352640",
    "end": "1355080"
  },
  {
    "text": "themselves like aware of all the user",
    "start": "1355080",
    "end": "1356880"
  },
  {
    "text": "level features so yeah that's uh yeah",
    "start": "1356880",
    "end": "1360400"
  },
  {
    "text": "before I go into the results of those",
    "start": "1360400",
    "end": "1362159"
  },
  {
    "text": "ablation studies like I'll just show you",
    "start": "1362159",
    "end": "1364320"
  },
  {
    "text": "uh some of the like our uh uh uh GP",
    "start": "1364320",
    "end": "1368919"
  },
  {
    "text": "utilization for the inference job so",
    "start": "1368919",
    "end": "1371000"
  },
  {
    "text": "when I said like I would B we can be",
    "start": "1371000",
    "end": "1372919"
  },
  {
    "text": "batch predict the embeddings for the um",
    "start": "1372919",
    "end": "1376640"
  },
  {
    "text": "user llm basically the user features we",
    "start": "1376640",
    "end": "1379799"
  },
  {
    "text": "do it on like daily active users so this",
    "start": "1379799",
    "end": "1381760"
  },
  {
    "text": "is how much uh sort of like we uh spend",
    "start": "1381760",
    "end": "1385600"
  },
  {
    "text": "on each uh day partition to uh infer the",
    "start": "1385600",
    "end": "1389760"
  },
  {
    "text": "embeddings on uh for those users that",
    "start": "1389760",
    "end": "1393760"
  },
  {
    "text": "were active on that day and basically",
    "start": "1393760",
    "end": "1396080"
  },
  {
    "text": "you can see that like uh just using Ray",
    "start": "1396080",
    "end": "1398720"
  },
  {
    "text": "cor alone like we were able to like",
    "start": "1398720",
    "end": "1400200"
  },
  {
    "text": "easily saturate uh a 100s uh and we",
    "start": "1400200",
    "end": "1403840"
  },
  {
    "text": "using a bigger bat size uh so even on",
    "start": "1403840",
    "end": "1405880"
  },
  {
    "text": "this infrance job like without back prop",
    "start": "1405880",
    "end": "1408400"
  },
  {
    "text": "we get like we we using higher memory",
    "start": "1408400",
    "end": "1411120"
  },
  {
    "text": "usage than the uh training job and yeah",
    "start": "1411120",
    "end": "1415080"
  },
  {
    "text": "so this is the uh results like after",
    "start": "1415080",
    "end": "1418440"
  },
  {
    "text": "adding these Tings we saw like a 3% lift",
    "start": "1418440",
    "end": "1421279"
  },
  {
    "text": "for retrieval uh over our Baseline and",
    "start": "1421279",
    "end": "1425039"
  },
  {
    "text": "uh uh ranker we still haven't done the",
    "start": "1425039",
    "end": "1428720"
  },
  {
    "text": "uh uh we haven't like uh compared it",
    "start": "1428720",
    "end": "1431360"
  },
  {
    "text": "properly like um but uh retrieval we did",
    "start": "1431360",
    "end": "1434880"
  },
  {
    "text": "see gains um and uh we also did did some",
    "start": "1434880",
    "end": "1438159"
  },
  {
    "text": "ablations like we removed the uh uh all",
    "start": "1438159",
    "end": "1442240"
  },
  {
    "text": "the user features actually all the user",
    "start": "1442240",
    "end": "1444120"
  },
  {
    "text": "feature groups and we saw big loss in",
    "start": "1444120",
    "end": "1446279"
  },
  {
    "text": "performance for retrieval and uh that's",
    "start": "1446279",
    "end": "1448640"
  },
  {
    "text": "the blue line but then when we added",
    "start": "1448640",
    "end": "1450600"
  },
  {
    "text": "just these embeddings uh so just a",
    "start": "1450600",
    "end": "1453200"
  },
  {
    "text": "single uh embedding feature right like",
    "start": "1453200",
    "end": "1455520"
  },
  {
    "text": "maybe easier to maintain uh uh we were",
    "start": "1455520",
    "end": "1458360"
  },
  {
    "text": "able to sort of like recover the lost",
    "start": "1458360",
    "end": "1460760"
  },
  {
    "text": "performance and yeah so uh that that's",
    "start": "1460760",
    "end": "1464159"
  },
  {
    "text": "sort of like uh uh uh was uh was all",
    "start": "1464159",
    "end": "1468240"
  },
  {
    "text": "that I had to uh stay today so I'll just",
    "start": "1468240",
    "end": "1471559"
  },
  {
    "text": "summarize we basically have a ray based",
    "start": "1471559",
    "end": "1474000"
  },
  {
    "text": "ml training infrastructure uh which",
    "start": "1474000",
    "end": "1476480"
  },
  {
    "text": "supports like training different kinds",
    "start": "1476480",
    "end": "1478000"
  },
  {
    "text": "of models uh also the different",
    "start": "1478000",
    "end": "1480760"
  },
  {
    "text": "workflows like trainer data loader",
    "start": "1480760",
    "end": "1482559"
  },
  {
    "text": "feature transforms Etc and it incl it",
    "start": "1482559",
    "end": "1485520"
  },
  {
    "text": "also supports specifically the rexus",
    "start": "1485520",
    "end": "1487760"
  },
  {
    "text": "kind of llm kind of jobs and uh uh yeah",
    "start": "1487760",
    "end": "1492200"
  },
  {
    "text": "we use Ray also for data processing for",
    "start": "1492200",
    "end": "1495120"
  },
  {
    "text": "B batch inference like for generating",
    "start": "1495120",
    "end": "1497240"
  },
  {
    "text": "the index for neural retrieval for uh",
    "start": "1497240",
    "end": "1500240"
  },
  {
    "text": "doing uh things like uh llm uh based",
    "start": "1500240",
    "end": "1504080"
  },
  {
    "text": "models use a summary generation like the",
    "start": "1504080",
    "end": "1506679"
  },
  {
    "text": "retrieval the the U the the task that I",
    "start": "1506679",
    "end": "1510960"
  },
  {
    "text": "talked about like basically augment the",
    "start": "1510960",
    "end": "1513720"
  },
  {
    "text": "retrieval and ranker models with user",
    "start": "1513720",
    "end": "1516200"
  },
  {
    "text": "history uh embeddings right the that",
    "start": "1516200",
    "end": "1518760"
  },
  {
    "text": "retrieval Al that uh batch prediction",
    "start": "1518760",
    "end": "1521279"
  },
  {
    "text": "also goes via Ray cor and all of this",
    "start": "1521279",
    "end": "1524200"
  },
  {
    "text": "was we were able to build it in like",
    "start": "1524200",
    "end": "1525880"
  },
  {
    "text": "about 6 months like just less than of 6",
    "start": "1525880",
    "end": "1528200"
  },
  {
    "text": "months with a small team like five or",
    "start": "1528200",
    "end": "1530240"
  },
  {
    "text": "six people so and all it was all",
    "start": "1530240",
    "end": "1532640"
  },
  {
    "text": "possible because of uh like Ray and uh a",
    "start": "1532640",
    "end": "1536159"
  },
  {
    "text": "lot of Open Source tools so yeah that's",
    "start": "1536159",
    "end": "1538600"
  },
  {
    "text": "it uh for me yeah if you guys have any",
    "start": "1538600",
    "end": "1541000"
  },
  {
    "text": "questions please uh ask yeah thank",
    "start": "1541000",
    "end": "1546480"
  },
  {
    "text": "you yes",
    "start": "1546720",
    "end": "1549960"
  },
  {
    "text": "please",
    "start": "1551039",
    "end": "1553720"
  },
  {
    "text": "yes yes yes",
    "start": "1553720",
    "end": "1556679"
  },
  {
    "text": "uh",
    "start": "1556679",
    "end": "1558760"
  },
  {
    "text": "yes uh so this is basically uh topk",
    "start": "1558760",
    "end": "1561440"
  },
  {
    "text": "recall at 50 for the batch so this is an",
    "start": "1561440",
    "end": "1565640"
  },
  {
    "text": "in batch metric not the not on the KNN",
    "start": "1565640",
    "end": "1568799"
  },
  {
    "text": "Corpus like not on KNN eval but on uh",
    "start": "1568799",
    "end": "1572120"
  },
  {
    "text": "just the training batch but our training",
    "start": "1572120",
    "end": "1574240"
  },
  {
    "text": "batches are pretty big uh so uh yeah",
    "start": "1574240",
    "end": "1577399"
  },
  {
    "text": "maybe I just repeat like the question",
    "start": "1577399",
    "end": "1579200"
  },
  {
    "text": "was uh uh what is the exact metric that",
    "start": "1579200",
    "end": "1581880"
  },
  {
    "text": "was used uh for this uh 3% uh gain and",
    "start": "1581880",
    "end": "1585520"
  },
  {
    "text": "uh we basically used uh uh recall at 50",
    "start": "1585520",
    "end": "1589279"
  },
  {
    "text": "using uh in batch negatives like based",
    "start": "1589279",
    "end": "1592720"
  },
  {
    "text": "recall and uh we used like maybe batch",
    "start": "1592720",
    "end": "1596039"
  },
  {
    "text": "size of like maybe 2 power 15 or",
    "start": "1596039",
    "end": "1597600"
  },
  {
    "text": "something 2 power 16 so pretty big batch",
    "start": "1597600",
    "end": "1600039"
  },
  {
    "text": "so I think this is somewhat approximate",
    "start": "1600039",
    "end": "1602120"
  },
  {
    "text": "uh proxy to like uh K and eal yeah thank",
    "start": "1602120",
    "end": "1606360"
  },
  {
    "text": "you yes please for the",
    "start": "1606360",
    "end": "1611200"
  },
  {
    "text": "tring",
    "start": "1612840",
    "end": "1615360"
  },
  {
    "text": "token yes does that you have to ret this",
    "start": "1615360",
    "end": "1618840"
  },
  {
    "text": "model on the regular no uh so yeah",
    "start": "1618840",
    "end": "1622039"
  },
  {
    "text": "that's a good question so do we have to",
    "start": "1622039",
    "end": "1624159"
  },
  {
    "text": "the question is like because we are",
    "start": "1624159",
    "end": "1625279"
  },
  {
    "text": "predicting tokens like do we have to",
    "start": "1625279",
    "end": "1626760"
  },
  {
    "text": "retrain this model regularly and the",
    "start": "1626760",
    "end": "1629399"
  },
  {
    "text": "answer is no because uh we basically do",
    "start": "1629399",
    "end": "1632320"
  },
  {
    "text": "not predict the uh uh to token exactly",
    "start": "1632320",
    "end": "1636679"
  },
  {
    "text": "what we do is we uh predict basically",
    "start": "1636679",
    "end": "1639640"
  },
  {
    "text": "the embeddings of the token and we use",
    "start": "1639640",
    "end": "1642480"
  },
  {
    "text": "embeddings as input uh essentially this",
    "start": "1642480",
    "end": "1644600"
  },
  {
    "text": "blip 32 memorizer that I have here uh is",
    "start": "1644600",
    "end": "1648919"
  },
  {
    "text": "that that alone will be changed",
    "start": "1648919",
    "end": "1650399"
  },
  {
    "text": "regularly because our Corpus will keep",
    "start": "1650399",
    "end": "1652320"
  },
  {
    "text": "changing that can be thought of like an",
    "start": "1652320",
    "end": "1654399"
  },
  {
    "text": "as N&M embedding right like just an",
    "start": "1654399",
    "end": "1656559"
  },
  {
    "text": "embedding ID to embedding mapper and we",
    "start": "1656559",
    "end": "1659200"
  },
  {
    "text": "just change that portion of the model",
    "start": "1659200",
    "end": "1660919"
  },
  {
    "text": "and uh input newer uh user",
    "start": "1660919",
    "end": "1663679"
  },
  {
    "text": "sequences uh so uh basically nothing has",
    "start": "1663679",
    "end": "1666399"
  },
  {
    "text": "to change because pre-training is a",
    "start": "1666399",
    "end": "1668480"
  },
  {
    "text": "little bit expensive or and it's also",
    "start": "1668480",
    "end": "1669840"
  },
  {
    "text": "timeconsuming like uh you cannot like",
    "start": "1669840",
    "end": "1672440"
  },
  {
    "text": "hope to train this model fast enough uh",
    "start": "1672440",
    "end": "1675000"
  },
  {
    "text": "to keep up with a changing Corpus like",
    "start": "1675000",
    "end": "1677039"
  },
  {
    "text": "in our case are Corpus changes every day",
    "start": "1677039",
    "end": "1679760"
  },
  {
    "text": "yeah so uh yeah so that uh that I hope",
    "start": "1679760",
    "end": "1683600"
  },
  {
    "text": "that answers the question",
    "start": "1683600",
    "end": "1685519"
  },
  {
    "text": "yeah I",
    "start": "1685519",
    "end": "1688120"
  },
  {
    "text": "question for theing you said it's too",
    "start": "1688120",
    "end": "1691720"
  },
  {
    "text": "slow yes things like help",
    "start": "1691720",
    "end": "1695600"
  },
  {
    "text": "you possible yeah so the question is",
    "start": "1695600",
    "end": "1698760"
  },
  {
    "text": "like uh so for serving uh this is a",
    "start": "1698760",
    "end": "1701480"
  },
  {
    "text": "little bit slow which is why we decided",
    "start": "1701480",
    "end": "1703159"
  },
  {
    "text": "to go with the batch prediction approach",
    "start": "1703159",
    "end": "1706399"
  },
  {
    "text": "like uh the question is can we use",
    "start": "1706399",
    "end": "1707919"
  },
  {
    "text": "things like paged attention I think it's",
    "start": "1707919",
    "end": "1709919"
  },
  {
    "text": "just a little uh my answer is basically",
    "start": "1709919",
    "end": "1711919"
  },
  {
    "text": "I think it's a little expensive uh to go",
    "start": "1711919",
    "end": "1713880"
  },
  {
    "text": "that route even if it's uh maybe uh",
    "start": "1713880",
    "end": "1716279"
  },
  {
    "text": "latency wise like solvable if that",
    "start": "1716279",
    "end": "1718519"
  },
  {
    "text": "problem because uh like uh uh I I just",
    "start": "1718519",
    "end": "1722000"
  },
  {
    "text": "think like we because you have typically",
    "start": "1722000",
    "end": "1724279"
  },
  {
    "text": "you have this kind of architecture where",
    "start": "1724279",
    "end": "1725840"
  },
  {
    "text": "you have some recent history in the",
    "start": "1725840",
    "end": "1728120"
  },
  {
    "text": "model itself so you can leverage that to",
    "start": "1728120",
    "end": "1731880"
  },
  {
    "text": "uh basically cover that gap between your",
    "start": "1731880",
    "end": "1735640"
  },
  {
    "text": "batch prediction updates which can be",
    "start": "1735640",
    "end": "1737840"
  },
  {
    "text": "like daily like you can uh update those",
    "start": "1737840",
    "end": "1740519"
  },
  {
    "text": "user embeddings on a daily Cadence and",
    "start": "1740519",
    "end": "1743080"
  },
  {
    "text": "get uh more frequent uh get like user",
    "start": "1743080",
    "end": "1745960"
  },
  {
    "text": "embeddings for last uh maybe 6 months or",
    "start": "1745960",
    "end": "1749200"
  },
  {
    "text": "something and then for the last one day",
    "start": "1749200",
    "end": "1751519"
  },
  {
    "text": "right like uh that Delta you sort of",
    "start": "1751519",
    "end": "1753919"
  },
  {
    "text": "like cover using the reason history",
    "start": "1753919",
    "end": "1757720"
  },
  {
    "text": "yeah this is the real yeah that's the",
    "start": "1757720",
    "end": "1760399"
  },
  {
    "text": "real time path yes that that will get",
    "start": "1760399",
    "end": "1762440"
  },
  {
    "text": "populated uh real time uh and uh fed",
    "start": "1762440",
    "end": "1765679"
  },
  {
    "text": "into the model yes",
    "start": "1765679",
    "end": "1768679"
  },
  {
    "text": "yeah okay yeah so",
    "start": "1768679",
    "end": "1772960"
  },
  {
    "text": "uh if you don't have any questions I",
    "start": "1772960",
    "end": "1775640"
  },
  {
    "text": "think uh thank you so much for attending",
    "start": "1775640",
    "end": "1779780"
  },
  {
    "text": "[Applause]",
    "start": "1779780",
    "end": "1782880"
  },
  {
    "text": "yeah",
    "start": "1783159",
    "end": "1786159"
  }
]