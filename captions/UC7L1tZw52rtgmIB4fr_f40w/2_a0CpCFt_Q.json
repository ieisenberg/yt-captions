[
  {
    "text": "[Applause] hi welcome everybody uh thank you for coming to talk I know that I'm between",
    "start": "2590",
    "end": "8400"
  },
  {
    "text": "you and the drinks out there um so thank you for coming uh we're going to talk about the evolution of the ray platform",
    "start": "8400",
    "end": "15440"
  },
  {
    "text": "and how we accommodate for the Gen era as you see we got the Gen in the title so my um",
    "start": "15440",
    "end": "22920"
  },
  {
    "text": "Netflix uh why do we personalize Netflix we have um",
    "start": "22920",
    "end": "30240"
  },
  {
    "text": "we help members find entertainment entertainment to watch and enjoy and we",
    "start": "30240",
    "end": "35920"
  },
  {
    "text": "are in the business of maximizing our members Joy what do we personalize almost",
    "start": "35920",
    "end": "42960"
  },
  {
    "text": "everything that we have in the site is personalized for you uh to the construction of the page which is uh",
    "start": "42960",
    "end": "48680"
  },
  {
    "text": "employing different rows from candidate of rows to the title that comes in the row to the order that comes in every",
    "start": "48680",
    "end": "55090"
  },
  {
    "text": "[Music] Row the titles that go in every row also U sorted in a way that it's suitable for",
    "start": "55090",
    "end": "62760"
  },
  {
    "text": "you the the most high rank titles go first and later",
    "start": "62760",
    "end": "69439"
  },
  {
    "text": "on the the L so um we also personalize the image that come to your site so",
    "start": "69439",
    "end": "74840"
  },
  {
    "text": "different users might see different artwork depending to what switch to",
    "start": "74840",
    "end": "79880"
  },
  {
    "text": "their appeals to [Music] them what is the model production",
    "start": "79880",
    "end": "86200"
  },
  {
    "text": "workflow like um we have end to end machine learning research uh that produces models uh to refine the product",
    "start": "86200",
    "end": "93200"
  },
  {
    "text": "on Netflix and this is the life cycle of a model you have you come with an idea you model that you produce the code that",
    "start": "93200",
    "end": "100560"
  },
  {
    "text": "will run an experiment you do offline experimentation you compare the metrics with the the ones that we have offline",
    "start": "100560",
    "end": "107040"
  },
  {
    "text": "and if your idea is successful you promote it into an AV test and in the a",
    "start": "107040",
    "end": "112759"
  },
  {
    "text": "test we decide if your model is better than the one that we have in production so for many many researchers many",
    "start": "112759",
    "end": "120600"
  },
  {
    "text": "experiments and every experiment has lots of cells in the Navy test uh so one",
    "start": "120600",
    "end": "126960"
  },
  {
    "text": "of these uh squares is a training so if we have 11 cells we have 11 models",
    "start": "126960",
    "end": "132959"
  },
  {
    "text": "trained with different variations each uh so you can see the explosions of models that that we have to train every",
    "start": "132959",
    "end": "139599"
  },
  {
    "text": "day these are uh personalized models that need to train daily in a 24-hour",
    "start": "139599",
    "end": "146000"
  },
  {
    "text": "cycle the Netflix training platform how we we we accommodate great for for doing this massive amount of",
    "start": "146000",
    "end": "152519"
  },
  {
    "text": "training we have a recommendation models that are um dlrm architecture and",
    "start": "152519",
    "end": "159760"
  },
  {
    "text": "they're heav on embedding tables and categorical features and MLP um so these",
    "start": "159760",
    "end": "165760"
  },
  {
    "text": "are these are very short uh model in competition time but very large data set so we need to uh fine-tune the the io in",
    "start": "165760",
    "end": "173200"
  },
  {
    "text": "order to accommodate for the for training this these type of models um we have a bottleneck in the grading",
    "start": "173200",
    "end": "180280"
  },
  {
    "text": "communication um and we do multi-gpu distributed for for those models we have uh for medl U multimodel",
    "start": "180280",
    "end": "189519"
  },
  {
    "text": "models that learn representations we have diverse set of data sources text we",
    "start": "189519",
    "end": "195000"
  },
  {
    "text": "have lots of text image audio videos you name it and we have um very large data",
    "start": "195000",
    "end": "202599"
  },
  {
    "text": "set that derive from the original assets um we also have a multi-gpu multi node",
    "start": "202599",
    "end": "207640"
  },
  {
    "text": "training for very massive models and distributed data parallel fashion um we",
    "start": "207640",
    "end": "213480"
  },
  {
    "text": "also employ uh a sizeable number of language models um we we don't produce",
    "start": "213480",
    "end": "219439"
  },
  {
    "text": "our own models but we find tune models that are out there llama being one um",
    "start": "219439",
    "end": "224879"
  },
  {
    "text": "and we have accomodated for different sizes of model to to train in multi gpus and we use fully sh data parallel for",
    "start": "224879",
    "end": "232079"
  },
  {
    "text": "fine tuning those um the computations are more intensive because these are Transformer based U but this is the",
    "start": "232079",
    "end": "240000"
  },
  {
    "text": "isation of what we have um we have given a talk last year about the the details",
    "start": "240000",
    "end": "246120"
  },
  {
    "text": "of how we build this training platform how we consider the io to be more important and how we have aeneous",
    "start": "246120",
    "end": "253159"
  },
  {
    "text": "selection of resources I invite you to go there and and see more if you want to know the details I will recap on on a",
    "start": "253159",
    "end": "260959"
  },
  {
    "text": "few things H this is a picture of uh the theog genous nature of the cluster so",
    "start": "260959",
    "end": "267400"
  },
  {
    "text": "you have an head note and then we organize the machines in pools of",
    "start": "267400",
    "end": "272720"
  },
  {
    "text": "similar types so if we have a pool of machines of P4 machines in AWS we have",
    "start": "272720",
    "end": "277840"
  },
  {
    "text": "800 uh gpus and another pool with A10 gpus and we accommodate the pools",
    "start": "277840",
    "end": "283639"
  },
  {
    "text": "according to the different type of gpus that you have and also we have a a a pull for CPU machines so whenever you",
    "start": "283639",
    "end": "291000"
  },
  {
    "text": "you come with a workload that spans different type of gpus or gpus plus CPU",
    "start": "291000",
    "end": "297479"
  },
  {
    "text": "you can accommodate in this three pools or more depending on your cluster we also have a dedicated f file",
    "start": "297479",
    "end": "304600"
  },
  {
    "text": "system FSX for High throughput and um another share file system for for",
    "start": "304600",
    "end": "310199"
  },
  {
    "text": "lowlevel uh things like a checkpoints or uh metrics or metadata that we need to",
    "start": "310199",
    "end": "315240"
  },
  {
    "text": "collect from all the Clusters um we employ many of these clusters we employ clusters in a durable",
    "start": "315240",
    "end": "321000"
  },
  {
    "text": "fashion we deploy one cluster and which the cluster stays up until we train all the models that we want uh and then we",
    "start": "321000",
    "end": "327039"
  },
  {
    "text": "deploy every every so often uh some sometimes the models the the cluster stay up for a month or",
    "start": "327039",
    "end": "334759"
  },
  {
    "text": "more we have also the idea of sharing Resources by team so there are there is",
    "start": "334759",
    "end": "341240"
  },
  {
    "text": "a model that you can say I have a cluster per job the model that we employ which is a",
    "start": "341240",
    "end": "347520"
  },
  {
    "text": "cluster per team so chances are that people in your team are training the",
    "start": "347520",
    "end": "352759"
  },
  {
    "text": "same type of model so the the the the the cluster doesn't have to accommodate",
    "start": "352759",
    "end": "358639"
  },
  {
    "text": "for different workload there all similar so so naturally they they they you can",
    "start": "358639",
    "end": "364240"
  },
  {
    "text": "isolate teams by having different consumption of clusters and there is another notion that is you have an Uber",
    "start": "364240",
    "end": "369880"
  },
  {
    "text": "cluster with everything you can toss everything there but then it's it's a little bit more difficult to manage the",
    "start": "369880",
    "end": "376520"
  },
  {
    "text": "inconvenient of having uh clusters per team is that one team cannot employ idle",
    "start": "376520",
    "end": "382360"
  },
  {
    "text": "resources in other team so then if uh if a workloads in one team finished and you have idle machines you cannot employ",
    "start": "382360",
    "end": "388560"
  },
  {
    "text": "them in your work so it's a um so in a process where where these machines are",
    "start": "388560",
    "end": "395360"
  },
  {
    "text": "very scarce and we only uh have so few of them uh we need to maximize the use of them uh one problem is that uh I say",
    "start": "395360",
    "end": "404160"
  },
  {
    "text": "we cannot share resources between clusters um another one is that operationally we need to stop all the",
    "start": "404160",
    "end": "411120"
  },
  {
    "text": "jobs that go to a cluster in order to redeploy or upgrade the cluster um and we also have a having multiple clusters",
    "start": "411120",
    "end": "418879"
  },
  {
    "text": "one per team it's difficult for us to collect visibility uis and metrics from all the",
    "start": "418879",
    "end": "424520"
  },
  {
    "text": "Clusters in a central location so the solution was to create a job scheduler a colleague that is here in the audience",
    "start": "424520",
    "end": "430039"
  },
  {
    "text": "Kar is in charge of um the job scheduler and the job scheduler has a routing",
    "start": "430039",
    "end": "436280"
  },
  {
    "text": "system between route clusters so you now submit to the job schedule and you don't care when it's going to be executed or",
    "start": "436280",
    "end": "442440"
  },
  {
    "text": "where it's going to be executed the job schedule will take care of it as long as it respects the SLA you're fine h the",
    "start": "442440",
    "end": "450520"
  },
  {
    "text": "the the system uses cues so we can we can we can have different cues for different teams and then uh the teams",
    "start": "450520",
    "end": "456879"
  },
  {
    "text": "allocate how many resources per queue you need and it has plugable policies so we",
    "start": "456879",
    "end": "462319"
  },
  {
    "text": "can fine-tune how we going to uh get the jobs executed in in at a given time to",
    "start": "462319",
    "end": "468280"
  },
  {
    "text": "maximize the the resources which is the important part for the schedu in a picture you can see it like this H you",
    "start": "468280",
    "end": "476039"
  },
  {
    "text": "have a central place where you submit the the the job and in in that job lands in AQ and and and you have a job monitor",
    "start": "476039",
    "end": "484039"
  },
  {
    "text": "that is constantly monitoring in R robing fashion all the jobs that are running in the cluster and with the",
    "start": "484039",
    "end": "491599"
  },
  {
    "text": "faculty of paing and resuming jobs if if the schedule SE so so if you are running",
    "start": "491599",
    "end": "497479"
  },
  {
    "text": "a low low priority job and an high priority job can needs to come in you can signal that job to checkpoint stop",
    "start": "497479",
    "end": "504759"
  },
  {
    "text": "and free the resources for the high priority job to come in um this is the",
    "start": "504759",
    "end": "509919"
  },
  {
    "text": "the the key advantage of having the schedule being the central brain of all the the Clusters and we have also the",
    "start": "509919",
    "end": "515839"
  },
  {
    "text": "resources being monitored constantly so the scheduler is a central place where you can monitor the usage of all the",
    "start": "515839",
    "end": "521440"
  },
  {
    "text": "Clusters that that we have um also it's a Simplicity for the users because you now send it to one place you don't need",
    "start": "521440",
    "end": "528360"
  },
  {
    "text": "to remember which cluster is assigned to you and how to use the shared clusters that that we have for ad hog",
    "start": "528360",
    "end": "535560"
  },
  {
    "text": "computations so to reiterate it's a um Fair resource allocation so we also make",
    "start": "535560",
    "end": "542440"
  },
  {
    "text": "a distinction between the ad hoc experimentation oneoff things that you want to run versus the production jobs",
    "start": "542440",
    "end": "548440"
  },
  {
    "text": "that need SLA uh we have plugable policies we we can design any policy we want and then",
    "start": "548440",
    "end": "555640"
  },
  {
    "text": "just plug it in into into uh in code in the in the in the Schuler and we obey",
    "start": "555640",
    "end": "561240"
  },
  {
    "text": "how we can we we we want to be the cues and the jobs in that queue to behave um",
    "start": "561240",
    "end": "568480"
  },
  {
    "text": "we also have dynamic resource configuration so if you if your job normally trains in an 800 H but an 800 is not available you",
    "start": "568480",
    "end": "576399"
  },
  {
    "text": "can downgrade to an 810 if your job supports it uh so you",
    "start": "576399",
    "end": "581680"
  },
  {
    "text": "can you can run in one type of GPU or another depending on on on the availability and we also have preemption",
    "start": "581680",
    "end": "588760"
  },
  {
    "text": "so let's say you have a long running job that is running seven days and a bunch of crunch comes from jobs that need to",
    "start": "588760",
    "end": "594680"
  },
  {
    "text": "use the cluster you can suspend the the the the long running job evict it from",
    "start": "594680",
    "end": "600720"
  },
  {
    "text": "the cluster and move all the other jobs to run on the resources and when they finish you can resume back with the long",
    "start": "600720",
    "end": "606600"
  },
  {
    "text": "run job um we also have simplify operations because having many clusters",
    "start": "606600",
    "end": "612360"
  },
  {
    "text": "we can direct all the traffic from one cluster to other clusters while we deploy it and once the cluster is up and",
    "start": "612360",
    "end": "619240"
  },
  {
    "text": "ready we can go back with the traffic the same way you have a load balancer in HTP conversations you can have that with",
    "start": "619240",
    "end": "625880"
  },
  {
    "text": "clusters and jobs and it had as an abstraction of clusters we can have different cluster",
    "start": "625880",
    "end": "631839"
  },
  {
    "text": "different size and different type of operations but the scheduler abstract that away as a single cluster we can",
    "start": "631839",
    "end": "637120"
  },
  {
    "text": "have this idea that everything is a single machine you a single cluster instead of many and we have a",
    "start": "637120",
    "end": "642480"
  },
  {
    "text": "centralized UI a centralized observability metrics and so on",
    "start": "642480",
    "end": "647839"
  },
  {
    "text": "um another addition to the training platform uh was the creation of this HBO",
    "start": "647839",
    "end": "653680"
  },
  {
    "text": "manager I know we know that Ray comes with tune but this is a simplified version of that and it's very",
    "start": "653680",
    "end": "659480"
  },
  {
    "text": "opinionated on using optuna which is the the engine that we choose as a default",
    "start": "659480",
    "end": "664920"
  },
  {
    "text": "and it works in conjunction with the scheduler the schedular can give a special queue with a available resource",
    "start": "664920",
    "end": "671000"
  },
  {
    "text": "for your HBO experiment remember an HBO is no other than a for loop with many training jobs at once so um the HBO",
    "start": "671000",
    "end": "679000"
  },
  {
    "text": "manager babysits in turn uh all the trainings that you need to do and then he controls the state of the experiment",
    "start": "679000",
    "end": "685800"
  },
  {
    "text": "because the experiment which is many training jobs at once can be also checkpointed and paed and evicted from",
    "start": "685800",
    "end": "692560"
  },
  {
    "text": "the cluster and then once you have the opportunity to run again you can resume where where you left off with many many",
    "start": "692560",
    "end": "698920"
  },
  {
    "text": "trials um and this is also from a colleague of mine vinko which might not",
    "start": "698920",
    "end": "704200"
  },
  {
    "text": "be here today but um we have the user experience uh it's if you're familiar",
    "start": "704200",
    "end": "710680"
  },
  {
    "text": "with optuna this is optuna in the cloud for many jobs H it has a minimal change",
    "start": "710680",
    "end": "717680"
  },
  {
    "text": "on the training API so if you have your your job already using a training the training API we just inject the",
    "start": "717680",
    "end": "723440"
  },
  {
    "text": "parameters you want to explore and then that immediately RS you to use the hpo manager instead of the training H use",
    "start": "723440",
    "end": "730079"
  },
  {
    "text": "visualization uh Central dashboard where we collect all the visualization so people can go and see their experiment while the experiment is running you can",
    "start": "730079",
    "end": "736519"
  },
  {
    "text": "go and see the the stream of data coming to to a dashboard um and it has the",
    "start": "736519",
    "end": "743279"
  },
  {
    "text": "operational things that you know the the scheduler takes care of the HBO and the HBO PES and resumes",
    "start": "743279",
    "end": "751160"
  },
  {
    "text": "automatically let's go to the Gen use cases um",
    "start": "752279",
    "end": "757560"
  },
  {
    "text": "so uh there are some things that we cannot talk too much about because we still don't know uh which is uh one is",
    "start": "757560",
    "end": "764160"
  },
  {
    "text": "the generative recommenders so I left that out uh for the sake of Simplicity",
    "start": "764160",
    "end": "769480"
  },
  {
    "text": "but uh we are exploring foundational models and generative recommendation systems which are uh very heavy um H",
    "start": "769480",
    "end": "778360"
  },
  {
    "text": "we're going to talk about llms uh we do fine tuning for LMS and why why would",
    "start": "778360",
    "end": "784120"
  },
  {
    "text": "you prefer that instead of using open AI or or or or any other the",
    "start": "784120",
    "end": "789160"
  },
  {
    "text": "um good llms out there is because of the customer ability we can just fine tuning",
    "start": "789160",
    "end": "795800"
  },
  {
    "text": "with more uh freedom and we also have sensitive cases we cannot send the data over the wi to some third party so it's",
    "start": "795800",
    "end": "803720"
  },
  {
    "text": "better if we can maintain it within the company um we have also a a different task that",
    "start": "803720",
    "end": "811839"
  },
  {
    "text": "depend on on fine tune models so we can build on top of fine tune models the the",
    "start": "811839",
    "end": "817839"
  },
  {
    "text": "the llms that we find tune Ser as a backbone for all the tasks and and we by",
    "start": "817839",
    "end": "823440"
  },
  {
    "text": "having access to the llms ourselves is like we we we can do specialized um",
    "start": "823440",
    "end": "830320"
  },
  {
    "text": "um serving Solutions so we can do for throughput or or for latency depending",
    "start": "830320",
    "end": "835680"
  },
  {
    "text": "on the case uh Ray makes it very easy for to scale the fine tuning and and",
    "start": "835680",
    "end": "841519"
  },
  {
    "text": "evaluation so we use it thoroughly and um for models that are",
    "start": "841519",
    "end": "848680"
  },
  {
    "text": "very large that you need to partition and do in multiple gpus or sometimes in multiple nodes a ray plus VM um was a",
    "start": "848680",
    "end": "858040"
  },
  {
    "text": "very nice alternative that simplifies all the mess there so we can we employ it this is also a a contribution of one",
    "start": "858040",
    "end": "865000"
  },
  {
    "text": "of our team members Dan which is also in the audience if you want to talk to him after",
    "start": "865000",
    "end": "870360"
  },
  {
    "text": "um in pictures it looks like this we have a a a pipeline that fine tunes and",
    "start": "870360",
    "end": "877639"
  },
  {
    "text": "after fine tuning evaluates and after evaluation publishes H the evaluation depending on the model might be very",
    "start": "877639",
    "end": "884399"
  },
  {
    "text": "large and you need to partition the model so that's why we employ a VM in the evaluation and H once you deposit",
    "start": "884399",
    "end": "890759"
  },
  {
    "text": "the model in in the model repository you can then pick it up with a serving system if you if you need a scale or you",
    "start": "890759",
    "end": "896720"
  },
  {
    "text": "can pick it up as a single Searcher just to evaluate if the model is right and",
    "start": "896720",
    "end": "902079"
  },
  {
    "text": "employed in your tasks um the most active project I've",
    "start": "902079",
    "end": "907720"
  },
  {
    "text": "been working lately is in multimodel uh data sets building multimodel data sets with all the data that we have at",
    "start": "907720",
    "end": "913600"
  },
  {
    "text": "Netflix is it's a a non-trivial task and we're going to describe a little bit how",
    "start": "913600",
    "end": "920160"
  },
  {
    "text": "we are employing the array in order to build this what is a multimodal data set",
    "start": "920160",
    "end": "925600"
  },
  {
    "text": "multimodal data set uh we employ some data that can be in the form of pictures",
    "start": "925600",
    "end": "931800"
  },
  {
    "text": "or video or audio or text and we augment the data with the aid of models so if",
    "start": "931800",
    "end": "938800"
  },
  {
    "text": "you have millions of images you cannot scan them by hand and see if the mod if the images are good or bad so in order",
    "start": "938800",
    "end": "945759"
  },
  {
    "text": "to assess quality you need to employ a model that looks into the pictures and give you a score so then you can then",
    "start": "945759",
    "end": "951199"
  },
  {
    "text": "filter the ones that are bad or the good the same way you cannot caption caption",
    "start": "951199",
    "end": "956519"
  },
  {
    "text": "mean annotate the images if you send it out to to to a third party service it will take six months to annotate all the",
    "start": "956519",
    "end": "962519"
  },
  {
    "text": "images and it cost a lot of money so we also employ a model in in this case his picture as lava 1.6 and you take a",
    "start": "962519",
    "end": "970199"
  },
  {
    "text": "picture with the aid of a prompt and you can add say describe this image with this data without naming pictures or",
    "start": "970199",
    "end": "976440"
  },
  {
    "text": "without describing the background or you need to craft the prompt in order to to",
    "start": "976440",
    "end": "981800"
  },
  {
    "text": "to get the the correct output some other models are more complex in the case of Florence here picture in green uh you",
    "start": "981800",
    "end": "988600"
  },
  {
    "text": "employ Loy uh an image together with a caption and with the two you generate bounding boxes of the objects present in",
    "start": "988600",
    "end": "996440"
  },
  {
    "text": "the caption so there is a man there is a woman there's a a cat there is a car and there is another uh column that",
    "start": "996440",
    "end": "1004000"
  },
  {
    "text": "we use in order to explore that uh data sets that is uh for embeddings so you",
    "start": "1004000",
    "end": "1009880"
  },
  {
    "text": "produce embeddings for either the images or or the audios or the videos and you can use that uh embedding column to do",
    "start": "1009880",
    "end": "1017319"
  },
  {
    "text": "nearest neighbor search or to cluster and to figure out the composition or the distribution of your of your huge dat",
    "start": "1017319",
    "end": "1024280"
  },
  {
    "text": "set so what is needed in order to create this data set you need a Model Management so you need a way to easily",
    "start": "1024280",
    "end": "1030199"
  },
  {
    "text": "and cheaply move away um and share models that people employ we can take",
    "start": "1030199",
    "end": "1036199"
  },
  {
    "text": "models from open source we can fine tune them and use those models to to create columns we need a batch inference system",
    "start": "1036199",
    "end": "1042520"
  },
  {
    "text": "in order to produce this at a very large scale and we also need a way to store this data because it's the data is huge",
    "start": "1042520",
    "end": "1048679"
  },
  {
    "text": "and then you need um and you need a lot of gpus uh uh",
    "start": "1048679",
    "end": "1056840"
  },
  {
    "text": "models so we have why we need models in in in",
    "start": "1057400",
    "end": "1063280"
  },
  {
    "text": "building data sets so so now the datas are very heavy in inference and you need to probably process them with models to",
    "start": "1063280",
    "end": "1069520"
  },
  {
    "text": "obtain results in this case the first model is a nons safe for work uh so this",
    "start": "1069520",
    "end": "1074559"
  },
  {
    "text": "is a vision Transformer that you need to fit in the image and the model will turn a high score if there is nudity in in",
    "start": "1074559",
    "end": "1081320"
  },
  {
    "text": "the image a low score if otherwise so then you can um you can filter the",
    "start": "1081320",
    "end": "1086600"
  },
  {
    "text": "images this way so you need to do a big pass on the images first and filter the ones that you're not going to use by different scorers um the same way uh",
    "start": "1086600",
    "end": "1095120"
  },
  {
    "text": "when you caption you use a a pre-train language model which also comes with a vision encoder in this case it's lava we",
    "start": "1095120",
    "end": "1102039"
  },
  {
    "text": "use many many captioners and for every caption you need to fine-tune the way you prompted uh to get the output that",
    "start": "1102039",
    "end": "1110320"
  },
  {
    "text": "you that you want without too many hallucinations audio has the same",
    "start": "1110320",
    "end": "1116760"
  },
  {
    "text": "treatment you can use employe a variety of models in order to separate audio to transcribe audio to do things with audio",
    "start": "1116760",
    "end": "1124799"
  },
  {
    "text": "um we use uh clip score uh to compare the similarities between the image and",
    "start": "1124799",
    "end": "1130480"
  },
  {
    "text": "the text the you get a high score the the text and the image are similar you get a low score if they have nothing to",
    "start": "1130480",
    "end": "1136760"
  },
  {
    "text": "do with each other and with that we can evaluate if the if the text uh that we caption with the previous model is good",
    "start": "1136760",
    "end": "1143159"
  },
  {
    "text": "or not and we can let go of the bad captions or retry with a different",
    "start": "1143159",
    "end": "1149039"
  },
  {
    "text": "captioner there is also the use of embedding models uh for images we use",
    "start": "1149039",
    "end": "1154159"
  },
  {
    "text": "clip for for text we use sentence Transformer and they generate different vectors from different sizes uh there",
    "start": "1154159",
    "end": "1161840"
  },
  {
    "text": "there are Universal models that you can embed anything into the same space uh so um you can use it to compare if the the",
    "start": "1161840",
    "end": "1169600"
  },
  {
    "text": "things are related or if the the data that you construct in a row is has a",
    "start": "1169600",
    "end": "1176120"
  },
  {
    "text": "mismatch new type of models are coming uh we are using these two as an instance of Auto regressive models that means one",
    "start": "1176120",
    "end": "1183120"
  },
  {
    "text": "token at a time they can describe um boxes labels and the text out of an",
    "start": "1183120",
    "end": "1191159"
  },
  {
    "text": "image uh so in one go they can do multitask and they also do this in a autor progressive man which is",
    "start": "1191159",
    "end": "1197440"
  },
  {
    "text": "impressive um two instances of those Microsoft Florence and Google p PMA um I don't",
    "start": "1197440",
    "end": "1204320"
  },
  {
    "text": "know if I pronounce it right um how does it look like in practic so",
    "start": "1204320",
    "end": "1210240"
  },
  {
    "text": "we have a um millions of images that come in a data set and we filter out the non-safe for work and we keep the rest",
    "start": "1210240",
    "end": "1218480"
  },
  {
    "text": "um another thing that we do is we have a models that assess the Aesthetics of the",
    "start": "1218480",
    "end": "1224440"
  },
  {
    "text": "image and you can you can say that the model will give you a high score the if the if the image has high Aesthetics and low",
    "start": "1224440",
    "end": "1232720"
  },
  {
    "text": "score otherwise and we keep the super high quality the top uh 10% of the the",
    "start": "1232720",
    "end": "1239000"
  },
  {
    "text": "images to have a high quality pass on data sets we keep the remaining ones as a bulk of the data set and we eliminate",
    "start": "1239000",
    "end": "1245520"
  },
  {
    "text": "the bottom for images that are not good for the task at hand so images that are completely black or images that have",
    "start": "1245520",
    "end": "1251760"
  },
  {
    "text": "blurry faces or things that are not good for the task this is also a a huge step in",
    "start": "1251760",
    "end": "1258600"
  },
  {
    "text": "automation because you don't have time or money to do this with with humans of",
    "start": "1258600",
    "end": "1264120"
  },
  {
    "text": "the the large size of the data sets captioning this is an example of",
    "start": "1264120",
    "end": "1270919"
  },
  {
    "text": "captioning uh we employ different captioners some captioners are short so a man and a woman hugging in the crowd",
    "start": "1270919",
    "end": "1276159"
  },
  {
    "text": "it's a very simple caption uh other other captioners are much more detailed and they they they exhibit a lot of",
    "start": "1276159",
    "end": "1283600"
  },
  {
    "text": "expressiveness when they describe all the things that happen in the scene we also have face recognition and and face",
    "start": "1283600",
    "end": "1290039"
  },
  {
    "text": "detection and face recognition so we can say that that woman is Jennifer Lopez so we can do another pass with another LM",
    "start": "1290039",
    "end": "1296080"
  },
  {
    "text": "saying like when you describe a man and a woman and if we give you a hint of who's the woman can you give me a new",
    "start": "1296080",
    "end": "1302000"
  },
  {
    "text": "description and the LM will rewrite the description putting the characters or the actors in in in the scene um we have",
    "start": "1302000",
    "end": "1311600"
  },
  {
    "text": "different sort of metadata that comes with the titles or the genre or you I don't know the season the time it was",
    "start": "1311600",
    "end": "1318120"
  },
  {
    "text": "the recorded who's in the uh photography uh who's in The Director so all of that",
    "start": "1318120",
    "end": "1325159"
  },
  {
    "text": "enriches the the assets that that that we have um hallucinations are very",
    "start": "1325159",
    "end": "1330320"
  },
  {
    "text": "common if we start employing this at scale Mo many of the models that we used",
    "start": "1330320",
    "end": "1335440"
  },
  {
    "text": "for example in the first picture it says the the title of this image is The Dark Knight Rises which is not this is",
    "start": "1335440",
    "end": "1341679"
  },
  {
    "text": "another movie completely different uh and and the second one says perhaps the reflection she's reflecting on the words",
    "start": "1341679",
    "end": "1349640"
  },
  {
    "text": "of the Korean drama blah blah blah I don't know if that Korean drama but it's not this uh so and you need to have",
    "start": "1349640",
    "end": "1356720"
  },
  {
    "text": "another model that inspects this because we cannot inspect this as human this a lot of data so you need to have a model",
    "start": "1356720",
    "end": "1362600"
  },
  {
    "text": "that inspect this the outputs of all model and assess the quality if it's good or bad um comparing blms visual",
    "start": "1362600",
    "end": "1371320"
  },
  {
    "text": "language models uh so we have here in one column we see cvlm lava and Florence",
    "start": "1371320",
    "end": "1377200"
  },
  {
    "text": "and they have different levels of granularity on the description so we",
    "start": "1377200",
    "end": "1382559"
  },
  {
    "text": "we need to employ another model to to to to say is there a a man in the picture",
    "start": "1382559",
    "end": "1387720"
  },
  {
    "text": "because the the the the sentence says there's a man is there a woman in the picture yes no is there a I don't know",
    "start": "1387720",
    "end": "1395600"
  },
  {
    "text": "dark background yes or no and then you start getting yes or NOS for all the questions that you ask to to the LM and",
    "start": "1395600",
    "end": "1401679"
  },
  {
    "text": "then you can do um Precision recall and then see if the the which one of the",
    "start": "1401679",
    "end": "1407279"
  },
  {
    "text": "captioners is better for you uh in the first one that is a black and white h i",
    "start": "1407279",
    "end": "1413640"
  },
  {
    "text": "highlighted there is a uh a sign saying in case of fire and believe me if I zoom",
    "start": "1413640",
    "end": "1420320"
  },
  {
    "text": "in there is a a tiny word on top of the telephone says in case of fire so the",
    "start": "1420320",
    "end": "1426000"
  },
  {
    "text": "lnms are very good at also doing OCR so you you capture the text coming from the from the picture um so batch inference",
    "start": "1426000",
    "end": "1434919"
  },
  {
    "text": "um how do we do this so normally people say oh if I have a model I put in a server and then I do the request",
    "start": "1434919",
    "end": "1440799"
  },
  {
    "text": "response and I can get back uh the inference produced by that model this is normally the case when you when you have",
    "start": "1440799",
    "end": "1446840"
  },
  {
    "text": "maybe a Java service and the the the inference run in a GPU in a different machine um or if you don't have access",
    "start": "1446840",
    "end": "1454880"
  },
  {
    "text": "to the model but if you if you need to have a cascading of models like model one two and three now the client engages",
    "start": "1454880",
    "end": "1461919"
  },
  {
    "text": "in this conversation with a lot of yellow payloads that come and go that are temporary payloads because you send",
    "start": "1461919",
    "end": "1467080"
  },
  {
    "text": "it to one model the output of one model needs to be fed to the second model and then the client becomes very bloated in",
    "start": "1467080",
    "end": "1472600"
  },
  {
    "text": "being a coordinator of all of this and also remember that every box that says model one and model two is many boxes",
    "start": "1472600",
    "end": "1478480"
  },
  {
    "text": "with many model many instances of the same model so the clients become very bloated in in in trying to coordinate",
    "start": "1478480",
    "end": "1484080"
  },
  {
    "text": "all of this um but is a solution if you don't have the same platform of your",
    "start": "1484080",
    "end": "1489919"
  },
  {
    "text": "model is close s another way of doing this is with bulk inference um bulk",
    "start": "1489919",
    "end": "1494960"
  },
  {
    "text": "inference coming from um the idea that you can do do the inference in the you can have the threads that do the IOP",
    "start": "1494960",
    "end": "1502200"
  },
  {
    "text": "operations in the same machine that does the inference so you in one in one thread you are reading the data then you",
    "start": "1502200",
    "end": "1507799"
  },
  {
    "text": "are doing the inference and you're writing the data in the same machine um er this uh is done with the bulga",
    "start": "1507799",
    "end": "1514799"
  },
  {
    "text": "synchronous parallel framework so you kind of have the idea if you have many of those they are doing that in parallel",
    "start": "1514799",
    "end": "1520799"
  },
  {
    "text": "in every stage you dump the data at the end in the dis if you have a cascading of models you have a lot of yellow",
    "start": "1520799",
    "end": "1527360"
  },
  {
    "text": "temporary data and you need to process the entire data with the first model dump the data then read the data again",
    "start": "1527360",
    "end": "1533360"
  },
  {
    "text": "process it with the second model and so on um how about Ray data if we use Ray",
    "start": "1533360",
    "end": "1539200"
  },
  {
    "text": "data Ray data introduce the notion of share memory and the the the beauty of this is that you now have a thread that",
    "start": "1539200",
    "end": "1545880"
  },
  {
    "text": "reads batches at a time and they put that that badges in memory now the only",
    "start": "1545880",
    "end": "1550960"
  },
  {
    "text": "thing that you exchange uh with models is the pointers to the data so you have a pointer that picks a batch and then",
    "start": "1550960",
    "end": "1556720"
  },
  {
    "text": "when you finish you deposit the batch in in the memory and another model picks up with a pointer and you only exchange",
    "start": "1556720",
    "end": "1562720"
  },
  {
    "text": "pointers between models um and and and this is more suitable for having you",
    "start": "1562720",
    "end": "1568279"
  },
  {
    "text": "know a streaming batch inference um this from the documentation this picture says",
    "start": "1568279",
    "end": "1574480"
  },
  {
    "text": "it all uh we can deploy every stage of a of a pipeline in CPUs or in gpus and you",
    "start": "1574480",
    "end": "1581880"
  },
  {
    "text": "you notice that the the actor pools that hold the two models the classification model and the segmentation model have",
    "start": "1581880",
    "end": "1587840"
  },
  {
    "text": "different number of instances so the classification model might be in cheap in in computation might employ only",
    "start": "1587840",
    "end": "1594399"
  },
  {
    "text": "three instances while the segmentation model be more heavy employee five so now you see that these pools are elastic and",
    "start": "1594399",
    "end": "1602159"
  },
  {
    "text": "you can as uh to trying to maintain the constant flow of of of batches from one",
    "start": "1602159",
    "end": "1609080"
  },
  {
    "text": "end to to another ray accommodates for the scale of this two pools so you have a constant stream all of this is",
    "start": "1609080",
    "end": "1615960"
  },
  {
    "text": "automated is done for you and and you don't you don't need to do anything other than write the the entire",
    "start": "1615960",
    "end": "1622039"
  },
  {
    "text": "transformation sequence um it's as easy as wrapping the model and and and doing",
    "start": "1622039",
    "end": "1628440"
  },
  {
    "text": "a process with a batch in and batch out so you read the data you do the inference and you write the data",
    "start": "1628440",
    "end": "1635320"
  },
  {
    "text": "um if you use an LM you instantiate an llm and then you you get this is an",
    "start": "1635320",
    "end": "1640960"
  },
  {
    "text": "example of summarizer and the beauty of this is that also this you can test you here you can run the summarizer in a",
    "start": "1640960",
    "end": "1647039"
  },
  {
    "text": "batch in in a test code without using the r the ray sub system we just evaluate all the Transformations",
    "start": "1647039",
    "end": "1653000"
  },
  {
    "text": "independently and then you you can toss it into the ray bch system what do you",
    "start": "1653000",
    "end": "1658200"
  },
  {
    "text": "need to worry about you don't need to worry about doing all of this because it's done for free for you but you need",
    "start": "1658200",
    "end": "1663640"
  },
  {
    "text": "to worry about how to fit the model in the GPU now you are concentrated on how to optimize and employ the model into",
    "start": "1663640",
    "end": "1671120"
  },
  {
    "text": "the memory of the GPU um also remembering that the extra run that you",
    "start": "1671120",
    "end": "1677760"
  },
  {
    "text": "get in the in the in the GPU card is for accommodating the number of uh um",
    "start": "1677760",
    "end": "1683320"
  },
  {
    "text": "elements that you can process at a given time so if you employ a small card the extra memory that you have might be not",
    "start": "1683320",
    "end": "1690840"
  },
  {
    "text": "enough for holding more than one element so you process one element at a time if you have a car with more RAM you can",
    "start": "1690840",
    "end": "1697000"
  },
  {
    "text": "process more in a batch that means you you get more throughput out of the the same so you can you need to play between",
    "start": "1697000",
    "end": "1703360"
  },
  {
    "text": "the quantization and trying to fit the model in the right place speed to get the throughput and and also memory free",
    "start": "1703360",
    "end": "1709279"
  },
  {
    "text": "so you can get batches uh continuously um and the last thing that you might",
    "start": "1709279",
    "end": "1715600"
  },
  {
    "text": "need to worry about is um how you start processing this data because you you have some data and you create one column",
    "start": "1715600",
    "end": "1722640"
  },
  {
    "text": "at a time and you have your data set now more data comes in a new show a new TV show a new set of audios something you",
    "start": "1722640",
    "end": "1729399"
  },
  {
    "text": "need to process if you if you can keep the Deltas it would be awesome because you can play with the Deltas and",
    "start": "1729399",
    "end": "1735919"
  },
  {
    "text": "Aggregate and but sometimes you need to also aggregate that and save the resulting data set so",
    "start": "1735919",
    "end": "1742760"
  },
  {
    "text": "the resulting data set keeps growing when you keep copies of the previous data sets and this in terabytes it",
    "start": "1742760",
    "end": "1748159"
  },
  {
    "text": "sounds ridiculous but when once you start reaching 100 and petabytes this becomes a very problematic the same way",
    "start": "1748159",
    "end": "1756000"
  },
  {
    "text": "you think about this in columns so if you have data set of 10 terabytes and you add another column that's one terabyte and then you had another column",
    "start": "1756000",
    "end": "1761960"
  },
  {
    "text": "another terabyte and if you keep the Deltas if you can keep the Deltas it would be awesome because you can just",
    "start": "1761960",
    "end": "1767200"
  },
  {
    "text": "grow the data set of what you need otherwise you need to keep having copies of your data lying around over and",
    "start": "1767200",
    "end": "1774840"
  },
  {
    "text": "over ER so here's the Segway to data set format we are testing the waters playing with ls",
    "start": "1774840",
    "end": "1782480"
  },
  {
    "text": "format which um has a couple of niceties compared to to dealing with parquet",
    "start": "1782480",
    "end": "1787760"
  },
  {
    "text": "files and the two ones that I want to mention very briefly is the vector search it comes included you have an",
    "start": "1787760",
    "end": "1793240"
  },
  {
    "text": "index on column so you can employ directly with the format and you also have S zero copy versioning so you can",
    "start": "1793240",
    "end": "1800399"
  },
  {
    "text": "you can keep the Deltas which is the thing that I I mentioned before um if you think about this pictorical you want",
    "start": "1800399",
    "end": "1806200"
  },
  {
    "text": "to remember um you have V1 and if you add rows the rows are appended",
    "start": "1806200",
    "end": "1813360"
  },
  {
    "text": "automically and there is a manifest that have pointers to the two file one and",
    "start": "1813360",
    "end": "1818440"
  },
  {
    "text": "version one and version two and that becomes the whole data set if you decide to add a column that is also appended up",
    "start": "1818440",
    "end": "1824320"
  },
  {
    "text": "automically and the Manifest file contain pointers to the to to all these objects and you keep adding then you can",
    "start": "1824320",
    "end": "1831200"
  },
  {
    "text": "far roll back and roll forward on the same data set H without confusing anyone",
    "start": "1831200",
    "end": "1836519"
  },
  {
    "text": "and the last one is you know Vector search it comes for free the the two algorithms that are are being used for",
    "start": "1836519",
    "end": "1841679"
  },
  {
    "text": "dis based um approximated nearest neighbor search um which is very",
    "start": "1841679",
    "end": "1847760"
  },
  {
    "text": "convenient when you need to explore this data set when you need to plot this data set and you need to understand what is it that I put inside because that will",
    "start": "1847760",
    "end": "1855000"
  },
  {
    "text": "determine the quality of your model and that's all I have to say thank",
    "start": "1855000",
    "end": "1861040"
  },
  {
    "text": "[Applause]",
    "start": "1861040",
    "end": "1867120"
  },
  {
    "text": "you now questions I need to repeat the questions when you ask",
    "start": "1867120",
    "end": "1872799"
  },
  {
    "text": "them question I made a joke in the past presentation and I mentioned you I quote you correctly",
    "start": "1875200",
    "end": "1882399"
  },
  {
    "text": "yes I was curious about Lance like what led you to explore it and what s were",
    "start": "1882399",
    "end": "1887480"
  },
  {
    "text": "you thinking about like deciding whether we move to like production or whatever here with the yeah this is an",
    "start": "1887480",
    "end": "1893200"
  },
  {
    "text": "exploration we're not I'm not saying that we should or you should but uh but I have a I have a lot of cuts also uh",
    "start": "1893200",
    "end": "1901279"
  },
  {
    "text": "razor blade Cuts uh but the promise there is that U uh you the format allows",
    "start": "1901279",
    "end": "1907200"
  },
  {
    "text": "for Arrow so everything is based on Arrow so if you if you if you're happy with all the ecosystem that employees",
    "start": "1907200",
    "end": "1912760"
  },
  {
    "text": "Arrow Arrow there's a talk also about LS tomorrow you should attend and see um",
    "start": "1912760",
    "end": "1919799"
  },
  {
    "text": "The Columns can be very wide it doesn't matter they it eliminates the row groups",
    "start": "1919799",
    "end": "1925240"
  },
  {
    "text": "uh that are pressing in parquet that impedes it's an impediment for us to store very large blobs in and and the",
    "start": "1925240",
    "end": "1932360"
  },
  {
    "text": "problem is that in paret you read a very large uh you want to do Point queries and read one element and if the element",
    "start": "1932360",
    "end": "1938639"
  },
  {
    "text": "is too big you fetch the entire Ro group and you exhaust the memory of the readers and you need to try it again and",
    "start": "1938639",
    "end": "1945360"
  },
  {
    "text": "there is no good row group that you can do it's always biting you it's very good for for for table data but it's kind of",
    "start": "1945360",
    "end": "1953120"
  },
  {
    "text": "inconvenient for blobs and blobs here also can be tensors we store tensors of,",
    "start": "1953120",
    "end": "1958600"
  },
  {
    "text": "for for 100,00 120 for 4,000 and those",
    "start": "1958600",
    "end": "1964039"
  },
  {
    "text": "are blobs and we store them in and the other convenience is that cheaply you",
    "start": "1964039",
    "end": "1969840"
  },
  {
    "text": "have an index in place you don't need to ingest the data into another system in order to query it you just have it and",
    "start": "1969840",
    "end": "1976600"
  },
  {
    "text": "then you pass it around you you have one pointer to the data set as three location SL SL my data set and I give it",
    "start": "1976600",
    "end": "1982919"
  },
  {
    "text": "to you you modify it and and I can query the same and it's the same it's like iceberg in in a in a sense you have one",
    "start": "1982919",
    "end": "1989559"
  },
  {
    "text": "table uh and um yeah versioning is it's also very",
    "start": "1989559",
    "end": "1997200"
  },
  {
    "text": "good to have U check it outk okay um I just had a question about when",
    "start": "1997200",
    "end": "2003919"
  },
  {
    "text": "you were talking about data processing and R scaling up and down of the models uh does it also work for",
    "start": "2003919",
    "end": "2011840"
  },
  {
    "text": "the case where you have limited amount of gpus and it needs to this the other so so so the question",
    "start": "2011840",
    "end": "2019840"
  },
  {
    "text": "is that if Ray data graph can scale works the same if",
    "start": "2019840",
    "end": "2026760"
  },
  {
    "text": "you have limited number of gpus uh you you need so if you have a stage",
    "start": "2026760",
    "end": "2034360"
  },
  {
    "text": "that has six uh stages you need at least six gpus if you have less you cannot run",
    "start": "2034360",
    "end": "2040320"
  },
  {
    "text": "it because you you you employ one GPU another GPU another and the last stage is waiting for resources and it will",
    "start": "2040320",
    "end": "2046279"
  },
  {
    "text": "never execute imagine that's the limit yeah like imagine the best like for is",
    "start": "2046279",
    "end": "2052878"
  },
  {
    "text": "first imagine I have 20 gpus minimum I need is six and the most optimal is",
    "start": "2052879",
    "end": "2059040"
  },
  {
    "text": "first I move like 10 to the first one for the others then I move 10 to the",
    "start": "2059040",
    "end": "2065000"
  },
  {
    "text": "second one as the data flows yeah so so if you see here H one of the",
    "start": "2065000",
    "end": "2073398"
  },
  {
    "text": "uh parameters concurrency and I have two to 10 that means that stage needs a minimum of two gpus and a maximum of 10",
    "start": "2073399",
    "end": "2081079"
  },
  {
    "text": "so if you if you leave the maximum to the maximum number of gpus you have in your cluster every stage will automatically ER adapt as as the flow",
    "start": "2081079",
    "end": "2089320"
  },
  {
    "text": "comes in so the optimal will be discovered by the system not by you you can do it by hand forcing to have every",
    "start": "2089320",
    "end": "2095079"
  },
  {
    "text": "stage consume this stage two this stage four but uh but if you let the system do it",
    "start": "2095079",
    "end": "2100720"
  },
  {
    "text": "it its thing it will automatically balance between the two and and scaler out and as you are finishing one stage",
    "start": "2100720",
    "end": "2107440"
  },
  {
    "text": "this uh pool can extinguish and that pool can assimilate the other and",
    "start": "2107440",
    "end": "2112760"
  },
  {
    "text": "finish think something like this We R into problem where because R has that cash and cash is tied to the node that",
    "start": "2112760",
    "end": "2120400"
  },
  {
    "text": "puts in the cash as it scale down the data that the next stage want to P from",
    "start": "2120400",
    "end": "2126079"
  },
  {
    "text": "the plasma C",
    "start": "2126079",
    "end": "2129640"
  },
  {
    "text": "but but this is uh so the the problem is that uh I need to repeat the question um",
    "start": "2131280",
    "end": "2137240"
  },
  {
    "text": "if you have two machines that machine has the data in the memory store and the other machine can pull it from the from",
    "start": "2137240",
    "end": "2146000"
  },
  {
    "text": "the but automatically Ray will pull the data from that node and copy to the next node in Arrow copy zero copy fashion and",
    "start": "2149160",
    "end": "2156520"
  },
  {
    "text": "then you can have it in the second not yes but should be the order should",
    "start": "2156520",
    "end": "2162160"
  },
  {
    "text": "bect first you need to P SC I I don't understand what you're",
    "start": "2162160",
    "end": "2168520"
  },
  {
    "text": "saying we can take it offline if you want the details but",
    "start": "2168520",
    "end": "2173599"
  },
  {
    "text": "yeah I don't see people so just say say what you want my",
    "start": "2173599",
    "end": "2180640"
  },
  {
    "text": "name of the content understanding that you show have you explored more of semantic understanding of video",
    "start": "2180640",
    "end": "2188400"
  },
  {
    "text": "camera creartive and have analis",
    "start": "2188400",
    "end": "2197000"
  },
  {
    "text": "that the question is if we have made a semantic analysis on",
    "start": "2198440",
    "end": "2204520"
  },
  {
    "text": "videos next it yeah so so there there are many teams",
    "start": "2204520",
    "end": "2210520"
  },
  {
    "text": "in Netflix that build models uh based on media uh and and yeah I can I can",
    "start": "2210520",
    "end": "2216760"
  },
  {
    "text": "mention a couple of problems that we sort out with with Gen but we we we we",
    "start": "2216760",
    "end": "2222720"
  },
  {
    "text": "need to employ models in order to understand the content that we because it it's kind out of hands it's too much",
    "start": "2222720",
    "end": "2228720"
  },
  {
    "text": "content for for let the machines do the work question is the nsw detection for",
    "start": "2228720",
    "end": "2236920"
  },
  {
    "text": "vetting Co before on the platform and are you using for 2 for that no so is",
    "start": "2236920",
    "end": "2243040"
  },
  {
    "text": "nons safe for work an option for us betting to the platform to exhibit or users no this is only internal this is",
    "start": "2243040",
    "end": "2249319"
  },
  {
    "text": "so we the researchers work in the cycle without looking at inconvenient photos",
    "start": "2249319",
    "end": "2254839"
  },
  {
    "text": "while you're working with your and then we don't want the models to produce inconvenient photos are you using 2 as",
    "start": "2254839",
    "end": "2261520"
  },
  {
    "text": "the city for that no we're using a model that we find tune uh that is a based on",
    "start": "2261520",
    "end": "2266920"
  },
  {
    "text": "Vision Transformer um it's a classifier it gives you yes nudity no nudity and then we yeah it's not sophisticated in",
    "start": "2266920",
    "end": "2274800"
  },
  {
    "text": "the sense that it will give you more",
    "start": "2274800",
    "end": "2280119"
  },
  {
    "text": "details we can take the rest of the questions offline and then they need to cut the transmission thank you for",
    "start": "2282240",
    "end": "2287960"
  },
  {
    "text": "coming",
    "start": "2287960",
    "end": "2290960"
  }
]