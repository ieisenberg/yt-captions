[
  {
    "start": "0",
    "end": "18000"
  },
  {
    "text": "Hi, my name is Jerry co-founder\nand CEO of LlamaIndex.",
    "start": "450",
    "end": "3190"
  },
  {
    "text": "Hey, my name is",
    "start": "3750",
    "end": "4380"
  },
  {
    "text": "Amog I'm one of the Ray\ndevelopers here at Anyscale,",
    "start": "4380",
    "end": "6609"
  },
  {
    "text": "And we're super excited today to\npresent how LlamaIndex and Ray can",
    "start": "6990",
    "end": "10440"
  },
  {
    "text": "help you build an advanced query\nengine over your data with LLMs.",
    "start": "10445",
    "end": "13799"
  },
  {
    "text": "So first, Jerry's gonna give some\ncontext of how to build LLM applications.",
    "start": "14399",
    "end": "18270"
  },
  {
    "start": "18000",
    "end": "183000"
  },
  {
    "text": "LLMs are a phenomenal piece of technology\nfor knowledge generation and reasoning.",
    "start": "18421",
    "end": "22842"
  },
  {
    "text": "But the main problem is that\nthey're pre-trained on large",
    "start": "22888",
    "end": "25648"
  },
  {
    "text": "amounts of publicly available data.",
    "start": "25648",
    "end": "27538"
  },
  {
    "text": "And so they don't inherently\nknow anything about you, yourself",
    "start": "27883",
    "end": "31182"
  },
  {
    "text": "as the user or an organization.",
    "start": "31243",
    "end": "32983"
  },
  {
    "text": "But they're so good at a variety of\ndifferent tasks from question answering",
    "start": "33261",
    "end": "36621"
  },
  {
    "text": "text summarization, even agent-like\ninteractions, like planning and using",
    "start": "36651",
    "end": "41001"
  },
  {
    "text": "different types of plug-ins and tools.",
    "start": "41001",
    "end": "42591"
  },
  {
    "text": "The main challenge here is figuring\nout how do you actually incorporate all",
    "start": "43391",
    "end": "47441"
  },
  {
    "text": "your private sources of information,\nwhether you're an individual or an",
    "start": "47441",
    "end": "51418"
  },
  {
    "text": "enterprise, into your LLM application.",
    "start": "51418",
    "end": "53668"
  },
  {
    "text": "This could include all different types\nof data from file formats like PDFs,",
    "start": "54208",
    "end": "59008"
  },
  {
    "text": "PowerPoints, documents, all the way to\nAPIs that you commonly use, like a Slack",
    "start": "59008",
    "end": "63178"
  },
  {
    "text": "or Notion all the way to databases,\nwithin your enterprise data lake.",
    "start": "63178",
    "end": "67358"
  },
  {
    "text": "LlamaIndex gives you the tools to\nactually solve this and manage and",
    "start": "68158",
    "end": "72537"
  },
  {
    "text": "query your data within your LLM app.",
    "start": "72538",
    "end": "74668"
  },
  {
    "text": "It provides a comprehensive set of\ntools to index and ingest your data,",
    "start": "75178",
    "end": "79558"
  },
  {
    "text": "as well as provide an advanced query\ninterface within your LLM application.",
    "start": "79888",
    "end": "83848"
  },
  {
    "text": "The first challenge that you must\nsolve if you're trying to incorporate",
    "start": "84648",
    "end": "87588"
  },
  {
    "text": "private sources of data into your LLM\napp is figuring out how do you actually",
    "start": "87588",
    "end": "91967"
  },
  {
    "text": "load this data and how do you actually\nstructure it in the right format",
    "start": "91968",
    "end": "94998"
  },
  {
    "text": "so that the LLM can understand it.",
    "start": "94998",
    "end": "96678"
  },
  {
    "text": "So to this end, we offer, our\ndata connectors as well as",
    "start": "97278",
    "end": "100768"
  },
  {
    "text": "our data indexing modules.",
    "start": "100768",
    "end": "102147"
  },
  {
    "text": "Our data connectors are offered\nthrough our site called Llama Hub,",
    "start": "102198",
    "end": "104897"
  },
  {
    "text": "which offers over a hundred different\ndata loaders from a ton of different",
    "start": "104928",
    "end": "108408"
  },
  {
    "text": "sources, from different file formats\nto databases, to workplace apps.",
    "start": "108408",
    "end": "112848"
  },
  {
    "text": "And this allows you to easily load\nin data from different types of, data",
    "start": "112938",
    "end": "116498"
  },
  {
    "text": "sources into your LLM application\ninto a centralized document object.",
    "start": "116503",
    "end": "120608"
  },
  {
    "text": "The next component here is\nbeing able to define the right",
    "start": "121408",
    "end": "124228"
  },
  {
    "text": "data structures over this data.",
    "start": "124228",
    "end": "125937"
  },
  {
    "text": "Data parsing, structuring and indexing is\nessential because it allows you to define",
    "start": "126568",
    "end": "131548"
  },
  {
    "text": "the units of computation and data that\nthe LLM will eventually operate over.",
    "start": "131608",
    "end": "136138"
  },
  {
    "text": "And so this includes, for\ninstance, being able to parse a",
    "start": "136558",
    "end": "139528"
  },
  {
    "text": "document into chunks of nodes.",
    "start": "139528",
    "end": "141898"
  },
  {
    "text": "This includes being able to define\nindexes on these nodes as well, for",
    "start": "142318",
    "end": "146558"
  },
  {
    "text": "instance, via keywords or embeddings.",
    "start": "146558",
    "end": "148597"
  },
  {
    "text": "And then putting it into a downstream\nstorage solution like a vector",
    "start": "148943",
    "end": "152122"
  },
  {
    "text": "database or a document store.",
    "start": "152123",
    "end": "153623"
  },
  {
    "text": "The second challenge that you need to\nsolve when building an LLM application",
    "start": "154423",
    "end": "158053"
  },
  {
    "text": "over your data is figuring out how the\nLLM is able to retrieve this data through",
    "start": "158058",
    "end": "163043"
  },
  {
    "text": "the indexes that you created, as well\nas synthesize a response and how to",
    "start": "163043",
    "end": "167303"
  },
  {
    "text": "make this interface general enough to\nsatisfy all different types of queries",
    "start": "167303",
    "end": "170963"
  },
  {
    "text": "that you might wanna run over this data.",
    "start": "170963",
    "end": "172493"
  },
  {
    "text": "So this could include very\nsimple questions that you",
    "start": "173198",
    "end": "175748"
  },
  {
    "text": "wanna ask over specific facts.",
    "start": "175748",
    "end": "177908"
  },
  {
    "text": "This could include summarization,\nqueries, and this could also include",
    "start": "178208",
    "end": "181508"
  },
  {
    "text": "complex, multi-step questions.",
    "start": "181598",
    "end": "183158"
  },
  {
    "text": "As Jerry mentioned, LlamaIndex\nprovides excellent abstractions",
    "start": "183608",
    "end": "186618"
  },
  {
    "text": "for solving two challenges with\ndeveloping online applications.",
    "start": "186618",
    "end": "189588"
  },
  {
    "text": "The first is during the data ingestion.",
    "start": "189918",
    "end": "191928"
  },
  {
    "text": "with LlamaIndex, you can easily load files\nfrom a variety of different data sources,",
    "start": "192388",
    "end": "196408"
  },
  {
    "text": "parse them, generate embeddings, and store\nthose embeddings into a vector store.",
    "start": "196618",
    "end": "200487"
  },
  {
    "text": "However, when you wanna scale this\nprocess up to thousands or millions",
    "start": "200998",
    "end": "203878"
  },
  {
    "text": "of documents, it becomes a challenge.",
    "start": "203883",
    "end": "205978"
  },
  {
    "text": "This is where Ray comes in.",
    "start": "206128",
    "end": "207238"
  },
  {
    "text": "By using Ray to parallelize this data\ningestion process, you can quickly",
    "start": "207598",
    "end": "212008"
  },
  {
    "text": "ingest and generate embeddings for\nmany different documents from a",
    "start": "212008",
    "end": "215547"
  },
  {
    "text": "variety of different sources, all\nin a much more performant fashion.",
    "start": "215548",
    "end": "219068"
  },
  {
    "text": "Now the second part of the LLM\napplication is the actual querying.",
    "start": "219868",
    "end": "222748"
  },
  {
    "text": "So once we've created these vector\nstores, you have an LLM which can access",
    "start": "223048",
    "end": "227518"
  },
  {
    "text": "information from these vector stores\nand answer user questions, for example.",
    "start": "227518",
    "end": "231208"
  },
  {
    "text": "So again, LlamaIndex provides\nthe abstraction for being able to",
    "start": "231478",
    "end": "234237"
  },
  {
    "text": "create these types of query engines.",
    "start": "234318",
    "end": "235797"
  },
  {
    "text": "So given the user input, you can\nuse a index query engine to get",
    "start": "235798",
    "end": "239158"
  },
  {
    "text": "some answer to your question.",
    "start": "239158",
    "end": "241228"
  },
  {
    "text": "But when you actually wanna deploy\nthese types of applications in",
    "start": "241658",
    "end": "244238"
  },
  {
    "text": "production, you need a good solution.",
    "start": "244238",
    "end": "245918"
  },
  {
    "text": "And this is where Ray Serve comes in.",
    "start": "245918",
    "end": "247298"
  },
  {
    "text": "Using Ray Serve, we can deploy our\napplication, be able to request it, and",
    "start": "247808",
    "end": "251698"
  },
  {
    "text": "be able to handle lots of different user\nloads through Ray Serve's, auto scaling.",
    "start": "251698",
    "end": "255328"
  },
  {
    "text": "So with the Ray Serve you get\nautomatic load balancing and auto",
    "start": "255328",
    "end": "258328"
  },
  {
    "text": "scaling, and a variety of other\nproduction features that are necessary",
    "start": "258328",
    "end": "260937"
  },
  {
    "text": "for building, LLM applications.",
    "start": "260938",
    "end": "262728"
  },
  {
    "start": "263000",
    "end": "366000"
  },
  {
    "text": "So let's take a look at an example\nuse case where we can actually compare",
    "start": "263529",
    "end": "267669"
  },
  {
    "text": "and contrast the Ray documentation\nitself and also the Ray blogs, and",
    "start": "267669",
    "end": "272079"
  },
  {
    "text": "we can compare and contrast how these\ntwo different data sources present",
    "start": "272084",
    "end": "275919"
  },
  {
    "text": "Ray Serve itself as a technology.",
    "start": "276039",
    "end": "277909"
  },
  {
    "text": "So this is a complex question\ncontaining multiple steps.",
    "start": "278709",
    "end": "282279"
  },
  {
    "text": "And the LLM query engine that\nwe defined in LlamaIndex is able",
    "start": "282714",
    "end": "287214"
  },
  {
    "text": "to first break down this complex\nquestion into two sub-components.",
    "start": "287214",
    "end": "290754"
  },
  {
    "text": "How does the Ray Docs present Ray Serve?",
    "start": "291174",
    "end": "293494"
  },
  {
    "text": "And then how does the Ray\nblogs present Ray Serve?",
    "start": "293694",
    "end": "295974"
  },
  {
    "text": "It is then able to execute these\nsub-questions, according to their",
    "start": "296804",
    "end": "300469"
  },
  {
    "text": "corresponding tools or subquery\nengines, and so these query engines",
    "start": "300474",
    "end": "304969"
  },
  {
    "text": "actually correspond to indexes\ndefined over each document itself",
    "start": "304969",
    "end": "308839"
  },
  {
    "text": "or each collection of documents.",
    "start": "308869",
    "end": "310249"
  },
  {
    "text": "So we have a collection of documents,\nrepresenting the Ray documentation.",
    "start": "311049",
    "end": "315169"
  },
  {
    "text": "We also have another collection of\ndocuments representing the Ray blogs",
    "start": "315409",
    "end": "318738"
  },
  {
    "text": "and we have indexes defined over both.",
    "start": "318859",
    "end": "320869"
  },
  {
    "text": "After we ask these sub-questions over\nthese different collections of documents,",
    "start": "321559",
    "end": "325369"
  },
  {
    "text": "we can get back some initial answers.",
    "start": "325519",
    "end": "327199"
  },
  {
    "text": "The Ray docs present Ray\nand then you get an answer.",
    "start": "327769",
    "end": "330629"
  },
  {
    "text": "And also the Ray blog presents\nRay Serve as a framework for it.",
    "start": "330809",
    "end": "333749"
  },
  {
    "text": "And then, you get back an\nanswer from the Ray blog.",
    "start": "333754",
    "end": "335344"
  },
  {
    "text": "After you get back the sub answers\nto these sub questions over these",
    "start": "336229",
    "end": "339919"
  },
  {
    "text": "different data sources, you can now\nsynthesize this into a final response",
    "start": "339919",
    "end": "344329"
  },
  {
    "text": "to present to the user that actually\nanswers the original question.",
    "start": "344329",
    "end": "347449"
  },
  {
    "text": "And the high level idea here is that\nthe query engine, abstraction within",
    "start": "348289",
    "end": "352204"
  },
  {
    "text": "LlamaIndex actually allows you to define\nadvanced interactions such as this query",
    "start": "352204",
    "end": "356974"
  },
  {
    "text": "decomposition over different document\nsources and multi-step interaction between",
    "start": "356974",
    "end": "362014"
  },
  {
    "text": "a retrieval model as well as the LLM to\nsynthesize a response to a task at hand.",
    "start": "362019",
    "end": "366364"
  },
  {
    "start": "366000",
    "end": "569000"
  },
  {
    "text": "Okay, so now let's take a look at what\nthis example looks like in action.",
    "start": "367164",
    "end": "370014"
  },
  {
    "text": "I'll be running this on the Anyscale\nplatform, but you could also do this",
    "start": "370554",
    "end": "374094"
  },
  {
    "text": "full example on open source only.",
    "start": "374099",
    "end": "375834"
  },
  {
    "text": "you can take a look at the blog\npost as well as the public repo for",
    "start": "376024",
    "end": "378934"
  },
  {
    "text": "the full details of the example.",
    "start": "378934",
    "end": "380404"
  },
  {
    "text": "So as we talked about, the first\nstep in this process is how we can",
    "start": "381154",
    "end": "383613"
  },
  {
    "text": "take our two data sources, the Ray\ndocumentation and the Anyscale blog",
    "start": "383614",
    "end": "387014"
  },
  {
    "text": "posts, and then create a pipeline\nto load those documents parse them,",
    "start": "387034",
    "end": "392268"
  },
  {
    "text": "generate embeddings for them and store\nthose embeddings into a vector store.",
    "start": "392869",
    "end": "395899"
  },
  {
    "text": "So we have a simple script for that\nwhere we're using the LlamaIndex",
    "start": "396739",
    "end": "400319"
  },
  {
    "text": "parsing, as well as, embedding\nabstractions and then using Ray",
    "start": "400339",
    "end": "403459"
  },
  {
    "text": "Datasets to scale out this process.",
    "start": "403459",
    "end": "405529"
  },
  {
    "text": "So we define some few functions to\nload and parse the files, convert",
    "start": "405829",
    "end": "409519"
  },
  {
    "text": "those documents into LlamaIndex\nnode abstractions, and then use a",
    "start": "409524",
    "end": "413179"
  },
  {
    "text": "local Hugging Face model to generate\nembeddings for these documents.",
    "start": "413179",
    "end": "416268"
  },
  {
    "text": "And then finally what we can\ndo is then stitch together all",
    "start": "416884",
    "end": "419824"
  },
  {
    "text": "this logic using Ray Datasets.",
    "start": "419824",
    "end": "421464"
  },
  {
    "text": "and by doing this, we can scale out\nthe whole processing, computation",
    "start": "421934",
    "end": "425423"
  },
  {
    "text": "across the resources, the CPUs\nand the GPUs of our cluster.",
    "start": "425424",
    "end": "428274"
  },
  {
    "text": "this allows us to parse all the documents\nin parallel, for much better performance.",
    "start": "428854",
    "end": "433514"
  },
  {
    "text": "Once we do that, we can store\nthe, Ray documentation and",
    "start": "434294",
    "end": "437054"
  },
  {
    "text": "embeddings into a vector store.",
    "start": "437054",
    "end": "438284"
  },
  {
    "text": "And then we wanna repeat the same\nsteps for the Anyscale blogs.",
    "start": "438614",
    "end": "441464"
  },
  {
    "text": "So we again, stitch\ntogether all the logic.",
    "start": "441644",
    "end": "443564"
  },
  {
    "text": "And then persist the Anyscale blog vector\nstore on our cluster so I can run this.",
    "start": "443999",
    "end": "449139"
  },
  {
    "text": "And as you can see, it'll start.",
    "start": "449939",
    "end": "452569"
  },
  {
    "text": "Churning away at our docs.",
    "start": "453214",
    "end": "455673"
  },
  {
    "text": "Cool.",
    "start": "456504",
    "end": "456894"
  },
  {
    "text": "And as we can see, we're making\nprogress, on parsing and creating",
    "start": "457164",
    "end": "460694"
  },
  {
    "text": "embeddings for our documentation.",
    "start": "460694",
    "end": "462254"
  },
  {
    "text": "Okay.",
    "start": "463054",
    "end": "463233"
  },
  {
    "text": "So now that we've embedded, both our\ndocumentation and blog posts and stored",
    "start": "463234",
    "end": "466344"
  },
  {
    "text": "them into vector stores, Now let's see\nhow we can create our LLM application.",
    "start": "466344",
    "end": "471084"
  },
  {
    "text": "So again, we use both LlamaIndex and Ray.",
    "start": "471884",
    "end": "474224"
  },
  {
    "text": "LlamaIndex provides the abstractions\nfor defining our query engines.",
    "start": "474414",
    "end": "477294"
  },
  {
    "text": "And in particular here, we're using\nRay Serve to create our deployment",
    "start": "477384",
    "end": "480744"
  },
  {
    "text": "that we can then query later.",
    "start": "480954",
    "end": "482094"
  },
  {
    "text": "So if we take a look at this, we\nhave two different, query engines.",
    "start": "482894",
    "end": "486304"
  },
  {
    "text": "One for the Ray documentation\nand one for the blog post.",
    "start": "486304",
    "end": "488284"
  },
  {
    "text": "And then we also create a third query\nengine, which as we showed earlier,",
    "start": "488284",
    "end": "491384"
  },
  {
    "text": "is a sub-question query engine.",
    "start": "491384",
    "end": "492854"
  },
  {
    "text": "and this one can intelligently\ncome up with sub questions that you",
    "start": "493179",
    "end": "495939"
  },
  {
    "text": "want to ask for each data source.",
    "start": "495939",
    "end": "497439"
  },
  {
    "text": "So this is all done via LlamaIndex.",
    "start": "498239",
    "end": "500499"
  },
  {
    "text": "And then finally, we wrap\neverything in a Serve deployment.",
    "start": "500519",
    "end": "503879"
  },
  {
    "text": "And to actually run this, what we\ncan do is, deploy this application.",
    "start": "504149",
    "end": "508189"
  },
  {
    "text": "So this will create, replicas,\nconsisting of our, application.",
    "start": "508989",
    "end": "512259"
  },
  {
    "text": "And once this is started, then we\ncan now query this to get responses.",
    "start": "512309",
    "end": "516119"
  },
  {
    "text": "Okay.",
    "start": "516919",
    "end": "517129"
  },
  {
    "text": "So now that we've deployed\nour Ray Serve application",
    "start": "517129",
    "end": "519049"
  },
  {
    "text": "successfully, we can now query  it.",
    "start": "519049",
    "end": "521143"
  },
  {
    "text": "So we have this query script,\nwhich will send it a request.",
    "start": "521173",
    "end": "523813"
  },
  {
    "text": "we'll tell it to use a sub-question\nquery engine, and then let's",
    "start": "524573",
    "end": "527393"
  },
  {
    "text": "ask it, to compare and contrast.",
    "start": "527393",
    "end": "529383"
  },
  {
    "text": "How the Ray docs and Ray\nblog posts present serve.",
    "start": "530183",
    "end": "536683"
  },
  {
    "text": "This will then generate two sub-questions.",
    "start": "537483",
    "end": "539943"
  },
  {
    "text": "so it's gonna first ask the Ray\ndocs engine,  how does the Ray",
    "start": "539998",
    "end": "543798"
  },
  {
    "text": "documentation  present Ray Serve?",
    "start": "543798",
    "end": "545178"
  },
  {
    "text": "Then get a response.",
    "start": "545498",
    "end": "546518"
  },
  {
    "text": "Then it's gonna ask the blogs engine,\nhow does the Ray blogs present Serve.",
    "start": "546878",
    "end": "551597"
  },
  {
    "text": "Get a response.",
    "start": "551598",
    "end": "552138"
  },
  {
    "text": "Then finally, we'll synthesize\nthese two to provide some",
    "start": "552143",
    "end": "554448"
  },
  {
    "text": "overall response closer asked.",
    "start": "554448",
    "end": "556868"
  },
  {
    "text": "So finished with one sub-question.",
    "start": "556898",
    "end": "558518"
  },
  {
    "text": "Now it's gonna do the second one.",
    "start": "558878",
    "end": "560048"
  },
  {
    "text": "Then finally we'll\nsynthesize the response.",
    "start": "560448",
    "end": "562223"
  },
  {
    "text": "And as you can see, we got the\noverall response, as well as the",
    "start": "563023",
    "end": "566993"
  },
  {
    "text": "two sub questions that were asked.",
    "start": "566993",
    "end": "568313"
  },
  {
    "start": "569000",
    "end": "618000"
  },
  {
    "text": "So in conclusion, there's four main\nchallenges in being able to build",
    "start": "569113",
    "end": "572173"
  },
  {
    "text": "an LLM application over your data.",
    "start": "572233",
    "end": "574152"
  },
  {
    "text": "The first is being able to index\nover different data sources.",
    "start": "574633",
    "end": "577753"
  },
  {
    "text": "The second is being able to create\ncomplex queries that act over",
    "start": "578113",
    "end": "581472"
  },
  {
    "text": "these different data sources.",
    "start": "581473",
    "end": "582463"
  },
  {
    "text": "The third is actually being able to scale\nindexing to a large array of documents.",
    "start": "583198",
    "end": "587518"
  },
  {
    "text": "And the fourth is being able to\nactually deploy these production",
    "start": "587818",
    "end": "590968"
  },
  {
    "text": "quality query engines to production.",
    "start": "590968",
    "end": "592738"
  },
  {
    "text": "And as we saw, LlamaIndex provides the\nabstractions to solve the first two",
    "start": "593488",
    "end": "596938"
  },
  {
    "text": "and how you can index from different\ndata sources and be able to build query",
    "start": "596968",
    "end": "601258"
  },
  {
    "text": "engines to handle complex queries.",
    "start": "601258",
    "end": "602758"
  },
  {
    "text": "And Ray in particular, Ray datasets can\nhelp scaling out the ingestion pipeline.",
    "start": "602968",
    "end": "607133"
  },
  {
    "text": "And Ray Serve can help deploying\nyour LLM application in production.",
    "start": "607133",
    "end": "610514"
  },
  {
    "text": "Thanks so much for coming and if\nyou want to learn more, please check",
    "start": "610514",
    "end": "614113"
  },
  {
    "text": "out the blog as well as if you wanna\nlearn more about LlamaIndex, please",
    "start": "614114",
    "end": "617294"
  },
  {
    "text": "check out our GitHub and docs.",
    "start": "617294",
    "end": "618674"
  }
]