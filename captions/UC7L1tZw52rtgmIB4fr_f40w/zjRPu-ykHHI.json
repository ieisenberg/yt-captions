[
  {
    "text": "all right thanks to everybody for coming to the talk we're almost at the end of",
    "start": "6680",
    "end": "14219"
  },
  {
    "text": "the day here but hopefully this talk is a deep dive and that can help you to bring the Journey of taking a look at",
    "start": "14219",
    "end": "20520"
  },
  {
    "text": "how race serve and record and high availability all work together why don't",
    "start": "20520",
    "end": "26820"
  },
  {
    "text": "we get started so uh again welcome to the talk highly available architecture for online serving in Rey so this talk",
    "start": "26820",
    "end": "34559"
  },
  {
    "text": "will be delivered by me Simon and E later and in particular our agenda today",
    "start": "34559",
    "end": "40079"
  },
  {
    "text": "is going to be pretty brief we're going to just start with some brief introduction and context and then we're",
    "start": "40079",
    "end": "45120"
  },
  {
    "text": "gonna straight up talk to you about the Improvement in 2.0 exactly what we did and then it's going to be all the",
    "start": "45120",
    "end": "51660"
  },
  {
    "text": "sequence of walkthrough about architecture failure scenario and others so yeah make sure you are paying",
    "start": "51660",
    "end": "59160"
  },
  {
    "text": "attention and following along so to start uh who are we so uh e will be",
    "start": "59160",
    "end": "64978"
  },
  {
    "text": "coming up later works in the recording I'm mostly working on the global control store component in Ray core as well as",
    "start": "64979",
    "end": "72180"
  },
  {
    "text": "the main scheduler component in your core it also helped author Ray workflow Library which you might have seen before",
    "start": "72180",
    "end": "79439"
  },
  {
    "text": "and then I'm Simon it is a reserve team mostly I've been building on this machine learning model serving library",
    "start": "79439",
    "end": "85080"
  },
  {
    "text": "on top of Ray cool let me just start with a quick todr of",
    "start": "85080",
    "end": "90240"
  },
  {
    "text": "what is reserve race serve is optimized for flexible scalable and efficient",
    "start": "90240",
    "end": "95340"
  },
  {
    "text": "compute in particular for the online inference scenario and what I mean by that is that the cases where you finish",
    "start": "95340",
    "end": "101159"
  },
  {
    "text": "training your model and when you want to put it in production for live interactive online querying and in",
    "start": "101159",
    "end": "107040"
  },
  {
    "text": "particular by building on top of Ray we have the property of being scalable and delivery requesting a low latency manner",
    "start": "107040",
    "end": "113820"
  },
  {
    "text": "as well as the ability to pack things together such that they're delivered inefficient and cost saving manner",
    "start": "113820",
    "end": "120240"
  },
  {
    "text": "and our library is optimized for this kind of low Mochi model composition use",
    "start": "120240",
    "end": "126000"
  },
  {
    "text": "case in particular cases where you have a request come in and you should put it through pre-process model inference and",
    "start": "126000",
    "end": "132840"
  },
  {
    "text": "post process this kind of chain workload as well as an ensembling workload where you have model command to send to",
    "start": "132840",
    "end": "138720"
  },
  {
    "text": "multiple model let you combines out their output together and lastly by the nature of integrating",
    "start": "138720",
    "end": "145800"
  },
  {
    "text": "with Ray I'm building on top of Ray we have direct we support arbitrary python code this means within Ray serve you can",
    "start": "145800",
    "end": "152520"
  },
  {
    "text": "integrate your business logic together like holding a database or call sending a web API call as well as performing",
    "start": "152520",
    "end": "159120"
  },
  {
    "text": "model inference and with all this coming together we are essentially we believe the most optimized framework for",
    "start": "159120",
    "end": "166019"
  },
  {
    "text": "multimodal inference and I'm just putting up here a very quick hello world example of reserve and",
    "start": "166019",
    "end": "171959"
  },
  {
    "text": "what the API looks like for this talk we don't need to understand it deeply and all we need to know here is what we need",
    "start": "171959",
    "end": "177900"
  },
  {
    "text": "to do here is Define some little python class for request handling logic it can be composed of one model or many",
    "start": "177900",
    "end": "183300"
  },
  {
    "text": "different models and then you just need to deploy the model and once you have deployed this to the ray cluster you can",
    "start": "183300",
    "end": "189360"
  },
  {
    "text": "start calling it using http okay so let's take a look a little bit",
    "start": "189360",
    "end": "194519"
  },
  {
    "text": "about why it takes to put this kind of online HTTP interactive service in",
    "start": "194519",
    "end": "199680"
  },
  {
    "text": "production so of course for this any source of serving uh scenario we need to make sure",
    "start": "199680",
    "end": "206340"
  },
  {
    "text": "that it is high throughput it has high throughput and delivers low living c as well as make sure the system is",
    "start": "206340",
    "end": "212099"
  },
  {
    "text": "observable such that you know what's going on at any given time and when anything goes wrong you should be able",
    "start": "212099",
    "end": "218159"
  },
  {
    "text": "to figure out what's going on and debug it but of course the most important Point here is to make sure is available",
    "start": "218159",
    "end": "225019"
  },
  {
    "text": "in particular what we mean by that is we need to make sure the system should continue to function and I as in serving",
    "start": "225019",
    "end": "232319"
  },
  {
    "text": "incoming request traffic without significant dropping error rate and this needs to happen even during the cases",
    "start": "232319",
    "end": "239099"
  },
  {
    "text": "where any components goes down so this is a requirement to make sure that when",
    "start": "239099",
    "end": "245220"
  },
  {
    "text": "in any scenario once you make sure the service continues to function such that",
    "start": "245220",
    "end": "250739"
  },
  {
    "text": "our user can continue to call these uh certain live online inference services so of course this component is very",
    "start": "250739",
    "end": "257760"
  },
  {
    "text": "Loosely defined here it can be involving for example application error for example the machine learning model might",
    "start": "257760",
    "end": "263699"
  },
  {
    "text": "goes out of memory right it can maybe a sec fault if some you might run into that of course it could also be part in",
    "start": "263699",
    "end": "270360"
  },
  {
    "text": "the internal components right here it could be reserved system component malfunction or record component went",
    "start": "270360",
    "end": "276060"
  },
  {
    "text": "down and in addition either since with our control for example for external dependency like a node is going through",
    "start": "276060",
    "end": "282540"
  },
  {
    "text": "a kernel upgrade as well as no going down or AZ becoming AWS availability",
    "start": "282540",
    "end": "287820"
  },
  {
    "text": "Zone becoming unavailable or even just some random Network jittering or failure",
    "start": "287820",
    "end": "293340"
  },
  {
    "text": "so to break this down like this three challenges are Huawei face and like well",
    "start": "293340",
    "end": "300139"
  },
  {
    "text": "essentially lead us to the compromise of the Avail availability throughout the",
    "start": "300139",
    "end": "305880"
  },
  {
    "text": "time and in raid 2.0 we made a lot of advancement to solve these one by one",
    "start": "305880",
    "end": "312540"
  },
  {
    "text": "for application error this is solved by a lot of improvement in the reserve layer to actively house check and",
    "start": "312540",
    "end": "318540"
  },
  {
    "text": "restart for the internal component failure we are doing this on a case-by-case basis in particular a very",
    "start": "318540",
    "end": "325199"
  },
  {
    "text": "important component is a global control store GCS used to be the single point of failure when you scale when it went down",
    "start": "325199",
    "end": "331860"
  },
  {
    "text": "the entire rate cluster will go down but now by fixing it by by improving it with",
    "start": "331860",
    "end": "338580"
  },
  {
    "text": "the gcs4 tolerance feature when it goes down now the other components will",
    "start": "338580",
    "end": "343800"
  },
  {
    "text": "continue to function it will dive into more about this later on and then of course there can be dependency issue",
    "start": "343800",
    "end": "349860"
  },
  {
    "text": "with node crashing and others and this is solved by the brand new Ray cubray operator as well as a reserve component",
    "start": "349860",
    "end": "357539"
  },
  {
    "text": "within it to operate the multi-node cluster so in the end what we deliver here is",
    "start": "357539",
    "end": "363600"
  },
  {
    "text": "essentially in a long-running workload testing by every few minutes killing a",
    "start": "363600",
    "end": "369360"
  },
  {
    "text": "random node regardless worker or head node and then just to crash the component while we're consistently",
    "start": "369360",
    "end": "375240"
  },
  {
    "text": "sending traffic requests we're able to achieve four nights of uptime and again we'll be diving through exactly how to",
    "start": "375240",
    "end": "382020"
  },
  {
    "text": "achieve this later on and then before I hand it over to you I want to give you some a very quick slide",
    "start": "382020",
    "end": "388740"
  },
  {
    "text": "about exactly the architecture of what we're working with here so we're working",
    "start": "388740",
    "end": "393960"
  },
  {
    "text": "with a very much layered architecture so at the most bottom level we'll have a qurani cluster this is a go-to place for",
    "start": "393960",
    "end": "400680"
  },
  {
    "text": "you to run one or many Ray cluster on top and then on the kubernetes cluster you",
    "start": "400680",
    "end": "406740"
  },
  {
    "text": "can see there can be the kubernetes nodes as well as on top of each node there can be many different parts right",
    "start": "406740",
    "end": "413220"
  },
  {
    "text": "this is a standard kubernetes architecture so the layer here we have a cubery operator that will be able to create and",
    "start": "413220",
    "end": "420419"
  },
  {
    "text": "manage multiple rig cluster each recluster is composed of different pods and within each part there is a",
    "start": "420419",
    "end": "427680"
  },
  {
    "text": "system components like Ray core components like GCS which ones there's only one copy of it on every single Ray",
    "start": "427680",
    "end": "434220"
  },
  {
    "text": "cluster as well as raylet which is managed scheduling and worker responsible for bringing up worker and",
    "start": "434220",
    "end": "441539"
  },
  {
    "text": "shutting down workers on all nodes and then on top of that race serve has component called controller to manage",
    "start": "441539",
    "end": "448740"
  },
  {
    "text": "all the other actors just similar to GCS functionality as well as the model replica which is hosting your say",
    "start": "448740",
    "end": "455520"
  },
  {
    "text": "tensorflow Pi switch model or human business logic and then on each node Ray serve also has a component called HD",
    "start": "455520",
    "end": "462360"
  },
  {
    "text": "proxy to manage the network Ingress and Route the traffic to different models in that way",
    "start": "462360",
    "end": "468419"
  },
  {
    "text": "and again as you can see here what we're working with is very much three different layers each layers can have",
    "start": "468419",
    "end": "474599"
  },
  {
    "text": "many components so race serve is responsible for managing your your application code I will have a component",
    "start": "474599",
    "end": "481440"
  },
  {
    "text": "called controller and an HTTP proxy Ray core has a component called Ray LED for scheduling and GCS for metadata",
    "start": "481440",
    "end": "488160"
  },
  {
    "text": "management as well as chromatic responsible for bringing up the node and managing different cluster and the case",
    "start": "488160",
    "end": "494220"
  },
  {
    "text": "here is basically any component here can go down at any time and want to make",
    "start": "494220",
    "end": "499440"
  },
  {
    "text": "sure the application is still healthy and available and again this is a layering",
    "start": "499440",
    "end": "504599"
  },
  {
    "text": "architecture diagram we'll be working with today where we'll think about what happened when Reserve goes down recall",
    "start": "504599",
    "end": "510599"
  },
  {
    "text": "goes down corvandis go down and at this point our transition to e to take it through the Journey of what happened",
    "start": "510599",
    "end": "517080"
  },
  {
    "text": "when each component failed one by one right",
    "start": "517080",
    "end": "521839"
  },
  {
    "text": "thanks Simon for the introduction and all the contacts here next we will just stick deep into each of the components",
    "start": "523500",
    "end": "530700"
  },
  {
    "text": "here and check how can recover in case of any kind of video whether I do it by checking the reserve",
    "start": "530700",
    "end": "537120"
  },
  {
    "text": "layer first and then we go to the record layer and we also check the loads video in the kubernetes name",
    "start": "537120",
    "end": "544019"
  },
  {
    "text": "okay cool before we get started let's just try to deploy a server application",
    "start": "544019",
    "end": "549120"
  },
  {
    "text": "in the cluster so we call the serve.star right then what happened",
    "start": "549120",
    "end": "555540"
  },
  {
    "text": "firstly the GCS will try to schedule the server controller by asking one of the",
    "start": "555540",
    "end": "560640"
  },
  {
    "text": "delete and then the server controller will be scheduled and once the server controller started",
    "start": "560640",
    "end": "567300"
  },
  {
    "text": "the server controller will try to schedule the HTTP proxy on each of the",
    "start": "567300",
    "end": "572640"
  },
  {
    "text": "nodes in this request then to do this server controller will",
    "start": "572640",
    "end": "577860"
  },
  {
    "text": "ask Jesus like I want to schedule the HD epoxy on each of Juliet then she says",
    "start": "577860",
    "end": "583019"
  },
  {
    "text": "we'll just schedule this HD proxy by asking the release then the delete will just start this HD",
    "start": "583019",
    "end": "589800"
  },
  {
    "text": "proxy then we have HD proxy deployed on each of this nodes in the request",
    "start": "589800",
    "end": "595620"
  },
  {
    "text": "cool now we have the third cluster running an app and then we want to deploy some deck",
    "start": "595620",
    "end": "602700"
  },
  {
    "text": "here that's just to try to deploy this simple",
    "start": "602700",
    "end": "608640"
  },
  {
    "text": "deck this is the attack with three models it has M1 M2 and M3 the input of",
    "start": "608640",
    "end": "615899"
  },
  {
    "text": "M3 will be the output of M1 and M2 so very simple model",
    "start": "615899",
    "end": "622200"
  },
  {
    "text": "so once we run the serv.1 this deployment request will be sent to the",
    "start": "622200",
    "end": "627480"
  },
  {
    "text": "server controller first once the server controller receives this request it will first checkpointed",
    "start": "627480",
    "end": "634019"
  },
  {
    "text": "inside of the storage this is important to do because in case of server controls",
    "start": "634019",
    "end": "640140"
  },
  {
    "text": "video you need to be able to load like what's the thing is deployed in this",
    "start": "640140",
    "end": "645360"
  },
  {
    "text": "cluster and it can recover itself we will cover this part later when we check about the server controllers video",
    "start": "645360",
    "end": "652380"
  },
  {
    "text": "and then after the serve controller checkpoint the deployments in the",
    "start": "652380",
    "end": "657899"
  },
  {
    "text": "storage then the subconscious will just try to schedule these models in the cluster so",
    "start": "657899",
    "end": "664680"
  },
  {
    "text": "the server culture will ask Jesus just like to actually epoxy and then the GCS",
    "start": "664680",
    "end": "669839"
  },
  {
    "text": "will schedule them in the release and eventually we will have something like this we have a deck driver which is the",
    "start": "669839",
    "end": "676200"
  },
  {
    "text": "ancient point for this model and then we have M1 and M2 and M3 which is the",
    "start": "676200",
    "end": "682019"
  },
  {
    "text": "models we deployed cool now suppose now we have a request coming",
    "start": "682019",
    "end": "687839"
  },
  {
    "text": "to the reserve it will first go to the HT proxy and then HT proxy will pick one deck",
    "start": "687839",
    "end": "695760"
  },
  {
    "text": "driver and this deck driver will execute this stack so it will just execute this tag like",
    "start": "695760",
    "end": "702480"
  },
  {
    "text": "this it will send the input to model 1 module and send the output of model 1",
    "start": "702480",
    "end": "708180"
  },
  {
    "text": "multitude models so by the way all of this are done in top of three so data",
    "start": "708180",
    "end": "714360"
  },
  {
    "text": "pattern here is done by the object so there is no copy then here it means that that driver will",
    "start": "714360",
    "end": "721800"
  },
  {
    "text": "use M1 m2m3 to execute the stack okay cool",
    "start": "721800",
    "end": "727320"
  },
  {
    "text": "now we have a department like this next let's just check the cell failure",
    "start": "727320",
    "end": "732839"
  },
  {
    "text": "cases like check the failure in the server layer first in the server layer there are three",
    "start": "732839",
    "end": "738300"
  },
  {
    "text": "kinds of failure one is the application failure like if there is some bug in the user code or",
    "start": "738300",
    "end": "744420"
  },
  {
    "text": "the user's model just failed due to something like the load video or something the other one is the controller failure a self-controlled",
    "start": "744420",
    "end": "751079"
  },
  {
    "text": "field as the last one is the St toxic let's just check them one by one",
    "start": "751079",
    "end": "756300"
  },
  {
    "text": "let's first check the serve application failure assume here M3 field maybe there",
    "start": "756300",
    "end": "762959"
  },
  {
    "text": "is the bug like the memory needs eventually M3 and I've got out of memory and it failed what will happen how can",
    "start": "762959",
    "end": "769740"
  },
  {
    "text": "we cover the video from this case choose recover any kind of failure we",
    "start": "769740",
    "end": "774779"
  },
  {
    "text": "first need to load there is the video to know that serve controller will send",
    "start": "774779",
    "end": "780720"
  },
  {
    "text": "the health check to the deck driver M1 M2 and M3 and then in case of any kind",
    "start": "780720",
    "end": "787800"
  },
  {
    "text": "of video self-controllable loaded and then once the server culture load okay M3 field the server control will try to",
    "start": "787800",
    "end": "795839"
  },
  {
    "text": "schedule the MC just like what we discussed at the beginning like the self-conscious schedule the whole models",
    "start": "795839",
    "end": "802800"
  },
  {
    "text": "it will ask Jesus and Jesus asked values and MC will be scheduled in the request we have MC back so this is how the serve",
    "start": "802800",
    "end": "811920"
  },
  {
    "text": "controller covers the application layer of it okay cool then how about the",
    "start": "811920",
    "end": "819060"
  },
  {
    "text": "subconscious what is the server controller field right actually to understand how server",
    "start": "819060",
    "end": "826380"
  },
  {
    "text": "controller can recover it's very important to know what serve controller like Android",
    "start": "826380",
    "end": "832560"
  },
  {
    "text": "because actually 3 will be the place which is responsible to recover the server control",
    "start": "832560",
    "end": "839339"
  },
  {
    "text": "usually serve controller is implemented as a detached actor",
    "start": "839339",
    "end": "844440"
  },
  {
    "text": "at the text reactor is the actor that will live until the three cluster being",
    "start": "844440",
    "end": "850139"
  },
  {
    "text": "shut down all this actor been deleted explicitly by the user",
    "start": "850139",
    "end": "856100"
  },
  {
    "text": "for the detached actor she says will just check the health of this detached",
    "start": "856500",
    "end": "861660"
  },
  {
    "text": "actor like by doing the health check periodically and then if this detects",
    "start": "861660",
    "end": "867120"
  },
  {
    "text": "act field Jesus will just try to reschedule this detection actor in sound really",
    "start": "867120",
    "end": "873060"
  },
  {
    "text": "and then once it's a scheduling some delete this server controller will be up and as we mentioned at the beginning the",
    "start": "873060",
    "end": "880199"
  },
  {
    "text": "server controller first checkpoint the deployments in the storage right and then when the result starts it will read",
    "start": "880199",
    "end": "886440"
  },
  {
    "text": "the deployment from the storage and it will try to recover it to the status",
    "start": "886440",
    "end": "891600"
  },
  {
    "text": "before it failed so this is how serve controller got recovered very simple and",
    "start": "891600",
    "end": "898019"
  },
  {
    "text": "she says first use health check notice it failed because it's detached Vector Jesus will schedule them in some unit",
    "start": "898019",
    "end": "907100"
  },
  {
    "text": "load the data from the storage and recover cool",
    "start": "907160",
    "end": "913380"
  },
  {
    "text": "then the last component in the reserve layer is actually epoxy the same as the conserve controller to",
    "start": "913380",
    "end": "919860"
  },
  {
    "text": "understand how HD policy you covered we need to know how httposite is",
    "start": "919860",
    "end": "925019"
  },
  {
    "text": "implemented on top of it HT proxy",
    "start": "925019",
    "end": "930120"
  },
  {
    "text": "is also a detached vector but the different part is that for HP proxy we want only one HT policy on each",
    "start": "930120",
    "end": "939000"
  },
  {
    "text": "of the nodes that's the difference compared with the server controller okay for the detached actor failure as",
    "start": "939000",
    "end": "946920"
  },
  {
    "text": "we mentioned for the server controller the G cells first will detect okay this",
    "start": "946920",
    "end": "952860"
  },
  {
    "text": "actually epoxy field like by doing the health check periodically and then the juices will just send the",
    "start": "952860",
    "end": "960420"
  },
  {
    "text": "scheduling of this HD proxy to the specific delete this HD proxy field",
    "start": "960420",
    "end": "966959"
  },
  {
    "text": "because we only want one HD process on each of these nodes right",
    "start": "966959",
    "end": "972000"
  },
  {
    "text": "okay cool that actually lost it will be back but there is something like different I",
    "start": "972000",
    "end": "978180"
  },
  {
    "text": "mean here compared with server controller let's think about another case what if you know this actually policy",
    "start": "978180",
    "end": "984959"
  },
  {
    "text": "was deployed just a gun it's possible like for example this node by another policy on the load also by",
    "start": "984959",
    "end": "993120"
  },
  {
    "text": "so when this happens Jesus will do the health check as we mentioned",
    "start": "993120",
    "end": "999120"
  },
  {
    "text": "and then Jesus will try to schedule it in South Village but the delete this",
    "start": "999120",
    "end": "1004279"
  },
  {
    "text": "HTTP proxy was scheduled actually has gone so we can we cannot schedule this HD proxy on other videos or other loads",
    "start": "1004279",
    "end": "1011839"
  },
  {
    "text": "because otherwise we will have too much epoxy there this is not what we want",
    "start": "1011839",
    "end": "1016880"
  },
  {
    "text": "right in case of this serve controller will notice there is",
    "start": "1016880",
    "end": "1022100"
  },
  {
    "text": "one actually positive pending to be scheduled expanding because it cannot be scheduled by the GCS",
    "start": "1022100",
    "end": "1028280"
  },
  {
    "text": "and then the server controller was just released that HT epoxy",
    "start": "1028280",
    "end": "1034339"
  },
  {
    "text": "because that's cheap oxy should be deployed on some loads which has gone so",
    "start": "1034339",
    "end": "1039558"
  },
  {
    "text": "we just delete that the load has gone right and then let's assume now the village",
    "start": "1039559",
    "end": "1045980"
  },
  {
    "text": "just come back now we have a new load being added to the cluster",
    "start": "1045980",
    "end": "1051320"
  },
  {
    "text": "and server controller will notice oh I have a new load and there is no HT epoxy deployed on that nodes",
    "start": "1051320",
    "end": "1057740"
  },
  {
    "text": "then the server controller will try to deploy a HD proxy on the Node by asking GCS and GCS were scheduled on that",
    "start": "1057740",
    "end": "1065360"
  },
  {
    "text": "delete and we have that she proxy back okay this is how we recover for the",
    "start": "1065360",
    "end": "1070700"
  },
  {
    "text": "actual proxy so here recovered all the cases for soft layers",
    "start": "1070700",
    "end": "1078200"
  },
  {
    "text": "failure and Melody is that to the cover itself failure it depends on a lot of",
    "start": "1078200",
    "end": "1084140"
  },
  {
    "text": "functionalities not provided by recall like we ask Jesus to schedule the actor she says financial ability to schedule",
    "start": "1084140",
    "end": "1090799"
  },
  {
    "text": "this actor so it's also it's also important like to cover the failure okay in the record",
    "start": "1090799",
    "end": "1097280"
  },
  {
    "text": "because otherwise it's really difficult or GPS field the server will not be able to come right",
    "start": "1097280",
    "end": "1103340"
  },
  {
    "text": "okay let's check the delete video case first suppose this will it failed",
    "start": "1103340",
    "end": "1109400"
  },
  {
    "text": "then the first question is that what will happen next so according to really all the workers",
    "start": "1109400",
    "end": "1117320"
  },
  {
    "text": "like the actors or reworks managed by this unit will feel if this really failed they are fit sharing with this",
    "start": "1117320",
    "end": "1123860"
  },
  {
    "text": "visit what does this mean it means all of the components here",
    "start": "1123860",
    "end": "1129200"
  },
  {
    "text": "could potentially fail if they are created by this Village here let's assume the tech driver M1 M2",
    "start": "1129200",
    "end": "1137780"
  },
  {
    "text": "and M3 field then Islamic similar to the first case",
    "start": "1137780",
    "end": "1143299"
  },
  {
    "text": "we talked about the application layer failure actually the recovery is also the same server controller will do the health",
    "start": "1143299",
    "end": "1149900"
  },
  {
    "text": "check load is revealed server controller will ask Jesus to the schedule them",
    "start": "1149900",
    "end": "1155360"
  },
  {
    "text": "serve controller sorry the Jesus will try to find some delete which has the capacity to run this M1 M2 and the M3",
    "start": "1155360",
    "end": "1163460"
  },
  {
    "text": "another attack driver and once they've been scheduled we are back so the",
    "start": "1163460",
    "end": "1169100"
  },
  {
    "text": "recovery for this case is very similar to what we have discussed at the beginning",
    "start": "1169100",
    "end": "1175780"
  },
  {
    "text": "for the Reddit here the recovery of this field really it will be covered by The",
    "start": "1177440",
    "end": "1182960"
  },
  {
    "text": "View operator which is an operator in Cuba in kubernetes",
    "start": "1182960",
    "end": "1188780"
  },
  {
    "text": "then real filter will notice okay I have a failed release it will just just try",
    "start": "1188780",
    "end": "1193820"
  },
  {
    "text": "to find some new loads and it will start a new release there and then we have a new release being",
    "start": "1193820",
    "end": "1200840"
  },
  {
    "text": "added to the cluster so enjoy once a load failed",
    "start": "1200840",
    "end": "1206840"
  },
  {
    "text": "it will not come back we can only add new loads to this cluster cool this is really",
    "start": "1206840",
    "end": "1215360"
  },
  {
    "text": "so how about the Jesus what if the Jesus failed",
    "start": "1215360",
    "end": "1221059"
  },
  {
    "text": "Jesus is very important in uh in here because it's supposed to schedule the",
    "start": "1221059",
    "end": "1227059"
  },
  {
    "text": "actors and also manage the notes in the clusters right if this is fair we are no",
    "start": "1227059",
    "end": "1232700"
  },
  {
    "text": "longer able to schedule any actors here Jesus actually is a stateful",
    "start": "1232700",
    "end": "1239360"
  },
  {
    "text": "components so to recover a failure for stateful components we need to store the",
    "start": "1239360",
    "end": "1245480"
  },
  {
    "text": "states of this this service to some places so that when we start the Jesus",
    "start": "1245480",
    "end": "1251120"
  },
  {
    "text": "we will be able to load the status from the storage back so here Jesus we are whenever when she",
    "start": "1251120",
    "end": "1258559"
  },
  {
    "text": "says update its internal status it will write the data into a an external High",
    "start": "1258559",
    "end": "1264080"
  },
  {
    "text": "available storage and then here are the three operator will first notice the GCS field",
    "start": "1264080",
    "end": "1271160"
  },
  {
    "text": "and it will start the Jesus on some of the nodes and she says start and she",
    "start": "1271160",
    "end": "1276740"
  },
  {
    "text": "says just try to load the data from the storage and GPS will recover",
    "start": "1276740",
    "end": "1282740"
  },
  {
    "text": "cool this is how she says recovery then what she says is back it will be",
    "start": "1282740",
    "end": "1288500"
  },
  {
    "text": "able to do to schedule actors the server will be able to do the config",
    "start": "1288500",
    "end": "1295539"
  },
  {
    "text": "nice so now we covered the serve this video record failure so how about the",
    "start": "1295760",
    "end": "1301700"
  },
  {
    "text": "load video right let's check this one so what will happen and when I load field it's very simple",
    "start": "1301700",
    "end": "1309380"
  },
  {
    "text": "all the components or services on that nodes were just gone right so here let's and also",
    "start": "1309380",
    "end": "1317720"
  },
  {
    "text": "here for each other component here if they failed it actually will just recover as what we have discussed",
    "start": "1317720",
    "end": "1324620"
  },
  {
    "text": "so for this one let's just use the cap let's think about very extreme case like all of these components here how can we",
    "start": "1324620",
    "end": "1331100"
  },
  {
    "text": "recover this very extreme video cases usually it will not happen because they were distributed across like different",
    "start": "1331100",
    "end": "1336799"
  },
  {
    "text": "nodes but here just let's assume of they are deployed in the same node once they need to be mentioned here that",
    "start": "1336799",
    "end": "1344059"
  },
  {
    "text": "even we have this kind of video served is still available why because the HT",
    "start": "1344059",
    "end": "1349700"
  },
  {
    "text": "proxy black driver M1 M2 and M3 they have replicas as long as we have",
    "start": "1349700",
    "end": "1356120"
  },
  {
    "text": "replicas there we will be able to solve traffic so service available",
    "start": "1356120",
    "end": "1362080"
  },
  {
    "text": "so how do we recover for this from this case firstly the three operator in",
    "start": "1362140",
    "end": "1367280"
  },
  {
    "text": "kubernetes will detect GSS field and also delete field let's assume and then the accumulate the",
    "start": "1367280",
    "end": "1374480"
  },
  {
    "text": "operator will try to schedule the Jesus and release in some nodes let's assume",
    "start": "1374480",
    "end": "1379820"
  },
  {
    "text": "here Jesus come back first just as well start here load the data from the storage and then Jesus is up",
    "start": "1379820",
    "end": "1387620"
  },
  {
    "text": "What's Left Right Jesus will just check the to the health check for detached vectors in this class which is actually",
    "start": "1387620",
    "end": "1394520"
  },
  {
    "text": "epoxy and serve controller try to reschedule them in sound really",
    "start": "1394520",
    "end": "1403940"
  },
  {
    "text": "and then server controller is back but not the HTTP proxy why because it loads",
    "start": "1403940",
    "end": "1409820"
  },
  {
    "text": "this Edge epoxy lives died there's no place for the G cell to schedule this Xbox GSS cannot find the field delete",
    "start": "1409820",
    "end": "1419179"
  },
  {
    "text": "so then the server controller will do the health check for the components here",
    "start": "1419179",
    "end": "1427600"
  },
  {
    "text": "m2m3 failed and also that's you foxy and we will try to schedule them past the",
    "start": "1428980",
    "end": "1434780"
  },
  {
    "text": "GCS this is the application layer video recovery and then she says rescheduled",
    "start": "1434780",
    "end": "1440299"
  },
  {
    "text": "in some Village and then they are back but how about is actually epoxy",
    "start": "1440299",
    "end": "1446559"
  },
  {
    "text": "she says Lord he's the laws is actually foxy deployed field so it will delete",
    "start": "1447440",
    "end": "1452960"
  },
  {
    "text": "this HD proxy and then let's assume this will it eventually",
    "start": "1452960",
    "end": "1459500"
  },
  {
    "text": "come back maybe just a slow due to the complex scheduling but eventually come back",
    "start": "1459500",
    "end": "1465799"
  },
  {
    "text": "and then here the server controller will notice there is one unit being added to the",
    "start": "1465799",
    "end": "1471679"
  },
  {
    "text": "cluster and for this delete there is no HD epoxy deployed there and we want each load has",
    "start": "1471679",
    "end": "1478520"
  },
  {
    "text": "one HD box so the server consider will ask the GCS to schedule this HD proxy to that widget",
    "start": "1478520",
    "end": "1485539"
  },
  {
    "text": "and then Jesus will schedule the X proxy to that delete directly and we have the",
    "start": "1485539",
    "end": "1491480"
  },
  {
    "text": "active oxy back now we will cover the phones a very extreme case of failure",
    "start": "1491480",
    "end": "1497780"
  },
  {
    "text": "okay nice now we have covered all the components",
    "start": "1497780",
    "end": "1503419"
  },
  {
    "text": "failure I think now it's very important for us to check what's available when the component failed",
    "start": "1503419",
    "end": "1510799"
  },
  {
    "text": "so first we need to be mentioned here that each of the component video it won't impact the serving because as long",
    "start": "1510799",
    "end": "1517880"
  },
  {
    "text": "as we have replicas like sub replicas and active P proxy object has been there",
    "start": "1517880",
    "end": "1523700"
  },
  {
    "text": "the server is able to serve traffic but you since also need to pay attention",
    "start": "1523700",
    "end": "1529279"
  },
  {
    "text": "here is that when server controller failed or G cells field we are not able to do deployment or we config of the",
    "start": "1529279",
    "end": "1536659"
  },
  {
    "text": "current deployment why because firstly as we save on the recovery Loop also the",
    "start": "1536659",
    "end": "1543980"
  },
  {
    "text": "first deployed the server deployment we introduced at the beginning self-controller is the place which is",
    "start": "1543980",
    "end": "1551659"
  },
  {
    "text": "during the deployments like subconscious driving the deployments so if serve controller failed we are no",
    "start": "1551659",
    "end": "1557960"
  },
  {
    "text": "longer able to deploy the server applications or through the config and",
    "start": "1557960",
    "end": "1563480"
  },
  {
    "text": "here server controller is back place which schedule the actors so if",
    "start": "1563480",
    "end": "1570320"
  },
  {
    "text": "Jesus failed we are no longer able to schedule actors entire respect so for this functionality it won't be available",
    "start": "1570320",
    "end": "1576679"
  },
  {
    "text": "until they're back okay cool now we have covered all the failure",
    "start": "1576679",
    "end": "1584120"
  },
  {
    "text": "cases and how can this recover let's just check the skills testing Simon mentioned at the beginning with more",
    "start": "1584120",
    "end": "1589820"
  },
  {
    "text": "details here so the setup so we deploy this deck",
    "start": "1589820",
    "end": "1596419"
  },
  {
    "text": "in every cluster which is managed by this service operator in Cuba",
    "start": "1596419",
    "end": "1603200"
  },
  {
    "text": "and for this week cluster each of the parts will have four CPUs",
    "start": "1603200",
    "end": "1609260"
  },
  {
    "text": "and we have one hard node and 12 workloads in total we're sending the traffic to this",
    "start": "1609260",
    "end": "1615799"
  },
  {
    "text": "deployment and in the meantime we also inject the failure actively by randomly",
    "start": "1615799",
    "end": "1622520"
  },
  {
    "text": "tuning a workload every minute and kill are handled every five minutes",
    "start": "1622520",
    "end": "1629059"
  },
  {
    "text": "radio Q new here because q a load will fill everything on that node so the",
    "start": "1629059",
    "end": "1634279"
  },
  {
    "text": "coverage is the best so this this test is a very heavy stress",
    "start": "1634279",
    "end": "1640100"
  },
  {
    "text": "test and then we get together results like this so uh here let's",
    "start": "1640100",
    "end": "1646520"
  },
  {
    "text": "first check about this uh red dashes there this means the Q happen I only",
    "start": "1646520",
    "end": "1651620"
  },
  {
    "text": "draw the six uh queuing here but the qna actually happens every minute",
    "start": "1651620",
    "end": "1659120"
  },
  {
    "text": "so we do the Q and then let's pay attention to the first part",
    "start": "1659120",
    "end": "1664720"
  },
  {
    "text": "the green one is the success qpf and the other one is third QPS",
    "start": "1664720",
    "end": "1670820"
  },
  {
    "text": "we can see that it's very stable the uh it's we have very high availability here",
    "start": "1670820",
    "end": "1677120"
  },
  {
    "text": "for the second part the the yellow one is the P99 latency",
    "start": "1677120",
    "end": "1682520"
  },
  {
    "text": "and the green one is the p50 then the p50 is very stable and for the P99 it's",
    "start": "1682520",
    "end": "1687620"
  },
  {
    "text": "a little bit bumpy because serve in case of failure servally the wire to detect that so for some requests it will just",
    "start": "1687620",
    "end": "1693620"
  },
  {
    "text": "run longer we got it for about four uh six hours and will achieve the four nice",
    "start": "1693620",
    "end": "1699919"
  },
  {
    "text": "availability cool so what's the next",
    "start": "1699919",
    "end": "1706120"
  },
  {
    "text": "next we will bring the actual architecture to J right now it's the alpha release so be ready for dog",
    "start": "1707179",
    "end": "1713419"
  },
  {
    "text": "fooding and please give us the feedback so we can bring it to GI and we also want to bring the actual architecture to",
    "start": "1713419",
    "end": "1719360"
  },
  {
    "text": "other libraries like training tuning audit processing currently it's tightly scope to support Reserve only last one",
    "start": "1719360",
    "end": "1727100"
  },
  {
    "text": "is about storage we run out the single point of video shifts to the activity",
    "start": "1727100",
    "end": "1732500"
  },
  {
    "text": "storage owned by you but in the future we probably will manage it for you so summary you enjoyed one error we",
    "start": "1732500",
    "end": "1739820"
  },
  {
    "text": "remove the single point of video from 1.0 server application only now can run",
    "start": "1739820",
    "end": "1745159"
  },
  {
    "text": "24 7 with high availability you can try it out today we appreciate your feedback",
    "start": "1745159",
    "end": "1750440"
  },
  {
    "text": "thank you everyone [Applause]",
    "start": "1750440",
    "end": "1757020"
  },
  {
    "text": "okay oh I I can also repeat the question",
    "start": "1771919",
    "end": "1778179"
  },
  {
    "text": "in the context of all of the other like architectural services and how does the",
    "start": "1780860",
    "end": "1787340"
  },
  {
    "text": "GCS know what services were created by the way so the question is uh what is the",
    "start": "1787340",
    "end": "1793520"
  },
  {
    "text": "purpose of re-let in the context of this architecture as well as what how does GCS know why Services that's really like",
    "start": "1793520",
    "end": "1800840"
  },
  {
    "text": "created right yeah okay yeah when it goes down yeah sure yeah so uh reality",
    "start": "1800840",
    "end": "1807500"
  },
  {
    "text": "is the component in record which manages the laws so relative will be responsible",
    "start": "1807500",
    "end": "1814039"
  },
  {
    "text": "to start the workers in this node running the actors in this now so you",
    "start": "1814039",
    "end": "1819919"
  },
  {
    "text": "have things really something like scheduling the task in that nose and then in in the context of serve so",
    "start": "1819919",
    "end": "1827720"
  },
  {
    "text": "serve the controllers on other departments the mlm2m3 I mentioned they",
    "start": "1827720",
    "end": "1833299"
  },
  {
    "text": "are all actors so do it will be the place which starts with actors",
    "start": "1833299",
    "end": "1838580"
  },
  {
    "text": "right that's yeah",
    "start": "1838580",
    "end": "1841898"
  },
  {
    "text": "yes that's one pillows here is the node in race point of view",
    "start": "1843620",
    "end": "1849980"
  },
  {
    "text": "it's the kubernetes Pod yeah yeah and then essentially we do have this kind of One agent per part no kind of",
    "start": "1849980",
    "end": "1857240"
  },
  {
    "text": "architecture as well yeah that's really it yeah and also for the two sets like",
    "start": "1857240",
    "end": "1862520"
  },
  {
    "text": "she says have a story there so it will store all of the information like I",
    "start": "1862520",
    "end": "1868520"
  },
  {
    "text": "start this actor in this know this kind of things in your storage so when Jesus failed and when she has the stuff it",
    "start": "1868520",
    "end": "1875179"
  },
  {
    "text": "will just load the information from the storage so that's how she says no what's the thing deployed in the cluster but in",
    "start": "1875179",
    "end": "1883340"
  },
  {
    "text": "terms of three uh self-services right resource service are actually are just",
    "start": "1883340",
    "end": "1889100"
  },
  {
    "text": "actors so Jesus load this information by storing them in the storage and loading",
    "start": "1889100",
    "end": "1894860"
  },
  {
    "text": "it back yeah nope any other question",
    "start": "1894860",
    "end": "1901419"
  },
  {
    "text": "does the mic still work or it works",
    "start": "1902000",
    "end": "1906820"
  },
  {
    "text": "great I don't have to yell um how do you find the availability of a",
    "start": "1907460",
    "end": "1913460"
  },
  {
    "text": "dag depends on its topology for instance uh if I have an ensemble where I have",
    "start": "1913460",
    "end": "1920059"
  },
  {
    "text": "like 100 different models feeding into my post-processing function I can imagine it's easier to cause a fault by",
    "start": "1920059",
    "end": "1927380"
  },
  {
    "text": "killing say one of the models right yeah uh so the availability in the end is",
    "start": "1927380",
    "end": "1933440"
  },
  {
    "text": "measured by the end-to-end request success rate so it doesn't really matter about the inch each individual",
    "start": "1933440",
    "end": "1939740"
  },
  {
    "text": "intermediate components and the way we help the availability right here is to",
    "start": "1939740",
    "end": "1945200"
  },
  {
    "text": "perform Ray tries at different level to make sure that any component here failed as part of the graph a serving graph if",
    "start": "1945200",
    "end": "1954559"
  },
  {
    "text": "any component fail we're doing some retry in the upper layer to make sure that we can't even do it the only thing",
    "start": "1954559",
    "end": "1960799"
  },
  {
    "text": "that will cause the sort of the 0.01 availability drop comes from like the cases where our rate try process itself",
    "start": "1960799",
    "end": "1967760"
  },
  {
    "text": "fail basically for example like the m2n like the HTTP TCP connection got killed like there's",
    "start": "1967760",
    "end": "1974659"
  },
  {
    "text": "no way you can do about it okay thank you",
    "start": "1974659",
    "end": "1979840"
  },
  {
    "text": "um do you find uh like from your work so far do you find that ha redis is",
    "start": "1981440",
    "end": "1987679"
  },
  {
    "text": "um do you feel like that that is like sort of the end of the road map as far",
    "start": "1987679",
    "end": "1992720"
  },
  {
    "text": "as the the storage engine for GCS State word are there other things that you've",
    "start": "1992720",
    "end": "1998419"
  },
  {
    "text": "considered or other storage engines that you might news in the future yeah sure so uh right",
    "start": "1998419",
    "end": "2005500"
  },
  {
    "text": "now uh they only support Jesus as the back end for the processing data but",
    "start": "2005500",
    "end": "2011380"
  },
  {
    "text": "it's uh but we plan to make it extendable so that in future not only",
    "start": "2011380",
    "end": "2016480"
  },
  {
    "text": "GCS will probably can have other kind of service there yeah we we use Jesus right",
    "start": "2016480",
    "end": "2022480"
  },
  {
    "text": "now because historically we run with Jesus right and so for the first step",
    "start": "2022480",
    "end": "2028779"
  },
  {
    "text": "it's much easier for us just to just reuse the G set but in the future we'll make it extensible and long in the end",
    "start": "2028779",
    "end": "2036340"
  },
  {
    "text": "we'll have a very nice apis defined so that you can just bring your own storage",
    "start": "2036340",
    "end": "2042760"
  },
  {
    "text": "and then Implement that API there or just like we will have some plugins so that you can just run with other",
    "start": "2042760",
    "end": "2048280"
  },
  {
    "text": "storages yeah in the end any key Value Store will work yes yeah",
    "start": "2048280",
    "end": "2054158"
  },
  {
    "text": "all right that's all the time we have four questions let's thank you and Simon thank you thank you",
    "start": "2054159",
    "end": "2059429"
  },
  {
    "text": "[Applause]",
    "start": "2059429",
    "end": "2063078"
  }
]