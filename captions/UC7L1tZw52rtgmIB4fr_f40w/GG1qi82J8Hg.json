[
  {
    "text": "my name is bingan I'm with IBM and I would like to present the work we did in the around the platform",
    "start": "4160",
    "end": "10679"
  },
  {
    "text": "portability I guess you have seen similar charts throughout the day that model sizes explode but also not only",
    "start": "10679",
    "end": "16840"
  },
  {
    "text": "model sizes explode also the gpus get larger and not only gpus get larger they",
    "start": "16840",
    "end": "22119"
  },
  {
    "text": "get even more diverse so we get new hardware they gets larger we get all kinds of accelerators you have seen many",
    "start": "22119",
    "end": "29000"
  },
  {
    "text": "many keyboards throughout today and now the question is how can we run these",
    "start": "29000",
    "end": "34360"
  },
  {
    "text": "large models on this diverse set of Hardware so this hog genous hardware and",
    "start": "34360",
    "end": "39520"
  },
  {
    "text": "to show you some numbers for this because for conferences like razor mid or distinguished crowd like you I would",
    "start": "39520",
    "end": "46320"
  },
  {
    "text": "like to have some numbers on my side I stole this uh curve from Hennessy and patteron some from their touring lecture",
    "start": "46320",
    "end": "54399"
  },
  {
    "text": "how you can achieve uh five orders of magnitude speed up for a simple Matrix",
    "start": "54399",
    "end": "59600"
  },
  {
    "text": "multi application by applying well-known optimization methods like Loop unrolling",
    "start": "59600",
    "end": "65360"
  },
  {
    "text": "and memory tiling and using Vector instructions and even on a somehow stupid CPU you need to take care of the",
    "start": "65360",
    "end": "72840"
  },
  {
    "text": "memory layout in order to achieve good performance so given that how can we",
    "start": "72840",
    "end": "79600"
  },
  {
    "text": "ensure that all these great models can be executed with the best possible performance on all Hardware without",
    "start": "79600",
    "end": "86079"
  },
  {
    "text": "reimplementing every model for every cach di for every GPU gener generation",
    "start": "86079",
    "end": "91240"
  },
  {
    "text": "again and that's why I would like to take you on a journey to Portable and fast llm inference and I will start with",
    "start": "91240",
    "end": "98600"
  },
  {
    "text": "some background around VM and Triton then I will introduce you the concept of",
    "start": "98600",
    "end": "104320"
  },
  {
    "text": "autotuning because autotuning is necessary to achieve platform portability however autot tuning has",
    "start": "104320",
    "end": "110280"
  },
  {
    "text": "some problems in practice and I will tell you how to overcome these problems in",
    "start": "110280",
    "end": "115799"
  },
  {
    "text": "practice and finally in roughly 23 minutes I will show you some results I",
    "start": "115799",
    "end": "121880"
  },
  {
    "text": "will demonstrate what I did and hopefully we arrive together at our conclusion my goal in this presentation",
    "start": "121880",
    "end": "128399"
  },
  {
    "text": "is to sketch you the future of performance and platform portability for large language model inference using VM",
    "start": "128399",
    "end": "134519"
  },
  {
    "text": "and Triton and somehow inofficial make you all fans of our introduced framework",
    "start": "134519",
    "end": "140040"
  },
  {
    "text": "Triton dja V which I will present later so in case you didn't hear and you were",
    "start": "140040",
    "end": "147680"
  },
  {
    "text": "the rest of the day somewhere else and not in the V track VM is a state-of-the-art uh framework for model",
    "start": "147680",
    "end": "154560"
  },
  {
    "text": "inference and the detail that is important for this talk is that the state-of-the-art performance comes from",
    "start": "154560",
    "end": "160760"
  },
  {
    "text": "custom model code um and low level optimizations with in this custom model",
    "start": "160760",
    "end": "165879"
  },
  {
    "text": "code so in the earlier talk from uh about the state of VM there was a list",
    "start": "165879",
    "end": "172200"
  },
  {
    "text": "of all the kernels say hand tuned to get the best performance some of them are in",
    "start": "172200",
    "end": "177319"
  },
  {
    "text": "pyot some of them are in Cuda however however if you have hand tuned if your performance depends on hand tuned",
    "start": "177319",
    "end": "184000"
  },
  {
    "text": "kernels then you have by definition limited portability besides these pyto and Cuda",
    "start": "184000",
    "end": "190959"
  },
  {
    "text": "kernels some of the kernels are also written in in open a Triton which I will introduce in a second and if we look at",
    "start": "190959",
    "end": "198159"
  },
  {
    "text": "this architecture the question comes to mind how can we ensure that all these hand tuned kernels achieve the best",
    "start": "198159",
    "end": "204680"
  },
  {
    "text": "possible performance on all platforms and what is this opening eye Triton",
    "start": "204680",
    "end": "210640"
  },
  {
    "text": "and in this presentation I will Al share with you how to do",
    "start": "210640",
    "end": "215799"
  },
  {
    "text": "that so next introd introduction in case you didn't hear of it Triton is a domain",
    "start": "215799",
    "end": "222280"
  },
  {
    "text": "specific language uh developed by a PhD uh thees in Harvard and then the first",
    "start": "222280",
    "end": "228720"
  },
  {
    "text": "author joined open Ai and they maintained it as open source code that's why it's called open AI Triton to",
    "start": "228720",
    "end": "234480"
  },
  {
    "text": "differen it from Nidia Triton which is another inference server so somehow a competitor to",
    "start": "234480",
    "end": "240799"
  },
  {
    "text": "VM and I won't talk about that I will talk always about in if I mention Triton",
    "start": "240799",
    "end": "246480"
  },
  {
    "text": "in this presentation always about this language open AI Triton the claim to fame for triton is that it has",
    "start": "246480",
    "end": "253280"
  },
  {
    "text": "competitive or Superior performance to hand tuned and proprietary libraries from Nidia like Q Plus or qdn but it's",
    "start": "253280",
    "end": "260120"
  },
  {
    "text": "purely open source its program model is based around",
    "start": "260120",
    "end": "265160"
  },
  {
    "text": "tiles and it also supports not only Nvidia but also other platforms like like AMD Intel and more and more",
    "start": "265160",
    "end": "271520"
  },
  {
    "text": "experimental platforms coming up so how can Triton achieve Superior",
    "start": "271520",
    "end": "278919"
  },
  {
    "text": "performance and but by just being very easy op uh open source optimizations so",
    "start": "278919",
    "end": "286039"
  },
  {
    "text": "Triton has a tile based program programming model that means the model is from the pro programmer point of view",
    "start": "286039",
    "end": "293520"
  },
  {
    "text": "around single program multiple data so spmd so the programmer does not need to",
    "start": "293520",
    "end": "299039"
  },
  {
    "text": "program the ization itself I stole here a figure from the",
    "start": "299039",
    "end": "304120"
  },
  {
    "text": "Triton paper comparing the cudar model and the Triton um programming model and",
    "start": "304120",
    "end": "310440"
  },
  {
    "text": "if you look at it from some distance it doesn't look so much uh different if you",
    "start": "310440",
    "end": "315759"
  },
  {
    "text": "code in Cuda you usually give each thread its own thing to do so you need if you iterate over Loops or you have",
    "start": "315759",
    "end": "321360"
  },
  {
    "text": "nested Loops for very complex matrix multiplication or attention kernels you need to keep track of your indexes and",
    "start": "321360",
    "end": "327919"
  },
  {
    "text": "then you tell each red what to do on a CPU however for triton first Triton can",
    "start": "327919",
    "end": "333880"
  },
  {
    "text": "be programmed in Python which I personally find is more convenient than Cuda and secondly you don't need to",
    "start": "333880",
    "end": "339600"
  },
  {
    "text": "Define every thread what it needs to be done you can Define ranges and then these ranges are split the compiler will",
    "start": "339600",
    "end": "346600"
  },
  {
    "text": "split these ranges into each thread into each sh memory what they need to",
    "start": "346600",
    "end": "352199"
  },
  {
    "text": "do so it looks like only a slight difference to this between this loop on",
    "start": "352199",
    "end": "357440"
  },
  {
    "text": "indexes and this ranges but apparently the slight difference allows much easier",
    "start": "357440",
    "end": "362919"
  },
  {
    "text": "optimizations of liveness analysis better memory management follows from",
    "start": "362919",
    "end": "368039"
  },
  {
    "text": "that uh it's easier to split these nested Loops into smaller tiles because",
    "start": "368039",
    "end": "374240"
  },
  {
    "text": "you can inere the ranges automatically and also static scheduling and so on gets",
    "start": "374240",
    "end": "380360"
  },
  {
    "text": "easier and as of today the Triton documentation still mentions the",
    "start": "380360",
    "end": "385479"
  },
  {
    "text": "resulting system works actually surprisingly well so they are still surprised that this small difference",
    "start": "385479",
    "end": "391400"
  },
  {
    "text": "apparently is very important to achieve high performance on processors like",
    "start": "391400",
    "end": "396639"
  },
  {
    "text": "gpus one footnote to that is that the performance mainly depends on how you",
    "start": "396639",
    "end": "402360"
  },
  {
    "text": "split these ranges into smaller blocks so and natively so it is I guess easy to",
    "start": "402360",
    "end": "409840"
  },
  {
    "text": "imagine that these blocks are then mapped to caches and shared memory and so on and that the size determines the",
    "start": "409840",
    "end": "416240"
  },
  {
    "text": "performance so and Triton allows you to do that so you code this like a variable and you",
    "start": "416240",
    "end": "424000"
  },
  {
    "text": "are able to Define this block size so how this range should be split at",
    "start": "424000",
    "end": "429440"
  },
  {
    "text": "runtime because TR is ajust in time compiler and for different scenarios so",
    "start": "429440",
    "end": "435400"
  },
  {
    "text": "different models different gpus we need then different configurations of these block size so that is what",
    "start": "435400",
    "end": "441360"
  },
  {
    "text": "configurations for triton kernels is about so now we put Triton into VM Simon",
    "start": "441360",
    "end": "446840"
  },
  {
    "text": "mentioned already there are a lot of kernels in VM using Triton um but none",
    "start": "446840",
    "end": "452599"
  },
  {
    "text": "of these kernels uh use the autot tuner so Auto tuner is a concept from Triton",
    "start": "452599",
    "end": "459039"
  },
  {
    "text": "that allows you to tune these block size I mentioned on the slide before to every different scenario usually the Tron curs",
    "start": "459039",
    "end": "466039"
  },
  {
    "text": "in VM have hand coded hardcoded uh configurations so block sizes or very",
    "start": "466039",
    "end": "472680"
  },
  {
    "text": "simple ristics or use custom Json lists so to get um",
    "start": "472680",
    "end": "480560"
  },
  {
    "text": "they assume best possible performance the result is that the Trion kernels run",
    "start": "480560",
    "end": "485680"
  },
  {
    "text": "well on for example a100 or h100 if the developer tuned it for that but do not",
    "start": "485680",
    "end": "491039"
  },
  {
    "text": "run well on other gpus so we again have in combination with the Cuda kernels limited portability and limited",
    "start": "491039",
    "end": "497360"
  },
  {
    "text": "performance portability so now this limited performance portability I mentioned",
    "start": "497360",
    "end": "503319"
  },
  {
    "text": "already quite some time in this talk and if you are not in an HPC or computer architecture background it may be new",
    "start": "503319",
    "end": "508680"
  },
  {
    "text": "for you so what does it mean it basically says that well you can tune your algorithm for one platform or you",
    "start": "508680",
    "end": "515159"
  },
  {
    "text": "developed your algorithm for a specific GPU but if you run it on a different GPU it has worse",
    "start": "515159",
    "end": "521518"
  },
  {
    "text": "performance and that is even true for gpus within the same vendor though not only between for example Nvidia AMD or",
    "start": "521519",
    "end": "527760"
  },
  {
    "text": "Intel but also if you have different Nvidia Generations the architecture changed a lot so you the performance",
    "start": "527760",
    "end": "534440"
  },
  {
    "text": "could decrease and here's an example of some VM kernel the reshape and cache kernel",
    "start": "534440",
    "end": "540440"
  },
  {
    "text": "and green is you see in this figure on the x-axis the size of the array so for",
    "start": "540440",
    "end": "545560"
  },
  {
    "text": "example bat size times head size and so on on the y axis the latency lower is",
    "start": "545560",
    "end": "551040"
  },
  {
    "text": "better that is a reshape and cache kernel to prepare the KV cache green is",
    "start": "551040",
    "end": "556240"
  },
  {
    "text": "a VM implementation Cuda and in purple is my Triton implementation which is",
    "start": "556240",
    "end": "562120"
  },
  {
    "text": "usually always faster if you execute the same test bench on an h100 then you notice Gap",
    "start": "562120",
    "end": "569640"
  },
  {
    "text": "here because actually the performance of the Cuda kernel is the same meaning the Cuda kernel cannot Leverage The Superior",
    "start": "569640",
    "end": "576920"
  },
  {
    "text": "architecture of an Hopper architect of the hopper GPU however the Triton code can",
    "start": "576920",
    "end": "583079"
  },
  {
    "text": "and the Triton can adapt to this new architecture due to the autot tuning",
    "start": "583079",
    "end": "588120"
  },
  {
    "text": "because this block sizes and the adaptation of the code to a new architecture is done",
    "start": "588120",
    "end": "594320"
  },
  {
    "text": "automatically so for and before we go there into more",
    "start": "594320",
    "end": "600399"
  },
  {
    "text": "details what does this autot tuning do in detail I have the same kernel here again",
    "start": "600399",
    "end": "607120"
  },
  {
    "text": "however I disabled the autot tuning of Triton and I hardcoded the block sizes and you see now two curves of Triton one",
    "start": "607120",
    "end": "613720"
  },
  {
    "text": "is a light purple curve that is has a large block size and the dark purple Cur",
    "start": "613720",
    "end": "620720"
  },
  {
    "text": "uh curve has a small block size and as you can see for smaller arrays the",
    "start": "620720",
    "end": "626640"
  },
  {
    "text": "smaller block size is superior for larger arrays at some point the larger block size is superior so that is not un",
    "start": "626640",
    "end": "633519"
  },
  {
    "text": "not unexpected and if you enable TR autot tuning then you merge the",
    "start": "633519",
    "end": "638959"
  },
  {
    "text": "performance Behavior because the autotuner picked for every array size meaning number of tokens multiplied by",
    "start": "638959",
    "end": "645279"
  },
  {
    "text": "badge size number multiplied by number of heads and so on the best possible configuration and that's why for this",
    "start": "645279",
    "end": "652519"
  },
  {
    "text": "kernel Tri is always better so looking at that the solution could be easy right we could use TR",
    "start": "652519",
    "end": "659320"
  },
  {
    "text": "autotune everywhere because it's built in in the Triton library and of course we should write more curs in Triton",
    "start": "659320",
    "end": "666399"
  },
  {
    "text": "however the Tron autotuner evaluate all possible configuration",
    "start": "666399",
    "end": "672560"
  },
  {
    "text": "sequentially and that take some time or could take some time and depending on",
    "start": "672560",
    "end": "679160"
  },
  {
    "text": "the Kernel depending on the search space of the autot tuner this could take even days meaning if you would use the native",
    "start": "679160",
    "end": "686320"
  },
  {
    "text": "Triton autot tuning in production you want to start a new inference server it",
    "start": "686320",
    "end": "691519"
  },
  {
    "text": "could take 3 days or eight days until this inference server is up and running which is of course prohibitive so cannot",
    "start": "691519",
    "end": "698839"
  },
  {
    "text": "be done within a production system so now we have the problem in",
    "start": "698839",
    "end": "704800"
  },
  {
    "text": "order to achieve best possible performance on multiple Hardware we need to use autot tuning but autot tuning is",
    "start": "704800",
    "end": "710240"
  },
  {
    "text": "prohibitive expensive in uh production so what can we do and that is the",
    "start": "710240",
    "end": "716120"
  },
  {
    "text": "contributions we in IBM research did we developed veled a framework called Triton dja view because our observation",
    "start": "716120",
    "end": "723399"
  },
  {
    "text": "was that for a given platform so a given GPU and a given kernel configuration the decision of the autotuner could more or",
    "start": "723399",
    "end": "730160"
  },
  {
    "text": "less be predicted because if you multiply a specific Matrix on an a100 it",
    "start": "730160",
    "end": "735240"
  },
  {
    "text": "will always be the best configuration for this particular GPU so we thought we just teach Triton to have some Deja",
    "start": "735240",
    "end": "742440"
  },
  {
    "text": "mechanism so it remember what it did before we open source this framework it has in the meantime also so",
    "start": "742440",
    "end": "749639"
  },
  {
    "text": "more stars and forks then on this slide and we are also in discussion with the Triton Community to merge this",
    "start": "749639",
    "end": "758639"
  },
  {
    "text": "Upstream so how does this dja view mechanism for AI inference can work the",
    "start": "759000",
    "end": "764880"
  },
  {
    "text": "first two phes here the kernel development and the is the same and also then the developer provides a list of",
    "start": "764880",
    "end": "772720"
  },
  {
    "text": "configurations that the developer thinks could be good for this kernel however then we also a third list a second list",
    "start": "772720",
    "end": "781000"
  },
  {
    "text": "here in the third step and Define the use cases because the tuning decision depends on the shapes of the arrays of",
    "start": "781000",
    "end": "789920"
  },
  {
    "text": "the Triton kernel so we need to Define for Which models for which badge size and so on these um kernels are intended",
    "start": "789920",
    "end": "799040"
  },
  {
    "text": "to run then in an automatic cach preparation phase we select for all",
    "start": "799040",
    "end": "804760"
  },
  {
    "text": "these defined use cases the best possible configurations across all hardware and say it to a simple Json",
    "start": "804760",
    "end": "810279"
  },
  {
    "text": "file and then this Json file can be included into a in the container and the container is ready for fast deployment",
    "start": "810279",
    "end": "817360"
  },
  {
    "text": "and of course all of that could be fully automated and it's also gated against changes so if we want to build a new",
    "start": "817360",
    "end": "824160"
  },
  {
    "text": "container but nothing the Triton kernel changed we don't need to run the cach preparation stage",
    "start": "824160",
    "end": "830079"
  },
  {
    "text": "again why is this safe to use so why can we be sure that our D view mechanism",
    "start": "830079",
    "end": "836360"
  },
  {
    "text": "does not pick the wrong configuration at the wrong time we did some research in this direction",
    "start": "836360",
    "end": "841639"
  },
  {
    "text": "and tried to ensure which criteria we need to know to say okay we can use this",
    "start": "841639",
    "end": "847000"
  },
  {
    "text": "uh configuration safely and we came up with this list and I guess the first are obvious",
    "start": "847000",
    "end": "853839"
  },
  {
    "text": "we need of course a performance and the selection of the Triton kernel depends on the Cuda version the pyo version the",
    "start": "853839",
    "end": "859759"
  },
  {
    "text": "Triton version itself and the GPU then of course it also depends on the code",
    "start": "859759",
    "end": "865160"
  },
  {
    "text": "version so we need to have the hash of the co of the Triton code of the Just in",
    "start": "865160",
    "end": "870360"
  },
  {
    "text": "Time compiled function then we need but only the code we don't need to have the hash of the",
    "start": "870360",
    "end": "876440"
  },
  {
    "text": "libraries also because that is actually deployment independent and of course we need the",
    "start": "876440",
    "end": "882880"
  },
  {
    "text": "hashes of the configurations we think this kernel should be good for and also the hash of the some",
    "start": "882880",
    "end": "889160"
  },
  {
    "text": "autotuner parameters like how long the autotuner should run because Al that influences the autotuner",
    "start": "889160",
    "end": "894959"
  },
  {
    "text": "decision then the version of our own framework and and the last one is",
    "start": "894959",
    "end": "900079"
  },
  {
    "text": "optional but while developing this framework I wanted to know which kernel",
    "start": "900079",
    "end": "905480"
  },
  {
    "text": "cache file was for which kernel so I added a human readable label there the signature of",
    "start": "905480",
    "end": "911199"
  },
  {
    "text": "the function and this Tre of Json files then",
    "start": "911199",
    "end": "916440"
  },
  {
    "text": "can then can be simply included into the containers as St before and",
    "start": "916440",
    "end": "922720"
  },
  {
    "text": "since we now have the option to go uh and can reuse large amounts of",
    "start": "922720",
    "end": "929279"
  },
  {
    "text": "Triton configurations we actually came across that not only we should uh use a",
    "start": "929279",
    "end": "935160"
  },
  {
    "text": "TH or 20 or 30 configurations for each Triton kernel but actually for the best possible performance we need hundreds of",
    "start": "935160",
    "end": "942480"
  },
  {
    "text": "configurations and then it's more efficient to go through this hundreds of configurations with configuration spaces",
    "start": "942480",
    "end": "948720"
  },
  {
    "text": "and not lists like Triton is doing it and we also need some htics in case in",
    "start": "948720",
    "end": "954519"
  },
  {
    "text": "production we hit a corner case that we did not optimize for we also add add this to our open source",
    "start": "954519",
    "end": "961279"
  },
  {
    "text": "Library so the previous figures I showed you were about different generation of gpus",
    "start": "961279",
    "end": "967279"
  },
  {
    "text": "and now what about performance portability for different vendors so can we actually actually",
    "start": "967279",
    "end": "974040"
  },
  {
    "text": "execute the same Tron code and AMD and Nvidia and the answer is yes it is possible I executed on this figure uh",
    "start": "974040",
    "end": "982480"
  },
  {
    "text": "kernel that was written by amd's Rock M team for Flash attention that is the brown curve here on an Nvidia a100 and",
    "start": "982480",
    "end": "991560"
  },
  {
    "text": "compared it to the latest and greatest flash attention package of trow and as you can see you don't see a",
    "start": "991560",
    "end": "997680"
  },
  {
    "text": "lot of difference here on this figure so sometimes the Triton Kern is",
    "start": "997680",
    "end": "1004120"
  },
  {
    "text": "actually faster some other times the Tron kernel in larger array sizes is slower but only minor it's getting",
    "start": "1004120",
    "end": "1011959"
  },
  {
    "text": "close the y- axis has a logarithmic scale so if you then look for example at",
    "start": "1011959",
    "end": "1017199"
  },
  {
    "text": "the red curve that is the same name Tron code executed but without my autot tuning using the uh AMD configurations",
    "start": "1017199",
    "end": "1024360"
  },
  {
    "text": "on an Nvidia gra GPU so obviously this configuration runs worse so we achieve",
    "start": "1024360",
    "end": "1029600"
  },
  {
    "text": "only 70% of possible performance if you disabl autotuning all together though how Triton is used most",
    "start": "1029600",
    "end": "1037798"
  },
  {
    "text": "of the time then you end up achieving only 15% of the possible performance or",
    "start": "1037799",
    "end": "1042918"
  },
  {
    "text": "6 to 7% slowdown uh 6 to 7 x slowdown and",
    "start": "1042919",
    "end": "1050240"
  },
  {
    "text": "and yeah so bottom line you can achieve true platform portability with",
    "start": "1050240",
    "end": "1056200"
  },
  {
    "text": "comparative performance with open air Triton however you need to use uh",
    "start": "1056200",
    "end": "1062840"
  },
  {
    "text": "autotuning one last point in this direction uh if you also look at the end",
    "start": "1062840",
    "end": "1068480"
  },
  {
    "text": "to end performance I did a Sim uh evaluating llama 7 billion model I did",
    "start": "1068480",
    "end": "1075799"
  },
  {
    "text": "this already in May so it was before all these Lama 3 models were out and if you compare Lama Z billion and on",
    "start": "1075799",
    "end": "1086360"
  },
  {
    "text": "the in this figure and on the x-axis you have the shut so and on the y axis you",
    "start": "1086360",
    "end": "1091480"
  },
  {
    "text": "have the time to First token as in similar figures earlier today the blue curve is VM 0.6 as of two",
    "start": "1091480",
    "end": "1100039"
  },
  {
    "text": "weeks ago the purple curve is the same model but with just Triton",
    "start": "1100039",
    "end": "1105840"
  },
  {
    "text": "kernel so without any Cuda code involved P torch native code and the orange curve",
    "start": "1105840",
    "end": "1112880"
  },
  {
    "text": "is the same model with only Triton but without autotuning and as you can see",
    "start": "1112880",
    "end": "1118440"
  },
  {
    "text": "that has a huge performance gap of 30% the Triton uh only model gets closed",
    "start": "1118440",
    "end": "1125440"
  },
  {
    "text": "with only 8% slower for large bad sizes and for smaller number of users so the",
    "start": "1125440",
    "end": "1131240"
  },
  {
    "text": "numbers here on the curve are actually the number of users I should have mentioned that for smaller number of",
    "start": "1131240",
    "end": "1136360"
  },
  {
    "text": "users we are actually faster and yeah we did this evaluation with",
    "start": "1136360",
    "end": "1141679"
  },
  {
    "text": "another project we developed FM per to do this evaluation I needed to uh",
    "start": "1141679",
    "end": "1148480"
  },
  {
    "text": "Implement a number of uh Triton kernels the activations the rehap and cach I",
    "start": "1148480",
    "end": "1153559"
  },
  {
    "text": "showed you earlier the layer norm and then I used the flash attention from AMD so that was about okay we can this",
    "start": "1153559",
    "end": "1161840"
  },
  {
    "text": "get this Triton only models actually close to state-of-the-art Performance or even Superior and there's also work of",
    "start": "1161840",
    "end": "1169120"
  },
  {
    "text": "my colleagues do uh done within the pyot community around in this direction so",
    "start": "1169120",
    "end": "1174559"
  },
  {
    "text": "now if we use Triton D Vu does it actually bring what we need and here I",
    "start": "1174559",
    "end": "1180640"
  },
  {
    "text": "evaluated the startup time of llama 7 billion again and then of a larger model called maxal on the x-axis you see one",
    "start": "1180640",
    "end": "1188840"
  },
  {
    "text": "if you use the Upstream Triton native autot tuning how long it would take until the container is ready to uh get",
    "start": "1188840",
    "end": "1195799"
  },
  {
    "text": "requests and on the second bar is then if you use the same container with Tron",
    "start": "1195799",
    "end": "1202880"
  },
  {
    "text": "D [Music] so yeah as you can see it takes",
    "start": "1202880",
    "end": "1210080"
  },
  {
    "text": "ridiculously long for if you use native autot tuning for llama 7 billion it",
    "start": "1210080",
    "end": "1215559"
  },
  {
    "text": "could take up to 17 hours for mixed 12 even 8 days which is as I said before prohibitive",
    "start": "1215559",
    "end": "1222320"
  },
  {
    "text": "expensive and if you look at the numbers here um for an h100 in Orange it takes",
    "start": "1222320",
    "end": "1228679"
  },
  {
    "text": "takes yeah 8 days or 56 seconds so then I could claim in theory I have a speed",
    "start": "1228679",
    "end": "1234559"
  },
  {
    "text": "up of five orders of magnitude by using our framework Tian dja but I also think this numbers is a little bit ridiculous",
    "start": "1234559",
    "end": "1240760"
  },
  {
    "text": "because no one would do it in practice like this right however with TR D it is possible to do it in",
    "start": "1240760",
    "end": "1247080"
  },
  {
    "text": "practice so and to show also that we cannot only make nice plots I want to",
    "start": "1247080",
    "end": "1253159"
  },
  {
    "text": "demonstrate that in a brief video I did a Titan only uh deployment for VM we",
    "start": "1253159",
    "end": "1259640"
  },
  {
    "text": "replaced all the custom cter kernels as I said before I implemented some of them and then we demonstrate",
    "start": "1259640",
    "end": "1267039"
  },
  {
    "text": "this in using VM of course so I hope that works now but it",
    "start": "1267039",
    "end": "1272720"
  },
  {
    "text": "looks great so that is a video I did before because I didn't want to bet on any Wi-Fi here in the",
    "start": "1272720",
    "end": "1278799"
  },
  {
    "text": "conference um so here you see I start a Docker container and I start a bash in",
    "start": "1278799",
    "end": "1284640"
  },
  {
    "text": "this Docker container first because I want to show you how this Triton dja Vu",
    "start": "1284640",
    "end": "1289960"
  },
  {
    "text": "cache looks like so I show you here the folder structure the folder tree where you see okay that is a cache tune for",
    "start": "1289960",
    "end": "1297360"
  },
  {
    "text": "Cuda 12.4 a100 and so on and there are a lot of cache files lying around with a",
    "start": "1297360",
    "end": "1303360"
  },
  {
    "text": "lot of hashes I can also show you how there even more Cas files lying around I",
    "start": "1303360",
    "end": "1309279"
  },
  {
    "text": "can show you even also how one of these Cas files looks like",
    "start": "1309279",
    "end": "1316320"
  },
  {
    "text": "um if if I open the first 10 lines of one of these files you see there again",
    "start": "1317320",
    "end": "1322960"
  },
  {
    "text": "the all of the hashes I presented before in the file then the signature how long it took to get this file and how many",
    "start": "1322960",
    "end": "1329320"
  },
  {
    "text": "configurations are evaluated and then for each uh case or that is a list of the autotuner key what is the best",
    "start": "1329320",
    "end": "1336120"
  },
  {
    "text": "configuration so very simple actually and then also to prove our",
    "start": "1336120",
    "end": "1342559"
  },
  {
    "text": "actually use Tron code that is a activation code I implemented and there I Define the configuration space that",
    "start": "1342559",
    "end": "1348919"
  },
  {
    "text": "tritan D we needed to go through earlier if I want to use CAG graph and so on",
    "start": "1348919",
    "end": "1355679"
  },
  {
    "text": "and now I start um the open AI server I use enforce eager",
    "start": "1355679",
    "end": "1364320"
  },
  {
    "text": "because we only care about the time to First token and here you see as some loock messages and that Triton D we actually",
    "start": "1364320",
    "end": "1372240"
  },
  {
    "text": "did get started and found the configurations and also I added some log messages that we have the Triton colel",
    "start": "1372240",
    "end": "1378559"
  },
  {
    "text": "up and running so now that are roughly 30 seconds uh the server is up and",
    "start": "1378559",
    "end": "1383640"
  },
  {
    "text": "running on the right hand side I now make a default request with a default op",
    "start": "1383640",
    "end": "1390400"
  },
  {
    "text": "uh open AP open AI API and ask about",
    "start": "1390400",
    "end": "1396320"
  },
  {
    "text": "what could I do in San Francisco and as you see well there's no",
    "start": "1396320",
    "end": "1401360"
  },
  {
    "text": "latency and it we can execute again there's no latency for autot tuning and",
    "start": "1401360",
    "end": "1407520"
  },
  {
    "text": "now we also can ask about my I like San Francisco but I would admit I like zich more so I let's see what the model says",
    "start": "1407520",
    "end": "1414000"
  },
  {
    "text": "about zich and yeah that's",
    "start": "1414000",
    "end": "1421200"
  },
  {
    "text": "it so that is a brief demonstration that we then can use",
    "start": "1422679",
    "end": "1428919"
  },
  {
    "text": "um try an autotuner to achieve platform true platform portability and",
    "start": "1431520",
    "end": "1436720"
  },
  {
    "text": "performance portability without without any overhead in production",
    "start": "1436720",
    "end": "1442240"
  },
  {
    "text": "so I hope in this brief talk I could convince you that performance pability is critical to scale to many different",
    "start": "1442240",
    "end": "1448880"
  },
  {
    "text": "scenarios if you don't have your own data center with your own homogeneous Hardware it's probably you have a lot of",
    "start": "1448880",
    "end": "1454200"
  },
  {
    "text": "different Hardware where a server needs to run well like and that also applies",
    "start": "1454200",
    "end": "1459840"
  },
  {
    "text": "to open source Frameworks like VM open ey Triton is a very interesting",
    "start": "1459840",
    "end": "1465240"
  },
  {
    "text": "uh domain specific language and it's used widely and it enhances the performance and",
    "start": "1465240",
    "end": "1471520"
  },
  {
    "text": "portability of VM kernels however we need autot tuning to achieve true the",
    "start": "1471520",
    "end": "1477520"
  },
  {
    "text": "the best possible performance and True Performance portability and we need to autotune and remember it otherwise we",
    "start": "1477520",
    "end": "1484039"
  },
  {
    "text": "cannot use it in production we open sourced the library a couple of weeks ago and the P request to",
    "start": "1484039",
    "end": "1491760"
  },
  {
    "text": "merge that into VM are in Preparation however there is still some work we want to do in this direction we",
    "start": "1491760",
    "end": "1497799"
  },
  {
    "text": "want to make the fallback hortic more efficient we want to use not only sequential search in the autotuner we",
    "start": "1497799",
    "end": "1504320"
  },
  {
    "text": "may want to use more better search algorithms like basan optimization or genetic",
    "start": "1504320",
    "end": "1509640"
  },
  {
    "text": "algorithms and of course it's open source so you can also contribute your",
    "start": "1509640",
    "end": "1515960"
  },
  {
    "text": "ideas we think that since there will be many many more Hardware coming out in",
    "start": "1515960",
    "end": "1522480"
  },
  {
    "text": "the future something performance portability is necessary for to make VM",
    "start": "1522480",
    "end": "1527640"
  },
  {
    "text": "future proof and we hope we contribute into this direction and now I look forward to all your",
    "start": "1527640",
    "end": "1534320"
  },
  {
    "text": "questions yes thanks the speaker all right please raise your hand",
    "start": "1534919",
    "end": "1540440"
  },
  {
    "text": "if you have questions I'll all right catch up thank you for the great work uh",
    "start": "1540440",
    "end": "1545840"
  },
  {
    "text": "I have one question like uh do you have some expected timeline to get into Triton I think this should be a",
    "start": "1545840",
    "end": "1552399"
  },
  {
    "text": "fantastic feature of Triton itself and it's surprising that it's still missing in Triton thank you for for this",
    "start": "1552399",
    "end": "1558640"
  },
  {
    "text": "question it surprises me too and I was also at the Triton conference two weeks",
    "start": "1558640",
    "end": "1563679"
  },
  {
    "text": "ago and we had a chat with the developers and yeah to put it diplomatically I",
    "start": "1563679",
    "end": "1569279"
  },
  {
    "text": "guess it's not on their pri priority list and you can talk about that more",
    "start": "1569279",
    "end": "1574440"
  },
  {
    "text": "offline of the record I think another followup is that we are",
    "start": "1574440",
    "end": "1581000"
  },
  {
    "text": "integrating uh torch compile and the inductor will directly generate these Trent kernels so if you want to we need",
    "start": "1581000",
    "end": "1588440"
  },
  {
    "text": "to manually insert me like this Library yeah uh it's difficult to do that in",
    "start": "1588440",
    "end": "1593799"
  },
  {
    "text": "inductor so yes absolutely as so actually inductor also uses the Triton autotuner and we were thinking maybe we",
    "start": "1593799",
    "end": "1600320"
  },
  {
    "text": "just contributed to P torch then directly for torch inductor but yeah",
    "start": "1600320",
    "end": "1606960"
  },
  {
    "text": "that's another thing we look into yeah thank you I didn't",
    "start": "1606960",
    "end": "1614520"
  },
  {
    "text": "take I hope Simon takes care of who yeah um yeah it looks great um the",
    "start": "1614520",
    "end": "1621399"
  },
  {
    "text": "question I had is you have that cash do that you generate that from the auto tuning yeah is the idea that V ships",
    "start": "1621399",
    "end": "1629320"
  },
  {
    "text": "with a lot of these caches by default so people don't have to even worry about that like you just because I think",
    "start": "1629320",
    "end": "1635600"
  },
  {
    "text": "everyone building their own image seems a bit like yeah hard oh no as of right now they as of right now there is",
    "start": "1635600",
    "end": "1642559"
  },
  {
    "text": "something similar for the fusede kernel already in VM where you have many Json files for different conf",
    "start": "1642559",
    "end": "1648720"
  },
  {
    "text": "erations and this solution would improve that and also make it more general for other Triton kernels so yeah the idea is",
    "start": "1648720",
    "end": "1655799"
  },
  {
    "text": "that many of these Chason files are then part of one way or the other the VM",
    "start": "1655799",
    "end": "1662320"
  },
  {
    "text": "project we have time for more I guess I are two questions follow up for the first",
    "start": "1662440",
    "end": "1668840"
  },
  {
    "text": "question do you have any insight they don't support this aish how do they run the system in production it takes eight",
    "start": "1668840",
    "end": "1675559"
  },
  {
    "text": "days to start up so uh you you mentioned that this feature is not in the Triton",
    "start": "1675559",
    "end": "1681039"
  },
  {
    "text": "road map do you have any insight how they run is in production",
    "start": "1681039",
    "end": "1688600"
  },
  {
    "text": "um um I think thanks for the production the question I think the company carry",
    "start": "1688600",
    "end": "1696000"
  },
  {
    "text": "as that most of the people using Triton in production right now have a very",
    "start": "1696000",
    "end": "1701039"
  },
  {
    "text": "homogeneous setup so they autotune they tune the curs manually and then have a",
    "start": "1701039",
    "end": "1706960"
  },
  {
    "text": "default configuration they always take however I must also admit I thought in the beginning I'm smarter than the",
    "start": "1706960",
    "end": "1713240"
  },
  {
    "text": "autotuner and since I did a lot with computer architecture I can find the best configuration by my gut feeling",
    "start": "1713240",
    "end": "1720519"
  },
  {
    "text": "however that is not true so I was constantly beaten by my own algorithm finding the better algorithm for a specific platform so I besides that many",
    "start": "1720519",
    "end": "1728399"
  },
  {
    "text": "people do it in production like that I would advise using autot tuning and Tron DET even if you think you know what the",
    "start": "1728399",
    "end": "1734240"
  },
  {
    "text": "best configuration is uh maybe it was already mentioned but I",
    "start": "1734240",
    "end": "1740480"
  },
  {
    "text": "just wanted to check so uh for example you said the the config that you want to",
    "start": "1740480",
    "end": "1745600"
  },
  {
    "text": "autotune over and let's say uh it runs it everything gets cached and now you",
    "start": "1745600",
    "end": "1752039"
  },
  {
    "text": "want to modify the config to expand it like to evaluate a wider set of parameters for the autotuner does it",
    "start": "1752039",
    "end": "1758440"
  },
  {
    "text": "reuse that cache or yeah yeah yes so that's the idea that we with this extensive list of hashes so that we know",
    "start": "1758440",
    "end": "1765240"
  },
  {
    "text": "exactly what we can reuse and what not not but reuse as much as possible because at some point compute gets",
    "start": "1765240",
    "end": "1771679"
  },
  {
    "text": "expensive yeah thanks you have another question which is about uh for example",
    "start": "1771679",
    "end": "1778120"
  },
  {
    "text": "in Laura I mean in the punica colonels uh they were a couple of weeks ago",
    "start": "1778120",
    "end": "1784279"
  },
  {
    "text": "ported to Triton right a couple of months so uh and they can be loaded uh",
    "start": "1784279",
    "end": "1791440"
  },
  {
    "text": "separately from the main model right so if you have a configuration like that would you have Json files for the model",
    "start": "1791440",
    "end": "1798240"
  },
  {
    "text": "and for the lower rats or it be one single Json file for both it would be as",
    "start": "1798240",
    "end": "1805200"
  },
  {
    "text": "a it I think you in thanks for your question I think in the sense of your",
    "start": "1805200",
    "end": "1811559"
  },
  {
    "text": "question it would be one Json file for each kernel for if you have then multiple kernels",
    "start": "1811559",
    "end": "1818320"
  },
  {
    "text": "then it would be multiple Chason file but you have one kernel that you use in different scenarios it's one cache because that's how because then it also",
    "start": "1818320",
    "end": "1825200"
  },
  {
    "text": "would be just in time compiled by Triton as one kernel so that's how we merged it there",
    "start": "1825200",
    "end": "1833640"
  },
  {
    "text": "thanks any other questions all right let's thank the",
    "start": "1833640",
    "end": "1839399"
  },
  {
    "text": "speaker again awesome talk thank you",
    "start": "1839399",
    "end": "1846120"
  }
]