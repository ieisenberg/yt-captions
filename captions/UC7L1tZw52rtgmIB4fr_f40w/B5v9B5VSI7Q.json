[
  {
    "text": "all right I think we can go ahead and get started then so thanks everyone for coming out to hear me show for my own",
    "start": "6839",
    "end": "13380"
  },
  {
    "text": "company today um actually I think even if you're not in the market for a deep learning",
    "start": "13380",
    "end": "19859"
  },
  {
    "text": "platform commercial platform I think we'll have a lot to uh tell you about how we've used Ray in some pretty",
    "start": "19859",
    "end": "25619"
  },
  {
    "text": "interesting ways that will hopefully be a benefit for you and whatever you do with Ray so thank you I'm Travis Adair",
    "start": "25619",
    "end": "32520"
  },
  {
    "text": "I'm the CTO of predabase previously I worked for a number of years on Uber's",
    "start": "32520",
    "end": "37620"
  },
  {
    "text": "Michelangelo team and you know at predabase we're building a low close a low code deep learning",
    "start": "37620",
    "end": "43620"
  },
  {
    "text": "platform built on top of a lot of the open source technologies that came out of uber AI like the Ludwig project which",
    "start": "43620",
    "end": "49680"
  },
  {
    "text": "I'll talk about as well as horabad which I think a lot of you have heard a lot about during this conference",
    "start": "49680",
    "end": "55140"
  },
  {
    "text": "and so what we'll talk about specifically is I'll give you an introduction to predabase and what we mean by declarative machine learning",
    "start": "55140",
    "end": "61860"
  },
  {
    "text": "I'll talk about Ludwig the open source declarative ml framework that were built on top of I'll explain how we've made",
    "start": "61860",
    "end": "67979"
  },
  {
    "text": "Ludwig production ready by uh integrating it fully with Ray and then I'll also talk about what we're doing on",
    "start": "67979",
    "end": "73680"
  },
  {
    "text": "the predibase side with an abstraction that we call engines which is a serverless multi-tenant layer that sits",
    "start": "73680",
    "end": "79439"
  },
  {
    "text": "on top of array and so to start I want to talk a little bit about this declarative ml concept so",
    "start": "79439",
    "end": "84720"
  },
  {
    "text": "we believe that today there's a bit of a false dichotomy in the industry between people who believe that the only way to",
    "start": "84720",
    "end": "90060"
  },
  {
    "text": "do machine learning is to write low-level apis like Pi torch or tensorflow and people who believe that",
    "start": "90060",
    "end": "95460"
  },
  {
    "text": "ml is essentially a solved problem and you can just throw lots and lots of compute at it and you know automl will eventually give you whatever result you",
    "start": "95460",
    "end": "102240"
  },
  {
    "text": "want and so on the low level side they have the benefit of being very flexible but there are a lot of downsides to this",
    "start": "102240",
    "end": "108000"
  },
  {
    "text": "approach oftentimes the infrastructure is very uh do it yourself you have to",
    "start": "108000",
    "end": "113340"
  },
  {
    "text": "stitch together the best in breed solutions by hand for your particular problem it's a very fragmented landscape",
    "start": "113340",
    "end": "118920"
  },
  {
    "text": "there aren't enough experts who know how to do this stuff and oftentimes these experts are only experts in this",
    "start": "118920",
    "end": "124740"
  },
  {
    "text": "particular subset of these Technologies and so you need a huge team with lots and lots of time to get everything into",
    "start": "124740",
    "end": "130319"
  },
  {
    "text": "production and so as an example of this you know oftentimes a pattern that we've seen at companies like like uber when we",
    "start": "130319",
    "end": "136260"
  },
  {
    "text": "were there was that you would have a project that maybe does intent classification it has some thousands of lines tensorflow it takes five months",
    "start": "136260",
    "end": "142739"
  },
  {
    "text": "and then your second project maybe is 900 lines of pie torch and there's no ability to reuse any of the things that",
    "start": "142739",
    "end": "148260"
  },
  {
    "text": "you did for the first project for the second one same for the third project add infinitum and so you have all these",
    "start": "148260",
    "end": "153959"
  },
  {
    "text": "bespoke solutions that lead to lots and lots of tech debt because there's no way to kind of reuse components across them",
    "start": "153959",
    "end": "159000"
  },
  {
    "text": "right and so on the other side of the spectrum you have this automl stuff which is very simple very easy to use",
    "start": "159000",
    "end": "165239"
  },
  {
    "text": "but lots of people have complaints about Auto Metal that's a black box it takes away control from the customer and so",
    "start": "165239",
    "end": "171420"
  },
  {
    "text": "what happens is someone trains a model it's not quite what they want there's nothing they can do to improve it and so they churn out and go back to the low",
    "start": "171420",
    "end": "177480"
  },
  {
    "text": "level Frameworks and so what's our answer to this problem so our answer is what we call declarative deep learning",
    "start": "177480",
    "end": "182819"
  },
  {
    "text": "and we have an open source project I encourage you to check out called Ludwig that is the the first declarative deep",
    "start": "182819",
    "end": "188640"
  },
  {
    "text": "learning framework and the nice thing about declarative deep learning is that's very easy to get going like if you want to train a model",
    "start": "188640",
    "end": "194940"
  },
  {
    "text": "that does in this case like text classification uh it's just those six",
    "start": "194940",
    "end": "200099"
  },
  {
    "text": "lines of yaml that's all it takes right but if you want to go deeper and say I actually want to use Bert for this model",
    "start": "200099",
    "end": "206760"
  },
  {
    "text": "and I want to use a dropout rate of five percent Etc you can control all those parameters and you can even go all the",
    "start": "206760",
    "end": "213120"
  },
  {
    "text": "way down to doing hyper parameter search over individual models like neural architecture search or hyper parameter",
    "start": "213120",
    "end": "219060"
  },
  {
    "text": "search over individual parameters of the model so all the flexibility is there to go as deep as you want but the benefit",
    "start": "219060",
    "end": "225060"
  },
  {
    "text": "is all of this complexity is very simple to express through this yaml instead of having to drill down into you know the",
    "start": "225060",
    "end": "232379"
  },
  {
    "text": "classic python pytorch world where you know you're having to constantly ship",
    "start": "232379",
    "end": "237659"
  },
  {
    "text": "around Jupiter notebooks between your colleagues Etc and so we believe that this gives us the best Best of Both",
    "start": "237659",
    "end": "242819"
  },
  {
    "text": "Worlds it gives you a high level abstraction and opens the door to non-expert users and expert users alike",
    "start": "242819",
    "end": "248040"
  },
  {
    "text": "to work together on solving ml problems and as I said we pioneered these with Ludwig as well as another project called",
    "start": "248040",
    "end": "254220"
  },
  {
    "text": "Overton so definitely check out Ludwig to learn more about how this all works and so with pratibase we're expanding",
    "start": "254220",
    "end": "261299"
  },
  {
    "text": "upon what we've done with Ludwig and also building on top of his state-of-the-art ml infrastructure that's built on top of horavod as well",
    "start": "261299",
    "end": "268500"
  },
  {
    "text": "as Rey and I also want to call out our very simple ways of interacting with the tools so we have several different entry",
    "start": "268500",
    "end": "274979"
  },
  {
    "text": "points to predabase there's a web UI there's also a very powerful SQL",
    "start": "274979",
    "end": "280440"
  },
  {
    "text": "extension that we call people that's shown there that allows you to do model creation model prediction as easy as you",
    "start": "280440",
    "end": "286860"
  },
  {
    "text": "would write a SQL query and there's also a python SDK which sends a lot of Ray users are python users that's what I'll",
    "start": "286860",
    "end": "292259"
  },
  {
    "text": "be focusing most of the time on today and so the way that typically you use predivases like this is the core Loop",
    "start": "292259",
    "end": "298680"
  },
  {
    "text": "you have your structured data in Snowflake or uh in uh bigquery or some other data warehouse or data Lake like",
    "start": "298680",
    "end": "304199"
  },
  {
    "text": "S3 you train a model you operationalize it either either using people or SQL like interface for kind of bi and",
    "start": "304199",
    "end": "311040"
  },
  {
    "text": "analytics workloads or you deploy it to a rest endpoint and then you can kind of go back and iterate on that over time",
    "start": "311040",
    "end": "317460"
  },
  {
    "text": "and I want to show you this example so I think in a lot of the times when people show these python examples it's like",
    "start": "317460",
    "end": "324120"
  },
  {
    "text": "there's a lot of you know like oh there's actually some you know thousand line code function or something that you're not showing here that's actually",
    "start": "324120",
    "end": "329639"
  },
  {
    "text": "not the case here this is literally all it takes to get them all into production with predabase which I think is a really powerful thing about the low code aspect",
    "start": "329639",
    "end": "336660"
  },
  {
    "text": "so it's really not like a fully no code solution that there is a lot of control and a lot of advanced functionality that",
    "start": "336660",
    "end": "342780"
  },
  {
    "text": "you can do but what you see here is we're essentially just connecting to some snowflake Warehouse we're creating",
    "start": "342780",
    "end": "348900"
  },
  {
    "text": "a data set out of a table like in this case support tickets from that we can create what we call a model draft from",
    "start": "348900",
    "end": "355199"
  },
  {
    "text": "which we can iterate on you know setting the encoder in this case to be Bert we've already defined what our label is",
    "start": "355199",
    "end": "361080"
  },
  {
    "text": "there in the first line we train a model which then becomes an immutable artifact in predabase and then once we have that",
    "start": "361080",
    "end": "366479"
  },
  {
    "text": "model we can do batch prediction on a pandas data frame we can do prediction on another snowflake table and then",
    "start": "366479",
    "end": "373860"
  },
  {
    "text": "write it out to a third snowflake table we can do these kind of icing and dicing of the data with people to in this case",
    "start": "373860",
    "end": "380160"
  },
  {
    "text": "say you know I want to predict the label for this data set where the creation time is greater than such and such and",
    "start": "380160",
    "end": "387300"
  },
  {
    "text": "then we can also deploy the model to rest and so all of this can be done in just these few lines of code",
    "start": "387300",
    "end": "393240"
  },
  {
    "text": "and so the benefits for Ray users of this is that uh I want to focus on three",
    "start": "393240",
    "end": "398580"
  },
  {
    "text": "things in particular one this high level API that literally provides which is this low code modular interface that",
    "start": "398580",
    "end": "404460"
  },
  {
    "text": "allows you to bring faster iteration lower time to Value gives you Best in Class models and reusability and then",
    "start": "404460",
    "end": "411419"
  },
  {
    "text": "the full integration of Ludwig on Ray which bundles end to end the full array ml stack so you have Cutting Edge Ray",
    "start": "411419",
    "end": "418020"
  },
  {
    "text": "features like raid data sets dask on Ray Horvat on Ray all bundled together and optimized for performance and unifies",
    "start": "418020",
    "end": "425220"
  },
  {
    "text": "the training hyperopt as well as serving and then finally the serverless component which is the predibase engines",
    "start": "425220",
    "end": "431280"
  },
  {
    "text": "I'll talk about so these are Ray cluster resources that automatically are right size to your data and model workloads",
    "start": "431280",
    "end": "436740"
  },
  {
    "text": "and it removes the complexity of figuring out what's the right Hardware that I need to run my training job and",
    "start": "436740",
    "end": "442380"
  },
  {
    "text": "allows us to Auto scale to these heterogeneous Hardware configurations that mix together CPUs and gpus for",
    "start": "442380",
    "end": "447900"
  },
  {
    "text": "different parts of the workload so first let's talk about the high level API so I think this to me describes like",
    "start": "447900",
    "end": "454259"
  },
  {
    "text": "a lot of the state of the art of data science today is that new research comes out which is indicate at the top there",
    "start": "454259",
    "end": "459780"
  },
  {
    "text": "tadmed vit T5 Etc and then some person on the internet like a researcher will",
    "start": "459780",
    "end": "465180"
  },
  {
    "text": "read the paper and then write some Jupiter notebooks that implement it or write some GitHub repositories and then",
    "start": "465180",
    "end": "471180"
  },
  {
    "text": "you have these verticalized data science teams that exist within organizations that will then try to Cobble together",
    "start": "471180",
    "end": "476520"
  },
  {
    "text": "solutions by pulling you know code from a git repository or from a jupyter notebook Etc and then you know they all",
    "start": "476520",
    "end": "483240"
  },
  {
    "text": "need to try to Cobble these things together and there's not really any cohesive way of like how we're going to modularize this or use it or share it",
    "start": "483240",
    "end": "489840"
  },
  {
    "text": "across our team and so what we believe is a much better and more natural way to do this is that",
    "start": "489840",
    "end": "495599"
  },
  {
    "text": "the people who read the papers or write the papers even and Implement them in his code can Implement these components",
    "start": "495599",
    "end": "501780"
  },
  {
    "text": "within Ludwig so that they can be reused for multiple different tasks and then within predabase we provide an",
    "start": "501780",
    "end": "508500"
  },
  {
    "text": "Enterprise layer of like this uh repository feature that I'll talk a little bit about as well as uh you know",
    "start": "508500",
    "end": "515039"
  },
  {
    "text": "lots of features around the infrastructure abstraction that allows these teams to kind of collaborate together through this declarative",
    "start": "515039",
    "end": "521339"
  },
  {
    "text": "interface and take advantage of all these state-of-the-art mall architectures without having to kind of",
    "start": "521339",
    "end": "526620"
  },
  {
    "text": "you know Cobble together solutions from many disparate places right and so all this is really possible",
    "start": "526620",
    "end": "533160"
  },
  {
    "text": "because of the Ludwig architecture so there's this architecture that was pioneered with Ludwig called encoder combiner decoder and the way it works is",
    "start": "533160",
    "end": "540180"
  },
  {
    "text": "that every feature of your data set input or output feature can be represented as a specific data type",
    "start": "540180",
    "end": "545760"
  },
  {
    "text": "which then gets transformed in its own way depending on what that data type is so if it's a text feature we might",
    "start": "545760",
    "end": "551279"
  },
  {
    "text": "convert it into a hidden representation with Bert or vit for image or any other types of models and then we combine all",
    "start": "551279",
    "end": "558420"
  },
  {
    "text": "these hidden representations together in this combiner layer which could be tabnet or concatenation with like a",
    "start": "558420",
    "end": "564060"
  },
  {
    "text": "multi-layer perceptron lots of different types and then whatever your output feature is gets its own decoder which",
    "start": "564060",
    "end": "570420"
  },
  {
    "text": "then gives you the prediction so it could be like a classification results or regression Etc and so this is kind of",
    "start": "570420",
    "end": "576420"
  },
  {
    "text": "the underlying abstraction in the on the Deep learning side that makes this modularity possible and the flexibility is very high as well",
    "start": "576420",
    "end": "583680"
  },
  {
    "text": "so you could have you know regression tests speech verification text classification forecasting binary",
    "start": "583680",
    "end": "588959"
  },
  {
    "text": "classification image captioning lots of different things you can do and it's all as simple as just these very small",
    "start": "588959",
    "end": "594060"
  },
  {
    "text": "configurations so here you can see lots of different ways that you can combine these things in different ways to",
    "start": "594060",
    "end": "600240"
  },
  {
    "text": "achieve the kind of result that you're looking for and now let me talk a little bit about",
    "start": "600240",
    "end": "605519"
  },
  {
    "text": "the model repo that I mentioned so one nice thing about predabase that it's very easy to iterate on your models over",
    "start": "605519",
    "end": "610980"
  },
  {
    "text": "time if you want to try different things so the first thing you might do is build a baseline model without specify",
    "start": "610980",
    "end": "617519"
  },
  {
    "text": "anything it's kind of like an automl feature in a way you train the model you then want to try a different combiner",
    "start": "617519",
    "end": "622740"
  },
  {
    "text": "say well the automl system selected whatever let's try tabnet train it again maybe you want to do hyper parameter",
    "start": "622740",
    "end": "628860"
  },
  {
    "text": "optimization over your learning rate that's again just a one-line configuration change and then maybe you",
    "start": "628860",
    "end": "634680"
  },
  {
    "text": "want to try a different type of normalization and then once again that's just a simple change and you can train a model and what do you get out of this",
    "start": "634680",
    "end": "640920"
  },
  {
    "text": "you get a very nice lineage this is not the predevice UI by the way it's just a little visual representation but essentially",
    "start": "640920",
    "end": "648480"
  },
  {
    "text": "what you get out of it is this very nice lineage that gives you multiple different things one is kind of the kind of the dag essentially that shows you",
    "start": "648480",
    "end": "655500"
  },
  {
    "text": "you know how the mall has evolved over time you also get the different the diffs between the yaml so it's very easy",
    "start": "655500",
    "end": "660959"
  },
  {
    "text": "to see you know the underlying declarative represent like what has changed it's usually just a couple lines",
    "start": "660959",
    "end": "666660"
  },
  {
    "text": "and then you also get that sort of nice visibility into how the model performance is changing over time as",
    "start": "666660",
    "end": "671880"
  },
  {
    "text": "well and so if you compare this to like you know someone shipping a Jupiter notebook to someone else that has to",
    "start": "671880",
    "end": "677220"
  },
  {
    "text": "figure out how to try to run that in their environment and make some changes and then send it back to someone else so much more natural way to iterate on your",
    "start": "677220",
    "end": "683700"
  },
  {
    "text": "models over time next I want to talk about the Ludwig on Ray component particularly since this is",
    "start": "683700",
    "end": "689279"
  },
  {
    "text": "the ray Summit and I think there's a lot of interesting stuff we're doing here so in ludwig0.4 which came out about a year",
    "start": "689279",
    "end": "695760"
  },
  {
    "text": "ago we introduced Ludwig on Ray as a back end it combines multiple different steps of uh of pre-processing training",
    "start": "695760",
    "end": "703079"
  },
  {
    "text": "and evaluation all together using different parts of the ray ecosystem so we use Das gun ray for pre-processing",
    "start": "703079",
    "end": "709019"
  },
  {
    "text": "Horvat on Ray for training and then dasco and ready for model evaluation and we also provide uh Ray tune as an",
    "start": "709019",
    "end": "716820"
  },
  {
    "text": "abstraction which I showed you a little bit up through the SDK which again we use uh I think one thing that's particularly interesting is that in",
    "start": "716820",
    "end": "722579"
  },
  {
    "text": "Ludwig on Ray again all open source you can do distribute training per trial so you can also test out different uh",
    "start": "722579",
    "end": "729360"
  },
  {
    "text": "parallel configurations very naturally just by modifying the config and so one thing that I think is very",
    "start": "729360",
    "end": "735959"
  },
  {
    "text": "interesting about what we're doing with Ludwig on Ray and predabase is that we're solving a lot of the nitty-gritty",
    "start": "735959",
    "end": "741720"
  },
  {
    "text": "optimization problems for you so you know while Ray is very powerful very flexible oftentimes there are some",
    "start": "741720",
    "end": "747720"
  },
  {
    "text": "challenges that arise when you're trying to eke out the Optimal Performance for different tasks and so you know with",
    "start": "747720",
    "end": "754260"
  },
  {
    "text": "Ludwig we try to hide all that complexity from you and a really good example of this is what we've done with Ray datasets so we've worked very",
    "start": "754260",
    "end": "760079"
  },
  {
    "text": "closely with the folks at Ray in any scale particularly Clark zenzo who gave a talk earlier with Ray datasets and",
    "start": "760079",
    "end": "767940"
  },
  {
    "text": "what I think is really powerful about this is that you know without making any changes to the user facing side of the",
    "start": "767940",
    "end": "773279"
  },
  {
    "text": "code we swapped out our previous data loader with raid data sets which you can see here now is a bridge between the",
    "start": "773279",
    "end": "779700"
  },
  {
    "text": "desk and the horrified portions of training and achieve some pretty impressive results so previously with",
    "start": "779700",
    "end": "785820"
  },
  {
    "text": "pedestorm we had a big bottleneck that existed with tabular data and as you can see here train this particular model",
    "start": "785820",
    "end": "791339"
  },
  {
    "text": "which I believe is the Higgs boson model which is about one gigabyte of parquet data um you can see that over time",
    "start": "791339",
    "end": "798680"
  },
  {
    "text": "the performance of Ray data sets was much faster in terms of gained a good performance than using pedestorm and it",
    "start": "798680",
    "end": "805200"
  },
  {
    "text": "got there also to a higher overall performance as well and that's in large part due to the global Shuffle that Ray",
    "start": "805200",
    "end": "811620"
  },
  {
    "text": "datasets provides which pedestorm is not able to provide and you can see that it was also able to get much better results",
    "start": "811620",
    "end": "817019"
  },
  {
    "text": "as we scaled up to more gpus so the scaling efficiency of the pipeline was a sustained 92 percent as compared to",
    "start": "817019",
    "end": "824820"
  },
  {
    "text": "pedestorm which was quickly dropping off as we scaled up to more gpus and so this is some of the example of how Ludwig",
    "start": "824820",
    "end": "831180"
  },
  {
    "text": "abstracts away a lot of this complexity and gives you the best in class performance without the user even having to be aware of it",
    "start": "831180",
    "end": "837480"
  },
  {
    "text": "finally I want to talk a little bit about what we're doing with the serverless abstraction of engines so one",
    "start": "837480",
    "end": "843060"
  },
  {
    "text": "thing that I think is often overlooked when we talk about doing distributed training Etc is how exactly you figure",
    "start": "843060",
    "end": "849480"
  },
  {
    "text": "out what the right set of resources should be to do your model training and so one thing that I think a lot of",
    "start": "849480",
    "end": "856079"
  },
  {
    "text": "people have not yet started doing but we make very heavy use of on the predabase side is heterogeneous Ray clusters so",
    "start": "856079",
    "end": "862680"
  },
  {
    "text": "instead of just having one worker type we actually mix together multiple worker types depending on the workload and so",
    "start": "862680",
    "end": "869820"
  },
  {
    "text": "we might reserve some CPU instances for doing pre-processing and data ingest and",
    "start": "869820",
    "end": "875220"
  },
  {
    "text": "some GPU instances for doing training and then we do fancy things with the ray API like saying num CPUs equals zero on",
    "start": "875220",
    "end": "881760"
  },
  {
    "text": "the GPU workers to make sure that none of the data processing tasks actually land on those machines right and the way",
    "start": "881760",
    "end": "888000"
  },
  {
    "text": "that we can achieve this is again through the the declarative aspect of Ludwig right so we know which model",
    "start": "888000",
    "end": "893880"
  },
  {
    "text": "types are more complex than others and so we know which ones require gpus versus which ones you know if the for",
    "start": "893880",
    "end": "900300"
  },
  {
    "text": "example the forecasting and binary classification down the bottom right are the models being used we don't necessarily need to scale up to a lot of",
    "start": "900300",
    "end": "906720"
  },
  {
    "text": "gpus maybe you know CPUs is sufficient for these model architectures right so we're very capable through the declare",
    "start": "906720",
    "end": "913139"
  },
  {
    "text": "of representation which you wouldn't have if you were just writing python right able to determine okay what's the",
    "start": "913139",
    "end": "919680"
  },
  {
    "text": "right type of Hardware we need to train this model and we also kind of take that a step",
    "start": "919680",
    "end": "925320"
  },
  {
    "text": "further in terms of our ability to run multiple parallel workloads on array cluster at a time so here's an example",
    "start": "925320",
    "end": "932100"
  },
  {
    "text": "of how you would normally run some kind of different workloads on array cluster",
    "start": "932100",
    "end": "938220"
  },
  {
    "text": "so you might have here one user who wants to queue up some work that's going",
    "start": "938220",
    "end": "943560"
  },
  {
    "text": "to do some pre-processing followed by data ingest and training so this is all like stuff that Ludwig will do for you",
    "start": "943560",
    "end": "948839"
  },
  {
    "text": "and then at the top there you can see what's happening in Rey under the hood and this is assuming that we're running",
    "start": "948839",
    "end": "954959"
  },
  {
    "text": "on like a kubernetes cluster so we're going to acquire the node from the kubernetes cluster to take some time to",
    "start": "954959",
    "end": "961500"
  },
  {
    "text": "image it Etc then we're going to actually do the pre-processing and then there's going to be some time that's going to be spent acquiring the GPU node",
    "start": "961500",
    "end": "968160"
  },
  {
    "text": "and then running the actual training and then you can see that after the pre-processing completes some of those CPU resources can be freed up While",
    "start": "968160",
    "end": "975480"
  },
  {
    "text": "others will be retained for doing the data ingest and shuffling and so as you can see there's no ability",
    "start": "975480",
    "end": "981660"
  },
  {
    "text": "to really like reuse resources across these two different workloads and so the total time to completion is exactly kind",
    "start": "981660",
    "end": "987360"
  },
  {
    "text": "of the horizontal width of these boxes but what we've been able to do with predabase is be able to pack multiple",
    "start": "987360",
    "end": "993899"
  },
  {
    "text": "different workloads onto the same Ray clusters when it's optimal to do so and",
    "start": "993899",
    "end": "999000"
  },
  {
    "text": "so the really nice benefit here is that we're able to take those resources that were otherwise sitting idle in the",
    "start": "999000",
    "end": "1004880"
  },
  {
    "text": "cluster and instead of just releasing them you know through the ray autoscaler we're able to essentially take those",
    "start": "1004880",
    "end": "1010519"
  },
  {
    "text": "resources and transfer them over to the other workload so the total time to completion is actually reduced by a significant amount",
    "start": "1010519",
    "end": "1017720"
  },
  {
    "text": "right so this kind of optimization is possible because of the fact that we provide these rate clusters as an",
    "start": "1017720",
    "end": "1023779"
  },
  {
    "text": "abstraction as opposed to giving the user kind of direct access so that's a difference between you know the low code",
    "start": "1023779",
    "end": "1029839"
  },
  {
    "text": "versus full code uh way of thinking about the problem and a quick kind of SDK example of how",
    "start": "1029839",
    "end": "1036798"
  },
  {
    "text": "we do this how an end user thinks about it in predabase is they create an engine which again is like this virtual Ray",
    "start": "1036799",
    "end": "1043280"
  },
  {
    "text": "cluster abstraction and in this case they say you know I want to use this type of CPU node this type of GPU node",
    "start": "1043280",
    "end": "1049760"
  },
  {
    "text": "what kind of the Min and max number of instances are what the auto suspend timeout is and then they just use that",
    "start": "1049760",
    "end": "1055160"
  },
  {
    "text": "engine for any one of their functions so in this case we have a pical query that wants to predict the sales price",
    "start": "1055160",
    "end": "1061360"
  },
  {
    "text": "from the housing prices table for all the houses in Los Angeles and then they",
    "start": "1061360",
    "end": "1067460"
  },
  {
    "text": "just give it that engine they can also optionally set their default engine as well",
    "start": "1067460",
    "end": "1073039"
  },
  {
    "text": "so here's where we kind of get into the nitty-gritty of like the predabase architecture so peeling back the current",
    "start": "1073039",
    "end": "1078140"
  },
  {
    "text": "a little bit so we run everything on top of kubernetes um and so uh we use very extensively the",
    "start": "1078140",
    "end": "1085640"
  },
  {
    "text": "kubrey project which some of you may have heard about earlier um and so what essentially we do is we",
    "start": "1085640",
    "end": "1091400"
  },
  {
    "text": "have a single control plane that uses uh temporal as a workflow orchestration layer that then creates the ray cluster",
    "start": "1091400",
    "end": "1098660"
  },
  {
    "text": "resources using the kubray operator and then kubernetes will provision those Ray clusters on behalf of of the tenants",
    "start": "1098660",
    "end": "1107120"
  },
  {
    "text": "and then within that we have a temporal worker that actually runs inside the ray cluster that pulls for model training",
    "start": "1107120",
    "end": "1115520"
  },
  {
    "text": "jobs to be performed or other workloads like pico queries Etc from the temporal",
    "start": "1115520",
    "end": "1121280"
  },
  {
    "text": "server then executes it using the standard Ray dot remote task scheduler and then from there the rest is pretty",
    "start": "1121280",
    "end": "1127520"
  },
  {
    "text": "standard ludagon Ray with the exception that we also have some deep Integrations with data lakes and data warehouses to",
    "start": "1127520",
    "end": "1133460"
  },
  {
    "text": "for example pull data at a snowflake or bigquery write data results back to snowflake or bigquery Etc and so one",
    "start": "1133460",
    "end": "1140480"
  },
  {
    "text": "reason why we do this instead of using a jobs like API is that because we have this pql capability which is like a SQL",
    "start": "1140480",
    "end": "1146900"
  },
  {
    "text": "execution engine right we need to be able to support these interactive workloads so for example if a user wants",
    "start": "1146900",
    "end": "1152720"
  },
  {
    "text": "to say select star from my table predict this thing oh I actually want to slice on this other set of data you don't want",
    "start": "1152720",
    "end": "1159320"
  },
  {
    "text": "each of those to be its own Ray cluster because then in that case you're having to constantly create tear down create",
    "start": "1159320",
    "end": "1166039"
  },
  {
    "text": "tear down resources you'd much rather be able to have those resources be hot and fresh so that you can you know very",
    "start": "1166039",
    "end": "1172700"
  },
  {
    "text": "quickly get results back to the user and so that's why we use this kind of live",
    "start": "1172700",
    "end": "1177980"
  },
  {
    "text": "Ray cluster abstraction instead of just you know thinking of it as like a batch processing job",
    "start": "1177980",
    "end": "1184760"
  },
  {
    "text": "and all this architecture is multi-cloud native so our control plane that I",
    "start": "1184760",
    "end": "1190220"
  },
  {
    "text": "showed you before which was this Orange Box here can also be reused across",
    "start": "1190220",
    "end": "1196220"
  },
  {
    "text": "different Cloud providers so what we do is we run all the compute within the customer's environment so if the",
    "start": "1196220",
    "end": "1202400"
  },
  {
    "text": "customer is running in Google cloud or AWS or Azure all of that can be done",
    "start": "1202400",
    "end": "1208220"
  },
  {
    "text": "from a single control plane that we manage and then ultimately you know we don't have to worry about data",
    "start": "1208220",
    "end": "1215179"
  },
  {
    "text": "management on our side right so all the data management you know we move the compute to where the data is essentially",
    "start": "1215179",
    "end": "1220640"
  },
  {
    "text": "instead of having to tell users like hey move your data into our environment right so we're not a data warehouse",
    "start": "1220640",
    "end": "1225980"
  },
  {
    "text": "we're just providing the ml ml layer on top of the data",
    "start": "1225980",
    "end": "1231200"
  },
  {
    "text": "all right and uh I actually finished way early so hopefully there'll be time for",
    "start": "1231200",
    "end": "1237020"
  },
  {
    "text": "questions um but yeah thank you for uh coming to my talk today um please check us out that is our",
    "start": "1237020",
    "end": "1243140"
  },
  {
    "text": "website we're also on Twitter uh we publish some stuff on medium a lot of it's Ludwig open source content and of",
    "start": "1243140",
    "end": "1249320"
  },
  {
    "text": "course uh you can find Ludwig on GitHub as well and I encourage you to check that out and with that um open it up to",
    "start": "1249320",
    "end": "1254960"
  },
  {
    "text": "any questions thank you",
    "start": "1254960",
    "end": "1258279"
  },
  {
    "text": "thank you",
    "start": "1260020",
    "end": "1263020"
  },
  {
    "text": "hi I'd love for you to go back to the slide on the different Technologies you used for different phases of the machine",
    "start": "1273260",
    "end": "1280640"
  },
  {
    "text": "learning life cycle on there you mentioned using dask on Ray",
    "start": "1280640",
    "end": "1285980"
  },
  {
    "text": "for certain operations and Ludwig on Ray for different ones I'd love if you could explain some of the pros and cons and",
    "start": "1285980",
    "end": "1293360"
  },
  {
    "text": "why you sort of made the decision to add extra like Technologies right right okay so maybe it's actually even better to",
    "start": "1293360",
    "end": "1300140"
  },
  {
    "text": "show you this one because I think it's a little bit simpler so Ludwig uh so this is all Ludwig",
    "start": "1300140",
    "end": "1306200"
  },
  {
    "text": "essentially so Ludwig is like a top level layer for this declarative ml system and under the hood we use dask on",
    "start": "1306200",
    "end": "1314000"
  },
  {
    "text": "Ray horovod on Ray as components within the architecture and the reason for that is that we don't just take data that's",
    "start": "1314000",
    "end": "1322940"
  },
  {
    "text": "like ready to be fed into the the pytorch model right it like the data might be raw strings or it might be",
    "start": "1322940",
    "end": "1329659"
  },
  {
    "text": "unnormalized integers or you know unindexed categorical features so what we need to do is run the kind of data",
    "start": "1329659",
    "end": "1336799"
  },
  {
    "text": "transformation which could be you know a fit operation to like for normalization computing like mean and standard",
    "start": "1336799",
    "end": "1341840"
  },
  {
    "text": "deviation or whatever or Computing like you know what the tokenization should be of the text",
    "start": "1341840",
    "end": "1348740"
  },
  {
    "text": "features Etc so and then applying that and so that all happens in dasc on Ray today although we're trying to move some",
    "start": "1348740",
    "end": "1354620"
  },
  {
    "text": "of that to Ray data sets to kind of get better pipelining and then the training part itself happens on Horvat on Ray",
    "start": "1354620",
    "end": "1360500"
  },
  {
    "text": "with data being fed from Ray datasets and that's you know to get that distributed data parallel training",
    "start": "1360500",
    "end": "1366380"
  },
  {
    "text": "aspect so it's ultimately to answer your question a question a matter of trying",
    "start": "1366380",
    "end": "1371659"
  },
  {
    "text": "to fuse together these parts that are normally thought of as separate into a single end-to-end job and then the",
    "start": "1371659",
    "end": "1377059"
  },
  {
    "text": "really cool thing about it is that because we do the pre-processing and the model inference together in one",
    "start": "1377059",
    "end": "1382940"
  },
  {
    "text": "application layer we can also compile a servable model that compasses both of those into one torch scriptable like",
    "start": "1382940",
    "end": "1391280"
  },
  {
    "text": "thing that can be served in like you know race serve or Trident Etc and that means that you don't have to figure out",
    "start": "1391280",
    "end": "1397220"
  },
  {
    "text": "how to rewrite your data transformation code to be servable right because normally you don't want to use pandas at",
    "start": "1397220",
    "end": "1403520"
  },
  {
    "text": "serving time in most cases so hopefully that answers your question",
    "start": "1403520",
    "end": "1409419"
  },
  {
    "text": "hi my name is nikunj in the beginning you showed a slide where we talked about automl and then making it more",
    "start": "1414980",
    "end": "1421460"
  },
  {
    "text": "complicated with Advanced use cases looking at that slide it feels like it",
    "start": "1421460",
    "end": "1427039"
  },
  {
    "text": "could be more suited like the entire application could be more suited for like speech or text speech Vision based",
    "start": "1427039",
    "end": "1434120"
  },
  {
    "text": "models and less so for like the very structured data set like recommendation systems or fraud detection is that a",
    "start": "1434120",
    "end": "1439580"
  },
  {
    "text": "fair understanding yeah so we actually um have done a lot to uh you know in",
    "start": "1439580",
    "end": "1445039"
  },
  {
    "text": "some ways I think the question is like a matter of you know where deep learning is at as well like you know is deep",
    "start": "1445039",
    "end": "1451460"
  },
  {
    "text": "learning state of the art on some of these tasks so what I would say is that definitely um I think uh cases where you have NLP",
    "start": "1451460",
    "end": "1458960"
  },
  {
    "text": "computer vision or in particular mixed modality data sets which I think is probably the most underrepresented but",
    "start": "1458960",
    "end": "1464720"
  },
  {
    "text": "most real world scenario is like you have companies that have some text from their users like comments some images",
    "start": "1464720",
    "end": "1470840"
  },
  {
    "text": "that they've uploaded plus some metadata about them like who they are where they live Etc like ideally you'd like to be",
    "start": "1470840",
    "end": "1476960"
  },
  {
    "text": "able to use all of those things in a model right not just the type of data so to me I think the reality is that like",
    "start": "1476960",
    "end": "1482780"
  },
  {
    "text": "yes people do a lot of like tabular only problems today but in reality you want to be able to combine the tabular in the",
    "start": "1482780",
    "end": "1489020"
  },
  {
    "text": "mix and the more Rich data together and that's exactly what Ludwig is able to do for you now if you do only have tabular",
    "start": "1489020",
    "end": "1495080"
  },
  {
    "text": "data deep learning you know sometimes we'll do well with tabnet sometimes it's not the best that's why in loot 0.6",
    "start": "1495080",
    "end": "1501799"
  },
  {
    "text": "we're adding support for gradient boosted trees as an option so this same syntax you can just say model equals",
    "start": "1501799",
    "end": "1508159"
  },
  {
    "text": "light GBM and then it will use light gbm1 Ray but by default we still you know for full flexibility like there's",
    "start": "1508159",
    "end": "1514760"
  },
  {
    "text": "some limitations about light G band like you only have one output feature it only supports the tabular data types for now",
    "start": "1514760",
    "end": "1520460"
  },
  {
    "text": "so for full flexibility like the kind of deep neural network encoder compiler decoder architecture still is the way to",
    "start": "1520460",
    "end": "1526520"
  },
  {
    "text": "go but you know that option is something that we're adding into the framework as well",
    "start": "1526520",
    "end": "1532179"
  },
  {
    "text": "hi uh I have seen the previous slide says that the the model right sizing right to",
    "start": "1537279",
    "end": "1546440"
  },
  {
    "text": "design the how many how many results will be used before the model training",
    "start": "1546440",
    "end": "1552320"
  },
  {
    "text": "so I'm wondering how to how to do last because in my personal experience is very hard to do that",
    "start": "1552320",
    "end": "1559640"
  },
  {
    "text": "because when we submit a distributed system a distributed training job we",
    "start": "1559640",
    "end": "1565279"
  },
  {
    "text": "cannot know that so sometimes it will be suffered from like all memory errors so",
    "start": "1565279",
    "end": "1570919"
  },
  {
    "text": "the the job will right and we deploy it with the deployment so if you create",
    "start": "1570919",
    "end": "1576260"
  },
  {
    "text": "again and our memory again right so I'm very interested about that thanks so one",
    "start": "1576260",
    "end": "1582200"
  },
  {
    "text": "thing that we do in Ludwig is there's a feature that we enable like by default in predabase that you can enable in",
    "start": "1582200",
    "end": "1587480"
  },
  {
    "text": "lootware called batch size equals Auto and the nice thing about that is that it will try to Auto detect what's the",
    "start": "1587480",
    "end": "1594380"
  },
  {
    "text": "biggest batch size I can fit in my buffer before I have an out of memory error right and then based on that we",
    "start": "1594380",
    "end": "1600440"
  },
  {
    "text": "can then make sure that we like in terms of vertical scale get the right size of",
    "start": "1600440",
    "end": "1606140"
  },
  {
    "text": "uh the right batch size to basically saturate the GPU memory right now there are still some cases where even with",
    "start": "1606140",
    "end": "1612380"
  },
  {
    "text": "that you'll get some like weird added memory errors and so the nice thing about Ludwig is that you notice that",
    "start": "1612380",
    "end": "1617419"
  },
  {
    "text": "there are a lot of parameters in there but the user isn't specifying all of them so we know which parameters",
    "start": "1617419",
    "end": "1622940"
  },
  {
    "text": "actually can contribute to things like memory pressure and so what we can do is if the user didn't specify those",
    "start": "1622940",
    "end": "1629240"
  },
  {
    "text": "parameters and there's an out of memory error that occurs we can do a retry where we like scale down some of those",
    "start": "1629240",
    "end": "1635179"
  },
  {
    "text": "parameters and then try it again and then you know hopefully it will eventually succeed and in the event",
    "start": "1635179",
    "end": "1640580"
  },
  {
    "text": "where the user specified all those parameters and they just said you know like I want a million batch size or",
    "start": "1640580",
    "end": "1645679"
  },
  {
    "text": "something then we just have to tell them like okay well we couldn't actually make this work like please reduce the batch",
    "start": "1645679",
    "end": "1651020"
  },
  {
    "text": "size right yes really simple question I'm curious",
    "start": "1651020",
    "end": "1658460"
  },
  {
    "text": "um from like the UI perspective and I don't know if you can tell it's this but do",
    "start": "1658460",
    "end": "1664640"
  },
  {
    "text": "you feel like customers are more likely to want to use the UI or more likely to use the SDK or have you seen anything like that it's a great question yeah so",
    "start": "1664640",
    "end": "1671539"
  },
  {
    "text": "it definitely depends on the Persona um so I would say that originally we didn't have the python SDK the python",
    "start": "1671539",
    "end": "1677960"
  },
  {
    "text": "SDK was a response to our customers that were wanting to do more advanced features and yeah shout out to Connor",
    "start": "1677960",
    "end": "1683840"
  },
  {
    "text": "who was the one who wrote that um but yeah so the python SDK actually",
    "start": "1683840",
    "end": "1688940"
  },
  {
    "text": "has seen a lot of adoption from our customers because you know that typically people will want to do things like Eda like exploratory data analysis",
    "start": "1688940",
    "end": "1695960"
  },
  {
    "text": "and their jupyter notebook and then they can run the python SDK in that same environment and the nice thing about our",
    "start": "1695960",
    "end": "1702140"
  },
  {
    "text": "python SDK is that it doesn't you can run a jupyter notebook and not incur a lot of tech debt because all those",
    "start": "1702140",
    "end": "1707419"
  },
  {
    "text": "artifacts are you know version controlled and managed on the back end so whatever you run there is not",
    "start": "1707419",
    "end": "1713360"
  },
  {
    "text": "something that you have to treat as like you know a source of Truth like it all lives in predabase now the UI on the",
    "start": "1713360",
    "end": "1719000"
  },
  {
    "text": "other hand is still very useful I think for like people who are like first-time users or maybe more the analyst Persona",
    "start": "1719000",
    "end": "1725480"
  },
  {
    "text": "that's not as comfortable writing python so you know the UI is a very simple way to get familiar with the tool",
    "start": "1725480",
    "end": "1732140"
  },
  {
    "text": "particularly if you don't know what all the options are like what are all the different you know parameters I can even",
    "start": "1732140",
    "end": "1737360"
  },
  {
    "text": "configure the UI we've done a huge investment on making it very clear to the user kind of like the settings page",
    "start": "1737360",
    "end": "1743659"
  },
  {
    "text": "in vs code like what are the commonly selected parameter is like one of the ones that give you the most bang for your buck what do they do like when",
    "start": "1743659",
    "end": "1749900"
  },
  {
    "text": "would you change set the value to one value versus another so I think the UI is a really great learning tool but then",
    "start": "1749900",
    "end": "1756020"
  },
  {
    "text": "you know the SDK and you know even doing some things with the people query language to create models are there for",
    "start": "1756020",
    "end": "1761120"
  },
  {
    "text": "like more advanced users who are ready to productionize things",
    "start": "1761120",
    "end": "1766480"
  },
  {
    "text": "yeah sorry if it's a very uh I'm new to this so I don't quite understand when you have an encoder and decoder you have",
    "start": "1769460",
    "end": "1776539"
  },
  {
    "text": "the combiner uh what exactly is happening if I have let's say two input feature to one single Auto feature is a",
    "start": "1776539",
    "end": "1784039"
  },
  {
    "text": "combiner similar to working in the latent space the Transformer tokens what are these yeah it's a great question I",
    "start": "1784039",
    "end": "1789620"
  },
  {
    "text": "did try to gloss over this because there's a lot of complexity to it but yeah essentially the way that we think",
    "start": "1789620",
    "end": "1794779"
  },
  {
    "text": "about it is like let's say I have a text feature and then I have a numerical feature or something like that it's like",
    "start": "1794779",
    "end": "1800539"
  },
  {
    "text": "how do I build a model that can incorporate both text and you know like tabular features at once and the",
    "start": "1800539",
    "end": "1807260"
  },
  {
    "text": "solution is exactly to create embeddings into a latent space right so the",
    "start": "1807260",
    "end": "1812720"
  },
  {
    "text": "encoders create the embeddings and then the with those embeddings we can combine them in a number of different ways one",
    "start": "1812720",
    "end": "1818659"
  },
  {
    "text": "of which might be to concatenate them all into a vector and so we have a a combiner called a concat combiner that",
    "start": "1818659",
    "end": "1825500"
  },
  {
    "text": "does exactly that and then feeds it through a multi-layer perceptron or you could use a trans former like so you can",
    "start": "1825500",
    "end": "1831140"
  },
  {
    "text": "have an attention mechanism that sits over each of these latent vectors and so that's another type of trans another",
    "start": "1831140",
    "end": "1836840"
  },
  {
    "text": "type combiner we have we also have a tabnet one which is like an attention mechanism plus some other fancy things",
    "start": "1836840",
    "end": "1842360"
  },
  {
    "text": "that try to simulate a decision tree so there are lots of different options available to you there but that's",
    "start": "1842360",
    "end": "1848299"
  },
  {
    "text": "essentially the way that these things are like you can take these like multiple different modalities and combine them all together into a single",
    "start": "1848299",
    "end": "1855260"
  },
  {
    "text": "model and so the encoders the combiners and the decoders are all learnable by default so you know you'll back",
    "start": "1855260",
    "end": "1861500"
  },
  {
    "text": "propagate fully from the decoders through the encoders it's not the case that like any of these are like fixed",
    "start": "1861500",
    "end": "1867740"
  },
  {
    "text": "parameters right okay thanks yeah",
    "start": "1867740",
    "end": "1872080"
  },
  {
    "text": "yeah let's take remaining questions offline all right well thank you all so much for coming I'm really glad we got",
    "start": "1880000",
    "end": "1885020"
  },
  {
    "text": "to have some good q a and uh yeah if you're interested in learning more about what we're doing with Ludwig or privates please come talk to me yeah",
    "start": "1885020",
    "end": "1893080"
  }
]