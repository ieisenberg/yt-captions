[
  {
    "start": "0",
    "end": "47000"
  },
  {
    "text": "thank you uh hi everyone uh today I'll be talking about the challenges and Lessons Learned uh from supporting uh",
    "start": "5920",
    "end": "11320"
  },
  {
    "text": "multimodality on Vim uh first of all just like overall agenda this talk uh this talk is about",
    "start": "11320",
    "end": "18080"
  },
  {
    "text": "like a brief history of how we supported multimodel models on v um including the",
    "start": "18080",
    "end": "23640"
  },
  {
    "text": "challenges we encountered how we solve them and the lessons we learned and I'll wrap up with the future role maps and",
    "start": "23640",
    "end": "29439"
  },
  {
    "text": "the future work uh in this work stream and this talk is not about using diffusion models on VM that's not on the",
    "start": "29439",
    "end": "35399"
  },
  {
    "text": "RO map yet and we also won't be talking about multimodel outputs like image or",
    "start": "35399",
    "end": "40879"
  },
  {
    "text": "AIO audio generation we'll be focusing on multimodel inputs and text",
    "start": "40879",
    "end": "46920"
  },
  {
    "text": "output uh before I kick in uh I want to introduce the multi modality work string on V uh first of all myself I'm a soft",
    "start": "46920",
    "end": "54199"
  },
  {
    "start": "47000",
    "end": "216000"
  },
  {
    "text": "engineer on the ml platform T Roblox uh we're activ hiring my manager is sitting right here so for talk to her right",
    "start": "54199",
    "end": "60719"
  },
  {
    "text": "after uh I'm also committer on the VM project uh I've been working on the project since february24 I as many of you here are the",
    "start": "60719",
    "end": "69720"
  },
  {
    "text": "was a user of VM so my first contribution to the project was like online serving Benchmark to compare",
    "start": "69720",
    "end": "76040"
  },
  {
    "text": "different serving back out there TGI T rtlm um dbmi and since April I took over",
    "start": "76040",
    "end": "83119"
  },
  {
    "text": "the multimodality support on vrn to add support for all different kinds of like large multimodal models uh I want to",
    "start": "83119",
    "end": "90759"
  },
  {
    "text": "emphasize that this is team effort uh and here are a few key contributors in this world stream uh first of all Cyrus",
    "start": "90759",
    "end": "97640"
  },
  {
    "text": "uh he a p student from Hong Kong he and I uh kind of co coad uh this grow stream",
    "start": "97640",
    "end": "102680"
  },
  {
    "text": "including all the RO roap items and the overall direction uh for multimodal models we so we also have Zang and Shia",
    "start": "102680",
    "end": "110040"
  },
  {
    "text": "who have uh contributed to the W stream extensively Shia is from n skill she actually made the first PPR uh for multi",
    "start": "110040",
    "end": "117719"
  },
  {
    "text": "Vision langage models I'll talk about in a bit and last but not least uh we have a few community members who have been uh",
    "start": "117719",
    "end": "123960"
  },
  {
    "text": "contributing to W stream adding models adding features uh optimizing uh the vision language support Vision language",
    "start": "123960",
    "end": "130200"
  },
  {
    "text": "model support uh in the very recent uh two months uh and finally I like to",
    "start": "130200",
    "end": "135280"
  },
  {
    "text": "thank my colleagues at ROBLOX uh for supporting me to work on vrn parttime and G me giving me feedback uh on my",
    "start": "135280",
    "end": "143200"
  },
  {
    "text": "overall design okay uh so I'd like to give you like overall like overview of are large",
    "start": "143200",
    "end": "150360"
  },
  {
    "text": "model large multimodal language models and why it makes sense to support them on VF so recently like as you might you",
    "start": "150360",
    "end": "156400"
  },
  {
    "text": "might heard like a lot of like Multi Image language models or audio language models all they do is they",
    "start": "156400",
    "end": "162840"
  },
  {
    "text": "leverage a language back language model backbone along with the encoder uh of a non-text modality uh meaning you first",
    "start": "162840",
    "end": "170280"
  },
  {
    "text": "uh kind of encode your image or audio clip or video clips uh with this Vision",
    "start": "170280",
    "end": "175959"
  },
  {
    "text": "encoder sometimes it's a clip encoder sometimes a whisper encoder into the embeding space and you project that to",
    "start": "175959",
    "end": "182080"
  },
  {
    "text": "the same embedding space with tax embeddings you concate or merge your the two embeddings and you send that through",
    "start": "182080",
    "end": "188319"
  },
  {
    "text": "the language model to do the actual understanding or generation and on the left we have the lava cure leverage a",
    "start": "188319",
    "end": "194920"
  },
  {
    "text": "clip uh Vision coder on the right we have the q1 Audio model that Lage whisper coder and you can as you can see",
    "start": "194920",
    "end": "201040"
  },
  {
    "text": "here the most competition happens in the language backbone not division encoder and this is why it makes sense to to",
    "start": "201040",
    "end": "207239"
  },
  {
    "text": "support this such models on vrm cuz as we all know vrm is a very space efficient fast uh engine for language",
    "start": "207239",
    "end": "215720"
  },
  {
    "text": "model so next I'll talk about like the brief history like of the support for multimodel models on vrn uh back in",
    "start": "215720",
    "end": "222879"
  },
  {
    "start": "216000",
    "end": "318000"
  },
  {
    "text": "March 2024 uh sh from incal made the first PR for PC support for 1.5 and I'll",
    "start": "222879",
    "end": "229840"
  },
  {
    "text": "briefly talk about why it's a PC support in a bit and in April 24 is where I came",
    "start": "229840",
    "end": "235319"
  },
  {
    "text": "in I met RFC uh for uh kind of refactoring the overall multi mity support and this RFC has since become",
    "start": "235319",
    "end": "242680"
  },
  {
    "text": "like like a Tracker uh for all the road maps updates for this world stream and",
    "start": "242680",
    "end": "248239"
  },
  {
    "text": "in June 24 is where we finish the first Mouse Stone where we refracture the user interface and support open a vision API",
    "start": "248239",
    "end": "255920"
  },
  {
    "text": "comp compatible inference meaning you can you can deploy a open source Vision language model and use the same open AI",
    "start": "255920",
    "end": "262479"
  },
  {
    "text": "client to call this model and July 24 is where we generalized the groundwork for",
    "start": "262479",
    "end": "269000"
  },
  {
    "text": "supporting vision language model to all modalities uh this is really like a developer facing uh change uh all",
    "start": "269000",
    "end": "275120"
  },
  {
    "text": "developers if they have like audio language model they can come in plug in the audio uh plugin to the to the VM and",
    "start": "275120",
    "end": "281199"
  },
  {
    "text": "support it on VM uh so it's not tied to image in particular and one more important thing here is that we support",
    "start": "281199",
    "end": "287080"
  },
  {
    "text": "Dynamic input shape that I'll talk about in a b as well and finally like September 24 is where we kind of wrap up",
    "start": "287080",
    "end": "294120"
  },
  {
    "text": "the third Milestone where we generalize we actually generalize to multi-image audio video models and we actually got",
    "start": "294120",
    "end": "301080"
  },
  {
    "text": "Day One release uh for Pixel the first multimodel model from M mroi and day",
    "start": "301080",
    "end": "306440"
  },
  {
    "text": "release uh for multimodel Lama 3.2 and finally I encourage you to check out the",
    "start": "306440",
    "end": "311960"
  },
  {
    "text": "r map it's in the same issue as RFC uh issue 4194 on the GitHub",
    "start": "311960",
    "end": "317840"
  },
  {
    "text": "report okay I mentioned uh back in March uh the P the kind of the first PC",
    "start": "317840",
    "end": "324120"
  },
  {
    "text": "support really PC why why is that so let's look at like what what they look like back there this is you initialize a",
    "start": "324120",
    "end": "330880"
  },
  {
    "text": "lava 1.5 back then uh you first initialize the model you format input PR you load the pixel values and you do the",
    "start": "330880",
    "end": "337240"
  },
  {
    "text": "actual inference there are a lot of overhead in this interface first of all there's some metadata that users will",
    "start": "337240",
    "end": "344319"
  },
  {
    "text": "have no idea about uh some developers don't they don't have any idea like what they are actually and you have to",
    "start": "344319",
    "end": "350560"
  },
  {
    "text": "preformat your image uh I guess your input Tok input prompt with the additional input tokens these are input",
    "start": "350560",
    "end": "357280"
  },
  {
    "text": "tokens as placeholders for the actual input uh image addings and this is not huging face format so sometimes the user",
    "start": "357280",
    "end": "364720"
  },
  {
    "text": "have no idea how to get this actual number and lastly uh the image data",
    "start": "364720",
    "end": "370160"
  },
  {
    "text": "format input format is actually pixel values uh a good way a good analogy for this is that pixel values to uh images",
    "start": "370160",
    "end": "378240"
  },
  {
    "text": "are like prompt Tok ID to the text prompt so the user have to understand how to actually process your uh jpeg",
    "start": "378240",
    "end": "385840"
  },
  {
    "text": "image and convert that to pixel values uh and sometimes they don't have no they will have no idea how to do it so",
    "start": "385840",
    "end": "392560"
  },
  {
    "text": "the first uh refactoring we did was to bring in the image processor from Hing face uh to skip that kind of a uh uh",
    "start": "392560",
    "end": "400759"
  },
  {
    "text": "image to pixel values uh uh processing as well as abstract all the user",
    "start": "400759",
    "end": "405919"
  },
  {
    "text": "interface to infer this metadata from the h repo or store somewhere in the in",
    "start": "405919",
    "end": "411319"
  },
  {
    "text": "the vrm repo so as you can see here we simplify the user interface a law uh to",
    "start": "411319",
    "end": "416800"
  },
  {
    "text": "initialize a model same way as how you initialize like a typical just give me the hugging face model ID and the pr",
    "start": "416800",
    "end": "423680"
  },
  {
    "text": "there's no additional format formatting needed it's the same hugging face example prompt and for the image uh you",
    "start": "423680",
    "end": "430280"
  },
  {
    "text": "just need to open that image with the pillow image Library pillow is a very standardized Library uh in Python to",
    "start": "430280",
    "end": "435960"
  },
  {
    "text": "kind of a load image uh into into the their pillow object and finally you just feed that to the prom to the to the LM",
    "start": "435960",
    "end": "442440"
  },
  {
    "text": "class to the actual generation uh this also unlock us to do the end to end like the uh open AI compatible inference",
    "start": "442440",
    "end": "448960"
  },
  {
    "text": "that'll talk that will give example later uh but that's kind of the first Milestone simplify user interface make",
    "start": "448960",
    "end": "454800"
  },
  {
    "text": "it easy to use and next uh once we wrap up the work for La 1.5 we move on to L",
    "start": "454800",
    "end": "461720"
  },
  {
    "text": "1.6 so this is where we we realize we need to support Dynamic size of the",
    "start": "461720",
    "end": "467080"
  },
  {
    "text": "multimodal embeddings uh recall in lava 1.5 the image embeding size for one",
    "start": "467080",
    "end": "472599"
  },
  {
    "text": "image is always going to be 576 this is a static number we always like it doesn't change depending on the input",
    "start": "472599",
    "end": "478039"
  },
  {
    "text": "image but for 1.6 the these changes and this has been the case for a lot of models today of the open source world",
    "start": "478039",
    "end": "485360"
  },
  {
    "text": "where your image embeding size actually V varies depending on the input",
    "start": "485360",
    "end": "490800"
  },
  {
    "text": "resolution so sometimes your image embedding can be 2,000 and sometimes it it can be 15,000 and you have to the the",
    "start": "490800",
    "end": "499039"
  },
  {
    "text": "main task here is that depending depending on your input payload we'll have to pre-process the Tex sequence uh",
    "start": "499039",
    "end": "506479"
  },
  {
    "text": "dynamically and to insert the right number of uh image Place image placeholder tokens because that's how",
    "start": "506479",
    "end": "512680"
  },
  {
    "text": "we're g to swap them with imag Bings in the in the model execution time uh in",
    "start": "512680",
    "end": "518399"
  },
  {
    "text": "hugging phase uh you can see here as example uh they look at the input image",
    "start": "518399",
    "end": "524519"
  },
  {
    "text": "they get and the right number of the image uh number patches these patches get converted to the actual length of",
    "start": "524519",
    "end": "530600"
  },
  {
    "text": "the uh image uh image edings and the finally merge it with input the tax input tokens but one problem is this",
    "start": "530600",
    "end": "538800"
  },
  {
    "text": "happens in the the model execution time meaning uh you uh the the GPU doesn't",
    "start": "538800",
    "end": "544680"
  },
  {
    "text": "know until you run this code and this doesn't work on vrm for two reasons um",
    "start": "544680",
    "end": "550079"
  },
  {
    "text": "first reason is that on vrm our schedule needs to know beforehand uh at a scheduling time how how long that final",
    "start": "550079",
    "end": "557240"
  },
  {
    "text": "sequence is because that's how we allocate the resource the KV cache for the sequence and how we decide if we're",
    "start": "557240",
    "end": "563640"
  },
  {
    "text": "going to run the sequence not open put the queue and secondly this kind of like Dynamic input uh shape changes execution",
    "start": "563640",
    "end": "570360"
  },
  {
    "text": "time is extremely for to own and also it doesn't work well with the torch compile",
    "start": "570360",
    "end": "575680"
  },
  {
    "text": "uh kind of kind of uh static shape uh input that we want to support in the long run and uh you could argue that you",
    "start": "575680",
    "end": "583600"
  },
  {
    "text": "know just move that logic out of the for pass make some helper function uh in the code Cod base make sure it works with",
    "start": "583600",
    "end": "589480"
  },
  {
    "text": "the model but then we quickly realize the challenges are you know what if we",
    "start": "589480",
    "end": "594560"
  },
  {
    "start": "592000",
    "end": "691000"
  },
  {
    "text": "want to generate to multiple modalities like what if going for we're going to have IM image audio Chan video frames",
    "start": "594560",
    "end": "600480"
  },
  {
    "text": "and each of them will have variable length and what if you each each payload is like multiple items right what if I",
    "start": "600480",
    "end": "606519"
  },
  {
    "text": "want to do multi image uh inference and each image itself is also VAR length and",
    "start": "606519",
    "end": "611760"
  },
  {
    "text": "what how do I properly profile the engine to uh to reserve the the GPU",
    "start": "611760",
    "end": "616800"
  },
  {
    "text": "memory for the KV cache with all these multim multimodal data items and what if",
    "start": "616800",
    "end": "622040"
  },
  {
    "text": "there's some kind of additional uh pre-processing needed or in this case uh some model vendors can just Define their",
    "start": "622040",
    "end": "628839"
  },
  {
    "text": "own conversion from image to pixel values inside their own model repo but not on hugging phas or Transformers how",
    "start": "628839",
    "end": "635880"
  },
  {
    "text": "do we do that and finally like what if we want to do mix modalities right today we don't have model like that but I I",
    "start": "635880",
    "end": "642560"
  },
  {
    "text": "won't be surprised going for it we have a model that supports like image and audio and text inputs all together in",
    "start": "642560",
    "end": "647720"
  },
  {
    "text": "the for paath so to kind of summarize the challenge here the the main goal here is to design a framework uh to Pro",
    "start": "647720",
    "end": "655880"
  },
  {
    "text": "to process and format your TX your input Tex sequence with the uh right number of multimodal",
    "start": "655880",
    "end": "662440"
  },
  {
    "text": "placeholder tokens as well as ADD add add a way so that the developers can add",
    "start": "662440",
    "end": "667800"
  },
  {
    "text": "their own logic to uh pre-process uh these multiple multimod data items",
    "start": "667800",
    "end": "672959"
  },
  {
    "text": "meaning convert image to pixel values convert a audio clip to a uh mp uh MP",
    "start": "672959",
    "end": "679519"
  },
  {
    "text": "array uh things like that so this sounds trivial uh but Cyrus and I actually",
    "start": "679519",
    "end": "685079"
  },
  {
    "text": "spend a lot of time on designing such framework and this is actually a very big design and Engineering problem uh",
    "start": "685079",
    "end": "691560"
  },
  {
    "start": "691000",
    "end": "817000"
  },
  {
    "text": "I'll just I'll just skip through uh the whole challenge I guess like the journey but I'll give you the solution so at the",
    "start": "691560",
    "end": "697639"
  },
  {
    "text": "end we we we uh kind of land on this registry based solution uh and you can",
    "start": "697639",
    "end": "703720"
  },
  {
    "text": "skip the Tex on the right on the left because I will show example so on the right we have example of the q& 2vl",
    "start": "703720",
    "end": "710920"
  },
  {
    "text": "model just came out actually they made a model PR from the q& team and here you",
    "start": "710920",
    "end": "717000"
  },
  {
    "text": "can see that uh this model takes image and video as input the way we register",
    "start": "717000",
    "end": "723560"
  },
  {
    "text": "uh kind of like accept the modality is that we have a register uh mapper uh",
    "start": "723560",
    "end": "728880"
  },
  {
    "text": "this defines which modalities are accepted to this model and how do you want to process uh the input data to the",
    "start": "728880",
    "end": "736279"
  },
  {
    "text": "actual kind of kind of last M processing here and you can see here they actually",
    "start": "736279",
    "end": "741519"
  },
  {
    "text": "write their own like input mapper uh for this um for this model and secondly this",
    "start": "741519",
    "end": "747440"
  },
  {
    "text": "multim model regory is also responsible for defining the max number of image tokens for each modality a good analogy",
    "start": "747440",
    "end": "753920"
  },
  {
    "text": "here is that is that a a Max number tokens for each modality is like how you",
    "start": "753920",
    "end": "759160"
  },
  {
    "text": "specify the content Lance of your input uh input uh input tax because you you you do want to send like a limit if it's",
    "start": "759160",
    "end": "765440"
  },
  {
    "text": "beyond the limit then we cannot reject or cannot accept that additional uh",
    "start": "765440",
    "end": "770839"
  },
  {
    "text": "modality uh and for the input stre what we do here is that uh in order to",
    "start": "770839",
    "end": "776040"
  },
  {
    "text": "profile the engine to allocate the right number of like uh KV cache for the engine we need to Define how a dummy uh",
    "start": "776040",
    "end": "783320"
  },
  {
    "text": "sequence looks like with your corresponding dummy input data items and",
    "start": "783320",
    "end": "788440"
  },
  {
    "text": "finally uh we have a input processor I know it sounds bit vague but this is where we put a logic to formulate I",
    "start": "788440",
    "end": "795160"
  },
  {
    "text": "guess to to to format your hugging phas uh compatible prompt to the actual uh",
    "start": "795160",
    "end": "802000"
  },
  {
    "text": "sequence that the model was see this means like extending the tokens by the actual number of like image iding size",
    "start": "802000",
    "end": "808079"
  },
  {
    "text": "tokens uh in the P so uh once we kind of like uh finish",
    "start": "808079",
    "end": "813839"
  },
  {
    "text": "this refracturing uh I'll just show you like the open AI Vision API compatible inference here so today you can use vrm",
    "start": "813839",
    "end": "821880"
  },
  {
    "start": "817000",
    "end": "863000"
  },
  {
    "text": "to serve picture uh good shout out uh I want to shout out to M AI they actually",
    "start": "821880",
    "end": "826920"
  },
  {
    "text": "came directly us directly to us to kind of a open sources model this model was not even on Transformers before V merg",
    "start": "826920",
    "end": "833759"
  },
  {
    "text": "the pr and as you can see here you can Ser the model uh without without too much like data the only me that you",
    "start": "833759",
    "end": "840560"
  },
  {
    "text": "specify here is that you want to limit the number of images to be two and once",
    "start": "840560",
    "end": "845759"
  },
  {
    "text": "you start the server uh you can use your open AI C client to call the server uh",
    "start": "845759",
    "end": "851759"
  },
  {
    "text": "we are fully compatible with the vision API meaning you can send in either a image URL a remote URL or a data uh Bas",
    "start": "851759",
    "end": "860160"
  },
  {
    "text": "64 encoded encoded image and uh I want to shout out to fix",
    "start": "860160",
    "end": "865880"
  },
  {
    "start": "863000",
    "end": "945000"
  },
  {
    "text": "a from Seattle their startup they they develop this audio language model same",
    "start": "865880",
    "end": "871000"
  },
  {
    "text": "way uh you do you you start a server and then uh to call a server you specify",
    "start": "871000",
    "end": "876519"
  },
  {
    "text": "audio Ur um you can put it you can put in like audio uh B 64 encoded uh kind of",
    "start": "876519",
    "end": "883440"
  },
  {
    "text": "a string or the actual remote location of the audio clip same uh it's basically",
    "start": "883440",
    "end": "888839"
  },
  {
    "text": "same interface yeah and uh I want to call out today that we support over 10 over 10 yeah",
    "start": "888839",
    "end": "896759"
  },
  {
    "text": "large multimodal models uh and and we support text input image input audio input and video input um I also want to",
    "start": "896759",
    "end": "904560"
  },
  {
    "text": "emphasize that uh F yeah F actually direct like they came the support came",
    "start": "904560",
    "end": "911120"
  },
  {
    "text": "directly from the model vendors as I mentioned pixel and llama 3.2 we got Day",
    "start": "911120",
    "end": "916519"
  },
  {
    "text": "One release uh of these models and we are very happy uh to work with model vendors going forward uh to open source",
    "start": "916519",
    "end": "923720"
  },
  {
    "text": "their models or release their models on vrm on day one so that users can try them out immediately actually I know a",
    "start": "923720",
    "end": "930399"
  },
  {
    "text": "lot of researchers they uh they use V for you know fast iteration uh evaluation when the model comes comes",
    "start": "930399",
    "end": "936680"
  },
  {
    "text": "out and VR has been their default choice to uh batch process uh these evaluation",
    "start": "936680",
    "end": "942079"
  },
  {
    "text": "site very quickly okay we don't talk about oh it's",
    "start": "942079",
    "end": "947199"
  },
  {
    "start": "945000",
    "end": "1014000"
  },
  {
    "text": "a VM talk so we always going to talk about like performance operations so we enable like uh large multi model multi",
    "start": "947199",
    "end": "954120"
  },
  {
    "text": "model models on vrm and next I'll talk about like how we optimize these models",
    "start": "954120",
    "end": "959720"
  },
  {
    "text": "uh to recall um these models are leveraging a decod only language model",
    "start": "959720",
    "end": "965240"
  },
  {
    "text": "plus a multimodel encoder meaning they are only different from the traditional LMS at prefill stage this is because you",
    "start": "965240",
    "end": "973240"
  },
  {
    "text": "in addition to tokenize your input tax you have to process your adding the additional logic to",
    "start": "973240",
    "end": "980279"
  },
  {
    "text": "process the input tax prom uh and as well as converting your u p images to",
    "start": "980279",
    "end": "986360"
  },
  {
    "text": "pixel values and finally dra your features through this evion encoder and",
    "start": "986360",
    "end": "992000"
  },
  {
    "text": "projector to getting Bings and merge them with your texting Bings that's all",
    "start": "992000",
    "end": "997399"
  },
  {
    "text": "down in the pref stage but at Deco stage basically the generation stage uh",
    "start": "997399",
    "end": "1002920"
  },
  {
    "text": "multimod data is is no longer needed because every information we have is already stored in the KV cache so the",
    "start": "1002920",
    "end": "1009440"
  },
  {
    "text": "inference pattern is identical to um decoding the first um optimization we",
    "start": "1009440",
    "end": "1017040"
  },
  {
    "start": "1014000",
    "end": "1105000"
  },
  {
    "text": "did was uh TP on the multimod encoders um just uh just as a background uh",
    "start": "1017040",
    "end": "1023240"
  },
  {
    "text": "typically these encoders clip encoder or Vis encoder they are very small compared to the actual language language model",
    "start": "1023240",
    "end": "1029558"
  },
  {
    "text": "backbone uh in for instance in lava 1.57 B the clip encoder is only uh smaller",
    "start": "1029559",
    "end": "1036438"
  },
  {
    "text": "than 500 million parameters and previously these encoders are replicated on each GPU uh if you",
    "start": "1036439",
    "end": "1043959"
  },
  {
    "text": "deploy these models like over multiple gpus uh because we feel like this small model",
    "start": "1043959",
    "end": "1049720"
  },
  {
    "text": "uh it doesn't really uh worst the engine effort to T parize this these encoders",
    "start": "1049720",
    "end": "1055280"
  },
  {
    "text": "but then later on we see a huge uh kind of influx of small multimodel models",
    "start": "1055280",
    "end": "1061880"
  },
  {
    "text": "pojama 3B 5 3.5 Vision instruct which also 4 4.5 4.2b and compared to these",
    "start": "1061880",
    "end": "1069640"
  },
  {
    "text": "models the vision Cod itself is actually relatively big uh meaning uh if you",
    "start": "1069640",
    "end": "1074720"
  },
  {
    "text": "replicate if someone want to serve these models over M gpus and each encoder is",
    "start": "1074720",
    "end": "1079880"
  },
  {
    "text": "replicated on each GPU you are in a four GPU kind of scenario you are basically",
    "start": "1079880",
    "end": "1085919"
  },
  {
    "text": "wasting like over a gig uh kind of a u memory uh in your in the GPU so we",
    "start": "1085919",
    "end": "1092320"
  },
  {
    "text": "actually did the TP on all these encoders so that uh we can sign significantly save the space that can be",
    "start": "1092320",
    "end": "1099440"
  },
  {
    "text": "reserved for the KV cache uh which leads to higher throughput overall the second uh kind of",
    "start": "1099440",
    "end": "1106679"
  },
  {
    "start": "1105000",
    "end": "1179000"
  },
  {
    "text": "optimization we did was enable uh multimodel embeddings as the input uh for the context um you know sometimes",
    "start": "1106679",
    "end": "1113640"
  },
  {
    "text": "you don't have to actually run your multimodel data through that encoder you can just like run it somewhere else",
    "start": "1113640",
    "end": "1119280"
  },
  {
    "text": "separately and generate these embeddings uh beforehand and during inference time you send the prompt and the embeding is",
    "start": "1119280",
    "end": "1125679"
  },
  {
    "text": "directly to the VM engine so the inference pattern is just like identical to the uh to a typical LM and you can",
    "start": "1125679",
    "end": "1133360"
  },
  {
    "text": "also cash or hash or and cach them separately somewhere else uh",
    "start": "1133360",
    "end": "1139240"
  },
  {
    "text": "all set of V so you have the the developer will have a lot of more freedom to control how they want to uh",
    "start": "1139240",
    "end": "1145840"
  },
  {
    "text": "save these ineds uh but it does come with a trade-off uh first of all um it",
    "start": "1145840",
    "end": "1151679"
  },
  {
    "text": "adds additional complexity before as a user you just need to worry about sending prom and your pillow image or",
    "start": "1151679",
    "end": "1157840"
  },
  {
    "text": "jpeg image but now you have to kind of pipeline that inference yourself and",
    "start": "1157840",
    "end": "1163440"
  },
  {
    "text": "also sometimes the prefill for the image is actually not that much compared to the prefill for the tax itself so it's a",
    "start": "1163440",
    "end": "1170640"
  },
  {
    "text": "debate whether it's it's really worth doing dealing with this kind of complexity to break up the inference",
    "start": "1170640",
    "end": "1176640"
  },
  {
    "text": "Pipeline and before I kind of wrap up to go to the Future Ro map I added bonus topic",
    "start": "1176640",
    "end": "1183039"
  },
  {
    "start": "1179000",
    "end": "1291000"
  },
  {
    "text": "here multim 3.2 uh actually added like I added this SL last night uh because it",
    "start": "1183039",
    "end": "1188360"
  },
  {
    "text": "came out last week so um first of all I want to give a brief overview of what",
    "start": "1188360",
    "end": "1194000"
  },
  {
    "text": "multimodal L 3.2 looks like it's actually very different it's a totally different Beast uh than the the other",
    "start": "1194000",
    "end": "1200840"
  },
  {
    "text": "kind of visual language models or audio language models out there because it involves cross attentions so unlike",
    "start": "1200840",
    "end": "1208000"
  },
  {
    "text": "other models who that concate your IM image embedding with your texting Bing",
    "start": "1208000",
    "end": "1214000"
  },
  {
    "text": "there's actually like uh cross cross tension layers uh in the language model itself uh and the initial PR uh for this",
    "start": "1214000",
    "end": "1222240"
  },
  {
    "text": "model it was contributed by Chen uh who's a visiting researcher at Skylab with a partnership with meta and some",
    "start": "1222240",
    "end": "1229440"
  },
  {
    "text": "challenge we we Face here is you know I mentioned there's a additional crossens layers only when the image uh is present",
    "start": "1229440",
    "end": "1237080"
  },
  {
    "text": "meaning if it's text input then these are not visited if there's an image then we have to run through these uh crossens",
    "start": "1237080",
    "end": "1243440"
  },
  {
    "text": "layers this kind of dynamic uh INF pattern is a little bit tricky to deal with and secondly uh there's custom",
    "start": "1243440",
    "end": "1250240"
  },
  {
    "text": "attention mask required between the text and images uh traditionally for attention module we assume a causal kind",
    "start": "1250240",
    "end": "1257440"
  },
  {
    "text": "of attention mask cuz that's how Works uh but this kind of new mask is something we have never dealt with",
    "start": "1257440",
    "end": "1262760"
  },
  {
    "text": "before and finally uh since this is a cross detention layer meaning there will be incoder and decoder uh block tables",
    "start": "1262760",
    "end": "1270440"
  },
  {
    "text": "so we have to come up with a way to like more efficiently uh uh manage the memory",
    "start": "1270440",
    "end": "1276200"
  },
  {
    "text": "for these architecture which Quai Quai mentioned uh in the RO map uh in the last",
    "start": "1276200",
    "end": "1281760"
  },
  {
    "text": "talk and there's a mini uh Ro map for this model in particular uh please see the issue a 22 26",
    "start": "1281760",
    "end": "1289440"
  },
  {
    "text": "and next I'll talk about the RO Maps so what are two dos first of all we want to",
    "start": "1289440",
    "end": "1295720"
  },
  {
    "start": "1291000",
    "end": "1377000"
  },
  {
    "text": "make uh multimodal multimodality support to be comp compatible with other features on VM uh first one is automatic",
    "start": "1295720",
    "end": "1303279"
  },
  {
    "text": "P caching which Johan talk about in the last talk so if you think about uh previous hashing right now it's down on",
    "start": "1303279",
    "end": "1309400"
  },
  {
    "text": "the token ID uh level but for image uh input in particular the token IDs are",
    "start": "1309400",
    "end": "1316159"
  },
  {
    "text": "identical to all of them no matter what image your send in meaning we we need to add additional hash on the image itself",
    "start": "1316159",
    "end": "1323559"
  },
  {
    "text": "second is chunk Prill we need to precisely track where are where these placeholders are in order to chunk",
    "start": "1323559",
    "end": "1330400"
  },
  {
    "text": "prefill them properly and we need also need to make sure is compatible with the latest uh optimization that we uh we",
    "start": "1330400",
    "end": "1337120"
  },
  {
    "text": "added to vrm multi multi string um on a on a on on on a second",
    "start": "1337120",
    "end": "1343200"
  },
  {
    "text": "track we also have uh as Raa mentioned CPU overhead something we really want to get rid of of uh in the whole inference",
    "start": "1343200",
    "end": "1350120"
  },
  {
    "text": "pipeline but we do have these processing to convert image to pixel values are",
    "start": "1350120",
    "end": "1356039"
  },
  {
    "text": "indeed happening on CPU today and we want to you know have better and more efficient uh implementation or",
    "start": "1356039",
    "end": "1362679"
  },
  {
    "text": "pipelining to pre-process this multimodal data and there's a few more items better",
    "start": "1362679",
    "end": "1369600"
  },
  {
    "text": "profiling strategy um you know quantize lmm and",
    "start": "1369600",
    "end": "1376039"
  },
  {
    "start": "1377000",
    "end": "1802000"
  },
  {
    "text": "lmas and I want to wrap up with some unknowns that I personally know you know",
    "start": "1377000",
    "end": "1382880"
  },
  {
    "text": "uh I I don't even have an answer yet but it's just for the interest of the audience so I think in the overall like",
    "start": "1382880",
    "end": "1389640"
  },
  {
    "text": "large multimodel large multimodel model word we still haven't figured out like the ultimate architecture of LMS like it",
    "start": "1389640",
    "end": "1396760"
  },
  {
    "text": "is lava based you you you you merge the eding together and send to a decod only langage model is a flamingo place so",
    "start": "1396760",
    "end": "1404360"
  },
  {
    "text": "multimod llama where you add know cross attention between tax and multimod data",
    "start": "1404360",
    "end": "1409440"
  },
  {
    "text": "is a comedian style where you actually you know tokenize all modalities into token IDs and you leverage language",
    "start": "1409440",
    "end": "1415640"
  },
  {
    "text": "backbone or is it transfusion it's a very interesting paper came out meta AI uh I think a month ago where you merge a",
    "start": "1415640",
    "end": "1424039"
  },
  {
    "text": "autograss Transformer with diffusion in the same model and your switch mode in doing the inference and we don't know what's",
    "start": "1424039",
    "end": "1430679"
  },
  {
    "text": "coming out next right uh and the other unnown is uh how we think about how do",
    "start": "1430679",
    "end": "1436159"
  },
  {
    "text": "we think about like multimodal output right now we are only limiting our scope to multimodal inputs uh to tax output",
    "start": "1436159",
    "end": "1443720"
  },
  {
    "text": "but does it make sense to for VM to support multimodal output until which point we see there's really strong need",
    "start": "1443720",
    "end": "1449880"
  },
  {
    "text": "for us to support multimodal output and doesn't make sense and I think that's it",
    "start": "1449880",
    "end": "1455559"
  },
  {
    "text": "for everything uh so thank you for listening uh please check out uh the W stream tracker issue uh 494 and uh",
    "start": "1455559",
    "end": "1463919"
  },
  {
    "text": "contact me uh for any questions and we welcome any feedback and contributions to to this",
    "start": "1463919",
    "end": "1470919"
  },
  {
    "text": "Wasing right please raise your hand if you have",
    "start": "1473200",
    "end": "1478960"
  },
  {
    "text": "questions for Roger and I'll run the mic any fun questions about multim",
    "start": "1478960",
    "end": "1490080"
  },
  {
    "text": "modality there's a question back there",
    "start": "1492320",
    "end": "1497120"
  },
  {
    "text": "okay first of all thanks for the presentation and all your contribution in VM very uh useful for the future",
    "start": "1499320",
    "end": "1505880"
  },
  {
    "text": "definitely I wanted to ask you about how how your approach to supporting multimodality uh compares to other",
    "start": "1505880",
    "end": "1512440"
  },
  {
    "text": "inference engines like uh TGI for hugging face for instance um so I actually like when I",
    "start": "1512440",
    "end": "1520360"
  },
  {
    "text": "design kind of the framework for um support for multimodalities uh on VM",
    "start": "1520360",
    "end": "1525640"
  },
  {
    "text": "there was nothing right uh the cuz V itself was designed to work with uh language",
    "start": "1525640",
    "end": "1531559"
  },
  {
    "text": "model itself decode only language model itself so when I checked uh you know I I I did check TGI for inspiration uh and I",
    "start": "1531559",
    "end": "1539320"
  },
  {
    "text": "think the way they did uh was as I mentioned adding some hacks so for laex",
    "start": "1539320",
    "end": "1545200"
  },
  {
    "text": "they put in some helper functions in the same model file uh it's kind of it's not",
    "start": "1545200",
    "end": "1550240"
  },
  {
    "text": "really like a core part of t uh TGI it seems like a Sidetrack support or second",
    "start": "1550240",
    "end": "1557159"
  },
  {
    "text": "class support for for the uh for the uh infin engine and that's why cus and I",
    "start": "1557159",
    "end": "1563159"
  },
  {
    "text": "spend a lot of time designing the overall like infrastructure uh to make sure like developers can come in very",
    "start": "1563159",
    "end": "1569880"
  },
  {
    "text": "easily and pull their models um versus like it's like a second class treatment",
    "start": "1569880",
    "end": "1575559"
  },
  {
    "text": "uh in the inference engine",
    "start": "1575559",
    "end": "1581120"
  },
  {
    "text": "yes so when you devel this uh uh this new framework like how",
    "start": "1586360",
    "end": "1591880"
  },
  {
    "text": "do you uh what's your workflow for optimizations do you start with like",
    "start": "1591880",
    "end": "1597559"
  },
  {
    "text": "profiling tools and collect the statistics and then writing trianal then",
    "start": "1597559",
    "end": "1604200"
  },
  {
    "text": "if not work then those areal yeah this is a great question um I think for Trion",
    "start": "1604200",
    "end": "1611039"
  },
  {
    "text": "in particular we didn't get to that uh the first step we did was to as I",
    "start": "1611039",
    "end": "1616200"
  },
  {
    "text": "mentioned for the TP of the vision encoder uh these are very small uh so sometimes",
    "start": "1616200",
    "end": "1621320"
  },
  {
    "text": "actually not worth actually won't improve like performance if you sh it over a lot of gpus because these are",
    "start": "1621320",
    "end": "1627520"
  },
  {
    "text": "very small so the communication overhead actually like overshadows the benefit you get from TP um when we test out",
    "start": "1627520",
    "end": "1634559"
  },
  {
    "text": "different like kind of models uh to see the effect of TP on these V encoders we",
    "start": "1634559",
    "end": "1640320"
  },
  {
    "text": "actually don't see that much uh speed benefit uh there's actually no speed benefit like at all but we did see that",
    "start": "1640320",
    "end": "1647799"
  },
  {
    "text": "um you you save a lot of gpus I guess save a lot of memories on GPU and these",
    "start": "1647799",
    "end": "1653520"
  },
  {
    "text": "memories can be kind of kind of freed up for the KV cache meaning you can send in",
    "start": "1653520",
    "end": "1659640"
  },
  {
    "text": "a higher batch of request uh competed before and this will overall lead to like higher throughput but for uh",
    "start": "1659640",
    "end": "1666760"
  },
  {
    "text": "performance itself tping these like applying TP to these encoders actually",
    "start": "1666760",
    "end": "1672440"
  },
  {
    "text": "don't itself actually don't bring the benefit of speed yeah",
    "start": "1672440",
    "end": "1678519"
  },
  {
    "text": "can you repeat that",
    "start": "1682960",
    "end": "1685480"
  },
  {
    "text": "again um not quite because again uh most",
    "start": "1691159",
    "end": "1696240"
  },
  {
    "text": "of the compute happens in the language model background itself and the activation for the vision encoder is",
    "start": "1696240",
    "end": "1702200"
  },
  {
    "text": "only one time at prefi stage and these are very small yeah because they the the clip",
    "start": "1702200",
    "end": "1708440"
  },
  {
    "text": "encoder is actually only like five or 400 million parameters these are very",
    "start": "1708440",
    "end": "1714120"
  },
  {
    "text": "small compared to like the langage model backb",
    "start": "1714120",
    "end": "1718200"
  },
  {
    "text": "itself for for the vision encoder yes that's actually very small compared to the actual language backbone",
    "start": "1723360",
    "end": "1730720"
  },
  {
    "text": "yeah cool maybe we'll have time to take one more",
    "start": "1730720",
    "end": "1735559"
  },
  {
    "text": "question that's I'll ask a question what's your favorite application of uh",
    "start": "1736559",
    "end": "1742440"
  },
  {
    "text": "multimodal models say that again what's your favorite application how are people using these",
    "start": "1742440",
    "end": "1748000"
  },
  {
    "text": "models um Can can I say",
    "start": "1748000",
    "end": "1754519"
  },
  {
    "text": "it I I I think I think a favorite application I have is like uh imagine",
    "start": "1754519",
    "end": "1759840"
  },
  {
    "text": "coming to like uh a SD War right uh you want to understand what's going on in this spe world one way to do it is",
    "start": "1759840",
    "end": "1765840"
  },
  {
    "text": "especially for multi imager is that know you can take different screenshots of this 3D World and send it to language",
    "start": "1765840",
    "end": "1771159"
  },
  {
    "text": "model to to to have it to have it describe it without you actually play through this 3D world uh right now we",
    "start": "1771159",
    "end": "1777200"
  },
  {
    "text": "can only limit that to kind of image based uh inference but we don't know",
    "start": "1777200",
    "end": "1783120"
  },
  {
    "text": "what's going on we don't know what's going to be happen in the future right maybe there will be like a 3D encoder coming out in the future that you can",
    "start": "1783120",
    "end": "1789840"
  },
  {
    "text": "kind of like put together with the language model and have the language model do the reasoning to describe this",
    "start": "1789840",
    "end": "1794919"
  },
  {
    "text": "3D World awesome thank you so much let's thanks the speaker",
    "start": "1794919",
    "end": "1802158"
  }
]