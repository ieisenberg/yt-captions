[
  {
    "text": "all right welcome to",
    "start": "3560",
    "end": "7460"
  },
  {
    "text": "welcome to running uh ml work codes with",
    "start": "7460",
    "end": "10260"
  },
  {
    "text": "AWS purpose-built ml accelerators and",
    "start": "10260",
    "end": "12420"
  },
  {
    "text": "Ray my name is Matt McLean and I lead",
    "start": "12420",
    "end": "14880"
  },
  {
    "text": "the Annapurna ml solution architecture",
    "start": "14880",
    "end": "17039"
  },
  {
    "text": "team so my team helped customers",
    "start": "17039",
    "end": "19080"
  },
  {
    "text": "essentially move their ml workloads onto",
    "start": "19080",
    "end": "21840"
  },
  {
    "text": "AWS training and friendship",
    "start": "21840",
    "end": "24720"
  },
  {
    "text": "so uh first of all hands up anyone heard",
    "start": "24720",
    "end": "27539"
  },
  {
    "text": "of AWS and Furniture premium okay got a",
    "start": "27539",
    "end": "30960"
  },
  {
    "text": "couple so we actually started this",
    "start": "30960",
    "end": "33059"
  },
  {
    "text": "journey back in 2019 we launched our",
    "start": "33059",
    "end": "36300"
  },
  {
    "text": "first purpose-built accelerator it's",
    "start": "36300",
    "end": "37860"
  },
  {
    "text": "called AWS inferentia",
    "start": "37860",
    "end": "40620"
  },
  {
    "text": "um really offering the lowest cost",
    "start": "40620",
    "end": "42059"
  },
  {
    "text": "inference for running your workloads in",
    "start": "42059",
    "end": "44640"
  },
  {
    "text": "the clouds such as Burt models resnets",
    "start": "44640",
    "end": "46980"
  },
  {
    "text": "yolos and we see customers getting up to",
    "start": "46980",
    "end": "50940"
  },
  {
    "text": "70 lower cost per inference compared to",
    "start": "50940",
    "end": "54120"
  },
  {
    "text": "alternatives",
    "start": "54120",
    "end": "55860"
  },
  {
    "text": "so that was our first",
    "start": "55860",
    "end": "57680"
  },
  {
    "text": "foray into the world of paper skilled",
    "start": "57680",
    "end": "60780"
  },
  {
    "text": "accelerators and essentially we're built",
    "start": "60780",
    "end": "63180"
  },
  {
    "text": "on to on this sort of history and we",
    "start": "63180",
    "end": "66119"
  },
  {
    "text": "launched last year AWS training",
    "start": "66119",
    "end": "68820"
  },
  {
    "text": "which is our second generation",
    "start": "68820",
    "end": "70260"
  },
  {
    "text": "purpose-built accelerator from ml",
    "start": "70260",
    "end": "72740"
  },
  {
    "text": "essentially training you can understand",
    "start": "72740",
    "end": "75780"
  },
  {
    "text": "from the title trainium is designed for",
    "start": "75780",
    "end": "78020"
  },
  {
    "text": "ML training workloads",
    "start": "78020",
    "end": "80460"
  },
  {
    "text": "unless you're a lot more complicated",
    "start": "80460",
    "end": "81960"
  },
  {
    "text": "than in prints so we took a lot of their",
    "start": "81960",
    "end": "84960"
  },
  {
    "text": "learnings from the infrentia and built",
    "start": "84960",
    "end": "86820"
  },
  {
    "text": "it into the second generation",
    "start": "86820",
    "end": "88740"
  },
  {
    "text": "we also earlier this year launched AWS",
    "start": "88740",
    "end": "91439"
  },
  {
    "text": "in printer 2 which is our second",
    "start": "91439",
    "end": "92939"
  },
  {
    "text": "generation inference offering I've had",
    "start": "92939",
    "end": "95220"
  },
  {
    "text": "this build inference offering and both",
    "start": "95220",
    "end": "97200"
  },
  {
    "text": "AWS training and inferential 2 are",
    "start": "97200",
    "end": "99479"
  },
  {
    "text": "really designed for your gen AI",
    "start": "99479",
    "end": "101040"
  },
  {
    "text": "workloads so stable diffusion llms are",
    "start": "101040",
    "end": "104400"
  },
  {
    "text": "really the target sweet spot for this",
    "start": "104400",
    "end": "106380"
  },
  {
    "text": "purpose-built accelerators",
    "start": "106380",
    "end": "109140"
  },
  {
    "text": "so let's have a look at then what comes",
    "start": "109140",
    "end": "111240"
  },
  {
    "text": "under the hood like what powers",
    "start": "111240",
    "end": "112799"
  },
  {
    "text": "essentially these chips so we have the",
    "start": "112799",
    "end": "115140"
  },
  {
    "text": "neuron core architecture so first point",
    "start": "115140",
    "end": "118259"
  },
  {
    "text": "is that this is not a GPU this is quite",
    "start": "118259",
    "end": "121799"
  },
  {
    "text": "a different architecture it's really",
    "start": "121799",
    "end": "123240"
  },
  {
    "text": "purpose-built for deep learning",
    "start": "123240",
    "end": "124860"
  },
  {
    "text": "workloads",
    "start": "124860",
    "end": "126540"
  },
  {
    "text": "and it comprises of this various engines",
    "start": "126540",
    "end": "128580"
  },
  {
    "text": "I mean the core of the accelerator is",
    "start": "128580",
    "end": "131520"
  },
  {
    "text": "this tensor engine so this does your",
    "start": "131520",
    "end": "133379"
  },
  {
    "text": "Matrix multiplications and as well as",
    "start": "133379",
    "end": "136140"
  },
  {
    "text": "other tensor kind of operations",
    "start": "136140",
    "end": "138480"
  },
  {
    "text": "we have a vector engine this does things",
    "start": "138480",
    "end": "140819"
  },
  {
    "text": "such as pooling they're your main",
    "start": "140819",
    "end": "142560"
  },
  {
    "text": "operations we have a scalar engine this",
    "start": "142560",
    "end": "145080"
  },
  {
    "text": "does a lot of your activation functions",
    "start": "145080",
    "end": "146819"
  },
  {
    "text": "like relu 10h sigmoid",
    "start": "146819",
    "end": "150120"
  },
  {
    "text": "and finally we have a general purpose",
    "start": "150120",
    "end": "151739"
  },
  {
    "text": "Sim Denton so this allows you to",
    "start": "151739",
    "end": "154379"
  },
  {
    "text": "actually bring your own programmable so",
    "start": "154379",
    "end": "156900"
  },
  {
    "text": "you can basically code up operators and",
    "start": "156900",
    "end": "158879"
  },
  {
    "text": "C plus plus and they run with eight of",
    "start": "158879",
    "end": "161879"
  },
  {
    "text": "these",
    "start": "161879",
    "end": "162860"
  },
  {
    "text": "512-bit processors on the same core",
    "start": "162860",
    "end": "167040"
  },
  {
    "text": "all of these access the same on-chip is",
    "start": "167040",
    "end": "170400"
  },
  {
    "text": "SRAM as well as high bandwidth memory",
    "start": "170400",
    "end": "174120"
  },
  {
    "text": "so we offer these purpose-built",
    "start": "174120",
    "end": "175860"
  },
  {
    "text": "accelerators through AWS ec2 instances",
    "start": "175860",
    "end": "178379"
  },
  {
    "text": "so the first instance is the trn1",
    "start": "178379",
    "end": "181800"
  },
  {
    "text": "instance which is the powered by AWS",
    "start": "181800",
    "end": "184379"
  },
  {
    "text": "trainium",
    "start": "184379",
    "end": "185400"
  },
  {
    "text": "we offer this in three different types",
    "start": "185400",
    "end": "187260"
  },
  {
    "text": "the Turin one two extra large this has",
    "start": "187260",
    "end": "190080"
  },
  {
    "text": "one accelerator with two neuron cores",
    "start": "190080",
    "end": "192140"
  },
  {
    "text": "offering up to 32 gigabytes of HP and",
    "start": "192140",
    "end": "195480"
  },
  {
    "text": "memory",
    "start": "195480",
    "end": "196500"
  },
  {
    "text": "really good kind of experimenting for",
    "start": "196500",
    "end": "199080"
  },
  {
    "text": "your data science doing experimenting",
    "start": "199080",
    "end": "200700"
  },
  {
    "text": "with different train workloads all the",
    "start": "200700",
    "end": "203459"
  },
  {
    "text": "way up to the Turin 132 extra large so",
    "start": "203459",
    "end": "206400"
  },
  {
    "text": "this has 16 accelerators and 32 neuron",
    "start": "206400",
    "end": "209940"
  },
  {
    "text": "cores",
    "start": "209940",
    "end": "210959"
  },
  {
    "text": "and 512 gigabytes of HP and memory and",
    "start": "210959",
    "end": "215340"
  },
  {
    "text": "then offers 800 gigabits per second and",
    "start": "215340",
    "end": "218220"
  },
  {
    "text": "up to 1600 gigabits per second of",
    "start": "218220",
    "end": "220860"
  },
  {
    "text": "network bandwidth so this is actually",
    "start": "220860",
    "end": "222900"
  },
  {
    "text": "four times more than you can get with",
    "start": "222900",
    "end": "225180"
  },
  {
    "text": "alternative accelerators on AWS",
    "start": "225180",
    "end": "228060"
  },
  {
    "text": "so these are really purpose built for",
    "start": "228060",
    "end": "230099"
  },
  {
    "text": "running your NLP and workloads training",
    "start": "230099",
    "end": "233640"
  },
  {
    "text": "your NLP models",
    "start": "233640",
    "end": "235200"
  },
  {
    "text": "and I offer up to 50 costs to train",
    "start": "235200",
    "end": "237720"
  },
  {
    "text": "savings compared to Alternative ec2",
    "start": "237720",
    "end": "240720"
  },
  {
    "text": "instances",
    "start": "240720",
    "end": "243120"
  },
  {
    "text": "so if we look at in French and Furniture",
    "start": "243120",
    "end": "245519"
  },
  {
    "text": "is actually offered in four different uh",
    "start": "245519",
    "end": "247860"
  },
  {
    "text": "think of excuse we have the extra large",
    "start": "247860",
    "end": "250920"
  },
  {
    "text": "and eight extra large they both have one",
    "start": "250920",
    "end": "252780"
  },
  {
    "text": "accelerator with two neuron cores",
    "start": "252780",
    "end": "254480"
  },
  {
    "text": "offering up to 32 gigabytes of base for",
    "start": "254480",
    "end": "257160"
  },
  {
    "text": "your memory",
    "start": "257160",
    "end": "258540"
  },
  {
    "text": "up to the end for two foot and extra",
    "start": "258540",
    "end": "260519"
  },
  {
    "text": "large so this has 12 accelerators and 24",
    "start": "260519",
    "end": "263220"
  },
  {
    "text": "neuron cores",
    "start": "263220",
    "end": "265100"
  },
  {
    "text": "and offers for example 100 gigabits per",
    "start": "265100",
    "end": "268080"
  },
  {
    "text": "second networking uh 384 gigabytes of",
    "start": "268080",
    "end": "271199"
  },
  {
    "text": "HPM memory",
    "start": "271199",
    "end": "272759"
  },
  {
    "text": "so there is a really designed for your",
    "start": "272759",
    "end": "275160"
  },
  {
    "text": "adrenitive eye general of AI inference",
    "start": "275160",
    "end": "277620"
  },
  {
    "text": "so for example really good for running",
    "start": "277620",
    "end": "279240"
  },
  {
    "text": "stable diffusion models llms and which",
    "start": "279240",
    "end": "282479"
  },
  {
    "text": "we'll see a little bit later will give",
    "start": "282479",
    "end": "283680"
  },
  {
    "text": "you a demo of how you can run an lln",
    "start": "283680",
    "end": "285720"
  },
  {
    "text": "with racer",
    "start": "285720",
    "end": "288380"
  },
  {
    "text": "so we also some of the latest models so",
    "start": "288780",
    "end": "291720"
  },
  {
    "text": "for example F2 you can run llama2 so",
    "start": "291720",
    "end": "295199"
  },
  {
    "text": "here are some benchmarking that we did",
    "start": "295199",
    "end": "297540"
  },
  {
    "text": "running llama 2 on inf2 and this shows",
    "start": "297540",
    "end": "302280"
  },
  {
    "text": "you the two different instance types so",
    "start": "302280",
    "end": "304259"
  },
  {
    "text": "depending if you want to optimize for",
    "start": "304259",
    "end": "305880"
  },
  {
    "text": "latency",
    "start": "305880",
    "end": "307500"
  },
  {
    "text": "you know you can run basically your",
    "start": "307500",
    "end": "309960"
  },
  {
    "text": "llama 2 model on The Institute 48 extra",
    "start": "309960",
    "end": "313320"
  },
  {
    "text": "large you get really good uh low leaving",
    "start": "313320",
    "end": "315960"
  },
  {
    "text": "Sea so we actually offer three times",
    "start": "315960",
    "end": "317940"
  },
  {
    "text": "lower latency than other ec2 instances",
    "start": "317940",
    "end": "321060"
  },
  {
    "text": "and it can also be four times lower cost",
    "start": "321060",
    "end": "324060"
  },
  {
    "text": "to deploy these models so here's a graph",
    "start": "324060",
    "end": "327479"
  },
  {
    "text": "showing essentially the difference",
    "start": "327479",
    "end": "329100"
  },
  {
    "text": "between launching your llama 2 model on",
    "start": "329100",
    "end": "332220"
  },
  {
    "text": "the larger systems and the smaller one",
    "start": "332220",
    "end": "334380"
  },
  {
    "text": "the M2",
    "start": "334380",
    "end": "336419"
  },
  {
    "text": "um extra eight extra large",
    "start": "336419",
    "end": "339259"
  },
  {
    "text": "so we're you're here at the rate",
    "start": "339259",
    "end": "341520"
  },
  {
    "text": "conference and you're asking what does",
    "start": "341520",
    "end": "342840"
  },
  {
    "text": "this have to do with Ray well I'm",
    "start": "342840",
    "end": "344759"
  },
  {
    "text": "pleased to announce that from the ray",
    "start": "344759",
    "end": "346500"
  },
  {
    "text": "2.7 release we actually have native",
    "start": "346500",
    "end": "349020"
  },
  {
    "text": "support for both AWS and furniture and",
    "start": "349020",
    "end": "351720"
  },
  {
    "text": "AWS trainium within the ray open source",
    "start": "351720",
    "end": "354120"
  },
  {
    "text": "project",
    "start": "354120",
    "end": "355139"
  },
  {
    "text": "so starting at raid 2.7 which I think",
    "start": "355139",
    "end": "357360"
  },
  {
    "text": "was just released",
    "start": "357360",
    "end": "358820"
  },
  {
    "text": "very recently which is part of this",
    "start": "358820",
    "end": "361440"
  },
  {
    "text": "conference you can natively deploy your",
    "start": "361440",
    "end": "364080"
  },
  {
    "text": "ray application and leverage the neuron",
    "start": "364080",
    "end": "366360"
  },
  {
    "text": "cores that I just mentioned",
    "start": "366360",
    "end": "368880"
  },
  {
    "text": "so it supports reserve and you can",
    "start": "368880",
    "end": "371759"
  },
  {
    "text": "easily Define your neuron cores in a",
    "start": "371759",
    "end": "374820"
  },
  {
    "text": "cluster enactors and also in tasks in",
    "start": "374820",
    "end": "377880"
  },
  {
    "text": "your array cluster also includes Auto",
    "start": "377880",
    "end": "380520"
  },
  {
    "text": "scaling so you have to scale your",
    "start": "380520",
    "end": "382199"
  },
  {
    "text": "workers or your actors you can do that",
    "start": "382199",
    "end": "385020"
  },
  {
    "text": "and it's built in natively",
    "start": "385020",
    "end": "387300"
  },
  {
    "text": "we're also working hard to support Ray",
    "start": "387300",
    "end": "389699"
  },
  {
    "text": "train so there will be coming in a",
    "start": "389699",
    "end": "392039"
  },
  {
    "text": "future re-release",
    "start": "392039",
    "end": "394500"
  },
  {
    "text": "so here's an example of essentially",
    "start": "394500",
    "end": "397199"
  },
  {
    "text": "running",
    "start": "397199",
    "end": "399500"
  },
  {
    "text": "your workload leveraging the neuron",
    "start": "399780",
    "end": "403080"
  },
  {
    "text": "cores",
    "start": "403080",
    "end": "404220"
  },
  {
    "text": "so it's super easy you can see in the",
    "start": "404220",
    "end": "406440"
  },
  {
    "text": "ray in it all you need to Define is the",
    "start": "406440",
    "end": "408360"
  },
  {
    "text": "number of cores that you need in your",
    "start": "408360",
    "end": "409800"
  },
  {
    "text": "cluster in this particular example we",
    "start": "409800",
    "end": "412560"
  },
  {
    "text": "are requesting the two neuron cores in",
    "start": "412560",
    "end": "415199"
  },
  {
    "text": "our cluster",
    "start": "415199",
    "end": "416340"
  },
  {
    "text": "and here also we've got a couple of",
    "start": "416340",
    "end": "418199"
  },
  {
    "text": "examples of running both an actor as",
    "start": "418199",
    "end": "420600"
  },
  {
    "text": "well as a",
    "start": "420600",
    "end": "421759"
  },
  {
    "text": "a task in in rain so essentially all",
    "start": "421759",
    "end": "426840"
  },
  {
    "text": "it's doing is the the reactor it's",
    "start": "426840",
    "end": "429240"
  },
  {
    "text": "asking for one neuron core and it's just",
    "start": "429240",
    "end": "432919"
  },
  {
    "text": "returning what is the sort of the neuron",
    "start": "432919",
    "end": "435780"
  },
  {
    "text": "core reference on that particular",
    "start": "435780",
    "end": "437220"
  },
  {
    "text": "instance as well as a uh a task is also",
    "start": "437220",
    "end": "442259"
  },
  {
    "text": "doing similar kind of thing so it's",
    "start": "442259",
    "end": "444120"
  },
  {
    "text": "super simple example",
    "start": "444120",
    "end": "446220"
  },
  {
    "text": "but what I wanted to show you is",
    "start": "446220",
    "end": "448080"
  },
  {
    "text": "actually an example of running llama 2",
    "start": "448080",
    "end": "450259"
  },
  {
    "text": "on Ray using uh using uh the AWS custom",
    "start": "450259",
    "end": "455520"
  },
  {
    "text": "accelerator so now I'm going to switch",
    "start": "455520",
    "end": "457560"
  },
  {
    "text": "to the demo",
    "start": "457560",
    "end": "460220"
  },
  {
    "text": "uh so here I have",
    "start": "460220",
    "end": "464479"
  },
  {
    "text": "an example script of running uh",
    "start": "465539",
    "end": "469919"
  },
  {
    "text": "llama two",
    "start": "469919",
    "end": "471319"
  },
  {
    "text": "I can switch to the uh not the",
    "start": "471319",
    "end": "476039"
  },
  {
    "text": "presentation do I have to get out of",
    "start": "476039",
    "end": "477960"
  },
  {
    "text": "presentation mode",
    "start": "477960",
    "end": "480620"
  },
  {
    "text": "yeah",
    "start": "480780",
    "end": "482520"
  },
  {
    "text": "oh yeah okay here we go yep",
    "start": "482520",
    "end": "486360"
  },
  {
    "text": "um",
    "start": "486360",
    "end": "488478"
  },
  {
    "text": "so let's blow it up a little bit",
    "start": "489599",
    "end": "493020"
  },
  {
    "text": "so here's an example of running this is",
    "start": "493020",
    "end": "495599"
  },
  {
    "text": "essentially a llama 2 model running on a",
    "start": "495599",
    "end": "498180"
  },
  {
    "text": "race of cluster",
    "start": "498180",
    "end": "499800"
  },
  {
    "text": "it's leveraging race serve and it's also",
    "start": "499800",
    "end": "502680"
  },
  {
    "text": "using the radio app if you're familiar",
    "start": "502680",
    "end": "504539"
  },
  {
    "text": "with Ray serve on this particular",
    "start": "504539",
    "end": "506840"
  },
  {
    "text": "cluster so here you can see uh the code",
    "start": "506840",
    "end": "510120"
  },
  {
    "text": "so we are using a specific library that",
    "start": "510120",
    "end": "513719"
  },
  {
    "text": "we've built for optimizing llms it's",
    "start": "513719",
    "end": "516120"
  },
  {
    "text": "called Transformers neuronex so you can",
    "start": "516120",
    "end": "518580"
  },
  {
    "text": "see that in the import statement here",
    "start": "518580",
    "end": "520919"
  },
  {
    "text": "uh we're using this particular library",
    "start": "520919",
    "end": "522599"
  },
  {
    "text": "that we've built this really optimizes",
    "start": "522599",
    "end": "524760"
  },
  {
    "text": "llm performance includes things like KV",
    "start": "524760",
    "end": "527519"
  },
  {
    "text": "caching bucketing quantization parallel",
    "start": "527519",
    "end": "531660"
  },
  {
    "text": "context encoding to make your llms",
    "start": "531660",
    "end": "533640"
  },
  {
    "text": "inference go really really fast",
    "start": "533640",
    "end": "536459"
  },
  {
    "text": "so it uses a very similar API to hugging",
    "start": "536459",
    "end": "540300"
  },
  {
    "text": "face if you're familiar with hugging",
    "start": "540300",
    "end": "541560"
  },
  {
    "text": "face you basically give it a reference",
    "start": "541560",
    "end": "543480"
  },
  {
    "text": "to where your model weights are and then",
    "start": "543480",
    "end": "546240"
  },
  {
    "text": "you define some extra parameters such as",
    "start": "546240",
    "end": "548339"
  },
  {
    "text": "things like if you we use tensor",
    "start": "548339",
    "end": "550560"
  },
  {
    "text": "parallelism to really Shard the model",
    "start": "550560",
    "end": "553440"
  },
  {
    "text": "across different neuron cores to get the",
    "start": "553440",
    "end": "555600"
  },
  {
    "text": "lowest latency possible",
    "start": "555600",
    "end": "558180"
  },
  {
    "text": "so we're going to code we're doing",
    "start": "558180",
    "end": "559740"
  },
  {
    "text": "things such as we're loading our weights",
    "start": "559740",
    "end": "561540"
  },
  {
    "text": "into the model",
    "start": "561540",
    "end": "563180"
  },
  {
    "text": "and then we are defining how we're going",
    "start": "563180",
    "end": "566040"
  },
  {
    "text": "to run this particular model so this",
    "start": "566040",
    "end": "567959"
  },
  {
    "text": "line of code here",
    "start": "567959",
    "end": "569940"
  },
  {
    "text": "that I'm that I'm highlighting is",
    "start": "569940",
    "end": "572339"
  },
  {
    "text": "showing what is the batch size that we",
    "start": "572339",
    "end": "575160"
  },
  {
    "text": "want to run our particular model",
    "start": "575160",
    "end": "577680"
  },
  {
    "text": "inference on it also defines the number",
    "start": "577680",
    "end": "580380"
  },
  {
    "text": "of we call this TP degree the tensor",
    "start": "580380",
    "end": "582420"
  },
  {
    "text": "parallelism degree so how we're going to",
    "start": "582420",
    "end": "583980"
  },
  {
    "text": "Shard the model across the various",
    "start": "583980",
    "end": "585779"
  },
  {
    "text": "neuron cores",
    "start": "585779",
    "end": "588180"
  },
  {
    "text": "and then we have a line here which is",
    "start": "588180",
    "end": "590160"
  },
  {
    "text": "actually compiles the model so we're a",
    "start": "590160",
    "end": "592320"
  },
  {
    "text": "compile driven platform this allows us",
    "start": "592320",
    "end": "595260"
  },
  {
    "text": "to get the best performance of those",
    "start": "595260",
    "end": "596940"
  },
  {
    "text": "things such as fusing operators",
    "start": "596940",
    "end": "599300"
  },
  {
    "text": "and various other optimization",
    "start": "599300",
    "end": "601440"
  },
  {
    "text": "techniques to get the best performance",
    "start": "601440",
    "end": "603120"
  },
  {
    "text": "possible so this is what the two neuron",
    "start": "603120",
    "end": "607100"
  },
  {
    "text": "columns do and then when we're actually",
    "start": "607100",
    "end": "609720"
  },
  {
    "text": "doing the inference it's just calling",
    "start": "609720",
    "end": "611220"
  },
  {
    "text": "this sample method which is uh basically",
    "start": "611220",
    "end": "614940"
  },
  {
    "text": "we're explaining how many what's the",
    "start": "614940",
    "end": "617100"
  },
  {
    "text": "sequence link what's the next sequence",
    "start": "617100",
    "end": "618720"
  },
  {
    "text": "link we're going to return and it's",
    "start": "618720",
    "end": "620580"
  },
  {
    "text": "using top K so it's giving the top K",
    "start": "620580",
    "end": "623100"
  },
  {
    "text": "parameter",
    "start": "623100",
    "end": "624839"
  },
  {
    "text": "our services are essentially the example",
    "start": "624839",
    "end": "628580"
  },
  {
    "text": "so what we can do is",
    "start": "628580",
    "end": "631780"
  },
  {
    "text": "[Applause]",
    "start": "631780",
    "end": "634850"
  },
  {
    "text": "so hopefully you can see this is just a",
    "start": "635519",
    "end": "637860"
  },
  {
    "text": "radio app here it's just taking in an",
    "start": "637860",
    "end": "640860"
  },
  {
    "text": "input prompt and it's going to return",
    "start": "640860",
    "end": "642540"
  },
  {
    "text": "we're using the Llama to chat model",
    "start": "642540",
    "end": "646980"
  },
  {
    "text": "so we can just select one of the example",
    "start": "646980",
    "end": "650760"
  },
  {
    "text": "prompts and click submit",
    "start": "650760",
    "end": "654540"
  },
  {
    "text": "and this is the output that the model",
    "start": "654540",
    "end": "656760"
  },
  {
    "text": "has returned",
    "start": "656760",
    "end": "658740"
  },
  {
    "text": "so it can actually do is actually see",
    "start": "658740",
    "end": "661860"
  },
  {
    "text": "the neuron cause in action",
    "start": "661860",
    "end": "663800"
  },
  {
    "text": "so I'm going to",
    "start": "663800",
    "end": "666959"
  },
  {
    "text": "just create another terminal here",
    "start": "666959",
    "end": "670620"
  },
  {
    "text": "we're going to run this neuron top",
    "start": "670620",
    "end": "672839"
  },
  {
    "text": "command so neuron top is essentially a",
    "start": "672839",
    "end": "674700"
  },
  {
    "text": "way to analyze all of the neuron cores",
    "start": "674700",
    "end": "677459"
  },
  {
    "text": "so I'm actually running on a trainium",
    "start": "677459",
    "end": "679260"
  },
  {
    "text": "instance a training instance has 16",
    "start": "679260",
    "end": "681860"
  },
  {
    "text": "accelerators and 32 neuron cores and you",
    "start": "681860",
    "end": "685140"
  },
  {
    "text": "can see here all the bars showing",
    "start": "685140",
    "end": "687360"
  },
  {
    "text": "uh utilization of those 32 cores in this",
    "start": "687360",
    "end": "691500"
  },
  {
    "text": "particular on this particular instance",
    "start": "691500",
    "end": "694920"
  },
  {
    "text": "so I'm going to just run this example",
    "start": "694920",
    "end": "696959"
  },
  {
    "text": "again and we can see the neuron course",
    "start": "696959",
    "end": "699839"
  },
  {
    "text": "in action",
    "start": "699839",
    "end": "701339"
  },
  {
    "text": "so let's just take a different prompt",
    "start": "701339",
    "end": "705200"
  },
  {
    "text": "and hopefully you can see at the Lin you",
    "start": "708420",
    "end": "710100"
  },
  {
    "text": "can see the uh the neuron cores were all",
    "start": "710100",
    "end": "712079"
  },
  {
    "text": "activated because we're actually shading",
    "start": "712079",
    "end": "713519"
  },
  {
    "text": "this model across all of the 32 neuron",
    "start": "713519",
    "end": "715980"
  },
  {
    "text": "cores and this will give us the lowest",
    "start": "715980",
    "end": "718260"
  },
  {
    "text": "latency possible so let's just take",
    "start": "718260",
    "end": "720240"
  },
  {
    "text": "another example prompt hopefully you'll",
    "start": "720240",
    "end": "722519"
  },
  {
    "text": "see it",
    "start": "722519",
    "end": "724820"
  },
  {
    "text": "so you can see the each neuron core is",
    "start": "725880",
    "end": "728040"
  },
  {
    "text": "you know you can what's activated as",
    "start": "728040",
    "end": "729600"
  },
  {
    "text": "we're doing this inference",
    "start": "729600",
    "end": "732620"
  },
  {
    "text": "let's just do another one",
    "start": "732660",
    "end": "736160"
  },
  {
    "text": "so yeah so uh that's a quick demo of uh",
    "start": "736440",
    "end": "740880"
  },
  {
    "text": "the",
    "start": "740880",
    "end": "742320"
  },
  {
    "text": "um yeah showing you how to run rain with",
    "start": "742320",
    "end": "746040"
  },
  {
    "text": "uh AWS surface called accelerators",
    "start": "746040",
    "end": "748019"
  },
  {
    "text": "that's AWS training and everything",
    "start": "748019",
    "end": "749160"
  },
  {
    "text": "friendship",
    "start": "749160",
    "end": "750899"
  },
  {
    "text": "um yeah I'm here for the rest of the day",
    "start": "750899",
    "end": "753240"
  },
  {
    "text": "and also my colleagues so feel free to",
    "start": "753240",
    "end": "755640"
  },
  {
    "text": "come and have a chat to us and yeah",
    "start": "755640",
    "end": "757800"
  },
  {
    "text": "we'll be excited to see what you guys",
    "start": "757800",
    "end": "760079"
  },
  {
    "text": "can build uh using AWS and Ray thanks so",
    "start": "760079",
    "end": "763560"
  },
  {
    "text": "much",
    "start": "763560",
    "end": "765460"
  },
  {
    "text": "[Applause]",
    "start": "765460",
    "end": "768639"
  }
]