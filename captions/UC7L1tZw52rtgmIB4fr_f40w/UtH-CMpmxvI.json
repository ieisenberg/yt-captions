[
  {
    "text": "okay um welcome a warm welcome to all of you to our meetup this monthly meetup happens",
    "start": "480",
    "end": "7759"
  },
  {
    "text": "every last wednesday of the month this time we actually had to move things around because of some scheduling",
    "start": "7759",
    "end": "13360"
  },
  {
    "text": "conflicts my name is jules danji i'm going to be your host today uh joining with me is our race of team edward oakes",
    "start": "13360",
    "end": "20640"
  },
  {
    "text": "simon ho dong and sria's crease now swami you guys want to say hello to the community",
    "start": "20640",
    "end": "28680"
  },
  {
    "text": "hey everyone thanks for joining thanks thanks for joining um anyway we got we got we got a big",
    "start": "30000",
    "end": "35200"
  },
  {
    "text": "agenda today so we're going to get actually started thanks a lot for really taking the time to join us as you know",
    "start": "35200",
    "end": "40320"
  },
  {
    "text": "community is the center of any open source project and you guys are being part of it taking the time to join us is",
    "start": "40320",
    "end": "46960"
  },
  {
    "text": "quite important for us and we really value your not only your contribution but also your participation and so if",
    "start": "46960",
    "end": "52559"
  },
  {
    "text": "you like to give a talk here at the meetup where there's a lightning talk or a general 30-minute talk about",
    "start": "52559",
    "end": "58399"
  },
  {
    "text": "your journey with ray or any of the libraries that you actually use to do your machine learning workloads we are very",
    "start": "58399",
    "end": "64878"
  },
  {
    "text": "much interested to hear about it so do let us know uh send me an email at joolz in the scale.com and we'll try to figure",
    "start": "64879",
    "end": "71600"
  },
  {
    "text": "out a way to put you in we accept lightning talks it's actually a great way to have that so let us know if you're actually interested to do that",
    "start": "71600",
    "end": "78159"
  },
  {
    "text": "now here's the agenda for today uh no meetup is is um complete without any",
    "start": "78159",
    "end": "83520"
  },
  {
    "text": "announcements i'll have some announcements before we actually head out we have a great uh series of talks",
    "start": "83520",
    "end": "88960"
  },
  {
    "text": "today we're going to start with edward ox who is the uh project lead for racer along with simon ma who's going to be",
    "start": "88960",
    "end": "95520"
  },
  {
    "text": "sitting in the background answering all your technical questions uh while simon is giving us an overview in a roadmap to",
    "start": "95520",
    "end": "102240"
  },
  {
    "text": "set the context of what the next two talks are going to be followed by xiao who is going to share with us how",
    "start": "102240",
    "end": "108159"
  },
  {
    "text": "do you actually build you know multi-node inference graphs and how do you actually scale it and why it is so",
    "start": "108159",
    "end": "113439"
  },
  {
    "text": "easy to use the pytonic apis to be able to actually build your directly executed graph so you can",
    "start": "113439",
    "end": "119439"
  },
  {
    "text": "actually deploy quite easily and then we'll have um uh shreyas who's going to",
    "start": "119439",
    "end": "124479"
  },
  {
    "text": "sort of tie up things how do you actually operationalize something that you actually have in your deployments at scale and now we actually manage that",
    "start": "124479",
    "end": "130879"
  },
  {
    "text": "through these very operational friendlies friendlies apis that we have coming up in our recent release and a",
    "start": "130879",
    "end": "137280"
  },
  {
    "text": "couple of series down the road and then finally you know we'll actually have a demo that actually ties up",
    "start": "137280",
    "end": "143040"
  },
  {
    "text": "everything that you have to hear from edward um zhao and the team so hang around for the final finale demo a couple of",
    "start": "143040",
    "end": "150080"
  },
  {
    "text": "announcements we actually have we are having our in first in-person race summit um sometime late in the august in",
    "start": "150080",
    "end": "157440"
  },
  {
    "text": "2013 and 24 we're still accepting papers uh they're going to be closing on april",
    "start": "157440",
    "end": "162720"
  },
  {
    "text": "the 18th so you still have time uh to submit those papers forget about doing",
    "start": "162720",
    "end": "167920"
  },
  {
    "text": "your taxes just go ahead and do submit your safety as i'm looking forward to to uh to uh read your submission so do that",
    "start": "167920",
    "end": "175440"
  },
  {
    "text": "and then obviously the race summit is coming up in august submission registration is up for open we're going",
    "start": "175440",
    "end": "181599"
  },
  {
    "text": "to be putting in our keynote speakers and agenda coming up very soon there's an early bird uh registration process do",
    "start": "181599",
    "end": "188400"
  },
  {
    "text": "feel free to uh peruse through it and we'll be putting more content about what trainings we are doing to do come and",
    "start": "188400",
    "end": "195200"
  },
  {
    "text": "and join us in the late summer and then finally um as i say and i keep on",
    "start": "195200",
    "end": "200720"
  },
  {
    "text": "repeating every time when i go to a meetup or every time i give a talk at a conference is that community is really a",
    "start": "200720",
    "end": "206400"
  },
  {
    "text": "center and at the core of any open source project and your participation",
    "start": "206400",
    "end": "212000"
  },
  {
    "text": "in contributing anything that you actually feel that you think should be part of a ray ecosystem please file an",
    "start": "212000",
    "end": "219440"
  },
  {
    "text": "issue on github or if you're willing to contribute please do that on the github we have a very lively and and vivish",
    "start": "219440",
    "end": "226400"
  },
  {
    "text": "discussions happening on discus forum and slack where we have all the communities and all the authors and the communities discussing about technical",
    "start": "226400",
    "end": "232959"
  },
  {
    "text": "issues with ray so do join us if you want to find out and keep abreast with",
    "start": "232959",
    "end": "238000"
  },
  {
    "text": "what's happening with ray we have a new monthly newsletter and obviously i live on twitter so if you actually want to",
    "start": "238000",
    "end": "243519"
  },
  {
    "text": "find out anything that and everything about ray what how what people are doing how they're actually using it what use",
    "start": "243519",
    "end": "249280"
  },
  {
    "text": "cases they're using follow us on twitter and join us in the community now with that said i'm gonna now sort of hand it",
    "start": "249280",
    "end": "256000"
  },
  {
    "text": "over to uh advert who's gonna sort of give us an overview to set the context for the rest",
    "start": "256000",
    "end": "261840"
  },
  {
    "text": "of the talks edward all yours thanks jules uh okay so you should just be seeing my",
    "start": "261840",
    "end": "268560"
  },
  {
    "text": "slides right now all right yep okay all right thanks so much jules um yeah",
    "start": "268560",
    "end": "274240"
  },
  {
    "text": "so my name is edward oakes like jules mentioned i am a software engineer at any scale where i lead the race serve",
    "start": "274240",
    "end": "279680"
  },
  {
    "text": "team so today i'll be giving kind of a gentle gentle introduction into what cert",
    "start": "279680",
    "end": "284720"
  },
  {
    "text": "reserve is um you know what we're seeing people use it for in the wild and then talk a little bit about some of the",
    "start": "284720",
    "end": "290960"
  },
  {
    "text": "current developments as well as what we're working on looking towards ray 2.0 which is going to come out later this",
    "start": "290960",
    "end": "296000"
  },
  {
    "text": "year and a lot of these things that i'm talking about will be covered in the next two talks by jiao and simon and",
    "start": "296000",
    "end": "301280"
  },
  {
    "text": "they'll go into a little bit more detail so let's get started",
    "start": "301280",
    "end": "307199"
  },
  {
    "text": "okay so outline of the talk um so i'm going to start with a quick introduction of racer um then i'll go into what we've",
    "start": "307280",
    "end": "313919"
  },
  {
    "text": "been hearing from our users um you know we do events like this meetup all the time we're really active on slack and we",
    "start": "313919",
    "end": "319360"
  },
  {
    "text": "love hearing from our users about you know what they love about racer and also what's missing so i'm going to go over",
    "start": "319360",
    "end": "324720"
  },
  {
    "text": "some of the trends that we've heard there um and then i'll talk about you know how we're taking that feedback and",
    "start": "324720",
    "end": "330320"
  },
  {
    "text": "improving things for ray 2.0 as well as a little bit about the extended roadmap and like i said this will be a preview",
    "start": "330320",
    "end": "336000"
  },
  {
    "text": "for the talks that you hear later today so before we dive into race serve let's",
    "start": "336000",
    "end": "341520"
  },
  {
    "text": "talk a little bit about um the ray ecosystem so ray is a universal framework for",
    "start": "341520",
    "end": "346880"
  },
  {
    "text": "distributed computing it offers a simple but powerful api for building distributed applications",
    "start": "346880",
    "end": "353120"
  },
  {
    "text": "and is especially useful for distributed machine learning applications it can run on top of all the major cloud",
    "start": "353120",
    "end": "358960"
  },
  {
    "text": "providers as well as kubernetes or in on-prem clusters and it has a wide ecosystem of libraries",
    "start": "358960",
    "end": "366880"
  },
  {
    "text": "built on top of it that allow you to really build incredibly powerful distributed",
    "start": "366880",
    "end": "373120"
  },
  {
    "text": "applications so some of these libraries you might be familiar with like desk or",
    "start": "373120",
    "end": "378160"
  },
  {
    "text": "mars or hugging face um and there are also some native libraries such as ray tune and aura lib",
    "start": "378160",
    "end": "385680"
  },
  {
    "text": "ray serve is one of these native libraries that is a compute layer for for model serving",
    "start": "385680",
    "end": "393880"
  },
  {
    "text": "let's go into a tldr of racer so racerv is a flexible scalable compute layer for",
    "start": "394240",
    "end": "399360"
  },
  {
    "text": "model server it's built on top of ray like i just mentioned which means that it inherits a lot of really great",
    "start": "399360",
    "end": "404800"
  },
  {
    "text": "benefits it's scalable you can build low latency serving applications and it's also",
    "start": "404800",
    "end": "410080"
  },
  {
    "text": "efficient which is really important in machine learning inference where you know you may be running workloads 24 7 and compute can be really",
    "start": "410080",
    "end": "416800"
  },
  {
    "text": "expensive it also has first class support for multi-model serving i'll talk more about",
    "start": "416800",
    "end": "423440"
  },
  {
    "text": "multi-model serving in a minute but this is a trend where we see more and more",
    "start": "423440",
    "end": "428960"
  },
  {
    "text": "machine learning applications require more than just a single machine learning model to work intent",
    "start": "428960",
    "end": "434400"
  },
  {
    "text": "and finally race serve is python native and it allows you to easily mix business logic with your machine learning uh with",
    "start": "434400",
    "end": "441199"
  },
  {
    "text": "your machine learning models um to build uh like end-to-end machine learning",
    "start": "441199",
    "end": "447680"
  },
  {
    "text": "applications so let's dive a little bit deeper into what i mean by multi-model survey",
    "start": "447680",
    "end": "455680"
  },
  {
    "text": "so we're seeing this pattern especially when talking to our users that oftentimes building a machine learning",
    "start": "455680",
    "end": "461440"
  },
  {
    "text": "model is not just taking not just training one classifier or training one regression model and then uh you know",
    "start": "461440",
    "end": "467919"
  },
  {
    "text": "sending requests to it and getting an output instead if you're building something for a real you know uh",
    "start": "467919",
    "end": "475599"
  },
  {
    "text": "for a real world workload you often have to combine multiple models even for just a single request",
    "start": "475599",
    "end": "482960"
  },
  {
    "text": "so let's dive into an example of this so imagine that we have a an application",
    "start": "482960",
    "end": "488479"
  },
  {
    "text": "where we want to synthesize a description for an image so we can't just train one basic model",
    "start": "488479",
    "end": "494560"
  },
  {
    "text": "in order to perform this complex task instead we have an image processing pipeline",
    "start": "494560",
    "end": "499599"
  },
  {
    "text": "um so in this example we first need to do some kind of pre-processing on the image so maybe decode it and",
    "start": "499599",
    "end": "505840"
  },
  {
    "text": "do some basic pre-processing like augment it or clip the image then we might have a classifier followed",
    "start": "505840",
    "end": "512080"
  },
  {
    "text": "by something like key point detection and then finally if we want to synthesize a description we need to use",
    "start": "512080",
    "end": "517120"
  },
  {
    "text": "some kind of nlp model so this is much more complex than just having a single trains machine learning",
    "start": "517120",
    "end": "523360"
  },
  {
    "text": "model and also note that all of these models may have different resource requirements",
    "start": "523360",
    "end": "528399"
  },
  {
    "text": "the pre-processing step might be pretty lightweight and run on cpus while the nlp model might be a heavyweight deep",
    "start": "528399",
    "end": "534720"
  },
  {
    "text": "learning model um and we want to be able to you know run this efficiently and scale it up to many requests",
    "start": "534720",
    "end": "542800"
  },
  {
    "text": "so i just described sort of a pipeline multi-model inference application um but this is just one of",
    "start": "543279",
    "end": "549519"
  },
  {
    "text": "the types of multi-modal serving patterns that we see we also often see ensembling where for a single request",
    "start": "549519",
    "end": "557200"
  },
  {
    "text": "you may fan out to many different models and then only return uh you know one of the outputs or maybe",
    "start": "557200",
    "end": "563120"
  },
  {
    "text": "combine the outputs and we also often see machine learning models mixed with business logic so you may make a request",
    "start": "563120",
    "end": "569040"
  },
  {
    "text": "to a model based on the response you may select one or more downstream models so a",
    "start": "569040",
    "end": "575360"
  },
  {
    "text": "common application here might be for something like fraud detection where you need to to mix hand tuned rules with",
    "start": "575360",
    "end": "581200"
  },
  {
    "text": "machine learning and race serve is is a really powerful tool for building these types of",
    "start": "581200",
    "end": "587200"
  },
  {
    "text": "multi-model serving applications so it allows you to define this entire distributed",
    "start": "587200",
    "end": "594000"
  },
  {
    "text": "inference graph as a single unified python program you can continue to use your favorite tools and libraries",
    "start": "594000",
    "end": "600720"
  },
  {
    "text": "for instance you might use pytorch for the actual uh for one of your models and you might want to use like an off the",
    "start": "600720",
    "end": "606160"
  },
  {
    "text": "face hugging face um model for another and it also allows you to scale out across cpus and gpus um and inherits",
    "start": "606160",
    "end": "614160"
  },
  {
    "text": "ray's flexible scheduling so that you can do this efficiently",
    "start": "614160",
    "end": "619399"
  },
  {
    "text": "so now that i gave a quick intro of ray serve and the the applications that we're seeing it",
    "start": "619760",
    "end": "624880"
  },
  {
    "text": "used for let me dive in a little bit into what we've heard from the community",
    "start": "624880",
    "end": "630240"
  },
  {
    "text": "so as i just described we're seeing more and more that people need to do this type of multi-model serving um so this",
    "start": "630240",
    "end": "636000"
  },
  {
    "text": "is a really big need and it's also something that race serve is really well tuned to",
    "start": "636000",
    "end": "642240"
  },
  {
    "text": "but while the python api is great and people you know are seeing a lot of success building their applications we're still",
    "start": "642640",
    "end": "649040"
  },
  {
    "text": "seeing some friction when people uh you know want to go to production and have a",
    "start": "649040",
    "end": "654240"
  },
  {
    "text": "automated workflow such as ci cd for deploying and updating their models um one of them one of the emerging",
    "start": "654240",
    "end": "661120"
  },
  {
    "text": "patterns that we're seeing is continual learning where people want to retrain their model weights very frequently in",
    "start": "661120",
    "end": "666880"
  },
  {
    "text": "order to ensure that they're you know providing the best outputs uh you know and many times this would be like",
    "start": "666880",
    "end": "672399"
  },
  {
    "text": "recommendations they're providing the best recommendations to their users and while there are users that are successful doing this today with racer",
    "start": "672399",
    "end": "679440"
  },
  {
    "text": "we want to make it as easy as possible so that you can really just focus on the application logic",
    "start": "679440",
    "end": "686800"
  },
  {
    "text": "and finally a constant thread is that machine learning inference is really expensive",
    "start": "686800",
    "end": "692480"
  },
  {
    "text": "a lot of these models are compute intensive and oftentimes you want to run on expensive hardware like gpus",
    "start": "692480",
    "end": "698240"
  },
  {
    "text": "so efficiency is always key and we're always looking for ways to uh make it easier for our users to have cost",
    "start": "698240",
    "end": "704000"
  },
  {
    "text": "savings so given those takeaways that we have",
    "start": "704000",
    "end": "709440"
  },
  {
    "text": "from talking with our users and uh you know seeing posts on the forum and slack",
    "start": "709440",
    "end": "715360"
  },
  {
    "text": "so let me talk a little bit about what we're working on for ray 2.0 so the first thing is we're doubling",
    "start": "715360",
    "end": "721519"
  },
  {
    "text": "down on this multi-model serving use case with a first class api called the deployment graph api",
    "start": "721519",
    "end": "728160"
  },
  {
    "text": "you'll hear about this from jiao later today but this gives you a really flexible powerful api um that can be",
    "start": "728160",
    "end": "734000"
  },
  {
    "text": "used to write high performance multi-model serving applications",
    "start": "734000",
    "end": "739279"
  },
  {
    "text": "we're also investing in improving um our our support for you know automation and",
    "start": "739279",
    "end": "745600"
  },
  {
    "text": "deployments so we're working on a structured rest api and uh",
    "start": "745600",
    "end": "751200"
  },
  {
    "text": "and using that to improve the kubernetes support and you'll hear more about this from shreyas in the talk later today",
    "start": "751200",
    "end": "757519"
  },
  {
    "text": "and we're also looking to integrate with a lot of the best and breed ml ops tools there's a huge ecosystem of really",
    "start": "757519",
    "end": "764160"
  },
  {
    "text": "amazing tools for things like model versioning and saving model artifacts such as weights and biases and",
    "start": "764160",
    "end": "770959"
  },
  {
    "text": "ml flow as well as uh you know monitoring things in production and doing model drift detection like arise",
    "start": "770959",
    "end": "777360"
  },
  {
    "text": "and y labs um and we're not really trying to reinvent this ourselves but we want to make it possible to integrate with the",
    "start": "777360",
    "end": "783360"
  },
  {
    "text": "best possible tools using racer and make that as seamless as possible",
    "start": "783360",
    "end": "788800"
  },
  {
    "text": "and finally earlier i talked about about the rate ecosystem and one of the",
    "start": "789120",
    "end": "794160"
  },
  {
    "text": "things that we're really trying to do across all of the ray libraries is to improve interoperability we want to make",
    "start": "794160",
    "end": "800639"
  },
  {
    "text": "it completely seamless to um go through the entire machine learning life cycle from data processing to training all the",
    "start": "800639",
    "end": "807279"
  },
  {
    "text": "way to serving in production um so there's an effort for ray 2.0 called ray air which where we're trying",
    "start": "807279",
    "end": "813360"
  },
  {
    "text": "to make all of the native libraries really interoperate as easily as possible",
    "start": "813360",
    "end": "819040"
  },
  {
    "text": "so in addition to the things that we have in progress for raid 2.0 we also have a",
    "start": "820320",
    "end": "825360"
  },
  {
    "text": "huge backlog of of items that we've heard from our users these are things like scale to zero or",
    "start": "825360",
    "end": "832160"
  },
  {
    "text": "supporting grpc or supporting many many small models on a small amount of",
    "start": "832160",
    "end": "837760"
  },
  {
    "text": "hardware and as you might have seen in this talk where you know i first talked about",
    "start": "837760",
    "end": "843440"
  },
  {
    "text": "what we're hearing from the community and then you can see how that directly maps onto what we're currently working for rate 2.0",
    "start": "843440",
    "end": "850000"
  },
  {
    "text": "so here i really want to encourage everyone to please um get in touch with us we really want",
    "start": "850000",
    "end": "855839"
  },
  {
    "text": "to hear from you because the only way that we know uh what to build in order to make your lives easier and enable you",
    "start": "855839",
    "end": "862160"
  },
  {
    "text": "to build really really cool powerful applications um is if we get that feedback and we hear about your use",
    "start": "862160",
    "end": "867760"
  },
  {
    "text": "cases so please get in touch um there's a lot of ways that you can do this you can",
    "start": "867760",
    "end": "873600"
  },
  {
    "text": "join the community and talk to us at um the discuss forum at discuss.ray.io",
    "start": "873600",
    "end": "879440"
  },
  {
    "text": "you can always file an issue or leave a comment on github or get in touch on twitter at ray",
    "start": "879440",
    "end": "884880"
  },
  {
    "text": "distributed or any scale compute and we also have this survey at the qr code below and i'll leave the screen up",
    "start": "884880",
    "end": "891279"
  },
  {
    "text": "for a while so that you can scan it and if you fill this out it'll make it really easy for us to get",
    "start": "891279",
    "end": "896959"
  },
  {
    "text": "in touch with you and we're happy to set up one-on-one sessions with the racer developers",
    "start": "896959",
    "end": "902000"
  },
  {
    "text": "or to keep you updated about any of these features that we talked about today or about the extended roadmap",
    "start": "902000",
    "end": "909120"
  },
  {
    "text": "so that concludes my talk and yeah i think we'll move on to q a now",
    "start": "909120",
    "end": "914160"
  },
  {
    "text": "edward thanks a lot for that wonderful introduction i think this is actually quite important what you actually brought up on the screen is because as i",
    "start": "914160",
    "end": "920560"
  },
  {
    "text": "said earlier you know community is at the core in the center of any open source project and having all these",
    "start": "920560",
    "end": "926720"
  },
  {
    "text": "people actually participate and give us constant feedback is how we actually improve both critical as well as",
    "start": "926720",
    "end": "932480"
  },
  {
    "text": "constructive feedback i think collectively we can actually make that a lot easier as i believe the innovation",
    "start": "932480",
    "end": "938639"
  },
  {
    "text": "doesn't happen in isolation it happens in in cooperation so this is actually a good way to move actually forward so do",
    "start": "938639",
    "end": "944639"
  },
  {
    "text": "please fill up this particular survey and get in touch with this and we'd like to talk to you now um i'm looking at",
    "start": "944639",
    "end": "951199"
  },
  {
    "text": "some q a panel a couple of questions that i have uh add for you you mentioned something about scaling to zero could",
    "start": "951199",
    "end": "958240"
  },
  {
    "text": "you actually elaborate what does that actually mean scale to zero i would like to hear about scale from one to x or one",
    "start": "958240",
    "end": "963440"
  },
  {
    "text": "to infinity but what does scale do zero mean can you can you talk about that in in terms of what does that mean with",
    "start": "963440",
    "end": "968959"
  },
  {
    "text": "reserve yeah absolutely so this has been um something that we're starting to hear somewhat frequently from users where",
    "start": "968959",
    "end": "975440"
  },
  {
    "text": "they may have you know many many different models you can imagine a scenario where you're doing something like product recommendations and you",
    "start": "975440",
    "end": "981600"
  },
  {
    "text": "have a thousand product line a thousand product lines or ten thousand product lines",
    "start": "981600",
    "end": "986639"
  },
  {
    "text": "uh and you have a request come in but it may only actually hit you know a small fraction of those so you only want",
    "start": "986639",
    "end": "994079"
  },
  {
    "text": "to have the models that are currently being used actually loaded into memory and consuming resources",
    "start": "994079",
    "end": "1000800"
  },
  {
    "text": "so if you have you know just a few products that are really really popular and you have a long tail of other ones",
    "start": "1000800",
    "end": "1006079"
  },
  {
    "text": "that don't get requested very often people want scale to zero meaning uh you can scale down to not consume any",
    "start": "1006079",
    "end": "1012240"
  },
  {
    "text": "resources while those less popular ones aren't being requested so scaling down to zero would be as if",
    "start": "1012240",
    "end": "1018639"
  },
  {
    "text": "you're having a stand called standby alone and if you actually need that then you just scale up is that is that metaphorically or euphemistically",
    "start": "1018639",
    "end": "1024880"
  },
  {
    "text": "translate to that yeah yeah that's exactly right okay um the other question i had that popping into my mind it's not",
    "start": "1024880",
    "end": "1031120"
  },
  {
    "text": "actually from the community but this is when you were going through you did mention that you are listening to the",
    "start": "1031120",
    "end": "1037199"
  },
  {
    "text": "community and some of the things that you found out that people build all these uh different models in",
    "start": "1037199",
    "end": "1044160"
  },
  {
    "text": "in production they have composite they have example they have their business logic their pipeline",
    "start": "1044160",
    "end": "1049760"
  },
  {
    "text": "where did you find out what's the common scenario in other words of the five deployment patterns that you see which",
    "start": "1049760",
    "end": "1055840"
  },
  {
    "text": "was the most common one that you actually felt that you you you heard uh invariably was it ensemble was it",
    "start": "1055840",
    "end": "1062400"
  },
  {
    "text": "composite uh was it only business logic where did you feel that that most people are using it so i",
    "start": "1062400",
    "end": "1069679"
  },
  {
    "text": "think you know honestly one of the the important things here is that every business application is is a little bit",
    "start": "1069679",
    "end": "1075440"
  },
  {
    "text": "different um so we do see a wide range of things here and that's why it's really important that race serve has this like",
    "start": "1075440",
    "end": "1081679"
  },
  {
    "text": "very flexible api that you can kind of tailor to your needs but i would say um for",
    "start": "1081679",
    "end": "1086799"
  },
  {
    "text": "there there are a few sort of um common patterns that we see so for instance with recommendation models uh",
    "start": "1086799",
    "end": "1092960"
  },
  {
    "text": "you often see like an ensemble approach where you have multiple models and then you like combine the outputs",
    "start": "1092960",
    "end": "1098160"
  },
  {
    "text": "and for image processing it's really common to see this type of pipeline sort of like the example that i gave earlier",
    "start": "1098160",
    "end": "1105200"
  },
  {
    "text": "where you first feed the image into one model and then to another and another another um",
    "start": "1105200",
    "end": "1110400"
  },
  {
    "text": "so i would say that they're all pretty common uh but we see kind of like clusterings in different application areas",
    "start": "1110400",
    "end": "1116400"
  },
  {
    "text": "okay i have a question from the community here from zach werner are there ways to hack around lack of scale",
    "start": "1116400",
    "end": "1122960"
  },
  {
    "text": "to zero support in race serve today are there ways to hack around lack of",
    "start": "1122960",
    "end": "1128240"
  },
  {
    "text": "skill to zero support inserted in other words if you don't have that how how would you hack around that we are not",
    "start": "1128240",
    "end": "1134559"
  },
  {
    "text": "using race serve today have rolled our own versions of lru cache because we have",
    "start": "1134559",
    "end": "1139919"
  },
  {
    "text": "thousands of models that we have to serve and you know we don't want to store them all in memory",
    "start": "1139919",
    "end": "1145520"
  },
  {
    "text": "so yeah um yeah i can answer that um yeah so yeah you you definitely could hack around",
    "start": "1145520",
    "end": "1151600"
  },
  {
    "text": "uh yeah hack around not having scale to zero support today um",
    "start": "1151600",
    "end": "1156880"
  },
  {
    "text": "so yeah you should definitely get in touch by the way on the survey or on slack and we can discuss this in more",
    "start": "1156880",
    "end": "1162240"
  },
  {
    "text": "detail but um you know using the existing python api you could definitely implement scale the xero yourself with",
    "start": "1162240",
    "end": "1168080"
  },
  {
    "text": "just kind of a lightweight like coordinator actor or something like that um so yeah this is definitely possible",
    "start": "1168080",
    "end": "1174400"
  },
  {
    "text": "it just doesn't sort of work out of the box um but yeah i would would love to get in touch and discuss how we can make that",
    "start": "1174400",
    "end": "1181039"
  },
  {
    "text": "work for you thanks a lot zach duke please uh do please send us um uh join join us join our slack channel and do",
    "start": "1181039",
    "end": "1187440"
  },
  {
    "text": "ping edward and we can actually have this conversation offline i think it's an important thing because i think it",
    "start": "1187440",
    "end": "1192480"
  },
  {
    "text": "might be a little while before we actually have that the roadmap but having having having a temporary solution would be great uh james lew",
    "start": "1192480",
    "end": "1199039"
  },
  {
    "text": "actually has a question could you elaborate on how you approach the continuous learning what is the pipeline",
    "start": "1199039",
    "end": "1204960"
  },
  {
    "text": "looks like in other words somebody wants to do confused learning by the find a model that's that needs to be uh",
    "start": "1204960",
    "end": "1211600"
  },
  {
    "text": "fed back into ci cd to train again then redeploy it um could you could you explain what that",
    "start": "1211600",
    "end": "1217919"
  },
  {
    "text": "what that means yeah yeah absolutely um so i would say that the most um",
    "start": "1217919",
    "end": "1224240"
  },
  {
    "text": "the most frequent uh way that we see this working is is basically you know people are serving a",
    "start": "1224240",
    "end": "1230559"
  },
  {
    "text": "model and then uh right now i would say like they have a periodic you know just sort of like cron job that will trigger",
    "start": "1230559",
    "end": "1237200"
  },
  {
    "text": "retraining the model on the most recent data um and then you'll have some kind of like uh you know just like batch",
    "start": "1237200",
    "end": "1243840"
  },
  {
    "text": "inference evaluation to test that that model is performing well um and then you actually",
    "start": "1243840",
    "end": "1249679"
  },
  {
    "text": "and that all happens sort of outside of racer and then when you go to deploy it in race serve oftentimes these happen",
    "start": "1249679",
    "end": "1255520"
  },
  {
    "text": "very frequently so people will use the dynamic update support in racer to",
    "start": "1255520",
    "end": "1261760"
  },
  {
    "text": "just apply the new model weights and that will trigger a pretty lightweight like rolling update in racer um and like",
    "start": "1261760",
    "end": "1268320"
  },
  {
    "text": "re-download the model and uh just start serving the new one um the way that you",
    "start": "1268320",
    "end": "1274000"
  },
  {
    "text": "actually oh yeah i was just going to say the way that you actually sort of apply that new",
    "start": "1274000",
    "end": "1279039"
  },
  {
    "text": "the new model weights is uh requires a little bit of of like manual implementation today and trace is",
    "start": "1279039",
    "end": "1284880"
  },
  {
    "text": "going to talk about that in his talk how that could be improved okay we're going to take one more question and then i'll have simon answer",
    "start": "1284880",
    "end": "1290720"
  },
  {
    "text": "the questions uh by typing the answer from osama as well as we actually have to go on to the next the next one um can",
    "start": "1290720",
    "end": "1298159"
  },
  {
    "text": "runtime environments be used to compose multiple models which might have conflicting dependency into a single",
    "start": "1298159",
    "end": "1304880"
  },
  {
    "text": "pipeline on a single rare cluster that's an interesting one yeah um yes that absolutely uh can be used um so you can",
    "start": "1304880",
    "end": "1312320"
  },
  {
    "text": "specify a runtime environment for a given like served deployment um and these could have conflicting model uh",
    "start": "1312320",
    "end": "1317600"
  },
  {
    "text": "conflicting dependencies uh you do have to be a little careful that you're not exchanging objects that uh",
    "start": "1317600",
    "end": "1324400"
  },
  {
    "text": "you know if you use two versions of pi torch and you pass like a pi torch object between them that might cause problems um but if you're just passing",
    "start": "1324400",
    "end": "1331039"
  },
  {
    "text": "normal python data types then there's there's no issue brilliant i think we'll end that with that but we can actually take some of",
    "start": "1331039",
    "end": "1336880"
  },
  {
    "text": "the question and answer since since all of us are here online by by typing the answers so everybody can actually see so",
    "start": "1336880",
    "end": "1343679"
  },
  {
    "text": "edward thanks a lot for uh that wonderful insight in giving us a roadmap and setting the context for the next two",
    "start": "1343679",
    "end": "1349840"
  },
  {
    "text": "talks now um xiao we actually have you as our next talk to tell us how do you",
    "start": "1349840",
    "end": "1354880"
  },
  {
    "text": "build graph apis and what's so great about it so please let us know",
    "start": "1354880",
    "end": "1360919"
  },
  {
    "text": "hey uh can i see my slice right now yes we can okay",
    "start": "1361280",
    "end": "1367280"
  },
  {
    "text": "let me see controls all right so thank you jose network for the introduction of research",
    "start": "1367280",
    "end": "1372799"
  },
  {
    "text": "in general and this is gel an engineer at any scale working open source reserve",
    "start": "1372799",
    "end": "1378159"
  },
  {
    "text": "i'm here to give you more overview and deep dive into the service department graph",
    "start": "1378159",
    "end": "1384159"
  },
  {
    "text": "and in this talk we're going to cover a few topics and the outline is we're gonna show you",
    "start": "1384159",
    "end": "1389520"
  },
  {
    "text": "some motivations we had in mind and learning from the community in this in the industry why we think multi-modal",
    "start": "1389520",
    "end": "1395360"
  },
  {
    "text": "inference graph is an important pattern we need to support uh we walk through some context of rare",
    "start": "1395360",
    "end": "1400799"
  },
  {
    "text": "research and why ray and reserve is really well suited to solve this type of problems",
    "start": "1400799",
    "end": "1407280"
  },
  {
    "text": "and then we will give a very comprehensive api walkthrough to show you a demo simple example end to end",
    "start": "1407280",
    "end": "1414000"
  },
  {
    "text": "with all the apis and details covered and show you an example how you build one end to end",
    "start": "1414000",
    "end": "1419440"
  },
  {
    "text": "and we also have a documentation in our website for all the details included and at the very end of this meetup we also",
    "start": "1419440",
    "end": "1426159"
  },
  {
    "text": "have gonna have a real life demo to have a real content understanding pipeline builds and we're going to show the",
    "start": "1426159",
    "end": "1431520"
  },
  {
    "text": "entire life cycle of authoring deploying and iterating on the infrastructure",
    "start": "1431520",
    "end": "1437440"
  },
  {
    "text": "okay so let's get started with motivation so machine learning inference graphs are getting longer wider and more dynamic",
    "start": "1437440",
    "end": "1444799"
  },
  {
    "text": "and this is an important pattern and trends we noticed by talking with users and learning from the industry",
    "start": "1444799",
    "end": "1451120"
  },
  {
    "text": "and by seeing longer as previous edward also showed you often have multiple models for",
    "start": "1451120",
    "end": "1456640"
  },
  {
    "text": "natural chain of actions for the final prediction and they have a strict dependency",
    "start": "1456640",
    "end": "1461760"
  },
  {
    "text": "and the more models you have the longer the chain would be that's what i mean by longer and basing",
    "start": "1461760",
    "end": "1467440"
  },
  {
    "text": "wider is usually for uh for some particular decision making",
    "start": "1467440",
    "end": "1472880"
  },
  {
    "text": "patterns for example a content understanding of a computer vision model uh we often see people need to send the",
    "start": "1472880",
    "end": "1478480"
  },
  {
    "text": "same image to a collection of models in parallel and that magnitude could be 10 or maybe 50 of those models collectively",
    "start": "1478480",
    "end": "1486320"
  },
  {
    "text": "and in sampling gonna happen at the end aggregate the results so that's what i mean by wider",
    "start": "1486320",
    "end": "1491520"
  },
  {
    "text": "and also it's getting more dynamic is we often see people need to mix business logic and dynamic dispatching",
    "start": "1491520",
    "end": "1498960"
  },
  {
    "text": "pattern into this complex inference for example i want to show on the very right hand",
    "start": "1498960",
    "end": "1504320"
  },
  {
    "text": "side we have a dynamic selection that we will dynamically decide to route the",
    "start": "1504320",
    "end": "1510640"
  },
  {
    "text": "request to a particular subset of models if i know something about the attribute of my input if it's a cat or if it's a",
    "start": "1510640",
    "end": "1517440"
  },
  {
    "text": "dog right and also we often see people need to continuously do lightweight updates for",
    "start": "1517440",
    "end": "1523440"
  },
  {
    "text": "their existing deployments to make it easier to configure and without turning down the entire cluster",
    "start": "1523440",
    "end": "1530320"
  },
  {
    "text": "so you can see more details of what one of the blocks we have to capture the patterns of ml models in production",
    "start": "1530320",
    "end": "1537360"
  },
  {
    "text": "okay and now this slide we're going to walk over unique strains of ray so rey is really powerful at writing",
    "start": "1537360",
    "end": "1543840"
  },
  {
    "text": "simple python applications but also make it very skilled as well as a low latency in the",
    "start": "1543840",
    "end": "1549200"
  },
  {
    "text": "particular scenario we're going to walk through more details about why we have advantages of building low latency and",
    "start": "1549200",
    "end": "1555919"
  },
  {
    "text": "more efficient multi-model inference ground and also we double down on the interop",
    "start": "1555919",
    "end": "1562880"
  },
  {
    "text": "between different libraries of the ecosystem as the computer substrate of ml platform so if you use your",
    "start": "1562880",
    "end": "1568400"
  },
  {
    "text": "application both retune retain using reserve we want to provide the end-to-end seamless experience so whole",
    "start": "1568400",
    "end": "1575120"
  },
  {
    "text": "piece is connected so let's start with one example of what the",
    "start": "1575120",
    "end": "1581679"
  },
  {
    "text": "existing api we have and how serv works in a little more detail and leads to",
    "start": "1581679",
    "end": "1588400"
  },
  {
    "text": "a single deployment experience as well as how that leads to multiple deployment graphs",
    "start": "1588400",
    "end": "1594080"
  },
  {
    "text": "so in our existing api you can start with a model class and that could be a cluster function let's call it model",
    "start": "1594080",
    "end": "1599919"
  },
  {
    "text": "which is a simple python implementation and then we can add a decorator to it we call that serve.deployment",
    "start": "1599919",
    "end": "1606320"
  },
  {
    "text": "and we can deploy this class directly let's call it my model that deployed",
    "start": "1606320",
    "end": "1611440"
  },
  {
    "text": "and by default we got two things all of it exposed to you as a user",
    "start": "1611440",
    "end": "1617600"
  },
  {
    "text": "we have an http endpoint and in this case i'd say if you're doing local testing we expose a local host endpoint",
    "start": "1617600",
    "end": "1623520"
  },
  {
    "text": "and you can hit it with your http data in the meantime we also expose a python",
    "start": "1623520",
    "end": "1628640"
  },
  {
    "text": "handle and the python handle means you can grab this handle corresponding by model deployment and just calling dot",
    "start": "1628640",
    "end": "1635360"
  },
  {
    "text": "remote and send your user request to it and we found this by design we want to",
    "start": "1635360",
    "end": "1640720"
  },
  {
    "text": "facilitate multi-model composition so you can use a python handle essentially program your deployment and program your",
    "start": "1640720",
    "end": "1647760"
  },
  {
    "text": "inference graph so this is the primary tool people are using nowadays to orchestrate and build",
    "start": "1647760",
    "end": "1654960"
  },
  {
    "text": "their multi-model inference now let's walk through one example and this slightly get more complicated",
    "start": "1654960",
    "end": "1661520"
  },
  {
    "text": "but it's a very common pattern we observe in the industry so at the left-hand side you can see we",
    "start": "1661520",
    "end": "1666559"
  },
  {
    "text": "have an input image and we want to go through some pre-processing let's say we normalize it we profit i reshape it",
    "start": "1666559",
    "end": "1672159"
  },
  {
    "text": "resize it and after pre-processing we dispatch this into three different classification",
    "start": "1672159",
    "end": "1678159"
  },
  {
    "text": "models or object detection models in parallel and i want to aggregate them the results",
    "start": "1678159",
    "end": "1684000"
  },
  {
    "text": "at the end through a combiner and then do the post-processing aggregation at the end",
    "start": "1684000",
    "end": "1690159"
  },
  {
    "text": "now on the right-hand side you can see this is the code snippet of the implementation we see our users are doing using the existing api",
    "start": "1690159",
    "end": "1696960"
  },
  {
    "text": "so you can see in the constructor we need to explicitly pass in a bunch of models and essentially the deployments",
    "start": "1696960",
    "end": "1702960"
  },
  {
    "text": "and in the constructor we need to get handle for each one of them and hold them hold the reference to it",
    "start": "1702960",
    "end": "1709600"
  },
  {
    "text": "and there are a few problems with those with this pattern and so at least one of the attributes you can observe here is",
    "start": "1709600",
    "end": "1716559"
  },
  {
    "text": "when you're looking at the classes and you assuming your inference graph gets really large and very complicated the",
    "start": "1716559",
    "end": "1722480"
  },
  {
    "text": "dependency between different nodes in the graph in this case the different classes is hidden from you",
    "start": "1722480",
    "end": "1728000"
  },
  {
    "text": "and also in here users need to explicitly call and get handle every single time for the model and deployment they want to use",
    "start": "1728000",
    "end": "1735279"
  },
  {
    "text": "and also one user need to call into those models when you send a request and you can see people need to manually",
    "start": "1735279",
    "end": "1741520"
  },
  {
    "text": "orchestrate this communication and write essentially they need to be familiar with re-apis",
    "start": "1741520",
    "end": "1747360"
  },
  {
    "text": "also need to use it correctly in order to not lose performance and here actually you can see this is",
    "start": "1747360",
    "end": "1753039"
  },
  {
    "text": "one of the anti-patterns that we discourage user to do because this person is doing a weight in uh for",
    "start": "1753039",
    "end": "1758720"
  },
  {
    "text": "loop which we rather have this object wraps called in the batch so it's better vectorized",
    "start": "1758720",
    "end": "1764480"
  },
  {
    "text": "so it's also hard to write efficient graph technically disco snippet runs but we can see a few patterns we noticed from",
    "start": "1764480",
    "end": "1771200"
  },
  {
    "text": "this case even though it's a really simple one and that will be make it hard to scale and",
    "start": "1771200",
    "end": "1776480"
  },
  {
    "text": "enable users to write more complicated applications so to summarize those are the challenges",
    "start": "1776480",
    "end": "1782799"
  },
  {
    "text": "observed with menu composition so the deployment graph topology is",
    "start": "1782799",
    "end": "1788080"
  },
  {
    "text": "hidden so there's no single place or easy way that you can know what my graph look like what my",
    "start": "1788080",
    "end": "1794320"
  },
  {
    "text": "competition graph looked like without reading through the code line by line model by model class by class",
    "start": "1794320",
    "end": "1801440"
  },
  {
    "text": "and this can make it very challenging for operationalizing for example let's say if you're trying to deploy a model",
    "start": "1801440",
    "end": "1807919"
  },
  {
    "text": "and you will need to know which order you need to deploy each model",
    "start": "1807919",
    "end": "1813760"
  },
  {
    "text": "with respect to the correct dependency that's one and also without a static definition of your deployment graph you'll be really",
    "start": "1813760",
    "end": "1820640"
  },
  {
    "text": "difficult for an ops team which we can assume we have a data science team i said we have ops team which is",
    "start": "1820640",
    "end": "1826080"
  },
  {
    "text": "responsible for operationalizing this econograph to know what is going on the required deep understanding of the code",
    "start": "1826080",
    "end": "1832000"
  },
  {
    "text": "base and the third issue we observed in the previous code snippet is it's also",
    "start": "1832000",
    "end": "1837360"
  },
  {
    "text": "really hard to write efficient graphs where everything is resolved at runtime with no high-level static definition",
    "start": "1837360",
    "end": "1844000"
  },
  {
    "text": "it also eliminates the chance opportunities we can provide to users because some of the computation patterns",
    "start": "1844000",
    "end": "1849600"
  },
  {
    "text": "we can easily auto-tune co-locate and optimize by putting different models and different nodes in",
    "start": "1849600",
    "end": "1856080"
  },
  {
    "text": "the same physical machine in the same container or even the same process",
    "start": "1856080",
    "end": "1861840"
  },
  {
    "text": "so therefore we observe those problems and we want to bring a solution to you is we need to introduce the graph",
    "start": "1861840",
    "end": "1867679"
  },
  {
    "text": "building api and talk through the solution we have in mind and a few few design principles and",
    "start": "1867679",
    "end": "1874559"
  },
  {
    "text": "considerations we have in mind while designing this api so first of all we take it very",
    "start": "1874559",
    "end": "1880399"
  },
  {
    "text": "seriously about this api to be python native which means we want to make it fully python programmable as much as",
    "start": "1880399",
    "end": "1886720"
  },
  {
    "text": "possible without the need of writing a yamaha to begin with so while we're researching existing",
    "start": "1886720",
    "end": "1892880"
  },
  {
    "text": "solutions in open source uh we we find one common pattern is the very first thing that people ask you to",
    "start": "1892880",
    "end": "1899200"
  },
  {
    "text": "do if you want to orchestrate an author a deployment pipeline is you need to learn their yaml format",
    "start": "1899200",
    "end": "1904720"
  },
  {
    "text": "you need to learn their schema and you need to write a yaml file with many handwritten dependencies between them",
    "start": "1904720",
    "end": "1910960"
  },
  {
    "text": "and we found this experience less desirable and we want to bring python as a first class programming language",
    "start": "1910960",
    "end": "1916960"
  },
  {
    "text": "and we are biased first in saying yamo itself is not a touring complete and it is not a programming language that users",
    "start": "1916960",
    "end": "1923440"
  },
  {
    "text": "want to use at the authoring time at least by the meantime we think yamaha is also",
    "start": "1923440",
    "end": "1929440"
  },
  {
    "text": "very valuable and is industry-based practice also use yama or yamaha equivalent",
    "start": "1929440",
    "end": "1936399"
  },
  {
    "text": "because it's very easy to for operations it's very easy to operate operate and understand your you know deployment",
    "start": "1936399",
    "end": "1942799"
  },
  {
    "text": "graph in a single file yaml file that you know exactly what is going on uh",
    "start": "1942799",
    "end": "1947840"
  },
  {
    "text": "what configuration you have used how is it deployed how many replicas you have and then you can easily turn the knobs",
    "start": "1947840",
    "end": "1953760"
  },
  {
    "text": "to reconfigure lightweight without understanding code we also highly value this pattern",
    "start": "1953760",
    "end": "1958799"
  },
  {
    "text": "so we not eliminating yaml and in our end-to-end solution we actually have a yama file for the configuration as well",
    "start": "1958799",
    "end": "1964960"
  },
  {
    "text": "because we think this is the right contract to use but our argument here is we can have the",
    "start": "1964960",
    "end": "1970559"
  },
  {
    "text": "benefit of book we can have a fully python programmable way to author your graph without writing yaml for the data",
    "start": "1970559",
    "end": "1976480"
  },
  {
    "text": "science theme mles while the authoring time for your iteration in any type we can automatically",
    "start": "1976480",
    "end": "1981760"
  },
  {
    "text": "generate the yellow file for you so you can later use for deployment and reconfiguration",
    "start": "1981760",
    "end": "1988240"
  },
  {
    "text": "and the second point is uh given we highly value local development experience we want to bring",
    "start": "1988480",
    "end": "1994480"
  },
  {
    "text": "the best experience to you and in pythonic way so which means by design we want to make",
    "start": "1994480",
    "end": "2000159"
  },
  {
    "text": "sure all the authentic iterations you can do can be developed instantiated tested locally on your laptop",
    "start": "2000159",
    "end": "2008158"
  },
  {
    "text": "and in the meantime in inference graph scenario it's very easy to see when we have multiple models combined they have",
    "start": "2008799",
    "end": "2015760"
  },
  {
    "text": "different hardware resource requirement they have very different computer patterns some of them could be cpu intensive some of them might be memory",
    "start": "2015760",
    "end": "2022240"
  },
  {
    "text": "intensive and we we take it very seriously about leveraging the strings of ray which we",
    "start": "2022240",
    "end": "2027600"
  },
  {
    "text": "showed earlier to make each model and each node in the deployment graph independently scalable and configurable",
    "start": "2027600",
    "end": "2035600"
  },
  {
    "text": "and we also use a unified graph api because we acknowledge the fact that dag",
    "start": "2035600",
    "end": "2040640"
  },
  {
    "text": "is a very common and graph is a very common abstraction people like to use and you see this in the inference graph",
    "start": "2040640",
    "end": "2046799"
  },
  {
    "text": "this is in machine learning models you see this in data pipelines so we want to make sure the api reintroduce is unified",
    "start": "2046799",
    "end": "2052800"
  },
  {
    "text": "across the entire ray ecosystem okay now",
    "start": "2052800",
    "end": "2058240"
  },
  {
    "text": "i'm going to walk you through one example we have in the documentation which you can check out on our website",
    "start": "2058240",
    "end": "2064560"
  },
  {
    "text": "and i just please have full attention here because this might be some details here that you might be get lost and i",
    "start": "2064560",
    "end": "2071118"
  },
  {
    "text": "will try my best to cover all everything we need but we have five steps here and it's",
    "start": "2071119",
    "end": "2077440"
  },
  {
    "text": "important i have a high level overview to show you what the application is about and i'm gonna walk through each",
    "start": "2077440",
    "end": "2082720"
  },
  {
    "text": "step later on so let's say i start with a single node that captures your input to this graph",
    "start": "2082720",
    "end": "2088800"
  },
  {
    "text": "and it could be any type of type of object maybe an image maybe a text maybe a paragraph maybe a list of numbers",
    "start": "2088800",
    "end": "2095839"
  },
  {
    "text": "and for the same input i want to send to two preprocessors and the preprocessor here could be a",
    "start": "2095839",
    "end": "2101359"
  },
  {
    "text": "class or function and for example let's say for simplicity let's use a function and the same request that you send in",
    "start": "2101359",
    "end": "2108000"
  },
  {
    "text": "will be sent out and dispatched to those preprocessors at the same time in parallel",
    "start": "2108000",
    "end": "2114240"
  },
  {
    "text": "now after the preprocessor finished the competition we're going to have a combiner to combine those outputs",
    "start": "2114240",
    "end": "2120400"
  },
  {
    "text": "as one as a single request and the single request then will be",
    "start": "2120400",
    "end": "2125440"
  },
  {
    "text": "dispatched to two downstream models and we can assume those to be a model to do some recommendation or computer",
    "start": "2125440",
    "end": "2131920"
  },
  {
    "text": "vision or electrical language processing regression model and at the very end we're gonna have an",
    "start": "2131920",
    "end": "2138960"
  },
  {
    "text": "aggregation to aggregate the results and i added one more arrow here straight from your inputs to the final stage of",
    "start": "2138960",
    "end": "2145599"
  },
  {
    "text": "dynamic aggregates this means this final aggregation not just take the output of model 1 and 2 which is my",
    "start": "2145599",
    "end": "2152400"
  },
  {
    "text": "directly adjacent neighbor but i can also come from arbitrarily random upstream nodes in this case it's",
    "start": "2152400",
    "end": "2158160"
  },
  {
    "text": "directly your inputs in your input we configure actually i want to take the sum of the output of",
    "start": "2158160",
    "end": "2164000"
  },
  {
    "text": "those two models or i may want to take the average or i want to take the max value of them depending on what i want",
    "start": "2164000",
    "end": "2170160"
  },
  {
    "text": "to do for this particular request so this shows about control flow and dynamic aggregation",
    "start": "2170160",
    "end": "2176720"
  },
  {
    "text": "okay so let's get started with the first step and say your input is here and",
    "start": "2176720",
    "end": "2183040"
  },
  {
    "text": "we introduce one of the key api here is we have a class called input node that represents your input to the graph and",
    "start": "2183040",
    "end": "2189520"
  },
  {
    "text": "this object will be resolved at runtime and the context we use here is",
    "start": "2189520",
    "end": "2196320"
  },
  {
    "text": "we you can use a with input node as a context manager and simply assign a a",
    "start": "2196320",
    "end": "2201599"
  },
  {
    "text": "name to a stack input this dac input is a global singleton",
    "start": "2201599",
    "end": "2207839"
  },
  {
    "text": "a resuspective particular graph because at each time your invocation of your graph we assume you won't have one input",
    "start": "2207839",
    "end": "2213520"
  },
  {
    "text": "even those input could be a complicated object now let's send it to the preprocessor",
    "start": "2213520",
    "end": "2219440"
  },
  {
    "text": "and here we're going to have the second api introduce dot bind and bind is our first class graph",
    "start": "2219440",
    "end": "2225599"
  },
  {
    "text": "building api that could work on any supported decorated class or function and what it means here is for this",
    "start": "2225599",
    "end": "2232480"
  },
  {
    "text": "particular processor as let's say it's a function decorated by surf.deployment",
    "start": "2232480",
    "end": "2237520"
  },
  {
    "text": "and we can directly calling.bind and the inputs we fit into it is the value",
    "start": "2237520",
    "end": "2243280"
  },
  {
    "text": "partial value of your usual input at index 0. and as a result we created the nodes in",
    "start": "2243280",
    "end": "2249680"
  },
  {
    "text": "this graph and that input node is also another node in this graph and they're connected and they're connected because",
    "start": "2249680",
    "end": "2255839"
  },
  {
    "text": "we claimed the preprocessor need to take parts of the original first input and we can assign a variable to it called",
    "start": "2255839",
    "end": "2262079"
  },
  {
    "text": "pre-process one similarly we can also do that for another preprocessor let's say we call",
    "start": "2262079",
    "end": "2267280"
  },
  {
    "text": "this and take an average of your input and we can use the same context calling dot bind but then we can take your user",
    "start": "2267280",
    "end": "2274960"
  },
  {
    "text": "input at index one and bind it to the average processor function and we assign a name to it called preprocess two",
    "start": "2274960",
    "end": "2282160"
  },
  {
    "text": "and we aim to make this as intuitive as if you're writing plain python code as much as possible and the only new",
    "start": "2282160",
    "end": "2287920"
  },
  {
    "text": "concept in here is essentially the bind we're you're in calling.bind to bind the",
    "start": "2287920",
    "end": "2294240"
  },
  {
    "text": "futures for the future inputs you're going to receive at the runtime and yes we have a static definition of",
    "start": "2294240",
    "end": "2299680"
  },
  {
    "text": "dependence and now the second step here now we have the input to the graph and let's start",
    "start": "2299680",
    "end": "2306320"
  },
  {
    "text": "adding models so that bind can also be called on a class or a class method",
    "start": "2306320",
    "end": "2312720"
  },
  {
    "text": "so what i mean here is let's say we have a model class decorated by surf deployment and is",
    "start": "2312720",
    "end": "2319119"
  },
  {
    "text": "function signature essentially i can instantiate this model class with the initial value that represents my weight",
    "start": "2319119",
    "end": "2324640"
  },
  {
    "text": "it's just the integer and in this case calling a model that bite one means i'm i want to get an instance of mod",
    "start": "2324640",
    "end": "2330960"
  },
  {
    "text": "model initializes value of one and i want to use it later on so i assign a variable name m1",
    "start": "2330960",
    "end": "2338240"
  },
  {
    "text": "similarly i can do the same for another model and note that in this case the model",
    "start": "2338240",
    "end": "2343520"
  },
  {
    "text": "implementation is exactly the same and we can reuse the same class definition and give a different instantiation value",
    "start": "2343520",
    "end": "2349520"
  },
  {
    "text": "input argument to it and now let's start combining our combiner have a dependency for m1 m2",
    "start": "2349520",
    "end": "2357920"
  },
  {
    "text": "because you need to take actions on those two models and to dispatch requests so a combiner if we check the can the",
    "start": "2357920",
    "end": "2364480"
  },
  {
    "text": "signature of the combiner could simply be i need to have two classes that i can take action on and i want this in my",
    "start": "2364480",
    "end": "2371440"
  },
  {
    "text": "constructor so essentially in here the combiner that binds m1 m2 means",
    "start": "2371440",
    "end": "2376560"
  },
  {
    "text": "i can use those two models in the combiner's implementation so in here you can notice um",
    "start": "2376560",
    "end": "2383200"
  },
  {
    "text": "even though we are creating nodes as intermediate values as we're building this graph we can pass around nodes as input",
    "start": "2383200",
    "end": "2389839"
  },
  {
    "text": "arguments into other parts of the graph either as a product constructor or as a",
    "start": "2389839",
    "end": "2395280"
  },
  {
    "text": "part of function call and now let's connect this with the previous parts we have for your inputs",
    "start": "2395280",
    "end": "2401599"
  },
  {
    "text": "going through two preprocessors and combining here and now we have another bind on a class",
    "start": "2401599",
    "end": "2407440"
  },
  {
    "text": "method our implementation of the combiner can have a user-provided implementation say just called run",
    "start": "2407440",
    "end": "2414240"
  },
  {
    "text": "and that that run once executed it will take some actions on the pre-process value upon evoking",
    "start": "2414240",
    "end": "2421040"
  },
  {
    "text": "the model one and two but i can use the previously computed values which i call preprocess one and",
    "start": "2421040",
    "end": "2426720"
  },
  {
    "text": "two the values i have computed previously i can directly pass this in assuming they'll be resolved at the",
    "start": "2426720",
    "end": "2432880"
  },
  {
    "text": "runtime as well as a user original input telling me if i want to do a sum or i",
    "start": "2432880",
    "end": "2439040"
  },
  {
    "text": "want to take a max so this is a valid that we can run it end-to-end essentially this means we",
    "start": "2439040",
    "end": "2445040"
  },
  {
    "text": "have the pre-process input ascending to a combiner and combiner will take the instantiation models and depend on the user input we put it will",
    "start": "2445040",
    "end": "2451920"
  },
  {
    "text": "call its run method now the third step is the last piece for",
    "start": "2451920",
    "end": "2457599"
  },
  {
    "text": "this deck building which is about dynamic aggregation and as i mentioned earlier we have three",
    "start": "2457599",
    "end": "2463599"
  },
  {
    "text": "arrows pointed out here let's say we have one model one output i just use",
    "start": "2463599",
    "end": "2468800"
  },
  {
    "text": "a color to represent this particular variable and model two has its own alto 2 and also user input is a particular",
    "start": "2468800",
    "end": "2476319"
  },
  {
    "text": "operation i want to tell what to do so let's have a dynamic aggregation here",
    "start": "2476319",
    "end": "2481680"
  },
  {
    "text": "it could be any function or a particular class method within the class implementation after your choice and all",
    "start": "2481680",
    "end": "2487680"
  },
  {
    "text": "we need to care about in this case is how we bind it and you can see dynamic aggregation could be done very trivially even though",
    "start": "2487680",
    "end": "2494079"
  },
  {
    "text": "the dependency could be complicated because we literally just make it as if you're writing a python application",
    "start": "2494079",
    "end": "2499920"
  },
  {
    "text": "you pass those values in which we are available upstream in the previous deck building",
    "start": "2499920",
    "end": "2505520"
  },
  {
    "text": "and upon receiving it in your implementation of this dynamic function you just simply write python code",
    "start": "2505520",
    "end": "2511040"
  },
  {
    "text": "if the operation i got is a sum do some i mean if the if the other one i get i just take a mass",
    "start": "2511040",
    "end": "2518319"
  },
  {
    "text": "so this essentially concludes of the backbone building of this entire deployment graph",
    "start": "2518319",
    "end": "2523760"
  },
  {
    "text": "now let's step into the research territory because now we need to expose http and",
    "start": "2523760",
    "end": "2529839"
  },
  {
    "text": "with the configured other attributes that are specific so let's start with the deck we have in",
    "start": "2529839",
    "end": "2536079"
  },
  {
    "text": "here already and this we call this the unit of tag now we need to have one more deployment",
    "start": "2536079",
    "end": "2543359"
  },
  {
    "text": "that exposed http and as is a user-facing component and a lot of things can happen here you can have fast",
    "start": "2543359",
    "end": "2549680"
  },
  {
    "text": "api integration you can have http endpoints you can have other things that you care about for particular exposing",
    "start": "2549680",
    "end": "2556160"
  },
  {
    "text": "this particular graph and driver is also a deployment so all",
    "start": "2556160",
    "end": "2561280"
  },
  {
    "text": "the nodes you see in here is a serve deployment and the driver is no different so when we bring the driver we provide a",
    "start": "2561280",
    "end": "2567760"
  },
  {
    "text": "default driver implementation that you can directly import but if you have need to customize you can also do free to",
    "start": "2567760",
    "end": "2573760"
  },
  {
    "text": "implement your own which is also very simple as if writing a simple dependent class",
    "start": "2573760",
    "end": "2578880"
  },
  {
    "text": "and now you can bring your own options for this deck driver and in this case let's say i just want",
    "start": "2578880",
    "end": "2584079"
  },
  {
    "text": "to customize the prefix i want to use for my http endpoint now you can easily just do the options and bring it here",
    "start": "2584079",
    "end": "2591200"
  },
  {
    "text": "and this i need then i need to find this driver as the ingress of my application",
    "start": "2591200",
    "end": "2597040"
  },
  {
    "text": "so i need to call in another top line so those pieces are connected and i need to wrap my dag into it as well",
    "start": "2597040",
    "end": "2603520"
  },
  {
    "text": "and we added one more piece for user convenience also to improve interop between io libraries in the ecosystem",
    "start": "2603520",
    "end": "2610560"
  },
  {
    "text": "which we provide a few default input schema adapters and you are also free to",
    "start": "2610560",
    "end": "2615680"
  },
  {
    "text": "implement your own so what it means is when you will receive the request from",
    "start": "2615680",
    "end": "2621200"
  },
  {
    "text": "hd event point which we can assume this is a play json object but in an actual dac execution you",
    "start": "2621200",
    "end": "2627680"
  },
  {
    "text": "probably want to have another python object in a more particular structure particular format",
    "start": "2627680",
    "end": "2632720"
  },
  {
    "text": "or dimension so we provide a few default ones for convenience you can also bring your own",
    "start": "2632720",
    "end": "2639520"
  },
  {
    "text": "and essentially those two equivalent one where the driver exposed here will",
    "start": "2639520",
    "end": "2644800"
  },
  {
    "text": "provide the best experience that we can aim and also with the flexibility in mind so that when you're calling the dac",
    "start": "2644800",
    "end": "2650640"
  },
  {
    "text": "application you literally mostly tested earlier and you have the correct python object",
    "start": "2650640",
    "end": "2656079"
  },
  {
    "text": "to invoke and call into this stack that essentially got fulfilled at runtime that represents your user input",
    "start": "2656079",
    "end": "2662480"
  },
  {
    "text": "so this concludes the pieces of back building iterators as well as exposing the search specific deployment and http",
    "start": "2662480",
    "end": "2670400"
  },
  {
    "text": "and the last step is just locally running and testing and the more concrete details of this is",
    "start": "2670400",
    "end": "2676960"
  },
  {
    "text": "at the end at the end of this meet-up which we have the end-to-end demo but in here we can show you a high-level",
    "start": "2676960",
    "end": "2682560"
  },
  {
    "text": "experience of the flow look like and all the everything up to step three",
    "start": "2682560",
    "end": "2688880"
  },
  {
    "text": "and it's about local developing we want you to have a really smooth pythonic local development experience you can",
    "start": "2688880",
    "end": "2694400"
  },
  {
    "text": "have quick updates fewer replicas locally tested in python then later on which is the talk that she",
    "start": "2694400",
    "end": "2701119"
  },
  {
    "text": "is going to cover more with more comprehensive overview as well as details is about how we",
    "start": "2701119",
    "end": "2706160"
  },
  {
    "text": "operationalizing this application that you're happy with on your laptop that includes structural deployment",
    "start": "2706160",
    "end": "2713119"
  },
  {
    "text": "deployment artifact and in the real deployment situation how you configure replicas you want to use what a yama",
    "start": "2713119",
    "end": "2719119"
  },
  {
    "text": "file is going to look like so similarly we want to have experience from laptop to",
    "start": "2719119",
    "end": "2724240"
  },
  {
    "text": "remote cluster for deployment and there are a few improvement areas we can",
    "start": "2724240",
    "end": "2730720"
  },
  {
    "text": "do given that we recently announced this so one of the key things is about really",
    "start": "2730720",
    "end": "2737520"
  },
  {
    "text": "making it polished for this path from your laptop's remote cluster which means you improve operationalizing story",
    "start": "2737520",
    "end": "2742960"
  },
  {
    "text": "and how we can not facilitate just the data science team but also the mle",
    "start": "2742960",
    "end": "2748960"
  },
  {
    "text": "team and the operationalizing devops and ops team which you can learn more from previous talk later on",
    "start": "2748960",
    "end": "2754960"
  },
  {
    "text": "and given the fact that we now have a static definition of the competition graph and there are a few low hanging",
    "start": "2754960",
    "end": "2760640"
  },
  {
    "text": "fruits you can grab that we can based on profiling of a deployment graph because now it runs in the right sim ray cluster",
    "start": "2760640",
    "end": "2768000"
  },
  {
    "text": "we can do some automatic performance operations uh regarding placement collocation",
    "start": "2768000",
    "end": "2773280"
  },
  {
    "text": "multiplexing and better leverage in memory and obviously there are a lot more work",
    "start": "2773280",
    "end": "2778960"
  },
  {
    "text": "we can do about ux and visualization ideally when you deploy your deployment graph we can have some simple api or",
    "start": "2778960",
    "end": "2785760"
  },
  {
    "text": "simple experience for you so you can visualize what it looks like and what the interface between those things look like maybe even later on if you",
    "start": "2785760",
    "end": "2792240"
  },
  {
    "text": "have enough resource we can even have a dragon drop which i heard from some users in particular like this experience",
    "start": "2792240",
    "end": "2797920"
  },
  {
    "text": "to build their deployment graph okay so as a bonus point uh i'm going to",
    "start": "2797920",
    "end": "2803200"
  },
  {
    "text": "wrestle this very quickly it's just saying back it dag and graph is a very common pattern in many competitions",
    "start": "2803200",
    "end": "2809200"
  },
  {
    "text": "workload you carry on data pipelines inference graphs workflows right so",
    "start": "2809200",
    "end": "2814400"
  },
  {
    "text": "uh we can you can learn more details about this in our public rfc for reef 2.0 but what we showed you here is",
    "start": "2814400",
    "end": "2822000"
  },
  {
    "text": "essentially completely unified with the common deck api as a first class in rate 2.0",
    "start": "2822000",
    "end": "2827280"
  },
  {
    "text": "which means we have a same set of apis for authoring and executing and we have each library specific piece",
    "start": "2827280",
    "end": "2835119"
  },
  {
    "text": "that fit in well together cool so in conclusion and essentially i walked you through",
    "start": "2835119",
    "end": "2841440"
  },
  {
    "text": "about a problem we observed in the industry and we think this is increasingly important problem for us to solve array is well positioned to solve",
    "start": "2841440",
    "end": "2848000"
  },
  {
    "text": "it and it's hard to author iterate locally and if there's a lot of terms how to",
    "start": "2848000",
    "end": "2854960"
  },
  {
    "text": "deploy a locally tested application even though it's very complicated and how to avoid terms of writing yammer files and",
    "start": "2854960",
    "end": "2860800"
  },
  {
    "text": "play around with those gamma files to be more catholic and in",
    "start": "2860800",
    "end": "2866040"
  },
  {
    "text": "multi-deployment graph scenario we also think performance is super critical because there's a lot of opportunities",
    "start": "2866040",
    "end": "2872400"
  },
  {
    "text": "that we can user can leverage on the missed opportunities that could be performed better that is simple application is",
    "start": "2872400",
    "end": "2878720"
  },
  {
    "text": "easy to see but when the graph got larger across the boundaries will be more difficult to start",
    "start": "2878720",
    "end": "2883839"
  },
  {
    "text": "and here we also present you the solution about serve deployment graph api and often in time we really make it",
    "start": "2883839",
    "end": "2890800"
  },
  {
    "text": "python native as much as we can and enables your local python local development and testing experience",
    "start": "2890800",
    "end": "2896480"
  },
  {
    "text": "and then we also showed you the path to to leverage the core streams of ray about making it more efficient and",
    "start": "2896480",
    "end": "2902319"
  },
  {
    "text": "scalable in production cool and this concludes my presentation",
    "start": "2902319",
    "end": "2908480"
  },
  {
    "text": "and here's the q a section and please get in touch with us and you can see a few links that if you want to get",
    "start": "2908480",
    "end": "2914160"
  },
  {
    "text": "engaged thank you thank you thank you joe thank you for that insightful uh commentary",
    "start": "2914160",
    "end": "2920400"
  },
  {
    "text": "and and great great um talk about the graph apis i think one of",
    "start": "2920400",
    "end": "2925680"
  },
  {
    "text": "the things i really liked about the fact that you can actually just pretty much use python to bind everything together",
    "start": "2925680",
    "end": "2931760"
  },
  {
    "text": "one thing i wasn't sure about was that i didn't see any specific call to",
    "start": "2931760",
    "end": "2936880"
  },
  {
    "text": "deployment where does where does all the all the deployments happen um for each",
    "start": "2936880",
    "end": "2942240"
  },
  {
    "text": "and every deployment node that you actually have is that in the last call where where the driver does that or",
    "start": "2942240",
    "end": "2948720"
  },
  {
    "text": "or that's somehow hidden when you do the bind could you could you explain how that happens",
    "start": "2948720",
    "end": "2953839"
  },
  {
    "text": "okay so uh so about the deployment so i can walk a little more detail about what",
    "start": "2953839",
    "end": "2959119"
  },
  {
    "text": "the deployment is and how it operates so as deployment you can we can think of a deployment essentially as a group of",
    "start": "2959119",
    "end": "2964640"
  },
  {
    "text": "actors and let's say it could be holding your application you might have 10 actors",
    "start": "2964640",
    "end": "2970000"
  },
  {
    "text": "right and every single piece you see in our graph building let's say our",
    "start": "2970000",
    "end": "2975040"
  },
  {
    "text": "preprocessor the preprocessor by default we make it a deployment and that deployment could have an arbitrary",
    "start": "2975040",
    "end": "2980240"
  },
  {
    "text": "number of replicas we could have maybe two maybe five depends on how scalable we want to make if we",
    "start": "2980240",
    "end": "2986319"
  },
  {
    "text": "observe this to be at a bottleneck so essentially you can see the backbone of the deployment graph at backed by",
    "start": "2986319",
    "end": "2992319"
  },
  {
    "text": "deployment and essentially there's a little abstraction of nodes drawing circles are backed by individual actors",
    "start": "2992319",
    "end": "2999599"
  },
  {
    "text": "and that is independently scalable and driver is also a deployment so",
    "start": "2999599",
    "end": "3005119"
  },
  {
    "text": "essentially under the hood when we are authoring this graph we are using deployment as a higher level abstraction",
    "start": "3005119",
    "end": "3011359"
  },
  {
    "text": "and one that the fundamental graph is involved we're caught into essentially is just rate actor calls under the hood",
    "start": "3011359",
    "end": "3018000"
  },
  {
    "text": "like we are rounding the request to the credit deployment and then we're out of the next note and the next time we're going to find the right replica to serve",
    "start": "3018000",
    "end": "3024240"
  },
  {
    "text": "for the right particular actor to serve a request and also by knowing the whole dependency graph you can fill up to the",
    "start": "3024240",
    "end": "3030640"
  },
  {
    "text": "end and give you the final result yeah i mean i do like i do like the before and answer before and after contrast when",
    "start": "3030640",
    "end": "3037119"
  },
  {
    "text": "people were manually composing all those all those nodes and then actually individually deploying each and every uh",
    "start": "3037119",
    "end": "3044240"
  },
  {
    "text": "each and every instance of the model deployment whereas over here everything is actually done implicit and it's very clean uh i think one question that pops",
    "start": "3044240",
    "end": "3051119"
  },
  {
    "text": "into mind with this new development graph apis you know what kind of applications would people build",
    "start": "3051119",
    "end": "3058079"
  },
  {
    "text": "using this particular api that's that's the first question and i think the corollary of the question is",
    "start": "3058079",
    "end": "3064559"
  },
  {
    "text": "what are some of the new possibilities that that unfold from this uh new api api that we have unfolded in",
    "start": "3064559",
    "end": "3072240"
  },
  {
    "text": "the uh in in the reserve okay and i think for the first part of",
    "start": "3072240",
    "end": "3078000"
  },
  {
    "text": "the question then we can think of a few patterns where we can assume the note uh the model fit",
    "start": "3078000",
    "end": "3083920"
  },
  {
    "text": "in the single node fit on a single machine right so in this sum of the applications and we learn from the",
    "start": "3083920",
    "end": "3089599"
  },
  {
    "text": "industry i can give you a few examples one of them is content understanding image classification we actually the the",
    "start": "3089599",
    "end": "3096160"
  },
  {
    "text": "end-to-end demo we have at the very end is exactly built for this purpose uh so for some product you might have a",
    "start": "3096160",
    "end": "3102720"
  },
  {
    "text": "product image that needs to go through a magnitude of maybe 10 to 50 models and each of them is fine-tuned for a",
    "start": "3102720",
    "end": "3108960"
  },
  {
    "text": "particular purpose maybe a model is fine-tuned to learn if this is a bag or not maybe the other model is about to",
    "start": "3108960",
    "end": "3114960"
  },
  {
    "text": "learn if user uploaded this image is the image violent or not and is this hateful",
    "start": "3114960",
    "end": "3120079"
  },
  {
    "text": "or is it integrity like integrity checks right and some of this could be",
    "start": "3120079",
    "end": "3125599"
  },
  {
    "text": "about just like so this image is one particular example essentially we're using the power of",
    "start": "3125599",
    "end": "3131280"
  },
  {
    "text": "machine learning multi-modal calculated to annotate this image so we know more about this image we can empower other",
    "start": "3131280",
    "end": "3137839"
  },
  {
    "text": "products down the line as they output from other teams that is one and the other we can think of is a same for nlp",
    "start": "3137839",
    "end": "3144079"
  },
  {
    "text": "like natural language understanding is the same then you can send a document document to a particular service",
    "start": "3144079",
    "end": "3150000"
  },
  {
    "text": "and you want to run through multiple models some of the models are good at detecting the boundary of different paragraphs some of the models are really",
    "start": "3150000",
    "end": "3156640"
  },
  {
    "text": "good at detecting a bunch of sentences and some of the parts of the model is good at tokenizing the sentence into a",
    "start": "3156640",
    "end": "3162960"
  },
  {
    "text": "sequence of inputs and get the output and some of them is good at aggregation and another pattern i can think of is a",
    "start": "3162960",
    "end": "3170640"
  },
  {
    "text": "recommendation system in recommendation system is very common not just mixing multiple models or",
    "start": "3170640",
    "end": "3176400"
  },
  {
    "text": "missing business logic you usually have a funnel of pipeline essentially let's say you start your candidate generation starting",
    "start": "3176400",
    "end": "3182880"
  },
  {
    "text": "from say you have over 100 000 candidates upstream you want to apply a subset of models",
    "start": "3182880",
    "end": "3189440"
  },
  {
    "text": "that you can fine tune this down to a thousand candidates so save calls make it simple but as you",
    "start": "3189440",
    "end": "3194960"
  },
  {
    "text": "proceed down the line the model is going to get more complicated you have you know more about the user you know more",
    "start": "3194960",
    "end": "3200559"
  },
  {
    "text": "features and the features got more expensive the model get more expensive you're going to fine-tune this later on",
    "start": "3200559",
    "end": "3205760"
  },
  {
    "text": "until the very end stage so this naturally forms a funnel and the photo i have various inputs multiple models",
    "start": "3205760",
    "end": "3212240"
  },
  {
    "text": "and you have mixed complexity right so i can i would say offering this type of",
    "start": "3212240",
    "end": "3217359"
  },
  {
    "text": "pipeline is extremely difficult and scaling is also very hard but but while we're designing this api",
    "start": "3217359",
    "end": "3223359"
  },
  {
    "text": "we want to make it easy for users brilliant and um i think one one question that really",
    "start": "3223359",
    "end": "3229760"
  },
  {
    "text": "begs is that you you showed the the directed acyclic graph whereby you actually had the input",
    "start": "3229760",
    "end": "3235440"
  },
  {
    "text": "that was coming in and you were fanning out to two possible preprocessors and those three processors could be anything",
    "start": "3235440",
    "end": "3240960"
  },
  {
    "text": "that could be something that the aggregating data say from a feature store or they're looking up something something in the database is it possible",
    "start": "3240960",
    "end": "3247920"
  },
  {
    "text": "even though this is a direct cyclic graph is it possible to put circularity in it i mean is it possible that i can",
    "start": "3247920",
    "end": "3254400"
  },
  {
    "text": "bind something to to the previous node and if if that what happens do we actually detect that that is a",
    "start": "3254400",
    "end": "3260000"
  },
  {
    "text": "circularity in in the directory uh that's the interesting pattern uh i",
    "start": "3260000",
    "end": "3265440"
  },
  {
    "text": "honestly wouldn't say we happily tested this pattern but in wild rival recent bug bashes we actually run it and it",
    "start": "3265440",
    "end": "3271680"
  },
  {
    "text": "worked but we don't think this is a this is a particular pattern we highly optimized yet but if someone have a",
    "start": "3271680",
    "end": "3278240"
  },
  {
    "text": "really strong need for this we can chat more but by design we have a simple example that runs",
    "start": "3278240",
    "end": "3284480"
  },
  {
    "text": "but i mean what you're suggesting is that is that you know they can actually have circularity but the but the",
    "start": "3284480",
    "end": "3291440"
  },
  {
    "text": "side effects of that uh we don't know yet yeah i would say it's less tested this",
    "start": "3291440",
    "end": "3296480"
  },
  {
    "text": "might be a long tail pattern maybe the last five percent of that okay i think francis has an interesting",
    "start": "3296480",
    "end": "3302559"
  },
  {
    "text": "question because i think this definitely would come up when you're actually building these infant graphs and you're building",
    "start": "3302559",
    "end": "3308319"
  },
  {
    "text": "this directx cyclic pipelines how do you deal with versioning you know how does",
    "start": "3308319",
    "end": "3313599"
  },
  {
    "text": "the api support a level of versioning programmatically so if i create an",
    "start": "3313599",
    "end": "3318720"
  },
  {
    "text": "inference graph one and then i want to go ahead and update that interest graph second how do you know which version to deploy",
    "start": "3318720",
    "end": "3324880"
  },
  {
    "text": "and when to deploy and how do you keep track of that is there something that we do that generically is it something that we have",
    "start": "3324880",
    "end": "3331359"
  },
  {
    "text": "to somehow store that you know in a meta store or something could you",
    "start": "3331359",
    "end": "3336880"
  },
  {
    "text": "this is a great question we actually have a lot of discussions around this topic while we're designing the operationalizing story around it",
    "start": "3336880",
    "end": "3344559"
  },
  {
    "text": "so um my personal take on this is i think we should categorize on the frequency and",
    "start": "3344559",
    "end": "3350480"
  },
  {
    "text": "the importance of different types of updates that could happen so uh from what we observe let's say in",
    "start": "3350480",
    "end": "3357680"
  },
  {
    "text": "let's say we deploy the inference graph end to end you have 10 nodes right now what we learned from the industry",
    "start": "3357680",
    "end": "3362720"
  },
  {
    "text": "the most frequent action that happened is you want to update your model weights and the model weight is a very simple",
    "start": "3362720",
    "end": "3368960"
  },
  {
    "text": "link to could be some f3 bucket then you can assume it's just the same contract same version that you can",
    "start": "3368960",
    "end": "3374640"
  },
  {
    "text": "update and you can assume some compatibility guarantee right in place right but that covers",
    "start": "3374640",
    "end": "3379680"
  },
  {
    "text": "probably 90 of the actual updates people gonna do and then proceed with that you probably",
    "start": "3379680",
    "end": "3384880"
  },
  {
    "text": "want to have reconfiguration but reconfiguration means for this particular deployment node i have my",
    "start": "3384880",
    "end": "3390079"
  },
  {
    "text": "graph maybe i have a dynamic value and say uh 50 50 chance i do something",
    "start": "3390079",
    "end": "3397680"
  },
  {
    "text": "other than that i do the other one and there's some knobs you can tune that actually does not even need to trigger",
    "start": "3397680",
    "end": "3402720"
  },
  {
    "text": "rolling updates that we can need to be supported and by design each deployment we can",
    "start": "3402720",
    "end": "3408240"
  },
  {
    "text": "also support updating it place within the same rig cluster let's say if you have your interface that you know it is",
    "start": "3408240",
    "end": "3415040"
  },
  {
    "text": "compatible you can actually trigger only object with your code uh in place which means like that you",
    "start": "3415040",
    "end": "3422160"
  },
  {
    "text": "can see your graph and somewhere in the middle of the graph then you're going to see two versions of the replicas",
    "start": "3422160",
    "end": "3427200"
  },
  {
    "text": "spawning in the real time and the request could be directed to either one of them okay",
    "start": "3427200",
    "end": "3432400"
  },
  {
    "text": "right so that could happen in in the case that if user is confident about the interface",
    "start": "3432400",
    "end": "3437920"
  },
  {
    "text": "is correct and the update is safe you can locally test and try to deploy this and try to make this update inputs",
    "start": "3437920",
    "end": "3444319"
  },
  {
    "text": "but then it is realistic but i think realistically it's about industry best",
    "start": "3444319",
    "end": "3451040"
  },
  {
    "text": "practice because uh from our discussions also my personal experience working in maybe larger",
    "start": "3451040",
    "end": "3457119"
  },
  {
    "text": "companies usually the safest way to do this type of update is",
    "start": "3457119",
    "end": "3462799"
  },
  {
    "text": "you start a clone at a smaller scale of the new version of the application you want to deploy you can redirect a very",
    "start": "3462799",
    "end": "3469119"
  },
  {
    "text": "small amount of traffic to it and watch the key metrics and see if you're confident about it and",
    "start": "3469119",
    "end": "3475119"
  },
  {
    "text": "then you increase your traffic load and see if you can see any regression if not then you can do a camera deployment slowly deploying and redirecting your",
    "start": "3475119",
    "end": "3481680"
  },
  {
    "text": "traffic to the new one i think this is the safest and recommended way i would suggest user",
    "start": "3481680",
    "end": "3486720"
  },
  {
    "text": "could do but in the meantime i'm saying we have all those knots open that probably cover 90 percent of the common update needs",
    "start": "3486720",
    "end": "3493599"
  },
  {
    "text": "you have reconfigure update model weights and do in place code updates assuming interface is safe that you're",
    "start": "3493599",
    "end": "3499440"
  },
  {
    "text": "confident that it works i think finally i think francis was alluding to the fact that can you",
    "start": "3499440",
    "end": "3504960"
  },
  {
    "text": "actually do what they call versioning pipeline or can you actually take a snapshot uh one last question from ego",
    "start": "3504960",
    "end": "3511359"
  },
  {
    "text": "before we actually move on because we have to do the finale and share with the community uh let me read something from",
    "start": "3511359",
    "end": "3518000"
  },
  {
    "text": "eco one common problem that they had actually encountered was when creating the model pipelines with serve",
    "start": "3518000",
    "end": "3523680"
  },
  {
    "text": "deployments was tuning and scaling the individual deployments to avoid any pipeline stall and i think",
    "start": "3523680",
    "end": "3530400"
  },
  {
    "text": "what he meant was was probably slow deployment what i faced most was was filling the",
    "start": "3530400",
    "end": "3536640"
  },
  {
    "text": "object store which which means that you know the objective was spilling uh into the disk it's consistently spilling",
    "start": "3536640",
    "end": "3543119"
  },
  {
    "text": "was was too fast while scaling up was was too slow in deployments does the new",
    "start": "3543119",
    "end": "3548720"
  },
  {
    "text": "dag api has any mechanism to prevent pipeline stall and i think basically it's asking that what about my my models",
    "start": "3548720",
    "end": "3555920"
  },
  {
    "text": "are really big my object is is is doesn't enough memory what would happen to your type of deployment would i start",
    "start": "3555920",
    "end": "3562160"
  },
  {
    "text": "spilling things at the disk see uh i think there are",
    "start": "3562160",
    "end": "3567200"
  },
  {
    "text": "a few parts of the problem i think one of them is about just disk spilling of object store now this is generally more",
    "start": "3567200",
    "end": "3574880"
  },
  {
    "text": "on the record or like a runtime level abstraction yeah yeah right that's one",
    "start": "3574880",
    "end": "3580319"
  },
  {
    "text": "part of the issue so i mean i think we're making active improvements in this particular area that's one on the core level but in the meantime at search",
    "start": "3580319",
    "end": "3587119"
  },
  {
    "text": "level we notice previously we assume we only have a few deployments running in this research cluster there",
    "start": "3587119",
    "end": "3593760"
  },
  {
    "text": "is not too much we need to do what we can do on the object store level but now with the deformed graph api",
    "start": "3593760",
    "end": "3599760"
  },
  {
    "text": "bring into the first class api and there are a lot of optimizations we will plan to do",
    "start": "3599760",
    "end": "3604880"
  },
  {
    "text": "to better make use of share objects board uh i can give two examples and i think edward also covered in in his",
    "start": "3604880",
    "end": "3610640"
  },
  {
    "text": "presentation earlier one of them is about let's say for a image recognition pipeline",
    "start": "3610640",
    "end": "3617520"
  },
  {
    "text": "the image in here traditionally if you just send this to multiple services you actually need to serialize this again",
    "start": "3617520",
    "end": "3623200"
  },
  {
    "text": "and visualize it and the image could be huge right and in in multi-graph deployment we actually",
    "start": "3623200",
    "end": "3629599"
  },
  {
    "text": "can avoid making any additional copies and just transfer or use the object",
    "start": "3629599",
    "end": "3635839"
  },
  {
    "text": "directly without the actual cost so that's one of the observations we spot when that's why we think reserve is",
    "start": "3635839",
    "end": "3641839"
  },
  {
    "text": "really what suited to solve this type of problem and the other is particularly from model weights",
    "start": "3641839",
    "end": "3647040"
  },
  {
    "text": "especially large model weights and we found there's a strong need to build uh it's let's call it a model",
    "start": "3647040",
    "end": "3654079"
  },
  {
    "text": "mesh or model cache management layer that is directly built on top of ray plasma store because the attribute of",
    "start": "3654079",
    "end": "3660960"
  },
  {
    "text": "zero copy model loading is very powerful like we work with one community member from ibm",
    "start": "3660960",
    "end": "3666480"
  },
  {
    "text": "and we showed with correct use of plasma store the cost of hot loading a model is even much",
    "start": "3666480",
    "end": "3672400"
  },
  {
    "text": "much lower than running a single inference that opens up a lot of new possibilities",
    "start": "3672400",
    "end": "3677440"
  },
  {
    "text": "and so i think the next item we are very likely looking into also looking for community engagement and feedback is",
    "start": "3677440",
    "end": "3684400"
  },
  {
    "text": "how i can better design this model mesh layer that can manage your model weights",
    "start": "3684400",
    "end": "3690160"
  },
  {
    "text": "either small ways but a lot of models or a super large model but it could take a lot of memory in the plasma store that",
    "start": "3690160",
    "end": "3697359"
  },
  {
    "text": "actually leads to objects building which we try to avoid so how to properly build this model casually on top of research",
    "start": "3697359",
    "end": "3702880"
  },
  {
    "text": "it's a very interesting optimization we're looking into well thanks a lot um joe i think igor",
    "start": "3702880",
    "end": "3709359"
  },
  {
    "text": "actually brought a good point and i think one of the uh segways well into the fact that these are the kind of",
    "start": "3709359",
    "end": "3714480"
  },
  {
    "text": "feedbacks we're actually looking for and so please you know take your iphone take your samsung phone just point the the",
    "start": "3714480",
    "end": "3721920"
  },
  {
    "text": "the thing to the uh to the scan and fill out the survey because we really want to get this particular feedback going so i",
    "start": "3721920",
    "end": "3728480"
  },
  {
    "text": "think just to tie things up together to find out you know what was the overview about that ed talked about uh what zhao",
    "start": "3728480",
    "end": "3734799"
  },
  {
    "text": "talked about in terms of you know scaling the the inference pipeline but then how do you actually operationalize",
    "start": "3734799",
    "end": "3740160"
  },
  {
    "text": "those things how do you actually put it into production and that's where shriyas comes into the the next talk to sort of tie things",
    "start": "3740160",
    "end": "3747359"
  },
  {
    "text": "up and then we'll have the finale demo that actually pretty much illustrates and talks about everything from from end",
    "start": "3747359",
    "end": "3753839"
  },
  {
    "text": "to end so stick around uh sure yes tell us what does it mean to operationalize race uh with this new friendly apis",
    "start": "3753839",
    "end": "3760480"
  },
  {
    "text": "we've been talking about definitely thanks so much jules um first of all can you all see the slides",
    "start": "3760480",
    "end": "3766640"
  },
  {
    "text": "uh yes we can awesome all right uh well everyone thanks for tuning in to this presentation about",
    "start": "3766640",
    "end": "3772640"
  },
  {
    "text": "operationalizing race serve and thanks for sticking to sticking with us through this uh meet up my name is trace and i'm",
    "start": "3772640",
    "end": "3778319"
  },
  {
    "text": "a racer engineer here at ending skill this talk is going to discuss how we're making ray serb in general more ops",
    "start": "3778319",
    "end": "3784160"
  },
  {
    "text": "friendly and some of the future work that we have planned for sir",
    "start": "3784160",
    "end": "3788559"
  },
  {
    "text": "so during the presentation first we're going to talk a little bit about the existing workflows that",
    "start": "3789920",
    "end": "3795520"
  },
  {
    "text": "we found that users are using to deploy on raceherv then we're going to discuss a new ops",
    "start": "3795520",
    "end": "3800880"
  },
  {
    "text": "friendly workflow that we're developing and we'll walk through some examples using this new workflow as well as a new",
    "start": "3800880",
    "end": "3806400"
  },
  {
    "text": "serve cli then we're going to circle back to js presentation about deployment graphs and show you how you can use this new",
    "start": "3806400",
    "end": "3812160"
  },
  {
    "text": "workflow to deploy your graphs in production so as you may have seen while working",
    "start": "3812160",
    "end": "3818640"
  },
  {
    "text": "with razer this is kind of a classic example of a serve deployment we have a class here my ml model that loads a",
    "start": "3818640",
    "end": "3824720"
  },
  {
    "text": "model when it's initialized using some sort of user configured path then whenever it receives an http request it parses the request and then",
    "start": "3824720",
    "end": "3831520"
  },
  {
    "text": "runs inference on the request input using that model now as we can see from this code it's",
    "start": "3831520",
    "end": "3837599"
  },
  {
    "text": "actually pretty simple to convert this model into a serve deployment uh and it's like pretty concise",
    "start": "3837599",
    "end": "3843359"
  },
  {
    "text": "but serve also allows you to configure your deployments for production so in this example a user can provide a path",
    "start": "3843359",
    "end": "3849680"
  },
  {
    "text": "to their model weights as an init argument to their class and they can also specify the http endpoint that",
    "start": "3849680",
    "end": "3855359"
  },
  {
    "text": "represents this model as well as the number of replicas the maximum concurrent queries and some resource requirements uh notice that there that",
    "start": "3855359",
    "end": "3862880"
  },
  {
    "text": "the actual model code is still the same as the previous slide only the configuration in the decorator has been",
    "start": "3862880",
    "end": "3868160"
  },
  {
    "text": "changed in this slide so as joe mentioned in his talk ideally",
    "start": "3868160",
    "end": "3874160"
  },
  {
    "text": "you want to be able to define your application logic and your deployment graphs in python for maximum flexibility",
    "start": "3874160",
    "end": "3881838"
  },
  {
    "text": "while you while at the same time you want to put your configuration parameters in some sort of standalone structured configuration file this gives",
    "start": "3882160",
    "end": "3888960"
  },
  {
    "text": "you maximum flexibility when you're writing your application but it also has operational advantages as you spend time",
    "start": "3888960",
    "end": "3894240"
  },
  {
    "text": "maintaining your configuration parameters with this config file you can provide straightforward version",
    "start": "3894240",
    "end": "3900240"
  },
  {
    "text": "control you can you can have ml ops capabilities as well as cli and rest interfaces the rest of",
    "start": "3900240",
    "end": "3906640"
  },
  {
    "text": "this talk is going to be about how we get from the mixed config and code on the left to the python centric api and",
    "start": "3906640",
    "end": "3911839"
  },
  {
    "text": "centralized config on the right so let's start by talking a little bit",
    "start": "3911839",
    "end": "3917839"
  },
  {
    "text": "about the processes or workflows that users generally follow to deploy their serve applications to production",
    "start": "3917839",
    "end": "3924319"
  },
  {
    "text": "this usually starts with developers data scientists and ml engineers developing and testing a racerb application locally",
    "start": "3924319",
    "end": "3931599"
  },
  {
    "text": "after that a team of ml ops folks deploys the application to production in some organizations this is like an",
    "start": "3931599",
    "end": "3937920"
  },
  {
    "text": "entirely separate team while in others the same people that develop the app are also the ones that deploy and manage it",
    "start": "3937920",
    "end": "3945280"
  },
  {
    "text": "lastly whenever you need to update your serve application you usually have the ml ops team write and apply python update scripts to their",
    "start": "3945680",
    "end": "3952480"
  },
  {
    "text": "live cluster now we've noticed some challenges with",
    "start": "3952480",
    "end": "3957760"
  },
  {
    "text": "this workflow specifically because you're defining the config in python first of all you don't have a source of",
    "start": "3957760",
    "end": "3963520"
  },
  {
    "text": "truth for the serve configuration running in your cluster since each update is applied as a standalone python script the true configuration running in",
    "start": "3963520",
    "end": "3970240"
  },
  {
    "text": "the cluster is the result of all of these different scripts combined second the configuration is mixed with",
    "start": "3970240",
    "end": "3976880"
  },
  {
    "text": "the code this is really challenging if your organization has a standalone ops or platform team because now they need",
    "start": "3976880",
    "end": "3983039"
  },
  {
    "text": "to navigate and stay up to date with your serv application code in order to understand all the configurations",
    "start": "3983039",
    "end": "3989599"
  },
  {
    "text": "and last it's a little hard to build custom ops tooling on top observe since there's no source of truth and no",
    "start": "3989599",
    "end": "3995520"
  },
  {
    "text": "standalone config there's no interface that you can use to build tools like custom ci cd pipelines on top of serv",
    "start": "3995520",
    "end": "4003200"
  },
  {
    "text": "so let's take a look at serv's new ops friendly workflow in this workflow developers still start",
    "start": "4004079",
    "end": "4010559"
  },
  {
    "text": "by developing and testing an app locally however now the ml ops folks use a structured standalone configuration file",
    "start": "4010559",
    "end": "4017599"
  },
  {
    "text": "to deploy to production at this stage you don't have to wrangle with python anymore whenever you want to apply an update you",
    "start": "4017599",
    "end": "4023920"
  },
  {
    "text": "can modify the config file and directly redeploy it to your ray cluster",
    "start": "4023920",
    "end": "4029680"
  },
  {
    "text": "when we compare these two workflows the current workflow and the new one the difference is in the operational loop in",
    "start": "4029760",
    "end": "4035039"
  },
  {
    "text": "the current workflow we're using python scripts to make incremental updates to our server application but in the new",
    "start": "4035039",
    "end": "4040079"
  },
  {
    "text": "ops friendly workflow we're accumulating all of our config changes into one config file and we're applying that file",
    "start": "4040079",
    "end": "4045520"
  },
  {
    "text": "directly to the server application this new workflow has some operational",
    "start": "4045520",
    "end": "4052079"
  },
  {
    "text": "advantages first it centralizes the config since the ops team's updates all have to go",
    "start": "4052079",
    "end": "4057599"
  },
  {
    "text": "through the same config file it's straightforward to track configuration history and roll back or roll forward the state of serve application",
    "start": "4057599",
    "end": "4064400"
  },
  {
    "text": "while you can perform this kind of version control on individual python update scripts it's a lot more straightforward to track a single",
    "start": "4064400",
    "end": "4071119"
  },
  {
    "text": "individual config file ultimately this file is the single source of truth for the serv application's running",
    "start": "4071119",
    "end": "4076960"
  },
  {
    "text": "configuration additionally by exposing the server applications configurations in a yaml",
    "start": "4076960",
    "end": "4082720"
  },
  {
    "text": "file these configurations become easier to access instead of digging through a python application to search for fields",
    "start": "4082720",
    "end": "4088559"
  },
  {
    "text": "like num replicas or max concurrent queries ops teams can read and write to a yaml config file that stores all these",
    "start": "4088559",
    "end": "4095200"
  },
  {
    "text": "settings in a single place this file also allows users to build custom ops tooling for serve it offers a",
    "start": "4095200",
    "end": "4102238"
  },
  {
    "text": "single unified interface for all of your config settings it also enables platform teams to build",
    "start": "4102239",
    "end": "4107600"
  },
  {
    "text": "operational tools on top of serv to better manage and operate serve deployments",
    "start": "4107600",
    "end": "4114480"
  },
  {
    "text": "all right let's take a look at what these config files look like so these config files are written in",
    "start": "4115679",
    "end": "4121758"
  },
  {
    "text": "yaml format and they all start with this deployments key that key contains a list of configurations for each deployment",
    "start": "4121759",
    "end": "4129440"
  },
  {
    "text": "so the file lets you specify your serve deployments by writing a deployment name and an import path as well as any",
    "start": "4129600",
    "end": "4135040"
  },
  {
    "text": "deployment configurations needed for each deployment for instance in this example this deployment's name is hello and it has",
    "start": "4135040",
    "end": "4142159"
  },
  {
    "text": "two replicas the code definition for the hello function or class can be imported from the file module which is why its import",
    "start": "4142159",
    "end": "4148560"
  },
  {
    "text": "path is file.hello likewise we have another deployment here and it's named world",
    "start": "4148560",
    "end": "4154880"
  },
  {
    "text": "and it serves a maximum of eight concurrent queries it can be imported from the file module as well all the",
    "start": "4154880",
    "end": "4160480"
  },
  {
    "text": "configurations you don't see on this image like the reactor options are going to use defaults just like they would if",
    "start": "4160480",
    "end": "4165838"
  },
  {
    "text": "you were working with a serve.deployment decorator",
    "start": "4165839",
    "end": "4170080"
  },
  {
    "text": "so with this config file race serve offers the best of both worlds it can be used by developers and",
    "start": "4170960",
    "end": "4176798"
  },
  {
    "text": "operators alike and with that let's take a let's briefly",
    "start": "4176799",
    "end": "4181838"
  },
  {
    "text": "take a look at how your server workloads might be different when you're developing your serve application versus when you're operating it",
    "start": "4181839",
    "end": "4188400"
  },
  {
    "text": "so during deploy during development you'll probably make a lot of quick updates to your served uh deployments",
    "start": "4188400",
    "end": "4194080"
  },
  {
    "text": "because you're trying out lots of different configurations like the init args in order to test out how they affect",
    "start": "4194080",
    "end": "4200000"
  },
  {
    "text": "your deployments but at this point you're probably using very few replicas because your goal is to test for the correctness of your",
    "start": "4200000",
    "end": "4205920"
  },
  {
    "text": "application rather than in scalability most of your work is going to be done locally using python as you iterate on",
    "start": "4205920",
    "end": "4211679"
  },
  {
    "text": "your application code on the other hand in production you'll need to operate and manage your deployments you'll probably have",
    "start": "4211679",
    "end": "4218480"
  },
  {
    "text": "consistent update cycles and these updates can be used to change model weights or even the application code",
    "start": "4218480",
    "end": "4224560"
  },
  {
    "text": "at this point there's going to be a lot of replicas because you need to serve traffic at scale so most of your work at",
    "start": "4224560",
    "end": "4230080"
  },
  {
    "text": "this point will be using yaml for your config files as well as the new server cli that lets you actually deploy these",
    "start": "4230080",
    "end": "4235120"
  },
  {
    "text": "config files so let's dive a little deeper into some of these examples to see how serve",
    "start": "4235120",
    "end": "4240640"
  },
  {
    "text": "offers the best of both worlds and provides a smooth experience in both development and in production",
    "start": "4240640",
    "end": "4247280"
  },
  {
    "text": "let's begin with updates so during development serv users usually deploy and iterate their server deployments",
    "start": "4248239",
    "end": "4254400"
  },
  {
    "text": "locally so in this example we have a user working with the python interpreter they've started a serve application and",
    "start": "4254400",
    "end": "4260960"
  },
  {
    "text": "they've defined a class called hello and they've deployed it with the init arg world",
    "start": "4260960",
    "end": "4266000"
  },
  {
    "text": "then it looks like they've done some additional work in the interpreter at the very end they redeploy the hello deployment that's running locally and",
    "start": "4266000",
    "end": "4273280"
  },
  {
    "text": "they update it with a new init arg in this case that in argus new world and now they can test it by making http",
    "start": "4273280",
    "end": "4280000"
  },
  {
    "text": "requests to this updated hello endpoint",
    "start": "4280000",
    "end": "4284159"
  },
  {
    "text": "now in production they would instead usually update a yaml config file and then redeploy it to a raycluster so for",
    "start": "4285199",
    "end": "4292400"
  },
  {
    "text": "instance in this example they update their they update their deployments num replicas to five",
    "start": "4292400",
    "end": "4298800"
  },
  {
    "text": "then they can deploy it using the new cli command serv deploy config.yml this command reads the config file and then",
    "start": "4298800",
    "end": "4305199"
  },
  {
    "text": "it sends a request to the user's ray cluster asking it to update its serv deployments to match the configuration",
    "start": "4305199",
    "end": "4310880"
  },
  {
    "text": "in the file the ops team can also use a version control system like github to track this config file if they detect anything",
    "start": "4310880",
    "end": "4317440"
  },
  {
    "text": "unexpected after the update they can take a previous version of the file and apply it to their raycluster using serve deploy this allows efficient and",
    "start": "4317440",
    "end": "4324080"
  },
  {
    "text": "convenient rollback after updates another useful operation is getting live",
    "start": "4324080",
    "end": "4330159"
  },
  {
    "text": "info on your live up on your live deployments during development you can use the serve.list deployments function to get a",
    "start": "4330159",
    "end": "4337120"
  },
  {
    "text": "list of deployment objects containing the configurations for the deployments these objects are useful when you're",
    "start": "4337120",
    "end": "4342480"
  },
  {
    "text": "reviewing your configurations and even when you're updating your deployments now in production you can use the new",
    "start": "4342480",
    "end": "4349199"
  },
  {
    "text": "serve config cli command which will connect to your ray cluster and print the yaml configuration for that cluster",
    "start": "4349199",
    "end": "4355760"
  },
  {
    "text": "you can monitor your deployments live config using this command additionally you can copy this yaml",
    "start": "4355760",
    "end": "4361600"
  },
  {
    "text": "into a separate yaml file and then edit that configuration and then you can update your deployments using that file",
    "start": "4361600",
    "end": "4370120"
  },
  {
    "text": "another useful tool for you is the serv status cli command this command connects to your ray cluster and then prints the",
    "start": "4371280",
    "end": "4376960"
  },
  {
    "text": "status info for all your running serve deployments you can use it to monitor your deployment's health and update",
    "start": "4376960",
    "end": "4382080"
  },
  {
    "text": "statuses especially during production you can use the included message to",
    "start": "4382080",
    "end": "4387360"
  },
  {
    "text": "debug unhealthy deployments and try to understand what's going on",
    "start": "4387360",
    "end": "4393040"
  },
  {
    "text": "so this cli is also really helpful when you're working with the deployment the deployment graphs that java talked about",
    "start": "4395120",
    "end": "4401199"
  },
  {
    "text": "when you're developing your deployment graph you can use the new serv run cli command along with your graph's python",
    "start": "4401199",
    "end": "4406800"
  },
  {
    "text": "import path to run your entire graph locally this command will deploy all the",
    "start": "4406800",
    "end": "4411920"
  },
  {
    "text": "deployments in your graph and then it'll block and print all of their statuses while this is happening you can open a",
    "start": "4411920",
    "end": "4417360"
  },
  {
    "text": "separate terminal window and make http request to your graph to test out your application",
    "start": "4417360",
    "end": "4423440"
  },
  {
    "text": "another helpful cli command is servbuild this command takes an import path to your graph and then it automatically",
    "start": "4425199",
    "end": "4430640"
  },
  {
    "text": "generates a structured config file for all the deployments in the graph this is especially useful when you're",
    "start": "4430640",
    "end": "4436320"
  },
  {
    "text": "defining complicated graphs that use for loops or conditionals in your python code this command will automatically",
    "start": "4436320",
    "end": "4442239"
  },
  {
    "text": "untangle that syntax and give you a yaml file that can be piped in and then give you a yellow file that can be deployed",
    "start": "4442239",
    "end": "4447600"
  },
  {
    "text": "to your ray cluster that file can then be deployed to your ray cluster using serve deploy",
    "start": "4447600",
    "end": "4454600"
  },
  {
    "text": "so everything i've talked about so far is already in progress in the future we also plan to use the",
    "start": "4458239",
    "end": "4464080"
  },
  {
    "text": "operational improvements that we've described in this uh in this talk as the basis for improved kubernetes support a",
    "start": "4464080",
    "end": "4470480"
  },
  {
    "text": "lot of our users run serv on kubernetes and we'd like to make it as easy as possible to deploy update and monitor",
    "start": "4470480",
    "end": "4476080"
  },
  {
    "text": "reserve in production one of our main goals is to improve automated workflows like continuous",
    "start": "4476080",
    "end": "4481199"
  },
  {
    "text": "deployments and continual learning which many of your users have been asking for",
    "start": "4481199",
    "end": "4487280"
  },
  {
    "text": "in the future we also plan to add more integrations with third-party ml ops tools rayserv is a scalable compute layer and",
    "start": "4488640",
    "end": "4495520"
  },
  {
    "text": "it's really useful for model serving but we also don't want to reinvent the wheel we're planning to provide useful",
    "start": "4495520",
    "end": "4500560"
  },
  {
    "text": "features like model monitoring and drift detection or experiment tracking and model management by adding integrations",
    "start": "4500560",
    "end": "4506400"
  },
  {
    "text": "into racer with some excellent ml ops tooling",
    "start": "4506400",
    "end": "4511840"
  },
  {
    "text": "and that brings us to the end of this part of the presentation but please stick around we're going to be doing a live demo of all the new features that",
    "start": "4513280",
    "end": "4519520"
  },
  {
    "text": "we talked about today after some q a before that i want to take a moment to ask you all to please get in touch with",
    "start": "4519520",
    "end": "4524719"
  },
  {
    "text": "us as we've been talking about throughout this presentation be sure to join the community at our discuss forums at discuss.ray.io and you",
    "start": "4524719",
    "end": "4531840"
  },
  {
    "text": "can also provide feedback and engage with ray developers at our github or these twitter pages",
    "start": "4531840",
    "end": "4537280"
  },
  {
    "text": "please also fill out our survey using this qr code to give feedback on the future shape of racer and we really look",
    "start": "4537280",
    "end": "4542400"
  },
  {
    "text": "forward to engaging with you more and keeping you updated at the upcoming race summit and over the coming months with",
    "start": "4542400",
    "end": "4547920"
  },
  {
    "text": "that let's open it up for q a thank you sure as wonderful to actually share the the friendly apis um",
    "start": "4547920",
    "end": "4555520"
  },
  {
    "text": "one question that actually popped in my mind as well as in nick hill's mind at the same time we",
    "start": "4555520",
    "end": "4561040"
  },
  {
    "text": "must be communicating telepathically but uh is it possible to do non-disruptive",
    "start": "4561040",
    "end": "4567679"
  },
  {
    "text": "updates uh of the rear cluster itself that is that when you create your structured",
    "start": "4567679",
    "end": "4573080"
  },
  {
    "text": "structure.config.yml and now you go and say go ahead and do a eraser for that particular tag but the dag is actually",
    "start": "4573080",
    "end": "4578880"
  },
  {
    "text": "running in production and you're trying to update it in live what happens what happens in the background do you",
    "start": "4578880",
    "end": "4583920"
  },
  {
    "text": "actually wait for no traffic to stop or you put it on the staging what are some of the the best practices that you",
    "start": "4583920",
    "end": "4589520"
  },
  {
    "text": "encounter or you envision in using this particular methodology to update live update uh",
    "start": "4589520",
    "end": "4595600"
  },
  {
    "text": "live live uh deployments yeah that's a that's a great question so",
    "start": "4595600",
    "end": "4601440"
  },
  {
    "text": "we envision a couple of different paths depending on how heavy these updates are um so as edward and joe mentioned",
    "start": "4601440",
    "end": "4607840"
  },
  {
    "text": "one new like pattern that we're noticing for machine learning is this idea of continual learning where you have a",
    "start": "4607840",
    "end": "4613280"
  },
  {
    "text": "model that's continually getting updated and that needs to be deployed for that kind of an update we envision",
    "start": "4613280",
    "end": "4618480"
  },
  {
    "text": "like a lighter weight path where you don't have to like tear down all of your deployments and then redeploy everything uh instead you can do like a lightweight",
    "start": "4618480",
    "end": "4624719"
  },
  {
    "text": "change where you change the path to the model and you're using like new model weights when you're running inference um",
    "start": "4624719",
    "end": "4630320"
  },
  {
    "text": "now on the other hand if you have like a heavy weight change meaning that it's very backwards incompatible uh it could",
    "start": "4630320",
    "end": "4636640"
  },
  {
    "text": "break things inside of the cluster then our recommended approach at the moment is to spin up like a new ray cluster uh",
    "start": "4636640",
    "end": "4642880"
  },
  {
    "text": "apply your serve deployments on that cluster which should be a lot simpler now because you have that structured config file that you can apply directly",
    "start": "4642880",
    "end": "4649520"
  },
  {
    "text": "and then switch traffic over to that cluster now uh one other thing to keep in mind is that",
    "start": "4649520",
    "end": "4655040"
  },
  {
    "text": "race serve already supports like this idea of rolling updates so already when you update your deployments you actually",
    "start": "4655040",
    "end": "4661520"
  },
  {
    "text": "don't tear down all your deployments and then start deploying new ones uh there's instead like this rolling update process",
    "start": "4661520",
    "end": "4667360"
  },
  {
    "text": "where you tear down a couple of the replicas running you deploy version you deploy a few replicas of the new version",
    "start": "4667360",
    "end": "4673040"
  },
  {
    "text": "and then you tear down the rest of the old version and deploy the rest of the new version so that's still in place",
    "start": "4673040",
    "end": "4678800"
  },
  {
    "text": "with uh this structured config file and this new ml this new ml ops workflow um",
    "start": "4678800",
    "end": "4684960"
  },
  {
    "text": "so you still have the availability provided by that rolling deployment brilliant brilliant i think that answers",
    "start": "4684960",
    "end": "4690000"
  },
  {
    "text": "the question quite brilliantly um another question that you you put in in your conclusion slide the magic word",
    "start": "4690000",
    "end": "4696640"
  },
  {
    "text": "kubernetes and when people think about kubernetes the first thing that pops into people's mind is that",
    "start": "4696640",
    "end": "4702320"
  },
  {
    "text": "is that how do you how does the new can you deep uh dive a little closely into",
    "start": "4702320",
    "end": "4708000"
  },
  {
    "text": "how does the structured config file uh interact with the kubernetes deployment",
    "start": "4708000",
    "end": "4713360"
  },
  {
    "text": "yeah that's a great question so the structured config file is really nice because like it takes all these",
    "start": "4713360",
    "end": "4718800"
  },
  {
    "text": "configurations that traditionally lived in like that serve.deployment decorator and then it pulls them out and puts them",
    "start": "4718800",
    "end": "4724400"
  },
  {
    "text": "inside of like a yaml format uh where all the all the configurations are centralized so now that we have that",
    "start": "4724400",
    "end": "4730480"
  },
  {
    "text": "file we're trying to think of ways or we're looking into ways that we can maybe define uh config files that can be",
    "start": "4730480",
    "end": "4735920"
  },
  {
    "text": "applied to kubernetes that might embed this information inside of them um so now that we have like this native this",
    "start": "4735920",
    "end": "4742480"
  },
  {
    "text": "native yaml format for serve deployments um we're looking into ways where we can also add like native kubernetes support",
    "start": "4742480",
    "end": "4749040"
  },
  {
    "text": "by integrating that format brilliant i think that actually answers all the questions we actually have uh uh",
    "start": "4749040",
    "end": "4756159"
  },
  {
    "text": "another half an hour left and i think this is actually a good time to segue into the end-to-end demo that everybody",
    "start": "4756159",
    "end": "4762880"
  },
  {
    "text": "is actually waiting for that somehow ties up everything that we talked about from the beginning what the future looks",
    "start": "4762880",
    "end": "4768400"
  },
  {
    "text": "like uh the the the graph apis and finally how we actually operationalize those things so i'm going to hand it",
    "start": "4768400",
    "end": "4774640"
  },
  {
    "text": "over now to xiao who is going to give us a demo and then we'll finally open it up",
    "start": "4774640",
    "end": "4779679"
  },
  {
    "text": "for q a for the entire team and i have my esteemed colleague old friend of mine paige who has joined us",
    "start": "4779679",
    "end": "4785840"
  },
  {
    "text": "so i'll have a couple of questions for her as well thanks a lot paige for joining us so uh go ahead shout out",
    "start": "4785840",
    "end": "4792000"
  },
  {
    "text": "and um and and go ahead and take over and and give us a demo",
    "start": "4792000",
    "end": "4797679"
  },
  {
    "text": "okay all right thank you chris for the talk and just for the introduction and",
    "start": "4797679",
    "end": "4803360"
  },
  {
    "text": "okay let me double check on one thing because in this case i need to swap between",
    "start": "4803360",
    "end": "4808639"
  },
  {
    "text": "slides and code so can i actually see my screen without yes we can actually do the",
    "start": "4808639",
    "end": "4814480"
  },
  {
    "text": "switch over yeah we loved it okay now you can see the slots too",
    "start": "4814480",
    "end": "4819920"
  },
  {
    "text": "and the size and everything look okay like yeah the magic works okay cool all right that's good to know",
    "start": "4819920",
    "end": "4827120"
  },
  {
    "text": "all right now we have an end-to-end demo for you to cover all the concepts and designs we walk through",
    "start": "4827120",
    "end": "4833440"
  },
  {
    "text": "and i will walk through the details of this particular demo and see what are we doing in the very high level",
    "start": "4833440",
    "end": "4840000"
  },
  {
    "text": "so uh we start taking your input we are building uh essentially image classification on content understanding",
    "start": "4840000",
    "end": "4845840"
  },
  {
    "text": "pipeline and there's a few ingredients in there that shows why graph building api is powerful and how operationalizing",
    "start": "4845840",
    "end": "4852080"
  },
  {
    "text": "api can help to save the data to actually deploy it so the user input is image url and this",
    "start": "4852080",
    "end": "4858159"
  },
  {
    "text": "case i just take a random let's say for example imagenet image of just a hummingbird just trying to get water",
    "start": "4858159",
    "end": "4863440"
  },
  {
    "text": "with food and also i pass in the user id that represents represents my user site information",
    "start": "4863440",
    "end": "4869520"
  },
  {
    "text": "and i then i will walk through this complex inference graph at the very end",
    "start": "4869520",
    "end": "4875199"
  },
  {
    "text": "i want to invoke multiple image models for classification for example",
    "start": "4875199",
    "end": "4881679"
  },
  {
    "text": "uh i want to have a top categorization of this particular image and say",
    "start": "4881679",
    "end": "4887040"
  },
  {
    "text": "we have a very high confidence level knowing this is a hummingbird and also record which particular version",
    "start": "4887040",
    "end": "4893360"
  },
  {
    "text": "of the model with these patterns which we can walk through more detail in the next slide and this is essentially the typical",
    "start": "4893360",
    "end": "4899280"
  },
  {
    "text": "resnet or image classification will fine-tune a product classification model",
    "start": "4899280",
    "end": "4905840"
  },
  {
    "text": "and the next one we're going to walk we're going to cut into is the image segmentation and you can see this image",
    "start": "4905840",
    "end": "4910880"
  },
  {
    "text": "is slightly uh cropped and slightly reshaped and transform a little bit but essentially",
    "start": "4910880",
    "end": "4916960"
  },
  {
    "text": "then we are calling in calling another state of our computer vision model called mask rcn",
    "start": "4916960",
    "end": "4922000"
  },
  {
    "text": "and it will generate a mask that capture the contour and as well as the detected image a detected",
    "start": "4922000",
    "end": "4929199"
  },
  {
    "text": "objects with a responding color into this image it's very common for like autonomous",
    "start": "4929199",
    "end": "4934560"
  },
  {
    "text": "vehicle other computer vision models that we try to detect like people walking or trees or buses",
    "start": "4934560",
    "end": "4941120"
  },
  {
    "text": "and third model we're going to call it into is the image caption and this image capture model is a combination of",
    "start": "4941120",
    "end": "4947600"
  },
  {
    "text": "uh computer vision and natural language processing because it's essentially the model architecture with resnet plus lstm",
    "start": "4947600",
    "end": "4954560"
  },
  {
    "text": "combined and what it's going to do is it will try to look at this image and it will try to tell us what this image is about and for",
    "start": "4954560",
    "end": "4961199"
  },
  {
    "text": "this particular image the output from the pre-tree model we got is a bird sitting on the table with a frisbee i",
    "start": "4961199",
    "end": "4968239"
  },
  {
    "text": "wouldn't call the phrase be very accurate but it's close enough this plastic is rounded so that's the best output the model can give us",
    "start": "4968239",
    "end": "4975440"
  },
  {
    "text": "cool and let's dive in a little bit more into details i'm going to walk through all the whole thing looks end to end then we",
    "start": "4975440",
    "end": "4982239"
  },
  {
    "text": "can incrementally build it step by step using the apis we do use early so",
    "start": "4982239",
    "end": "4987600"
  },
  {
    "text": "and we're going to start with your input here uh just the image the url user id and we're going to pass through",
    "start": "4987600",
    "end": "4993760"
  },
  {
    "text": "downloader and the download are going to download the image for you and then you will pre-process it by",
    "start": "4993760",
    "end": "4999280"
  },
  {
    "text": "pre-processing means we're going to transform and reshape it to 224 by 224 which is center size for image net",
    "start": "4999280",
    "end": "5005600"
  },
  {
    "text": "recognition and it will also normalize again the rgb channels",
    "start": "5005600",
    "end": "5011280"
  },
  {
    "text": "and then it will dispatch the two models in uh two particular models in parallel so one of them is about dynamic dispatch",
    "start": "5011280",
    "end": "5018880"
  },
  {
    "text": "and then here we have three instances of the image classifier all running under the same",
    "start": "5018880",
    "end": "5025840"
  },
  {
    "text": "node and what that means is we are assuming that we try to mimic one use case that we have a lot of classifier in place but",
    "start": "5025840",
    "end": "5032400"
  },
  {
    "text": "based on the particular image or input we got we only want to dynamically dispatch to a particular subset of the",
    "start": "5032400",
    "end": "5037600"
  },
  {
    "text": "classification models and at the meantime for the same prefrontal image we want to send it to",
    "start": "5037600",
    "end": "5042880"
  },
  {
    "text": "the image segmentation model which is preview what i showed as a mask rcn",
    "start": "5042880",
    "end": "5047920"
  },
  {
    "text": "and then i'm going to assign the outputs of those the futurization as well as the last fc",
    "start": "5047920",
    "end": "5054400"
  },
  {
    "text": "layer output into the image caption we're going to run the rest side plus the lstm",
    "start": "5054400",
    "end": "5059840"
  },
  {
    "text": "and we will generate the final caption description of this image and this entire pipeline at the very end",
    "start": "5059840",
    "end": "5066639"
  },
  {
    "text": "we're going to render the output and combining all these pieces of information",
    "start": "5066639",
    "end": "5071840"
  },
  {
    "text": "so now you can see this captures a few attributes of the the deployment graph api we walked",
    "start": "5071840",
    "end": "5077679"
  },
  {
    "text": "through earlier we have a user input and we have separated responsibility for downloading image and pre-processing the",
    "start": "5077679",
    "end": "5083920"
  },
  {
    "text": "image we have parallel calls for the same image being dispatched to two distinct",
    "start": "5083920",
    "end": "5089520"
  },
  {
    "text": "models we have dynamic dispatch that actually make decision with our business logic or",
    "start": "5089520",
    "end": "5095360"
  },
  {
    "text": "routing logic with respect to user input feature now we have ensembling that those two",
    "start": "5095360",
    "end": "5100960"
  },
  {
    "text": "inputs actually come to the same capturing model as well as the final output",
    "start": "5100960",
    "end": "5106000"
  },
  {
    "text": "okay so that's the entire graph and let's start building it from the very beginning so let's start with the first",
    "start": "5106000",
    "end": "5112560"
  },
  {
    "text": "part and i just got my user input and just want to download image so in here i kind of put out some uh",
    "start": "5112560",
    "end": "5119920"
  },
  {
    "text": "some pre like provided inputs already just to make it simpler to walk through and",
    "start": "5119920",
    "end": "5125600"
  },
  {
    "text": "i have i created a data class called content input you just take two fields image url and user id it's just a plain",
    "start": "5125600",
    "end": "5131360"
  },
  {
    "text": "python data class right now so the very first thing i can do here is i can use the contact manager",
    "start": "5131360",
    "end": "5137040"
  },
  {
    "text": "say this is my user input and i have a download implementation provided up there already uh we have a",
    "start": "5137040",
    "end": "5143040"
  },
  {
    "text": "github link to this entire demo if you want to play around with later on but for simplicity of a demo we're not going",
    "start": "5143040",
    "end": "5148800"
  },
  {
    "text": "to dig into the details of each class or function you can assume the interface is correct okay so now we have a downloader",
    "start": "5148800",
    "end": "5155360"
  },
  {
    "text": "that take our user input and at the output it will be the byte of this image now i want to see if i'm getting this as",
    "start": "5155360",
    "end": "5162159"
  },
  {
    "text": "correct as i expected and i'm gonna just create essentially the first download as the",
    "start": "5162159",
    "end": "5168159"
  },
  {
    "text": "only node i have in my deck and i just want this one to run it standalone",
    "start": "5168159",
    "end": "5173360"
  },
  {
    "text": "so i'm going to do just stack f2 and provide this user input that captures the url that represents the hummingbird",
    "start": "5173360",
    "end": "5178960"
  },
  {
    "text": "and let me just print the outputs here and see what we got",
    "start": "5178960",
    "end": "5185119"
  },
  {
    "text": "and you can see we started running deck locally we started kick off our local recluster and there's a bunch of",
    "start": "5190080",
    "end": "5195840"
  },
  {
    "text": "non-human readable stuff which is expected because those are just bytes of the image okay now we're done with the",
    "start": "5195840",
    "end": "5200960"
  },
  {
    "text": "first part we",
    "start": "5200960",
    "end": "5203920"
  },
  {
    "text": "order to make this download and this is not my root note of the deck anymore i just swapped it back and say well i know",
    "start": "5208960",
    "end": "5214480"
  },
  {
    "text": "how to send the image bytes to the image to the preprocessor",
    "start": "5214480",
    "end": "5221199"
  },
  {
    "text": "and given that pre-processing here is a class so i need to actually instantiate and create the upstream",
    "start": "5221199",
    "end": "5227840"
  },
  {
    "text": "and i can just say i define an instantiated instance of preprocessor and the professors take no",
    "start": "5227840",
    "end": "5234320"
  },
  {
    "text": "argument in the constructor so i just created and use it has a method called preprocess and what happens here is i",
    "start": "5234320",
    "end": "5240880"
  },
  {
    "text": "can just send the image by that downloaded to the preprocessor i'm calling the cross method preprocess on",
    "start": "5240880",
    "end": "5246000"
  },
  {
    "text": "it and as a result for this particular function i put this image to the rear object",
    "start": "5246000",
    "end": "5251920"
  },
  {
    "text": "store and you want to i want to put it here because i need to fend out the same image support multiple models and i do",
    "start": "5251920",
    "end": "5258719"
  },
  {
    "text": "not want to make a redundant copy or invoke additional civilization when i don't need to",
    "start": "5258719",
    "end": "5263760"
  },
  {
    "text": "so the ideal outcome of this is when i run this deck i should just get an object wrapped to this",
    "start": "5263760",
    "end": "5271040"
  },
  {
    "text": "particular image so let's run it again and this is my local iteration",
    "start": "5271040",
    "end": "5277280"
  },
  {
    "text": "and you can see here here's the object rest of this particular image now let's revert the back set image",
    "start": "5279040",
    "end": "5285440"
  },
  {
    "text": "sensor and then we can proceed with the next step we can start creating the dynamics at first",
    "start": "5285440",
    "end": "5290719"
  },
  {
    "text": "so you can see this is some parent-child relationship here and if you recall the example i walked through earlier we have",
    "start": "5290719",
    "end": "5296880"
  },
  {
    "text": "a combiner that takes two models and this is very similar we have a dynamic dispatch as a upper parent class that",
    "start": "5296880",
    "end": "5302880"
  },
  {
    "text": "holds references to three instances of the image classifier and you will do some simple dynamic routing and you will",
    "start": "5302880",
    "end": "5309520"
  },
  {
    "text": "give us an output about which version of the classifier used so",
    "start": "5309520",
    "end": "5314560"
  },
  {
    "text": "and we can create the classifiers here and just say image classification retina",
    "start": "5314560",
    "end": "5320800"
  },
  {
    "text": "and i just created this in the problem for a little list and i find it essentially i initialize it with a different version because",
    "start": "5320800",
    "end": "5327679"
  },
  {
    "text": "and then i create my dispatcher and dispatch i only take one argument just a list of the classifiers i want to use",
    "start": "5327679",
    "end": "5332960"
  },
  {
    "text": "and here's my dispatcher now and like we can do a very quick",
    "start": "5332960",
    "end": "5339040"
  },
  {
    "text": "check on the classification output and to see if we are getting what we want to see now uh the dispatch essentially has",
    "start": "5339040",
    "end": "5345760"
  },
  {
    "text": "a forward implementation of this class implementation and it take the image as well as the user input",
    "start": "5345760",
    "end": "5351920"
  },
  {
    "text": "we need user input here because remember in here i said my user id is essentially the key factor to determine",
    "start": "5351920",
    "end": "5358719"
  },
  {
    "text": "which uh one of the image classifier id pass dispatch my requesting and i think i take a simple module like",
    "start": "5358719",
    "end": "5365199"
  },
  {
    "text": "say like just mock three or something and it will pick one we don't know which one it is depending on the user input",
    "start": "5365199",
    "end": "5371360"
  },
  {
    "text": "so now let's run this again and see what we got",
    "start": "5371360",
    "end": "5375760"
  },
  {
    "text": "started running deck locally and",
    "start": "5377520",
    "end": "5382080"
  },
  {
    "text": "cool now you can see we printed some 3d printed log from revnet which is our image classification model and",
    "start": "5385679",
    "end": "5392639"
  },
  {
    "text": "some printout from pytorch we use the python pre-trained model from search vision and if you think the classification",
    "start": "5392639",
    "end": "5398639"
  },
  {
    "text": "results of this is high probability very high probability hummingbird could be a bucket",
    "start": "5398639",
    "end": "5404719"
  },
  {
    "text": "maybe a crossover or maybe pencil sharpener it's very very low profit mode and the module version that is patched",
    "start": "5404719",
    "end": "5410639"
  },
  {
    "text": "to corresponds to one uh which is essentially this guy actually the index was half by one but based on the user id",
    "start": "5410639",
    "end": "5416880"
  },
  {
    "text": "we decided throughout the request difficult and now let's change this back to the",
    "start": "5416880",
    "end": "5422639"
  },
  {
    "text": "classification output and let's start adding the segmentation part of it and in order to use the segmentation",
    "start": "5422639",
    "end": "5428080"
  },
  {
    "text": "model and i just also need to instantiate it and this will be a mask rcn model to just add the mask and",
    "start": "5428080",
    "end": "5433679"
  },
  {
    "text": "layering of the image now for simplicity i'm just not going to show it here and just let's start to",
    "start": "5433679",
    "end": "5440639"
  },
  {
    "text": "just dive into the image captioning and we're going to have the output of this render at the very end",
    "start": "5440639",
    "end": "5446639"
  },
  {
    "text": "so and now we have two outputs you can come from calling forward class method that",
    "start": "5446639",
    "end": "5451920"
  },
  {
    "text": "you can also change to a different name if you want to as long as the signature and the input is correct",
    "start": "5451920",
    "end": "5456960"
  },
  {
    "text": "and let's pass it to the question model with that and the capture model is a combination of resnet plus as a lstm and it takes no",
    "start": "5456960",
    "end": "5464159"
  },
  {
    "text": "init argument but at the runtime you take the correct input which in this case",
    "start": "5464159",
    "end": "5469520"
  },
  {
    "text": "our segmentation output will be fit into the forward function of captioning as well as the classification of the",
    "start": "5469520",
    "end": "5475600"
  },
  {
    "text": "caption and how about let's make this our the",
    "start": "5475600",
    "end": "5480719"
  },
  {
    "text": "active point of our deck and let's run it and see what we got supposedly we should get a description",
    "start": "5480719",
    "end": "5487040"
  },
  {
    "text": "of this image based on the inputs we provide to",
    "start": "5487040",
    "end": "5491840"
  },
  {
    "text": "all right so you can see the output we got here is uh lcm is like a sequence model so you",
    "start": "5498960",
    "end": "5504320"
  },
  {
    "text": "always have a token for start and end also have a just like the end of sentence uh it's describing the image as a bird",
    "start": "5504320",
    "end": "5510800"
  },
  {
    "text": "sitting on the table where's the free speech and this is what this model think this picture is about which i would call",
    "start": "5510800",
    "end": "5516639"
  },
  {
    "text": "mostly correct and it's very difficult to recognize what this thing is okay now let's put it together end to",
    "start": "5516639",
    "end": "5523520"
  },
  {
    "text": "end and we have the caption output and let's start combining and actually render this thing in the future notebook",
    "start": "5523520",
    "end": "5529600"
  },
  {
    "text": "we have a function called combined and",
    "start": "5529600",
    "end": "5533840"
  },
  {
    "text": "now let's simply put the deck here and this will be new active point that i can try to get the result from which is just",
    "start": "5539280",
    "end": "5545120"
  },
  {
    "text": "aggravating everything and i implemented some visualization helper to show us the segmentation",
    "start": "5545120",
    "end": "5551280"
  },
  {
    "text": "results and the xcode has a pretty nice built-in functionality to just run this thing as",
    "start": "5551280",
    "end": "5557120"
  },
  {
    "text": "a cell so we can run it and see what we got",
    "start": "5557120",
    "end": "5562239"
  },
  {
    "text": "start running back locally",
    "start": "5565120",
    "end": "5568400"
  },
  {
    "text": "and this is going to go through mask rc and this is a pretty expensive model by the way it takes a while to run",
    "start": "5573199",
    "end": "5578639"
  },
  {
    "text": "and here's the final output now you can see we aggregated outputs from all those models with the some",
    "start": "5578639",
    "end": "5584719"
  },
  {
    "text": "dynamic dispatch behavior with respect to user input and the captioning thing this is a birth certain table with a",
    "start": "5584719",
    "end": "5590560"
  },
  {
    "text": "frisbee and it went to a classifier version one that we dispatched to the classifier and the top results of this",
    "start": "5590560",
    "end": "5596719"
  },
  {
    "text": "particular image is we have very high confidence i think this is something and mask rcnn segmentation model is",
    "start": "5596719",
    "end": "5602480"
  },
  {
    "text": "telling us this is what i think the counter of this image should be now you can see this is actually got normalized",
    "start": "5602480",
    "end": "5607840"
  },
  {
    "text": "a little bit into a square and because imagenet models were better this way and we transform in that way",
    "start": "5607840",
    "end": "5613040"
  },
  {
    "text": "cool all right now we have walked through everything up to this point for local",
    "start": "5613040",
    "end": "5618080"
  },
  {
    "text": "development uh now we can start from local laptop to production deployment this is the",
    "start": "5618080",
    "end": "5624719"
  },
  {
    "text": "part that she has went through with all the details we want about a generating demo now",
    "start": "5624719",
    "end": "5630960"
  },
  {
    "text": "essentially we have a little circle here just to run the iterate and i show you what could happen here you can see it's",
    "start": "5630960",
    "end": "5636080"
  },
  {
    "text": "very easy and quick iteration to get the results you want right now and we need to add",
    "start": "5636080",
    "end": "5643360"
  },
  {
    "text": "some flavor of serving to it because now we want to expose this entire deck",
    "start": "5643360",
    "end": "5648560"
  },
  {
    "text": "as an http to entity endpoint and now this is where we bring the driver into the picture",
    "start": "5648560",
    "end": "5655920"
  },
  {
    "text": "now essentially it's this line so a text driver would provide a special implementation or default one for you",
    "start": "5657360",
    "end": "5662800"
  },
  {
    "text": "and if you just want convenience but you also you are free to improve your own it's just a simple class deployment",
    "start": "5662800",
    "end": "5668719"
  },
  {
    "text": "if you want to and there are two things in here so we that driver essentially takes it back",
    "start": "5668719",
    "end": "5674800"
  },
  {
    "text": "and just say well i'm here to run this deck for you and i am here to configure http options",
    "start": "5674800",
    "end": "5681040"
  },
  {
    "text": "and also you can have a default input schema and you put schema here let's say i can bring my own we also have a few",
    "start": "5681040",
    "end": "5687520"
  },
  {
    "text": "default ones provided but if you put this particular case if you have a strong need to say well i just want to use this particular",
    "start": "5687520",
    "end": "5693920"
  },
  {
    "text": "input adapter you could do so and this is our reserve entry point",
    "start": "5693920",
    "end": "5700000"
  },
  {
    "text": "essentially you encapsulate and wrap the entire application and expose http to it now the next thing i'm going to do is i",
    "start": "5700000",
    "end": "5706880"
  },
  {
    "text": "will do the serv cli and serve.run and actually test this http and see what we got",
    "start": "5706880",
    "end": "5712800"
  },
  {
    "text": "so the ceiling here is uh",
    "start": "5712800",
    "end": "5717280"
  },
  {
    "text": "okay so essentially you want to fold the screen essentially just saying uh",
    "start": "5717920",
    "end": "5722960"
  },
  {
    "text": "run and it's the module path when our file is called become a graph so let's do it",
    "start": "5722960",
    "end": "5728400"
  },
  {
    "text": "that way and the particular dag with the deck driver is called serve",
    "start": "5728400",
    "end": "5733920"
  },
  {
    "text": "entry so let's run it",
    "start": "5733920",
    "end": "5739159"
  },
  {
    "text": "okay now here you can see a lot of logs printed on research because we actually try to deploy this particular deployment",
    "start": "5745679",
    "end": "5751600"
  },
  {
    "text": "graph to a local research cluster and you can see the cell answering deployed successfully we're adding a",
    "start": "5751600",
    "end": "5757199"
  },
  {
    "text": "replica each to image classification one two and three dynamic dispatch",
    "start": "5757199",
    "end": "5762320"
  },
  {
    "text": "segmentation and lstm now let's test it with the the rest api so by default the our default deck",
    "start": "5762320",
    "end": "5769040"
  },
  {
    "text": "driver actually provided the default fast api integration so which you can pretty conveniently use right here and",
    "start": "5769040",
    "end": "5776719"
  },
  {
    "text": "i need to copy this image url let's see let's say user ids1",
    "start": "5776719",
    "end": "5784400"
  },
  {
    "text": "and as i'm invoking it i can also show the read dashboard and say we have those replicas running in the re-cluster we",
    "start": "5784400",
    "end": "5791040"
  },
  {
    "text": "have a projector web downloader you can see uh like the mask rcan get really busy when i'm hitting this request because",
    "start": "5791040",
    "end": "5797280"
  },
  {
    "text": "this is a pretty expensive model to run internet and yeah at the end we can see the",
    "start": "5797280",
    "end": "5803119"
  },
  {
    "text": "output here with the json and just saying birds in our table is a free speed resonant version one and classification result",
    "start": "5803119",
    "end": "5810719"
  },
  {
    "text": "pretty much similar as what we have seen locally cool all right now my local serve cluster",
    "start": "5810719",
    "end": "5817600"
  },
  {
    "text": "your money is also done now i start want to deploy it and the next thing i'm going to do is serve that build and this is the part about",
    "start": "5817600",
    "end": "5824480"
  },
  {
    "text": "offering your dac in python iterate locally has http you can do all of that",
    "start": "5824480",
    "end": "5830159"
  },
  {
    "text": "then serve the build essentially the one that generates yamaha for you and",
    "start": "5830159",
    "end": "5835360"
  },
  {
    "text": "okay let me kill it here but also make sure i stop the requested",
    "start": "5835360",
    "end": "5841080"
  },
  {
    "text": "okay so let me to serve.build i can use the same syntax here deployment graph",
    "start": "5842639",
    "end": "5848880"
  },
  {
    "text": "and what is my variable here a serve entry point i want to save it to a yamaha product",
    "start": "5848880",
    "end": "5854960"
  },
  {
    "text": "say uh graph.young",
    "start": "5854960",
    "end": "5858679"
  },
  {
    "text": "okay i ran into an error because these we in the actual yaml generation we found json serializable is a pretty",
    "start": "5860320",
    "end": "5866960"
  },
  {
    "text": "valuable contract otherwise you're going to have your online created",
    "start": "5866960",
    "end": "5872400"
  },
  {
    "text": "class or function that we don't know where do not have full confidence to reconstruct completely so the fix for",
    "start": "5872400",
    "end": "5878159"
  },
  {
    "text": "this is also pretty simple essentially we require we want to user to use json serialized versions like the",
    "start": "5878159",
    "end": "5885119"
  },
  {
    "text": "primitives or import paths of the cluster function you want to use as long as they are defined and visible at top",
    "start": "5885119",
    "end": "5890639"
  },
  {
    "text": "the file so the fix for this is essentially instead of input adapter i just changed this to the top of my input",
    "start": "5890639",
    "end": "5897760"
  },
  {
    "text": "axis so this is defined in deployment graph dot input adapter",
    "start": "5897760",
    "end": "5903760"
  },
  {
    "text": "and i just do the same thing and this path now",
    "start": "5903760",
    "end": "5909760"
  },
  {
    "text": "i have generated yammo let's take a pic into what the file looks like",
    "start": "5909760",
    "end": "5915118"
  },
  {
    "text": "cool and you can see our dag looks like this right and [Music] it has a bunch of like binding that has",
    "start": "5916480",
    "end": "5923280"
  },
  {
    "text": "multiple instances with classifiers that dispatcher and you can see the jet pre the final generation file essentially",
    "start": "5923280",
    "end": "5929199"
  },
  {
    "text": "captures all the deployments as the backbone of each node in the deployment graph used",
    "start": "5929199",
    "end": "5934239"
  },
  {
    "text": "first for example the first one is we start with downloading with the import path of this downloader",
    "start": "5934239",
    "end": "5940639"
  },
  {
    "text": "from the code we use and load the iteration is deployment graphic downloader that means as long as the",
    "start": "5940639",
    "end": "5946000"
  },
  {
    "text": "runtime environment and the document is set up correctly we can use this import path to find the body of your downloader",
    "start": "5946000",
    "end": "5952560"
  },
  {
    "text": "class or function and by default you use one replica and by if i give it to 0.5 cpu",
    "start": "5952560",
    "end": "5959840"
  },
  {
    "text": "and similarly you can also see preprocessor essentially with one replica 0.5 cpu",
    "start": "5959840",
    "end": "5965520"
  },
  {
    "text": "and another point i want to show you here is about a classifier remember we use the same class but we",
    "start": "5965520",
    "end": "5971440"
  },
  {
    "text": "instantiate it multiple times so so like if we need to have an identifier",
    "start": "5971440",
    "end": "5978239"
  },
  {
    "text": "to distinguish those otherwise we're going to run to conflict we're going to think the name is not unique anymore now",
    "start": "5978239",
    "end": "5983360"
  },
  {
    "text": "you can see the first resonant model we generate for classification we have you need the arc of zero that corresponds to",
    "start": "5983360",
    "end": "5988480"
  },
  {
    "text": "the first value we got from the for loop and then we have automatically generated the",
    "start": "5988480",
    "end": "5994480"
  },
  {
    "text": "new unique name knowing this first one was taken to suffix underscore one and it was",
    "start": "5994480",
    "end": "6000080"
  },
  {
    "text": "initialized with number of value of one similarly we also have underscore two that has value of two",
    "start": "6000080",
    "end": "6006560"
  },
  {
    "text": "okay now this is a yama file and we want this essentially to be the ground true",
    "start": "6006560",
    "end": "6012560"
  },
  {
    "text": "of your serve deployment graph application as a one yama plot if you're off stream ops person this is only file",
    "start": "6012560",
    "end": "6018159"
  },
  {
    "text": "supposedly you need to know and you have all the knobs exposed to you they want you involved",
    "start": "6018159",
    "end": "6024159"
  },
  {
    "text": "so let's go to the next step about serve that deploy and before i do that i need to start my requested",
    "start": "6024159",
    "end": "6032679"
  },
  {
    "text": "and serve start",
    "start": "6037520",
    "end": "6041040"
  },
  {
    "text": "and serve dot play and my yamaha",
    "start": "6046239",
    "end": "6051840"
  },
  {
    "text": "cool and in the cli we'll be we're basically sending a request and say i want to make this yama file the branches",
    "start": "6053440",
    "end": "6061040"
  },
  {
    "text": "of my current research cluster and please fulfill it and the controller is busy in the loop and try to do the work",
    "start": "6061040",
    "end": "6066320"
  },
  {
    "text": "for us and you can see the replica status using cli those are healthy",
    "start": "6066320",
    "end": "6071360"
  },
  {
    "text": "let's see what is happening here on the read dashboard so we see that download let me refresh it again so you can see the downloaded",
    "start": "6071360",
    "end": "6078080"
  },
  {
    "text": "preprocessors are populated and let's shake the api again",
    "start": "6078080",
    "end": "6083119"
  },
  {
    "text": "so let's play around this user id a little bit because in the first one we the resume version is one because it is",
    "start": "6083119",
    "end": "6088719"
  },
  {
    "text": "user id smart so it can hit with the same image but with slightly different",
    "start": "6088719",
    "end": "6094080"
  },
  {
    "text": "user id so the expected outcome here is uh i supposedly got the same result because i'm still using the same input",
    "start": "6094080",
    "end": "6101040"
  },
  {
    "text": "image but the redstone version that it dispatches to essentially means i'd rather make requests to another",
    "start": "6101040",
    "end": "6107360"
  },
  {
    "text": "classifier model and for simplicity i use the same model definition so they're the same results",
    "start": "6107360",
    "end": "6113360"
  },
  {
    "text": "cool so now we have to serve the deploy now this is one common problem that we also",
    "start": "6113360",
    "end": "6119280"
  },
  {
    "text": "tested for http now the other thing is we have an issue",
    "start": "6119280",
    "end": "6124560"
  },
  {
    "text": "which could be very common if you have a complex deployment graph end to end you might find one of those nodes just",
    "start": "6124560",
    "end": "6130239"
  },
  {
    "text": "running really slow like you put a bottleneck of your you want to optimize your throughput but",
    "start": "6130239",
    "end": "6135760"
  },
  {
    "text": "you have one stricture just keep blocking that pipeline that's not ideal so i'm here i can show you with this",
    "start": "6135760",
    "end": "6141280"
  },
  {
    "text": "elmo file how simple it is to just scale and change the number of replicas or upscale",
    "start": "6141280",
    "end": "6146719"
  },
  {
    "text": "or downscale a particular node in the deployment so they're not a bottleneck anymore and in this downloader i simply change",
    "start": "6146719",
    "end": "6153280"
  },
  {
    "text": "my number references to two and let's say preprocessor i change it to two and before i do that let's say let's",
    "start": "6153280",
    "end": "6160000"
  },
  {
    "text": "check the dashboard again so i have only one downloaded instance in my requester",
    "start": "6160000",
    "end": "6165360"
  },
  {
    "text": "why is this showing up okay and i have another i have one",
    "start": "6165360",
    "end": "6170480"
  },
  {
    "text": "preprocessor as well okay but now i change it to two let me try to redefine",
    "start": "6170480",
    "end": "6176560"
  },
  {
    "text": "and and given the api itself is meant to be end important uh essentially i don't care what the previous example",
    "start": "6176560",
    "end": "6182560"
  },
  {
    "text": "file looked like but i modified a little bit of it and i just redeployed a yama file this is new gram shoes and let's",
    "start": "6182560",
    "end": "6188880"
  },
  {
    "text": "fulfill it and you can see let's keep an eye on the",
    "start": "6188880",
    "end": "6194400"
  },
  {
    "text": "dashboard and this dashboard just gonna run and it's gonna say oh okay i need to make some updates",
    "start": "6194400",
    "end": "6200320"
  },
  {
    "text": "and let's see what up this should be now you can see is we have all the",
    "start": "6200320",
    "end": "6205679"
  },
  {
    "text": "resident model populated up again but now we have two downloaded replicas since it's running we also have two",
    "start": "6205679",
    "end": "6212000"
  },
  {
    "text": "preprocessors running as well and just for safety let's also hit this one again let's say i call it this three",
    "start": "6212000",
    "end": "6219280"
  },
  {
    "text": "and this whole pipeline is too executable we might see a different number here up on the new request",
    "start": "6219280",
    "end": "6227520"
  },
  {
    "text": "yeah so this one is actually this time to draw the zero because we have a model 3 or something in the code implementation",
    "start": "6227679",
    "end": "6233520"
  },
  {
    "text": "all right so here i walk you through of everything we did from local graph building you can see",
    "start": "6233520",
    "end": "6239520"
  },
  {
    "text": "it's a full pythonic local dev experience for quick iteration and you can incrementally build and play",
    "start": "6239520",
    "end": "6245600"
  },
  {
    "text": "once with your dac building that just all the values of your field as if you're writing plain python code",
    "start": "6245600",
    "end": "6251040"
  },
  {
    "text": "then when you want to get to the exposing http you can bring your own dac driver you can have your own input",
    "start": "6251040",
    "end": "6256320"
  },
  {
    "text": "adapter and then we get in the cli for operationalizing how we do serve run and",
    "start": "6256320",
    "end": "6261600"
  },
  {
    "text": "then deploy to our local research cluster for another http endpoint validation how we're going to do serve build to",
    "start": "6261600",
    "end": "6267440"
  },
  {
    "text": "generate yaml how we deploy it how we test http and how we reconfigure the entire application again so yeah so here",
    "start": "6267440",
    "end": "6274560"
  },
  {
    "text": "concludes my demo thanks for watching any questions you have",
    "start": "6274560",
    "end": "6279840"
  },
  {
    "text": "well thank you joe i was completely mesmerized this was really magic i mean you could do everything",
    "start": "6279840",
    "end": "6285280"
  },
  {
    "text": "in python right on your write visual editor run everything locally test everything locally and then",
    "start": "6285280",
    "end": "6291600"
  },
  {
    "text": "by just creating the the yamaha structure config file you can deploy it update it uh in real time this",
    "start": "6291600",
    "end": "6298320"
  },
  {
    "text": "is absolutely brilliant it's so much different from going to do the the the python code and",
    "start": "6298320",
    "end": "6303920"
  },
  {
    "text": "changing the number of replicas and the number of the kind of resources you want you just change the file and then deploy",
    "start": "6303920",
    "end": "6309600"
  },
  {
    "text": "it again this is what this is what ease of apis that shares was talking about and the graph",
    "start": "6309600",
    "end": "6315520"
  },
  {
    "text": "building from local locally and then testing it remotely that was actually brilliant i think thank you very much",
    "start": "6315520",
    "end": "6320719"
  },
  {
    "text": "for the wonderful demo i think we have about uh 10 minutes left um i don't have any",
    "start": "6320719",
    "end": "6327679"
  },
  {
    "text": "questions on there but uh wanted to thank all of you really for attending oh",
    "start": "6327679",
    "end": "6335040"
  },
  {
    "text": "paige is here i had a question for her i heard that paige you were actually doing a webinar in two weeks uh",
    "start": "6335040",
    "end": "6341920"
  },
  {
    "text": "something to do with in two weeks something to do with manage race serve uh i hope you can bid this demo because",
    "start": "6341920",
    "end": "6348719"
  },
  {
    "text": "this is now we have we have set the bar absolutely and uh i am dialing in today",
    "start": "6348719",
    "end": "6356159"
  },
  {
    "text": "from the any scale offices watching the great presentations that have",
    "start": "6356159",
    "end": "6361360"
  },
  {
    "text": "happened today i'm really excited to see zhao and treyas and ed's presentations about",
    "start": "6361360",
    "end": "6367840"
  },
  {
    "text": "serve and to your point we're going to be having an exciting seminar series about",
    "start": "6367840",
    "end": "6374320"
  },
  {
    "text": "serve in the next couple of weeks so if anybody wants to know how to deploy their first ml model using serv",
    "start": "6374320",
    "end": "6383280"
  },
  {
    "text": "and do it locally and as well as host it on any scale um you should come and say",
    "start": "6383280",
    "end": "6388400"
  },
  {
    "text": "hi to us in about uh when was it april 12th 27th i think i think sophia i just",
    "start": "6388400",
    "end": "6394320"
  },
  {
    "text": "put the link on the on the chat and i'll definitely share it with people",
    "start": "6394320",
    "end": "6399760"
  },
  {
    "text": "excellent thank you so much and i uh just again was completely blown away by the presentations today they were",
    "start": "6399760",
    "end": "6406080"
  },
  {
    "text": "fantastic um and uh very much excited to to be able to get my hands dirty and and do it",
    "start": "6406080",
    "end": "6412880"
  },
  {
    "text": "myself in a couple of weeks um hopefully somebody there um and yes thank you",
    "start": "6412880",
    "end": "6419760"
  },
  {
    "text": "well thank you all uh let me see do you have a um hi i may i know when will the kubernetes",
    "start": "6419760",
    "end": "6426239"
  },
  {
    "text": "reintegration will be out i think he's talking about i think uh uh game is talking about the cube ray",
    "start": "6426239",
    "end": "6432880"
  },
  {
    "text": "integration and does anybody on the team um",
    "start": "6432880",
    "end": "6437920"
  },
  {
    "text": "simon or paige or edward have an idea when are we actually rolling out cube ray i know we are",
    "start": "6437920",
    "end": "6444239"
  },
  {
    "text": "constantly working on it um so cubrey um cubrey i believe is is",
    "start": "6444239",
    "end": "6452480"
  },
  {
    "text": "already uh kind of available and and ready for people uh ready for people to",
    "start": "6452480",
    "end": "6458320"
  },
  {
    "text": "see and the ray dash prod slash cubrey yes yes yes",
    "start": "6458320",
    "end": "6464800"
  },
  {
    "text": "ready to test out file feature requests um and we're certainly excited to have",
    "start": "6464800",
    "end": "6470159"
  },
  {
    "text": "people um test it out the uh and",
    "start": "6470159",
    "end": "6475199"
  },
  {
    "text": "yeah that's that's kind of the the path yeah yeah yeah we just we just have a link on",
    "start": "6475199",
    "end": "6481199"
  },
  {
    "text": "on the specific server operator you'll be part of cube ray moving forward and",
    "start": "6481199",
    "end": "6486560"
  },
  {
    "text": "we're still in development mode and like we're not going to guarantee your time but",
    "start": "6486560",
    "end": "6491600"
  },
  {
    "text": "uh but like i will be more than happy to basically show any pre-release version",
    "start": "6491600",
    "end": "6497600"
  },
  {
    "text": "and get a new tester and trying to get your hands on it yes and then you can actually do follow the progress on the",
    "start": "6497600",
    "end": "6504480"
  },
  {
    "text": "github com ray project slash cube ray because all the all the array project related and one one particular guitar",
    "start": "6504480",
    "end": "6511280"
  },
  {
    "text": "bend so any progress you want to find out about cube ray or you'll find it about air it's it's in there and so we",
    "start": "6511280",
    "end": "6518320"
  },
  {
    "text": "want to thank uh simon we want to thank shreyas we want to thank xiao for the wonderful magic that you actually showed",
    "start": "6518320",
    "end": "6523679"
  },
  {
    "text": "us exactly look like the documentation exactly look like how you deployed the craft the apis were 10 lines of code and",
    "start": "6523679",
    "end": "6530320"
  },
  {
    "text": "you just showed all the different magic you didn't have to train a model you could actually use transfer learning that was just absolutely brilliant oh by",
    "start": "6530320",
    "end": "6536480"
  },
  {
    "text": "the way the hummingbird looks like a sharpener that i have",
    "start": "6536480",
    "end": "6541840"
  },
  {
    "text": "so i mean there was there was some probability that you could be a sharpener with the pencil and and so i",
    "start": "6542159",
    "end": "6549360"
  },
  {
    "text": "thought that i thought that was rather really cute uh and then srihas thanks a lot for the demo and welcome back sofia",
    "start": "6549360",
    "end": "6555360"
  },
  {
    "text": "i know you had the call we really like to thank you for um you know coming early from your vacation and then being with us and",
    "start": "6555360",
    "end": "6562000"
  },
  {
    "text": "helping us with all the logistics so my heartfelt thank to all of you by creating this this wonderful uh communal",
    "start": "6562000",
    "end": "6569679"
  },
  {
    "text": "meetup and thank you all of you for attending this this and staying up late at night we hope to see you again in a",
    "start": "6569679",
    "end": "6575599"
  },
  {
    "text": "month's time uh hope to see you at the summit as well so thanks a lot cheers and",
    "start": "6575599",
    "end": "6580880"
  },
  {
    "text": "have a nice pint now",
    "start": "6580880",
    "end": "6584760"
  }
]