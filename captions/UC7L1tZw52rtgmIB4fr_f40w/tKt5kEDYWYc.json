[
  {
    "text": "hi um thanks for having me any scale my name is Tobias I'm a senior software",
    "start": "4080",
    "end": "9420"
  },
  {
    "text": "engineer deep set and today I'm going to talk about how we run a question answer system on rayserv",
    "start": "9420",
    "end": "16619"
  },
  {
    "text": "it's a bit of background about about me and and the people doing this talk so",
    "start": "16619",
    "end": "21840"
  },
  {
    "text": "actually this talk was supposed to be given by my colleague Dimitri but he can't come due to Visa issues so I'm his",
    "start": "21840",
    "end": "27480"
  },
  {
    "text": "backup today and I'm hoping I'm doing like the honest for him we are both working at Deep set deep set",
    "start": "27480",
    "end": "34559"
  },
  {
    "text": "is a company based in Germany and we were funded in 2018 we started out by",
    "start": "34559",
    "end": "40260"
  },
  {
    "text": "providing Professional Services around natural language processing to companies using that to kind of gain some",
    "start": "40260",
    "end": "46980"
  },
  {
    "text": "knowledge on okay what are each other needs what are their pain points and then we distilled all of this knowledge from providing Professional Services to",
    "start": "46980",
    "end": "54420"
  },
  {
    "text": "actually build a framework which you can utilize to kind of yeah make all of this more quick more yeah more efficient and",
    "start": "54420",
    "end": "60899"
  },
  {
    "text": "quicker and this is our framework called Haystack it's open source you can find it online and you can it's very flexible",
    "start": "60899",
    "end": "67920"
  },
  {
    "text": "and allows you to build NLP Pipelines from really like from the beginning from training and fine-tuning to evaluating",
    "start": "67920",
    "end": "74340"
  },
  {
    "text": "models and so on in addition to this you also got deepsea Cloud it's a SAS platform kind of on top",
    "start": "74340",
    "end": "81299"
  },
  {
    "text": "of our Haystack framework which allows you to really manage the entire NLP",
    "start": "81299",
    "end": "86700"
  },
  {
    "text": "workflow so from actually uploading all of your documents you want to go through to then finding the right pipeline for",
    "start": "86700",
    "end": "94380"
  },
  {
    "text": "use case comparing the pipelines making sure you're getting the most out of it and then actually putting in Productions",
    "start": "94380",
    "end": "100740"
  },
  {
    "text": "monitoring it and yeah and so on and for that we also collect the funding by GV",
    "start": "100740",
    "end": "106680"
  },
  {
    "text": "for example Harpoon and others so in today's talk and this breakout",
    "start": "106680",
    "end": "112320"
  },
  {
    "text": "session um I've kind of three sections for for this talk so the first one is what is",
    "start": "112320",
    "end": "118200"
  },
  {
    "text": "question answering for those who are not familiar with it then I want to tell you how we're using that with Ray surf",
    "start": "118200",
    "end": "123840"
  },
  {
    "text": "together kind of connecting to this conference and then how we want to go forward with what you already have what",
    "start": "123840",
    "end": "130619"
  },
  {
    "text": "are the limitations we're hitting with Ray surf um yeah and also kind of what things you want to explore in the future",
    "start": "130619",
    "end": "137220"
  },
  {
    "text": "so who of you in the room do you know what question answering is",
    "start": "137220",
    "end": "143000"
  },
  {
    "text": "yeah it's a bunch we would say like 10 30 percent so question answering is basically all",
    "start": "143220",
    "end": "149340"
  },
  {
    "text": "about finding the needle in the haystack and there should probably be like a star using natural language right you have",
    "start": "149340",
    "end": "156660"
  },
  {
    "text": "like a natural query to find your information so finding a needle in the haystack if you think of the Internet",
    "start": "156660",
    "end": "162000"
  },
  {
    "text": "it's somewhat solved right if you have a question then you can just Google it and",
    "start": "162000",
    "end": "167280"
  },
  {
    "text": "you can even ask a question natural language right like how tall is Barack Obama and Google is going to do that and",
    "start": "167280",
    "end": "173220"
  },
  {
    "text": "Google is super powerful um I mean I'm a software engineer so Googling is probably 80 of my job and but it's not",
    "start": "173220",
    "end": "180540"
  },
  {
    "text": "solved for your own documents in your private documents right so you might have PDFs you might have extra sheets you might have notion in your company",
    "start": "180540",
    "end": "186720"
  },
  {
    "text": "and you might have Word files and there's all of this data but it's not in Google right and you probably also don't",
    "start": "186720",
    "end": "192180"
  },
  {
    "text": "want to hand it over to Google so you want to build your own search system and you can do that but then these usually",
    "start": "192180",
    "end": "198000"
  },
  {
    "text": "are super bad right you have like keyword searched based so all of your users kind of they try to engineer the",
    "start": "198000",
    "end": "203760"
  },
  {
    "text": "search query to optimize it to find like as many results as possible and it's usually bad and we kind of fix the",
    "start": "203760",
    "end": "211379"
  },
  {
    "text": "question answering because you can actually ask queries in natural language you don't have to engineer anything you",
    "start": "211379",
    "end": "216780"
  },
  {
    "text": "can use a natural language to find semantic data so um just like how that kind of looks",
    "start": "216780",
    "end": "223500"
  },
  {
    "text": "in in production or like to make this a more bit more tangible so in this case",
    "start": "223500",
    "end": "228780"
  },
  {
    "text": "all of our our data set is Game of Thrones trivia so um for example like who played in this",
    "start": "228780",
    "end": "235379"
  },
  {
    "text": "in that episode some meter data like how many viewers did this episode have what",
    "start": "235379",
    "end": "240420"
  },
  {
    "text": "is the plot of each episode and what you can then do is just ask um in this question in this case who played Jon",
    "start": "240420",
    "end": "246599"
  },
  {
    "text": "Snow in Game of Thrones and with semantic search and question answering you do not only have all the",
    "start": "246599",
    "end": "253560"
  },
  {
    "text": "the documents kind of lined up kind of containing possibly the answer and it even highlights the answer for you right",
    "start": "253560",
    "end": "259979"
  },
  {
    "text": "so you can see for the first result it actually kind of highlights You Kit Harington and you know okay this is",
    "start": "259979",
    "end": "266699"
  },
  {
    "text": "actually the answer there's also other answers but they have a lower relevance score right and you can see it the model",
    "start": "266699",
    "end": "272699"
  },
  {
    "text": "is very confident the kit haringness indeed um playing Jon Snow in Game of Thrones",
    "start": "272699",
    "end": "278639"
  },
  {
    "text": "this demo is online so you can just like use the QR code to play around it with yourself",
    "start": "278639",
    "end": "285479"
  },
  {
    "text": "and the whole thing enabling semantic search and question answering is word",
    "start": "285479",
    "end": "290639"
  },
  {
    "text": "embedding so word embeddings are produced by large machine learning models these machine learning models are",
    "start": "290639",
    "end": "297060"
  },
  {
    "text": "exposed to large tax Corporal so usually the entire internet right and by doing",
    "start": "297060",
    "end": "303360"
  },
  {
    "text": "that they are able to attach numeric values to words and these are not only",
    "start": "303360",
    "end": "308520"
  },
  {
    "text": "numeric values no they actually also capture some of the semantic meaning Within A Word so each model can then",
    "start": "308520",
    "end": "315479"
  },
  {
    "text": "represent the word with a long Vector of numbers so in this example you can see it's just seven dimensions but it's not",
    "start": "315479",
    "end": "322800"
  },
  {
    "text": "really the typical use case right usually you have hundreds of dimensions for example 700",
    "start": "322800",
    "end": "328199"
  },
  {
    "text": "but the really cool thing is that we are able to capture semantics so if you look for example on the right example with",
    "start": "328199",
    "end": "334560"
  },
  {
    "text": "cat kitten and dog you can see that cat and kitten it's not the same word but they are very very similar right so if",
    "start": "334560",
    "end": "341100"
  },
  {
    "text": "you map this very long Vector actually in a two-dimensional space to make it more tangible for us you can see that",
    "start": "341100",
    "end": "347340"
  },
  {
    "text": "cat and kitten actually indeed kind of end up in a space very close to each other dog for example is still reasonably",
    "start": "347340",
    "end": "355080"
  },
  {
    "text": "close but further apart right because it's also a part of pet you a type of a",
    "start": "355080",
    "end": "360240"
  },
  {
    "text": "kind of pet you can have at home but it's not a cat or kitten on the other side houses for example is very far",
    "start": "360240",
    "end": "367080"
  },
  {
    "text": "apart from things like cat kitten and dog maybe kind of similar because you",
    "start": "367080",
    "end": "372720"
  },
  {
    "text": "know pets live usually in a house but like you can see from the distance that there's no real like similarity between",
    "start": "372720",
    "end": "379560"
  },
  {
    "text": "these and these red embeddings are even so powerful you can even do math with it so",
    "start": "379560",
    "end": "384720"
  },
  {
    "text": "if you look at the lower right example you can see men and women kind of have a similar relationship even than king and",
    "start": "384720",
    "end": "390180"
  },
  {
    "text": "queen so you can see like kind of the relationship of these terms are kind of like even like embedded in there so this",
    "start": "390180",
    "end": "396539"
  },
  {
    "text": "is super powerful and this is what enables actually natural language processing and what enables question",
    "start": "396539",
    "end": "402539"
  },
  {
    "text": "answering systems so how do you build a question answer systems so usually you start with a big",
    "start": "402539",
    "end": "409380"
  },
  {
    "text": "pile of data right as I said Word files PDF files Excel sheets maybe some stuff",
    "start": "409380",
    "end": "415560"
  },
  {
    "text": "in ocean then you have someone or somewhat organized this right so they actually",
    "start": "415560",
    "end": "421020"
  },
  {
    "text": "have kind of like taking all of this data and putting it in a library so you kind of know where to find what and we",
    "start": "421020",
    "end": "427680"
  },
  {
    "text": "are usually using our framework Haystack to do exactly that right so the admin would take our framework and collect all",
    "start": "427680",
    "end": "434580"
  },
  {
    "text": "the data and organize it and once it's organized your users can actually ask the system questions like for example",
    "start": "434580",
    "end": "440699"
  },
  {
    "text": "who played actually Jon Snow and the system would then answer it this is very high level so now we want",
    "start": "440699",
    "end": "447599"
  },
  {
    "text": "to dive a little bit deeper into the single components of a QA system so the most basic component in this is",
    "start": "447599",
    "end": "454080"
  },
  {
    "text": "the database storing all of the documents right and so somewhere they have to go and somewhere you also have to store these word embeddings which are",
    "start": "454080",
    "end": "460919"
  },
  {
    "text": "generated by a machine learning model they're The Usual Suspects for databases",
    "start": "460919",
    "end": "466319"
  },
  {
    "text": "out there text databases like elasticsearch and open search but it's also optimized and specialized databases",
    "start": "466319",
    "end": "472620"
  },
  {
    "text": "out there like for example milvers or Pinecone and they're optimized in dealing with these large vectors which",
    "start": "472620",
    "end": "479520"
  },
  {
    "text": "are produced by our machine learning models because we have to be able to compare these vectors within the database to actually find documents",
    "start": "479520",
    "end": "486240"
  },
  {
    "text": "which are similar to what we're searching for and you can see that there's really a use case for these specialized databases because Vector",
    "start": "486240",
    "end": "493740"
  },
  {
    "text": "optimized ones are much much faster than non-vector optimized ones",
    "start": "493740",
    "end": "498840"
  },
  {
    "text": "the other component is the indexing pipeline which you would usually build with Haystack so the indexing pipeline",
    "start": "498840",
    "end": "505440"
  },
  {
    "text": "does several things first of all it has to pre-process the documents you feed into the system so extract the text from",
    "start": "505440",
    "end": "511919"
  },
  {
    "text": "it right so this could be from a PDF you need to extract it if you have a table it has to extract the headers and then",
    "start": "511919",
    "end": "517440"
  },
  {
    "text": "the columns and so on then you have the machine learning model going through the text extracting these",
    "start": "517440",
    "end": "523800"
  },
  {
    "text": "word embeddings for the single words and then storing the word vectors as well as",
    "start": "523800",
    "end": "528839"
  },
  {
    "text": "the documents in our document store which we had before and actually this interacting pipeline",
    "start": "528839",
    "end": "534180"
  },
  {
    "text": "comes with a couple of problems which we have to deal with first of all is that",
    "start": "534180",
    "end": "539399"
  },
  {
    "text": "we need a lot of like a high throughput if you think of customers at Large Scale",
    "start": "539399",
    "end": "544680"
  },
  {
    "text": "starting with our systems they usually have like millions of documents sitting around and they want to take all of",
    "start": "544680",
    "end": "550200"
  },
  {
    "text": "these documents and put it in our system so the indexing pipeline has especially",
    "start": "550200",
    "end": "555899"
  },
  {
    "text": "at the beginning to scale like like crazy fast and to crazy high peaks once all of these documents are in our",
    "start": "555899",
    "end": "562080"
  },
  {
    "text": "document store with embeddings it's kinda all right right because then you still just have this continuous stream",
    "start": "562080",
    "end": "567420"
  },
  {
    "text": "but until you get there you have like we have to really have to to scale",
    "start": "567420",
    "end": "572459"
  },
  {
    "text": "um very quickly and then the other thing is because you're using these machine learning models we also have special resource",
    "start": "572459",
    "end": "578779"
  },
  {
    "text": "infrastructure requirements which means we need gpus and our staff has to run on",
    "start": "578779",
    "end": "584580"
  },
  {
    "text": "gpus um so you both got the high throughput entity GPU requirement which kind of",
    "start": "584580",
    "end": "589920"
  },
  {
    "text": "makes it a bit clearly to deploy and render some production the complementary part to the indexing",
    "start": "589920",
    "end": "595620"
  },
  {
    "text": "pipeline is the query pipeline so this is the pipeline taking the user's input going to the through the to the document",
    "start": "595620",
    "end": "601620"
  },
  {
    "text": "store retrieving documents which are similar and then actually giving this back to the user",
    "start": "601620",
    "end": "606839"
  },
  {
    "text": "so um in this case there are even two machine learning models um in the game I would say so first the",
    "start": "606839",
    "end": "613019"
  },
  {
    "text": "machine learning model taking the user's query so who is who is playing Jon Snow",
    "start": "613019",
    "end": "619320"
  },
  {
    "text": "translating these to word embeddings so you can actually use these word embeddings in this Vector to do a",
    "start": "619320",
    "end": "624839"
  },
  {
    "text": "nearest neighbor search in our document store to find documents which are similar to the user search query then",
    "start": "624839",
    "end": "631080"
  },
  {
    "text": "retrieve documents which are similar to The query from this document store but at this point you would just have like",
    "start": "631080",
    "end": "636660"
  },
  {
    "text": "similar documents right you would not know like where's the answer in this document so there's a separate machine learning model actually going through",
    "start": "636660",
    "end": "642540"
  },
  {
    "text": "the documents and then kind of highlighting okay actually Kit Harington in this document is actually the answer",
    "start": "642540",
    "end": "647940"
  },
  {
    "text": "so making this from a yeah give me gives me okay ish document results actually",
    "start": "647940",
    "end": "654060"
  },
  {
    "text": "about this gives me the answer right at the hand and you can even have another machine learning model in there even",
    "start": "654060",
    "end": "659820"
  },
  {
    "text": "generating the answer for you like having actually a full sentence um responding to the query",
    "start": "659820",
    "end": "667100"
  },
  {
    "text": "um so you're probably wondering how does Ray Surf and Ray come into all of this so if you look at the right side of this",
    "start": "667980",
    "end": "674339"
  },
  {
    "text": "picture um running the document store in production is not a big deal usually you",
    "start": "674339",
    "end": "679980"
  },
  {
    "text": "can just go to the cloud provider of your choice for example AWS and pick any",
    "start": "679980",
    "end": "685440"
  },
  {
    "text": "up out of off-the-shelf component like open search for example at then to plug",
    "start": "685440",
    "end": "690600"
  },
  {
    "text": "in so you can do the nearest neighbor search with your word vectors and you're off the hook basically the really",
    "start": "690600",
    "end": "696300"
  },
  {
    "text": "interesting part is how do we deal with indexing Pipeline and how do we deal with the query pipeline",
    "start": "696300",
    "end": "701640"
  },
  {
    "text": "and both of these we're running in our setup on Ray and more specifically on",
    "start": "701640",
    "end": "707459"
  },
  {
    "text": "Ray surfs we actually have endpoints exposed for both the indexing Pipeline and our query pipeline because users",
    "start": "707459",
    "end": "714000"
  },
  {
    "text": "searching for data is usually like they send a request right we're forwarding this to to raise serve to the query",
    "start": "714000",
    "end": "719279"
  },
  {
    "text": "pipeline yeah um you have a question",
    "start": "719279",
    "end": "724160"
  },
  {
    "text": "yeah they are actually on the it's a very good question they're running on the same array cluster in our case but",
    "start": "729680",
    "end": "735839"
  },
  {
    "text": "uh two different Ray deployments so separate cluster two different",
    "start": "735839",
    "end": "741000"
  },
  {
    "text": "deployment or seven kubernetes same cluster yeah same rate cluster cool",
    "start": "741000",
    "end": "746940"
  },
  {
    "text": "um let's just do and wait with the question until the end and then we have a q a section and then we can answer the",
    "start": "746940",
    "end": "752760"
  },
  {
    "text": "rest of the questions if you have any so yeah everything is running on the same Ray cluster but we have two Ray",
    "start": "752760",
    "end": "758279"
  },
  {
    "text": "serve deployments so um this is just like a kind of a code example how this works for indexing",
    "start": "758279",
    "end": "765360"
  },
  {
    "text": "pipeline so just also want to show you how powerful and how simple Rey actually",
    "start": "765360",
    "end": "771660"
  },
  {
    "text": "is so to actually to make our indexing pipeline Deployable with Ray all we need",
    "start": "771660",
    "end": "777180"
  },
  {
    "text": "to do is just adding the ad serve deployment deck Creator to our class and then we",
    "start": "777180",
    "end": "783300"
  },
  {
    "text": "are ready to use this with Ray surf so this is developer velocity times 100 I",
    "start": "783300",
    "end": "789180"
  },
  {
    "text": "would say and then all we need to do is to load our indexing pipeline the machine learning models and so on to to progress",
    "start": "789180",
    "end": "796560"
  },
  {
    "text": "to process um the indexing query and then another very like yeah I would",
    "start": "796560",
    "end": "804300"
  },
  {
    "text": "say something which makes it way more efficient for us to use rayserv is to serve dot batch decorator provided by",
    "start": "804300",
    "end": "810660"
  },
  {
    "text": "Ray so these machine learning models you can of course run one query by one query",
    "start": "810660",
    "end": "816000"
  },
  {
    "text": "but it's not really efficient in gpus you actually want to have usually multiple queries patched because you can",
    "start": "816000",
    "end": "821459"
  },
  {
    "text": "actually handle these workloads way more efficient than on the GPU and this could",
    "start": "821459",
    "end": "827040"
  },
  {
    "text": "be a bit tricky right how to do actually batch requests and and so on but with Ray you just add this deck creator with",
    "start": "827040",
    "end": "833820"
  },
  {
    "text": "Ray surf batch say how big the the patch is and all the HTTP queries coming to",
    "start": "833820",
    "end": "839760"
  },
  {
    "text": "this endpoints automatically batch by Ray and then we can process them all at once which makes it way more efficient",
    "start": "839760",
    "end": "845639"
  },
  {
    "text": "for us and we don't have to reinvent the wheel for anything of this query pipeline is very similar so in",
    "start": "845639",
    "end": "853620"
  },
  {
    "text": "this case we have like this a bit more options and configuration out there for example for how to scale the the query",
    "start": "853620",
    "end": "860639"
  },
  {
    "text": "pipelines in this case we say okay they always have to be one query pipeline but they can be up to 10 of the query",
    "start": "860639",
    "end": "867300"
  },
  {
    "text": "pipelines you could also imagine that this kind of is up to the customer right like how much they're paying and then",
    "start": "867300",
    "end": "872700"
  },
  {
    "text": "they get more max replicas and maybe maybe than others or you have multiple",
    "start": "872700",
    "end": "877860"
  },
  {
    "text": "versions of a pipeline so you can do all of this just within the deck creator",
    "start": "877860",
    "end": "883680"
  },
  {
    "text": "um and otherwise it's very similar to our indexing pipeline we just load this so This is actually our Haystack code",
    "start": "883680",
    "end": "889500"
  },
  {
    "text": "and then we can just run the request in our pipeline the code is actually also linked at the",
    "start": "889500",
    "end": "896160"
  },
  {
    "text": "end of our slides so you can actually check this out on GitHub this is of course not the code which we're using in",
    "start": "896160",
    "end": "901199"
  },
  {
    "text": "our Enterprise product but um very similar kind of so you can actually play around with it and yeah see how you can",
    "start": "901199",
    "end": "907920"
  },
  {
    "text": "use it for yourself so how are we going forward what are the",
    "start": "907920",
    "end": "913199"
  },
  {
    "text": "limitations which you're hitting with race or what is actually things you're liking and what are things we're going",
    "start": "913199",
    "end": "918660"
  },
  {
    "text": "to explore in the in the close future so quick recap what is working really",
    "start": "918660",
    "end": "925139"
  },
  {
    "text": "great for us we are startup we run 40 to 50 people at the moment for us the python interface is really",
    "start": "925139",
    "end": "931680"
  },
  {
    "text": "like a huge speed up because you can just use Python code to do deployments auto scaling deployments and we don't",
    "start": "931680",
    "end": "938699"
  },
  {
    "text": "have to fiddle around with kubernetes too much and especially in the beginning this was really helpful now we are of",
    "start": "938699",
    "end": "944399"
  },
  {
    "text": "course getting more and more Specialists on board which kind of can take on the more fiddly stuff with kubernetes and so",
    "start": "944399",
    "end": "949500"
  },
  {
    "text": "on but that's also another problem then because with Ray you can just go forward or already mentioned Auto scaling is",
    "start": "949500",
    "end": "956579"
  },
  {
    "text": "just there out of the box with Ray for these indexing requests we needed we have these like three million spikes at",
    "start": "956579",
    "end": "963240"
  },
  {
    "text": "the beginning right so we really have to to scale up to handle these loads because users can't wait for days or",
    "start": "963240",
    "end": "969420"
  },
  {
    "text": "weeks until their documents are actually searchable they ideally want to get started with our product right away and",
    "start": "969420",
    "end": "974940"
  },
  {
    "text": "this is still not possible within seconds but at least minutes to hours depending on on the load",
    "start": "974940",
    "end": "980880"
  },
  {
    "text": "I pointed out the request patching which is great and a very specific but very interesting point is the precise GPU",
    "start": "980880",
    "end": "987779"
  },
  {
    "text": "allocation actually we often have pipelines which don't require an entire GPU to run so they can actually share",
    "start": "987779",
    "end": "993720"
  },
  {
    "text": "workloads and until like I think a month back or so this was kind of very specific Rave feature which you couldn't",
    "start": "993720",
    "end": "1000980"
  },
  {
    "text": "use on kubernetes so in kubernetes you only could allow allocating entire gpus to to workloads and well if you don't",
    "start": "1000980",
    "end": "1008959"
  },
  {
    "text": "have the the Google money then you um actually have to be a bit more efficient with your resources and in a way could",
    "start": "1008959",
    "end": "1014839"
  },
  {
    "text": "just do that and that was really a big decisive factor for us and yeah if your cluster goes down or anything happens",
    "start": "1014839",
    "end": "1020959"
  },
  {
    "text": "then you have to rate checkpoints you can just like re-establish the entire point and which was really great for us",
    "start": "1020959",
    "end": "1027260"
  },
  {
    "text": "so which is really great for us but um there are also a couple of downsides which I want to go through now",
    "start": "1027260",
    "end": "1034220"
  },
  {
    "text": "um I think it's important to point out like these downsets are not all Ray downsides I think it's also to just how",
    "start": "1034220",
    "end": "1041000"
  },
  {
    "text": "we used Rey um for example if you if you look at the first issue is yes we're having three",
    "start": "1041000",
    "end": "1047000"
  },
  {
    "text": "million requests with one customer for example they have three million documents and they need to be all indexed at once so you have like a",
    "start": "1047000",
    "end": "1053720"
  },
  {
    "text": "really really huge Peak at the beginning for these indexing pipelines and it has to scale a lot right this is not from 0",
    "start": "1053720",
    "end": "1060740"
  },
  {
    "text": "to 10 it might be from 0 to 50 replicas for example and Rey is definitely",
    "start": "1060740",
    "end": "1066140"
  },
  {
    "text": "struggling handling all these requests flying around and then scaling fast enough not timing out and so on so this",
    "start": "1066140",
    "end": "1073340"
  },
  {
    "text": "is yeah a bit problematic I would say but also on our side just how we send out these indexing requests",
    "start": "1073340",
    "end": "1079640"
  },
  {
    "text": "then follow-up problem to this is there's currently a bug that our cluster or the direct cluster doesn't scale down",
    "start": "1079640",
    "end": "1086260"
  },
  {
    "text": "so as I said right you have these like huge peaks in the beginning um where everything has to scale up and",
    "start": "1086260",
    "end": "1092660"
  },
  {
    "text": "then you have 50 replicas of the indexing pipeline sitting there but then the whole thing doesn't scale down only",
    "start": "1092660",
    "end": "1097820"
  },
  {
    "text": "if you restart the cluster but that causes downtime so you don't want to do that and then just all these replicas",
    "start": "1097820",
    "end": "1103460"
  },
  {
    "text": "are idling um and yeah that's obviously not very cost efficient and something you want to fix",
    "start": "1103460",
    "end": "1110600"
  },
  {
    "text": "um as you restart the cluster you're usually waiting for the Hat node because there's a single point of failure",
    "start": "1110600",
    "end": "1115880"
  },
  {
    "text": "um at the moment um in the setup and we have to restart the cluster whenever we do a deployment",
    "start": "1115880",
    "end": "1121880"
  },
  {
    "text": "um so we are not not very should be I would say with deployments if we deploy a couple of times a week right so this",
    "start": "1121880",
    "end": "1127940"
  },
  {
    "text": "is kind of bearable but ideally in the future if you deploy a couple of times a day or even 100 of times a day um",
    "start": "1127940",
    "end": "1134660"
  },
  {
    "text": "ideally yeah you definitely need to raise the bar there and then waiting for the head mode and not being able to do",
    "start": "1134660",
    "end": "1140000"
  },
  {
    "text": "rolling deployments is a huge blocker for us other thing is that we would like to have multiple base images so as I said",
    "start": "1140000",
    "end": "1148220"
  },
  {
    "text": "we have our framework Haystack and as you know Enterprise customers they",
    "start": "1148220",
    "end": "1153380"
  },
  {
    "text": "usually get stuck on some versions so some of them are using an older version some of them are really more early adopters and using a newer version so we",
    "start": "1153380",
    "end": "1160940"
  },
  {
    "text": "don't have this same dependency version in our cluster but actually multiple of them ideally so you could just say okay",
    "start": "1160940",
    "end": "1167179"
  },
  {
    "text": "if you're using Haystack 1.5 use it if you're using Haystack 2 then also use it",
    "start": "1167179",
    "end": "1172640"
  },
  {
    "text": "we just kind of do schedule a different deployment for you and we wouldn't bother too much but at the moment we",
    "start": "1172640",
    "end": "1178580"
  },
  {
    "text": "really can't do this because it's pinned to one version um there's a couple of next steps you",
    "start": "1178580",
    "end": "1184880"
  },
  {
    "text": "would want to tackle um for addressing um these problems which are just lined out one of them is",
    "start": "1184880",
    "end": "1191000"
  },
  {
    "text": "for example improve our indexing architecture so how we're doing this",
    "start": "1191000",
    "end": "1196460"
  },
  {
    "text": "um yes Ray is struggling definitely with the amount of requests we're sending around but we don't have to have very",
    "start": "1196460",
    "end": "1202280"
  },
  {
    "text": "struggling right um for example we can use batching already before we're",
    "start": "1202280",
    "end": "1207380"
  },
  {
    "text": "sending out the requests for example reducing the the number of requests from",
    "start": "1207380",
    "end": "1213080"
  },
  {
    "text": "three million to just three thousands if we use um just batch that before it's a lot of less HTTP requests flying around",
    "start": "1213080",
    "end": "1220220"
  },
  {
    "text": "taking load of Ray serve but there's also the alternative for using great core and so on so luckily the array",
    "start": "1220220",
    "end": "1226039"
  },
  {
    "text": "framework really gives us a lot of opportunities to improve here and that's something we're already trying out and",
    "start": "1226039",
    "end": "1231320"
  },
  {
    "text": "benchmarking at this moment then also announced today the keynote is three deployment graphs I think this has huge",
    "start": "1231320",
    "end": "1238520"
  },
  {
    "text": "potential um especially for us so if you look at the code snippet which I showed you before you had this like",
    "start": "1238520",
    "end": "1244760"
  },
  {
    "text": "very monolithic pipeline I would say we just deployed it as one Ray surf deployment but ideally I mean that's",
    "start": "1244760",
    "end": "1252020"
  },
  {
    "text": "what you can do with Ray deployment graphs really take it apart and have like I am the simple the the single",
    "start": "1252020",
    "end": "1258500"
  },
  {
    "text": "nodes deployed in individually and this is really cool because some of these nodes need more con um compute than",
    "start": "1258500",
    "end": "1265640"
  },
  {
    "text": "others right if you think of the machine learning models you might need 10 or 20 replicas of them but then if you think",
    "start": "1265640",
    "end": "1271940"
  },
  {
    "text": "of something like extracting text from a PDF file probably than one node is enough even for three million files",
    "start": "1271940",
    "end": "1277460"
  },
  {
    "text": "right and ideally we are able to scale these individually and maybe even share them among customers right like why",
    "start": "1277460",
    "end": "1283760"
  },
  {
    "text": "actually you have the same node 10 times or per customer in our cluster if it's",
    "start": "1283760",
    "end": "1288980"
  },
  {
    "text": "actually fast enough to handle load for multiple customers customers at once right",
    "start": "1288980",
    "end": "1294380"
  },
  {
    "text": "um so this is something we want to explore to make everything more efficient but also more more powerful and offer this to our customers",
    "start": "1294380",
    "end": "1302840"
  },
  {
    "text": "yeah as I said if you want to check out the source code which I showed also player but around with it just use this",
    "start": "1302840",
    "end": "1309559"
  },
  {
    "text": "QR code so you can actually go to the GitHub repo otherwise thanks a lot for your",
    "start": "1309559",
    "end": "1315500"
  },
  {
    "text": "attention I added a couple of links down here which might be interesting for you so first of all for our open source",
    "start": "1315500",
    "end": "1321440"
  },
  {
    "text": "framework Haystack then to our company's website and deep set AI but then also to",
    "start": "1321440",
    "end": "1326780"
  },
  {
    "text": "to my colleague I'm Dima who unfortunately couldn't give this talk today and then to my LinkedIn",
    "start": "1326780",
    "end": "1333260"
  },
  {
    "text": "yeah um thank you very much and if you have any questions I think we still have eight minutes left actually so pretty",
    "start": "1333260",
    "end": "1339620"
  },
  {
    "text": "good timing um just shoot yeah [Applause]",
    "start": "1339620",
    "end": "1349910"
  },
  {
    "text": "yeah yeah I should wait wait do you get a microphone yeah",
    "start": "1350539",
    "end": "1355360"
  },
  {
    "text": "yeah I have three questions so you mentioned three million is that per day per hour uh what's it QPS basically yeah",
    "start": "1356720",
    "end": "1365720"
  },
  {
    "text": "um I think so this really depends on the customers right so how many documents they have sitting around and wanna",
    "start": "1365720",
    "end": "1371419"
  },
  {
    "text": "actually make searchable so this is actually at once I know this",
    "start": "1371419",
    "end": "1376460"
  },
  {
    "text": "is very untypical right usually you're talking to customers and they say or to users and they say we have three million",
    "start": "1376460",
    "end": "1382580"
  },
  {
    "text": "requests and then you're like okay in a year in a second like give me some time frame but this is actually they are",
    "start": "1382580",
    "end": "1389299"
  },
  {
    "text": "uploading all of these documents at once right um either through an HTTP endpoint or just connecting there is three bucket",
    "start": "1389299",
    "end": "1395600"
  },
  {
    "text": "and then we have these three million documents sitting there and want to index them literally all at once",
    "start": "1395600",
    "end": "1402940"
  },
  {
    "text": "yeah and then once these documents are indexed then we don't need to index them",
    "start": "1403059",
    "end": "1408380"
  },
  {
    "text": "anymore unless they want to use a different machine learning model for example and then you have more discontinuous stream of arcade as a new",
    "start": "1408380",
    "end": "1414679"
  },
  {
    "text": "document and we want to index it now and that's then really just a couple of requests right I mean depending on the",
    "start": "1414679",
    "end": "1420919"
  },
  {
    "text": "customer but usually they don't create like thousands of documents per second that's not the big deal I think it's",
    "start": "1420919",
    "end": "1426500"
  },
  {
    "text": "really the the first initial Peak and but it's important for us right because customers coming to our platform and",
    "start": "1426500",
    "end": "1433220"
  },
  {
    "text": "they want to play around with it and we want to make it searchable right and if they just upload their things and then",
    "start": "1433220",
    "end": "1438860"
  },
  {
    "text": "it takes I don't know it tells them come back in a week and play around with it then the entire wow effect has just gone",
    "start": "1438860",
    "end": "1446360"
  },
  {
    "text": "I see so your issue or your concern was that it takes too long to index at three",
    "start": "1446360",
    "end": "1452720"
  },
  {
    "text": "million yeah but we're also definitely just seeing um scalability problems right like Ray actually coming up just in terms of like",
    "start": "1452720",
    "end": "1460280"
  },
  {
    "text": "deploying new workers for example but then also seeing like timeouts in between because it's just a lot of HTTP",
    "start": "1460280",
    "end": "1466159"
  },
  {
    "text": "requests I'm flying around but I think it's not necessarily I think the",
    "start": "1466159",
    "end": "1472700"
  },
  {
    "text": "question is like is race serve the right tool for this right Israel core for example more fit for this and I I think",
    "start": "1472700",
    "end": "1480260"
  },
  {
    "text": "so yeah okay so second question the embeddings is do you use a tokenizer a standard",
    "start": "1480260",
    "end": "1487340"
  },
  {
    "text": "tokenizer from Ray or from the Transformer to convert your string tokens to a",
    "start": "1487340",
    "end": "1496039"
  },
  {
    "text": "number good question to be honest I don't know because I haven't looked too much into",
    "start": "1496039",
    "end": "1501740"
  },
  {
    "text": "our open source and in this part of the code base okay what we definitely do is split the documents actually and kind of",
    "start": "1501740",
    "end": "1507679"
  },
  {
    "text": "paragraphs um because I mean they all have a different size right and",
    "start": "1507679",
    "end": "1513320"
  },
  {
    "text": "um then I think we're probably using the default packing like yeah tokenizer but",
    "start": "1513320",
    "end": "1518480"
  },
  {
    "text": "you can definitely configure that in just the pipelines so I think it's pretty very much up to the to the",
    "start": "1518480",
    "end": "1523820"
  },
  {
    "text": "customer yeah okay and the last question was you say Auto downscaling what was",
    "start": "1523820",
    "end": "1529400"
  },
  {
    "text": "the kind of lag time typically yes it just doesn't downscale at all",
    "start": "1529400",
    "end": "1535700"
  },
  {
    "text": "right the worker nodes are just sitting there and not downscaling and until like unless you restart the cluster which you",
    "start": "1535700",
    "end": "1543080"
  },
  {
    "text": "can obviously do right but I think because we are also serving production payloads",
    "start": "1543080",
    "end": "1548840"
  },
  {
    "text": "um like you you can't have this downtime right you ideally have a rolling deployment",
    "start": "1548840",
    "end": "1553880"
  },
  {
    "text": "um and if there's maybe like a few seconds it's kind of all right but until like the whole cluster stabilizes again",
    "start": "1553880",
    "end": "1558980"
  },
  {
    "text": "and finds on it's not usable yeah um okay I mean at the moment it's kind of",
    "start": "1558980",
    "end": "1564020"
  },
  {
    "text": "all right right in a close beta State um so that's all right but like going forward",
    "start": "1564020",
    "end": "1569360"
  },
  {
    "text": "um this is definitely a big blocker for us okay um yeah sorry that was my three questions",
    "start": "1569360",
    "end": "1576340"
  },
  {
    "text": "but they're very interesting talks about high availability and Cube Ray later",
    "start": "1576340",
    "end": "1582080"
  },
  {
    "text": "um at the conference I'm definitely you're definitely going to find me in one of these yeah",
    "start": "1582080",
    "end": "1588039"
  },
  {
    "text": "yeah thank you hi my name is Sean I have a question about what other serving framework have you evaluated and why did",
    "start": "1588200",
    "end": "1595220"
  },
  {
    "text": "the racer win um good good question um to be honest this was before my time",
    "start": "1595220",
    "end": "1601159"
  },
  {
    "text": "at Deep set um I think we used um Ray mostly for for the auto scaling",
    "start": "1601159",
    "end": "1607340"
  },
  {
    "text": "and the python interface definitely and and the the ease of use um yeah you could also use I'm playing",
    "start": "1607340",
    "end": "1614120"
  },
  {
    "text": "kubernetes for example right but this comes with a lot more cost I would say in terms of of Team size and Specialists",
    "start": "1614120",
    "end": "1621500"
  },
  {
    "text": "actually maintaining this and this is something which you some early startup not necessarily have right now as the",
    "start": "1621500",
    "end": "1627980"
  },
  {
    "text": "team is growing and we're getting more and more people we have more more devops Specialists and they they can handle this but I think for for kind of the",
    "start": "1627980",
    "end": "1634640"
  },
  {
    "text": "size of the company back then um it was just a reasonable and good choice that we have this flexibility in",
    "start": "1634640",
    "end": "1641179"
  },
  {
    "text": "deploying anything we we want to just from python",
    "start": "1641179",
    "end": "1646240"
  },
  {
    "text": "um what has your experience been like like training new Engineers with Rey uh who have maybe never used it before",
    "start": "1648440",
    "end": "1655159"
  },
  {
    "text": "um good question yeah um I mean I think my answer would be it's",
    "start": "1655159",
    "end": "1662059"
  },
  {
    "text": "just one more thing to learn right um there's always tools you you have to",
    "start": "1662059",
    "end": "1668120"
  },
  {
    "text": "train Engineers in right fiction plus kubernetes there is grafana there is",
    "start": "1668120",
    "end": "1673880"
  },
  {
    "text": "and every company it's a little bit different so um it's definitely an overhead but I think it's fine right",
    "start": "1673880",
    "end": "1680240"
  },
  {
    "text": "because um well it's kind of part of very essential to our stack right and it's",
    "start": "1680240",
    "end": "1685760"
  },
  {
    "text": "very close to what Engineers also need but I think also as a company grows it will",
    "start": "1685760",
    "end": "1691580"
  },
  {
    "text": "change right because it's in a very particular part of the code base which might be just addressed by one platform",
    "start": "1691580",
    "end": "1698000"
  },
  {
    "text": "team for example which kind of encapsulate that so I think not everyone in the team has to be trained in that",
    "start": "1698000",
    "end": "1704059"
  },
  {
    "text": "and it's also a thing about kind of the devops team or the company providing the",
    "start": "1704059",
    "end": "1710480"
  },
  {
    "text": "right tooling around like how to run everything in a cluster right it's more than the SAS problem so I guess the",
    "start": "1710480",
    "end": "1716299"
  },
  {
    "text": "follow-up would be do you see your ml teams using gray in the future as well or do you see it more as like a devops",
    "start": "1716299",
    "end": "1722480"
  },
  {
    "text": "tool for you guys um I definitely also see the ml team also using it right because these models",
    "start": "1722480",
    "end": "1728480"
  },
  {
    "text": "are large um they don't want to run this on their machines and there's tons of problems sometimes with MacBook m1s and stuff",
    "start": "1728480",
    "end": "1735559"
  },
  {
    "text": "right um so yes you need to scale those right because you don't want to either have them waiting for for results for for",
    "start": "1735559",
    "end": "1743120"
  },
  {
    "text": "days ideally yeah thanks",
    "start": "1743120",
    "end": "1747220"
  },
  {
    "text": "okay thank you that was a great talk uh",
    "start": "1751159",
    "end": "1757520"
  },
  {
    "text": "question about uh troubleshooting and debugging so when you run into timeout",
    "start": "1757520",
    "end": "1763700"
  },
  {
    "text": "errors or or other errors what sort of troubleshooting diagnostic mechanisms",
    "start": "1763700",
    "end": "1769460"
  },
  {
    "text": "did you use and then similarly for logging like how did you did you use",
    "start": "1769460",
    "end": "1774679"
  },
  {
    "text": "something like splunked forward all of your logs to a central system or did you",
    "start": "1774679",
    "end": "1781159"
  },
  {
    "text": "like exec into each part to see the logs yeah can you speak more to that and then also",
    "start": "1781159",
    "end": "1787399"
  },
  {
    "text": "for chaos testing like for example in kubernetes people use istio for injecting random delays between",
    "start": "1787399",
    "end": "1792559"
  },
  {
    "text": "different microservices did you use any of that or does it enable like adding a sidecar container",
    "start": "1792559",
    "end": "1799640"
  },
  {
    "text": "to its pods um I don't know if you can speak to any of that but yeah a good question I think",
    "start": "1799640",
    "end": "1805460"
  },
  {
    "text": "most of this was set up by our devops Guru to be honest um using definitely mean the all of the",
    "start": "1805460",
    "end": "1811279"
  },
  {
    "text": "logs are forwarded to grafana so that's where you usually look at the logs we don't have to accept into anything and I",
    "start": "1811279",
    "end": "1818179"
  },
  {
    "text": "think they're just sidecar containers then right for our deployments um then we're using struct log on python",
    "start": "1818179",
    "end": "1825799"
  },
  {
    "text": "to actually make the logs more easier searchable and more transparent",
    "start": "1825799",
    "end": "1832039"
  },
  {
    "text": "um yeah that's that kind of answer your question at",
    "start": "1832039",
    "end": "1838039"
  },
  {
    "text": "okay I think we're out of time yeah also feel free to direct kubernetes",
    "start": "1838039",
    "end": "1843919"
  },
  {
    "text": "infrastructure questions with Ray directly to to me I'm kind of the the subject matter expert for that",
    "start": "1843919",
    "end": "1850100"
  },
  {
    "text": "cool thank you very much [Applause]",
    "start": "1850100",
    "end": "1855849"
  }
]