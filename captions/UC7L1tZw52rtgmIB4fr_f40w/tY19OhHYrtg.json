[
  {
    "start": "0",
    "end": "53000"
  },
  {
    "text": "hi everybody uh thanks for choosing to be here now I know this is an action-packed half hour across the rest",
    "start": "3760",
    "end": "10840"
  },
  {
    "text": "of the conference so glad to have you here um as you heard today I'm going to talk about fine-tuning llms at scale for",
    "start": "10840",
    "end": "18160"
  },
  {
    "text": "secure multi-tenant generation which is a mouthful uh but we'll go ahead and break that down as we go my name is",
    "start": "18160",
    "end": "24240"
  },
  {
    "text": "Trevor uh I've been a machine learning engineer at workday for four and a half years now um and and uh let's just go",
    "start": "24240",
    "end": "31000"
  },
  {
    "text": "ahead and dive into it uh we're going to talk a little bit about workday ml at workday specifically",
    "start": "31000",
    "end": "37840"
  },
  {
    "text": "uh we're going to talk about really the motivator for what we built here the quest that we are on I'm going to share",
    "start": "37840",
    "end": "44960"
  },
  {
    "text": "the solution that we built um and provide even some lessons that we learned uh throughout the process so",
    "start": "44960",
    "end": "50280"
  },
  {
    "text": "that you all can benefit from those as well so ml at workday you might be",
    "start": "50280",
    "end": "57359"
  },
  {
    "start": "53000",
    "end": "140000"
  },
  {
    "text": "thinking what is workday uh you know workday being a businessto business",
    "start": "57359",
    "end": "62840"
  },
  {
    "text": "company we sell to plenty of other large companies if your employer has not",
    "start": "62840",
    "end": "68159"
  },
  {
    "text": "decided that they want to own or you know use workday then you've probably not interacted with them unless maybe",
    "start": "68159",
    "end": "73759"
  },
  {
    "text": "you've applied for a lot of jobs um we actually our customers include 60% of",
    "start": "73759",
    "end": "78920"
  },
  {
    "text": "the Fortune 500 uh that gives us about 65 million users uh we process almost a",
    "start": "78920",
    "end": "85400"
  },
  {
    "text": "trillion transactions a year um and we are one of the most uh one of the largest software",
    "start": "85400",
    "end": "92079"
  },
  {
    "text": "Enterprises enterprise software companies in the world um that's just to",
    "start": "92079",
    "end": "97520"
  },
  {
    "text": "say that uh we do a lot of businessto business stuff and there are lots of places where we can invest in machine",
    "start": "97520",
    "end": "104320"
  },
  {
    "text": "learning to make everybody's day-to-day lives running their businesses easier",
    "start": "104320",
    "end": "109960"
  },
  {
    "text": "there's a lot of opportunities for us to automate tum uh that's effectively like what running a business is is TDM uh we",
    "start": "109960",
    "end": "117520"
  },
  {
    "text": "can help companies grow faster we have some statistics on uh the 60% of the Fortune 500 that R our customers have",
    "start": "117520",
    "end": "124399"
  },
  {
    "text": "grown faster than uh companies who did not adopt",
    "start": "124399",
    "end": "129560"
  },
  {
    "text": "workday uh we can simplify interfaces we can make it easier to uh perform tasks",
    "start": "129560",
    "end": "135120"
  },
  {
    "text": "uh streamline processes there's tons of places where we can invest in machine",
    "start": "135120",
    "end": "140800"
  },
  {
    "start": "140000",
    "end": "180000"
  },
  {
    "text": "learning so just a little aside about workdays tenants um you know we have a",
    "start": "140800",
    "end": "146120"
  },
  {
    "text": "tenanted architecture we have many different customers when I say the word tenant you can think customer think some",
    "start": "146120",
    "end": "152040"
  },
  {
    "text": "other business out there in the world uh that has decided to use workday to manage their Enterprise resources we",
    "start": "152040",
    "end": "159519"
  },
  {
    "text": "have on the order of tens of thousands of tenants um and an important aside is",
    "start": "159519",
    "end": "165120"
  },
  {
    "text": "that we don't own our customers data while it lives in our systems they are",
    "start": "165120",
    "end": "170239"
  },
  {
    "text": "the ones who own it which means that we are a data processor not a data owner and this has of course reflected in a",
    "start": "170239",
    "end": "176599"
  },
  {
    "text": "few ways in the architectures uh that you'll see in a little bit",
    "start": "176599",
    "end": "181640"
  },
  {
    "start": "180000",
    "end": "283000"
  },
  {
    "text": "so what's our Quest today um we're going to figure out how we at workday manage",
    "start": "181640",
    "end": "188159"
  },
  {
    "text": "Enterprise grade large language models so we're all here because large",
    "start": "188159",
    "end": "193480"
  },
  {
    "text": "language models are really hot right now and as we heard this morning in the keynote they can be kind of hard to work",
    "start": "193480",
    "end": "199120"
  },
  {
    "text": "with there's so many moving Parts the the stack is really deep it can be very",
    "start": "199120",
    "end": "204959"
  },
  {
    "text": "complex complicated however you want to look at it it can be really difficult to get started um and we were F feeling",
    "start": "204959",
    "end": "212000"
  },
  {
    "text": "this pain at workday specifically we would have teams that could take almost two weeks to just have an llm booted on",
    "start": "212000",
    "end": "218080"
  },
  {
    "text": "a cluster in a way that they can start training and many people were writing the same code over and over again",
    "start": "218080",
    "end": "223680"
  },
  {
    "text": "there's you know a ripe opportunity to consolidate and make an easy to use platform that we can share with lots of",
    "start": "223680",
    "end": "230720"
  },
  {
    "text": "people so these are our general Enterprise grade requirements um it",
    "start": "230720",
    "end": "236519"
  },
  {
    "text": "needs to be secure obviously uh it needs to be scalable because we're getting new",
    "start": "236519",
    "end": "242640"
  },
  {
    "text": "tenants all the time the data is always getting bigger it needs to be cost effective uh uh workday likes to provide",
    "start": "242640",
    "end": "250200"
  },
  {
    "text": "machine learning at the core of its product and in as many opportunities as possible with outch charging a premium",
    "start": "250200",
    "end": "257280"
  },
  {
    "text": "to our customers to use this so if we can release a new ml powered feature for every customer and it's uh viable for us",
    "start": "257280",
    "end": "265639"
  },
  {
    "text": "to do that from a cost standpoint then that's a major win for us uh and and of course easy to use we want this llm",
    "start": "265639",
    "end": "272400"
  },
  {
    "text": "platform end to end to be easy enough to use that anybody can pick it up and get going um because that means that we can",
    "start": "272400",
    "end": "278160"
  },
  {
    "text": "gener uh create more features more people can generate value for workday as a",
    "start": "278160",
    "end": "283639"
  },
  {
    "start": "283000",
    "end": "407000"
  },
  {
    "text": "company and through a bunch of initial research we realized we wanted to use Laura so I'm going to go through a quick",
    "start": "283639",
    "end": "289160"
  },
  {
    "text": "refresher on what Laura is in case you're unaware um you can see here on the left we have pre-trained weights uh",
    "start": "289160",
    "end": "296400"
  },
  {
    "text": "this would be a base model that you might pull off the shelf a llama 3 .23 billion model that just released",
    "start": "296400",
    "end": "303280"
  },
  {
    "text": "last week say well to fine-tune a model like that for some other particular use",
    "start": "303280",
    "end": "308919"
  },
  {
    "text": "case uh you might not want to fine-tune all of those parameters you know three billion parameters is still a lot you",
    "start": "308919",
    "end": "314600"
  },
  {
    "text": "know when you start looking at a 70 billion parameter it's even more so we're leveraging this technology that",
    "start": "314600",
    "end": "320840"
  },
  {
    "text": "came out of Microsoft research some time ago called low rank adapters that enable you to train just that an adapter this",
    "start": "320840",
    "end": "327680"
  },
  {
    "text": "is a separate set of Weights that can be added on top of a model uh and provide a",
    "start": "327680",
    "end": "333600"
  },
  {
    "text": "small fraction of uh the parameters and total but shift the model's Behavior",
    "start": "333600",
    "end": "339280"
  },
  {
    "text": "enough to make it better at some specific task so workday we might want to uh train a model to better write job",
    "start": "339280",
    "end": "347280"
  },
  {
    "text": "descriptions um in order to do that we can leverage a Laura adapter uh this",
    "start": "347280",
    "end": "352960"
  },
  {
    "text": "then takes the foundational language uh information from the base model and sort",
    "start": "352960",
    "end": "358240"
  },
  {
    "text": "of directs it uh toward towards generating job descriptions so uh you can see how this",
    "start": "358240",
    "end": "365199"
  },
  {
    "text": "might play out uh at the end here every single tenant that we have can have their own adapter uh with their own",
    "start": "365199",
    "end": "371520"
  },
  {
    "text": "information baked into there you know in a way that makes basically makes this large language model work specifically",
    "start": "371520",
    "end": "378360"
  },
  {
    "text": "for them and we can scale these independently with different tenants so",
    "start": "378360",
    "end": "383759"
  },
  {
    "text": "as you can imagine not all of our customers are the same size uh we have some smaller companies that use work we",
    "start": "383759",
    "end": "389960"
  },
  {
    "text": "also have some of the largest companies in the world uh with hundreds of thousands of employees and so the usage",
    "start": "389960",
    "end": "397319"
  },
  {
    "text": "of any particular model in our system is going to shift with the size of the tenant uh if we use adapters we can of",
    "start": "397319",
    "end": "403360"
  },
  {
    "text": "course scale more granularly uh to adapt here now all of this is Made Easy by VM",
    "start": "403360",
    "end": "410800"
  },
  {
    "start": "407000",
    "end": "422000"
  },
  {
    "text": "I just want to throw a quick plug in here they've been having lots of talks uh at this conference as well as well",
    "start": "410800",
    "end": "416120"
  },
  {
    "text": "definitely check them out they make uh using many different adapters dead",
    "start": "416120",
    "end": "422560"
  },
  {
    "start": "422000",
    "end": "530000"
  },
  {
    "text": "simple so our solution we call avat and if you've seen silicon value might be",
    "start": "422560",
    "end": "428560"
  },
  {
    "text": "thinking my avato uh well sorry erck this is not your avato um but we do like to have fun",
    "start": "428560",
    "end": "436680"
  },
  {
    "text": "at workday we thought that this was a a fun name to pick uh luckily avato had a successful exit so you know we were",
    "start": "436680",
    "end": "442759"
  },
  {
    "text": "setting ourselves up for Success here um our avato looks a little bit like this",
    "start": "442759",
    "end": "449080"
  },
  {
    "text": "uh conceptual level you know our users are going to prepare their data they're going to run a fine-tuning job they'll",
    "start": "449080",
    "end": "455639"
  },
  {
    "text": "then want to play uh with their model especially with large language models you may not have a really nice",
    "start": "455639",
    "end": "461720"
  },
  {
    "text": "evaluation metric you might actually have to engage with it to get a sense of how good it is um so you're going to",
    "start": "461720",
    "end": "467319"
  },
  {
    "text": "evaluate it and at some point you're going to want to promote it and you might be thinking to yourself Trevor this sounds a lot like machine learning",
    "start": "467319",
    "end": "474720"
  },
  {
    "text": "and that's the point uh you know we want this to be to feel exactly like any other machine learning somebody might do",
    "start": "474720",
    "end": "480960"
  },
  {
    "text": "get your data together which let's not forget is the most important part of any machine learning project you know",
    "start": "480960",
    "end": "486440"
  },
  {
    "text": "kickoff fine tuning if this is just as easy as you might think start fine-tuning great let's see how it goes",
    "start": "486440",
    "end": "492479"
  },
  {
    "text": "plan evaluate and move on so what we started with uh was a",
    "start": "492479",
    "end": "497560"
  },
  {
    "text": "script that uh we pulled from the ray repository um number four here",
    "start": "497560",
    "end": "502800"
  },
  {
    "text": "fine-tuning llms with deep speed uh we knew deep speed was quite capable of spreading models across multiple nodes",
    "start": "502800",
    "end": "509319"
  },
  {
    "text": "um and we were interested in being able to leverage many small cheap nodes because getting the bigger ones can be",
    "start": "509319",
    "end": "515360"
  },
  {
    "text": "very difficult as we also heard this morning um especially the hottest gpus now there's a lot of contention out",
    "start": "515360",
    "end": "521479"
  },
  {
    "text": "there so if we can find a way to stitch together a whole lot of smaller easier to acquire instances that gives us a leg",
    "start": "521479",
    "end": "529880"
  },
  {
    "text": "up um and about four to six weeks after we pulled this uh example off the shelf",
    "start": "529880",
    "end": "536480"
  },
  {
    "start": "530000",
    "end": "768000"
  },
  {
    "text": "we had something that looks like this um this is the avato architecture as a",
    "start": "536480",
    "end": "541839"
  },
  {
    "text": "whole uh so let's go ahead and break it down a little bit and see how each of these components fit together so as a",
    "start": "541839",
    "end": "549200"
  },
  {
    "text": "user um after preparing your data of course that's not a very interesting part of this picture uh so we're leaving",
    "start": "549200",
    "end": "554839"
  },
  {
    "text": "it out um you would go ahead and submit your job uh this job would land in what",
    "start": "554839",
    "end": "560680"
  },
  {
    "text": "in our avat web service this is effectively the Ingress to our system that can help to then manage all of the",
    "start": "560680",
    "end": "566480"
  },
  {
    "text": "other things that are going on within it and in this case what we're doing is we're queuing a job up into our uh",
    "start": "566480",
    "end": "572440"
  },
  {
    "text": "dagster job orchestration queuing system in the background we use dagster for a few other uh logistical things uh within",
    "start": "572440",
    "end": "580000"
  },
  {
    "text": "our ml training ecosystem so this is an easy way for us to then automate training later in a number of different",
    "start": "580000",
    "end": "586000"
  },
  {
    "text": "ways um but you can think of it here as really just a cue to make sure that we're never requesting too many",
    "start": "586000",
    "end": "592160"
  },
  {
    "text": "resources at once by only processing a certain number of jobs at a time uh next up is the actual job",
    "start": "592160",
    "end": "599399"
  },
  {
    "text": "execution itself um and this is where these individual dagster job pods will",
    "start": "599399",
    "end": "604600"
  },
  {
    "text": "pop open a ray job cluster so using Cube you can submit uh a custom resource",
    "start": "604600",
    "end": "611920"
  },
  {
    "text": "definition that describes the cluster you'd like to have to run your training you send that to kubernetes and the CU",
    "start": "611920",
    "end": "617920"
  },
  {
    "text": "operator spins up an entire cluster for you to specifically run just your job uh",
    "start": "617920",
    "end": "623959"
  },
  {
    "text": "when it's done it shuts down uh and that's it so any skill provides some nice ways to to wrap jobs for you we",
    "start": "623959",
    "end": "631120"
  },
  {
    "text": "basically just did that ourselves um so we have ways to monitor these jobs to make sure that they're running collect the logs make sure that all of this",
    "start": "631120",
    "end": "638040"
  },
  {
    "text": "stuff is easy to then find and uh debug later in case any errors occur um these",
    "start": "638040",
    "end": "645000"
  },
  {
    "text": "jobs then all individually interact with an S3 bucket um sort of the team bucket",
    "start": "645000",
    "end": "651200"
  },
  {
    "text": "where uh any particular team would put their data uh these jobs then also read",
    "start": "651200",
    "end": "656680"
  },
  {
    "text": "that data out of the bucket and then they save the artifacts at the end of the day back to the bucket um pretty",
    "start": "656680",
    "end": "663079"
  },
  {
    "text": "simple um the avat web service here also provides the ability for users to check",
    "start": "663079",
    "end": "669800"
  },
  {
    "text": "ongoing logs that we can pull from the dagster system and fetch some detailed status information that we write in a",
    "start": "669800",
    "end": "676200"
  },
  {
    "text": "custom Json blob format to S3 to make it easier for us to provide fine grain",
    "start": "676200",
    "end": "681560"
  },
  {
    "text": "information for our users on where their jobs are at any particular",
    "start": "681560",
    "end": "686920"
  },
  {
    "text": "time once fine tuning is complete of course an adapter then uh will land in the team bucket um and this means that",
    "start": "686920",
    "end": "694600"
  },
  {
    "text": "we can now we have to provide some way for our users to play with this uh since it's not feasible to have users pull up",
    "start": "694600",
    "end": "701079"
  },
  {
    "text": "their own machine boot a base model load an adapter on top of it on their own we of course built this into the system and",
    "start": "701079",
    "end": "707519"
  },
  {
    "text": "this is where we start to leverage a lot of the autoscaling capabilities of a race serve cluster um so users submit a",
    "start": "707519",
    "end": "714279"
  },
  {
    "text": "request to the avat web service since they had really like to run my model over here we can detect which base model",
    "start": "714279",
    "end": "721440"
  },
  {
    "text": "that requires we can boot that base model up we can grab their adapter out of S3 and we can load them all together",
    "start": "721440",
    "end": "727800"
  },
  {
    "text": "to provide them with whatever they want um I'll talk about this in a little bit but uh the nice part is after they're",
    "start": "727800",
    "end": "735959"
  },
  {
    "text": "done using it we can detect that they haven't sent any requests in about half an hour and we can shut it off and we can send those gpus back out into the",
    "start": "735959",
    "end": "742639"
  },
  {
    "text": "cloud because we're not using them anymore finally when a user decides that",
    "start": "742639",
    "end": "748839"
  },
  {
    "text": "it's time to ship uh it's very easy they can just send off a request and say hey that model that I had it's ready to go",
    "start": "748839",
    "end": "755199"
  },
  {
    "text": "to production um we can package it up appropriately and pass it off to our production model store that will then",
    "start": "755199",
    "end": "761600"
  },
  {
    "text": "make sure that it gets delivered to the production system uh for use uh by our",
    "start": "761600",
    "end": "768560"
  },
  {
    "start": "768000",
    "end": "915000"
  },
  {
    "text": "customers so how does this solution then help us to provide security for our",
    "start": "768560",
    "end": "775320"
  },
  {
    "text": "tenants the first way is how we've structured each individual deployment so",
    "start": "775320",
    "end": "781800"
  },
  {
    "text": "uh as you can see here every deployment here runs in a kubernetes namespace that is specific for the team that is going",
    "start": "781800",
    "end": "789399"
  },
  {
    "text": "to be leveraging the compute within there now any of our users here are able to use a role that's been defined to",
    "start": "789399",
    "end": "797480"
  },
  {
    "text": "access their data in their team bucket so we propagate that role to the",
    "start": "797480",
    "end": "803399"
  },
  {
    "text": "kubernetes namespace and all of the resources that run in that namespace as well but then you might say okay well if",
    "start": "803399",
    "end": "809399"
  },
  {
    "text": "the user living outside of this name space how do they then how do you make sure that they only have access to the",
    "start": "809399",
    "end": "814920"
  },
  {
    "text": "name space they should um in this case we leverage aws's private certificate",
    "start": "814920",
    "end": "820639"
  },
  {
    "text": "Authority uh which allows the user to use their credentials to acquire a certificate to prove that they have they",
    "start": "820639",
    "end": "828279"
  },
  {
    "text": "should have access to this uh cluster and then the cluster can validate their credentials their certificate against",
    "start": "828279",
    "end": "834720"
  },
  {
    "text": "the certificate Authority um at the Ingress which is just an application load balance",
    "start": "834720",
    "end": "839959"
  },
  {
    "text": "um and now users don't have access to clusters they shouldn't um because those",
    "start": "839959",
    "end": "845160"
  },
  {
    "text": "clusters then may have access to data that the user shouldn't and this sort of separates it all now with a little bit",
    "start": "845160",
    "end": "851440"
  },
  {
    "text": "of uh Network isolation within the kubernetes cluster itself manages to make sure that we don't have any uh",
    "start": "851440",
    "end": "859399"
  },
  {
    "text": "bleed between any of the particular roles uh in our",
    "start": "859399",
    "end": "865240"
  },
  {
    "text": "system and inference time we can prevent leakage uh between customers by having trained a customer specific lore adapter",
    "start": "865240",
    "end": "873959"
  },
  {
    "text": "um so you've probably heard Tales of Samsung employees putting sensitive documents into chat GPT and somebody",
    "start": "873959",
    "end": "879920"
  },
  {
    "text": "else effectively getting them right back out uh this prevents all of that from happening because any particular tenants",
    "start": "879920",
    "end": "886440"
  },
  {
    "text": "information is only in their adapter the base model knows nothing about their their specific data so in this way we",
    "start": "886440",
    "end": "893440"
  },
  {
    "text": "can provide guarantees that uh for any particular tenant we know that this request is coming in for that tenant we",
    "start": "893440",
    "end": "899759"
  },
  {
    "text": "can load specifically that tenanted adapter and we don't have to worry about any transfer of information across um",
    "start": "899759",
    "end": "906800"
  },
  {
    "text": "which significantly reduces stress uh when it comes to training these large language models for a lot of the things",
    "start": "906800",
    "end": "912480"
  },
  {
    "text": "that we uh use them for workday okay how does this then scale",
    "start": "912480",
    "end": "918800"
  },
  {
    "start": "915000",
    "end": "1054000"
  },
  {
    "text": "well I mean we're using Ray so obviously that makes it pretty easy um the cluster",
    "start": "918800",
    "end": "923839"
  },
  {
    "text": "size scales seamlessly um cubra makes this very easy uh both for scaling the",
    "start": "923839",
    "end": "930240"
  },
  {
    "text": "cluster and for scaling the number of clusters that we're using um you can see",
    "start": "930240",
    "end": "935920"
  },
  {
    "text": "here I have an image that I pulled from Argo where we have a jobs the head note",
    "start": "935920",
    "end": "941360"
  },
  {
    "text": "of a cluster and it's pulling in four more machines that it needs to do its job uh and we don't have to touch any of",
    "start": "941360",
    "end": "947959"
  },
  {
    "text": "this at all completely seamless we've found that experience to be just super",
    "start": "947959",
    "end": "954399"
  },
  {
    "text": "slick and the number of deployments that we have is Made Easy by iCal infrastructure is code uh flavored",
    "start": "954399",
    "end": "961720"
  },
  {
    "text": "things so we use terraform Argo you can use whatever you want uh to repeat this deployment as many times as you want so",
    "start": "961720",
    "end": "967839"
  },
  {
    "text": "we have any number of teams and any number of different regions uh like I said we operate around the world a lot",
    "start": "967839",
    "end": "974120"
  },
  {
    "text": "of times the data that we're dealing with needs to maintain residency in a particular region EU say um and some",
    "start": "974120",
    "end": "982639"
  },
  {
    "text": "teams may not want some deployments in some regions and all of this is very easy for us to manage because the entire",
    "start": "982639",
    "end": "988160"
  },
  {
    "text": "deployment is self-contained and can scale on its own we currently have about 50 of these",
    "start": "988160",
    "end": "994560"
  },
  {
    "text": "in production uh maintaining them has not been a headache uh so we think that",
    "start": "994560",
    "end": "999720"
  },
  {
    "text": "this is this is working out pretty well for what we need uh we also find that the deployment",
    "start": "999720",
    "end": "1006199"
  },
  {
    "text": "patterns that are made possible by this architecture are very flexible so on the left here you can see what it might look",
    "start": "1006199",
    "end": "1012319"
  },
  {
    "text": "like for development we can run the entire stack on a single machine uh this makes it very easy to iterate quickly uh",
    "start": "1012319",
    "end": "1018880"
  },
  {
    "text": "in a very simple uh setup there's also the option and this the one in the middle is effectively describing what I",
    "start": "1018880",
    "end": "1025959"
  },
  {
    "text": "was showing you before where many users have access to one Consolidated uh but autoscaling deployment um but because",
    "start": "1025959",
    "end": "1033438"
  },
  {
    "text": "it's so easy to deploy it's also easy for us to stand up personal clusters if people want to really dig deep into the",
    "start": "1033439",
    "end": "1040160"
  },
  {
    "text": "code build out different you know approaches to fine tuning than the ones that we've already created um or",
    "start": "1040160",
    "end": "1046240"
  },
  {
    "text": "entirely different job types alog together then they have an easy way of doing that also backed by",
    "start": "1046240",
    "end": "1052440"
  },
  {
    "text": "autoscaling uh if they desire so we've alluded to some of this stuff so far but uh let's dig into what",
    "start": "1052440",
    "end": "1059600"
  },
  {
    "start": "1054000",
    "end": "1207000"
  },
  {
    "text": "makes this cost effective as well uh the big one is that we can scale to meet",
    "start": "1059600",
    "end": "1065280"
  },
  {
    "text": "demand and we can scale back down again um so the whole system is designed to",
    "start": "1065280",
    "end": "1070720"
  },
  {
    "text": "idle with about two to three CPUs maybe and that's May a little",
    "start": "1070720",
    "end": "1076159"
  },
  {
    "text": "over-provisioned um so all of the expensive machines GPU machines all of that we we don't need any of them to sit",
    "start": "1076159",
    "end": "1083520"
  },
  {
    "text": "there and wait for your user to submit a job once a user submits a job we can scale up you can see here uh the the",
    "start": "1083520",
    "end": "1089799"
  },
  {
    "text": "blue dotted line is showing total cluster capacity uh for one of the Clusters scale up seamlessly to 32 gpus",
    "start": "1089799",
    "end": "1097120"
  },
  {
    "text": "at times um or even more uh and then eventually scale back down when we realize all of the work uh has been done",
    "start": "1097120",
    "end": "1104280"
  },
  {
    "text": "and we don't need them anymore uh when it comes to Laura uh",
    "start": "1104280",
    "end": "1110280"
  },
  {
    "text": "this is a little bit more of a nuanced uh cost savings uh it makes training",
    "start": "1110280",
    "end": "1116919"
  },
  {
    "text": "considerably cheaper um like I said before Laura is a fraction of the tunable parameters of a full uh model so",
    "start": "1116919",
    "end": "1124400"
  },
  {
    "text": "you can speed up your job in multiple ways you need fewer gpus to be able to",
    "start": "1124400",
    "end": "1129559"
  },
  {
    "text": "hold all of the different states throughout training um and it doesn't",
    "start": "1129559",
    "end": "1134760"
  },
  {
    "text": "take as long because you have fewer weights that you need to update every single time that you make a",
    "start": "1134760",
    "end": "1140559"
  },
  {
    "text": "pass uh when it comes to inference time this also amortizes the cost of a base model so like I said before we have",
    "start": "1140559",
    "end": "1147240"
  },
  {
    "text": "differ sized tenants some tenants just aren't big enough to saturate a base",
    "start": "1147240",
    "end": "1152400"
  },
  {
    "text": "model so if we were doing a full parameter fine tuning on a base model there's no way we could make full use of",
    "start": "1152400",
    "end": "1158240"
  },
  {
    "text": "that model with for every single tenant that we have um this allows us to group",
    "start": "1158240",
    "end": "1163280"
  },
  {
    "text": "tenants together on top of Base models in a way that we can actually see the economy of scale by grouping them",
    "start": "1163280",
    "end": "1170880"
  },
  {
    "text": "together now of course nothing's a free launch um the uh effect of using lur",
    "start": "1170880",
    "end": "1178120"
  },
  {
    "text": "adapters will decrease uh your throughput or increase your latency at",
    "start": "1178120",
    "end": "1183280"
  },
  {
    "text": "about 10 to 20% um this is a even a disclaimer uh on",
    "start": "1183280",
    "end": "1188480"
  },
  {
    "text": "the any scale platform as well they mention that uh that they see this same",
    "start": "1188480",
    "end": "1194080"
  },
  {
    "text": "outcome um but we find that the additional cost here",
    "start": "1194080",
    "end": "1199480"
  },
  {
    "text": "uh really is far outweighed by the benefits that we get from tenant isolation and cheaper",
    "start": "1199480",
    "end": "1207080"
  },
  {
    "start": "1207000",
    "end": "1274000"
  },
  {
    "text": "training and finally easy to use um this is what it might look like to use the",
    "start": "1207440",
    "end": "1213679"
  },
  {
    "text": "aatta platform we publish a python SDK or client Library uh you can simply",
    "start": "1213679",
    "end": "1219799"
  },
  {
    "text": "import a few uh nice classes and build yourself a uh a training job out of this",
    "start": "1219799",
    "end": "1226400"
  },
  {
    "text": "you submit the data that you'd like to train it on you can IFI a you know an updatable template to change you know",
    "start": "1226400",
    "end": "1232640"
  },
  {
    "text": "maybe you want to train based on different prompts uh maybe you want to train with different base models you simply change your configuration and",
    "start": "1232640",
    "end": "1238640"
  },
  {
    "text": "ship this off um but we also subscribe to this concept of progressive disclosure in that you know it should be",
    "start": "1238640",
    "end": "1245760"
  },
  {
    "text": "really easy to get started but if you feel like you you know need a little more control it's all still there so",
    "start": "1245760",
    "end": "1252320"
  },
  {
    "text": "here you can see a much more complicated configuration where you can submit uh",
    "start": "1252320",
    "end": "1257480"
  },
  {
    "text": "you can decide how long you want want your max sequence length to be pick special tokens you can describe everything you want about your trainer",
    "start": "1257480",
    "end": "1263640"
  },
  {
    "text": "your your cluster size your there's a ton of configurations available to you here if you felt like you needed to",
    "start": "1263640",
    "end": "1270080"
  },
  {
    "text": "Although our sensible defaults do pretty well for most of our users so since we've got plenty of time",
    "start": "1270080",
    "end": "1277080"
  },
  {
    "start": "1274000",
    "end": "1461000"
  },
  {
    "text": "here let's go ahead and talk through some of the lessons that we learned along the way um in particular they boil",
    "start": "1277080",
    "end": "1282559"
  },
  {
    "text": "down to one thing and that is that this wouldn't have been a issue with per job",
    "start": "1282559",
    "end": "1288200"
  },
  {
    "text": "cluster that's a quote from me my team of course is was sick of hearing this for quite a while um I remember being",
    "start": "1288200",
    "end": "1294720"
  },
  {
    "text": "here a year ago and hearing per job clusters were the way to go um but when",
    "start": "1294720",
    "end": "1300240"
  },
  {
    "text": "I got started on building this project it was more difficult you know I was going to require kubernetes I would",
    "start": "1300240",
    "end": "1305559"
  },
  {
    "text": "require Cube uh Cube Ray at hand in order to spin up a per job cluster and",
    "start": "1305559",
    "end": "1312320"
  },
  {
    "text": "developing on a single machine was just a lot easier given the infrastructure that we had at the time um so I started",
    "start": "1312320",
    "end": "1318120"
  },
  {
    "text": "with that one single autoscaling cluster uh and here I'm going to share a quick illustration of one of the ways in which",
    "start": "1318120",
    "end": "1324320"
  },
  {
    "text": "we found that this was suboptimal so that maybe you can remember why it's important uh to go ahead and start with",
    "start": "1324320",
    "end": "1330360"
  },
  {
    "text": "per job clusters so imagine this you have a job you'd like to run it on six",
    "start": "1330360",
    "end": "1335440"
  },
  {
    "text": "different machines uh a job just finished you have two of those machines they're in your",
    "start": "1335440",
    "end": "1341919"
  },
  {
    "text": "cluster they're healthy they're ready to go reporting great this next time step the uh",
    "start": "1341919",
    "end": "1349000"
  },
  {
    "text": "kubernetes Auto scaler has managed to bring three more machines in they're getting booted up added to the cluster",
    "start": "1349000",
    "end": "1355200"
  },
  {
    "text": "the two that you had before are still hanging on just fine but that last one it's been a little harder to Source it",
    "start": "1355200",
    "end": "1361480"
  },
  {
    "text": "just isn't quite there yet now this next step those three",
    "start": "1361480",
    "end": "1367120"
  },
  {
    "text": "machines that were just added they're reporting healthy we finally found that sixth one everything's great except the",
    "start": "1367120",
    "end": "1373080"
  },
  {
    "text": "first two Ry has realized have been sitting idle for a little while and",
    "start": "1373080",
    "end": "1378159"
  },
  {
    "text": "because you have the auto scaling settings say after it's been idle for some number of minutes let's go ahead",
    "start": "1378159",
    "end": "1384200"
  },
  {
    "text": "and kick this note out well now they're getting kicked out of the cluster luckily they're still part of kubernetes",
    "start": "1384200",
    "end": "1390000"
  },
  {
    "text": "so it doesn't take them long to come back but the problem is they have a little bit of time that they take to",
    "start": "1390000",
    "end": "1395400"
  },
  {
    "text": "boot back up again and this can stick you in an infinite Loop of trying to",
    "start": "1395400",
    "end": "1401600"
  },
  {
    "text": "balance a whole lot of machines um at one time so especially if you're dealing with 16 nodes 32 nodes like I said",
    "start": "1401600",
    "end": "1408840"
  },
  {
    "text": "before we like to stitch a bunch of cheap instances together um you know single GPU nodes we can train across 32",
    "start": "1408840",
    "end": "1415840"
  },
  {
    "text": "of them but not if we can't get them all started at the same time um so you have to change your autoscaler settings a",
    "start": "1415840",
    "end": "1421880"
  },
  {
    "text": "little bit say hey a little longer of a timeout before we go ahead and kick them out of the cluster um and this gives us",
    "start": "1421880",
    "end": "1427679"
  },
  {
    "text": "more time to make sure that everything aligns um there's also a few other nuances here uh that I'd be happy to",
    "start": "1427679",
    "end": "1434480"
  },
  {
    "text": "talk about if people are curious about those um but so far what we've covered",
    "start": "1434480",
    "end": "1439679"
  },
  {
    "text": "is what ml looks like at workday to some extent um how we put together a system",
    "start": "1439679",
    "end": "1445000"
  },
  {
    "text": "that helps us to support Enterprise grade large language models that is cheap to run secure in both the training",
    "start": "1445000",
    "end": "1452480"
  },
  {
    "text": "and the inference side and scalable in many d uh dimensions and how we learned",
    "start": "1452480",
    "end": "1458080"
  },
  {
    "text": "the hard way that Pur job clusters are the way to go so thank you very much and we'll go ahead and open it up for",
    "start": "1458080",
    "end": "1463559"
  },
  {
    "start": "1461000",
    "end": "1850000"
  },
  {
    "text": "questions [Applause]",
    "start": "1463559",
    "end": "1471679"
  },
  {
    "text": "question you",
    "start": "1474399",
    "end": "1477000"
  },
  {
    "text": "said um what do you mean by that kubernetes oh oh providers yeah yeah so",
    "start": "1481039",
    "end": "1487880"
  },
  {
    "text": "uh the question was are there any specific kubernetes providers we using um we chose the kubernetes foundation",
    "start": "1487880",
    "end": "1494559"
  },
  {
    "text": "because of the fact that this can run on any kubernetes provider specifically we use eks um but that is",
    "start": "1494559",
    "end": "1501559"
  },
  {
    "text": "effectively uh here neither here nor there um one of the major benefits with",
    "start": "1501559",
    "end": "1506760"
  },
  {
    "text": "this is that uh we could eventually adapt all of this code to run on tpus we",
    "start": "1506760",
    "end": "1512320"
  },
  {
    "text": "could leverage Google Cloud um we could put it onto Azure just as easily everybody provides kubernetes it's a",
    "start": "1512320",
    "end": "1518720"
  },
  {
    "text": "great way to to make whatever it is that you're building Cloud agnostic we'll",
    "start": "1518720",
    "end": "1524760"
  },
  {
    "text": "say Yes um for the Laura um approach is",
    "start": "1525200",
    "end": "1530840"
  },
  {
    "text": "there limitation about what's the size of the data for each one that it can hold yeah so there's some you know back",
    "start": "1530840",
    "end": "1538320"
  },
  {
    "text": "of the envelope calculations there people have a few rules of thumb we find that if you have on the order of a th to",
    "start": "1538320",
    "end": "1545559"
  },
  {
    "text": "3,000 samples um and you run you know two to three Epoch o o over your",
    "start": "1545559",
    "end": "1551679"
  },
  {
    "text": "training setup you can do pretty well um there because there aren't many uh parameters to tune uh find that they",
    "start": "1551679",
    "end": "1558799"
  },
  {
    "text": "converge relatively quickly and generally speaking uh don't require a ton of data to actually adapt to the um",
    "start": "1558799",
    "end": "1567200"
  },
  {
    "text": "task that you're trying to tune them for um we've done a bunch of experiments on",
    "start": "1567200",
    "end": "1573760"
  },
  {
    "text": "you know Laura versus full parameter fine-tuning there's actually been a bunch of any scale blog posts about this",
    "start": "1573760",
    "end": "1578840"
  },
  {
    "text": "and in most cases Laura is sufficient um unless you're really trying to adapt a",
    "start": "1578840",
    "end": "1586000"
  },
  {
    "text": "model like deeply in its ability to reason about something might you need",
    "start": "1586000",
    "end": "1591159"
  },
  {
    "text": "full parameter fine tuning uh but again as as generally consumers of a lot of",
    "start": "1591159",
    "end": "1596320"
  },
  {
    "text": "this Tech uh and applying it to these specific tasks that we need we find that",
    "start": "1596320",
    "end": "1602039"
  },
  {
    "text": "many of the other people in the world meta you know will release a new model with considerably better capabilities",
    "start": "1602039",
    "end": "1608080"
  },
  {
    "text": "before we would be able to find two in one ourselves on our own internal data and so having the ability to you know",
    "start": "1608080",
    "end": "1614919"
  },
  {
    "text": "refine any one of these particular models for a specific task is uh proving",
    "start": "1614919",
    "end": "1620000"
  },
  {
    "text": "to be enough for what we need to do yeah",
    "start": "1620000",
    "end": "1628120"
  },
  {
    "text": "found model yeah so the question is whether or not you know one base model can rule",
    "start": "1638039",
    "end": "1645120"
  },
  {
    "text": "them all basically um we've had a strong gravitation towards the Llama 3 Series",
    "start": "1645120",
    "end": "1652520"
  },
  {
    "text": "the 8B 8 million instruct models seem to provide us a pretty solid foundation for",
    "start": "1652520",
    "end": "1657799"
  },
  {
    "text": "most of the things that we care to do um we also you know need to run big",
    "start": "1657799",
    "end": "1662880"
  },
  {
    "text": "embedding workloads or things like that in which case uh you know there may be additional other models that are more",
    "start": "1662880",
    "end": "1669519"
  },
  {
    "text": "spe suited for that specific task but for most of the things that we want we find that the Llama 3 Series even the",
    "start": "1669519",
    "end": "1676120"
  },
  {
    "text": "small ones if they can't do it out of the that can absolutely do it with fine",
    "start": "1676120",
    "end": "1681760"
  },
  {
    "text": "tuning yeah so you started to talk talking about your data processing dat",
    "start": "1681960",
    "end": "1688200"
  },
  {
    "text": "how do you approach model yeah so uh we pulled the base",
    "start": "1688200",
    "end": "1694559"
  },
  {
    "text": "models off the shelf and then we just train these adapters um when it comes time to or I guess when",
    "start": "1694559",
    "end": "1702880"
  },
  {
    "text": "we're sort of in production uh we will generally retrain",
    "start": "1702880",
    "end": "1707919"
  },
  {
    "text": "the these things every two weeks or so um because that's sort of our keep data fresh expiration policy limit uh which",
    "start": "1707919",
    "end": "1716440"
  },
  {
    "text": "means that we will expire uh an adapter we'll get new data that we can extract",
    "start": "1716440",
    "end": "1721679"
  },
  {
    "text": "out of the tenant uh to sort of refresh the The Source um we'll run that through",
    "start": "1721679",
    "end": "1726799"
  },
  {
    "text": "training to pop out a new adapter and then we can ship that adapter into production uh and then um just sort of",
    "start": "1726799",
    "end": "1733799"
  },
  {
    "text": "keep that cycle running uh week over week there separate adapters for each",
    "start": "1733799",
    "end": "1740279"
  },
  {
    "text": "customer yeah yep yeah yeah so that's that's where it really starts to scale",
    "start": "1740279",
    "end": "1745320"
  },
  {
    "text": "out this is where the um uh yes so separate adapters for each customer uh",
    "start": "1745320",
    "end": "1751440"
  },
  {
    "text": "having that dagster queuing orchestration layer really helps us to both kick off jobs when we detect new",
    "start": "1751440",
    "end": "1758279"
  },
  {
    "text": "data new tenants new whatever um and also make sure that we aren't trying to do too many of them at the same time",
    "start": "1758279",
    "end": "1764399"
  },
  {
    "text": "because that may ask just be asking for too many resources from the CL L at any one point so yeah I think we had a",
    "start": "1764399",
    "end": "1771799"
  },
  {
    "text": "question over here first sorry could you share more details about your observable stack and some of the hard Lessons",
    "start": "1771799",
    "end": "1780399"
  },
  {
    "text": "Learned yeah sure so um at workday we run an in-house uh Prometheus",
    "start": "1780399",
    "end": "1786279"
  },
  {
    "text": "observability stack uh Ray and uh well Ray specifically ships with excellent",
    "start": "1786279",
    "end": "1792880"
  },
  {
    "text": "Prometheus interoperability um so we basically just",
    "start": "1792880",
    "end": "1798159"
  },
  {
    "text": "tell Prometheus to scrape the metrics from the cluster and they all show up Ray provides dashboards and grafana that",
    "start": "1798159",
    "end": "1805080"
  },
  {
    "text": "give you good observation of race serve um compute resource usage all of that",
    "start": "1805080",
    "end": "1810200"
  },
  {
    "text": "kind of stuff uh so we actually had a really easy time getting that set up yeah and then this prev question on",
    "start": "1810200",
    "end": "1818399"
  },
  {
    "text": "tuning the model using your latest data you consider rag implementation to get",
    "start": "1818399",
    "end": "1824320"
  },
  {
    "text": "yeah what are the challenges the question is rag uh that's a whole different presentation uh we do leverage",
    "start": "1824320",
    "end": "1830200"
  },
  {
    "text": "rag for a number of our uh features as well um but of course you know we find",
    "start": "1830200",
    "end": "1836799"
  },
  {
    "text": "fine-tuning fine tuning and rag solve different problems so if you want to talk more about that we can do that uh because I think we're at",
    "start": "1836799",
    "end": "1843960"
  },
  {
    "text": "time so thank you everybody [Applause]",
    "start": "1843960",
    "end": "1851229"
  }
]