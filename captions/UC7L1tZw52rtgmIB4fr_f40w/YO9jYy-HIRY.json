[
  {
    "text": "so as you guys saw in the morning we we",
    "start": "5000",
    "end": "8099"
  },
  {
    "text": "wanted to start actually building LM",
    "start": "8099",
    "end": "10080"
  },
  {
    "text": "applications ourselves instead of as",
    "start": "10080",
    "end": "11940"
  },
  {
    "text": "opposed to only kind of focusing on the",
    "start": "11940",
    "end": "14280"
  },
  {
    "text": "infra and making it kind of cheaper and",
    "start": "14280",
    "end": "15839"
  },
  {
    "text": "faster uh this way we kind of actually",
    "start": "15839",
    "end": "17760"
  },
  {
    "text": "experience the problems that you guys",
    "start": "17760",
    "end": "19260"
  },
  {
    "text": "will hopefully a lot sooner and actually",
    "start": "19260",
    "end": "21060"
  },
  {
    "text": "make the whole experience of products a",
    "start": "21060",
    "end": "22680"
  },
  {
    "text": "lot better so the first thing that we",
    "start": "22680",
    "end": "24300"
  },
  {
    "text": "did was build a rag application",
    "start": "24300",
    "end": "26760"
  },
  {
    "text": "and this is like a canonical use case",
    "start": "26760",
    "end": "28740"
  },
  {
    "text": "right everybody has their own data a lot",
    "start": "28740",
    "end": "30720"
  },
  {
    "text": "of tech companies have their own",
    "start": "30720",
    "end": "31679"
  },
  {
    "text": "documentation so this is usually the",
    "start": "31679",
    "end": "33480"
  },
  {
    "text": "first use case that a lot of teams",
    "start": "33480",
    "end": "35280"
  },
  {
    "text": "gravitate towards just making it easier",
    "start": "35280",
    "end": "37260"
  },
  {
    "text": "for people to work with their products",
    "start": "37260",
    "end": "38700"
  },
  {
    "text": "so we decided to do the same and you",
    "start": "38700",
    "end": "41520"
  },
  {
    "text": "know our Ray as many of you know does a",
    "start": "41520",
    "end": "44160"
  },
  {
    "text": "lot of different things so for us it was",
    "start": "44160",
    "end": "46440"
  },
  {
    "text": "very useful to want to build something",
    "start": "46440",
    "end": "48120"
  },
  {
    "text": "like this on top of all the different",
    "start": "48120",
    "end": "49640"
  },
  {
    "text": "capabilities and functions that Ray can",
    "start": "49640",
    "end": "51960"
  },
  {
    "text": "do and then just help developers do",
    "start": "51960",
    "end": "54719"
  },
  {
    "text": "things a lot faster and better as well",
    "start": "54719",
    "end": "57120"
  },
  {
    "text": "to that topic I want to emphasize the",
    "start": "57120",
    "end": "59219"
  },
  {
    "text": "two values you're going to have when you",
    "start": "59219",
    "end": "60780"
  },
  {
    "text": "build such an application are the",
    "start": "60780",
    "end": "62699"
  },
  {
    "text": "underlying documents and there there has",
    "start": "62699",
    "end": "64920"
  },
  {
    "text": "been really great work by the right",
    "start": "64920",
    "end": "66180"
  },
  {
    "text": "documentation team including Angelina",
    "start": "66180",
    "end": "68040"
  },
  {
    "text": "and others and then the other one is",
    "start": "68040",
    "end": "70500"
  },
  {
    "text": "like users and questions from users so",
    "start": "70500",
    "end": "73080"
  },
  {
    "text": "like if you have these two things then",
    "start": "73080",
    "end": "74820"
  },
  {
    "text": "that makes it much easier to build this",
    "start": "74820",
    "end": "76140"
  },
  {
    "text": "kind of application yeah",
    "start": "76140",
    "end": "77580"
  },
  {
    "text": "actually I just want to quick see a",
    "start": "77580",
    "end": "79320"
  },
  {
    "text": "quick show of handset how many folks",
    "start": "79320",
    "end": "80700"
  },
  {
    "text": "internally or I guess externally have",
    "start": "80700",
    "end": "82799"
  },
  {
    "text": "started building rag-based applications",
    "start": "82799",
    "end": "84780"
  },
  {
    "text": "at work okay well that's a lot of you",
    "start": "84780",
    "end": "86939"
  },
  {
    "text": "okay so I'd love to hear like everyone",
    "start": "86939",
    "end": "88860"
  },
  {
    "text": "else's insights tonight as well",
    "start": "88860",
    "end": "91140"
  },
  {
    "text": "um the things that we will share are",
    "start": "91140",
    "end": "92820"
  },
  {
    "text": "very empirically driven so if you find",
    "start": "92820",
    "end": "94920"
  },
  {
    "text": "you found a different insight for",
    "start": "94920",
    "end": "96720"
  },
  {
    "text": "example like chunking size that we'll",
    "start": "96720",
    "end": "98280"
  },
  {
    "text": "talk about",
    "start": "98280",
    "end": "99299"
  },
  {
    "text": "please definitely share those kinds of",
    "start": "99299",
    "end": "101100"
  },
  {
    "text": "things because I we're very early uh as",
    "start": "101100",
    "end": "103799"
  },
  {
    "text": "uh kind of a community in the space so",
    "start": "103799",
    "end": "106680"
  },
  {
    "text": "it'd be great to hear kind of different",
    "start": "106680",
    "end": "108299"
  },
  {
    "text": "people's take on all this",
    "start": "108299",
    "end": "110399"
  },
  {
    "text": "um so obviously kind of starting simple",
    "start": "110399",
    "end": "112140"
  },
  {
    "text": "we started our whole application with",
    "start": "112140",
    "end": "114119"
  },
  {
    "text": "just seeing how will a base LM do we try",
    "start": "114119",
    "end": "116579"
  },
  {
    "text": "to do with gpd4 llama 7B 70b and we",
    "start": "116579",
    "end": "119579"
  },
  {
    "text": "would just ask a question and very",
    "start": "119579",
    "end": "121079"
  },
  {
    "text": "quickly you know we'd realize that these",
    "start": "121079",
    "end": "122939"
  },
  {
    "text": "models have no context or very little",
    "start": "122939",
    "end": "124320"
  },
  {
    "text": "context of how things work and if they",
    "start": "124320",
    "end": "127020"
  },
  {
    "text": "did it'll be outdated right uh September",
    "start": "127020",
    "end": "129360"
  },
  {
    "text": "21 sometimes and Rey looked uh I think",
    "start": "129360",
    "end": "131459"
  },
  {
    "text": "very different back then if it even had",
    "start": "131459",
    "end": "133379"
  },
  {
    "text": "access to it so we very quickly got to",
    "start": "133379",
    "end": "136200"
  },
  {
    "text": "actually building the rag app this is",
    "start": "136200",
    "end": "138180"
  },
  {
    "text": "kind of like the high level overview",
    "start": "138180",
    "end": "139739"
  },
  {
    "text": "here",
    "start": "139739",
    "end": "141180"
  },
  {
    "text": "um but this is the forward pass once you",
    "start": "141180",
    "end": "143160"
  },
  {
    "text": "have a query so assume the vector",
    "start": "143160",
    "end": "144480"
  },
  {
    "text": "database is already made we'll talk",
    "start": "144480",
    "end": "146040"
  },
  {
    "text": "about what that looks like in a second",
    "start": "146040",
    "end": "147180"
  },
  {
    "text": "but somebody asked the question the",
    "start": "147180",
    "end": "149340"
  },
  {
    "text": "query gets embedded by an embedding",
    "start": "149340",
    "end": "150959"
  },
  {
    "text": "model you have a couple options there",
    "start": "150959",
    "end": "152280"
  },
  {
    "text": "then that gets passed into a vector",
    "start": "152280",
    "end": "154260"
  },
  {
    "text": "database and now you can have a couple",
    "start": "154260",
    "end": "156900"
  },
  {
    "text": "different options for how you calculate",
    "start": "156900",
    "end": "158640"
  },
  {
    "text": "distance but that embedded query is now",
    "start": "158640",
    "end": "160920"
  },
  {
    "text": "used to fetch the top K contexts you",
    "start": "160920",
    "end": "163440"
  },
  {
    "text": "have options for how many top case as",
    "start": "163440",
    "end": "165239"
  },
  {
    "text": "well once you get those contexts you can",
    "start": "165239",
    "end": "167640"
  },
  {
    "text": "now feed in both the text from those",
    "start": "167640",
    "end": "169440"
  },
  {
    "text": "contacts and the text from the query",
    "start": "169440",
    "end": "170819"
  },
  {
    "text": "both into the LM now you've augmented",
    "start": "170819",
    "end": "173459"
  },
  {
    "text": "the base LM with this additional context",
    "start": "173459",
    "end": "175260"
  },
  {
    "text": "to be able to hopefully generate a",
    "start": "175260",
    "end": "177480"
  },
  {
    "text": "correct response",
    "start": "177480",
    "end": "180060"
  },
  {
    "text": "the actual Vector database piece here",
    "start": "180060",
    "end": "183860"
  },
  {
    "text": "simplifying we'll zoom into each of",
    "start": "183860",
    "end": "185700"
  },
  {
    "text": "these but basically we have a bunch of",
    "start": "185700",
    "end": "187200"
  },
  {
    "text": "data sources so we started with our Ray",
    "start": "187200",
    "end": "189000"
  },
  {
    "text": "documentation and then we wanted to be",
    "start": "189000",
    "end": "191819"
  },
  {
    "text": "able to load them so this is very",
    "start": "191819",
    "end": "193140"
  },
  {
    "text": "similar to what we did this morning and",
    "start": "193140",
    "end": "195360"
  },
  {
    "text": "this is the first kind of Step where we",
    "start": "195360",
    "end": "197640"
  },
  {
    "text": "started to get a little experimental the",
    "start": "197640",
    "end": "200340"
  },
  {
    "text": "actual like chunking right how do we",
    "start": "200340",
    "end": "201720"
  },
  {
    "text": "want to represent our data",
    "start": "201720",
    "end": "203420"
  },
  {
    "text": "and maybe I'll let Philip talk about the",
    "start": "203420",
    "end": "206099"
  },
  {
    "text": "different strategies we tried but the",
    "start": "206099",
    "end": "208200"
  },
  {
    "text": "thing the naive thing that everyone does",
    "start": "208200",
    "end": "209580"
  },
  {
    "text": "is just randomly chunk this right I want",
    "start": "209580",
    "end": "211379"
  },
  {
    "text": "to just set chunk size 100 or 300 chunk",
    "start": "211379",
    "end": "214319"
  },
  {
    "text": "overlap 50 and just go through all of my",
    "start": "214319",
    "end": "217140"
  },
  {
    "text": "different documents but that that starts",
    "start": "217140",
    "end": "219780"
  },
  {
    "text": "to not be as effective so we started",
    "start": "219780",
    "end": "222420"
  },
  {
    "text": "thinking about a lot of other ways to",
    "start": "222420",
    "end": "223739"
  },
  {
    "text": "more efficiently chunk the data so one",
    "start": "223739",
    "end": "226620"
  },
  {
    "text": "thing we did then was to use the",
    "start": "226620",
    "end": "228780"
  },
  {
    "text": "sections of the HTML document I would",
    "start": "228780",
    "end": "231299"
  },
  {
    "text": "say there's two benefits of that one is",
    "start": "231299",
    "end": "233459"
  },
  {
    "text": "that you can give you also want to give",
    "start": "233459",
    "end": "235319"
  },
  {
    "text": "the referee in some applications you",
    "start": "235319",
    "end": "236879"
  },
  {
    "text": "want to give the references on where you",
    "start": "236879",
    "end": "238560"
  },
  {
    "text": "got information from and then you can",
    "start": "238560",
    "end": "239940"
  },
  {
    "text": "get it's a lot more precise references",
    "start": "239940",
    "end": "241560"
  },
  {
    "text": "in terms of instead of people pointing",
    "start": "241560",
    "end": "243540"
  },
  {
    "text": "people to the whole like long document",
    "start": "243540",
    "end": "245159"
  },
  {
    "text": "you can just point them to the reference",
    "start": "245159",
    "end": "247980"
  },
  {
    "text": "and then when they click the link it",
    "start": "247980",
    "end": "249120"
  },
  {
    "text": "will the browser will like go to the",
    "start": "249120",
    "end": "250920"
  },
  {
    "text": "right section and that's very valuable",
    "start": "250920",
    "end": "252480"
  },
  {
    "text": "and then the second one is the sanctions",
    "start": "252480",
    "end": "254700"
  },
  {
    "text": "often give the right sort of like first",
    "start": "254700",
    "end": "256739"
  },
  {
    "text": "idea of like where something ends so",
    "start": "256739",
    "end": "259079"
  },
  {
    "text": "like it makes sure that I have no like",
    "start": "259079",
    "end": "261359"
  },
  {
    "text": "um I'm chunking in between functions",
    "start": "261359",
    "end": "265220"
  },
  {
    "text": "um a lot of different other strategies",
    "start": "265440",
    "end": "267360"
  },
  {
    "text": "we could do here while we were doing all",
    "start": "267360",
    "end": "269820"
  },
  {
    "text": "this chunking we tried to keep it as",
    "start": "269820",
    "end": "271199"
  },
  {
    "text": "generalizable as possible",
    "start": "271199",
    "end": "273180"
  },
  {
    "text": "um we're still working towards this",
    "start": "273180",
    "end": "274380"
  },
  {
    "text": "actually but we want to try to come up",
    "start": "274380",
    "end": "275820"
  },
  {
    "text": "with a template maybe we can do kind of",
    "start": "275820",
    "end": "277440"
  },
  {
    "text": "an open source solution as well where",
    "start": "277440",
    "end": "279240"
  },
  {
    "text": "this would work for the vast majority of",
    "start": "279240",
    "end": "280979"
  },
  {
    "text": "people's documents right not necessarily",
    "start": "280979",
    "end": "283740"
  },
  {
    "text": "has to be for a library but any kind of",
    "start": "283740",
    "end": "285960"
  },
  {
    "text": "HTML documents but",
    "start": "285960",
    "end": "288180"
  },
  {
    "text": "um after we chunk this we basically have",
    "start": "288180",
    "end": "291479"
  },
  {
    "text": "all these different chunks that we can",
    "start": "291479",
    "end": "292620"
  },
  {
    "text": "work with we now feed it into an",
    "start": "292620",
    "end": "294479"
  },
  {
    "text": "embedding model we'll talk about which",
    "start": "294479",
    "end": "295860"
  },
  {
    "text": "ones we experimented with in a second",
    "start": "295860",
    "end": "297360"
  },
  {
    "text": "and then after we have the semantic",
    "start": "297360",
    "end": "299820"
  },
  {
    "text": "representation of all of our different",
    "start": "299820",
    "end": "301080"
  },
  {
    "text": "chunks we can now index that into our",
    "start": "301080",
    "end": "303000"
  },
  {
    "text": "Vector database",
    "start": "303000",
    "end": "304380"
  },
  {
    "text": "the actual content that we're putting",
    "start": "304380",
    "end": "306600"
  },
  {
    "text": "into our database is the text the source",
    "start": "306600",
    "end": "309060"
  },
  {
    "text": "and the embedding as well",
    "start": "309060",
    "end": "310860"
  },
  {
    "text": "a lot of different options for Vector",
    "start": "310860",
    "end": "312479"
  },
  {
    "text": "databases as well I think in the last",
    "start": "312479",
    "end": "314580"
  },
  {
    "text": "maybe a year and a half we have had an",
    "start": "314580",
    "end": "316560"
  },
  {
    "text": "explosion of new databases that we've",
    "start": "316560",
    "end": "318840"
  },
  {
    "text": "never kind of part of",
    "start": "318840",
    "end": "320759"
  },
  {
    "text": "but",
    "start": "320759",
    "end": "322020"
  },
  {
    "text": "um we we kind of stuck with uh postgres",
    "start": "322020",
    "end": "325500"
  },
  {
    "text": "nice and simple we've worked with it for",
    "start": "325500",
    "end": "327900"
  },
  {
    "text": "many years I think even postgres has a",
    "start": "327900",
    "end": "330240"
  },
  {
    "text": "lot of up and coming features around",
    "start": "330240",
    "end": "331380"
  },
  {
    "text": "this honestly our Our advice here would",
    "start": "331380",
    "end": "334139"
  },
  {
    "text": "be to go with what you're already",
    "start": "334139",
    "end": "335280"
  },
  {
    "text": "familiar with or comfortable with or",
    "start": "335280",
    "end": "336840"
  },
  {
    "text": "what your team uses but a lot of the new",
    "start": "336840",
    "end": "339180"
  },
  {
    "text": "ones are are definitely worth looking at",
    "start": "339180",
    "end": "341100"
  },
  {
    "text": "they're coming up with a lot of like LM",
    "start": "341100",
    "end": "343560"
  },
  {
    "text": "app specific features which could be a",
    "start": "343560",
    "end": "346020"
  },
  {
    "text": "really interesting feature as well",
    "start": "346020",
    "end": "349740"
  },
  {
    "text": "um awesome so now when we repeat all",
    "start": "349740",
    "end": "351960"
  },
  {
    "text": "this across all of our docs you have",
    "start": "351960",
    "end": "353699"
  },
  {
    "text": "your vector database actually created",
    "start": "353699",
    "end": "356520"
  },
  {
    "text": "um we'll talk about how to update that",
    "start": "356520",
    "end": "358020"
  },
  {
    "text": "in a second but to actually now do the",
    "start": "358020",
    "end": "360120"
  },
  {
    "text": "retrieval you have a query you embed the",
    "start": "360120",
    "end": "362460"
  },
  {
    "text": "query using ideally the same embedding",
    "start": "362460",
    "end": "365160"
  },
  {
    "text": "model and you have a query embedding now",
    "start": "365160",
    "end": "366840"
  },
  {
    "text": "now you can",
    "start": "366840",
    "end": "368340"
  },
  {
    "text": "pass that over to the database and use",
    "start": "368340",
    "end": "370440"
  },
  {
    "text": "different distance metrics we use cosine",
    "start": "370440",
    "end": "372900"
  },
  {
    "text": "distance cosine similarity to retrieve",
    "start": "372900",
    "end": "375539"
  },
  {
    "text": "the top K chunks from this",
    "start": "375539",
    "end": "377820"
  },
  {
    "text": "and once you have the chunks now you can",
    "start": "377820",
    "end": "379740"
  },
  {
    "text": "feed in the text from the relevant",
    "start": "379740",
    "end": "381919"
  },
  {
    "text": "sources and the query itself to the LM",
    "start": "381919",
    "end": "384900"
  },
  {
    "text": "and get the response",
    "start": "384900",
    "end": "387060"
  },
  {
    "text": "so any questions so far with this kind",
    "start": "387060",
    "end": "390000"
  },
  {
    "text": "of V1 yes",
    "start": "390000",
    "end": "391620"
  },
  {
    "text": "oh sorry one second we have a runner",
    "start": "391620",
    "end": "395720"
  },
  {
    "text": "who's the pros and cons building Vector",
    "start": "398039",
    "end": "400740"
  },
  {
    "text": "DB on top of postgres versus using",
    "start": "400740",
    "end": "403199"
  },
  {
    "text": "Samsung out of the box like vv8 or",
    "start": "403199",
    "end": "405240"
  },
  {
    "text": "chroma DB",
    "start": "405240",
    "end": "407960"
  },
  {
    "text": "um one pro is definitely if you already",
    "start": "408120",
    "end": "409560"
  },
  {
    "text": "have expertise and maybe you're already",
    "start": "409560",
    "end": "411240"
  },
  {
    "text": "some data in there there's this Vector",
    "start": "411240",
    "end": "413160"
  },
  {
    "text": "extension called PG vectors that you can",
    "start": "413160",
    "end": "415259"
  },
  {
    "text": "just basically create a new data type",
    "start": "415259",
    "end": "417600"
  },
  {
    "text": "um and then and then you can like use",
    "start": "417600",
    "end": "419220"
  },
  {
    "text": "all existing Machinery you can even like",
    "start": "419220",
    "end": "421199"
  },
  {
    "text": "combine it with existing like filters",
    "start": "421199",
    "end": "423660"
  },
  {
    "text": "and stuff like that to like filter down",
    "start": "423660",
    "end": "426240"
  },
  {
    "text": "um",
    "start": "426240",
    "end": "427340"
  },
  {
    "text": "and then like one of the possible",
    "start": "427340",
    "end": "430500"
  },
  {
    "text": "downsides is once you have get at a very",
    "start": "430500",
    "end": "432960"
  },
  {
    "text": "large scale so if you have a huge amount",
    "start": "432960",
    "end": "434699"
  },
  {
    "text": "of document then it might not be the",
    "start": "434699",
    "end": "436740"
  },
  {
    "text": "right solution anymore but I mean it",
    "start": "436740",
    "end": "438720"
  },
  {
    "text": "depends on your application yeah are all",
    "start": "438720",
    "end": "441539"
  },
  {
    "text": "of our redox I think comes out to less",
    "start": "441539",
    "end": "444000"
  },
  {
    "text": "than a gig if I recall correctly so yeah",
    "start": "444000",
    "end": "446460"
  },
  {
    "text": "really depends on your use case but uh",
    "start": "446460",
    "end": "448800"
  },
  {
    "text": "use case for you know database like vv8",
    "start": "448800",
    "end": "451919"
  },
  {
    "text": "there are a lot of great Integrations",
    "start": "451919",
    "end": "453780"
  },
  {
    "text": "that we see almost on a weekly basis I",
    "start": "453780",
    "end": "455940"
  },
  {
    "text": "think cohere is like re-ranking is now",
    "start": "455940",
    "end": "457680"
  },
  {
    "text": "something you can get out of the box so",
    "start": "457680",
    "end": "459180"
  },
  {
    "text": "there's some amazing features so I would",
    "start": "459180",
    "end": "461880"
  },
  {
    "text": "um to get started maybe don't uh",
    "start": "461880",
    "end": "463860"
  },
  {
    "text": "experiment with everything just go with",
    "start": "463860",
    "end": "465479"
  },
  {
    "text": "what you're already familiar with but",
    "start": "465479",
    "end": "467340"
  },
  {
    "text": "um as you start getting towards",
    "start": "467340",
    "end": "469259"
  },
  {
    "text": "production and for some of these more",
    "start": "469259",
    "end": "470819"
  },
  {
    "text": "kind of Niche features might be worth",
    "start": "470819",
    "end": "472259"
  },
  {
    "text": "exploring some of the others there's",
    "start": "472259",
    "end": "474300"
  },
  {
    "text": "also elasticsearch I think it's also",
    "start": "474300",
    "end": "475620"
  },
  {
    "text": "coming out with more things in that",
    "start": "475620",
    "end": "477240"
  },
  {
    "text": "direction so it's worth looking at your",
    "start": "477240",
    "end": "478800"
  },
  {
    "text": "existing things if they can do it um and",
    "start": "478800",
    "end": "481319"
  },
  {
    "text": "then looking at the other things I mean",
    "start": "481319",
    "end": "483060"
  },
  {
    "text": "it depends on your use case yeah",
    "start": "483060",
    "end": "485940"
  },
  {
    "text": "so any other questions",
    "start": "485940",
    "end": "488960"
  },
  {
    "text": "yes",
    "start": "489780",
    "end": "491580"
  },
  {
    "text": "where are you",
    "start": "491580",
    "end": "493979"
  },
  {
    "text": "oh",
    "start": "493979",
    "end": "495840"
  },
  {
    "text": "um on the last slide",
    "start": "495840",
    "end": "498419"
  },
  {
    "text": "um yeah is there a limit on the number",
    "start": "498419",
    "end": "500879"
  },
  {
    "text": "of tokens in the context",
    "start": "500879",
    "end": "503160"
  },
  {
    "text": "there is and each model has different",
    "start": "503160",
    "end": "505259"
  },
  {
    "text": "limits we'll talk about these as well",
    "start": "505259",
    "end": "508020"
  },
  {
    "text": "um",
    "start": "508020",
    "end": "508860"
  },
  {
    "text": "yeah great question you when we do our",
    "start": "508860",
    "end": "511259"
  },
  {
    "text": "experiments we try to treat them all as",
    "start": "511259",
    "end": "513240"
  },
  {
    "text": "independent like uh the chunk size but",
    "start": "513240",
    "end": "516000"
  },
  {
    "text": "you can't right each model is different",
    "start": "516000",
    "end": "517440"
  },
  {
    "text": "and the number of chunks times the chunk",
    "start": "517440",
    "end": "519899"
  },
  {
    "text": "size together dictates how much like how",
    "start": "519899",
    "end": "522719"
  },
  {
    "text": "much context you can fill in so I'll",
    "start": "522719",
    "end": "524219"
  },
  {
    "text": "talk about that but",
    "start": "524219",
    "end": "525720"
  },
  {
    "text": "um and we'll talk about the need for",
    "start": "525720",
    "end": "527899"
  },
  {
    "text": "llms with higher context Windows as well",
    "start": "527899",
    "end": "530160"
  },
  {
    "text": "and generally that's the trend that we",
    "start": "530160",
    "end": "531660"
  },
  {
    "text": "should be going towards but great",
    "start": "531660",
    "end": "533220"
  },
  {
    "text": "question we'll get to that there's also",
    "start": "533220",
    "end": "534959"
  },
  {
    "text": "two things like some of the",
    "start": "534959",
    "end": "537000"
  },
  {
    "text": "um models have sort of like heart limits",
    "start": "537000",
    "end": "538980"
  },
  {
    "text": "maybe like 512 tokens or so and then",
    "start": "538980",
    "end": "541080"
  },
  {
    "text": "there's also things where like they",
    "start": "541080",
    "end": "542459"
  },
  {
    "text": "might not work super well in like",
    "start": "542459",
    "end": "543839"
  },
  {
    "text": "different regimes so it's the best",
    "start": "543839",
    "end": "545279"
  },
  {
    "text": "experimenting and also if you if you if",
    "start": "545279",
    "end": "548160"
  },
  {
    "text": "your data dictates longer chunks it's",
    "start": "548160",
    "end": "550200"
  },
  {
    "text": "also worth experiencing with using",
    "start": "550200",
    "end": "551700"
  },
  {
    "text": "multiple embeddings for each chunk and",
    "start": "551700",
    "end": "553680"
  },
  {
    "text": "then like I'm looking up and then and",
    "start": "553680",
    "end": "555540"
  },
  {
    "text": "then retrieving the larger document",
    "start": "555540",
    "end": "556800"
  },
  {
    "text": "based on your retrieval",
    "start": "556800",
    "end": "560040"
  },
  {
    "text": "sorry and the embedding models also have",
    "start": "560040",
    "end": "562380"
  },
  {
    "text": "cutoffs as well",
    "start": "562380",
    "end": "564480"
  },
  {
    "text": "okay so now before we kind of talk get",
    "start": "564480",
    "end": "567779"
  },
  {
    "text": "to our experiments we'll briefly talk",
    "start": "567779",
    "end": "569519"
  },
  {
    "text": "about how we're performing evaluation",
    "start": "569519",
    "end": "572580"
  },
  {
    "text": "um first we'll look at kind of the",
    "start": "572580",
    "end": "574500"
  },
  {
    "text": "component wise and to us there are two",
    "start": "574500",
    "end": "576779"
  },
  {
    "text": "major components we wanted to focus on I",
    "start": "576779",
    "end": "578399"
  },
  {
    "text": "think there are other pieces here as",
    "start": "578399",
    "end": "579600"
  },
  {
    "text": "well but for us first is like the whole",
    "start": "579600",
    "end": "582000"
  },
  {
    "text": "retrieval workflow itself so assume that",
    "start": "582000",
    "end": "584820"
  },
  {
    "text": "you have",
    "start": "584820",
    "end": "585899"
  },
  {
    "text": "um",
    "start": "585899",
    "end": "586440"
  },
  {
    "text": "uh",
    "start": "586440",
    "end": "587880"
  },
  {
    "text": "kind of a golden Source right and let's",
    "start": "587880",
    "end": "589680"
  },
  {
    "text": "just simplify this and say there's one",
    "start": "589680",
    "end": "591839"
  },
  {
    "text": "golden source for let's say a particular",
    "start": "591839",
    "end": "593459"
  },
  {
    "text": "query",
    "start": "593459",
    "end": "594480"
  },
  {
    "text": "I want to pass the query through our",
    "start": "594480",
    "end": "596820"
  },
  {
    "text": "system and I wanted to retrieve let's",
    "start": "596820",
    "end": "598740"
  },
  {
    "text": "say top K context",
    "start": "598740",
    "end": "600959"
  },
  {
    "text": "if the golden source is in one of those",
    "start": "600959",
    "end": "603540"
  },
  {
    "text": "top k then we'll count that as a success",
    "start": "603540",
    "end": "605940"
  },
  {
    "text": "we'll count that as a hit so we use this",
    "start": "605940",
    "end": "608820"
  },
  {
    "text": "kind of metric here to score or just our",
    "start": "608820",
    "end": "611519"
  },
  {
    "text": "retrieval process and isolate it away",
    "start": "611519",
    "end": "613140"
  },
  {
    "text": "from kind of what's happening with the",
    "start": "613140",
    "end": "614459"
  },
  {
    "text": "llms here",
    "start": "614459",
    "end": "616160"
  },
  {
    "text": "similarly we wanted to isolate just the",
    "start": "616160",
    "end": "618959"
  },
  {
    "text": "llm piece here so forget about",
    "start": "618959",
    "end": "621120"
  },
  {
    "text": "retrieving context assume you have the",
    "start": "621120",
    "end": "622860"
  },
  {
    "text": "best source and the text from that best",
    "start": "622860",
    "end": "624600"
  },
  {
    "text": "source and assuming it fits in the LM",
    "start": "624600",
    "end": "626519"
  },
  {
    "text": "context window given that best source",
    "start": "626519",
    "end": "629640"
  },
  {
    "text": "text how well can our llm generate a",
    "start": "629640",
    "end": "632220"
  },
  {
    "text": "response and this as you may notice is a",
    "start": "632220",
    "end": "634500"
  },
  {
    "text": "bit more generative right it's a bit",
    "start": "634500",
    "end": "637140"
  },
  {
    "text": "definitely not as objective as the",
    "start": "637140",
    "end": "638760"
  },
  {
    "text": "previous one but we have these two",
    "start": "638760",
    "end": "640680"
  },
  {
    "text": "scores to kind of compare the component",
    "start": "640680",
    "end": "642779"
  },
  {
    "text": "wise here",
    "start": "642779",
    "end": "644220"
  },
  {
    "text": "now with the quality score on just the",
    "start": "644220",
    "end": "647279"
  },
  {
    "text": "LOM side here's kind of what it looks",
    "start": "647279",
    "end": "648660"
  },
  {
    "text": "like you have a question we have the",
    "start": "648660",
    "end": "650820"
  },
  {
    "text": "golden Source you get the text from it",
    "start": "650820",
    "end": "652680"
  },
  {
    "text": "we would ask a large language model like",
    "start": "652680",
    "end": "655560"
  },
  {
    "text": "gpt4 to answer using the source and the",
    "start": "655560",
    "end": "659040"
  },
  {
    "text": "question give it give us an answer and",
    "start": "659040",
    "end": "661079"
  },
  {
    "text": "then score that answer and then provide",
    "start": "661079",
    "end": "663600"
  },
  {
    "text": "a reasoning for it and we could repeat",
    "start": "663600",
    "end": "665339"
  },
  {
    "text": "this process across different evaluators",
    "start": "665339",
    "end": "667560"
  },
  {
    "text": "so gpd4 llama 70b 7B Etc and this was",
    "start": "667560",
    "end": "671700"
  },
  {
    "text": "kind of like the first like Vibe check",
    "start": "671700",
    "end": "673019"
  },
  {
    "text": "right you can I think for Arts field we",
    "start": "673019",
    "end": "674880"
  },
  {
    "text": "had over 200 data samples here and this",
    "start": "674880",
    "end": "678240"
  },
  {
    "text": "is again why it's really important to",
    "start": "678240",
    "end": "679740"
  },
  {
    "text": "work with an application that you really",
    "start": "679740",
    "end": "681240"
  },
  {
    "text": "understand so we knew kind of the",
    "start": "681240",
    "end": "683579"
  },
  {
    "text": "answers to a lot of these questions we",
    "start": "683579",
    "end": "684839"
  },
  {
    "text": "know where it comes from we knew what",
    "start": "684839",
    "end": "686339"
  },
  {
    "text": "kind of what the answer should look like",
    "start": "686339",
    "end": "687720"
  },
  {
    "text": "so we're able to say at the end of the",
    "start": "687720",
    "end": "689760"
  },
  {
    "text": "day gpt4 is a quality evaluator that we",
    "start": "689760",
    "end": "692820"
  },
  {
    "text": "can then use for subsequent experiments",
    "start": "692820",
    "end": "695040"
  },
  {
    "text": "but Phil if you want to mention like",
    "start": "695040",
    "end": "696600"
  },
  {
    "text": "what the other ones look like so we",
    "start": "696600",
    "end": "698459"
  },
  {
    "text": "basically um what we did is we looked",
    "start": "698459",
    "end": "700320"
  },
  {
    "text": "through the whole data set we animated",
    "start": "700320",
    "end": "702060"
  },
  {
    "text": "everything with GPT including the reason",
    "start": "702060",
    "end": "703620"
  },
  {
    "text": "and then we basically removed the data",
    "start": "703620",
    "end": "705959"
  },
  {
    "text": "points where we thought gpt4 was not",
    "start": "705959",
    "end": "707820"
  },
  {
    "text": "doing a good job on and then and when we",
    "start": "707820",
    "end": "710339"
  },
  {
    "text": "use that as the golden like um",
    "start": "710339",
    "end": "711959"
  },
  {
    "text": "comparison",
    "start": "711959",
    "end": "712980"
  },
  {
    "text": "um to use for the author tried to use um",
    "start": "712980",
    "end": "715620"
  },
  {
    "text": "Lama 70b for evaluation that was not we",
    "start": "715620",
    "end": "720060"
  },
  {
    "text": "had a feeling that the performance there",
    "start": "720060",
    "end": "722100"
  },
  {
    "text": "was not as good so there's still some",
    "start": "722100",
    "end": "723839"
  },
  {
    "text": "Lee wave of like um open source models",
    "start": "723839",
    "end": "726060"
  },
  {
    "text": "to to become better yeah and I think",
    "start": "726060",
    "end": "728640"
  },
  {
    "text": "someone posted on social media but I'm",
    "start": "728640",
    "end": "730380"
  },
  {
    "text": "not sure if we're the ones to coin it",
    "start": "730380",
    "end": "731579"
  },
  {
    "text": "but there's a lot of nepotism going on",
    "start": "731579",
    "end": "732959"
  },
  {
    "text": "with llama 70b uh favor itself a lot and",
    "start": "732959",
    "end": "736980"
  },
  {
    "text": "you just see scores four out of five",
    "start": "736980",
    "end": "738720"
  },
  {
    "text": "across the board",
    "start": "738720",
    "end": "740399"
  },
  {
    "text": "um so something to keep keep an eye on",
    "start": "740399",
    "end": "743339"
  },
  {
    "text": "um also on the scoring side right we",
    "start": "743339",
    "end": "745260"
  },
  {
    "text": "picked one to five we've like worked",
    "start": "745260",
    "end": "747360"
  },
  {
    "text": "with a lot of data sets where this is",
    "start": "747360",
    "end": "748680"
  },
  {
    "text": "the case I'm sure a lot of you have seen",
    "start": "748680",
    "end": "749940"
  },
  {
    "text": "like the Yelp reviews data set and",
    "start": "749940",
    "end": "751380"
  },
  {
    "text": "things like that",
    "start": "751380",
    "end": "752339"
  },
  {
    "text": "honestly I think in terms of",
    "start": "752339",
    "end": "754079"
  },
  {
    "text": "interpretability maybe a binary uh kind",
    "start": "754079",
    "end": "756600"
  },
  {
    "text": "of scoring might have been better did",
    "start": "756600",
    "end": "758940"
  },
  {
    "text": "this work or did this not work but we",
    "start": "758940",
    "end": "760920"
  },
  {
    "text": "kind of wanted to understand on a more",
    "start": "760920",
    "end": "762720"
  },
  {
    "text": "granular level like how is how are these",
    "start": "762720",
    "end": "765180"
  },
  {
    "text": "LM scoring something like this how does",
    "start": "765180",
    "end": "766920"
  },
  {
    "text": "this reasoning relate to that score so",
    "start": "766920",
    "end": "768420"
  },
  {
    "text": "we decided to do five and actually when",
    "start": "768420",
    "end": "770579"
  },
  {
    "text": "we do our experiments and compare it and",
    "start": "770579",
    "end": "772620"
  },
  {
    "text": "then we're kind of thankful that we had",
    "start": "772620",
    "end": "773940"
  },
  {
    "text": "this kind of a spread yes",
    "start": "773940",
    "end": "777440"
  },
  {
    "text": "what he's asking what logic besides the",
    "start": "780180",
    "end": "782399"
  },
  {
    "text": "scoring sorry thanks man",
    "start": "782399",
    "end": "784139"
  },
  {
    "text": "um so if you want to talk about that so",
    "start": "784139",
    "end": "785820"
  },
  {
    "text": "you you just asked the LM so you you",
    "start": "785820",
    "end": "788279"
  },
  {
    "text": "give it the contact so you annotate the",
    "start": "788279",
    "end": "790560"
  },
  {
    "text": "question with the like golden thoughts",
    "start": "790560",
    "end": "792660"
  },
  {
    "text": "of where the answer can be found in the",
    "start": "792660",
    "end": "794040"
  },
  {
    "text": "documentation and then you asked um the",
    "start": "794040",
    "end": "796560"
  },
  {
    "text": "LM given the context and the like right",
    "start": "796560",
    "end": "800160"
  },
  {
    "text": "answer how would you evaluate the",
    "start": "800160",
    "end": "801959"
  },
  {
    "text": "following proposed answer on a scale",
    "start": "801959",
    "end": "803700"
  },
  {
    "text": "between one and five and then the LM",
    "start": "803700",
    "end": "806220"
  },
  {
    "text": "will like respond with the score",
    "start": "806220",
    "end": "809100"
  },
  {
    "text": "um",
    "start": "809100",
    "end": "811079"
  },
  {
    "text": "yeah",
    "start": "811079",
    "end": "814079"
  },
  {
    "text": "gradient 354 over",
    "start": "814920",
    "end": "818420"
  },
  {
    "text": "so in order to decide which LM to use as",
    "start": "820380",
    "end": "823380"
  },
  {
    "text": "the evaluator we just read through",
    "start": "823380",
    "end": "824880"
  },
  {
    "text": "everything and what of course it gave",
    "start": "824880",
    "end": "827040"
  },
  {
    "text": "and compared with because we know about",
    "start": "827040",
    "end": "828660"
  },
  {
    "text": "Ray right so we think about like how",
    "start": "828660",
    "end": "830880"
  },
  {
    "text": "would we think Does this answer do and",
    "start": "830880",
    "end": "832980"
  },
  {
    "text": "then we thought like which one looks",
    "start": "832980",
    "end": "835200"
  },
  {
    "text": "better",
    "start": "835200",
    "end": "835980"
  },
  {
    "text": "um I mean it's it's a bit of like like",
    "start": "835980",
    "end": "837839"
  },
  {
    "text": "magic but like um it it's a LMS here",
    "start": "837839",
    "end": "841380"
  },
  {
    "text": "black magic it's um I would say it's a",
    "start": "841380",
    "end": "844079"
  },
  {
    "text": "first pass so at the end of the day we",
    "start": "844079",
    "end": "845459"
  },
  {
    "text": "did before we get an automatic pipeline",
    "start": "845459",
    "end": "847740"
  },
  {
    "text": "so we can generate new ideas and then",
    "start": "847740",
    "end": "849540"
  },
  {
    "text": "automatically um evaluate",
    "start": "849540",
    "end": "851579"
  },
  {
    "text": "um these ideas of course as a second",
    "start": "851579",
    "end": "853920"
  },
  {
    "text": "step you then need to actually like do",
    "start": "853920",
    "end": "855779"
  },
  {
    "text": "evaluations with humans and and yourself",
    "start": "855779",
    "end": "857760"
  },
  {
    "text": "and things like that and but it's a good",
    "start": "857760",
    "end": "860040"
  },
  {
    "text": "way to get sort of cheap a cheap",
    "start": "860040",
    "end": "862200"
  },
  {
    "text": "feedback loop on like how well things",
    "start": "862200",
    "end": "864360"
  },
  {
    "text": "are working yeah so we want to reduce",
    "start": "864360",
    "end": "866279"
  },
  {
    "text": "this black magic as much as possible so",
    "start": "866279",
    "end": "868320"
  },
  {
    "text": "this piece here is not to evaluate the",
    "start": "868320",
    "end": "870600"
  },
  {
    "text": "whole system but just to know which one",
    "start": "870600",
    "end": "872700"
  },
  {
    "text": "of these LMS is a good evaluator that we",
    "start": "872700",
    "end": "874740"
  },
  {
    "text": "can use going forward so I'll show you",
    "start": "874740",
    "end": "876600"
  },
  {
    "text": "what the overall evaluation looks like",
    "start": "876600",
    "end": "878760"
  },
  {
    "text": "um so given the golden and golden source",
    "start": "878760",
    "end": "881040"
  },
  {
    "text": "which of these LMS can generate a good",
    "start": "881040",
    "end": "883440"
  },
  {
    "text": "answer and then actually attach a good",
    "start": "883440",
    "end": "885720"
  },
  {
    "text": "or appropriate score to the answers that",
    "start": "885720",
    "end": "887760"
  },
  {
    "text": "it's generating this way we can build",
    "start": "887760",
    "end": "889500"
  },
  {
    "text": "trust on one of these LMS to be used as",
    "start": "889500",
    "end": "892500"
  },
  {
    "text": "a judge going forward",
    "start": "892500",
    "end": "894019"
  },
  {
    "text": "and by the way this strategy uh we",
    "start": "894019",
    "end": "896760"
  },
  {
    "text": "didn't necessarily come up with this I",
    "start": "896760",
    "end": "897959"
  },
  {
    "text": "think uh the link chain folks llama",
    "start": "897959",
    "end": "899699"
  },
  {
    "text": "index many other kind of LM developers",
    "start": "899699",
    "end": "902820"
  },
  {
    "text": "online I think last couple months have",
    "start": "902820",
    "end": "904440"
  },
  {
    "text": "been using a similar philosophy of using",
    "start": "904440",
    "end": "906420"
  },
  {
    "text": "an evaluator or judge as the at least",
    "start": "906420",
    "end": "908760"
  },
  {
    "text": "the first pass evaluator so with an",
    "start": "908760",
    "end": "911040"
  },
  {
    "text": "evaluator set we can now do like an",
    "start": "911040",
    "end": "913139"
  },
  {
    "text": "overall evaluation here and maybe let me",
    "start": "913139",
    "end": "915839"
  },
  {
    "text": "show this diagram that might be a little",
    "start": "915839",
    "end": "917459"
  },
  {
    "text": "bit better",
    "start": "917459",
    "end": "919079"
  },
  {
    "text": "so assume forget about the evaluator for",
    "start": "919079",
    "end": "922019"
  },
  {
    "text": "a second let's say you have a certain",
    "start": "922019",
    "end": "923040"
  },
  {
    "text": "configuration for your application uh",
    "start": "923040",
    "end": "925860"
  },
  {
    "text": "chunking logic embedding model any any",
    "start": "925860",
    "end": "928260"
  },
  {
    "text": "base LM that you're using you're going",
    "start": "928260",
    "end": "930060"
  },
  {
    "text": "to use that configuration of your rag",
    "start": "930060",
    "end": "931560"
  },
  {
    "text": "app to generate responses first",
    "start": "931560",
    "end": "934019"
  },
  {
    "text": "then with that gen with those generated",
    "start": "934019",
    "end": "936060"
  },
  {
    "text": "responses you're now going to use your",
    "start": "936060",
    "end": "937560"
  },
  {
    "text": "evaluator which you've previously",
    "start": "937560",
    "end": "938940"
  },
  {
    "text": "evaluated to now ask that evaluator on",
    "start": "938940",
    "end": "942000"
  },
  {
    "text": "these generated responses how how what's",
    "start": "942000",
    "end": "944699"
  },
  {
    "text": "the quality of this response what's the",
    "start": "944699",
    "end": "946320"
  },
  {
    "text": "score you're giving it what's the reason",
    "start": "946320",
    "end": "947519"
  },
  {
    "text": "you're giving it so this is a way for us",
    "start": "947519",
    "end": "949199"
  },
  {
    "text": "to now actually you've you First Trust",
    "start": "949199",
    "end": "951240"
  },
  {
    "text": "the judge and then now you're trusting",
    "start": "951240",
    "end": "953220"
  },
  {
    "text": "the outputs of that judge across",
    "start": "953220",
    "end": "954720"
  },
  {
    "text": "different configurations that you want",
    "start": "954720",
    "end": "956220"
  },
  {
    "text": "to test",
    "start": "956220",
    "end": "958500"
  },
  {
    "text": "um I skipped this one so let me just",
    "start": "958500",
    "end": "959699"
  },
  {
    "text": "quickly talk about this so these are",
    "start": "959699",
    "end": "960899"
  },
  {
    "text": "these are the experiments that we ran",
    "start": "960899",
    "end": "962339"
  },
  {
    "text": "there's a lot more that we could do",
    "start": "962339",
    "end": "964199"
  },
  {
    "text": "across different components as well like",
    "start": "964199",
    "end": "966000"
  },
  {
    "text": "a few that aren't here is maybe the",
    "start": "966000",
    "end": "967380"
  },
  {
    "text": "distance metric you want to use in your",
    "start": "967380",
    "end": "968880"
  },
  {
    "text": "vector database",
    "start": "968880",
    "end": "970800"
  },
  {
    "text": "um for chunking maybe you want to",
    "start": "970800",
    "end": "972240"
  },
  {
    "text": "combine these things but uh we did the",
    "start": "972240",
    "end": "974699"
  },
  {
    "text": "first we tried it like with and without",
    "start": "974699",
    "end": "976260"
  },
  {
    "text": "context at all then we do the number of",
    "start": "976260",
    "end": "979680"
  },
  {
    "text": "chunks the chunk size the embedding",
    "start": "979680",
    "end": "982320"
  },
  {
    "text": "models and then the base llms as well",
    "start": "982320",
    "end": "985740"
  },
  {
    "text": "so let's uh we're going to share a",
    "start": "985740",
    "end": "987720"
  },
  {
    "text": "couple of things",
    "start": "987720",
    "end": "989519"
  },
  {
    "text": "um actually before that I so we were",
    "start": "989519",
    "end": "991560"
  },
  {
    "text": "demoing this to somebody actually we're",
    "start": "991560",
    "end": "992880"
  },
  {
    "text": "demoing this the uh we're we're",
    "start": "992880",
    "end": "994800"
  },
  {
    "text": "collaborating with the um one of the",
    "start": "994800",
    "end": "996540"
  },
  {
    "text": "co-founders of llama index and he",
    "start": "996540",
    "end": "998579"
  },
  {
    "text": "mentioned the fact that hey you know you",
    "start": "998579",
    "end": "1000320"
  },
  {
    "text": "guys have a rich vibrant ecosystem you",
    "start": "1000320",
    "end": "1002180"
  },
  {
    "text": "have docs you have people that",
    "start": "1002180",
    "end": "1003500"
  },
  {
    "text": "understand this can and you have a lot",
    "start": "1003500",
    "end": "1004940"
  },
  {
    "text": "of label data that you have what about",
    "start": "1004940",
    "end": "1007160"
  },
  {
    "text": "folks that are just starting or don't",
    "start": "1007160",
    "end": "1008899"
  },
  {
    "text": "have the time slash don't want to invest",
    "start": "1008899",
    "end": "1010699"
  },
  {
    "text": "in creating data sets",
    "start": "1010699",
    "end": "1012620"
  },
  {
    "text": "um so there's there's a lot we can do in",
    "start": "1012620",
    "end": "1014720"
  },
  {
    "text": "terms of cold start so again this is",
    "start": "1014720",
    "end": "1016880"
  },
  {
    "text": "where a good chunking comes in handy so",
    "start": "1016880",
    "end": "1018440"
  },
  {
    "text": "let's say you've chunked your data you",
    "start": "1018440",
    "end": "1020480"
  },
  {
    "text": "can now use chunks of your data to now",
    "start": "1020480",
    "end": "1022220"
  },
  {
    "text": "generate questions so for this one I",
    "start": "1022220",
    "end": "1024620"
  },
  {
    "text": "think we would take a specific chunk of",
    "start": "1024620",
    "end": "1026298"
  },
  {
    "text": "text we would ask a good quality llm",
    "start": "1026299",
    "end": "1028819"
  },
  {
    "text": "like gpt4",
    "start": "1028819",
    "end": "1030558"
  },
  {
    "text": "um you know given the the source of the",
    "start": "1030559",
    "end": "1032959"
  },
  {
    "text": "answers generate some queries this is a",
    "start": "1032959",
    "end": "1035240"
  },
  {
    "text": "very noisy approach so our kind of",
    "start": "1035240",
    "end": "1037160"
  },
  {
    "text": "additions we would do is you can do this",
    "start": "1037160",
    "end": "1039740"
  },
  {
    "text": "maybe uh isolate what chunks of data is",
    "start": "1039740",
    "end": "1043699"
  },
  {
    "text": "actually being looked at to generate the",
    "start": "1043699",
    "end": "1044959"
  },
  {
    "text": "questions so that's that's the first",
    "start": "1044959",
    "end": "1046220"
  },
  {
    "text": "thing you should do second thing",
    "start": "1046220",
    "end": "1047660"
  },
  {
    "text": "actually look at the questions and take",
    "start": "1047660",
    "end": "1049580"
  },
  {
    "text": "out the ones that maybe don't make sense",
    "start": "1049580",
    "end": "1050960"
  },
  {
    "text": "some of them are just going to be super",
    "start": "1050960",
    "end": "1052400"
  },
  {
    "text": "basic and things that your users will",
    "start": "1052400",
    "end": "1054020"
  },
  {
    "text": "never ask",
    "start": "1054020",
    "end": "1054980"
  },
  {
    "text": "and the third thing that we found was",
    "start": "1054980",
    "end": "1056900"
  },
  {
    "text": "the questions are kind of basic right",
    "start": "1056900",
    "end": "1058520"
  },
  {
    "text": "and maybe you can use some prompting to",
    "start": "1058520",
    "end": "1060020"
  },
  {
    "text": "generate more integrate questions like",
    "start": "1060020",
    "end": "1061580"
  },
  {
    "text": "what users would ask but usually it's",
    "start": "1061580",
    "end": "1063620"
  },
  {
    "text": "like this is fact a and the question",
    "start": "1063620",
    "end": "1066200"
  },
  {
    "text": "will be like what is fact day and it'll",
    "start": "1066200",
    "end": "1068000"
  },
  {
    "text": "just be a copy paste so you should be a",
    "start": "1068000",
    "end": "1070039"
  },
  {
    "text": "little bit more creative but this is a",
    "start": "1070039",
    "end": "1072200"
  },
  {
    "text": "great way to start but very quickly you",
    "start": "1072200",
    "end": "1075140"
  },
  {
    "text": "can use this to seed",
    "start": "1075140",
    "end": "1077000"
  },
  {
    "text": "a version one or version 0 of your",
    "start": "1077000",
    "end": "1079039"
  },
  {
    "text": "application put it on staging have real",
    "start": "1079039",
    "end": "1081679"
  },
  {
    "text": "people use it and then now start using",
    "start": "1081679",
    "end": "1083299"
  },
  {
    "text": "that to generate actual data and",
    "start": "1083299",
    "end": "1084860"
  },
  {
    "text": "actually labeling that so if you don't",
    "start": "1084860",
    "end": "1087020"
  },
  {
    "text": "have a lot of time this is still a great",
    "start": "1087020",
    "end": "1088460"
  },
  {
    "text": "way to start to actually get towards",
    "start": "1088460",
    "end": "1090260"
  },
  {
    "text": "high quality data sets",
    "start": "1090260",
    "end": "1092000"
  },
  {
    "text": "and there's a nice bootstrapping aspect",
    "start": "1092000",
    "end": "1093679"
  },
  {
    "text": "here unrelated to this but like um at",
    "start": "1093679",
    "end": "1097039"
  },
  {
    "text": "the beginning you start with a",
    "start": "1097039",
    "end": "1098299"
  },
  {
    "text": "completely clean slate right and then",
    "start": "1098299",
    "end": "1099980"
  },
  {
    "text": "you have this data set and then you hit",
    "start": "1099980",
    "end": "1102200"
  },
  {
    "text": "that hand labeling like where would I",
    "start": "1102200",
    "end": "1104480"
  },
  {
    "text": "answer this but then once you have the",
    "start": "1104480",
    "end": "1105919"
  },
  {
    "text": "first version you can actually like um",
    "start": "1105919",
    "end": "1107840"
  },
  {
    "text": "use launcher data set use the system to",
    "start": "1107840",
    "end": "1109760"
  },
  {
    "text": "annotate and then check it's much easier",
    "start": "1109760",
    "end": "1111740"
  },
  {
    "text": "to check if the answer is actually",
    "start": "1111740",
    "end": "1113000"
  },
  {
    "text": "provided in the context",
    "start": "1113000",
    "end": "1114740"
  },
  {
    "text": "um so that's that's like a good way to",
    "start": "1114740",
    "end": "1116600"
  },
  {
    "text": "get things bootstrapped",
    "start": "1116600",
    "end": "1118940"
  },
  {
    "text": "okay we actually just have 10 minutes",
    "start": "1118940",
    "end": "1120679"
  },
  {
    "text": "left yes",
    "start": "1120679",
    "end": "1123940"
  },
  {
    "text": "oh that's a good question",
    "start": "1126140",
    "end": "1128299"
  },
  {
    "text": "um number of examples for a cold start",
    "start": "1128299",
    "end": "1131900"
  },
  {
    "text": "um for our eval set we had over 200",
    "start": "1131900",
    "end": "1135440"
  },
  {
    "text": "samples and then we'll talk about the",
    "start": "1135440",
    "end": "1137539"
  },
  {
    "text": "classifier that we trained for LM",
    "start": "1137539",
    "end": "1139039"
  },
  {
    "text": "routing for that we had two thousand",
    "start": "1139039",
    "end": "1141620"
  },
  {
    "text": "um I think it's really context based",
    "start": "1141620",
    "end": "1144020"
  },
  {
    "text": "here like if you can't",
    "start": "1144020",
    "end": "1146840"
  },
  {
    "text": "we have a training session on Wednesday",
    "start": "1146840",
    "end": "1148520"
  },
  {
    "text": "for where we actually teach how to build",
    "start": "1148520",
    "end": "1150260"
  },
  {
    "text": "this we do 10 samples you can't really",
    "start": "1150260",
    "end": "1152720"
  },
  {
    "text": "do it it's not a good idea we just do it",
    "start": "1152720",
    "end": "1154280"
  },
  {
    "text": "because of time",
    "start": "1154280",
    "end": "1155360"
  },
  {
    "text": "um but I think you're going to need a",
    "start": "1155360",
    "end": "1157340"
  },
  {
    "text": "couple hundred at least to get a good",
    "start": "1157340",
    "end": "1159919"
  },
  {
    "text": "sense but more important than the number",
    "start": "1159919",
    "end": "1161419"
  },
  {
    "text": "you want a good spread of queries across",
    "start": "1161419",
    "end": "1164120"
  },
  {
    "text": "different parts of your product so for",
    "start": "1164120",
    "end": "1165919"
  },
  {
    "text": "us you know we want questions about core",
    "start": "1165919",
    "end": "1168020"
  },
  {
    "text": "infra train all of these different",
    "start": "1168020",
    "end": "1169880"
  },
  {
    "text": "pieces so",
    "start": "1169880",
    "end": "1171440"
  },
  {
    "text": "um like kind of testing machine learning",
    "start": "1171440",
    "end": "1172940"
  },
  {
    "text": "models hopefully you can go back and",
    "start": "1172940",
    "end": "1174260"
  },
  {
    "text": "have reports not just overall evaluation",
    "start": "1174260",
    "end": "1176000"
  },
  {
    "text": "but like evaluation on different parts",
    "start": "1176000",
    "end": "1177380"
  },
  {
    "text": "of your product as well so yeah you want",
    "start": "1177380",
    "end": "1180020"
  },
  {
    "text": "to have a good spread",
    "start": "1180020",
    "end": "1181940"
  },
  {
    "text": "okay",
    "start": "1181940",
    "end": "1183080"
  },
  {
    "text": "um running out of time so maybe we'll do",
    "start": "1183080",
    "end": "1185299"
  },
  {
    "text": "this part quickly uh I was going to ask",
    "start": "1185299",
    "end": "1187400"
  },
  {
    "text": "whether what people think context helps",
    "start": "1187400",
    "end": "1189559"
  },
  {
    "text": "or not it does so rag is definitely the",
    "start": "1189559",
    "end": "1191900"
  },
  {
    "text": "right way to go here big jump in kind of",
    "start": "1191900",
    "end": "1194299"
  },
  {
    "text": "quality here for this one",
    "start": "1194299",
    "end": "1196460"
  },
  {
    "text": "um and there's a lot of Sanity checks",
    "start": "1196460",
    "end": "1198860"
  },
  {
    "text": "along the way like with no context",
    "start": "1198860",
    "end": "1200120"
  },
  {
    "text": "obviously retrieval score is zero",
    "start": "1200120",
    "end": "1202460"
  },
  {
    "text": "uh chunk size what do people think here",
    "start": "1202460",
    "end": "1204740"
  },
  {
    "text": "bigger better or is there yeah do people",
    "start": "1204740",
    "end": "1208280"
  },
  {
    "text": "think it kind of tapers off any kind of",
    "start": "1208280",
    "end": "1210200"
  },
  {
    "text": "predictions is it smaller okay this",
    "start": "1210200",
    "end": "1213559"
  },
  {
    "text": "man's a smaller smaller anyone going for",
    "start": "1213559",
    "end": "1215360"
  },
  {
    "text": "a really",
    "start": "1215360",
    "end": "1217039"
  },
  {
    "text": "huh",
    "start": "1217039",
    "end": "1219580"
  },
  {
    "text": "oh okay nice okay that's uh got some",
    "start": "1221000",
    "end": "1223520"
  },
  {
    "text": "hints no one's going for bigger is",
    "start": "1223520",
    "end": "1224960"
  },
  {
    "text": "better for the chunk size",
    "start": "1224960",
    "end": "1226640"
  },
  {
    "text": "oh we have a couple couple folks there",
    "start": "1226640",
    "end": "1228320"
  },
  {
    "text": "okay",
    "start": "1228320",
    "end": "1229660"
  },
  {
    "text": "I guess that's true that is true",
    "start": "1229660",
    "end": "1232940"
  },
  {
    "text": "um so for us uh in terms of retrieval",
    "start": "1232940",
    "end": "1235760"
  },
  {
    "text": "you can see kept going up and then again",
    "start": "1235760",
    "end": "1238640"
  },
  {
    "text": "this is empirical for our data set might",
    "start": "1238640",
    "end": "1240740"
  },
  {
    "text": "could be different for you but in",
    "start": "1240740",
    "end": "1242360"
  },
  {
    "text": "general we expect retrieval score to go",
    "start": "1242360",
    "end": "1243860"
  },
  {
    "text": "up but it starts tapering off and",
    "start": "1243860",
    "end": "1245660"
  },
  {
    "text": "quality actually continues to go up here",
    "start": "1245660",
    "end": "1249380"
  },
  {
    "text": "um but uh you can see the difference",
    "start": "1249380",
    "end": "1251360"
  },
  {
    "text": "between uh don't necessarily continue at",
    "start": "1251360",
    "end": "1254120"
  },
  {
    "text": "the same rate as the chunk sizes are",
    "start": "1254120",
    "end": "1255740"
  },
  {
    "text": "increasing",
    "start": "1255740",
    "end": "1257000"
  },
  {
    "text": "um one thing that was definitely special",
    "start": "1257000",
    "end": "1258140"
  },
  {
    "text": "about our data set is there's a decent",
    "start": "1258140",
    "end": "1259700"
  },
  {
    "text": "amount of code Snippets yeah and so if",
    "start": "1259700",
    "end": "1261440"
  },
  {
    "text": "you get the whole code snippet that's",
    "start": "1261440",
    "end": "1262820"
  },
  {
    "text": "very good so like um either you take it",
    "start": "1262820",
    "end": "1265520"
  },
  {
    "text": "longer context to include the customer",
    "start": "1265520",
    "end": "1266960"
  },
  {
    "text": "or you have some special chunking logic",
    "start": "1266960",
    "end": "1268880"
  },
  {
    "text": "that tries to get the whole code snippet",
    "start": "1268880",
    "end": "1270440"
  },
  {
    "text": "yeah",
    "start": "1270440",
    "end": "1271640"
  },
  {
    "text": "uh number of chunks",
    "start": "1271640",
    "end": "1273740"
  },
  {
    "text": "tickers for don't use too many use as",
    "start": "1273740",
    "end": "1276980"
  },
  {
    "text": "much as you can",
    "start": "1276980",
    "end": "1279520"
  },
  {
    "text": "oh yeah what do you think",
    "start": "1280100",
    "end": "1282799"
  },
  {
    "text": "as much as you can okay",
    "start": "1282799",
    "end": "1285919"
  },
  {
    "text": "yes so uh at least you know we kind of",
    "start": "1285919",
    "end": "1288919"
  },
  {
    "text": "again going back to that gentleman's",
    "start": "1288919",
    "end": "1290960"
  },
  {
    "text": "question over there we uh eventually",
    "start": "1290960",
    "end": "1292880"
  },
  {
    "text": "stopped at seven because uh we wanted to",
    "start": "1292880",
    "end": "1294980"
  },
  {
    "text": "respect the context Links of these LM so",
    "start": "1294980",
    "end": "1297140"
  },
  {
    "text": "we could have continued to feed in more",
    "start": "1297140",
    "end": "1298520"
  },
  {
    "text": "but it would be truncated but for us in",
    "start": "1298520",
    "end": "1300980"
  },
  {
    "text": "general we found more context more a",
    "start": "1300980",
    "end": "1303320"
  },
  {
    "text": "number of chunks is better both for the",
    "start": "1303320",
    "end": "1305600"
  },
  {
    "text": "retrieval score obviously but certainly",
    "start": "1305600",
    "end": "1307580"
  },
  {
    "text": "for the quality score as well even there",
    "start": "1307580",
    "end": "1309679"
  },
  {
    "text": "kind of the increase in quality starts",
    "start": "1309679",
    "end": "1312500"
  },
  {
    "text": "to taper off as well but in general I",
    "start": "1312500",
    "end": "1314780"
  },
  {
    "text": "think we're gonna We already see but",
    "start": "1314780",
    "end": "1316280"
  },
  {
    "text": "we're gonna see more of a trend for LMS",
    "start": "1316280",
    "end": "1318799"
  },
  {
    "text": "with larger and larger context Windows",
    "start": "1318799",
    "end": "1320419"
  },
  {
    "text": "there's a lot of Open Source efforts",
    "start": "1320419",
    "end": "1322220"
  },
  {
    "text": "happening here as well so internally",
    "start": "1322220",
    "end": "1323419"
  },
  {
    "text": "we're experimenting with you know",
    "start": "1323419",
    "end": "1324799"
  },
  {
    "text": "techniques we're going to experience",
    "start": "1324799",
    "end": "1326059"
  },
  {
    "text": "with techniques like rope and others to",
    "start": "1326059",
    "end": "1327919"
  },
  {
    "text": "try to extend this as much as we can if",
    "start": "1327919",
    "end": "1331220"
  },
  {
    "text": "other folks are working on this",
    "start": "1331220",
    "end": "1332120"
  },
  {
    "text": "definitely reach out to us because this",
    "start": "1332120",
    "end": "1333440"
  },
  {
    "text": "is one of the things that it's tough in",
    "start": "1333440",
    "end": "1335299"
  },
  {
    "text": "mind for us",
    "start": "1335299",
    "end": "1337700"
  },
  {
    "text": "yes",
    "start": "1337700",
    "end": "1340299"
  },
  {
    "text": "yes oh so a great question I forgot to",
    "start": "1342140",
    "end": "1344240"
  },
  {
    "text": "mention um when you're doing like a this",
    "start": "1344240",
    "end": "1346580"
  },
  {
    "text": "is kind of like hyper parameter tuning",
    "start": "1346580",
    "end": "1347960"
  },
  {
    "text": "but like uh component tuning as well you",
    "start": "1347960",
    "end": "1350659"
  },
  {
    "text": "could do the whole spread and sometimes",
    "start": "1350659",
    "end": "1352940"
  },
  {
    "text": "you'll have to multiply things to make",
    "start": "1352940",
    "end": "1354080"
  },
  {
    "text": "sure it fits in the context window et",
    "start": "1354080",
    "end": "1355400"
  },
  {
    "text": "cetera uh we decided to fix things along",
    "start": "1355400",
    "end": "1357620"
  },
  {
    "text": "the way so first we'll experiment with",
    "start": "1357620",
    "end": "1360140"
  },
  {
    "text": "context no context then like the chunk",
    "start": "1360140",
    "end": "1362000"
  },
  {
    "text": "size once we decide on which one's good",
    "start": "1362000",
    "end": "1363799"
  },
  {
    "text": "we fixed it there so that's uh you can",
    "start": "1363799",
    "end": "1366740"
  },
  {
    "text": "you can certainly do it this way but you",
    "start": "1366740",
    "end": "1368419"
  },
  {
    "text": "can also open it up completely and do it",
    "start": "1368419",
    "end": "1370580"
  },
  {
    "text": "that way so when we did this we fixed",
    "start": "1370580",
    "end": "1372260"
  },
  {
    "text": "the chunk size to a 500 at this point",
    "start": "1372260",
    "end": "1375200"
  },
  {
    "text": "so we did the same for embedding models",
    "start": "1375200",
    "end": "1377419"
  },
  {
    "text": "as well the big takeaway here is that if",
    "start": "1377419",
    "end": "1380720"
  },
  {
    "text": "you guys look at the hugging face",
    "start": "1380720",
    "end": "1382000"
  },
  {
    "text": "leaderboard you'll find that GTE base is",
    "start": "1382000",
    "end": "1385280"
  },
  {
    "text": "actually one of the smallest models it's",
    "start": "1385280",
    "end": "1386720"
  },
  {
    "text": "in the top five now I think I may be",
    "start": "1386720",
    "end": "1389059"
  },
  {
    "text": "wrong",
    "start": "1389059",
    "end": "1389840"
  },
  {
    "text": "um but it's actually more at least for",
    "start": "1389840",
    "end": "1391340"
  },
  {
    "text": "our use case we found it to be more",
    "start": "1391340",
    "end": "1392720"
  },
  {
    "text": "performant than number one on the",
    "start": "1392720",
    "end": "1394039"
  },
  {
    "text": "leaderboard so uh I guess the takeaway",
    "start": "1394039",
    "end": "1396320"
  },
  {
    "text": "here is don't strictly go with what you",
    "start": "1396320",
    "end": "1398000"
  },
  {
    "text": "see is number one sometimes it could be",
    "start": "1398000",
    "end": "1399679"
  },
  {
    "text": "just like a giant model and yeah maybe",
    "start": "1399679",
    "end": "1401780"
  },
  {
    "text": "you perform well on the benchmarks that",
    "start": "1401780",
    "end": "1403340"
  },
  {
    "text": "they're testing and they test on quite a",
    "start": "1403340",
    "end": "1404720"
  },
  {
    "text": "few right I think it's like five or six",
    "start": "1404720",
    "end": "1405980"
  },
  {
    "text": "different dimensions and tasks but",
    "start": "1405980",
    "end": "1409039"
  },
  {
    "text": "do it on your own kind of use case here",
    "start": "1409039",
    "end": "1411200"
  },
  {
    "text": "and just see how it performs and we",
    "start": "1411200",
    "end": "1413600"
  },
  {
    "text": "compared it with open uh sorry open ai's",
    "start": "1413600",
    "end": "1415580"
  },
  {
    "text": "text embedding as well and we were able",
    "start": "1415580",
    "end": "1418220"
  },
  {
    "text": "to decide to use the smaller Open Source",
    "start": "1418220",
    "end": "1420919"
  },
  {
    "text": "One",
    "start": "1420919",
    "end": "1423039"
  },
  {
    "text": "okay and as for the uh the main so",
    "start": "1423200",
    "end": "1426020"
  },
  {
    "text": "everything is fixed along the way no",
    "start": "1426020",
    "end": "1427640"
  },
  {
    "text": "finally with the llms you know we tested",
    "start": "1427640",
    "end": "1430280"
  },
  {
    "text": "out these options here",
    "start": "1430280",
    "end": "1432740"
  },
  {
    "text": "um",
    "start": "1432740",
    "end": "1433280"
  },
  {
    "text": "because we fixed everything retrieval",
    "start": "1433280",
    "end": "1435380"
  },
  {
    "text": "score obviously doesn't change at this",
    "start": "1435380",
    "end": "1436940"
  },
  {
    "text": "point that logic is fixed but for the",
    "start": "1436940",
    "end": "1439039"
  },
  {
    "text": "quality score you can kind of see it all",
    "start": "1439039",
    "end": "1440480"
  },
  {
    "text": "here gpd4 was the clear winner here but",
    "start": "1440480",
    "end": "1442700"
  },
  {
    "text": "actually 70b and three fight turbo you",
    "start": "1442700",
    "end": "1445400"
  },
  {
    "text": "know they're not too far behind and also",
    "start": "1445400",
    "end": "1448039"
  },
  {
    "text": "there's no tuning done of any kind right",
    "start": "1448039",
    "end": "1450860"
  },
  {
    "text": "no no fine tuning on the embetting side",
    "start": "1450860",
    "end": "1452419"
  },
  {
    "text": "or these llms yet",
    "start": "1452419",
    "end": "1455900"
  },
  {
    "text": "um",
    "start": "1455900",
    "end": "1457960"
  },
  {
    "text": "awesome and as for the cost analysis",
    "start": "1458000",
    "end": "1461360"
  },
  {
    "text": "um for chat DVD models we're using",
    "start": "1461360",
    "end": "1463400"
  },
  {
    "text": "openai for the open source ones for",
    "start": "1463400",
    "end": "1465860"
  },
  {
    "text": "llama we're using any scale endpoints",
    "start": "1465860",
    "end": "1468740"
  },
  {
    "text": "kind of a kind of a shopping shocking",
    "start": "1468740",
    "end": "1470720"
  },
  {
    "text": "Factor here uh the the plot in the",
    "start": "1470720",
    "end": "1473840"
  },
  {
    "text": "bottom here is quality score and on the",
    "start": "1473840",
    "end": "1475640"
  },
  {
    "text": "the y-axis is actually uh cost but a log",
    "start": "1475640",
    "end": "1478940"
  },
  {
    "text": "scale",
    "start": "1478940",
    "end": "1480380"
  },
  {
    "text": "um so you can see here that gpd4 is",
    "start": "1480380",
    "end": "1483200"
  },
  {
    "text": "definitely uh much further a lot more",
    "start": "1483200",
    "end": "1486020"
  },
  {
    "text": "expensive but quality wise",
    "start": "1486020",
    "end": "1488360"
  },
  {
    "text": "um the others are are relatively close",
    "start": "1488360",
    "end": "1490520"
  },
  {
    "text": "but as I mentioned in the morning we",
    "start": "1490520",
    "end": "1492440"
  },
  {
    "text": "want we kind of wanted to combine The",
    "start": "1492440",
    "end": "1494000"
  },
  {
    "text": "Best of Both Worlds we wanted to serve",
    "start": "1494000",
    "end": "1495679"
  },
  {
    "text": "the most performant but also the most",
    "start": "1495679",
    "end": "1497419"
  },
  {
    "text": "cost effective so that's when we",
    "start": "1497419",
    "end": "1499460"
  },
  {
    "text": "employed this hybrid LM routing approach",
    "start": "1499460",
    "end": "1501500"
  },
  {
    "text": "they'll do it if you wanted to say a few",
    "start": "1501500",
    "end": "1503720"
  },
  {
    "text": "words about this one so in this case we",
    "start": "1503720",
    "end": "1505640"
  },
  {
    "text": "just annotated a data set with um which",
    "start": "1505640",
    "end": "1508760"
  },
  {
    "text": "model was better and then we're trying",
    "start": "1508760",
    "end": "1510500"
  },
  {
    "text": "to classify our um and there's I think",
    "start": "1510500",
    "end": "1512360"
  },
  {
    "text": "many different techniques it depends a",
    "start": "1512360",
    "end": "1513679"
  },
  {
    "text": "lot on the um I think honestly like um",
    "start": "1513679",
    "end": "1516320"
  },
  {
    "text": "in this case the main difference was if",
    "start": "1516320",
    "end": "1518419"
  },
  {
    "text": "there's like a lot of code things",
    "start": "1518419",
    "end": "1519620"
  },
  {
    "text": "involved then tbd4 does a lot better",
    "start": "1519620",
    "end": "1522559"
  },
  {
    "text": "um I think if you study your examples a",
    "start": "1522559",
    "end": "1524539"
  },
  {
    "text": "lot then you can come up with like good",
    "start": "1524539",
    "end": "1526279"
  },
  {
    "text": "rules here and and also rule my Approach",
    "start": "1526279",
    "end": "1528919"
  },
  {
    "text": "do pretty well yeah",
    "start": "1528919",
    "end": "1531200"
  },
  {
    "text": "um and I got some we got some feedback",
    "start": "1531200",
    "end": "1532580"
  },
  {
    "text": "on the blog post I haven't updated this",
    "start": "1532580",
    "end": "1534860"
  },
  {
    "text": "yet but there's a classifier actually",
    "start": "1534860",
    "end": "1537320"
  },
  {
    "text": "number four in the in the blog post I",
    "start": "1537320",
    "end": "1538940"
  },
  {
    "text": "write I write out that we use the closet",
    "start": "1538940",
    "end": "1540380"
  },
  {
    "text": "fire but we trained to classifier we had",
    "start": "1540380",
    "end": "1542299"
  },
  {
    "text": "around 1.8 uh sorry 1800 data samples",
    "start": "1542299",
    "end": "1545120"
  },
  {
    "text": "where we would say which for the given",
    "start": "1545120",
    "end": "1548419"
  },
  {
    "text": "query which of these uh LMS you should",
    "start": "1548419",
    "end": "1551480"
  },
  {
    "text": "go to and then we we trained to",
    "start": "1551480",
    "end": "1553520"
  },
  {
    "text": "supervised classifier to be able to",
    "start": "1553520",
    "end": "1554840"
  },
  {
    "text": "learn this",
    "start": "1554840",
    "end": "1556580"
  },
  {
    "text": "um and for this you know Ray train tune",
    "start": "1556580",
    "end": "1558679"
  },
  {
    "text": "all of these were just made made all",
    "start": "1558679",
    "end": "1561260"
  },
  {
    "text": "that super easy",
    "start": "1561260",
    "end": "1562580"
  },
  {
    "text": "um I think we what do we end up using",
    "start": "1562580",
    "end": "1563900"
  },
  {
    "text": "Philip we tried with Spacey first and",
    "start": "1563900",
    "end": "1565520"
  },
  {
    "text": "then I think um",
    "start": "1565520",
    "end": "1567020"
  },
  {
    "text": "actually we just needed using a simple",
    "start": "1567020",
    "end": "1569000"
  },
  {
    "text": "logistic with softmax slapped on but",
    "start": "1569000",
    "end": "1571220"
  },
  {
    "text": "depending on your use case I think I",
    "start": "1571220",
    "end": "1573080"
  },
  {
    "text": "wrote If there's more complexity or more",
    "start": "1573080",
    "end": "1574880"
  },
  {
    "text": "binning uh maybe you might need to use",
    "start": "1574880",
    "end": "1577100"
  },
  {
    "text": "something a little bit more but still",
    "start": "1577100",
    "end": "1578720"
  },
  {
    "text": "smaller than an LM like a Burt model and",
    "start": "1578720",
    "end": "1580520"
  },
  {
    "text": "tune that we also tried bird and",
    "start": "1580520",
    "end": "1583279"
  },
  {
    "text": "bettings",
    "start": "1583279",
    "end": "1584299"
  },
  {
    "text": "um I think our data set was a little bit",
    "start": "1584299",
    "end": "1585799"
  },
  {
    "text": "too small but we have more data so we'll",
    "start": "1585799",
    "end": "1588559"
  },
  {
    "text": "we'll try that again yeah and someone",
    "start": "1588559",
    "end": "1590779"
  },
  {
    "text": "this morning asked me like oh you know",
    "start": "1590779",
    "end": "1592520"
  },
  {
    "text": "what do we have to use classifiers for",
    "start": "1592520",
    "end": "1593960"
  },
  {
    "text": "this no you could use an LM here as well",
    "start": "1593960",
    "end": "1596179"
  },
  {
    "text": "but we we don't want our users waiting",
    "start": "1596179",
    "end": "1598820"
  },
  {
    "text": "like you know two minutes for a response",
    "start": "1598820",
    "end": "1600740"
  },
  {
    "text": "so we have we have a let's say a certain",
    "start": "1600740",
    "end": "1604340"
  },
  {
    "text": "SLA that we want to stick to for how",
    "start": "1604340",
    "end": "1606679"
  },
  {
    "text": "long we think a user should wait for",
    "start": "1606679",
    "end": "1607940"
  },
  {
    "text": "we're never going to go past that so to",
    "start": "1607940",
    "end": "1610580"
  },
  {
    "text": "make that happen we use the classifier",
    "start": "1610580",
    "end": "1611840"
  },
  {
    "text": "here but I think it has LM inference",
    "start": "1611840",
    "end": "1613940"
  },
  {
    "text": "gets faster I don't see any reason why",
    "start": "1613940",
    "end": "1615679"
  },
  {
    "text": "we can't use LMS to maybe make some of",
    "start": "1615679",
    "end": "1618020"
  },
  {
    "text": "these judgment calls as well especially",
    "start": "1618020",
    "end": "1619760"
  },
  {
    "text": "when maybe things can't be easily binned",
    "start": "1619760",
    "end": "1622279"
  },
  {
    "text": "across or if you want to get responses",
    "start": "1622279",
    "end": "1624500"
  },
  {
    "text": "from all the agents and get them all and",
    "start": "1624500",
    "end": "1626600"
  },
  {
    "text": "try to do something from there so this",
    "start": "1626600",
    "end": "1629659"
  },
  {
    "text": "is kind of just like the beginning I",
    "start": "1629659",
    "end": "1631400"
  },
  {
    "text": "think there's a lot more that could be",
    "start": "1631400",
    "end": "1632419"
  },
  {
    "text": "done with just like the concept of",
    "start": "1632419",
    "end": "1633740"
  },
  {
    "text": "routing and all the different components",
    "start": "1633740",
    "end": "1635360"
  },
  {
    "text": "you can use in routing and I'll just",
    "start": "1635360",
    "end": "1637400"
  },
  {
    "text": "kind of end with this",
    "start": "1637400",
    "end": "1639260"
  },
  {
    "text": "um you saw Sophia this morning who had",
    "start": "1639260",
    "end": "1641480"
  },
  {
    "text": "any scale doctor that was an application",
    "start": "1641480",
    "end": "1643279"
  },
  {
    "text": "that's built on many components",
    "start": "1643279",
    "end": "1644960"
  },
  {
    "text": "including what we built as one of its",
    "start": "1644960",
    "end": "1647419"
  },
  {
    "text": "many agents this is another major theme",
    "start": "1647419",
    "end": "1650059"
  },
  {
    "text": "that's already been happening I think",
    "start": "1650059",
    "end": "1651620"
  },
  {
    "text": "we're going to see more and more of this",
    "start": "1651620",
    "end": "1653720"
  },
  {
    "text": "um and and now like you know using Ray",
    "start": "1653720",
    "end": "1656120"
  },
  {
    "text": "in any scale to actually take something",
    "start": "1656120",
    "end": "1657440"
  },
  {
    "text": "like this to production I think is going",
    "start": "1657440",
    "end": "1659059"
  },
  {
    "text": "to be uh a big change in our field um",
    "start": "1659059",
    "end": "1661640"
  },
  {
    "text": "but yeah I think those are those are all",
    "start": "1661640",
    "end": "1663320"
  },
  {
    "text": "the slides I wanted to cover today we",
    "start": "1663320",
    "end": "1664940"
  },
  {
    "text": "have about a minute left but if people",
    "start": "1664940",
    "end": "1667460"
  },
  {
    "text": "have questions we can do those I'll",
    "start": "1667460",
    "end": "1669559"
  },
  {
    "text": "definitely check out the blog post all",
    "start": "1669559",
    "end": "1672620"
  },
  {
    "text": "the code is open sourced as well",
    "start": "1672620",
    "end": "1675740"
  },
  {
    "text": "um and we're I think we're gonna have",
    "start": "1675740",
    "end": "1676700"
  },
  {
    "text": "part two maybe more parts coming as well",
    "start": "1676700",
    "end": "1678559"
  },
  {
    "text": "in the next couple weeks slash months",
    "start": "1678559",
    "end": "1681020"
  },
  {
    "text": "but there's a lot of things that are top",
    "start": "1681020",
    "end": "1682700"
  },
  {
    "text": "of mind for us we're going to be",
    "start": "1682700",
    "end": "1684620"
  },
  {
    "text": "focusing on on a few of them",
    "start": "1684620",
    "end": "1686539"
  },
  {
    "text": "um but there's there's just so much that",
    "start": "1686539",
    "end": "1687980"
  },
  {
    "text": "can be done here the I think the big",
    "start": "1687980",
    "end": "1689659"
  },
  {
    "text": "takeaway we want to leave everyone here",
    "start": "1689659",
    "end": "1691400"
  },
  {
    "text": "with is that",
    "start": "1691400",
    "end": "1692779"
  },
  {
    "text": "um iteration is key here we built",
    "start": "1692779",
    "end": "1695059"
  },
  {
    "text": "something out we get it out we get",
    "start": "1695059",
    "end": "1696799"
  },
  {
    "text": "feedback you have to iterate on this um",
    "start": "1696799",
    "end": "1699620"
  },
  {
    "text": "uh you know when Philip mentioned this",
    "start": "1699620",
    "end": "1701299"
  },
  {
    "text": "first it kind of reminded me of like the",
    "start": "1701299",
    "end": "1702559"
  },
  {
    "text": "Tesla flywheel uh there's a lot",
    "start": "1702559",
    "end": "1705140"
  },
  {
    "text": "iteration is absolutely key here and",
    "start": "1705140",
    "end": "1706580"
  },
  {
    "text": "eventually we can get to a state where",
    "start": "1706580",
    "end": "1707840"
  },
  {
    "text": "vast majority of use cases are covered",
    "start": "1707840",
    "end": "1710120"
  },
  {
    "text": "and there will be very fewer and fewer",
    "start": "1710120",
    "end": "1711799"
  },
  {
    "text": "touch points coming from us but in the",
    "start": "1711799",
    "end": "1713360"
  },
  {
    "text": "beginning there's a lot of hard work in",
    "start": "1713360",
    "end": "1715039"
  },
  {
    "text": "terms of uh what it takes to build",
    "start": "1715039",
    "end": "1716539"
  },
  {
    "text": "something like this that will actually",
    "start": "1716539",
    "end": "1717620"
  },
  {
    "text": "answer people's questions and also one",
    "start": "1717620",
    "end": "1719480"
  },
  {
    "text": "thing here is I think using this um sort",
    "start": "1719480",
    "end": "1721700"
  },
  {
    "text": "of as a as a way to improve your",
    "start": "1721700",
    "end": "1723740"
  },
  {
    "text": "documentation your underlying documents",
    "start": "1723740",
    "end": "1725480"
  },
  {
    "text": "that can be very powerful I've had",
    "start": "1725480",
    "end": "1726860"
  },
  {
    "text": "multiple people said this now and this",
    "start": "1726860",
    "end": "1728299"
  },
  {
    "text": "we also have seen this like if you can",
    "start": "1728299",
    "end": "1730640"
  },
  {
    "text": "if you see the wrong answers and you see",
    "start": "1730640",
    "end": "1732500"
  },
  {
    "text": "which documents fed in and then and then",
    "start": "1732500",
    "end": "1734539"
  },
  {
    "text": "sometimes you actually just cover like",
    "start": "1734539",
    "end": "1736159"
  },
  {
    "text": "errors and things in the documentation",
    "start": "1736159",
    "end": "1737600"
  },
  {
    "text": "so that can be very useful yeah awesome",
    "start": "1737600",
    "end": "1740539"
  },
  {
    "text": "so that's everything and yeah we'll be",
    "start": "1740539",
    "end": "1742700"
  },
  {
    "text": "around people have questions afterwards",
    "start": "1742700",
    "end": "1746260"
  }
]