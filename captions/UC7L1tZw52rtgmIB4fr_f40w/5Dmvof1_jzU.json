[
  {
    "start": "0",
    "end": "63000"
  },
  {
    "text": "i'm simon uh you guys know me already uh the people who are watching probably not",
    "start": "320",
    "end": "6160"
  },
  {
    "text": "um i'm a data scientist uh like specialized in reinforcement learning from germany",
    "start": "6160",
    "end": "12799"
  },
  {
    "text": "and uh using our lip also contributing to it",
    "start": "12799",
    "end": "18000"
  },
  {
    "text": "and i'm the founder of new way dot ai where i try to consult companies in the area",
    "start": "18160",
    "end": "24160"
  },
  {
    "text": "of reinforcement learning yeah artery",
    "start": "24160",
    "end": "30160"
  },
  {
    "text": "uh i'm anthony alvarez also from germany and i joined any scale as an intern",
    "start": "30160",
    "end": "36640"
  },
  {
    "text": "um one month ago approximately yeah but i've been around before so i know my way",
    "start": "36640",
    "end": "42640"
  },
  {
    "text": "around the library a little bit and um yeah i hope i can ask answer uh simon's",
    "start": "42640",
    "end": "48000"
  },
  {
    "text": "question to you thanks and my name is christy i'm dev advocate at any scale and i'm here to",
    "start": "48000",
    "end": "53680"
  },
  {
    "text": "help out okay so i see on the q a that simon had a question can you",
    "start": "53680",
    "end": "59440"
  },
  {
    "text": "ask a question simon yes i'm happy to say",
    "start": "59440",
    "end": "65280"
  },
  {
    "start": "63000",
    "end": "1211000"
  },
  {
    "text": "i'm working right now on exploration and reinforcement learning and especially there in",
    "start": "65280",
    "end": "73200"
  },
  {
    "text": "an algorithm that's called random network installation and what it needs are",
    "start": "73200",
    "end": "79200"
  },
  {
    "text": "two value heads actually because i have an um extrinsic reward stream and an intrinsic",
    "start": "79200",
    "end": "86560"
  },
  {
    "text": "reward stream from these two streams i want to build up actually like two value hats",
    "start": "86560",
    "end": "92799"
  },
  {
    "text": "um because one stream can be non-episodic even if one stream is episodic",
    "start": "92799",
    "end": "100640"
  },
  {
    "text": "so um right now i have not an idea how to implement that and where there are some",
    "start": "100640",
    "end": "107040"
  },
  {
    "text": "hooks maybe in the rp's um i can use so i'm very excited what archer may have",
    "start": "107040",
    "end": "113759"
  },
  {
    "text": "figured out here yeah so we have a couple of um",
    "start": "113759",
    "end": "119520"
  },
  {
    "text": "of such exploration modules in ireland and i think the first one that assembled",
    "start": "119520",
    "end": "124799"
  },
  {
    "text": "a couple of months ago was curiosity and then now",
    "start": "124799",
    "end": "131120"
  },
  {
    "text": "simon has a ongoing vr that is called a novelty and when i read what you were",
    "start": "131120",
    "end": "139040"
  },
  {
    "text": "writing with the intrinsic rewards i assumed that it has a lot to do with the",
    "start": "139040",
    "end": "144319"
  },
  {
    "text": "model id is that right or is it another intrinsic reward signal",
    "start": "144319",
    "end": "150560"
  },
  {
    "text": "i'm now like focusing on the one from the distillation network",
    "start": "152319",
    "end": "158640"
  },
  {
    "text": "as a novelty but it's uh another algorithm novelty builds up on",
    "start": "158640",
    "end": "164000"
  },
  {
    "text": "so it's actually the same error [Music] that is taken as a intrinsic reward",
    "start": "164000",
    "end": "170959"
  },
  {
    "text": "and i just want to um to create from this reward stream a",
    "start": "170959",
    "end": "176000"
  },
  {
    "text": "second value head which i could use to later on combined but that one is non-episodic",
    "start": "176000",
    "end": "183360"
  },
  {
    "text": "and could have a known discount",
    "start": "183360",
    "end": "188720"
  },
  {
    "text": "and why would that be episodic i mean if you're talking about",
    "start": "188720",
    "end": "196159"
  },
  {
    "text": "i mean this is a loss that you can this is a reward signal that you can that you can um",
    "start": "196159",
    "end": "202080"
  },
  {
    "text": "use on a time step basis right there and the last you can calculate in your time basis right",
    "start": "202080",
    "end": "209680"
  },
  {
    "text": "yeah so what i what i usually like have it's like um i have a",
    "start": "209680",
    "end": "215280"
  },
  {
    "text": "extrinsic reward which the environment is giving me right that is usually a stationary",
    "start": "215280",
    "end": "222640"
  },
  {
    "text": "um and then i have the intrinsic reward which is very non-stationary",
    "start": "222640",
    "end": "230799"
  },
  {
    "text": "and to uh combine these two together but practice has at least shown that it",
    "start": "230799",
    "end": "237519"
  },
  {
    "text": "makes sense to let the intrinsic reward run um non-app is episodically",
    "start": "237519",
    "end": "245439"
  },
  {
    "text": "even if uh the environment is episodic therefore i can't like take just the",
    "start": "245439",
    "end": "251360"
  },
  {
    "text": "reward from the sample batch at the intrinsic reward to it and",
    "start": "251360",
    "end": "256799"
  },
  {
    "text": "give it further on for training okay right and you said there is another",
    "start": "256799",
    "end": "264800"
  },
  {
    "text": "there's supposed to be another value you have right that is i assume",
    "start": "264800",
    "end": "270080"
  },
  {
    "text": "meant to approximate the value that is relates to the intrinsic rewards",
    "start": "270080",
    "end": "277600"
  },
  {
    "text": "exactly it's like playing the game without extrinsic rewards just intrinsic",
    "start": "277600",
    "end": "283440"
  },
  {
    "text": "just doing actually the exploration game right um",
    "start": "283440",
    "end": "289680"
  },
  {
    "text": "and for this the office of the random network distillation paper use the",
    "start": "289680",
    "end": "294960"
  },
  {
    "text": "second value head and show that this works quite well",
    "start": "294960",
    "end": "300320"
  },
  {
    "text": "okay and you you just train these in in parallel and have i assume like an",
    "start": "300320",
    "end": "306000"
  },
  {
    "text": "extra hyper parameter to tune like a factor with",
    "start": "306000",
    "end": "311360"
  },
  {
    "text": "which the second auxiliary reward signal is merged into the first one and you",
    "start": "311360",
    "end": "317520"
  },
  {
    "text": "would use the same optimizer or are you thinking about this um",
    "start": "317520",
    "end": "322800"
  },
  {
    "text": "you use at the end uh the same optimizer um but you actually like accumulate um",
    "start": "322800",
    "end": "330479"
  },
  {
    "text": "for uh the one value you had and the rewards non-episodically",
    "start": "330479",
    "end": "336479"
  },
  {
    "text": "so also that the discount factory gets uh calculated differently in there",
    "start": "336479",
    "end": "342080"
  },
  {
    "text": "how does it get calculated just curiosity um",
    "start": "342080",
    "end": "348880"
  },
  {
    "text": "so i try to read through their code which is pretty hard because they take a naming which is fully unintuitive",
    "start": "348880",
    "end": "357919"
  },
  {
    "text": "what i see so far is they actually calculate the return right",
    "start": "359600",
    "end": "365520"
  },
  {
    "text": "um and try to um",
    "start": "365520",
    "end": "370960"
  },
  {
    "text": "actually like um estimate the value you had",
    "start": "370960",
    "end": "376560"
  },
  {
    "text": "by creating a second network the second network",
    "start": "376639",
    "end": "382720"
  },
  {
    "text": "tries to to fit the values towards",
    "start": "382720",
    "end": "388479"
  },
  {
    "text": "the data which comes from this intrinsic reward stream okay but that sounds very",
    "start": "388479",
    "end": "395120"
  },
  {
    "text": "episodic yes but only that the rewards are actually non-episodic",
    "start": "395120",
    "end": "402080"
  },
  {
    "text": "right okay anyway so you have uh so you have this this additional reward signal and um your your something uh",
    "start": "402080",
    "end": "409280"
  },
  {
    "text": "from the environment and then your your workers return batches and that's like this is already kind of figured out",
    "start": "409280",
    "end": "415919"
  },
  {
    "text": "right you get like at the end of your sampling stuff you get a badge that has um",
    "start": "415919",
    "end": "422560"
  },
  {
    "text": "state action um reward pairs and then uh you calculate your uh your intrinsic rewards uh based",
    "start": "422560",
    "end": "430080"
  },
  {
    "text": "on ones right exactly so i could add actually the intrinsic reward as an extra key in the",
    "start": "430080",
    "end": "438080"
  },
  {
    "text": "sample batch for example yeah right so yeah i mean this is um",
    "start": "438080",
    "end": "445039"
  },
  {
    "text": "this is something that you would put into the pros process trajectory",
    "start": "445039",
    "end": "450880"
  },
  {
    "text": "function right so our um like these policies that you use in our other um all of them have this",
    "start": "450880",
    "end": "457759"
  },
  {
    "text": "post-processed trajectory method and i think you you probably know that",
    "start": "457759",
    "end": "462800"
  },
  {
    "text": "whoever is is viewing this video afterwards not um and we are actually uh reverting this",
    "start": "462800",
    "end": "469039"
  },
  {
    "text": "right now so in a couple of weeks hopefully or um or days even um you would be able to",
    "start": "469039",
    "end": "477680"
  },
  {
    "text": "um just inherit from any rl policy and then",
    "start": "477680",
    "end": "483599"
  },
  {
    "text": "by money and then um [Music] right over this post process trajectory",
    "start": "483599",
    "end": "489680"
  },
  {
    "text": "function right and then they're just like in and like very similar to how the exploration um",
    "start": "489680",
    "end": "497039"
  },
  {
    "text": "classes do it um there you would um calculate another",
    "start": "497039",
    "end": "502560"
  },
  {
    "text": "reward signal right so this is probably where we are on the same page right yeah",
    "start": "502560",
    "end": "508479"
  },
  {
    "text": "have you done this already uh no i have not done it um",
    "start": "508479",
    "end": "513919"
  },
  {
    "text": "maybe uh you can show some code uh some source code where this is happening for example for",
    "start": "513919",
    "end": "520800"
  },
  {
    "text": "ppo or for another algorithm that's implemented in our lab",
    "start": "520800",
    "end": "527279"
  },
  {
    "text": "um perfect right so",
    "start": "527279",
    "end": "533240"
  },
  {
    "text": "so let's have a look for let's say a ppo policy",
    "start": "536240",
    "end": "543199"
  },
  {
    "text": "this has a post process trajectory method and",
    "start": "544399",
    "end": "550639"
  },
  {
    "text": "so yeah this essentially just overrides the the pulse process trajectory method of",
    "start": "551120",
    "end": "558320"
  },
  {
    "text": "either dynamic or the ertf um but it would be",
    "start": "558320",
    "end": "563760"
  },
  {
    "text": "obviously the same concept if you would um inherit from this app otf policy then",
    "start": "563760",
    "end": "568800"
  },
  {
    "text": "you could just um override this process trajectory",
    "start": "568800",
    "end": "574080"
  },
  {
    "text": "method and then do maybe something similar like uh what it's doing here",
    "start": "574080",
    "end": "579200"
  },
  {
    "text": "right i mean since you're if you're dealing with another reward signal then you could",
    "start": "579200",
    "end": "585120"
  },
  {
    "text": "compute this reward signal on the batch basis here and then possibly do something like the",
    "start": "585120",
    "end": "592480"
  },
  {
    "text": "um generalized advantage estimation right um so this is already this is",
    "start": "592480",
    "end": "600560"
  },
  {
    "text": "actually already how we build policies right now or how we uh hope that users um build upon our",
    "start": "600560",
    "end": "607600"
  },
  {
    "text": "policies right with the classic inheritance inheritance model so we have like a base policy here that is um",
    "start": "607600",
    "end": "615040"
  },
  {
    "text": "this is how you would do it right so you would take any policy class that you would uh want to add this additional",
    "start": "615040",
    "end": "620320"
  },
  {
    "text": "intrinsic word stream to um and um just subclass it and write over post",
    "start": "620320",
    "end": "627440"
  },
  {
    "text": "press post process trajectory like it's done here in a3c or here in",
    "start": "627440",
    "end": "635040"
  },
  {
    "text": "appeal right so um then we would have a intrinsic",
    "start": "635040",
    "end": "640399"
  },
  {
    "text": "reward signal uh any questions so far i think i've been talking for like a couple",
    "start": "640399",
    "end": "647120"
  },
  {
    "text": "um yeah say um if i would like to do that",
    "start": "648079",
    "end": "653760"
  },
  {
    "text": "in such a way that i can plug in my explorer exploration algorithm",
    "start": "653760",
    "end": "661279"
  },
  {
    "text": "like yeah such that the user can automatically like uh use uh rd but just",
    "start": "661279",
    "end": "668079"
  },
  {
    "text": "using it as exploration how does it work like with",
    "start": "668079",
    "end": "673600"
  },
  {
    "text": "all the algorithms in our lab because as far as i see here i",
    "start": "673600",
    "end": "679760"
  },
  {
    "text": "should um actually subclass this algorithm right",
    "start": "679760",
    "end": "686720"
  },
  {
    "text": "and then override the post-process trajectory which i would do if i want to write a custom",
    "start": "686720",
    "end": "693120"
  },
  {
    "text": "algorithm um how i would do it if i use um",
    "start": "693120",
    "end": "700800"
  },
  {
    "text": "the exploration module i do not yet understand",
    "start": "700800",
    "end": "706240"
  },
  {
    "text": "so um this is our exploration code and i think i mean you've just written",
    "start": "708320",
    "end": "713760"
  },
  {
    "text": "this pr right so somewhere here uh you have a post process trajectory",
    "start": "713760",
    "end": "718959"
  },
  {
    "text": "method as well and um this is where you would um calculate a loss i guess right",
    "start": "718959",
    "end": "726240"
  },
  {
    "text": "yeah it could i yeah it could do that there yeah how how does it get to the to",
    "start": "726399",
    "end": "732480"
  },
  {
    "text": "the policy at the end such that it can train on it",
    "start": "732480",
    "end": "737600"
  },
  {
    "text": "or how can i then add this to the other value head",
    "start": "737600",
    "end": "744680"
  },
  {
    "text": "i mean the the exploration class um the the the exploration classes methods are",
    "start": "746079",
    "end": "752959"
  },
  {
    "text": "um called as soon as you switch on exploration right this is um",
    "start": "752959",
    "end": "758560"
  },
  {
    "text": "this is where it comes into play and and otherwise it's just a switch job and um i mean for what for what you are",
    "start": "758560",
    "end": "766480"
  },
  {
    "text": "doing um can you",
    "start": "766480",
    "end": "771680"
  },
  {
    "text": "can you re-explain what you're trying to do um with a focus on are you actually uh",
    "start": "771680",
    "end": "778000"
  },
  {
    "text": "exploring is this like an uh exploratory uh intrinsic reward signal okay well then i think this this law should go um",
    "start": "778000",
    "end": "785519"
  },
  {
    "text": "into the exploration api yeah so into the",
    "start": "785519",
    "end": "791120"
  },
  {
    "text": "exploration class so for the second um okay so i think then we're",
    "start": "791120",
    "end": "797200"
  },
  {
    "text": "we're on the same page in the sense that we've got our we've sampled our trajectories right we have",
    "start": "797200",
    "end": "802240"
  },
  {
    "text": "post-procedure system into including a um another reward signal and then now the",
    "start": "802240",
    "end": "809839"
  },
  {
    "text": "question is okay how do we get um our new value estimates for the",
    "start": "809839",
    "end": "816560"
  },
  {
    "text": "auxiliary reward signal uh and um computer loss for them and apply that",
    "start": "816560",
    "end": "822320"
  },
  {
    "text": "one and choose an optimizer and stuff like that right oh yes exactly",
    "start": "822320",
    "end": "830000"
  },
  {
    "text": "awesome so uh i think what you would have to do for that is um have a look at",
    "start": "830000",
    "end": "837360"
  },
  {
    "text": "the model v2 api right so uh this this model that you're trying to build uh for",
    "start": "837360",
    "end": "843279"
  },
  {
    "text": "this needs uh three heads and that's not how modules usually look in our life",
    "start": "843279",
    "end": "848880"
  },
  {
    "text": "obviously um so you you would create your own model",
    "start": "848880",
    "end": "853920"
  },
  {
    "text": "that has three ads like just like you would create any other uh custom model that you have in order just",
    "start": "853920",
    "end": "861440"
  },
  {
    "text": "using the model v2 api and then you would hand that to your algorithm so",
    "start": "861440",
    "end": "867120"
  },
  {
    "text": "that it constructs this model behind the scenes and then in the exploration class um you",
    "start": "867120",
    "end": "874000"
  },
  {
    "text": "would um you would use this model to um",
    "start": "874000",
    "end": "879440"
  },
  {
    "text": "to compute your your auxiliary value estimates and then",
    "start": "879440",
    "end": "884720"
  },
  {
    "text": "at this loss term um maybe to to the um",
    "start": "884720",
    "end": "890480"
  },
  {
    "text": "to the um ordinary value that you have or like",
    "start": "890480",
    "end": "896560"
  },
  {
    "text": "however you want to do it from their own but i think if you're if you're using the same optimizer for example then you",
    "start": "896560",
    "end": "902560"
  },
  {
    "text": "would have to scale this signal right",
    "start": "902560",
    "end": "907760"
  },
  {
    "text": "if i could can i ask one clarification question because i feel like there's a",
    "start": "907760",
    "end": "913680"
  },
  {
    "text": "small detail that's being spoken of differently so i think what simon is asking and saying please",
    "start": "913680",
    "end": "920399"
  },
  {
    "text": "correct me if i misunderstand but i think he's asking how is there a way to set this up for other end users of rlwip",
    "start": "920399",
    "end": "927920"
  },
  {
    "text": "such that they don't have to specify like a custom model right they could just plug in his exploration with the",
    "start": "927920",
    "end": "934560"
  },
  {
    "text": "models that relic provides yeah exactly yeah okay because",
    "start": "934560",
    "end": "941680"
  },
  {
    "text": "yeah because he certainly could make a custom model stuff but i think he's trying to ask how could it be easy",
    "start": "941680",
    "end": "947279"
  },
  {
    "text": "streamlined for other users who may not you",
    "start": "947279",
    "end": "952839"
  },
  {
    "text": "know yeah the the hard part about this is that it's uh i think",
    "start": "952839",
    "end": "958079"
  },
  {
    "text": "it wouldn't be applicable to to all algorithms basically because we have some algorithms that have",
    "start": "958079",
    "end": "964000"
  },
  {
    "text": "a base model um and from what i understand you would want to",
    "start": "964000",
    "end": "969120"
  },
  {
    "text": "build up on this base model and add like a third head right um",
    "start": "969120",
    "end": "974399"
  },
  {
    "text": "and the way uh algorithms treat their their models or our policies",
    "start": "974399",
    "end": "981360"
  },
  {
    "text": "treat their models is not unified right so you could just you can't",
    "start": "981360",
    "end": "986720"
  },
  {
    "text": "i'm thinking right now that you could you can't just plug this expiration api in and then",
    "start": "986720",
    "end": "994519"
  },
  {
    "text": "yeah use it anywhere really i mean i think you would have to on a [Music]",
    "start": "996560",
    "end": "1002959"
  },
  {
    "text": "you would have to to provide a couple of different um models actually to fit this exploration",
    "start": "1002959",
    "end": "1009279"
  },
  {
    "text": "api okay see um it's also that when the exploration",
    "start": "1009279",
    "end": "1017680"
  },
  {
    "text": "class gets initialized the policy is already initialized with the model",
    "start": "1017680",
    "end": "1023600"
  },
  {
    "text": "that has already been done then right so there's no possibility to maybe like",
    "start": "1023600",
    "end": "1029038"
  },
  {
    "text": "in the initialization of the exploration class to somehow",
    "start": "1029039",
    "end": "1035839"
  },
  {
    "text": "get access to the configuration that's building the model later on",
    "start": "1035839",
    "end": "1041760"
  },
  {
    "text": "uh you have access to the to the configuration of this model inside the inside the qrs inside the exploration",
    "start": "1041760",
    "end": "1048720"
  },
  {
    "text": "class right because that's in this part of the config which",
    "start": "1048720",
    "end": "1056080"
  },
  {
    "text": "all right now there's the model actually but but um when i initialize the exploration um the",
    "start": "1056880",
    "end": "1064080"
  },
  {
    "text": "model isn't already like initialized or com does does the model gets initialized",
    "start": "1064080",
    "end": "1069280"
  },
  {
    "text": "later um yeah that's a good question so",
    "start": "1069280",
    "end": "1075200"
  },
  {
    "text": "um the initialization happens afterwards",
    "start": "1075200",
    "end": "1080640"
  },
  {
    "text": "all right um [Music] all right maybe maybe there's a way to",
    "start": "1080640",
    "end": "1086160"
  },
  {
    "text": "somehow like take the the model that has been either passed and but by the user",
    "start": "1086160",
    "end": "1092559"
  },
  {
    "text": "or uh would be um like used by uh by our lip by automatic",
    "start": "1092559",
    "end": "1100000"
  },
  {
    "text": "uh uh referring yeah but you wanted to get around um",
    "start": "1100000",
    "end": "1105919"
  },
  {
    "text": "providing custom model right so uh true but um",
    "start": "1105919",
    "end": "1112240"
  },
  {
    "text": "so that a user provides that but if the exploration on our class gets",
    "start": "1112240",
    "end": "1119520"
  },
  {
    "text": "the config and knows how the model should look like that is just take the model that has",
    "start": "1119520",
    "end": "1125679"
  },
  {
    "text": "been passed in by the user or take the model that our lip would use",
    "start": "1125679",
    "end": "1132000"
  },
  {
    "text": "put a second value head on it and use that as a custom model but then",
    "start": "1132000",
    "end": "1138240"
  },
  {
    "text": "our lip let produce that custom model right yeah so yeah i think what he's saying is",
    "start": "1138240",
    "end": "1144400"
  },
  {
    "text": "you could add in the in the model instance you could add a third head as a instance variable",
    "start": "1144400",
    "end": "1151679"
  },
  {
    "text": "at some other point right and if the exploration module knew it was there it could access it",
    "start": "1151679",
    "end": "1158559"
  },
  {
    "text": "and how would this look for the vanilla policy gradient",
    "start": "1158559",
    "end": "1165280"
  },
  {
    "text": "we have this um this registry essentially right that you that you prairie with your um",
    "start": "1165280",
    "end": "1172000"
  },
  {
    "text": "with your model parameters with your model settings and then it gives you uh some model that um",
    "start": "1172000",
    "end": "1180080"
  },
  {
    "text": "basically just fits to your to your environment um and",
    "start": "1180080",
    "end": "1185440"
  },
  {
    "text": "yeah on whatever that gives you uh we want to now build another value head which is",
    "start": "1185440",
    "end": "1191679"
  },
  {
    "text": "essentially like totally possible uh even like inside this module but it's just i think it would be um",
    "start": "1191679",
    "end": "1198400"
  },
  {
    "text": "dangerous to assume a certain structure here for kind of every algorithm that we have right",
    "start": "1198400",
    "end": "1205120"
  },
  {
    "text": "yeah gotcha i mean but the the steps that you would have just to take here i think to to to",
    "start": "1205120",
    "end": "1211200"
  },
  {
    "start": "1211000",
    "end": "1474000"
  },
  {
    "text": "summarize um if you would do this for yourself like",
    "start": "1211200",
    "end": "1216960"
  },
  {
    "text": "just to keep it a little more simple if you would do this for yourself as first and then i think after doing",
    "start": "1216960",
    "end": "1222400"
  },
  {
    "text": "that you could try to generalize to all algorithms but i think a good step to start here would be",
    "start": "1222400",
    "end": "1227840"
  },
  {
    "text": "to do this just for ppo for example um would be to calculate the digital",
    "start": "1227840",
    "end": "1234880"
  },
  {
    "text": "reward signal in the post-process trajectory method and",
    "start": "1234880",
    "end": "1240480"
  },
  {
    "text": "then calculate in the additional loss and",
    "start": "1240480",
    "end": "1246240"
  },
  {
    "text": "then figure out whether you want to do use the same optimizer or another one and um",
    "start": "1246240",
    "end": "1251919"
  },
  {
    "text": "apply this loss just on a batch base how just like you do it in",
    "start": "1251919",
    "end": "1257039"
  },
  {
    "text": "the novelty algorithm actually yeah um for an example with the ppo could you",
    "start": "1257039",
    "end": "1264000"
  },
  {
    "text": "point me to the point where the ppo is actually calculating um or training the",
    "start": "1264000",
    "end": "1270320"
  },
  {
    "text": "value hat yeah so",
    "start": "1270320",
    "end": "1277799"
  },
  {
    "text": "so look at these files here",
    "start": "1281679",
    "end": "1286919"
  },
  {
    "text": "where it's calculating the value are you talking about the um calculation of the loss or where it's",
    "start": "1291760",
    "end": "1297360"
  },
  {
    "text": "building the value you have the loss",
    "start": "1297360",
    "end": "1302000"
  },
  {
    "text": "so for for ppo that would just be yeah",
    "start": "1302880",
    "end": "1309440"
  },
  {
    "text": "like with this font size there's not a lot of text that fits onto",
    "start": "1309440",
    "end": "1314640"
  },
  {
    "text": "the screen here but yeah essentially that's the um that would be the loss function right um so since our since our",
    "start": "1314640",
    "end": "1321840"
  },
  {
    "text": "transition to a like a policy subclassing scheme um",
    "start": "1321840",
    "end": "1327360"
  },
  {
    "text": "this is uh how that looks here and this is where you could possibly take some code",
    "start": "1327360",
    "end": "1333919"
  },
  {
    "text": "from now uh so wait you you were asking about the where the",
    "start": "1333919",
    "end": "1339520"
  },
  {
    "text": "um yeah there you have the circuit list from the advantages",
    "start": "1339520",
    "end": "1346279"
  },
  {
    "text": "and this is essentially what you're using to train your value hydrate",
    "start": "1346400",
    "end": "1352000"
  },
  {
    "text": "it goes into your total lust and where are the um",
    "start": "1352000",
    "end": "1357440"
  },
  {
    "text": "the extrinsic rewards used in in the ppo algorithm",
    "start": "1357440",
    "end": "1364960"
  },
  {
    "text": "the extrinsic rewards yes they're just just the rewards where are",
    "start": "1366559",
    "end": "1372799"
  },
  {
    "text": "they used for calculation so they i mean the the um",
    "start": "1372799",
    "end": "1380000"
  },
  {
    "text": "the rewards are just parts of the batches that you sample right so every time you sample from a rolled out worker",
    "start": "1380000",
    "end": "1386720"
  },
  {
    "text": "gets um you get back from cash that has [Music]",
    "start": "1386720",
    "end": "1392240"
  },
  {
    "text": "rewarded as part of them and also actually advantages because these",
    "start": "1392240",
    "end": "1397600"
  },
  {
    "text": "advantages will be part of the post-processing step of the sample batch",
    "start": "1397600",
    "end": "1402799"
  },
  {
    "text": "so like this is something that you uh that you would",
    "start": "1402799",
    "end": "1408080"
  },
  {
    "text": "normally do on on rollout workers already i think [Music]",
    "start": "1408080",
    "end": "1414240"
  },
  {
    "text": "and in the the ordinary case you would do this with a generalized advantage estimation",
    "start": "1414240",
    "end": "1420799"
  },
  {
    "text": "so this would have",
    "start": "1420799",
    "end": "1424278"
  },
  {
    "text": "yeah i mean this is part of the the policy so there's this",
    "start": "1430000",
    "end": "1435200"
  },
  {
    "text": "i'm not sure whether we use we do post process to trajectory here on the rollout workers or on the on the driver",
    "start": "1435200",
    "end": "1441679"
  },
  {
    "text": "process but anyway that's that's basically where it happens right so there's a sample that's coming here with",
    "start": "1441679",
    "end": "1447039"
  },
  {
    "text": "some rewards and then you uh have an advantage like an estimate of the advantage function",
    "start": "1447039",
    "end": "1453120"
  },
  {
    "text": "i just have to i have to open the door sorry yeah go ahead",
    "start": "1453120",
    "end": "1459360"
  },
  {
    "text": "right should we uh do like another uh introduction of money well uh someone is",
    "start": "1461760",
    "end": "1467360"
  },
  {
    "text": "gone or should we just wait and you yeah",
    "start": "1467360",
    "end": "1473520"
  },
  {
    "text": "yeah that sounds good manny would you like to introduce yourself yeah i'm just say hi my name is danny",
    "start": "1473520",
    "end": "1479279"
  },
  {
    "start": "1474000",
    "end": "1772000"
  },
  {
    "text": "i'm i don't know long time user of rlib contribute when i have time",
    "start": "1479279",
    "end": "1485600"
  },
  {
    "text": "and i very much liked i've liked the library",
    "start": "1485600",
    "end": "1490880"
  },
  {
    "text": "yeah simon and many are actually i think uh are like our two most active community members",
    "start": "1490880",
    "end": "1498720"
  },
  {
    "text": "like i mean where are we now do do do we have all the steps uh that you would need to to finish this like kind of i",
    "start": "1498720",
    "end": "1505360"
  },
  {
    "text": "mean not not to do this in a generalized way but to",
    "start": "1505360",
    "end": "1510640"
  },
  {
    "text": "to do this for ppr right um yeah so i definitely have uh have a",
    "start": "1510640",
    "end": "1518400"
  },
  {
    "text": "point like a post-process trajectory right where the value",
    "start": "1518400",
    "end": "1524080"
  },
  {
    "text": "targets of ppo are calculated as far as i understood",
    "start": "1524080",
    "end": "1530320"
  },
  {
    "text": "so by this i can i can look up um how this is done in ppo",
    "start": "1530320",
    "end": "1535360"
  },
  {
    "text": "and i can just do that um likewise for the intrinsic rewards",
    "start": "1535360",
    "end": "1541200"
  },
  {
    "text": "yeah i mean the only thing that you would um that i hope you would have to uh",
    "start": "1541200",
    "end": "1546559"
  },
  {
    "text": "change between these algorithms is the structure of the model right because uh the like how you form this intrinsic",
    "start": "1546559",
    "end": "1552159"
  },
  {
    "text": "reward signal is uh would stay the same yeah true",
    "start": "1552159",
    "end": "1557440"
  },
  {
    "text": "so it has has to have a second value hatch that is trained with this uh",
    "start": "1557440",
    "end": "1563039"
  },
  {
    "text": "different value targets and then um from this um",
    "start": "1563039",
    "end": "1568640"
  },
  {
    "text": "i could pop probably like in the post process trajectory of the exploration rp",
    "start": "1568640",
    "end": "1574640"
  },
  {
    "text": "that calculate um [Music] further steps absolutely yeah",
    "start": "1574640",
    "end": "1580960"
  },
  {
    "text": "so you take this post processing step here",
    "start": "1580960",
    "end": "1586559"
  },
  {
    "text": "and then differentially differentiate between tensorflow and",
    "start": "1586559",
    "end": "1591970"
  },
  {
    "text": "[Music] pytorch possibly and then you would go into your",
    "start": "1591970",
    "end": "1599240"
  },
  {
    "text": "[Music] and then put something similar here",
    "start": "1602210",
    "end": "1608159"
  },
  {
    "text": "whatever whatever kind of post-processing you want to do to the trajectory and then you would need",
    "start": "1608159",
    "end": "1613200"
  },
  {
    "text": "another model that has um that yeah produces these um",
    "start": "1613200",
    "end": "1620240"
  },
  {
    "text": "these value estimates for the intrinsic rework signal and then you just put whatever that wants without",
    "start": "1620240",
    "end": "1628080"
  },
  {
    "text": "in your make it part of your loss and that's what i think",
    "start": "1628080",
    "end": "1634240"
  },
  {
    "text": "okay can i say the model the model is actually um custom that i have to produce",
    "start": "1634240",
    "end": "1641919"
  },
  {
    "text": "i mean in the first iteration of doing this i would use a custom i would hand craft the model",
    "start": "1641919",
    "end": "1647440"
  },
  {
    "text": "and then from there have a look at how how these how other algorithms",
    "start": "1647440",
    "end": "1653760"
  },
  {
    "text": "produce models and if you can derive and everything but to be honest i don't know",
    "start": "1653760",
    "end": "1660159"
  },
  {
    "text": "how models typically look for each and every one of our algorithms but i know that they have um",
    "start": "1660159",
    "end": "1668559"
  },
  {
    "text": "different structures so yeah yeah and possibly uh",
    "start": "1668559",
    "end": "1675600"
  },
  {
    "text": "like if you're doing this and it works though just being spin and i suppose he would be",
    "start": "1675600",
    "end": "1682399"
  },
  {
    "text": "happy to help to maybe have an idea to generalize this to our models yeah",
    "start": "1682399",
    "end": "1688799"
  },
  {
    "text": "money many you want to say something to you yeah",
    "start": "1688799",
    "end": "1694080"
  },
  {
    "text": "it's just i think it's a subtle thing but um",
    "start": "1694080",
    "end": "1699200"
  },
  {
    "text": "so it i don't know the state now right it used to be the case that the at some point the sample batch was converted",
    "start": "1699200",
    "end": "1706480"
  },
  {
    "text": "uh to a numpy array um it may still be i don't know but that",
    "start": "1706480",
    "end": "1713440"
  },
  {
    "text": "is only to put out a caution then for simon that you know if that still happens you won't have a computation",
    "start": "1713440",
    "end": "1720320"
  },
  {
    "text": "graph on the data stored in the sample batch so this is i think where archer",
    "start": "1720320",
    "end": "1725440"
  },
  {
    "text": "keeps talking about you know calculating the loss function right you'll have to rerun this stuff through the model in",
    "start": "1725440",
    "end": "1731760"
  },
  {
    "text": "the last function probably to be able to have the computation graph to do your backwards",
    "start": "1731760",
    "end": "1737679"
  },
  {
    "text": "pass yeah it was just a you know subtle bookkeeping thing that's got me in the past so i wanted to offer it yeah",
    "start": "1737679",
    "end": "1746080"
  },
  {
    "text": "beware danger be here yeah sure i mean that's yeah i think",
    "start": "1746080",
    "end": "1751760"
  },
  {
    "text": "super helpful also for people watching yeah you might be watching this",
    "start": "1751760",
    "end": "1757279"
  },
  {
    "text": "oh i was just gonna say we got 15 minutes left so either we discuss this some more or if",
    "start": "1757279",
    "end": "1762960"
  },
  {
    "text": "manny has a question i'm good uh i've i've got the",
    "start": "1762960",
    "end": "1768240"
  },
  {
    "text": "information that helps me to uh go on uh yeah i don't have anything big there is",
    "start": "1768240",
    "end": "1774320"
  },
  {
    "start": "1772000",
    "end": "1956000"
  },
  {
    "text": "one thing i came across yesterday that i spent a little bit tracking down and so",
    "start": "1774320",
    "end": "1780000"
  },
  {
    "text": "i just thought i'd bring it up and see if it's something you guys would want to fix for or not so",
    "start": "1780000",
    "end": "1785039"
  },
  {
    "text": "it turns out that when you have grad clip enabled and the metrics are being",
    "start": "1785039",
    "end": "1790559"
  },
  {
    "text": "stored in tensorboard or json or whatever",
    "start": "1790559",
    "end": "1795760"
  },
  {
    "text": "the way it's done in pi torch and the way it's done in tensorflow are different",
    "start": "1795760",
    "end": "1801760"
  },
  {
    "text": "so the tensorflow version is returning the grad norm after the clipping",
    "start": "1801760",
    "end": "1808640"
  },
  {
    "text": "and the torch one is returning it before the clipping",
    "start": "1808640",
    "end": "1815039"
  },
  {
    "text": "so for example the the default grad norm grad clip right in our lib at least",
    "start": "1815200",
    "end": "1821679"
  },
  {
    "text": "for a2c which is i think what i was looking at at that point was 40. so in tensorflow it was always 40 but in pie",
    "start": "1821679",
    "end": "1828399"
  },
  {
    "text": "torch it was like 2 000. i went back and confirmed that after the",
    "start": "1828399",
    "end": "1834080"
  },
  {
    "text": "clipping if i recalculated the grad norm it would be 40 but",
    "start": "1834080",
    "end": "1839120"
  },
  {
    "text": "just the value store and there's actually an old closed issue number 4965",
    "start": "1839120",
    "end": "1846200"
  },
  {
    "text": "that reported this from like 2019 um",
    "start": "1846640",
    "end": "1852399"
  },
  {
    "text": "yeah right okay well um [Music] thanks",
    "start": "1852399",
    "end": "1857679"
  },
  {
    "text": "it sounds more like like you're reporting this instead of like asking a question to be honest i i would have to",
    "start": "1857679",
    "end": "1863200"
  },
  {
    "text": "to yeah look into this and so i don't have the ability to reopen an",
    "start": "1863200",
    "end": "1870080"
  },
  {
    "text": "issue i guess i could open a new issue and just link to the closed issue can you ping me the issue here in chat",
    "start": "1870080",
    "end": "1876720"
  },
  {
    "text": "and i'll copy it four nine six five four nine six five okay",
    "start": "1876720",
    "end": "1882240"
  },
  {
    "text": "and that that one just got stale or yeah yeah it just got closed by the",
    "start": "1882240",
    "end": "1888399"
  },
  {
    "text": "stale yeah do you know where this happens actually where the uh normal clippings happen so just so it's in",
    "start": "1888399",
    "end": "1895840"
  },
  {
    "text": "utils",
    "start": "1895840",
    "end": "1898480"
  },
  {
    "text": "okay so i'll ping i'll ping richard then to be open tf clip by norm is that the operation",
    "start": "1902880",
    "end": "1909200"
  },
  {
    "text": "you're talking about",
    "start": "1909200",
    "end": "1911840"
  },
  {
    "text": "okay yeah yeah i'm just like um after this i'm just gonna",
    "start": "1915600",
    "end": "1921600"
  },
  {
    "text": "look up these two and see if they actually they're specified to do something different and so on yeah but",
    "start": "1921600",
    "end": "1926960"
  },
  {
    "text": "thanks for yeah i'm gonna submit and if we haven't reopened i'm",
    "start": "1926960",
    "end": "1932240"
  },
  {
    "text": "just checking to see if i can reopen it myself and it looks like i can't so um yeah",
    "start": "1932240",
    "end": "1937679"
  },
  {
    "text": "yeah i think you know we can we can stop early if everybody's um we got what we need right for now",
    "start": "1937679",
    "end": "1945039"
  },
  {
    "text": "yeah that's all i wanted to raise thank you so much okay thank you yeah",
    "start": "1945039",
    "end": "1950720"
  },
  {
    "text": "thanks archer thank you guys",
    "start": "1950720",
    "end": "1955799"
  }
]