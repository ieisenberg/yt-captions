[
  {
    "text": "okay we're gonna get started good afternoon good evening whatever time zone you're on Welcome to our first Ray",
    "start": "480",
    "end": "7740"
  },
  {
    "text": "uh New York Meetup in association with the NYC ml Nai hosted by Justin so",
    "start": "7740",
    "end": "14219"
  },
  {
    "text": "thanks a lot for uh having us over there in New York and thanks a lot for all of our speakers who actually joining us my",
    "start": "14219",
    "end": "19920"
  },
  {
    "text": "name is Jules damji I'm going to be your host today and your moderator I'm part of the ray advocacy team and those are",
    "start": "19920",
    "end": "27000"
  },
  {
    "text": "my email address and my contact information if you want to get in touch with me so here's",
    "start": "27000",
    "end": "33239"
  },
  {
    "text": "um the agenda today but before we get into the agenda I just want to sort of get uh with some Logistics today all",
    "start": "33239",
    "end": "39780"
  },
  {
    "text": "your mics unfortunately are muted so we can't hear you it's going to be sort of a one-way conversation if you have any",
    "start": "39780",
    "end": "46920"
  },
  {
    "text": "problems that you're actually encountering whether the slides are not being shown or the font is too small at",
    "start": "46920",
    "end": "52020"
  },
  {
    "text": "the meet in the um in the notebooks um put something on the chat so we can",
    "start": "52020",
    "end": "57840"
  },
  {
    "text": "actually address that if you have any questions for the speakers normally at the end put them in the Q a because that",
    "start": "57840",
    "end": "63840"
  },
  {
    "text": "way it's a lot easier for me to moderate them rather than on chat because otherwise it would just go up and down and I might lose that so put all your",
    "start": "63840",
    "end": "70560"
  },
  {
    "text": "questions in the Q a if you want to tell us where you're from just put something on the chat and let us know where you're",
    "start": "70560",
    "end": "77040"
  },
  {
    "text": "heading from uh we like to know uh where where people are sort of you know",
    "start": "77040",
    "end": "83400"
  },
  {
    "text": "dialing in from so feel free to feel free to let us know say hello on the chat and that way we know that that we",
    "start": "83400",
    "end": "89759"
  },
  {
    "text": "can actually hear you okay so today's agenda um we've got two speakers I'm just gonna",
    "start": "89759",
    "end": "95759"
  },
  {
    "text": "go quickly over some of the upcoming events and some of the things that are happening in the ray community and then we'll have on the first talk by",
    "start": "95759",
    "end": "103380"
  },
  {
    "text": "um uh Max and Federico Max is the CEO and the founder of nixler uh he's going",
    "start": "103380",
    "end": "109259"
  },
  {
    "text": "to talk about how they actually do forecasting get Scaled with Ray and then Federico is also going to be part of the",
    "start": "109259",
    "end": "114600"
  },
  {
    "text": "second half of this presentation that's going to be followed by um by Daft Jay Chi who is the CEO and",
    "start": "114600",
    "end": "121920"
  },
  {
    "text": "founder of eventual is going to give us you know what is Daft uh is all about and why they actually chose Rey as a",
    "start": "121920",
    "end": "128580"
  },
  {
    "text": "native python data frame and then that way um we can actually find out how people are likely using using desk or",
    "start": "128580",
    "end": "135300"
  },
  {
    "text": "using using Daft at scale and also why people are using Ray and time series at scale so those are going to be the two",
    "start": "135300",
    "end": "141540"
  },
  {
    "text": "main topics just as part of our upcoming events we have Summit coming up again in in September",
    "start": "141540",
    "end": "150060"
  },
  {
    "text": "uh early bird registration is started so those you guys who want to save some money go ahead and look at this",
    "start": "150060",
    "end": "157379"
  },
  {
    "text": "particular URL and sign up we have a three-day conference the two days are the sessions and then the third day is",
    "start": "157379",
    "end": "163920"
  },
  {
    "text": "going to be where we actually have training so we have six training sessions happening it'll be actually an exciting three-day event so don't miss",
    "start": "163920",
    "end": "170459"
  },
  {
    "text": "it out it's in September in San Francisco good result to come we won't have rain we won't have trees falling",
    "start": "170459",
    "end": "175800"
  },
  {
    "text": "down so I think that's going to be good oh we have Keynotes that we actually announced we just announced Adrian Gomez",
    "start": "175800",
    "end": "181319"
  },
  {
    "text": "who is the co-founder of uh coher and and Brian McLean who is also begin",
    "start": "181319",
    "end": "187260"
  },
  {
    "text": "joining us we've got uh three astounding Keynotes along with the founders Robert",
    "start": "187260",
    "end": "192720"
  },
  {
    "text": "and Jan stork I was going to be giving over the keynote so don't miss this the opportunity to join and join the global",
    "start": "192720",
    "end": "198840"
  },
  {
    "text": "Community I think it's going to be an exciting time to be a part of the community the other things are coming up",
    "start": "198840",
    "end": "204120"
  },
  {
    "text": "we also have an aw years Loft in any scale break even tomorrow in New York",
    "start": "204120",
    "end": "209879"
  },
  {
    "text": "and I believe Max is going to be there correct yeah so max is going to be so you could you get to hear him again",
    "start": "209879",
    "end": "214980"
  },
  {
    "text": "twice in a row if you miss anything today and if you're there in person we will ask questions and then we're",
    "start": "214980",
    "end": "220019"
  },
  {
    "text": "repeating the event the following week in San Francisco at the AWS Loft so it's",
    "start": "220019",
    "end": "225659"
  },
  {
    "text": "going to be pretty much the same exciting way to find out what the ray Community is working on and who are the",
    "start": "225659",
    "end": "231299"
  },
  {
    "text": "vendors who are actually using rain any sale at scale so this is actually a good opportunity for you to to meet and",
    "start": "231299",
    "end": "236700"
  },
  {
    "text": "mingle and and meet the rest of the crew another Meetup coming up uh Spotify is a",
    "start": "236700",
    "end": "242519"
  },
  {
    "text": "big user array they actually have been using Rey for a long time and they also built an entire platform they call use",
    "start": "242519",
    "end": "250319"
  },
  {
    "text": "cases they actually use a Ponder on it so this is actually a good Meetup to talk about how they're actually using",
    "start": "250319",
    "end": "255900"
  },
  {
    "text": "Ponder with Ray and why they actually went with race it's a good opportunity to find out how you can actually use",
    "start": "255900",
    "end": "261000"
  },
  {
    "text": "that another a couple of things that are happening this week as you know generation AI has really Tech enough has",
    "start": "261000",
    "end": "268320"
  },
  {
    "text": "been sort of capturing the media aligns what people actually work you know generate the AI and we wanted to sort of",
    "start": "268320",
    "end": "273840"
  },
  {
    "text": "tell a story of how Ray fits in into this grand scheme of things and some of you probably don't know but you know",
    "start": "273840",
    "end": "279900"
  },
  {
    "text": "open Ai and and cohere and Electra actually use Ray to actually build a GPT",
    "start": "279900",
    "end": "285780"
  },
  {
    "text": "model so this is sort of a way of of telling the story what are the challenges involved in in dealing with",
    "start": "285780",
    "end": "292620"
  },
  {
    "text": "General workloads and how you can actually do things at scale so we have a three-part Globe series which are going",
    "start": "292620",
    "end": "298199"
  },
  {
    "text": "out the first went out yesterday which sort of lays out the foundation generally AI is going and how Ray fits",
    "start": "298199",
    "end": "304080"
  },
  {
    "text": "in the grand scheme of things what are some of the challenges involved when you're dealing with large models of billions of parameters and you're",
    "start": "304080",
    "end": "309660"
  },
  {
    "text": "dealing with workloads such as inferencing and training and fine-tuning and all that so that is in in blog one",
    "start": "309660",
    "end": "316020"
  },
  {
    "text": "the second blog which went out today was a talk that we actually gave at the GTC conference today where we're actually",
    "start": "316020",
    "end": "322199"
  },
  {
    "text": "using Alpine array to to do a train over a thousand GPU scale of 175 billion",
    "start": "322199",
    "end": "329160"
  },
  {
    "text": "parameter or PGA model so that's actually a a quite interesting blog that you actually got and then the third one",
    "start": "329160",
    "end": "335460"
  },
  {
    "text": "is going to be how we actually use and fine-tune uh division AI using using gray so these are sort of good technical",
    "start": "335460",
    "end": "342720"
  },
  {
    "text": "engineering series of blogs going out to have gone out this week and and when it's going to be going out on the",
    "start": "342720",
    "end": "349020"
  },
  {
    "text": "following so just go to uh WWE anyscale.com.blog and you'll actually see part one part two uh finally two",
    "start": "349020",
    "end": "356759"
  },
  {
    "text": "books are out or O'Reilly um I think tomorrow we're actually having book signing so if you come to or",
    "start": "356759",
    "end": "362820"
  },
  {
    "text": "at the event you might be able to grab a book and meet the author one of the authors over there I believe that in New",
    "start": "362820",
    "end": "368759"
  },
  {
    "text": "York City and then we have another book uh by Holden Corral which is a big open source Advocate and she works at Netflix",
    "start": "368759",
    "end": "375300"
  },
  {
    "text": "and her author Boris so those are the two books available so you can actually reveal all your knowledge in Rey and and",
    "start": "375300",
    "end": "382199"
  },
  {
    "text": "let's go uh let's go buy those books uh this is a good way to to to apply your",
    "start": "382199",
    "end": "387960"
  },
  {
    "text": "skills on right and then finally a lot of things actually happening in the community you know this is one of the ways we actually",
    "start": "387960",
    "end": "395039"
  },
  {
    "text": "communicate and and meet once once a month every Thursday and so if you want",
    "start": "395039",
    "end": "400500"
  },
  {
    "text": "to have join us at the either snack Channel or join our discussion group or join follow us on Twitter this is the",
    "start": "400500",
    "end": "406620"
  },
  {
    "text": "way to join us and just like any other open source conferences or any other in person we actually have a code of",
    "start": "406620",
    "end": "413100"
  },
  {
    "text": "conduct we are very Strictly adhering To those and so we just want to make sure that I'm gonna live up there for about a",
    "start": "413100",
    "end": "419819"
  },
  {
    "text": "couple of seconds so you actually have a chance to read it this is all about of a conduct to make sure that we adhere to",
    "start": "419819",
    "end": "426000"
  },
  {
    "text": "it and we we really follow it uh word by word so I'm going to leave it here for a couple of seconds and then we'll get",
    "start": "426000",
    "end": "431460"
  },
  {
    "text": "started with Max as our first speakers on how they actually use array and why they use Ray and what what are the",
    "start": "431460",
    "end": "437520"
  },
  {
    "text": "problems that time series and force casting allows you to do okay with that now I'm going to stop",
    "start": "437520",
    "end": "444000"
  },
  {
    "text": "sharing the screen and I'm gonna have uh Max Deco from here",
    "start": "444000",
    "end": "449460"
  },
  {
    "text": "thank you very much can you can you hear me yes I can hear you loud and clear",
    "start": "449460",
    "end": "454800"
  },
  {
    "text": "thank you very much and thank you Jules for for the introduction and thank you to all the participants for being here",
    "start": "454800",
    "end": "461039"
  },
  {
    "text": "we're very excited to talk to you about uh how to use Rey and Nick's lab to open",
    "start": "461039",
    "end": "467880"
  },
  {
    "text": "source we think two amazing open source projects to do scalable time series forecasting and all and then you could",
    "start": "467880",
    "end": "475319"
  },
  {
    "text": "also start doing other timesheet related tasks like anomaly detection so in",
    "start": "475319",
    "end": "481080"
  },
  {
    "text": "summary what we're gonna do and learn today is we will explore how to scale time series tasks across different",
    "start": "481080",
    "end": "488419"
  },
  {
    "text": "distributed machines using this two technologies Ray and nixler a brief",
    "start": "488419",
    "end": "495120"
  },
  {
    "text": "definition before we start time series forecasting is normally defined as the",
    "start": "495120",
    "end": "500819"
  },
  {
    "text": "process of using historical data to predict future Trends or patterns in time dependent data this is normally How",
    "start": "500819",
    "end": "507720"
  },
  {
    "text": "Time series data looks like it has a classical components like seasonalities",
    "start": "507720",
    "end": "513120"
  },
  {
    "text": "Trends noise and what we're trying to do is understand the past and maybe to influence of some exogenous variables to",
    "start": "513120",
    "end": "521039"
  },
  {
    "text": "predict future distributions of the target variable and this has many",
    "start": "521039",
    "end": "527040"
  },
  {
    "text": "different use cases time series are really the operational DNA of the world",
    "start": "527040",
    "end": "532500"
  },
  {
    "text": "everything that is not on structured data everything that is a tabular data",
    "start": "532500",
    "end": "537540"
  },
  {
    "text": "can be understood as time series data so it's widely used in healthcare and finance Telemetry observability",
    "start": "537540",
    "end": "545360"
  },
  {
    "text": "e-commerce retail Etc the most prominent use cases for time series applications",
    "start": "545360",
    "end": "551040"
  },
  {
    "text": "are forecasting and anomaly detection today we're gonna to be focusing on on forecasting and normally if we try to",
    "start": "551040",
    "end": "560760"
  },
  {
    "text": "separate forecasting tasks into two big categories there are strategic",
    "start": "560760",
    "end": "566820"
  },
  {
    "text": "forecasting problems and operational forecasting problems if this is this is",
    "start": "566820",
    "end": "572100"
  },
  {
    "text": "a separation that is done by John gall's house and Anton janukowski who worked at Amazon and helped created AWS forecast",
    "start": "572100",
    "end": "580680"
  },
  {
    "text": "the main difference between strategic and operational forecast is a strategic forecast normally is done using local",
    "start": "580680",
    "end": "588440"
  },
  {
    "text": "models that means that a single model or a couple of models are trained for every",
    "start": "588440",
    "end": "594480"
  },
  {
    "text": "unique combination of Time series here a journey combination of Time series could be the lowest granularity",
    "start": "594480",
    "end": "601440"
  },
  {
    "text": "of the data that is available so to bring it back to the real world imagine you're a retailer and you have different",
    "start": "601440",
    "end": "608339"
  },
  {
    "text": "products and different stores and those products are part of categories and Brands and then",
    "start": "608339",
    "end": "614660"
  },
  {
    "text": "the the the the the lowest granularity that you could have is making forecasts",
    "start": "614660",
    "end": "620220"
  },
  {
    "text": "as a product store level because every that would be a unique time series then",
    "start": "620220",
    "end": "625980"
  },
  {
    "text": "you could start aggregating in different hierarchies like a Paul a one product in all the different",
    "start": "625980",
    "end": "633000"
  },
  {
    "text": "stores or one brand for every store or one brand for every different store and",
    "start": "633000",
    "end": "638040"
  },
  {
    "text": "also you start that's how you start building a hierarchies in Time series related tasks well strategic forecasting",
    "start": "638040",
    "end": "645720"
  },
  {
    "text": "is normally done when you have few unique combinations of IDs and you can start or train",
    "start": "645720",
    "end": "652860"
  },
  {
    "text": "um local models for every one of them if you start having a lot more of skus",
    "start": "652860",
    "end": "658320"
  },
  {
    "text": "and a lot more of different stores like for example Amazon 200 million skus across thousands of locations then local",
    "start": "658320",
    "end": "666420"
  },
  {
    "text": "models start to be very hard to train very hard to evaluate and they don't",
    "start": "666420",
    "end": "671459"
  },
  {
    "text": "really scale that well so that's when you start using Global models and and",
    "start": "671459",
    "end": "676680"
  },
  {
    "text": "I'm going to exemplify what I mean with this graph a local model means that you are feeding one individual model for",
    "start": "676680",
    "end": "683579"
  },
  {
    "text": "every series a global model means that you are feeling one model for one or",
    "start": "683579",
    "end": "689519"
  },
  {
    "text": "many series examples of the first ones are classical Auto regressive models",
    "start": "689519",
    "end": "695160"
  },
  {
    "text": "like arima Re Max but also other prominent statistical models like ETS and maybe even Facebook profit Global",
    "start": "695160",
    "end": "703620"
  },
  {
    "text": "models are what we normally refer to as classical machine learning models like a",
    "start": "703620",
    "end": "711660"
  },
  {
    "text": "grading boost increase or a also newer developments in in deep learning like",
    "start": "711660",
    "end": "718560"
  },
  {
    "text": "tpr n beats or even some of the models that we have helped create it like the end hits",
    "start": "718560",
    "end": "724800"
  },
  {
    "text": "well that's that's the introduction to the problem why is it important because it's",
    "start": "724800",
    "end": "731100"
  },
  {
    "text": "widely used why is it that challenging because with the new availability of",
    "start": "731100",
    "end": "736860"
  },
  {
    "text": "data and the growing availability of data scalability becomes an issue and",
    "start": "736860",
    "end": "742380"
  },
  {
    "text": "the first and the same for both local and Global models scaling this sort of",
    "start": "742380",
    "end": "748440"
  },
  {
    "text": "algorithms doing cross-validation at scale is not a trivial task and one of",
    "start": "748440",
    "end": "754019"
  },
  {
    "text": "the way one of the ways that we have found is most helpful to address this is by using Distributing Computing engines",
    "start": "754019",
    "end": "761579"
  },
  {
    "text": "like like Ray and libraries that have been created from the beginning or",
    "start": "761579",
    "end": "767639"
  },
  {
    "text": "decide from the beginning designed from the beginning to address the problems of scalability and one of the the projects",
    "start": "767639",
    "end": "775019"
  },
  {
    "text": "that we think uh or whatever we we did we started next the time series project",
    "start": "775019",
    "end": "781980"
  },
  {
    "text": "we're in series ecosystem with that whole idea in mind and we created this time series ecosystem",
    "start": "781980",
    "end": "788300"
  },
  {
    "text": "that helps address the problems that I just mentioned and offers a wide variety",
    "start": "788300",
    "end": "794700"
  },
  {
    "text": "of models in the classical statistical families but also for newer machine",
    "start": "794700",
    "end": "799920"
  },
  {
    "text": "learning models and neural forecasting models So today we're gonna be",
    "start": "799920",
    "end": "806360"
  },
  {
    "text": "showing you how to set up an environment in any scale or a great cluster how to",
    "start": "806360",
    "end": "813060"
  },
  {
    "text": "train local models and then how to train Global models and at the end we're going to show you some tooling around a model",
    "start": "813060",
    "end": "820200"
  },
  {
    "text": "performance evaluation so the first thing that we did is we created a an",
    "start": "820200",
    "end": "826079"
  },
  {
    "text": "account here at any scale it's really a very intuitive interface then we're going to be creating a cluster and once",
    "start": "826079",
    "end": "833220"
  },
  {
    "text": "we have a cluster then we can open a Jupiter notebook that is",
    "start": "833220",
    "end": "838680"
  },
  {
    "text": "connected to that cluster that's what we show up here in this in this notebook by the way very importantly this this",
    "start": "838680",
    "end": "845100"
  },
  {
    "text": "notebook is going to be shared with you so you don't have to to take any notes we're gonna share everything that we are",
    "start": "845100",
    "end": "852060"
  },
  {
    "text": "presenting so the first thing that we do is we set up the environment by creating this cluster on any scale we are",
    "start": "852060",
    "end": "858720"
  },
  {
    "text": "defining certain dependencies that we need in this case we're going to be using this particular image that you see",
    "start": "858720",
    "end": "864959"
  },
  {
    "text": "there this particular Docker image with this three libraries in this case we're",
    "start": "864959",
    "end": "871079"
  },
  {
    "text": "gonna run the examples using seven instances of these machines these machines have 16 cores and 64 gigabytes",
    "start": "871079",
    "end": "878220"
  },
  {
    "text": "of RAM once we have set up the data we can start playing around with it in this",
    "start": "878220",
    "end": "884519"
  },
  {
    "text": "particular case we're gonna prepare the data we're gonna use a very famous Benchmark data set from the N5",
    "start": "884519",
    "end": "891240"
  },
  {
    "text": "competition that actually has real a sales data from Walmart at the SKU level",
    "start": "891240",
    "end": "898680"
  },
  {
    "text": "for different stores the data is publicly available and there it is URL and the first thing that we need to",
    "start": "898680",
    "end": "905220"
  },
  {
    "text": "start playing around with this data set on a distributed matter is in it this",
    "start": "905220",
    "end": "910920"
  },
  {
    "text": "this Ray cluster here we can access the dashboard here and see that everything is running and",
    "start": "910920",
    "end": "917639"
  },
  {
    "text": "we can start reading the data with this simple repeat command here you have a",
    "start": "917639",
    "end": "924600"
  },
  {
    "text": "the head or the first series as you can see we have unique ID in this case the",
    "start": "924600",
    "end": "930540"
  },
  {
    "text": "unique ID is product a product number one from the foods category in this",
    "start": "930540",
    "end": "936420"
  },
  {
    "text": "particular store namely California store one there are n stores in California and other other other locations",
    "start": "936420",
    "end": "943519"
  },
  {
    "text": "to begin with we're going to do some data exploration everything that we're doing here is just selecting some IDs so",
    "start": "943519",
    "end": "950820"
  },
  {
    "text": "we can plot the series as you can see this is classical time series data for",
    "start": "950820",
    "end": "956940"
  },
  {
    "text": "those of you who work in the in the industry you're gonna notice that this particular data set exhibits classical",
    "start": "956940",
    "end": "963260"
  },
  {
    "text": "characteristics of retail data namely that it's a spark that it has a some",
    "start": "963260",
    "end": "970380"
  },
  {
    "text": "important gaps in missing values and that is it's it's it's it's it's it's a",
    "start": "970380",
    "end": "976320"
  },
  {
    "text": "hard problem in terms of filling different models so the first thing that we recommend",
    "start": "976320",
    "end": "982160"
  },
  {
    "text": "doing while working with time series data is fitting local models as I was",
    "start": "982160",
    "end": "987660"
  },
  {
    "text": "explaining at the beginning before next slide was very hard to fit local statistical models in a distributed",
    "start": "987660",
    "end": "994560"
  },
  {
    "text": "manner because the existing libraries didn't or weren't built for scale level computations and that's a what we",
    "start": "994560",
    "end": "1002420"
  },
  {
    "text": "created as you can see really the the start forecasting a a c in in this particular",
    "start": "1002420",
    "end": "1011180"
  },
  {
    "text": "case we're gonna be importing three models the auto ETS the dynamic",
    "start": "1011180",
    "end": "1016579"
  },
  {
    "text": "optimized data and decisional naive and we're gonna be fitting all of these",
    "start": "1016579",
    "end": "1022160"
  },
  {
    "text": "three models for all 30 000 series that means we're gonna be fitting 90 000",
    "start": "1022160",
    "end": "1027798"
  },
  {
    "text": "models the only thing that you need to do is to instantiate the stats forecast class it takes a list of models that we",
    "start": "1027799",
    "end": "1035600"
  },
  {
    "text": "defined previously the frequency of the data and we have an interesting parameter called the fallback model",
    "start": "1035600",
    "end": "1041780"
  },
  {
    "text": "which is very helpful for a time series forecasting at scale given",
    "start": "1041780",
    "end": "1047240"
  },
  {
    "text": "that some of these models might fail for specific series if the series contains",
    "start": "1047240",
    "end": "1052640"
  },
  {
    "text": "for example too few values so in case one of these models fails it automatically the stats forecast glass",
    "start": "1052640",
    "end": "1059419"
  },
  {
    "text": "is gonna replace that model with a with a fault tolerant model like decisional naive",
    "start": "1059419",
    "end": "1066860"
  },
  {
    "text": "um one of the interesting features of the Nix lab of the nixler verse is that we",
    "start": "1066860",
    "end": "1073220"
  },
  {
    "text": "designed all the libraries to take a this format of of this input format",
    "start": "1073220",
    "end": "1080780"
  },
  {
    "text": "where you have a unique ID that identifies the series a date stamp and the value that you're trying to predict",
    "start": "1080780",
    "end": "1086660"
  },
  {
    "text": "if you want to include exogenous variables you can also do that with with other columns but important thing here",
    "start": "1086660",
    "end": "1092660"
  },
  {
    "text": "is that you can have one unified data frame where you can fit many different",
    "start": "1092660",
    "end": "1098059"
  },
  {
    "text": "models so if you work with other libraries before you're gonna realize that one of the main difference is that",
    "start": "1098059",
    "end": "1104299"
  },
  {
    "text": "you don't have to iterate through all the different unique IDs and then iterate through all the different models",
    "start": "1104299",
    "end": "1109940"
  },
  {
    "text": "but you can fit uh three models for all different Series in one single call so",
    "start": "1109940",
    "end": "1116179"
  },
  {
    "text": "the only thing that you need to do is really focus on this a stats forecast",
    "start": "1116179",
    "end": "1121820"
  },
  {
    "text": "class dot forecast method where we pass the data set that we read before we",
    "start": "1121820",
    "end": "1127160"
  },
  {
    "text": "start average that we want to predict the next 28 Horizons and we compare it to pandas for easy easier manipulation",
    "start": "1127160",
    "end": "1134720"
  },
  {
    "text": "and what you see here before is the way that we connect to them to them to the few to the array cluster and here the",
    "start": "1134720",
    "end": "1143480"
  },
  {
    "text": "the time calls are there so we can see how long it takes to fit this 90 000",
    "start": "1143480",
    "end": "1149600"
  },
  {
    "text": "models and as you can see we were able to fit 90 000 models a little bit more",
    "start": "1149600",
    "end": "1156559"
  },
  {
    "text": "than 90 000 models in under 20 minutes with using seven machines in here we",
    "start": "1156559",
    "end": "1162980"
  },
  {
    "text": "took a screenshot of the dashboard when we were training this of the any scale dashboard and you can see that CPU utilization is quite High and the",
    "start": "1162980",
    "end": "1169880"
  },
  {
    "text": "computational being very efficiently distributed across the different machines",
    "start": "1169880",
    "end": "1175700"
  },
  {
    "text": "uh here is how the forecast looks like we have the unique ID and we have the timestamps of the future and then we",
    "start": "1175700",
    "end": "1183260"
  },
  {
    "text": "have the values for the different models this this this format makes it very easy",
    "start": "1183260",
    "end": "1191240"
  },
  {
    "text": "to evaluate the performance of the of the forecast we just need to to create",
    "start": "1191240",
    "end": "1196820"
  },
  {
    "text": "this evaluation function that is going to do some data wrangling and it's gonna uh give us a the different errors for",
    "start": "1196820",
    "end": "1206840"
  },
  {
    "text": "and the different models in this particular case uh don't be distracted",
    "start": "1206840",
    "end": "1212120"
  },
  {
    "text": "by the fact that we have different levels this is a peculiarity of the data set that we are using the way it is",
    "start": "1212120",
    "end": "1218179"
  },
  {
    "text": "evaluated given that it's hierarchical is across levels as I was explaining at the beginning but we can focus on this",
    "start": "1218179",
    "end": "1224179"
  },
  {
    "text": "column namely total and see the different error metrics that we have and for weighted would mean square error",
    "start": "1224179",
    "end": "1231860"
  },
  {
    "text": "metrics in this in this uh for this particular models so in summary it took",
    "start": "1231860",
    "end": "1238340"
  },
  {
    "text": "us around 18 minutes to train the Baseline models that we're going to be using as you can see here the best",
    "start": "1238340",
    "end": "1245900"
  },
  {
    "text": "performing model is the etx and then the worst performing model is decision alive",
    "start": "1245900",
    "end": "1251179"
  },
  {
    "text": "this means that we have a value added of using these models above a certain",
    "start": "1251179",
    "end": "1257780"
  },
  {
    "text": "Baseline but now we want to see if we can improve efficiency and or accuracy",
    "start": "1257780",
    "end": "1265039"
  },
  {
    "text": "using Global models as we were explaining at the beginning so we we",
    "start": "1265039",
    "end": "1270440"
  },
  {
    "text": "already have a strong Baseline for this Walmart data set we already were able to predict the next 28 observations for",
    "start": "1270440",
    "end": "1276740"
  },
  {
    "text": "this big retailer now we want to start iterating and make it better we're gonna use two",
    "start": "1276740",
    "end": "1283580"
  },
  {
    "text": "or three very famous machine learning models we're going to establish a baseline with a linear regression here",
    "start": "1283580",
    "end": "1290360"
  },
  {
    "text": "it's very important to to mention and this is something that we just released for for this conference today",
    "start": "1290360",
    "end": "1297440"
  },
  {
    "text": "next last ml forecast now supports all models for from sklearn so you can use",
    "start": "1297440",
    "end": "1303320"
  },
  {
    "text": "linear regression but you could also use support Vector machines or nearest neighbors or whatever you want to use to",
    "start": "1303320",
    "end": "1309260"
  },
  {
    "text": "hit in lasso Etc and but we are also announcing that you can",
    "start": "1309260",
    "end": "1315500"
  },
  {
    "text": "now use native the native integration of light GBM and actually Boost from Ray with Nick's last ml forecast to",
    "start": "1315500",
    "end": "1322400"
  },
  {
    "text": "distribute all your different uh time series tasks so what we're doing now is we're gonna use these three models to",
    "start": "1322400",
    "end": "1330919"
  },
  {
    "text": "fit all 30 000 series and evaluate that one of the amazing things of of the of",
    "start": "1330919",
    "end": "1337280"
  },
  {
    "text": "the of the stats forecast is that it makes it very easy to instantiate and do",
    "start": "1337280",
    "end": "1342620"
  },
  {
    "text": "a whole pipeline for ML distributed tasks so the",
    "start": "1342620",
    "end": "1348260"
  },
  {
    "text": "only thing that you need to do is create a list with the models that we're going to be using obviously you have to import them before uh Define the frequency same",
    "start": "1348260",
    "end": "1356480"
  },
  {
    "text": "as the stats forecast and then this particular step can be very complicated",
    "start": "1356480",
    "end": "1362900"
  },
  {
    "text": "when you are not using a Nexus ml if you want to create features for your machine",
    "start": "1362900",
    "end": "1368720"
  },
  {
    "text": "learning tasks like create locks or Lux transforms this could be a whole pipeline we support a lack a Feature",
    "start": "1368720",
    "end": "1376220"
  },
  {
    "text": "Feature engineering natively inside the ml forecast Labs the only thing that you",
    "start": "1376220",
    "end": "1382520"
  },
  {
    "text": "need to do is establish the lag values that you want to use in this case we're going to use one week and four weeks in",
    "start": "1382520",
    "end": "1388700"
  },
  {
    "text": "the past why because a monthly seasonality and weekly",
    "start": "1388700",
    "end": "1394039"
  },
  {
    "text": "seasonality might be very important for a retail data and then we can create",
    "start": "1394039",
    "end": "1399140"
  },
  {
    "text": "certain lag transforms like rolling means for the seven day rolling mean 28 rolling mean but also a seasonal rolling",
    "start": "1399140",
    "end": "1406580"
  },
  {
    "text": "means for different combinations this here is a very powerful feature we",
    "start": "1406580",
    "end": "1412460"
  },
  {
    "text": "think of the ammo forecast class so so feel free to play around with it and and use it",
    "start": "1412460",
    "end": "1418299"
  },
  {
    "text": "and you can also create date features for example a gear quartermon weekday",
    "start": "1418299",
    "end": "1424280"
  },
  {
    "text": "and day of week and at the end the only thing that we need to have this uh",
    "start": "1424280",
    "end": "1430220"
  },
  {
    "text": "distributed demo forecast object instantiated is a specified the engine configuration don't Focus too much on",
    "start": "1430220",
    "end": "1437000"
  },
  {
    "text": "this again this is how we communicate with Ray cluster so you just need to establish the number of partitions that",
    "start": "1437000",
    "end": "1443299"
  },
  {
    "text": "you want to use uh once you have established the the ml",
    "start": "1443299",
    "end": "1449539"
  },
  {
    "text": "forecast class or the distributed demo forecast class you simply call the fit a",
    "start": "1449539",
    "end": "1454640"
  },
  {
    "text": "method on that class by passing it the ray series that we defined before",
    "start": "1454640",
    "end": "1459880"
  },
  {
    "text": "identifying the unique ID column identifying the timestamp column identifying the target column and then",
    "start": "1459880",
    "end": "1466520"
  },
  {
    "text": "we can simply call the ammo forecast.predict for the next 28",
    "start": "1466520",
    "end": "1471740"
  },
  {
    "text": "observations and we're gonna see that one of the promises that we did at the beginning",
    "start": "1471740",
    "end": "1478159"
  },
  {
    "text": "namely that Global models are faster it does indeed hold the whole pipeline",
    "start": "1478159",
    "end": "1485299"
  },
  {
    "text": "takes a 9.7 minutes so we are a little bit below uh the total time of the",
    "start": "1485299",
    "end": "1493280"
  },
  {
    "text": "statistical methods this means this is twice as fast as the first attempt that",
    "start": "1493280",
    "end": "1498679"
  },
  {
    "text": "we did it and it's also being a it's using very efficiently all the different",
    "start": "1498679",
    "end": "1504620"
  },
  {
    "text": "resources that we have available for computation the forecast looks the same we have the unique ID date stamp and the",
    "start": "1504620",
    "end": "1511640"
  },
  {
    "text": "different values for the models that we used this is this is a great feature because this means that you can easily",
    "start": "1511640",
    "end": "1518360"
  },
  {
    "text": "append the results from stats forecasts and the results for ML forecast to do evaluation and here we do the exact same",
    "start": "1518360",
    "end": "1525559"
  },
  {
    "text": "function evaluate the the different error metrics for the models and we can see",
    "start": "1525559",
    "end": "1531260"
  },
  {
    "text": "that using the action boost forecast we can achieve uh three percent improvement",
    "start": "1531260",
    "end": "1538760"
  },
  {
    "text": "over statistical methods and gain a hundred percent Improvement in terms of",
    "start": "1538760",
    "end": "1545120"
  },
  {
    "text": "a computational efficiency the the wonderful thing is that we don't",
    "start": "1545120",
    "end": "1550820"
  },
  {
    "text": "have to stop there we can take the best of the Two Worlds and create an ensemble",
    "start": "1550820",
    "end": "1556340"
  },
  {
    "text": "um I'm gonna kind of spend that much time explaining why assembles are good but they help in summary increase",
    "start": "1556340",
    "end": "1562100"
  },
  {
    "text": "accuracy they make the predictions more robust they also improve generalization",
    "start": "1562100",
    "end": "1568039"
  },
  {
    "text": "uh features of of the forecast and they offer certain flexibility in this case",
    "start": "1568039",
    "end": "1573980"
  },
  {
    "text": "we're gonna introduce one of the simplest and Sample that one can make namely take all the different",
    "start": "1573980",
    "end": "1580700"
  },
  {
    "text": "predictions that we generated and create a median so the only thing that we're",
    "start": "1580700",
    "end": "1585799"
  },
  {
    "text": "doing is creating taking the medium value for all of these different forecasts and here you can see we have a",
    "start": "1585799",
    "end": "1592460"
  },
  {
    "text": "new model namely this median Ensemble which is nothing else but a median aggregation of the of the of the former",
    "start": "1592460",
    "end": "1599659"
  },
  {
    "text": "models and here we show a way to calculate the the Improvement and at the",
    "start": "1599659",
    "end": "1607700"
  },
  {
    "text": "end you can see that you can achieve a four percent Improvement of over the",
    "start": "1607700",
    "end": "1615080"
  },
  {
    "text": "best Global model so you can achieve a significant improvement over a baselines",
    "start": "1615080",
    "end": "1621500"
  },
  {
    "text": "and a very good Improvement in respect to classical statistical methods and",
    "start": "1621500",
    "end": "1627200"
  },
  {
    "text": "classical models Global methods so in summary this notebook shows how you can",
    "start": "1627200",
    "end": "1634460"
  },
  {
    "text": "Leverage The Power of a distributed engine like Ray and a distributed",
    "start": "1634460",
    "end": "1639500"
  },
  {
    "text": "libraries like ml forecast from nyxla and stats forecast for next LA from next Lab to create an end-to-end pipeline to",
    "start": "1639500",
    "end": "1647840"
  },
  {
    "text": "create to to do a forecasts at scale so again here we are forecasting more than",
    "start": "1647840",
    "end": "1653120"
  },
  {
    "text": "30 000 series the whole M5 competition in under 30 minutes achieving is a",
    "start": "1653120",
    "end": "1658820"
  },
  {
    "text": "wonderful results by using this exact Pipeline and including exogenous variables that are",
    "start": "1658820",
    "end": "1666020"
  },
  {
    "text": "also available in the competition you could actually be in the top five",
    "start": "1666020",
    "end": "1671179"
  },
  {
    "text": "percent of the M5 a leaderboard so we encourage you to go and and try this for",
    "start": "1671179",
    "end": "1678020"
  },
  {
    "text": "yourself and if there's a time for questions we wanted to keep the presentation short and and give you the",
    "start": "1678020",
    "end": "1686059"
  },
  {
    "text": "opportunity to to do to ask your questions to me or to feather thank you very much you also",
    "start": "1686059",
    "end": "1692840"
  },
  {
    "text": "would be our presentation thanks a lot Max uh thanks a lot for the very detailed technical presentation of how",
    "start": "1692840",
    "end": "1699140"
  },
  {
    "text": "we're actually using the notebook and how you could actually scale uh at the level that you're thinking about using",
    "start": "1699140",
    "end": "1705620"
  },
  {
    "text": "using the any scale and Ray cluster uh I don't see any question but I think a couple of things popped into my mind",
    "start": "1705620",
    "end": "1711380"
  },
  {
    "text": "when you showed the diagram about you know forecasting has sort of two different parts one is the one is the",
    "start": "1711380",
    "end": "1719000"
  },
  {
    "text": "strategy the other one is operational now you have this little tree that actually says operational models and",
    "start": "1719000",
    "end": "1724700"
  },
  {
    "text": "strategic models in your um in your working with with people who you who are",
    "start": "1724700",
    "end": "1732080"
  },
  {
    "text": "doing this where do you think is the ratio are people building more strategic models or people are building more",
    "start": "1732080",
    "end": "1737600"
  },
  {
    "text": "operation models or there isn't uh even even uh distribution of that",
    "start": "1737600",
    "end": "1743779"
  },
  {
    "text": "Jim I I think we have seen more people",
    "start": "1743779",
    "end": "1749419"
  },
  {
    "text": "trying to build strategic models it's the classical approach of using",
    "start": "1749419",
    "end": "1755299"
  },
  {
    "text": "statistical models for time series but one of the Hidden agendas that we have",
    "start": "1755299",
    "end": "1761360"
  },
  {
    "text": "by creating this Technologies and and pairing it with a Technologies like Ray",
    "start": "1761360",
    "end": "1766640"
  },
  {
    "text": "is to really make the boundaries um more blurry",
    "start": "1766640",
    "end": "1772240"
  },
  {
    "text": "I think that what you can achieve now with a stats forecast with distributed",
    "start": "1772240",
    "end": "1778159"
  },
  {
    "text": "engines engines is you can really start fitting thousands of of or even millions",
    "start": "1778159",
    "end": "1784580"
  },
  {
    "text": "of series we have published different benchmarks where we show that you can use Rey and nyxla to forecast or do",
    "start": "1784580",
    "end": "1792020"
  },
  {
    "text": "Baseline models in on more than one million Series in less than 30 minutes",
    "start": "1792020",
    "end": "1797299"
  },
  {
    "text": "and and I think that's why we are seeing more and more companies replacing their",
    "start": "1797299",
    "end": "1802940"
  },
  {
    "text": "entire 4 casting systems with nixler and doing both strategic and operational and",
    "start": "1802940",
    "end": "1809179"
  },
  {
    "text": "then in sampling so so to to make the answer very concise",
    "start": "1809179",
    "end": "1815419"
  },
  {
    "text": "we think like right now 70 of the people are doing strategic 30 percent are doing",
    "start": "1815419",
    "end": "1821000"
  },
  {
    "text": "operational but we forecast that more and more people are gonna start using both and leveraging the best approaches",
    "start": "1821000",
    "end": "1828320"
  },
  {
    "text": "okay and when you when you were showing all the time series data in did you",
    "start": "1828320",
    "end": "1833419"
  },
  {
    "text": "actually use any particular toolkit uh you know the the little vertical lines or was just something that's actually",
    "start": "1833419",
    "end": "1838880"
  },
  {
    "text": "part of the ml4 ml ml Forest yes this is this is actually a utility that we use",
    "start": "1838880",
    "end": "1847039"
  },
  {
    "text": "in um from the stats forecast Library okay",
    "start": "1847039",
    "end": "1852860"
  },
  {
    "text": "that we have and it's very nice because you can do like Explorations here and",
    "start": "1852860",
    "end": "1858200"
  },
  {
    "text": "and and you can also it has some nice features that takes a random series and",
    "start": "1858200",
    "end": "1864799"
  },
  {
    "text": "plots them um so it's very useful for for EDS you can also specify a a",
    "start": "1864799",
    "end": "1873519"
  },
  {
    "text": "engine so you can print this this is this is Javascript using plotly under the hood but this is part of the of the",
    "start": "1874059",
    "end": "1882440"
  },
  {
    "text": "stats forecast package now is that is that something they have to install pip install it actually comes",
    "start": "1882440",
    "end": "1887899"
  },
  {
    "text": "as part of the uh the the next package it's part of the stats forecast package",
    "start": "1887899",
    "end": "1893179"
  },
  {
    "text": "so if you if you just import from them you should",
    "start": "1893179",
    "end": "1898279"
  },
  {
    "text": "be able to use the utility yeah so the the only the only three requirements to reproduce everything here is obviously",
    "start": "1898279",
    "end": "1904159"
  },
  {
    "text": "right so it seems like you know when I looked at when I looked at the way you were",
    "start": "1904159",
    "end": "1909799"
  },
  {
    "text": "actually training the model you're actually using a different function and the same data right so you actually have the same data set but you were sharding",
    "start": "1909799",
    "end": "1916760"
  },
  {
    "text": "certain data based on on unique ID this could be a zip code or this could be",
    "start": "1916760",
    "end": "1922460"
  },
  {
    "text": "in the financial district this could be a stock uh identification right and so",
    "start": "1922460",
    "end": "1927740"
  },
  {
    "text": "you would actually have these three different models for you know 10 amount of stock series over a period of time",
    "start": "1927740",
    "end": "1934159"
  },
  {
    "text": "you can actually do forecasting is that is that the correct way of putting it yes absolutely uh that's that's the",
    "start": "1934159",
    "end": "1941360"
  },
  {
    "text": "whole idea behind it you can take one data frame with all your different stock data or all your different retail data",
    "start": "1941360",
    "end": "1947059"
  },
  {
    "text": "and in in in a few line of a few lines of code fit a three statistical models",
    "start": "1947059",
    "end": "1953960"
  },
  {
    "text": "and three machine learning models and then in Sample them and and and and and we think that's great because you don't",
    "start": "1953960",
    "end": "1959179"
  },
  {
    "text": "have to do a lot of data wrangling to start gaining the best of the",
    "start": "1959179",
    "end": "1964340"
  },
  {
    "text": "statistical world and the Machine learning world right",
    "start": "1964340",
    "end": "1969799"
  },
  {
    "text": "um okay I see I see a question from um oh of our one of our panelists so so",
    "start": "1969799",
    "end": "1976700"
  },
  {
    "text": "GC companies serving these thousands of models at once for live predictions or",
    "start": "1976700",
    "end": "1981919"
  },
  {
    "text": "would they be hosting it thousands of micro services to do this we we have seen both Jay and that's",
    "start": "1981919",
    "end": "1988700"
  },
  {
    "text": "that's a very interesting question we have seen we have seen the following we have seen people re-running the models",
    "start": "1988700",
    "end": "1995059"
  },
  {
    "text": "uh uh every every I don't know five minutes and trying to do near real time",
    "start": "1995059",
    "end": "2001120"
  },
  {
    "text": "anomaly detection and they are like not even serving the models but retraining",
    "start": "2001120",
    "end": "2006340"
  },
  {
    "text": "them on on the spot this is more expensive in terms of computation but it's more accurate",
    "start": "2006340",
    "end": "2012880"
  },
  {
    "text": "because you are including the latest observations that's that we have seen that in production and one very uh right",
    "start": "2012880",
    "end": "2019240"
  },
  {
    "text": "here one very big right sharing company is doing that we have also seen people or companies that pre-train the models",
    "start": "2019240",
    "end": "2025840"
  },
  {
    "text": "and then expose them globally and do distributed uh inference so they",
    "start": "2025840",
    "end": "2031000"
  },
  {
    "text": "distribute the calls for like online predictions and then we have also seen a",
    "start": "2031000",
    "end": "2038019"
  },
  {
    "text": "more Edge use cases where people pre-training on a different data set and",
    "start": "2038019",
    "end": "2043480"
  },
  {
    "text": "doing transfer learning for a Time series and and and in that particular case the",
    "start": "2043480",
    "end": "2050200"
  },
  {
    "text": "we have seen micro services that take the the API call and try to",
    "start": "2050200",
    "end": "2056618"
  },
  {
    "text": "resolve it on the pre-trained model and and but the microservices part is more on the like API call stand on the real",
    "start": "2056619",
    "end": "2063520"
  },
  {
    "text": "model training or model events but I don't know if that was clear Jay yeah does it answer your question",
    "start": "2063520",
    "end": "2070240"
  },
  {
    "text": "yeah very cool thanks uh one last question before we actually go on so you you showed that you actually abstracted",
    "start": "2070240",
    "end": "2076898"
  },
  {
    "text": "your your DL Forest class you have a top level python class you actually created and then you really decoratively sort of",
    "start": "2076899",
    "end": "2084040"
  },
  {
    "text": "Define all the parameters that you actually want this to train on how does that how does that translate to",
    "start": "2084040",
    "end": "2091480"
  },
  {
    "text": "to where you actually how are you using Ray so does this does this translate into a reactor that actually goes It",
    "start": "2091480",
    "end": "2098680"
  },
  {
    "text": "goes it and does a distributed training could you just go one level below and and say what Ray core functionality that",
    "start": "2098680",
    "end": "2105580"
  },
  {
    "text": "you're actually using to do the distributed training are you using tasks are you using reactors to do this remote",
    "start": "2105580",
    "end": "2112780"
  },
  {
    "text": "distribute training um whether you you want to answer this one yeah sure so basically we are we are",
    "start": "2112780",
    "end": "2120160"
  },
  {
    "text": "using an amazing Library which is uh called a field and and field allows you",
    "start": "2120160",
    "end": "2126460"
  },
  {
    "text": "to to distribute the the pre-processing part of the for example the ml forecast library and in an efficient manner so",
    "start": "2126460",
    "end": "2134800"
  },
  {
    "text": "you only have to pass the rate data set and a few handles the the the",
    "start": "2134800",
    "end": "2140740"
  },
  {
    "text": "partitioning and the distribution part of the of the pre-processing and after that once the the",
    "start": "2140740",
    "end": "2148079"
  },
  {
    "text": "preprocessing data is is ready then we just simply called the the model we want",
    "start": "2148079",
    "end": "2155560"
  },
  {
    "text": "to to fit and after that the the third part of the of the pipeline basically",
    "start": "2155560",
    "end": "2160920"
  },
  {
    "text": "uses the affiliate model to um to generate the the predictions again",
    "start": "2160920",
    "end": "2167680"
  },
  {
    "text": "using using few yeah so those fig is the one that actually does does the the",
    "start": "2167680",
    "end": "2174040"
  },
  {
    "text": "implementation of reactors and Ray tasks to distributed across the ray cluster so presumably it presumably it's taking",
    "start": "2174040",
    "end": "2180700"
  },
  {
    "text": "your uh your declarative parameters and creating array task to go ahead and",
    "start": "2180700",
    "end": "2185859"
  },
  {
    "text": "create a functions the fuse is the one that's doing the orchestrating and talking to the red cluster is that okay",
    "start": "2185859",
    "end": "2191680"
  },
  {
    "text": "all right there's another question from Farzad where are some of the new",
    "start": "2191680",
    "end": "2196839"
  },
  {
    "text": "development for next slide packages that you are looking forward in 2023",
    "start": "2196839",
    "end": "2202300"
  },
  {
    "text": "hey yep thank you we we have um a very active member in the community called",
    "start": "2202300",
    "end": "2207880"
  },
  {
    "text": "farsat if that's you thank you for all your contributions if if not we invite you to be part of the community and help",
    "start": "2207880",
    "end": "2214660"
  },
  {
    "text": "us build that so we're gonna be trying to find the most",
    "start": "2214660",
    "end": "2222400"
  },
  {
    "text": "requested features uh that we have been hearing for the community and try to",
    "start": "2222400",
    "end": "2227560"
  },
  {
    "text": "integrate them in a different Library so that's that's you're gonna see a lot of us trying to be more responsive to the",
    "start": "2227560",
    "end": "2233619"
  },
  {
    "text": "community and integrating features that they are asking for more generally speaking you can expect conformal",
    "start": "2233619",
    "end": "2240280"
  },
  {
    "text": "prediction in all of the three libraries that's forecast ml forecast neural forecast you can expect",
    "start": "2240280",
    "end": "2246720"
  },
  {
    "text": "multivariate deep learning models in your forecast that's going to be exciting because it offers the",
    "start": "2246720",
    "end": "2252400"
  },
  {
    "text": "possibility to start thinking about casual or Ranger cultural relationships",
    "start": "2252400",
    "end": "2257440"
  },
  {
    "text": "in data and that's exciting uh in terms of stats forecast we have a couple of",
    "start": "2257440",
    "end": "2262660"
  },
  {
    "text": "models that we want to include pivots I think it's the last big model that we need",
    "start": "2262660",
    "end": "2269020"
  },
  {
    "text": "um and in ammo forecast you can expect more work on under documentation and in",
    "start": "2269020",
    "end": "2274119"
  },
  {
    "text": "general in all three libraries we want to work a little bit around helping people deploy this into production so",
    "start": "2274119",
    "end": "2281020"
  },
  {
    "text": "for example being able to save a pre-trained models and call forward methods on on them and they used a and",
    "start": "2281020",
    "end": "2290260"
  },
  {
    "text": "include them in unified pipelines so you can run all three libraries in a big pipeline so those those are the general",
    "start": "2290260",
    "end": "2296380"
  },
  {
    "text": "ideas for for the next eight months first up thank you for your question uh Max one last question before we move on",
    "start": "2296380",
    "end": "2303099"
  },
  {
    "text": "um I know you're based in New York and forecasting financial markets is huge there are you actually seeing any",
    "start": "2303099",
    "end": "2309400"
  },
  {
    "text": "clients who are actually using nixler today to do the forecasting of stocks I know this is I mean if it's if it's a",
    "start": "2309400",
    "end": "2316420"
  },
  {
    "text": "proprietary question you can answer then you don't have to but but in in terms of where is the big use cases people are",
    "start": "2316420",
    "end": "2323380"
  },
  {
    "text": "actually using next slide today yeah we're if there are different users cases of different Industries but I'm going to",
    "start": "2323380",
    "end": "2330160"
  },
  {
    "text": "speak more about the financial thing because I think it's it's it's better for the audience we are indeed working",
    "start": "2330160",
    "end": "2336460"
  },
  {
    "text": "with some Financial researchers to see how we could use Nick's lab to forecast",
    "start": "2336460",
    "end": "2341740"
  },
  {
    "text": "certain derivative metrics of financial markets that being said we think one of the",
    "start": "2341740",
    "end": "2349380"
  },
  {
    "text": "riskiest things that you can do with time series is assume that you can simply take nixler Andre and forecast",
    "start": "2349380",
    "end": "2355780"
  },
  {
    "text": "the stock market and start trading so if you are doing that don't do that it's probably not gonna work and then in",
    "start": "2355780",
    "end": "2362200"
  },
  {
    "text": "terms of who is using Us in that market it's very hard to say because one of the like secrets of the quantitative hedge",
    "start": "2362200",
    "end": "2370420"
  },
  {
    "text": "funds is the algorithms that they use to forecast so it's tomorrow JPMorgan would say like yeah we're using autorimas to",
    "start": "2370420",
    "end": "2377560"
  },
  {
    "text": "forecast the market with this and this exogenous variables then the whole like secret mistake around such a hedge fund",
    "start": "2377560",
    "end": "2384640"
  },
  {
    "text": "would fall so so we don't know if they're using us and if they are using they're not going to tell us so so it's",
    "start": "2384640",
    "end": "2391780"
  },
  {
    "text": "good maybe maybe the question will come up tomorrow because they're going to have live audience uh probably a lot of",
    "start": "2391780",
    "end": "2397599"
  },
  {
    "text": "them from the financial Market one last question I promise one last question before Jay comes on",
    "start": "2397599",
    "end": "2403540"
  },
  {
    "text": "um how do I get started with Nick's slime and can you can you share something on the uh on the chat with",
    "start": "2403540",
    "end": "2409599"
  },
  {
    "text": "where they should where they should go if they actually wanted to download next what's the GitHub URL and and uh how do",
    "start": "2409599",
    "end": "2417640"
  },
  {
    "text": "I start getting to the documentation thank you Jules I'm going to prepare a",
    "start": "2417640",
    "end": "2422980"
  },
  {
    "text": "little a little message here on the on the chat uh so people can just click it right all right or alternatively",
    "start": "2422980",
    "end": "2429579"
  },
  {
    "text": "alternately you know what you can do is actually um share a couple of slides with me along with the URL to the to the uh to",
    "start": "2429579",
    "end": "2435880"
  },
  {
    "text": "the notebook and then I'll I'll post it as part of the uh presentation along with the video and share with the",
    "start": "2435880",
    "end": "2442119"
  },
  {
    "text": "community so anybody who's attended you can actually get to the URL and get all the documentation all the resources that",
    "start": "2442119",
    "end": "2447280"
  },
  {
    "text": "actually need for them to get started with nixler and Ray and then um",
    "start": "2447280",
    "end": "2452500"
  },
  {
    "text": "um yeah I'll I'll share that along with the notebook with that said thanks a lot for Draco and thanks a lot Max uh for",
    "start": "2452500",
    "end": "2459579"
  },
  {
    "text": "your wonderful presentation quite insightful to see how forecastling has been done at scale both using Rey and",
    "start": "2459579",
    "end": "2465400"
  },
  {
    "text": "especially at any scale and uh hopefully you have the uh we have a good live audience tomorrow as well so good luck",
    "start": "2465400",
    "end": "2472240"
  },
  {
    "text": "tomorrow all right Jay you're next all right very sad that I'm not gonna get rich or free at Nixon just forecasting",
    "start": "2472240",
    "end": "2479200"
  },
  {
    "text": "this stock market um cool so yeah uh let me share this yet",
    "start": "2479200",
    "end": "2487000"
  },
  {
    "text": "but yeah so I'm really excited today to present everybody uh Daft which is the",
    "start": "2487000",
    "end": "2492040"
  },
  {
    "text": "python distributed data frame for complex data um Daft is a data frame Library like you",
    "start": "2492040",
    "end": "2497320"
  },
  {
    "text": "know pandas or polars or Pi spark if you use those it's fairly uh young it's a",
    "start": "2497320",
    "end": "2502540"
  },
  {
    "text": "fairly young project it's about five months old and it runs completely distributed on record and that's why we're here talking",
    "start": "2502540",
    "end": "2508660"
  },
  {
    "text": "um I'm especially excited to give this talk because that I think is really complementary to all of the uh Ray",
    "start": "2508660",
    "end": "2514480"
  },
  {
    "text": "ecosystem like Ray data retune reserve and we hope it can be you know useful to you as part of your toolkit when you",
    "start": "2514480",
    "end": "2521140"
  },
  {
    "text": "work with data in brain um so a little bit about me so my name",
    "start": "2521140",
    "end": "2526240"
  },
  {
    "text": "is Jay I am a co-founder at Adventure Computing um and I was a software lead at phenome",
    "start": "2526240",
    "end": "2532240"
  },
  {
    "text": "and nif level five uh it was in biotech and autonomous driving and I've been building you know these distributed data",
    "start": "2532240",
    "end": "2538599"
  },
  {
    "text": "systems for machine learning for a while now and uh across these interesting domains where you have like genomic data",
    "start": "2538599",
    "end": "2545020"
  },
  {
    "text": "and like lidar data and images and that kind of uh prompted me to think about like hey what would working with these",
    "start": "2545020",
    "end": "2551800"
  },
  {
    "text": "pieces of complex data look like at scale and that's why we built depth and so now I'm a maintainer of Daft and uh",
    "start": "2551800",
    "end": "2558339"
  },
  {
    "text": "yeah we love data frames and we think that data frames are kind of the future uh for for data processing",
    "start": "2558339",
    "end": "2565180"
  },
  {
    "text": "foreign and so yeah let's dive a little bit deeper what is that really so I'm gonna",
    "start": "2565180",
    "end": "2570760"
  },
  {
    "text": "throw out a few descriptions and we'll go a bit deeper into uh some of them um so that is a python library right you",
    "start": "2570760",
    "end": "2577119"
  },
  {
    "text": "can pip install it use it from your notebooks you just do that using pip install or get Daft",
    "start": "2577119",
    "end": "2582220"
  },
  {
    "text": "um it is a data free so if you're familiar with data frame libraries they have you know they're made of cables",
    "start": "2582220",
    "end": "2588220"
  },
  {
    "text": "every table has rows and columns uh you can do operations like join them together you can you know filter the",
    "start": "2588220",
    "end": "2594160"
  },
  {
    "text": "roads you can filter The Columns modify the columns uh Etc um Daft is by default distributed with",
    "start": "2594160",
    "end": "2601180"
  },
  {
    "text": "Ray and so you can run it on a cluster of machines using Ray right to leverage more hardware and even gpus for larger",
    "start": "2601180",
    "end": "2607900"
  },
  {
    "text": "data sets and last of all which is really interesting adapt is built for complex data which is not something",
    "start": "2607900",
    "end": "2613720"
  },
  {
    "text": "you'll usually see with a lot of these data frame libraries so the idea with Daft is that why should you know images",
    "start": "2613720",
    "end": "2620020"
  },
  {
    "text": "be any different from my strings why should videos be any different from my numbers right you should just be able to",
    "start": "2620020",
    "end": "2625060"
  },
  {
    "text": "put all of them in a data frame and work with them as if they were part of this data frame and so sometimes it's easier",
    "start": "2625060",
    "end": "2631300"
  },
  {
    "text": "for me to think of depth as just this like table right where you have that images Us in the columns you have you know PDFs or documents in the columns",
    "start": "2631300",
    "end": "2637980"
  },
  {
    "text": "jsons protobots audio files and all that and kind of visualize that as being this",
    "start": "2637980",
    "end": "2643180"
  },
  {
    "text": "giant distributed table of this complex data so I'll give those a deeper dive into",
    "start": "2643180",
    "end": "2650319"
  },
  {
    "text": "what we mean when we say complex data we see that data kind of lies on the Spectrum right between primitive and",
    "start": "2650319",
    "end": "2656920"
  },
  {
    "text": "complex the most primitive end of things we have you know in most programming languages you have primitive data types",
    "start": "2656920",
    "end": "2663280"
  },
  {
    "text": "that are Atomic like integers 42 strings like true a float value like Pi here",
    "start": "2663280",
    "end": "2669460"
  },
  {
    "text": "3.14 and boolean's like true and false and when you want data to be more useful",
    "start": "2669460",
    "end": "2676000"
  },
  {
    "text": "you start composing them right into these objects and um and so then you start taking on some",
    "start": "2676000",
    "end": "2682060"
  },
  {
    "text": "additional meaning for example when you put two numbers together into a latitude longitude it doesn't just it doesn't",
    "start": "2682060",
    "end": "2687579"
  },
  {
    "text": "just a pair of numbers anymore it now represents a location on Earth right a geographical coordinate or when you're",
    "start": "2687579",
    "end": "2693280"
  },
  {
    "text": "trying to represent a person and you put a string together with a uh a number that could be the person's name and a",
    "start": "2693280",
    "end": "2699880"
  },
  {
    "text": "person's age right and then when you get to the most complex end of the spectrum with something like an image",
    "start": "2699880",
    "end": "2706240"
  },
  {
    "text": "um really you can think of an image as being composed of these paradigms which is maybe a height of it then a buffer of",
    "start": "2706240",
    "end": "2712599"
  },
  {
    "text": "pixel values right which is just numbers and so there's a lot of pivoters in there but at the core of it it's fairly",
    "start": "2712599",
    "end": "2719319"
  },
  {
    "text": "structured and and you know just made up a lot of a lot more Primitives and so with complex data",
    "start": "2719319",
    "end": "2725440"
  },
  {
    "text": "um these are the things that make data complex first of all it's composed of more Primitives",
    "start": "2725440",
    "end": "2730839"
  },
  {
    "text": "um an image for example could have you know a million pixels and that's a lot more Primitives uh per item in your in",
    "start": "2730839",
    "end": "2737200"
  },
  {
    "text": "your data set right compared to just a single number um oftentimes complex data has very domain specific semantics so the example",
    "start": "2737200",
    "end": "2744460"
  },
  {
    "text": "we gave from before a geographical coordinate right it's more than just a pair of numbers",
    "start": "2744460",
    "end": "2750760"
  },
  {
    "text": "um you know you can think of it as a location on Earth and because it's domain specific you want to run domain",
    "start": "2750760",
    "end": "2756460"
  },
  {
    "text": "specific functions on it for example give me the the zip code of this geographical coordinate or you know give",
    "start": "2756460",
    "end": "2762520"
  },
  {
    "text": "me the distance between two geographical coordinates right those are all very domain specific uh functions and and",
    "start": "2762520",
    "end": "2769800"
  },
  {
    "text": "algorithms that you will be running and last of all uh complex data is often very hard to represent I have here an",
    "start": "2769800",
    "end": "2776380"
  },
  {
    "text": "example of videos and videos are surprisingly hard to represent because you have kind of these anchor frames and",
    "start": "2776380",
    "end": "2781780"
  },
  {
    "text": "then you have this these different frames in between uh in order for you to be able to access the data that you want",
    "start": "2781780",
    "end": "2787720"
  },
  {
    "text": "to access and so how does that help here right so first of all because complex data it's",
    "start": "2787720",
    "end": "2793420"
  },
  {
    "text": "just composed of more uh Primitives and it's just larger and requires more compute we can actually leverage rate",
    "start": "2793420",
    "end": "2799180"
  },
  {
    "text": "for distributed computing right and that that gives us the ability to use more cores more gpus to process uh complex",
    "start": "2799180",
    "end": "2805960"
  },
  {
    "text": "data we also have the easy execution and query optimization to efficiently run",
    "start": "2805960",
    "end": "2812079"
  },
  {
    "text": "your queries and and make sure that you know we're not running any uh extra unneeded computation",
    "start": "2812079",
    "end": "2819400"
  },
  {
    "text": "um secondly to address the issue that complexity is often very domain specific we allow you to run any arbitrary python",
    "start": "2819400",
    "end": "2826359"
  },
  {
    "text": "function as what we call a gdf or user defined function in Daft we're also",
    "start": "2826359",
    "end": "2832119"
  },
  {
    "text": "working on a lot of interesting complex data kernels right so you know kernels for example the crop images or to",
    "start": "2832119",
    "end": "2838000"
  },
  {
    "text": "convert images between color spaces and that's coming soon and lastly which is a",
    "start": "2838000",
    "end": "2843460"
  },
  {
    "text": "really interesting problem that complex data is hard to represent uh coming soon also that will be working on a lot of",
    "start": "2843460",
    "end": "2849099"
  },
  {
    "text": "complex data types for you to things such as images tensors documents embeddings and these interesting data",
    "start": "2849099",
    "end": "2856720"
  },
  {
    "text": "types as native Dev types in your column so why why am I here why why are we",
    "start": "2856720",
    "end": "2862420"
  },
  {
    "text": "talking about death at a rate conference and we meet up well uh you know Daft works really really well with me",
    "start": "2862420",
    "end": "2869020"
  },
  {
    "text": "um and we leverage all of Ray core to to do this um and this also means that Daft will work really well with all of your",
    "start": "2869020",
    "end": "2874960"
  },
  {
    "text": "existing great tooling like read data sets retune and Reserve since it runs on the same cluster and uses the same",
    "start": "2874960",
    "end": "2881200"
  },
  {
    "text": "abstractions uh to run right so under the hood let's speak a little bit under the hood how the staff work",
    "start": "2881200",
    "end": "2887200"
  },
  {
    "text": "um really Daft is just this kind of a virtual table and it points to these partitions of smaller tables which we",
    "start": "2887200",
    "end": "2893619"
  },
  {
    "text": "call partitions uh and restore them on the recluster as Ray objects and these",
    "start": "2893619",
    "end": "2899500"
  },
  {
    "text": "uh partitions are actually just in the Apache Arrow format if this looks familiar to you you might be uh familiar",
    "start": "2899500",
    "end": "2906460"
  },
  {
    "text": "with the data set which actually looks very similar right array data set is also kind of this like big I guess a",
    "start": "2906460",
    "end": "2912819"
  },
  {
    "text": "collection of partitions and the partitions are also made out of error and that means that conversion between a",
    "start": "2912819",
    "end": "2917980"
  },
  {
    "text": "death data frame and a red data set is super easy and we'll see that a little bit more in the demo um but yeah it's",
    "start": "2917980",
    "end": "2923319"
  },
  {
    "text": "very cheap and and that allows you to kind of interoperate Daft with your training and machine learning pipelines",
    "start": "2923319",
    "end": "2929680"
  },
  {
    "text": "really effectively and that's how we see Daft fitting into the whole ecosystem right so for this demo we'll see an",
    "start": "2929680",
    "end": "2936339"
  },
  {
    "text": "example of how we can use theft to query your data storage in this case we're",
    "start": "2936339",
    "end": "2941800"
  },
  {
    "text": "querying the cocoa image image data set and then we can do all all the querying",
    "start": "2941800",
    "end": "2947079"
  },
  {
    "text": "and joining and filters you know ETL data processing or analytics uh interactive data science and then when",
    "start": "2947079",
    "end": "2954520"
  },
  {
    "text": "we're ready to do machine learning we can kind of hand that off by converting into a red data set and then do",
    "start": "2954520",
    "end": "2959680"
  },
  {
    "text": "everything machine learning related and read pop it back into adapt to do uh you know any analytics or or uh uh you know",
    "start": "2959680",
    "end": "2966720"
  },
  {
    "text": "evaluating our model performance and so let's pop straight into our demo",
    "start": "2966720",
    "end": "2972940"
  },
  {
    "text": "um and this is kind of the overview of the demo but um for everyone who is not maybe as familiar with the cocoa dataset this is",
    "start": "2972940",
    "end": "2980319"
  },
  {
    "text": "kind of what that looks like it's a bunch of images with um you know these segmentation masks on",
    "start": "2980319",
    "end": "2985480"
  },
  {
    "text": "each image which tells you also uh what kind of object it is for example these are dogs and also it gives you a box",
    "start": "2985480",
    "end": "2992200"
  },
  {
    "text": "over uh a kind of a rectangular box to surround these images with that what we",
    "start": "2992200",
    "end": "2999220"
  },
  {
    "text": "can do is that we can query this entire data set do any pre-processing that we need to do",
    "start": "2999220",
    "end": "3005579"
  },
  {
    "text": "for example here we're actually going to do a bunch of crops and crop out these individual objects and then we can you",
    "start": "3005579",
    "end": "3012900"
  },
  {
    "text": "know perform things such as data saturation we can queue it maybe a data set of like just dogs or curate the data",
    "start": "3012900",
    "end": "3018839"
  },
  {
    "text": "set of just people and then lastly as adapt is also just really powerful at you know exploring your data and",
    "start": "3018839",
    "end": "3024660"
  },
  {
    "text": "understanding your data interactively in a notebook so first of all we'll just import Daft and",
    "start": "3024660",
    "end": "3031859"
  },
  {
    "text": "then since this is array Meetup we're going to be immediately using left with Ray and I'm going to define a few Flags",
    "start": "3031859",
    "end": "3038400"
  },
  {
    "text": "here but the important bit is this bit where we're going to tell deaf to set",
    "start": "3038400",
    "end": "3043560"
  },
  {
    "text": "the runner to Ray and we pass in an address currently I'm going to pass in none because I'm going to run everything",
    "start": "3043560",
    "end": "3048839"
  },
  {
    "text": "just on my laptop for now or connect to a remote cluster later but right now everything will be running on my laptop",
    "start": "3048839",
    "end": "3056339"
  },
  {
    "text": "and you can tell uh Daft to read from these part k files which I have stored in S3",
    "start": "3056339",
    "end": "3063180"
  },
  {
    "text": "um and then once it's done reading the files you'll see that Daft actually knows the schema of these files but it",
    "start": "3063180",
    "end": "3069359"
  },
  {
    "text": "will tell you there's no data to display yet because stuff is lazy right it won't actually execute until you tell it to go",
    "start": "3069359",
    "end": "3075240"
  },
  {
    "text": "and run essentially and so now we can see like that you know show me show me the rows show me the money and uh here",
    "start": "3075240",
    "end": "3081780"
  },
  {
    "text": "we go we have eight rows here I was just showing the first eight rows and um I",
    "start": "3081780",
    "end": "3087599"
  },
  {
    "text": "think there are about eight columns as well and and this this specific data frame the annotations data frame is made",
    "start": "3087599",
    "end": "3093059"
  },
  {
    "text": "out of a bunch of boxes right on the images these are image IDs then there's",
    "start": "3093059",
    "end": "3098579"
  },
  {
    "text": "also a bunch of other columns here such as the category ID this tells you what kind of object this is and then a bunch",
    "start": "3098579",
    "end": "3105660"
  },
  {
    "text": "of segmentation polygons which we won't be using for this demo but um so let's go ahead and run some you",
    "start": "3105660",
    "end": "3112740"
  },
  {
    "text": "know very traditional kind of relational style analytics so the first thing we want we might want to do is ask the",
    "start": "3112740",
    "end": "3119040"
  },
  {
    "text": "question hey like for these category IDs how many the objects are there in this",
    "start": "3119040",
    "end": "3124079"
  },
  {
    "text": "data set right can we Group by the category IDs and just count them and so Daft exposes this goodbye syntax Group",
    "start": "3124079",
    "end": "3130319"
  },
  {
    "text": "by act and then we'll sort by the account it will show the first 10 rows and we'll see that category ID one",
    "start": "3130319",
    "end": "3136440"
  },
  {
    "text": "actually massively outweighs all the other objects and this is actually a people and that's why there's so many of",
    "start": "3136440",
    "end": "3142260"
  },
  {
    "text": "them um and so what we actually should notice that this data set is fairly imbalanced uh in in favor of category IE one",
    "start": "3142260",
    "end": "3150599"
  },
  {
    "text": "um one thing we're going to do now is we're going to filter the data set for category IDs less than 10 and we'll see",
    "start": "3150599",
    "end": "3157500"
  },
  {
    "text": "that depth is really powerful Expressions syntax right where you can access a column just like this by giving",
    "start": "3157500",
    "end": "3163800"
  },
  {
    "text": "it a square brackets and give it a column name and then you can use a column to say hey I only want columns",
    "start": "3163800",
    "end": "3168960"
  },
  {
    "text": "that are less than 10 and then you pass that into the data frame syntax and then",
    "start": "3168960",
    "end": "3174300"
  },
  {
    "text": "secondly we're just going to limit it to just the category ID the bonding box image IDE columns and I'm going to run",
    "start": "3174300",
    "end": "3181500"
  },
  {
    "text": "dot collect which will then materialize our entire data frame in memory and this is what I'll do right now it looks like",
    "start": "3181500",
    "end": "3188119"
  },
  {
    "text": "and it has a category ID all of which are under 10 and just these three",
    "start": "3188119",
    "end": "3193260"
  },
  {
    "text": "columns which are the boxes on these images um cool and so how does it kind of work",
    "start": "3193260",
    "end": "3198960"
  },
  {
    "text": "under the hood let's speak a little bit under the HUD and see how adapt does what it does really all depth is doing",
    "start": "3198960",
    "end": "3204960"
  },
  {
    "text": "is it's constructing this thing we call a logical plan um and that plan currently looks like",
    "start": "3204960",
    "end": "3210180"
  },
  {
    "text": "this we have this projection that we just ran for just three columns we have a filter operation where we're saying we",
    "start": "3210180",
    "end": "3216900"
  },
  {
    "text": "want rows where the category ID is less than 10 and then there's a scan plan",
    "start": "3216900",
    "end": "3222300"
  },
  {
    "text": "right which is Staff saying I'm going to be scanning a file um and so what happens under the hood is",
    "start": "3222300",
    "end": "3227700"
  },
  {
    "text": "when you tell adapt to execute what it will do is it will translate this plan into a bunch of way function calls",
    "start": "3227700",
    "end": "3233700"
  },
  {
    "text": "essentially that it then forms on to the array cluster and that's how Daft leverages ready for distributed",
    "start": "3233700",
    "end": "3238740"
  },
  {
    "text": "computing um one really cool thing is because staff is lazy and uh uh can you know",
    "start": "3238740",
    "end": "3245160"
  },
  {
    "text": "construct this plan what it will do is that it can perform query optimization right so we'll see if you optimized form",
    "start": "3245160",
    "end": "3251760"
  },
  {
    "text": "of this plan by passing in this flag and you'll notice that daf does some pretty interesting things in this case",
    "start": "3251760",
    "end": "3257280"
  },
  {
    "text": "specifically what it can do is it can say hey looks like you only need three columns I'm going to only read these",
    "start": "3257280",
    "end": "3264180"
  },
  {
    "text": "three columns when I'm scanning the parquet file right so then it will you know avoid scan the rest of the columns",
    "start": "3264180",
    "end": "3269339"
  },
  {
    "text": "and there are a whole bunch of these really powerful query optimizations that daf can do for you",
    "start": "3269339",
    "end": "3274800"
  },
  {
    "text": "um that we leverage throughout this tutorial kind of transparity under the hood you don't have to think about it",
    "start": "3274800",
    "end": "3279839"
  },
  {
    "text": "def does it intelligently okay now that we're done kind of peeking under the hood there let's carry on with the",
    "start": "3279839",
    "end": "3286079"
  },
  {
    "text": "tutorial so we're going to join our annotations data frame with the images data frame right we want the actual",
    "start": "3286079",
    "end": "3292020"
  },
  {
    "text": "images for each uh box that we have and so we're gonna run a DOT join which is a",
    "start": "3292020",
    "end": "3298140"
  },
  {
    "text": "very familiar operation if you've ever bunch of tables um and then we're going to select just a",
    "start": "3298140",
    "end": "3303300"
  },
  {
    "text": "few uh columns to look at and so we'll see now that we have the box the category ID and now we also have the",
    "start": "3303300",
    "end": "3310260"
  },
  {
    "text": "image itself right which is the URL uh so yeah more accurately it's not the image itself it's a URL which is a",
    "start": "3310260",
    "end": "3316559"
  },
  {
    "text": "pointer to the image which we can resolve later on all right now we're going to run some",
    "start": "3316559",
    "end": "3322319"
  },
  {
    "text": "simple repartitioning which will allow Daft to run in parallel across all these",
    "start": "3322319",
    "end": "3327359"
  },
  {
    "text": "partitions and because we're running just on my laptop and I want this demo to actually proceed in a time efficient",
    "start": "3327359",
    "end": "3333960"
  },
  {
    "text": "manner we're going to limit the number of rows it's just 128 for now um and again this is the data thing",
    "start": "3333960",
    "end": "3340559"
  },
  {
    "text": "we're working with cool now all of that we could have done in any kind of relational",
    "start": "3340559",
    "end": "3346400"
  },
  {
    "text": "data frame library or it's SQL really what makes staff so special it's the ability to work with complex data types",
    "start": "3346400",
    "end": "3352740"
  },
  {
    "text": "right and this here this URL is actually an example of complex data type if you",
    "start": "3352740",
    "end": "3357780"
  },
  {
    "text": "think about it this URL isn't just a string it actually actually represents a location right a a a pointer to a file",
    "start": "3357780",
    "end": "3365280"
  },
  {
    "text": "and when you work with these pointers one common operation you might want to do is download data from that from that",
    "start": "3365280",
    "end": "3371040"
  },
  {
    "text": "pointer right and that makes it really easy to do that because you can just say hey Daft for this Coco URL column right",
    "start": "3371040",
    "end": "3378180"
  },
  {
    "text": "I'm going to call Dot url.download it and what that will do is it will now return a new column called image bytes",
    "start": "3378180",
    "end": "3384780"
  },
  {
    "text": "and we'll see that Daft will just download the DNA for you um spikes and this works for you know",
    "start": "3384780",
    "end": "3390599"
  },
  {
    "text": "these HTTP URLs it also works for S3 URLs and local file Parts as well",
    "start": "3390599",
    "end": "3395819"
  },
  {
    "text": "um and then going from these bytes now to images we can run these python",
    "start": "3395819",
    "end": "3401339"
  },
  {
    "text": "functions in depth and so I defined a very simple function here to go from bytes to a pill image and again we run",
    "start": "3401339",
    "end": "3408119"
  },
  {
    "text": "the same kind of with column apply syntax applies here just says run this",
    "start": "3408119",
    "end": "3413280"
  },
  {
    "text": "python function on on my column on my image bytes column right now we need power.show we'll see voila we have",
    "start": "3413280",
    "end": "3420660"
  },
  {
    "text": "images and so we kind of went from URL to images and I think that was like five lines of code but we can do even more",
    "start": "3420660",
    "end": "3426180"
  },
  {
    "text": "which is you know one thing we want to do is you want to crop these boxes and",
    "start": "3426180",
    "end": "3431280"
  },
  {
    "text": "then we'll want to convert those copped images into numpy areas right for pre-processing for training essentially",
    "start": "3431280",
    "end": "3437220"
  },
  {
    "text": "and so again we run a crop here we're using a Daft UDF which allows you to",
    "start": "3437220",
    "end": "3442680"
  },
  {
    "text": "define a function that takes in columns as input and so this will run the crop",
    "start": "3442680",
    "end": "3447900"
  },
  {
    "text": "as we can see now all these are small crops from this image",
    "start": "3447900",
    "end": "3453119"
  },
  {
    "text": "um because it's a person here which is a you know annotation level one and then now we can convert these crops into",
    "start": "3453119",
    "end": "3458760"
  },
  {
    "text": "numpy right with again a very simple function and we'll call Dot apply on uh the cut image and so now our data frame",
    "start": "3458760",
    "end": "3465900"
  },
  {
    "text": "looks like this where we have these URLs um the original image the cropped image",
    "start": "3465900",
    "end": "3471540"
  },
  {
    "text": "and then we have these numpy arrays which are all the same shape great so the data is now pre-processed right it's",
    "start": "3471540",
    "end": "3477480"
  },
  {
    "text": "ready for training we have all these um numpyries that are like well formed",
    "start": "3477480",
    "end": "3482640"
  },
  {
    "text": "and they're all cleaned up and ready for training and so to convert this data frame into a raid data set all you need",
    "start": "3482640",
    "end": "3489599"
  },
  {
    "text": "to do is call this dot two Ray data set right super simple here we're also just selecting two columns the numpy array",
    "start": "3489599",
    "end": "3495960"
  },
  {
    "text": "column and the category ID column and then we're renaming them but now yeah you have a uh you know run-of-the-mill",
    "start": "3495960",
    "end": "3502559"
  },
  {
    "text": "red data set and really all you need to do with this is pipe it into ring um and",
    "start": "3502559",
    "end": "3507900"
  },
  {
    "text": "so I literally just copied and pasted uh reairs image classification tutorial so",
    "start": "3507900",
    "end": "3513359"
  },
  {
    "text": "that I can run it and show you that this actually runs um and all this right now is running on my local laptop so it's going to run",
    "start": "3513359",
    "end": "3519059"
  },
  {
    "text": "some training so this is Ray running training it should complete in about 10",
    "start": "3519059",
    "end": "3524160"
  },
  {
    "text": "seconds or so because it only runs two inbox and remember we only have 128 rows right now",
    "start": "3524160",
    "end": "3530280"
  },
  {
    "text": "um and then here is where it stores the results of training the checkpoint and we're going to use the results of",
    "start": "3530280",
    "end": "3535920"
  },
  {
    "text": "training to evaluate uh our model again on the on the training data set and we",
    "start": "3535920",
    "end": "3541140"
  },
  {
    "text": "have now a data set that is the evaluated result of our trained model right this isn't as useful so we can",
    "start": "3541140",
    "end": "3548880"
  },
  {
    "text": "actually go the other way which is to convert a free data set back into Daft right and all you have to do is call",
    "start": "3548880",
    "end": "3554940"
  },
  {
    "text": "dataframe Dot from the dataset again it's very cheap to compute um because of",
    "start": "3554940",
    "end": "3560460"
  },
  {
    "text": "the underlying data representations and this is what the data looks like right we have these predictions produced by",
    "start": "3560460",
    "end": "3565740"
  },
  {
    "text": "our model and the actual ground Choice label and so we'll do some pre-processing to convert these",
    "start": "3565740",
    "end": "3571440"
  },
  {
    "text": "predictions from these Empire Rays one hot encoded numpyries into numbers so",
    "start": "3571440",
    "end": "3576599"
  },
  {
    "text": "it's easier to work with that looks like this right now the predictions are all nice and just numbers and now we want to",
    "start": "3576599",
    "end": "3582599"
  },
  {
    "text": "compare the predictions or the labels to make sure that they are correct so again we run a very simple Daft expression",
    "start": "3582599",
    "end": "3588420"
  },
  {
    "text": "syntax to compare these two columns and make sure that they're equal you cast them to integers so that they're either",
    "start": "3588420",
    "end": "3593880"
  },
  {
    "text": "zero or one and lastly to see how well our crappy small model performed we can call",
    "start": "3593880",
    "end": "3601440"
  },
  {
    "text": "you know Group by the label and then sum or the number of correctors um obviously it didn't perform that well",
    "start": "3601440",
    "end": "3607200"
  },
  {
    "text": "because we're only trading 128 rows and we're running everything locally and only around two epochs",
    "start": "3607200",
    "end": "3613380"
  },
  {
    "text": "um so our model does very very badly and now we can actually maybe just go back restart the entire notebook and we can",
    "start": "3613380",
    "end": "3620160"
  },
  {
    "text": "run on the entire data set by leveraging uh array and run it remotely so that we",
    "start": "3620160",
    "end": "3625980"
  },
  {
    "text": "have way more compute so let me clear all the cells and we can start again",
    "start": "3625980",
    "end": "3632520"
  },
  {
    "text": "um so same thing import Daft and this time instead of running locally I'm going to run remotely so I have proxy",
    "start": "3632520",
    "end": "3640559"
  },
  {
    "text": "locally to a ray cluster that I'm running in my own my own Ray cluster and",
    "start": "3640559",
    "end": "3646980"
  },
  {
    "text": "we can run this cell which will it's just your usual rate in it right so I'm",
    "start": "3646980",
    "end": "3652140"
  },
  {
    "text": "connecting to the array cluster now I'm making sure to install daf and our heat cluster and then I connect Daft to that",
    "start": "3652140",
    "end": "3660119"
  },
  {
    "text": "web cluster and you'll notice now uh retails we have now 32 CPUs right so way more compute than I would have just",
    "start": "3660119",
    "end": "3666359"
  },
  {
    "text": "locally so really from here right now all the code is the same you can load the data",
    "start": "3666359",
    "end": "3671819"
  },
  {
    "text": "in just as we did before and the data is the same as well right it's just this annotations uh data frame we can query",
    "start": "3671819",
    "end": "3678540"
  },
  {
    "text": "the data frame run the same group buys to count the number of um occurrences per label we can run the",
    "start": "3678540",
    "end": "3686460"
  },
  {
    "text": "same you know filtering and selection on The annotation data frame and then we will be joining the annotations data",
    "start": "3686460",
    "end": "3693119"
  },
  {
    "text": "frame with the image data frame same thing no change in syntax whatsoever the same operations just uh now is it's",
    "start": "3693119",
    "end": "3699540"
  },
  {
    "text": "executing in the remote read cluster on 32 cores instead of locally and we'll do every partition again and this is the",
    "start": "3699540",
    "end": "3706680"
  },
  {
    "text": "one part where it's different where we're not going to limit it to 128 rows anymore instead our data frame now has",
    "start": "3706680",
    "end": "3713040"
  },
  {
    "text": "uh 15 000 rows right so a lot more data that we'll be running on and this is",
    "start": "3713040",
    "end": "3718200"
  },
  {
    "text": "what the data data frame currently looks like um 15 000 rows uh same kind of schema uh",
    "start": "3718200",
    "end": "3725220"
  },
  {
    "text": "that we were working with before cool and then we're gonna go ahead download all the data",
    "start": "3725220",
    "end": "3730740"
  },
  {
    "text": "um with this URL download that they don't look like this but this new bytes column and we are going to load that as",
    "start": "3730740",
    "end": "3736740"
  },
  {
    "text": "images just like that so now we have image and then we're going to do our crop and",
    "start": "3736740",
    "end": "3742619"
  },
  {
    "text": "conversion to numpy just as we did before and now we have this data frame",
    "start": "3742619",
    "end": "3747960"
  },
  {
    "text": "again and lastly we're going to convert it to a ray data set just like we did before and we can look at the schema of",
    "start": "3747960",
    "end": "3754920"
  },
  {
    "text": "that Ray data set once we're done with that take a little bit of time to run because there's a lot more data now",
    "start": "3754920",
    "end": "3762680"
  },
  {
    "text": "oh looks like we had an error uh it's probably because the uh cocoa data set is storing is returning in none",
    "start": "3762780",
    "end": "3769740"
  },
  {
    "text": "for yeah so the cocoa data set now uh is returning in none so we can actually",
    "start": "3769740",
    "end": "3774900"
  },
  {
    "text": "just rerun it I think and it should uh be more well-behaved",
    "start": "3774900",
    "end": "3782420"
  },
  {
    "text": "let's try it again",
    "start": "3783900",
    "end": "3786859"
  },
  {
    "text": "all right so our data frame is running again hopefully this time the Coco CDN",
    "start": "3807660",
    "end": "3813119"
  },
  {
    "text": "is well behaved and doesn't throw an error when we try to access the data",
    "start": "3813119",
    "end": "3818720"
  },
  {
    "text": "cool it worked this time and now we have this trading data set uh that we can now pipe into rate and so we're gonna we're",
    "start": "3822540",
    "end": "3828900"
  },
  {
    "text": "gonna be running the same Ray training pipeline I hit the code because there's a lot of code here but once that's done",
    "start": "3828900",
    "end": "3835140"
  },
  {
    "text": "it takes about 30 seconds I believe to train um and then we can take a look at the",
    "start": "3835140",
    "end": "3841559"
  },
  {
    "text": "model weights and then run evaluation again so yeah this is trailing spinning",
    "start": "3841559",
    "end": "3847619"
  },
  {
    "text": "up",
    "start": "3847619",
    "end": "3849798"
  },
  {
    "text": "and we can take a look at what Ray is doing so uh this is the the rate cluster",
    "start": "3853079",
    "end": "3858359"
  },
  {
    "text": "and then it's requesting for 17 CPUs um I'm actually running 16 workers for",
    "start": "3858359",
    "end": "3864180"
  },
  {
    "text": "training um and so this is really doing this training the first iteration second iteration we'll notice here also that um",
    "start": "3864180",
    "end": "3872099"
  },
  {
    "text": "now the the checkpoint is stored in S3 uh because this training happened uh",
    "start": "3872099",
    "end": "3877140"
  },
  {
    "text": "remotely and so the checkpoint goes to remote storage in S3 and so when we run evaluation it's going to download that",
    "start": "3877140",
    "end": "3883680"
  },
  {
    "text": "checkpoint and then run model evaluation and once model evaluation is finished we'll have this uh model evaluation",
    "start": "3883680",
    "end": "3891119"
  },
  {
    "text": "results as a raid data set um which we can then type in back into",
    "start": "3891119",
    "end": "3896520"
  },
  {
    "text": "Daft and run uh Daft uh kind of reporting again so again same kind of",
    "start": "3896520",
    "end": "3904260"
  },
  {
    "text": "data frame where we have these one hot encoding um except that now we have 15 000 results right because we ran model",
    "start": "3904260",
    "end": "3910079"
  },
  {
    "text": "evaluation on 15 000 rows we'll run some pre-processing to convert those one hot",
    "start": "3910079",
    "end": "3915839"
  },
  {
    "text": "encodings into these numbers so we can compare them then now we can",
    "start": "3915839",
    "end": "3921319"
  },
  {
    "text": "run the same analytics to figure out how well our model did so it seems like a model is doing a lot better now but what",
    "start": "3921319",
    "end": "3927480"
  },
  {
    "text": "what it's doing now is actually it's predicting everybody as a human and so",
    "start": "3927480",
    "end": "3932520"
  },
  {
    "text": "you know um uh if we had trained the model for a much longer time uh it would have",
    "start": "3932520",
    "end": "3937740"
  },
  {
    "text": "performed a lot better but just for the sake of this demo um that's that's what I did but yeah so that's that was kind",
    "start": "3937740",
    "end": "3942900"
  },
  {
    "text": "of the demo to show how easy it is to work with uh Ray using Daft as kind of",
    "start": "3942900",
    "end": "3948839"
  },
  {
    "text": "this engine for running all your data frame operations and all your querying and Analytics",
    "start": "3948839",
    "end": "3955440"
  },
  {
    "text": "so um back to these slides so just as a",
    "start": "3955440",
    "end": "3961020"
  },
  {
    "text": "recap how we used that here is that depth kind of sits between uh your raid data and Ray ml uh ecosystem and your",
    "start": "3961020",
    "end": "3968099"
  },
  {
    "text": "data storage right so you will use staff to do things such as querying your data right join your tables filter your",
    "start": "3968099",
    "end": "3974220"
  },
  {
    "text": "tables uh process your data you can actually use gpus as well in depth and you can request that as a resource and",
    "start": "3974220",
    "end": "3981480"
  },
  {
    "text": "you can do a lot of ETL style jobs where you join two tables together and materialize a new table for",
    "start": "3981480",
    "end": "3988200"
  },
  {
    "text": "future Downstream consumption and then you can also do analytics with group buys and and things you usually do in",
    "start": "3988200",
    "end": "3993900"
  },
  {
    "text": "SQL and also it's just really nice for interactive data science where you're just working out of a notebook and then",
    "start": "3993900",
    "end": "3999359"
  },
  {
    "text": "when you're ready to kind of uh you know pass the data off into a machine learning training ecosystem which is",
    "start": "3999359",
    "end": "4005660"
  },
  {
    "text": "where we data sets and re-air really shines and you can just add that off using the array to radiator function",
    "start": "4005660",
    "end": "4013700"
  },
  {
    "text": "call in Daft um so kind of some of the things that we have on our roadmap uh we're in the",
    "start": "4013700",
    "end": "4019880"
  },
  {
    "text": "process of uh writing a lot of the core code in Rust um so that we have you know performance",
    "start": "4019880",
    "end": "4025460"
  },
  {
    "text": "increases and just like better civilization um we're also working on what's really interesting uh which is the native",
    "start": "4025460",
    "end": "4031700"
  },
  {
    "text": "complex types um so you can now call it you know this column is an image that I want to",
    "start": "4031700",
    "end": "4037400"
  },
  {
    "text": "convert it to an embedding you know this column is a tensor I'm going to calculate that cosine distances and lastly we're working on like really nice",
    "start": "4037400",
    "end": "4043700"
  },
  {
    "text": "Integrations with radiator sets to do kind of end-to-end lazy evaluation within real data sets",
    "start": "4043700",
    "end": "4050299"
  },
  {
    "text": "um and that's it yes if you like what you see come chat with us my emails on this slide is Jay at adventurecomplete.com and you know we'll",
    "start": "4050299",
    "end": "4056539"
  },
  {
    "text": "be around and we submit uh so come talk to us and we'll hopefully be able to tell you guys more about uh deaf",
    "start": "4056539",
    "end": "4062660"
  },
  {
    "text": "architecture and kind of how we built this on Ray I'm looking forward to seeing you then thank you Jay wonderful",
    "start": "4062660",
    "end": "4068000"
  },
  {
    "text": "I did like the seamless integration and flow between changing the data sets heading out to Ray to do all the uh",
    "start": "4068000",
    "end": "4074780"
  },
  {
    "text": "heavy lifting for Ray training using Ray data sets and also very integration gray Air and then coming back it seems like",
    "start": "4074780",
    "end": "4081319"
  },
  {
    "text": "you actually have built an entire DSL on top of uh yeah it felt like you know coming from spark",
    "start": "4081319",
    "end": "4089000"
  },
  {
    "text": "background I felt like I was looking at this product data frame being on the DSi phrase right and then when you watch",
    "start": "4089000",
    "end": "4094339"
  },
  {
    "text": "yeah and when you when you when you mentioned that what we're doing because I was thinking okay so I I see you know",
    "start": "4094339",
    "end": "4099738"
  },
  {
    "text": "I see all the I see I see the projection I see the group by do you select I do all those things and it it felt very",
    "start": "4099739",
    "end": "4106460"
  },
  {
    "text": "much like as if I was actually doing a TSL and then and then you talked about we create a logical plan and then we go",
    "start": "4106460",
    "end": "4111620"
  },
  {
    "text": "through uh optimization to create a more more conjoined plan",
    "start": "4111620",
    "end": "4116778"
  },
  {
    "text": "um do you do any kind of push down predicates if you actually needed to do a projection or say if",
    "start": "4116779",
    "end": "4123318"
  },
  {
    "text": "you're reading it a parquet file for example presumably presumably you you allowing to read things from arcade file",
    "start": "4123319",
    "end": "4129380"
  },
  {
    "text": "if I do a projection then you're probably going to do a push down predicate for that yeah 100 um so we do things such as projection uh",
    "start": "4129380",
    "end": "4136338"
  },
  {
    "text": "predicate push down so if you have a query that can be pushed down into the parquet layer or if you're using something like Delta link or",
    "start": "4136339",
    "end": "4143120"
  },
  {
    "text": "I often you can do that push down for you um you also are thinking of really interesting push Downs in the realm of",
    "start": "4143120",
    "end": "4149359"
  },
  {
    "text": "more like complex data right for example if you are interested in uh images",
    "start": "4149359",
    "end": "4155440"
  },
  {
    "text": "download any images uh Beyond a certain size right so it seems like currently I",
    "start": "4157480",
    "end": "4163160"
  },
  {
    "text": "mean your data story is like this abstract store right you have connectors to Delta Lake you have unexpected those",
    "start": "4163160",
    "end": "4168679"
  },
  {
    "text": "you're connected to uh probably snowflake down the road or that's right so my data could be actually sitting",
    "start": "4168679",
    "end": "4175460"
  },
  {
    "text": "anywhere right which is which is currently unstructured and and you're using daf to do I would you say you can",
    "start": "4175460",
    "end": "4182900"
  },
  {
    "text": "use daf to do both Eda as well as ETL and then once you have pre-processed",
    "start": "4182900",
    "end": "4188298"
  },
  {
    "text": "that particular area you can store it back is that where you can actually write it back or you have to write it to a real data set and then store it as a",
    "start": "4188299",
    "end": "4194900"
  },
  {
    "text": "parquet or from within the within the data frame you can save it as so we we think of that is like the",
    "start": "4194900",
    "end": "4201080"
  },
  {
    "text": "Switzerland uh of data so we're sitting in between all these other like places very neutral right we don't care where",
    "start": "4201080",
    "end": "4207500"
  },
  {
    "text": "your data came from we'll query it we'll process it and then we'll split it back out um and so that is kind of the",
    "start": "4207500",
    "end": "4212719"
  },
  {
    "text": "Switzerland um but what makes Ray really interesting for us is that like I think traditionally it's been kind of hard to",
    "start": "4212719",
    "end": "4219860"
  },
  {
    "text": "go from your data warehouse into Ray uh itself and and processing especially",
    "start": "4219860",
    "end": "4225500"
  },
  {
    "text": "these like image types and all that and that transition is kind of hard and so by being kind of the Switzerland that",
    "start": "4225500",
    "end": "4231020"
  },
  {
    "text": "sits in between everything we have the ability to uh make sure that we can access all the data really efficiently",
    "start": "4231020",
    "end": "4237260"
  },
  {
    "text": "um in the future we are thinking of more efficient ways of storing data on disk in S3",
    "start": "4237260",
    "end": "4242900"
  },
  {
    "text": "um and so that will be you know how can we efficiently uh encode images for example maybe instead of having a",
    "start": "4242900",
    "end": "4248900"
  },
  {
    "text": "million images setting one in S3 how can we efficiently you know like compact them and store them efficiently so",
    "start": "4248900",
    "end": "4254600"
  },
  {
    "text": "that's kind of in the future but for now they have this purely kind of compute query engine on top of these other uh I",
    "start": "4254600",
    "end": "4261140"
  },
  {
    "text": "mean they actually make it they definitely actually makes sense like like you said it sort of sits between you know you you your Downstream data",
    "start": "4261140",
    "end": "4269780"
  },
  {
    "text": "ingestion after you've done some initial analytics because the ray data sets is not really an ETL query language it has",
    "start": "4269780",
    "end": "4276560"
  },
  {
    "text": "few high-level uh things that you can actually do for pre-processing whereas whereas you know the doubt sort of",
    "start": "4276560",
    "end": "4282679"
  },
  {
    "text": "provides kind of a full-blown DSL so you can actually have sqlite query and so if you're reading data from from an SQL",
    "start": "4282679",
    "end": "4289219"
  },
  {
    "text": "engine you can actually somehow mimic that kind of analytics um in terms of in terms of the size I",
    "start": "4289219",
    "end": "4295400"
  },
  {
    "text": "mean you give me an idea about how the cocoa data sits what is the largest kind of data set you actually work with you",
    "start": "4295400",
    "end": "4302300"
  },
  {
    "text": "have done some analytics and where you have handed off eventually to to Ray",
    "start": "4302300",
    "end": "4307640"
  },
  {
    "text": "data to to do the distributed training um maybe working on so so we've worked",
    "start": "4307640",
    "end": "4313040"
  },
  {
    "text": "with uh Benchmark data sets like the tpch data sets and and the like and we've worked with you know up to",
    "start": "4313040",
    "end": "4318260"
  },
  {
    "text": "terabytes of worth of data um so it's fairly performing there I will say that there are a lot of optimizations we",
    "start": "4318260",
    "end": "4324320"
  },
  {
    "text": "haven't made yet um so there's a lot of still room for us to grow in that area right we're fairly performant now I",
    "start": "4324320",
    "end": "4329900"
  },
  {
    "text": "think comparable to Apache smart right benchmarks right and you mentioned something in the road maybe I actually",
    "start": "4329900",
    "end": "4335719"
  },
  {
    "text": "have certain things that you're planning to do more integration Ray data set what were you thinking I mean could you could",
    "start": "4335719",
    "end": "4342020"
  },
  {
    "text": "you talk a little more about is this is just a new is this a new uh execution engine that we have which takes care of",
    "start": "4342020",
    "end": "4347659"
  },
  {
    "text": "the streaming uh right right exactly so you've tickle pick up my inner nerd um",
    "start": "4347659",
    "end": "4352940"
  },
  {
    "text": "the idea the idea is that like right now all of the real data set Integrations",
    "start": "4352940",
    "end": "4358100"
  },
  {
    "text": "require you to materialize including us actually for you to materialize before uh before you convert it but there's",
    "start": "4358100",
    "end": "4364640"
  },
  {
    "text": "actually no reason why we wouldn't be able to go fully kind of lazy from your party store into Daft into data sets",
    "start": "4364640",
    "end": "4372620"
  },
  {
    "text": "right and so the idea is that in Ray data sets when you request for the first partition it goes to Def and since I",
    "start": "4372620",
    "end": "4378860"
  },
  {
    "text": "need just one partition and then Daft goes all the way down to the store and says I need one partition and so we kind",
    "start": "4378860",
    "end": "4384140"
  },
  {
    "text": "of fully end-to-end streaming and lazy uh down to three data sets and we're just very a very nice pattern uh for in",
    "start": "4384140",
    "end": "4392300"
  },
  {
    "text": "the case where you don't have enough like cluster memory to you know materialize everything all at once which is what currently all the engines do so",
    "start": "4392300",
    "end": "4398840"
  },
  {
    "text": "we're very excited about that particular uh development when you when you when you do a data",
    "start": "4398840",
    "end": "4405320"
  },
  {
    "text": "frame repartitioning does it actually then map one to one to Ray data set partitioning yes it does the beauty of",
    "start": "4405320",
    "end": "4410900"
  },
  {
    "text": "it exactly you know when you do anything special it's almost almost a zero copy where like the data at rest and rate we",
    "start": "4410900",
    "end": "4417560"
  },
  {
    "text": "just convert it into the ray datasets format which is also arrow and then we just pass it up to three data sets",
    "start": "4417560",
    "end": "4423800"
  },
  {
    "text": "yeah so I think anybody who's sort of coming from you know either spark data data frame background or or pandas data",
    "start": "4423800",
    "end": "4430820"
  },
  {
    "text": "frame or modern would feel very comfortable in terms of using the Daft data frame because it's not like as if",
    "start": "4430820",
    "end": "4437900"
  },
  {
    "text": "you're learning something new you already have familiar with with the DSL kind of uh syntax and again the",
    "start": "4437900",
    "end": "4444080"
  },
  {
    "text": "semantics your term was DSL semantics so you can actually use that project fit I",
    "start": "4444080",
    "end": "4449480"
  },
  {
    "text": "like that that's the hope and uh we're really looking forward to building a lot more interesting kernels so for example",
    "start": "4449480",
    "end": "4455600"
  },
  {
    "text": "like image kernels for cropping for converting images to embeddings and video kernels for like sampling images",
    "start": "4455600",
    "end": "4461560"
  },
  {
    "text": "so all these will be all of these will be building kind of in Rust and make them really performant so that users",
    "start": "4461560",
    "end": "4467360"
  },
  {
    "text": "you'll never have to touch like pill that crop or like you know pill dot two numpy and figure all that out it should",
    "start": "4467360",
    "end": "4473060"
  },
  {
    "text": "all just be contained within our DSL for all most of the pre-processing operations lovely I don't see any questions from",
    "start": "4473060",
    "end": "4480020"
  },
  {
    "text": "the audience but also all the questions I had but I think this was a good good demo to see the you know the Switzerland",
    "start": "4480020",
    "end": "4487100"
  },
  {
    "text": "sitting and negotiating with the data warehouse and and Ray and I like the way",
    "start": "4487100",
    "end": "4492440"
  },
  {
    "text": "you could actually seamlessly work with the small data sets and when you're ready to see that you can actually hand",
    "start": "4492440",
    "end": "4498199"
  },
  {
    "text": "it or end it up to Ray Buster to do things at massive scale so thanks a lot for sharing with us and thanks a lot for",
    "start": "4498199",
    "end": "4503960"
  },
  {
    "text": "sharing with the community uh I don't see any questions coming up but um",
    "start": "4503960",
    "end": "4509179"
  },
  {
    "text": "if they don't have anything maybe we can just end early but I do want to thank Max and Federico for joining us and uh",
    "start": "4509179",
    "end": "4517460"
  },
  {
    "text": "you taking the time uh Jay to to share with us where Daft fits into the grand",
    "start": "4517460",
    "end": "4523760"
  },
  {
    "text": "scheme of things and uh hope to see you soon at the ray Summit and Max I'll see",
    "start": "4523760",
    "end": "4529520"
  },
  {
    "text": "I'll I'll see you I'll see you in data Council oh no Jay you're going to be there as well too so all of us are going",
    "start": "4529520",
    "end": "4536120"
  },
  {
    "text": "to be there so brilliant we're hoping to see you all of you next we're next week in person and thanks a lot for joining",
    "start": "4536120",
    "end": "4542300"
  },
  {
    "text": "and those of you who joined us from New York and California thanks a lot for joining us and hope to see you again in",
    "start": "4542300",
    "end": "4547820"
  },
  {
    "text": "the month like I said this is a rare Meetup this is all about how Ray is being used by the community and these",
    "start": "4547820",
    "end": "4553040"
  },
  {
    "text": "are the people who are actually using Rey at scale and then sharing the story with us so if you do want to have a talk",
    "start": "4553040",
    "end": "4559460"
  },
  {
    "text": "if you do have a story that you actually want to share with us um contact me Jules nscl.com and we'll",
    "start": "4559460",
    "end": "4566000"
  },
  {
    "text": "try to fit you in at one of the talks so thanks a lot and ciao and see you",
    "start": "4566000",
    "end": "4571460"
  },
  {
    "text": "tomorrow or or next week bye thank you everyone thank you",
    "start": "4571460",
    "end": "4578380"
  }
]