[
  {
    "text": "thank you everyone so today I'm very",
    "start": "4400",
    "end": "6680"
  },
  {
    "text": "happy to share some user experience",
    "start": "6680",
    "end": "8679"
  },
  {
    "text": "about reshaping our NTN ml Pipeline with",
    "start": "8679",
    "end": "13160"
  },
  {
    "text": "Ray so first uh actually a background",
    "start": "13160",
    "end": "17119"
  },
  {
    "text": "like about our my company uh motional is",
    "start": "17119",
    "end": "20240"
  },
  {
    "text": "like uh we are robot taxi company so we",
    "start": "20240",
    "end": "23000"
  },
  {
    "text": "have provide we can provide some limited",
    "start": "23000",
    "end": "25080"
  },
  {
    "text": "self-driving service in Las Vegas and we",
    "start": "25080",
    "end": "27480"
  },
  {
    "text": "are backed by Hyundai um so I'm I'm",
    "start": "27480",
    "end": "30599"
  },
  {
    "text": "working in the ml in the field of ml",
    "start": "30599",
    "end": "32920"
  },
  {
    "text": "infra so the two top challenges we have",
    "start": "32920",
    "end": "36960"
  },
  {
    "text": "is like uh we are we actually we are",
    "start": "36960",
    "end": "39680"
  },
  {
    "text": "rapidly expand our Drive L expansion uh",
    "start": "39680",
    "end": "42760"
  },
  {
    "text": "area which means like we need to scale",
    "start": "42760",
    "end": "44719"
  },
  {
    "text": "our Mi pipeline aggressively because we",
    "start": "44719",
    "end": "47000"
  },
  {
    "text": "need to process more data we need to",
    "start": "47000",
    "end": "49320"
  },
  {
    "text": "train larger models uh compute more",
    "start": "49320",
    "end": "52199"
  },
  {
    "text": "metrics and also simulate more things",
    "start": "52199",
    "end": "55559"
  },
  {
    "text": "and also we we need to support",
    "start": "55559",
    "end": "57120"
  },
  {
    "text": "continuous learning and fast iterations",
    "start": "57120",
    "end": "60960"
  },
  {
    "text": "so today uh I will touch three phases in",
    "start": "60960",
    "end": "64439"
  },
  {
    "text": "our end to end pipeline Three core",
    "start": "64439",
    "end": "66760"
  },
  {
    "text": "phases why is the first one is training",
    "start": "66760",
    "end": "68920"
  },
  {
    "text": "data preparation before training and the",
    "start": "68920",
    "end": "71119"
  },
  {
    "text": "second one is the evaluation after",
    "start": "71119",
    "end": "73560"
  },
  {
    "text": "training and then I go to the",
    "start": "73560",
    "end": "76119"
  },
  {
    "text": "training so the first part is like Last",
    "start": "76119",
    "end": "79479"
  },
  {
    "text": "Mile data processing before",
    "start": "79479",
    "end": "82159"
  },
  {
    "text": "training so before we before Ray is adop",
    "start": "82159",
    "end": "86520"
  },
  {
    "text": "adopted in our company so when we um",
    "start": "86520",
    "end": "91119"
  },
  {
    "text": "here is how we process data we first we",
    "start": "91119",
    "end": "94119"
  },
  {
    "text": "use our Drive list cards to collect data",
    "start": "94119",
    "end": "97000"
  },
  {
    "text": "um at in the form of daily Drive log and",
    "start": "97000",
    "end": "100399"
  },
  {
    "text": "then here is the analogy so it's it's",
    "start": "100399",
    "end": "103159"
  },
  {
    "text": "kind of like a river we have new data",
    "start": "103159",
    "end": "104920"
  },
  {
    "text": "flowing in every day and then we need to",
    "start": "104920",
    "end": "107680"
  },
  {
    "text": "First reload our data um to a data",
    "start": "107680",
    "end": "111840"
  },
  {
    "text": "warehouse and then we need to somehow",
    "start": "111840",
    "end": "115119"
  },
  {
    "text": "package our data basically we create a",
    "start": "115119",
    "end": "117039"
  },
  {
    "text": "few snapshots data snapshots and then",
    "start": "117039",
    "end": "119360"
  },
  {
    "text": "we'll syn iiz it with our in-house",
    "start": "119360",
    "end": "121439"
  },
  {
    "text": "training platform so it it looks like",
    "start": "121439",
    "end": "124399"
  },
  {
    "text": "just we route out the a river the water",
    "start": "124399",
    "end": "126960"
  },
  {
    "text": "in the river to a water Factory and then",
    "start": "126960",
    "end": "129679"
  },
  {
    "text": "we will basically create a bottle water",
    "start": "129679",
    "end": "132599"
  },
  {
    "text": "and then basically from the uh then we",
    "start": "132599",
    "end": "136000"
  },
  {
    "text": "will ship deliver those bottle water to",
    "start": "136000",
    "end": "138280"
  },
  {
    "text": "a village basally our ml Engineers so",
    "start": "138280",
    "end": "140840"
  },
  {
    "text": "this is kind of like our Legacy workflow",
    "start": "140840",
    "end": "144120"
  },
  {
    "text": "so you can see like um it's it requires",
    "start": "144120",
    "end": "147080"
  },
  {
    "text": "a new data release every time and also",
    "start": "147080",
    "end": "150120"
  },
  {
    "text": "it involves non-trivial operational",
    "start": "150120",
    "end": "152840"
  },
  {
    "text": "workflows so after we adop rate what we",
    "start": "152840",
    "end": "156040"
  },
  {
    "text": "are what we are doing is like uh we",
    "start": "156040",
    "end": "158920"
  },
  {
    "text": "directly build a very high performance",
    "start": "158920",
    "end": "161840"
  },
  {
    "text": "pipe to routes from the river directly",
    "start": "161840",
    "end": "164800"
  },
  {
    "text": "to a village now the village is 10x",
    "start": "164800",
    "end": "166920"
  },
  {
    "text": "larger because we have 10x more data",
    "start": "166920",
    "end": "169319"
  },
  {
    "text": "requests uh so instead of optimizing",
    "start": "169319",
    "end": "172519"
  },
  {
    "text": "each step we just rebu this with Ray",
    "start": "172519",
    "end": "175480"
  },
  {
    "text": "data so the benefits first is like uh",
    "start": "175480",
    "end": "179480"
  },
  {
    "text": "there's no data reloading we don't need",
    "start": "179480",
    "end": "181319"
  },
  {
    "text": "to reload data to a warehouse to another",
    "start": "181319",
    "end": "183799"
  },
  {
    "text": "storage and no synchronization to our",
    "start": "183799",
    "end": "186280"
  },
  {
    "text": "training platform and we can support",
    "start": "186280",
    "end": "188680"
  },
  {
    "text": "continuous learning and moreover we can",
    "start": "188680",
    "end": "191879"
  },
  {
    "text": "also the this data processing part we",
    "start": "191879",
    "end": "195879"
  },
  {
    "text": "have Incorporated it into our",
    "start": "195879",
    "end": "198120"
  },
  {
    "text": "optimization feedback loop uh along with",
    "start": "198120",
    "end": "201400"
  },
  {
    "text": "training and eval so I explain it",
    "start": "201400",
    "end": "205799"
  },
  {
    "text": "later so now after the rate adoption",
    "start": "206159",
    "end": "210439"
  },
  {
    "text": "this our new landscape at Upstream we",
    "start": "210439",
    "end": "213640"
  },
  {
    "text": "have a few data",
    "start": "213640",
    "end": "215159"
  },
  {
    "text": "Publishers um basically it's like a we",
    "start": "215159",
    "end": "218159"
  },
  {
    "text": "have a few different data sources you",
    "start": "218159",
    "end": "219840"
  },
  {
    "text": "can imagine here like we have different",
    "start": "219840",
    "end": "222439"
  },
  {
    "text": "sensors like liar radar and the uh and",
    "start": "222439",
    "end": "226200"
  },
  {
    "text": "camera and then we also have map data",
    "start": "226200",
    "end": "228879"
  },
  {
    "text": "and among all of these data some data",
    "start": "228879",
    "end": "231480"
  },
  {
    "text": "they annotated by humans some data they",
    "start": "231480",
    "end": "233680"
  },
  {
    "text": "are Auto automatically annotated kind of",
    "start": "233680",
    "end": "236480"
  },
  {
    "text": "like a semi supervised learning so at a",
    "start": "236480",
    "end": "239239"
  },
  {
    "text": "downstream",
    "start": "239239",
    "end": "240280"
  },
  {
    "text": "so this is this part we build it on top",
    "start": "240280",
    "end": "242159"
  },
  {
    "text": "of Ray this is more our more like our ml",
    "start": "242159",
    "end": "244840"
  },
  {
    "text": "code base at at at Upstream basically is",
    "start": "244840",
    "end": "248680"
  },
  {
    "text": "fully owned by our ml our infra teams",
    "start": "248680",
    "end": "252319"
  },
  {
    "text": "but for the at the downstream the green",
    "start": "252319",
    "end": "254560"
  },
  {
    "text": "area actually this this a part um we",
    "start": "254560",
    "end": "258000"
  },
  {
    "text": "build it on top of Ray and it's kind of",
    "start": "258000",
    "end": "260040"
  },
  {
    "text": "owned by our ml Engineers specifically",
    "start": "260040",
    "end": "263120"
  },
  {
    "text": "this part is our training data",
    "start": "263120",
    "end": "265160"
  },
  {
    "text": "preparation pipelines so you can see",
    "start": "265160",
    "end": "268000"
  },
  {
    "text": "like uh each pipeline is kind of model",
    "start": "268000",
    "end": "271240"
  },
  {
    "text": "specific and you can subscribe an",
    "start": "271240",
    "end": "274160"
  },
  {
    "text": "arbitrary number of data sources and we",
    "start": "274160",
    "end": "277680"
  },
  {
    "text": "can prepare data prepare features",
    "start": "277680",
    "end": "280000"
  },
  {
    "text": "here so you can see like a one important",
    "start": "280000",
    "end": "283320"
  },
  {
    "text": "inside is like uh uh as our model",
    "start": "283320",
    "end": "285840"
  },
  {
    "text": "becomes more and more mature actually",
    "start": "285840",
    "end": "288080"
  },
  {
    "text": "it's more common for us to fix data box",
    "start": "288080",
    "end": "292039"
  },
  {
    "text": "than not not only like a model",
    "start": "292039",
    "end": "293840"
  },
  {
    "text": "architecture bux and also we need to",
    "start": "293840",
    "end": "295639"
  },
  {
    "text": "optimize our data besides optimizing uh",
    "start": "295639",
    "end": "299039"
  },
  {
    "text": "our model",
    "start": "299039",
    "end": "300759"
  },
  {
    "text": "uh particularly when we use Ray actually",
    "start": "300759",
    "end": "304360"
  },
  {
    "text": "uh it can help us to uh unlock some data",
    "start": "304360",
    "end": "307759"
  },
  {
    "text": "optimization opportunities one uh",
    "start": "307759",
    "end": "310639"
  },
  {
    "text": "specific benefit is like uh we can use",
    "start": "310639",
    "end": "312840"
  },
  {
    "text": "JIS uh to generate some syntactic data",
    "start": "312840",
    "end": "316479"
  },
  {
    "text": "and doing it as a data argumentation",
    "start": "316479",
    "end": "319080"
  },
  {
    "text": "approach um but this requires a",
    "start": "319080",
    "end": "321800"
  },
  {
    "text": "hetrogeneous setup so Ray actually is",
    "start": "321800",
    "end": "323919"
  },
  {
    "text": "very good for this purpose on the other",
    "start": "323919",
    "end": "326240"
  },
  {
    "text": "hand we can also remove some negative",
    "start": "326240",
    "end": "328520"
  },
  {
    "text": "value or low value data",
    "start": "328520",
    "end": "330360"
  },
  {
    "text": "I will explain it later in another",
    "start": "330360",
    "end": "334120"
  },
  {
    "text": "slide um so because uh last my data",
    "start": "334160",
    "end": "338919"
  },
  {
    "text": "processing is a very classical uh Ray",
    "start": "338919",
    "end": "341759"
  },
  {
    "text": "data use case I don't want to spend too",
    "start": "341759",
    "end": "343840"
  },
  {
    "text": "much time here but I also want to point",
    "start": "343840",
    "end": "345720"
  },
  {
    "text": "out like there is a caveat here is like",
    "start": "345720",
    "end": "348240"
  },
  {
    "text": "a you can see like when we subscribe",
    "start": "348240",
    "end": "350120"
  },
  {
    "text": "multiple data sources we need to do some",
    "start": "350120",
    "end": "352120"
  },
  {
    "text": "join but for Ray data actually is not",
    "start": "352120",
    "end": "354639"
  },
  {
    "text": "really designed for there is no building",
    "start": "354639",
    "end": "356960"
  },
  {
    "text": "joint on Ray data and uh especially",
    "start": "356960",
    "end": "359199"
  },
  {
    "text": "there's no very efficient hash drawing",
    "start": "359199",
    "end": "362199"
  },
  {
    "text": "so what we can do is like a kind of a",
    "start": "362199",
    "end": "364639"
  },
  {
    "text": "Zep two data sets that have exactly the",
    "start": "364639",
    "end": "367319"
  },
  {
    "text": "same number of rows which is not",
    "start": "367319",
    "end": "369120"
  },
  {
    "text": "convenient and we try an in-house",
    "start": "369120",
    "end": "371560"
  },
  {
    "text": "distributor um joint uh implementation",
    "start": "371560",
    "end": "374479"
  },
  {
    "text": "it's scalable we can uh basically the",
    "start": "374479",
    "end": "378039"
  },
  {
    "text": "data size we can handle can be larger",
    "start": "378039",
    "end": "379919"
  },
  {
    "text": "than memory data size but there are some",
    "start": "379919",
    "end": "382319"
  },
  {
    "text": "annoying issues like batch formats",
    "start": "382319",
    "end": "384880"
  },
  {
    "text": "mismatch errors we need to manually",
    "start": "384880",
    "end": "387479"
  },
  {
    "text": "convert our our batch formats and",
    "start": "387479",
    "end": "390280"
  },
  {
    "text": "sometimes we also found the error info",
    "start": "390280",
    "end": "392000"
  },
  {
    "text": "is kind of misleading and we suspect",
    "start": "392000",
    "end": "394639"
  },
  {
    "text": "that we can suspect it because of the",
    "start": "394639",
    "end": "396720"
  },
  {
    "text": "metadata corruption because for",
    "start": "396720",
    "end": "400280"
  },
  {
    "text": "sometimes we can have some work around",
    "start": "400280",
    "end": "401919"
  },
  {
    "text": "by just re repartition the data sets or",
    "start": "401919",
    "end": "405240"
  },
  {
    "text": "like a just write out the data set and",
    "start": "405240",
    "end": "407080"
  },
  {
    "text": "read it back in then it can be fixed but",
    "start": "407080",
    "end": "410039"
  },
  {
    "text": "anyway I think for this part actually",
    "start": "410039",
    "end": "413240"
  },
  {
    "text": "there can be some potential workarounds",
    "start": "413240",
    "end": "415240"
  },
  {
    "text": "if we really need to large scale join on",
    "start": "415240",
    "end": "418240"
  },
  {
    "text": "Ray data we can leverage some other",
    "start": "418240",
    "end": "421080"
  },
  {
    "text": "libraries um another uh point I want to",
    "start": "421080",
    "end": "424280"
  },
  {
    "text": "bring up is like uh even if you just use",
    "start": "424280",
    "end": "427560"
  },
  {
    "text": "Ray data it may not be sufficient",
    "start": "427560",
    "end": "430639"
  },
  {
    "text": "sometimes it will cause a lot of",
    "start": "430639",
    "end": "433639"
  },
  {
    "text": "unnecessary data copies so we need to do",
    "start": "433639",
    "end": "436240"
  },
  {
    "text": "the optimizations in our own libraries",
    "start": "436240",
    "end": "439440"
  },
  {
    "text": "for example when you do join you can",
    "start": "439440",
    "end": "441360"
  },
  {
    "text": "imagine like you mainly operate on the",
    "start": "441360",
    "end": "443560"
  },
  {
    "text": "joint keys but then a lot of other",
    "start": "443560",
    "end": "446280"
  },
  {
    "text": "fields they may can be potentially large",
    "start": "446280",
    "end": "448599"
  },
  {
    "text": "and if you directly use use the ray data",
    "start": "448599",
    "end": "451120"
  },
  {
    "text": "they will just make unnecessary copies",
    "start": "451120",
    "end": "453160"
  },
  {
    "text": "so we need to do some optimizations",
    "start": "453160",
    "end": "456759"
  },
  {
    "text": "here okay so the next part is the model",
    "start": "456759",
    "end": "460400"
  },
  {
    "text": "evaluation after",
    "start": "460400",
    "end": "462240"
  },
  {
    "text": "training so this part I think is a bit",
    "start": "462240",
    "end": "464520"
  },
  {
    "text": "more interesting because in our av",
    "start": "464520",
    "end": "466720"
  },
  {
    "text": "industry our eval actually has some",
    "start": "466720",
    "end": "469319"
  },
  {
    "text": "unique",
    "start": "469319",
    "end": "470360"
  },
  {
    "text": "challenges um one reason is that",
    "start": "470360",
    "end": "473319"
  },
  {
    "text": "actually we need to compute hundreds of",
    "start": "473319",
    "end": "475520"
  },
  {
    "text": "Matrix so when I I remember that when I",
    "start": "475520",
    "end": "478240"
  },
  {
    "text": "work in big tag compies um for example",
    "start": "478240",
    "end": "481560"
  },
  {
    "text": "when we optimize as ranking model or",
    "start": "481560",
    "end": "483960"
  },
  {
    "text": "optimize language models those uh The",
    "start": "483960",
    "end": "487240"
  },
  {
    "text": "Matrix we need to evaluate they are",
    "start": "487240",
    "end": "489120"
  },
  {
    "text": "relatively high level aggregated Matrix",
    "start": "489120",
    "end": "491440"
  },
  {
    "text": "like entropy or perplexity but for in",
    "start": "491440",
    "end": "495080"
  },
  {
    "text": "our a industry because safety is really",
    "start": "495080",
    "end": "497960"
  },
  {
    "text": "critical so actually we need to evaluate",
    "start": "497960",
    "end": "501240"
  },
  {
    "text": "Matrix uh in many many cases including a",
    "start": "501240",
    "end": "504599"
  },
  {
    "text": "lot of edge cases so what does it mean",
    "start": "504599",
    "end": "507479"
  },
  {
    "text": "it means like a a lot of matric",
    "start": "507479",
    "end": "510000"
  },
  {
    "text": "actually we also need to do a lot of",
    "start": "510000",
    "end": "512000"
  },
  {
    "text": "grouping the results will be rendered as",
    "start": "512000",
    "end": "515080"
  },
  {
    "text": "like kind of like ond histogram or 2D",
    "start": "515080",
    "end": "518360"
  },
  {
    "text": "hit map so a lot of matrics and also",
    "start": "518360",
    "end": "521959"
  },
  {
    "text": "among this matric there usually can be",
    "start": "521959",
    "end": "524240"
  },
  {
    "text": "some dependencies so some Matrix you if",
    "start": "524240",
    "end": "527200"
  },
  {
    "text": "you comput it this one first you may",
    "start": "527200",
    "end": "529480"
  },
  {
    "text": "need to recompute it later because there",
    "start": "529480",
    "end": "531600"
  },
  {
    "text": "is a dependency so it can be",
    "start": "531600",
    "end": "534640"
  },
  {
    "text": "tricky and another special case in our",
    "start": "534640",
    "end": "538360"
  },
  {
    "text": "scenario is that uh sometimes you also",
    "start": "538360",
    "end": "542120"
  },
  {
    "text": "prefer a hetrogeneous",
    "start": "542120",
    "end": "543480"
  },
  {
    "text": "setup because for some matrics like IOU",
    "start": "543480",
    "end": "547200"
  },
  {
    "text": "and some MATC basically is like a you",
    "start": "547200",
    "end": "549560"
  },
  {
    "text": "have one object detected and another one",
    "start": "549560",
    "end": "552680"
  },
  {
    "text": "is your pre uh ground of choose then",
    "start": "552680",
    "end": "556279"
  },
  {
    "text": "overlap between these two objects this",
    "start": "556279",
    "end": "558680"
  },
  {
    "text": "kind of geometry Matrix actually it will",
    "start": "558680",
    "end": "561120"
  },
  {
    "text": "be much much faster to compute them on",
    "start": "561120",
    "end": "564120"
  },
  {
    "text": "gpus so how can we overcome these",
    "start": "564120",
    "end": "568000"
  },
  {
    "text": "challenges before rate adoption so our",
    "start": "568000",
    "end": "572000"
  },
  {
    "text": "throughput actually is quite low U for",
    "start": "572000",
    "end": "574360"
  },
  {
    "text": "eval jobs we just run on single node um",
    "start": "574360",
    "end": "578920"
  },
  {
    "text": "although like our training runs on",
    "start": "578920",
    "end": "580519"
  },
  {
    "text": "multiple nodes and um if we don't",
    "start": "580519",
    "end": "583360"
  },
  {
    "text": "carefully organize our user code",
    "start": "583360",
    "end": "586240"
  },
  {
    "text": "actually we will recompute our",
    "start": "586240",
    "end": "587680"
  },
  {
    "text": "intermediate results when we when we",
    "start": "587680",
    "end": "589920"
  },
  {
    "text": "compute a lot of metrics and also when",
    "start": "589920",
    "end": "593079"
  },
  {
    "text": "we need to accelerate those computation",
    "start": "593079",
    "end": "596200"
  },
  {
    "text": "we initially we use like a",
    "start": "596200",
    "end": "598320"
  },
  {
    "text": "multiprocessing MTI threading but as a",
    "start": "598320",
    "end": "602040"
  },
  {
    "text": "consequence as a result actually it can",
    "start": "602040",
    "end": "604399"
  },
  {
    "text": "cause some risk conditions cause some",
    "start": "604399",
    "end": "606480"
  },
  {
    "text": "reliability issues because this part of",
    "start": "606480",
    "end": "608760"
  },
  {
    "text": "optimization can be kind of error",
    "start": "608760",
    "end": "611399"
  },
  {
    "text": "prone and another issue is like U it the",
    "start": "611399",
    "end": "616600"
  },
  {
    "text": "it's kind of composability is not that",
    "start": "616600",
    "end": "618600"
  },
  {
    "text": "good because you can imagine if you need",
    "start": "618600",
    "end": "620640"
  },
  {
    "text": "compute tons of Matrix Al together and",
    "start": "620640",
    "end": "623800"
  },
  {
    "text": "also need to pay attention to the",
    "start": "623800",
    "end": "625480"
  },
  {
    "text": "dependencies you need to group them",
    "start": "625480",
    "end": "627600"
  },
  {
    "text": "together so you introduce a lot code",
    "start": "627600",
    "end": "630399"
  },
  {
    "text": "coupling um it's kind of convoluted Lo",
    "start": "630399",
    "end": "633160"
  },
  {
    "text": "logic which really makes us really hard",
    "start": "633160",
    "end": "635959"
  },
  {
    "text": "to add new",
    "start": "635959",
    "end": "637839"
  },
  {
    "text": "metric and uh another issue is like uh",
    "start": "637839",
    "end": "641120"
  },
  {
    "text": "for example when we use uh some",
    "start": "641120",
    "end": "643040"
  },
  {
    "text": "libraries like a tensor Matrix actually",
    "start": "643040",
    "end": "646440"
  },
  {
    "text": "it's hard for us to express some more",
    "start": "646440",
    "end": "648560"
  },
  {
    "text": "advanced Matrix that involves grouping",
    "start": "648560",
    "end": "651959"
  },
  {
    "text": "logic so what are we are we going to do",
    "start": "651959",
    "end": "654519"
  },
  {
    "text": "here so actually we develop a entirely",
    "start": "654519",
    "end": "657399"
  },
  {
    "text": "new eval framework on top top of Ray",
    "start": "657399",
    "end": "659920"
  },
  {
    "text": "data so there are two very important",
    "start": "659920",
    "end": "664000"
  },
  {
    "text": "features one is like it's easy of you",
    "start": "664000",
    "end": "667120"
  },
  {
    "text": "EAS of use so unlike torch Matrix which",
    "start": "667120",
    "end": "670519"
  },
  {
    "text": "presents a tensor view actually we found",
    "start": "670519",
    "end": "673320"
  },
  {
    "text": "like our ml Engineers they prefer",
    "start": "673320",
    "end": "675880"
  },
  {
    "text": "tabular view um it will be easier for",
    "start": "675880",
    "end": "678959"
  },
  {
    "text": "some Ops like uh especially for grouping",
    "start": "678959",
    "end": "681560"
  },
  {
    "text": "and also can easily support filtering",
    "start": "681560",
    "end": "683680"
  },
  {
    "text": "aggregation all of",
    "start": "683680",
    "end": "685240"
  },
  {
    "text": "this and in our design each metric",
    "start": "685240",
    "end": "689360"
  },
  {
    "text": "similar to the design of torch Matrix",
    "start": "689360",
    "end": "691920"
  },
  {
    "text": "each Matrix is defined as class but we",
    "start": "691920",
    "end": "694800"
  },
  {
    "text": "also have predefined dependencies within",
    "start": "694800",
    "end": "697560"
  },
  {
    "text": "each",
    "start": "697560",
    "end": "698360"
  },
  {
    "text": "class then how are we going to use",
    "start": "698360",
    "end": "701519"
  },
  {
    "text": "it um so for User it's very simple let's",
    "start": "701519",
    "end": "705079"
  },
  {
    "text": "say I need to compute 100 Matrix I just",
    "start": "705079",
    "end": "708040"
  },
  {
    "text": "specify this Matrix and then run the",
    "start": "708040",
    "end": "710519"
  },
  {
    "text": "engine then here we go so this is super",
    "start": "710519",
    "end": "713320"
  },
  {
    "text": "uh it's kind of declarative interface",
    "start": "713320",
    "end": "716600"
  },
  {
    "text": "for the engine itself actually can hide",
    "start": "716600",
    "end": "718880"
  },
  {
    "text": "a lot lot of complexities first by",
    "start": "718880",
    "end": "721360"
  },
  {
    "text": "leveraging Ray data we can paralyze all",
    "start": "721360",
    "end": "724200"
  },
  {
    "text": "the matri",
    "start": "724200",
    "end": "725839"
  },
  {
    "text": "computations and secondly uh we can",
    "start": "725839",
    "end": "728760"
  },
  {
    "text": "handle the dependencies automatically",
    "start": "728760",
    "end": "731040"
  },
  {
    "text": "basically we can we know which one we",
    "start": "731040",
    "end": "732680"
  },
  {
    "text": "need to compute first and then we can",
    "start": "732680",
    "end": "734760"
  },
  {
    "text": "reuse we can kind of memorize the data",
    "start": "734760",
    "end": "738760"
  },
  {
    "text": "the results and reuse it later so we can",
    "start": "738760",
    "end": "741680"
  },
  {
    "text": "reuse the data efficiently as well uh on",
    "start": "741680",
    "end": "745040"
  },
  {
    "text": "the other hand we can also provide high",
    "start": "745040",
    "end": "747440"
  },
  {
    "text": "performance first since we use Ray data",
    "start": "747440",
    "end": "750560"
  },
  {
    "text": "actually it can have out of box",
    "start": "750560",
    "end": "752880"
  },
  {
    "text": "horizontal scaling and moreover we can",
    "start": "752880",
    "end": "756199"
  },
  {
    "text": "also support like vectorization very",
    "start": "756199",
    "end": "758440"
  },
  {
    "text": "easily uh in our eval process a lot of",
    "start": "758440",
    "end": "762519"
  },
  {
    "text": "operators they can be",
    "start": "762519",
    "end": "765040"
  },
  {
    "text": "vectorized and we can use Ray for",
    "start": "765040",
    "end": "767399"
  },
  {
    "text": "building forance uh sometimes it's",
    "start": "767399",
    "end": "769959"
  },
  {
    "text": "important because we have occasional out",
    "start": "769959",
    "end": "772399"
  },
  {
    "text": "memory uh and also if we want have more",
    "start": "772399",
    "end": "776519"
  },
  {
    "text": "speed UPS such as Computing IOU we can",
    "start": "776519",
    "end": "779279"
  },
  {
    "text": "move it to GPU to compute those",
    "start": "779279",
    "end": "783560"
  },
  {
    "text": "Matrix so here is the improvements for",
    "start": "784440",
    "end": "787639"
  },
  {
    "text": "the through U throughput wise actually",
    "start": "787639",
    "end": "789720"
  },
  {
    "text": "we can improve by over 1,000 times so",
    "start": "789720",
    "end": "794240"
  },
  {
    "text": "even if we compute 20x more data 10",
    "start": "794240",
    "end": "798279"
  },
  {
    "text": "comput 10x more Matrix we can still get",
    "start": "798279",
    "end": "800519"
  },
  {
    "text": "10x speed up and by using Ray actually",
    "start": "800519",
    "end": "804639"
  },
  {
    "text": "our reliability is also much better than",
    "start": "804639",
    "end": "807120"
  },
  {
    "text": "just doing MTI processing multi string",
    "start": "807120",
    "end": "810760"
  },
  {
    "text": "another uh important benefit is like uh",
    "start": "810760",
    "end": "813800"
  },
  {
    "text": "by using our framework because each",
    "start": "813800",
    "end": "815639"
  },
  {
    "text": "metric is defined as class so it's more",
    "start": "815639",
    "end": "818240"
  },
  {
    "text": "like a plug-in so it's very easy for us",
    "start": "818240",
    "end": "821000"
  },
  {
    "text": "to add new",
    "start": "821000",
    "end": "823600"
  },
  {
    "text": "metrix so now this is I think this part",
    "start": "824160",
    "end": "827600"
  },
  {
    "text": "is more fun this is the third part is",
    "start": "827600",
    "end": "830160"
  },
  {
    "text": "we'll focus on training and we will do",
    "start": "830160",
    "end": "832480"
  },
  {
    "text": "data cation training and eval",
    "start": "832480",
    "end": "835959"
  },
  {
    "text": "together so how are we going to do that",
    "start": "835959",
    "end": "839800"
  },
  {
    "text": "and by using Ray so first I want to",
    "start": "839800",
    "end": "843759"
  },
  {
    "text": "point out like in our at least in our um",
    "start": "843759",
    "end": "847079"
  },
  {
    "text": "for our jobs data selection is super",
    "start": "847079",
    "end": "850079"
  },
  {
    "text": "important because imagine like you are",
    "start": "850079",
    "end": "852399"
  },
  {
    "text": "doing driving a lot of time you don't",
    "start": "852399",
    "end": "854519"
  },
  {
    "text": "really need to change your Direction you",
    "start": "854519",
    "end": "857000"
  },
  {
    "text": "don't need to change your speed that",
    "start": "857000",
    "end": "858320"
  },
  {
    "text": "much so in other words this kind of data",
    "start": "858320",
    "end": "861320"
  },
  {
    "text": "could be very boring so if you don't do",
    "start": "861320",
    "end": "864160"
  },
  {
    "text": "any data selection actually you may not",
    "start": "864160",
    "end": "866399"
  },
  {
    "text": "be able to train a very good like a a",
    "start": "866399",
    "end": "869160"
  },
  {
    "text": "capabilities like how you when you turn",
    "start": "869160",
    "end": "871680"
  },
  {
    "text": "right turn left when you should slow",
    "start": "871680",
    "end": "873600"
  },
  {
    "text": "down when you should cut in change",
    "start": "873600",
    "end": "875680"
  },
  {
    "text": "lanes and how we how do we do that so",
    "start": "875680",
    "end": "880040"
  },
  {
    "text": "one approach is we use humoristic based",
    "start": "880040",
    "end": "882920"
  },
  {
    "text": "offline data",
    "start": "882920",
    "end": "884279"
  },
  {
    "text": "selection um so this is the workflow",
    "start": "884279",
    "end": "889199"
  },
  {
    "text": "first we have relativ large dat pre-",
    "start": "889199",
    "end": "892120"
  },
  {
    "text": "selected data and then first we do some",
    "start": "892120",
    "end": "894839"
  },
  {
    "text": "data denoising we create a few Futures",
    "start": "894839",
    "end": "897440"
  },
  {
    "text": "to Deno the data and then Bas uh we will",
    "start": "897440",
    "end": "901399"
  },
  {
    "text": "do some data mining to create different",
    "start": "901399",
    "end": "903480"
  },
  {
    "text": "data sets for example uh what's what",
    "start": "903480",
    "end": "906279"
  },
  {
    "text": "other scenarios when you turn left what",
    "start": "906279",
    "end": "908199"
  },
  {
    "text": "are scenarios turn turn right when are",
    "start": "908199",
    "end": "910720"
  },
  {
    "text": "you going to stop at the stop sign or",
    "start": "910720",
    "end": "912839"
  },
  {
    "text": "whatever when you change lanes so all of",
    "start": "912839",
    "end": "915240"
  },
  {
    "text": "this and then when you have all of these",
    "start": "915240",
    "end": "917839"
  },
  {
    "text": "smaller data sets you kind of trying to",
    "start": "917839",
    "end": "921279"
  },
  {
    "text": "combine them into into a mixed data set",
    "start": "921279",
    "end": "924839"
  },
  {
    "text": "and when you combine them you need to",
    "start": "924839",
    "end": "926639"
  },
  {
    "text": "resampling them some of them you want to",
    "start": "926639",
    "end": "928560"
  },
  {
    "text": "upamp Ling them some of them you want to",
    "start": "928560",
    "end": "930440"
  },
  {
    "text": "down sampling",
    "start": "930440",
    "end": "931560"
  },
  {
    "text": "them so this is how we do",
    "start": "931560",
    "end": "934199"
  },
  {
    "text": "that and as you can see the main issue",
    "start": "934199",
    "end": "938199"
  },
  {
    "text": "is like it has too many heris because",
    "start": "938199",
    "end": "941560"
  },
  {
    "text": "when you do linking you do Den noising",
    "start": "941560",
    "end": "943800"
  },
  {
    "text": "the filtering is kind of a lot of",
    "start": "943800",
    "end": "945440"
  },
  {
    "text": "heuristics when you do data mining the",
    "start": "945440",
    "end": "947519"
  },
  {
    "text": "queres are a lot of heuristics when you",
    "start": "947519",
    "end": "949720"
  },
  {
    "text": "do sampling you need to wait all these",
    "start": "949720",
    "end": "952519"
  },
  {
    "text": "small data set is also kind of heris",
    "start": "952519",
    "end": "955079"
  },
  {
    "text": "sixs and another problem is that this",
    "start": "955079",
    "end": "958560"
  },
  {
    "text": "phase",
    "start": "958560",
    "end": "959800"
  },
  {
    "text": "because it we do it offline before",
    "start": "959800",
    "end": "961920"
  },
  {
    "text": "training so this phase and with we I",
    "start": "961920",
    "end": "965399"
  },
  {
    "text": "doubt that it's not this phase and the",
    "start": "965399",
    "end": "968319"
  },
  {
    "text": "training and eval they are kind of",
    "start": "968319",
    "end": "970399"
  },
  {
    "text": "disconnected because why we believe the",
    "start": "970399",
    "end": "972920"
  },
  {
    "text": "selected data this way can is good for",
    "start": "972920",
    "end": "975600"
  },
  {
    "text": "training a high quality model so it's",
    "start": "975600",
    "end": "978720"
  },
  {
    "text": "more like a based on our intuition I",
    "start": "978720",
    "end": "981199"
  },
  {
    "text": "think",
    "start": "981199",
    "end": "983519"
  },
  {
    "text": "so but what's a new approach the new",
    "start": "983519",
    "end": "987120"
  },
  {
    "text": "approach here is like a the key Insight",
    "start": "987120",
    "end": "990440"
  },
  {
    "text": "is like I believe our eval results",
    "start": "990440",
    "end": "993680"
  },
  {
    "text": "should guide our data cation instead of",
    "start": "993680",
    "end": "996639"
  },
  {
    "text": "like a it previously is a totally",
    "start": "996639",
    "end": "998800"
  },
  {
    "text": "independent phase then how I going to do",
    "start": "998800",
    "end": "1002279"
  },
  {
    "text": "that what we'll do is like we can train",
    "start": "1002279",
    "end": "1005360"
  },
  {
    "text": "over Dynamic created data set so what",
    "start": "1005360",
    "end": "1009440"
  },
  {
    "text": "does mean the dynamic dynamic means like",
    "start": "1009440",
    "end": "1012240"
  },
  {
    "text": "uh every time when we have a data subset",
    "start": "1012240",
    "end": "1015720"
  },
  {
    "text": "actually it will represent the most",
    "start": "1015720",
    "end": "1017720"
  },
  {
    "text": "worst learning data regarding the",
    "start": "1017720",
    "end": "1019680"
  },
  {
    "text": "current model checkpoint and after each",
    "start": "1019680",
    "end": "1023040"
  },
  {
    "text": "checkpoint actually we can refresh this",
    "start": "1023040",
    "end": "1026000"
  },
  {
    "text": "data uh every time so it's Dynamic it is",
    "start": "1026000",
    "end": "1029199"
  },
  {
    "text": "different for each Epoch and how are I",
    "start": "1029199",
    "end": "1031880"
  },
  {
    "text": "going to do that um so for here actually",
    "start": "1031880",
    "end": "1035558"
  },
  {
    "text": "we need to do some approximation for our",
    "start": "1035559",
    "end": "1037678"
  },
  {
    "text": "evaluation phase because we cannot do",
    "start": "1037679",
    "end": "1039959"
  },
  {
    "text": "that heavyweight evaluation what we will",
    "start": "1039959",
    "end": "1043038"
  },
  {
    "text": "do is like we replace it by a by a proxy",
    "start": "1043039",
    "end": "1047120"
  },
  {
    "text": "to quickly measure the learn ility and",
    "start": "1047120",
    "end": "1050320"
  },
  {
    "text": "how I going to define the learnability",
    "start": "1050320",
    "end": "1052520"
  },
  {
    "text": "this is a very active research area and",
    "start": "1052520",
    "end": "1056039"
  },
  {
    "text": "here is a paper basically I this is just",
    "start": "1056039",
    "end": "1058000"
  },
  {
    "text": "one example because for my presentation",
    "start": "1058000",
    "end": "1059960"
  },
  {
    "text": "it's more like system level optimization",
    "start": "1059960",
    "end": "1061880"
  },
  {
    "text": "not algorithm level just one example",
    "start": "1061880",
    "end": "1064320"
  },
  {
    "text": "this a paper uh recommended by a friend",
    "start": "1064320",
    "end": "1067039"
  },
  {
    "text": "works in a data startup so the idea is",
    "start": "1067039",
    "end": "1070080"
  },
  {
    "text": "like we want you",
    "start": "1070080",
    "end": "1071840"
  },
  {
    "text": "know um this is loss we we computed by",
    "start": "1071840",
    "end": "1076360"
  },
  {
    "text": "by the current checkpoint and then we",
    "start": "1076360",
    "end": "1078320"
  },
  {
    "text": "have another the hold out model then can",
    "start": "1078320",
    "end": "1080480"
  },
  {
    "text": "compute another uh loss and here we",
    "start": "1080480",
    "end": "1083240"
  },
  {
    "text": "comput a Delta basically trying to",
    "start": "1083240",
    "end": "1085200"
  },
  {
    "text": "measure the",
    "start": "1085200",
    "end": "1087640"
  },
  {
    "text": "learnability so this is our second",
    "start": "1087640",
    "end": "1090400"
  },
  {
    "text": "version how we going to do",
    "start": "1090400",
    "end": "1093280"
  },
  {
    "text": "that um first we prepare a hold holdout",
    "start": "1093280",
    "end": "1098280"
  },
  {
    "text": "model for computing this metric",
    "start": "1098280",
    "end": "1100480"
  },
  {
    "text": "basically the Delta here and for this",
    "start": "1100480",
    "end": "1103440"
  },
  {
    "text": "one this is a more like a frozen model",
    "start": "1103440",
    "end": "1106640"
  },
  {
    "text": "so it's a small proxy model",
    "start": "1106640",
    "end": "1109559"
  },
  {
    "text": "and then we use this one to compute uh",
    "start": "1109559",
    "end": "1112640"
  },
  {
    "text": "basically we call it holdout laws so",
    "start": "1112640",
    "end": "1114960"
  },
  {
    "text": "this one as a new feature colum so we",
    "start": "1114960",
    "end": "1117640"
  },
  {
    "text": "store them in our input data we can do",
    "start": "1117640",
    "end": "1119679"
  },
  {
    "text": "that and then we need to train over",
    "start": "1119679",
    "end": "1123240"
  },
  {
    "text": "Dynamic data set how are going to do",
    "start": "1123240",
    "end": "1125240"
  },
  {
    "text": "that so first let's say we have a pre-",
    "start": "1125240",
    "end": "1129280"
  },
  {
    "text": "select data still a pre- select data",
    "start": "1129280",
    "end": "1132000"
  },
  {
    "text": "then we need to compute this kind of",
    "start": "1132000",
    "end": "1133799"
  },
  {
    "text": "metric to measure the learn ability and",
    "start": "1133799",
    "end": "1136600"
  },
  {
    "text": "then we select the top 10 let's say we",
    "start": "1136600",
    "end": "1138200"
  },
  {
    "text": "select the top 10% then we get this",
    "start": "1138200",
    "end": "1141440"
  },
  {
    "text": "small subset data subset and then we can",
    "start": "1141440",
    "end": "1145480"
  },
  {
    "text": "use it to train one Epoch and then",
    "start": "1145480",
    "end": "1148919"
  },
  {
    "text": "produce a new model checkpoint and then",
    "start": "1148919",
    "end": "1152919"
  },
  {
    "text": "at the same time what we can do is like",
    "start": "1152919",
    "end": "1155440"
  },
  {
    "text": "uh we have a validation during training",
    "start": "1155440",
    "end": "1157640"
  },
  {
    "text": "basically to uh make sure we can fail",
    "start": "1157640",
    "end": "1160200"
  },
  {
    "text": "fast if metric does not look right and",
    "start": "1160200",
    "end": "1163120"
  },
  {
    "text": "then at the same time we can use Ray",
    "start": "1163120",
    "end": "1166320"
  },
  {
    "text": "here to kick off another job uh",
    "start": "1166320",
    "end": "1169080"
  },
  {
    "text": "basically this is a data selection job",
    "start": "1169080",
    "end": "1171159"
  },
  {
    "text": "to run another data selection phase over",
    "start": "1171159",
    "end": "1174880"
  },
  {
    "text": "the uh the original data to produce a",
    "start": "1174880",
    "end": "1177360"
  },
  {
    "text": "new subset so as you can see how I going",
    "start": "1177360",
    "end": "1181039"
  },
  {
    "text": "to do that there is a overlap",
    "start": "1181039",
    "end": "1183039"
  },
  {
    "text": "opportunity basically we can overlap",
    "start": "1183039",
    "end": "1185679"
  },
  {
    "text": "data selection phase for the next Epoch",
    "start": "1185679",
    "end": "1188400"
  },
  {
    "text": "and the validation for the current Epoch",
    "start": "1188400",
    "end": "1190880"
  },
  {
    "text": "so here we can by using Ray we can",
    "start": "1190880",
    "end": "1193400"
  },
  {
    "text": "easily kick off two async tasks one is",
    "start": "1193400",
    "end": "1196080"
  },
  {
    "text": "data selection one is validation",
    "start": "1196080",
    "end": "1200399"
  },
  {
    "text": "uh so what are simplified here so here",
    "start": "1201480",
    "end": "1205480"
  },
  {
    "text": "is a screenshot I basically I um",
    "start": "1205480",
    "end": "1210400"
  },
  {
    "text": "basically from the paper original paper",
    "start": "1210400",
    "end": "1212880"
  },
  {
    "text": "in this paper they claim it it claims",
    "start": "1212880",
    "end": "1215320"
  },
  {
    "text": "that it by using this we are able to",
    "start": "1215320",
    "end": "1217960"
  },
  {
    "text": "choose low noise data task relevant data",
    "start": "1217960",
    "end": "1221280"
  },
  {
    "text": "non- redundant data so what does it mean",
    "start": "1221280",
    "end": "1225280"
  },
  {
    "text": "here it means like if we are able to",
    "start": "1225280",
    "end": "1227480"
  },
  {
    "text": "select low noise data then actually we",
    "start": "1227480",
    "end": "1229880"
  },
  {
    "text": "don't need to do much Den noising uh so",
    "start": "1229880",
    "end": "1232600"
  },
  {
    "text": "it means like we don't need to design",
    "start": "1232600",
    "end": "1234400"
  },
  {
    "text": "very complex data",
    "start": "1234400",
    "end": "1236000"
  },
  {
    "text": "linking and it should be able to aut",
    "start": "1236000",
    "end": "1239080"
  },
  {
    "text": "select task relevant data so it means",
    "start": "1239080",
    "end": "1242200"
  },
  {
    "text": "like when we do we probably don't need",
    "start": "1242200",
    "end": "1244039"
  },
  {
    "text": "to do too much complex data mining to",
    "start": "1244039",
    "end": "1246240"
  },
  {
    "text": "mine different",
    "start": "1246240",
    "end": "1247919"
  },
  {
    "text": "scenarios to make sure it's task",
    "start": "1247919",
    "end": "1250240"
  },
  {
    "text": "relevant and for we also we expect that",
    "start": "1250240",
    "end": "1254400"
  },
  {
    "text": "it can uh be non-redundant then when we",
    "start": "1254400",
    "end": "1257480"
  },
  {
    "text": "merge the data set actually we don't",
    "start": "1257480",
    "end": "1259559"
  },
  {
    "text": "need to D duplicate the data sets the",
    "start": "1259559",
    "end": "1262679"
  },
  {
    "text": "most important one is like we hopefully",
    "start": "1262679",
    "end": "1265240"
  },
  {
    "text": "the data we selected can be the most",
    "start": "1265240",
    "end": "1267440"
  },
  {
    "text": "worst learning",
    "start": "1267440",
    "end": "1268760"
  },
  {
    "text": "data then we don't really need to",
    "start": "1268760",
    "end": "1271120"
  },
  {
    "text": "resample the data sets because we know",
    "start": "1271120",
    "end": "1273480"
  },
  {
    "text": "these are the data uh we",
    "start": "1273480",
    "end": "1277120"
  },
  {
    "text": "need",
    "start": "1277120",
    "end": "1279440"
  },
  {
    "text": "uh so here uh is a comparison how we do",
    "start": "1279440",
    "end": "1284840"
  },
  {
    "text": "that um first the background is like a",
    "start": "1284840",
    "end": "1288120"
  },
  {
    "text": "currently let's say if we do traditional",
    "start": "1288120",
    "end": "1290480"
  },
  {
    "text": "online data selection what are we going",
    "start": "1290480",
    "end": "1292400"
  },
  {
    "text": "to do so when we do data loading first",
    "start": "1292400",
    "end": "1295720"
  },
  {
    "text": "we will load our super batch which is a",
    "start": "1295720",
    "end": "1297600"
  },
  {
    "text": "large batch and then by using the same",
    "start": "1297600",
    "end": "1300240"
  },
  {
    "text": "metric we can compute a top top K right",
    "start": "1300240",
    "end": "1303200"
  },
  {
    "text": "and then we can get more like local",
    "start": "1303200",
    "end": "1305120"
  },
  {
    "text": "maximum because it's the maximum within",
    "start": "1305120",
    "end": "1307679"
  },
  {
    "text": "that",
    "start": "1307679",
    "end": "1308440"
  },
  {
    "text": "batch and then data selection basically",
    "start": "1308440",
    "end": "1311640"
  },
  {
    "text": "is part of their loading but for our",
    "start": "1311640",
    "end": "1314360"
  },
  {
    "text": "design actually we clearly when we",
    "start": "1314360",
    "end": "1317720"
  },
  {
    "text": "compute the top C actually computed over",
    "start": "1317720",
    "end": "1320200"
  },
  {
    "text": "the entire data set so what we get is a",
    "start": "1320200",
    "end": "1322799"
  },
  {
    "text": "global",
    "start": "1322799",
    "end": "1323919"
  },
  {
    "text": "maximum uh what's the benefit here so we",
    "start": "1323919",
    "end": "1326919"
  },
  {
    "text": "expect like uh this could be potentially",
    "start": "1326919",
    "end": "1329200"
  },
  {
    "text": "give a higher model quality because we",
    "start": "1329200",
    "end": "1331279"
  },
  {
    "text": "use Global maximum instead of a union of",
    "start": "1331279",
    "end": "1334120"
  },
  {
    "text": "multiple local maximums and this is",
    "start": "1334120",
    "end": "1337400"
  },
  {
    "text": "actually I believe it can be important",
    "start": "1337400",
    "end": "1340120"
  },
  {
    "text": "especially when the case of uh the batch",
    "start": "1340120",
    "end": "1342640"
  },
  {
    "text": "size is relative small so um maybe some",
    "start": "1342640",
    "end": "1346279"
  },
  {
    "text": "people know like uh the difference",
    "start": "1346279",
    "end": "1348080"
  },
  {
    "text": "between",
    "start": "1348080",
    "end": "1349080"
  },
  {
    "text": "batch norm and group Norm when",
    "start": "1349080",
    "end": "1351880"
  },
  {
    "text": "especially in Vision tasks uh the batch",
    "start": "1351880",
    "end": "1354279"
  },
  {
    "text": "size sometimes can be very small",
    "start": "1354279",
    "end": "1355799"
  },
  {
    "text": "relative small and when it's small",
    "start": "1355799",
    "end": "1357559"
  },
  {
    "text": "actually batch Norm may not work very",
    "start": "1357559",
    "end": "1359240"
  },
  {
    "text": "well and that's why actually there group",
    "start": "1359240",
    "end": "1362520"
  },
  {
    "text": "Norm is designed so it's similar here",
    "start": "1362520",
    "end": "1365360"
  },
  {
    "text": "and another uh benefit is like data",
    "start": "1365360",
    "end": "1367640"
  },
  {
    "text": "selection now is independent of their",
    "start": "1367640",
    "end": "1369799"
  },
  {
    "text": "loading in our design which means like",
    "start": "1369799",
    "end": "1372200"
  },
  {
    "text": "uh we can use more data selection",
    "start": "1372200",
    "end": "1374159"
  },
  {
    "text": "workers than the DAT loader",
    "start": "1374159",
    "end": "1376440"
  },
  {
    "text": "workers and then we can do the we can do",
    "start": "1376440",
    "end": "1379360"
  },
  {
    "text": "it faster with more resources and it can",
    "start": "1379360",
    "end": "1381880"
  },
  {
    "text": "also create overlap opportunities for us",
    "start": "1381880",
    "end": "1384240"
  },
  {
    "text": "we can overlap data selection with",
    "start": "1384240",
    "end": "1387760"
  },
  {
    "text": "validation and all of this I I really",
    "start": "1387760",
    "end": "1390720"
  },
  {
    "text": "appreciate Ray which can provide like a",
    "start": "1390720",
    "end": "1393600"
  },
  {
    "text": "good elasticity so in my opinion so also",
    "start": "1393600",
    "end": "1397520"
  },
  {
    "text": "for example if you want to do elastic",
    "start": "1397520",
    "end": "1399360"
  },
  {
    "text": "training actually you can use torch",
    "start": "1399360",
    "end": "1400960"
  },
  {
    "text": "elastic but here I believe Ray is more",
    "start": "1400960",
    "end": "1404159"
  },
  {
    "text": "generic because you can do um do all of",
    "start": "1404159",
    "end": "1407720"
  },
  {
    "text": "this like a elasticity um including the",
    "start": "1407720",
    "end": "1411039"
  },
  {
    "text": "data processing around training I think",
    "start": "1411039",
    "end": "1413279"
  },
  {
    "text": "that is",
    "start": "1413279",
    "end": "1414760"
  },
  {
    "text": "important okay so the takeaway for me is",
    "start": "1414760",
    "end": "1418159"
  },
  {
    "text": "like that's my personal opinion uh my",
    "start": "1418159",
    "end": "1420480"
  },
  {
    "text": "favorite array features the first one is",
    "start": "1420480",
    "end": "1422640"
  },
  {
    "text": "hetrogeneous",
    "start": "1422640",
    "end": "1423559"
  },
  {
    "text": "Computing so this is important even when",
    "start": "1423559",
    "end": "1426720"
  },
  {
    "text": "we do some data processing because you",
    "start": "1426720",
    "end": "1428640"
  },
  {
    "text": "will never know if when you want to do",
    "start": "1428640",
    "end": "1430760"
  },
  {
    "text": "some offload some some workload to GPU",
    "start": "1430760",
    "end": "1434679"
  },
  {
    "text": "in in our examples for example when we",
    "start": "1434679",
    "end": "1436640"
  },
  {
    "text": "do data preparation we may run some",
    "start": "1436640",
    "end": "1438960"
  },
  {
    "text": "model inference on GPU to generate some",
    "start": "1438960",
    "end": "1441520"
  },
  {
    "text": "synthetic data or sometimes if the when",
    "start": "1441520",
    "end": "1444760"
  },
  {
    "text": "we do the eval right so some geometry uh",
    "start": "1444760",
    "end": "1447880"
  },
  {
    "text": "matrics we can move them to GPU to",
    "start": "1447880",
    "end": "1450720"
  },
  {
    "text": "compute and another feature I really",
    "start": "1450720",
    "end": "1453159"
  },
  {
    "text": "like is elastic Computing it's very easy",
    "start": "1453159",
    "end": "1456039"
  },
  {
    "text": "for us to spawn an async uh like a job",
    "start": "1456039",
    "end": "1460120"
  },
  {
    "text": "here is like a data selection task",
    "start": "1460120",
    "end": "1461799"
  },
  {
    "text": "during trading we can do that and this",
    "start": "1461799",
    "end": "1464200"
  },
  {
    "text": "can be generically applied to data",
    "start": "1464200",
    "end": "1466480"
  },
  {
    "text": "processing task around training so I",
    "start": "1466480",
    "end": "1468520"
  },
  {
    "text": "think this is more generic than as I",
    "start": "1468520",
    "end": "1470520"
  },
  {
    "text": "said than torch matri torch",
    "start": "1470520",
    "end": "1472960"
  },
  {
    "text": "elastic uh however there are some also",
    "start": "1472960",
    "end": "1475720"
  },
  {
    "text": "Cav as sometimes we need to have some",
    "start": "1475720",
    "end": "1478960"
  },
  {
    "text": "workarounds for the cases not designed",
    "start": "1478960",
    "end": "1481399"
  },
  {
    "text": "for that um not designed by by Ray so",
    "start": "1481399",
    "end": "1485360"
  },
  {
    "text": "for example like a data frame actually I",
    "start": "1485360",
    "end": "1487760"
  },
  {
    "text": "believe is a first class citizen in Ray",
    "start": "1487760",
    "end": "1490640"
  },
  {
    "text": "data Library it's easy for vectorization",
    "start": "1490640",
    "end": "1493399"
  },
  {
    "text": "because it's more like a column store",
    "start": "1493399",
    "end": "1495480"
  },
  {
    "text": "but it will make it will make it very",
    "start": "1495480",
    "end": "1497200"
  },
  {
    "text": "hard to do efficient join Over large",
    "start": "1497200",
    "end": "1500039"
  },
  {
    "text": "data sets because you do let's say harsh",
    "start": "1500039",
    "end": "1502399"
  },
  {
    "text": "based join it's really hard to do that",
    "start": "1502399",
    "end": "1504240"
  },
  {
    "text": "so you may need to have some",
    "start": "1504240",
    "end": "1507480"
  },
  {
    "text": "workarounds okay I think that's all",
    "start": "1507880",
    "end": "1510720"
  },
  {
    "text": "thank",
    "start": "1510720",
    "end": "1512919"
  },
  {
    "text": "you all right for",
    "start": "1514559",
    "end": "1518158"
  },
  {
    "text": "q& thank you for the talk you you I just",
    "start": "1518360",
    "end": "1521480"
  },
  {
    "text": "wonder in your last slide you mentioned",
    "start": "1521480",
    "end": "1523760"
  },
  {
    "text": "the join problem um that is pretty hard",
    "start": "1523760",
    "end": "1526080"
  },
  {
    "text": "to do have you solved this issue um",
    "start": "1526080",
    "end": "1529080"
  },
  {
    "text": "joining yeah for us we kind of bypass",
    "start": "1529080",
    "end": "1532039"
  },
  {
    "text": "the issue we actually our um one thought",
    "start": "1532039",
    "end": "1535880"
  },
  {
    "text": "is like we can leverage other libraries",
    "start": "1535880",
    "end": "1537640"
  },
  {
    "text": "because Ray data is very easy to convert",
    "start": "1537640",
    "end": "1540200"
  },
  {
    "text": "to some other libraries like moding like",
    "start": "1540200",
    "end": "1543000"
  },
  {
    "text": "Mars like dask so we can do that but",
    "start": "1543000",
    "end": "1546240"
  },
  {
    "text": "another way for us actually we for our",
    "start": "1546240",
    "end": "1549480"
  },
  {
    "text": "case because we already have a log level",
    "start": "1549480",
    "end": "1551520"
  },
  {
    "text": "parallelism when we process our data we",
    "start": "1551520",
    "end": "1553919"
  },
  {
    "text": "have different Drive lcks So within each",
    "start": "1553919",
    "end": "1556200"
  },
  {
    "text": "Drive lock basically we have already",
    "start": "1556200",
    "end": "1557679"
  },
  {
    "text": "have a one level of parallelism but",
    "start": "1557679",
    "end": "1560240"
  },
  {
    "text": "within each one we are able to do that",
    "start": "1560240",
    "end": "1562440"
  },
  {
    "text": "within single node so it's okay we need",
    "start": "1562440",
    "end": "1564679"
  },
  {
    "text": "to just do some optimization for example",
    "start": "1564679",
    "end": "1567159"
  },
  {
    "text": "we don't want to we want to avoid like a",
    "start": "1567159",
    "end": "1569480"
  },
  {
    "text": "very expensive data copies when we do",
    "start": "1569480",
    "end": "1571440"
  },
  {
    "text": "join so we do some optimizations then we",
    "start": "1571440",
    "end": "1573960"
  },
  {
    "text": "bypass this issue but if not if we want",
    "start": "1573960",
    "end": "1577080"
  },
  {
    "text": "to do it in a scalable way probably",
    "start": "1577080",
    "end": "1578799"
  },
  {
    "text": "we'll just use some other libraries",
    "start": "1578799",
    "end": "1582760"
  },
  {
    "text": "yeah um uh thank you for the",
    "start": "1582799",
    "end": "1585360"
  },
  {
    "text": "presentation I have a question regarding",
    "start": "1585360",
    "end": "1587039"
  },
  {
    "text": "the evaluation part so you mention you",
    "start": "1587039",
    "end": "1589440"
  },
  {
    "text": "have lots of kind of Matrix I wondering",
    "start": "1589440",
    "end": "1592159"
  },
  {
    "text": "the first question do you have any",
    "start": "1592159",
    "end": "1593799"
  },
  {
    "text": "abstraction on your side uh to Define",
    "start": "1593799",
    "end": "1597440"
  },
  {
    "text": "how you compute The Matrix how you",
    "start": "1597440",
    "end": "1599000"
  },
  {
    "text": "aggregate Matrix and then if you have a",
    "start": "1599000",
    "end": "1601559"
  },
  {
    "text": "abstraction how you integrate that one",
    "start": "1601559",
    "end": "1603240"
  },
  {
    "text": "with the r data SE for Ray data uh so",
    "start": "1603240",
    "end": "1607880"
  },
  {
    "text": "basically the data is expressed as a",
    "start": "1607880",
    "end": "1610279"
  },
  {
    "text": "data frame like kind of data frame and",
    "start": "1610279",
    "end": "1612120"
  },
  {
    "text": "then you computer metric basically you",
    "start": "1612120",
    "end": "1614880"
  },
  {
    "text": "paralyze it at low level and then R data",
    "start": "1614880",
    "end": "1617600"
  },
  {
    "text": "has",
    "start": "1617600",
    "end": "1618840"
  },
  {
    "text": "agregation aggregation basically the API",
    "start": "1618840",
    "end": "1621480"
  },
  {
    "text": "right you can do that you can do some or",
    "start": "1621480",
    "end": "1623760"
  },
  {
    "text": "something so so your matrix definition",
    "start": "1623760",
    "end": "1626399"
  },
  {
    "text": "is directly like coding against the r",
    "start": "1626399",
    "end": "1628840"
  },
  {
    "text": "data API how computer is is already",
    "start": "1628840",
    "end": "1631039"
  },
  {
    "text": "encoded in that class right so people",
    "start": "1631039",
    "end": "1633840"
  },
  {
    "text": "basically for us the ideal case like we",
    "start": "1633840",
    "end": "1636200"
  },
  {
    "text": "can provide just a declarative interface",
    "start": "1636200",
    "end": "1638919"
  },
  {
    "text": "so people just need to specify what",
    "start": "1638919",
    "end": "1640840"
  },
  {
    "text": "matric I need to compute then just click",
    "start": "1640840",
    "end": "1643159"
  },
  {
    "text": "it then here we",
    "start": "1643159",
    "end": "1646080"
  },
  {
    "text": "go cool any other question",
    "start": "1646520",
    "end": "1650640"
  },
  {
    "text": "yep hey thanks for the talk uh may I ask",
    "start": "1651679",
    "end": "1655039"
  },
  {
    "text": "what is the rational for choosing a ppup",
    "start": "1655039",
    "end": "1658559"
  },
  {
    "text": "system for a Last Mile data processing",
    "start": "1658559",
    "end": "1661760"
  },
  {
    "text": "system compared to just traditional ETL",
    "start": "1661760",
    "end": "1664279"
  },
  {
    "text": "processing which computes data and puts",
    "start": "1664279",
    "end": "1666000"
  },
  {
    "text": "it into a data L and then use that for",
    "start": "1666000",
    "end": "1669039"
  },
  {
    "text": "um your data preparation for model",
    "start": "1669039",
    "end": "1671559"
  },
  {
    "text": "training um the main I think the main",
    "start": "1671559",
    "end": "1674519"
  },
  {
    "text": "intuition is like uh we need to",
    "start": "1674519",
    "end": "1676120"
  },
  {
    "text": "subscribe a few different data sources",
    "start": "1676120",
    "end": "1678240"
  },
  {
    "text": "and we join here so we want first we",
    "start": "1678240",
    "end": "1681279"
  },
  {
    "text": "want to separate our responsibility the",
    "start": "1681279",
    "end": "1683760"
  },
  {
    "text": "purple area basically is fully owned by",
    "start": "1683760",
    "end": "1685799"
  },
  {
    "text": "our infra teams we don't care about as",
    "start": "1685799",
    "end": "1688080"
  },
  {
    "text": "like a uh they just they maintain the",
    "start": "1688080",
    "end": "1690519"
  },
  {
    "text": "data sources and the second part this is",
    "start": "1690519",
    "end": "1692679"
  },
  {
    "text": "our ml code base so Ray is really is",
    "start": "1692679",
    "end": "1695960"
  },
  {
    "text": "like 100% pythonic so really good for",
    "start": "1695960",
    "end": "1698320"
  },
  {
    "text": "that at Upstream is more like we",
    "start": "1698320",
    "end": "1700279"
  },
  {
    "text": "probably use spark doing traditional ETL",
    "start": "1700279",
    "end": "1702760"
  },
  {
    "text": "jobs so we want to decoupling this first",
    "start": "1702760",
    "end": "1706440"
  },
  {
    "text": "I think that's Main and for for this",
    "start": "1706440",
    "end": "1708440"
  },
  {
    "text": "part actually is not really that um time",
    "start": "1708440",
    "end": "1711519"
  },
  {
    "text": "sensitive it's like a because when we",
    "start": "1711519",
    "end": "1713120"
  },
  {
    "text": "release data it's like a daily so it's",
    "start": "1713120",
    "end": "1715000"
  },
  {
    "text": "not that time sensitive so it's okay",
    "start": "1715000",
    "end": "1717360"
  },
  {
    "text": "even if you don't use that I would say I",
    "start": "1717360",
    "end": "1719760"
  },
  {
    "text": "think the coupling is the main the key",
    "start": "1719760",
    "end": "1721519"
  },
  {
    "text": "here",
    "start": "1721519",
    "end": "1724519"
  }
]