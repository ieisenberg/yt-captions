[
  {
    "text": "hello everyone my name is Alex",
    "start": "3060",
    "end": "4980"
  },
  {
    "text": "spiritanov and I'm a product manager for",
    "start": "4980",
    "end": "7379"
  },
  {
    "text": "Google Cloud AI accelerators and I'm",
    "start": "7379",
    "end": "9599"
  },
  {
    "text": "here today with my colleague Ellen Wong",
    "start": "9599",
    "end": "11340"
  },
  {
    "text": "to tell you more about developing rate",
    "start": "11340",
    "end": "13500"
  },
  {
    "text": "applications with Google Cloud tpus",
    "start": "13500",
    "end": "17400"
  },
  {
    "text": "at Google Cloud we are proud to be the",
    "start": "17400",
    "end": "19920"
  },
  {
    "text": "only cloud provider that offers the full",
    "start": "19920",
    "end": "22320"
  },
  {
    "text": "range of AI accelerator choices",
    "start": "22320",
    "end": "24960"
  },
  {
    "text": "from Nvidia powered GPU offerings to our",
    "start": "24960",
    "end": "28019"
  },
  {
    "text": "own custom-built tensor processing units",
    "start": "28019",
    "end": "30599"
  },
  {
    "text": "or tpus",
    "start": "30599",
    "end": "32520"
  },
  {
    "text": "now Google Cloud gpus represent nearly a",
    "start": "32520",
    "end": "35280"
  },
  {
    "text": "decade of innovation in AI super",
    "start": "35280",
    "end": "37380"
  },
  {
    "text": "computing we started in 2015 with TPU V1",
    "start": "37380",
    "end": "40800"
  },
  {
    "text": "our inference only accelerator",
    "start": "40800",
    "end": "43260"
  },
  {
    "text": "in 2018 we'll launched Cloud TPU V2 pods",
    "start": "43260",
    "end": "46620"
  },
  {
    "text": "where for the first time we connected",
    "start": "46620",
    "end": "49160"
  },
  {
    "text": "256 tpus with an ultra fast interconnect",
    "start": "49160",
    "end": "52920"
  },
  {
    "text": "to enable training at scale",
    "start": "52920",
    "end": "55800"
  },
  {
    "text": "two years later we quadrupled the number",
    "start": "55800",
    "end": "57840"
  },
  {
    "text": "of chips to 1024 and introduced liquid",
    "start": "57840",
    "end": "61020"
  },
  {
    "text": "cooling for even more performance",
    "start": "61020",
    "end": "63059"
  },
  {
    "text": "and two years later we quadrupled the",
    "start": "63059",
    "end": "65760"
  },
  {
    "text": "number of tpus again to 4096 and",
    "start": "65760",
    "end": "69299"
  },
  {
    "text": "introduced an ultra fast Optical",
    "start": "69299",
    "end": "71520"
  },
  {
    "text": "interconnect",
    "start": "71520",
    "end": "72960"
  },
  {
    "text": "so this brings us to today",
    "start": "72960",
    "end": "74939"
  },
  {
    "text": "just two weeks ago at Google Cloud next",
    "start": "74939",
    "end": "77280"
  },
  {
    "text": "we introduced the cloud TPU v5e our",
    "start": "77280",
    "end": "80880"
  },
  {
    "text": "newest and greatest AI supercomputer",
    "start": "80880",
    "end": "85259"
  },
  {
    "text": "now what makes Cloud TPU V5 is so great",
    "start": "85259",
    "end": "87840"
  },
  {
    "text": "well in His Brilliant keynote this",
    "start": "87840",
    "end": "90240"
  },
  {
    "text": "morning Robert talked about the",
    "start": "90240",
    "end": "92520"
  },
  {
    "text": "importance of cost efficiency and scale",
    "start": "92520",
    "end": "95280"
  },
  {
    "text": "and that is exactly what the cloud TPU",
    "start": "95280",
    "end": "97560"
  },
  {
    "text": "v5e was designed to do",
    "start": "97560",
    "end": "99960"
  },
  {
    "text": "v5e is incredibly cost efficient it",
    "start": "99960",
    "end": "103259"
  },
  {
    "text": "offers up to two times more performance",
    "start": "103259",
    "end": "105420"
  },
  {
    "text": "per dollar for training and up to two",
    "start": "105420",
    "end": "107759"
  },
  {
    "text": "and a half times more performance per",
    "start": "107759",
    "end": "109860"
  },
  {
    "text": "dollar for inference compared to Cloud",
    "start": "109860",
    "end": "111899"
  },
  {
    "text": "TPU V4",
    "start": "111899",
    "end": "113640"
  },
  {
    "text": "v5e is also incredibly versatile we",
    "start": "113640",
    "end": "116640"
  },
  {
    "text": "support the most popular ml Frameworks",
    "start": "116640",
    "end": "118740"
  },
  {
    "text": "including pytorch Jacks and tensorflow",
    "start": "118740",
    "end": "121520"
  },
  {
    "text": "and of course we integrate closely with",
    "start": "121520",
    "end": "124380"
  },
  {
    "text": "Ray",
    "start": "124380",
    "end": "125579"
  },
  {
    "text": "finally a superpower of Claude tpu5e is",
    "start": "125579",
    "end": "129840"
  },
  {
    "text": "its amazing scalability",
    "start": "129840",
    "end": "132000"
  },
  {
    "text": "with v5e you can train large language",
    "start": "132000",
    "end": "134459"
  },
  {
    "text": "models and generative AI models using",
    "start": "134459",
    "end": "137340"
  },
  {
    "text": "tens of thousands of TPU chips with our",
    "start": "137340",
    "end": "139980"
  },
  {
    "text": "new technology which we call multi-slice",
    "start": "139980",
    "end": "142140"
  },
  {
    "text": "training",
    "start": "142140",
    "end": "143459"
  },
  {
    "text": "and after training or fine-tuning you",
    "start": "143459",
    "end": "146099"
  },
  {
    "text": "can run inference on the same tpv5e",
    "start": "146099",
    "end": "148560"
  },
  {
    "text": "Hardware with great cost efficiency and",
    "start": "148560",
    "end": "151860"
  },
  {
    "text": "you can serve models with up to two",
    "start": "151860",
    "end": "154140"
  },
  {
    "text": "trillion parameters using a technology",
    "start": "154140",
    "end": "156780"
  },
  {
    "text": "which we call multi-host inference",
    "start": "156780",
    "end": "159480"
  },
  {
    "text": "now all of those capabilities come from",
    "start": "159480",
    "end": "162120"
  },
  {
    "text": "the magic of combined hardware and",
    "start": "162120",
    "end": "164519"
  },
  {
    "text": "software optimizations on the hardware",
    "start": "164519",
    "end": "166680"
  },
  {
    "text": "side they're powered with our Ultra fast",
    "start": "166680",
    "end": "169920"
  },
  {
    "text": "inner chip interconnect and with our",
    "start": "169920",
    "end": "172080"
  },
  {
    "text": "highly efficient Google cloud data",
    "start": "172080",
    "end": "173819"
  },
  {
    "text": "center networking and on the software",
    "start": "173819",
    "end": "176040"
  },
  {
    "text": "side we constantly optimize the software",
    "start": "176040",
    "end": "178980"
  },
  {
    "text": "stack including xla our AI compiler to",
    "start": "178980",
    "end": "182879"
  },
  {
    "text": "get the highest performance and",
    "start": "182879",
    "end": "184140"
  },
  {
    "text": "efficiency for both training and",
    "start": "184140",
    "end": "186000"
  },
  {
    "text": "inference",
    "start": "186000",
    "end": "188040"
  },
  {
    "text": "now let's take a closer look at our",
    "start": "188040",
    "end": "189900"
  },
  {
    "text": "performance numbers",
    "start": "189900",
    "end": "191280"
  },
  {
    "text": "as I mentioned TPU v5e is highly cost",
    "start": "191280",
    "end": "194640"
  },
  {
    "text": "efficient offering up to two times",
    "start": "194640",
    "end": "197099"
  },
  {
    "text": "greater performance per dollar for",
    "start": "197099",
    "end": "199080"
  },
  {
    "text": "training large language models and",
    "start": "199080",
    "end": "200700"
  },
  {
    "text": "generative AI models",
    "start": "200700",
    "end": "202379"
  },
  {
    "text": "and you can achieve even greater",
    "start": "202379",
    "end": "204659"
  },
  {
    "text": "performance per dollar if you do your",
    "start": "204659",
    "end": "206580"
  },
  {
    "text": "training in int 8 Precision which is",
    "start": "206580",
    "end": "209340"
  },
  {
    "text": "natively supported by the v5e hardware",
    "start": "209340",
    "end": "211260"
  },
  {
    "text": "and the software stack",
    "start": "211260",
    "end": "214080"
  },
  {
    "text": "now Robert talked this morning about the",
    "start": "214080",
    "end": "216360"
  },
  {
    "text": "importance of scaling to meet the",
    "start": "216360",
    "end": "218459"
  },
  {
    "text": "growing demand for AI compute and this",
    "start": "218459",
    "end": "220920"
  },
  {
    "text": "is where Cloud TPU V5 here really shines",
    "start": "220920",
    "end": "222980"
  },
  {
    "text": "you can seamlessly scale training your",
    "start": "222980",
    "end": "226140"
  },
  {
    "text": "models from 256 chips all the way to",
    "start": "226140",
    "end": "229860"
  },
  {
    "text": "more than 32 000 TPU chips",
    "start": "229860",
    "end": "233220"
  },
  {
    "text": "32 000 TPU chips to train a single model",
    "start": "233220",
    "end": "235920"
  },
  {
    "text": "that is 128 full TPU v5e pods that's an",
    "start": "235920",
    "end": "241260"
  },
  {
    "text": "incredible amount of compute and with",
    "start": "241260",
    "end": "243900"
  },
  {
    "text": "multi-slice training you can achieve",
    "start": "243900",
    "end": "246599"
  },
  {
    "text": "nearly perfect linear scaling for your",
    "start": "246599",
    "end": "249299"
  },
  {
    "text": "training job",
    "start": "249299",
    "end": "250860"
  },
  {
    "text": "that is a game changer for training your",
    "start": "250860",
    "end": "253560"
  },
  {
    "text": "large language models and generative AI",
    "start": "253560",
    "end": "255299"
  },
  {
    "text": "models because now you can train even",
    "start": "255299",
    "end": "257940"
  },
  {
    "text": "larger models in less time simply by",
    "start": "257940",
    "end": "261120"
  },
  {
    "text": "adding more tpus",
    "start": "261120",
    "end": "264000"
  },
  {
    "text": "now of course after you have trained or",
    "start": "264000",
    "end": "266160"
  },
  {
    "text": "fine-tuned your model you need to run",
    "start": "266160",
    "end": "268139"
  },
  {
    "text": "inference on it very cost efficiently",
    "start": "268139",
    "end": "271020"
  },
  {
    "text": "this is another area where Cloud tpv5e",
    "start": "271020",
    "end": "273840"
  },
  {
    "text": "achieves great cost efficiency and great",
    "start": "273840",
    "end": "276240"
  },
  {
    "text": "performance per dollar",
    "start": "276240",
    "end": "277620"
  },
  {
    "text": "with up to two and a half times greater",
    "start": "277620",
    "end": "280500"
  },
  {
    "text": "performance per dollar on leading large",
    "start": "280500",
    "end": "282540"
  },
  {
    "text": "language models such as Lama 2 and",
    "start": "282540",
    "end": "284880"
  },
  {
    "text": "generative AI models such as stable",
    "start": "284880",
    "end": "286500"
  },
  {
    "text": "diffusion",
    "start": "286500",
    "end": "289080"
  },
  {
    "text": "and of course as AI developers We Care",
    "start": "289080",
    "end": "291900"
  },
  {
    "text": "both about throughput and latency",
    "start": "291900",
    "end": "293639"
  },
  {
    "text": "because we want our AI applications to",
    "start": "293639",
    "end": "296100"
  },
  {
    "text": "be very responsive for our users and so",
    "start": "296100",
    "end": "298800"
  },
  {
    "text": "on latency v5e achieves up to 1.7 times",
    "start": "298800",
    "end": "302699"
  },
  {
    "text": "the speed up compared to Cloud TPU V4",
    "start": "302699",
    "end": "307340"
  },
  {
    "text": "now of course we know that as developers",
    "start": "307620",
    "end": "309780"
  },
  {
    "text": "you want to use tpus in your preferred",
    "start": "309780",
    "end": "312660"
  },
  {
    "text": "way to fit your development lifecycle",
    "start": "312660",
    "end": "316320"
  },
  {
    "text": "and this is where Ray on cloud tpus",
    "start": "316320",
    "end": "318600"
  },
  {
    "text": "really shines because Ray makes it",
    "start": "318600",
    "end": "321419"
  },
  {
    "text": "seamless to train and serve llms and",
    "start": "321419",
    "end": "324600"
  },
  {
    "text": "generative AI models using the latest",
    "start": "324600",
    "end": "326759"
  },
  {
    "text": "and greatest cloud tpus",
    "start": "326759",
    "end": "329820"
  },
  {
    "text": "for training you can start by developing",
    "start": "329820",
    "end": "332220"
  },
  {
    "text": "a model locally on your laptop and then",
    "start": "332220",
    "end": "334979"
  },
  {
    "text": "you can seamlessly scale it up to",
    "start": "334979",
    "end": "337199"
  },
  {
    "text": "hundreds or even thousands and tens of",
    "start": "337199",
    "end": "339360"
  },
  {
    "text": "thousands of tpus using the same",
    "start": "339360",
    "end": "341460"
  },
  {
    "text": "familiar array apis that you know and",
    "start": "341460",
    "end": "344100"
  },
  {
    "text": "love",
    "start": "344100",
    "end": "345419"
  },
  {
    "text": "you don't need to be an expert or have a",
    "start": "345419",
    "end": "347880"
  },
  {
    "text": "PhD in distributed computing the magic",
    "start": "347880",
    "end": "351060"
  },
  {
    "text": "of Rey is that it manages all of the",
    "start": "351060",
    "end": "353880"
  },
  {
    "text": "complexity of orchestrating this massive",
    "start": "353880",
    "end": "356400"
  },
  {
    "text": "amount of compute",
    "start": "356400",
    "end": "358080"
  },
  {
    "text": "with just a few lines of code you can",
    "start": "358080",
    "end": "360479"
  },
  {
    "text": "take advantage of Ray Auto scaling Auto",
    "start": "360479",
    "end": "363000"
  },
  {
    "text": "provisioning and fault tolerance just as",
    "start": "363000",
    "end": "365520"
  },
  {
    "text": "you do today",
    "start": "365520",
    "end": "368000"
  },
  {
    "text": "needless to say array is also great at",
    "start": "368699",
    "end": "371400"
  },
  {
    "text": "fine-tuning pre-trained models using",
    "start": "371400",
    "end": "373380"
  },
  {
    "text": "your data and your tasks to solve your",
    "start": "373380",
    "end": "375780"
  },
  {
    "text": "users needs",
    "start": "375780",
    "end": "377940"
  },
  {
    "text": "and finally after you have trained or",
    "start": "377940",
    "end": "380280"
  },
  {
    "text": "fine-tuned your model you can very",
    "start": "380280",
    "end": "382380"
  },
  {
    "text": "efficiently use Reserve to have your",
    "start": "382380",
    "end": "385380"
  },
  {
    "text": "model running on a TPU within seconds",
    "start": "385380",
    "end": "387660"
  },
  {
    "text": "for inference",
    "start": "387660",
    "end": "389520"
  },
  {
    "text": "and when you're ready to deploy that",
    "start": "389520",
    "end": "391080"
  },
  {
    "text": "model in production you can use Rey to",
    "start": "391080",
    "end": "393720"
  },
  {
    "text": "harness hundreds of TPU accelerators to",
    "start": "393720",
    "end": "396780"
  },
  {
    "text": "meet the most demanding needs of your",
    "start": "396780",
    "end": "398520"
  },
  {
    "text": "users in production",
    "start": "398520",
    "end": "401280"
  },
  {
    "text": "at Google Cloud we love array because it",
    "start": "401280",
    "end": "404340"
  },
  {
    "text": "really makes developing on tpus for",
    "start": "404340",
    "end": "406139"
  },
  {
    "text": "training and inference super easy",
    "start": "406139",
    "end": "409020"
  },
  {
    "text": "but don't just take my word for it we",
    "start": "409020",
    "end": "411240"
  },
  {
    "text": "are here to show you some amazing demos",
    "start": "411240",
    "end": "413340"
  },
  {
    "text": "that show you just how easy it is to get",
    "start": "413340",
    "end": "415919"
  },
  {
    "text": "started with rayon cloud tpus and to",
    "start": "415919",
    "end": "418440"
  },
  {
    "text": "show you more please welcome to the",
    "start": "418440",
    "end": "420300"
  },
  {
    "text": "stage my colleague Alan Wong",
    "start": "420300",
    "end": "424159"
  },
  {
    "text": "thank you Alex",
    "start": "428880",
    "end": "430620"
  },
  {
    "text": "hi everyone my name is Alan Wong I'm the",
    "start": "430620",
    "end": "433680"
  },
  {
    "text": "software engineer at Google and today",
    "start": "433680",
    "end": "435840"
  },
  {
    "text": "I'm excited to show you how you can",
    "start": "435840",
    "end": "437460"
  },
  {
    "text": "easily take advantage of all of these",
    "start": "437460",
    "end": "439500"
  },
  {
    "text": "exciting advancements by demonstrating",
    "start": "439500",
    "end": "441720"
  },
  {
    "text": "to you how easy it is to train and serve",
    "start": "441720",
    "end": "444360"
  },
  {
    "text": "llama 2 using Rey with cloud tpus",
    "start": "444360",
    "end": "449240"
  },
  {
    "text": "so I'm going to go ahead and show you",
    "start": "450479",
    "end": "452220"
  },
  {
    "text": "this notebook here",
    "start": "452220",
    "end": "453599"
  },
  {
    "text": "and",
    "start": "453599",
    "end": "455759"
  },
  {
    "text": "essentially I have some starter code",
    "start": "455759",
    "end": "457259"
  },
  {
    "text": "just to get us started",
    "start": "457259",
    "end": "459120"
  },
  {
    "text": "so I've already used the ray cluster",
    "start": "459120",
    "end": "461520"
  },
  {
    "text": "launcher to spin up array cluster with",
    "start": "461520",
    "end": "463020"
  },
  {
    "text": "tpus attached to it so I'm going to",
    "start": "463020",
    "end": "464940"
  },
  {
    "text": "first do is connect to it",
    "start": "464940",
    "end": "468259"
  },
  {
    "text": "and what you can see here is a snippet",
    "start": "468960",
    "end": "471419"
  },
  {
    "text": "of code that essentially wraps hugging",
    "start": "471419",
    "end": "473580"
  },
  {
    "text": "face",
    "start": "473580",
    "end": "474360"
  },
  {
    "text": "and what I'm trying to say here is that",
    "start": "474360",
    "end": "476099"
  },
  {
    "text": "I want to use a llama 2 model 2 billion",
    "start": "476099",
    "end": "478560"
  },
  {
    "text": "parameter variant and generally here I'm",
    "start": "478560",
    "end": "481259"
  },
  {
    "text": "just going to initialize and then run",
    "start": "481259",
    "end": "482520"
  },
  {
    "text": "some training",
    "start": "482520",
    "end": "484319"
  },
  {
    "text": "if you were to use something like gray",
    "start": "484319",
    "end": "485940"
  },
  {
    "text": "data for all of your pre-processing it's",
    "start": "485940",
    "end": "487800"
  },
  {
    "text": "a great opportunity for you to insert",
    "start": "487800",
    "end": "489300"
  },
  {
    "text": "that here",
    "start": "489300",
    "end": "491340"
  },
  {
    "text": "but regardless this could be pretty much",
    "start": "491340",
    "end": "493319"
  },
  {
    "text": "any standard workflow right now I'm",
    "start": "493319",
    "end": "495000"
  },
  {
    "text": "using llama 2B what I'm going to show",
    "start": "495000",
    "end": "496919"
  },
  {
    "text": "you first is actually how can you run",
    "start": "496919",
    "end": "499080"
  },
  {
    "text": "this on a ray cluster with tpus",
    "start": "499080",
    "end": "503280"
  },
  {
    "text": "so step one of that is actually let's",
    "start": "503280",
    "end": "505979"
  },
  {
    "text": "just connect to Ray I know a lot of you",
    "start": "505979",
    "end": "508020"
  },
  {
    "text": "are familiar with Ray this is pretty",
    "start": "508020",
    "end": "509340"
  },
  {
    "text": "simple all I'm going to do is Mark Ray",
    "start": "509340",
    "end": "511199"
  },
  {
    "text": "dot remote",
    "start": "511199",
    "end": "512700"
  },
  {
    "text": "so if I were to execute this for",
    "start": "512700",
    "end": "514860"
  },
  {
    "text": "instance",
    "start": "514860",
    "end": "516060"
  },
  {
    "text": "in it and run HF dot remote",
    "start": "516060",
    "end": "518580"
  },
  {
    "text": "then this will execute all my rate",
    "start": "518580",
    "end": "520440"
  },
  {
    "text": "cluster",
    "start": "520440",
    "end": "521880"
  },
  {
    "text": "but I don't want this to run on any",
    "start": "521880",
    "end": "523560"
  },
  {
    "text": "resources specifically I want this to",
    "start": "523560",
    "end": "525959"
  },
  {
    "text": "run on array worker node that has tpus",
    "start": "525959",
    "end": "527940"
  },
  {
    "text": "attached to it so I can easily Mark a",
    "start": "527940",
    "end": "530820"
  },
  {
    "text": "resource",
    "start": "530820",
    "end": "531899"
  },
  {
    "text": "tpu4",
    "start": "531899",
    "end": "534120"
  },
  {
    "text": "and now if I were to invoke this",
    "start": "534120",
    "end": "535560"
  },
  {
    "text": "function this will run on the ray",
    "start": "535560",
    "end": "538080"
  },
  {
    "text": "cluster with array worker with four tpus",
    "start": "538080",
    "end": "540180"
  },
  {
    "text": "attached because tpus are a native",
    "start": "540180",
    "end": "542580"
  },
  {
    "text": "resource in Rey as of 2.7",
    "start": "542580",
    "end": "546839"
  },
  {
    "text": "but that's not all I also want to make",
    "start": "546839",
    "end": "548820"
  },
  {
    "text": "sure that the function that's executing",
    "start": "548820",
    "end": "550500"
  },
  {
    "text": "can also take advantage of the",
    "start": "550500",
    "end": "552480"
  },
  {
    "text": "underlying hardware and we do that",
    "start": "552480",
    "end": "554220"
  },
  {
    "text": "through the ml framework",
    "start": "554220",
    "end": "556680"
  },
  {
    "text": "so what I'm going to do next is I'm",
    "start": "556680",
    "end": "558300"
  },
  {
    "text": "going to actually import torch which is",
    "start": "558300",
    "end": "560220"
  },
  {
    "text": "what hugging face is using under the",
    "start": "560220",
    "end": "561839"
  },
  {
    "text": "hood",
    "start": "561839",
    "end": "562680"
  },
  {
    "text": "and I'm going to import torch xla and",
    "start": "562680",
    "end": "566040"
  },
  {
    "text": "now whenever I execute this then hugging",
    "start": "566040",
    "end": "568019"
  },
  {
    "text": "face will actually take advantage of the",
    "start": "568019",
    "end": "569459"
  },
  {
    "text": "underlying Hardware",
    "start": "569459",
    "end": "570839"
  },
  {
    "text": "so we should be good to go let's go",
    "start": "570839",
    "end": "572640"
  },
  {
    "text": "ahead and execute it",
    "start": "572640",
    "end": "575040"
  },
  {
    "text": "hmm",
    "start": "575040",
    "end": "576779"
  },
  {
    "text": "so what you can see pretty quickly is",
    "start": "576779",
    "end": "578700"
  },
  {
    "text": "that we were able to import pytotec slay",
    "start": "578700",
    "end": "580740"
  },
  {
    "text": "load a 2 billion parameter variant and",
    "start": "580740",
    "end": "583500"
  },
  {
    "text": "we are using libtpu and these are really",
    "start": "583500",
    "end": "586019"
  },
  {
    "text": "key indicators that we are training on",
    "start": "586019",
    "end": "587760"
  },
  {
    "text": "tpus so just like that I showed you how",
    "start": "587760",
    "end": "590940"
  },
  {
    "text": "you can easily take a 2 billion",
    "start": "590940",
    "end": "592380"
  },
  {
    "text": "parameter model and run it on array",
    "start": "592380",
    "end": "593880"
  },
  {
    "text": "cluster from your notebook",
    "start": "593880",
    "end": "596959"
  },
  {
    "text": "but honestly this is a two billion",
    "start": "597360",
    "end": "599700"
  },
  {
    "text": "parameter model I actually want my next",
    "start": "599700",
    "end": "601140"
  },
  {
    "text": "model to be bigger I wanted to use",
    "start": "601140",
    "end": "603180"
  },
  {
    "text": "hundreds if not thousands of tpus",
    "start": "603180",
    "end": "606360"
  },
  {
    "text": "so what I'm actually going to show you",
    "start": "606360",
    "end": "607680"
  },
  {
    "text": "next is how easy it is to take code like",
    "start": "607680",
    "end": "609839"
  },
  {
    "text": "this and then scale it up to a TPU",
    "start": "609839",
    "end": "611760"
  },
  {
    "text": "cluster with much more compute than this",
    "start": "611760",
    "end": "613680"
  },
  {
    "text": "so I actually already have this main",
    "start": "613680",
    "end": "617399"
  },
  {
    "text": "file here it's pretty much the exact",
    "start": "617399",
    "end": "619560"
  },
  {
    "text": "same code that you just saw before with",
    "start": "619560",
    "end": "621480"
  },
  {
    "text": "the bit more logging",
    "start": "621480",
    "end": "623640"
  },
  {
    "text": "and what I'm saying now is I want to use",
    "start": "623640",
    "end": "625620"
  },
  {
    "text": "a 70 billion parameter variant as well",
    "start": "625620",
    "end": "628620"
  },
  {
    "text": "as a few other arguments",
    "start": "628620",
    "end": "630720"
  },
  {
    "text": "so main challenge here is that I",
    "start": "630720",
    "end": "633060"
  },
  {
    "text": "actually want to run this remote",
    "start": "633060",
    "end": "634080"
  },
  {
    "text": "function on all of my radio worker nodes",
    "start": "634080",
    "end": "636420"
  },
  {
    "text": "that have tpus attached to it so",
    "start": "636420",
    "end": "638640"
  },
  {
    "text": "fortunately it's very simple through Ray",
    "start": "638640",
    "end": "641519"
  },
  {
    "text": "so one thing I know is that Ray already",
    "start": "641519",
    "end": "644100"
  },
  {
    "text": "knows about the number of tpus that are",
    "start": "644100",
    "end": "646620"
  },
  {
    "text": "within its cluster",
    "start": "646620",
    "end": "648660"
  },
  {
    "text": "each TPU has four chips so I can easily",
    "start": "648660",
    "end": "651899"
  },
  {
    "text": "calculate the number of hosts",
    "start": "651899",
    "end": "653579"
  },
  {
    "text": "automatically through Ray",
    "start": "653579",
    "end": "656519"
  },
  {
    "text": "and now what I want to do is take this",
    "start": "656519",
    "end": "658320"
  },
  {
    "text": "remote function and run this on every",
    "start": "658320",
    "end": "659700"
  },
  {
    "text": "single worker node so I can just do as",
    "start": "659700",
    "end": "661800"
  },
  {
    "text": "simple as comprehension",
    "start": "661800",
    "end": "664800"
  },
  {
    "text": "like this",
    "start": "664800",
    "end": "667880"
  },
  {
    "text": "and this returns a list of handles so",
    "start": "670200",
    "end": "672720"
  },
  {
    "text": "let's just go ahead and get the",
    "start": "672720",
    "end": "673680"
  },
  {
    "text": "responses from that so",
    "start": "673680",
    "end": "675779"
  },
  {
    "text": "just like that this will actually take",
    "start": "675779",
    "end": "677820"
  },
  {
    "text": "advantage of all the compute in my rate",
    "start": "677820",
    "end": "679440"
  },
  {
    "text": "cluster with hundreds of TPU chips",
    "start": "679440",
    "end": "682200"
  },
  {
    "text": "this will actually take some time for me",
    "start": "682200",
    "end": "683820"
  },
  {
    "text": "to kick this off and load but pretty",
    "start": "683820",
    "end": "685500"
  },
  {
    "text": "much what I would say is something like",
    "start": "685500",
    "end": "687120"
  },
  {
    "text": "you know raid job run python main I've",
    "start": "687120",
    "end": "691140"
  },
  {
    "text": "already kicked this off ahead of time",
    "start": "691140",
    "end": "692279"
  },
  {
    "text": "let me quickly just show you",
    "start": "692279",
    "end": "695360"
  },
  {
    "text": "so you can see this job dashboard you",
    "start": "695700",
    "end": "698040"
  },
  {
    "text": "can see the same logs imported python",
    "start": "698040",
    "end": "699779"
  },
  {
    "text": "6la using libtp and if I were to scroll",
    "start": "699779",
    "end": "701820"
  },
  {
    "text": "down",
    "start": "701820",
    "end": "702959"
  },
  {
    "text": "pretty easily you're able to see that",
    "start": "702959",
    "end": "704459"
  },
  {
    "text": "this is running on tpus and just like",
    "start": "704459",
    "end": "707100"
  },
  {
    "text": "that I showed you how you can take the",
    "start": "707100",
    "end": "709320"
  },
  {
    "text": "code that you were running in a notebook",
    "start": "709320",
    "end": "711240"
  },
  {
    "text": "to train on a production level cluster",
    "start": "711240",
    "end": "713880"
  },
  {
    "text": "with hundreds of TPU chips and this is",
    "start": "713880",
    "end": "716040"
  },
  {
    "text": "all thanks to race Simplicity and its",
    "start": "716040",
    "end": "718320"
  },
  {
    "text": "orchestration capabilities under the",
    "start": "718320",
    "end": "719640"
  },
  {
    "text": "hood",
    "start": "719640",
    "end": "721680"
  },
  {
    "text": "so I'd actually love to you know just",
    "start": "721680",
    "end": "724740"
  },
  {
    "text": "wait here with you and watch this",
    "start": "724740",
    "end": "726420"
  },
  {
    "text": "pre-train for several weeks but we have",
    "start": "726420",
    "end": "728519"
  },
  {
    "text": "an exciting race Summit ahead of us so",
    "start": "728519",
    "end": "730140"
  },
  {
    "text": "let's just fast forward to whenever the",
    "start": "730140",
    "end": "732360"
  },
  {
    "text": "model has actually been trained",
    "start": "732360",
    "end": "734220"
  },
  {
    "text": "so",
    "start": "734220",
    "end": "736079"
  },
  {
    "text": "serving large models is actually pretty",
    "start": "736079",
    "end": "738300"
  },
  {
    "text": "unique",
    "start": "738300",
    "end": "739800"
  },
  {
    "text": "you typically need a large amount of",
    "start": "739800",
    "end": "741600"
  },
  {
    "text": "compute to even run these things",
    "start": "741600",
    "end": "743100"
  },
  {
    "text": "especially you have a if you have a",
    "start": "743100",
    "end": "744899"
  },
  {
    "text": "latency concern which you probably do",
    "start": "744899",
    "end": "748440"
  },
  {
    "text": "if you are looking for an accelerator",
    "start": "748440",
    "end": "750060"
  },
  {
    "text": "option that is both performant and price",
    "start": "750060",
    "end": "752220"
  },
  {
    "text": "efficient TPU v5b is a great option for",
    "start": "752220",
    "end": "754680"
  },
  {
    "text": "this",
    "start": "754680",
    "end": "755880"
  },
  {
    "text": "but like I mentioned serving large",
    "start": "755880",
    "end": "757440"
  },
  {
    "text": "language models is pretty unique so",
    "start": "757440",
    "end": "759600"
  },
  {
    "text": "question is like how do I even do it",
    "start": "759600",
    "end": "761399"
  },
  {
    "text": "it's not so simple",
    "start": "761399",
    "end": "763139"
  },
  {
    "text": "or is it let me go ahead and show you",
    "start": "763139",
    "end": "766820"
  },
  {
    "text": "so I have a bit of starter code here and",
    "start": "766980",
    "end": "769800"
  },
  {
    "text": "essentially as an ml developer the gist",
    "start": "769800",
    "end": "772380"
  },
  {
    "text": "is I have done all my pre-training in",
    "start": "772380",
    "end": "774660"
  },
  {
    "text": "Python and generally what I would like",
    "start": "774660",
    "end": "777000"
  },
  {
    "text": "to do is actually just write an",
    "start": "777000",
    "end": "778260"
  },
  {
    "text": "abstraction like this",
    "start": "778260",
    "end": "779760"
  },
  {
    "text": "I want to write a model server that can",
    "start": "779760",
    "end": "782279"
  },
  {
    "text": "load a checkpoint",
    "start": "782279",
    "end": "783720"
  },
  {
    "text": "once the checkpoint has been loaded I",
    "start": "783720",
    "end": "785639"
  },
  {
    "text": "should be able to send any batch to it",
    "start": "785639",
    "end": "788339"
  },
  {
    "text": "whenever I want to",
    "start": "788339",
    "end": "790440"
  },
  {
    "text": "and you know generally I wrote this in",
    "start": "790440",
    "end": "792839"
  },
  {
    "text": "Python I don't want to have to cross a",
    "start": "792839",
    "end": "794639"
  },
  {
    "text": "programming language barrier switch",
    "start": "794639",
    "end": "796260"
  },
  {
    "text": "Stacks or anything like that just to",
    "start": "796260",
    "end": "798000"
  },
  {
    "text": "serve something like that",
    "start": "798000",
    "end": "799740"
  },
  {
    "text": "luckily racer makes this really easy for",
    "start": "799740",
    "end": "801839"
  },
  {
    "text": "us to do all I have to do is",
    "start": "801839",
    "end": "804480"
  },
  {
    "text": "import Ray serve",
    "start": "804480",
    "end": "808040"
  },
  {
    "text": "and then Mark this as a deployment",
    "start": "808139",
    "end": "811579"
  },
  {
    "text": "and again",
    "start": "811740",
    "end": "813000"
  },
  {
    "text": "I actually want to make sure that this",
    "start": "813000",
    "end": "814500"
  },
  {
    "text": "runs on the TPU Hardware under the hood",
    "start": "814500",
    "end": "816060"
  },
  {
    "text": "so I'm going to say react or options and",
    "start": "816060",
    "end": "819360"
  },
  {
    "text": "take advantage of the TPU",
    "start": "819360",
    "end": "821880"
  },
  {
    "text": "and it's actually just as easy as that",
    "start": "821880",
    "end": "823440"
  },
  {
    "text": "this is a deployment that would work",
    "start": "823440",
    "end": "826920"
  },
  {
    "text": "but honestly",
    "start": "826920",
    "end": "829139"
  },
  {
    "text": "Rey has some other great things it",
    "start": "829139",
    "end": "831360"
  },
  {
    "text": "already has native integration with",
    "start": "831360",
    "end": "832620"
  },
  {
    "text": "radio so if I were to just copy and",
    "start": "832620",
    "end": "834720"
  },
  {
    "text": "paste this code into my deployment then",
    "start": "834720",
    "end": "837300"
  },
  {
    "text": "I can easily just say something like",
    "start": "837300",
    "end": "838920"
  },
  {
    "text": "serve run",
    "start": "838920",
    "end": "840260"
  },
  {
    "text": "and then you know gradio but again this",
    "start": "840260",
    "end": "843660"
  },
  {
    "text": "can take some time to set up so let's",
    "start": "843660",
    "end": "845639"
  },
  {
    "text": "just fast forward",
    "start": "845639",
    "end": "847740"
  },
  {
    "text": "I already have this set up here as a",
    "start": "847740",
    "end": "849839"
  },
  {
    "text": "radio server and you know let's actually",
    "start": "849839",
    "end": "852899"
  },
  {
    "text": "just send in a prompt this is a text",
    "start": "852899",
    "end": "854160"
  },
  {
    "text": "completion prompt let's just ask it a",
    "start": "854160",
    "end": "856380"
  },
  {
    "text": "very easy question so",
    "start": "856380",
    "end": "858600"
  },
  {
    "text": "I believe the meaning of life is",
    "start": "858600",
    "end": "862759"
  },
  {
    "text": "to be happy what a great answer",
    "start": "863220",
    "end": "866040"
  },
  {
    "text": "and just like that I showed you how",
    "start": "866040",
    "end": "867779"
  },
  {
    "text": "you're able to easily take your",
    "start": "867779",
    "end": "869160"
  },
  {
    "text": "pre-trained model using Rey on tpus to",
    "start": "869160",
    "end": "872160"
  },
  {
    "text": "serve it using the exact same stack that",
    "start": "872160",
    "end": "874019"
  },
  {
    "text": "you use to pre-train it",
    "start": "874019",
    "end": "876420"
  },
  {
    "text": "and overall in conclusion what I've",
    "start": "876420",
    "end": "878220"
  },
  {
    "text": "shown you today is that you're able to",
    "start": "878220",
    "end": "881839"
  },
  {
    "text": "start with any arbitrary code Pi torture",
    "start": "882180",
    "end": "885180"
  },
  {
    "text": "Jacks test training on a single TPU",
    "start": "885180",
    "end": "887660"
  },
  {
    "text": "easily scale it out to hundreds if not",
    "start": "887660",
    "end": "889980"
  },
  {
    "text": "thousands of TPU chips",
    "start": "889980",
    "end": "892019"
  },
  {
    "text": "serve it on a single TPU and then scale",
    "start": "892019",
    "end": "894180"
  },
  {
    "text": "your model in production all with tpus",
    "start": "894180",
    "end": "898019"
  },
  {
    "text": "there's a lot of other features in Rey",
    "start": "898019",
    "end": "899579"
  },
  {
    "text": "that I didn't get a chance to show you",
    "start": "899579",
    "end": "900779"
  },
  {
    "text": "today but we get these all for free",
    "start": "900779",
    "end": "903180"
  },
  {
    "text": "because Rey is heterogeneous it's",
    "start": "903180",
    "end": "905399"
  },
  {
    "text": "hardware and software agnostic",
    "start": "905399",
    "end": "907260"
  },
  {
    "text": "and overall I think",
    "start": "907260",
    "end": "910199"
  },
  {
    "text": "we are at a point where we can use we",
    "start": "910199",
    "end": "913139"
  },
  {
    "text": "have a software stack that allows us to",
    "start": "913139",
    "end": "914639"
  },
  {
    "text": "have performant modern and robust Stacks",
    "start": "914639",
    "end": "917459"
  },
  {
    "text": "all powered with rayon tpus",
    "start": "917459",
    "end": "920279"
  },
  {
    "text": "thank you for your time",
    "start": "920279",
    "end": "923180"
  }
]