[
  {
    "text": "hello and welcome to ascal connect today we're lucky to be joined by arctic kokarni who's a software engineer at any",
    "start": "560",
    "end": "6240"
  },
  {
    "text": "scale architect joined any scale in 2020 after completing his phd mathematics at uc",
    "start": "6240",
    "end": "11280"
  },
  {
    "text": "berkeley among his other duties at any scale he works in race serv which is an open source library for serving machine",
    "start": "11280",
    "end": "16720"
  },
  {
    "text": "learning models at scale in his talk he'll demonstrate some new features of ray serve including integration with the open source machine",
    "start": "16720",
    "end": "22960"
  },
  {
    "text": "learning lifecycle management platform mlflow as well as an easy way to use raceserve to scale up the existing python web",
    "start": "22960",
    "end": "28320"
  },
  {
    "text": "server with that here's arctic kucarani hi my name is archie calcarni i'm a software engineer at any scale and",
    "start": "28320",
    "end": "35440"
  },
  {
    "text": "today i'll be telling you how to seamlessly scale your machine learning pipelines using base server",
    "start": "35440",
    "end": "45760"
  },
  {
    "text": "so first let's get started with an overview of racer and to put it in context here's a",
    "start": "45760",
    "end": "51920"
  },
  {
    "text": "summary of the machine learning lifecycle so you've got the first phase roughly",
    "start": "51920",
    "end": "58000"
  },
  {
    "text": "which is model development and then you go into training you're trying to get the best parameters for",
    "start": "58000",
    "end": "63680"
  },
  {
    "text": "your model to make it the most accurate and then finally you have the third stage which is inference where you're",
    "start": "63680",
    "end": "69200"
  },
  {
    "text": "actually trying to make prediction volume models and that's where the end user application comes in the end user is",
    "start": "69200",
    "end": "74880"
  },
  {
    "text": "going to use your model to manipulation",
    "start": "74880",
    "end": "78880"
  },
  {
    "text": "so in this talk we'll be talking about we'll be focusing on the last part which is just inference",
    "start": "80000",
    "end": "87840"
  },
  {
    "text": "so race served in a nutshell is a web framework built for machine learning model serving",
    "start": "89119",
    "end": "96479"
  },
  {
    "text": "it's a python framework and here's how something like this would look in python you'll have your machine learning model",
    "start": "97439",
    "end": "104320"
  },
  {
    "text": "and you can wrap it in class so in the constructor you'll load your machine learning model",
    "start": "104320",
    "end": "111840"
  },
  {
    "text": "and this might seem time maybe you have to download a lot of data and then when it's called you'll pass in",
    "start": "111840",
    "end": "119840"
  },
  {
    "text": "a web request object and maybe that has some user input some other data",
    "start": "119840",
    "end": "126399"
  },
  {
    "text": "data comes in you'll probably do some pre-processing on it because the model is going to want input of a certain form maybe a tensor or a",
    "start": "126399",
    "end": "133520"
  },
  {
    "text": "list of numbers or something like that then the model you'll call predict on",
    "start": "133520",
    "end": "139120"
  },
  {
    "text": "the model and then finally you'll transform the output using some post-processing into a form that's more user-friendly",
    "start": "139120",
    "end": "147200"
  },
  {
    "text": "so let me give an outline of the rest of the talk i'll continue with my overview of race serve and how it fits into the",
    "start": "149520",
    "end": "155200"
  },
  {
    "text": "machine learning life cycle and then i'll talk about two integrations of racer with other popular",
    "start": "155200",
    "end": "161040"
  },
  {
    "text": "tools ml flow and fast api",
    "start": "161040",
    "end": "165920"
  },
  {
    "text": "so first let's talk about some of the requirements of model inference so what is inference here's an example",
    "start": "167920",
    "end": "174560"
  },
  {
    "text": "you have an image and you want to know what the image is about so maybe some machine learning model can tell you",
    "start": "174560",
    "end": "180959"
  },
  {
    "text": "it's a cap and so some requirements that you might want and we'll go into more detail on",
    "start": "180959",
    "end": "186000"
  },
  {
    "text": "each of these are that your model inference solution is framework agnostic and it supports arbitrary business logic",
    "start": "186000",
    "end": "193360"
  },
  {
    "text": "and that it's scalable so let's go into the first requirement",
    "start": "193360",
    "end": "198959"
  },
  {
    "text": "in more detail so you want your model serving your uh your model serving solution to be able to work seamlessly with multiple",
    "start": "198959",
    "end": "205360"
  },
  {
    "text": "frameworks there's a lot of different machine learning frameworks out there and they each have their pros and cons and",
    "start": "205360",
    "end": "212000"
  },
  {
    "text": "specialties different architectures different algorithms but when you're solving a",
    "start": "212000",
    "end": "217200"
  },
  {
    "text": "real world problem you want to take the best solution that's out there and you don't want to have to worry about whether it's",
    "start": "217200",
    "end": "223040"
  },
  {
    "text": "compatible with the framework that you're already using so ideally your model solution solution",
    "start": "223040",
    "end": "228720"
  },
  {
    "text": "should be able to just work with any framework",
    "start": "228720",
    "end": "232959"
  },
  {
    "text": "another requirement is that your model serving solution should be able to support arbitrary business logic and so that",
    "start": "234400",
    "end": "240640"
  },
  {
    "text": "includes this pre-processing and post-processing that we were talking about earlier because a lot of machine learning models",
    "start": "240640",
    "end": "247360"
  },
  {
    "text": "will just accept a numeric tensor as input and will output another tensor list of",
    "start": "247360",
    "end": "252599"
  },
  {
    "text": "numbers and that's not very user-friendly so you certainly want to be able to do pre-processing post-processing but even",
    "start": "252599",
    "end": "260320"
  },
  {
    "text": "more than that in a full practical application you might want to",
    "start": "260320",
    "end": "265918"
  },
  {
    "text": "you might want to authorize input think databases or just you want the freedom to be able to",
    "start": "266720",
    "end": "271840"
  },
  {
    "text": "do any kind of business logic that you want so that's critical for model sorting solution",
    "start": "271840",
    "end": "278240"
  },
  {
    "text": "and finally you want your model serving solution to of course be able to scale up your machine learning models",
    "start": "279600",
    "end": "286160"
  },
  {
    "text": "so that'll increase the speed with which users can make predictions and the number of users that can predict",
    "start": "286160",
    "end": "294080"
  },
  {
    "text": "can use your model so for example if you just had one machine running one replica of a",
    "start": "294080",
    "end": "300240"
  },
  {
    "text": "model and you wanted to make a bunch of predictions on it you don't want the predictions to take forever and pile up because",
    "start": "300240",
    "end": "307120"
  },
  {
    "text": "that's a pretty bad user experience so in theory a simple way to improve",
    "start": "307120",
    "end": "313680"
  },
  {
    "text": "this is to have multiple copies of your models so multiple replicas all serving things in parallel",
    "start": "313680",
    "end": "320800"
  },
  {
    "text": "and while that can be kind of tricky to do actually racer makes it very easy and we'll see an example of this later",
    "start": "320800",
    "end": "327280"
  },
  {
    "text": "on in the top and on top of scaling another requirement",
    "start": "327280",
    "end": "332880"
  },
  {
    "text": "is bashing so you want your solution to support taking in bash requests and",
    "start": "332880",
    "end": "341039"
  },
  {
    "text": "well let me put it this way a lot of machine learning models do very well with accepting batches of",
    "start": "341039",
    "end": "346960"
  },
  {
    "text": "say for requests and they'll be able to make those four predictions",
    "start": "346960",
    "end": "352400"
  },
  {
    "text": "utilizing hardware for currency and things like that and it won't take four times as long as with a single",
    "start": "352400",
    "end": "357680"
  },
  {
    "text": "request there will be some speed up there so for the machine learning setting you want to be able to take advantage of",
    "start": "357680",
    "end": "363440"
  },
  {
    "text": "batching too and finally ideally you want the",
    "start": "363440",
    "end": "369840"
  },
  {
    "text": "flexibility to be able to use your model serving solution in multiple scenarios and here's",
    "start": "369840",
    "end": "375120"
  },
  {
    "text": "just a few so one one thing you might want to do is take your machine learning model and just run a single large job",
    "start": "375120",
    "end": "382240"
  },
  {
    "text": "locally on your laptop or on some local cluster uh just for for your own personal user",
    "start": "382240",
    "end": "388400"
  },
  {
    "text": "for your experiment another use case might be you want to serve a model over http",
    "start": "388400",
    "end": "394479"
  },
  {
    "text": "over the internet to many public users and those are kind of different scenarios ideally you want to be able to support both",
    "start": "394479",
    "end": "402160"
  },
  {
    "text": "and also you might want to use your model and integrate it with other services as well so your other services should be able to",
    "start": "402240",
    "end": "409120"
  },
  {
    "text": "query your model and you don't want to have to make too many modifications to your code",
    "start": "409120",
    "end": "414319"
  },
  {
    "text": "to be able to handle all these different kind of scenarios",
    "start": "414319",
    "end": "418319"
  },
  {
    "text": "so race serves meets all these requirements that we talked about so on the one hand it's a high",
    "start": "420880",
    "end": "425919"
  },
  {
    "text": "performance machine learning model serving my group it's framework agnostic scales easily",
    "start": "425919",
    "end": "432479"
  },
  {
    "text": "and we'll see exactly how easy it is in our demo later and it supports batching",
    "start": "432479",
    "end": "439039"
  },
  {
    "text": "and on top of that it's flexible so you can query your model from http and also from python using",
    "start": "439120",
    "end": "448080"
  },
  {
    "text": "using the same api and this lets you easily integrate racer with other tools",
    "start": "448080",
    "end": "455599"
  },
  {
    "text": "and what makes this possible is that ray serve is built on a really useful distributed",
    "start": "458000",
    "end": "463520"
  },
  {
    "text": "computational engine called red and that means that as a user even",
    "start": "463520",
    "end": "468560"
  },
  {
    "text": "though you're writing a distributed application you have all these replicas on all these different cores all these different machines",
    "start": "468560",
    "end": "474800"
  },
  {
    "text": "you don't have to think about all the things that make distributed computing hard for example enterprise communication",
    "start": "474800",
    "end": "481280"
  },
  {
    "text": "failure management fault tolerance job scheduling all these things are taken care of by array",
    "start": "481280",
    "end": "487759"
  },
  {
    "text": "and so that makes it a really nice experience for you the user to just write your code",
    "start": "487759",
    "end": "493120"
  },
  {
    "text": "and then have ray handle all the distributed uh computing issues",
    "start": "493120",
    "end": "499520"
  },
  {
    "text": "you just tell racer to scale up the model and it's as simple as that",
    "start": "499520",
    "end": "506720"
  },
  {
    "text": "so here's what the api looks like and it's pretty similar to just ordinary",
    "start": "506720",
    "end": "512080"
  },
  {
    "text": "functions and classes in python so you'll be able to serve any function and or any staple class that you want",
    "start": "512080",
    "end": "519120"
  },
  {
    "text": "and racer will use multiple replicas to make copies of your model and",
    "start": "519120",
    "end": "525680"
  },
  {
    "text": "it'll be able to accept parallel requests and that'll increase your throughput across cores on a single machine or",
    "start": "525680",
    "end": "531920"
  },
  {
    "text": "across multiple machines on a cluster",
    "start": "531920",
    "end": "536160"
  },
  {
    "text": "and how you would actually query your model is you could do it in two ways you can query it from http",
    "start": "537760",
    "end": "543920"
  },
  {
    "text": "so using a route like this and you can customize the route or you can query it from python using",
    "start": "543920",
    "end": "550480"
  },
  {
    "text": "what we call a server and that's just an ordinary python handle and we'll call it",
    "start": "550480",
    "end": "557040"
  },
  {
    "text": "like this and one of the benefits of a python",
    "start": "557040",
    "end": "562640"
  },
  {
    "text": "server handle being able to just call your models from python is so that you can compose models in",
    "start": "562640",
    "end": "568160"
  },
  {
    "text": "arbitrary ways so oftentimes you're not just using one model in isolation",
    "start": "568160",
    "end": "573760"
  },
  {
    "text": "you might want to have a chain of models each one doing a different thing transforming the input in some way and",
    "start": "573760",
    "end": "579200"
  },
  {
    "text": "then feeding it into another model that's represented by this chain here",
    "start": "579200",
    "end": "585600"
  },
  {
    "text": "or you might have something that looks like this maybe you're doing a b testing you're sending some traffic from one model to another",
    "start": "585600",
    "end": "592240"
  },
  {
    "text": "model and some to the second model and you can think of even more",
    "start": "592240",
    "end": "598080"
  },
  {
    "text": "complicated or convoluted ways of combining models but the point is there's no limitations",
    "start": "598080",
    "end": "604000"
  },
  {
    "text": "on that anything that you've been code in python uh you can do using racer because you'll just be able to query",
    "start": "604000",
    "end": "609920"
  },
  {
    "text": "your models using a normal python handle and this also makes it easy to plug",
    "start": "609920",
    "end": "616399"
  },
  {
    "text": "racer into other python projects such as in this talk we'll talk about fast api and ml flow",
    "start": "616399",
    "end": "623839"
  },
  {
    "text": "but really it's a great amount of flexibility",
    "start": "623839",
    "end": "627839"
  },
  {
    "text": "so let's start talking about reverse integration which is a new integration between racer",
    "start": "629200",
    "end": "634560"
  },
  {
    "text": "and ml flow so going back to the whole machine learning life cycle that we talked about",
    "start": "634560",
    "end": "641200"
  },
  {
    "text": "at the beginning of the talk there's a lot that goes into it and it's often difficult to",
    "start": "641200",
    "end": "647760"
  },
  {
    "text": "manage all the complexity there so for example keeping track of experiments your data is changing your code is",
    "start": "647760",
    "end": "654240"
  },
  {
    "text": "changing they might live on your laptop or live somewhere else it's difficult to",
    "start": "654240",
    "end": "659519"
  },
  {
    "text": "keep track of dependencies you know if you want something reproducible you want to send a project",
    "start": "659519",
    "end": "664959"
  },
  {
    "text": "to a colleague it would be nice if you could package everything together and just have them be able to run it",
    "start": "664959",
    "end": "672079"
  },
  {
    "text": "so ml flow is a tool for um just simplifying the machine learning life cycle and keeping track of all",
    "start": "672079",
    "end": "679040"
  },
  {
    "text": "these things so there's a few different aspects to upload there's the tracking so you can",
    "start": "679040",
    "end": "685680"
  },
  {
    "text": "train your models and keep track of the runs and keep track of the parameters and configurations",
    "start": "685680",
    "end": "691519"
  },
  {
    "text": "all automatically with automatic logging you can package them in reproducible formats with these ml",
    "start": "691519",
    "end": "697680"
  },
  {
    "text": "projects store all your models together in a model registry and racer",
    "start": "697680",
    "end": "705279"
  },
  {
    "text": "will come in at the last part which is when you're actually serving models so let's take a closer",
    "start": "705279",
    "end": "710959"
  },
  {
    "text": "look at what that looks like so to get started uh serving your",
    "start": "710959",
    "end": "717360"
  },
  {
    "text": "machine learn your ml flow models with racer so this is assuming you know you've been using ml flow",
    "start": "717360",
    "end": "722560"
  },
  {
    "text": "you've trained your models and now you're ready to deploy them in production so they're already sitting in an mflow model registry or something",
    "start": "722560",
    "end": "729440"
  },
  {
    "text": "like that so you'll install it as a python package",
    "start": "729440",
    "end": "735200"
  },
  {
    "text": "i'm outflow eraser it's just a deployment plugin call race start dash dash head that",
    "start": "735200",
    "end": "742480"
  },
  {
    "text": "starts array cluster on your laptop and you can also start a cluster you can start a cluster on multiple machines as",
    "start": "742480",
    "end": "748480"
  },
  {
    "text": "well and then once your array cluster is up called the command server start that",
    "start": "748480",
    "end": "753920"
  },
  {
    "text": "starts a racer instance and once that's set up now you can call",
    "start": "753920",
    "end": "758959"
  },
  {
    "text": "this razer ammo api so first i'll talk about the command line interface there's also a python api they have",
    "start": "758959",
    "end": "764800"
  },
  {
    "text": "the same features but let's say you have your ml flow model and you just",
    "start": "764800",
    "end": "770800"
  },
  {
    "text": "want to scale it up and you want to use racer to do it so that's just one command",
    "start": "770800",
    "end": "778560"
  },
  {
    "text": "i'm also deployments that's the api that we're using we're going to create a deployment so",
    "start": "778880",
    "end": "784839"
  },
  {
    "text": "create we're going to serve one model dash t eraser that's the deployment",
    "start": "784839",
    "end": "791040"
  },
  {
    "text": "target and then in here we put the model uri",
    "start": "791040",
    "end": "796079"
  },
  {
    "text": "which is a reference to the specific model that you want to use and give it a name and then you pass in",
    "start": "796079",
    "end": "801920"
  },
  {
    "text": "this configuration parameter various configuration parameters will be used for acer like the maximum number of",
    "start": "801920",
    "end": "807920"
  },
  {
    "text": "requests you want to use in your batch or whatnot here we'll just use it's num replicas equals 100 so that just means i",
    "start": "807920",
    "end": "814240"
  },
  {
    "text": "want to serve 100 replicas in my model on parallel",
    "start": "814240",
    "end": "819839"
  },
  {
    "text": "and this model uri this is where you actually pull the model from ml flow and ammo flow provides",
    "start": "820079",
    "end": "826639"
  },
  {
    "text": "a variety of ways of storing your model so you might have it in a model registry in which case the uri looks like something like this or you might want to",
    "start": "826639",
    "end": "834320"
  },
  {
    "text": "access a particularly a particular run if you were doing some experiment that has its own id or you can also pull",
    "start": "834320",
    "end": "842079"
  },
  {
    "text": "your model from some cloud storage um and then there's even more ways so no",
    "start": "842079",
    "end": "847199"
  },
  {
    "text": "matter where your model is saved you'll be able to just plug it into eraser using this identifier",
    "start": "847199",
    "end": "854160"
  },
  {
    "text": "that's the command line and then on the python side you have the same same features",
    "start": "854399",
    "end": "862320"
  },
  {
    "text": "this is the this is what command would look like and you'll just pass them in as parameters here",
    "start": "862320",
    "end": "867760"
  },
  {
    "text": "and the rest of the api on the command line side and on the python side and create models delete them update",
    "start": "869040",
    "end": "875440"
  },
  {
    "text": "them everything that you would want to do to manage your deployment",
    "start": "875440",
    "end": "884480"
  },
  {
    "text": "so writing this plugin was quite simple um that's because race serve is so flexible",
    "start": "884480",
    "end": "891760"
  },
  {
    "text": "the race of endpoints can be called from python and so integrating with ml flow another python",
    "start": "892639",
    "end": "898720"
  },
  {
    "text": "project was it's quite simple because you can just call the eraser",
    "start": "898720",
    "end": "904480"
  },
  {
    "text": "event directly from the ml flow plugin",
    "start": "904480",
    "end": "909839"
  },
  {
    "text": "and it gives you a clean conceptual separation uh between two things so racer",
    "start": "909839",
    "end": "915279"
  },
  {
    "text": "just handles the data plane the practicing and mlflow is in charge of the control",
    "start": "915279",
    "end": "921600"
  },
  {
    "text": "point so the metadata configuring your models so really",
    "start": "921600",
    "end": "926639"
  },
  {
    "text": "separating the computation from everything else means that reserve is easy to plug into anything else",
    "start": "926639",
    "end": "934320"
  },
  {
    "text": "and while we're on the subject of ammo flow i'll mention another integration this time involving",
    "start": "934639",
    "end": "940399"
  },
  {
    "text": "ray tune ray tune is a library built on top of grade that is for hyper parameter 2",
    "start": "940399",
    "end": "946880"
  },
  {
    "text": "so this is the training phase of the of the machine learning life cycle so",
    "start": "946880",
    "end": "952800"
  },
  {
    "text": "you can do state-of-the-art algorithms for hyperparameter sweep across multiple nodes in your cluster",
    "start": "952800",
    "end": "959120"
  },
  {
    "text": "and it's great because it'll automatically log all these runs to mflow and mlflow has a nice ui which you can",
    "start": "959120",
    "end": "965920"
  },
  {
    "text": "use to inspect all of these and it's just a great way to keep track of everything",
    "start": "965920",
    "end": "972000"
  },
  {
    "text": "so now let's move on to the second integration i'll talk about which is how would you use race serve in",
    "start": "973519",
    "end": "980320"
  },
  {
    "text": "conjunction with a python web framework such as fast api",
    "start": "980320",
    "end": "987439"
  },
  {
    "text": "so first a little bit about fast api the modern high performance web frame for building apis",
    "start": "987519",
    "end": "993440"
  },
  {
    "text": "using standard python type-ins super popular very easy to use",
    "start": "993440",
    "end": "998959"
  },
  {
    "text": "and some of the benefits include automatic input validation so you can just focus on writing",
    "start": "998959",
    "end": "1004000"
  },
  {
    "text": "your business logic you don't have to worry about sanitizing the input or anything like that",
    "start": "1004000",
    "end": "1010399"
  },
  {
    "text": "generates automatic documentation and it's just really easy to use an easy to write bug free code",
    "start": "1010399",
    "end": "1016000"
  },
  {
    "text": "using faster api so what's one problem you might have if you're trying to serve machine learning",
    "start": "1016000",
    "end": "1021120"
  },
  {
    "text": "models so maybe you have an existing python web server so fast api or aiohtp or any python web",
    "start": "1021120",
    "end": "1028959"
  },
  {
    "text": "server and you're trying to serve some really computationally intensive machine learning model",
    "start": "1028959",
    "end": "1034959"
  },
  {
    "text": "and it's just too slow you're getting too many requests they're piling up and it's a bad user experience",
    "start": "1034959",
    "end": "1042159"
  },
  {
    "text": "so how can you scale this up without starting from scratch or without finding a new solution",
    "start": "1042640",
    "end": "1050319"
  },
  {
    "text": "well the solution is to just keep the existing web server and offload the computation to racer",
    "start": "1050480",
    "end": "1057360"
  },
  {
    "text": "so because of the clean separation we were talking about earlier this is pretty straightforward to do",
    "start": "1057360",
    "end": "1064720"
  },
  {
    "text": "and so let's jump right into a demo",
    "start": "1065760",
    "end": "1069679"
  },
  {
    "text": "so here's a really simple fast api application",
    "start": "1073360",
    "end": "1078720"
  },
  {
    "text": "and i'll walk you through it so in the first line we've imported fast api package",
    "start": "1078720",
    "end": "1084799"
  },
  {
    "text": "this is the fast api app and now what we're actually going to serve",
    "start": "1085200",
    "end": "1090320"
  },
  {
    "text": "is a pre-trained model so you know in real life you probably spent a lot of time",
    "start": "1090320",
    "end": "1095760"
  },
  {
    "text": "tweaking and training the model here we're just going to use something off the shelf so this is something from hugging face",
    "start": "1095760",
    "end": "1102240"
  },
  {
    "text": "transforms and it's going to be a text generation model and behind it it's",
    "start": "1102240",
    "end": "1107760"
  },
  {
    "text": "using openai's gpt2 text generation model",
    "start": "1107760",
    "end": "1114000"
  },
  {
    "text": "and what it does is you'll input a few words maybe the start of a sentence and then",
    "start": "1114000",
    "end": "1119520"
  },
  {
    "text": "the model will automatically continue it into coherent english and it's really",
    "start": "1119520",
    "end": "1125440"
  },
  {
    "text": "amazing we'll be able to see it in action but also this app is pretty simple as it is all",
    "start": "1125440",
    "end": "1131600"
  },
  {
    "text": "you need is one generate endpoint and we've loaded our model here and then",
    "start": "1131600",
    "end": "1136880"
  },
  {
    "text": "the generate endpoint will handle get requests to our to this url and based on the input",
    "start": "1136880",
    "end": "1143679"
  },
  {
    "text": "query we'll we'll set it to output safety words so i've already started this running the",
    "start": "1143679",
    "end": "1150320"
  },
  {
    "text": "server so let's just go ahead and run this command so right now i've put",
    "start": "1150320",
    "end": "1156960"
  },
  {
    "text": "in my query as machine learning is and let's see how it continues that initial",
    "start": "1156960",
    "end": "1165840"
  },
  {
    "text": "initial part okay machine learning is one of the most okay it's all true great and we can keep doing it and it'll auto generate",
    "start": "1166880",
    "end": "1174000"
  },
  {
    "text": "using the machine level model and so one downside to this as it is",
    "start": "1174000",
    "end": "1182240"
  },
  {
    "text": "is it's quite slow so i have some benchmarking code here which",
    "start": "1182240",
    "end": "1187520"
  },
  {
    "text": "is going to generate a bunch of requests to this and",
    "start": "1187520",
    "end": "1194240"
  },
  {
    "text": "yeah let's take a look at this",
    "start": "1197120",
    "end": "1200320"
  },
  {
    "text": "okay so it's taking a few seconds per quarter",
    "start": "1203360",
    "end": "1207440"
  },
  {
    "text": "so let me just cut this off before it gets to 100 so now let's see how we can scale this",
    "start": "1209200",
    "end": "1215600"
  },
  {
    "text": "up and really speed it up using research so let me go back into the fast api",
    "start": "1215600",
    "end": "1221919"
  },
  {
    "text": "app and let's start making some changes so first we'll need to import ray and",
    "start": "1221919",
    "end": "1230400"
  },
  {
    "text": "racer now we need to connect to our array",
    "start": "1232840",
    "end": "1239919"
  },
  {
    "text": "cluster so there's already a ray cluster running so here's a dashboard",
    "start": "1239919",
    "end": "1246799"
  },
  {
    "text": "where you can see our various nodes each node has multiple cores nothing's happening on it",
    "start": "1246799",
    "end": "1253120"
  },
  {
    "text": "right now so on startup we want to connect to this",
    "start": "1253120",
    "end": "1260640"
  },
  {
    "text": "red cluster from our python script so in fast api this is how we handle a",
    "start": "1260640",
    "end": "1268480"
  },
  {
    "text": "startup event so this will just run startup",
    "start": "1268480",
    "end": "1277120"
  },
  {
    "text": "all right inside here we need to call ray.net that connects to a cluster",
    "start": "1277120",
    "end": "1285360"
  },
  {
    "text": "and then we want to start a racer instance",
    "start": "1287520",
    "end": "1292159"
  },
  {
    "text": "so this starts the racer and now we want to define a callable",
    "start": "1293520",
    "end": "1299520"
  },
  {
    "text": "class that contains the model that we want to serve so i'll just call it gpt2",
    "start": "1299520",
    "end": "1306880"
  },
  {
    "text": "and we'll initialize it by just taking this initial isolation code",
    "start": "1309679",
    "end": "1316240"
  },
  {
    "text": "and putting it inside the class so now we have self",
    "start": "1316240",
    "end": "1322159"
  },
  {
    "text": "dot model that's just the model",
    "start": "1322159",
    "end": "1326240"
  },
  {
    "text": "and then when we call this class this call function should take in",
    "start": "1327679",
    "end": "1332720"
  },
  {
    "text": "request which is web request and what we'll return is",
    "start": "1332720",
    "end": "1341679"
  },
  {
    "text": "well okay it's we're going to do the same query that we had before but",
    "start": "1342559",
    "end": "1348400"
  },
  {
    "text": "but it's the member of this class self.nlp model and instead of query it's",
    "start": "1351760",
    "end": "1360000"
  },
  {
    "text": "coming from this web request so that's request.data",
    "start": "1360000",
    "end": "1364960"
  },
  {
    "text": "so that's our class and now we need to create a reserve endpoint for this for",
    "start": "1365919",
    "end": "1371760"
  },
  {
    "text": "this class to look behind so let's call client.createbackend",
    "start": "1371760",
    "end": "1378559"
  },
  {
    "text": "call this gpt2 just a name for this back end pass in the class that we just created",
    "start": "1378880",
    "end": "1386159"
  },
  {
    "text": "and then as configuration parameters let's set the number of replicas to be",
    "start": "1386159",
    "end": "1394000"
  },
  {
    "text": "10.",
    "start": "1394000",
    "end": "1396320"
  },
  {
    "text": "now let's create an endpoint to put this back up behind college and b2b endpoint name doesn't",
    "start": "1399280",
    "end": "1405280"
  },
  {
    "text": "really matter which is one [Music] and it's passing the backend that we",
    "start": "1405280",
    "end": "1411039"
  },
  {
    "text": "just created and now how do we actually call it now",
    "start": "1411039",
    "end": "1417679"
  },
  {
    "text": "i'll use the serve handle api that we talked about earlier",
    "start": "1417679",
    "end": "1422720"
  },
  {
    "text": "so we'll get a handle to this endpoint of course we can query it from http but we already have this fast api web",
    "start": "1423679",
    "end": "1429919"
  },
  {
    "text": "server that's handling all the http stuff we just want to use racer to offload the computation",
    "start": "1429919",
    "end": "1437520"
  },
  {
    "text": "so we'll get a python handle to the gpt encoder that we just created",
    "start": "1437840",
    "end": "1444480"
  },
  {
    "text": "and now we want to serve it inside the fast api endpoint generate we don't need this anymore",
    "start": "1444480",
    "end": "1452158"
  },
  {
    "text": "so we want to this is how you call the certain handle",
    "start": "1454000",
    "end": "1461679"
  },
  {
    "text": "the dot remote is just a reminder that it's a remote function it's happening across many replicas it's",
    "start": "1461679",
    "end": "1468080"
  },
  {
    "text": "asynchronous so you put in the weight here and for this demo that i want let's just make this a",
    "start": "1468080",
    "end": "1473440"
  },
  {
    "text": "global variable so",
    "start": "1473440",
    "end": "1476399"
  },
  {
    "text": "okay save here's my usercoin server let's kill it",
    "start": "1483520",
    "end": "1490000"
  },
  {
    "text": "and restart it with the new main.pi that we have",
    "start": "1490000",
    "end": "1501840"
  },
  {
    "text": "all right so while that's starting up what what did we build uvicorn",
    "start": "1509440",
    "end": "1516400"
  },
  {
    "text": "is running the fast api app even recording is handling the http request http response",
    "start": "1516400",
    "end": "1522159"
  },
  {
    "text": "and then inside the fest api it's using the serv handle api to offload the computation to racer and",
    "start": "1522159",
    "end": "1528880"
  },
  {
    "text": "racer will manage all of the replicas",
    "start": "1528880",
    "end": "1533520"
  },
  {
    "text": "okay it's running if i look at my array dashboard i can see there's one replica sitting on",
    "start": "1535120",
    "end": "1542080"
  },
  {
    "text": "one node more replicas so we're good to go",
    "start": "1542080",
    "end": "1551360"
  },
  {
    "text": "so let me now run the same benchmark that we had earlier since we have eight replicas",
    "start": "1552240",
    "end": "1557760"
  },
  {
    "text": "let's use eight simultaneous connections just so we can really saturate our replicas",
    "start": "1557760",
    "end": "1571840"
  },
  {
    "text": "we need to pass in the query of course so let's start the server again",
    "start": "1574799",
    "end": "1585840"
  },
  {
    "text": "you",
    "start": "1596840",
    "end": "1599840"
  },
  {
    "text": "okay just wait for the startup these warnings just mean there's no gpu installed that's fine",
    "start": "1604880",
    "end": "1610320"
  },
  {
    "text": "all right so now let's go ahead and run a benchmark",
    "start": "1610320",
    "end": "1616080"
  },
  {
    "text": "okay and you can see that it's much faster than it was before right and that's just because we well we set the number of",
    "start": "1620000",
    "end": "1625840"
  },
  {
    "text": "replicas to eight so these requests are being handled in parallel as we keep",
    "start": "1625840",
    "end": "1631200"
  },
  {
    "text": "uh querying so this is great let's stop that um",
    "start": "1631200",
    "end": "1639440"
  },
  {
    "text": "so i did run some tests on this earlier so using a cluster with eight machines and 16 cores each",
    "start": "1639440",
    "end": "1645919"
  },
  {
    "text": "here's the results qps is queries per second so with one replica it took you know like two seconds per",
    "start": "1645919",
    "end": "1652880"
  },
  {
    "text": "query and then if you multiply the number it goes by 10 this throughput also",
    "start": "1652880",
    "end": "1658000"
  },
  {
    "text": "increases by a factor of 10. the next multiple of 10 we don't quite get a 10x improvement",
    "start": "1658000",
    "end": "1663679"
  },
  {
    "text": "there's still only eight machines so the multi-core parallelism isn't as uh",
    "start": "1663679",
    "end": "1670000"
  },
  {
    "text": "isn't as powerful as just having separate machines but it's still quite a significant improvement and of",
    "start": "1670000",
    "end": "1675039"
  },
  {
    "text": "course you can use as many machines as you like buster",
    "start": "1675039",
    "end": "1679360"
  },
  {
    "text": "so to summarize what what did we do we had a fast api app it was serving a",
    "start": "1680720",
    "end": "1686640"
  },
  {
    "text": "machine learning model that was really computational intensive and so quite slow but with just a few",
    "start": "1686640",
    "end": "1692399"
  },
  {
    "text": "lines of code we were able to make it much faster by using racer letting it scale up the model multiple",
    "start": "1692399",
    "end": "1698159"
  },
  {
    "text": "that's it",
    "start": "1698159",
    "end": "1700480"
  },
  {
    "text": "all right so in summary race serve is a high performance machine learning model serving library it has all these",
    "start": "1703919",
    "end": "1710640"
  },
  {
    "text": "great features that it's framework agnostic it's easily scalable supports batching uh takes advantage",
    "start": "1710640",
    "end": "1716640"
  },
  {
    "text": "of python currency by having all these replicas to provide low latency for the user and a bunch of other stuff that we",
    "start": "1716640",
    "end": "1722640"
  },
  {
    "text": "didn't talk about and on top of that it's flexible so we",
    "start": "1722640",
    "end": "1729200"
  },
  {
    "text": "already saw in this talk some simple integrations with fast api and also and racer allows you to serve",
    "start": "1729200",
    "end": "1735679"
  },
  {
    "text": "arbitrary python functions and classes and this also means you can sort of",
    "start": "1735679",
    "end": "1740720"
  },
  {
    "text": "arbitrary ray applications so arbitrary highly distributed applications you can put them behind",
    "start": "1740720",
    "end": "1746559"
  },
  {
    "text": "reserve and that's really powerful",
    "start": "1746559",
    "end": "1751360"
  },
  {
    "text": "so try out research today you can clip install eraser get started with the quick start on our",
    "start": "1751760",
    "end": "1757520"
  },
  {
    "text": "documentation and if you have any questions or you just want to chat about ray check out our forum at discuss.ray.io",
    "start": "1757520",
    "end": "1765360"
  },
  {
    "text": "and please reach out we'd love to hear about your use cases",
    "start": "1765360",
    "end": "1770240"
  },
  {
    "text": "some acknowledgements i'd like to give thanks to the jules damji sid merchant and paul ogilvie from the",
    "start": "1771440",
    "end": "1776640"
  },
  {
    "text": "mlfo team and simon moe and edward oaks for helping out with this presentation",
    "start": "1776640",
    "end": "1782559"
  },
  {
    "text": "and the rest of the ray team thank you very much",
    "start": "1782559",
    "end": "1789440"
  },
  {
    "text": "so with that we'll take questions so thank you so much uh archit uh for your excellent talk on",
    "start": "1790720",
    "end": "1797120"
  },
  {
    "text": "race serve as well as some integrations with it particularly emma flow um it was excellent so",
    "start": "1797120",
    "end": "1804880"
  },
  {
    "text": "let's start with questions thanks michael",
    "start": "1804880",
    "end": "1809840"
  },
  {
    "text": "so um the first question is is it possible to use spark nlp pipeline",
    "start": "1810880",
    "end": "1816320"
  },
  {
    "text": "inside fast api right so i haven't used spark nlp",
    "start": "1816320",
    "end": "1823120"
  },
  {
    "text": "pipeline before but it looks from what i'm seeing online it looks like you can you should be able to",
    "start": "1823120",
    "end": "1828559"
  },
  {
    "text": "just wrap it in a python class similar to what we did in our demo uh and reserve is so flexible that any",
    "start": "1828559",
    "end": "1834080"
  },
  {
    "text": "class where you can load the model like in the init function and then serve it in the call function it should work",
    "start": "1834080",
    "end": "1839919"
  },
  {
    "text": "pretty much the same way so i imagine that you could use it inside fast api and use",
    "start": "1839919",
    "end": "1844960"
  },
  {
    "text": "reserve to scale it up oh wonderful so the next question is",
    "start": "1844960",
    "end": "1850159"
  },
  {
    "text": "since raised serve also gives uh an atp https endpoint basically out of the box",
    "start": "1850159",
    "end": "1856880"
  },
  {
    "text": "is there reason to use fast api at all in the example presented would it just be better to only use race serve",
    "start": "1856880",
    "end": "1863760"
  },
  {
    "text": "uh-huh yeah good question so you're right this example could have been done just with reserve",
    "start": "1863760",
    "end": "1869200"
  },
  {
    "text": "because racer comes with an http uh server out of the box um this example",
    "start": "1869200",
    "end": "1874720"
  },
  {
    "text": "was sort of for the perspective of someone who was already using fast api or was using some advanced features of",
    "start": "1874720",
    "end": "1880799"
  },
  {
    "text": "fast api and they just wanted to increase their throughput and so it was to highlight the way you",
    "start": "1880799",
    "end": "1887840"
  },
  {
    "text": "can separate the computation from the http serving uh",
    "start": "1887840",
    "end": "1893519"
  },
  {
    "text": "and offload the computation eraser but you could totally have done the example of just with reserve",
    "start": "1893519",
    "end": "1898559"
  },
  {
    "text": "okay so in your example uh where is your rate running a local machine a kubernetes cluster",
    "start": "1898559",
    "end": "1905519"
  },
  {
    "text": "or a cloud cluster yeah great question i probably should have mentioned this in the talk so it is a cloud cluster it's actually",
    "start": "1905519",
    "end": "1912080"
  },
  {
    "text": "running on the any scale product so it made it very easy for me to just start up the cluster",
    "start": "1912080",
    "end": "1917760"
  },
  {
    "text": "using array docker image with everything pre-installed um yeah so it's a cloud cluster uh on",
    "start": "1917760",
    "end": "1924480"
  },
  {
    "text": "aws i believe okay um so we don't have a ton of questions so",
    "start": "1924480",
    "end": "1930320"
  },
  {
    "text": "everyone feel free to ask a lot of questions um",
    "start": "1930320",
    "end": "1936480"
  },
  {
    "text": "so um just in general um how recent this is my own question how recent are",
    "start": "1938640",
    "end": "1944559"
  },
  {
    "text": "the ml flow integrations um yeah these i mean we started working on these just in the last",
    "start": "1944559",
    "end": "1950720"
  },
  {
    "text": "uh couple of months um and they're ready to go so we're excited for people to",
    "start": "1950720",
    "end": "1955919"
  },
  {
    "text": "uh to try them out yeah okay um so some other questions are",
    "start": "1955919",
    "end": "1963120"
  },
  {
    "text": "so what's next race serve so you've introduced a lot of new integrations a lot of new features",
    "start": "1963120",
    "end": "1968960"
  },
  {
    "text": "so what's coming next yeah uh that's a great question um so",
    "start": "1968960",
    "end": "1975679"
  },
  {
    "text": "one of the things i'm working on is getting racer to more closely",
    "start": "1975679",
    "end": "1980720"
  },
  {
    "text": "integrate with the any scale product we're also really trying to get uh ray serve",
    "start": "1980720",
    "end": "1989440"
  },
  {
    "text": "um being adopted in production and so we're very eager to hear um",
    "start": "1989440",
    "end": "1995840"
  },
  {
    "text": "from users about their use cases um so we can you know build features",
    "start": "1995840",
    "end": "2001679"
  },
  {
    "text": "very certificated to those needs okay um so the next question is more just like comment but i",
    "start": "2001679",
    "end": "2007200"
  },
  {
    "text": "figured i'd ask it anyway so uh someone said excellent work we're using it internally",
    "start": "2007200",
    "end": "2012960"
  },
  {
    "text": "um so they've integrated with emma flow with rate on their own a few months back uh but now",
    "start": "2012960",
    "end": "2018720"
  },
  {
    "text": "we're using your implementation so your work has you know been used which is wonderful um so is there documentation somewhere",
    "start": "2018720",
    "end": "2025600"
  },
  {
    "text": "for uh these integrations yeah um so first of all yeah harvard that's that's great",
    "start": "2025600",
    "end": "2030720"
  },
  {
    "text": "to hear and if you do get a chance to try out the ml flow razer plug-in you know please let us know your",
    "start": "2030720",
    "end": "2036480"
  },
  {
    "text": "thoughts um yeah it's a very i guess it's the first release so there's definitely",
    "start": "2036480",
    "end": "2042240"
  },
  {
    "text": "improvements we can make um yeah as for the documentation um racerv.org you can also just google",
    "start": "2042240",
    "end": "2050240"
  },
  {
    "text": "mlflow racer the first result should be um the github page for the ammo float racer",
    "start": "2050240",
    "end": "2056398"
  },
  {
    "text": "plugin um and that should work out of the box but yeah let us know your experience with that",
    "start": "2056399",
    "end": "2061839"
  },
  {
    "text": "and we'd be happy to help you out yeah i've also noticed there's a blog post on uh ray and mlflow that was",
    "start": "2061839",
    "end": "2068878"
  },
  {
    "text": "released relatively recently from googling during this presentation so um the other next question is what's",
    "start": "2068879",
    "end": "2075919"
  },
  {
    "text": "the timeline for releasing race serve 2.0 uh-huh relatively speaking here",
    "start": "2075919",
    "end": "2083280"
  },
  {
    "text": "yeah well so ray's continuously improving um and",
    "start": "2083280",
    "end": "2089599"
  },
  {
    "text": "it the release cycle is tied to the rave release cycle so ray 1.2",
    "start": "2089599",
    "end": "2096320"
  },
  {
    "text": "just came out um uh yeah so racer 2.0 you know no",
    "start": "2096320",
    "end": "2103520"
  },
  {
    "text": "specific timeline but we do release new ray versions and versions of all the ray libraries every",
    "start": "2103520",
    "end": "2109040"
  },
  {
    "text": "say one or two months okay we always check the release notes as well um and ask in slack and",
    "start": "2109040",
    "end": "2116320"
  },
  {
    "text": "discuss and such so the next question is from andre on how do you specify the",
    "start": "2116320",
    "end": "2122720"
  },
  {
    "text": "mlflow tracking server url so um is it through",
    "start": "2122720",
    "end": "2128640"
  },
  {
    "text": "or something right yeah so we're this is i mean we're working with the mlflow uh team on this",
    "start": "2128640",
    "end": "2136400"
  },
  {
    "text": "um but yeah so we'll we'll get back to you when when there's a fix for that so",
    "start": "2136400",
    "end": "2141440"
  },
  {
    "text": "maybe um yeah there should be a fix for that very soon",
    "start": "2141440",
    "end": "2146800"
  },
  {
    "text": "but yeah we're aware of that issue okay um sure so i mean someone asked a question",
    "start": "2146800",
    "end": "2152480"
  },
  {
    "text": "can i ask a ray tune related question so would that be okay with you um i probably i mean i'm not a ray tune",
    "start": "2152480",
    "end": "2160000"
  },
  {
    "text": "expert um we i mean we can you can yeah feel free to ask but i'm",
    "start": "2160000",
    "end": "2166880"
  },
  {
    "text": "not i probably won't be able to answer okay um we also probably have",
    "start": "2166880",
    "end": "2172160"
  },
  {
    "text": "other experts here so um if you ask it we'll probably find a way to find a solution",
    "start": "2172160",
    "end": "2177359"
  },
  {
    "text": "so another question um is about what's the major difference",
    "start": "2177359",
    "end": "2183599"
  },
  {
    "text": "between ml serve and other restful services offerings",
    "start": "2183599",
    "end": "2189200"
  },
  {
    "text": "uh-huh um yeah good question so the machine learning models are very",
    "start": "2189200",
    "end": "2195359"
  },
  {
    "text": "computationally intensive um that's i guess one one difference um yeah so sort of some of the",
    "start": "2195359",
    "end": "2203119"
  },
  {
    "text": "requirements that we highlighted in the beginning of the talk you know you want it to be",
    "start": "2203119",
    "end": "2208240"
  },
  {
    "text": "framework agnostic you want um you want it to support batching there's certain",
    "start": "2208240",
    "end": "2214400"
  },
  {
    "text": "things like this that are more specific to machine learning models another thing is your machine",
    "start": "2214400",
    "end": "2220400"
  },
  {
    "text": "learning models are always being updated and so on",
    "start": "2220400",
    "end": "2225520"
  },
  {
    "text": "so the deployment is maybe more dynamic than a non-machine learning service",
    "start": "2225520",
    "end": "2231359"
  },
  {
    "text": "application um so the next question is does ray handle container provisioning",
    "start": "2231359",
    "end": "2237839"
  },
  {
    "text": "or do my containers need to be available in advance",
    "start": "2237839",
    "end": "2243040"
  },
  {
    "text": "so this question i'm not actually sure about the answer to",
    "start": "2243040",
    "end": "2248960"
  },
  {
    "text": "this um maybe i'm sure people would be able to help out on the on the ray forum",
    "start": "2248960",
    "end": "2256960"
  },
  {
    "text": "yeah they're pretty useful um so the next question is",
    "start": "2256960",
    "end": "2263119"
  },
  {
    "text": "are there plans to integrate with managed machine learning services like gcp's ai platform",
    "start": "2263119",
    "end": "2270000"
  },
  {
    "text": "um i don't think there are any plans along those lines at the moment no okay",
    "start": "2270000",
    "end": "2276800"
  },
  {
    "text": "um one thing also just to mention is that if people have questions about you",
    "start": "2276800",
    "end": "2282480"
  },
  {
    "text": "know anything regulated because this is a very big sort of you know project rape",
    "start": "2282480",
    "end": "2287520"
  },
  {
    "text": "project's a massive project there's uh race serve there's ray tune um",
    "start": "2287520",
    "end": "2292560"
  },
  {
    "text": "there's libras reinforcement learning and more um always feel free to ask either here um in the slack channels",
    "start": "2292560",
    "end": "2299359"
  },
  {
    "text": "on under discuss forums because there's a lot of experts in various um different forms you can get your",
    "start": "2299359",
    "end": "2305200"
  },
  {
    "text": "questions asked and answered so um does anyone else have more",
    "start": "2305200",
    "end": "2310839"
  },
  {
    "text": "questions as we continue",
    "start": "2310839",
    "end": "2314800"
  },
  {
    "text": "um one thing i also um oh we have more questions so",
    "start": "2317520",
    "end": "2324000"
  },
  {
    "text": "the next question is can we pass class methods inside uh tune.run arguments",
    "start": "2324000",
    "end": "2331599"
  },
  {
    "text": "yeah i think that might be a question for for discuss.radio the the ray forum",
    "start": "2331599",
    "end": "2336720"
  },
  {
    "text": "or the ray slack um yeah so the one thing i also um",
    "start": "2336720",
    "end": "2343440"
  },
  {
    "text": "like to mention here is that we we have discussed we have slack but you",
    "start": "2343440",
    "end": "2348720"
  },
  {
    "text": "can also uh send us a message um through events at asgl.com",
    "start": "2348720",
    "end": "2354880"
  },
  {
    "text": "and we can answer questions about future events upcoming things we have going on i also like to",
    "start": "2354880",
    "end": "2361520"
  },
  {
    "text": "mention that in a couple weeks we will have race summit so you can submit your talk now and the",
    "start": "2361520",
    "end": "2368000"
  },
  {
    "text": "call for proposals closes on february 24th uh so we'd love to hear your talk we'd",
    "start": "2368000",
    "end": "2373280"
  },
  {
    "text": "love to have you at the event because you can learn so much about ray and the different use cases",
    "start": "2373280",
    "end": "2379280"
  },
  {
    "text": "i'll check to see if you have more questions um and if not then we'll go from there",
    "start": "2379280",
    "end": "2387280"
  },
  {
    "text": "okay um so are you okay with answering another like two or three questions yeah okay wonderful so the next question",
    "start": "2387280",
    "end": "2396000"
  },
  {
    "text": "is is there a best practices guide for maintaining and updating models of production without downtime uh-huh",
    "start": "2396000",
    "end": "2404000"
  },
  {
    "text": "so in the racer documentation there is an example a sample code for",
    "start": "2404000",
    "end": "2410480"
  },
  {
    "text": "um behind one end point switching from one race serve back end to another without downtime using some of these",
    "start": "2410480",
    "end": "2417200"
  },
  {
    "text": "advanced features like uh traffic splitting and so on so you should be able to find something",
    "start": "2417200",
    "end": "2422800"
  },
  {
    "text": "useful there but please let us know if we can provide more help there so",
    "start": "2422800",
    "end": "2428960"
  },
  {
    "text": "the final question and i really appreciate all these questions um is can i integrate my ray serve",
    "start": "2428960",
    "end": "2435520"
  },
  {
    "text": "within a flask app as well like you did with fast api um yeah this the same thing should be",
    "start": "2435520",
    "end": "2443040"
  },
  {
    "text": "possible so it's very general right you just you have your python class and your serve handle uh",
    "start": "2443040",
    "end": "2448240"
  },
  {
    "text": "which you query from python so i don't see any roadblocks there yeah okay and i promise it's the final",
    "start": "2448240",
    "end": "2454079"
  },
  {
    "text": "question because people are uh very interested in uh integrations with",
    "start": "2454079",
    "end": "2459280"
  },
  {
    "text": "raceerv so i could put fast api plus raycode in a",
    "start": "2459280",
    "end": "2464960"
  },
  {
    "text": "container and publish it in azure web app",
    "start": "2464960",
    "end": "2469760"
  },
  {
    "text": "um so i'm not too familiar with azure web app so",
    "start": "2470160",
    "end": "2476720"
  },
  {
    "text": "but i mean i don't see any reason why something like this shouldn't be possible",
    "start": "2476720",
    "end": "2482000"
  },
  {
    "text": "to put put this code in a container and serve it from there yeah race serve is very flexible so um",
    "start": "2482000",
    "end": "2488640"
  },
  {
    "text": "just about anything you want to productionalize racerv can you know help or there will",
    "start": "2488640",
    "end": "2493920"
  },
  {
    "text": "be some way to make it work in some way um so that's it for",
    "start": "2493920",
    "end": "2499200"
  },
  {
    "text": "questions um i want to thank you so much for giving the talk taking time to work on it as well as answer all these",
    "start": "2499200",
    "end": "2505839"
  },
  {
    "text": "questions from a vast vast variety of use cases um so thank you archit thanks yeah thank",
    "start": "2505839",
    "end": "2513280"
  },
  {
    "text": "you yeah thanks everyone for watching",
    "start": "2513280",
    "end": "2520400"
  }
]