[
  {
    "text": "hello everyone",
    "start": "7680",
    "end": "8480"
  },
  {
    "text": "i'm simon mo and today i'll talk about",
    "start": "8480",
    "end": "10400"
  },
  {
    "text": "machine learning model serving",
    "start": "10400",
    "end": "12320"
  },
  {
    "text": "i'll give you an overview of what survey",
    "start": "12320",
    "end": "15040"
  },
  {
    "text": "machine learning model means",
    "start": "15040",
    "end": "16720"
  },
  {
    "text": "what it requires and after this talk i",
    "start": "16720",
    "end": "19199"
  },
  {
    "text": "hope you can take away",
    "start": "19199",
    "end": "20160"
  },
  {
    "text": "some understanding of model serving",
    "start": "20160",
    "end": "22720"
  },
  {
    "text": "landscape",
    "start": "22720",
    "end": "23840"
  },
  {
    "text": "learn common approaches and pitfalls and",
    "start": "23840",
    "end": "26080"
  },
  {
    "text": "learn about new library having",
    "start": "26080",
    "end": "27760"
  },
  {
    "text": "been developing to overcome these common",
    "start": "27760",
    "end": "30560"
  },
  {
    "text": "pain points",
    "start": "30560",
    "end": "32880"
  },
  {
    "text": "so let's start with what is model",
    "start": "32880",
    "end": "35040"
  },
  {
    "text": "serving",
    "start": "35040",
    "end": "36000"
  },
  {
    "text": "and in particular where does it fit into",
    "start": "36000",
    "end": "38960"
  },
  {
    "text": "the machine learning life cycle",
    "start": "38960",
    "end": "41200"
  },
  {
    "text": "to build and deploy machine learning",
    "start": "41200",
    "end": "43200"
  },
  {
    "text": "powered applications",
    "start": "43200",
    "end": "44640"
  },
  {
    "text": "we roughly have these three major steps",
    "start": "44640",
    "end": "47280"
  },
  {
    "text": "you develop the model",
    "start": "47280",
    "end": "48719"
  },
  {
    "text": "train the model and perform inference on",
    "start": "48719",
    "end": "50800"
  },
  {
    "text": "the model",
    "start": "50800",
    "end": "52079"
  },
  {
    "text": "model development is where we collect",
    "start": "52079",
    "end": "54239"
  },
  {
    "text": "the data analyze the data",
    "start": "54239",
    "end": "56239"
  },
  {
    "text": "and iteratively improve the modeling",
    "start": "56239",
    "end": "58719"
  },
  {
    "text": "once we have the model",
    "start": "58719",
    "end": "60160"
  },
  {
    "text": "is pushed into training pipeline that",
    "start": "60160",
    "end": "62320"
  },
  {
    "text": "learns from my production data",
    "start": "62320",
    "end": "64559"
  },
  {
    "text": "what happens then after we have server",
    "start": "64559",
    "end": "66720"
  },
  {
    "text": "trend and validated model",
    "start": "66720",
    "end": "68560"
  },
  {
    "text": "these models need to be put inside a",
    "start": "68560",
    "end": "70400"
  },
  {
    "text": "prediction service",
    "start": "70400",
    "end": "71600"
  },
  {
    "text": "suitable for any user application",
    "start": "71600",
    "end": "75360"
  },
  {
    "text": "and the last stage is what we care about",
    "start": "75360",
    "end": "78080"
  },
  {
    "text": "when",
    "start": "78080",
    "end": "78479"
  },
  {
    "text": "serving machine learning model and",
    "start": "78479",
    "end": "80320"
  },
  {
    "text": "deploying production",
    "start": "80320",
    "end": "82560"
  },
  {
    "text": "so this talk will be structured as",
    "start": "82560",
    "end": "84880"
  },
  {
    "text": "follows",
    "start": "84880",
    "end": "85600"
  },
  {
    "text": "i'll first discuss some requirement for",
    "start": "85600",
    "end": "87520"
  },
  {
    "text": "model serving to get some overview of",
    "start": "87520",
    "end": "89759"
  },
  {
    "text": "what a typical",
    "start": "89759",
    "end": "91360"
  },
  {
    "text": "model serving workload looks like and",
    "start": "91360",
    "end": "93840"
  },
  {
    "text": "then i'll talk about two common",
    "start": "93840",
    "end": "95439"
  },
  {
    "text": "approaches",
    "start": "95439",
    "end": "96159"
  },
  {
    "text": "for model serving these two camera",
    "start": "96159",
    "end": "98159"
  },
  {
    "text": "approaches roughly cover",
    "start": "98159",
    "end": "100000"
  },
  {
    "text": "the current model serving solution out",
    "start": "100000",
    "end": "102240"
  },
  {
    "text": "here",
    "start": "102240",
    "end": "103360"
  },
  {
    "text": "and then lastly i'll introduce reserve a",
    "start": "103360",
    "end": "105840"
  },
  {
    "text": "web framework built on top of",
    "start": "105840",
    "end": "107759"
  },
  {
    "text": "distributed runtime rate and that may",
    "start": "107759",
    "end": "109920"
  },
  {
    "text": "make it easy for serving machine",
    "start": "109920",
    "end": "111600"
  },
  {
    "text": "learning models",
    "start": "111600",
    "end": "112640"
  },
  {
    "text": "and scaling them to many machines while",
    "start": "112640",
    "end": "115119"
  },
  {
    "text": "delivering engineering control over the",
    "start": "115119",
    "end": "116880"
  },
  {
    "text": "production",
    "start": "116880",
    "end": "118000"
  },
  {
    "text": "let's start with the requirements what",
    "start": "118000",
    "end": "119759"
  },
  {
    "text": "are some basic requirements for model",
    "start": "119759",
    "end": "121119"
  },
  {
    "text": "servers",
    "start": "121119",
    "end": "122799"
  },
  {
    "text": "well first of all it needs to deliver",
    "start": "122799",
    "end": "125360"
  },
  {
    "text": "predictions",
    "start": "125360",
    "end": "126640"
  },
  {
    "text": "given an input right there we have a cat",
    "start": "126640",
    "end": "129759"
  },
  {
    "text": "it should run through the machine",
    "start": "129759",
    "end": "131200"
  },
  {
    "text": "learning model and output the prediction",
    "start": "131200",
    "end": "133040"
  },
  {
    "text": "say",
    "start": "133040",
    "end": "133599"
  },
  {
    "text": "it's a cat but in many cases we have",
    "start": "133599",
    "end": "136720"
  },
  {
    "text": "more than just",
    "start": "136720",
    "end": "137440"
  },
  {
    "text": "one type of model there's a need to",
    "start": "137440",
    "end": "140640"
  },
  {
    "text": "multiplex different models",
    "start": "140640",
    "end": "142640"
  },
  {
    "text": "regardless of different versions",
    "start": "142640",
    "end": "145360"
  },
  {
    "text": "different algorithms or different",
    "start": "145360",
    "end": "146879"
  },
  {
    "text": "architecture",
    "start": "146879",
    "end": "148000"
  },
  {
    "text": "it is more than practical to use",
    "start": "148000",
    "end": "150239"
  },
  {
    "text": "different models for e",
    "start": "150239",
    "end": "151920"
  },
  {
    "text": "for even just the same problem different",
    "start": "151920",
    "end": "154560"
  },
  {
    "text": "framework each have their own pros and",
    "start": "154560",
    "end": "156480"
  },
  {
    "text": "cons and specialties",
    "start": "156480",
    "end": "158239"
  },
  {
    "text": "to solve the real world problem we just",
    "start": "158239",
    "end": "160879"
  },
  {
    "text": "grab the best tool",
    "start": "160879",
    "end": "162400"
  },
  {
    "text": "available out there and the model server",
    "start": "162400",
    "end": "164720"
  },
  {
    "text": "should take care of multiplexing many",
    "start": "164720",
    "end": "166560"
  },
  {
    "text": "different models",
    "start": "166560",
    "end": "168480"
  },
  {
    "text": "another key characteristic of machine",
    "start": "168480",
    "end": "170560"
  },
  {
    "text": "learning models",
    "start": "170560",
    "end": "171599"
  },
  {
    "text": "is that they are quite slow and resource",
    "start": "171599",
    "end": "174480"
  },
  {
    "text": "intensive",
    "start": "174480",
    "end": "176080"
  },
  {
    "text": "why we we don't want our user to wait",
    "start": "176080",
    "end": "178800"
  },
  {
    "text": "for the request to pile up",
    "start": "178800",
    "end": "180800"
  },
  {
    "text": "what can we do model servers should be",
    "start": "180800",
    "end": "182800"
  },
  {
    "text": "able to transparently scale the model to",
    "start": "182800",
    "end": "185280"
  },
  {
    "text": "multiple replicas and no balancing among",
    "start": "185280",
    "end": "187760"
  },
  {
    "text": "them",
    "start": "187760",
    "end": "188480"
  },
  {
    "text": "so requests can be parallelized",
    "start": "188480",
    "end": "191519"
  },
  {
    "text": "and despite being resource intensive and",
    "start": "191519",
    "end": "193680"
  },
  {
    "text": "often times",
    "start": "193680",
    "end": "194560"
  },
  {
    "text": "slow machinery model can utilize",
    "start": "194560",
    "end": "196800"
  },
  {
    "text": "hardware parallelism like vectorize",
    "start": "196800",
    "end": "198959"
  },
  {
    "text": "instructions",
    "start": "198959",
    "end": "199680"
  },
  {
    "text": "or multi-threading to efficiently",
    "start": "199680",
    "end": "201760"
  },
  {
    "text": "perform inference on a batch of requests",
    "start": "201760",
    "end": "204640"
  },
  {
    "text": "many times performing inferences on a",
    "start": "204640",
    "end": "206480"
  },
  {
    "text": "batch say four images",
    "start": "206480",
    "end": "208080"
  },
  {
    "text": "only come with a very small cost",
    "start": "208080",
    "end": "210080"
  },
  {
    "text": "compared to just one image",
    "start": "210080",
    "end": "212239"
  },
  {
    "text": "batching is a simple technique to",
    "start": "212239",
    "end": "213840"
  },
  {
    "text": "improve utilization and throughput of",
    "start": "213840",
    "end": "216000"
  },
  {
    "text": "the models",
    "start": "216000",
    "end": "218560"
  },
  {
    "text": "when you put modeling production we",
    "start": "218560",
    "end": "220480"
  },
  {
    "text": "often want to periodically retrain the",
    "start": "220480",
    "end": "222239"
  },
  {
    "text": "model on fresh data",
    "start": "222239",
    "end": "223680"
  },
  {
    "text": "or experiment with different hybrid",
    "start": "223680",
    "end": "225280"
  },
  {
    "text": "parameters this is when versioning",
    "start": "225280",
    "end": "227519"
  },
  {
    "text": "becomes an important requirement",
    "start": "227519",
    "end": "229680"
  },
  {
    "text": "conserving system enables zero downtime",
    "start": "229680",
    "end": "232560"
  },
  {
    "text": "deployment",
    "start": "232560",
    "end": "233680"
  },
  {
    "text": "gradual rollout rollback or a b testing",
    "start": "233680",
    "end": "236720"
  },
  {
    "text": "can we transparently update the version",
    "start": "236720",
    "end": "238720"
  },
  {
    "text": "of the model or test in your version",
    "start": "238720",
    "end": "240959"
  },
  {
    "text": "without user noticing",
    "start": "240959",
    "end": "242400"
  },
  {
    "text": "any difference there are also",
    "start": "242400",
    "end": "245439"
  },
  {
    "text": "requirements for handling pre-processing",
    "start": "245439",
    "end": "247519"
  },
  {
    "text": "and post-processing logic for the models",
    "start": "247519",
    "end": "249760"
  },
  {
    "text": "typically our model accepts numeric",
    "start": "249760",
    "end": "251599"
  },
  {
    "text": "tensors and",
    "start": "251599",
    "end": "252799"
  },
  {
    "text": "output numeric tensors but this is not",
    "start": "252799",
    "end": "255840"
  },
  {
    "text": "what end user wants",
    "start": "255840",
    "end": "257359"
  },
  {
    "text": "for the model expect a validated",
    "start": "257359",
    "end": "259600"
  },
  {
    "text": "transformed input",
    "start": "259600",
    "end": "261040"
  },
  {
    "text": "the output array of floating point",
    "start": "261040",
    "end": "262800"
  },
  {
    "text": "numbers means two",
    "start": "262800",
    "end": "264080"
  },
  {
    "text": "means nothing to the end user for the",
    "start": "264080",
    "end": "266639"
  },
  {
    "text": "machinery application",
    "start": "266639",
    "end": "268320"
  },
  {
    "text": "therefore pre-processing and",
    "start": "268320",
    "end": "269759"
  },
  {
    "text": "post-processing logic is critical to ml",
    "start": "269759",
    "end": "272080"
  },
  {
    "text": "model servers",
    "start": "272080",
    "end": "274240"
  },
  {
    "text": "and last but not least we need to pay",
    "start": "274240",
    "end": "276400"
  },
  {
    "text": "attention to how much performance are we",
    "start": "276400",
    "end": "278400"
  },
  {
    "text": "getting out of the hardware and relative",
    "start": "278400",
    "end": "280240"
  },
  {
    "text": "cost",
    "start": "280240",
    "end": "281040"
  },
  {
    "text": "because ml models are computationally",
    "start": "281040",
    "end": "283680"
  },
  {
    "text": "expensive",
    "start": "283680",
    "end": "284560"
  },
  {
    "text": "their relative throughput is slow to",
    "start": "284560",
    "end": "286880"
  },
  {
    "text": "handle large volume of incoming data",
    "start": "286880",
    "end": "289040"
  },
  {
    "text": "we need to trade off the cost",
    "start": "289040",
    "end": "290800"
  },
  {
    "text": "performance",
    "start": "290800",
    "end": "292080"
  },
  {
    "text": "latency and throughput does your serving",
    "start": "292080",
    "end": "294639"
  },
  {
    "text": "system allow you to tweak this parameter",
    "start": "294639",
    "end": "297120"
  },
  {
    "text": "that your serving system help you",
    "start": "297120",
    "end": "298560"
  },
  {
    "text": "optimize your cost given your",
    "start": "298560",
    "end": "300560"
  },
  {
    "text": "performance requirements",
    "start": "300560",
    "end": "303600"
  },
  {
    "text": "and in summary serving machinery models",
    "start": "303600",
    "end": "306479"
  },
  {
    "text": "have its",
    "start": "306479",
    "end": "307120"
  },
  {
    "text": "unique set of requirements now let's",
    "start": "307120",
    "end": "309520"
  },
  {
    "text": "take a look",
    "start": "309520",
    "end": "310240"
  },
  {
    "text": "at how people deploy models in",
    "start": "310240",
    "end": "311840"
  },
  {
    "text": "production today",
    "start": "311840",
    "end": "314160"
  },
  {
    "text": "there are two common approaches to to",
    "start": "314160",
    "end": "316080"
  },
  {
    "text": "deploy models in production",
    "start": "316080",
    "end": "318000"
  },
  {
    "text": "i'll discuss them one by one one",
    "start": "318000",
    "end": "320479"
  },
  {
    "text": "approach",
    "start": "320479",
    "end": "321039"
  },
  {
    "text": "is doing better model evaluation logic",
    "start": "321039",
    "end": "324000"
  },
  {
    "text": "inside a typical web server",
    "start": "324000",
    "end": "326240"
  },
  {
    "text": "and the second approach is to offload",
    "start": "326240",
    "end": "328320"
  },
  {
    "text": "the model",
    "start": "328320",
    "end": "329280"
  },
  {
    "text": "to an external specialized service",
    "start": "329280",
    "end": "333199"
  },
  {
    "text": "let's take a look at the first approach",
    "start": "333199",
    "end": "335199"
  },
  {
    "text": "the first approach",
    "start": "335199",
    "end": "336560"
  },
  {
    "text": "typically wraps a model inside a",
    "start": "336560",
    "end": "338560"
  },
  {
    "text": "conventional web server",
    "start": "338560",
    "end": "340000"
  },
  {
    "text": "i'll use flask a popular python web",
    "start": "340000",
    "end": "342320"
  },
  {
    "text": "framework as an example here",
    "start": "342320",
    "end": "344320"
  },
  {
    "text": "when http requests come in the flash",
    "start": "344320",
    "end": "346960"
  },
  {
    "text": "server can dispatch the request to",
    "start": "346960",
    "end": "348880"
  },
  {
    "text": "different api endpoint",
    "start": "348880",
    "end": "350720"
  },
  {
    "text": "we can make one of the endpoint handler",
    "start": "350720",
    "end": "352800"
  },
  {
    "text": "our model evaluation logic",
    "start": "352800",
    "end": "355360"
  },
  {
    "text": "let's say we reach slash api slash image",
    "start": "355360",
    "end": "358639"
  },
  {
    "text": "slash prediction point",
    "start": "358639",
    "end": "360319"
  },
  {
    "text": "the input will be transformed and",
    "start": "360319",
    "end": "362160"
  },
  {
    "text": "evaluated in",
    "start": "362160",
    "end": "363680"
  },
  {
    "text": "a tensorflow model the output result is",
    "start": "363680",
    "end": "366400"
  },
  {
    "text": "captured",
    "start": "366400",
    "end": "367199"
  },
  {
    "text": "inside the return response let's take a",
    "start": "367199",
    "end": "370400"
  },
  {
    "text": "look at some pros and cons of this",
    "start": "370400",
    "end": "372080"
  },
  {
    "text": "approach",
    "start": "372080",
    "end": "373919"
  },
  {
    "text": "the web server approach is simple and",
    "start": "373919",
    "end": "375919"
  },
  {
    "text": "easy to build and understand",
    "start": "375919",
    "end": "378080"
  },
  {
    "text": "it gives the developer end-to-end",
    "start": "378080",
    "end": "380479"
  },
  {
    "text": "control",
    "start": "380479",
    "end": "381280"
  },
  {
    "text": "over how the request is handled and how",
    "start": "381280",
    "end": "383680"
  },
  {
    "text": "the model is served",
    "start": "383680",
    "end": "385680"
  },
  {
    "text": "if you're just doing a simple demo of",
    "start": "385680",
    "end": "387360"
  },
  {
    "text": "proof of concept this approach is",
    "start": "387360",
    "end": "389120"
  },
  {
    "text": "perfect because it gets a job done",
    "start": "389120",
    "end": "392080"
  },
  {
    "text": "however a big downside of this approach",
    "start": "392080",
    "end": "394160"
  },
  {
    "text": "is whenever you want to scale more than",
    "start": "394160",
    "end": "395840"
  },
  {
    "text": "just one query at a time",
    "start": "395840",
    "end": "397759"
  },
  {
    "text": "or have multiple models to be served at",
    "start": "397759",
    "end": "400000"
  },
  {
    "text": "the same time",
    "start": "400000",
    "end": "401280"
  },
  {
    "text": "the model is typically loaded as some",
    "start": "401280",
    "end": "403440"
  },
  {
    "text": "sort of global variable",
    "start": "403440",
    "end": "405039"
  },
  {
    "text": "and there's no way to manage the startup",
    "start": "405039",
    "end": "407120"
  },
  {
    "text": "initialization",
    "start": "407120",
    "end": "408479"
  },
  {
    "text": "warm-up waste loading and other",
    "start": "408479",
    "end": "410639"
  },
  {
    "text": "lifecycle of the model",
    "start": "410639",
    "end": "412560"
  },
  {
    "text": "and lastly if one model crashes it can",
    "start": "412560",
    "end": "415520"
  },
  {
    "text": "bring the whole web server down",
    "start": "415520",
    "end": "418000"
  },
  {
    "text": "there are few more hurdles to jump",
    "start": "418000",
    "end": "419680"
  },
  {
    "text": "through with this approach",
    "start": "419680",
    "end": "421280"
  },
  {
    "text": "a typical deployment model for python",
    "start": "421280",
    "end": "424560"
  },
  {
    "text": "web servers is a process pool based",
    "start": "424560",
    "end": "427120"
  },
  {
    "text": "this means requests are low balance",
    "start": "427120",
    "end": "428880"
  },
  {
    "text": "around different instantiation of the",
    "start": "428880",
    "end": "430720"
  },
  {
    "text": "worker process",
    "start": "430720",
    "end": "433039"
  },
  {
    "text": "as we discussed before the model is",
    "start": "433039",
    "end": "435360"
  },
  {
    "text": "typically stored as a global variable",
    "start": "435360",
    "end": "437840"
  },
  {
    "text": "and there is no control over the loading",
    "start": "437840",
    "end": "439759"
  },
  {
    "text": "the model what happens when",
    "start": "439759",
    "end": "442400"
  },
  {
    "text": "say have three different version for the",
    "start": "442400",
    "end": "445520"
  },
  {
    "text": "web server",
    "start": "445520",
    "end": "447360"
  },
  {
    "text": "let's take a closer look let's say we",
    "start": "447360",
    "end": "449199"
  },
  {
    "text": "have three models",
    "start": "449199",
    "end": "450400"
  },
  {
    "text": "for our model server and then use this",
    "start": "450400",
    "end": "453759"
  },
  {
    "text": "process put deployment three model",
    "start": "453759",
    "end": "456960"
  },
  {
    "text": "per web server can quickly turn into",
    "start": "456960",
    "end": "460160"
  },
  {
    "text": "say tens of instantiation",
    "start": "460160",
    "end": "463440"
  },
  {
    "text": "of the model in memory without any any",
    "start": "463440",
    "end": "466000"
  },
  {
    "text": "optimization",
    "start": "466000",
    "end": "467120"
  },
  {
    "text": "a typical deployment can often encounter",
    "start": "467120",
    "end": "469680"
  },
  {
    "text": "out of memory issue",
    "start": "469680",
    "end": "472319"
  },
  {
    "text": "and another issue is this kind of web",
    "start": "472319",
    "end": "474479"
  },
  {
    "text": "server approach is that it is difficult",
    "start": "474479",
    "end": "476080"
  },
  {
    "text": "to form inference pipeline what do i",
    "start": "476080",
    "end": "478319"
  },
  {
    "text": "mean by that",
    "start": "478319",
    "end": "479520"
  },
  {
    "text": "let's say one of this kind of circle",
    "start": "479520",
    "end": "482080"
  },
  {
    "text": "represents a machine learning model",
    "start": "482080",
    "end": "484400"
  },
  {
    "text": "in a typical user-facing ml application",
    "start": "484400",
    "end": "486879"
  },
  {
    "text": "model does not work in isolation",
    "start": "486879",
    "end": "490639"
  },
  {
    "text": "there are typically chains of operators",
    "start": "490639",
    "end": "493039"
  },
  {
    "text": "that involve pre-processing",
    "start": "493039",
    "end": "494560"
  },
  {
    "text": "post-processing",
    "start": "494560",
    "end": "495759"
  },
  {
    "text": "or embedding lookup visualization and",
    "start": "495759",
    "end": "498080"
  },
  {
    "text": "ranking",
    "start": "498080",
    "end": "499599"
  },
  {
    "text": "there are also cases where you need to",
    "start": "499599",
    "end": "501360"
  },
  {
    "text": "split the request",
    "start": "501360",
    "end": "502639"
  },
  {
    "text": "between different models to perform a b",
    "start": "502639",
    "end": "505120"
  },
  {
    "text": "testing",
    "start": "505120",
    "end": "506000"
  },
  {
    "text": "for example we want to send 80 percent",
    "start": "506000",
    "end": "507919"
  },
  {
    "text": "of traffic to one of the model and 20",
    "start": "507919",
    "end": "509919"
  },
  {
    "text": "to the other sometimes you can use",
    "start": "509919",
    "end": "512880"
  },
  {
    "text": "ensemble as a technique to improve",
    "start": "512880",
    "end": "514800"
  },
  {
    "text": "accuracy of the prediction",
    "start": "514800",
    "end": "516640"
  },
  {
    "text": "combining output from different model is",
    "start": "516640",
    "end": "518560"
  },
  {
    "text": "through averaging or voting",
    "start": "518560",
    "end": "520240"
  },
  {
    "text": "help combat model bias and to improve",
    "start": "520240",
    "end": "523599"
  },
  {
    "text": "the performance",
    "start": "523599",
    "end": "525680"
  },
  {
    "text": "and more complex pattern is also",
    "start": "525680",
    "end": "527839"
  },
  {
    "text": "possible as well",
    "start": "527839",
    "end": "529200"
  },
  {
    "text": "for example to optimize engineering",
    "start": "529200",
    "end": "531040"
  },
  {
    "text": "latency we can try the input on a faster",
    "start": "531040",
    "end": "534160"
  },
  {
    "text": "but simpler model only if the model",
    "start": "534160",
    "end": "537120"
  },
  {
    "text": "output low confidence",
    "start": "537120",
    "end": "538959"
  },
  {
    "text": "will route relative requests to a",
    "start": "538959",
    "end": "541600"
  },
  {
    "text": "heavier weight",
    "start": "541600",
    "end": "542959"
  },
  {
    "text": "but more accurate models otherwise we",
    "start": "542959",
    "end": "546080"
  },
  {
    "text": "can just assume that since the model has",
    "start": "546080",
    "end": "548080"
  },
  {
    "text": "given high confidence we can just return",
    "start": "548080",
    "end": "550480"
  },
  {
    "text": "the results",
    "start": "550480",
    "end": "552560"
  },
  {
    "text": "so in summary a simple web survey is",
    "start": "552560",
    "end": "556000"
  },
  {
    "text": "great for demo",
    "start": "556000",
    "end": "556880"
  },
  {
    "text": "proof of concept however it creates a",
    "start": "556880",
    "end": "559200"
  },
  {
    "text": "lot of additional optimization to make",
    "start": "559200",
    "end": "561120"
  },
  {
    "text": "it production ready",
    "start": "561120",
    "end": "563600"
  },
  {
    "text": "when just a simple web server is not",
    "start": "563600",
    "end": "566240"
  },
  {
    "text": "enough",
    "start": "566240",
    "end": "566800"
  },
  {
    "text": "people turn to the second approach that",
    "start": "566800",
    "end": "569200"
  },
  {
    "text": "is to offload the model serving to",
    "start": "569200",
    "end": "571120"
  },
  {
    "text": "specialized external service",
    "start": "571120",
    "end": "574720"
  },
  {
    "text": "using your previous flash server example",
    "start": "574720",
    "end": "577200"
  },
  {
    "text": "the",
    "start": "577200",
    "end": "577839"
  },
  {
    "text": "api endpoint when you receive the",
    "start": "577839",
    "end": "580560"
  },
  {
    "text": "request",
    "start": "580560",
    "end": "581279"
  },
  {
    "text": "instead of passing it through tensorflow",
    "start": "581279",
    "end": "583279"
  },
  {
    "text": "you'll now forward the request to some",
    "start": "583279",
    "end": "584959"
  },
  {
    "text": "sort of external service",
    "start": "584959",
    "end": "586480"
  },
  {
    "text": "here we represent it as the cloud",
    "start": "586480",
    "end": "589360"
  },
  {
    "text": "example of this kind of services",
    "start": "589360",
    "end": "590959"
  },
  {
    "text": "include tensorflow serving from google",
    "start": "590959",
    "end": "592880"
  },
  {
    "text": "on extraordinary facebook and microsoft",
    "start": "592880",
    "end": "595279"
  },
  {
    "text": "seldom tensor rt from nvidia as well as",
    "start": "595279",
    "end": "598399"
  },
  {
    "text": "aws search maker",
    "start": "598399",
    "end": "600480"
  },
  {
    "text": "let's take a look let's take a step back",
    "start": "600480",
    "end": "602480"
  },
  {
    "text": "and look at sort of examine this",
    "start": "602480",
    "end": "604399"
  },
  {
    "text": "approach",
    "start": "604399",
    "end": "605519"
  },
  {
    "text": "when requests come into http there are",
    "start": "605519",
    "end": "607760"
  },
  {
    "text": "actually many steps before you reach the",
    "start": "607760",
    "end": "609600"
  },
  {
    "text": "model for",
    "start": "609600",
    "end": "610480"
  },
  {
    "text": "core inference bits and many steps after",
    "start": "610480",
    "end": "613360"
  },
  {
    "text": "the inference",
    "start": "613360",
    "end": "614720"
  },
  {
    "text": "when http requests come in it needs to",
    "start": "614720",
    "end": "616959"
  },
  {
    "text": "be checked and validated",
    "start": "616959",
    "end": "618640"
  },
  {
    "text": "parsed and transferred into custom",
    "start": "618640",
    "end": "621600"
  },
  {
    "text": "transform with custom business logic",
    "start": "621600",
    "end": "624480"
  },
  {
    "text": "specific to the workload and then the",
    "start": "624480",
    "end": "627279"
  },
  {
    "text": "trend",
    "start": "627279",
    "end": "627680"
  },
  {
    "text": "to transform the input so that it can be",
    "start": "627680",
    "end": "629440"
  },
  {
    "text": "run by the model",
    "start": "629440",
    "end": "631040"
  },
  {
    "text": "and then once you have the model",
    "start": "631040",
    "end": "632640"
  },
  {
    "text": "inference and the model output still",
    "start": "632640",
    "end": "634800"
  },
  {
    "text": "need to be transformed",
    "start": "634800",
    "end": "636160"
  },
  {
    "text": "and be applied with custom business",
    "start": "636160",
    "end": "638240"
  },
  {
    "text": "logic and finally results need to go out",
    "start": "638240",
    "end": "640959"
  },
  {
    "text": "in",
    "start": "640959",
    "end": "641440"
  },
  {
    "text": "http response format",
    "start": "641440",
    "end": "645200"
  },
  {
    "text": "what happened when we have external",
    "start": "645200",
    "end": "646720"
  },
  {
    "text": "service originally the",
    "start": "646720",
    "end": "648959"
  },
  {
    "text": "entire stack of steps happened in the",
    "start": "648959",
    "end": "651760"
  },
  {
    "text": "web server on the left",
    "start": "651760",
    "end": "654320"
  },
  {
    "text": "when we offload the influence to",
    "start": "654320",
    "end": "655920"
  },
  {
    "text": "external service only the inference is",
    "start": "655920",
    "end": "658480"
  },
  {
    "text": "shipped to external service",
    "start": "658480",
    "end": "660640"
  },
  {
    "text": "the input and output transformation",
    "start": "660640",
    "end": "664320"
  },
  {
    "text": "span across web server and external",
    "start": "664320",
    "end": "667440"
  },
  {
    "text": "service",
    "start": "667440",
    "end": "669839"
  },
  {
    "text": "so in essence i will characterize these",
    "start": "670320",
    "end": "672480"
  },
  {
    "text": "external services",
    "start": "672480",
    "end": "673600"
  },
  {
    "text": "as tensor and tensor out their interface",
    "start": "673600",
    "end": "677040"
  },
  {
    "text": "is pretty limited to just numeric tensor",
    "start": "677040",
    "end": "678880"
  },
  {
    "text": "or some varying of it",
    "start": "678880",
    "end": "680480"
  },
  {
    "text": "for a concrete example for image",
    "start": "680480",
    "end": "682320"
  },
  {
    "text": "classification we want to take in",
    "start": "682320",
    "end": "684240"
  },
  {
    "text": "bytes from api request sort of decode it",
    "start": "684240",
    "end": "686880"
  },
  {
    "text": "and then do some color normalization",
    "start": "686880",
    "end": "689120"
  },
  {
    "text": "and then like for image pre-processing",
    "start": "689120",
    "end": "691120"
  },
  {
    "text": "like cropping",
    "start": "691120",
    "end": "692240"
  },
  {
    "text": "and then convert it to 3d tensor and",
    "start": "692240",
    "end": "695040"
  },
  {
    "text": "then",
    "start": "695040",
    "end": "695360"
  },
  {
    "text": "i'll put it through the model and the",
    "start": "695360",
    "end": "697040"
  },
  {
    "text": "output from the inference state is also",
    "start": "697040",
    "end": "699200"
  },
  {
    "text": "a tensor",
    "start": "699200",
    "end": "700000"
  },
  {
    "text": "and then when you translate this tensor",
    "start": "700000",
    "end": "701760"
  },
  {
    "text": "into some sort of english label or",
    "start": "701760",
    "end": "703680"
  },
  {
    "text": "database id",
    "start": "703680",
    "end": "705279"
  },
  {
    "text": "it is worth noting that some of the",
    "start": "705279",
    "end": "706880"
  },
  {
    "text": "logic are tightly coupled",
    "start": "706880",
    "end": "708640"
  },
  {
    "text": "with the model itself when we improve",
    "start": "708640",
    "end": "710639"
  },
  {
    "text": "the model accuracy",
    "start": "710639",
    "end": "712160"
  },
  {
    "text": "some of these pre or post processing",
    "start": "712160",
    "end": "714399"
  },
  {
    "text": "procedure might change",
    "start": "714399",
    "end": "716639"
  },
  {
    "text": "additionally the split between one part",
    "start": "716639",
    "end": "719440"
  },
  {
    "text": "of the request handling logic",
    "start": "719440",
    "end": "721440"
  },
  {
    "text": "and the other part of model inference",
    "start": "721440",
    "end": "723279"
  },
  {
    "text": "logic brings a lot of",
    "start": "723279",
    "end": "725040"
  },
  {
    "text": "complexity",
    "start": "725040",
    "end": "727759"
  },
  {
    "text": "so in summary as an external service",
    "start": "728320",
    "end": "730720"
  },
  {
    "text": "approach encourage",
    "start": "730720",
    "end": "731839"
  },
  {
    "text": "separation concerns but you do need to",
    "start": "731839",
    "end": "734320"
  },
  {
    "text": "manage different",
    "start": "734320",
    "end": "735440"
  },
  {
    "text": "servers separately and tune them",
    "start": "735440",
    "end": "737279"
  },
  {
    "text": "separately",
    "start": "737279",
    "end": "738560"
  },
  {
    "text": "more importantly the model evolution",
    "start": "738560",
    "end": "739920"
  },
  {
    "text": "logic is not split from this logic",
    "start": "739920",
    "end": "743440"
  },
  {
    "text": "and the approach is hard to learn",
    "start": "743440",
    "end": "745360"
  },
  {
    "text": "because of this sort of new custom",
    "start": "745360",
    "end": "747360"
  },
  {
    "text": "system and in addition it's hard to",
    "start": "747360",
    "end": "749839"
  },
  {
    "text": "debug",
    "start": "749839",
    "end": "750639"
  },
  {
    "text": "because it's no longer the training",
    "start": "750639",
    "end": "752320"
  },
  {
    "text": "framework or data scientists you are",
    "start": "752320",
    "end": "754079"
  },
  {
    "text": "familiar with",
    "start": "754079",
    "end": "755279"
  },
  {
    "text": "you have to work with rpc setting up",
    "start": "755279",
    "end": "757519"
  },
  {
    "text": "external service configuration or even",
    "start": "757519",
    "end": "759519"
  },
  {
    "text": "different",
    "start": "759519",
    "end": "760000"
  },
  {
    "text": "like computation system and deploy to",
    "start": "760000",
    "end": "762480"
  },
  {
    "text": "the system",
    "start": "762480",
    "end": "764000"
  },
  {
    "text": "is there a better way we build reserve",
    "start": "764000",
    "end": "766720"
  },
  {
    "text": "reserve leveraged race system to",
    "start": "766720",
    "end": "768639"
  },
  {
    "text": "transparently scale",
    "start": "768639",
    "end": "770399"
  },
  {
    "text": "the model 200 course it gives entering",
    "start": "770399",
    "end": "773600"
  },
  {
    "text": "control",
    "start": "773600",
    "end": "774240"
  },
  {
    "text": "over how the request is handled",
    "start": "774240",
    "end": "777440"
  },
  {
    "text": "as well just like the web server",
    "start": "777440",
    "end": "779279"
  },
  {
    "text": "approach but the model is now isolated",
    "start": "779279",
    "end": "781440"
  },
  {
    "text": "and scaled",
    "start": "781440",
    "end": "782320"
  },
  {
    "text": "independently reserve is also pipeline",
    "start": "782320",
    "end": "785519"
  },
  {
    "text": "native",
    "start": "785519",
    "end": "786000"
  },
  {
    "text": "will enable users to easily combine",
    "start": "786000",
    "end": "787839"
  },
  {
    "text": "multiple models together and easily",
    "start": "787839",
    "end": "789839"
  },
  {
    "text": "construct and connect this model",
    "start": "789839",
    "end": "792000"
  },
  {
    "text": "we also have built-in extensibility it",
    "start": "792000",
    "end": "794399"
  },
  {
    "text": "is a programmable serving system that",
    "start": "794399",
    "end": "796000"
  },
  {
    "text": "allows you to modify how it works while",
    "start": "796000",
    "end": "797839"
  },
  {
    "text": "it's running",
    "start": "797839",
    "end": "798720"
  },
  {
    "text": "as well as offering monitoring and",
    "start": "798720",
    "end": "800639"
  },
  {
    "text": "observability out of the box",
    "start": "800639",
    "end": "803279"
  },
  {
    "text": "let's take a closer look at the reserve",
    "start": "803279",
    "end": "805200"
  },
  {
    "text": "api models can be defined with a simple",
    "start": "805200",
    "end": "808000"
  },
  {
    "text": "python class",
    "start": "808000",
    "end": "809200"
  },
  {
    "text": "the model can be loaded in the init",
    "start": "809200",
    "end": "811519"
  },
  {
    "text": "method",
    "start": "811519",
    "end": "812480"
  },
  {
    "text": "the call method sort of identifies the",
    "start": "812480",
    "end": "814399"
  },
  {
    "text": "http request handler",
    "start": "814399",
    "end": "816079"
  },
  {
    "text": "reserve explosive flash request",
    "start": "816079",
    "end": "817920"
  },
  {
    "text": "interface so it's easy to use",
    "start": "817920",
    "end": "819600"
  },
  {
    "text": "understand and migrate your existing",
    "start": "819600",
    "end": "821279"
  },
  {
    "text": "flask application",
    "start": "821279",
    "end": "823440"
  },
  {
    "text": "you can add arbitrary business logic",
    "start": "823440",
    "end": "825360"
  },
  {
    "text": "pre-processing or post-processing in",
    "start": "825360",
    "end": "827279"
  },
  {
    "text": "this handler",
    "start": "827279",
    "end": "828639"
  },
  {
    "text": "the key is that model device evaluation",
    "start": "828639",
    "end": "832399"
  },
  {
    "text": "and surrounding business logic is in one",
    "start": "832399",
    "end": "834959"
  },
  {
    "text": "place",
    "start": "834959",
    "end": "836079"
  },
  {
    "text": "we serve as a concept of backend and",
    "start": "836079",
    "end": "838079"
  },
  {
    "text": "endpoints a packing is associated with",
    "start": "838079",
    "end": "840720"
  },
  {
    "text": "model implementation",
    "start": "840720",
    "end": "842079"
  },
  {
    "text": "and is versioned and scalable for",
    "start": "842079",
    "end": "844560"
  },
  {
    "text": "example you can specify the number of",
    "start": "844560",
    "end": "846320"
  },
  {
    "text": "replica in the create backing call",
    "start": "846320",
    "end": "848880"
  },
  {
    "text": "endpoint is an user reachable endpoint",
    "start": "848880",
    "end": "851440"
  },
  {
    "text": "and it is associated with",
    "start": "851440",
    "end": "853120"
  },
  {
    "text": "with one or more backends here we",
    "start": "853120",
    "end": "855440"
  },
  {
    "text": "declare",
    "start": "855440",
    "end": "856240"
  },
  {
    "text": "a http route for that endpoint slash api",
    "start": "856240",
    "end": "861360"
  },
  {
    "text": "so as a uh so",
    "start": "861920",
    "end": "865120"
  },
  {
    "text": "as a summary we have sort of the class",
    "start": "865120",
    "end": "867279"
  },
  {
    "text": "interface",
    "start": "867279",
    "end": "868160"
  },
  {
    "text": "to define a backend we have flash",
    "start": "868160",
    "end": "872079"
  },
  {
    "text": "requests make it easy to sort of make it",
    "start": "872079",
    "end": "875040"
  },
  {
    "text": "easy to",
    "start": "875040",
    "end": "875760"
  },
  {
    "text": "migrate your existing flash application",
    "start": "875760",
    "end": "878880"
  },
  {
    "text": "the backend is a scalable individual",
    "start": "878880",
    "end": "881199"
  },
  {
    "text": "units",
    "start": "881199",
    "end": "881920"
  },
  {
    "text": "an endpoint is a user reachable http",
    "start": "881920",
    "end": "884800"
  },
  {
    "text": "endpoint",
    "start": "884800",
    "end": "886399"
  },
  {
    "text": "i call reserve a programmable serving",
    "start": "886399",
    "end": "888399"
  },
  {
    "text": "system the concept of endpoint",
    "start": "888399",
    "end": "890000"
  },
  {
    "text": "associated with logical service",
    "start": "890000",
    "end": "891440"
  },
  {
    "text": "enrichable by url route",
    "start": "891440",
    "end": "893199"
  },
  {
    "text": "under endpoint you can associate it with",
    "start": "893199",
    "end": "895040"
  },
  {
    "text": "multiple different backends",
    "start": "895040",
    "end": "897120"
  },
  {
    "text": "reserve have built-in support for",
    "start": "897120",
    "end": "898560"
  },
  {
    "text": "composing model together",
    "start": "898560",
    "end": "901120"
  },
  {
    "text": "because in your backend implementation",
    "start": "901120",
    "end": "903040"
  },
  {
    "text": "you can invoke message on the endpoints",
    "start": "903040",
    "end": "905600"
  },
  {
    "text": "instead of configuring your service with",
    "start": "905600",
    "end": "907279"
  },
  {
    "text": "yaml research configuration language is",
    "start": "907279",
    "end": "909839"
  },
  {
    "text": "just",
    "start": "909839",
    "end": "910240"
  },
  {
    "text": "python the language and tool data",
    "start": "910240",
    "end": "912240"
  },
  {
    "text": "scientists are familiar with",
    "start": "912240",
    "end": "914160"
  },
  {
    "text": "and by building on top of ray the",
    "start": "914160",
    "end": "915680"
  },
  {
    "text": "distributor runtime allows us to update",
    "start": "915680",
    "end": "918000"
  },
  {
    "text": "running cluster and make it make every",
    "start": "918000",
    "end": "920000"
  },
  {
    "text": "single component fully fall tolerant uh",
    "start": "920000",
    "end": "926160"
  },
  {
    "text": "as we discussed like",
    "start": "926160",
    "end": "927600"
  },
  {
    "text": "as compared to end uh tendencies to",
    "start": "927600",
    "end": "929440"
  },
  {
    "text": "tensor system",
    "start": "929440",
    "end": "930720"
  },
  {
    "text": "uh reserve give you end-to-end control",
    "start": "930720",
    "end": "932639"
  },
  {
    "text": "over the request here's an example of",
    "start": "932639",
    "end": "934720"
  },
  {
    "text": "how to",
    "start": "934720",
    "end": "935360"
  },
  {
    "text": "implement a very basic pi torch image",
    "start": "935360",
    "end": "938560"
  },
  {
    "text": "classifier end-to-end",
    "start": "938560",
    "end": "941519"
  },
  {
    "text": "razer also has the ability to let you",
    "start": "941519",
    "end": "944399"
  },
  {
    "text": "scale out",
    "start": "944399",
    "end": "945600"
  },
  {
    "text": "implement batching as well as utilize",
    "start": "945600",
    "end": "948399"
  },
  {
    "text": "custom",
    "start": "948399",
    "end": "949040"
  },
  {
    "text": "gpu reserve is just a python package you",
    "start": "949040",
    "end": "952959"
  },
  {
    "text": "can run it anywhere",
    "start": "952959",
    "end": "954320"
  },
  {
    "text": "and we can run it on your laptop for",
    "start": "954320",
    "end": "956000"
  },
  {
    "text": "quick demo and development",
    "start": "956000",
    "end": "957519"
  },
  {
    "text": "you can deploy it to a server we'll have",
    "start": "957519",
    "end": "959519"
  },
  {
    "text": "built-in template for you to run reserve",
    "start": "959519",
    "end": "961360"
  },
  {
    "text": "on kubernetes",
    "start": "961360",
    "end": "962320"
  },
  {
    "text": "as well as all major public cloud you",
    "start": "962320",
    "end": "965440"
  },
  {
    "text": "can try it out today by just pep install",
    "start": "965440",
    "end": "967440"
  },
  {
    "text": "i'm from rainforestserv",
    "start": "967440",
    "end": "969199"
  },
  {
    "text": "we have a vibrant community discussing",
    "start": "969199",
    "end": "971279"
  },
  {
    "text": "serve in the ray slack",
    "start": "971279",
    "end": "973279"
  },
  {
    "text": "and you can check our documentation on",
    "start": "973279",
    "end": "975160"
  },
  {
    "text": "docs.radiio",
    "start": "975160",
    "end": "976800"
  },
  {
    "text": "also recently i published several blog",
    "start": "976800",
    "end": "979199"
  },
  {
    "text": "posts about reserve",
    "start": "979199",
    "end": "980639"
  },
  {
    "text": "including one how to use research",
    "start": "980639",
    "end": "983759"
  },
  {
    "text": "to deploy natural language processing",
    "start": "983759",
    "end": "985920"
  },
  {
    "text": "model with hugging face",
    "start": "985920",
    "end": "987920"
  },
  {
    "text": "and there you can check out this link",
    "start": "987920",
    "end": "991120"
  },
  {
    "text": "to check it out so the key takeaway for",
    "start": "991120",
    "end": "994240"
  },
  {
    "text": "this talk",
    "start": "994240",
    "end": "994880"
  },
  {
    "text": "is that there are two common approaches",
    "start": "994880",
    "end": "997360"
  },
  {
    "text": "for model serving",
    "start": "997360",
    "end": "998720"
  },
  {
    "text": "you can wrap your model in web server or",
    "start": "998720",
    "end": "1000959"
  },
  {
    "text": "just this is ten story intensive",
    "start": "1000959",
    "end": "1002880"
  },
  {
    "text": "specialized system both have their own",
    "start": "1002880",
    "end": "1005360"
  },
  {
    "text": "flaws",
    "start": "1005360",
    "end": "1006880"
  },
  {
    "text": "reserve is a web framework that bridges",
    "start": "1006880",
    "end": "1009199"
  },
  {
    "text": "the benefits of the two approaches",
    "start": "1009199",
    "end": "1011759"
  },
  {
    "text": "scale the model to many machines as well",
    "start": "1011759",
    "end": "1014720"
  },
  {
    "text": "as grants a data scientist",
    "start": "1014720",
    "end": "1016480"
  },
  {
    "text": "and two and control over the machine",
    "start": "1016480",
    "end": "1018399"
  },
  {
    "text": "learning api",
    "start": "1018399",
    "end": "1020320"
  },
  {
    "text": "and thank you i'm going to take any",
    "start": "1020320",
    "end": "1022839"
  },
  {
    "text": "question",
    "start": "1022839",
    "end": "1025839"
  }
]