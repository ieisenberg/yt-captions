[
  {
    "text": "hello everyone we are going",
    "start": "6140",
    "end": "9620"
  },
  {
    "text": "from ungroup and today's topic is from",
    "start": "9620",
    "end": "14040"
  },
  {
    "text": "our body team lead by Tang Wei Tsai but",
    "start": "14040",
    "end": "17400"
  },
  {
    "text": "he cannot come here today so we'll be",
    "start": "17400",
    "end": "20699"
  },
  {
    "text": "the speakers",
    "start": "20699",
    "end": "22080"
  },
  {
    "text": "and this is this is the first time we",
    "start": "22080",
    "end": "24960"
  },
  {
    "text": "come here and we are so glad to share",
    "start": "24960",
    "end": "27900"
  },
  {
    "text": "our work in we submit",
    "start": "27900",
    "end": "30539"
  },
  {
    "text": "and today's topic is about how we",
    "start": "30539",
    "end": "33840"
  },
  {
    "text": "empowered and group to deliver a large",
    "start": "33840",
    "end": "36120"
  },
  {
    "text": "scale online service platform",
    "start": "36120",
    "end": "39180"
  },
  {
    "text": "let's start",
    "start": "39180",
    "end": "41780"
  },
  {
    "text": "first I'd like to introduce my company",
    "start": "41780",
    "end": "45300"
  },
  {
    "text": "and my team",
    "start": "45300",
    "end": "47160"
  },
  {
    "text": "to summarize and group there are five",
    "start": "47160",
    "end": "50160"
  },
  {
    "text": "directions digital payment digital",
    "start": "50160",
    "end": "53460"
  },
  {
    "text": "connectivity digital Finance digital",
    "start": "53460",
    "end": "56640"
  },
  {
    "text": "technology and globalization",
    "start": "56640",
    "end": "58860"
  },
  {
    "text": "and the most favorite",
    "start": "58860",
    "end": "62300"
  },
  {
    "text": "or popular application of one on group",
    "start": "62300",
    "end": "65760"
  },
  {
    "text": "is alipay which is leading the Internet",
    "start": "65760",
    "end": "69439"
  },
  {
    "text": "payment and Finance in China",
    "start": "69439",
    "end": "72960"
  },
  {
    "text": "and my theme is Android team",
    "start": "72960",
    "end": "77340"
  },
  {
    "text": "um",
    "start": "77340",
    "end": "78000"
  },
  {
    "text": "in the next page I will show you a lot",
    "start": "78000",
    "end": "80600"
  },
  {
    "text": "history information of my team back",
    "start": "80600",
    "end": "83880"
  },
  {
    "text": "there I want to share some high level",
    "start": "83880",
    "end": "87900"
  },
  {
    "text": "information first",
    "start": "87900",
    "end": "90799"
  },
  {
    "text": "um",
    "start": "92340",
    "end": "92939"
  },
  {
    "text": "we are the second largest team",
    "start": "92939",
    "end": "95040"
  },
  {
    "text": "contributing to the ray open source",
    "start": "95040",
    "end": "97799"
  },
  {
    "text": "software",
    "start": "97799",
    "end": "99079"
  },
  {
    "text": "we are collaborating with the unscale",
    "start": "99079",
    "end": "102119"
  },
  {
    "text": "and",
    "start": "102119",
    "end": "103159"
  },
  {
    "text": "a rest lab for several years",
    "start": "103159",
    "end": "106740"
  },
  {
    "text": "and we have over",
    "start": "106740",
    "end": "110060"
  },
  {
    "text": "26 percent",
    "start": "110060",
    "end": "111960"
  },
  {
    "text": "code counter build of the record part in",
    "start": "111960",
    "end": "114840"
  },
  {
    "text": "the open source",
    "start": "114840",
    "end": "116759"
  },
  {
    "text": "second",
    "start": "116759",
    "end": "118040"
  },
  {
    "text": "in addition to the developing rib we",
    "start": "118040",
    "end": "121979"
  },
  {
    "text": "also have a lot of large-scale",
    "start": "121979",
    "end": "124439"
  },
  {
    "text": "application of Ray and",
    "start": "124439",
    "end": "127399"
  },
  {
    "text": "now in our production we have over 1",
    "start": "127399",
    "end": "131940"
  },
  {
    "text": "million CPU course",
    "start": "131940",
    "end": "135020"
  },
  {
    "text": "and then in China we also have a real",
    "start": "135300",
    "end": "138780"
  },
  {
    "text": "china Community which is created and",
    "start": "138780",
    "end": "141660"
  },
  {
    "text": "operated by my team and the average for",
    "start": "141660",
    "end": "146099"
  },
  {
    "text": "over the past five years we hosted the",
    "start": "146099",
    "end": "149459"
  },
  {
    "text": "reformed Meetup every year and the fifth",
    "start": "149459",
    "end": "153120"
  },
  {
    "text": "three forward has been finished",
    "start": "153120",
    "end": "156420"
  },
  {
    "text": "in the past in the past July and in this",
    "start": "156420",
    "end": "161280"
  },
  {
    "text": "conference and scale across Ai and",
    "start": "161280",
    "end": "165920"
  },
  {
    "text": "I and and",
    "start": "165920",
    "end": "169340"
  },
  {
    "text": "patterns has shared their Works in this",
    "start": "169340",
    "end": "171840"
  },
  {
    "text": "conference so",
    "start": "171840",
    "end": "173840"
  },
  {
    "text": "so we are we hope that we could be more",
    "start": "173840",
    "end": "178739"
  },
  {
    "text": "popular in China as in U.S",
    "start": "178739",
    "end": "182900"
  },
  {
    "text": "let's go to the part of the history of",
    "start": "183120",
    "end": "185819"
  },
  {
    "text": "reaching out",
    "start": "185819",
    "end": "187319"
  },
  {
    "text": "my team is created in 2017 and in this",
    "start": "187319",
    "end": "192659"
  },
  {
    "text": "year the ray is more diff different from",
    "start": "192659",
    "end": "195180"
  },
  {
    "text": "33.",
    "start": "195180",
    "end": "196800"
  },
  {
    "text": "we we consider it as the general",
    "start": "196800",
    "end": "199280"
  },
  {
    "text": "distributive system",
    "start": "199280",
    "end": "201239"
  },
  {
    "text": "which means that",
    "start": "201239",
    "end": "204260"
  },
  {
    "text": "we have we have a lot of non-air",
    "start": "204260",
    "end": "206940"
  },
  {
    "text": "applications and",
    "start": "206940",
    "end": "209120"
  },
  {
    "text": "framework you know that in the really",
    "start": "209120",
    "end": "212280"
  },
  {
    "text": "open source",
    "start": "212280",
    "end": "215300"
  },
  {
    "text": "focuses on the python and AI which means",
    "start": "215900",
    "end": "220379"
  },
  {
    "text": "python force and Air Force but Young",
    "start": "220379",
    "end": "224760"
  },
  {
    "text": "we are different so you can know that",
    "start": "224760",
    "end": "227099"
  },
  {
    "text": "you can see that we already contributed",
    "start": "227099",
    "end": "230159"
  },
  {
    "text": "the Java and the C plus plus API in the",
    "start": "230159",
    "end": "234360"
  },
  {
    "text": "reproject",
    "start": "234360",
    "end": "236099"
  },
  {
    "text": "and in",
    "start": "236099",
    "end": "237840"
  },
  {
    "text": "2018 our first our player",
    "start": "237840",
    "end": "242420"
  },
  {
    "text": "applier engine was deployed in our",
    "start": "242420",
    "end": "246480"
  },
  {
    "text": "production the engine name is G flow and",
    "start": "246480",
    "end": "250260"
  },
  {
    "text": "it is a streaming graph engine it is",
    "start": "250260",
    "end": "253019"
  },
  {
    "text": "used in Risk control in payment of",
    "start": "253019",
    "end": "257100"
  },
  {
    "text": "alipay and in 2020 29 in 2019",
    "start": "257100",
    "end": "263180"
  },
  {
    "text": "another another Fusion engine it was",
    "start": "263180",
    "end": "267600"
  },
  {
    "text": "deployed in our production the engine is",
    "start": "267600",
    "end": "270000"
  },
  {
    "text": "only learning we feel the three",
    "start": "270000",
    "end": "273380"
  },
  {
    "text": "Computing tabs",
    "start": "273380",
    "end": "275900"
  },
  {
    "text": "including streaming ML and online",
    "start": "275900",
    "end": "278940"
  },
  {
    "text": "serving so you can know that online",
    "start": "278940",
    "end": "281400"
  },
  {
    "text": "serving this time is also the only",
    "start": "281400",
    "end": "283919"
  },
  {
    "text": "serving first time it was deployed in",
    "start": "283919",
    "end": "286380"
  },
  {
    "text": "out and it is today's topic",
    "start": "286380",
    "end": "289680"
  },
  {
    "text": "and in 2020",
    "start": "289680",
    "end": "292460"
  },
  {
    "text": "given the diversity of business we made",
    "start": "292460",
    "end": "298199"
  },
  {
    "text": "a new descendent architecture of re",
    "start": "298199",
    "end": "301139"
  },
  {
    "text": "which means you can run a lot of results",
    "start": "301139",
    "end": "305940"
  },
  {
    "text": "in a big requester and in in this job",
    "start": "305940",
    "end": "310560"
  },
  {
    "text": "you can support different Computing",
    "start": "310560",
    "end": "314340"
  },
  {
    "text": "programs",
    "start": "314340",
    "end": "316380"
  },
  {
    "text": "and in 2021 on the Android was being",
    "start": "316380",
    "end": "322080"
  },
  {
    "text": "more stable we have to do some more",
    "start": "322080",
    "end": "324840"
  },
  {
    "text": "collaboration collaboration with our",
    "start": "324840",
    "end": "327600"
  },
  {
    "text": "ecosystem companies such as Alibaba Damo",
    "start": "327600",
    "end": "331520"
  },
  {
    "text": "Academia tenure Network and version Bank",
    "start": "331520",
    "end": "336479"
  },
  {
    "text": "and in 2022 we explored privacy",
    "start": "336479",
    "end": "341340"
  },
  {
    "text": "Computing which is a new scenario",
    "start": "341340",
    "end": "345000"
  },
  {
    "text": "um and we think that it is a good",
    "start": "345000",
    "end": "348680"
  },
  {
    "text": "scenario for for real because in private",
    "start": "348680",
    "end": "353220"
  },
  {
    "text": "Computing we need a flexible framework",
    "start": "353220",
    "end": "357240"
  },
  {
    "text": "to support common function data a and AI",
    "start": "357240",
    "end": "361680"
  },
  {
    "text": "and it is there are a lot of advantages",
    "start": "361680",
    "end": "365460"
  },
  {
    "text": "injury for it",
    "start": "365460",
    "end": "368900"
  },
  {
    "text": "okay in this year's 2020 2023 we aim to",
    "start": "369120",
    "end": "374699"
  },
  {
    "text": "build a general AI serving framework",
    "start": "374699",
    "end": "378960"
  },
  {
    "text": "wait in this framework work we want to",
    "start": "378960",
    "end": "382500"
  },
  {
    "text": "unified traditional AI large language",
    "start": "382500",
    "end": "385560"
  },
  {
    "text": "model and search engine",
    "start": "385560",
    "end": "389160"
  },
  {
    "text": "so this is the today's topic",
    "start": "389160",
    "end": "393180"
  },
  {
    "text": "back to the online serving from the",
    "start": "393180",
    "end": "396900"
  },
  {
    "text": "historical timeline you can know that",
    "start": "396900",
    "end": "400039"
  },
  {
    "text": "the online serving was before was first",
    "start": "400039",
    "end": "404819"
  },
  {
    "text": "deployed in ant in 2019 and",
    "start": "404819",
    "end": "410520"
  },
  {
    "text": "it is integrated to the online learning",
    "start": "410520",
    "end": "413220"
  },
  {
    "text": "engine",
    "start": "413220",
    "end": "415199"
  },
  {
    "text": "depends on this we build the basic",
    "start": "415199",
    "end": "417539"
  },
  {
    "text": "ability of reserving and integrated it",
    "start": "417539",
    "end": "420900"
  },
  {
    "text": "into infrastructure of art group",
    "start": "420900",
    "end": "424800"
  },
  {
    "text": "you know that and each company has is",
    "start": "424800",
    "end": "428100"
  },
  {
    "text": "customer interest infrastructure so we",
    "start": "428100",
    "end": "431460"
  },
  {
    "text": "we need to adopt it",
    "start": "431460",
    "end": "433500"
  },
  {
    "text": "and in 2021 we we had we have another",
    "start": "433500",
    "end": "438300"
  },
  {
    "text": "scenario",
    "start": "438300",
    "end": "439740"
  },
  {
    "text": "online resource allocation",
    "start": "439740",
    "end": "442380"
  },
  {
    "text": "you can you can you can imagine that if",
    "start": "442380",
    "end": "445800"
  },
  {
    "text": "the",
    "start": "445800",
    "end": "446819"
  },
  {
    "text": "result is a useful payment and finance",
    "start": "446819",
    "end": "450599"
  },
  {
    "text": "what's the challenges",
    "start": "450599",
    "end": "452639"
  },
  {
    "text": "we're single challenges is that we need",
    "start": "452639",
    "end": "455400"
  },
  {
    "text": "a more flexible high performance and",
    "start": "455400",
    "end": "459000"
  },
  {
    "text": "scalable stable framework",
    "start": "459000",
    "end": "463979"
  },
  {
    "text": "serving",
    "start": "464280",
    "end": "467280"
  },
  {
    "text": "and in 2020 in 2022 we support the",
    "start": "470099",
    "end": "474960"
  },
  {
    "text": "large-scale online serverless platform",
    "start": "474960",
    "end": "477440"
  },
  {
    "text": "at this time we served models with 240",
    "start": "477440",
    "end": "483300"
  },
  {
    "text": "000 cores and our unit driven server is",
    "start": "483300",
    "end": "485940"
  },
  {
    "text": "per platform which is the 1.3 7 million",
    "start": "485940",
    "end": "490620"
  },
  {
    "text": "Peak TPS",
    "start": "490620",
    "end": "492479"
  },
  {
    "text": "okay that's all of our driver",
    "start": "492479",
    "end": "496220"
  },
  {
    "text": "private works next tune will introduce",
    "start": "496220",
    "end": "500580"
  },
  {
    "text": "our recent achievements this year",
    "start": "500580",
    "end": "505379"
  },
  {
    "text": "welcome",
    "start": "505379",
    "end": "506940"
  },
  {
    "text": "all right uh thanks kuya and hello",
    "start": "506940",
    "end": "510060"
  },
  {
    "text": "everyone",
    "start": "510060",
    "end": "511860"
  },
  {
    "text": "my name is Chong Li and I'm very glad to",
    "start": "511860",
    "end": "515159"
  },
  {
    "text": "be here to share our most recent",
    "start": "515159",
    "end": "517979"
  },
  {
    "text": "achievements in 2023",
    "start": "517979",
    "end": "520440"
  },
  {
    "text": "in N group we have already built the",
    "start": "520440",
    "end": "523020"
  },
  {
    "text": "largest influence platform our influence",
    "start": "523020",
    "end": "526260"
  },
  {
    "text": "clusters have 0.5 million CPU cores and",
    "start": "526260",
    "end": "529860"
  },
  {
    "text": "4K GPU cards in total and I believe you",
    "start": "529860",
    "end": "534060"
  },
  {
    "text": "guys have already seen these numbers at",
    "start": "534060",
    "end": "536700"
  },
  {
    "text": "Dr stoykers Kino speech yesterday",
    "start": "536700",
    "end": "539880"
  },
  {
    "text": "and to deliver this Hardware we are",
    "start": "539880",
    "end": "543360"
  },
  {
    "text": "using over 27 000 worker nodes in total",
    "start": "543360",
    "end": "546899"
  },
  {
    "text": "and as a influence platform uh we have a",
    "start": "546899",
    "end": "551220"
  },
  {
    "text": "very highly active model deployments in",
    "start": "551220",
    "end": "554220"
  },
  {
    "text": "our platform we have over 3000 new model",
    "start": "554220",
    "end": "557100"
  },
  {
    "text": "deployments every week and over 100",
    "start": "557100",
    "end": "560760"
  },
  {
    "text": "000 model updates every week",
    "start": "560760",
    "end": "563160"
  },
  {
    "text": "and our influence clusters are highly",
    "start": "563160",
    "end": "566820"
  },
  {
    "text": "Auto scalable so uh thanks to our",
    "start": "566820",
    "end": "569459"
  },
  {
    "text": "product types in the Standalone Auto",
    "start": "569459",
    "end": "572519"
  },
  {
    "text": "scaler our influence classes can now be",
    "start": "572519",
    "end": "576959"
  },
  {
    "text": "Auto scaled over three thousand times",
    "start": "576959",
    "end": "579480"
  },
  {
    "text": "every week proactively and by doing so",
    "start": "579480",
    "end": "583140"
  },
  {
    "text": "we have increased our average CPU",
    "start": "583140",
    "end": "586560"
  },
  {
    "text": "utilization by over 20 percent",
    "start": "586560",
    "end": "590600"
  },
  {
    "text": "so next I will talk about some new",
    "start": "591720",
    "end": "594420"
  },
  {
    "text": "scenarios in our Productions and some",
    "start": "594420",
    "end": "596760"
  },
  {
    "text": "new features we did",
    "start": "596760",
    "end": "599399"
  },
  {
    "text": "before getting into the details I like",
    "start": "599399",
    "end": "602459"
  },
  {
    "text": "to firstly show our overview about our",
    "start": "602459",
    "end": "606300"
  },
  {
    "text": "reserving architecture it is not much",
    "start": "606300",
    "end": "609360"
  },
  {
    "text": "different from the open source Reserve",
    "start": "609360",
    "end": "611959"
  },
  {
    "text": "in this architecture we have",
    "start": "611959",
    "end": "615720"
  },
  {
    "text": "serving keeper it receives all the",
    "start": "615720",
    "end": "619040"
  },
  {
    "text": "serving influence job submissions from",
    "start": "619040",
    "end": "622080"
  },
  {
    "text": "the user clients and it has to",
    "start": "622080",
    "end": "624720"
  },
  {
    "text": "distribute these serving jobs across our",
    "start": "624720",
    "end": "628380"
  },
  {
    "text": "serving clusters but for each of them",
    "start": "628380",
    "end": "631220"
  },
  {
    "text": "the serving keeper has to create a",
    "start": "631220",
    "end": "634500"
  },
  {
    "text": "corresponding application master in one",
    "start": "634500",
    "end": "637440"
  },
  {
    "text": "particular serving cluster",
    "start": "637440",
    "end": "639240"
  },
  {
    "text": "and this application Master will create",
    "start": "639240",
    "end": "642420"
  },
  {
    "text": "multiple proxies and deployment actors",
    "start": "642420",
    "end": "646200"
  },
  {
    "text": "within the cluster",
    "start": "646200",
    "end": "648540"
  },
  {
    "text": "and when the proxies are ready they will",
    "start": "648540",
    "end": "651600"
  },
  {
    "text": "register themselves to a independent",
    "start": "651600",
    "end": "655140"
  },
  {
    "text": "service Discovery component",
    "start": "655140",
    "end": "657300"
  },
  {
    "text": "and by subscribing to this component our",
    "start": "657300",
    "end": "661140"
  },
  {
    "text": "user applications can get to know the",
    "start": "661140",
    "end": "663899"
  },
  {
    "text": "locations of every proxy and carefully",
    "start": "663899",
    "end": "667140"
  },
  {
    "text": "pick the most suitable one to send their",
    "start": "667140",
    "end": "670399"
  },
  {
    "text": "inference requests",
    "start": "670399",
    "end": "673500"
  },
  {
    "text": "and in every proxy you have to assign",
    "start": "673500",
    "end": "678600"
  },
  {
    "text": "the incoming inference requests to use",
    "start": "678600",
    "end": "681720"
  },
  {
    "text": "local deployment actors where the model",
    "start": "681720",
    "end": "685140"
  },
  {
    "text": "serving work will be done there",
    "start": "685140",
    "end": "689000"
  },
  {
    "text": "okay so uh new scenarios the first one",
    "start": "690180",
    "end": "693420"
  },
  {
    "text": "is doing the model solving work on gpus",
    "start": "693420",
    "end": "696980"
  },
  {
    "text": "to make it happen we chose to use Nvidia",
    "start": "696980",
    "end": "700440"
  },
  {
    "text": "Triton for those of you unfamiliar with",
    "start": "700440",
    "end": "703339"
  },
  {
    "text": "Triton it is",
    "start": "703339",
    "end": "705740"
  },
  {
    "text": "nvidia's single node model influence",
    "start": "705740",
    "end": "709019"
  },
  {
    "text": "example it supports multiple influence",
    "start": "709019",
    "end": "712260"
  },
  {
    "text": "backends and",
    "start": "712260",
    "end": "714899"
  },
  {
    "text": "in our resolving system the Triton",
    "start": "714899",
    "end": "717540"
  },
  {
    "text": "servers could be very easily distributed",
    "start": "717540",
    "end": "719940"
  },
  {
    "text": "all we need to do is running the making",
    "start": "719940",
    "end": "724140"
  },
  {
    "text": "our deployment actor become a Triton",
    "start": "724140",
    "end": "727560"
  },
  {
    "text": "bootstrapper so whenever our application",
    "start": "727560",
    "end": "731040"
  },
  {
    "text": "Master creates a new deployment actor it",
    "start": "731040",
    "end": "734880"
  },
  {
    "text": "will immediately puts up the Triton",
    "start": "734880",
    "end": "737279"
  },
  {
    "text": "server inside and with help of our actor",
    "start": "737279",
    "end": "741899"
  },
  {
    "text": "long-term environment this long-term",
    "start": "741899",
    "end": "744720"
  },
  {
    "text": "environment feature is very useful it is",
    "start": "744720",
    "end": "748380"
  },
  {
    "text": "provided by the open source recall and",
    "start": "748380",
    "end": "751620"
  },
  {
    "text": "it helps a lot when we are dealing with",
    "start": "751620",
    "end": "754140"
  },
  {
    "text": "data dependency package or Library",
    "start": "754140",
    "end": "756959"
  },
  {
    "text": "dependency programs and I believe our",
    "start": "756959",
    "end": "760560"
  },
  {
    "text": "recall team contributed nearly half of",
    "start": "760560",
    "end": "763019"
  },
  {
    "text": "their implementations to the open source",
    "start": "763019",
    "end": "767600"
  },
  {
    "text": "um",
    "start": "767760",
    "end": "768779"
  },
  {
    "text": "right so uh when this trident servers",
    "start": "768779",
    "end": "771720"
  },
  {
    "text": "are ready they can choose to register",
    "start": "771720",
    "end": "773700"
  },
  {
    "text": "themselves uh to this discovery service",
    "start": "773700",
    "end": "776579"
  },
  {
    "text": "Discovery component and the upcoming",
    "start": "776579",
    "end": "780480"
  },
  {
    "text": "inference requests can be connected to",
    "start": "780480",
    "end": "783540"
  },
  {
    "text": "this uh Triton servers directly",
    "start": "783540",
    "end": "787079"
  },
  {
    "text": "and one thing to to mention that is that",
    "start": "787079",
    "end": "790260"
  },
  {
    "text": "because our serving cluster is highly",
    "start": "790260",
    "end": "793740"
  },
  {
    "text": "Auto scalable so when you run the Triton",
    "start": "793740",
    "end": "797160"
  },
  {
    "text": "servers in our platform they become Auto",
    "start": "797160",
    "end": "799860"
  },
  {
    "text": "scalable as well",
    "start": "799860",
    "end": "802760"
  },
  {
    "text": "okay next scenario is serving the llm",
    "start": "804120",
    "end": "808500"
  },
  {
    "text": "applications",
    "start": "808500",
    "end": "810139"
  },
  {
    "text": "the background of this thing is in our",
    "start": "810139",
    "end": "813959"
  },
  {
    "text": "end group we have a system called gbt",
    "start": "813959",
    "end": "817380"
  },
  {
    "text": "cache in this system we will try to",
    "start": "817380",
    "end": "821760"
  },
  {
    "text": "store the historical LM responses in our",
    "start": "821760",
    "end": "826019"
  },
  {
    "text": "vector or Cash Source so whenever there",
    "start": "826019",
    "end": "829800"
  },
  {
    "text": "is a new inference request coming in the",
    "start": "829800",
    "end": "832620"
  },
  {
    "text": "first thing we do is running a",
    "start": "832620",
    "end": "834600"
  },
  {
    "text": "similarity comparison so if the cache is",
    "start": "834600",
    "end": "837420"
  },
  {
    "text": "hit then we just respond the pre-store",
    "start": "837420",
    "end": "840540"
  },
  {
    "text": "responses back to the user and the EF",
    "start": "840540",
    "end": "843899"
  },
  {
    "text": "cache misses then the inference request",
    "start": "843899",
    "end": "847079"
  },
  {
    "text": "still has to go through the model",
    "start": "847079",
    "end": "849480"
  },
  {
    "text": "serving pipeline",
    "start": "849480",
    "end": "852620"
  },
  {
    "text": "and in our reserving platform this uh",
    "start": "853019",
    "end": "856760"
  },
  {
    "text": "this gbt cache system could be very",
    "start": "856760",
    "end": "860339"
  },
  {
    "text": "easily integrated all we need to do is",
    "start": "860339",
    "end": "863220"
  },
  {
    "text": "rounding this",
    "start": "863220",
    "end": "864839"
  },
  {
    "text": "GPT cache system inside our deployment",
    "start": "864839",
    "end": "868019"
  },
  {
    "text": "actor and still with the help of the",
    "start": "868019",
    "end": "870839"
  },
  {
    "text": "actor runtime feature",
    "start": "870839",
    "end": "874079"
  },
  {
    "text": "and",
    "start": "874079",
    "end": "875399"
  },
  {
    "text": "of course we need a Ingress actor in the",
    "start": "875399",
    "end": "878220"
  },
  {
    "text": "cluster and it will help us forward",
    "start": "878220",
    "end": "881040"
  },
  {
    "text": "every incoming inference request to the",
    "start": "881040",
    "end": "884519"
  },
  {
    "text": "uh",
    "start": "884519",
    "end": "885560"
  },
  {
    "text": "gbt cache active at the first place and",
    "start": "885560",
    "end": "889079"
  },
  {
    "text": "then we still do the same thing if cash",
    "start": "889079",
    "end": "890940"
  },
  {
    "text": "hit then we do a quick response if cache",
    "start": "890940",
    "end": "893940"
  },
  {
    "text": "misses then we just forward this request",
    "start": "893940",
    "end": "898399"
  },
  {
    "text": "to another Triton server which is also",
    "start": "898399",
    "end": "902160"
  },
  {
    "text": "running in a deployment actor",
    "start": "902160",
    "end": "906319"
  },
  {
    "text": "so when mixing can see here is in this",
    "start": "907139",
    "end": "911279"
  },
  {
    "text": "picture the GP gbd cache actors are",
    "start": "911279",
    "end": "915420"
  },
  {
    "text": "running on CPU nodes and the Trident",
    "start": "915420",
    "end": "918779"
  },
  {
    "text": "servers are running on a GPU nodes so",
    "start": "918779",
    "end": "922139"
  },
  {
    "text": "thanks to recourse",
    "start": "922139",
    "end": "924920"
  },
  {
    "text": "heterogeneous results scheduling we can",
    "start": "924920",
    "end": "928380"
  },
  {
    "text": "now distribute these different kinds of",
    "start": "928380",
    "end": "931320"
  },
  {
    "text": "actors more wisely and optimize the",
    "start": "931320",
    "end": "934740"
  },
  {
    "text": "overall results utilization",
    "start": "934740",
    "end": "938720"
  },
  {
    "text": "okay so next is uh some new features the",
    "start": "939480",
    "end": "942959"
  },
  {
    "text": "first one is the building async broker",
    "start": "942959",
    "end": "946519"
  },
  {
    "text": "we did this because in most of our model",
    "start": "946519",
    "end": "951120"
  },
  {
    "text": "serving scenarios we found the inference",
    "start": "951120",
    "end": "954660"
  },
  {
    "text": "requests have very different execution",
    "start": "954660",
    "end": "957480"
  },
  {
    "text": "time so for this long requests people",
    "start": "957480",
    "end": "961680"
  },
  {
    "text": "always have to pay a lot of",
    "start": "961680",
    "end": "964100"
  },
  {
    "text": "synchronized waiting overhead",
    "start": "964100",
    "end": "967079"
  },
  {
    "text": "so what we did here is deploying an",
    "start": "967079",
    "end": "970860"
  },
  {
    "text": "async broker in the serving cluster and",
    "start": "970860",
    "end": "974339"
  },
  {
    "text": "of course this is a async broker is",
    "start": "974339",
    "end": "976139"
  },
  {
    "text": "running in a deployment actor as well",
    "start": "976139",
    "end": "979380"
  },
  {
    "text": "and this async broker can receive in",
    "start": "979380",
    "end": "982620"
  },
  {
    "text": "queue every incoming inference request",
    "start": "982620",
    "end": "985820"
  },
  {
    "text": "and for this long request when the",
    "start": "985820",
    "end": "989399"
  },
  {
    "text": "results are ready the async broker can",
    "start": "989399",
    "end": "992040"
  },
  {
    "text": "help us returning this results back to",
    "start": "992040",
    "end": "995760"
  },
  {
    "text": "the users asynchronically",
    "start": "995760",
    "end": "999899"
  },
  {
    "text": "in the one big fit from this async",
    "start": "1001220",
    "end": "1004820"
  },
  {
    "text": "broker is that",
    "start": "1004820",
    "end": "1007339"
  },
  {
    "text": "with this inside the cluster the other",
    "start": "1007339",
    "end": "1010699"
  },
  {
    "text": "deployment actors can actually pour",
    "start": "1010699",
    "end": "1014600"
  },
  {
    "text": "their inference requests from this",
    "start": "1014600",
    "end": "1017060"
  },
  {
    "text": "broker adaptively",
    "start": "1017060",
    "end": "1019360"
  },
  {
    "text": "based on their own busy or idle status",
    "start": "1019360",
    "end": "1023559"
  },
  {
    "text": "so by doing so in every deployment actor",
    "start": "1023559",
    "end": "1028280"
  },
  {
    "text": "which is running the model serving with",
    "start": "1028280",
    "end": "1031400"
  },
  {
    "text": "Triton server in each of these",
    "start": "1031400",
    "end": "1033678"
  },
  {
    "text": "deployment actor we can see much less",
    "start": "1033679",
    "end": "1036620"
  },
  {
    "text": "head-of-line delays",
    "start": "1036620",
    "end": "1039880"
  },
  {
    "text": "so in our evaluation compared to the",
    "start": "1040040",
    "end": "1043839"
  },
  {
    "text": "Baseline which is a wrong Robin",
    "start": "1043839",
    "end": "1046360"
  },
  {
    "text": "push-based",
    "start": "1046360",
    "end": "1047918"
  },
  {
    "text": "request assignment our poor based method",
    "start": "1047919",
    "end": "1052580"
  },
  {
    "text": "can give us two times better throughput",
    "start": "1052580",
    "end": "1056740"
  },
  {
    "text": "uh the next feature is C plus plus",
    "start": "1058820",
    "end": "1061480"
  },
  {
    "text": "deployment uh this one this feature has",
    "start": "1061480",
    "end": "1065000"
  },
  {
    "text": "been widely deployed in our",
    "start": "1065000",
    "end": "1066799"
  },
  {
    "text": "recommendation and advertising Services",
    "start": "1066799",
    "end": "1070179"
  },
  {
    "text": "these services are latency sensitive",
    "start": "1070179",
    "end": "1073220"
  },
  {
    "text": "with high throughput so because of this",
    "start": "1073220",
    "end": "1076520"
  },
  {
    "text": "performance requirements our deployment",
    "start": "1076520",
    "end": "1080120"
  },
  {
    "text": "actors has to be high performance as",
    "start": "1080120",
    "end": "1083480"
  },
  {
    "text": "well",
    "start": "1083480",
    "end": "1084620"
  },
  {
    "text": "so what we did is using C plus",
    "start": "1084620",
    "end": "1088419"
  },
  {
    "text": "deployment actors based on open open",
    "start": "1088419",
    "end": "1092960"
  },
  {
    "text": "source rates C plus workers and by the",
    "start": "1092960",
    "end": "1096500"
  },
  {
    "text": "way this C plus workers is also mainly",
    "start": "1096500",
    "end": "1100460"
  },
  {
    "text": "contributed by our array team so thanks",
    "start": "1100460",
    "end": "1102440"
  },
  {
    "text": "Google and for his effort on this",
    "start": "1102440",
    "end": "1105860"
  },
  {
    "text": "okay so uh with this C plus plus",
    "start": "1105860",
    "end": "1109700"
  },
  {
    "text": "deploying actors in hand",
    "start": "1109700",
    "end": "1111700"
  },
  {
    "text": "one big thing we did is implementing a c",
    "start": "1111700",
    "end": "1116179"
  },
  {
    "text": "plus direct Ingress this is a high",
    "start": "1116179",
    "end": "1119240"
  },
  {
    "text": "performance RPC service and with this",
    "start": "1119240",
    "end": "1121880"
  },
  {
    "text": "feature our inference request can",
    "start": "1121880",
    "end": "1125240"
  },
  {
    "text": "directly connected to our c plus",
    "start": "1125240",
    "end": "1127700"
  },
  {
    "text": "deployment actors bypassing all these",
    "start": "1127700",
    "end": "1131260"
  },
  {
    "text": "proxies that I have been mentioning",
    "start": "1131260",
    "end": "1133400"
  },
  {
    "text": "about",
    "start": "1133400",
    "end": "1135700"
  },
  {
    "text": "and another big thing is native Triton",
    "start": "1136460",
    "end": "1139940"
  },
  {
    "text": "influence call this thing can be done",
    "start": "1139940",
    "end": "1142520"
  },
  {
    "text": "because we now have these C plus plus",
    "start": "1142520",
    "end": "1145059"
  },
  {
    "text": "deploying actors and the Triton server",
    "start": "1145059",
    "end": "1149539"
  },
  {
    "text": "which can be considered as a C plus plus",
    "start": "1149539",
    "end": "1152120"
  },
  {
    "text": "Library could be more closely and",
    "start": "1152120",
    "end": "1155380"
  },
  {
    "text": "efficiently integrated without any",
    "start": "1155380",
    "end": "1158120"
  },
  {
    "text": "cross-language overhead",
    "start": "1158120",
    "end": "1160220"
  },
  {
    "text": "and when we are making the C plus plus",
    "start": "1160220",
    "end": "1165140"
  },
  {
    "text": "deployment actors working together with",
    "start": "1165140",
    "end": "1168320"
  },
  {
    "text": "other C plus plus components in our",
    "start": "1168320",
    "end": "1171520"
  },
  {
    "text": "recommendation and advertising timelines",
    "start": "1171520",
    "end": "1174880"
  },
  {
    "text": "and we are actually doing a more",
    "start": "1174880",
    "end": "1178039"
  },
  {
    "text": "holistic distributed system",
    "start": "1178039",
    "end": "1182080"
  },
  {
    "text": "okay next I will talk about some future",
    "start": "1183740",
    "end": "1187280"
  },
  {
    "text": "future plans",
    "start": "1187280",
    "end": "1188780"
  },
  {
    "text": "the first thing we are going to do is",
    "start": "1188780",
    "end": "1192200"
  },
  {
    "text": "Charlie deployment uh",
    "start": "1192200",
    "end": "1195500"
  },
  {
    "text": "the motivation for this thing is in our",
    "start": "1195500",
    "end": "1198559"
  },
  {
    "text": "recommendation and search services with",
    "start": "1198559",
    "end": "1201679"
  },
  {
    "text": "CDR or CTR models we can always sing see",
    "start": "1201679",
    "end": "1206500"
  },
  {
    "text": "numerous items with large feature size",
    "start": "1206500",
    "end": "1210559"
  },
  {
    "text": "feature data and this feature data size",
    "start": "1210559",
    "end": "1214280"
  },
  {
    "text": "is too large to be fit in any single",
    "start": "1214280",
    "end": "1218780"
  },
  {
    "text": "worker node so what we",
    "start": "1218780",
    "end": "1222260"
  },
  {
    "text": "have to do is trying to shut these large",
    "start": "1222260",
    "end": "1226340"
  },
  {
    "text": "size feature data across multiple worker",
    "start": "1226340",
    "end": "1229400"
  },
  {
    "text": "nodes",
    "start": "1229400",
    "end": "1230299"
  },
  {
    "text": "and of course we will create a proxy",
    "start": "1230299",
    "end": "1234559"
  },
  {
    "text": "deployment actor actually and use this",
    "start": "1234559",
    "end": "1238880"
  },
  {
    "text": "proxy to help us do a shut away a",
    "start": "1238880",
    "end": "1243679"
  },
  {
    "text": "routing strategy and make sure every",
    "start": "1243679",
    "end": "1246200"
  },
  {
    "text": "incoming",
    "start": "1246200",
    "end": "1247480"
  },
  {
    "text": "inference request could be forwarded to",
    "start": "1247480",
    "end": "1250640"
  },
  {
    "text": "a specific node with the data dependency",
    "start": "1250640",
    "end": "1254179"
  },
  {
    "text": "locally available",
    "start": "1254179",
    "end": "1257320"
  },
  {
    "text": "okay so the last thing is actually a",
    "start": "1259520",
    "end": "1263480"
  },
  {
    "text": "very big picture we are hoping to",
    "start": "1263480",
    "end": "1266660"
  },
  {
    "text": "deliver our high performance General AI",
    "start": "1266660",
    "end": "1269660"
  },
  {
    "text": "framework",
    "start": "1269660",
    "end": "1272440"
  },
  {
    "text": "to make it happen the first thing we are",
    "start": "1272440",
    "end": "1276080"
  },
  {
    "text": "going to do is keep constructing and",
    "start": "1276080",
    "end": "1279520"
  },
  {
    "text": "polishing our race serving platform",
    "start": "1279520",
    "end": "1282799"
  },
  {
    "text": "based on open source Reserve which has",
    "start": "1282799",
    "end": "1286580"
  },
  {
    "text": "been proven to be highly distributed",
    "start": "1286580",
    "end": "1290200"
  },
  {
    "text": "multi-language supportive and scalable",
    "start": "1290200",
    "end": "1293539"
  },
  {
    "text": "and we do believe this could be a very",
    "start": "1293539",
    "end": "1296000"
  },
  {
    "text": "good foundation for many of the models",
    "start": "1296000",
    "end": "1299240"
  },
  {
    "text": "serving scenarios",
    "start": "1299240",
    "end": "1302360"
  },
  {
    "text": "uh we will definitely use our direct",
    "start": "1302360",
    "end": "1305720"
  },
  {
    "text": "Ingress",
    "start": "1305720",
    "end": "1307000"
  },
  {
    "text": "and use it to build a high performance",
    "start": "1307000",
    "end": "1310159"
  },
  {
    "text": "RPC service",
    "start": "1310159",
    "end": "1312799"
  },
  {
    "text": "We Will We Will widely use the C plus",
    "start": "1312799",
    "end": "1316400"
  },
  {
    "text": "plus deployment actors because it can",
    "start": "1316400",
    "end": "1320480"
  },
  {
    "text": "give us high performance Computing",
    "start": "1320480",
    "end": "1322820"
  },
  {
    "text": "capability",
    "start": "1322820",
    "end": "1325658"
  },
  {
    "text": "and we will start working on this",
    "start": "1325700",
    "end": "1328039"
  },
  {
    "text": "Charlotte deployment which give us a",
    "start": "1328039",
    "end": "1332120"
  },
  {
    "text": "high performance local data access and",
    "start": "1332120",
    "end": "1335059"
  },
  {
    "text": "data retrieval and we think this feature",
    "start": "1335059",
    "end": "1339320"
  },
  {
    "text": "could be very critical when we are",
    "start": "1339320",
    "end": "1341720"
  },
  {
    "text": "dealing with large-scale model serving",
    "start": "1341720",
    "end": "1344620"
  },
  {
    "text": "problems in the future",
    "start": "1344620",
    "end": "1348039"
  },
  {
    "text": "so uh that's basically all we have for",
    "start": "1348500",
    "end": "1351380"
  },
  {
    "text": "today so and as I said in the beginning",
    "start": "1351380",
    "end": "1354679"
  },
  {
    "text": "this work is mostly done by tongue from",
    "start": "1354679",
    "end": "1358159"
  },
  {
    "text": "our body team so",
    "start": "1358159",
    "end": "1359919"
  },
  {
    "text": "we will try our best to answer questions",
    "start": "1359919",
    "end": "1362360"
  },
  {
    "text": "here but if we cannot uh I do uh",
    "start": "1362360",
    "end": "1366679"
  },
  {
    "text": "encourage you guys send emails to this",
    "start": "1366679",
    "end": "1368900"
  },
  {
    "text": "address and I believe he can give the",
    "start": "1368900",
    "end": "1372320"
  },
  {
    "text": "most detailed answers thank you",
    "start": "1372320",
    "end": "1376220"
  },
  {
    "text": "foreign",
    "start": "1376220",
    "end": "1379220"
  },
  {
    "text": "so the direct Ingress and the C plus",
    "start": "1383140",
    "end": "1385640"
  },
  {
    "text": "plus work that you have done uh for how",
    "start": "1385640",
    "end": "1388039"
  },
  {
    "text": "much of that is integrated into the",
    "start": "1388039",
    "end": "1389840"
  },
  {
    "text": "record",
    "start": "1389840",
    "end": "1392000"
  },
  {
    "text": "very open source it's already open",
    "start": "1392000",
    "end": "1394159"
  },
  {
    "text": "sourced uh but it's not very popular",
    "start": "1394159",
    "end": "1398600"
  },
  {
    "text": "actually in the open source Community",
    "start": "1398600",
    "end": "1400400"
  },
  {
    "text": "but we use that a lot in our Productions",
    "start": "1400400",
    "end": "1403039"
  },
  {
    "text": "because in the in the recommendation and",
    "start": "1403039",
    "end": "1406419"
  },
  {
    "text": "advertising systems because we hope to",
    "start": "1406419",
    "end": "1410299"
  },
  {
    "text": "make all components in our pipeline the",
    "start": "1410299",
    "end": "1414080"
  },
  {
    "text": "recommendation timeline are all",
    "start": "1414080",
    "end": "1416059"
  },
  {
    "text": "implemented in C Pass Plus and making",
    "start": "1416059",
    "end": "1418760"
  },
  {
    "text": "the whole pipeline more holistic",
    "start": "1418760",
    "end": "1422260"
  },
  {
    "text": "one follow-up question on that is that",
    "start": "1422720",
    "end": "1424840"
  },
  {
    "text": "in that space that you just mentioned",
    "start": "1424840",
    "end": "1427700"
  },
  {
    "text": "which is the recommendation",
    "start": "1427700",
    "end": "1429280"
  },
  {
    "text": "pipeline how",
    "start": "1429280",
    "end": "1431900"
  },
  {
    "text": "like what are you really using Ray for",
    "start": "1431900",
    "end": "1434299"
  },
  {
    "text": "because using Triton for inference right",
    "start": "1434299",
    "end": "1436360"
  },
  {
    "text": "and you're using this direct Ingress but",
    "start": "1436360",
    "end": "1438860"
  },
  {
    "text": "what role what what role is replaying",
    "start": "1438860",
    "end": "1441679"
  },
  {
    "text": "there what how is Ray helping you uh in",
    "start": "1441679",
    "end": "1444559"
  },
  {
    "text": "that space",
    "start": "1444559",
    "end": "1445400"
  },
  {
    "text": "yeah we definitely use race uh uh",
    "start": "1445400",
    "end": "1449840"
  },
  {
    "text": "runtime environment to help us to",
    "start": "1449840",
    "end": "1452600"
  },
  {
    "text": "integrate this Triton dependency in our",
    "start": "1452600",
    "end": "1456799"
  },
  {
    "text": "uh",
    "start": "1456799",
    "end": "1458260"
  },
  {
    "text": "deployment actor and we use race uh uh",
    "start": "1458260",
    "end": "1463880"
  },
  {
    "text": "heterogeneous uh uh scheduling",
    "start": "1463880",
    "end": "1467179"
  },
  {
    "text": "to help us schedule these actors CPU",
    "start": "1467179",
    "end": "1471320"
  },
  {
    "text": "depend actors or GPU dependent actors",
    "start": "1471320",
    "end": "1473659"
  },
  {
    "text": "they can all be scheduled well and help",
    "start": "1473659",
    "end": "1476600"
  },
  {
    "text": "us increase the results utilization",
    "start": "1476600",
    "end": "1481600"
  },
  {
    "text": "yeah thank you for the talk quick",
    "start": "1487039",
    "end": "1488780"
  },
  {
    "text": "question so you mentioned that uh they",
    "start": "1488780",
    "end": "1491240"
  },
  {
    "text": "are about 3 000 plus deployments model",
    "start": "1491240",
    "end": "1494900"
  },
  {
    "text": "deployment per week yeah and my question",
    "start": "1494900",
    "end": "1498080"
  },
  {
    "text": "is for one type of model like what's the",
    "start": "1498080",
    "end": "1501980"
  },
  {
    "text": "like",
    "start": "1501980",
    "end": "1503080"
  },
  {
    "text": "average or usual frequency for",
    "start": "1503080",
    "end": "1506179"
  },
  {
    "text": "deployments let's say we have a large",
    "start": "1506179",
    "end": "1509360"
  },
  {
    "text": "language model and then how often do you",
    "start": "1509360",
    "end": "1512840"
  },
  {
    "text": "do updates and deployment",
    "start": "1512840",
    "end": "1516460"
  },
  {
    "text": "yeah I guess there's a range you can you",
    "start": "1516740",
    "end": "1519500"
  },
  {
    "text": "got me here I don't really know the",
    "start": "1519500",
    "end": "1522320"
  },
  {
    "text": "details yeah so that question that's",
    "start": "1522320",
    "end": "1524659"
  },
  {
    "text": "what you you should send emails",
    "start": "1524659",
    "end": "1528140"
  },
  {
    "text": "right because there's some details from",
    "start": "1528140",
    "end": "1530240"
  },
  {
    "text": "their serving team",
    "start": "1530240",
    "end": "1533080"
  },
  {
    "text": "it's Australia deployment related to",
    "start": "1537500",
    "end": "1539960"
  },
  {
    "text": "your choice of Ray",
    "start": "1539960",
    "end": "1541760"
  },
  {
    "text": "the Charlotte deploying what yes is the",
    "start": "1541760",
    "end": "1544220"
  },
  {
    "text": "Charlotte deployment related to the",
    "start": "1544220",
    "end": "1546200"
  },
  {
    "text": "choice of Ray",
    "start": "1546200",
    "end": "1547880"
  },
  {
    "text": "it's yes it's still on right we use we",
    "start": "1547880",
    "end": "1551539"
  },
  {
    "text": "just need to shut this uh feature data",
    "start": "1551539",
    "end": "1555320"
  },
  {
    "text": "across multiple worker nodes but the",
    "start": "1555320",
    "end": "1557539"
  },
  {
    "text": "worker nodes still is built on Ray and",
    "start": "1557539",
    "end": "1560179"
  },
  {
    "text": "all the actors and are handled are are",
    "start": "1560179",
    "end": "1563659"
  },
  {
    "text": "reactors",
    "start": "1563659",
    "end": "1566440"
  },
  {
    "text": "so all of this is running on top of like",
    "start": "1573679",
    "end": "1575720"
  },
  {
    "text": "VMS right not kubernetes or anything",
    "start": "1575720",
    "end": "1577700"
  },
  {
    "text": "else in between just bear bien uh MC1 is",
    "start": "1577700",
    "end": "1581779"
  },
  {
    "text": "this all just running on like VMS or is",
    "start": "1581779",
    "end": "1584600"
  },
  {
    "text": "it running on kubernetes or is it",
    "start": "1584600",
    "end": "1586039"
  },
  {
    "text": "running on it's like yeah yeah we are we",
    "start": "1586039",
    "end": "1587960"
  },
  {
    "text": "are definitely running our really",
    "start": "1587960",
    "end": "1589640"
  },
  {
    "text": "classic clusters in the uh Cloud native",
    "start": "1589640",
    "end": "1592760"
  },
  {
    "text": "environment on top of kubernetes",
    "start": "1592760",
    "end": "1597039"
  },
  {
    "text": "well if there are no more questions then",
    "start": "1603799",
    "end": "1606860"
  },
  {
    "text": "thank you so much",
    "start": "1606860",
    "end": "1608360"
  },
  {
    "text": "okay thank you guys",
    "start": "1608360",
    "end": "1609640"
  },
  {
    "text": "[Applause]",
    "start": "1609640",
    "end": "1613089"
  }
]