[
  {
    "text": "hi I'm fanai so I'm Tech leader of a go",
    "start": "3679",
    "end": "6399"
  },
  {
    "text": "Cloud ml computer INF team so I will",
    "start": "6399",
    "end": "10519"
  },
  {
    "text": "spend next few minutes talk about how we",
    "start": "10519",
    "end": "13120"
  },
  {
    "text": "do the high performance ml serving with",
    "start": "13120",
    "end": "16039"
  },
  {
    "text": "the r TPU and",
    "start": "16039",
    "end": "18119"
  },
  {
    "text": "G yeah here first let's see okay what",
    "start": "18119",
    "end": "22640"
  },
  {
    "text": "where our stack located first if you see",
    "start": "22640",
    "end": "25320"
  },
  {
    "text": "the bottom right is Hardware is TPU GPU",
    "start": "25320",
    "end": "28320"
  },
  {
    "text": "is memory your hbm",
    "start": "28320",
    "end": "30560"
  },
  {
    "text": "and your suram L1 L2 cach and then your",
    "start": "30560",
    "end": "33760"
  },
  {
    "text": "computer this is your flops and then",
    "start": "33760",
    "end": "35840"
  },
  {
    "text": "your IO side PCI network infinite band",
    "start": "35840",
    "end": "39800"
  },
  {
    "text": "and all M link or RCI so this is",
    "start": "39800",
    "end": "42960"
  },
  {
    "text": "hardware and what is user expectation",
    "start": "42960",
    "end": "46199"
  },
  {
    "text": "user exp first is high performance the",
    "start": "46199",
    "end": "48960"
  },
  {
    "text": "low latency and high accuracy the",
    "start": "48960",
    "end": "51520"
  },
  {
    "text": "inference output and here we located we",
    "start": "51520",
    "end": "54359"
  },
  {
    "text": "are in the software we are end to end",
    "start": "54359",
    "end": "56760"
  },
  {
    "text": "INF deack so we leverage the hardware",
    "start": "56760",
    "end": "60079"
  },
  {
    "text": "and we output the best performance low",
    "start": "60079",
    "end": "62800"
  },
  {
    "text": "latency and high accuracy result for our",
    "start": "62800",
    "end": "67680"
  },
  {
    "text": "users yeah here is our main project the",
    "start": "68360",
    "end": "71680"
  },
  {
    "text": "jream engine uh here I'm give you very",
    "start": "71680",
    "end": "75159"
  },
  {
    "text": "high level 1,000 square fet High",
    "start": "75159",
    "end": "78439"
  },
  {
    "text": "overview how we do the optimization the",
    "start": "78439",
    "end": "80799"
  },
  {
    "text": "first one we do the Contin bashing as",
    "start": "80799",
    "end": "82920"
  },
  {
    "text": "long as the request come in we will",
    "start": "82920",
    "end": "84560"
  },
  {
    "text": "serve the request and uh we do the",
    "start": "84560",
    "end": "87280"
  },
  {
    "text": "parallel compute we have the Tens parm",
    "start": "87280",
    "end": "89720"
  },
  {
    "text": "dat periodism and we are planning doing",
    "start": "89720",
    "end": "91920"
  },
  {
    "text": "the peline periodism and this side we",
    "start": "91920",
    "end": "94640"
  },
  {
    "text": "right now have the in four and in8",
    "start": "94640",
    "end": "98240"
  },
  {
    "text": "quation uh another two is about",
    "start": "98240",
    "end": "100560"
  },
  {
    "text": "attention kernels yeah thanks for the",
    "start": "100560",
    "end": "102479"
  },
  {
    "text": "research for this good good papers we",
    "start": "102479",
    "end": "104719"
  },
  {
    "text": "have the flash attention and patient",
    "start": "104719",
    "end": "106719"
  },
  {
    "text": "attention and if the user or our",
    "start": "106719",
    "end": "109040"
  },
  {
    "text": "engineer want the blockwise computer we",
    "start": "109040",
    "end": "111560"
  },
  {
    "text": "do have the pal kernel and another",
    "start": "111560",
    "end": "114640"
  },
  {
    "text": "optimization okay we do a lot of",
    "start": "114640",
    "end": "116960"
  },
  {
    "text": "communication and computer overlap like",
    "start": "116960",
    "end": "120000"
  },
  {
    "text": "how can we push from the hbm to the CPU",
    "start": "120000",
    "end": "123079"
  },
  {
    "text": "dram how can we overlap the",
    "start": "123079",
    "end": "125399"
  },
  {
    "text": "communication from the in blog to the in",
    "start": "125399",
    "end": "127600"
  },
  {
    "text": "plus one block and another one is about",
    "start": "127600",
    "end": "130000"
  },
  {
    "text": "how can we scale to the multiple host I",
    "start": "130000",
    "end": "132640"
  },
  {
    "text": "will talk more on the multiple host in",
    "start": "132640",
    "end": "134800"
  },
  {
    "text": "next slide yeah talk is cheap show me",
    "start": "134800",
    "end": "137360"
  },
  {
    "text": "the code here is our engine report yeah",
    "start": "137360",
    "end": "140239"
  },
  {
    "text": "please feel free to check our engine",
    "start": "140239",
    "end": "142440"
  },
  {
    "text": "report if you like please give us a",
    "start": "142440",
    "end": "146519"
  },
  {
    "text": "start yeah so next slide I give you the",
    "start": "146920",
    "end": "150040"
  },
  {
    "text": "ground so KV cach is one of",
    "start": "150040",
    "end": "152120"
  },
  {
    "text": "authorization to save the Redundant",
    "start": "152120",
    "end": "155319"
  },
  {
    "text": "computation so I give the comparation",
    "start": "155319",
    "end": "157760"
  },
  {
    "text": "right on the first half you do the",
    "start": "157760",
    "end": "160599"
  },
  {
    "text": "computer without KV cach on the bottom",
    "start": "160599",
    "end": "163000"
  },
  {
    "text": "side we reuse the KV cach with you can",
    "start": "163000",
    "end": "166280"
  },
  {
    "text": "see the difference right so with the",
    "start": "166280",
    "end": "168319"
  },
  {
    "text": "about side even your current token is Q3",
    "start": "168319",
    "end": "172400"
  },
  {
    "text": "you still need to process all the",
    "start": "172400",
    "end": "174120"
  },
  {
    "text": "previous token but with QV cash right",
    "start": "174120",
    "end": "176959"
  },
  {
    "text": "what we need do is only the Q3 token so",
    "start": "176959",
    "end": "179840"
  },
  {
    "text": "so this save the true minutes of a",
    "start": "179840",
    "end": "184159"
  },
  {
    "text": "computation uh another to SL share the",
    "start": "186720",
    "end": "189440"
  },
  {
    "text": "difference between the stack bashing and",
    "start": "189440",
    "end": "191840"
  },
  {
    "text": "bashing so stack Bing is very easy to",
    "start": "191840",
    "end": "195519"
  },
  {
    "text": "implement so you with all the request",
    "start": "195519",
    "end": "197680"
  },
  {
    "text": "ready and you compute after everything",
    "start": "197680",
    "end": "200280"
  },
  {
    "text": "finished you release the result so but",
    "start": "200280",
    "end": "203000"
  },
  {
    "text": "the things if you see the white part",
    "start": "203000",
    "end": "204640"
  },
  {
    "text": "right this is computer Idol so two",
    "start": "204640",
    "end": "207239"
  },
  {
    "text": "things to the computer Idol I need to",
    "start": "207239",
    "end": "209159"
  },
  {
    "text": "with every request be ared and second",
    "start": "209159",
    "end": "211680"
  },
  {
    "text": "one I needed to release the result until",
    "start": "211680",
    "end": "214560"
  },
  {
    "text": "the longest sequence to be finished so",
    "start": "214560",
    "end": "217080"
  },
  {
    "text": "you can see almost have them they are",
    "start": "217080",
    "end": "218879"
  },
  {
    "text": "compute",
    "start": "218879",
    "end": "220439"
  },
  {
    "text": "Idol so with continue Bing wrer once the",
    "start": "220439",
    "end": "223760"
  },
  {
    "text": "request come in I can process as soon as",
    "start": "223760",
    "end": "226760"
  },
  {
    "text": "possible and the second one is that",
    "start": "226760",
    "end": "229000"
  },
  {
    "text": "after the short thing finish it will",
    "start": "229000",
    "end": "231640"
  },
  {
    "text": "release that result for that slot if you",
    "start": "231640",
    "end": "234280"
  },
  {
    "text": "see the white part with can B there's",
    "start": "234280",
    "end": "237280"
  },
  {
    "text": "almost no computer Idol",
    "start": "237280",
    "end": "241720"
  },
  {
    "text": "yeah so for the next slide I will show",
    "start": "242760",
    "end": "245959"
  },
  {
    "text": "how we do the scale to the multiple host",
    "start": "245959",
    "end": "249079"
  },
  {
    "text": "currently there is three skill there are",
    "start": "249079",
    "end": "251319"
  },
  {
    "text": "several skills first the motor size is",
    "start": "251319",
    "end": "254760"
  },
  {
    "text": "billions and the count length right from",
    "start": "254760",
    "end": "256919"
  },
  {
    "text": "1K to 1 million and latest people could",
    "start": "256919",
    "end": "260120"
  },
  {
    "text": "even do 10 million or unlimited sequence",
    "start": "260120",
    "end": "263960"
  },
  {
    "text": "and another one is that okay to utilize",
    "start": "263960",
    "end": "266720"
  },
  {
    "text": "the computer FL the engineer use large b",
    "start": "266720",
    "end": "269600"
  },
  {
    "text": "side this cause another memory pressure",
    "start": "269600",
    "end": "272240"
  },
  {
    "text": "so we need a multiple host give you one",
    "start": "272240",
    "end": "274240"
  },
  {
    "text": "example right for the latest llama",
    "start": "274240",
    "end": "277039"
  },
  {
    "text": "345b if we use the V5 4 as one host we",
    "start": "277039",
    "end": "281320"
  },
  {
    "text": "need 16 this host VM to serve the Llama",
    "start": "281320",
    "end": "287600"
  },
  {
    "text": "345b yeah uh since this is res submit um",
    "start": "288880",
    "end": "293199"
  },
  {
    "text": "with the re plus just dream we can",
    "start": "293199",
    "end": "295639"
  },
  {
    "text": "support the multiple host three",
    "start": "295639",
    "end": "297919"
  },
  {
    "text": "dimension here we want the high",
    "start": "297919",
    "end": "300240"
  },
  {
    "text": "inform informance performance and also",
    "start": "300240",
    "end": "303160"
  },
  {
    "text": "the good usability and also henus so how",
    "start": "303160",
    "end": "306400"
  },
  {
    "text": "can we do that right so on the",
    "start": "306400",
    "end": "308479"
  },
  {
    "text": "performance side we are native J",
    "start": "308479",
    "end": "310400"
  },
  {
    "text": "multiple controller is same as you run",
    "start": "310400",
    "end": "312560"
  },
  {
    "text": "the ja on the one chip same performance",
    "start": "312560",
    "end": "315280"
  },
  {
    "text": "and on the US b side right both re and",
    "start": "315280",
    "end": "317759"
  },
  {
    "text": "CH stream are open source if you find",
    "start": "317759",
    "end": "319479"
  },
  {
    "text": "the issue you can go there check code or",
    "start": "319479",
    "end": "322199"
  },
  {
    "text": "or open a issue and then the engineer",
    "start": "322199",
    "end": "324160"
  },
  {
    "text": "will help you and from the single to",
    "start": "324160",
    "end": "326639"
  },
  {
    "text": "multiple H you don't need to change any",
    "start": "326639",
    "end": "328240"
  },
  {
    "text": "code it's just a flag change",
    "start": "328240",
    "end": "330280"
  },
  {
    "text": "and on the henus side you can run on CPU",
    "start": "330280",
    "end": "333639"
  },
  {
    "text": "TPU and you can also run on the GPU",
    "start": "333639",
    "end": "337160"
  },
  {
    "text": "side uh for next two slide I will show",
    "start": "337160",
    "end": "340120"
  },
  {
    "text": "the high level arure how we do the",
    "start": "340120",
    "end": "342319"
  },
  {
    "text": "multiple",
    "start": "342319",
    "end": "343720"
  },
  {
    "text": "host yeah first one is inter leave",
    "start": "343720",
    "end": "346840"
  },
  {
    "text": "serving so when we talk about inter",
    "start": "346840",
    "end": "348639"
  },
  {
    "text": "leave serving is about we do the pref",
    "start": "348639",
    "end": "352080"
  },
  {
    "text": "and we do the decoder in the same host",
    "start": "352080",
    "end": "354360"
  },
  {
    "text": "and the same chips here I'm show the",
    "start": "354360",
    "end": "357199"
  },
  {
    "text": "example how we do with the Jacks so",
    "start": "357199",
    "end": "359440"
  },
  {
    "text": "there is three layers the top side is",
    "start": "359440",
    "end": "361560"
  },
  {
    "text": "non P so this you can with your jack",
    "start": "361560",
    "end": "364280"
  },
  {
    "text": "engine or with your py toch engine and",
    "start": "364280",
    "end": "366360"
  },
  {
    "text": "in the bot sign is something like you",
    "start": "366360",
    "end": "368360"
  },
  {
    "text": "need to compile your code to the machine",
    "start": "368360",
    "end": "371759"
  },
  {
    "text": "code and in the between is just code to",
    "start": "371759",
    "end": "374520"
  },
  {
    "text": "do all together to collect all those",
    "start": "374520",
    "end": "376680"
  },
  {
    "text": "things from the different",
    "start": "376680",
    "end": "379919"
  },
  {
    "text": "host yeah uh next slide is a discretion",
    "start": "381240",
    "end": "385520"
  },
  {
    "text": "serving if we have inter leave serving",
    "start": "385520",
    "end": "388120"
  },
  {
    "text": "why we need discretion Serv serving",
    "start": "388120",
    "end": "390319"
  },
  {
    "text": "because Inter leave serving is good you",
    "start": "390319",
    "end": "392639"
  },
  {
    "text": "can do the pref and decode in the single",
    "start": "392639",
    "end": "395800"
  },
  {
    "text": "chips or the single host or multiple H",
    "start": "395800",
    "end": "398240"
  },
  {
    "text": "but they need do together once you",
    "start": "398240",
    "end": "399800"
  },
  {
    "text": "finish the prefill you do decod then the",
    "start": "399800",
    "end": "402720"
  },
  {
    "text": "prefill but you cannot optimize only for",
    "start": "402720",
    "end": "405440"
  },
  {
    "text": "the prefill or decode because they are",
    "start": "405440",
    "end": "407400"
  },
  {
    "text": "bound together if you want to optimize",
    "start": "407400",
    "end": "409560"
  },
  {
    "text": "the prefill latency or you focus on the",
    "start": "409560",
    "end": "412720"
  },
  {
    "text": "decoder you need a separate them do them",
    "start": "412720",
    "end": "415400"
  },
  {
    "text": "separately so uh another M change if we",
    "start": "415400",
    "end": "419199"
  },
  {
    "text": "need a to the distribution serving right",
    "start": "419199",
    "end": "421680"
  },
  {
    "text": "so it's about the KV cach you need to",
    "start": "421680",
    "end": "424840"
  },
  {
    "text": "transfer your KV cach from this pref",
    "start": "424840",
    "end": "428360"
  },
  {
    "text": "host to the decoder host so we leverage",
    "start": "428360",
    "end": "430919"
  },
  {
    "text": "the re Object Store to transfer this",
    "start": "430919",
    "end": "433400"
  },
  {
    "text": "pref cach and we have the re engine to",
    "start": "433400",
    "end": "436800"
  },
  {
    "text": "add a trator to say okay which Hoster do",
    "start": "436800",
    "end": "440240"
  },
  {
    "text": "the prev and what other Hoster to do the",
    "start": "440240",
    "end": "442639"
  },
  {
    "text": "decode and for this is is pretty",
    "start": "442639",
    "end": "445360"
  },
  {
    "text": "flexible you can do the config see how",
    "start": "445360",
    "end": "447919"
  },
  {
    "text": "many host I want to the prefer how many",
    "start": "447919",
    "end": "450599"
  },
  {
    "text": "host I want to do the decode and",
    "start": "450599",
    "end": "453000"
  },
  {
    "text": "Especial you have the production traffic",
    "start": "453000",
    "end": "455080"
  },
  {
    "text": "you can scale up and scale Down based on",
    "start": "455080",
    "end": "457960"
  },
  {
    "text": "your traffic yeah so I will handle to my",
    "start": "457960",
    "end": "460919"
  },
  {
    "text": "colleague Richard to talk more about re",
    "start": "460919",
    "end": "464000"
  },
  {
    "text": "TV and Ked thank",
    "start": "464000",
    "end": "466800"
  },
  {
    "text": "you all right thank",
    "start": "466800",
    "end": "469440"
  },
  {
    "text": "you so uh first I want to talk about uh",
    "start": "469440",
    "end": "472720"
  },
  {
    "text": "where can R help with all of this so F I",
    "start": "472720",
    "end": "475479"
  },
  {
    "text": "just talk about how uh uh gesturing",
    "start": "475479",
    "end": "477919"
  },
  {
    "text": "works on a high level right so uh one",
    "start": "477919",
    "end": "481720"
  },
  {
    "text": "thing that F talked about is that uh in",
    "start": "481720",
    "end": "484960"
  },
  {
    "text": "uh when you're experimenting with models",
    "start": "484960",
    "end": "486800"
  },
  {
    "text": "you could be scaling from a single host",
    "start": "486800",
    "end": "488919"
  },
  {
    "text": "to multi host depending on the size of",
    "start": "488919",
    "end": "490479"
  },
  {
    "text": "the model number of parameters and a",
    "start": "490479",
    "end": "492440"
  },
  {
    "text": "number of things so one uh what you",
    "start": "492440",
    "end": "495520"
  },
  {
    "text": "really need is a compute engine that can",
    "start": "495520",
    "end": "497479"
  },
  {
    "text": "really simul scale from single host to",
    "start": "497479",
    "end": "500400"
  },
  {
    "text": "multi host um without having you to",
    "start": "500400",
    "end": "503280"
  },
  {
    "text": "change any of the code and uh Ray can do",
    "start": "503280",
    "end": "505919"
  },
  {
    "text": "this in a very elegant way and um not",
    "start": "505919",
    "end": "509080"
  },
  {
    "text": "only that",
    "start": "509080",
    "end": "510560"
  },
  {
    "text": "Ray also uh is able to have a a flexible",
    "start": "510560",
    "end": "514719"
  },
  {
    "text": "support for inter leave and fully",
    "start": "514719",
    "end": "516518"
  },
  {
    "text": "disaggregated mode and um as say we",
    "start": "516519",
    "end": "519279"
  },
  {
    "text": "we'll be demonstrating how this will",
    "start": "519279",
    "end": "521039"
  },
  {
    "text": "work uh with the just stream uh",
    "start": "521039",
    "end": "524159"
  },
  {
    "text": "furthermore for the race serve um",
    "start": "524159",
    "end": "527640"
  },
  {
    "text": "sometimes uh in production you need a",
    "start": "527640",
    "end": "530200"
  },
  {
    "text": "production level support for features",
    "start": "530200",
    "end": "532519"
  },
  {
    "text": "like Auto scaling and load balancing uh",
    "start": "532519",
    "end": "535080"
  },
  {
    "text": "race serve offers these production level",
    "start": "535080",
    "end": "537440"
  },
  {
    "text": "features through a easy to use pythonic",
    "start": "537440",
    "end": "539880"
  },
  {
    "text": "API that uh integrates really nicely",
    "start": "539880",
    "end": "542959"
  },
  {
    "text": "with uh uh pytorch uh jet",
    "start": "542959",
    "end": "546600"
  },
  {
    "text": "stream and finally if you're running in",
    "start": "546600",
    "end": "550200"
  },
  {
    "text": "a platform like",
    "start": "550200",
    "end": "551760"
  },
  {
    "text": "kubernetes uh Ray also has a number of",
    "start": "551760",
    "end": "555399"
  },
  {
    "text": "OSS uh kubernetes Native uh support",
    "start": "555399",
    "end": "559040"
  },
  {
    "text": "through uh the cubra operator so this",
    "start": "559040",
    "end": "561839"
  },
  {
    "text": "makes it really easy to deploy your red",
    "start": "561839",
    "end": "563720"
  },
  {
    "text": "clusters on top of your existing uh",
    "start": "563720",
    "end": "566320"
  },
  {
    "text": "kubernetes clusters including GK",
    "start": "566320",
    "end": "570480"
  },
  {
    "text": "uh so this is a quick overview of uh the",
    "start": "570480",
    "end": "572959"
  },
  {
    "text": "Ray on TPU use cases um so if you look",
    "start": "572959",
    "end": "576360"
  },
  {
    "text": "at the technical stack U on the",
    "start": "576360",
    "end": "578760"
  },
  {
    "text": "foundational level you have the cloud",
    "start": "578760",
    "end": "580320"
  },
  {
    "text": "providers uh which of course uh so gke",
    "start": "580320",
    "end": "583920"
  },
  {
    "text": "will be one of the providers and as you",
    "start": "583920",
    "end": "585959"
  },
  {
    "text": "move up the stack you you uh there is a",
    "start": "585959",
    "end": "588800"
  },
  {
    "text": "ray core Library which is a lowlevel",
    "start": "588800",
    "end": "590839"
  },
  {
    "text": "distributed uh Computing framework with",
    "start": "590839",
    "end": "593839"
  },
  {
    "text": "the the the core",
    "start": "593839",
    "end": "596560"
  },
  {
    "text": "apis um so over the past year we've uh",
    "start": "596560",
    "end": "601240"
  },
  {
    "text": "done a number of U uh changes in Ray",
    "start": "601240",
    "end": "605000"
  },
  {
    "text": "core itself to enable Ray to support",
    "start": "605000",
    "end": "608120"
  },
  {
    "text": "tpus natively uh this includes the",
    "start": "608120",
    "end": "611200"
  },
  {
    "text": "ability for Ray to recognize uh TPU",
    "start": "611200",
    "end": "614519"
  },
  {
    "text": "devices as if they're uh just gpus so in",
    "start": "614519",
    "end": "618640"
  },
  {
    "text": "the same same uh in the same pattern as",
    "start": "618640",
    "end": "622000"
  },
  {
    "text": "gpus um we've uh enabled Ray to schedule",
    "start": "622000",
    "end": "626600"
  },
  {
    "text": "uh task and actors on tvus and and um we",
    "start": "626600",
    "end": "630279"
  },
  {
    "text": "also added support for Autos scaling so",
    "start": "630279",
    "end": "633000"
  },
  {
    "text": "Ray can use US Auto autoscaler to",
    "start": "633000",
    "end": "635440"
  },
  {
    "text": "provision new TPU no",
    "start": "635440",
    "end": "638160"
  },
  {
    "text": "pools um as we move uh even further up",
    "start": "638160",
    "end": "641519"
  },
  {
    "text": "to stack we have the ray AI",
    "start": "641519",
    "end": "644480"
  },
  {
    "text": "libraries um so today we'll be talking",
    "start": "644480",
    "end": "647519"
  },
  {
    "text": "about uh jet stream um onr which is a",
    "start": "647519",
    "end": "650320"
  },
  {
    "text": "part of the which is uh part of the U",
    "start": "650320",
    "end": "653720"
  },
  {
    "text": "inference use cases um in the future",
    "start": "653720",
    "end": "657079"
  },
  {
    "text": "we'll have other uh Integrations with",
    "start": "657079",
    "end": "660560"
  },
  {
    "text": "the fine tuning and",
    "start": "660560",
    "end": "663639"
  },
  {
    "text": "training so this is sort of a high level",
    "start": "664040",
    "end": "667000"
  },
  {
    "text": "picture of how a ray cluster on",
    "start": "667000",
    "end": "670000"
  },
  {
    "text": "kubernetes works with the tpus so uh",
    "start": "670000",
    "end": "674000"
  },
  {
    "text": "suppose you have a CPU node that's uh",
    "start": "674000",
    "end": "676880"
  },
  {
    "text": "hosting the ray head um this will be",
    "start": "676880",
    "end": "680079"
  },
  {
    "text": "where we place the jet stream deployment",
    "start": "680079",
    "end": "683720"
  },
  {
    "text": "and um this is also where processes like",
    "start": "683720",
    "end": "686200"
  },
  {
    "text": "the ray Auto scaler would reside",
    "start": "686200",
    "end": "690079"
  },
  {
    "text": "um and then suppose you have multiple",
    "start": "690079",
    "end": "692839"
  },
  {
    "text": "TPU nodes and um imagine that these are",
    "start": "692839",
    "end": "696959"
  },
  {
    "text": "uh part of uh TPU topologies so each one",
    "start": "696959",
    "end": "700360"
  },
  {
    "text": "is a multihost group um on the multihost",
    "start": "700360",
    "end": "704800"
  },
  {
    "text": "group you will host your uh real workers",
    "start": "704800",
    "end": "708079"
  },
  {
    "text": "and each one will contain A Shard of the",
    "start": "708079",
    "end": "710519"
  },
  {
    "text": "model so uh there are multiple ways to",
    "start": "710519",
    "end": "713399"
  },
  {
    "text": "start a large language model you could",
    "start": "713399",
    "end": "716040"
  },
  {
    "text": "use uh data parallelism or tensor",
    "start": "716040",
    "end": "718880"
  },
  {
    "text": "parallelism",
    "start": "718880",
    "end": "720079"
  },
  {
    "text": "but uh for now just assume that somehow",
    "start": "720079",
    "end": "721760"
  },
  {
    "text": "we've uh shed this model across multiple",
    "start": "721760",
    "end": "724639"
  },
  {
    "text": "workers and the key here is that um when",
    "start": "724639",
    "end": "728839"
  },
  {
    "text": "you're autoscaling this cluster the TPU",
    "start": "728839",
    "end": "732880"
  },
  {
    "text": "group uh the TPU slice has to scale up",
    "start": "732880",
    "end": "735600"
  },
  {
    "text": "or down as a group so you have you have",
    "start": "735600",
    "end": "739199"
  },
  {
    "text": "a model that's uh that's too large to",
    "start": "739199",
    "end": "741480"
  },
  {
    "text": "fit on a single node so the model has to",
    "start": "741480",
    "end": "744279"
  },
  {
    "text": "fit on um multiple TPU hosts so when",
    "start": "744279",
    "end": "747480"
  },
  {
    "text": "you're scaling up it is critical that",
    "start": "747480",
    "end": "749920"
  },
  {
    "text": "these groups uh scale up and down uh as",
    "start": "749920",
    "end": "753240"
  },
  {
    "text": "an atomic group rather than by itself",
    "start": "753240",
    "end": "756079"
  },
  {
    "text": "right so we made U changes to the cubra",
    "start": "756079",
    "end": "760360"
  },
  {
    "text": "operator to support this pattern uh this",
    "start": "760360",
    "end": "763199"
  },
  {
    "text": "was previously not supported in uh Cub",
    "start": "763199",
    "end": "765720"
  },
  {
    "text": "because they understood uh each uh",
    "start": "765720",
    "end": "768480"
  },
  {
    "text": "replica as a single cor part but um over",
    "start": "768480",
    "end": "772680"
  },
  {
    "text": "the past year we've enabled the features",
    "start": "772680",
    "end": "775360"
  },
  {
    "text": "in the cubra to scale up multiple cor",
    "start": "775360",
    "end": "778959"
  },
  {
    "text": "power as a single Atomic group so when",
    "start": "778959",
    "end": "781800"
  },
  {
    "text": "the ray Auto scaler receives a scaleup",
    "start": "781800",
    "end": "783880"
  },
  {
    "text": "request uh this would uh talk through",
    "start": "783880",
    "end": "787279"
  },
  {
    "text": "the cubra node provider to the cubr",
    "start": "787279",
    "end": "790240"
  },
  {
    "text": "operator to say that we are adding one",
    "start": "790240",
    "end": "792639"
  },
  {
    "text": "more replica of a multihost uh Ray",
    "start": "792639",
    "end": "796639"
  },
  {
    "text": "worker group and this will translate",
    "start": "796639",
    "end": "799000"
  },
  {
    "text": "into a coret part creation request and",
    "start": "799000",
    "end": "802480"
  },
  {
    "text": "uh this will make the uh nodes available",
    "start": "802480",
    "end": "804399"
  },
  {
    "text": "at uh at the ray level",
    "start": "804399",
    "end": "809600"
  },
  {
    "text": "so what does a a a single replica of a j",
    "start": "810040",
    "end": "813760"
  },
  {
    "text": "stream deployment look like so if we",
    "start": "813760",
    "end": "816639"
  },
  {
    "text": "look uh into the J stream replica each",
    "start": "816639",
    "end": "820279"
  },
  {
    "text": "one is a group of uh Ray workers and uh",
    "start": "820279",
    "end": "824800"
  },
  {
    "text": "each one is hosting an engine",
    "start": "824800",
    "end": "826800"
  },
  {
    "text": "Shard um the each of The Shard is",
    "start": "826800",
    "end": "830199"
  },
  {
    "text": "deployed on a single Ray worker node",
    "start": "830199",
    "end": "833000"
  },
  {
    "text": "with uh potentially uh with many tpus so",
    "start": "833000",
    "end": "836000"
  },
  {
    "text": "in this case there are four tpus in uh",
    "start": "836000",
    "end": "837880"
  },
  {
    "text": "in each uh r",
    "start": "837880",
    "end": "840079"
  },
  {
    "text": "worker and um Ray has a nice feature for",
    "start": "840079",
    "end": "844800"
  },
  {
    "text": "uh called placement groups and uh it",
    "start": "844800",
    "end": "847440"
  },
  {
    "text": "makes it really easy to schedule this",
    "start": "847440",
    "end": "851000"
  },
  {
    "text": "multi replicas at the application layer",
    "start": "851000",
    "end": "854839"
  },
  {
    "text": "and then we can use a ray itself to",
    "start": "854839",
    "end": "857040"
  },
  {
    "text": "schedule the prefill and decoding task",
    "start": "857040",
    "end": "859320"
  },
  {
    "text": "on Ray",
    "start": "859320",
    "end": "861920"
  },
  {
    "text": "workers so this is a uh a walk through",
    "start": "864560",
    "end": "868360"
  },
  {
    "text": "of uh",
    "start": "868360",
    "end": "869560"
  },
  {
    "text": "what a just stream uh deployment looks",
    "start": "869560",
    "end": "872079"
  },
  {
    "text": "like um it's a little hard to see the",
    "start": "872079",
    "end": "874880"
  },
  {
    "text": "code but uh on the right hand sign you",
    "start": "874880",
    "end": "877360"
  },
  {
    "text": "can see that uh in a uh if you're",
    "start": "877360",
    "end": "881240"
  },
  {
    "text": "familiar with the race serve this will",
    "start": "881240",
    "end": "883519"
  },
  {
    "text": "look fa Fair familiar to you there is a",
    "start": "883519",
    "end": "887519"
  },
  {
    "text": "init function which uh basically this is",
    "start": "887519",
    "end": "890560"
  },
  {
    "text": "where you initialize your your",
    "start": "890560",
    "end": "893199"
  },
  {
    "text": "L and um here we're doing this uh",
    "start": "893199",
    "end": "896880"
  },
  {
    "text": "initialization with the inter leave",
    "start": "896880",
    "end": "899800"
  },
  {
    "text": "uh slices so suppose you detect the",
    "start": "899800",
    "end": "902759"
  },
  {
    "text": "number of TPU slices in your rare",
    "start": "902759",
    "end": "904720"
  },
  {
    "text": "cluster and if you're running inter Le",
    "start": "904720",
    "end": "907000"
  },
  {
    "text": "mode so as fi mentioned earlier uh in",
    "start": "907000",
    "end": "910160"
  },
  {
    "text": "interly mode each TPU slice uh performs",
    "start": "910160",
    "end": "913639"
  },
  {
    "text": "both a prefill and a decode so you can",
    "start": "913639",
    "end": "916120"
  },
  {
    "text": "just uh assign all your TPU slices to",
    "start": "916120",
    "end": "919360"
  },
  {
    "text": "inter leave but if you were doing",
    "start": "919360",
    "end": "922079"
  },
  {
    "text": "disaggregated mode here uh you would you",
    "start": "922079",
    "end": "925240"
  },
  {
    "text": "can actually uh assign separate TV",
    "start": "925240",
    "end": "927720"
  },
  {
    "text": "slices for prefill and code now this is",
    "start": "927720",
    "end": "930600"
  },
  {
    "text": "actually very uh flexible and it's it's",
    "start": "930600",
    "end": "932959"
  },
  {
    "text": "quite powerful because potentially you",
    "start": "932959",
    "end": "935480"
  },
  {
    "text": "could have different uh Hardware",
    "start": "935480",
    "end": "938000"
  },
  {
    "text": "requirements for prefill and decode and",
    "start": "938000",
    "end": "941079"
  },
  {
    "text": "you can uh assign different uh runtime",
    "start": "941079",
    "end": "944440"
  },
  {
    "text": "parameters like the different batch",
    "start": "944440",
    "end": "946480"
  },
  {
    "text": "sizes to achieve higher throughput so if",
    "start": "946480",
    "end": "949680"
  },
  {
    "text": "you really want to customize your setup",
    "start": "949680",
    "end": "952600"
  },
  {
    "text": "to optimize your through prooof",
    "start": "952600",
    "end": "954600"
  },
  {
    "text": "performance um this is uh R has a",
    "start": "954600",
    "end": "958399"
  },
  {
    "text": "flexibility to allow you to do",
    "start": "958399",
    "end": "961639"
  },
  {
    "text": "that and um scaling up is very easy so",
    "start": "961639",
    "end": "965680"
  },
  {
    "text": "uh all you had to do is um annotate the",
    "start": "965680",
    "end": "967639"
  },
  {
    "text": "jream deployment with the the TPU",
    "start": "967639",
    "end": "970160"
  },
  {
    "text": "topology that is desired uh the number",
    "start": "970160",
    "end": "972680"
  },
  {
    "text": "replicas and the Autos scaling",
    "start": "972680",
    "end": "975120"
  },
  {
    "text": "configurations and then you can create",
    "start": "975120",
    "end": "977759"
  },
  {
    "text": "this uh decode function which calls into",
    "start": "977759",
    "end": "980720"
  },
  {
    "text": "the uh jream orchestrator to uh to send",
    "start": "980720",
    "end": "984440"
  },
  {
    "text": "your uh to process the decode request",
    "start": "984440",
    "end": "989040"
  },
  {
    "text": "yeah so this is a quick walkr of uh J",
    "start": "989040",
    "end": "993519"
  },
  {
    "text": "stream deployment so yeah and uh this",
    "start": "993519",
    "end": "997360"
  },
  {
    "text": "wraps up our talk so uh I hope you find",
    "start": "997360",
    "end": "1000440"
  },
  {
    "text": "it",
    "start": "1000440",
    "end": "1002399"
  },
  {
    "text": "helpful thank you",
    "start": "1002399",
    "end": "1006560"
  }
]