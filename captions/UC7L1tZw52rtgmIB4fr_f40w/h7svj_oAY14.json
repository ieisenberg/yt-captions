[
  {
    "start": "0",
    "end": "46000"
  },
  {
    "text": "[Music]",
    "start": "190",
    "end": "14559"
  },
  {
    "text": "hi everybody my name is patrick and i'm",
    "start": "14559",
    "end": "16720"
  },
  {
    "text": "a senior software engineer working on",
    "start": "16720",
    "end": "18400"
  },
  {
    "text": "data management and optimization for",
    "start": "18400",
    "end": "20160"
  },
  {
    "text": "business intelligence at amazon",
    "start": "20160",
    "end": "22480"
  },
  {
    "text": "today i'm going to discuss the current",
    "start": "22480",
    "end": "24480"
  },
  {
    "text": "state of petabyte scale data lake table",
    "start": "24480",
    "end": "26800"
  },
  {
    "text": "management at amazon and our current",
    "start": "26800",
    "end": "29039"
  },
  {
    "text": "initiative to shift management of our",
    "start": "29039",
    "end": "30720"
  },
  {
    "text": "data lake to a core technology stack",
    "start": "30720",
    "end": "33120"
  },
  {
    "text": "built around aero park a and s3",
    "start": "33120",
    "end": "36239"
  },
  {
    "text": "specifically this talk will focus on the",
    "start": "36239",
    "end": "38399"
  },
  {
    "text": "flash compactor a new service we're",
    "start": "38399",
    "end": "40559"
  },
  {
    "text": "developing to apply all of our data lake",
    "start": "40559",
    "end": "42399"
  },
  {
    "text": "table updates inserts and deletes",
    "start": "42399",
    "end": "45520"
  },
  {
    "text": "so before we begin a bit of context is",
    "start": "45520",
    "end": "47920"
  },
  {
    "start": "46000",
    "end": "513000"
  },
  {
    "text": "warranted i've been at amazon for about",
    "start": "47920",
    "end": "50239"
  },
  {
    "text": "five years now and i joined right as we",
    "start": "50239",
    "end": "52239"
  },
  {
    "text": "were starting a project to migrate over",
    "start": "52239",
    "end": "54399"
  },
  {
    "text": "50 petabytes of oracle data warehouse",
    "start": "54399",
    "end": "56239"
  },
  {
    "text": "data to an s3 based data lake you can",
    "start": "56239",
    "end": "58800"
  },
  {
    "text": "read more about this project at the",
    "start": "58800",
    "end": "60239"
  },
  {
    "text": "above link the service my team designed",
    "start": "60239",
    "end": "62960"
  },
  {
    "text": "and built was responsible for actually",
    "start": "62960",
    "end": "64559"
  },
  {
    "text": "copying each oracle table to s3 not long",
    "start": "64559",
    "end": "67360"
  },
  {
    "text": "after this project completed in 2019 our",
    "start": "67360",
    "end": "70080"
  },
  {
    "text": "s3 based data lake grew to over 200",
    "start": "70080",
    "end": "72159"
  },
  {
    "text": "petabytes with more than 3 000 redshift",
    "start": "72159",
    "end": "74560"
  },
  {
    "text": "and emr clusters running data processing",
    "start": "74560",
    "end": "76799"
  },
  {
    "text": "jobs and these figures are only growing",
    "start": "76799",
    "end": "78799"
  },
  {
    "text": "larger over time during this time we",
    "start": "78799",
    "end": "81040"
  },
  {
    "text": "extended our oracle table migrator into",
    "start": "81040",
    "end": "82880"
  },
  {
    "text": "a serverless data lake table",
    "start": "82880",
    "end": "84159"
  },
  {
    "text": "administration platform the table",
    "start": "84159",
    "end": "86080"
  },
  {
    "text": "administrators could use to create alter",
    "start": "86080",
    "end": "88159"
  },
  {
    "text": "re-partition move copy and repair their",
    "start": "88159",
    "end": "90240"
  },
  {
    "text": "tables at the same time we also",
    "start": "90240",
    "end": "92560"
  },
  {
    "text": "introduced a serverless solution for",
    "start": "92560",
    "end": "94079"
  },
  {
    "text": "applying data lake table updates inserts",
    "start": "94079",
    "end": "96720"
  },
  {
    "text": "and deletes based on apache spark on amr",
    "start": "96720",
    "end": "100479"
  },
  {
    "text": "since this process reduces a table",
    "start": "100479",
    "end": "102240"
  },
  {
    "text": "change log previously composed of",
    "start": "102240",
    "end": "103920"
  },
  {
    "text": "thousands or millions of data sets down",
    "start": "103920",
    "end": "105840"
  },
  {
    "text": "to a single data set holding the final",
    "start": "105840",
    "end": "108000"
  },
  {
    "text": "table state we call it compaction",
    "start": "108000",
    "end": "111280"
  },
  {
    "text": "compaction jobs are critical to keeping",
    "start": "111280",
    "end": "113040"
  },
  {
    "text": "our data lake operational since many",
    "start": "113040",
    "end": "115119"
  },
  {
    "text": "compute platforms require a table to be",
    "start": "115119",
    "end": "117040"
  },
  {
    "text": "compacted before they can consume it and",
    "start": "117040",
    "end": "119759"
  },
  {
    "text": "since row deletes must be applied to",
    "start": "119759",
    "end": "121759"
  },
  {
    "text": "many tables for gdpr compliance",
    "start": "121759",
    "end": "124399"
  },
  {
    "text": "even if a compute platform knows how to",
    "start": "124399",
    "end": "126240"
  },
  {
    "text": "compact a table's change log reading",
    "start": "126240",
    "end": "128319"
  },
  {
    "text": "from a pre-compacted data lake table",
    "start": "128319",
    "end": "130160"
  },
  {
    "text": "provides substantial time and cost",
    "start": "130160",
    "end": "132080"
  },
  {
    "text": "benefits",
    "start": "132080",
    "end": "134160"
  },
  {
    "text": "while our spark compactor served us well",
    "start": "134160",
    "end": "136080"
  },
  {
    "text": "for the past two years it is",
    "start": "136080",
    "end": "137680"
  },
  {
    "text": "increasingly falling short of our ideals",
    "start": "137680",
    "end": "139599"
  },
  {
    "text": "due to the following pain points",
    "start": "139599",
    "end": "141599"
  },
  {
    "text": "for one our existing spark compactor's",
    "start": "141599",
    "end": "143440"
  },
  {
    "text": "cost per byte compacted is too high to",
    "start": "143440",
    "end": "145599"
  },
  {
    "text": "support ongoing compaction job runs on",
    "start": "145599",
    "end": "148080"
  },
  {
    "text": "all data lake tables so it is restricted",
    "start": "148080",
    "end": "150560"
  },
  {
    "text": "to running only on tables that",
    "start": "150560",
    "end": "151840"
  },
  {
    "text": "absolutely require it instead of",
    "start": "151840",
    "end": "153519"
  },
  {
    "text": "extending its benefits to all tables it",
    "start": "153519",
    "end": "155840"
  },
  {
    "text": "also requires that tables to compact fit",
    "start": "155840",
    "end": "157840"
  },
  {
    "text": "on clusters so if it doesn't fit on our",
    "start": "157840",
    "end": "160319"
  },
  {
    "text": "largest emr cluster then an",
    "start": "160319",
    "end": "162160"
  },
  {
    "text": "operationally intensive data retention",
    "start": "162160",
    "end": "164080"
  },
  {
    "text": "campaign must be run by table",
    "start": "164080",
    "end": "166160"
  },
  {
    "text": "administrators to reduce its size we've",
    "start": "166160",
    "end": "168800"
  },
  {
    "text": "also been unable to provide firm sla",
    "start": "168800",
    "end": "171040"
  },
  {
    "text": "guarantees for spark compaction",
    "start": "171040",
    "end": "172800"
  },
  {
    "text": "completion times due to a wide and",
    "start": "172800",
    "end": "175120"
  },
  {
    "text": "sometimes unpredictable variance in",
    "start": "175120",
    "end": "176800"
  },
  {
    "text": "compaction job run latency together with",
    "start": "176800",
    "end": "179120"
  },
  {
    "text": "unanticipated failures",
    "start": "179120",
    "end": "182560"
  },
  {
    "text": "so we became interested in rey as a",
    "start": "182879",
    "end": "184800"
  },
  {
    "text": "potential solution to these problems",
    "start": "184800",
    "end": "186879"
  },
  {
    "text": "since its foundational primitives of",
    "start": "186879",
    "end": "189040"
  },
  {
    "text": "tasks and actors were general purpose",
    "start": "189040",
    "end": "191599"
  },
  {
    "text": "enough to not only produce another",
    "start": "191599",
    "end": "193120"
  },
  {
    "text": "compactor",
    "start": "193120",
    "end": "194480"
  },
  {
    "text": "but to also unify our disparate table",
    "start": "194480",
    "end": "196800"
  },
  {
    "text": "administration machine learning and etl",
    "start": "196800",
    "end": "199519"
  },
  {
    "text": "jobs under a single compute platform",
    "start": "199519",
    "end": "202000"
  },
  {
    "text": "its idiomatic apis for distributed",
    "start": "202000",
    "end": "204159"
  },
  {
    "text": "python application development also",
    "start": "204159",
    "end": "206239"
  },
  {
    "text": "provided an ideal way for a growing",
    "start": "206239",
    "end": "208560"
  },
  {
    "text": "scientific customer base to interface",
    "start": "208560",
    "end": "210720"
  },
  {
    "text": "with our data lake we could also reduce",
    "start": "210720",
    "end": "212879"
  },
  {
    "text": "the complexity and cost of maintaining",
    "start": "212879",
    "end": "214959"
  },
  {
    "text": "distributed compute state by using",
    "start": "214959",
    "end": "217120"
  },
  {
    "text": "actors instead of depending on external",
    "start": "217120",
    "end": "218879"
  },
  {
    "text": "services like dynamodb elasticache and",
    "start": "218879",
    "end": "221680"
  },
  {
    "text": "s3 to achieve similar functionality",
    "start": "221680",
    "end": "224720"
  },
  {
    "text": "we also wanted to explore the",
    "start": "224720",
    "end": "226080"
  },
  {
    "text": "performance scalability and efficiency",
    "start": "226080",
    "end": "228319"
  },
  {
    "text": "benefits of ray's distributed scheduler",
    "start": "228319",
    "end": "230319"
  },
  {
    "text": "and object store finally we found that",
    "start": "230319",
    "end": "232720"
  },
  {
    "text": "rey's aws cluster launcher and auto",
    "start": "232720",
    "end": "235120"
  },
  {
    "text": "scaler aligned well with our goals to",
    "start": "235120",
    "end": "236640"
  },
  {
    "text": "shift our jobs to isolated heterogeneous",
    "start": "236640",
    "end": "239200"
  },
  {
    "text": "autoscaling clusters while reusing our",
    "start": "239200",
    "end": "241360"
  },
  {
    "text": "existing ec2 instance pools",
    "start": "241360",
    "end": "244000"
  },
  {
    "text": "so with this context in mind let's now",
    "start": "244000",
    "end": "246400"
  },
  {
    "text": "dive into the design of the flash",
    "start": "246400",
    "end": "248480"
  },
  {
    "text": "compactor",
    "start": "248480",
    "end": "251040"
  },
  {
    "text": "to better understand why compaction is",
    "start": "251040",
    "end": "252959"
  },
  {
    "text": "needed we'll walk step by step through",
    "start": "252959",
    "end": "255120"
  },
  {
    "text": "the workflow it participates in starting",
    "start": "255120",
    "end": "256959"
  },
  {
    "text": "with our etl job run orchestration",
    "start": "256959",
    "end": "258639"
  },
  {
    "text": "service highlighted in green above",
    "start": "258639",
    "end": "262639"
  },
  {
    "text": "before a compaction job runs scheduled a",
    "start": "262800",
    "end": "264960"
  },
  {
    "text": "load job run is submitted by this job",
    "start": "264960",
    "end": "266479"
  },
  {
    "text": "run orchestrator to a registered low job",
    "start": "266479",
    "end": "268720"
  },
  {
    "text": "run executor",
    "start": "268720",
    "end": "270560"
  },
  {
    "text": "let's assume that the input table to",
    "start": "270560",
    "end": "272080"
  },
  {
    "text": "this load job run is a single 16 row",
    "start": "272080",
    "end": "274000"
  },
  {
    "text": "parquet file stored in s3 tracking the",
    "start": "274000",
    "end": "276240"
  },
  {
    "text": "status of the first orders ever",
    "start": "276240",
    "end": "277520"
  },
  {
    "text": "submitted to amazon",
    "start": "277520",
    "end": "279520"
  },
  {
    "text": "let's also assume that the order day",
    "start": "279520",
    "end": "281040"
  },
  {
    "text": "column has been flagged as the table's",
    "start": "281040",
    "end": "282560"
  },
  {
    "text": "partition key",
    "start": "282560",
    "end": "284560"
  },
  {
    "text": "finally let's assume that this is the",
    "start": "284560",
    "end": "286080"
  },
  {
    "text": "first load job run ever scheduled",
    "start": "286080",
    "end": "287759"
  },
  {
    "text": "against version 1 of the currently empty",
    "start": "287759",
    "end": "289600"
  },
  {
    "text": "customer order status table",
    "start": "289600",
    "end": "293280"
  },
  {
    "text": "all rows of the table will be loaded",
    "start": "293280",
    "end": "294880"
  },
  {
    "text": "into a table partition matching your",
    "start": "294880",
    "end": "296720"
  },
  {
    "text": "order date value",
    "start": "296720",
    "end": "299040"
  },
  {
    "text": "each partition contains a version",
    "start": "299040",
    "end": "300800"
  },
  {
    "text": "partition stream with a lifecycle state",
    "start": "300800",
    "end": "302720"
  },
  {
    "text": "indicating whether it is ready to be",
    "start": "302720",
    "end": "304080"
  },
  {
    "text": "consumed or not an active life cycle",
    "start": "304080",
    "end": "306639"
  },
  {
    "text": "state shows that it is ready for",
    "start": "306639",
    "end": "308240"
  },
  {
    "text": "external consumption each partition",
    "start": "308240",
    "end": "310320"
  },
  {
    "text": "string version contains one or more",
    "start": "310320",
    "end": "312080"
  },
  {
    "text": "immutable data sets annotated with their",
    "start": "312080",
    "end": "314400"
  },
  {
    "text": "operation type and ordered partition",
    "start": "314400",
    "end": "316240"
  },
  {
    "text": "stream position each dataset contains",
    "start": "316240",
    "end": "318639"
  },
  {
    "text": "one or more ordered immutable s3 files",
    "start": "318639",
    "end": "321039"
  },
  {
    "text": "together with stats like file count row",
    "start": "321039",
    "end": "323440"
  },
  {
    "text": "count content type and bytes at rest for",
    "start": "323440",
    "end": "326400"
  },
  {
    "text": "the sake of our example we'll assume",
    "start": "326400",
    "end": "328160"
  },
  {
    "text": "that this load job run decided to evenly",
    "start": "328160",
    "end": "330080"
  },
  {
    "text": "split the results across two data sets",
    "start": "330080",
    "end": "332160"
  },
  {
    "text": "per partition and eight total files",
    "start": "332160",
    "end": "334880"
  },
  {
    "text": "we'll also assume that the 1995 0403",
    "start": "334880",
    "end": "337919"
  },
  {
    "text": "partition stream is already on version 2",
    "start": "337919",
    "end": "340960"
  },
  {
    "text": "perhaps due to corrupting and",
    "start": "340960",
    "end": "342320"
  },
  {
    "text": "deprecating the data in partition stream",
    "start": "342320",
    "end": "344400"
  },
  {
    "text": "version 1.",
    "start": "344400",
    "end": "346400"
  },
  {
    "text": "so once the load job run completes we're",
    "start": "346400",
    "end": "348560"
  },
  {
    "text": "left with the following annotated table",
    "start": "348560",
    "end": "350560"
  },
  {
    "text": "in s3",
    "start": "350560",
    "end": "352400"
  },
  {
    "text": "and the low job run executor tells the",
    "start": "352400",
    "end": "354639"
  },
  {
    "text": "job run orchestrator that the load job",
    "start": "354639",
    "end": "356800"
  },
  {
    "text": "run is complete",
    "start": "356800",
    "end": "359759"
  },
  {
    "text": "which in turn triggers a compaction job",
    "start": "359759",
    "end": "361759"
  },
  {
    "text": "run to be sent to our incoming",
    "start": "361759",
    "end": "363120"
  },
  {
    "text": "compaction job run sqs queue",
    "start": "363120",
    "end": "366560"
  },
  {
    "text": "this sqs queue automatically publishes",
    "start": "366560",
    "end": "368960"
  },
  {
    "text": "metrics like number of messages age of",
    "start": "368960",
    "end": "370960"
  },
  {
    "text": "the oldest message and message send and",
    "start": "370960",
    "end": "372880"
  },
  {
    "text": "receive counts to amazon cloudwatch",
    "start": "372880",
    "end": "376160"
  },
  {
    "text": "if any of these metrics are outside the",
    "start": "376160",
    "end": "377759"
  },
  {
    "text": "bounds of pre-configured thresholds an",
    "start": "377759",
    "end": "379919"
  },
  {
    "text": "alarm will be triggered",
    "start": "379919",
    "end": "381600"
  },
  {
    "text": "which will notify oncall",
    "start": "381600",
    "end": "384080"
  },
  {
    "text": "who can use cloudwatch metrics and logs",
    "start": "384080",
    "end": "386160"
  },
  {
    "text": "to investigate the root cause as we",
    "start": "386160",
    "end": "388160"
  },
  {
    "text": "continue through this workflow note that",
    "start": "388160",
    "end": "389680"
  },
  {
    "text": "nearly every component of our system",
    "start": "389680",
    "end": "391199"
  },
  {
    "text": "publishes logs and metrics to cloudwatch",
    "start": "391199",
    "end": "393520"
  },
  {
    "text": "to ensure that errors are quickly",
    "start": "393520",
    "end": "394960"
  },
  {
    "text": "flagged investigated and resolved",
    "start": "394960",
    "end": "397680"
  },
  {
    "text": "back in the primary workflow the",
    "start": "397680",
    "end": "399280"
  },
  {
    "text": "compaction job run instruction document",
    "start": "399280",
    "end": "401199"
  },
  {
    "text": "is received by a job run dispatcher a",
    "start": "401199",
    "end": "403520"
  },
  {
    "text": "high availability array cluster that",
    "start": "403520",
    "end": "405280"
  },
  {
    "text": "auto scales in response to the number of",
    "start": "405280",
    "end": "407280"
  },
  {
    "text": "job runs currently enqueued",
    "start": "407280",
    "end": "409360"
  },
  {
    "text": "high availability is achieved by",
    "start": "409360",
    "end": "410960"
  },
  {
    "text": "provisioning two or more ray clusters",
    "start": "410960",
    "end": "412720"
  },
  {
    "text": "via aws step function tasks using a",
    "start": "412720",
    "end": "415440"
  },
  {
    "text": "default heartbeat timeout of 5 minutes",
    "start": "415440",
    "end": "417840"
  },
  {
    "text": "if any cluster fails to emit a timely",
    "start": "417840",
    "end": "419919"
  },
  {
    "text": "heartbeat then our set function workflow",
    "start": "419919",
    "end": "421840"
  },
  {
    "text": "reprovisions the cluster a typical a",
    "start": "421840",
    "end": "424240"
  },
  {
    "text": "typical job run instruction document",
    "start": "424240",
    "end": "426160"
  },
  {
    "text": "contains a minimal auto scaler config",
    "start": "426160",
    "end": "428560"
  },
  {
    "text": "and a command to start the ray job run",
    "start": "428560",
    "end": "430960"
  },
  {
    "text": "job run dependencies are typically",
    "start": "430960",
    "end": "432479"
  },
  {
    "text": "packaged in a docker container or ec2",
    "start": "432479",
    "end": "434560"
  },
  {
    "text": "ami for simplicity we use a fifo queue",
    "start": "434560",
    "end": "437360"
  },
  {
    "text": "for incoming compaction job runs",
    "start": "437360",
    "end": "439280"
  },
  {
    "text": "standard sqs cues can also be used but",
    "start": "439280",
    "end": "441599"
  },
  {
    "text": "some additional overhead is required to",
    "start": "441599",
    "end": "443440"
  },
  {
    "text": "dedupe messages received more than once",
    "start": "443440",
    "end": "447360"
  },
  {
    "text": "the job run dispatcher publishes logs",
    "start": "447360",
    "end": "449759"
  },
  {
    "text": "and metrics to amazon cloudwatch",
    "start": "449759",
    "end": "451919"
  },
  {
    "text": "including extended ec2 instance metrics",
    "start": "451919",
    "end": "454160"
  },
  {
    "text": "collected by the cloudwatch unified",
    "start": "454160",
    "end": "455680"
  },
  {
    "text": "agent and ray system metrics scraped by",
    "start": "455680",
    "end": "458319"
  },
  {
    "text": "prometheus for every job run it receives",
    "start": "458319",
    "end": "461039"
  },
  {
    "text": "it publishes a job run received event to",
    "start": "461039",
    "end": "463039"
  },
  {
    "text": "its sns job run event fifo topic",
    "start": "463039",
    "end": "467199"
  },
  {
    "text": "which forwards the job run received",
    "start": "467199",
    "end": "469440"
  },
  {
    "text": "event to the sqs java advanced fifoq",
    "start": "469440",
    "end": "473199"
  },
  {
    "text": "which is received by the jobrun event",
    "start": "473199",
    "end": "475039"
  },
  {
    "text": "handler cluster",
    "start": "475039",
    "end": "477039"
  },
  {
    "text": "which persists the event in the dynamodb",
    "start": "477039",
    "end": "479280"
  },
  {
    "text": "job run event store",
    "start": "479280",
    "end": "481280"
  },
  {
    "text": "and queries the latest aggregate event",
    "start": "481280",
    "end": "483120"
  },
  {
    "text": "status for that job run",
    "start": "483120",
    "end": "486080"
  },
  {
    "text": "then sends a job run status update to",
    "start": "486080",
    "end": "488479"
  },
  {
    "text": "the orchestrator",
    "start": "488479",
    "end": "491039"
  },
  {
    "text": "in parallel the dispatcher launches the",
    "start": "491039",
    "end": "492960"
  },
  {
    "text": "compaction job run session launcher",
    "start": "492960",
    "end": "495280"
  },
  {
    "text": "array cluster whose ez2 instances are",
    "start": "495280",
    "end": "497280"
  },
  {
    "text": "bootstrapped with the flash compact",
    "start": "497280",
    "end": "498800"
  },
  {
    "text": "array app pre-installed",
    "start": "498800",
    "end": "501199"
  },
  {
    "text": "once the session launcher is up the",
    "start": "501199",
    "end": "503039"
  },
  {
    "text": "dispatcher invokes the compact api on",
    "start": "503039",
    "end": "505360"
  },
  {
    "text": "its head node and publishes a job run",
    "start": "505360",
    "end": "507440"
  },
  {
    "text": "dispatched sns event while the session",
    "start": "507440",
    "end": "509759"
  },
  {
    "text": "launcher publishes a corresponding job",
    "start": "509759",
    "end": "511599"
  },
  {
    "text": "run started event",
    "start": "511599",
    "end": "514159"
  },
  {
    "start": "513000",
    "end": "792000"
  },
  {
    "text": "required arguments to the python compact",
    "start": "514159",
    "end": "516080"
  },
  {
    "text": "api are simply the source table to",
    "start": "516080",
    "end": "517760"
  },
  {
    "text": "compact and a set of primary key columns",
    "start": "517760",
    "end": "521120"
  },
  {
    "text": "optional arguments to the compact api",
    "start": "521120",
    "end": "523200"
  },
  {
    "text": "include a list of sort keys desired sort",
    "start": "523200",
    "end": "525920"
  },
  {
    "text": "order desired sla epic time stamp data",
    "start": "525920",
    "end": "528640"
  },
  {
    "text": "catalog client implementation",
    "start": "528640",
    "end": "530560"
  },
  {
    "text": "destination come back to table id number",
    "start": "530560",
    "end": "532800"
  },
  {
    "text": "of rows per primary key index file",
    "start": "532800",
    "end": "534640"
  },
  {
    "text": "number of rows per compacted data set",
    "start": "534640",
    "end": "536800"
  },
  {
    "text": "file number of hash buckets to split the",
    "start": "536800",
    "end": "538959"
  },
  {
    "text": "primary key index across",
    "start": "538959",
    "end": "541040"
  },
  {
    "text": "sns event topic rn and content type of",
    "start": "541040",
    "end": "543279"
  },
  {
    "text": "compacted output files primary key",
    "start": "543279",
    "end": "545760"
  },
  {
    "text": "column values are used to detect",
    "start": "545760",
    "end": "547279"
  },
  {
    "text": "duplicates and must be non-null sort key",
    "start": "547279",
    "end": "549760"
  },
  {
    "text": "columns determine which duplicates to",
    "start": "549760",
    "end": "551760"
  },
  {
    "text": "keep the primary key index is used for",
    "start": "551760",
    "end": "553839"
  },
  {
    "text": "scalable detection of duplicate rows by",
    "start": "553839",
    "end": "556240"
  },
  {
    "text": "grouping like primary keys into one or",
    "start": "556240",
    "end": "558160"
  },
  {
    "text": "more hash buckets apache arrows feather",
    "start": "558160",
    "end": "560640"
  },
  {
    "text": "content type can also be used for",
    "start": "560640",
    "end": "562480"
  },
  {
    "text": "compacted output files",
    "start": "562480",
    "end": "565279"
  },
  {
    "text": "let's assume that our job run",
    "start": "565279",
    "end": "566720"
  },
  {
    "text": "instruction document specified the",
    "start": "566720",
    "end": "568399"
  },
  {
    "text": "following arguments for the compact api",
    "start": "568399",
    "end": "571040"
  },
  {
    "text": "we'll use the order id column as our",
    "start": "571040",
    "end": "573120"
  },
  {
    "text": "primary key the last updated column as",
    "start": "573120",
    "end": "576080"
  },
  {
    "text": "our sort key and write the compacted",
    "start": "576080",
    "end": "578080"
  },
  {
    "text": "output to a different destination table",
    "start": "578080",
    "end": "580080"
  },
  {
    "text": "id we'll also use two rows per primary",
    "start": "580080",
    "end": "582240"
  },
  {
    "text": "key index and compacted file and group",
    "start": "582240",
    "end": "584240"
  },
  {
    "text": "our primary key index files into four",
    "start": "584240",
    "end": "586000"
  },
  {
    "text": "hash buckets",
    "start": "586000",
    "end": "588160"
  },
  {
    "text": "if we return to the original table to be",
    "start": "588160",
    "end": "589920"
  },
  {
    "text": "compacted we can now highlight the",
    "start": "589920",
    "end": "592080"
  },
  {
    "text": "primary and sort key columns to",
    "start": "592080",
    "end": "593600"
  },
  {
    "text": "visualize our compaction job run input",
    "start": "593600",
    "end": "596480"
  },
  {
    "text": "we can also visualize our compaction job",
    "start": "596480",
    "end": "598399"
  },
  {
    "text": "run output by showing the rows dropped",
    "start": "598399",
    "end": "600160"
  },
  {
    "text": "when keeping only the max last updated",
    "start": "600160",
    "end": "602560"
  },
  {
    "text": "value per distinct order id",
    "start": "602560",
    "end": "605200"
  },
  {
    "text": "dropping these rows shows us our",
    "start": "605200",
    "end": "607200"
  },
  {
    "text": "expected compacted table",
    "start": "607200",
    "end": "609839"
  },
  {
    "text": "which will also have following primary",
    "start": "609839",
    "end": "612000"
  },
  {
    "text": "key index output alongside it the",
    "start": "612000",
    "end": "614240"
  },
  {
    "text": "primary key index is written to one or",
    "start": "614240",
    "end": "615839"
  },
  {
    "text": "more parquet files with each of its",
    "start": "615839",
    "end": "617920"
  },
  {
    "text": "fixed width rows containing at least 32",
    "start": "617920",
    "end": "620160"
  },
  {
    "text": "bytes with no custom sort keys and at",
    "start": "620160",
    "end": "622880"
  },
  {
    "text": "most 64 bytes with custom sword keys the",
    "start": "622880",
    "end": "626000"
  },
  {
    "text": "first 20 bytes hold a sha-1 digest of",
    "start": "626000",
    "end": "628640"
  },
  {
    "text": "all ordered concatenated primary key",
    "start": "628640",
    "end": "630959"
  },
  {
    "text": "bytes the next 12 bytes point to the",
    "start": "630959",
    "end": "633279"
  },
  {
    "text": "compacted table row that this shaw line",
    "start": "633279",
    "end": "635200"
  },
  {
    "text": "was generated from with 8 bytes for the",
    "start": "635200",
    "end": "637519"
  },
  {
    "text": "row number and 4 bytes for the data set",
    "start": "637519",
    "end": "639760"
  },
  {
    "text": "file number up to 32 bytes are set aside",
    "start": "639760",
    "end": "642560"
  },
  {
    "text": "to store optional custom sort keys in",
    "start": "642560",
    "end": "645040"
  },
  {
    "text": "this case let's assume we use four byte",
    "start": "645040",
    "end": "646880"
  },
  {
    "text": "epic time stamps for each last updated",
    "start": "646880",
    "end": "648800"
  },
  {
    "text": "value such that our primary key index",
    "start": "648800",
    "end": "651120"
  },
  {
    "text": "consumes 36 bytes per row",
    "start": "651120",
    "end": "654480"
  },
  {
    "text": "if we switch back to an annotated view",
    "start": "654480",
    "end": "656240"
  },
  {
    "text": "of the compaction job run output we can",
    "start": "656240",
    "end": "658399"
  },
  {
    "text": "trace the primary key index row pointers",
    "start": "658399",
    "end": "661360"
  },
  {
    "text": "back to the original rows that they were",
    "start": "661360",
    "end": "663360"
  },
  {
    "text": "derived from in the compacted output",
    "start": "663360",
    "end": "665279"
  },
  {
    "text": "table",
    "start": "665279",
    "end": "666560"
  },
  {
    "text": "viewing the fully annotated primary key",
    "start": "666560",
    "end": "668480"
  },
  {
    "text": "index reveals the hash bucket that each",
    "start": "668480",
    "end": "670640"
  },
  {
    "text": "row belongs to along with the table",
    "start": "670640",
    "end": "672800"
  },
  {
    "text": "partition stream version they're",
    "start": "672800",
    "end": "674240"
  },
  {
    "text": "indexing hash bucket assignment works by",
    "start": "674240",
    "end": "676720"
  },
  {
    "text": "taking the integer value of its sha-1",
    "start": "676720",
    "end": "678560"
  },
  {
    "text": "modulo the hash bucket count",
    "start": "678560",
    "end": "681040"
  },
  {
    "text": "note also that each partition's primary",
    "start": "681040",
    "end": "682959"
  },
  {
    "text": "key index is versioned one use case for",
    "start": "682959",
    "end": "685440"
  },
  {
    "text": "this is to support rehashing as each",
    "start": "685440",
    "end": "687440"
  },
  {
    "text": "partition grows larger in the absence of",
    "start": "687440",
    "end": "690000"
  },
  {
    "text": "a user-specified hash bucket count the",
    "start": "690000",
    "end": "691839"
  },
  {
    "text": "initial number of hash buckets for each",
    "start": "691839",
    "end": "693519"
  },
  {
    "text": "partition's primary key index is chosen",
    "start": "693519",
    "end": "696320"
  },
  {
    "text": "such that its compaction session",
    "start": "696320",
    "end": "698399"
  },
  {
    "text": "consumes approximately half of available",
    "start": "698399",
    "end": "700480"
  },
  {
    "text": "python heat memory during deduplication",
    "start": "700480",
    "end": "703200"
  },
  {
    "text": "if the hash bucket count inherited from",
    "start": "703200",
    "end": "705040"
  },
  {
    "text": "a prior compaction session is small",
    "start": "705040",
    "end": "707360"
  },
  {
    "text": "enough that over ninety percent of",
    "start": "707360",
    "end": "708640"
  },
  {
    "text": "python heat memory will be consumed then",
    "start": "708640",
    "end": "710880"
  },
  {
    "text": "the primary key index is first rehashed",
    "start": "710880",
    "end": "713839"
  },
  {
    "text": "to a hash bucket count large enough to",
    "start": "713839",
    "end": "715680"
  },
  {
    "text": "ensure that it will consume",
    "start": "715680",
    "end": "716720"
  },
  {
    "text": "approximately half of available python",
    "start": "716720",
    "end": "718800"
  },
  {
    "text": "heat memory",
    "start": "718800",
    "end": "720880"
  },
  {
    "text": "returning to our workflow the compaction",
    "start": "720880",
    "end": "722720"
  },
  {
    "text": "draw run session launcher creates a",
    "start": "722720",
    "end": "724399"
  },
  {
    "text": "dedicated compaction session for each",
    "start": "724399",
    "end": "726720"
  },
  {
    "text": "table partitioned to be compacted since",
    "start": "726720",
    "end": "729200"
  },
  {
    "text": "all rows consume a fixed amount of",
    "start": "729200",
    "end": "730800"
  },
  {
    "text": "memory during compaction via fixed",
    "start": "730800",
    "end": "733040"
  },
  {
    "text": "primary key index row byte widths and",
    "start": "733040",
    "end": "735519"
  },
  {
    "text": "have nearly perfect uniform random",
    "start": "735519",
    "end": "737360"
  },
  {
    "text": "distribution across cluster nodes via",
    "start": "737360",
    "end": "739839"
  },
  {
    "text": "primary key shall one digest based",
    "start": "739839",
    "end": "742079"
  },
  {
    "text": "shuffling ray compaction session cluster",
    "start": "742079",
    "end": "745040"
  },
  {
    "text": "resource requirements and hash bucket",
    "start": "745040",
    "end": "747200"
  },
  {
    "text": "counts required to both successfully",
    "start": "747200",
    "end": "749120"
  },
  {
    "text": "complete a compaction session and meet",
    "start": "749120",
    "end": "752000"
  },
  {
    "text": "the compaction job runs desired sla can",
    "start": "752000",
    "end": "754320"
  },
  {
    "text": "be accurately computed by the session",
    "start": "754320",
    "end": "756800"
  },
  {
    "text": "launcher prior to launching each session",
    "start": "756800",
    "end": "760399"
  },
  {
    "text": "the session manager publishes a child",
    "start": "760399",
    "end": "762639"
  },
  {
    "text": "launch event for each session it creates",
    "start": "762639",
    "end": "765120"
  },
  {
    "text": "when the job run event handler later",
    "start": "765120",
    "end": "766880"
  },
  {
    "text": "queries all events for this job run it",
    "start": "766880",
    "end": "769120"
  },
  {
    "text": "will only mark the job run as successful",
    "start": "769120",
    "end": "771200"
  },
  {
    "text": "if all of its children have published",
    "start": "771200",
    "end": "773200"
  },
  {
    "text": "success events",
    "start": "773200",
    "end": "776000"
  },
  {
    "text": "once all compaction sessions are running",
    "start": "776000",
    "end": "778240"
  },
  {
    "text": "the compaction job run session launcher",
    "start": "778240",
    "end": "780560"
  },
  {
    "text": "shuts down",
    "start": "780560",
    "end": "782399"
  },
  {
    "text": "each compaction session continues to",
    "start": "782399",
    "end": "784639"
  },
  {
    "text": "publish regular progress events to both",
    "start": "784639",
    "end": "786639"
  },
  {
    "text": "service heartbeats and to update",
    "start": "786639",
    "end": "788720"
  },
  {
    "text": "progress made compacting its assigned",
    "start": "788720",
    "end": "790480"
  },
  {
    "text": "partition",
    "start": "790480",
    "end": "792880"
  },
  {
    "start": "792000",
    "end": "885000"
  },
  {
    "text": "now each compaction session starts by",
    "start": "792880",
    "end": "795279"
  },
  {
    "text": "taking the s3 input files for its",
    "start": "795279",
    "end": "796959"
  },
  {
    "text": "assigned partition",
    "start": "796959",
    "end": "798720"
  },
  {
    "text": "and if the intermediate output produced",
    "start": "798720",
    "end": "801120"
  },
  {
    "text": "while compacting it will be too large to",
    "start": "801120",
    "end": "802959"
  },
  {
    "text": "fit in ray's object store splits it into",
    "start": "802959",
    "end": "805839"
  },
  {
    "text": "n equivalently sized slices where each",
    "start": "805839",
    "end": "808160"
  },
  {
    "text": "slice contains at least one full data",
    "start": "808160",
    "end": "810240"
  },
  {
    "text": "set",
    "start": "810240",
    "end": "811360"
  },
  {
    "text": "the first slice is then read by a",
    "start": "811360",
    "end": "813120"
  },
  {
    "text": "compaction round executor",
    "start": "813120",
    "end": "815760"
  },
  {
    "text": "which writes the compacted table and",
    "start": "815760",
    "end": "817360"
  },
  {
    "text": "primary key index for that slice to s3",
    "start": "817360",
    "end": "820480"
  },
  {
    "text": "and marks the round as complete by",
    "start": "820480",
    "end": "822000"
  },
  {
    "text": "writing a compaction round completion",
    "start": "822000",
    "end": "823760"
  },
  {
    "text": "file to s3",
    "start": "823760",
    "end": "825600"
  },
  {
    "text": "the compaction run completion file is",
    "start": "825600",
    "end": "827279"
  },
  {
    "text": "associated with the source partition",
    "start": "827279",
    "end": "828720"
  },
  {
    "text": "stream being compacted and contains its",
    "start": "828720",
    "end": "831199"
  },
  {
    "text": "latest stream position process together",
    "start": "831199",
    "end": "833120"
  },
  {
    "text": "with the latest primary key index hash",
    "start": "833120",
    "end": "834800"
  },
  {
    "text": "bucket count primary key index version",
    "start": "834800",
    "end": "837680"
  },
  {
    "text": "compacted partition stream location and",
    "start": "837680",
    "end": "840000"
  },
  {
    "text": "bytes at rest",
    "start": "840000",
    "end": "841440"
  },
  {
    "text": "when the next compaction round begins",
    "start": "841440",
    "end": "844079"
  },
  {
    "text": "it starts by reading the last round's",
    "start": "844079",
    "end": "845920"
  },
  {
    "text": "compaction completion file from s3",
    "start": "845920",
    "end": "850399"
  },
  {
    "text": "then reads the files for the next slice",
    "start": "850399",
    "end": "852320"
  },
  {
    "text": "from ice from s3",
    "start": "852320",
    "end": "855120"
  },
  {
    "text": "and merges the compacted output from",
    "start": "855120",
    "end": "856800"
  },
  {
    "text": "this round with the output of the last",
    "start": "856800",
    "end": "858639"
  },
  {
    "text": "round",
    "start": "858639",
    "end": "859680"
  },
  {
    "text": "before finally writing a new completion",
    "start": "859680",
    "end": "862000"
  },
  {
    "text": "file to s3",
    "start": "862000",
    "end": "864079"
  },
  {
    "text": "this iterative process continues until",
    "start": "864079",
    "end": "866320"
  },
  {
    "text": "all slices have been compacted",
    "start": "866320",
    "end": "869040"
  },
  {
    "text": "to produce the final compacted partition",
    "start": "869040",
    "end": "872000"
  },
  {
    "text": "for this session",
    "start": "872000",
    "end": "873839"
  },
  {
    "text": "to illustrate how a compaction round",
    "start": "873839",
    "end": "875760"
  },
  {
    "text": "works",
    "start": "875760",
    "end": "877120"
  },
  {
    "text": "we'll now walk through the 1995 0403",
    "start": "877120",
    "end": "880480"
  },
  {
    "text": "compaction sessions one and only round",
    "start": "880480",
    "end": "885040"
  },
  {
    "start": "885000",
    "end": "946000"
  },
  {
    "text": "which starts with this input from the",
    "start": "885040",
    "end": "887880"
  },
  {
    "text": "1995.0403 partition we will refer to all",
    "start": "887880",
    "end": "890959"
  },
  {
    "text": "files in this partition as delta files",
    "start": "890959",
    "end": "893600"
  },
  {
    "text": "since they represent the changes applied",
    "start": "893600",
    "end": "895279"
  },
  {
    "text": "to the table by the last load job run",
    "start": "895279",
    "end": "899040"
  },
  {
    "text": "the hash bucket step is the first",
    "start": "899040",
    "end": "900560"
  },
  {
    "text": "parallel step executed during a",
    "start": "900560",
    "end": "902160"
  },
  {
    "text": "compaction round it takes all input",
    "start": "902160",
    "end": "904560"
  },
  {
    "text": "delta files for the round as input and",
    "start": "904560",
    "end": "906800"
  },
  {
    "text": "writes an in-memory",
    "start": "906800",
    "end": "908320"
  },
  {
    "text": "delta primary key index to raise plasma",
    "start": "908320",
    "end": "910800"
  },
  {
    "text": "object stores output",
    "start": "910800",
    "end": "912480"
  },
  {
    "text": "each hash bucket task consumes one cpu",
    "start": "912480",
    "end": "915440"
  },
  {
    "text": "each hash bucket task is assigned an",
    "start": "915440",
    "end": "917040"
  },
  {
    "text": "approximately equal size subset of this",
    "start": "917040",
    "end": "919440"
  },
  {
    "text": "round's input delta files where each",
    "start": "919440",
    "end": "921519"
  },
  {
    "text": "subset is limited to fit in a single",
    "start": "921519",
    "end": "923519"
  },
  {
    "text": "task's available python heap memory the",
    "start": "923519",
    "end": "926240"
  },
  {
    "text": "total number of hash bucket tasks",
    "start": "926240",
    "end": "927920"
  },
  {
    "text": "launched is equal to the number of input",
    "start": "927920",
    "end": "929759"
  },
  {
    "text": "delta file subsets",
    "start": "929759",
    "end": "932000"
  },
  {
    "text": "let's assume that we're running",
    "start": "932000",
    "end": "933120"
  },
  {
    "text": "compaction on a tiny two node cluster",
    "start": "933120",
    "end": "935759"
  },
  {
    "text": "with two cpus per node and a minuscule",
    "start": "935759",
    "end": "938320"
  },
  {
    "text": "amount of memory per node enough to fit",
    "start": "938320",
    "end": "940720"
  },
  {
    "text": "only four rows of our input table into",
    "start": "940720",
    "end": "943199"
  },
  {
    "text": "each node's available python heap memory",
    "start": "943199",
    "end": "946800"
  },
  {
    "start": "946000",
    "end": "1032000"
  },
  {
    "text": "since we explicitly specified that we",
    "start": "946800",
    "end": "948480"
  },
  {
    "text": "wanted to group our primary key index",
    "start": "948480",
    "end": "950079"
  },
  {
    "text": "into four hash buckets we start with",
    "start": "950079",
    "end": "952079"
  },
  {
    "text": "four empty buckets displayed to the left",
    "start": "952079",
    "end": "953920"
  },
  {
    "text": "and right of the partition being",
    "start": "953920",
    "end": "955199"
  },
  {
    "text": "compacted",
    "start": "955199",
    "end": "956639"
  },
  {
    "text": "the first hash bucket task is scheduled",
    "start": "956639",
    "end": "958560"
  },
  {
    "text": "on node 1 and is assigned all four rows",
    "start": "958560",
    "end": "960880"
  },
  {
    "text": "from the first data set",
    "start": "960880",
    "end": "962639"
  },
  {
    "text": "the second hash bucket task is scheduled",
    "start": "962639",
    "end": "964639"
  },
  {
    "text": "on node 2 and is assigned all four rows",
    "start": "964639",
    "end": "966720"
  },
  {
    "text": "from the second data set",
    "start": "966720",
    "end": "968880"
  },
  {
    "text": "the first two rows of each dataset are",
    "start": "968880",
    "end": "970560"
  },
  {
    "text": "processed in parallel and assigned to",
    "start": "970560",
    "end": "972240"
  },
  {
    "text": "hash bucket four",
    "start": "972240",
    "end": "973839"
  },
  {
    "text": "since we're reading column-oriented",
    "start": "973839",
    "end": "975279"
  },
  {
    "text": "parquet files into pi arrow tables we",
    "start": "975279",
    "end": "977920"
  },
  {
    "text": "only read primary and sort key columns",
    "start": "977920",
    "end": "980000"
  },
  {
    "text": "into memory",
    "start": "980000",
    "end": "981120"
  },
  {
    "text": "although this may not provide a big",
    "start": "981120",
    "end": "983120"
  },
  {
    "text": "benefit for this tiny example table it",
    "start": "983120",
    "end": "985920"
  },
  {
    "text": "typically provides an over 95 reduction",
    "start": "985920",
    "end": "988399"
  },
  {
    "text": "in bytes red versus row oriented content",
    "start": "988399",
    "end": "991199"
  },
  {
    "text": "types in our production data lake tables",
    "start": "991199",
    "end": "994240"
  },
  {
    "text": "the column oriented layout of parque and",
    "start": "994240",
    "end": "996399"
  },
  {
    "text": "piero also support rapid ingestion of",
    "start": "996399",
    "end": "999519"
  },
  {
    "text": "contiguous primary key column bytes when",
    "start": "999519",
    "end": "1001839"
  },
  {
    "text": "generating sha-1 digests and allow us to",
    "start": "1001839",
    "end": "1004480"
  },
  {
    "text": "quickly free memory by dropping these",
    "start": "1004480",
    "end": "1006000"
  },
  {
    "text": "primary key columns once all digests",
    "start": "1006000",
    "end": "1007920"
  },
  {
    "text": "have been obtained",
    "start": "1007920",
    "end": "1010079"
  },
  {
    "text": "the assignment of rows to their",
    "start": "1010079",
    "end": "1011279"
  },
  {
    "text": "respective hash buckets continues in",
    "start": "1011279",
    "end": "1013360"
  },
  {
    "text": "parallel",
    "start": "1013360",
    "end": "1014480"
  },
  {
    "text": "until all rows are assigned to hash",
    "start": "1014480",
    "end": "1017040"
  },
  {
    "text": "buckets once all rows have been assigned",
    "start": "1017040",
    "end": "1019199"
  },
  {
    "text": "to hash buckets the hash bucketing step",
    "start": "1019199",
    "end": "1021360"
  },
  {
    "text": "is complete",
    "start": "1021360",
    "end": "1023120"
  },
  {
    "text": "and each hash bucket task is shut down",
    "start": "1023120",
    "end": "1026640"
  },
  {
    "text": "we're now ready to run the second",
    "start": "1026640",
    "end": "1027839"
  },
  {
    "text": "parallel step of a compaction round",
    "start": "1027839",
    "end": "1029760"
  },
  {
    "text": "deduplication",
    "start": "1029760",
    "end": "1032240"
  },
  {
    "text": "the dedup step takes the delta primary",
    "start": "1032240",
    "end": "1034400"
  },
  {
    "text": "key index from the hash bucket step as",
    "start": "1034400",
    "end": "1036240"
  },
  {
    "text": "input write source file row indices to",
    "start": "1036240",
    "end": "1038480"
  },
  {
    "text": "materialize to raise plasma object store",
    "start": "1038480",
    "end": "1040319"
  },
  {
    "text": "as output group by source file and",
    "start": "1040319",
    "end": "1042880"
  },
  {
    "text": "writes primary key index files to s3 as",
    "start": "1042880",
    "end": "1045120"
  },
  {
    "text": "output each dedupe task consumes one cpu",
    "start": "1045120",
    "end": "1048640"
  },
  {
    "text": "each task is given the in-memory delta",
    "start": "1048640",
    "end": "1050559"
  },
  {
    "text": "primary key index for a single hash",
    "start": "1050559",
    "end": "1052400"
  },
  {
    "text": "bucket as input each task is assigned a",
    "start": "1052400",
    "end": "1054799"
  },
  {
    "text": "monotonically increasing index equal to",
    "start": "1054799",
    "end": "1057200"
  },
  {
    "text": "its assigned hash bucket index this",
    "start": "1057200",
    "end": "1059360"
  },
  {
    "text": "index determines the order of",
    "start": "1059360",
    "end": "1060880"
  },
  {
    "text": "materialization of its source file row",
    "start": "1060880",
    "end": "1062960"
  },
  {
    "text": "indices relative to every other dedupe",
    "start": "1062960",
    "end": "1065360"
  },
  {
    "text": "task each hadoop task is also given a",
    "start": "1065360",
    "end": "1067919"
  },
  {
    "text": "handle to the same row counts pending",
    "start": "1067919",
    "end": "1069919"
  },
  {
    "text": "materialize ray actor source file row",
    "start": "1069919",
    "end": "1073360"
  },
  {
    "text": "counts pending materialization are",
    "start": "1073360",
    "end": "1074960"
  },
  {
    "text": "stored in the shared actor grouped by",
    "start": "1074960",
    "end": "1077280"
  },
  {
    "text": "dedupe task index these row counts are",
    "start": "1077280",
    "end": "1080160"
  },
  {
    "text": "then used to build a predictive primary",
    "start": "1080160",
    "end": "1081919"
  },
  {
    "text": "key index whose row pointers reference",
    "start": "1081919",
    "end": "1084480"
  },
  {
    "text": "rows in the compacted table pending",
    "start": "1084480",
    "end": "1086320"
  },
  {
    "text": "materialization",
    "start": "1086320",
    "end": "1087919"
  },
  {
    "text": "this provides substantial performance",
    "start": "1087919",
    "end": "1089679"
  },
  {
    "text": "and efficiency benefits versus",
    "start": "1089679",
    "end": "1091200"
  },
  {
    "text": "recomputing the same primary key index",
    "start": "1091200",
    "end": "1093600"
  },
  {
    "text": "post materialization and is a unique",
    "start": "1093600",
    "end": "1096000"
  },
  {
    "text": "feature of our design made possible by",
    "start": "1096000",
    "end": "1098000"
  },
  {
    "text": "ray",
    "start": "1098000",
    "end": "1100080"
  },
  {
    "text": "the dedupe step starts by scheduling to",
    "start": "1100080",
    "end": "1102000"
  },
  {
    "text": "do task 1 on node 1 to dedupe all rows",
    "start": "1102000",
    "end": "1104559"
  },
  {
    "text": "in hash bucket 1.",
    "start": "1104559",
    "end": "1106880"
  },
  {
    "text": "it then schedules dedupe task 2 on node",
    "start": "1106880",
    "end": "1109280"
  },
  {
    "text": "2 to do paul rose assigned to bucket 2",
    "start": "1109280",
    "end": "1112400"
  },
  {
    "text": "and so on for hash bucket 3",
    "start": "1112400",
    "end": "1114720"
  },
  {
    "text": "and 4.",
    "start": "1114720",
    "end": "1115840"
  },
  {
    "text": "as the hadoop tasks are initialized the",
    "start": "1115840",
    "end": "1118400"
  },
  {
    "text": "first thing they do is transfer all",
    "start": "1118400",
    "end": "1120559"
  },
  {
    "text": "input objects to their local node's",
    "start": "1120559",
    "end": "1122559"
  },
  {
    "text": "object store",
    "start": "1122559",
    "end": "1124160"
  },
  {
    "text": "each task then drops rows with duplicate",
    "start": "1124160",
    "end": "1126640"
  },
  {
    "text": "primary key shell and digests preserving",
    "start": "1126640",
    "end": "1129200"
  },
  {
    "text": "only the row with the max sort key value",
    "start": "1129200",
    "end": "1130960"
  },
  {
    "text": "for each unique digest note that the",
    "start": "1130960",
    "end": "1133120"
  },
  {
    "text": "runtime of this operation is linear for",
    "start": "1133120",
    "end": "1134960"
  },
  {
    "text": "each hash bucket since it never requires",
    "start": "1134960",
    "end": "1136960"
  },
  {
    "text": "the data to be fully sorted by sort key",
    "start": "1136960",
    "end": "1139600"
  },
  {
    "text": "finally the rows left are assigned to",
    "start": "1139600",
    "end": "1141360"
  },
  {
    "text": "the materialized task indices that will",
    "start": "1141360",
    "end": "1143200"
  },
  {
    "text": "process them",
    "start": "1143200",
    "end": "1145360"
  },
  {
    "start": "1145000",
    "end": "1170000"
  },
  {
    "text": "rows from the same source file are same",
    "start": "1145360",
    "end": "1147280"
  },
  {
    "text": "are assigned to the same materialized",
    "start": "1147280",
    "end": "1148880"
  },
  {
    "text": "task index and file counts are balanced",
    "start": "1148880",
    "end": "1151440"
  },
  {
    "text": "across all materialized task indices by",
    "start": "1151440",
    "end": "1153600"
  },
  {
    "text": "taking a hash of the source file id",
    "start": "1153600",
    "end": "1156160"
  },
  {
    "text": "modulo the max number of materialized",
    "start": "1156160",
    "end": "1158080"
  },
  {
    "text": "tasks that our cluster can support",
    "start": "1158080",
    "end": "1160960"
  },
  {
    "text": "with all rows to materialize assigned to",
    "start": "1160960",
    "end": "1162799"
  },
  {
    "text": "their respective materialized task",
    "start": "1162799",
    "end": "1164480"
  },
  {
    "text": "indices we're ready for the final step",
    "start": "1164480",
    "end": "1166640"
  },
  {
    "text": "of our compaction round materialization",
    "start": "1166640",
    "end": "1170400"
  },
  {
    "start": "1170000",
    "end": "1235000"
  },
  {
    "text": "the materialized step takes all delta",
    "start": "1170400",
    "end": "1172320"
  },
  {
    "text": "file row indices to materialize from the",
    "start": "1172320",
    "end": "1174240"
  },
  {
    "text": "dedupe step as input and writes s3 files",
    "start": "1174240",
    "end": "1176960"
  },
  {
    "text": "annotated with data lake metadata as",
    "start": "1176960",
    "end": "1178880"
  },
  {
    "text": "output",
    "start": "1178880",
    "end": "1180000"
  },
  {
    "text": "the materialized step starts by",
    "start": "1180000",
    "end": "1181679"
  },
  {
    "text": "scheduling materialize task 1 on node 1",
    "start": "1181679",
    "end": "1185039"
  },
  {
    "text": "and then scheduling materialize task 2",
    "start": "1185039",
    "end": "1187120"
  },
  {
    "text": "on node 2.",
    "start": "1187120",
    "end": "1189039"
  },
  {
    "text": "again as the materialized tasks are",
    "start": "1189039",
    "end": "1190720"
  },
  {
    "text": "initialized the first thing they do is",
    "start": "1190720",
    "end": "1192240"
  },
  {
    "text": "transfer all input objects to their",
    "start": "1192240",
    "end": "1193919"
  },
  {
    "text": "local node's object store each",
    "start": "1193919",
    "end": "1196160"
  },
  {
    "text": "materialized task starts by generating a",
    "start": "1196160",
    "end": "1198320"
  },
  {
    "text": "bit mask of row indices to filter from",
    "start": "1198320",
    "end": "1200400"
  },
  {
    "text": "each file",
    "start": "1200400",
    "end": "1202159"
  },
  {
    "text": "which are read into an in-memory pi",
    "start": "1202159",
    "end": "1203840"
  },
  {
    "text": "arrow table",
    "start": "1203840",
    "end": "1205440"
  },
  {
    "text": "and then written to a single data set in",
    "start": "1205440",
    "end": "1207200"
  },
  {
    "text": "the compacted destination table's",
    "start": "1207200",
    "end": "1208960"
  },
  {
    "text": "partition stream",
    "start": "1208960",
    "end": "1210559"
  },
  {
    "text": "this continues until all rows have been",
    "start": "1210559",
    "end": "1212480"
  },
  {
    "text": "materialized to form the final compacted",
    "start": "1212480",
    "end": "1215760"
  },
  {
    "text": "output table",
    "start": "1215760",
    "end": "1217200"
  },
  {
    "text": "again we have pi arrow and par k to",
    "start": "1217200",
    "end": "1219200"
  },
  {
    "text": "thank for the i o efficiency of this",
    "start": "1219200",
    "end": "1220880"
  },
  {
    "text": "step since parquet provides the",
    "start": "1220880",
    "end": "1222720"
  },
  {
    "text": "necessary file structure to support",
    "start": "1222720",
    "end": "1224240"
  },
  {
    "text": "efficient road lookup and extraction by",
    "start": "1224240",
    "end": "1226080"
  },
  {
    "text": "index empire provides an implementation",
    "start": "1226080",
    "end": "1228960"
  },
  {
    "text": "that efficiently leverages this",
    "start": "1228960",
    "end": "1230559"
  },
  {
    "text": "structure to both read it into memory",
    "start": "1230559",
    "end": "1232960"
  },
  {
    "text": "and write it back out to new part k",
    "start": "1232960",
    "end": "1234640"
  },
  {
    "text": "files",
    "start": "1234640",
    "end": "1236080"
  },
  {
    "start": "1235000",
    "end": "1285000"
  },
  {
    "text": "to recap the whole process the",
    "start": "1236080",
    "end": "1237919"
  },
  {
    "text": "compaction round just took this table as",
    "start": "1237919",
    "end": "1239679"
  },
  {
    "text": "input",
    "start": "1239679",
    "end": "1240799"
  },
  {
    "text": "dropped these rows from it",
    "start": "1240799",
    "end": "1242880"
  },
  {
    "text": "and wrote this result back to s3 in",
    "start": "1242880",
    "end": "1245520"
  },
  {
    "text": "reality we would never need to",
    "start": "1245520",
    "end": "1247120"
  },
  {
    "text": "distribute the processing of such a",
    "start": "1247120",
    "end": "1248880"
  },
  {
    "text": "small table but remember the primary",
    "start": "1248880",
    "end": "1251440"
  },
  {
    "text": "purpose of our application is to apply",
    "start": "1251440",
    "end": "1253280"
  },
  {
    "text": "inserts updates and deletes to petabyte",
    "start": "1253280",
    "end": "1256000"
  },
  {
    "text": "scale production partition streams and",
    "start": "1256000",
    "end": "1258240"
  },
  {
    "text": "this tiny table simply provided an",
    "start": "1258240",
    "end": "1260640"
  },
  {
    "text": "illustration of how the algorithm works",
    "start": "1260640",
    "end": "1263919"
  },
  {
    "text": "finally let's assume we completed both",
    "start": "1263919",
    "end": "1266880"
  },
  {
    "text": "of our compaction sessions",
    "start": "1266880",
    "end": "1270000"
  },
  {
    "text": "we publish a final success event for",
    "start": "1270000",
    "end": "1272400"
  },
  {
    "text": "each session and let the job run event",
    "start": "1272400",
    "end": "1274559"
  },
  {
    "text": "handler pick up these events and tell",
    "start": "1274559",
    "end": "1276640"
  },
  {
    "text": "the orchestrator that the compaction job",
    "start": "1276640",
    "end": "1278480"
  },
  {
    "text": "run succeeded",
    "start": "1278480",
    "end": "1280080"
  },
  {
    "text": "while we shut down",
    "start": "1280080",
    "end": "1281760"
  },
  {
    "text": "the array cluster associated with each",
    "start": "1281760",
    "end": "1283679"
  },
  {
    "text": "session",
    "start": "1283679",
    "end": "1286080"
  },
  {
    "start": "1285000",
    "end": "1361000"
  },
  {
    "text": "to determine the adherence of our design",
    "start": "1286080",
    "end": "1287840"
  },
  {
    "text": "and implementation to our goals we ran",
    "start": "1287840",
    "end": "1289840"
  },
  {
    "text": "over 400 compaction test cases ingesting",
    "start": "1289840",
    "end": "1292240"
  },
  {
    "text": "production parquet datasets the results",
    "start": "1292240",
    "end": "1294799"
  },
  {
    "text": "exceeded our expectations with high sla",
    "start": "1294799",
    "end": "1297280"
  },
  {
    "text": "adherence rates and over an order of",
    "start": "1297280",
    "end": "1299360"
  },
  {
    "text": "magnitude improvement versus our emr",
    "start": "1299360",
    "end": "1301600"
  },
  {
    "text": "spark compactor in terms of performance",
    "start": "1301600",
    "end": "1303840"
  },
  {
    "text": "scalability and efficiency we were able",
    "start": "1303840",
    "end": "1306480"
  },
  {
    "text": "to achieve a maximum throughput rate of",
    "start": "1306480",
    "end": "1308159"
  },
  {
    "text": "about 24 terabytes a minute on an 8000",
    "start": "1308159",
    "end": "1310559"
  },
  {
    "text": "vcpu cluster compacting 117 tabi bytes",
    "start": "1310559",
    "end": "1314320"
  },
  {
    "text": "of parquet input data this is about 13",
    "start": "1314320",
    "end": "1316960"
  },
  {
    "text": "times faster",
    "start": "1316960",
    "end": "1318400"
  },
  {
    "text": "than the throughput rate we've been able",
    "start": "1318400",
    "end": "1319919"
  },
  {
    "text": "to achieve on spark we were also able to",
    "start": "1319919",
    "end": "1322640"
  },
  {
    "text": "compact a one pebby byte partition",
    "start": "1322640",
    "end": "1324400"
  },
  {
    "text": "stream on a cluster providing only 27",
    "start": "1324400",
    "end": "1326400"
  },
  {
    "text": "tbi bytes of memory which is about 12",
    "start": "1326400",
    "end": "1328880"
  },
  {
    "text": "times larger than the largest partition",
    "start": "1328880",
    "end": "1330720"
  },
  {
    "text": "stream compacted on an equivalent spark",
    "start": "1330720",
    "end": "1332720"
  },
  {
    "text": "emr cluster our most efficient",
    "start": "1332720",
    "end": "1334880"
  },
  {
    "text": "compaction job run cost 24 cents per",
    "start": "1334880",
    "end": "1337280"
  },
  {
    "text": "terabyte and the average job run cost 46",
    "start": "1337280",
    "end": "1340320"
  },
  {
    "text": "cents per terabyte yielding an",
    "start": "1340320",
    "end": "1342080"
  },
  {
    "text": "impressive 91 cost reduction versus",
    "start": "1342080",
    "end": "1344400"
  },
  {
    "text": "spark on emr our success rate of 99.3",
    "start": "1344400",
    "end": "1347679"
  },
  {
    "text": "percent provides a three percent",
    "start": "1347679",
    "end": "1349120"
  },
  {
    "text": "improvement over the trailing one-year",
    "start": "1349120",
    "end": "1350559"
  },
  {
    "text": "success rate of spark and also forms our",
    "start": "1350559",
    "end": "1352720"
  },
  {
    "text": "sla adherence rate since we've been able",
    "start": "1352720",
    "end": "1354799"
  },
  {
    "text": "to meet the expected sla calculated for",
    "start": "1354799",
    "end": "1357200"
  },
  {
    "text": "each compaction job run as long as it",
    "start": "1357200",
    "end": "1359360"
  },
  {
    "text": "succeeds",
    "start": "1359360",
    "end": "1361520"
  },
  {
    "text": "we also compiled a reference table based",
    "start": "1361520",
    "end": "1363360"
  },
  {
    "text": "on these results providing guidelines",
    "start": "1363360",
    "end": "1365120"
  },
  {
    "text": "for the recommended number of hash",
    "start": "1365120",
    "end": "1366320"
  },
  {
    "text": "buckets to use when convecting terabytes",
    "start": "1366320",
    "end": "1368720"
  },
  {
    "text": "or petabytes of data which typically",
    "start": "1368720",
    "end": "1370640"
  },
  {
    "text": "correspond to billions or trillions of",
    "start": "1370640",
    "end": "1372840"
  },
  {
    "text": "rows as more hash buckets are required",
    "start": "1372840",
    "end": "1375360"
  },
  {
    "text": "to compact increasingly large data sets",
    "start": "1375360",
    "end": "1377679"
  },
  {
    "text": "the cost efficiency of compaction slowly",
    "start": "1377679",
    "end": "1379760"
  },
  {
    "text": "decreases so we recommend starting with",
    "start": "1379760",
    "end": "1381919"
  },
  {
    "text": "the minimum number of hash buckets",
    "start": "1381919",
    "end": "1383679"
  },
  {
    "text": "required for your use case",
    "start": "1383679",
    "end": "1386559"
  },
  {
    "text": "we can also visualize the correlation",
    "start": "1386559",
    "end": "1388080"
  },
  {
    "text": "between cluster size throughput rate and",
    "start": "1388080",
    "end": "1390320"
  },
  {
    "text": "hash bucket count as follows in general",
    "start": "1390320",
    "end": "1392799"
  },
  {
    "text": "increasing cluster size provides",
    "start": "1392799",
    "end": "1394480"
  },
  {
    "text": "slightly sublinear speed up while",
    "start": "1394480",
    "end": "1396480"
  },
  {
    "text": "increasing hash bucket counts beyond the",
    "start": "1396480",
    "end": "1398400"
  },
  {
    "text": "number of cluster cpus results in",
    "start": "1398400",
    "end": "1400240"
  },
  {
    "text": "slowdown however note that decreasing",
    "start": "1400240",
    "end": "1403360"
  },
  {
    "text": "hash bucket counts too far below the",
    "start": "1403360",
    "end": "1405679"
  },
  {
    "text": "number of available cluster cpus can",
    "start": "1405679",
    "end": "1408000"
  },
  {
    "text": "also result in lower throughput due to",
    "start": "1408000",
    "end": "1410159"
  },
  {
    "text": "insufficient parallelization of the",
    "start": "1410159",
    "end": "1411919"
  },
  {
    "text": "dedupe step",
    "start": "1411919",
    "end": "1413520"
  },
  {
    "text": "in general we can control the time it",
    "start": "1413520",
    "end": "1415200"
  },
  {
    "text": "takes to compact increasingly large data",
    "start": "1415200",
    "end": "1417200"
  },
  {
    "text": "sets and stay within a desired sla by",
    "start": "1417200",
    "end": "1419760"
  },
  {
    "text": "simply increasing the size of our",
    "start": "1419760",
    "end": "1421039"
  },
  {
    "text": "cluster we'd ideally like to provision a",
    "start": "1421039",
    "end": "1423279"
  },
  {
    "text": "sufficiently large cluster to complete",
    "start": "1423279",
    "end": "1425200"
  },
  {
    "text": "every compaction session in a single",
    "start": "1425200",
    "end": "1426960"
  },
  {
    "text": "round this is not always practical also",
    "start": "1426960",
    "end": "1429440"
  },
  {
    "text": "note that increasing the cluster size",
    "start": "1429440",
    "end": "1431360"
  },
  {
    "text": "for a data set that already fits",
    "start": "1431360",
    "end": "1432720"
  },
  {
    "text": "comfortably within memory provides",
    "start": "1432720",
    "end": "1434960"
  },
  {
    "text": "minimal to new improvements to latency",
    "start": "1434960",
    "end": "1437520"
  },
  {
    "text": "the far right data point of this graph",
    "start": "1437520",
    "end": "1439200"
  },
  {
    "text": "illustrates the latency impact of",
    "start": "1439200",
    "end": "1440720"
  },
  {
    "text": "switching to multi-round compaction on a",
    "start": "1440720",
    "end": "1442880"
  },
  {
    "text": "cluster too small to fit intermediate",
    "start": "1442880",
    "end": "1445200"
  },
  {
    "text": "output in memory in this case the bytes",
    "start": "1445200",
    "end": "1447919"
  },
  {
    "text": "process per cpu increased about 10-fold",
    "start": "1447919",
    "end": "1450480"
  },
  {
    "text": "while the latency increased about",
    "start": "1450480",
    "end": "1451919"
  },
  {
    "text": "23-fold",
    "start": "1451919",
    "end": "1454240"
  },
  {
    "text": "using ec2 on-demand pricing the cost to",
    "start": "1454240",
    "end": "1456960"
  },
  {
    "text": "compact 10 pebby bytes of data typically",
    "start": "1456960",
    "end": "1459200"
  },
  {
    "text": "hovers around 3 500",
    "start": "1459200",
    "end": "1462240"
  },
  {
    "text": "but if you need to run multi-round",
    "start": "1462240",
    "end": "1463919"
  },
  {
    "text": "compaction on small clusters with a",
    "start": "1463919",
    "end": "1466240"
  },
  {
    "text": "large number of hash buckets your cost",
    "start": "1466240",
    "end": "1468400"
  },
  {
    "text": "per byte compacted may nearly double on",
    "start": "1468400",
    "end": "1470960"
  },
  {
    "text": "the other hand if your compaction",
    "start": "1470960",
    "end": "1473039"
  },
  {
    "text": "sessions typically handle less than 100",
    "start": "1473039",
    "end": "1474960"
  },
  {
    "text": "debit bytes of data your cost may be",
    "start": "1474960",
    "end": "1476799"
  },
  {
    "text": "closer to 2 500 per 10 pebby bytes",
    "start": "1476799",
    "end": "1480159"
  },
  {
    "text": "compacted",
    "start": "1480159",
    "end": "1482240"
  },
  {
    "start": "1482000",
    "end": "1532000"
  },
  {
    "text": "now it should be noted that we hit a few",
    "start": "1482240",
    "end": "1484400"
  },
  {
    "text": "bumps in the road before achieving these",
    "start": "1484400",
    "end": "1486400"
  },
  {
    "text": "results",
    "start": "1486400",
    "end": "1487520"
  },
  {
    "text": "one of the first problems we encountered",
    "start": "1487520",
    "end": "1488880"
  },
  {
    "text": "involved exchanging far too many objects",
    "start": "1488880",
    "end": "1490799"
  },
  {
    "text": "between successive parallel steps of our",
    "start": "1490799",
    "end": "1492480"
  },
  {
    "text": "compaction workflow which we worked",
    "start": "1492480",
    "end": "1494480"
  },
  {
    "text": "around by grouping multiple individual",
    "start": "1494480",
    "end": "1496320"
  },
  {
    "text": "object refs inside a single object roth",
    "start": "1496320",
    "end": "1498799"
  },
  {
    "text": "to a larger data structure",
    "start": "1498799",
    "end": "1500400"
  },
  {
    "text": "and then having each task iteratively",
    "start": "1500400",
    "end": "1502159"
  },
  {
    "text": "process the embedded object refs a",
    "start": "1502159",
    "end": "1504480"
  },
  {
    "text": "second problem that we worked around",
    "start": "1504480",
    "end": "1506720"
  },
  {
    "text": "involved very high latency and out of",
    "start": "1506720",
    "end": "1508960"
  },
  {
    "text": "memory errors stemming from raised",
    "start": "1508960",
    "end": "1510559"
  },
  {
    "text": "reference counting and garbage",
    "start": "1510559",
    "end": "1511679"
  },
  {
    "text": "collection mechanisms to work around",
    "start": "1511679",
    "end": "1513600"
  },
  {
    "text": "this we assumed the responsibility of",
    "start": "1513600",
    "end": "1515760"
  },
  {
    "text": "garbage collection and started hiding",
    "start": "1515760",
    "end": "1517520"
  },
  {
    "text": "our embedded object references from",
    "start": "1517520",
    "end": "1519120"
  },
  {
    "text": "rey's reference counter by serializing",
    "start": "1519120",
    "end": "1521120"
  },
  {
    "text": "and deserializing them via rey cloud",
    "start": "1521120",
    "end": "1523600"
  },
  {
    "text": "pickle so to summarize these results",
    "start": "1523600",
    "end": "1526400"
  },
  {
    "text": "took time to achieve and required about",
    "start": "1526400",
    "end": "1528320"
  },
  {
    "text": "13 major revisions to our compactor",
    "start": "1528320",
    "end": "1530400"
  },
  {
    "text": "implementation on rey",
    "start": "1530400",
    "end": "1533200"
  },
  {
    "start": "1532000",
    "end": "1580000"
  },
  {
    "text": "finally appear s3 implementation the",
    "start": "1533200",
    "end": "1536240"
  },
  {
    "text": "flash compactor is pending contribution",
    "start": "1536240",
    "end": "1538720"
  },
  {
    "text": "back to open source will initially be",
    "start": "1538720",
    "end": "1541200"
  },
  {
    "text": "staged in the experimental branch of the",
    "start": "1541200",
    "end": "1543600"
  },
  {
    "text": "amazon raid github repo",
    "start": "1543600",
    "end": "1545760"
  },
  {
    "text": "we'll also discuss its design in greater",
    "start": "1545760",
    "end": "1548000"
  },
  {
    "text": "detail and follow-up blog posts together",
    "start": "1548000",
    "end": "1550559"
  },
  {
    "text": "with the progress we made towards",
    "start": "1550559",
    "end": "1551919"
  },
  {
    "text": "shifting other data lake table",
    "start": "1551919",
    "end": "1553760"
  },
  {
    "text": "management activities over toward ray",
    "start": "1553760",
    "end": "1556640"
  },
  {
    "text": "we would also like to see all tables in",
    "start": "1556640",
    "end": "1558799"
  },
  {
    "text": "our data lake compacted with ray in the",
    "start": "1558799",
    "end": "1560240"
  },
  {
    "text": "near future and would like to expose",
    "start": "1560240",
    "end": "1562080"
  },
  {
    "text": "fully server-less ray jobs to all",
    "start": "1562080",
    "end": "1563600"
  },
  {
    "text": "internal users of our data lake that",
    "start": "1563600",
    "end": "1565600"
  },
  {
    "text": "allow them to more easily leverage the",
    "start": "1565600",
    "end": "1567200"
  },
  {
    "text": "power of ray in their day-to-day tasks",
    "start": "1567200",
    "end": "1569679"
  },
  {
    "text": "so thanks everyone for tuning in and",
    "start": "1569679",
    "end": "1571679"
  },
  {
    "text": "feel free to reach out to me on the ray",
    "start": "1571679",
    "end": "1573279"
  },
  {
    "text": "community slack with questions comments",
    "start": "1573279",
    "end": "1575919"
  },
  {
    "text": "or just to say hi",
    "start": "1575919",
    "end": "1579799"
  }
]