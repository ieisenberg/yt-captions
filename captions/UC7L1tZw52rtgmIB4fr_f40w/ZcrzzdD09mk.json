[
  {
    "start": "0",
    "end": "160000"
  },
  {
    "text": "had some questions regarding uh sac generally regarding the algorithm or",
    "start": "80",
    "end": "5200"
  },
  {
    "text": "regarding the rlab implementation and maybe initially or something just regarding the algorithms",
    "start": "5200",
    "end": "10639"
  },
  {
    "text": "uh performance questions and uh some very general questions i guess",
    "start": "10639",
    "end": "15679"
  },
  {
    "text": "well i've been training our sach we've noticed that a lot of the controls um are really noisy",
    "start": "15679",
    "end": "23760"
  },
  {
    "text": "and we're wondering like what could we do to reduce that noise whether it be",
    "start": "23760",
    "end": "30080"
  },
  {
    "text": "introducing say some sort of reward function component where it looks the delta of the",
    "start": "30080",
    "end": "35280"
  },
  {
    "text": "uh change in the controls or is there something else we should tune in our agent",
    "start": "35280",
    "end": "41360"
  },
  {
    "text": "right so so your your actions are super nice is that what you're saying does does the algorithm uh learn let's",
    "start": "41360",
    "end": "47760"
  },
  {
    "text": "start with that yeah does it learn to begin with",
    "start": "47760",
    "end": "52879"
  },
  {
    "text": "yeah i like it it converges eventually um hey so my question is regarding um an sac",
    "start": "52879",
    "end": "60559"
  },
  {
    "text": "agent uh yielding a bunch of noisy actions and even while uh it conver even after",
    "start": "60559",
    "end": "66240"
  },
  {
    "text": "it converges to our optimal state it's we are still seeing um very noisy action so you're wondering",
    "start": "66240",
    "end": "73680"
  },
  {
    "text": "what would be the best way to tame that whether it be through a uh a set of reward uh reward functions or",
    "start": "73680",
    "end": "80479"
  },
  {
    "text": "some sort of hyper parameter tuning but does it does it reach some optimal state",
    "start": "80479",
    "end": "85920"
  },
  {
    "text": "and then it kind of collapses again or it's just the the entropy just keeps",
    "start": "85920",
    "end": "91759"
  },
  {
    "text": "or stays very high for some for some real reason like like how happy are you with your with the policies performance",
    "start": "91759",
    "end": "97439"
  },
  {
    "text": "in the end let's say despite the the noisy actions which is a very valid problem that we should that",
    "start": "97439",
    "end": "103920"
  },
  {
    "text": "you we should fix um is it learning at all or is it just really yeah i mean and the prior one we",
    "start": "103920",
    "end": "110000"
  },
  {
    "text": "had a very simple environment and we were we settled on it um now that we're",
    "start": "110000",
    "end": "115040"
  },
  {
    "text": "having to do a bit more complex tasks uh by the way the",
    "start": "115040",
    "end": "120159"
  },
  {
    "text": "sac is communicating is working with a dynamic simulator",
    "start": "120159",
    "end": "125920"
  },
  {
    "text": "yeah so uh you know we want to minimize that no uh that noise in the actions as much as possible that way you know that",
    "start": "125920",
    "end": "133360"
  },
  {
    "text": "doesn't get too screwed up right okay so like of course like initially when",
    "start": "133360",
    "end": "139120"
  },
  {
    "text": "you start training and the entropy is going to be super high so it's gonna do that yeah that makes sense yeah yeah",
    "start": "139120",
    "end": "144640"
  },
  {
    "text": "exploration and then yeah as it's training it doesn't seem to be um",
    "start": "144640",
    "end": "151599"
  },
  {
    "text": "uh i mean the performance stays good but it's just again the actions are noisy yeah i see i see it okay yeah um let's",
    "start": "151599",
    "end": "159120"
  },
  {
    "text": "see uh um what is the action space like is it are the values around zero or like minus one",
    "start": "159120",
    "end": "166000"
  },
  {
    "start": "160000",
    "end": "516000"
  },
  {
    "text": "um yeah we have uh for example is a 4d action space uh all",
    "start": "166000",
    "end": "172640"
  },
  {
    "text": "all actions go from negative one to one okay",
    "start": "172640",
    "end": "177760"
  },
  {
    "text": "okay and uh okay so that's and then your environment also accepts these minus one",
    "start": "177760",
    "end": "183680"
  },
  {
    "text": "to one doesn't have to be scaled or anything uh we actually have to rescale them uh okay",
    "start": "183680",
    "end": "188720"
  },
  {
    "text": "yeah then we feed rescale values to the simulator",
    "start": "188720",
    "end": "193840"
  },
  {
    "text": "uh and how do you how does that happen so",
    "start": "193840",
    "end": "199239"
  },
  {
    "text": "ah maybe okay so you basically you do that yourself basically the scaling before you send it to the simulator you",
    "start": "199599",
    "end": "205040"
  },
  {
    "text": "do something in your environment that's what i'm trying to get at is maybe in",
    "start": "205040",
    "end": "210720"
  },
  {
    "text": "your replay buffer you have these these scaled actions or did you do did you make sure that that's not the case",
    "start": "210720",
    "end": "216959"
  },
  {
    "text": "um shouldn't be like your so so sac has this uh squash gaussian distribution",
    "start": "216959",
    "end": "223440"
  },
  {
    "text": "right it's a very specific thing about it that you don't have with the other algorithms so everything that comes out of your",
    "start": "223440",
    "end": "229440"
  },
  {
    "text": "network or out of your sound or of your policy uh compute single action or compute",
    "start": "229440",
    "end": "234879"
  },
  {
    "text": "action um is already squashed which means it's nicely sampled",
    "start": "234879",
    "end": "240080"
  },
  {
    "text": "and then the distribution itself takes care of the the making sure it's everything is between minus one and one",
    "start": "240080",
    "end": "245760"
  },
  {
    "text": "um and then if you're if you're actually if you specify your",
    "start": "245760",
    "end": "251599"
  },
  {
    "text": "environment's action space as already the scaled one let's say let's say minus 100 to 100 then our lip would",
    "start": "251599",
    "end": "259120"
  },
  {
    "text": "take care of that and it would allow you to basically not do anything you would it would take the the value that comes out of sac which is between minus one",
    "start": "259120",
    "end": "266080"
  },
  {
    "text": "and one would automatically scale it send to the environment and make sure that the",
    "start": "266080",
    "end": "272320"
  },
  {
    "text": "uh that the algorithm itself still keeps working in this normalized small minus one to one uh action space",
    "start": "272320",
    "end": "279440"
  },
  {
    "text": "um [Music] so you're you're saying that you're when you when i say when i take your environment",
    "start": "279440",
    "end": "284720"
  },
  {
    "text": "and i say environment dot action space like what would i get would i get minus one to one or would i get you get minus",
    "start": "284720",
    "end": "290000"
  },
  {
    "text": "one to one okay okay and then you scale it inside the environment got it okay okay",
    "start": "290000",
    "end": "296400"
  },
  {
    "text": "maybe that's fine but if you if you handle all the scaling inside the environment that should be okay then",
    "start": "296400",
    "end": "301520"
  },
  {
    "text": "yeah so i'm just just brainstorming here so uh entropy and sac how does this is this something",
    "start": "301520",
    "end": "308000"
  },
  {
    "text": "uh did you did you uh monitor your alpha um parameter or your alpha loss does it",
    "start": "308000",
    "end": "313759"
  },
  {
    "text": "does it um let me hold it it's when you wanted my alpha loss",
    "start": "313759",
    "end": "320080"
  },
  {
    "text": "yeah correctly i wanted to check the the what is what is the what it does of a loss uh the alpha value itself or the",
    "start": "320080",
    "end": "325759"
  },
  {
    "text": "log alpha whatever whatever so it starts at zero and then it goes down to negative eight and slowly climbs back",
    "start": "325759",
    "end": "332479"
  },
  {
    "text": "up but in a rather noisy way it's kind of like an inverted noisy gaussian kind of yeah okay",
    "start": "332479",
    "end": "339120"
  },
  {
    "text": "and the the alpha value itself not the alpha loss but the alpha value itself uh alpha value uh starts at one and",
    "start": "339120",
    "end": "347600"
  },
  {
    "text": "goes down to zero in a logarithmic curve okay okay that sounds that sounds good actually um",
    "start": "347600",
    "end": "356960"
  },
  {
    "text": "oh what is your how do you which alpha do you start with maybe you're starting with",
    "start": "357680",
    "end": "363600"
  },
  {
    "text": "one point yeah i haven't i i'm using the default alpha for essays yeah sure",
    "start": "363600",
    "end": "369199"
  },
  {
    "text": "sure um that may be worth playing around with that has some effect on the entropy",
    "start": "369199",
    "end": "374319"
  },
  {
    "text": "uh maybe you want to start with a smaller value you may want to play around with that and you also may want to play around",
    "start": "374319",
    "end": "379360"
  },
  {
    "text": "with the target entropy yeah so the target entropy by default is the the product",
    "start": "379360",
    "end": "385520"
  },
  {
    "text": "or the negative product of your action spaces shape which you said it was 40 is that correct",
    "start": "385520",
    "end": "392160"
  },
  {
    "text": "it's uh yeah 4d oh four d mm-hmm",
    "start": "392160",
    "end": "397280"
  },
  {
    "text": "four four dimensional yeah so then it's just uh then it's just four that are entropy okay uh or minus four so it's",
    "start": "397280",
    "end": "402800"
  },
  {
    "text": "minus minus four uh is your target entropy so maybe you wanna set this to let's see maybe set this to even lower",
    "start": "402800",
    "end": "408800"
  },
  {
    "text": "values um try like minus 10 or minus 40 or some some orders of magnitudes",
    "start": "408800",
    "end": "415840"
  },
  {
    "text": "to figure out whether that helps you reduce reduce action entropy so this it could simply be that it could be that",
    "start": "415840",
    "end": "421759"
  },
  {
    "text": "you just have too much entrepreneur enforced by your wire loss into the network um gotcha",
    "start": "421759",
    "end": "428880"
  },
  {
    "text": "yeah i'll mess around with that yeah one thing to try out definitely um",
    "start": "428880",
    "end": "434240"
  },
  {
    "text": "if this is like purely like a problem in when you do inference in or like evaluation um you can try to set explore equals",
    "start": "434240",
    "end": "440960"
  },
  {
    "text": "false um so when you when you uh let's say you're done with training and you think",
    "start": "440960",
    "end": "446560"
  },
  {
    "text": "it's okay uh reward this is okay but yeah my actions are super noisy um you should try uh just simply",
    "start": "446560",
    "end": "453120"
  },
  {
    "text": "switching off when you do a compute single action in your in your inference or in your",
    "start": "453120",
    "end": "458160"
  },
  {
    "text": "you should probably try to set explore equals false you should generally always do that uh because during inference or",
    "start": "458160",
    "end": "464080"
  },
  {
    "text": "during evaluation most of the time it's better to not have uh sampling or stochasticity switched on",
    "start": "464080",
    "end": "470160"
  },
  {
    "text": "or like exploration switched on um so that could help there too that's to stabilize things it could also like be",
    "start": "470160",
    "end": "477919"
  },
  {
    "text": "not working well but you have to try this uh i would i would recommend trying explore equals faults first so when you",
    "start": "477919",
    "end": "483520"
  },
  {
    "text": "do what i mean or is this a problem that's that you already think it's it's a problem during during",
    "start": "483520",
    "end": "489199"
  },
  {
    "text": "training phase or is it just yeah supposed to problem during training phase like late into training",
    "start": "489199",
    "end": "494720"
  },
  {
    "text": "absolutely so then try to try the uh entropy",
    "start": "494720",
    "end": "500080"
  },
  {
    "text": "settings first the target entropy sounds good thank you sure",
    "start": "500080",
    "end": "507120"
  },
  {
    "text": "excuse me i'm wondering what's the difference between compute action once exploration is off and training the",
    "start": "507120",
    "end": "513680"
  },
  {
    "text": "agent once the exploration is off yeah great question um so during",
    "start": "513680",
    "end": "520080"
  },
  {
    "start": "519000",
    "end": "714000"
  },
  {
    "text": "training um [Music] uh we basically call the same method in",
    "start": "520080",
    "end": "525120"
  },
  {
    "text": "our samplers uh we call the the record direct there's a policy method as well uh which the policy sits inside the",
    "start": "525120",
    "end": "530880"
  },
  {
    "text": "agent or in the inside the algorithm but it's kind of the same thing you basically tell the neural network to",
    "start": "530880",
    "end": "536080"
  },
  {
    "text": "produce an output um and during training you certainly want exploration to be on because you want to",
    "start": "536080",
    "end": "543360"
  },
  {
    "text": "gather exploratory data right and you don't you want to explore the state space and and",
    "start": "543360",
    "end": "548399"
  },
  {
    "text": "learn from it um so it almost never makes sense uh in",
    "start": "548399",
    "end": "553839"
  },
  {
    "text": "your regular conflict in your regular training conflict to switch explore to to false",
    "start": "553839",
    "end": "560000"
  },
  {
    "text": "but most of the time when you do when you're done with training or you uh you're evaluating maybe you set up evaluation workers um and you you",
    "start": "560000",
    "end": "566959"
  },
  {
    "text": "overwrite some of those uh configurations in your evaluation config um it makes sense to switch off exploration every algorithm has a",
    "start": "566959",
    "end": "573680"
  },
  {
    "text": "different exploration strategy um for let me give you a simple example dqn uses epsilon greedy right so every every",
    "start": "573680",
    "end": "579600"
  },
  {
    "text": "n time steps you act randomly um you don't listen to your q network you",
    "start": "579600",
    "end": "584640"
  },
  {
    "text": "just actually pick a random action from the action space um you want to do that during training because you want to explore the the",
    "start": "584640",
    "end": "591120"
  },
  {
    "text": "state space but during inference you most certainly don't want to act randomly at any given point right you",
    "start": "591120",
    "end": "596560"
  },
  {
    "text": "always want to listen to your q network and pick the greedy action that has a large sku value",
    "start": "596560",
    "end": "603120"
  },
  {
    "text": "just does that make sense um yeah so technically the bots should perform similar if i use train with the",
    "start": "603120",
    "end": "609680"
  },
  {
    "text": "exploration off after the bot is fully trained let's say during the evaluation or if i use this compute single action",
    "start": "609680",
    "end": "617360"
  },
  {
    "text": "with expiration off correct switch it off it will it will normally again it will",
    "start": "617360",
    "end": "622880"
  },
  {
    "text": "normally perform stronger because it doesn't have this this randomness at all anymore which it shouldn't it should be already trained it should be already",
    "start": "622880",
    "end": "629120"
  },
  {
    "text": "smart enough to um to give you the right q values to pick from or the right log",
    "start": "629120",
    "end": "634720"
  },
  {
    "text": "it's two to um um yeah to sample from or like two to also",
    "start": "634720",
    "end": "640560"
  },
  {
    "text": "breedily pick from in case you have the expiration switched off so um it should normally if you should",
    "start": "640560",
    "end": "646160"
  },
  {
    "text": "normally see like another boost in in your evaluation because you're switching off expiration and now you're really just focusing on the on the perfect",
    "start": "646160",
    "end": "652800"
  },
  {
    "text": "solution right and you're not distracting yourself anymore with trying to sample um",
    "start": "652800",
    "end": "658480"
  },
  {
    "text": "exploratory stuff like you really would not want to do that it doesn't make sense because you're doing it for instance you don't you",
    "start": "658480",
    "end": "663920"
  },
  {
    "text": "don't use the data for anything else other than yeah in your production system",
    "start": "663920",
    "end": "671920"
  },
  {
    "text": "um there are i'm not saying there are probably some weird cases where the optimal policy is like stochastic",
    "start": "672000",
    "end": "678720"
  },
  {
    "text": "and you need uh like if you play like like rock paper scissors the optimal policy is a random",
    "start": "678720",
    "end": "685120"
  },
  {
    "text": "one so in this case switching off exploration is probably really bad um but that's uh i feel like that's if",
    "start": "685120",
    "end": "691760"
  },
  {
    "text": "we have learned that this is more like a theoretical problem and it's in practice and in real systems it's almost always",
    "start": "691760",
    "end": "697279"
  },
  {
    "text": "better to switch off expiration once you are in in pure inference modes slash evaluation mode",
    "start": "697279",
    "end": "703440"
  },
  {
    "text": "yeah i am it's more related to this curriculum learning um",
    "start": "703440",
    "end": "709440"
  },
  {
    "text": "so let's say for our case um we we are training a continuous",
    "start": "709440",
    "end": "715360"
  },
  {
    "start": "714000",
    "end": "750000"
  },
  {
    "text": "bought with continuous action space let's say it's it's like it has three",
    "start": "715360",
    "end": "720959"
  },
  {
    "text": "actions but later on that we kind of make the task more and more",
    "start": "720959",
    "end": "726800"
  },
  {
    "text": "complex the number of action space also increases um so i was thinking is there any way",
    "start": "726800",
    "end": "732880"
  },
  {
    "text": "that we can leverage this uh curriculum learning approach to you to leverage the",
    "start": "732880",
    "end": "738079"
  },
  {
    "text": "pre-trained lower action space part and then start from there or any any approach that we",
    "start": "738079",
    "end": "744800"
  },
  {
    "text": "can leverage the simpler bot for the more complex approach that's a very nice question um",
    "start": "744800",
    "end": "753120"
  },
  {
    "start": "750000",
    "end": "1007000"
  },
  {
    "text": "that's a tough one uh it's kind of it's almost like saying i want to change my network architecture",
    "start": "753360",
    "end": "758480"
  },
  {
    "text": "um right after after some time i want to i want to without throwing away everything i've done i've trained so far",
    "start": "758480",
    "end": "764800"
  },
  {
    "text": "um that kind of you're trying you're kind of like making your network a little bit larger at least on the action head side",
    "start": "764800",
    "end": "771680"
  },
  {
    "text": "um yeah or anywhere else that that i can for example let's say if i start with",
    "start": "771680",
    "end": "777440"
  },
  {
    "text": "the like uh larger action space but it just learns",
    "start": "777440",
    "end": "782480"
  },
  {
    "text": "to do the simple task and then at the next step after learning this simple task it starts learning a more complex",
    "start": "782480",
    "end": "788880"
  },
  {
    "text": "task what you could do is you could so there's two ways of doing this the first one is you start with the with the",
    "start": "788880",
    "end": "794399"
  },
  {
    "text": "largest network with the large action space but you mask all the actions that initially you don't need because your",
    "start": "794399",
    "end": "799920"
  },
  {
    "text": "action space your real action space under the hood in your arm is actually smaller um",
    "start": "799920",
    "end": "806000"
  },
  {
    "text": "so your environment would have to look at the the action masking examples that we have in our lip uh where your",
    "start": "806000",
    "end": "811360"
  },
  {
    "text": "environment would have to provide uh those actions that are valid um and then with some algorithms you can",
    "start": "811360",
    "end": "817600"
  },
  {
    "text": "actually um this works with ppo app or dq and you can you can master the action",
    "start": "817600",
    "end": "824320"
  },
  {
    "text": "i've never tried this but you could you could of course you could instead of throwing the network away and training a completely new one that's a little bit",
    "start": "824320",
    "end": "830079"
  },
  {
    "text": "larger on the excellent action head side um you could keep all the initial layers maybe up to the",
    "start": "830079",
    "end": "836320"
  },
  {
    "text": "either the last one or the last two ones and say okay at least i have learned already the feature",
    "start": "836320",
    "end": "843440"
  },
  {
    "text": "until the the automatic feature learning until that's until that layer that i'm going to keep and then only the last uh",
    "start": "843440",
    "end": "850320"
  },
  {
    "text": "action head layer or the last maybe the last two layers um you throw away and re-initialize or",
    "start": "850320",
    "end": "855680"
  },
  {
    "text": "like but that's yeah it's but you could also just like keep all",
    "start": "855680",
    "end": "861519"
  },
  {
    "text": "the layers and then from the let's say third last layers then like add another",
    "start": "861519",
    "end": "866560"
  },
  {
    "text": "two after you incentive from the checkpoint and what would you do with the uh let's",
    "start": "866560",
    "end": "873360"
  },
  {
    "text": "say you have let's say you have two actions and so your last layer is like uh uh four",
    "start": "873360",
    "end": "878560"
  },
  {
    "text": "outputs right two for the mean and two for the standard deviation and now you increase the action space to let you just double it or something uh would you",
    "start": "878560",
    "end": "885519"
  },
  {
    "text": "also keep you wouldn't keep the last layer right you would because that's you would definitely throw away the left",
    "start": "885519",
    "end": "891680"
  },
  {
    "text": "around for the actions that uh that i've learned already and then oh okay okay kind of you know",
    "start": "891680",
    "end": "897199"
  },
  {
    "text": "add yeah add another head so to say yeah yeah but in that ad in that hat you keep the",
    "start": "897199",
    "end": "903760"
  },
  {
    "text": "the weights that you've already cooled yeah that's that could also work yeah that could also work so you mean just adding one single",
    "start": "903760",
    "end": "909680"
  },
  {
    "text": "neuron to the latest layer yeah that's what he's saying right after",
    "start": "909680",
    "end": "915360"
  },
  {
    "text": "uh yeah or maybe add like another layer",
    "start": "915360",
    "end": "920720"
  },
  {
    "text": "to the second last layer and then like a another neuron on top or something like",
    "start": "920720",
    "end": "926079"
  },
  {
    "text": "that i mean there's tons of variations here yeah",
    "start": "926079",
    "end": "931440"
  },
  {
    "text": "yeah this is pretty bleeding edge stuff i think even in research like how to how to do this right um",
    "start": "931440",
    "end": "936560"
  },
  {
    "text": "how to manage like changing architectures without losing the pre-training success that you offered",
    "start": "936560",
    "end": "942079"
  },
  {
    "text": "with um yeah yeah this will require you to mess a little bit with the with the get state set",
    "start": "942079",
    "end": "948320"
  },
  {
    "text": "state methods in your algorithm [Music] but you can probably hack that uh you",
    "start": "948320",
    "end": "954639"
  },
  {
    "text": "can or like subclass uh yeah either your policy or your your",
    "start": "954639",
    "end": "960560"
  },
  {
    "text": "whole property of i don't know whatever algorithm you use the policy for that the policy class and then make sure that",
    "start": "960560",
    "end": "965920"
  },
  {
    "text": "gets saved instead states uh handle this properly but it's it's quite of a hack",
    "start": "965920",
    "end": "971040"
  },
  {
    "text": "we don't we don't have an example for that definitely not uh yeah what gave me good idea about i need just to learn how",
    "start": "971040",
    "end": "977120"
  },
  {
    "text": "to do this kind of uh architecture changing yeah and again this is something that's",
    "start": "977120",
    "end": "982959"
  },
  {
    "text": "i don't know i've never done this and this is uh very rare that's that's but yeah it's definitely it's we we've had this question before uh",
    "start": "982959",
    "end": "989519"
  },
  {
    "text": "from from even one of our customers that wanted to uh utilize all the because they put so much resources into training something",
    "start": "989519",
    "end": "996000"
  },
  {
    "text": "and then all of a sudden they wanted to increase the action space so they have the same problem yeah i'm not sure it has to do with",
    "start": "996000",
    "end": "1001440"
  },
  {
    "text": "curriculum curriculum is more like on the environment side you know in these cases that let's say",
    "start": "1001440",
    "end": "1007600"
  },
  {
    "start": "1007000",
    "end": "1040000"
  },
  {
    "text": "for example for continuous action space because what we noticed once we added",
    "start": "1007600",
    "end": "1013279"
  },
  {
    "text": "one extra action to our to our bot the things got very much complex and all of",
    "start": "1013279",
    "end": "1019040"
  },
  {
    "text": "a sudden the training was much slower and unstable um",
    "start": "1019040",
    "end": "1025520"
  },
  {
    "text": "that's do you think if it works yeah do you think if it works if we",
    "start": "1025520",
    "end": "1031199"
  },
  {
    "text": "change to discrete action space while using the sac is it possible with tray to to have",
    "start": "1031199",
    "end": "1038160"
  },
  {
    "text": "multiple discrete variables theoretically possible of course but",
    "start": "1038160",
    "end": "1043280"
  },
  {
    "start": "1040000",
    "end": "1060000"
  },
  {
    "text": "it's uh i think ours doesn't work with multiple speed ours requires like a single",
    "start": "1043280",
    "end": "1048400"
  },
  {
    "text": "uh similar to our bqn we don't we don't support this rejection sorry",
    "start": "1048400",
    "end": "1054000"
  },
  {
    "text": "if no one's coming up with another question i can answer the initial question from",
    "start": "1054000",
    "end": "1059280"
  },
  {
    "text": "sawgrass who has been waiting so patiently throughout this are you there can you",
    "start": "1059280",
    "end": "1064799"
  },
  {
    "start": "1060000",
    "end": "1098000"
  },
  {
    "text": "like um yeah i'm here oh yeah awesome perfect so",
    "start": "1064799",
    "end": "1071120"
  },
  {
    "text": "um let me share the screen again because um you're the one who posted",
    "start": "1071120",
    "end": "1077919"
  },
  {
    "text": "an issue in the document before so i had some time to have a look at this um so",
    "start": "1077919",
    "end": "1083840"
  },
  {
    "text": "you're doing some some interaction with civilization right",
    "start": "1083840",
    "end": "1089039"
  },
  {
    "text": "yep that's correct which is uh really awesome by the way",
    "start": "1089039",
    "end": "1094559"
  },
  {
    "text": "okay cool so um i tried putting lots of your code together",
    "start": "1094559",
    "end": "1100559"
  },
  {
    "start": "1098000",
    "end": "1525000"
  },
  {
    "text": "to just get a like a running example of um of what you're doing and then",
    "start": "1100559",
    "end": "1108160"
  },
  {
    "text": "ran into some issues there where i didn't have enough time to like",
    "start": "1108160",
    "end": "1113600"
  },
  {
    "text": "dig into there so i just kind of switched the environment to do something that is super similar so instead of",
    "start": "1113600",
    "end": "1120480"
  },
  {
    "text": "civilization i went for this water world environment",
    "start": "1120480",
    "end": "1127440"
  },
  {
    "text": "and obviously like it is like a civilization you said is like",
    "start": "1127440",
    "end": "1132880"
  },
  {
    "text": "adversarial right i don't know if you uh have you seen waterworld before",
    "start": "1132880",
    "end": "1138559"
  },
  {
    "text": "i haven't oh yeah no that's different right it's this one right",
    "start": "1138559",
    "end": "1143919"
  },
  {
    "text": "okay so this is like a semi adversarial but i think that it doesn't really matter for your um",
    "start": "1143919",
    "end": "1150799"
  },
  {
    "text": "question right the the thing in the end is i mean to do to rephrase your problem um you're running this this uh this",
    "start": "1150799",
    "end": "1157840"
  },
  {
    "text": "algorithm facing uh civilization and then you have this multi-agent setting where",
    "start": "1157840",
    "end": "1165039"
  },
  {
    "text": "uh all these agents differ report different metrics and then in the end",
    "start": "1165039",
    "end": "1171039"
  },
  {
    "text": "rlp gives you the sum of uh rewards like the mean some of",
    "start": "1171039",
    "end": "1177280"
  },
  {
    "text": "the mean rewards right i mean and you and you want the um you want to",
    "start": "1177280",
    "end": "1182799"
  },
  {
    "text": "you want them separated and then possibly you want to just uh i don't know stop running the algorithm",
    "start": "1182799",
    "end": "1188960"
  },
  {
    "text": "if just one of the policies hits a certain reward and stuff like that right yep",
    "start": "1188960",
    "end": "1195039"
  },
  {
    "text": "so let's look at this code right um we can get rid of this for now to start",
    "start": "1195039",
    "end": "1200320"
  },
  {
    "text": "with the code how it would look without the",
    "start": "1200320",
    "end": "1206080"
  },
  {
    "text": "solution basically um go over it real quick so we have like a",
    "start": "1206080",
    "end": "1211120"
  },
  {
    "text": "petting zoo environment here um then the the algorithm is",
    "start": "1211120",
    "end": "1217760"
  },
  {
    "text": "doesn't really matter i used ppo here and then obviously you will want to since this is",
    "start": "1217760",
    "end": "1223520"
  },
  {
    "text": "like a multi-agent setting you will want to put something in the multi-agent",
    "start": "1223520",
    "end": "1229039"
  },
  {
    "text": "configuration options here which would be a list of policy mappings um a list",
    "start": "1229039",
    "end": "1234240"
  },
  {
    "text": "of policy again a dictionary of policy and then a mapping mapping function so that you kind of know which",
    "start": "1234240",
    "end": "1242480"
  },
  {
    "text": "agent should be using which policy right so if you run this right now i don't",
    "start": "1242480",
    "end": "1248480"
  },
  {
    "text": "even know if it [Music] would would run without the code i just delete it but if you would run it now it",
    "start": "1248480",
    "end": "1255520"
  },
  {
    "text": "would report back like a reward which is the sum of all the agent rewards that",
    "start": "1255520",
    "end": "1262080"
  },
  {
    "text": "are reported and what you actually want is",
    "start": "1262080",
    "end": "1268240"
  },
  {
    "text": "to have them separated right so",
    "start": "1268240",
    "end": "1272400"
  },
  {
    "text": "for example i mean this stopping condition here is uh doesn't really relate to your problem but you would",
    "start": "1273679",
    "end": "1279039"
  },
  {
    "text": "want more metrics reported right you want you would want um like uh um",
    "start": "1279039",
    "end": "1284799"
  },
  {
    "text": "in the overview the tune gives you let me run this look real quick that you can actually if you can actually see what i'm talking about so the in the overview",
    "start": "1284799",
    "end": "1291280"
  },
  {
    "text": "that tune gives you in every iteration uh you would want them separated so",
    "start": "1291280",
    "end": "1297440"
  },
  {
    "text": "you could i mean in the newest version of ray in case you're working with that you could do this with the",
    "start": "1297440",
    "end": "1303120"
  },
  {
    "text": "cli reporter which is a",
    "start": "1303120",
    "end": "1308080"
  },
  {
    "text": "tune feature essentially which kind of out of the metrics that your algorithm",
    "start": "1308240",
    "end": "1313280"
  },
  {
    "text": "reports select certain metrics and then places them into this overview here that you",
    "start": "1313280",
    "end": "1318559"
  },
  {
    "text": "see after each iteration so all right but the same will same thing will happen",
    "start": "1318559",
    "end": "1324880"
  },
  {
    "text": "if you run the water whatever example so initially there won't be",
    "start": "1324880",
    "end": "1329919"
  },
  {
    "text": "much to report back because the algorithm is just stepping over the environment that there's just uh",
    "start": "1329919",
    "end": "1336080"
  },
  {
    "text": "like there's no single episode that has been um",
    "start": "1336080",
    "end": "1341120"
  },
  {
    "text": "like a complete episode that has been learned right",
    "start": "1341120",
    "end": "1346480"
  },
  {
    "text": "um yeah let's let this run a little bit so like in the meanwhile i can talk about",
    "start": "1346480",
    "end": "1351840"
  },
  {
    "text": "these callbacks so um like for for many issues for many",
    "start": "1351840",
    "end": "1357520"
  },
  {
    "text": "problems that you're facing this would be enough you could just you could just",
    "start": "1357520",
    "end": "1363120"
  },
  {
    "text": "select those those metrics from the metrics columns that you would find report to be",
    "start": "1363120",
    "end": "1369039"
  },
  {
    "text": "reported back by by the algorithm itself on every iteration so this is like this this",
    "start": "1369039",
    "end": "1374720"
  },
  {
    "text": "large list of stuff that you see um every couple of seconds probably if you're running",
    "start": "1374720",
    "end": "1380799"
  },
  {
    "text": "something um so in there you would see let's see this looks good okay so um we have this",
    "start": "1380799",
    "end": "1388880"
  },
  {
    "text": "metrics column here that is called policy reward max and pause the reward mean",
    "start": "1388880",
    "end": "1395200"
  },
  {
    "text": "and this is the one that we would actually want to report normally but in the beginning as i said in the we just",
    "start": "1395200",
    "end": "1401679"
  },
  {
    "text": "saw it's not there so if we would uh let's say want to stop on that metric",
    "start": "1401679",
    "end": "1406880"
  },
  {
    "text": "uh or checkpoint at that metric uh we wouldn't be able to so i don't know if that's the case for",
    "start": "1406880",
    "end": "1412720"
  },
  {
    "text": "civilization it's just like a pitfall to them mentioning here",
    "start": "1412720",
    "end": "1417840"
  },
  {
    "text": "so what you would have to do is you would have to add this callback here",
    "start": "1417840",
    "end": "1423520"
  },
  {
    "text": "to just set them to to not a number um",
    "start": "1423520",
    "end": "1431679"
  },
  {
    "text": "on like every time we're reporting um these metrics so",
    "start": "1431679",
    "end": "1437440"
  },
  {
    "text": "this is like this this is my callbacks classes um like it can hold",
    "start": "1437440",
    "end": "1443039"
  },
  {
    "text": "many different callbacks but on training results would be the one that you would want to choose here and you would hand it to the",
    "start": "1443039",
    "end": "1449890"
  },
  {
    "text": "[Music] algorithm config down here right config.callbacks",
    "start": "1449890",
    "end": "1457039"
  },
  {
    "text": "mycallbacks and then the algorithm in this case ppo but any other algorithm",
    "start": "1457039",
    "end": "1463760"
  },
  {
    "text": "would know that every time it reports back to all those training results it will add in",
    "start": "1463760",
    "end": "1469460"
  },
  {
    "text": "[Music] these columns here um and these columns",
    "start": "1469460",
    "end": "1474960"
  },
  {
    "text": "are the ones that we are referring to down here",
    "start": "1474960",
    "end": "1480080"
  },
  {
    "text": "right so this is pursuer one um",
    "start": "1480080",
    "end": "1486159"
  },
  {
    "text": "at this custom metrics pulse reward mean of pursuer one and we want to put in there the just the policy reward mean of",
    "start": "1486159",
    "end": "1493279"
  },
  {
    "text": "pursuer one which we uh find in later iterations of the algorithms or we put in there none",
    "start": "1493279",
    "end": "1499679"
  },
  {
    "text": "so um yeah that's i mean that's essentially it right so then if if you do that then as soon as tune uh",
    "start": "1499679",
    "end": "1506799"
  },
  {
    "text": "reports stuff to you it reports these metrics to you then you'll see these rewards separated there",
    "start": "1506799",
    "end": "1512880"
  },
  {
    "text": "and then also you'll be able to stop on them um if you reach a desired reward",
    "start": "1512880",
    "end": "1519760"
  },
  {
    "text": "okay yeah that seems to answer my questions i just have one more additional question this is a bit",
    "start": "1519760",
    "end": "1525840"
  },
  {
    "start": "1525000",
    "end": "1564000"
  },
  {
    "text": "unrelated so i tried without the separated rewards and i tried running it",
    "start": "1525840",
    "end": "1532159"
  },
  {
    "text": "for a bit of while then i noticed that even if i set up my maximum steps to be",
    "start": "1532159",
    "end": "1538720"
  },
  {
    "text": "about in 20 turns so rate june for some",
    "start": "1538720",
    "end": "1543760"
  },
  {
    "text": "reason stopped the time steps before the time step that i have initialized so",
    "start": "1543760",
    "end": "1549919"
  },
  {
    "text": "they have stopped at turn 12 instead of 10 20 that i set so was there any",
    "start": "1549919",
    "end": "1555279"
  },
  {
    "text": "additional parameters i need to set up like rayton set up a default maximum",
    "start": "1555279",
    "end": "1560559"
  },
  {
    "text": "time steps that they have or something else yeah so in rlp we have kind of two",
    "start": "1560559",
    "end": "1566320"
  },
  {
    "start": "1564000",
    "end": "1601000"
  },
  {
    "text": "different ways of counting time steps i mean this could be something that's happening here so if you're looking at a",
    "start": "1566320",
    "end": "1571440"
  },
  {
    "text": "multi-agent setting um then environment time steps might be different from agents time steps",
    "start": "1571440",
    "end": "1577919"
  },
  {
    "text": "um and i think this might be what's happening right the single agent case obviously that's the same you have only one agent that's stepping",
    "start": "1577919",
    "end": "1585760"
  },
  {
    "text": "through the environment one time step at a time but then in some multi-agent settings maybe for your uh",
    "start": "1585760",
    "end": "1591120"
  },
  {
    "text": "for your environment as well an agent might take a turn while other agents are not taking turns for example",
    "start": "1591120",
    "end": "1597520"
  },
  {
    "text": "yeah okay that makes sense okay thank you how do you make it learn an episode so",
    "start": "1597520",
    "end": "1603679"
  },
  {
    "start": "1601000",
    "end": "1759000"
  },
  {
    "text": "to say right today i think i guess that was the essence of the question um so i",
    "start": "1603679",
    "end": "1609120"
  },
  {
    "text": "have a script running here um which might learn eventually i'll show you this real quick um",
    "start": "1609120",
    "end": "1616000"
  },
  {
    "text": "so i mean this this would be how we would uh run sac",
    "start": "1616000",
    "end": "1621760"
  },
  {
    "text": "learning on episodes here you're using the default ssc config i think for for these uh discrete action spaces you need",
    "start": "1621760",
    "end": "1628559"
  },
  {
    "text": "a very specific um you need different settings then the default ones for sac um for example your",
    "start": "1628559",
    "end": "1636880"
  },
  {
    "text": "target network update has to be a certain i think it has to be you should set tau to one so that you get a full",
    "start": "1636880",
    "end": "1643919"
  },
  {
    "text": "harsh uh one or zero set it to 1.0 and then the frequency has",
    "start": "1643919",
    "end": "1651039"
  },
  {
    "text": "to be not one but like something larger like similar to very very similar to what you how you would run dq in",
    "start": "1651039",
    "end": "1656640"
  },
  {
    "text": "uh i don't know what exactly is but we can uh and i remember this made a depiction",
    "start": "1656640",
    "end": "1662399"
  },
  {
    "text": "for uh tuning card police in particular um so",
    "start": "1662399",
    "end": "1668000"
  },
  {
    "text": "the default frequency is zero uh yeah exactly or one i mean it updates",
    "start": "1668000",
    "end": "1673679"
  },
  {
    "text": "every every time but it updates only according to like a small tower so it doesn't doesn't fully overwrite the way it's just kind of pushes them in the",
    "start": "1673679",
    "end": "1679919"
  },
  {
    "text": "right direction uh let's see sac tuned examples",
    "start": "1679919",
    "end": "1686080"
  },
  {
    "text": "um yeah set the train batch size to like 32",
    "start": "1687039",
    "end": "1692159"
  },
  {
    "text": "for example that's what it says here in our tuned example and then the entire network update frequency also to 32.",
    "start": "1692159",
    "end": "1699278"
  },
  {
    "text": "um to one",
    "start": "1699919",
    "end": "1705039"
  },
  {
    "text": "and then i think you can increase the learning rates a little bit but it may not be necessary just to",
    "start": "1705039",
    "end": "1710960"
  },
  {
    "text": "just look at the tuned examples",
    "start": "1710960",
    "end": "1714880"
  },
  {
    "text": "yeah i see yeah okay yeah let's see if it's if it does",
    "start": "1717200",
    "end": "1722880"
  },
  {
    "text": "actually uh these optimization parameters think we",
    "start": "1722880",
    "end": "1728720"
  },
  {
    "text": "should keep these around yeah try them yeah we can it should maybe it",
    "start": "1728720",
    "end": "1734240"
  },
  {
    "text": "also works with the other with the defaults learning words",
    "start": "1734240",
    "end": "1739840"
  },
  {
    "text": "oh it's called excuse me regarding this setting i'm",
    "start": "1748880",
    "end": "1754159"
  },
  {
    "text": "just wondering why the default is uh just a random sampling because probably",
    "start": "1754159",
    "end": "1759279"
  },
  {
    "start": "1759000",
    "end": "1894000"
  },
  {
    "text": "in most of the cases the reinforcement learning agent is working with some sort of sequence of",
    "start": "1759279",
    "end": "1765760"
  },
  {
    "text": "actions the the time related uh you know",
    "start": "1765760",
    "end": "1772080"
  },
  {
    "text": "the time is actually important i mean right but you also you in every in every",
    "start": "1772240",
    "end": "1778320"
  },
  {
    "text": "sample that you take from the individual sample you have the observation the reward and the next observation so there there you have the td",
    "start": "1778320",
    "end": "1784880"
  },
  {
    "text": "mechanism we're kind of like in place working and if you want you can you can set n-step as well to some larger values",
    "start": "1784880",
    "end": "1790640"
  },
  {
    "text": "so you have um you have the transition including the the sum of some n-step",
    "start": "1790640",
    "end": "1797840"
  },
  {
    "text": "rewards uh and then the next observation is not the really next one but like the one like n steps after that",
    "start": "1797840",
    "end": "1804240"
  },
  {
    "text": "um yeah that's that's the basic mechanism of td learning um",
    "start": "1804240",
    "end": "1810799"
  },
  {
    "text": "the basic form is just you learn by single time steps or by single transitions um",
    "start": "1810799",
    "end": "1816080"
  },
  {
    "text": "i see okay yeah well this turns out if it does any",
    "start": "1816080",
    "end": "1821120"
  },
  {
    "text": "better i mean so what this will result is in is actually something very close to this",
    "start": "1821120",
    "end": "1827520"
  },
  {
    "text": "running a train that is very close to like a hyper parameter optimized training that",
    "start": "1827520",
    "end": "1833120"
  },
  {
    "text": "we have run previously with the only thing being different that something would not be",
    "start": "1833120",
    "end": "1838960"
  },
  {
    "text": "random time steps but uh complete episodes should be the only difference here so",
    "start": "1838960",
    "end": "1846240"
  },
  {
    "text": "it sounds good yeah yeah sac is weird the ssc likes kind of like works best if you almost don't do",
    "start": "1846240",
    "end": "1853360"
  },
  {
    "text": "any sound like if you sample really slowly you just put everything in the buffer and you keep like updating updating updating and in this setup with",
    "start": "1853360",
    "end": "1859200"
  },
  {
    "text": "you with the buffer you you do sample an entire episode first right and then put that in the buffer",
    "start": "1859200",
    "end": "1866000"
  },
  {
    "text": "so you're basically sampling very fast you're being very sample inefficient right now with this setup if i",
    "start": "1866000",
    "end": "1871120"
  },
  {
    "text": "understand everything correctly um i wouldn't understand why it hurts",
    "start": "1871120",
    "end": "1876960"
  },
  {
    "text": "this the learning process it should still learn because here yeah",
    "start": "1876960",
    "end": "1883360"
  },
  {
    "text": "not sure we might just take it out i mean excuse me another name",
    "start": "1883360",
    "end": "1890080"
  },
  {
    "text": "i don't know this there could be some another follow-up question on this so",
    "start": "1890080",
    "end": "1895440"
  },
  {
    "start": "1894000",
    "end": "1936000"
  },
  {
    "text": "let's say for example we have some good episodes considered like offline learning we have some good episodes and i've seen",
    "start": "1895440",
    "end": "1902880"
  },
  {
    "text": "on the great documentation that we can kind of mix during replay buffer we can mix the",
    "start": "1902880",
    "end": "1909440"
  },
  {
    "text": "offline data with data that we have with the data that the",
    "start": "1909440",
    "end": "1914720"
  },
  {
    "text": "bot is you know collecting during training could you also let me know like how i",
    "start": "1914720",
    "end": "1920880"
  },
  {
    "text": "can uh so i wanna use kind of leverage that uh good data or good episode",
    "start": "1920880",
    "end": "1927679"
  },
  {
    "text": "um to kind of initialize the training of my bot based on that and yeah",
    "start": "1927679",
    "end": "1934799"
  },
  {
    "text": "um i think i think it's referring to uh to where you basically in the conflict",
    "start": "1934799",
    "end": "1940159"
  },
  {
    "start": "1936000",
    "end": "2038000"
  },
  {
    "text": "where you say uh input equals and then you do like a dictionary um yeah but he tries to mix red",
    "start": "1940159",
    "end": "1947120"
  },
  {
    "text": "you wants to mix yeah offline exactly so you you let it some somewhat sample in the environment but",
    "start": "1947120",
    "end": "1953840"
  },
  {
    "text": "some fraction also comes from a json file for example you can set this yeah yeah and then you just you can set this",
    "start": "1953840",
    "end": "1960480"
  },
  {
    "text": "up yeah this is this is possible um and you can even you can specify the",
    "start": "1960480",
    "end": "1965760"
  },
  {
    "text": "ratio you could say 50 i want 50 samples coming from my environment or 30 uh and 70 percent from from",
    "start": "1965760",
    "end": "1972960"
  },
  {
    "text": "from this json file and five percent from the other json you can set this up uh with the",
    "start": "1972960",
    "end": "1979600"
  },
  {
    "text": "uh yeah with the with the inputs config settings um there's it's in the documentation as",
    "start": "1979600",
    "end": "1984880"
  },
  {
    "text": "well uh but that's that's not they're kind of the same as preloading",
    "start": "1984880",
    "end": "1990240"
  },
  {
    "text": "your buffer with with samples and then specifying",
    "start": "1990240",
    "end": "1995360"
  },
  {
    "text": "so basically your json file is basically your buffer um it doesn't change over time so maybe",
    "start": "1995440",
    "end": "2000960"
  },
  {
    "text": "that's something that you would want that you just want the initially the buffer to look a certain way and then",
    "start": "2000960",
    "end": "2006320"
  },
  {
    "text": "slowly like purge the sam that's that's definitely that's not possible unless you have a custom buffer that you can pre-load with",
    "start": "2006320",
    "end": "2012799"
  },
  {
    "text": "something um but this what we're talking about right now this setting with the dictionary and with the ratios is purely you have some",
    "start": "2012799",
    "end": "2019840"
  },
  {
    "text": "json file maybe some sampling going on and you can mix this",
    "start": "2019840",
    "end": "2025200"
  },
  {
    "text": "do you have an example file for this uh yeah let me just take a look at the",
    "start": "2025200",
    "end": "2031440"
  },
  {
    "text": "yeah so it should be in the form of json or like csv file also it can read no has to be json",
    "start": "2031440",
    "end": "2037600"
  },
  {
    "text": "files could we start out uh having the agent learn from the json file",
    "start": "2037600",
    "end": "2042960"
  },
  {
    "start": "2038000",
    "end": "2054000"
  },
  {
    "text": "and then save uh save the agent a checkpoint reload that checkpoint but uh just have that buffer be",
    "start": "2042960",
    "end": "2050079"
  },
  {
    "text": "completely filled by its uh the experiences it generates during training uh yeah if you if you use checkpoints",
    "start": "2050079",
    "end": "2057118"
  },
  {
    "start": "2054000",
    "end": "2195000"
  },
  {
    "text": "you can yes you can let's say set up one particular sac config pointing it to some json file or",
    "start": "2057119",
    "end": "2064158"
  },
  {
    "text": "or letting it sample then you can checkpoint um and then you can bring up an sac that's",
    "start": "2064159",
    "end": "2069839"
  },
  {
    "text": "similarly configured but has maybe a different input setting and continue training that way yes uh you could",
    "start": "2069839",
    "end": "2076480"
  },
  {
    "text": "it's more like the complex way of things but that's yeah how would you modify the checkpoint",
    "start": "2076480",
    "end": "2082240"
  },
  {
    "text": "by hand and then just run from the checkpoint no i no i would you just i would just use the",
    "start": "2082240",
    "end": "2088240"
  },
  {
    "text": "checkpoint to keep the weights that are currently and of course all the other the the [Music]",
    "start": "2088240",
    "end": "2093839"
  },
  {
    "text": "what else in the checkpoint yeah just the weights and the time step counters maybe um right um",
    "start": "2093839",
    "end": "2099920"
  },
  {
    "text": "but then because you're creating a new agent or a new new algorithm from scratch and then you load the checkpoint",
    "start": "2100480",
    "end": "2105760"
  },
  {
    "text": "into that you can you can basically pick a slightly different config you have to be careful of course you cannot change the network architecture then it will",
    "start": "2105760",
    "end": "2111839"
  },
  {
    "text": "explode um but you can definitely change the input source for that new new created",
    "start": "2111839",
    "end": "2117920"
  },
  {
    "text": "algorithm then it will just continue doing with the uh learning with the with the new input",
    "start": "2117920",
    "end": "2123280"
  },
  {
    "text": "um to load into the buffer we we didn't finish this right archer we didn't do we",
    "start": "2123280",
    "end": "2130720"
  },
  {
    "text": "don't have a buffer off the shelf that you can kind of say i want to pre-load it with something",
    "start": "2130720",
    "end": "2137440"
  },
  {
    "text": "or is that is that possible also no like this this this pre-loading is not possible by configuration you would",
    "start": "2137440",
    "end": "2143599"
  },
  {
    "text": "be able to modify the complete algorithm kind of and call",
    "start": "2143599",
    "end": "2149440"
  },
  {
    "text": "it yourself um yeah but i mean you can you can get to the buffer right if you have the algorithm uh in your script as an object",
    "start": "2149440",
    "end": "2157200"
  },
  {
    "text": "you can you can access the replay buffer right it's it's a property and yeah yeah absolutely and then you can say uh i",
    "start": "2157200",
    "end": "2163040"
  },
  {
    "text": "don't know set state of the replay buffer and just push in your your uh json uh so your your data basically",
    "start": "2163040",
    "end": "2169520"
  },
  {
    "text": "um yeah that would be possible yeah um",
    "start": "2169520",
    "end": "2174800"
  },
  {
    "text": "it's a little hacky you would have to be familiar with how the buffer stores the data i think we'd store it as um",
    "start": "2175440",
    "end": "2182160"
  },
  {
    "text": "it's just a dictionary of numpy errors right um yeah it's a big area of lists of numbers",
    "start": "2182880",
    "end": "2190800"
  },
  {
    "text": "i mean you yeah yeah i mean you can just you can basically you can you can instantiate",
    "start": "2190800",
    "end": "2196079"
  },
  {
    "start": "2195000",
    "end": "2367000"
  },
  {
    "text": "the algorithm and then i don't know run it for a couple of steps if you like and then stop and then access the buffer",
    "start": "2196079",
    "end": "2203440"
  },
  {
    "text": "and then manually basically call add batch",
    "start": "2203440",
    "end": "2208640"
  },
  {
    "text": "for however many batches you want to add and then continue running the algorithm",
    "start": "2208640",
    "end": "2213839"
  },
  {
    "text": "so that would be possible again that's like a not like a real configuration it's like",
    "start": "2213839",
    "end": "2219599"
  },
  {
    "text": "a small hack yeah that's right you can use the add methods and or add batch method and that's",
    "start": "2219599",
    "end": "2225920"
  },
  {
    "text": "sample that just yeah um i just posted a link to the to the mixed input setting",
    "start": "2225920",
    "end": "2231599"
  },
  {
    "text": "uh this is an example of on the command line using the uh the",
    "start": "2231599",
    "end": "2237119"
  },
  {
    "text": "client uh but you can do the same thing in your in your configuration you can say",
    "start": "2237119",
    "end": "2243200"
  },
  {
    "text": "config object dot offline offline data that's the method and then you pass the input parameter",
    "start": "2243200",
    "end": "2250160"
  },
  {
    "text": "and set this to some dictionary where the keys are basically um",
    "start": "2251599",
    "end": "2257200"
  },
  {
    "text": "uh passed through your files uh their json file for example um or you say sampler",
    "start": "2257200",
    "end": "2263599"
  },
  {
    "text": "and then you give the ratio of the of that particular method",
    "start": "2263599",
    "end": "2269040"
  },
  {
    "text": "okay go ahead take a look at an example that's that's how you can set up mixed input creating pretty simply with a just",
    "start": "2269040",
    "end": "2275200"
  },
  {
    "text": "through the configuration actually something that i wanted to do today uh together with christy was a",
    "start": "2275200",
    "end": "2281119"
  },
  {
    "text": "presentation of like how basically to get up and running with a sort of developer style setup of our",
    "start": "2281119",
    "end": "2287839"
  },
  {
    "text": "labs right so that you can make changes to our code if you find a bug or something",
    "start": "2287839",
    "end": "2293040"
  },
  {
    "text": "or if you want to contribute a feature and then you know like make your uh",
    "start": "2293040",
    "end": "2299520"
  },
  {
    "text": "first pr and like how that goes basically uh yeah i mean you can also do it like maybe it's okay to do it next time um",
    "start": "2299520",
    "end": "2308320"
  },
  {
    "text": "like yeah i feel like we're at time maybe people have to go somewhere else um",
    "start": "2308800",
    "end": "2315839"
  },
  {
    "text": "yeah yeah absolutely yeah uh we can we can shift this this agenda",
    "start": "2315839",
    "end": "2321440"
  },
  {
    "text": "to like next to the next meeting in like two weeks right next week is the summit uh is anyone coming to the summit from you guys",
    "start": "2321440",
    "end": "2327920"
  },
  {
    "text": "next week we'll all be there so you have a chance to say hello and",
    "start": "2328000",
    "end": "2335280"
  },
  {
    "text": "uh anyway yeah uh yeah thanks so much",
    "start": "2335280",
    "end": "2341280"
  },
  {
    "text": "i'm coming you're you're coming to water right oh yeah i'm absolutely coming and sam is coming and we're actually like he posed",
    "start": "2341280",
    "end": "2347839"
  },
  {
    "text": "an issue for us to work on together today but it's just very complex and not well suited to like uh",
    "start": "2347839",
    "end": "2355599"
  },
  {
    "text": "dive into it together with so many people so so we're actually meeting up in san francisco to talk",
    "start": "2355599",
    "end": "2361520"
  },
  {
    "text": "about this all right all right thanks everyone see you later guys thank you guys",
    "start": "2361520",
    "end": "2368560"
  }
]