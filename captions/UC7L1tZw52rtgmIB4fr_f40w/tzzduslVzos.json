[
  {
    "text": "okay well let's go ahead and get started really excited to be hosting this webinar today um uh you know there's",
    "start": "2360",
    "end": "8880"
  },
  {
    "text": "been a lot of great work that's gone in behind this so let's uh let's jump in um",
    "start": "8880",
    "end": "14000"
  },
  {
    "text": "you know on behalf of the team thank you everyone for attending today we're going to be covering with our partners over at AWS optimizing llm inference with Ray VM",
    "start": "14000",
    "end": "22720"
  },
  {
    "text": "neuron and eks uh by way of introduction I'm Matt Connor product manager at any",
    "start": "22720",
    "end": "27880"
  },
  {
    "text": "scale I'll be joined today by my colleague OE who's one of the engineering managers at any scale responsible for serving as well as our",
    "start": "27880",
    "end": "34840"
  },
  {
    "text": "partners over at AWS Vara and art they will introduce themselves as they come up but in the interest of time we'll uh",
    "start": "34840",
    "end": "41399"
  },
  {
    "text": "we'll keep we'll keep things moving here for an agenda today a couple of quick announcements we'd like to start with a",
    "start": "41399",
    "end": "47079"
  },
  {
    "text": "few really exciting events coming up over the next few weeks that we want to make everyone on the call aware of we'll",
    "start": "47079",
    "end": "52320"
  },
  {
    "text": "then jump in with a very brief Ray and int any scale introduction before handing off um to discuss Ai and L",
    "start": "52320",
    "end": "60199"
  },
  {
    "text": "Trends more broadly oay will do a bit of a deep dive into VM and Ray serve before",
    "start": "60199",
    "end": "66000"
  },
  {
    "text": "VAR will jump in with a demo bringing everything we talk about to life and then time pending we'll uh we'll open it",
    "start": "66000",
    "end": "72600"
  },
  {
    "text": "up for some questions at the end here so this is a this is what we're planning to to cover today now first on the",
    "start": "72600",
    "end": "79320"
  },
  {
    "text": "announcement side um we have coming up at the end of the month here our annual race Summit this is our gathering of",
    "start": "79320",
    "end": "86439"
  },
  {
    "text": "about a thousand plus um ml practitioners and Builders uh that we're",
    "start": "86439",
    "end": "91640"
  },
  {
    "text": "really excited about so September 30th to October 2nd in San Francisco there's a full day training we have a number of",
    "start": "91640",
    "end": "98159"
  },
  {
    "text": "very exciting speakers including Mera moradi the open AI CTO attending we also have a very specific VM track at Ray",
    "start": "98159",
    "end": "105560"
  },
  {
    "text": "Summit this year so for those of you on this call learning about optimizing LM inference a lot of the key contributors",
    "start": "105560",
    "end": "112040"
  },
  {
    "text": "uh of that project will be at Ray Summit and about 10 talks dedicated to VM and",
    "start": "112040",
    "end": "117320"
  },
  {
    "text": "all the great uh uh work going on there so be sure to check this out we can offer everyone who's on this call a 15%",
    "start": "117320",
    "end": "124280"
  },
  {
    "text": "off code we'll share these materials after um but go ahead to race summit. scale.com and you can go ahead and",
    "start": "124280",
    "end": "131239"
  },
  {
    "text": "register second uh announcement I wanted to share is a little further out but our teams were will also be at reinvent",
    "start": "131239",
    "end": "138760"
  },
  {
    "text": "coming up in early December so art here as well as our uh head of training",
    "start": "138760",
    "end": "144239"
  },
  {
    "text": "Camille and one of our in-house machine learning engineering experts will be there for training uh we'd love to see",
    "start": "144239",
    "end": "150319"
  },
  {
    "text": "you there so if you're making uh plans to be at reinvent please please please register and uh I think we have a",
    "start": "150319",
    "end": "156200"
  },
  {
    "text": "phenomenal program put together to uh to help train and and share more about our",
    "start": "156200",
    "end": "161239"
  },
  {
    "text": "joint solution and how it's powering llm and gen applications and more so that's it for a couple of announcements we'll",
    "start": "161239",
    "end": "167800"
  },
  {
    "text": "we'll Circle back to these at the end as I said we'll share these materials later on so let's go ahead and do a very brief",
    "start": "167800",
    "end": "172959"
  },
  {
    "text": "uh Ray and any scale introduction um by way of introduction any scale the company we are the creators and",
    "start": "172959",
    "end": "178480"
  },
  {
    "text": "maintainers of Ray which is a an open- Source compute project um that actually",
    "start": "178480",
    "end": "183879"
  },
  {
    "text": "came out of research done at the UC Berkeley rise lab Ray was created as our",
    "start": "183879",
    "end": "189640"
  },
  {
    "text": "co-founders were trying to figure out how to scale their AI workloads um both training serving and Beyond and so even",
    "start": "189640",
    "end": "196840"
  },
  {
    "text": "back in 2016 they were running into issues doing so and created Ray to address that obviously now here in 20124",
    "start": "196840",
    "end": "204280"
  },
  {
    "text": "you're going to hear about some of the big uh AI Trends and so very well positioned for what we've seen with this",
    "start": "204280",
    "end": "210239"
  },
  {
    "text": "new era of generative AI any scale the company was founded in 2019 to commercialize that project and provides",
    "start": "210239",
    "end": "217000"
  },
  {
    "text": "a fully managed platform to make uh scaling AI seamless easy uh uh with the",
    "start": "217000",
    "end": "223840"
  },
  {
    "text": "very best performance we can today uh Ray has been widely adopted uh and is",
    "start": "223840",
    "end": "230040"
  },
  {
    "text": "the standard for running AI workloads at scale um you know at the same point in its history it's seen um tremendous",
    "start": "230040",
    "end": "237280"
  },
  {
    "text": "adoption compared to other very popular open source projects and a number of huge organizations rely on Ray for their",
    "start": "237280",
    "end": "243640"
  },
  {
    "text": "AI workloads ranging from open AI who trained GPT with Ray to Uber Spotify",
    "start": "243640",
    "end": "249760"
  },
  {
    "text": "Netflix apple coinbase and more that are all building Ray based AI platforms so",
    "start": "249760",
    "end": "255239"
  },
  {
    "text": "really exciting to see the adoption and where we are today last comments from me and then we'll jump into the meat of the",
    "start": "255239",
    "end": "261359"
  },
  {
    "text": "program um you know what does any scale do on top of this well any scale provides a managed Ray platform uh where",
    "start": "261359",
    "end": "267680"
  },
  {
    "text": "we've really focused on four key parts s of of enhancing the ray experience first",
    "start": "267680",
    "end": "272880"
  },
  {
    "text": "is optimizations we've made to Ray to make it more performant reliable scalable and cost efficient the second",
    "start": "272880",
    "end": "279880"
  },
  {
    "text": "is a suite of developer tooling to make it very easy to build applications uh with Ray ranging from data processing to",
    "start": "279880",
    "end": "287240"
  },
  {
    "text": "training to llm inferencing we've added security and governance governance to meet Enterprise requirements and then",
    "start": "287240",
    "end": "293759"
  },
  {
    "text": "seamless Integrations into an ecosystem of mlops tools to make it a um you know fully configurable powerful AI platform",
    "start": "293759",
    "end": "302160"
  },
  {
    "text": "so that's uh that's it for me I'm going to hand it over to uh my uh colleague",
    "start": "302160",
    "end": "307360"
  },
  {
    "text": "here art who will shed some light on uh what are some of the major Trends we're seeing around llm inference Broadley and",
    "start": "307360",
    "end": "313720"
  },
  {
    "text": "and and where are things",
    "start": "313720",
    "end": "316600"
  },
  {
    "text": "going thank you Matt",
    "start": "322000",
    "end": "326319"
  },
  {
    "text": "hi folks uh my name is ariki and I am a Solutions architect with AWS gen team I",
    "start": "332240",
    "end": "339360"
  },
  {
    "text": "wanted to start by reviewing the latest trends that that are propelling the fly wheel forward with geni customers are",
    "start": "339360",
    "end": "346960"
  },
  {
    "text": "driving AI Innovation at an accelerating Pace in order to keep up with application use cases in the possibly",
    "start": "346960",
    "end": "354479"
  },
  {
    "text": "the hottest Topic in deep learning is the explosive growth of large models based on Transformer",
    "start": "354479",
    "end": "360840"
  },
  {
    "text": "architectures in the recent years these models gain popularity due to their soda accuracy and scaling properties with",
    "start": "360840",
    "end": "367440"
  },
  {
    "text": "some customers uh training trillion parameter language and computer vision",
    "start": "367440",
    "end": "372520"
  },
  {
    "text": "models another critical enabler is the self-supervised training approach which has helped data scientists tap into the",
    "start": "372520",
    "end": "380479"
  },
  {
    "text": "vast wealth of unstructured and unlabeled knowledge available online",
    "start": "380479",
    "end": "385720"
  },
  {
    "text": "this in turn helps to boost model accuracy and supports training these",
    "start": "385720",
    "end": "390800"
  },
  {
    "text": "large language models with billions and trillions of parameters in the last development in",
    "start": "390800",
    "end": "397840"
  },
  {
    "text": "the last element but still critical is that the state-of-the-art models such as stable diffusion OTP Bloom open fold are",
    "start": "397840",
    "end": "405160"
  },
  {
    "text": "becoming readily available via open source contributions these open- Source models help increase adoption of new",
    "start": "405160",
    "end": "412199"
  },
  {
    "text": "model architecture and often enable data scientists to improve model efficiency",
    "start": "412199",
    "end": "420080"
  },
  {
    "text": "uh just as a note uh when we say these AI models are getting bigger we mean exponentially bigger as I shown here uh",
    "start": "420080",
    "end": "427639"
  },
  {
    "text": "the number of parameters in machine learning models have been growing at a staggering Pace roughly about 10x a year",
    "start": "427639",
    "end": "434199"
  },
  {
    "text": "and even faster for some specific model types uh this C this curve is uh just",
    "start": "434199",
    "end": "439400"
  },
  {
    "text": "getting steeper these models are demonstrating new emerging capabilities",
    "start": "439400",
    "end": "444840"
  },
  {
    "text": "that are unlocking even more applications from an applications point of view what are some of these",
    "start": "444840",
    "end": "450199"
  },
  {
    "text": "applications and use cases there're plenty of examples but I just I'm just going to go through some of them here uh",
    "start": "450199",
    "end": "456520"
  },
  {
    "text": "generating new piece of original content such as short stories jokes Arts songs",
    "start": "456520",
    "end": "463440"
  },
  {
    "text": "images uh helps you with automatic code recommendations to accelerate application",
    "start": "463440",
    "end": "469159"
  },
  {
    "text": "development uh getting a brief summary of Articles books and documents writing assistant to rephrase",
    "start": "469159",
    "end": "475879"
  },
  {
    "text": "a text you wrote or reshape a message in a particular tone chatbot and virtual",
    "start": "475879",
    "end": "482240"
  },
  {
    "text": "assistance to enhance user experience for your customers allows you to search find and synthesize information from",
    "start": "482240",
    "end": "489479"
  },
  {
    "text": "large Corpus of data that's available on the internet image classification of you",
    "start": "489479",
    "end": "494759"
  },
  {
    "text": "know 1 million plus skes image generation some wild",
    "start": "494759",
    "end": "499840"
  },
  {
    "text": "images uh these are just some use cases but regardless of the use case one thing that uh these use cases all agree on is",
    "start": "499840",
    "end": "507400"
  },
  {
    "text": "the need to lower the cost of running these models in",
    "start": "507400",
    "end": "512680"
  },
  {
    "text": "production but training and deploying these large models to power these use cases is far from Easy there are several",
    "start": "512680",
    "end": "520000"
  },
  {
    "text": "challenges and trade-offs that machine learning developers and Engineers face daily developers are trading performance",
    "start": "520000",
    "end": "526480"
  },
  {
    "text": "and cost if you want to train these models quickly you need to deploy them",
    "start": "526480",
    "end": "531560"
  },
  {
    "text": "to a cluster of gpus but that turns out to be very expensive uh you could use less expensive Hardware that you know",
    "start": "531560",
    "end": "538360"
  },
  {
    "text": "then the training time can take months or even longer uh this this is where",
    "start": "538360",
    "end": "543760"
  },
  {
    "text": "purpose buil accelerators come to the rescue right we have buil we have purpose buil tranum and INF frenia for",
    "start": "543760",
    "end": "550480"
  },
  {
    "text": "these Ultra large gen models and large language models this allows developers",
    "start": "550480",
    "end": "555600"
  },
  {
    "text": "not to have to make these tradeoffs and these trainum and infen instances allow",
    "start": "555600",
    "end": "560880"
  },
  {
    "text": "developers to focus on their applications and still get the best performance for training and the best",
    "start": "560880",
    "end": "567440"
  },
  {
    "text": "inference performance possible for inference which is the topic of this",
    "start": "567440",
    "end": "574360"
  },
  {
    "text": "webinar the goal is to increase inference throughput we conducted a benchmark of a birth model where we",
    "start": "574360",
    "end": "580880"
  },
  {
    "text": "wanted to hit 10 million inferences per hour if we deploy the this model to a",
    "start": "580880",
    "end": "587000"
  },
  {
    "text": "GPU based instance it would take about eight gpus to achieve this performance but the same workload will just take",
    "start": "587000",
    "end": "593760"
  },
  {
    "text": "four INF frener two instances cutting the required instances by half and",
    "start": "593760",
    "end": "599440"
  },
  {
    "text": "because we using more efficient hardware and fear of them we also cut out the power consumption overall we were able",
    "start": "599440",
    "end": "605760"
  },
  {
    "text": "to reduce the inference cost by",
    "start": "605760",
    "end": "609839"
  },
  {
    "text": "65% putting this all together we have innovated across accelerators surver types data centers and rebinding them",
    "start": "610839",
    "end": "618200"
  },
  {
    "text": "together with the right software Services we have been able to build the largest clusters to date we have",
    "start": "618200",
    "end": "625040"
  },
  {
    "text": "integrated trinium with a wide range of aw services that you are potentially",
    "start": "625040",
    "end": "630640"
  },
  {
    "text": "familiar with this includes standard storage and networking services such as S3 FSX or",
    "start": "630640",
    "end": "636680"
  },
  {
    "text": "EFA orchestration services such as eks ECS par clusters or B in addition deep",
    "start": "636680",
    "end": "644639"
  },
  {
    "text": "learning specific capability such as the Deep learning Ami and the Deep learning containers also support trainum and infr",
    "start": "644639",
    "end": "651639"
  },
  {
    "text": "instances this allows you to get up to speed and get started very quickly with that I will hand it to my",
    "start": "651639",
    "end": "658639"
  },
  {
    "text": "colleague Ash to speak about any scale race serve and vlm ashk I'll stop",
    "start": "658639",
    "end": "664800"
  },
  {
    "text": "sharing and you can take over please yeah thanks",
    "start": "664800",
    "end": "671320"
  },
  {
    "text": "s cool um yeah so let's talk about VM and racer um so VM is uh has been",
    "start": "674399",
    "end": "682519"
  },
  {
    "text": "growing in popularity massively in the last year or so U you can see the GitHub",
    "start": "682519",
    "end": "688279"
  },
  {
    "text": "it's already reach 1,000 GitHub stars um and also like popular in many many",
    "start": "688279",
    "end": "696800"
  },
  {
    "text": "companies as their inference engine um so what was the goal that BLM started",
    "start": "696800",
    "end": "702480"
  },
  {
    "text": "with they wanted to build the easiest and fastest um open source engine for",
    "start": "702480",
    "end": "707600"
  },
  {
    "text": "L&M inferencing um and they started with some simple um ideas around batching and",
    "start": "707600",
    "end": "714959"
  },
  {
    "text": "like doing optimized scheduling of requests that come in as well as uh better caching of requests um so there's",
    "start": "714959",
    "end": "722399"
  },
  {
    "text": "a lot of data that's computed across many decoding steps that uh happened",
    "start": "722399",
    "end": "727560"
  },
  {
    "text": "during an llm inference and uh they built some uh novel Innovations around",
    "start": "727560",
    "end": "732760"
  },
  {
    "text": "caching data across these uh multiple decoding steps so since then um they've",
    "start": "732760",
    "end": "739800"
  },
  {
    "text": "had two major uh patterns of usage there is an offline API so this is for offline",
    "start": "739800",
    "end": "747000"
  },
  {
    "text": "batched inance where people want to infer on let's say your entire uh Corpus",
    "start": "747000",
    "end": "753240"
  },
  {
    "text": "of documents offline um and BLM has a lot of inference optimizations around",
    "start": "753240",
    "end": "759360"
  },
  {
    "text": "that um as well as an API server for online um inference so uh this is where",
    "start": "759360",
    "end": "766760"
  },
  {
    "text": "you're using it in real time these are some of the features um",
    "start": "766760",
    "end": "772040"
  },
  {
    "text": "and WM at this point has grown and become like this big project that originally started out of the rise lab",
    "start": "772040",
    "end": "777920"
  },
  {
    "text": "in Berkeley but now uh there's been multiple contributions from many companies across the board uh",
    "start": "777920",
    "end": "783680"
  },
  {
    "text": "any scale as well as one of the major contributors to BLM uh contributing many",
    "start": "783680",
    "end": "790199"
  },
  {
    "text": "uh new inference optimizations like speculative decoding um but there's a lot of other companies with a lot of",
    "start": "790199",
    "end": "796519"
  },
  {
    "text": "interest and part of it is because VM is so easy to use extend um and not close",
    "start": "796519",
    "end": "803519"
  },
  {
    "text": "Source like some of like some other engines like tensor RT llm so today it's",
    "start": "803519",
    "end": "808680"
  },
  {
    "text": "support it's a wide range of model um and different model architectures it has",
    "start": "808680",
    "end": "813760"
  },
  {
    "text": "llama mixl all of your popular open source models um as well as Vision",
    "start": "813760",
    "end": "819800"
  },
  {
    "text": "language models like lava pixol that just came out recently so new architectures are added very quickly",
    "start": "819800",
    "end": "826800"
  },
  {
    "text": "within days of announcement um there's a wide range of Hardware support as well",
    "start": "826800",
    "end": "833199"
  },
  {
    "text": "uh which is another reason why it's so um it's so popular because of its",
    "start": "833199",
    "end": "838279"
  },
  {
    "text": "extensibility so it supports Nvidia gpus but also AMD",
    "start": "838279",
    "end": "843880"
  },
  {
    "text": "AWS um infr and tranium as well as Intel um Na Intel CPUs um and the new stuff",
    "start": "843880",
    "end": "852600"
  },
  {
    "text": "that Intel is launching around Habana um processing units as well um and then it",
    "start": "852600",
    "end": "858480"
  },
  {
    "text": "has a wide range of inference optimization so you can set up tensor parallelism quantization uh which is",
    "start": "858480",
    "end": "866120"
  },
  {
    "text": "popularly used to reduce model size prefix hashing uh serving multiple",
    "start": "866120",
    "end": "871360"
  },
  {
    "text": "fine-tune models behind the same endpoint and spec decoding which is really used for U speeding up infint",
    "start": "871360",
    "end": "878160"
  },
  {
    "text": "times uh by using a smaller draft model to predict some tokens so all of these",
    "start": "878160",
    "end": "884120"
  },
  {
    "text": "combined are very easy to set up behind one simple VM API U for any of these",
    "start": "884120",
    "end": "890079"
  },
  {
    "text": "popular open source models on any type of Hardware so get grer um so race serve",
    "start": "890079",
    "end": "900600"
  },
  {
    "text": "was originally we looked at what are the real problems to solve in ml serving so",
    "start": "900600",
    "end": "907399"
  },
  {
    "text": "some of the big problems that exist today and these were relevant two or three years ago these are even more",
    "start": "907399",
    "end": "913440"
  },
  {
    "text": "relevant now uh is there is long lead times to go from development to production so people would train their",
    "start": "913440",
    "end": "919519"
  },
  {
    "text": "models offline using their notebooks or whatever bisoke setup um and then they",
    "start": "919519",
    "end": "926160"
  },
  {
    "text": "would use different kinds of hardware and infrastructure to do that and then they would have to figure out how to",
    "start": "926160",
    "end": "931440"
  },
  {
    "text": "translate their code to run online and how you get the models over from your data scientist to the software Engineers",
    "start": "931440",
    "end": "938399"
  },
  {
    "text": "who are developing your web applications um and then it's expensive to build and",
    "start": "938399",
    "end": "943600"
  },
  {
    "text": "maintain many different uh models in the same application so if you are building",
    "start": "943600",
    "end": "949720"
  },
  {
    "text": "a recommendation system for example it's uh most likely a combination of two or",
    "start": "949720",
    "end": "956040"
  },
  {
    "text": "three different models that are chained together um and they all might have their own like",
    "start": "956040",
    "end": "961680"
  },
  {
    "text": "memory and resource requirements like different GPU different CPU requirements",
    "start": "961680",
    "end": "967079"
  },
  {
    "text": "uh but there's no good patterns for deploying these while maintaining low",
    "start": "967079",
    "end": "972519"
  },
  {
    "text": "operational overhead like people either set it up as separate microservices um as the sage maker kind",
    "start": "972519",
    "end": "979880"
  },
  {
    "text": "of Route um where you you can scale these independently but now you have a lot of operational overhead of",
    "start": "979880",
    "end": "987199"
  },
  {
    "text": "maintaining all these services or you can set up a monolith but then you're not scaling as well because you're not",
    "start": "987199",
    "end": "993079"
  },
  {
    "text": "utilizing your gpus and CPUs the best you can um the third one is you want to have",
    "start": "993079",
    "end": "999920"
  },
  {
    "text": "a very extensible framework so um a lot of Frameworks like tensorflow serving or",
    "start": "999920",
    "end": "1005880"
  },
  {
    "text": "pyod serving were really uh Limited in like the libraries they can support um",
    "start": "1005880",
    "end": "1011360"
  },
  {
    "text": "or even Triton was very limited in um what types of inference engines you can",
    "start": "1011360",
    "end": "1017519"
  },
  {
    "text": "run and then you also want to be able to run on any type of Hardware uh gpus um",
    "start": "1017519",
    "end": "1024400"
  },
  {
    "text": "new hardware coming out from AWS or other vendors so this is where rerf came in uh",
    "start": "1024400",
    "end": "1032959"
  },
  {
    "text": "where it the idea was build something that is high performance flexible and",
    "start": "1032959",
    "end": "1038760"
  },
  {
    "text": "super easy to use so racer is a scalable python framework to put uh ml",
    "start": "1038760",
    "end": "1044760"
  },
  {
    "text": "applications in production in Python with just a few lines of code change so you add a couple of annotations uh to",
    "start": "1044760",
    "end": "1051880"
  },
  {
    "text": "your python code and you're good to go and you can mix and match many different",
    "start": "1051880",
    "end": "1056960"
  },
  {
    "text": "models and build a complex ml application where each of these models can scale independently but everything",
    "start": "1056960",
    "end": "1063880"
  },
  {
    "text": "is behind one single service so there's features in racer called Model composition model",
    "start": "1063880",
    "end": "1069919"
  },
  {
    "text": "multiplexing uh fractional gpus all of these allow you to get the most out of your hardware and deploy these more",
    "start": "1069919",
    "end": "1076960"
  },
  {
    "text": "complex ml applications that are more more than just one single model behind an endpoint and then it's extensible by",
    "start": "1076960",
    "end": "1083440"
  },
  {
    "text": "Design so you can bring in any framework like P horch tlow any optimization",
    "start": "1083440",
    "end": "1089039"
  },
  {
    "text": "engine like you can run VM you could run uh TGI you could run uh tensor RT llm as",
    "start": "1089039",
    "end": "1096280"
  },
  {
    "text": "well all behind one single pan of glass so um this is this was the big",
    "start": "1096280",
    "end": "1104080"
  },
  {
    "text": "motivation behind racer and a lot of people deploy VM with racer to get that",
    "start": "1104080",
    "end": "1110400"
  },
  {
    "text": "uh complete production Readiness that R serve offers with the optimizations that realm has now I'll hand it off to V to",
    "start": "1110400",
    "end": "1119120"
  },
  {
    "text": "do a quick demo thanks aai um let me share my",
    "start": "1119120",
    "end": "1127880"
  },
  {
    "text": "screen right uh hi everyone U my name is vuntu I'm principal open source speci",
    "start": "1134640",
    "end": "1141120"
  },
  {
    "text": "architect working with uh AWS and you heard from AA and you heard from art and",
    "start": "1141120",
    "end": "1147320"
  },
  {
    "text": "Matthew as well and this is there are a number of ways that you can deploy uh llms today uh inference on um various",
    "start": "1147320",
    "end": "1155600"
  },
  {
    "text": "platforms and within AWS you have sagemaker bedrock and and even Amazon eks um for the customers who actually",
    "start": "1155600",
    "end": "1164400"
  },
  {
    "text": "sanitized uh their workloads running data and ml workloads running on",
    "start": "1164400",
    "end": "1169480"
  },
  {
    "text": "a kuet is and this demo is for you where we will show you how easy it is to",
    "start": "1169480",
    "end": "1175440"
  },
  {
    "text": "actually deploy large language models such as Lama 38b model on Amazon eks",
    "start": "1175440",
    "end": "1181919"
  },
  {
    "text": "using Ray and neuron uh and we will today specifically we will be using",
    "start": "1181919",
    "end": "1188080"
  },
  {
    "text": "inferential instances so before we get into um uh looking at the demo let's",
    "start": "1188080",
    "end": "1194880"
  },
  {
    "text": "talk about some of the key components that you need to deploy any model on Amazon eks and let's talk about some of",
    "start": "1194880",
    "end": "1201159"
  },
  {
    "text": "the plugins right uh as you see on uh on the",
    "start": "1201159",
    "end": "1207720"
  },
  {
    "text": "slide on the left hand side you have Amazon eks control plane that is fully managed by AWS so once you deploy eks",
    "start": "1207720",
    "end": "1214799"
  },
  {
    "text": "cluster that's the first step that you want to deploy any cluster and then to scale up the nodes and you will use car",
    "start": "1214799",
    "end": "1221679"
  },
  {
    "text": "cluster out of scaler uh where it will create necessary nodes for you to deploy the workloads but actually to deploy uh",
    "start": "1221679",
    "end": "1229520"
  },
  {
    "text": "uh llm influence on neuron instances such as infu instances or training",
    "start": "1229520",
    "end": "1235320"
  },
  {
    "text": "instances you need couple of plugins right so let's talk about some of those things and to start with uh eks army",
    "start": "1235320",
    "end": "1242440"
  },
  {
    "text": "with the neuron drivers so when you want to deploy your workloads you need to make sure that you use the right Ami uh",
    "start": "1242440",
    "end": "1249200"
  },
  {
    "text": "currently the neuron drivers are available as a part of Amazon Linux 2 which is what you see in this uh and it",
    "start": "1249200",
    "end": "1256080"
  },
  {
    "text": "comes with the drivers like AWS neuronic uh drivers so uh once you use Ami and",
    "start": "1256080",
    "end": "1261960"
  },
  {
    "text": "you should be ready to start working and deploying the models or even training the",
    "start": "1261960",
    "end": "1267440"
  },
  {
    "text": "models uh and then step two uh to expose some of these neuron accelerator such as",
    "start": "1267440",
    "end": "1273760"
  },
  {
    "text": "from INF instances even train stances to kubernetes API server uh you need a",
    "start": "1273760",
    "end": "1278840"
  },
  {
    "text": "neuron device pluging to be deployed it's a demon Set uh and we uh you can",
    "start": "1278840",
    "end": "1284240"
  },
  {
    "text": "use C CDL manifest to apply it or you can use some of the hel charts that we are planning to publish it soon and and",
    "start": "1284240",
    "end": "1290600"
  },
  {
    "text": "that's going to run on every single training mod inferentia instances that make sure that these resources are",
    "start": "1290600",
    "end": "1297279"
  },
  {
    "text": "available to kubernetes API server to schedule your workloads and then the next plugin is a",
    "start": "1297279",
    "end": "1304039"
  },
  {
    "text": "neuron schedular extension uh as a part of the neuron schu extension uh this is really useful when you are using a",
    "start": "1304039",
    "end": "1310039"
  },
  {
    "text": "larger instances with multiple neuron devices and course are available where you are only requesting fewer number of",
    "start": "1310039",
    "end": "1316320"
  },
  {
    "text": "devices say for example you want to deploy multiple replicas of the same model within the B bigger instance and",
    "start": "1316320",
    "end": "1323240"
  },
  {
    "text": "you want the Schuler to actually uh uh schedule your workload it ensures that",
    "start": "1323240",
    "end": "1328960"
  },
  {
    "text": "your um scheduling of those devices happens in a contigous manner based on",
    "start": "1328960",
    "end": "1335240"
  },
  {
    "text": "the the model that Cranium supports as well as inferential",
    "start": "1335240",
    "end": "1340200"
  },
  {
    "text": "supports uh and then moving on to uron problem detector this is one of the common thing and you might have heard",
    "start": "1340559",
    "end": "1345760"
  },
  {
    "text": "already even the gpus or even neuron and system sometimes there are Hardware failures while you're doing training or",
    "start": "1345760",
    "end": "1351559"
  },
  {
    "text": "influence what you really need is and you want the instance to be identify those Hardware failures Hardware",
    "start": "1351559",
    "end": "1358120"
  },
  {
    "text": "failures and then um um restart the node and re you know redeploy the workloads",
    "start": "1358120",
    "end": "1364400"
  },
  {
    "text": "and so neuron problem detector the add-on that we you know once you deploy as a d set that identifies it and",
    "start": "1364400",
    "end": "1371679"
  },
  {
    "text": "replaces the note for you and then finally neuron monitor demer uh this is",
    "start": "1371679",
    "end": "1377159"
  },
  {
    "text": "useful to actually expose uh export all your metrics uh such as uh neuron",
    "start": "1377159",
    "end": "1382840"
  },
  {
    "text": "metrics to either Prometheus or cloudwatch so that's all you need in terms of the plugins uh once you have",
    "start": "1382840",
    "end": "1389679"
  },
  {
    "text": "eks cluster uh eks cluster and all the plugins you're ready to deploy your",
    "start": "1389679",
    "end": "1395120"
  },
  {
    "text": "work so let's talk about the architecture before we go into the demo itself uh this architecture it's",
    "start": "1395120",
    "end": "1400919"
  },
  {
    "text": "deploying Lama 3B Lama 38b 8 bilon parameter inference on eks using Ray VM",
    "start": "1400919",
    "end": "1407880"
  },
  {
    "text": "and infen it's a great combination uh Ray is very popular uh among a lot of",
    "start": "1407880",
    "end": "1413039"
  },
  {
    "text": "the users and the customers and it helps you to do it simplifies the whole journey of deploying your workloads on",
    "start": "1413039",
    "end": "1418720"
  },
  {
    "text": "kubernetes and then with the combination of BLM and that's a powerful back end engine that actually I mentioned before",
    "start": "1418720",
    "end": "1425080"
  },
  {
    "text": "uh which improves the performance of your inference in terms of batching and request we can take a look at that once",
    "start": "1425080",
    "end": "1430520"
  },
  {
    "text": "you go through once you go through this architecture but in this architecture as you see we have an eks cluster and I",
    "start": "1430520",
    "end": "1436559"
  },
  {
    "text": "have Carpenter autoscaler very powerful autoscaler I highly recommend for customers who are using deploying your",
    "start": "1436559",
    "end": "1443120"
  },
  {
    "text": "workload especially the inflence workloads uh with the rray it always comes with the array headp and the",
    "start": "1443120",
    "end": "1449559"
  },
  {
    "text": "worker part so when you use aray service API uh it creates array cluster and also",
    "start": "1449559",
    "end": "1456159"
  },
  {
    "text": "creates an a service so that you can interact with the service and work with the workloads so we can look at the uh",
    "start": "1456159",
    "end": "1463039"
  },
  {
    "text": "actual spec of how to deploy this but in summary uh the virtual assistant as you see on the left hand where the users",
    "start": "1463039",
    "end": "1469120"
  },
  {
    "text": "interact with the network load balancer or application load balancer sendor request to engress controllers like",
    "start": "1469120",
    "end": "1474840"
  },
  {
    "text": "enginex and that reaches to the uh the individual nodes where the model is deployed but here I'm using in2 aex",
    "start": "1474840",
    "end": "1482320"
  },
  {
    "text": "large uh as you can see there are two instances that means you can scale horizontally based on the autoscaling",
    "start": "1482320",
    "end": "1488720"
  },
  {
    "text": "capabilities that Ray provides that can be any metric that you choose that based",
    "start": "1488720",
    "end": "1493760"
  },
  {
    "text": "on that metric and array creates in multiple actors and you scale your workloads even within the node even",
    "start": "1493760",
    "end": "1501240"
  },
  {
    "text": "across the nodes so with INF to 8X Notch we only have one inferential chip and we",
    "start": "1501240",
    "end": "1507240"
  },
  {
    "text": "have an accelerator memory of 32 GB but to load the Lama 38b instruct models I",
    "start": "1507240",
    "end": "1512880"
  },
  {
    "text": "think we need somewhere around 20 2 GB of accelerator memory that fits into a",
    "start": "1512880",
    "end": "1518120"
  },
  {
    "text": "single neuron uh device and each neuron device comes with the two neuron cor so we are",
    "start": "1518120",
    "end": "1524960"
  },
  {
    "text": "using a tensor parallelism of two to ensure model loads and to uh neuron um",
    "start": "1524960",
    "end": "1531360"
  },
  {
    "text": "uh course and then you can also see the neuron device plugin which is running as a d set on every single",
    "start": "1531360",
    "end": "1537919"
  },
  {
    "text": "instance so the model can be pulled from hugging face but in this example we are pulling from hugging face but you can",
    "start": "1537919",
    "end": "1544320"
  },
  {
    "text": "also use uh a model stored in your local desk and model stored in S3 or even S3",
    "start": "1544320",
    "end": "1550480"
  },
  {
    "text": "Mount Point various approaches and Amazon ECR is used to actually store",
    "start": "1550480",
    "end": "1555640"
  },
  {
    "text": "your container images like today uh we are pulling an image from ECR that",
    "start": "1555640",
    "end": "1561200"
  },
  {
    "text": "contains all the necessary plugins and everything that's pretty much an end to end it's a simple configuration and you",
    "start": "1561200",
    "end": "1567039"
  },
  {
    "text": "can deploy all the models which are uh supported today with the neuron for the inference using either INF to instances",
    "start": "1567039",
    "end": "1574520"
  },
  {
    "text": "even training instances say for example when you want to look at deploying Lama",
    "start": "1574520",
    "end": "1579559"
  },
  {
    "text": "3.1 45b model which is like a distributed in front model you can look at using tranium 32 x large across",
    "start": "1579559",
    "end": "1587000"
  },
  {
    "text": "multiple nodes as well but then the question is how to deploy",
    "start": "1587000",
    "end": "1592720"
  },
  {
    "text": "right so we want to run all of these workloads on eks and and do you have any patterns or examples that we can go",
    "start": "1592720",
    "end": "1599279"
  },
  {
    "text": "ahead but this is uh at AWS we created this data on eks initiative uh which is",
    "start": "1599279",
    "end": "1604720"
  },
  {
    "text": "an open source solution you get all the patterns under gen when you click on explore gen uh you have number of",
    "start": "1604720",
    "end": "1611760"
  },
  {
    "text": "training and inference patterns available so we're going to look at the the key pattern that we are going to see",
    "start": "1611760",
    "end": "1618399"
  },
  {
    "text": "a demo now right let's get started with the demo so for the sake of uh deployment um",
    "start": "1618399",
    "end": "1626960"
  },
  {
    "text": "so the actual deployment of eks cluster will take let me go into the code uh uh",
    "start": "1626960",
    "end": "1632559"
  },
  {
    "text": "this is the the repo I mentioned about and these are s of some of terraform templates uh but if you using any",
    "start": "1632559",
    "end": "1638559"
  },
  {
    "text": "infrastructure as a code you can use this as a reference but if you're using terraform uh it makes it a lot easier so",
    "start": "1638559",
    "end": "1644600"
  },
  {
    "text": "there is only one command here install.sh uh you go to training frena path under the REO once you clone the",
    "start": "1644600",
    "end": "1651000"
  },
  {
    "text": "repo you go to the AML trainman frener and run install that will take nearly",
    "start": "1651000",
    "end": "1656919"
  },
  {
    "text": "around 17 to 18 minutes that will create eks cluster it creates deploys all the necessary plugins including Carpenter",
    "start": "1656919",
    "end": "1663720"
  },
  {
    "text": "and so on so once that is done uh then the next step is to go and deploy your",
    "start": "1663720",
    "end": "1669360"
  },
  {
    "text": "actual workloads because with the ray uh there's a CU operator available and this",
    "start": "1669360",
    "end": "1674960"
  },
  {
    "text": "module also deploys the Cub operator and all those plugins are available the code",
    "start": "1674960",
    "end": "1680200"
  },
  {
    "text": "is within the add-on Str here right so now let's get into the",
    "start": "1680200",
    "end": "1685519"
  },
  {
    "text": "actual deployment uh Ray yaml file this is what where you are interested in so once you have V cluster running Cube Ray",
    "start": "1685519",
    "end": "1692480"
  },
  {
    "text": "operator running and all the neuron device plugins are running with that previous step uh and the next thing you",
    "start": "1692480",
    "end": "1698360"
  },
  {
    "text": "do is to create a manifest and that manifest we have an example here under generative AI uh VM race Ser inf2 um but",
    "start": "1698360",
    "end": "1707039"
  },
  {
    "text": "before we go into the manif first uh this is the docker file where I'm using Ray project to create a custom Docker",
    "start": "1707039",
    "end": "1713840"
  },
  {
    "text": "file with all the necessary packages sometimes you want to bring your own packages and youron next packages and so",
    "start": "1713840",
    "end": "1719440"
  },
  {
    "text": "on uh that's pretty much once the container is ready uh you push it to the ECR and we will use that container image",
    "start": "1719440",
    "end": "1727159"
  },
  {
    "text": "uh within the ray SER spec right so this is the VM face of",
    "start": "1727159",
    "end": "1732960"
  },
  {
    "text": "deployment file uh that's I have deployed it but we'll take a look at in a minute",
    "start": "1732960",
    "end": "1738600"
  },
  {
    "text": "um in this I'm just creating a dedicated namespace and I'm creating a config map",
    "start": "1738600",
    "end": "1743760"
  },
  {
    "text": "and config map contains that serving stript the serving script is open AI APA",
    "start": "1743760",
    "end": "1748919"
  },
  {
    "text": "compatible uh so basically pretty much you can use the streaming way of you",
    "start": "1748919",
    "end": "1754120"
  },
  {
    "text": "know serving that model uh this loads the model and uh and you know influence",
    "start": "1754120",
    "end": "1760600"
  },
  {
    "text": "the model pretty much that's SC and this is the race API spec Race Service API",
    "start": "1760600",
    "end": "1766200"
  },
  {
    "text": "spec that you're interested in um uh in this case we're using the same name space and the key configurations",
    "start": "1766200",
    "end": "1773159"
  },
  {
    "text": "here I'm pulling this model from hugging face which is in this case you need to get um an approval from Lama 3 uh you",
    "start": "1773159",
    "end": "1780440"
  },
  {
    "text": "know the license and so on through hugging pH once you are enabled to pull that model you can use that but this",
    "start": "1780440",
    "end": "1787000"
  },
  {
    "text": "model does not require so I'm using some open source model from hugging face uh we using T parallelism uh you can set",
    "start": "1787000",
    "end": "1793880"
  },
  {
    "text": "all these parameters based on your model size and so on and and this is where the Autos scaling configuration comes into",
    "start": "1793880",
    "end": "1800279"
  },
  {
    "text": "number of replicas that you want these are the active replicas and this is the configuration",
    "start": "1800279",
    "end": "1806240"
  },
  {
    "text": "for the neuron course because I'm using in2 atex Lodge I only specify neuron course 2 per actor and I need two CPUs",
    "start": "1806240",
    "end": "1813200"
  },
  {
    "text": "but that can be further customized and and this is the image where we specified",
    "start": "1813200",
    "end": "1818919"
  },
  {
    "text": "my head part is using and even the worker part is using the image does not contain this serving script itself that",
    "start": "1818919",
    "end": "1824960"
  },
  {
    "text": "is coming from the config config map that we mentioned before so we have set of other configurations",
    "start": "1824960",
    "end": "1830960"
  },
  {
    "text": "but the key configuration that I want to highlight here is the neuron resources here that you see in line number 267 uh",
    "start": "1830960",
    "end": "1837919"
  },
  {
    "text": "for the worker group spe this is where you see how many number of workers that you want to spin up and sometimes one",
    "start": "1837919",
    "end": "1844240"
  },
  {
    "text": "worker can run one replica but the larger instances you can run multiple replicas within one worker but each",
    "start": "1844240",
    "end": "1850600"
  },
  {
    "text": "worker uh it's configuring as neuron claser 2 this is ensures that horizontal",
    "start": "1850600",
    "end": "1856000"
  },
  {
    "text": "scaling of number of nodes uh happens in the you know expected Manel so uh we are also using custom",
    "start": "1856000",
    "end": "1863519"
  },
  {
    "text": "neuron Schuler uh that will ensure you know uh scheduling the Pod and this is",
    "start": "1863519",
    "end": "1869360"
  },
  {
    "text": "the key configuration to run on inf2 instance because I'm aiming at inf2 8X",
    "start": "1869360",
    "end": "1874399"
  },
  {
    "text": "which is a smaller instance with one neuron core uh so I'm using CPU and memory is aligned to that because uh",
    "start": "1874399",
    "end": "1881559"
  },
  {
    "text": "using these requests and Carpenter actually kicks in uh uh right uh uh is my video off right",
    "start": "1881559",
    "end": "1890440"
  },
  {
    "text": "sorry I'm a bit late on that uh right okay so using the uh uh cinter looks at these CP and",
    "start": "1890440",
    "end": "1898320"
  },
  {
    "text": "memory requirements and then identifies the right instance and Spins up so for example if I wanted to spin up a larger",
    "start": "1898320",
    "end": "1904240"
  },
  {
    "text": "instance and if I can like in to 48x large I just need to change my CPU on memory to the larger uh based on that",
    "start": "1904240",
    "end": "1911679"
  },
  {
    "text": "instance capacity and car interp of the right instance for it so in these cases you can have multiple",
    "start": "1911679",
    "end": "1918399"
  },
  {
    "text": "running within the same 48x large instance uh because in 48x large I think",
    "start": "1918399",
    "end": "1923799"
  },
  {
    "text": "provides six neuron CES but I need to double check again but in to 8X large has only one neuron code so that's",
    "start": "1923799",
    "end": "1930720"
  },
  {
    "text": "pretty much this is the only configuration but to deploy this all that you will be doing is Cube CDL apply",
    "start": "1930720",
    "end": "1937720"
  },
  {
    "text": "on that VM race of deployment that's the only command once you run that command",
    "start": "1937720",
    "end": "1943720"
  },
  {
    "text": "you should see the model and so as you can see here I have a I",
    "start": "1943720",
    "end": "1948960"
  },
  {
    "text": "already ran that deployment and it's under VM I have one headp pod running",
    "start": "1948960",
    "end": "1955720"
  },
  {
    "text": "and one worker part rning uh and the head Port within the head Port uh and the worker part scales",
    "start": "1955720",
    "end": "1963000"
  },
  {
    "text": "horizontally based on the request that you are sending because we have Autos scaling enabled for that uh but to uh",
    "start": "1963000",
    "end": "1969960"
  },
  {
    "text": "send an inference request I have also created open AI web UI which is very",
    "start": "1969960",
    "end": "1975440"
  },
  {
    "text": "powerful web UI Rich web UI connects to this our service and uh and then",
    "start": "1975440",
    "end": "1981159"
  },
  {
    "text": "showcases you know uh we can take a look at how it interacts with the service model and gets a response back but let's",
    "start": "1981159",
    "end": "1988399"
  },
  {
    "text": "look at the ray dashboards right so once this creates two one for headp one for",
    "start": "1988399",
    "end": "1993600"
  },
  {
    "text": "service part uh um sorry worker part and then it also creates a bunch of services",
    "start": "1993600",
    "end": "1999320"
  },
  {
    "text": "one for head Service uh the head service exposes that uh R",
    "start": "1999320",
    "end": "2004399"
  },
  {
    "text": "dashboard which is uh here uh",
    "start": "2004399",
    "end": "2009760"
  },
  {
    "text": "865 uh and the actual model is exposed",
    "start": "2013600",
    "end": "2018760"
  },
  {
    "text": "through uh the port 8,000 which is part of the specific service that you see here VM L 3 into serve",
    "start": "2018760",
    "end": "2027398"
  },
  {
    "text": "service right so I already done that so let's take a look at rid",
    "start": "2027720",
    "end": "2033919"
  },
  {
    "text": "dashboard so this is the raid dashboard you can see the deployment V deployment that we deployed so when I click on the",
    "start": "2033919",
    "end": "2039919"
  },
  {
    "text": "deployment that will take you to that deployment specific deployment and racer shows the deployment here with one",
    "start": "2039919",
    "end": "2046760"
  },
  {
    "text": "replica let's go back to that that's the one replica is running uh and then you can also see the",
    "start": "2046760",
    "end": "2054118"
  },
  {
    "text": "individual actors so now this model uh we looked at this model is available",
    "start": "2054119",
    "end": "2061919"
  },
  {
    "text": "under uh the service name is VM Lama 3 inut service and open web UI the",
    "start": "2061919",
    "end": "2067720"
  },
  {
    "text": "deployment that we created that is also part of that code um that actually interacts with the service and does",
    "start": "2067720",
    "end": "2075118"
  },
  {
    "text": "inference so let's look at the actual demo here um so let start with the new",
    "start": "2075119",
    "end": "2081158"
  },
  {
    "text": "chat and what you see here is a local host 880 is a p forward on the um open",
    "start": "2081159",
    "end": "2088599"
  },
  {
    "text": "web UI service and and the model is exposed here so uh this connects to the",
    "start": "2088599",
    "end": "2096638"
  },
  {
    "text": "model that we deployed on eks uh so you can simply select that model uh this is the model uh once you select the model",
    "start": "2096639",
    "end": "2104119"
  },
  {
    "text": "you can pretty much ask um what is generative Ai",
    "start": "2104119",
    "end": "2111920"
  },
  {
    "text": "and",
    "start": "2111920",
    "end": "2114920"
  },
  {
    "text": "explain so you can see the streaming output it's basically going to uh uh",
    "start": "2118920",
    "end": "2124760"
  },
  {
    "text": "hitting our in front server in to x large and getting the response back but if you send uh hundreds of requests and",
    "start": "2124760",
    "end": "2131960"
  },
  {
    "text": "Autos scaling kicks in and start showing uh some of these uh uh the response pack",
    "start": "2131960",
    "end": "2138800"
  },
  {
    "text": "uh and it also I think we can take a look at the uh aut scaling now uh one",
    "start": "2138800",
    "end": "2144720"
  },
  {
    "text": "way to do is while this response is coming through uh we have bunch of",
    "start": "2144720",
    "end": "2150200"
  },
  {
    "text": "scripts within VM racer um one script is a open",
    "start": "2150200",
    "end": "2156040"
  },
  {
    "text": "clim so if i",
    "start": "2156040",
    "end": "2160000"
  },
  {
    "text": "t so we only have one replica at the moment uh that we looked at as a part of",
    "start": "2167319",
    "end": "2172760"
  },
  {
    "text": "the ray dashboard uh actor um go to the Sur one replica but uh the one way to",
    "start": "2172760",
    "end": "2179240"
  },
  {
    "text": "test is putting uh some stress on it sending multiple requests so that you should see more replicas showing up as",
    "start": "2179240",
    "end": "2185400"
  },
  {
    "text": "well uh before I run this command and I think we need to do a port forward on",
    "start": "2185400",
    "end": "2190760"
  },
  {
    "text": "specific um service which is",
    "start": "2190760",
    "end": "2195800"
  },
  {
    "text": "8,000 because that client is using this specific",
    "start": "2195800",
    "end": "2201160"
  },
  {
    "text": "service so the put forward is done uh going back and running this",
    "start": "2203000",
    "end": "2210480"
  },
  {
    "text": "so uh I'm running multiple uh just sending multiple requests to open",
    "start": "2235640",
    "end": "2242720"
  },
  {
    "text": "client to see the behavior probably take uh couple of minutes",
    "start": "2242720",
    "end": "2249520"
  },
  {
    "text": "um so up uh so I've sent number of requests so far and and we should",
    "start": "2251359",
    "end": "2259800"
  },
  {
    "text": "uh and the r should show the replicas and probably take a couple of minutes before it actually identifies that and",
    "start": "2259800",
    "end": "2267680"
  },
  {
    "text": "then we should start seeing some pending replicas to load uh but then while that",
    "start": "2267680",
    "end": "2273040"
  },
  {
    "text": "is coming up um let's go back to the service again",
    "start": "2273040",
    "end": "2279480"
  },
  {
    "text": "with the code so right so we looked at open web UI as",
    "start": "2279480",
    "end": "2287079"
  },
  {
    "text": "well and the way the open web UI again we containerize the solution so that",
    "start": "2287079",
    "end": "2292280"
  },
  {
    "text": "interacts with the VM service that we deployed um that's running on in",
    "start": "2292280",
    "end": "2298079"
  },
  {
    "text": "instance but one thing uh probably would be nice to see is uh let me show you",
    "start": "2298079",
    "end": "2305040"
  },
  {
    "text": "guys",
    "start": "2305040",
    "end": "2308040"
  },
  {
    "text": "the instance itself so here what you see here is the",
    "start": "2315160",
    "end": "2323560"
  },
  {
    "text": "neuron PL coming um so if you want to this is the note that we are using as a",
    "start": "2323560",
    "end": "2329880"
  },
  {
    "text": "part of this deployment and neuron LS will show you how many neuron devices and course are running and neuron top",
    "start": "2329880",
    "end": "2337240"
  },
  {
    "text": "will will show you the the usage of the accelerator memory say for example uh we",
    "start": "2337240",
    "end": "2343680"
  },
  {
    "text": "want to send some requests now",
    "start": "2343680",
    "end": "2347720"
  },
  {
    "text": "um you should see from the green thing uh L up in the both the places that's",
    "start": "2358839",
    "end": "2364119"
  },
  {
    "text": "basically uh sending inference and and the accelerator memory uh is is used and",
    "start": "2364119",
    "end": "2371640"
  },
  {
    "text": "getting the response back there's a usage of 95% there so yeah that that's pretty much uh",
    "start": "2371640",
    "end": "2379160"
  },
  {
    "text": "what I wanted to show today um as you can see here um uh you know there's",
    "start": "2379160",
    "end": "2385240"
  },
  {
    "text": "couple of steps that you can deploy and uh and then start influen start",
    "start": "2385240",
    "end": "2390520"
  },
  {
    "text": "influencing the workl and eks so with that uh I will open up for",
    "start": "2390520",
    "end": "2396440"
  },
  {
    "text": "the questions and Matthew you're bringing you back sounds",
    "start": "2396440",
    "end": "2403680"
  },
  {
    "text": "good thanks VAR I see a handful of questions here so we'll uh we'll go through um a number of them and then",
    "start": "2403680",
    "end": "2410000"
  },
  {
    "text": "wrap up um so first we had a question on doing batch inference for text embedding",
    "start": "2410000",
    "end": "2416440"
  },
  {
    "text": "models perhaps uh OE um do you want to address if we can do batch inference for",
    "start": "2416440",
    "end": "2422000"
  },
  {
    "text": "text embedding models as well yeah uh yeah absolutely so uh the typical",
    "start": "2422000",
    "end": "2427880"
  },
  {
    "text": "Library that's used for batch entrance is Ray data and with Ray data you can set up U again like these hetrogeneous",
    "start": "2427880",
    "end": "2435119"
  },
  {
    "text": "workloads um on a very large scale of data set um offline and it has a lot of",
    "start": "2435119",
    "end": "2441480"
  },
  {
    "text": "connectors to data cataloges as well and it can run any models under the hood so a lot of that's the typical pattern so",
    "start": "2441480",
    "end": "2448520"
  },
  {
    "text": "even when you set up batch and frints with BLM as the engine you would set it up with Ray data orchestrating the VM",
    "start": "2448520",
    "end": "2456599"
  },
  {
    "text": "engines and preparing data to be processed great thank you um next",
    "start": "2456599",
    "end": "2463720"
  },
  {
    "text": "question on a book or online course to learn Ray serve I'll give a uh Shameless",
    "start": "2463720",
    "end": "2469200"
  },
  {
    "text": "plug for Ray Summit again and some of the training going on at at that event um I would say maybe perhaps some of the",
    "start": "2469200",
    "end": "2475960"
  },
  {
    "text": "best resources are the documentation around Ray serve um so if you go to ray.",
    "start": "2475960",
    "end": "2481280"
  },
  {
    "text": "and take a look at some of the docs there's some really good tutorials that walk through Ray serve in in significant detail and can get you start started we",
    "start": "2481280",
    "end": "2488160"
  },
  {
    "text": "are hosting a number of different training events so keep an eye out on that as well um sharing the code base I think",
    "start": "2488160",
    "end": "2495319"
  },
  {
    "text": "will we be able to do that uh Vara can we share some of the things that um you put together",
    "start": "2495319",
    "end": "2502200"
  },
  {
    "text": "today oh I think you're muted bar sorry sure so uh let me send a link here to",
    "start": "2503440",
    "end": "2509119"
  },
  {
    "text": "all the participants great uh that's the a link where the code is",
    "start": "2509119",
    "end": "2514960"
  },
  {
    "text": "available uh and the link where uh yeah so if you want to deploy whether",
    "start": "2514960",
    "end": "2521040"
  },
  {
    "text": "it's a training or in print yeah you have pretty much all the examples in the code in there awesome thank you uh there",
    "start": "2521040",
    "end": "2526880"
  },
  {
    "text": "was a question on do we need a separate head node for each model deployment so AUA you were talking about model",
    "start": "2526880",
    "end": "2533560"
  },
  {
    "text": "multiplexing and deploying multiple models at once um can you talk a little bit more about you know any requirements",
    "start": "2533560",
    "end": "2539359"
  },
  {
    "text": "around multiple you know head nodes or or how that works yeah so uh with rayer",
    "start": "2539359",
    "end": "2545520"
  },
  {
    "text": "you can set up one service with many different models so each service requires one head node uh so you can",
    "start": "2545520",
    "end": "2551680"
  },
  {
    "text": "have one head node shared across many different models that you orchestrate in one",
    "start": "2551680",
    "end": "2558520"
  },
  {
    "text": "service great um next question on sort of monitoring utilization I think you",
    "start": "2559760",
    "end": "2566599"
  },
  {
    "text": "you touched on that VAR but some questions on perhaps being able to forward that information to something",
    "start": "2566599",
    "end": "2571839"
  },
  {
    "text": "like Prometheus so can you talk a little bit about that yeah sure so um currently like um",
    "start": "2571839",
    "end": "2578680"
  },
  {
    "text": "like we talked about neuron monitor uh that's running as a you know as a Damon set of the deployment and that actually",
    "start": "2578680",
    "end": "2585599"
  },
  {
    "text": "exposes the metrics to uh both cloudwatch as well as Prometheus as well",
    "start": "2585599",
    "end": "2590760"
  },
  {
    "text": "so we can visualize those metrics and on top of that you have this neuron LS and",
    "start": "2590760",
    "end": "2595839"
  },
  {
    "text": "neuron top which I showed as a part of the demo uh you can look at individual monitoring but um so we are planning to",
    "start": "2595839",
    "end": "2603440"
  },
  {
    "text": "publish a little bit more observability around the example I shared so you will have all the uh grafana dashboards which",
    "start": "2603440",
    "end": "2610559"
  },
  {
    "text": "visualizes those metrics great uh and there was a question on um you know the tool that",
    "start": "2610559",
    "end": "2616480"
  },
  {
    "text": "you were using to access the the pods um you know when you deployed your um KU",
    "start": "2616480",
    "end": "2622000"
  },
  {
    "text": "cluster and kubra can you comment on what that was yeah sure so uh to simplify this accessing the kubernetes",
    "start": "2622000",
    "end": "2628880"
  },
  {
    "text": "I'm using lens uh it's not an open source tool just to be aware um so uh",
    "start": "2628880",
    "end": "2636400"
  },
  {
    "text": "that will give you like if you want to manage multiple kuet clusters uh it makes it easy to you know",
    "start": "2636400",
    "end": "2643800"
  },
  {
    "text": "access great and then we'll take uh one more and then have some final comments um there's a clarification question on",
    "start": "2643800",
    "end": "2650359"
  },
  {
    "text": "if we need a different head node for serving Tex generation and image generation models OE my understanding",
    "start": "2650359",
    "end": "2655800"
  },
  {
    "text": "I'll jump in and give my answer is no you can have one head node um and then you'll just have multiple replicas",
    "start": "2655800",
    "end": "2661559"
  },
  {
    "text": "deployed that can can serve different models but one head node not a different head node",
    "start": "2661559",
    "end": "2668160"
  },
  {
    "text": "yeah great uh so with that uh I'm just going to share my screen uh one last",
    "start": "2669040",
    "end": "2674280"
  },
  {
    "text": "time here uh again plug for Ray Summit we have an enormous slate of",
    "start": "2674280",
    "end": "2679960"
  },
  {
    "text": "announcements uh that are coming as as part of Ray Summit ranging from uh new",
    "start": "2679960",
    "end": "2685359"
  },
  {
    "text": "functionality to improv performance of Ray um the capability we've internally",
    "start": "2685359",
    "end": "2690720"
  },
  {
    "text": "I'll tease it here that we've been calling accelerated dags uh to really boost performance there's a whole host",
    "start": "2690720",
    "end": "2697200"
  },
  {
    "text": "of of other announcements that we are incredibly excited to share uh that the team's been been heads down working on",
    "start": "2697200",
    "end": "2703440"
  },
  {
    "text": "over the past couple months so we'd love to see you there um firsthand to hear all of the you know exciting updates and",
    "start": "2703440",
    "end": "2709520"
  },
  {
    "text": "things we've been up to uh and then also uh another plug um again for for reinvent so for those of you planning to",
    "start": "2709520",
    "end": "2716000"
  },
  {
    "text": "make their way out uh to Las Vegas in in early December we' love to see you there",
    "start": "2716000",
    "end": "2721200"
  },
  {
    "text": "uh amazing opportunity to uh see us and do some training and meet art and",
    "start": "2721200",
    "end": "2726599"
  },
  {
    "text": "Camille Marwan and the team so uh I think that wraps it up um be on the lookout for another webinar coming soon",
    "start": "2726599",
    "end": "2732680"
  },
  {
    "text": "and hope to see you at some of these events thank you um OE art Vara for for all your help and uh appreciate everyone",
    "start": "2732680",
    "end": "2739800"
  },
  {
    "text": "joining thank you very much thank you all thank you",
    "start": "2739800",
    "end": "2746200"
  }
]