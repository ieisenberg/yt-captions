[
  {
    "text": "sorry hi uh my name is Mega and I am a",
    "start": "2679",
    "end": "6640"
  },
  {
    "text": "software engineer at data bricks um",
    "start": "6640",
    "end": "9280"
  },
  {
    "text": "today I'll be talking about some of the",
    "start": "9280",
    "end": "11000"
  },
  {
    "text": "work that we have been doing at data",
    "start": "11000",
    "end": "13080"
  },
  {
    "text": "bricks related to improving llm",
    "start": "13080",
    "end": "15360"
  },
  {
    "text": "inference performance",
    "start": "15360",
    "end": "18039"
  },
  {
    "text": "so yeah let's get started so this is the",
    "start": "18039",
    "end": "21279"
  },
  {
    "text": "outline of the talk um I'll start with",
    "start": "21279",
    "end": "23720"
  },
  {
    "text": "just going brief introduction of our LM",
    "start": "23720",
    "end": "25920"
  },
  {
    "text": "serving products why do we continue",
    "start": "25920",
    "end": "27519"
  },
  {
    "text": "using WM some of the key improvements",
    "start": "27519",
    "end": "30039"
  },
  {
    "text": "that came with the latest release um",
    "start": "30039",
    "end": "32840"
  },
  {
    "text": "also focusing on some of the",
    "start": "32840",
    "end": "34360"
  },
  {
    "text": "optimizations that we have upstreamed to",
    "start": "34360",
    "end": "36719"
  },
  {
    "text": "vlm and also some other projects related",
    "start": "36719",
    "end": "39200"
  },
  {
    "text": "to fp8 and speculative decoding",
    "start": "39200",
    "end": "41719"
  },
  {
    "text": "hopefully these 30 minutes will be",
    "start": "41719",
    "end": "43280"
  },
  {
    "text": "useful before you guys can head to happy",
    "start": "43280",
    "end": "45680"
  },
  {
    "text": "R um so yeah um starting with what data",
    "start": "45680",
    "end": "50719"
  },
  {
    "text": "bricks has to offer and why do we care",
    "start": "50719",
    "end": "52359"
  },
  {
    "text": "about um llm influence performance so",
    "start": "52359",
    "end": "55559"
  },
  {
    "text": "foundational models API is one of our um",
    "start": "55559",
    "end": "58800"
  },
  {
    "text": "products uh um and it has two modes um",
    "start": "58800",
    "end": "63800"
  },
  {
    "text": "for different use cases so the first is",
    "start": "63800",
    "end": "66280"
  },
  {
    "text": "paper token llm serving um this is where",
    "start": "66280",
    "end": "70240"
  },
  {
    "text": "any user can access and query any open",
    "start": "70240",
    "end": "72920"
  },
  {
    "text": "source model or any proprietary models",
    "start": "72920",
    "end": "76119"
  },
  {
    "text": "including dbrx which is which was one of",
    "start": "76119",
    "end": "78479"
  },
  {
    "text": "our own uh mixture of experts model",
    "start": "78479",
    "end": "80640"
  },
  {
    "text": "trained in-house um and it has very",
    "start": "80640",
    "end": "83960"
  },
  {
    "text": "simple pricing for input and output",
    "start": "83960",
    "end": "85759"
  },
  {
    "text": "tokens this is one of the products that",
    "start": "85759",
    "end": "87920"
  },
  {
    "text": "are very widely used by a lot of our us",
    "start": "87920",
    "end": "89920"
  },
  {
    "text": "users especially developers and the",
    "start": "89920",
    "end": "92680"
  },
  {
    "text": "second one that we have is provision",
    "start": "92680",
    "end": "95000"
  },
  {
    "text": "throughput um this is again where you",
    "start": "95000",
    "end": "97280"
  },
  {
    "text": "can use any popular open- Source models",
    "start": "97280",
    "end": "99360"
  },
  {
    "text": "or even bring your own you can also",
    "start": "99360",
    "end": "101680"
  },
  {
    "text": "fine-tune your models on our platform",
    "start": "101680",
    "end": "103799"
  },
  {
    "text": "and then serve them with this uh",
    "start": "103799",
    "end": "105560"
  },
  {
    "text": "provision throughput as an end to-end",
    "start": "105560",
    "end": "107320"
  },
  {
    "text": "experience and what we will provide is",
    "start": "107320",
    "end": "109840"
  },
  {
    "text": "the entire um whole",
    "start": "109840",
    "end": "112200"
  },
  {
    "text": "infrastructure uh where you can pick",
    "start": "112200",
    "end": "114439"
  },
  {
    "text": "minimum and maximum throughput under",
    "start": "114439",
    "end": "116159"
  },
  {
    "text": "some latency constraints and we will",
    "start": "116159",
    "end": "118960"
  },
  {
    "text": "provide the hardware that is running uh",
    "start": "118960",
    "end": "120799"
  },
  {
    "text": "on and also autoscale it for you up or",
    "start": "120799",
    "end": "122960"
  },
  {
    "text": "down based on your unique traffic",
    "start": "122960",
    "end": "124560"
  },
  {
    "text": "patterns with guaranteed",
    "start": "124560",
    "end": "128119"
  },
  {
    "text": "throughput yeah so um our inference",
    "start": "128399",
    "end": "131959"
  },
  {
    "text": "engine is a mixture of backends and we",
    "start": "131959",
    "end": "134000"
  },
  {
    "text": "have uh integrated VM um for to support",
    "start": "134000",
    "end": "138239"
  },
  {
    "text": "many of our serving use cases so why do",
    "start": "138239",
    "end": "140599"
  },
  {
    "text": "we continue uh using VM so some of the",
    "start": "140599",
    "end": "143440"
  },
  {
    "text": "key features are highlighted in this",
    "start": "143440",
    "end": "145519"
  },
  {
    "text": "slide um first it's flexible a lot of uh",
    "start": "145519",
    "end": "149040"
  },
  {
    "text": "broad range of model supports exist on",
    "start": "149040",
    "end": "151440"
  },
  {
    "text": "VM as soon as you have like a new open",
    "start": "151440",
    "end": "154440"
  },
  {
    "text": "source model you will instantly find it",
    "start": "154440",
    "end": "156319"
  },
  {
    "text": "launched on the same day with VM uh we",
    "start": "156319",
    "end": "159040"
  },
  {
    "text": "ourselves Upstream the support uh for",
    "start": "159040",
    "end": "161519"
  },
  {
    "text": "dbrx um when it at the time of the",
    "start": "161519",
    "end": "163640"
  },
  {
    "text": "launch and it was very easy to add a",
    "start": "163640",
    "end": "165640"
  },
  {
    "text": "custom",
    "start": "165640",
    "end": "166599"
  },
  {
    "text": "model um it also has very efficient",
    "start": "166599",
    "end": "169400"
  },
  {
    "text": "memory management continuous batching",
    "start": "169400",
    "end": "171760"
  },
  {
    "text": "for improved through boot page attention",
    "start": "171760",
    "end": "174680"
  },
  {
    "text": "was as you all might be aware was one of",
    "start": "174680",
    "end": "176680"
  },
  {
    "text": "the key optimizations that was",
    "start": "176680",
    "end": "178040"
  },
  {
    "text": "introduced by vnm uh that enables",
    "start": "178040",
    "end": "180480"
  },
  {
    "text": "continuous patching and lastly it has a",
    "start": "180480",
    "end": "183159"
  },
  {
    "text": "very active open source Community",
    "start": "183159",
    "end": "184680"
  },
  {
    "text": "engagement some of the people who are",
    "start": "184680",
    "end": "186360"
  },
  {
    "text": "also here in the audience um that",
    "start": "186360",
    "end": "188440"
  },
  {
    "text": "continue pushing the boundaries for llm",
    "start": "188440",
    "end": "190599"
  },
  {
    "text": "inference",
    "start": "190599",
    "end": "193000"
  },
  {
    "text": "performance so uh taking a bird's eye",
    "start": "193560",
    "end": "196519"
  },
  {
    "text": "view of how vm's architecture looks like",
    "start": "196519",
    "end": "199799"
  },
  {
    "text": "um on a very high level uh the core of",
    "start": "199799",
    "end": "202720"
  },
  {
    "text": "VM has an nlm engine which runs an",
    "start": "202720",
    "end": "205560"
  },
  {
    "text": "infinite Loop in the background and all",
    "start": "205560",
    "end": "207799"
  },
  {
    "text": "the infinite Loop uh happening inside",
    "start": "207799",
    "end": "210360"
  },
  {
    "text": "that Loop is that there is one step that",
    "start": "210360",
    "end": "212040"
  },
  {
    "text": "is iteratively generating a batch",
    "start": "212040",
    "end": "214120"
  },
  {
    "text": "executing it and then processing it so",
    "start": "214120",
    "end": "216799"
  },
  {
    "text": "inside every step in VM you have a",
    "start": "216799",
    "end": "219560"
  },
  {
    "text": "scheduler which prepares the input",
    "start": "219560",
    "end": "222120"
  },
  {
    "text": "tensors to run this on the GPU so it",
    "start": "222120",
    "end": "225640"
  },
  {
    "text": "allocates KV cache for any new requests",
    "start": "225640",
    "end": "229280"
  },
  {
    "text": "um it updates the status of the",
    "start": "229280",
    "end": "231439"
  },
  {
    "text": "sequences as pre-fill or decode and it",
    "start": "231439",
    "end": "233439"
  },
  {
    "text": "mixes all the badge together and then",
    "start": "233439",
    "end": "235560"
  },
  {
    "text": "finally ships this off and broadcast it",
    "start": "235560",
    "end": "237879"
  },
  {
    "text": "to the workers which then runs its own",
    "start": "237879",
    "end": "240760"
  },
  {
    "text": "model Shard of Weights um so this all of",
    "start": "240760",
    "end": "244879"
  },
  {
    "text": "this is happening on the CPU and now",
    "start": "244879",
    "end": "248159"
  },
  {
    "text": "this these input tensors are then",
    "start": "248159",
    "end": "250280"
  },
  {
    "text": "running um on the GPU and you have a",
    "start": "250280",
    "end": "253319"
  },
  {
    "text": "bunch of Kernel launches and um",
    "start": "253319",
    "end": "255879"
  },
  {
    "text": "basically a Cuda graph that is replaying",
    "start": "255879",
    "end": "257440"
  },
  {
    "text": "all these forward passs on the GPU and",
    "start": "257440",
    "end": "259759"
  },
  {
    "text": "then the sampler which then generates",
    "start": "259759",
    "end": "261600"
  },
  {
    "text": "the next token ID this is then shipped",
    "start": "261600",
    "end": "264360"
  },
  {
    "text": "back to the um CPU where the output",
    "start": "264360",
    "end": "267040"
  },
  {
    "text": "processing is happening and inside that",
    "start": "267040",
    "end": "269360"
  },
  {
    "text": "what you have is DET",
    "start": "269360",
    "end": "272280"
  },
  {
    "text": "toonize for like stopping sequences um",
    "start": "272440",
    "end": "276160"
  },
  {
    "text": "removing any of the sequences that have",
    "start": "276160",
    "end": "277919"
  },
  {
    "text": "been finished from the badge um and then",
    "start": "277919",
    "end": "280560"
  },
  {
    "text": "finally streaming the output tokens back",
    "start": "280560",
    "end": "282600"
  },
  {
    "text": "to the user right so a lot of things",
    "start": "282600",
    "end": "285000"
  },
  {
    "text": "happening inside one decoding step but",
    "start": "285000",
    "end": "287240"
  },
  {
    "text": "the same thing keeps up happening from",
    "start": "287240",
    "end": "288800"
  },
  {
    "text": "every um you know in in in an infinite",
    "start": "288800",
    "end": "292680"
  },
  {
    "text": "Loop so we um ran some uh profiling to",
    "start": "292680",
    "end": "298680"
  },
  {
    "text": "see how much of the time the GPU",
    "start": "298680",
    "end": "300720"
  },
  {
    "text": "actually stays idle inside each decoding",
    "start": "300720",
    "end": "303039"
  },
  {
    "text": "step so as you can see here uh only the",
    "start": "303039",
    "end": "305400"
  },
  {
    "text": "forward pass is where the GPU is",
    "start": "305400",
    "end": "307199"
  },
  {
    "text": "actually being used so um we ran some",
    "start": "307199",
    "end": "310680"
  },
  {
    "text": "torch traces for llama 3.1 70b on Forex",
    "start": "310680",
    "end": "315320"
  },
  {
    "text": "h100 with fp8 per tensor",
    "start": "315320",
    "end": "318720"
  },
  {
    "text": "quantization and we saw that at bat size",
    "start": "318720",
    "end": "321800"
  },
  {
    "text": "1 2.6% of the time GPU",
    "start": "321800",
    "end": "325520"
  },
  {
    "text": "is uh ID and not being utilized and as",
    "start": "325520",
    "end": "329360"
  },
  {
    "text": "you increase the bath size it becomes",
    "start": "329360",
    "end": "331400"
  },
  {
    "text": "14% And if you increase it even further",
    "start": "331400",
    "end": "333840"
  },
  {
    "text": "it becomes 25% so clearly uh very",
    "start": "333840",
    "end": "338240"
  },
  {
    "text": "inefficient and um lots of room for",
    "start": "338240",
    "end": "341560"
  },
  {
    "text": "improvement um but this was the state",
    "start": "341560",
    "end": "346039"
  },
  {
    "text": "before we had this latest release uh",
    "start": "346039",
    "end": "348759"
  },
  {
    "text": "which was",
    "start": "348759",
    "end": "349840"
  },
  {
    "text": "0.6.0 plus and a lot of upgrade",
    "start": "349840",
    "end": "353240"
  },
  {
    "text": "performance upgrade happened in this",
    "start": "353240",
    "end": "354680"
  },
  {
    "text": "particular release so there was 2.7x",
    "start": "354680",
    "end": "356880"
  },
  {
    "text": "throughput Improvement and 5x latency",
    "start": "356880",
    "end": "358840"
  },
  {
    "text": "reduction um and this was a result of",
    "start": "358840",
    "end": "362600"
  },
  {
    "text": "three main key changes that were brought",
    "start": "362600",
    "end": "364520"
  },
  {
    "text": "about by the entire um open source",
    "start": "364520",
    "end": "367000"
  },
  {
    "text": "Community um and this was a cross",
    "start": "367000",
    "end": "370120"
  },
  {
    "text": "collaboration across different teams and",
    "start": "370120",
    "end": "373479"
  },
  {
    "text": "um unlike the snail developers in this",
    "start": "373479",
    "end": "376840"
  },
  {
    "text": "meme uh the community was pretty fast at",
    "start": "376840",
    "end": "379280"
  },
  {
    "text": "shipping these uh all of these features",
    "start": "379280",
    "end": "380960"
  },
  {
    "text": "in less than a",
    "start": "380960",
    "end": "382680"
  },
  {
    "text": "month so uh but the next few slides I'm",
    "start": "382680",
    "end": "385960"
  },
  {
    "text": "going to focus more on the last",
    "start": "385960",
    "end": "388639"
  },
  {
    "text": "optimization which was a joint",
    "start": "388639",
    "end": "390319"
  },
  {
    "text": "collaboration between neural magic and",
    "start": "390319",
    "end": "392199"
  },
  {
    "text": "data bricks um so asynchronous output",
    "start": "392199",
    "end": "396319"
  },
  {
    "text": "processing is the key idea behind",
    "start": "396319",
    "end": "399759"
  },
  {
    "text": "asynchronous output processing is that",
    "start": "399759",
    "end": "402599"
  },
  {
    "text": "um you delay the output processing of",
    "start": "402599",
    "end": "405039"
  },
  {
    "text": "Step I to step I + one so if you see",
    "start": "405039",
    "end": "408919"
  },
  {
    "text": "this uh diagram on the top is the state",
    "start": "408919",
    "end": "412599"
  },
  {
    "text": "before asynchronous output processing",
    "start": "412599",
    "end": "414039"
  },
  {
    "text": "was enabled so you have schedule forward",
    "start": "414039",
    "end": "415879"
  },
  {
    "text": "and output but with the asynchronous",
    "start": "415879",
    "end": "417800"
  },
  {
    "text": "output processing all we are doing is",
    "start": "417800",
    "end": "419680"
  },
  {
    "text": "that while the model executor is running",
    "start": "419680",
    "end": "423080"
  },
  {
    "text": "uh for step I we are simultaneously",
    "start": "423080",
    "end": "426840"
  },
  {
    "text": "processing the outputs from the previous",
    "start": "426840",
    "end": "428950"
  },
  {
    "text": "[Music]",
    "start": "428950",
    "end": "430400"
  },
  {
    "text": "step uh previous step I minus one so",
    "start": "430400",
    "end": "433160"
  },
  {
    "text": "what happens here is that this entire",
    "start": "433160",
    "end": "435240"
  },
  {
    "text": "large white bubble gets reduced so your",
    "start": "435240",
    "end": "438039"
  },
  {
    "text": "per token output latency is reduced",
    "start": "438039",
    "end": "440960"
  },
  {
    "text": "because you're reducing the GPU idle",
    "start": "440960",
    "end": "442440"
  },
  {
    "text": "time by concurrently running your output",
    "start": "442440",
    "end": "444879"
  },
  {
    "text": "processor with the model",
    "start": "444879",
    "end": "446720"
  },
  {
    "text": "executor um however there are some",
    "start": "446720",
    "end": "449120"
  },
  {
    "text": "tradeoffs",
    "start": "449120",
    "end": "450199"
  },
  {
    "text": "because this is delayed the output",
    "start": "450199",
    "end": "452080"
  },
  {
    "text": "processing the time to First token will",
    "start": "452080",
    "end": "454879"
  },
  {
    "text": "be slightly increased by a few",
    "start": "454879",
    "end": "457199"
  },
  {
    "text": "milliseconds um also this approach",
    "start": "457199",
    "end": "461199"
  },
  {
    "text": "assumes that all the sequences from the",
    "start": "461199",
    "end": "464080"
  },
  {
    "text": "previous step will be scheduled for the",
    "start": "464080",
    "end": "465800"
  },
  {
    "text": "next step because the output processing",
    "start": "465800",
    "end": "467319"
  },
  {
    "text": "has been delayed so you might end up",
    "start": "467319",
    "end": "470159"
  },
  {
    "text": "decoding one extra token per request um",
    "start": "470159",
    "end": "474240"
  },
  {
    "text": "but overall the benefits of the",
    "start": "474240",
    "end": "476599"
  },
  {
    "text": "reduction in the time per output token",
    "start": "476599",
    "end": "478919"
  },
  {
    "text": "and late in and improved throughput",
    "start": "478919",
    "end": "481280"
  },
  {
    "text": "makes this a worthwhile tradeoff for",
    "start": "481280",
    "end": "483759"
  },
  {
    "text": "most of our use",
    "start": "483759",
    "end": "486560"
  },
  {
    "text": "cases yeah so um while this is a very",
    "start": "489240",
    "end": "492280"
  },
  {
    "text": "simple idea in theory implementing it",
    "start": "492280",
    "end": "494639"
  },
  {
    "text": "had some",
    "start": "494639",
    "end": "495800"
  },
  {
    "text": "challenges um especially because of the",
    "start": "495800",
    "end": "499080"
  },
  {
    "text": "complexity of vm's",
    "start": "499080",
    "end": "500759"
  },
  {
    "text": "architecture and um so some of the",
    "start": "500759",
    "end": "503800"
  },
  {
    "text": "details I mentioned here are so this",
    "start": "503800",
    "end": "506159"
  },
  {
    "text": "particular asynchronous output",
    "start": "506159",
    "end": "507400"
  },
  {
    "text": "processing is compatible only when you",
    "start": "507400",
    "end": "509520"
  },
  {
    "text": "enable Cuda graphs for VM now um VM has",
    "start": "509520",
    "end": "514719"
  },
  {
    "text": "integrated Cuda graphs very nicely in",
    "start": "514719",
    "end": "516279"
  },
  {
    "text": "the sense that uh you could Cuda graphs",
    "start": "516279",
    "end": "519719"
  },
  {
    "text": "essentially the power of cudar graphs is",
    "start": "519719",
    "end": "521159"
  },
  {
    "text": "that you can reduce all the kernel",
    "start": "521159",
    "end": "523120"
  },
  {
    "text": "launch overheads and you can also fuse",
    "start": "523120",
    "end": "524880"
  },
  {
    "text": "operations together and get very high uh",
    "start": "524880",
    "end": "527040"
  },
  {
    "text": "get very low latency on the just the",
    "start": "527040",
    "end": "529080"
  },
  {
    "text": "model executor part so while that is",
    "start": "529080",
    "end": "532080"
  },
  {
    "text": "happening um the CPU was always running",
    "start": "532080",
    "end": "534720"
  },
  {
    "text": "ahead of GPU so inside a model executor",
    "start": "534720",
    "end": "537440"
  },
  {
    "text": "the CPU has already lined up a bunch of",
    "start": "537440",
    "end": "539480"
  },
  {
    "text": "of operations so the way we implemented",
    "start": "539480",
    "end": "542399"
  },
  {
    "text": "asynchronous output processing was in a",
    "start": "542399",
    "end": "544279"
  },
  {
    "text": "call back as a callback function um so",
    "start": "544279",
    "end": "547880"
  },
  {
    "text": "we basically line up right after the CAG",
    "start": "547880",
    "end": "550200"
  },
  {
    "text": "graph is replaying on the model executor",
    "start": "550200",
    "end": "552519"
  },
  {
    "text": "and we uh call the output processing as",
    "start": "552519",
    "end": "554959"
  },
  {
    "text": "a callback function so and this way we",
    "start": "554959",
    "end": "558040"
  },
  {
    "text": "achieve",
    "start": "558040",
    "end": "560399"
  },
  {
    "text": "concurrency um also we tried",
    "start": "560680",
    "end": "564120"
  },
  {
    "text": "implementing uh a very seamless",
    "start": "564120",
    "end": "566480"
  },
  {
    "text": "switching of scheduler States so the",
    "start": "566480",
    "end": "569959"
  },
  {
    "text": "main idea is like if uh the cellular can",
    "start": "569959",
    "end": "572399"
  },
  {
    "text": "easily switch from an asynchronous to",
    "start": "572399",
    "end": "574880"
  },
  {
    "text": "synchronous state so if you have",
    "start": "574880",
    "end": "576760"
  },
  {
    "text": "especially this is uh important for when",
    "start": "576760",
    "end": "578640"
  },
  {
    "text": "you have requests with beam search",
    "start": "578640",
    "end": "581040"
  },
  {
    "text": "parameters or sampling parameters with",
    "start": "581040",
    "end": "583079"
  },
  {
    "text": "best of n greater than one so in such",
    "start": "583079",
    "end": "585440"
  },
  {
    "text": "cases asynchronous output processing is",
    "start": "585440",
    "end": "587600"
  },
  {
    "text": "not compatible so um right now what",
    "start": "587600",
    "end": "590760"
  },
  {
    "text": "would happen is that scheduler if it",
    "start": "590760",
    "end": "592600"
  },
  {
    "text": "sees any request with beam search it",
    "start": "592600",
    "end": "594680"
  },
  {
    "text": "will automatically switch to a",
    "start": "594680",
    "end": "596200"
  },
  {
    "text": "synchronous state so you can you will",
    "start": "596200",
    "end": "598399"
  },
  {
    "text": "default back to the original behavior of",
    "start": "598399",
    "end": "602680"
  },
  {
    "text": "VM um yeah so some results of",
    "start": "605440",
    "end": "609399"
  },
  {
    "text": "asynchronous output processing is that",
    "start": "609399",
    "end": "610839"
  },
  {
    "text": "we ran uh again 317b on forx h100 with",
    "start": "610839",
    "end": "615279"
  },
  {
    "text": "um fp8 per per tensor",
    "start": "615279",
    "end": "619720"
  },
  {
    "text": "quantization and the speed up that we",
    "start": "619839",
    "end": "622519"
  },
  {
    "text": "see on prefill heavy",
    "start": "622519",
    "end": "624399"
  },
  {
    "text": "workloads is so in this graph what you",
    "start": "624399",
    "end": "627880"
  },
  {
    "text": "see is the blue line is the base line",
    "start": "627880",
    "end": "630279"
  },
  {
    "text": "and the red line is with the output",
    "start": "630279",
    "end": "632560"
  },
  {
    "text": "asynchronous output processing uh x-axis",
    "start": "632560",
    "end": "635440"
  },
  {
    "text": "has throughput Y axis has latency so you",
    "start": "635440",
    "end": "638279"
  },
  {
    "text": "see a lower throughput uh low lower",
    "start": "638279",
    "end": "640839"
  },
  {
    "text": "latency for the same throughput",
    "start": "640839",
    "end": "642160"
  },
  {
    "text": "essentially there is going from um 1.01",
    "start": "642160",
    "end": "646760"
  },
  {
    "text": "to",
    "start": "646760",
    "end": "648279"
  },
  {
    "text": "1.09% uh 9x speed up for prefill heavy",
    "start": "648279",
    "end": "651480"
  },
  {
    "text": "workloads so when you're uh this is",
    "start": "651480",
    "end": "653839"
  },
  {
    "text": "basically for 3500 input and 300 output",
    "start": "653839",
    "end": "656800"
  },
  {
    "text": "tokens so you see at most for so",
    "start": "656800",
    "end": "659600"
  },
  {
    "text": "basically the advantage here lies at",
    "start": "659600",
    "end": "661320"
  },
  {
    "text": "high bat sizes because the CPU overheads",
    "start": "661320",
    "end": "664240"
  },
  {
    "text": "grows linearly with the bat size so you",
    "start": "664240",
    "end": "667079"
  },
  {
    "text": "see a much higher speed up which is the",
    "start": "667079",
    "end": "669000"
  },
  {
    "text": "green line at higher bad higher request",
    "start": "669000",
    "end": "671320"
  },
  {
    "text": "per",
    "start": "671320",
    "end": "672600"
  },
  {
    "text": "seconds similarly for decode heavy",
    "start": "672600",
    "end": "674920"
  },
  {
    "text": "workloads so this is where I send",
    "start": "674920",
    "end": "676800"
  },
  {
    "text": "request of like 100 input and 300 output",
    "start": "676800",
    "end": "679600"
  },
  {
    "text": "tokens uh the speed up is larger so you",
    "start": "679600",
    "end": "682399"
  },
  {
    "text": "get almost one uh 14% speed up with the",
    "start": "682399",
    "end": "686360"
  },
  {
    "text": "asynchronous output processing at high",
    "start": "686360",
    "end": "688680"
  },
  {
    "text": "request",
    "start": "688680",
    "end": "691240"
  },
  {
    "text": "second um another optimization that I",
    "start": "692680",
    "end": "695680"
  },
  {
    "text": "would like to talk about this was led by",
    "start": "695680",
    "end": "697480"
  },
  {
    "text": "any scale and neural magic um multi-step",
    "start": "697480",
    "end": "700839"
  },
  {
    "text": "scheduling is something that brought",
    "start": "700839",
    "end": "702519"
  },
  {
    "text": "about a huge Improvement in the latency",
    "start": "702519",
    "end": "704959"
  },
  {
    "text": "and throughput so the core idea behind",
    "start": "704959",
    "end": "707360"
  },
  {
    "text": "multi-step scheduling is that you",
    "start": "707360",
    "end": "709920"
  },
  {
    "text": "schedule multiple decode passes in one",
    "start": "709920",
    "end": "712360"
  },
  {
    "text": "large step and what this essentially",
    "start": "712360",
    "end": "715519"
  },
  {
    "text": "does is that it EMES the cost of",
    "start": "715519",
    "end": "717639"
  },
  {
    "text": "scheduling and post-processing",
    "start": "717639",
    "end": "719880"
  },
  {
    "text": "uh over end steps so remember the graph",
    "start": "719880",
    "end": "722560"
  },
  {
    "text": "that we saw before which had scheduled",
    "start": "722560",
    "end": "724079"
  },
  {
    "text": "forward and out output processing here",
    "start": "724079",
    "end": "726800"
  },
  {
    "text": "so what with multi-step what you'll do",
    "start": "726800",
    "end": "728560"
  },
  {
    "text": "is you schedule end steps and then you",
    "start": "728560",
    "end": "730480"
  },
  {
    "text": "do forward pass for end steps and then",
    "start": "730480",
    "end": "732519"
  },
  {
    "text": "you do output processing of end steps",
    "start": "732519",
    "end": "735079"
  },
  {
    "text": "right um but the disadvantage is that if",
    "start": "735079",
    "end": "740160"
  },
  {
    "text": "you just have multi-step scheduling the",
    "start": "740160",
    "end": "742639"
  },
  {
    "text": "output tokens are streamed back to user",
    "start": "742639",
    "end": "745000"
  },
  {
    "text": "only one out of end steps so let's",
    "start": "745000",
    "end": "747120"
  },
  {
    "text": "assume you run your multi-step schedule",
    "start": "747120",
    "end": "749360"
  },
  {
    "text": "for eight steps you're only streaming",
    "start": "749360",
    "end": "751480"
  },
  {
    "text": "back without asynchronous output",
    "start": "751480",
    "end": "752920"
  },
  {
    "text": "processing you will be streaming back",
    "start": "752920",
    "end": "755160"
  },
  {
    "text": "the output tokens one out of every eight",
    "start": "755160",
    "end": "757320"
  },
  {
    "text": "steps",
    "start": "757320",
    "end": "759040"
  },
  {
    "text": "but now uh the Asing outut processing",
    "start": "759040",
    "end": "761880"
  },
  {
    "text": "Works quite nicely with multi-step",
    "start": "761880",
    "end": "763839"
  },
  {
    "text": "scheduling and you end up even seeing",
    "start": "763839",
    "end": "766120"
  },
  {
    "text": "further boost to the performance so",
    "start": "766120",
    "end": "768000"
  },
  {
    "text": "essentially as soon as the output tokens",
    "start": "768000",
    "end": "769519"
  },
  {
    "text": "are ready asynchronous output processor",
    "start": "769519",
    "end": "771440"
  },
  {
    "text": "will stream it back to the",
    "start": "771440",
    "end": "775279"
  },
  {
    "text": "user yeah so here is some here are some",
    "start": "777000",
    "end": "780560"
  },
  {
    "text": "results when you have all uh these two",
    "start": "780560",
    "end": "783199"
  },
  {
    "text": "features together so in this graph I",
    "start": "783199",
    "end": "785920"
  },
  {
    "text": "again look at we look at 3.1 um 70b",
    "start": "785920",
    "end": "789320"
  },
  {
    "text": "model fp8 per tensor quantization on",
    "start": "789320",
    "end": "792000"
  },
  {
    "text": "forx",
    "start": "792000",
    "end": "793160"
  },
  {
    "text": "h100 and the blue line is the Baseline",
    "start": "793160",
    "end": "796519"
  },
  {
    "text": "where you have neither multi-step nor",
    "start": "796519",
    "end": "798800"
  },
  {
    "text": "asynchronous output",
    "start": "798800",
    "end": "800519"
  },
  {
    "text": "processing then red line is when you",
    "start": "800519",
    "end": "802600"
  },
  {
    "text": "just enable the asynchronous output",
    "start": "802600",
    "end": "804440"
  },
  {
    "text": "processor and you get 9% speed up over",
    "start": "804440",
    "end": "807160"
  },
  {
    "text": "the Baseline at especially as you can",
    "start": "807160",
    "end": "809760"
  },
  {
    "text": "see the advantage of asynchronous output",
    "start": "809760",
    "end": "811800"
  },
  {
    "text": "processing is only at high RPS right",
    "start": "811800",
    "end": "815160"
  },
  {
    "text": "because the CPU overhead in that case",
    "start": "815160",
    "end": "817079"
  },
  {
    "text": "only grows linearly with the bat size",
    "start": "817079",
    "end": "820160"
  },
  {
    "text": "for uh the yellow line is when you",
    "start": "820160",
    "end": "822360"
  },
  {
    "text": "enable multi-steps and this one is for",
    "start": "822360",
    "end": "824320"
  },
  {
    "text": "eight steps so for multi-step you get an",
    "start": "824320",
    "end": "828040"
  },
  {
    "text": "overall Improvement even at low RPS",
    "start": "828040",
    "end": "830120"
  },
  {
    "text": "because it is actually even helping at",
    "start": "830120",
    "end": "832920"
  },
  {
    "text": "um low bat sizes and trying to amortize",
    "start": "832920",
    "end": "835120"
  },
  {
    "text": "the cost of",
    "start": "835120",
    "end": "837000"
  },
  {
    "text": "scheduler so that's why you see this um",
    "start": "837000",
    "end": "840600"
  },
  {
    "text": "lower latency even with just multi-step",
    "start": "840600",
    "end": "843279"
  },
  {
    "text": "but if you again add on to that",
    "start": "843279",
    "end": "845320"
  },
  {
    "text": "asynchronous output processor you will",
    "start": "845320",
    "end": "847440"
  },
  {
    "text": "get again 8% speed up over multi-step so",
    "start": "847440",
    "end": "851160"
  },
  {
    "text": "overall uh from the going from the blue",
    "start": "851160",
    "end": "853399"
  },
  {
    "text": "line to the green line you get a 1 uh",
    "start": "853399",
    "end": "856279"
  },
  {
    "text": "1.2 3x speed up over the Baseline which",
    "start": "856279",
    "end": "859519"
  },
  {
    "text": "is pretty great",
    "start": "859519",
    "end": "862000"
  },
  {
    "text": "um yeah so this was an effort uh led by",
    "start": "862000",
    "end": "865639"
  },
  {
    "text": "any scale and uh neural Magic",
    "start": "865639",
    "end": "869279"
  },
  {
    "text": "and um yeah so now I guess the next",
    "start": "869279",
    "end": "873399"
  },
  {
    "text": "question that comes is are there more",
    "start": "873399",
    "end": "877079"
  },
  {
    "text": "CPU overheads that we can still tackle",
    "start": "877079",
    "end": "879720"
  },
  {
    "text": "so um I we ran some profiler again after",
    "start": "879720",
    "end": "884079"
  },
  {
    "text": "applying multi-step scheduling and uh",
    "start": "884079",
    "end": "886560"
  },
  {
    "text": "output processor as synus output",
    "start": "886560",
    "end": "888839"
  },
  {
    "text": "processor and we saw that um because",
    "start": "888839",
    "end": "892240"
  },
  {
    "text": "you're still if you go back to this",
    "start": "892240",
    "end": "893959"
  },
  {
    "text": "graph right where you're still",
    "start": "893959",
    "end": "895000"
  },
  {
    "text": "scheduling end steps here now even if",
    "start": "895000",
    "end": "897519"
  },
  {
    "text": "you have asynchronous output process",
    "start": "897519",
    "end": "899240"
  },
  {
    "text": "processor this scheduler part is still",
    "start": "899240",
    "end": "901720"
  },
  {
    "text": "blocking your critical path all right it",
    "start": "901720",
    "end": "904519"
  },
  {
    "text": "is this is still uh the remaining thing",
    "start": "904519",
    "end": "907279"
  },
  {
    "text": "that we can tackle and make it",
    "start": "907279",
    "end": "908399"
  },
  {
    "text": "completely asynchronous and that's about",
    "start": "908399",
    "end": "910399"
  },
  {
    "text": "like 5 to 10% Improvement that you can",
    "start": "910399",
    "end": "912680"
  },
  {
    "text": "still get from the current state of VM",
    "start": "912680",
    "end": "914880"
  },
  {
    "text": "and that's where I would imagine the",
    "start": "914880",
    "end": "917040"
  },
  {
    "text": "future of U VM would be where even the",
    "start": "917040",
    "end": "920079"
  },
  {
    "text": "scheduling becomes fully asynchronous",
    "start": "920079",
    "end": "922440"
  },
  {
    "text": "and you basically when you're running",
    "start": "922440",
    "end": "924399"
  },
  {
    "text": "your model executor for step T you start",
    "start": "924399",
    "end": "926680"
  },
  {
    "text": "prefetching the input tensors for the",
    "start": "926680",
    "end": "928639"
  },
  {
    "text": "next step and at the same time do lagged",
    "start": "928639",
    "end": "931079"
  },
  {
    "text": "output processing from the previous step",
    "start": "931079",
    "end": "933079"
  },
  {
    "text": "so basically when you have an Hardware",
    "start": "933079",
    "end": "935279"
  },
  {
    "text": "like h100 you want to keep it busy all",
    "start": "935279",
    "end": "937279"
  },
  {
    "text": "the",
    "start": "937279",
    "end": "939480"
  },
  {
    "text": "time so um talking about some of our",
    "start": "941079",
    "end": "946040"
  },
  {
    "text": "other",
    "start": "946040",
    "end": "948240"
  },
  {
    "text": "optimizations um so we have also apart",
    "start": "948240",
    "end": "950639"
  },
  {
    "text": "from asynchronous output processor",
    "start": "950639",
    "end": "952160"
  },
  {
    "text": "processing we also uh performance team",
    "start": "952160",
    "end": "954639"
  },
  {
    "text": "at data bricks also looks at other",
    "start": "954639",
    "end": "956639"
  },
  {
    "text": "optimizations such as fp8 and",
    "start": "956639",
    "end": "959000"
  },
  {
    "text": "speculative",
    "start": "959000",
    "end": "960399"
  },
  {
    "text": "decoding so briefly um as you all might",
    "start": "960399",
    "end": "964160"
  },
  {
    "text": "be aware like first project which is FBA",
    "start": "964160",
    "end": "966519"
  },
  {
    "text": "performance quantization the reason why",
    "start": "966519",
    "end": "968880"
  },
  {
    "text": "we use quantization in general is",
    "start": "968880",
    "end": "970920"
  },
  {
    "text": "because it makes your you can serve a",
    "start": "970920",
    "end": "973600"
  },
  {
    "text": "model at lower Precision on half the",
    "start": "973600",
    "end": "976120"
  },
  {
    "text": "hardware let's say if you have like a",
    "start": "976120",
    "end": "977519"
  },
  {
    "text": "16-bit model and you decide to serve",
    "start": "977519",
    "end": "979880"
  },
  {
    "text": "this on uh with 8bit you can use half",
    "start": "979880",
    "end": "983040"
  },
  {
    "text": "the hardware and also get like twice the",
    "start": "983040",
    "end": "985440"
  },
  {
    "text": "speed up um and the another greatest",
    "start": "985440",
    "end": "989040"
  },
  {
    "text": "benefit that of quantization that you",
    "start": "989040",
    "end": "990519"
  },
  {
    "text": "get is that you can unlock much higher",
    "start": "990519",
    "end": "992480"
  },
  {
    "text": "concurrency right so you can basically",
    "start": "992480",
    "end": "995240"
  },
  {
    "text": "fit twice the bat size and Achieve much",
    "start": "995240",
    "end": "998399"
  },
  {
    "text": "higher",
    "start": "998399",
    "end": "999360"
  },
  {
    "text": "throughput so um one of our projects is",
    "start": "999360",
    "end": "1002399"
  },
  {
    "text": "obviously improving our FPA performance",
    "start": "1002399",
    "end": "1005079"
  },
  {
    "text": "and we run uh and this is a trade-off",
    "start": "1005079",
    "end": "1007560"
  },
  {
    "text": "between both accuracy and performance so",
    "start": "1007560",
    "end": "1009519"
  },
  {
    "text": "we continuously work on improving uh",
    "start": "1009519",
    "end": "1012240"
  },
  {
    "text": "both of them and we have seen no",
    "start": "1012240",
    "end": "1014360"
  },
  {
    "text": "significant accuracy degradation between",
    "start": "1014360",
    "end": "1016279"
  },
  {
    "text": "like bf16 and fp8 um",
    "start": "1016279",
    "end": "1019360"
  },
  {
    "text": "here is where we run our own ml Gauntlet",
    "start": "1019360",
    "end": "1021959"
  },
  {
    "text": "which is a big bunch of evals which",
    "start": "1021959",
    "end": "1025120"
  },
  {
    "text": "looks at the quality of generation",
    "start": "1025120",
    "end": "1026918"
  },
  {
    "text": "prefill language understanding and um we",
    "start": "1026919",
    "end": "1032038"
  },
  {
    "text": "we basically evaluate across all the",
    "start": "1032039",
    "end": "1033438"
  },
  {
    "text": "evals and make sure that there is no",
    "start": "1033439",
    "end": "1035199"
  },
  {
    "text": "accuracy degradation furthermore uh we",
    "start": "1035199",
    "end": "1038160"
  },
  {
    "text": "also improv the performance using uh our",
    "start": "1038160",
    "end": "1040600"
  },
  {
    "text": "own custom kernels in our runtime where",
    "start": "1040600",
    "end": "1043120"
  },
  {
    "text": "we have fused uh some of quantization um",
    "start": "1043120",
    "end": "1046720"
  },
  {
    "text": "operations and that has also led to good",
    "start": "1046720",
    "end": "1051080"
  },
  {
    "text": "speedups we are also exploring",
    "start": "1051080",
    "end": "1054080"
  },
  {
    "text": "speculative decoding um and we have seen",
    "start": "1054080",
    "end": "1057520"
  },
  {
    "text": "with draft models you can get 2 to 2.5x",
    "start": "1057520",
    "end": "1060520"
  },
  {
    "text": "speed up with llama 3.1 70b and 3.14 5B",
    "start": "1060520",
    "end": "1066400"
  },
  {
    "text": "at uh low bath sizes so speculative",
    "start": "1066400",
    "end": "1068880"
  },
  {
    "text": "decoding is mostly helpful for at memory",
    "start": "1068880",
    "end": "1072360"
  },
  {
    "text": "bound regimes where you can generate",
    "start": "1072360",
    "end": "1074080"
  },
  {
    "text": "more tokens at a time so in in a memory",
    "start": "1074080",
    "end": "1077640"
  },
  {
    "text": "bound regime we have seen pretty great",
    "start": "1077640",
    "end": "1079320"
  },
  {
    "text": "performance with using uh draft models",
    "start": "1079320",
    "end": "1081400"
  },
  {
    "text": "out of the box with",
    "start": "1081400",
    "end": "1083640"
  },
  {
    "text": "VM also we have been recently exploring",
    "start": "1083640",
    "end": "1086559"
  },
  {
    "text": "Medusa heads uh fine tuned on Lama 3.1",
    "start": "1086559",
    "end": "1089799"
  },
  {
    "text": "70b um the quality aspect is still a",
    "start": "1089799",
    "end": "1093360"
  },
  {
    "text": "question which is why we are continuing",
    "start": "1093360",
    "end": "1094880"
  },
  {
    "text": "to improve um the training and the",
    "start": "1094880",
    "end": "1098320"
  },
  {
    "text": "fine-tuning part of Medusa heads uh but",
    "start": "1098320",
    "end": "1101039"
  },
  {
    "text": "we again expect good speed up with",
    "start": "1101039",
    "end": "1103760"
  },
  {
    "text": "Medusa",
    "start": "1103760",
    "end": "1106760"
  },
  {
    "text": "uh so yeah in conclusion um we spoke",
    "start": "1109080",
    "end": "1112960"
  },
  {
    "text": "about asynchronous outut processing",
    "start": "1112960",
    "end": "1115080"
  },
  {
    "text": "which helps in reducing overhead at high",
    "start": "1115080",
    "end": "1118559"
  },
  {
    "text": "RPS",
    "start": "1118559",
    "end": "1120159"
  },
  {
    "text": "um and also multi-step and asyncronous",
    "start": "1120159",
    "end": "1122840"
  },
  {
    "text": "out processing together gives like 1.25x",
    "start": "1122840",
    "end": "1125919"
  },
  {
    "text": "to 1.35x speed up across all workloads",
    "start": "1125919",
    "end": "1130799"
  },
  {
    "text": "um fp8 quantization helps unlock higher",
    "start": "1130799",
    "end": "1133480"
  },
  {
    "text": "concurrency without accuracy loss and uh",
    "start": "1133480",
    "end": "1136600"
  },
  {
    "text": "the speculative decoding is has very",
    "start": "1136600",
    "end": "1139240"
  },
  {
    "text": "promising results at especially at low",
    "start": "1139240",
    "end": "1140919"
  },
  {
    "text": "batch sizes and uh mostly I think the if",
    "start": "1140919",
    "end": "1145000"
  },
  {
    "text": "you look at the new Q4 road map for VM a",
    "start": "1145000",
    "end": "1148679"
  },
  {
    "text": "lot of exciting work is about to come",
    "start": "1148679",
    "end": "1150440"
  },
  {
    "text": "where uh you know you can have",
    "start": "1150440",
    "end": "1152000"
  },
  {
    "text": "asynchronous out scheduling and also",
    "start": "1152000",
    "end": "1154760"
  },
  {
    "text": "much cleaner core which I think is very",
    "start": "1154760",
    "end": "1156880"
  },
  {
    "text": "important for faster Dev Loops um so",
    "start": "1156880",
    "end": "1160520"
  },
  {
    "text": "yeah excited about all of",
    "start": "1160520",
    "end": "1162080"
  },
  {
    "text": "that um overall thank you to the um",
    "start": "1162080",
    "end": "1166240"
  },
  {
    "text": "entire um vnm community members and",
    "start": "1166240",
    "end": "1168720"
  },
  {
    "text": "special thanks to Alex from neural magic",
    "start": "1168720",
    "end": "1170960"
  },
  {
    "text": "and vuk from UC Bley for collaborating",
    "start": "1170960",
    "end": "1173679"
  },
  {
    "text": "on asynchronous output",
    "start": "1173679",
    "end": "1176840"
  },
  {
    "text": "processing let's see if we have yeah any",
    "start": "1181720",
    "end": "1184799"
  },
  {
    "text": "more if there are any questions I would",
    "start": "1184799",
    "end": "1186240"
  },
  {
    "text": "love to answer hey yeah great talk I I",
    "start": "1186240",
    "end": "1188760"
  },
  {
    "text": "was curious for multistep scheduling",
    "start": "1188760",
    "end": "1192000"
  },
  {
    "text": "like how do you decide how many steps",
    "start": "1192000",
    "end": "1194120"
  },
  {
    "text": "you used eight but why not four why not",
    "start": "1194120",
    "end": "1196039"
  },
  {
    "text": "16 like how do you get to the number of",
    "start": "1196039",
    "end": "1199159"
  },
  {
    "text": "steps to use there yeah yeah that's a",
    "start": "1199159",
    "end": "1201840"
  },
  {
    "text": "good question so it's actually a",
    "start": "1201840",
    "end": "1204000"
  },
  {
    "text": "tradeoff the I if you increase the",
    "start": "1204000",
    "end": "1207120"
  },
  {
    "text": "decoding steps you would get definitely",
    "start": "1207120",
    "end": "1209640"
  },
  {
    "text": "lower latency because you're doing",
    "start": "1209640",
    "end": "1211679"
  },
  {
    "text": "lesser CPU to GPU things but at the same",
    "start": "1211679",
    "end": "1214360"
  },
  {
    "text": "time this means that during those steps",
    "start": "1214360",
    "end": "1216720"
  },
  {
    "text": "no new request will be scheduled because",
    "start": "1216720",
    "end": "1219320"
  },
  {
    "text": "your the scheduler has decided to run",
    "start": "1219320",
    "end": "1222120"
  },
  {
    "text": "all these 16 steps in advance let's say",
    "start": "1222120",
    "end": "1224080"
  },
  {
    "text": "you make it 16 and if there are any",
    "start": "1224080",
    "end": "1226039"
  },
  {
    "text": "other new requests that are coming",
    "start": "1226039",
    "end": "1227679"
  },
  {
    "text": "during that time um those will be",
    "start": "1227679",
    "end": "1230919"
  },
  {
    "text": "delayed so it is like a trade-off there",
    "start": "1230919",
    "end": "1233159"
  },
  {
    "text": "are other uh questions related to memory",
    "start": "1233159",
    "end": "1235159"
  },
  {
    "text": "allocation if you run out of it then you",
    "start": "1235159",
    "end": "1237280"
  },
  {
    "text": "would want to come back and again uh",
    "start": "1237280",
    "end": "1239039"
  },
  {
    "text": "schedule but um a lot of that is handled",
    "start": "1239039",
    "end": "1241880"
  },
  {
    "text": "by multi-step scheduler but yeah it's",
    "start": "1241880",
    "end": "1243520"
  },
  {
    "text": "it's a trade-off essentially eight is",
    "start": "1243520",
    "end": "1245880"
  },
  {
    "text": "the recommended",
    "start": "1245880",
    "end": "1248480"
  },
  {
    "text": "number um you you mentioned that you use",
    "start": "1248840",
    "end": "1252360"
  },
  {
    "text": "like the profiler to find out the like",
    "start": "1252360",
    "end": "1255039"
  },
  {
    "text": "interaction between CPU and GPU and",
    "start": "1255039",
    "end": "1257360"
  },
  {
    "text": "where you spend most of the time time",
    "start": "1257360",
    "end": "1259200"
  },
  {
    "text": "could you tell us more details like",
    "start": "1259200",
    "end": "1261480"
  },
  {
    "text": "first what profiler you find to be",
    "start": "1261480",
    "end": "1263960"
  },
  {
    "text": "particularly useful for that and second",
    "start": "1263960",
    "end": "1267559"
  },
  {
    "text": "um how do you like figure out like from",
    "start": "1267559",
    "end": "1270840"
  },
  {
    "text": "that profiler result what's the most uh",
    "start": "1270840",
    "end": "1274440"
  },
  {
    "text": "profitable part to be uh",
    "start": "1274440",
    "end": "1278039"
  },
  {
    "text": "optimized uh sorry what was the second",
    "start": "1278039",
    "end": "1280039"
  },
  {
    "text": "part of your question what so so once",
    "start": "1280039",
    "end": "1282679"
  },
  {
    "text": "you have a profiler result like how do",
    "start": "1282679",
    "end": "1285279"
  },
  {
    "text": "you get the from that profiler result",
    "start": "1285279",
    "end": "1288120"
  },
  {
    "text": "you find",
    "start": "1288120",
    "end": "1289240"
  },
  {
    "text": "which part of the system is most uh most",
    "start": "1289240",
    "end": "1292880"
  },
  {
    "text": "profitable to optimize on got it yeah",
    "start": "1292880",
    "end": "1296880"
  },
  {
    "text": "yeah so uh we have we have used uh torch",
    "start": "1296880",
    "end": "1300600"
  },
  {
    "text": "profiler um so something very easy we",
    "start": "1300600",
    "end": "1303320"
  },
  {
    "text": "can just dump out uh the traces from a",
    "start": "1303320",
    "end": "1306000"
  },
  {
    "text": "simple torch profiler but the we've also",
    "start": "1306000",
    "end": "1308679"
  },
  {
    "text": "had we dump the results like in a Json I",
    "start": "1308679",
    "end": "1310919"
  },
  {
    "text": "have like some wrapper functions on top",
    "start": "1310919",
    "end": "1312360"
  },
  {
    "text": "of it to get like an aggregate uh",
    "start": "1312360",
    "end": "1314919"
  },
  {
    "text": "information about where the next",
    "start": "1314919",
    "end": "1316440"
  },
  {
    "text": "bottlenecks are but essentially",
    "start": "1316440",
    "end": "1319240"
  },
  {
    "text": "it just comes down to looking at the",
    "start": "1319240",
    "end": "1320799"
  },
  {
    "text": "traces one by one and finding at the",
    "start": "1320799",
    "end": "1323080"
  },
  {
    "text": "Cuda streams and also the CPU where",
    "start": "1323080",
    "end": "1325520"
  },
  {
    "text": "exactly do you see those white spaces um",
    "start": "1325520",
    "end": "1328799"
  },
  {
    "text": "so that's basically how we Tred to look",
    "start": "1328799",
    "end": "1331679"
  },
  {
    "text": "at all the traces so",
    "start": "1331679",
    "end": "1334760"
  },
  {
    "text": "yeah yeah so you can you can dump out",
    "start": "1343720",
    "end": "1346480"
  },
  {
    "text": "the um the traces in general General as",
    "start": "1346480",
    "end": "1348919"
  },
  {
    "text": "Json and then you from that like raw",
    "start": "1348919",
    "end": "1352320"
  },
  {
    "text": "data we have some aggregate functions",
    "start": "1352320",
    "end": "1354200"
  },
  {
    "text": "that we can use overall but yeah you're",
    "start": "1354200",
    "end": "1355679"
  },
  {
    "text": "right that torch profiler does not",
    "start": "1355679",
    "end": "1357799"
  },
  {
    "text": "natively support aggregating the",
    "start": "1357799",
    "end": "1361919"
  },
  {
    "text": "metrics so yeah it it be basically like",
    "start": "1361919",
    "end": "1364480"
  },
  {
    "text": "you export the in raw data and then do",
    "start": "1364480",
    "end": "1367360"
  },
  {
    "text": "like a data analysis on of like in a",
    "start": "1367360",
    "end": "1370799"
  },
  {
    "text": "different notebook or something",
    "start": "1370799",
    "end": "1374640"
  },
  {
    "text": "yeah hi do you have any empirical data",
    "start": "1374640",
    "end": "1377400"
  },
  {
    "text": "you can share on on speculative decoding",
    "start": "1377400",
    "end": "1380120"
  },
  {
    "text": "metrics um things like the number of",
    "start": "1380120",
    "end": "1383400"
  },
  {
    "text": "tokens um that you look ahead for or",
    "start": "1383400",
    "end": "1386039"
  },
  {
    "text": "like the acceptance like method mhm uh",
    "start": "1386039",
    "end": "1390960"
  },
  {
    "text": "to the extent that I can share",
    "start": "1390960",
    "end": "1392520"
  },
  {
    "text": "information uh we have",
    "start": "1392520",
    "end": "1394320"
  },
  {
    "text": "seen we've tried like uh with draft",
    "start": "1394320",
    "end": "1397279"
  },
  {
    "text": "models we've tried from four to eight",
    "start": "1397279",
    "end": "1399799"
  },
  {
    "text": "speculating like four to eight tokens",
    "start": "1399799",
    "end": "1402080"
  },
  {
    "text": "ahead uh and",
    "start": "1402080",
    "end": "1404080"
  },
  {
    "text": "then we see pretty good acceptance rate",
    "start": "1404080",
    "end": "1407159"
  },
  {
    "text": "more than like 70% for across all of",
    "start": "1407159",
    "end": "1409880"
  },
  {
    "text": "them",
    "start": "1409880",
    "end": "1412200"
  },
  {
    "text": "yeah uh we still have some time",
    "start": "1413159",
    "end": "1417679"
  },
  {
    "text": "okay uh thanks for the talk uh speaking",
    "start": "1417679",
    "end": "1420640"
  },
  {
    "text": "of speculative decoding and uh we found",
    "start": "1420640",
    "end": "1423679"
  },
  {
    "text": "that uh and as you mentioned the the the",
    "start": "1423679",
    "end": "1426520"
  },
  {
    "text": "benefits of speculative decoding is is",
    "start": "1426520",
    "end": "1429679"
  },
  {
    "text": "high when the bch size is is small right",
    "start": "1429679",
    "end": "1432320"
  },
  {
    "text": "and the the the benefits could be",
    "start": "1432320",
    "end": "1434720"
  },
  {
    "text": "limited or even negative when when the B",
    "start": "1434720",
    "end": "1437240"
  },
  {
    "text": "size is is large so so do you have a",
    "start": "1437240",
    "end": "1440360"
  },
  {
    "text": "plan of like um dynamically changing the",
    "start": "1440360",
    "end": "1444799"
  },
  {
    "text": "the speculative decoding uh strategy uh",
    "start": "1444799",
    "end": "1448039"
  },
  {
    "text": "based on real time workloads or",
    "start": "1448039",
    "end": "1450120"
  },
  {
    "text": "something like that yeah that's a good",
    "start": "1450120",
    "end": "1452840"
  },
  {
    "text": "question um yeah you're right that there",
    "start": "1452840",
    "end": "1455720"
  },
  {
    "text": "it's a graph which sort of like",
    "start": "1455720",
    "end": "1457120"
  },
  {
    "text": "decreases and the maximum benefit you",
    "start": "1457120",
    "end": "1458880"
  },
  {
    "text": "get is at the bat size one and then it",
    "start": "1458880",
    "end": "1460600"
  },
  {
    "text": "just linearly decreases and even goes",
    "start": "1460600",
    "end": "1462720"
  },
  {
    "text": "negative Beyond I would say like 16 or",
    "start": "1462720",
    "end": "1464520"
  },
  {
    "text": "32 bat size um mostly right now we we",
    "start": "1464520",
    "end": "1468440"
  },
  {
    "text": "are in a very early stage so uh it's",
    "start": "1468440",
    "end": "1471640"
  },
  {
    "text": "emperically we have observed like a",
    "start": "1471640",
    "end": "1473360"
  },
  {
    "text": "particular bad size and then we would",
    "start": "1473360",
    "end": "1475200"
  },
  {
    "text": "just toggle it on our inference service",
    "start": "1475200",
    "end": "1478159"
  },
  {
    "text": "but uh yeah we're still thinking around",
    "start": "1478159",
    "end": "1480960"
  },
  {
    "text": "like more smarter techn uh smarter meth",
    "start": "1480960",
    "end": "1484200"
  },
  {
    "text": "methods to dynamically change based on",
    "start": "1484200",
    "end": "1486520"
  },
  {
    "text": "actual traffic patterns but right now",
    "start": "1486520",
    "end": "1488640"
  },
  {
    "text": "it's mostly very something very basic",
    "start": "1488640",
    "end": "1490120"
  },
  {
    "text": "that where you see like a certain after",
    "start": "1490120",
    "end": "1491640"
  },
  {
    "text": "certain bad size you just turn it off",
    "start": "1491640",
    "end": "1494720"
  },
  {
    "text": "yeah",
    "start": "1494720",
    "end": "1497720"
  },
  {
    "text": "hi so um I saw some updates on the",
    "start": "1500640",
    "end": "1503240"
  },
  {
    "text": "schedule right and uh you had the",
    "start": "1503240",
    "end": "1505240"
  },
  {
    "text": "speculative decoding slide in the in the",
    "start": "1505240",
    "end": "1507480"
  },
  {
    "text": "end after the scheduler update so is the",
    "start": "1507480",
    "end": "1510760"
  },
  {
    "text": "scheduler is going to look at uh",
    "start": "1510760",
    "end": "1512200"
  },
  {
    "text": "speculative decoding because there's a",
    "start": "1512200",
    "end": "1513640"
  },
  {
    "text": "dependency between speculation and uh",
    "start": "1513640",
    "end": "1516279"
  },
  {
    "text": "confirmation right so is the schedul",
    "start": "1516279",
    "end": "1518279"
  },
  {
    "text": "going to do something there to yeah",
    "start": "1518279",
    "end": "1520840"
  },
  {
    "text": "that's a good catch uh right now I think",
    "start": "1520840",
    "end": "1523279"
  },
  {
    "text": "uh the plans uh and I maybe I might not",
    "start": "1523279",
    "end": "1527200"
  },
  {
    "text": "be updated uh what the plans are related",
    "start": "1527200",
    "end": "1529520"
  },
  {
    "text": "to speculative decoding but the",
    "start": "1529520",
    "end": "1530679"
  },
  {
    "text": "asynchronous scheduling if I understand",
    "start": "1530679",
    "end": "1533360"
  },
  {
    "text": "it's only for non-speculative decoding",
    "start": "1533360",
    "end": "1536919"
  },
  {
    "text": "cases um I think some more work needs to",
    "start": "1536919",
    "end": "1539919"
  },
  {
    "text": "be done on the speculative side on how",
    "start": "1539919",
    "end": "1542080"
  },
  {
    "text": "to do this completely asynchronous which",
    "start": "1542080",
    "end": "1544520"
  },
  {
    "text": "is not currently easy in VM architecture",
    "start": "1544520",
    "end": "1548080"
  },
  {
    "text": "yeah okay we may have like one or two",
    "start": "1548080",
    "end": "1551000"
  },
  {
    "text": "more",
    "start": "1551000",
    "end": "1551960"
  },
  {
    "text": "questions hi uh I was just wondering",
    "start": "1551960",
    "end": "1555640"
  },
  {
    "text": "like in your architecture was there a",
    "start": "1555640",
    "end": "1557760"
  },
  {
    "text": "Men uh component like a cash layer",
    "start": "1557760",
    "end": "1560159"
  },
  {
    "text": "component like that can also the role of",
    "start": "1560159",
    "end": "1562320"
  },
  {
    "text": "any",
    "start": "1562320",
    "end": "1563159"
  },
  {
    "text": "cash subsystems or something that can",
    "start": "1563159",
    "end": "1565799"
  },
  {
    "text": "improve the",
    "start": "1565799",
    "end": "1567240"
  },
  {
    "text": "latency is there a room for that uh",
    "start": "1567240",
    "end": "1570840"
  },
  {
    "text": "caching in schedular or um in the entire",
    "start": "1570840",
    "end": "1573840"
  },
  {
    "text": "flow that you are mentioning uh to",
    "start": "1573840",
    "end": "1576399"
  },
  {
    "text": "improve the latency is like when you're",
    "start": "1576399",
    "end": "1579200"
  },
  {
    "text": "talking about memory optimization is",
    "start": "1579200",
    "end": "1581799"
  },
  {
    "text": "there",
    "start": "1581799",
    "end": "1582600"
  },
  {
    "text": "any caching subsystems or something that",
    "start": "1582600",
    "end": "1585600"
  },
  {
    "text": "play a role this whole",
    "start": "1585600",
    "end": "1589159"
  },
  {
    "text": "uh end to endend workflow yeah uh I know",
    "start": "1589159",
    "end": "1593960"
  },
  {
    "text": "that we have some python objects to cach",
    "start": "1593960",
    "end": "1597039"
  },
  {
    "text": "like schedu States and also there is",
    "start": "1597039",
    "end": "1598799"
  },
  {
    "text": "something like prefix caching that's",
    "start": "1598799",
    "end": "1600840"
  },
  {
    "text": "generally done uh to Cache any if you",
    "start": "1600840",
    "end": "1603960"
  },
  {
    "text": "have like a a new request coming in",
    "start": "1603960",
    "end": "1606480"
  },
  {
    "text": "which shares prefix with the previous",
    "start": "1606480",
    "end": "1608240"
  },
  {
    "text": "one you can instantly avoid prefilling",
    "start": "1608240",
    "end": "1611360"
  },
  {
    "text": "that part of the prefix and just take um",
    "start": "1611360",
    "end": "1614320"
  },
  {
    "text": "load it back um so if I don't know if",
    "start": "1614320",
    "end": "1616880"
  },
  {
    "text": "that answers but that's like one of the",
    "start": "1616880",
    "end": "1618559"
  },
  {
    "text": "caching mechanism that does exist right",
    "start": "1618559",
    "end": "1620480"
  },
  {
    "text": "now in",
    "start": "1620480",
    "end": "1623039"
  },
  {
    "text": "VM oh",
    "start": "1623679",
    "end": "1626600"
  },
  {
    "text": "um so that is actually this is base",
    "start": "1626600",
    "end": "1630399"
  },
  {
    "text": "optimized the numbers that I showed in",
    "start": "1630399",
    "end": "1631640"
  },
  {
    "text": "the slide here right now are without any",
    "start": "1631640",
    "end": "1633399"
  },
  {
    "text": "of the caching so even with caching it",
    "start": "1633399",
    "end": "1635279"
  },
  {
    "text": "would still help in fact even more uh",
    "start": "1635279",
    "end": "1638760"
  },
  {
    "text": "but this is base performance over with",
    "start": "1638760",
    "end": "1640679"
  },
  {
    "text": "no prefixed",
    "start": "1640679",
    "end": "1643080"
  },
  {
    "text": "caching uh given the interest of time we",
    "start": "1643080",
    "end": "1646480"
  },
  {
    "text": "don't take more QA but but yeah Mig is",
    "start": "1646480",
    "end": "1649679"
  },
  {
    "text": "still here we can like talk in person",
    "start": "1649679",
    "end": "1652200"
  },
  {
    "text": "yeah so thanks again for the great talk",
    "start": "1652200",
    "end": "1656679"
  }
]