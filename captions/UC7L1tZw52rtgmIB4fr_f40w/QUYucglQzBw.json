[
  {
    "text": "QC hunt for the introductions",
    "start": "3060",
    "end": "5759"
  },
  {
    "text": "um so today's talk will be split into",
    "start": "5759",
    "end": "7259"
  },
  {
    "text": "three sections each section is a",
    "start": "7259",
    "end": "9480"
  },
  {
    "text": "different many model serving solution in",
    "start": "9480",
    "end": "11099"
  },
  {
    "text": "race serve although you can combine them",
    "start": "11099",
    "end": "13080"
  },
  {
    "text": "in any way that you'd like for your use",
    "start": "13080",
    "end": "14580"
  },
  {
    "text": "case",
    "start": "14580",
    "end": "16139"
  },
  {
    "text": "first we'll dive into model composition",
    "start": "16139",
    "end": "19680"
  },
  {
    "text": "generally a big Trend that we've seen",
    "start": "19680",
    "end": "21420"
  },
  {
    "text": "recently is that more and more",
    "start": "21420",
    "end": "22920"
  },
  {
    "text": "applications require multiple models",
    "start": "22920",
    "end": "24660"
  },
  {
    "text": "rather than just a single model for",
    "start": "24660",
    "end": "26220"
  },
  {
    "text": "inference some common examples are audio",
    "start": "26220",
    "end": "29099"
  },
  {
    "text": "transcription which requires multiple",
    "start": "29099",
    "end": "30900"
  },
  {
    "text": "models to serve a single request or",
    "start": "30900",
    "end": "33360"
  },
  {
    "text": "computer vision which also requires",
    "start": "33360",
    "end": "35760"
  },
  {
    "text": "usually more than one type of image",
    "start": "35760",
    "end": "37620"
  },
  {
    "text": "processing model like classifiers or",
    "start": "37620",
    "end": "39480"
  },
  {
    "text": "detectors to serve that single request",
    "start": "39480",
    "end": "43140"
  },
  {
    "text": "but there's a lot of challenges that",
    "start": "43140",
    "end": "44640"
  },
  {
    "text": "come with many model serving the first",
    "start": "44640",
    "end": "46980"
  },
  {
    "text": "one is efficient Hardware usage which",
    "start": "46980",
    "end": "49200"
  },
  {
    "text": "really just means you have a certain",
    "start": "49200",
    "end": "51000"
  },
  {
    "text": "amount of CPU compute and GPU compute",
    "start": "51000",
    "end": "53399"
  },
  {
    "text": "how do you use that as efficiently as",
    "start": "53399",
    "end": "55320"
  },
  {
    "text": "possible especially when you have a",
    "start": "55320",
    "end": "57360"
  },
  {
    "text": "heterogeneous mix of models with",
    "start": "57360",
    "end": "59399"
  },
  {
    "text": "different Hardware resource requirements",
    "start": "59399",
    "end": "61680"
  },
  {
    "text": "especially with the recent rise of llms",
    "start": "61680",
    "end": "64440"
  },
  {
    "text": "there's a huge GPU shortage right now so",
    "start": "64440",
    "end": "66780"
  },
  {
    "text": "every GPU counts",
    "start": "66780",
    "end": "68400"
  },
  {
    "text": "so it's especially important to be able",
    "start": "68400",
    "end": "70619"
  },
  {
    "text": "to share your GPU resource compute",
    "start": "70619",
    "end": "73799"
  },
  {
    "text": "between the models in your application",
    "start": "73799",
    "end": "76140"
  },
  {
    "text": "it's also important to be able to scale",
    "start": "76140",
    "end": "78060"
  },
  {
    "text": "the models in your application",
    "start": "78060",
    "end": "79320"
  },
  {
    "text": "independently of each other especially",
    "start": "79320",
    "end": "81299"
  },
  {
    "text": "if different models have different",
    "start": "81299",
    "end": "82799"
  },
  {
    "text": "latencies or different traffic patterns",
    "start": "82799",
    "end": "85619"
  },
  {
    "text": "and then there's operational overhead a",
    "start": "85619",
    "end": "87720"
  },
  {
    "text": "lot of these things are important when",
    "start": "87720",
    "end": "89100"
  },
  {
    "text": "you're doing development or when your",
    "start": "89100",
    "end": "90960"
  },
  {
    "text": "application is running in production",
    "start": "90960",
    "end": "92180"
  },
  {
    "text": "testing your application untend should",
    "start": "92180",
    "end": "95100"
  },
  {
    "text": "be simple observability and monitoring",
    "start": "95100",
    "end": "97259"
  },
  {
    "text": "is really important production to know",
    "start": "97259",
    "end": "99360"
  },
  {
    "text": "how your application is doing or if",
    "start": "99360",
    "end": "100979"
  },
  {
    "text": "something's going wrong and being able",
    "start": "100979",
    "end": "103020"
  },
  {
    "text": "to upgrade parts of your application",
    "start": "103020",
    "end": "104400"
  },
  {
    "text": "independently of each other",
    "start": "104400",
    "end": "106200"
  },
  {
    "text": "should also be flexible",
    "start": "106200",
    "end": "108960"
  },
  {
    "text": "but all of these become more complicated",
    "start": "108960",
    "end": "110759"
  },
  {
    "text": "once you start adding more models to",
    "start": "110759",
    "end": "112439"
  },
  {
    "text": "your application",
    "start": "112439",
    "end": "114960"
  },
  {
    "text": "let's take a deeper look at the computer",
    "start": "114960",
    "end": "117060"
  },
  {
    "text": "vision example we showed here",
    "start": "117060",
    "end": "119640"
  },
  {
    "text": "suppose you take the raw image as input",
    "start": "119640",
    "end": "122340"
  },
  {
    "text": "pre-process it and then run the",
    "start": "122340",
    "end": "124380"
  },
  {
    "text": "pre-process data through a classifier",
    "start": "124380",
    "end": "126119"
  },
  {
    "text": "and a detector model and then finally",
    "start": "126119",
    "end": "128280"
  },
  {
    "text": "run some custom business logic to decide",
    "start": "128280",
    "end": "130560"
  },
  {
    "text": "on the final output to return to the",
    "start": "130560",
    "end": "132660"
  },
  {
    "text": "client",
    "start": "132660",
    "end": "133440"
  },
  {
    "text": "we'll be using this as a reference",
    "start": "133440",
    "end": "135420"
  },
  {
    "text": "example for the next few slides",
    "start": "135420",
    "end": "138420"
  },
  {
    "text": "so how might you solve these challenges",
    "start": "138420",
    "end": "140459"
  },
  {
    "text": "that we mentioned without using race",
    "start": "140459",
    "end": "141959"
  },
  {
    "text": "serve the first thing that you might try",
    "start": "141959",
    "end": "144000"
  },
  {
    "text": "is the monolith approach which is",
    "start": "144000",
    "end": "146099"
  },
  {
    "text": "essentially the shove all of your models",
    "start": "146099",
    "end": "147720"
  },
  {
    "text": "into one box solution",
    "start": "147720",
    "end": "149160"
  },
  {
    "text": "you can take your models put it into a",
    "start": "149160",
    "end": "151500"
  },
  {
    "text": "container scale the container up",
    "start": "151500",
    "end": "153420"
  },
  {
    "text": "together as a monolith",
    "start": "153420",
    "end": "155340"
  },
  {
    "text": "and then that will be your service",
    "start": "155340",
    "end": "157200"
  },
  {
    "text": "but a big problem here is you aren't",
    "start": "157200",
    "end": "158940"
  },
  {
    "text": "able to scale part of your application",
    "start": "158940",
    "end": "160879"
  },
  {
    "text": "independently of the other parts because",
    "start": "160879",
    "end": "163620"
  },
  {
    "text": "everything is packaged together and",
    "start": "163620",
    "end": "165540"
  },
  {
    "text": "glued together as one piece",
    "start": "165540",
    "end": "167459"
  },
  {
    "text": "for the same reason you aren't able to",
    "start": "167459",
    "end": "169500"
  },
  {
    "text": "make independent upgrades if you upgrade",
    "start": "169500",
    "end": "171660"
  },
  {
    "text": "one part of your application all the",
    "start": "171660",
    "end": "173220"
  },
  {
    "text": "other parts are forced to update as well",
    "start": "173220",
    "end": "175260"
  },
  {
    "text": "which makes the upgrade process both",
    "start": "175260",
    "end": "177300"
  },
  {
    "text": "very risky and very costly",
    "start": "177300",
    "end": "180720"
  },
  {
    "text": "the next thing you might try is",
    "start": "180720",
    "end": "182220"
  },
  {
    "text": "separating your models into individual",
    "start": "182220",
    "end": "183720"
  },
  {
    "text": "microservices",
    "start": "183720",
    "end": "184920"
  },
  {
    "text": "you can have an API Gateway for Access",
    "start": "184920",
    "end": "186660"
  },
  {
    "text": "Control",
    "start": "186660",
    "end": "187739"
  },
  {
    "text": "there'll be communication between",
    "start": "187739",
    "end": "188940"
  },
  {
    "text": "microservices since each request",
    "start": "188940",
    "end": "190739"
  },
  {
    "text": "requires multiple models for inference",
    "start": "190739",
    "end": "193620"
  },
  {
    "text": "and now you can scale up each",
    "start": "193620",
    "end": "195060"
  },
  {
    "text": "microservice independently of each other",
    "start": "195060",
    "end": "197879"
  },
  {
    "text": "so like we said there's independent",
    "start": "197879",
    "end": "199319"
  },
  {
    "text": "scaling and you can independently",
    "start": "199319",
    "end": "201540"
  },
  {
    "text": "upgrade one microservice without",
    "start": "201540",
    "end": "203040"
  },
  {
    "text": "affecting your other models",
    "start": "203040",
    "end": "205560"
  },
  {
    "text": "but it's exactly because everything is",
    "start": "205560",
    "end": "208140"
  },
  {
    "text": "separated into its own system that you",
    "start": "208140",
    "end": "210060"
  },
  {
    "text": "can't share resources between your",
    "start": "210060",
    "end": "211980"
  },
  {
    "text": "models anymore",
    "start": "211980",
    "end": "213239"
  },
  {
    "text": "and you have to set up a lot of things",
    "start": "213239",
    "end": "215040"
  },
  {
    "text": "like inter-service communication maybe",
    "start": "215040",
    "end": "217260"
  },
  {
    "text": "databases for each individual",
    "start": "217260",
    "end": "218940"
  },
  {
    "text": "microservice monitoring and",
    "start": "218940",
    "end": "220860"
  },
  {
    "text": "observability has to be set up",
    "start": "220860",
    "end": "222379"
  },
  {
    "text": "individually for each microservice",
    "start": "222379",
    "end": "224940"
  },
  {
    "text": "testing your application end to end is",
    "start": "224940",
    "end": "227280"
  },
  {
    "text": "also much more complex now because you",
    "start": "227280",
    "end": "228780"
  },
  {
    "text": "have to deploy multiple subsystems to",
    "start": "228780",
    "end": "230879"
  },
  {
    "text": "test your application",
    "start": "230879",
    "end": "232260"
  },
  {
    "text": "so there's also a lot of drawbacks to",
    "start": "232260",
    "end": "234060"
  },
  {
    "text": "this solution",
    "start": "234060",
    "end": "235739"
  },
  {
    "text": "this is why in racer we built a feature",
    "start": "235739",
    "end": "238260"
  },
  {
    "text": "that combines the advantages of both the",
    "start": "238260",
    "end": "240180"
  },
  {
    "text": "microservices and the monolith approach",
    "start": "240180",
    "end": "242159"
  },
  {
    "text": "called Model composition",
    "start": "242159",
    "end": "244200"
  },
  {
    "text": "so one application in racer consists of",
    "start": "244200",
    "end": "246599"
  },
  {
    "text": "multiple models composed together",
    "start": "246599",
    "end": "248940"
  },
  {
    "text": "each model can have its own Hardware",
    "start": "248940",
    "end": "251099"
  },
  {
    "text": "resource requirements for example some",
    "start": "251099",
    "end": "253140"
  },
  {
    "text": "might run on a GPU some might be CPU",
    "start": "253140",
    "end": "256019"
  },
  {
    "text": "only and some can even have a",
    "start": "256019",
    "end": "257760"
  },
  {
    "text": "combination of both",
    "start": "257760",
    "end": "259799"
  },
  {
    "text": "you can also Auto scale the models in",
    "start": "259799",
    "end": "261900"
  },
  {
    "text": "your application independently of each",
    "start": "261900",
    "end": "263460"
  },
  {
    "text": "other this means if one of the models in",
    "start": "263460",
    "end": "265860"
  },
  {
    "text": "your application is the bottleneck you",
    "start": "265860",
    "end": "267780"
  },
  {
    "text": "can allocate more resources to that one",
    "start": "267780",
    "end": "270120"
  },
  {
    "text": "model instead of having to scale",
    "start": "270120",
    "end": "271560"
  },
  {
    "text": "everything up together",
    "start": "271560",
    "end": "273419"
  },
  {
    "text": "it also scales at a process level which",
    "start": "273419",
    "end": "275699"
  },
  {
    "text": "is even more flexible than scaling at a",
    "start": "275699",
    "end": "278460"
  },
  {
    "text": "container level which is what the",
    "start": "278460",
    "end": "280080"
  },
  {
    "text": "microservices approach does this means",
    "start": "280080",
    "end": "282479"
  },
  {
    "text": "when you're scaling up your application",
    "start": "282479",
    "end": "284220"
  },
  {
    "text": "you don't have to necessarily add more",
    "start": "284220",
    "end": "286380"
  },
  {
    "text": "nodes just to scale it up you can add",
    "start": "286380",
    "end": "289199"
  },
  {
    "text": "more copies of your model onto a single",
    "start": "289199",
    "end": "291360"
  },
  {
    "text": "node until you've maximized the resource",
    "start": "291360",
    "end": "292979"
  },
  {
    "text": "utilization on that node",
    "start": "292979",
    "end": "295740"
  },
  {
    "text": "and finally something about using Ray",
    "start": "295740",
    "end": "297479"
  },
  {
    "text": "uniquely gives us is the ability to use",
    "start": "297479",
    "end": "299520"
  },
  {
    "text": "fractional resources for example if this",
    "start": "299520",
    "end": "302820"
  },
  {
    "text": "single model uses half a GPU each",
    "start": "302820",
    "end": "305699"
  },
  {
    "text": "having three copies of it means you can",
    "start": "305699",
    "end": "307500"
  },
  {
    "text": "schedule all three copies on just two",
    "start": "307500",
    "end": "309060"
  },
  {
    "text": "gpus and you can even use the remaining",
    "start": "309060",
    "end": "311340"
  },
  {
    "text": "space for another model",
    "start": "311340",
    "end": "314759"
  },
  {
    "text": "now let's see how we would go through",
    "start": "314759",
    "end": "316860"
  },
  {
    "text": "this computer vision example using model",
    "start": "316860",
    "end": "318780"
  },
  {
    "text": "composition",
    "start": "318780",
    "end": "320040"
  },
  {
    "text": "you take your image pre-process it send",
    "start": "320040",
    "end": "323220"
  },
  {
    "text": "it to a certain classifier model",
    "start": "323220",
    "end": "325620"
  },
  {
    "text": "and a detector model and finally run",
    "start": "325620",
    "end": "327960"
  },
  {
    "text": "your custom business logic and this is",
    "start": "327960",
    "end": "329639"
  },
  {
    "text": "your entire multi-model application",
    "start": "329639",
    "end": "331620"
  },
  {
    "text": "notice how as we added components of the",
    "start": "331620",
    "end": "333840"
  },
  {
    "text": "application one by one we also showed",
    "start": "333840",
    "end": "336000"
  },
  {
    "text": "what you would need to add on the right",
    "start": "336000",
    "end": "337440"
  },
  {
    "text": "here to the python file the point here",
    "start": "337440",
    "end": "340199"
  },
  {
    "text": "is you are able to Define and Link",
    "start": "340199",
    "end": "342240"
  },
  {
    "text": "together your entire application",
    "start": "342240",
    "end": "344699"
  },
  {
    "text": "using just a single python file and",
    "start": "344699",
    "end": "347100"
  },
  {
    "text": "that's all you need to deploy your",
    "start": "347100",
    "end": "348240"
  },
  {
    "text": "application to array cluster",
    "start": "348240",
    "end": "350940"
  },
  {
    "text": "we can look at an example of resource",
    "start": "350940",
    "end": "352919"
  },
  {
    "text": "allocation for this application",
    "start": "352919",
    "end": "354680"
  },
  {
    "text": "classifiers might take one CPU and a",
    "start": "354680",
    "end": "357419"
  },
  {
    "text": "third of a GPU each pre-processing",
    "start": "357419",
    "end": "359460"
  },
  {
    "text": "usually runs on CPU so we can put those",
    "start": "359460",
    "end": "361860"
  },
  {
    "text": "two deployments on a node with four CPUs",
    "start": "361860",
    "end": "364080"
  },
  {
    "text": "and one GPU available",
    "start": "364080",
    "end": "366060"
  },
  {
    "text": "and then the detector and business logic",
    "start": "366060",
    "end": "367919"
  },
  {
    "text": "probably also runs on CPU so you can put",
    "start": "367919",
    "end": "369840"
  },
  {
    "text": "those two on the second node with two",
    "start": "369840",
    "end": "371580"
  },
  {
    "text": "CPUs available so it's really easy to",
    "start": "371580",
    "end": "373979"
  },
  {
    "text": "share resources between the models in",
    "start": "373979",
    "end": "375840"
  },
  {
    "text": "your application",
    "start": "375840",
    "end": "377220"
  },
  {
    "text": "in a similar real world use case samsara",
    "start": "377220",
    "end": "379380"
  },
  {
    "text": "switched from using the microservices",
    "start": "379380",
    "end": "380940"
  },
  {
    "text": "architecture that we mentioned to using",
    "start": "380940",
    "end": "383460"
  },
  {
    "text": "Reserve model composition and managed to",
    "start": "383460",
    "end": "385380"
  },
  {
    "text": "save 50 of their ML infras cost per year",
    "start": "385380",
    "end": "390080"
  },
  {
    "text": "now let's revisit the challenges that we",
    "start": "390600",
    "end": "392580"
  },
  {
    "text": "started with like we mentioned it's",
    "start": "392580",
    "end": "394560"
  },
  {
    "text": "really easy to share resources between",
    "start": "394560",
    "end": "396360"
  },
  {
    "text": "models in your application and you are",
    "start": "396360",
    "end": "398520"
  },
  {
    "text": "able to independently scale models in",
    "start": "398520",
    "end": "400860"
  },
  {
    "text": "your application",
    "start": "400860",
    "end": "402240"
  },
  {
    "text": "as for operational overheads since",
    "start": "402240",
    "end": "403979"
  },
  {
    "text": "everything is on the same cluster",
    "start": "403979",
    "end": "405419"
  },
  {
    "text": "testing your application n10 is easy and",
    "start": "405419",
    "end": "408419"
  },
  {
    "text": "it's very simple to set up monitoring",
    "start": "408419",
    "end": "410039"
  },
  {
    "text": "observability for just your single rig",
    "start": "410039",
    "end": "412560"
  },
  {
    "text": "cluster",
    "start": "412560",
    "end": "413580"
  },
  {
    "text": "but if we limit ourselves to one single",
    "start": "413580",
    "end": "415919"
  },
  {
    "text": "application",
    "start": "415919",
    "end": "417000"
  },
  {
    "text": "what we're missing here is independent",
    "start": "417000",
    "end": "418800"
  },
  {
    "text": "upgrades if you update one part of your",
    "start": "418800",
    "end": "420600"
  },
  {
    "text": "application it still requires updating",
    "start": "420600",
    "end": "422639"
  },
  {
    "text": "all the other parts of your application",
    "start": "422639",
    "end": "424680"
  },
  {
    "text": "as well",
    "start": "424680",
    "end": "425759"
  },
  {
    "text": "which is the perfect segue into the",
    "start": "425759",
    "end": "427440"
  },
  {
    "text": "second part of our talk which is",
    "start": "427440",
    "end": "428759"
  },
  {
    "text": "multi-application",
    "start": "428759",
    "end": "431039"
  },
  {
    "text": "so like we mentioned one application",
    "start": "431039",
    "end": "432780"
  },
  {
    "text": "with model composition doesn't work for",
    "start": "432780",
    "end": "434880"
  },
  {
    "text": "all use cases suppose you have an ml",
    "start": "434880",
    "end": "437639"
  },
  {
    "text": "team working on computer vision models",
    "start": "437639",
    "end": "439380"
  },
  {
    "text": "for autonomous driving",
    "start": "439380",
    "end": "441000"
  },
  {
    "text": "but maybe these models don't work well",
    "start": "441000",
    "end": "442740"
  },
  {
    "text": "during the night time so you have",
    "start": "442740",
    "end": "444780"
  },
  {
    "text": "another team working on algorithms",
    "start": "444780",
    "end": "446220"
  },
  {
    "text": "specialized for night time and ideally",
    "start": "446220",
    "end": "447720"
  },
  {
    "text": "these would work they was they would",
    "start": "447720",
    "end": "449340"
  },
  {
    "text": "live on the same cluster so that the",
    "start": "449340",
    "end": "451199"
  },
  {
    "text": "vehicle can just switch over to the",
    "start": "451199",
    "end": "453180"
  },
  {
    "text": "specialized algorithms at night time",
    "start": "453180",
    "end": "455759"
  },
  {
    "text": "you might have another team working on",
    "start": "455759",
    "end": "457400"
  },
  {
    "text": "algorithms for snowstorms or rainstorms",
    "start": "457400",
    "end": "460099"
  },
  {
    "text": "lidars and Radars are also really",
    "start": "460099",
    "end": "462479"
  },
  {
    "text": "important for gauging the exact distance",
    "start": "462479",
    "end": "463919"
  },
  {
    "text": "between your car and other objects on",
    "start": "463919",
    "end": "467520"
  },
  {
    "text": "the road",
    "start": "467520",
    "end": "468780"
  },
  {
    "text": "ideally all of these will live on the",
    "start": "468780",
    "end": "470460"
  },
  {
    "text": "same cluster since they're all serving",
    "start": "470460",
    "end": "471960"
  },
  {
    "text": "the same use case but they're probably",
    "start": "471960",
    "end": "474360"
  },
  {
    "text": "all living in different repositories and",
    "start": "474360",
    "end": "476400"
  },
  {
    "text": "certainly managed by different teams so",
    "start": "476400",
    "end": "478560"
  },
  {
    "text": "if we combine all of them together into",
    "start": "478560",
    "end": "480599"
  },
  {
    "text": "a single group using model composition",
    "start": "480599",
    "end": "482759"
  },
  {
    "text": "they would all share the same upgrade",
    "start": "482759",
    "end": "484680"
  },
  {
    "text": "lifecycle",
    "start": "484680",
    "end": "485699"
  },
  {
    "text": "that means if one team wanted to make a",
    "start": "485699",
    "end": "487740"
  },
  {
    "text": "change to their model in production the",
    "start": "487740",
    "end": "489720"
  },
  {
    "text": "models managed by other teams would be",
    "start": "489720",
    "end": "491639"
  },
  {
    "text": "forced to update as well so that makes",
    "start": "491639",
    "end": "493979"
  },
  {
    "text": "the upgrade process risky costly and",
    "start": "493979",
    "end": "496440"
  },
  {
    "text": "overall just very complicated you might",
    "start": "496440",
    "end": "498180"
  },
  {
    "text": "even need another team",
    "start": "498180",
    "end": "500160"
  },
  {
    "text": "just to manage deployments across ml",
    "start": "500160",
    "end": "502560"
  },
  {
    "text": "teams and making sure nothing goes wrong",
    "start": "502560",
    "end": "504060"
  },
  {
    "text": "during deployment",
    "start": "504060",
    "end": "506160"
  },
  {
    "text": "this is why we added the",
    "start": "506160",
    "end": "507599"
  },
  {
    "text": "multi-application feature which Builds",
    "start": "507599",
    "end": "509699"
  },
  {
    "text": "on top of model composition",
    "start": "509699",
    "end": "511379"
  },
  {
    "text": "so your application can still have",
    "start": "511379",
    "end": "513060"
  },
  {
    "text": "models composed together",
    "start": "513060",
    "end": "515039"
  },
  {
    "text": "but now you can have multiple",
    "start": "515039",
    "end": "516300"
  },
  {
    "text": "applications living on your same rate",
    "start": "516300",
    "end": "518339"
  },
  {
    "text": "cluster each application is its own",
    "start": "518339",
    "end": "520800"
  },
  {
    "text": "endpoint and has its own upgrade",
    "start": "520800",
    "end": "522719"
  },
  {
    "text": "lifecycle many you can easily add delete",
    "start": "522719",
    "end": "525240"
  },
  {
    "text": "or update applications without worrying",
    "start": "525240",
    "end": "527700"
  },
  {
    "text": "about affecting the other models on the",
    "start": "527700",
    "end": "529740"
  },
  {
    "text": "cluster",
    "start": "529740",
    "end": "531120"
  },
  {
    "text": "at the same time you still have the",
    "start": "531120",
    "end": "532800"
  },
  {
    "text": "benefit of efficient and flexible",
    "start": "532800",
    "end": "535200"
  },
  {
    "text": "resource allocation and sharing Hardware",
    "start": "535200",
    "end": "537360"
  },
  {
    "text": "resources between your applications",
    "start": "537360",
    "end": "540540"
  },
  {
    "text": "let's just go to go through a really",
    "start": "540540",
    "end": "542700"
  },
  {
    "text": "quick real world use case of",
    "start": "542700",
    "end": "544920"
  },
  {
    "text": "multi-application any scale endpoints is",
    "start": "544920",
    "end": "547260"
  },
  {
    "text": "the any scale hosted API endpoints for",
    "start": "547260",
    "end": "549480"
  },
  {
    "text": "llm inference and fine-tuning",
    "start": "549480",
    "end": "552420"
  },
  {
    "text": "and recently the Llama family of models",
    "start": "552420",
    "end": "555180"
  },
  {
    "text": "as some of you might know was released",
    "start": "555180",
    "end": "557100"
  },
  {
    "text": "by a meta Ai and has really great",
    "start": "557100",
    "end": "558899"
  },
  {
    "text": "performance so suppose it was just",
    "start": "558899",
    "end": "560820"
  },
  {
    "text": "recently very released maybe a few days",
    "start": "560820",
    "end": "563339"
  },
  {
    "text": "ago and you we wanted to add this to any",
    "start": "563339",
    "end": "566459"
  },
  {
    "text": "skill endpoints for our users to try out",
    "start": "566459",
    "end": "568200"
  },
  {
    "text": "and fine-tune",
    "start": "568200",
    "end": "570000"
  },
  {
    "text": "so suppose you wanted to add the 7",
    "start": "570000",
    "end": "571560"
  },
  {
    "text": "billion parameter model to any skill",
    "start": "571560",
    "end": "573660"
  },
  {
    "text": "endpoints you would just have to add it",
    "start": "573660",
    "end": "575459"
  },
  {
    "text": "to the config file deployed to the",
    "start": "575459",
    "end": "577320"
  },
  {
    "text": "cluster and this would load it onto your",
    "start": "577320",
    "end": "578820"
  },
  {
    "text": "cluster ready to serve users traffic you",
    "start": "578820",
    "end": "582060"
  },
  {
    "text": "can also add more models like the third",
    "start": "582060",
    "end": "583500"
  },
  {
    "text": "TP model and the hypothetically you",
    "start": "583500",
    "end": "585720"
  },
  {
    "text": "could also delete that and replace it",
    "start": "585720",
    "end": "587519"
  },
  {
    "text": "with the 7db model all by using a Serv",
    "start": "587519",
    "end": "590519"
  },
  {
    "text": "config file and making changes to the",
    "start": "590519",
    "end": "593040"
  },
  {
    "text": "entries and notice how the entire time",
    "start": "593040",
    "end": "595140"
  },
  {
    "text": "the 7B model continues to serve traffic",
    "start": "595140",
    "end": "599480"
  },
  {
    "text": "regardless of what's happening to the",
    "start": "599519",
    "end": "601620"
  },
  {
    "text": "other applications on your cluster",
    "start": "601620",
    "end": "604560"
  },
  {
    "text": "again let's revisit the challenges that",
    "start": "604560",
    "end": "606300"
  },
  {
    "text": "we started with uh",
    "start": "606300",
    "end": "608399"
  },
  {
    "text": "like we mentioned a single application",
    "start": "608399",
    "end": "610260"
  },
  {
    "text": "with model composition already it gives",
    "start": "610260",
    "end": "612180"
  },
  {
    "text": "you a lot of these benefits like",
    "start": "612180",
    "end": "613620"
  },
  {
    "text": "efficient resource allocation and",
    "start": "613620",
    "end": "615720"
  },
  {
    "text": "independent scaling of the models",
    "start": "615720",
    "end": "617820"
  },
  {
    "text": "as well as testing your application n10",
    "start": "617820",
    "end": "620160"
  },
  {
    "text": "is very easy observability and",
    "start": "620160",
    "end": "622500"
  },
  {
    "text": "monitoring only has to be set up once",
    "start": "622500",
    "end": "624000"
  },
  {
    "text": "for the entire cluster but the one thing",
    "start": "624000",
    "end": "626160"
  },
  {
    "text": "that was missing was independent",
    "start": "626160",
    "end": "627540"
  },
  {
    "text": "upgrades with multi-application you can",
    "start": "627540",
    "end": "630480"
  },
  {
    "text": "group the models on your cluster into",
    "start": "630480",
    "end": "633180"
  },
  {
    "text": "groups that make sense to you for",
    "start": "633180",
    "end": "634560"
  },
  {
    "text": "example if they're living in different",
    "start": "634560",
    "end": "636120"
  },
  {
    "text": "repositories already or they're managed",
    "start": "636120",
    "end": "638220"
  },
  {
    "text": "by different teams",
    "start": "638220",
    "end": "639540"
  },
  {
    "text": "or any other requirements that you have",
    "start": "639540",
    "end": "641220"
  },
  {
    "text": "you can form those groups and then",
    "start": "641220",
    "end": "643080"
  },
  {
    "text": "update those groups independently of",
    "start": "643080",
    "end": "644880"
  },
  {
    "text": "each other",
    "start": "644880",
    "end": "646380"
  },
  {
    "text": "now I'll hand it off to sihan for the",
    "start": "646380",
    "end": "648839"
  },
  {
    "text": "rest of the talk",
    "start": "648839",
    "end": "651320"
  },
  {
    "text": "thank you for sending deep diving the",
    "start": "653760",
    "end": "656100"
  },
  {
    "text": "order conversation and multi-application",
    "start": "656100",
    "end": "658260"
  },
  {
    "text": "use cases in previous scenario we",
    "start": "658260",
    "end": "661320"
  },
  {
    "text": "typically typically loading all the",
    "start": "661320",
    "end": "662880"
  },
  {
    "text": "models into the cell clusters at once",
    "start": "662880",
    "end": "664800"
  },
  {
    "text": "for serving",
    "start": "664800",
    "end": "665880"
  },
  {
    "text": "but as we see the emergence of the use",
    "start": "665880",
    "end": "668339"
  },
  {
    "text": "cases we see serving large number of",
    "start": "668339",
    "end": "670560"
  },
  {
    "text": "models requiring more resources than",
    "start": "670560",
    "end": "672660"
  },
  {
    "text": "what your provider can provide oh sorry",
    "start": "672660",
    "end": "674760"
  },
  {
    "text": "the cluster can provide so to support",
    "start": "674760",
    "end": "677160"
  },
  {
    "text": "the scenario we introduced a new Reserve",
    "start": "677160",
    "end": "679920"
  },
  {
    "text": "API which is called a multiplex to",
    "start": "679920",
    "end": "681779"
  },
  {
    "text": "support this use case",
    "start": "681779",
    "end": "683700"
  },
  {
    "text": "before going to the API I want to go for",
    "start": "683700",
    "end": "686519"
  },
  {
    "text": "a simple examples",
    "start": "686519",
    "end": "688140"
  },
  {
    "text": "let's imagine we want to build a RM",
    "start": "688140",
    "end": "690180"
  },
  {
    "text": "inference platform service such as like",
    "start": "690180",
    "end": "693060"
  },
  {
    "text": "unscale endpoint or Firefox Ai and we",
    "start": "693060",
    "end": "696000"
  },
  {
    "text": "want to support different open source",
    "start": "696000",
    "end": "698640"
  },
  {
    "text": "model for example like different number",
    "start": "698640",
    "end": "700260"
  },
  {
    "text": "of parameters of the lamba models and",
    "start": "700260",
    "end": "703260"
  },
  {
    "text": "more importantly you want to support in",
    "start": "703260",
    "end": "704700"
  },
  {
    "text": "different customers",
    "start": "704700",
    "end": "706860"
  },
  {
    "text": "customer models to supporting their own",
    "start": "706860",
    "end": "708899"
  },
  {
    "text": "business use case so this is going to be",
    "start": "708899",
    "end": "710880"
  },
  {
    "text": "the two challenges for supporting this",
    "start": "710880",
    "end": "713220"
  },
  {
    "text": "use case",
    "start": "713220",
    "end": "714480"
  },
  {
    "text": "one is the limited Hardware resources",
    "start": "714480",
    "end": "717420"
  },
  {
    "text": "so you're typically not able to load in",
    "start": "717420",
    "end": "719160"
  },
  {
    "text": "all this model into your cluster at once",
    "start": "719160",
    "end": "722160"
  },
  {
    "text": "um one is like it's very expensive",
    "start": "722160",
    "end": "724320"
  },
  {
    "text": "especially for GPU Hardware",
    "start": "724320",
    "end": "726360"
  },
  {
    "text": "secondly for",
    "start": "726360",
    "end": "727920"
  },
  {
    "text": "inactive models it's now very efficient",
    "start": "727920",
    "end": "730560"
  },
  {
    "text": "you'll load these models into your",
    "start": "730560",
    "end": "733140"
  },
  {
    "text": "memory without being used",
    "start": "733140",
    "end": "735600"
  },
  {
    "text": "Second Challenge is the high inference",
    "start": "735600",
    "end": "737820"
  },
  {
    "text": "latency",
    "start": "737820",
    "end": "739440"
  },
  {
    "text": "if you're loading the models all the",
    "start": "739440",
    "end": "741360"
  },
  {
    "text": "times your online inference service will",
    "start": "741360",
    "end": "744240"
  },
  {
    "text": "suffer the co-startup time all the time",
    "start": "744240",
    "end": "747360"
  },
  {
    "text": "so let's see how it looks like without",
    "start": "747360",
    "end": "749579"
  },
  {
    "text": "multiplexing",
    "start": "749579",
    "end": "750899"
  },
  {
    "text": "so normally we put an or model into S3",
    "start": "750899",
    "end": "754079"
  },
  {
    "text": "and user will just specifies your model",
    "start": "754079",
    "end": "756540"
  },
  {
    "text": "ID and sending the HTTP request to the",
    "start": "756540",
    "end": "759660"
  },
  {
    "text": "to your clusters",
    "start": "759660",
    "end": "761760"
  },
  {
    "text": "and",
    "start": "761760",
    "end": "763500"
  },
  {
    "text": "the cluster proxy receiving this request",
    "start": "763500",
    "end": "765779"
  },
  {
    "text": "right now there's no for this examples",
    "start": "765779",
    "end": "768480"
  },
  {
    "text": "we're using two replicas in this cluster",
    "start": "768480",
    "end": "770639"
  },
  {
    "text": "proxy will randomly pick up replicas to",
    "start": "770639",
    "end": "773339"
  },
  {
    "text": "to serve this request let's say we go",
    "start": "773339",
    "end": "775620"
  },
  {
    "text": "for replica one and we're loading the",
    "start": "775620",
    "end": "777480"
  },
  {
    "text": "model from S3 and load the models and do",
    "start": "777480",
    "end": "780180"
  },
  {
    "text": "the inference and get response back to",
    "start": "780180",
    "end": "782519"
  },
  {
    "text": "the client",
    "start": "782519",
    "end": "783779"
  },
  {
    "text": "if kinda keeps sending the model one",
    "start": "783779",
    "end": "786360"
  },
  {
    "text": "given proxy doesn't even know what the",
    "start": "786360",
    "end": "789540"
  },
  {
    "text": "model ID is located what's the model is",
    "start": "789540",
    "end": "791639"
  },
  {
    "text": "located in which replicas so proxy",
    "start": "791639",
    "end": "794040"
  },
  {
    "text": "potentially will choose replica 2 to",
    "start": "794040",
    "end": "796440"
  },
  {
    "text": "serving your request",
    "start": "796440",
    "end": "798480"
  },
  {
    "text": "which means you have to loading the",
    "start": "798480",
    "end": "800100"
  },
  {
    "text": "model 1 again into the different",
    "start": "800100",
    "end": "801839"
  },
  {
    "text": "replicas from S3 into your clusters to",
    "start": "801839",
    "end": "805320"
  },
  {
    "text": "serve this request",
    "start": "805320",
    "end": "806820"
  },
  {
    "text": "this will be the same for the model 2",
    "start": "806820",
    "end": "808440"
  },
  {
    "text": "and the model 3.",
    "start": "808440",
    "end": "810000"
  },
  {
    "text": "Singapore if you have a large number of",
    "start": "810000",
    "end": "811800"
  },
  {
    "text": "models you have lots of model on S3 you",
    "start": "811800",
    "end": "815040"
  },
  {
    "text": "end up have to stop in your models given",
    "start": "815040",
    "end": "817380"
  },
  {
    "text": "your Hardware resources and your all",
    "start": "817380",
    "end": "820740"
  },
  {
    "text": "your code inference code start time will",
    "start": "820740",
    "end": "823079"
  },
  {
    "text": "be very high for your online inference",
    "start": "823079",
    "end": "825540"
  },
  {
    "text": "let's say how the multiplexing results",
    "start": "825540",
    "end": "827579"
  },
  {
    "text": "these issues",
    "start": "827579",
    "end": "829320"
  },
  {
    "text": "it's the same let's say client also set",
    "start": "829320",
    "end": "831959"
  },
  {
    "text": "the model 1 into the ATV request and",
    "start": "831959",
    "end": "835560"
  },
  {
    "text": "sending the proxy and this is the same",
    "start": "835560",
    "end": "837660"
  },
  {
    "text": "number as the previous one we're loading",
    "start": "837660",
    "end": "839459"
  },
  {
    "text": "the model into the replica one from S3",
    "start": "839459",
    "end": "841440"
  },
  {
    "text": "and do the inference get results back",
    "start": "841440",
    "end": "844260"
  },
  {
    "text": "if users send the model 1 again with",
    "start": "844260",
    "end": "846959"
  },
  {
    "text": "multiplexing API proxy will know model 1",
    "start": "846959",
    "end": "850620"
  },
  {
    "text": "is located inside the replica one it",
    "start": "850620",
    "end": "852959"
  },
  {
    "text": "will smartly drop in the traffic to this",
    "start": "852959",
    "end": "855060"
  },
  {
    "text": "to the replica one to do the inference",
    "start": "855060",
    "end": "857100"
  },
  {
    "text": "instead of like instead of choosing",
    "start": "857100",
    "end": "859740"
  },
  {
    "text": "different replicas so in this case you",
    "start": "859740",
    "end": "862260"
  },
  {
    "text": "are able to do the model 2 and model",
    "start": "862260",
    "end": "863880"
  },
  {
    "text": "save is the same",
    "start": "863880",
    "end": "865440"
  },
  {
    "text": "so we're saving the more memory you are",
    "start": "865440",
    "end": "868320"
  },
  {
    "text": "able your potential is saving more",
    "start": "868320",
    "end": "870720"
  },
  {
    "text": "resource for loading all the models from",
    "start": "870720",
    "end": "873360"
  },
  {
    "text": "from your money model use cases this is",
    "start": "873360",
    "end": "876180"
  },
  {
    "text": "how the multiplexing works to help you",
    "start": "876180",
    "end": "878279"
  },
  {
    "text": "to boost in your performance",
    "start": "878279",
    "end": "881220"
  },
  {
    "text": "for our we also did a benchmarking with",
    "start": "881220",
    "end": "883860"
  },
  {
    "text": "Stage maker multimodal endpoint",
    "start": "883860",
    "end": "885959"
  },
  {
    "text": "so sagemaker multimodal endpoint the",
    "start": "885959",
    "end": "888180"
  },
  {
    "text": "similar technology for serving these use",
    "start": "888180",
    "end": "890339"
  },
  {
    "text": "cases we're using the 10050 models with",
    "start": "890339",
    "end": "893820"
  },
  {
    "text": "four and five large instances",
    "start": "893820",
    "end": "896040"
  },
  {
    "text": "and we're using the HTTP client to",
    "start": "896040",
    "end": "898139"
  },
  {
    "text": "choose the different image categories",
    "start": "898139",
    "end": "900180"
  },
  {
    "text": "and sending the image to our to the",
    "start": "900180",
    "end": "902940"
  },
  {
    "text": "Clusters and based on the different",
    "start": "902940",
    "end": "904680"
  },
  {
    "text": "categories we're loading these models",
    "start": "904680",
    "end": "906480"
  },
  {
    "text": "into our replicas and our worker in the",
    "start": "906480",
    "end": "909360"
  },
  {
    "text": "sagemaker and to do the inference",
    "start": "909360",
    "end": "912720"
  },
  {
    "text": "and this is the result we get from the",
    "start": "912720",
    "end": "914639"
  },
  {
    "text": "openchmarking is the green one is the",
    "start": "914639",
    "end": "917040"
  },
  {
    "text": "Baseline Baseline is no multiplexing",
    "start": "917040",
    "end": "919139"
  },
  {
    "text": "which I show you at the first it's going",
    "start": "919139",
    "end": "921839"
  },
  {
    "text": "to end up like a very bad throughput",
    "start": "921839",
    "end": "923699"
  },
  {
    "text": "because you have to swap in all the",
    "start": "923699",
    "end": "925199"
  },
  {
    "text": "model models all the time",
    "start": "925199",
    "end": "927000"
  },
  {
    "text": "the blue one is a stage maker one and",
    "start": "927000",
    "end": "929399"
  },
  {
    "text": "the orange one is a reserve multiplexing",
    "start": "929399",
    "end": "932040"
  },
  {
    "text": "as you can see research multiplexing",
    "start": "932040",
    "end": "934320"
  },
  {
    "text": "from our benchmarking is around like 30",
    "start": "934320",
    "end": "937160"
  },
  {
    "text": "30 better throughput than the sage maker",
    "start": "937160",
    "end": "941639"
  },
  {
    "text": "the main reason for the better",
    "start": "941639",
    "end": "943199"
  },
  {
    "text": "throughput is the model cache hitter",
    "start": "943199",
    "end": "945540"
  },
  {
    "text": "rate",
    "start": "945540",
    "end": "946740"
  },
  {
    "text": "so the model calculated rate means",
    "start": "946740",
    "end": "949019"
  },
  {
    "text": "whenever proxy losing the traffic to",
    "start": "949019",
    "end": "951600"
  },
  {
    "text": "specific replicas if replica already",
    "start": "951600",
    "end": "954120"
  },
  {
    "text": "hold this model into the memory we take",
    "start": "954120",
    "end": "956639"
  },
  {
    "text": "it as a model cache hit",
    "start": "956639",
    "end": "958320"
  },
  {
    "text": "otherwise we take as model cache Miss",
    "start": "958320",
    "end": "962040"
  },
  {
    "text": "so as you can see Reserve multiplexing",
    "start": "962040",
    "end": "964800"
  },
  {
    "text": "is better than the sagemaker one from a",
    "start": "964800",
    "end": "966420"
  },
  {
    "text": "benchmarking",
    "start": "966420",
    "end": "967560"
  },
  {
    "text": "with better cash hit rate Reserve can",
    "start": "967560",
    "end": "971100"
  },
  {
    "text": "avoid model loading times as much as",
    "start": "971100",
    "end": "973380"
  },
  {
    "text": "possible than other Solutions",
    "start": "973380",
    "end": "975779"
  },
  {
    "text": "so that we can maintain better latency",
    "start": "975779",
    "end": "978779"
  },
  {
    "text": "and very good throughput",
    "start": "978779",
    "end": "980940"
  },
  {
    "text": "in this case",
    "start": "980940",
    "end": "983699"
  },
  {
    "text": "now let's look at them how the",
    "start": "983699",
    "end": "985740"
  },
  {
    "text": "multiplexing use case",
    "start": "985740",
    "end": "987360"
  },
  {
    "text": "into the underscale endpoints",
    "start": "987360",
    "end": "989760"
  },
  {
    "text": "as I gave you the examples and scale",
    "start": "989760",
    "end": "991980"
  },
  {
    "text": "endpoint is the platform to supporting",
    "start": "991980",
    "end": "994500"
  },
  {
    "text": "different our use cases we also have",
    "start": "994500",
    "end": "997440"
  },
  {
    "text": "these challenges as I mentioned before",
    "start": "997440",
    "end": "999300"
  },
  {
    "text": "we have very limited resources we want",
    "start": "999300",
    "end": "1001940"
  },
  {
    "text": "to want to maintain very good latency in",
    "start": "1001940",
    "end": "1004880"
  },
  {
    "text": "this case we're using Laura techniques",
    "start": "1004880",
    "end": "1007160"
  },
  {
    "text": "to reduce models with Laura we are able",
    "start": "1007160",
    "end": "1010040"
  },
  {
    "text": "to make the customers share the models",
    "start": "1010040",
    "end": "1012079"
  },
  {
    "text": "with one big base model and load it into",
    "start": "1012079",
    "end": "1014420"
  },
  {
    "text": "memory so customers doesn't need to",
    "start": "1014420",
    "end": "1016339"
  },
  {
    "text": "loading so each customer then includes",
    "start": "1016339",
    "end": "1018800"
  },
  {
    "text": "all the models into memory",
    "start": "1018800",
    "end": "1021019"
  },
  {
    "text": "so let's see how it looks like how we",
    "start": "1021019",
    "end": "1023540"
  },
  {
    "text": "combined the slower and the multiplexing",
    "start": "1023540",
    "end": "1025520"
  },
  {
    "text": "together into land scouting points",
    "start": "1025520",
    "end": "1028339"
  },
  {
    "text": "whenever we started replicas",
    "start": "1028339",
    "end": "1030380"
  },
  {
    "text": "and we will load the base model first",
    "start": "1030380",
    "end": "1032480"
  },
  {
    "text": "into a GPU memory",
    "start": "1032480",
    "end": "1034280"
  },
  {
    "text": "and whenever you receive the request we",
    "start": "1034280",
    "end": "1036319"
  },
  {
    "text": "are loading the model from our S3 and",
    "start": "1036319",
    "end": "1038418"
  },
  {
    "text": "CPU memory and GPU memory to do the",
    "start": "1038419",
    "end": "1040520"
  },
  {
    "text": "inference",
    "start": "1040520",
    "end": "1041600"
  },
  {
    "text": "with the multiplexing API we are able to",
    "start": "1041600",
    "end": "1044360"
  },
  {
    "text": "not in the traffic specifically mode for",
    "start": "1044360",
    "end": "1047600"
  },
  {
    "text": "System model to the specific replicas in",
    "start": "1047600",
    "end": "1050000"
  },
  {
    "text": "this case we are able to maintain very",
    "start": "1050000",
    "end": "1051500"
  },
  {
    "text": "good cash hit rate and to avoid model",
    "start": "1051500",
    "end": "1054440"
  },
  {
    "text": "swapping all the times",
    "start": "1054440",
    "end": "1056120"
  },
  {
    "text": "so that we are able to support many more",
    "start": "1056120",
    "end": "1058160"
  },
  {
    "text": "models with our limited Hardware",
    "start": "1058160",
    "end": "1060260"
  },
  {
    "text": "resources",
    "start": "1060260",
    "end": "1061700"
  },
  {
    "text": "now I'm going to hand up the uh John to",
    "start": "1061700",
    "end": "1064580"
  },
  {
    "text": "talk about the clarity use case",
    "start": "1064580",
    "end": "1067659"
  },
  {
    "text": "thanks John",
    "start": "1068780",
    "end": "1071440"
  },
  {
    "text": "so um first of all what does Clary do",
    "start": "1071600",
    "end": "1075620"
  },
  {
    "text": "clearly provides a platform for Revenue",
    "start": "1075620",
    "end": "1078260"
  },
  {
    "text": "operations you can think of clay",
    "start": "1078260",
    "end": "1080900"
  },
  {
    "text": "products as a superset of CRM",
    "start": "1080900",
    "end": "1083059"
  },
  {
    "text": "application",
    "start": "1083059",
    "end": "1085039"
  },
  {
    "text": "one of the products we have is",
    "start": "1085039",
    "end": "1087020"
  },
  {
    "text": "forecasting what the sales is going to",
    "start": "1087020",
    "end": "1089059"
  },
  {
    "text": "look like at the end of the quarter we",
    "start": "1089059",
    "end": "1091160"
  },
  {
    "text": "use AI model we build this AI model per",
    "start": "1091160",
    "end": "1094160"
  },
  {
    "text": "customer using per customer data set",
    "start": "1094160",
    "end": "1098240"
  },
  {
    "text": "so one of the problems that we have",
    "start": "1098240",
    "end": "1100520"
  },
  {
    "text": "um since we built model per customer we",
    "start": "1100520",
    "end": "1103100"
  },
  {
    "text": "could have potentially thousands of",
    "start": "1103100",
    "end": "1104720"
  },
  {
    "text": "models",
    "start": "1104720",
    "end": "1105799"
  },
  {
    "text": "we have multiple versions of the same",
    "start": "1105799",
    "end": "1108799"
  },
  {
    "text": "model",
    "start": "1108799",
    "end": "1109820"
  },
  {
    "text": "because we continue to improve the model",
    "start": "1109820",
    "end": "1112160"
  },
  {
    "text": "performance",
    "start": "1112160",
    "end": "1113720"
  },
  {
    "text": "and each model could have its own",
    "start": "1113720",
    "end": "1115880"
  },
  {
    "text": "deployment cycle",
    "start": "1115880",
    "end": "1118640"
  },
  {
    "text": "on the interesting side we make online",
    "start": "1118640",
    "end": "1121460"
  },
  {
    "text": "influencing because we want to use the",
    "start": "1121460",
    "end": "1123860"
  },
  {
    "text": "latest sales activity data to make",
    "start": "1123860",
    "end": "1125900"
  },
  {
    "text": "accurate prediction",
    "start": "1125900",
    "end": "1128919"
  },
  {
    "text": "on the traffic side we experience uneven",
    "start": "1129020",
    "end": "1132559"
  },
  {
    "text": "traffic patterns depending on the time",
    "start": "1132559",
    "end": "1135559"
  },
  {
    "text": "of day week or month",
    "start": "1135559",
    "end": "1138980"
  },
  {
    "text": "another interesting aspect is that these",
    "start": "1138980",
    "end": "1141740"
  },
  {
    "text": "models are invoked sparsely which means",
    "start": "1141740",
    "end": "1145760"
  },
  {
    "text": "we don't have to load all the models all",
    "start": "1145760",
    "end": "1148760"
  },
  {
    "text": "at the same time but there will be a set",
    "start": "1148760",
    "end": "1150679"
  },
  {
    "text": "of models that will be used at a given",
    "start": "1150679",
    "end": "1152960"
  },
  {
    "text": "time but we cannot predict Which models",
    "start": "1152960",
    "end": "1156020"
  },
  {
    "text": "are going to be used at a given time",
    "start": "1156020",
    "end": "1160419"
  },
  {
    "text": "so before I talk about how Ray helped us",
    "start": "1160580",
    "end": "1163280"
  },
  {
    "text": "with this scaling challenge",
    "start": "1163280",
    "end": "1165380"
  },
  {
    "text": "I'm sharing",
    "start": "1165380",
    "end": "1166780"
  },
  {
    "text": "Cloud native based solution",
    "start": "1166780",
    "end": "1169760"
  },
  {
    "text": "so we take model code build a custom",
    "start": "1169760",
    "end": "1173179"
  },
  {
    "text": "train container and put it through",
    "start": "1173179",
    "end": "1175780"
  },
  {
    "text": "airflow Pipelines and have a cloud",
    "start": "1175780",
    "end": "1179179"
  },
  {
    "text": "native training service to train the",
    "start": "1179179",
    "end": "1181700"
  },
  {
    "text": "models and also use cloud native service",
    "start": "1181700",
    "end": "1185360"
  },
  {
    "text": "endpoint to serve those models",
    "start": "1185360",
    "end": "1188740"
  },
  {
    "text": "it worked initially and actually works",
    "start": "1188799",
    "end": "1192020"
  },
  {
    "text": "up to a few hundred models",
    "start": "1192020",
    "end": "1193880"
  },
  {
    "text": "but once we pass that number when the",
    "start": "1193880",
    "end": "1197120"
  },
  {
    "text": "number of models gets larger we start",
    "start": "1197120",
    "end": "1199340"
  },
  {
    "text": "seeing a scaling problem",
    "start": "1199340",
    "end": "1202340"
  },
  {
    "text": "so with Ray",
    "start": "1202340",
    "end": "1204140"
  },
  {
    "text": "we were able to cut down the number of",
    "start": "1204140",
    "end": "1206179"
  },
  {
    "text": "components",
    "start": "1206179",
    "end": "1208220"
  },
  {
    "text": "and more importantly we were able to see",
    "start": "1208220",
    "end": "1211340"
  },
  {
    "text": "a significant boost in the performance",
    "start": "1211340",
    "end": "1213740"
  },
  {
    "text": "in both the training time and also",
    "start": "1213740",
    "end": "1217280"
  },
  {
    "text": "serving latency",
    "start": "1217280",
    "end": "1220299"
  },
  {
    "text": "sharing our peers test numbers",
    "start": "1221720",
    "end": "1225140"
  },
  {
    "text": "you can see that trading time was",
    "start": "1225140",
    "end": "1227179"
  },
  {
    "text": "reduced by about 80 percent and also",
    "start": "1227179",
    "end": "1230660"
  },
  {
    "text": "core style latency was reduced by more",
    "start": "1230660",
    "end": "1233720"
  },
  {
    "text": "than 80 percent the cold start latency",
    "start": "1233720",
    "end": "1235940"
  },
  {
    "text": "is important to us because these models",
    "start": "1235940",
    "end": "1238400"
  },
  {
    "text": "are invoked partially and there will be",
    "start": "1238400",
    "end": "1241280"
  },
  {
    "text": "a lot of model swapping happening",
    "start": "1241280",
    "end": "1244900"
  },
  {
    "text": "some additional benefits that we got",
    "start": "1246380",
    "end": "1248059"
  },
  {
    "text": "from",
    "start": "1248059",
    "end": "1249160"
  },
  {
    "text": "Reserve is the memory efficiency as Shia",
    "start": "1249160",
    "end": "1253580"
  },
  {
    "text": "said",
    "start": "1253580",
    "end": "1255039"
  },
  {
    "text": "multiplexing does a really good job in",
    "start": "1255039",
    "end": "1257600"
  },
  {
    "text": "managing the model loading into replicas",
    "start": "1257600",
    "end": "1261679"
  },
  {
    "text": "we don't have to load the model in every",
    "start": "1261679",
    "end": "1264020"
  },
  {
    "text": "replica but to only one replica",
    "start": "1264020",
    "end": "1267919"
  },
  {
    "text": "also we get the auto scaling out of the",
    "start": "1267919",
    "end": "1270380"
  },
  {
    "text": "box",
    "start": "1270380",
    "end": "1271220"
  },
  {
    "text": "these native solutions to other",
    "start": "1271220",
    "end": "1272660"
  },
  {
    "text": "solutions could provide maybe a better",
    "start": "1272660",
    "end": "1274940"
  },
  {
    "text": "performance by tuning or adding",
    "start": "1274940",
    "end": "1277520"
  },
  {
    "text": "additional engineering resource to it",
    "start": "1277520",
    "end": "1280460"
  },
  {
    "text": "but already just worked out of the box",
    "start": "1280460",
    "end": "1284179"
  },
  {
    "text": "also we could use a single service to",
    "start": "1284179",
    "end": "1286960"
  },
  {
    "text": "serve all these models while this each",
    "start": "1286960",
    "end": "1289760"
  },
  {
    "text": "of the the model is having its own life",
    "start": "1289760",
    "end": "1293179"
  },
  {
    "text": "cycle",
    "start": "1293179",
    "end": "1295400"
  },
  {
    "text": "well the cool thing about Reserve is",
    "start": "1295400",
    "end": "1297260"
  },
  {
    "text": "that it's very easy to integrate with",
    "start": "1297260",
    "end": "1300740"
  },
  {
    "text": "our own code all we have to do is throw",
    "start": "1300740",
    "end": "1303620"
  },
  {
    "text": "in throw a few python decorators to",
    "start": "1303620",
    "end": "1307100"
  },
  {
    "text": "existing fast API implementation",
    "start": "1307100",
    "end": "1310820"
  },
  {
    "text": "with that I'll hand it back to Asian",
    "start": "1310820",
    "end": "1315340"
  },
  {
    "text": "sounds strong so I'm going to do a recap",
    "start": "1317360",
    "end": "1320299"
  },
  {
    "text": "for this talk so reserves support",
    "start": "1320299",
    "end": "1322340"
  },
  {
    "text": "different solutions to support different",
    "start": "1322340",
    "end": "1324080"
  },
  {
    "text": "money model use cases we talk about the",
    "start": "1324080",
    "end": "1326600"
  },
  {
    "text": "model competitions how you boost your",
    "start": "1326600",
    "end": "1328400"
  },
  {
    "text": "Hardware resource with hybrid clusters",
    "start": "1328400",
    "end": "1331159"
  },
  {
    "text": "how to input and scaling your each",
    "start": "1331159",
    "end": "1333860"
  },
  {
    "text": "component but also talk about the multi",
    "start": "1333860",
    "end": "1335900"
  },
  {
    "text": "applications how to simplify your",
    "start": "1335900",
    "end": "1337700"
  },
  {
    "text": "end-to-end model ml life cycles how to",
    "start": "1337700",
    "end": "1340700"
  },
  {
    "text": "manage a different use cases with single",
    "start": "1340700",
    "end": "1342260"
  },
  {
    "text": "clusters and also we talk about the",
    "start": "1342260",
    "end": "1344720"
  },
  {
    "text": "serving large number of models with",
    "start": "1344720",
    "end": "1346880"
  },
  {
    "text": "Reserve multiplexing you can get the 2x",
    "start": "1346880",
    "end": "1349700"
  },
  {
    "text": "performance Boost from the Baseline",
    "start": "1349700",
    "end": "1351700"
  },
  {
    "text": "hopefully I can get all these features",
    "start": "1351700",
    "end": "1354080"
  },
  {
    "text": "inside the reserve 2.7 check out to use",
    "start": "1354080",
    "end": "1357799"
  },
  {
    "text": "it thank you for coming",
    "start": "1357799",
    "end": "1361600"
  },
  {
    "text": "any any questions",
    "start": "1365780",
    "end": "1369100"
  },
  {
    "text": "yeah I think there's two questions one",
    "start": "1388520",
    "end": "1390679"
  },
  {
    "text": "is the model deployment so um",
    "start": "1390679",
    "end": "1393320"
  },
  {
    "text": "in the counter multiplexing model",
    "start": "1393320",
    "end": "1395480"
  },
  {
    "text": "deployments can be deployed on the fly",
    "start": "1395480",
    "end": "1397460"
  },
  {
    "text": "so users specify the model ID and the",
    "start": "1397460",
    "end": "1400100"
  },
  {
    "text": "user specified the model loading",
    "start": "1400100",
    "end": "1401539"
  },
  {
    "text": "functions so with the model ID modeling",
    "start": "1401539",
    "end": "1403760"
  },
  {
    "text": "functions Reserve will run this function",
    "start": "1403760",
    "end": "1405500"
  },
  {
    "text": "to loading loading the model from your",
    "start": "1405500",
    "end": "1407840"
  },
  {
    "text": "discourse S3 and to know which model is",
    "start": "1407840",
    "end": "1411860"
  },
  {
    "text": "located inside the replica so we are",
    "start": "1411860",
    "end": "1414500"
  },
  {
    "text": "broadcast broadcasting the information",
    "start": "1414500",
    "end": "1416059"
  },
  {
    "text": "to the proxy and after loading is done",
    "start": "1416059",
    "end": "1419179"
  },
  {
    "text": "and proxy will know which replicas",
    "start": "1419179",
    "end": "1421280"
  },
  {
    "text": "having these models and we will serving",
    "start": "1421280",
    "end": "1423140"
  },
  {
    "text": "the traffic following this pattern",
    "start": "1423140",
    "end": "1426820"
  },
  {
    "text": "there's only one",
    "start": "1430039",
    "end": "1432200"
  },
  {
    "text": "uh yes that's true",
    "start": "1432200",
    "end": "1435340"
  },
  {
    "text": "yeah the question is uh do we have any",
    "start": "1450580",
    "end": "1453260"
  },
  {
    "text": "uh limit how many applications that we",
    "start": "1453260",
    "end": "1456799"
  },
  {
    "text": "have to support in one cluster right so",
    "start": "1456799",
    "end": "1461000"
  },
  {
    "text": "we don't have specific we are doing some",
    "start": "1461000",
    "end": "1463520"
  },
  {
    "text": "benchmarking with solvent applications",
    "start": "1463520",
    "end": "1466059"
  },
  {
    "text": "it potentially",
    "start": "1466059",
    "end": "1468260"
  },
  {
    "text": "having very little reconciling Overlook",
    "start": "1468260",
    "end": "1471200"
  },
  {
    "text": "reconciling latency in the controller",
    "start": "1471200",
    "end": "1473000"
  },
  {
    "text": "but it can be supported but most likely",
    "start": "1473000",
    "end": "1475340"
  },
  {
    "text": "you also need to we're also testing some",
    "start": "1475340",
    "end": "1478159"
  },
  {
    "text": "Southern nodes so it's also working fine",
    "start": "1478159",
    "end": "1480799"
  },
  {
    "text": "so hopefully it's answer your question",
    "start": "1480799",
    "end": "1482299"
  },
  {
    "text": "but it also depends on your per clusters",
    "start": "1482299",
    "end": "1484280"
  },
  {
    "text": "uh",
    "start": "1484280",
    "end": "1486580"
  },
  {
    "text": "uh any more questions",
    "start": "1488659",
    "end": "1492220"
  },
  {
    "text": "right yes listen better that's true",
    "start": "1497179",
    "end": "1500679"
  },
  {
    "text": "okay that's a good question so so",
    "start": "1500780",
    "end": "1503059"
  },
  {
    "text": "multifacing um we are trying to get a",
    "start": "1503059",
    "end": "1505520"
  },
  {
    "text": "more",
    "start": "1505520",
    "end": "1506500"
  },
  {
    "text": "use cases to be in production so we",
    "start": "1506500",
    "end": "1509960"
  },
  {
    "text": "potentially have some further",
    "start": "1509960",
    "end": "1511400"
  },
  {
    "text": "improvements to make it a general",
    "start": "1511400",
    "end": "1512960"
  },
  {
    "text": "available for example like more memory",
    "start": "1512960",
    "end": "1515539"
  },
  {
    "text": "protections for your multiplexing but",
    "start": "1515539",
    "end": "1518059"
  },
  {
    "text": "right now it's under the beta so yeah",
    "start": "1518059",
    "end": "1522100"
  },
  {
    "text": "any more questions",
    "start": "1525080",
    "end": "1528039"
  },
  {
    "text": "cool thank you thank you very much",
    "start": "1534020",
    "end": "1536680"
  },
  {
    "text": "[Applause]",
    "start": "1536680",
    "end": "1539869"
  }
]