[
  {
    "text": "all right let's get started I'm very pleased to welcome the team from bite dance to give their talk",
    "start": "2600",
    "end": "10880"
  },
  {
    "text": "on building unified largescale data processing pipelines for multimodal models using Ray please join me in a",
    "start": "10880",
    "end": "18680"
  },
  {
    "text": "warm welcome to Shia Hong D and Li",
    "start": "18680",
    "end": "24560"
  },
  {
    "text": "Guang uh hello everyone it's so many people here we are honored to be hello",
    "start": "27760",
    "end": "34040"
  },
  {
    "text": "we are honored to be here to share our topic building UniFi large scale data",
    "start": "34040",
    "end": "39480"
  },
  {
    "text": "processing pants for multimodel using r with my colleagues J",
    "start": "39480",
    "end": "46600"
  },
  {
    "text": "andang in the beginning before we start today's content please allow me to give",
    "start": "49280",
    "end": "56079"
  },
  {
    "text": "Bri background introduction we are from badon C and the data in team our main",
    "start": "56079",
    "end": "62559"
  },
  {
    "text": "responsibility is to build a high performance skill and distributed the",
    "start": "62559",
    "end": "68320"
  },
  {
    "text": "data processing platform we aim to enhance model performance through data",
    "start": "68320",
    "end": "74320"
  },
  {
    "text": "driven approach because we are dealing with three major challenges first we are",
    "start": "74320",
    "end": "82119"
  },
  {
    "text": "face and it potential growth in data volume with each data site reaching",
    "start": "82119",
    "end": "88400"
  },
  {
    "text": "several pabes second we have limited GPU and CPU resources third processing task",
    "start": "88400",
    "end": "97720"
  },
  {
    "text": "are increasingly complex with M STS and certific",
    "start": "97720",
    "end": "103479"
  },
  {
    "text": "algorithms our main objective is to enhance efficiency and minimize resource",
    "start": "103479",
    "end": "110240"
  },
  {
    "text": "consumption re is the onome it can handle large data volum optimize",
    "start": "110240",
    "end": "117039"
  },
  {
    "text": "hetrogeneous result application and flexible Administration compability",
    "start": "117039",
    "end": "123200"
  },
  {
    "text": "leverage recall red data and the result let's dial into three key aspects",
    "start": "123200",
    "end": "131879"
  },
  {
    "text": "audio data processing video data processing and underlying re info I will",
    "start": "131879",
    "end": "138480"
  },
  {
    "text": "share the first section audio data",
    "start": "138480",
    "end": "142760"
  },
  {
    "text": "processing after briefly after Briefly summarize the background now let's into",
    "start": "144480",
    "end": "150360"
  },
  {
    "text": "the detail of our data processing platform and how we help us deal with",
    "start": "150360",
    "end": "155640"
  },
  {
    "text": "the problem we mentioned earlier the architecture of our data processing",
    "start": "155640",
    "end": "161080"
  },
  {
    "text": "platform can be divided into three layers just like you see from the slides",
    "start": "161080",
    "end": "167000"
  },
  {
    "text": "the first layer is the info layer it plays a crucial role in manage and",
    "start": "167000",
    "end": "172879"
  },
  {
    "text": "providing essential storage and the computation resources additionally is responsible",
    "start": "172879",
    "end": "180879"
  },
  {
    "text": "for task scheduling ensure efficient utilize available resources and smooth",
    "start": "180879",
    "end": "188480"
  },
  {
    "text": "execution and overest process the second layer is the self develop data pipeline",
    "start": "188480",
    "end": "195280"
  },
  {
    "text": "layer which is specially crafted to handle the complex data flow and the",
    "start": "195280",
    "end": "200879"
  },
  {
    "text": "performance a series of data Pro transformation and process",
    "start": "200879",
    "end": "207360"
  },
  {
    "text": "tasks the top layer is application",
    "start": "207360",
    "end": "212680"
  },
  {
    "text": "er uh the top layer is the application layer where the processed data is",
    "start": "215040",
    "end": "220599"
  },
  {
    "text": "utilized to power r large speech model such like music generation and we have a",
    "start": "220599",
    "end": "227760"
  },
  {
    "text": "demo after this slides let's zoom in the P layer how we arrate the data",
    "start": "227760",
    "end": "234439"
  },
  {
    "text": "processing P the first concept is note what is",
    "start": "234439",
    "end": "240360"
  },
  {
    "text": "named node is typically present a specific task or operator such as the",
    "start": "240360",
    "end": "247480"
  },
  {
    "text": "fotor operator annotation operator or the duper operator some node need c",
    "start": "247480",
    "end": "255159"
  },
  {
    "text": "resources and some other need GPU",
    "start": "255159",
    "end": "259759"
  },
  {
    "text": "resources the second concept is flow flow is defined as a data or control",
    "start": "261360",
    "end": "267479"
  },
  {
    "text": "transfer relationship between node and is a direction connection a",
    "start": "267479",
    "end": "275039"
  },
  {
    "text": "node can have multiple input flows or upo grow our D is defined by no and flow",
    "start": "275039",
    "end": "282320"
  },
  {
    "text": "and assembled through yam so it's efficient for",
    "start": "282320",
    "end": "288240"
  },
  {
    "text": "us we have redefined a series of common P plants by resembling this common pip",
    "start": "288520",
    "end": "296360"
  },
  {
    "text": "PLS we can meet the data requirement for model training this approach greatly infu our",
    "start": "296360",
    "end": "303639"
  },
  {
    "text": "work efficiency and provide a m flexibility solution for data processing",
    "start": "303639",
    "end": "309720"
  },
  {
    "text": "and the model training so there are diagram of other P plant so it's the",
    "start": "309720",
    "end": "315039"
  },
  {
    "text": "Comm P plans during the initialized plan",
    "start": "315039",
    "end": "321280"
  },
  {
    "text": "implementation we encounter the fall main issue that through the project Pro",
    "start": "321280",
    "end": "326680"
  },
  {
    "text": "progress the first issue is house ability there are difficult to handle L",
    "start": "326680",
    "end": "333199"
  },
  {
    "text": "gr data due to efficient results utilization moreover they are",
    "start": "333199",
    "end": "339240"
  },
  {
    "text": "challenging in skill from a single node to multiple node so it's a difficult",
    "start": "339240",
    "end": "344759"
  },
  {
    "text": "work for us at first the second issue is Task scattering and loading balancing",
    "start": "344759",
    "end": "351360"
  },
  {
    "text": "the complexity of mly managed task allocation has increased leading to an",
    "start": "351360",
    "end": "357919"
  },
  {
    "text": "unreasonable task distribution tion and affect the execution efficiency of the",
    "start": "357919",
    "end": "363919"
  },
  {
    "text": "project the third issue is high ailable and F tolerance you know the AI scenario",
    "start": "363919",
    "end": "372080"
  },
  {
    "text": "is very difficult and the model is so complex is a lack of",
    "start": "372080",
    "end": "377680"
  },
  {
    "text": "automatic me mechanism for FAS retri and no for failure detection as a result",
    "start": "377680",
    "end": "385800"
  },
  {
    "text": "during node Interruption they may be test failure and data loss the last",
    "start": "385800",
    "end": "393160"
  },
  {
    "text": "issue is data transfer and sharing need to mainly Implement efficient data",
    "start": "393160",
    "end": "399280"
  },
  {
    "text": "transfer and synchronization among task and node performance B NE occur and node",
    "start": "399280",
    "end": "406800"
  },
  {
    "text": "compensation increase after in in integration uh",
    "start": "406800",
    "end": "412919"
  },
  {
    "text": "investigating rate we start using rate call compability why can we quickly",
    "start": "412919",
    "end": "418599"
  },
  {
    "text": "transformation from to R solution is also lar R is Pyon friendly and it's not",
    "start": "418599",
    "end": "425560"
  },
  {
    "text": "so hard for us to uh transform the solution to",
    "start": "425560",
    "end": "432639"
  },
  {
    "text": "rate recall Prime robust distribute compute capability however only depend",
    "start": "434720",
    "end": "442759"
  },
  {
    "text": "on on recal is still we still face some issues you can imagine you",
    "start": "442759",
    "end": "450840"
  },
  {
    "text": "just how the distribution Computing capability but the data still show to",
    "start": "450840",
    "end": "458479"
  },
  {
    "text": "cover by yourself so the first issue is data distribution and shutting",
    "start": "458479",
    "end": "463840"
  },
  {
    "text": "management it require manual management of data shutting and distribution which",
    "start": "463840",
    "end": "470599"
  },
  {
    "text": "not only increase compability is not but also increase the risk of",
    "start": "470599",
    "end": "477840"
  },
  {
    "text": "imbalance the second issue is problem of how data reading and",
    "start": "477840",
    "end": "485199"
  },
  {
    "text": "loading it's lock efficient automatic data reading and loading mechanism which",
    "start": "485199",
    "end": "491879"
  },
  {
    "text": "is likely to affect overall efficiency the last issue is lack of",
    "start": "491879",
    "end": "499120"
  },
  {
    "text": "high level data operation functions so we show",
    "start": "499120",
    "end": "504639"
  },
  {
    "text": "to uh CA so many time to deployment the functionality",
    "start": "504639",
    "end": "510120"
  },
  {
    "text": "so re Red Data is apply as the high level data",
    "start": "510120",
    "end": "517800"
  },
  {
    "text": "operation is manually Implement and can give us common data operators is",
    "start": "517800",
    "end": "524959"
  },
  {
    "text": "increase the M",
    "start": "524959",
    "end": "530440"
  },
  {
    "text": "cost to conclude my part you can imagine the other things if someone want to use",
    "start": "533640",
    "end": "541959"
  },
  {
    "text": "the uniz model how you so for the others if we just with recall Red Data we still",
    "start": "541959",
    "end": "549480"
  },
  {
    "text": "have other things to make the model as a service so the other things Is",
    "start": "549480",
    "end": "557920"
  },
  {
    "text": "Res it can help us something is the first one is complexity to model",
    "start": "557920",
    "end": "564480"
  },
  {
    "text": "Implement and the second one is give a suable service",
    "start": "564480",
    "end": "569880"
  },
  {
    "text": "on the one hand for efficient model deployment and service res pro pro",
    "start": "569880",
    "end": "576399"
  },
  {
    "text": "provide easy easy apis and enable model",
    "start": "576399",
    "end": "581440"
  },
  {
    "text": "to be quickly transferred into access service it means we can push our model",
    "start": "581440",
    "end": "588399"
  },
  {
    "text": "efficieny and make it run faster and faster if we have enough data and the",
    "start": "588399",
    "end": "596720"
  },
  {
    "text": "model can have a more performance so it's just Nam the scattering law on the",
    "start": "596720",
    "end": "603040"
  },
  {
    "text": "other hand in term of high available and fall tolerance res have build in high",
    "start": "603040",
    "end": "609560"
  },
  {
    "text": "available and for tance machines and can automatically detect and recover from",
    "start": "609560",
    "end": "616560"
  },
  {
    "text": "the 40 uh 40 node and the insurance stable of the service so it's very",
    "start": "616560",
    "end": "622240"
  },
  {
    "text": "important for our scenarios if you manage so many part and the US should to",
    "start": "622240",
    "end": "628600"
  },
  {
    "text": "scale up your our data processing processing efficiency so the the the",
    "start": "628600",
    "end": "637000"
  },
  {
    "text": "stable is very very important there also a draft of our p",
    "start": "637000",
    "end": "643600"
  },
  {
    "text": "and there is a data block and uh we just use the data data uh R data to get a map",
    "start": "643600",
    "end": "654040"
  },
  {
    "text": "map stop",
    "start": "654040",
    "end": "657480"
  },
  {
    "text": "uh I also draft one page for for for after we use a so it's time pass to my",
    "start": "663079",
    "end": "673720"
  },
  {
    "text": "colleagues thank you sh for the comprehensive introduction to audio data processing by using Ray this is from the",
    "start": "675920",
    "end": "683720"
  },
  {
    "text": "video generation Foundation model team in bance in the next few minutes I will use example to demonstrate how we use",
    "start": "683720",
    "end": "690800"
  },
  {
    "text": "aray to empower our video data processing PIP plan to begin with let me introduce the",
    "start": "690800",
    "end": "698440"
  },
  {
    "text": "background and the problem so our team is building a video generation Foundation models everybody knows the",
    "start": "698440",
    "end": "705399"
  },
  {
    "text": "high volume and high quality video data is the key for the model training however video data is quite",
    "start": "705399",
    "end": "712880"
  },
  {
    "text": "large compared with like a text image and audio and also video data takes more",
    "start": "712880",
    "end": "718839"
  },
  {
    "text": "competition resource and takes more time compared with the other data formats so",
    "start": "718839",
    "end": "724240"
  },
  {
    "text": "for example like uh for videos most of the common operation will be like using FM pack to do video encoding and",
    "start": "724240",
    "end": "731320"
  },
  {
    "text": "decoding it takes some time yeah so for us efficient processing a vast amount of",
    "start": "731320",
    "end": "737720"
  },
  {
    "text": "video data to achieve high throughput and also scalability is a key challenge",
    "start": "737720",
    "end": "743199"
  },
  {
    "text": "for us so let me walk you through and",
    "start": "743199",
    "end": "749519"
  },
  {
    "text": "overview of our video data processing PIP plan basically it have three steps",
    "start": "749519",
    "end": "755040"
  },
  {
    "text": "so first we have like a list of a row videos the Row videos can be several seconds to be several hours the next",
    "start": "755040",
    "end": "763160"
  },
  {
    "text": "step we will highlight video splitter algorithm which will split videos into different clips and each clip can be",
    "start": "763160",
    "end": "770440"
  },
  {
    "text": "several seconds maybe to uh 1 minute something and then next step we have the",
    "start": "770440",
    "end": "776600"
  },
  {
    "text": "video processing algorithm it has two things to do first crop the videos so we",
    "start": "776600",
    "end": "782519"
  },
  {
    "text": "only select the videos we we want and also we will score the videos for",
    "start": "782519",
    "end": "787959"
  },
  {
    "text": "example we run some like a motion processors we run some quality processors to give the scores for these",
    "start": "787959",
    "end": "794600"
  },
  {
    "text": "clips in this process we are store the metadata into our database and we will",
    "start": "794600",
    "end": "800199"
  },
  {
    "text": "uplo upload the copit clips to our object storage for the future usage in",
    "start": "800199",
    "end": "806360"
  },
  {
    "text": "the next step is called packing so in this this step we are using Ray to empower our packing step so basically we",
    "start": "806360",
    "end": "813519"
  },
  {
    "text": "select the clips that we want and then put all the objects put the clips from",
    "start": "813519",
    "end": "820199"
  },
  {
    "text": "the object store and metadata to a pocket file so this pocket file will be used for our training in the next in the",
    "start": "820199",
    "end": "827279"
  },
  {
    "text": "last step so what is the video data packing",
    "start": "827279",
    "end": "832399"
  },
  {
    "text": "let me demonstrate more so basically as I mentioned video data packing is to",
    "start": "832399",
    "end": "837480"
  },
  {
    "text": "store a set of video C lipse within pocket files to facilitate efficient",
    "start": "837480",
    "end": "843000"
  },
  {
    "text": "data management and access the reason why do we need to do this step is also",
    "start": "843000",
    "end": "848639"
  },
  {
    "text": "quite clear assuming you have tons of small files which are stored in the object storage the overhead to load all",
    "start": "848639",
    "end": "855600"
  },
  {
    "text": "the clipse takes lots of time so in our solution we just put everything into",
    "start": "855600",
    "end": "862160"
  },
  {
    "text": "packet files and then in the training stage we will just load this data set directly",
    "start": "862160",
    "end": "869920"
  },
  {
    "text": "so here comes our first solution and or first attempt to using R data to do it",
    "start": "870480",
    "end": "875839"
  },
  {
    "text": "so you can see in this P pipelines the first step is using R data to read from",
    "start": "875839",
    "end": "880920"
  },
  {
    "text": "our database and then do some filterings according to different conditions and",
    "start": "880920",
    "end": "886920"
  },
  {
    "text": "then next we just repartition our R data set into like different blocks for the",
    "start": "886920",
    "end": "893399"
  },
  {
    "text": "future paraly processing the next part is the video processing so we have three",
    "start": "893399",
    "end": "900120"
  },
  {
    "text": "key items the first is download so we download data from the object storage",
    "start": "900120",
    "end": "905320"
  },
  {
    "text": "and then we process the videos using the FM pack so for example encod the video into different formats or decode videos",
    "start": "905320",
    "end": "912759"
  },
  {
    "text": "to together images from video for the future usage lastly we will just upload",
    "start": "912759",
    "end": "918120"
  },
  {
    "text": "the data to the pet and and then run the training so for",
    "start": "918120",
    "end": "924079"
  },
  {
    "text": "these Solutions we just encounter two issues if you come to the training",
    "start": "924079",
    "end": "929440"
  },
  {
    "text": "yesterday you will also feel it so the first issue is that serialize and deserialize binary object especially for",
    "start": "929440",
    "end": "937160"
  },
  {
    "text": "the large ban objects takes sometimes and another thing is that really will spare objects into disk once the object",
    "start": "937160",
    "end": "945360"
  },
  {
    "text": "is full so this one is will be Anatomy for us assuming all the data will be",
    "start": "945360",
    "end": "951079"
  },
  {
    "text": "stored in the disc and you want to read it it takes lots of",
    "start": "951079",
    "end": "956560"
  },
  {
    "text": "time based on our first attempt we figure out it's not maybe it's not a",
    "start": "957040",
    "end": "962440"
  },
  {
    "text": "good idea to store video data or like a image data in ra Object Store especially",
    "start": "962440",
    "end": "968440"
  },
  {
    "text": "in large volume then we come up with another solution so the version two how",
    "start": "968440",
    "end": "974600"
  },
  {
    "text": "about fuse all the operations into single actor to avoid data transfer between different actors and avoid using",
    "start": "974600",
    "end": "982199"
  },
  {
    "text": "the object store so in this diagram you can see we have several threats running",
    "start": "982199",
    "end": "989399"
  },
  {
    "text": "together and in each thread we will download the videos run some video",
    "start": "989399",
    "end": "994560"
  },
  {
    "text": "processing and the next step everything we're running in streaming go to the pet",
    "start": "994560",
    "end": "1000680"
  },
  {
    "text": "file and finally we just upload it to the our database so this solution works pretty",
    "start": "1000680",
    "end": "1007480"
  },
  {
    "text": "well and it can achieve high throughput and also it is scalable so for example",
    "start": "1007480",
    "end": "1012519"
  },
  {
    "text": "if we add more CPU resources the band with the high the host Ro put will also",
    "start": "1012519",
    "end": "1017920"
  },
  {
    "text": "scale up as well in the last two two slides I want to",
    "start": "1017920",
    "end": "1023800"
  },
  {
    "text": "share some lessons we learned and also something that we love about R first thing is about the scalability and",
    "start": "1023800",
    "end": "1029798"
  },
  {
    "text": "flexibility I think sh also mentioned that we can easily move from like a lcal",
    "start": "1029799",
    "end": "1035319"
  },
  {
    "text": "python script to large scale cluster we have tried thousands of workers it works pretty well another thing is suppos a w",
    "start": "1035319",
    "end": "1043520"
  },
  {
    "text": "wide range of workloads so for example video like a video decoding encoding it's support for some like deep learning",
    "start": "1043520",
    "end": "1050880"
  },
  {
    "text": "models we also supports another thing is about user friendly I mean everybody I think in",
    "start": "1050880",
    "end": "1056720"
  },
  {
    "text": "this uh in this uh session uh using python a lot so python friendly is",
    "start": "1056720",
    "end": "1062960"
  },
  {
    "text": "really key for us and it's quite easy to debug lastly repr provides unified and",
    "start": "1062960",
    "end": "1068720"
  },
  {
    "text": "user friend API so for example repr provides rest for API so we can integrated the its rest for API into our",
    "start": "1068720",
    "end": "1075760"
  },
  {
    "text": "own platform to monitor the r job or submitt job finally I just want to share like",
    "start": "1075760",
    "end": "1083320"
  },
  {
    "text": "one tip by using Ray for a while it's called profiling profiling and",
    "start": "1083320",
    "end": "1089960"
  },
  {
    "text": "profiling so profiling and benchmarking for me is always the first thing to do",
    "start": "1089960",
    "end": "1095600"
  },
  {
    "text": "profiling gives me better understanding of how the system performs and it could set a baseline For You especially like",
    "start": "1095600",
    "end": "1102320"
  },
  {
    "text": "in the early stage of the project in this example we are using gra dashboard",
    "start": "1102320",
    "end": "1108360"
  },
  {
    "text": "to to monitor the throughput of our input and output data by monitoring this",
    "start": "1108360",
    "end": "1114039"
  },
  {
    "text": "dashboard we can easily know that how the system is operating and avoid lots",
    "start": "1114039",
    "end": "1119200"
  },
  {
    "text": "of issues so for example a of memory issues disk SP issues or even disk four",
    "start": "1119200",
    "end": "1125559"
  },
  {
    "text": "issues that's uh pretty much for my part uh let's hand over to my",
    "start": "1125559",
    "end": "1132080"
  },
  {
    "text": "colleague thank you very much uh sh and Sh and for sharing the GR insight to the",
    "start": "1133080",
    "end": "1140159"
  },
  {
    "text": "challenge when using When developing a very robust and with scalable audio and",
    "start": "1140159",
    "end": "1145600"
  },
  {
    "text": "video data processing Pipeline and also uh share how they uh leverage capability",
    "start": "1145600",
    "end": "1151600"
  },
  {
    "text": "of recall red data and Reserve to successfully construct a efficient data",
    "start": "1151600",
    "end": "1157760"
  },
  {
    "text": "processing pipelines uh within byens so next I'm going to talk about uh the",
    "start": "1157760",
    "end": "1163480"
  },
  {
    "text": "underlying r infrastructures that power a number of user cases uh production user cases at scale uh at by Dan so at",
    "start": "1163480",
    "end": "1172840"
  },
  {
    "text": "by Dan we are leveraging rate in multiple user cases uh including but not",
    "start": "1172840",
    "end": "1178240"
  },
  {
    "text": "limited to audio video data processing we're also using R for I HF uh with a",
    "start": "1178240",
    "end": "1184440"
  },
  {
    "text": "pretty sizable deployment uh we're going to uh talk about that in another uh section another talk uh right after this",
    "start": "1184440",
    "end": "1192000"
  },
  {
    "text": "one at 2 p uh 2:30 PMS uh by uh by Dan uh researchers and at by then we are",
    "start": "1192000",
    "end": "1199360"
  },
  {
    "text": "leveraging R race Pyon native apis as Shong and CH mentioned multiple times",
    "start": "1199360",
    "end": "1206360"
  },
  {
    "text": "and uh also really supports uh very flexible uh orchestrations and the",
    "start": "1206360",
    "end": "1211760"
  },
  {
    "text": "scheduling capability of the het heterogeneous uh resources including",
    "start": "1211760",
    "end": "1216799"
  },
  {
    "text": "various models of gpus and CPUs is awesome we also love raise acing comp uh",
    "start": "1216799",
    "end": "1223559"
  },
  {
    "text": "computations and it's flexible multi Rose d orchestration which are Super",
    "start": "1223559",
    "end": "1229760"
  },
  {
    "text": "desirable to build a large scale high performance AIML infrastructure but like",
    "start": "1229760",
    "end": "1235880"
  },
  {
    "text": "many other building many other uh large scale distributed systems we also face",
    "start": "1235880",
    "end": "1241640"
  },
  {
    "text": "tremendous of challenges uh including manying production issues when scaling",
    "start": "1241640",
    "end": "1246799"
  },
  {
    "text": "the production workload to thousands of nodes and Beyond and in the next few slides I'm going to dip die a little bit",
    "start": "1246799",
    "end": "1253440"
  },
  {
    "text": "we would like to share what we have learned dur during this journey and with the broader R",
    "start": "1253440",
    "end": "1259960"
  },
  {
    "text": "community so as you may know the large language models and many others generative AIS data processing task are",
    "start": "1259960",
    "end": "1268640"
  },
  {
    "text": "are typical handle off lines due to the huge resource demands and Torance of",
    "start": "1268640",
    "end": "1274159"
  },
  {
    "text": "relative long processing time but it all this this is all about cost but in order",
    "start": "1274159",
    "end": "1280799"
  },
  {
    "text": "to further reduce cost we use a significant amount of V volatiles",
    "start": "1280799",
    "end": "1286400"
  },
  {
    "text": "kubernetes ports to run those data processing task and those kubernetes",
    "start": "1286400",
    "end": "1291640"
  },
  {
    "text": "ports can be preempted at any times and R worker notes running on those",
    "start": "1291640",
    "end": "1299919"
  },
  {
    "text": "Port can be added or removed as needed but at the same time",
    "start": "1299919",
    "end": "1306279"
  },
  {
    "text": "we also want to ensure that the removal of worker nodes doesn't interrupt the",
    "start": "1306279",
    "end": "1311840"
  },
  {
    "text": "normal execution of any uh data processing task and the task should not",
    "start": "1311840",
    "end": "1317799"
  },
  {
    "text": "fail or be interrupted for example if a task running on 100 gpus for example and",
    "start": "1317799",
    "end": "1324480"
  },
  {
    "text": "40 of them are preemptive we expect the remaining 60 GPU to continue operating",
    "start": "1324480",
    "end": "1332240"
  },
  {
    "text": "efficiently with high utilization so this but this pose a very significant",
    "start": "1332240",
    "end": "1337919"
  },
  {
    "text": "challenge to us uh if you're familiar with the recall recall does provide a very robust actor and task recover",
    "start": "1337919",
    "end": "1345520"
  },
  {
    "text": "mechanism but in the current uh red dat design design when an actor crashes the",
    "start": "1345520",
    "end": "1351520"
  },
  {
    "text": "system must wait for the actor to restart and it will stay in pending status until the request resource is",
    "start": "1351520",
    "end": "1360640"
  },
  {
    "text": "surprised and if the resource are insufficient then the actor Will REM in",
    "start": "1360640",
    "end": "1366679"
  },
  {
    "text": "pending status so in the next slides we're going to uh uh propose some",
    "start": "1366679",
    "end": "1372720"
  },
  {
    "text": "enhancement to Red Data to further address those issues and the first in uh",
    "start": "1372720",
    "end": "1378919"
  },
  {
    "text": "enhancement we propose is Task reassignment in Red Data schedule and in",
    "start": "1378919",
    "end": "1384440"
  },
  {
    "text": "simple terms this means red disputing the task of the fail actors in the actor",
    "start": "1384440",
    "end": "1389559"
  },
  {
    "text": "pools to other AV variable actors and here is a little bit technical detail",
    "start": "1389559",
    "end": "1394880"
  },
  {
    "text": "here we set the max restart parameters of actor in the actor pool to zero this",
    "start": "1394880",
    "end": "1400320"
  },
  {
    "text": "means that the Red Data will fully control the life cycle of an actor so",
    "start": "1400320",
    "end": "1405960"
  },
  {
    "text": "when an actor crashes the rec will no longer uh maintain its life cycle and",
    "start": "1405960",
    "end": "1412000"
  },
  {
    "text": "will no longer attempt to restart the actor so when the driver detect a actors",
    "start": "1412000",
    "end": "1420000"
  },
  {
    "text": "of normal exit any unfinished pre uh any unfinished uh uh task we're going to uh",
    "start": "1420000",
    "end": "1428559"
  },
  {
    "text": "that previously uh previously assigned to the actor are re assign to other",
    "start": "1428559",
    "end": "1433840"
  },
  {
    "text": "active running actors and if the red dat scheduler uh and the re data scheduler",
    "start": "1433840",
    "end": "1440559"
  },
  {
    "text": "will create a new map actors and if there's no uh a variable resource the",
    "start": "1440559",
    "end": "1446760"
  },
  {
    "text": "actor will be in a pending stat but this does not Brock any execution of the red dat task so once a new resources are",
    "start": "1446760",
    "end": "1455799"
  },
  {
    "text": "added the actor will move to the running State and rejoy the uh actor",
    "start": "1455799",
    "end": "1463120"
  },
  {
    "text": "pool the this uh the proposed task uh reassignment is pretty straightforward",
    "start": "1463120",
    "end": "1469799"
  },
  {
    "text": "but it may introduce a new issues remember the little uh technical detail",
    "start": "1469799",
    "end": "1475120"
  },
  {
    "text": "we mentioned in the previous slides as part of the task reassignment enhancement we set the max restart uh",
    "start": "1475120",
    "end": "1483120"
  },
  {
    "text": "parameter to zero so when but when a object is lost we can no longer rely on",
    "start": "1483120",
    "end": "1490440"
  },
  {
    "text": "recourse linkage reconstruction capability to rebuild the object so for",
    "start": "1490440",
    "end": "1496640"
  },
  {
    "text": "example in the diagram on the right right uh the output of the map operation",
    "start": "1496640",
    "end": "1502600"
  },
  {
    "text": "one serve as the input of the map operation two if the nose running map",
    "start": "1502600",
    "end": "1509399"
  },
  {
    "text": "operation one crash the output object is actually is lost so when a map to map",
    "start": "1509399",
    "end": "1517080"
  },
  {
    "text": "operation to uh try to read the object it cannot it will encounter various of",
    "start": "1517080",
    "end": "1523600"
  },
  {
    "text": "object loss error so to adjust this issue issue uh",
    "start": "1523600",
    "end": "1529240"
  },
  {
    "text": "we further propos the second enhancement uh by asking R data to manage the",
    "start": "1529240",
    "end": "1535480"
  },
  {
    "text": "linkage between the operator input and the operator output instead of rely on recall to do that so the read datat",
    "start": "1535480",
    "end": "1543320"
  },
  {
    "text": "schedule in this case we're going to uh uh introduce a linkage related data",
    "start": "1543320",
    "end": "1548600"
  },
  {
    "text": "structure and E each uh operator has a table that record the relation between",
    "start": "1548600",
    "end": "1555360"
  },
  {
    "text": "the input and output datas where the key is the output reference and the input uh",
    "start": "1555360",
    "end": "1561080"
  },
  {
    "text": "and the value is the input reference this allow the easy look up of the input data from the alut data so the when the",
    "start": "1561080",
    "end": "1569200"
  },
  {
    "text": "actor pool uh of the downstream operator in this case is the map B encounter an",
    "start": "1569200",
    "end": "1575480"
  },
  {
    "text": "object lost error it will indicate immediate indicate that the upst strring output map a has been lost so the Red",
    "start": "1575480",
    "end": "1583840"
  },
  {
    "text": "Data schedule here will use the linkage table to find the input and uh recompute",
    "start": "1583840",
    "end": "1589520"
  },
  {
    "text": "the output and there's one trick here we uh have one C uh we set up a clean up",
    "start": "1589520",
    "end": "1595120"
  },
  {
    "text": "mechanism so once a Brock has passed through all the operation operators and",
    "start": "1595120",
    "end": "1600279"
  },
  {
    "text": "produce an output we will the the Upstream linkage entry will be deleted",
    "start": "1600279",
    "end": "1606360"
  },
  {
    "text": "automatically but currently there's one limitation the solution we Pro propose here so far is support one to one",
    "start": "1606360",
    "end": "1614000"
  },
  {
    "text": "operators type of operators uh but we do believe the aut wall operator type can",
    "start": "1614000",
    "end": "1620600"
  },
  {
    "text": "be supported as well in gr data all right so put it into a broader context",
    "start": "1620600",
    "end": "1626640"
  },
  {
    "text": "so uh most red dat user work with the stable resources uh so they may not",
    "start": "1626640",
    "end": "1633159"
  },
  {
    "text": "encounter the issue uh we just described earlier however we believe that this issue is quite Universal uh basically if",
    "start": "1633159",
    "end": "1640799"
  },
  {
    "text": "you trying to run a r data on for example uh uh any unre reliable",
    "start": "1640799",
    "end": "1647240"
  },
  {
    "text": "resources uh you're going to hit the same issue so we believe this uh",
    "start": "1647240",
    "end": "1652720"
  },
  {
    "text": "enhancement can further uh uh provid a higher reliability and stabilities for",
    "start": "1652720",
    "end": "1658919"
  },
  {
    "text": "Red Data okay let's putting the technical details aside let's enjoy some aigc",
    "start": "1658919",
    "end": "1666200"
  },
  {
    "text": "music and Power by Ray uh and the the music is actually generated uh uh for",
    "start": "1666200",
    "end": "1672960"
  },
  {
    "text": "resubmit 2024 and the music generation data is from our data processing peles",
    "start": "1672960",
    "end": "1679080"
  },
  {
    "text": "which uh running on Ray Let's Get It Started one",
    "start": "1679080",
    "end": "1685398"
  },
  {
    "text": "second so actually we have some",
    "start": "1689799",
    "end": "1694760"
  },
  {
    "text": "uh all I'm going to skip because we are running up time so",
    "start": "1699000",
    "end": "1705799"
  },
  {
    "text": "all all right all right uh the the bident team here uh we deep di into uh",
    "start": "1705799",
    "end": "1711919"
  },
  {
    "text": "how to scale uh scaling data processing pipelines uh to parabis scale",
    "start": "1711919",
    "end": "1718880"
  },
  {
    "text": "and and we we we believe R can provide a great solution to handle a large volumes",
    "start": "1718880",
    "end": "1725880"
  },
  {
    "text": "data volumes optimize the heterogeneous resource allocation and also provide a flexible uh orchestration and uh",
    "start": "1725880",
    "end": "1735279"
  },
  {
    "text": "with uh deep that into the r data uh in infra uh by Den how to handle the large",
    "start": "1735279",
    "end": "1742200"
  },
  {
    "text": "scale offline data processing we share Journey on running red data on unstable kubernetes ports and we we also prop uh",
    "start": "1742200",
    "end": "1750679"
  },
  {
    "text": "propose enhancement in Red Data schedule to uh handle task reassignment and also",
    "start": "1750679",
    "end": "1756600"
  },
  {
    "text": "manage the object linkage without further Ado I think thank you very much for your attentions",
    "start": "1756600",
    "end": "1762799"
  },
  {
    "text": "and would like to take questions and also one more things uh by Dan is active hiring here so we put a lot of QR code",
    "start": "1762799",
    "end": "1769399"
  },
  {
    "text": "feel free to scan it basically we we have uh there are three teams here uh audio Pro uh data processing team video",
    "start": "1769399",
    "end": "1777120"
  },
  {
    "text": "data processing team and also the tech infra team uh here in the US so a lot of opening feel free to scan and apply",
    "start": "1777120",
    "end": "1783919"
  },
  {
    "text": "thank you very [Applause] much to ask a question please come to",
    "start": "1783919",
    "end": "1791320"
  },
  {
    "text": "the mic so we can get it on the recording there's a lot of people so um just queue up there thanks",
    "start": "1791320",
    "end": "1799360"
  },
  {
    "text": "hello so I have one question uh so have you thought of like upstreaming this",
    "start": "1799360",
    "end": "1804519"
  },
  {
    "text": "feature to to Ray uh open source Library so uh that's good question actually we",
    "start": "1804519",
    "end": "1810919"
  },
  {
    "text": "uh discussed with the Red Data uh team at any scale I think there's a bunch of",
    "start": "1810919",
    "end": "1816840"
  },
  {
    "text": "a good thing we can follow ups for example we can defy uh uh interface first in uh the ray uh",
    "start": "1816840",
    "end": "1825840"
  },
  {
    "text": "open source upstream and then allow different implementation uh by different",
    "start": "1825840",
    "end": "1831640"
  },
  {
    "text": "company for example and then U later on we probably can uh further discuss and",
    "start": "1831640",
    "end": "1836880"
  },
  {
    "text": "with the any scale team to see if we can contribute back to the Upstream yeah so that would be the two step uh we're",
    "start": "1836880",
    "end": "1842480"
  },
  {
    "text": "going to take thank you so another question is uh regarding the fusion of operator Fusion so when you do the",
    "start": "1842480",
    "end": "1849440"
  },
  {
    "text": "operator Fusion did you do it at the application code level or like a underlying infra Level under uh underly",
    "start": "1849440",
    "end": "1856039"
  },
  {
    "text": "uh and then when you do that uh it's more like element wise operators or can you also F like a uh aggregation and",
    "start": "1856039",
    "end": "1863519"
  },
  {
    "text": "grouping operators uh right now we only support as mentioned the limitation we only support one to one operator but we",
    "start": "1863519",
    "end": "1870519"
  },
  {
    "text": "do believe the all to operator can be supported among others operators but yeah but that's good one yeah thank you",
    "start": "1870519",
    "end": "1876480"
  },
  {
    "text": "thank you uh hello uh great talks so I have a question related to When you mention",
    "start": "1876480",
    "end": "1882600"
  },
  {
    "text": "that your cluster going to scale to like for example s of nose right um have you encountered anything related to GCS um",
    "start": "1882600",
    "end": "1890240"
  },
  {
    "text": "you know the scalability and stability issues um that you know GCS constantly",
    "start": "1890240",
    "end": "1895880"
  },
  {
    "text": "drop off and things like that and how do you deal with that yeah goal schedule",
    "start": "1895880",
    "end": "1902279"
  },
  {
    "text": "have you heard this looks like our uh client which is the data and uh the",
    "start": "1902279",
    "end": "1907880"
  },
  {
    "text": "audio and video proc data processing they haven't hit that issue yet okay all right thank",
    "start": "1907880",
    "end": "1915200"
  },
  {
    "text": "you I uh so related skill ility question um so you run uh jobs with th thousands",
    "start": "1915440",
    "end": "1922760"
  },
  {
    "text": "of nodes on kubernetes infrastructure um how do you um address the scalability",
    "start": "1922760",
    "end": "1930159"
  },
  {
    "text": "challenges of kubernetes itself so yeah uh that one we build we",
    "start": "1930159",
    "end": "1935720"
  },
  {
    "text": "we are building r on top of kubernetes as shared previously right so so far I",
    "start": "1935720",
    "end": "1941000"
  },
  {
    "text": "think the kubernetes the document limitation is about 15K notes right so uh that's",
    "start": "1941000",
    "end": "1948440"
  },
  {
    "text": "I think so far we haven't hit that limit yet that's what I can share so far makes",
    "start": "1948440",
    "end": "1953840"
  },
  {
    "text": "sense and uh one more question so uh the um so the ray head node is also",
    "start": "1953840",
    "end": "1961679"
  },
  {
    "text": "running in a kubernetes pod that's uh inherently volatile um so how do you",
    "start": "1961679",
    "end": "1969559"
  },
  {
    "text": "think about um so fa tolerance of the of the",
    "start": "1969559",
    "end": "1974760"
  },
  {
    "text": "whole system or failures of the of the head node the can you repeat that question yeah",
    "start": "1974760",
    "end": "1980720"
  },
  {
    "text": "how do you think about potential failures of the ray head pod the me the RE we have uh I think as",
    "start": "1980720",
    "end": "1990080"
  },
  {
    "text": "far as I know is Will rely on uh raise uh ha ability a lot in this at this time",
    "start": "1990080",
    "end": "1996279"
  },
  {
    "text": "yeah and also kubernetes H yeah Mak sense",
    "start": "1996279",
    "end": "2001919"
  },
  {
    "text": "thanks hello uh great presentation I have a question regarding scalability as well so not everyone had the luxury to",
    "start": "2002760",
    "end": "2009519"
  },
  {
    "text": "have a th node in a single place uh have you guys ever thought about deploying r on multiple clusters is this something",
    "start": "2009519",
    "end": "2016360"
  },
  {
    "text": "you have tried can you want to take this one",
    "start": "2016360",
    "end": "2022760"
  },
  {
    "text": "oh yeah yeah so first I'm the user of the aray and our like Ray data infra",
    "start": "2022760",
    "end": "2029399"
  },
  {
    "text": "team they will set the aray across different cluster but each cluster is running separately yes Does this answer",
    "start": "2029399",
    "end": "2036200"
  },
  {
    "text": "your question yeah yes but like is there any uh coordination between clusters you guys have encountered or is it probably",
    "start": "2036200",
    "end": "2043039"
  },
  {
    "text": "fine because you're just sharding the data properly uh so each class is running",
    "start": "2043039",
    "end": "2048398"
  },
  {
    "text": "like separately so there's no like communication between them I see does it ever come to you there is a",
    "start": "2048399",
    "end": "2054000"
  },
  {
    "text": "communication need or that's not a usual case uh so far so good because basically",
    "start": "2054000",
    "end": "2059520"
  },
  {
    "text": "we just split our data into different parts and send the data to each cluster to process it yeah gotcha thank you",
    "start": "2059520",
    "end": "2066560"
  },
  {
    "text": "thank you yeah",
    "start": "2066560",
    "end": "2069720"
  }
]