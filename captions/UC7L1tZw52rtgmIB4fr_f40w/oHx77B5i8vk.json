[
  {
    "start": "0",
    "end": "52000"
  },
  {
    "text": "hi everyone I'm AE uh I lead the race serve team here at any skill and with me today I have NE uh and Ryan over there",
    "start": "2760",
    "end": "11080"
  },
  {
    "text": "who will be talking to you about Nvidia and Triton as well um so for a long time",
    "start": "11080",
    "end": "16600"
  },
  {
    "text": "people have been asking us what framework should I use Triton versus Ray serve what are the benefits of each uh",
    "start": "16600",
    "end": "23840"
  },
  {
    "text": "and this year we work together with the Nvidia folks to bring to you an integration so you don't have to choose",
    "start": "23840",
    "end": "30000"
  },
  {
    "text": "anymore you can get the best of both world with the performance efficiencies of Triton as well as the scaling and",
    "start": "30000",
    "end": "37160"
  },
  {
    "text": "flexibility of Ray serve so um for this talk we'll kind of go over the high",
    "start": "37160",
    "end": "42920"
  },
  {
    "text": "level architecture of both RAC serve and Triton and then talk about the integration how it was made possible and",
    "start": "42920",
    "end": "49079"
  },
  {
    "text": "what are the benefits for all of you guys so first I'll cover Ray serves",
    "start": "49079",
    "end": "56280"
  },
  {
    "start": "52000",
    "end": "315000"
  },
  {
    "text": "architecture uh our original motivation behind rayer was to build a very flexible and scalable framework for",
    "start": "56280",
    "end": "63120"
  },
  {
    "text": "online inference uh so what does that mean we wanted to make it very simple for you to go to production and then",
    "start": "63120",
    "end": "70119"
  },
  {
    "text": "also handle things like Autos scaling and all the infrastructure complexities out of the box uh so the one primary",
    "start": "70119",
    "end": "78600"
  },
  {
    "text": "thing for racer was we wanted for it to be python native so just with this couple of simple decorators you could go",
    "start": "78600",
    "end": "85560"
  },
  {
    "text": "from development to production very easily uh and then you could not just just deploy ml models but a full ml",
    "start": "85560",
    "end": "93040"
  },
  {
    "text": "application so that can contain your business logic many different models chain together and you can do this in",
    "start": "93040",
    "end": "99960"
  },
  {
    "text": "one system with just simple python decorators so that was the Simplicity",
    "start": "99960",
    "end": "105719"
  },
  {
    "text": "was the big Focus from the beginning the other big point was how do we construct",
    "start": "105719",
    "end": "111200"
  },
  {
    "text": "these like complicated multimodel type of applications very efficiently uh so",
    "start": "111200",
    "end": "117920"
  },
  {
    "text": "with racer we added patterns such as model composition and model multiplexing so for example if you're building a",
    "start": "117920",
    "end": "124439"
  },
  {
    "text": "recommendation system where uh you might have some business logic and then you might have an ensemble of models like",
    "start": "124439",
    "end": "130520"
  },
  {
    "text": "collaborative filtering some multiarm Bandit and so on you can declare all of these in one application and then scale",
    "start": "130520",
    "end": "137560"
  },
  {
    "text": "each of these components independently and they can even take different resources so maybe your business logic",
    "start": "137560",
    "end": "143560"
  },
  {
    "text": "can just run on CPUs but your collaborative filtering model needs a GPU and maybe your multiarm it needs a",
    "start": "143560",
    "end": "150640"
  },
  {
    "text": "different type of GPU and you can do all of those together within Ray serve and",
    "start": "150640",
    "end": "156000"
  },
  {
    "text": "then scale these independently within the same service so you get kind of these scaling efficiencies while also",
    "start": "156000",
    "end": "162440"
  },
  {
    "text": "better operational maintenance over time um and the other aspect that's really",
    "start": "162440",
    "end": "168480"
  },
  {
    "text": "cool is you can also do fractional resourcing so if you have something that's very lightweight and just takes",
    "start": "168480",
    "end": "174519"
  },
  {
    "text": "half of a GPU you can declare a deployment that does half of a GPU so uh",
    "start": "174519",
    "end": "180239"
  },
  {
    "text": "now Ray serve is in production and used by many large companies and many data scientists ml engineers and ml devops",
    "start": "180239",
    "end": "187720"
  },
  {
    "text": "folks love it to take ml models to production",
    "start": "187720",
    "end": "192760"
  },
  {
    "text": "quickly so let's cover some high level Concepts in RAC serve the deployment is",
    "start": "192799",
    "end": "198360"
  },
  {
    "text": "the central component in in Ray serve and that is the unit where you can",
    "start": "198360",
    "end": "203519"
  },
  {
    "text": "declare your resources and scale and um as well as the Autos scaling",
    "start": "203519",
    "end": "209799"
  },
  {
    "text": "configuration so uh a deployment can consist of ml models or business logic",
    "start": "209799",
    "end": "215400"
  },
  {
    "text": "and then each deployment will scale in units of replicas and these replicas could be located on different nodes in",
    "start": "215400",
    "end": "221319"
  },
  {
    "text": "your cluster um and not all nodes have to have all kinds of deployment replicas an application in Ray serve can",
    "start": "221319",
    "end": "228840"
  },
  {
    "text": "consist of one or more deployments so this is where you can like put together your recommendation system with some",
    "start": "228840",
    "end": "234680"
  },
  {
    "text": "fast API Ingress and Then followed by a bunch of different deployments that get called through the inference um the",
    "start": "234680",
    "end": "241560"
  },
  {
    "text": "inference call itself now since like all these different deployments and replicas",
    "start": "241560",
    "end": "246959"
  },
  {
    "text": "are not homogene homogeneously located on all the nodes you need some component",
    "start": "246959",
    "end": "253159"
  },
  {
    "text": "which can route to different nodes and different replicas for this we have this",
    "start": "253159",
    "end": "258239"
  },
  {
    "text": "concept of uh proxy whether it's HTTP or grpc um and these can do the load",
    "start": "258239",
    "end": "264800"
  },
  {
    "text": "balancing across the entire cluster and then finally we have this",
    "start": "264800",
    "end": "270960"
  },
  {
    "text": "component that will actually handle the auto scaling and life cycle management of these replicas across the cluster",
    "start": "270960",
    "end": "277600"
  },
  {
    "text": "this is called the serve controller which runs on the head node so each replica is constantly pushing its",
    "start": "277600",
    "end": "283280"
  },
  {
    "text": "metrics to the autoscaler in the serf controller and then as the serve controller sees the requests are kind of",
    "start": "283280",
    "end": "290800"
  },
  {
    "text": "piling up or uh the request rate is higher than your target then it will scale the replicas up and vice versa if",
    "start": "290800",
    "end": "298280"
  },
  {
    "text": "it needs to scale down so this is kind of the high level architecture of racer now I'll uh ask n",
    "start": "298280",
    "end": "305479"
  },
  {
    "text": "to come on stage and talk about the architecture of Triton and finally Ryan will go over how we made the integration",
    "start": "305479",
    "end": "314280"
  },
  {
    "start": "315000",
    "end": "582000"
  },
  {
    "text": "possible uh thank you actually for that wonderful introduction um and as you mentioned my name is NE and I'm going to",
    "start": "321639",
    "end": "328039"
  },
  {
    "text": "go over a little bit of the Triton INF",
    "start": "328039",
    "end": "332680"
  },
  {
    "text": "server okay um so there's basically three main things I want you to take",
    "start": "334759",
    "end": "339960"
  },
  {
    "text": "away about the Triton inference server um first that it's part of a I keep",
    "start": "339960",
    "end": "345120"
  },
  {
    "text": "expecting to be behind me but it's not um first that it's part of an overall inferencing stack from Nvidia right that",
    "start": "345120",
    "end": "351280"
  },
  {
    "text": "obviously starts at the hardware accelerators uh but also includes um optimized run times U Cuda as well um",
    "start": "351280",
    "end": "359400"
  },
  {
    "text": "and then then we get to the model serving part right so again it's part of a larger stack the second thing um it's",
    "start": "359400",
    "end": "364880"
  },
  {
    "text": "all about trying to get the most out of your gpus right so we have a lot of different batching techniques that's",
    "start": "364880",
    "end": "370720"
  },
  {
    "text": "really focused on trying to really um focus on single node performance um",
    "start": "370720",
    "end": "375840"
  },
  {
    "text": "getting the most out of uh your GPU utilization and the final thing is that it's really about taking your models and",
    "start": "375840",
    "end": "382840"
  },
  {
    "text": "making them easy to deploy in an optimized way uh into your applications",
    "start": "382840",
    "end": "388440"
  },
  {
    "text": "and it's the last part that I'm going to talk a little bit more about how we achieve that through a series of",
    "start": "388440",
    "end": "395680"
  },
  {
    "text": "abstractions so at the lowest level um we have what we call the Triton backends um these are essentially optimized run",
    "start": "396599",
    "end": "403800"
  },
  {
    "text": "times uh for different major Frameworks things like optimized versions of tensor",
    "start": "403800",
    "end": "408840"
  },
  {
    "text": "rtlm VM um onx runtime even open Veno for running on CPUs um the major thing",
    "start": "408840",
    "end": "416879"
  },
  {
    "text": "here to remember is that this is provides the latest optimized libraries for NVIDIA Hardware as well as other",
    "start": "416879",
    "end": "422639"
  },
  {
    "text": "Hardware these are community community maintained so we work with other uh teams and other um the framework",
    "start": "422639",
    "end": "429000"
  },
  {
    "text": "maintainers to make sure that they are optimized um and the next thing is that it essentially gives you a unified API",
    "start": "429000",
    "end": "436440"
  },
  {
    "text": "that abstracts the Frameworks and the models from the rest of the system so",
    "start": "436440",
    "end": "442720"
  },
  {
    "text": "Triton is a multi framework system um interacting with an onx model looks the same as interacting with an open V model",
    "start": "442720",
    "end": "449680"
  },
  {
    "text": "is looks the same as um operating with the tensorrt model and we really like",
    "start": "449680",
    "end": "455440"
  },
  {
    "text": "that flexibility because it allows you to easily change models and change Frameworks easily so once you've built",
    "start": "455440",
    "end": "461120"
  },
  {
    "text": "your entire stack let's say for stable diffusion which will show you later um you could easily move from a pi torch",
    "start": "461120",
    "end": "466440"
  },
  {
    "text": "model to a tensorrt model and not have to change anything about the rest of your",
    "start": "466440",
    "end": "472120"
  },
  {
    "text": "stack next piece of how we do that is the Triton core so as I mentioned the",
    "start": "472280",
    "end": "477680"
  },
  {
    "text": "back ends are really wrappers and optimizations around Frameworks the Triton core is kind of the middle piece",
    "start": "477680",
    "end": "483759"
  },
  {
    "text": "right it sits in between an application um and the back ends so it uh implements",
    "start": "483759",
    "end": "489680"
  },
  {
    "text": "various framework independent uh optimizations such as Dynamic batching",
    "start": "489680",
    "end": "495199"
  },
  {
    "text": "uh such as pipelining between different um models um really focused on zeroc copy memory making sure that uh the",
    "start": "495199",
    "end": "502360"
  },
  {
    "text": "memory transfers don't go back to the CPU if you don't need to right even if you're going across framework we can",
    "start": "502360",
    "end": "508039"
  },
  {
    "text": "make sure that um we use a a zero copy between let's say a p torch model and a",
    "start": "508039",
    "end": "513080"
  },
  {
    "text": "tensor RT model um it also offers State inversion management so you can update",
    "start": "513080",
    "end": "518959"
  },
  {
    "text": "your models on the fly so we have uh apis around that um the other nice thing",
    "start": "518959",
    "end": "524440"
  },
  {
    "text": "is it really provides a common set of metrics and tracing so again being framework agnostic uh allows you to",
    "start": "524440",
    "end": "531360"
  },
  {
    "text": "build applications that auto scale uh with a common set of metrics if you change your model your metrics don't",
    "start": "531360",
    "end": "537160"
  },
  {
    "text": "have to change and neither does your Autos scaling um",
    "start": "537160",
    "end": "541839"
  },
  {
    "text": "strategies final piece at the very front uh is the Triton front end um and as I",
    "start": "542360",
    "end": "547480"
  },
  {
    "text": "mentioned our whole goal is to really make it easy to take optimized models optimized Frameworks and then integrate",
    "start": "547480",
    "end": "553360"
  },
  {
    "text": "them into your larger application and that's where the front end sits right so it's kind of a customizable piece we",
    "start": "553360",
    "end": "559320"
  },
  {
    "text": "offer out of the box front ends for kerve v2 which is kind of a microservice oriented API we have a generate endpoint",
    "start": "559320",
    "end": "566399"
  },
  {
    "text": "that's more text focused we also offer front ends for sage Aker in vertex AI as well and there's a um fairly easy well-",
    "start": "566399",
    "end": "574680"
  },
  {
    "text": "defined interface to write custom front ends and that's actually one of the things that uh Ryan's going to talk a",
    "start": "574680",
    "end": "580600"
  },
  {
    "text": "little bit more about so I'm going to let uh Ryan come up and now talk about how we really got",
    "start": "580600",
    "end": "586839"
  },
  {
    "start": "582000",
    "end": "675000"
  },
  {
    "text": "the best of both of these by",
    "start": "586839",
    "end": "589920"
  },
  {
    "text": "integration so thanks NE uh yeah so o and Ne gave you kind of a highlight",
    "start": "593959",
    "end": "599640"
  },
  {
    "text": "architectural overview of both racer and Triton and for those of you who may be familiar with one but not the other um",
    "start": "599640",
    "end": "606120"
  },
  {
    "text": "I'm going to do a bit of a deep dive on how they actually integrate at the technical or application Level so let's",
    "start": "606120",
    "end": "612720"
  },
  {
    "text": "start off with the architectural diagrams you saw before um essentially on the top we have the racer",
    "start": "612720",
    "end": "618040"
  },
  {
    "text": "architecture that o spoke about and on the bottom you have the Triton architecture that NE spoke about and the main kind of connection layer here that",
    "start": "618040",
    "end": "624360"
  },
  {
    "text": "we're looking at is a ray serves notion of a deployment and how TR interacts with that is through Triton's python",
    "start": "624360",
    "end": "631120"
  },
  {
    "text": "apis um when you combine these two things you get the best of both worlds choosing sort of what you want from each",
    "start": "631120",
    "end": "637519"
  },
  {
    "text": "so from racer we like the python first python native approach as well as the easy load balancing and scaling that you",
    "start": "637519",
    "end": "643720"
  },
  {
    "text": "get through things just like a simple decorator and from Triton we like the things such as the optimized backends",
    "start": "643720",
    "end": "649320"
  },
  {
    "text": "the common interface and the model repository that you can kind of take using Triton Standalone using racer",
    "start": "649320",
    "end": "655800"
  },
  {
    "text": "Standalone join them Etc um and when you marry these two things through this uh",
    "start": "655800",
    "end": "661839"
  },
  {
    "text": "dotted Green Line uh you get basically the fully compatible with Ray seres Notions of model composition and these",
    "start": "661839",
    "end": "668040"
  },
  {
    "text": "distributor pipelines as well as Triton in process Notions of a Ensemble or",
    "start": "668040",
    "end": "673160"
  },
  {
    "text": "model pipeline so let's talk about that python API box so Triton has a few",
    "start": "673160",
    "end": "679279"
  },
  {
    "start": "675000",
    "end": "915000"
  },
  {
    "text": "python apis um as NE mentioned there's various components like the front end the core the back",
    "start": "679279",
    "end": "684639"
  },
  {
    "text": "end uh for defining your inference or your business logic you'd probably start in the uh backend uh realm so here is an",
    "start": "684639",
    "end": "693160"
  },
  {
    "text": "example of Triton's backend python API it's a pretty similar uh simple interface where you just define a class",
    "start": "693160",
    "end": "699200"
  },
  {
    "text": "and adhere to some interface right so you just need these three functions initialize execute and",
    "start": "699200",
    "end": "704560"
  },
  {
    "text": "finalize um initialize and finalize are just sort of for clean up and uh setup",
    "start": "704560",
    "end": "709600"
  },
  {
    "text": "but they're optional if you don't need anything and then execute is where uh most of the magic happens which is",
    "start": "709600",
    "end": "715160"
  },
  {
    "text": "actually taking the requests that came through your pipeline whether that was through some Gateway way or through RAC serve or however you define it doing",
    "start": "715160",
    "end": "722720"
  },
  {
    "text": "whatever you want to it and then returning responses ultimately back to the client or the application um things that come out of",
    "start": "722720",
    "end": "728959"
  },
  {
    "text": "using this python backend API are a lot of the common Technologies you probably see such as metrics through a Prometheus format tracing through an open Telemetry",
    "start": "728959",
    "end": "736000"
  },
  {
    "text": "format um as well as like logging and trying to be optimal with how we move data around so NE mentioned trying to",
    "start": "736000",
    "end": "742279"
  },
  {
    "text": "keep zero copy as much as possible the objects you're passing around to here in the python back end are very compatible with other Frameworks so taking like a",
    "start": "742279",
    "end": "749160"
  },
  {
    "text": "TR and tensor moving it to P torch um ideally with no copy if it stays on the same device or if you need to move from",
    "start": "749160",
    "end": "756199"
  },
  {
    "text": "GPU to CPU for some CPU bound operation you can do that converting to numpy or something as",
    "start": "756199",
    "end": "761800"
  },
  {
    "text": "well um so this is all for kind of your business logic how you define a model in a python Centric way but how do you",
    "start": "761800",
    "end": "768880"
  },
  {
    "text": "actually take that and start a racer application that would be this front end",
    "start": "768880",
    "end": "774240"
  },
  {
    "text": "API so uh this here import Triton server is the bread and butter to how this",
    "start": "774240",
    "end": "780040"
  },
  {
    "text": "integration Works um this is primarily python bindings to Triton set of capis that you saw in the architectural",
    "start": "780040",
    "end": "786920"
  },
  {
    "text": "diagrams um what this means is that you also get access to not only the python backend apis like we saw but also all of",
    "start": "786920",
    "end": "793480"
  },
  {
    "text": "the backends that Triton supports and most of these backends are actually written in C++ and optimized for their",
    "start": "793480",
    "end": "798639"
  },
  {
    "text": "various um use cases or Hardwares Etc so even though you are uh at at the",
    "start": "798639",
    "end": "804480"
  },
  {
    "text": "application layer writing something in Python it can ultimately go through Triton and C and C++",
    "start": "804480",
    "end": "811519"
  },
  {
    "text": "backends however at that layer where you're writing the code as the developer uh it is somewhat python native and that",
    "start": "811519",
    "end": "817519"
  },
  {
    "text": "you can pass these uh dictionaries and lists directly as the inputs so you can um refer to the example on the right and",
    "start": "817519",
    "end": "823399"
  },
  {
    "text": "see like the inputs text input and stream these are just python objects right and then ultimately uh when doing",
    "start": "823399",
    "end": "830199"
  },
  {
    "text": "inference we keep performance and larger scale pipelines in mind so this inference is obviously not blocking it",
    "start": "830199",
    "end": "836279"
  },
  {
    "text": "can be an asynchronous operation where while waiting for triton C++ back end to compute something for example you can be",
    "start": "836279",
    "end": "842680"
  },
  {
    "text": "processing more requests on your racer front end uh or doing some kind of pre- or post processing in",
    "start": "842680",
    "end": "848399"
  },
  {
    "text": "parallel so given all that how do we put these two things together so many of you",
    "start": "848399",
    "end": "854720"
  },
  {
    "text": "in the room here at the ray suit are probably familiar with rerf so you've probably seen this type of uh example",
    "start": "854720",
    "end": "860600"
  },
  {
    "text": "where we have a deployment and we add a few decorators from racer and we now have something that uh has all the one",
    "start": "860600",
    "end": "867440"
  },
  {
    "text": "great things that actually was talking about like fraction resourcing scaling Etc um the main differentiator here is",
    "start": "867440",
    "end": "874279"
  },
  {
    "text": "in the initialize and implementation details where we uh initialize Triton do the python API I described and",
    "start": "874279",
    "end": "881720"
  },
  {
    "text": "everything else kind of just works right so now that you have a Triton object handle in Python you can Define your",
    "start": "881720",
    "end": "888040"
  },
  {
    "text": "inference logic we have a example generate function here that I'll go into more detail um but through a simple call to",
    "start": "888040",
    "end": "894519"
  },
  {
    "text": "the Triton process API you get uh things like the scheduling the model management",
    "start": "894519",
    "end": "899800"
  },
  {
    "text": "and the inference layer from Triton while also getting the like Fast API front ends the uh deployment scalability",
    "start": "899800",
    "end": "907079"
  },
  {
    "text": "and just the ease of use that Ray serve offers in Python so let's walk through a",
    "start": "907079",
    "end": "912480"
  },
  {
    "text": "bit of a example of how this looks so uh Vision models at large",
    "start": "912480",
    "end": "919440"
  },
  {
    "start": "915000",
    "end": "1120000"
  },
  {
    "text": "language models these things are trending they're very popular uh one such example is stable diffusion so we",
    "start": "919440",
    "end": "925480"
  },
  {
    "text": "actually have an example here end to end going from stable Fusion taken in P for",
    "start": "925480",
    "end": "930920"
  },
  {
    "text": "example bringing it through the journey through whatever framework you like in Triton um ultimately turning into an",
    "start": "930920",
    "end": "938120"
  },
  {
    "text": "application and scaling so on the right hand side here uh the first step for any Tron Triton user is to prepare a model",
    "start": "938120",
    "end": "945120"
  },
  {
    "text": "repository this is just a defined uh structure for how Triton will read and",
    "start": "945120",
    "end": "950360"
  },
  {
    "text": "manage your models uh in this example here you can see a a model folder called stable diffusion XEL where for version",
    "start": "950360",
    "end": "958040"
  },
  {
    "text": "one of this model we have a few files here um at inference time we actually only need one of these um and you can",
    "start": "958040",
    "end": "963959"
  },
  {
    "text": "choose in this example so we have uh Exel engine batch size one this is a tensor RT model uh we have XEL 1.0 Onyx",
    "start": "963959",
    "end": "972519"
  },
  {
    "text": "and then we have the same for pytorch so the reason these are all here is to kind of tell you that everything is very",
    "start": "972519",
    "end": "978480"
  },
  {
    "text": "customizable and flexible so you can take a p torch model if you're happy with P torch and you can run it through",
    "start": "978480",
    "end": "984600"
  },
  {
    "text": "Triton if for some reason you prefer onx R time you can convert through torch export to get an onyx model still run it",
    "start": "984600",
    "end": "991240"
  },
  {
    "text": "through ton or if you want to really optimize for a specific GPU specific batch size certain parameters through",
    "start": "991240",
    "end": "997199"
  },
  {
    "text": "things like a deeping compiler then you can use tensor RT so you can kind of go this whole flow P torch Onyx Onyx tensor",
    "start": "997199",
    "end": "1003720"
  },
  {
    "text": "RT take and choose whatever you like but ultimately you get this model repository and then a configuration file that tells",
    "start": "1003720",
    "end": "1010079"
  },
  {
    "text": "Tron what to do with it which model to use uh what type of resources to use and",
    "start": "1010079",
    "end": "1015399"
  },
  {
    "text": "uh what the io looks like to actually do the inference or uh form some kind of pipeline out of",
    "start": "1015399",
    "end": "1021120"
  },
  {
    "text": "it once you have all that and we have a tutorial here on the side but once you have all that then you can move on to",
    "start": "1021120",
    "end": "1027400"
  },
  {
    "text": "the deployment so earlier I showed just a high Lev example um but this actually goes into some detail on what that",
    "start": "1027400",
    "end": "1032880"
  },
  {
    "text": "generate function might look like so what this function is doing is uh in the context of stable diffusion we're trying",
    "start": "1032880",
    "end": "1038280"
  },
  {
    "text": "to generate images based on a prompt right so through not many lines of code we can pass the uh requests that come",
    "start": "1038280",
    "end": "1045558"
  },
  {
    "text": "through the generate endpoint exposed by rer all the way through Triton calling tensor RT and get back an image returned",
    "start": "1045559",
    "end": "1052679"
  },
  {
    "text": "to the user through fast API um once you have that this like I",
    "start": "1052679",
    "end": "1058840"
  },
  {
    "text": "said this is a ray serve deployment right so you get all the tools that come with Ray serve as well including the ray serve CLI so using uh serve run we can",
    "start": "1058840",
    "end": "1067120"
  },
  {
    "text": "launch this deployment and going off the uh decorators that Define the resources",
    "start": "1067120",
    "end": "1073840"
  },
  {
    "text": "and uh options needed for the deployment you'll get uh exactly that and your",
    "start": "1073840",
    "end": "1079159"
  },
  {
    "text": "deployment so it's actually a secret fourth step here uh because maybe this uh output is not exciting enough so we",
    "start": "1079159",
    "end": "1087600"
  },
  {
    "text": "can use it and here's an example of generating pandas so uh and not the not",
    "start": "1087600",
    "end": "1092799"
  },
  {
    "text": "the data frames library but the animal um so here we have a example client talking to the fast API generate",
    "start": "1092799",
    "end": "1099360"
  },
  {
    "text": "endpoint where we request a scuba diving Panda and we get this image and we can",
    "start": "1099360",
    "end": "1105240"
  },
  {
    "text": "kind of take this as a fun example but we can also profile it and see you know uh taken a real application into mind",
    "start": "1105240",
    "end": "1111919"
  },
  {
    "text": "how do we actually Benchmark and determine when we need to scale how we observe our deployment Etc so we'll look",
    "start": "1111919",
    "end": "1118559"
  },
  {
    "text": "at that so you may be asking why would I go",
    "start": "1118559",
    "end": "1125440"
  },
  {
    "start": "1120000",
    "end": "1289000"
  },
  {
    "text": "through all this work why would I want to embed Triton in RAC serve or vice",
    "start": "1125440",
    "end": "1130919"
  },
  {
    "text": "versa and some of that is the optimized backends that we talked about so in this example we highlight the uh tensorrt",
    "start": "1130919",
    "end": "1138400"
  },
  {
    "text": "backend running through Triton you know most of that is hidden away from you when you're writing your deployments and",
    "start": "1138400",
    "end": "1143600"
  },
  {
    "text": "you're going through python apis uh but through very minimal code change on the application side we actually get about a",
    "start": "1143600",
    "end": "1149559"
  },
  {
    "text": "9 to 10% uh throughput uh Improvement for this example of just generating",
    "start": "1149559",
    "end": "1155120"
  },
  {
    "text": "images through a stable diffusion model and this is just on you know single node single GPU single client um not too",
    "start": "1155120",
    "end": "1163159"
  },
  {
    "text": "shabby but you may want to know how do I scale this thing right so once again this is where racer comes in",
    "start": "1163159",
    "end": "1170280"
  },
  {
    "text": "so just by changing The Decorator that you apply to your deployment you can Define Autos scaling criteria and run",
    "start": "1170280",
    "end": "1177440"
  },
  {
    "text": "with it uh you don't need to make any changes to Triton's code you don't need to make any changes to the inference",
    "start": "1177440",
    "end": "1183039"
  },
  {
    "text": "code uh you just redeploy with the same thing and you get the load balancing",
    "start": "1183039",
    "end": "1188640"
  },
  {
    "text": "flexibility that uh Ray offers so we can run our same experiment and see how we do when Autos",
    "start": "1188640",
    "end": "1194960"
  },
  {
    "text": "scaling so that's what we get here we scale up to uh we find a Max of eight gpus in our deployment in the previous",
    "start": "1194960",
    "end": "1201679"
  },
  {
    "text": "slide and we saw actually a 45% Improvement when you start to scale this up so uh as you scale it out you get",
    "start": "1201679",
    "end": "1208960"
  },
  {
    "text": "bigger and bigger gains uh you may wonder why this uh if it was like a 10% Improvement for one GPU why is it not",
    "start": "1208960",
    "end": "1215559"
  },
  {
    "text": "80% Improvement for AG gpus um and that's just a artifact of The Benchmark",
    "start": "1215559",
    "end": "1220840"
  },
  {
    "text": "so this one is actually including Auto scaling in The Benchmark so while the requests are coming nodes are spinning",
    "start": "1220840",
    "end": "1226120"
  },
  {
    "text": "up gpus are spinning up um and so it ends up being a net 45% uh but if you",
    "start": "1226120",
    "end": "1232480"
  },
  {
    "text": "were to take this Standalone uh pre-loaded H gpus Etc you'd probably see more linear",
    "start": "1232480",
    "end": "1238799"
  },
  {
    "text": "gains and now that you have a deployment now that you figured out how to scale",
    "start": "1238799",
    "end": "1243880"
  },
  {
    "text": "and you figured out how to integrate these things how do you observe that deployment well once again rayer comes",
    "start": "1243880",
    "end": "1249600"
  },
  {
    "text": "to save the day so rayer has the dashboards that many of you are probably familiar with and it consumes things",
    "start": "1249600",
    "end": "1255640"
  },
  {
    "text": "like prome metrics in a Prometheus format and if you remember Triton has those metrics Triton also exposes custom",
    "start": "1255640",
    "end": "1262159"
  },
  {
    "text": "metrics if you'd like to add your own to your business logic or to your model and you can view these all in the dashboard",
    "start": "1262159",
    "end": "1268559"
  },
  {
    "text": "so uh what this dashboard is showing is actually from the prior example where we see that the deployment scaled up to",
    "start": "1268559",
    "end": "1274640"
  },
  {
    "text": "eight replicas and then as The Benchmark ended it actually scaled back down so to save you resources when you're not using",
    "start": "1274640",
    "end": "1280720"
  },
  {
    "text": "it and then you can observe all the replicas independently and sort of gauge",
    "start": "1280720",
    "end": "1286200"
  },
  {
    "text": "the health of your deployment so with all this I'd like to summarize",
    "start": "1286200",
    "end": "1292120"
  },
  {
    "start": "1289000",
    "end": "1400000"
  },
  {
    "text": "uh essentially you know for triton users why is racer interesting for racer users",
    "start": "1292120",
    "end": "1297720"
  },
  {
    "text": "why is Triton interesting maybe if you're either they are both interesting to you so I just highlight a few things",
    "start": "1297720",
    "end": "1303600"
  },
  {
    "text": "here if you're a race serve user there's many benefit uh benefits of Triton so we have the example of the optimized",
    "start": "1303600",
    "end": "1309400"
  },
  {
    "text": "backends and the flexible model repositories where given the same model you can run on a framework of your",
    "start": "1309400",
    "end": "1314679"
  },
  {
    "text": "choice uh as accelerated or not accelerated as you want it to be um you got the latest libraries in terms",
    "start": "1314679",
    "end": "1321080"
  },
  {
    "text": "of like Cuda CNN GPU driver support that come in the Triton software stack you",
    "start": "1321080",
    "end": "1326400"
  },
  {
    "text": "got the inprocess apis that let you define things such as a inprocess model pipeline so maybe on a single node you",
    "start": "1326400",
    "end": "1333720"
  },
  {
    "text": "have a optimized zeroc copy pre-processing compute press processing pipeline but then with Ray you actually",
    "start": "1333720",
    "end": "1340480"
  },
  {
    "text": "scale that to multi noes and have distributed pipelines of these pipelines or have your models uh scaled out on",
    "start": "1340480",
    "end": "1346440"
  },
  {
    "text": "separate nodes then for triton users you may ask why Ray so as we walked through in this",
    "start": "1346440",
    "end": "1353360"
  },
  {
    "text": "example you get that python first easy deployment uh developer experience with",
    "start": "1353360",
    "end": "1358400"
  },
  {
    "text": "the load balancing and Autos scaling essentially just being a decorator in Python uh you get the distribution of",
    "start": "1358400",
    "end": "1364840"
  },
  {
    "text": "the pipeline through the ray cluster and kind of the quick Dev cycle that you get",
    "start": "1364840",
    "end": "1370320"
  },
  {
    "text": "out of python right so maybe if you've used Triton uh you've had to go through maybe some C++ code or or some binaries",
    "start": "1370320",
    "end": "1376919"
  },
  {
    "text": "in certain containers but in this workflow it's all python you can you know kill your service change a few",
    "start": "1376919",
    "end": "1384360"
  },
  {
    "text": "lines start it up again no compilation no builds Etc and you're off on your way to a autoscaling deployment of optimized",
    "start": "1384360",
    "end": "1391559"
  },
  {
    "text": "models through T RT and Triton for example so with all that we have a few",
    "start": "1391559",
    "end": "1396760"
  },
  {
    "text": "resources and then I think we'll spend some time on questions thank you",
    "start": "1396760",
    "end": "1404559"
  },
  {
    "start": "1400000",
    "end": "1489000"
  },
  {
    "text": "[Applause]",
    "start": "1404670",
    "end": "1411519"
  },
  {
    "text": "do we have a mic for questions or maybe raise of hand or",
    "start": "1411880",
    "end": "1417240"
  },
  {
    "text": "anything you guys want to come up",
    "start": "1418000",
    "end": "1421799"
  },
  {
    "text": "yeah I'm curious uh if you have only the combination between Rive and tensor RT",
    "start": "1424039",
    "end": "1430480"
  },
  {
    "text": "without the middleman uh Triton how much performance Improvement you can gain from the B charts you've seen you've",
    "start": "1430480",
    "end": "1437039"
  },
  {
    "text": "shown earlier I can take this one um yeah so we don't uh have a direct Benchmark um like",
    "start": "1437039",
    "end": "1443960"
  },
  {
    "text": "you're saying uh but sort of the goal of what we're trying to show is that so in",
    "start": "1443960",
    "end": "1449880"
  },
  {
    "text": "this example we had a pytorch Onyx and tensor for the same model for example uh",
    "start": "1449880",
    "end": "1455279"
  },
  {
    "text": "through Triton as the middleman in the rer deployment it's just a change of a flag in the configuration file which",
    "start": "1455279",
    "end": "1461600"
  },
  {
    "text": "model gets run if you wanted to uh write that in process in the racer deployment using 10rt python apis you'd have to go",
    "start": "1461600",
    "end": "1468440"
  },
  {
    "text": "figure out how to write T RT code figure out all that and change your deployment significantly for each framework right",
    "start": "1468440",
    "end": "1475080"
  },
  {
    "text": "so kind of the value uh we're trying to show is that you get that reusable mod repository through a common interface",
    "start": "1475080",
    "end": "1481039"
  },
  {
    "text": "where the code really doesn't have to change very much and you just swap out based on your needs yeah yeah just also comment I think that",
    "start": "1481039",
    "end": "1489440"
  },
  {
    "text": "there's a lot of kind of engineering years that have gone into some of the backends and the way they use the",
    "start": "1489440",
    "end": "1494520"
  },
  {
    "text": "Frameworks so in a previous version of this we did actually use torch compile with the tens RT backend but still saw",
    "start": "1494520",
    "end": "1501240"
  },
  {
    "text": "performance Improvement by using the tens RT backend directly",
    "start": "1501240",
    "end": "1506200"
  },
  {
    "text": "so uh thank you for the interesting presentation I'm a little bit new to this uh software stack so I'd like to",
    "start": "1511840",
    "end": "1517360"
  },
  {
    "text": "start my question from a little bit below level from the infrastructure uh do I understand correctly that in order",
    "start": "1517360",
    "end": "1522679"
  },
  {
    "text": "to run this stack a Tron plus race serve you have to have um Nvidia compatible uh",
    "start": "1522679",
    "end": "1530360"
  },
  {
    "text": "processor architecture on whatever node you use um and also related question uh",
    "start": "1530360",
    "end": "1536000"
  },
  {
    "text": "can this be run in a container form factor and if so what is the compatibility I'm particularly",
    "start": "1536000",
    "end": "1541600"
  },
  {
    "text": "interested if you're compatible say you know Amazon infuential graviton and such",
    "start": "1541600",
    "end": "1547039"
  },
  {
    "text": "thank you I can um yeah so for running Ray in",
    "start": "1547039",
    "end": "1553000"
  },
  {
    "text": "general um you can kind of run it on any hardware like whether it's bare metal VMS uh or whether it's kubernetes um and",
    "start": "1553000",
    "end": "1562520"
  },
  {
    "text": "like in most people in production tend to use kubernetes uh for running Ray and Ray serve uh in terms of inferentia and",
    "start": "1562520",
    "end": "1570840"
  },
  {
    "text": "these other accelerators there's also uh support for it within Ray so you can have instead of gpus you can have like",
    "start": "1570840",
    "end": "1578080"
  },
  {
    "text": "uh infanta uh resource or like tpus and and so on um I mean the stock obviously is",
    "start": "1578080",
    "end": "1585760"
  },
  {
    "text": "for working on Nvidia accelerators only um but you can extend R to work on any",
    "start": "1585760",
    "end": "1594158"
  },
  {
    "text": "accelerator yeah Triton okay go yeah so I mean obviously",
    "start": "1595600",
    "end": "1601039"
  },
  {
    "text": "Triton is accelerated for gpus um but we uh also support CPUs and there are um",
    "start": "1601039",
    "end": "1606320"
  },
  {
    "text": "like holistically like python models for example many of which can just run on python uh on CPU um but there even",
    "start": "1606320",
    "end": "1612440"
  },
  {
    "text": "optimized CPU backends like NE mentioned open V for example um and it's not like a requirement that you have have the uh",
    "start": "1612440",
    "end": "1619360"
  },
  {
    "text": "stack or or so so for example we ship containers monthly with all the stack prepared for you it's not a requirement",
    "start": "1619360",
    "end": "1625080"
  },
  {
    "text": "that you have a GPU to run that container it it'll work with no gpus on the system um and then for something",
    "start": "1625080",
    "end": "1632399"
  },
  {
    "text": "like inferential like a maybe custom accelerator that's not a GPU uh we Triton has examples for running",
    "start": "1632399",
    "end": "1638200"
  },
  {
    "text": "inferentia um so theoretically composing these things should all work and Harmony together",
    "start": "1638200",
    "end": "1645080"
  },
  {
    "text": "yeah sorry hello we'll go with the mic wherever it went yes do you recommend to use this for batch prediction I mean",
    "start": "1646440",
    "end": "1653880"
  },
  {
    "text": "offline sorry could you repeat the question do you recommend to use this for badge",
    "start": "1653880",
    "end": "1659399"
  },
  {
    "text": "prediction this is for online right say it's good for online it's good for offline like badge",
    "start": "1659399",
    "end": "1666399"
  },
  {
    "text": "prediction yeah so um I guess I I can speak about Triton generically and maybe",
    "start": "1666600",
    "end": "1671720"
  },
  {
    "text": "o can speak about um Ray but so in general uh Triton's very configurable",
    "start": "1671720",
    "end": "1677240"
  },
  {
    "text": "and there may be certain back ends or certain configurations that are preferred for something like an online",
    "start": "1677240",
    "end": "1683039"
  },
  {
    "text": "scenario versus an offline scenario but there's nothing limiting you at like the framework level to say like Triton is",
    "start": "1683039",
    "end": "1688480"
  },
  {
    "text": "better at one than the other uh so for example like a large language model uh for example you may tune things about",
    "start": "1688480",
    "end": "1694480"
  },
  {
    "text": "the model to prioritize something like time to First token which might be more important in an online scenario uh",
    "start": "1694480",
    "end": "1700320"
  },
  {
    "text": "whereas you could separately tune the model for an offline scenario where you just care about throughput and you don't care about that uh latency for example",
    "start": "1700320",
    "end": "1706399"
  },
  {
    "text": "right um most of that would require not much or little no change to Triton and is more so model oriented and then I",
    "start": "1706399",
    "end": "1713840"
  },
  {
    "text": "think Ray would have some similar Concepts yeah for uh offline batch scenarios for Ray we uh typically",
    "start": "1713840",
    "end": "1720640"
  },
  {
    "text": "recommend Ray data um so that's more like that can handle batching more",
    "start": "1720640",
    "end": "1726200"
  },
  {
    "text": "effectively like kind of perform more effective like batched offline data map",
    "start": "1726200",
    "end": "1731880"
  },
  {
    "text": "reduce operations uh and without the HTTP overhead that you will have in like",
    "start": "1731880",
    "end": "1737279"
  },
  {
    "text": "a request response",
    "start": "1737279",
    "end": "1740039"
  },
  {
    "text": "framework how about VM actually how doeses tens um and VM actually I do see",
    "start": "1743000",
    "end": "1749600"
  },
  {
    "text": "some performance comparisons between VM and tens showing higher performance in",
    "start": "1749600",
    "end": "1755039"
  },
  {
    "text": "some cases um do you have any comments on that",
    "start": "1755039",
    "end": "1760799"
  },
  {
    "text": "one right yeah I mean we um I say in general I think we're big fans of the V",
    "start": "1760880",
    "end": "1768200"
  },
  {
    "text": "project so Triton also has a VM back end as well so you can run get all the the",
    "start": "1768200",
    "end": "1774279"
  },
  {
    "text": "common batching and metrics and things like that and then easily swap between VM and tens tlm um so I think out of the",
    "start": "1774279",
    "end": "1782120"
  },
  {
    "text": "box uh tens TM performance um it requires some investigation and tuning",
    "start": "1782120",
    "end": "1788919"
  },
  {
    "text": "generally if you spend the time to tune it um you can outperform out of the box VM as well um but I think it's",
    "start": "1788919",
    "end": "1796240"
  },
  {
    "text": "continually an arms race right with the l space I think both are moving very",
    "start": "1796240",
    "end": "1802000"
  },
  {
    "text": "fast hello I I have one question um it's it's clear for me when to use like race",
    "start": "1803000",
    "end": "1808559"
  },
  {
    "text": "ser and Tron and all the WIS it has is there any case where like you all recommend just to use like Tron",
    "start": "1808559",
    "end": "1814600"
  },
  {
    "text": "Standalone without combining it with racer is there like any performance differ with using like Triton Standalone",
    "start": "1814600",
    "end": "1822360"
  },
  {
    "text": "instead of combining it by Red by with Reser",
    "start": "1822360",
    "end": "1827480"
  },
  {
    "text": "I can I can give a a generic answer to start uh so for example most of this time I was focused uh uh the interaction",
    "start": "1828480",
    "end": "1835519"
  },
  {
    "text": "was focused on like scaling for example so if you know in your scenario you're",
    "start": "1835519",
    "end": "1840720"
  },
  {
    "text": "just running like single node single GPU or something maybe you don't need to add RAC serve into the mix to achieve the",
    "start": "1840720",
    "end": "1846799"
  },
  {
    "text": "same goal maybe you could just run Triton sand Alone um but uh for example",
    "start": "1846799",
    "end": "1851960"
  },
  {
    "text": "Triton has a set of uh like web serving uh front end schemas like kerve predict",
    "start": "1851960",
    "end": "1857600"
  },
  {
    "text": "V2 or open a or something like this maybe you want a custom schema or something for your use case when you go",
    "start": "1857600",
    "end": "1863600"
  },
  {
    "text": "through a rer deployment like when it has that fast API integration uh you can Define your own I mean you can do the",
    "start": "1863600",
    "end": "1869000"
  },
  {
    "text": "same just with fast API and without race serve as well but uh it's sort of weighing pros and cons like there are",
    "start": "1869000",
    "end": "1875720"
  },
  {
    "text": "clear benefits from introducing race serve into the mix so are those things that interest you if so you can do it if",
    "start": "1875720",
    "end": "1881360"
  },
  {
    "text": "not maybe it's one less thing in the mix for that particular scenario but yeah I don't think there's major like Bott next",
    "start": "1881360",
    "end": "1888679"
  },
  {
    "text": "um with the collaboration for example yeah great is there any performance",
    "start": "1888679",
    "end": "1893760"
  },
  {
    "text": "degradation when introducing res serve on top of",
    "start": "1893760",
    "end": "1898799"
  },
  {
    "text": "Triton sorry could you repeat the yes sir if like they like I see you you Benchmark like the performance with like",
    "start": "1898799",
    "end": "1905320"
  },
  {
    "text": "rer and against like rer plus Tron I was wondering what is the performance of Triton Standalone in that case if you",
    "start": "1905320",
    "end": "1911880"
  },
  {
    "text": "know the numbers okay yeah so we we didn't run a benchmark for this example we're not like comparing directly um",
    "start": "1911880",
    "end": "1918799"
  },
  {
    "text": "uh they may be different input formats for example when in this example versus like Triton's native front ends so we",
    "start": "1918799",
    "end": "1925760"
  },
  {
    "text": "don't have a number or anything to share",
    "start": "1925760",
    "end": "1929399"
  },
  {
    "text": "yeah yeah I think we're at we're at time so uh we'll hang out here if there's any",
    "start": "1935039",
    "end": "1940120"
  },
  {
    "text": "more questions [Applause]",
    "start": "1940120",
    "end": "1946520"
  }
]