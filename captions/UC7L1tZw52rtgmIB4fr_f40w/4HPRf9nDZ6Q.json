[
  {
    "text": "for the first ever uh track uh at V for VM at the race Summit so I'm Simon I'll",
    "start": "3000",
    "end": "10880"
  },
  {
    "text": "be sort of the session chair today to introduce you to all the speakers and then also handle Q&A and so yeah uh",
    "start": "10880",
    "end": "19840"
  },
  {
    "text": "without further Ado I'm going to get started our first talk is going to be from uh Joan and Quai who are both at uh",
    "start": "19840",
    "end": "27119"
  },
  {
    "text": "UC Berkeley part of the original VM team who started this project last year and",
    "start": "27119",
    "end": "32360"
  },
  {
    "text": "this will be a very special talk given that previously what have been all doing sort of just quick update what have we",
    "start": "32360",
    "end": "39960"
  },
  {
    "text": "done in the last two months but for this talk Johan and Quint will take us",
    "start": "39960",
    "end": "45239"
  },
  {
    "text": "through the the history of V what have we done in the last year as well as",
    "start": "45239",
    "end": "51360"
  },
  {
    "text": "what's going to be happening uh next year so yeah Johan take it away um yeah",
    "start": "51360",
    "end": "57600"
  },
  {
    "text": "hello everyone uh welcome to the first Talk of the V raceit VM track and today",
    "start": "57600",
    "end": "63440"
  },
  {
    "text": "Quinta and I will be uh talking about what happened in the past year of VM and",
    "start": "63440",
    "end": "68920"
  },
  {
    "text": "what are we going to do next and yeah let's get started so vm's goal is to build the fastest and easiest",
    "start": "68920",
    "end": "76080"
  },
  {
    "text": "to use open source L influence and serving engine and vm's original story",
    "start": "76080",
    "end": "82640"
  },
  {
    "text": "can actually be dated back all the way to August 2022 so back then we were",
    "start": "82640",
    "end": "88000"
  },
  {
    "text": "serving an opt7 75 billion model and as a demo and this is used as a demo for",
    "start": "88000",
    "end": "94600"
  },
  {
    "text": "one of our previous research project and we found that this demo was super super slow so we started to think okay LM",
    "start": "94600",
    "end": "102200"
  },
  {
    "text": "inflence can be an interesting field and we started started to build an open source library for RM serving and and",
    "start": "102200",
    "end": "109399"
  },
  {
    "text": "this is actually this was actually before the chat gbd coming out and so while building this Library we found",
    "start": "109399",
    "end": "115960"
  },
  {
    "text": "that the previous systems actually manages the now called KV cash memory very very poorly so we iterate multiple",
    "start": "115960",
    "end": "123159"
  },
  {
    "text": "times and convert to the page attention idea and we started to work on the idea",
    "start": "123159",
    "end": "128640"
  },
  {
    "text": "and eventually submitted the paper in April and meanwhile VM is also used to",
    "start": "128640",
    "end": "133879"
  },
  {
    "text": "power the vuna demo and the chatboard arena so and they are being actually as",
    "start": "133879",
    "end": "139640"
  },
  {
    "text": "they are the first actual production deployment of vrm and then we spent uh",
    "start": "139640",
    "end": "145480"
  },
  {
    "text": "so we submitted the paper in April and then we spent another two months to polishing the rort to make it open",
    "start": "145480",
    "end": "151840"
  },
  {
    "text": "source ready and we actually open source the project in June 2023 so back then uh serving a is very",
    "start": "151840",
    "end": "159400"
  },
  {
    "text": "expensive and VM delivers the best cost so very fortunately we got the a lot of",
    "start": "159400",
    "end": "164760"
  },
  {
    "text": "attractions from the very beginning and VM has continued to grow rapidly with the help of community and today VM is",
    "start": "164760",
    "end": "172519"
  },
  {
    "text": "the most popular open source LM serving engine in the world and VM has a pretty simple and",
    "start": "172519",
    "end": "179599"
  },
  {
    "text": "nice end user API so first if you are say an MLP researcher and you want to use start",
    "start": "179599",
    "end": "186120"
  },
  {
    "text": "an L in your python script you can use the LM class in VM which is a python",
    "start": "186120",
    "end": "191840"
  },
  {
    "text": "interface for offline batched inference so you can simply create an LM object",
    "start": "191840",
    "end": "197080"
  },
  {
    "text": "and with the model name and then you can call the generate method to efficiently run inference on a batch of requests",
    "start": "197080",
    "end": "203959"
  },
  {
    "text": "also you can call the new lm. chat interface to chat with a instruction tune LM",
    "start": "203959",
    "end": "211760"
  },
  {
    "text": "and also vrm is a fully like you know serving engine fully capable serving engine so you can also start VM as an",
    "start": "211760",
    "end": "218799"
  },
  {
    "text": "open API compatible server with a single command then you will you you can reuse",
    "start": "218799",
    "end": "224640"
  },
  {
    "text": "this any kind of openi client by simply replacing the URL to the new URL you",
    "start": "224640",
    "end": "230640"
  },
  {
    "text": "just started the server and yeah today I will do a review",
    "start": "230640",
    "end": "237120"
  },
  {
    "text": "on VM progress and main features developed in the past year which include the following six aspects so we will",
    "start": "237120",
    "end": "244079"
  },
  {
    "text": "talk about model support Hardware support model Performance Engine features distributed system architecture",
    "start": "244079",
    "end": "250200"
  },
  {
    "text": "and in the end open source community and after this one year one year review and we I will hand over to Quai to talk",
    "start": "250200",
    "end": "257040"
  },
  {
    "text": "about our future plan and also our like Q4 road map and yeah let's get started with",
    "start": "257040",
    "end": "263919"
  },
  {
    "text": "model support So nowadays we have a very broad support of model support in VM and",
    "start": "263919",
    "end": "270120"
  },
  {
    "text": "we started from Transformer like LM for example Lama and then we gradually added many different kind of other large",
    "start": "270120",
    "end": "276759"
  },
  {
    "text": "language models so most noticeably we first added mixture of expert models like mixture mixture and then we also",
    "start": "276759",
    "end": "283520"
  },
  {
    "text": "added many multimodel large language models like lava and right now we are also actively exploring and other kind",
    "start": "283520",
    "end": "290720"
  },
  {
    "text": "of models like Stace Bas models and also we are exploring other uses of language models like embedding models and also",
    "start": "290720",
    "end": "298039"
  },
  {
    "text": "reward models and one so one thing to highlight is that VM supports almost all open source",
    "start": "298039",
    "end": "304800"
  },
  {
    "text": "LMS and vision language models nowadays and we are very proud to be actually the",
    "start": "304800",
    "end": "310199"
  },
  {
    "text": "launching partner of many open source models with the model providers for example for llama we are actually a",
    "start": "310199",
    "end": "317120"
  },
  {
    "text": "official launching partner with llama 3.1 and llama 3.2 and we have the official support on these two models on",
    "start": "317120",
    "end": "323280"
  },
  {
    "text": "day one and also we have many models in VM repo that are directly contributed from the model create",
    "start": "323280",
    "end": "330160"
  },
  {
    "text": "so for example the pix2 model and the q1 2 VL model they are all directly contributed from the model producer",
    "start": "330160",
    "end": "337080"
  },
  {
    "text": "model model producer so we can we can have a high confidence of the correctness of these",
    "start": "337080",
    "end": "343000"
  },
  {
    "text": "models yeah and and that's about our model support",
    "start": "343000",
    "end": "349319"
  },
  {
    "text": "and next we will talk about the hardware support So VM started with Nvidia gpus",
    "start": "349319",
    "end": "356360"
  },
  {
    "text": "because of it's like you know uh very it's like broad popularity but another",
    "start": "356360",
    "end": "362280"
  },
  {
    "text": "major stream of our project is to support diverse accelerators so VM currently support seven different type",
    "start": "362280",
    "end": "369080"
  },
  {
    "text": "of accelerators so where our primary focus is on Nvidia gpus we are also",
    "start": "369080",
    "end": "374440"
  },
  {
    "text": "actively collaborating with other Hardware vendors to provide various competitive options to the users so",
    "start": "374440",
    "end": "380599"
  },
  {
    "text": "right now we also support uh AMD GPU ad neural and x86 CPU Intel CPU Google TPU",
    "start": "380599",
    "end": "387440"
  },
  {
    "text": "and Intel GDI so you may ask how did we uh how do we manage these like all",
    "start": "387440",
    "end": "393759"
  },
  {
    "text": "different type of Hardware so our our approach is to use py toch as a narrow waste so we use py toch as a general",
    "start": "393759",
    "end": "400599"
  },
  {
    "text": "abstraction for the model code and also many other utilities Hardware specific utilities for different Hardwares and",
    "start": "400599",
    "end": "407560"
  },
  {
    "text": "then we utilize the hard py generality to gen to generalize our support to",
    "start": "407560",
    "end": "414160"
  },
  {
    "text": "different Hardwares yeah and next I will talk",
    "start": "414160",
    "end": "420639"
  },
  {
    "text": "about model performance so model performance is always one of our most important priority of vrm we want to",
    "start": "420639",
    "end": "426960"
  },
  {
    "text": "make out we want our model to run fast and efficient so first let from the from the lowest level we always we have a lot",
    "start": "426960",
    "end": "434199"
  },
  {
    "text": "of deeply optimized CA Kola kernels for all of the different operators within a",
    "start": "434199",
    "end": "439720"
  },
  {
    "text": "large language model so for example for matrix multiplication and we have we have a we have a cutas kernel to",
    "start": "439720",
    "end": "447080"
  },
  {
    "text": "optimize the quantized matrix multiplication and also fore layers we have uh we have",
    "start": "447080",
    "end": "453360"
  },
  {
    "text": "special optimized the Triton kernels to optimize the performance ofe layers and for communication especially all reduce",
    "start": "453360",
    "end": "460160"
  },
  {
    "text": "we have a custom all reduce kernel CA all redu kernel to optimize the communication performance and also",
    "start": "460160",
    "end": "466319"
  },
  {
    "text": "starting from day one we have a special support special kernel for our attention operator we started from our own page",
    "start": "466319",
    "end": "472440"
  },
  {
    "text": "attention operator and now we have actually adopted many other like Calas based kernels and also all kind of",
    "start": "472440",
    "end": "479639"
  },
  {
    "text": "different attention kernels including flash attention x x forers and also actually flash infer from University of",
    "start": "479639",
    "end": "486159"
  },
  {
    "text": "Washington and for many other layers for example like R Rope Rope layers we also",
    "start": "486159",
    "end": "491759"
  },
  {
    "text": "do have some customized customized optimized qu kernel and also we are actively utilizing torch. compile to to",
    "start": "491759",
    "end": "499159"
  },
  {
    "text": "compile the these like operators to make the uh the model run",
    "start": "499159",
    "end": "504599"
  },
  {
    "text": "faster so uh after we actually implemented a lot of the Cuda current",
    "start": "504599",
    "end": "510879"
  },
  {
    "text": "optimization the model runs faster and faster and we started to figure out that okay Python and pyto overhead actually",
    "start": "510879",
    "end": "517599"
  },
  {
    "text": "start to be a major bottleneck of the model execution so from our like measurement if we don't do any",
    "start": "517599",
    "end": "524240"
  },
  {
    "text": "optimization we can see like pyto and Pyon pyto overhead can Tak up to 50% of",
    "start": "524240",
    "end": "529680"
  },
  {
    "text": "the overall latency so in other words so CPU is too slow to launching kernels and",
    "start": "529680",
    "end": "534760"
  },
  {
    "text": "GPU so GPU is always waiting for CPU to finish launching kernels to start and DP",
    "start": "534760",
    "end": "539800"
  },
  {
    "text": "is always waiting for work so that's why we introduced this uh Cuda graph optimization so we can actually with",
    "start": "539800",
    "end": "546279"
  },
  {
    "text": "Cuda graph we can record all of the kernel launching sequences within a",
    "start": "546279",
    "end": "551640"
  },
  {
    "text": "model execution so that in the next step we can just launch this CA graph once so",
    "start": "551640",
    "end": "557120"
  },
  {
    "text": "there will be no other CPU overhead and the model will just the and the GPU will",
    "start": "557120",
    "end": "562360"
  },
  {
    "text": "execute all of the kernos all at once so this way we can eliminate the python overhead on the model side so we can get",
    "start": "562360",
    "end": "568440"
  },
  {
    "text": "a better performance and also as the model becomes bigger and bigger quantization becomes more and",
    "start": "568440",
    "end": "575600"
  },
  {
    "text": "more important feature so uh this is actually our quantization support plan in 2023 so we we our goal back then is",
    "start": "575600",
    "end": "584279"
  },
  {
    "text": "to support a wide range of quantization methods so our basically we have a three step plan our first step is to develop a",
    "start": "584279",
    "end": "591640"
  },
  {
    "text": "general abstraction for diverse quantization method and step two is to implement these diverse quantization",
    "start": "591640",
    "end": "596920"
  },
  {
    "text": "method and step three is to optimize this performance of these quantied Ops and right now I'm very happy to",
    "start": "596920",
    "end": "604360"
  },
  {
    "text": "report that we have a wide range of support of almost all most popular quantization methods including fp8 in8",
    "start": "604360",
    "end": "612279"
  },
  {
    "text": "and gpdq awq and B10 bytes and also we have many efficient optimized quantized",
    "start": "612279",
    "end": "618399"
  },
  {
    "text": "kernels in VM such as Marlin also special shout out to neurom Magic on this effort and also we would like to",
    "start": "618399",
    "end": "625880"
  },
  {
    "text": "highlight that we actually have another new project in called LM compressor to Kit under VM project that is a a library",
    "start": "625880",
    "end": "633720"
  },
  {
    "text": "that can help you quantizing model so that like it can with with all different kind of quantization methods so you can",
    "start": "633720",
    "end": "640320"
  },
  {
    "text": "efficiently quantize this model into a format that VM can understand and and so",
    "start": "640320",
    "end": "645639"
  },
  {
    "text": "you can you can use your basically you can quantize your model in your your own favorite like quantization methods and",
    "start": "645639",
    "end": "651639"
  },
  {
    "text": "to get the best performance out of",
    "start": "651639",
    "end": "655560"
  },
  {
    "text": "the um yeah and that's a a brief summary that's",
    "start": "656680",
    "end": "662240"
  },
  {
    "text": "a brief uh recap of what we did on the model performance side in vrm and also we did a lot",
    "start": "662240",
    "end": "669040"
  },
  {
    "text": "of one L above the model we did a lot of also other optimizations and also added a lot of features in VM engine to make",
    "start": "669040",
    "end": "676680"
  },
  {
    "text": "the models run better and support more different kind of functionalities so first optimization we",
    "start": "676680",
    "end": "683399"
  },
  {
    "text": "did on in vrm engine is trunk preu so we would like to thank uh thank a the the",
    "start": "683399",
    "end": "690120"
  },
  {
    "text": "auor of trunk pref and also send being in from N scale for implementing this so so what did the chunk pref do imagine",
    "start": "690120",
    "end": "696519"
  },
  {
    "text": "you have a very very long prompt then if you process this very long prompt all at once then this prompt will be block all",
    "start": "696519",
    "end": "703079"
  },
  {
    "text": "of the other ongoing requests so your user will get a very sudden jump on the",
    "start": "703079",
    "end": "709160"
  },
  {
    "text": "latency and on all on the tpot so what trun PR will do is that you can actually chunk this very very long prom into like",
    "start": "709160",
    "end": "715639"
  },
  {
    "text": "you know smaller chunks and you Pro you can process these smaller chunks one one by one and also while you are processing",
    "start": "715639",
    "end": "721399"
  },
  {
    "text": "these Sol smaller trunks you can also pass this smaller trunk with the previous existing running request",
    "start": "721399",
    "end": "727360"
  },
  {
    "text": "together so so the existing running request latency will not be affected that much so here with trunk prie we can",
    "start": "727360",
    "end": "735079"
  },
  {
    "text": "see that we can get a 2X performance latency job compared to without chunk pref and the user can get a higher both",
    "start": "735079",
    "end": "743279"
  },
  {
    "text": "can get both higher throughput and also a lower lower exe lower execution latency drum",
    "start": "743279",
    "end": "750519"
  },
  {
    "text": "and next is uh speculative decoding so for specul decoding the the goal here is to we can like while running a large",
    "start": "750519",
    "end": "757160"
  },
  {
    "text": "language model is very very slow and we are thinking whether we can use a smaller model to predict what the large",
    "start": "757160",
    "end": "762440"
  },
  {
    "text": "model is with doing and then it will be use the small model to accelerate the execution of large models so for example",
    "start": "762440",
    "end": "769800"
  },
  {
    "text": "for this small model we can we can use the small model to Auto regressively sequentially generate a candidate and",
    "start": "769800",
    "end": "777320"
  },
  {
    "text": "then we can use feed candidate sequence and we can feed feed this candidate sequence or this draft sequence to the",
    "start": "777320",
    "end": "783839"
  },
  {
    "text": "large model and then the large model can process this uh draft sequence all in parallel so you can utilize the",
    "start": "783839",
    "end": "790560"
  },
  {
    "text": "underlying Hardware GPU parallel Hardware GPU and then the large model will very can verify whether these",
    "start": "790560",
    "end": "796880"
  },
  {
    "text": "output tokens are correct or not so if they are all correct then we can use we",
    "start": "796880",
    "end": "802240"
  },
  {
    "text": "can use we can directly you know you know generate like four tokens all at once and then generate another",
    "start": "802240",
    "end": "809560"
  },
  {
    "text": "like and also generate another token so we can use these small models to uh propose draft propose draft tokens and",
    "start": "809560",
    "end": "816639"
  },
  {
    "text": "then use large model to verify the draft tokens in this case large models can have larger bat size can also batch more",
    "start": "816639",
    "end": "822959"
  },
  {
    "text": "tokens together so it can have a better redu latency and Al reduce latency of",
    "start": "822959",
    "end": "828320"
  },
  {
    "text": "generating like very long long sequences of large models yeah I think uh yes so this is a",
    "start": "828320",
    "end": "835320"
  },
  {
    "text": "large community effort led by Kate from N scale and also really from uh UC berky",
    "start": "835320",
    "end": "840759"
  },
  {
    "text": "and I think one thing we would like to highlight from this SP decoding effort is that this actually doesn't end as a",
    "start": "840759",
    "end": "847519"
  },
  {
    "text": "the engineering feature we also have a great re great research project out of",
    "start": "847519",
    "end": "852759"
  },
  {
    "text": "the spectr decoding so the the the motivation here is that when we are",
    "start": "852759",
    "end": "858160"
  },
  {
    "text": "think trying to add SP decoding into VM we found that uh actually like the VM",
    "start": "858160",
    "end": "864680"
  },
  {
    "text": "does like does continuous batching so when your system load is very high then VM can already batch a lot of requests",
    "start": "864680",
    "end": "871360"
  },
  {
    "text": "in this case if you still do a lot of spectular decoding it won't help your performance so so ideally what you want",
    "start": "871360",
    "end": "877279"
  },
  {
    "text": "to do is that when system load is low then you have a lot of extra compute then you want to do a lot of extra",
    "start": "877279",
    "end": "883519"
  },
  {
    "text": "specular decoding to reduce your latency but when your system load is high you want to reduce the number of specul",
    "start": "883519",
    "end": "889920"
  },
  {
    "text": "decoding tokens you do and even you just don't do specul decoding to maximize the throughput and this research is about",
    "start": "889920",
    "end": "896720"
  },
  {
    "text": "how can we optimally decide okay where when should we do more do more specul decoding and when should we do less",
    "start": "896720",
    "end": "902880"
  },
  {
    "text": "specul decoding so take uh if you are interested feel free to take a look at the P paper and yet to learn",
    "start": "902880",
    "end": "910199"
  },
  {
    "text": "more and next feature I would like to highlight is our har based automatic pref prefix caching so one example is",
    "start": "910199",
    "end": "917920"
  },
  {
    "text": "that say like uh for all of the like LM service typically people write a long",
    "start": "917920",
    "end": "923040"
  },
  {
    "text": "system prom to to describe the behavior of this model so for example here we",
    "start": "923040",
    "end": "928560"
  },
  {
    "text": "have a two request sharing the whole system prompt then the actually the KV cat and",
    "start": "928560",
    "end": "933920"
  },
  {
    "text": "all of the computation of this KV cach of the system prompt can be casted and",
    "start": "933920",
    "end": "939360"
  },
  {
    "text": "can be shared between these two requests and another example is multiun conversation so say you finish one round",
    "start": "939360",
    "end": "946000"
  },
  {
    "text": "of conversation and when you start the second round of your conversation actually your first round the KV cach of",
    "start": "946000",
    "end": "951519"
  },
  {
    "text": "your first round conversation can be cast in the memory and then you on the second round you can directly reuse the",
    "start": "951519",
    "end": "957920"
  },
  {
    "text": "first round of uh first round of the KV cash and you don't need to redo the computation again okay",
    "start": "957920",
    "end": "965199"
  },
  {
    "text": "so in vrm we actually we implemented a hash based pre automatic pre method so",
    "start": "965199",
    "end": "970639"
  },
  {
    "text": "what we do is that say for an example request a chat between a curious user so",
    "start": "970639",
    "end": "975800"
  },
  {
    "text": "we will actually uh M give each of these KV blocks a hash so here right now we",
    "start": "975800",
    "end": "982440"
  },
  {
    "text": "assume the KV block size is two so for block a we have a chat Block B we have",
    "start": "982440",
    "end": "988040"
  },
  {
    "text": "represent tokens of between a and Block C so on so we will give each block a",
    "start": "988040",
    "end": "993160"
  },
  {
    "text": "hash of its prefix tokens so for example block a will have a hash of a chat and",
    "start": "993160",
    "end": "998279"
  },
  {
    "text": "Block B will have will have a hash of a chat between a and so on so each block",
    "start": "998279",
    "end": "1004480"
  },
  {
    "text": "will have a unique ID that is based on its prefix hash and so if we have",
    "start": "1004480",
    "end": "1010240"
  },
  {
    "text": "another request coming in say a chat between a cute poppy then we can reuse the first two blocks so because they",
    "start": "1010240",
    "end": "1016880"
  },
  {
    "text": "have the same hash and this for the Third block because it's a different hash we will have a different block so we implemented this hash based preface",
    "start": "1016880",
    "end": "1023560"
  },
  {
    "text": "caching method in vrm and that is how we uh du the prefix caching in",
    "start": "1023560",
    "end": "1030240"
  },
  {
    "text": "V uh yeah and also yeah multi serving so so we in VM we supported multi Laura",
    "start": "1030240",
    "end": "1037480"
  },
  {
    "text": "multiple Laura adapters can be loaded to VM on the flight and the request to different luras can also be B together",
    "start": "1037480",
    "end": "1043918"
  },
  {
    "text": "and we want to shout out to the esor authors and pun authors and any scale for the contribution of this",
    "start": "1043919",
    "end": "1050799"
  },
  {
    "text": "feature and yeah I think one big user ask is for structured output and Tool use in VM and uh right now in VM we",
    "start": "1050799",
    "end": "1058520"
  },
  {
    "text": "actually have open AI compatibility for the Json output format which is one highlighted feature from open AI",
    "start": "1058520",
    "end": "1064679"
  },
  {
    "text": "recently and also we have extensive Integrations with all different kinds of structur decoding and guided decoding",
    "start": "1064679",
    "end": "1072520"
  },
  {
    "text": "Frameworks for example like like outlines MF and AI also recent we added ability to use",
    "start": "1072520",
    "end": "1080799"
  },
  {
    "text": "two calling template and yeah and also I think as a uh to besides of these you",
    "start": "1080799",
    "end": "1088960"
  },
  {
    "text": "know LM direct feature directly related to LM we want also VM to be a production",
    "start": "1088960",
    "end": "1094840"
  },
  {
    "text": "ready serving engine so we added a lot of like a lot of metrics that you can OB",
    "start": "1094840",
    "end": "1100080"
  },
  {
    "text": "observe directly observe to see whether your VM instance is running correctly or not in your production environment so",
    "start": "1100080",
    "end": "1106280"
  },
  {
    "text": "right now we can use uh you can use uh monitor VM in real time using say say for example tools like raana you can",
    "start": "1106280",
    "end": "1112559"
  },
  {
    "text": "check all different kinds of uh like serving engines like metrics and to see",
    "start": "1112559",
    "end": "1118640"
  },
  {
    "text": "whether your your engine is running healthily or not yeah and that's for the",
    "start": "1118640",
    "end": "1124080"
  },
  {
    "text": "engine features and next I will briefly talk about our distributed system architecture so uh one thing is that V",
    "start": "1124080",
    "end": "1130640"
  },
  {
    "text": "handles distributed inference like for you so from day one we actually support start to support tensor paradism but",
    "start": "1130640",
    "end": "1137039"
  },
  {
    "text": "even just supporting tensor Paralis we found is actually become can be become pretty buggy when you have multiple gpus",
    "start": "1137039",
    "end": "1143440"
  },
  {
    "text": "and especially have multiple nodes and so there are like a lot of like uh we actually fix a lot of bugs related to",
    "start": "1143440",
    "end": "1150280"
  },
  {
    "text": "communication and we have a debugging guide for handing and crashing in",
    "start": "1150280",
    "end": "1155440"
  },
  {
    "text": "v and yeah and also like recently we added support for Pipeline paradism and",
    "start": "1155440",
    "end": "1161039"
  },
  {
    "text": "we shout out to morali from s ML and also snowflake for this effort so starting from V 0.5.1 and VM will start",
    "start": "1161039",
    "end": "1168840"
  },
  {
    "text": "to start to support a pipeline paradism and this is actually best used for larger models spanning across multiple",
    "start": "1168840",
    "end": "1175799"
  },
  {
    "text": "nodes and have low bandwidth connection and we recently also added a",
    "start": "1175799",
    "end": "1181080"
  },
  {
    "text": "cute feature to offload your part of your uh weight to CPU so if you really",
    "start": "1181080",
    "end": "1186799"
  },
  {
    "text": "really want to run a very big model on a small small gpus then you can offload part of your weight to CPU to actually",
    "start": "1186799",
    "end": "1194520"
  },
  {
    "text": "execute the model so although this can be much slower then directly like running on a full bigger GPU but it's",
    "start": "1194520",
    "end": "1201200"
  },
  {
    "text": "still like if you only have a limited resource this can help you actually run the",
    "start": "1201200",
    "end": "1206600"
  },
  {
    "text": "model and also like one thing we we learned recently is that so we did a lot",
    "start": "1206600",
    "end": "1212640"
  },
  {
    "text": "of optimization on GPU and actually we find out like to fully utilize on the GPU you actually need to pay close",
    "start": "1212640",
    "end": "1218840"
  },
  {
    "text": "attention to everything that is happening on the CPU so gpus can be very very fast but CPUs are often not so we",
    "start": "1218840",
    "end": "1225320"
  },
  {
    "text": "actually in our latest blog post we been we did a lot of optimization to minimize",
    "start": "1225320",
    "end": "1230480"
  },
  {
    "text": "the CPU overhead and the these optimization techniques include multi-step scheduling Asing output",
    "start": "1230480",
    "end": "1236919"
  },
  {
    "text": "processing efficient API server and also Z Zer mq based entry point so feel free",
    "start": "1236919",
    "end": "1242080"
  },
  {
    "text": "to take a look at our blog post to learn more about these recent uh system architecture",
    "start": "1242080",
    "end": "1249600"
  },
  {
    "text": "optimizations and lastly I would like to highlight our this one year of like open",
    "start": "1249679",
    "end": "1255679"
  },
  {
    "text": "source Community the development on that part so we would like to always want to make",
    "start": "1255679",
    "end": "1261080"
  },
  {
    "text": "sure our contributors and users can easily start to use vrm and start to develop vrm so if you want to use the",
    "start": "1261080",
    "end": "1268400"
  },
  {
    "text": "latest release you can actually uh simply install VM as a p package use P",
    "start": "1268400",
    "end": "1274080"
  },
  {
    "text": "install and also you can you can directly use VM uh Docker from dockerhub",
    "start": "1274080",
    "end": "1279640"
  },
  {
    "text": "and also we have a pre-commit every comit will come with its doer images and",
    "start": "1279640",
    "end": "1285240"
  },
  {
    "text": "wheels for like with the latest Spar fixes and also we recently added a small feature that if you only want to",
    "start": "1285240",
    "end": "1291520"
  },
  {
    "text": "developing on top of V without touching the kernels you can have you can do you draun this command to start to do python",
    "start": "1291520",
    "end": "1299200"
  },
  {
    "text": "only development so you you don't need to compile the ca Kos and you can develop much much",
    "start": "1299200",
    "end": "1305279"
  },
  {
    "text": "faster and also like we added a cicd for all of the test all of the commits we",
    "start": "1305279",
    "end": "1311159"
  },
  {
    "text": "have and every commit is now fully tested with thousand of test and we would especially want to shout out to",
    "start": "1311159",
    "end": "1316600"
  },
  {
    "text": "Kevin L from any scale to for maintain is uh for maintaining this cicd and also we want to thank all of our computer",
    "start": "1316600",
    "end": "1322919"
  },
  {
    "text": "sponsors which can sponsor our very expensive GPU based cicd cicd Pipeline and also we recently",
    "start": "1322919",
    "end": "1330520"
  },
  {
    "text": "added a performance checking since performance is so important and everyone is saying they are faster than vrm so we",
    "start": "1330520",
    "end": "1335960"
  },
  {
    "text": "would like to make sure that yeah and we would like to make sure that this is uh we would like to keep cheack our",
    "start": "1335960",
    "end": "1341760"
  },
  {
    "text": "performance and have confidence of oursel so we actually set it up performance actually thanks to Quai",
    "start": "1341760",
    "end": "1346960"
  },
  {
    "text": "performance Benchmark checking so you can check go to this website and see the performance of every commit and see",
    "start": "1346960",
    "end": "1352000"
  },
  {
    "text": "whether your commit is giving you a performance regression or not and one thing we want to say is that VM is",
    "start": "1352000",
    "end": "1359400"
  },
  {
    "text": "always owned and driven by community so we would like happy to announce that we are actually in the process of joining",
    "start": "1359400",
    "end": "1365240"
  },
  {
    "text": "the Linux foundation and also we are uh we host meetups every two months and",
    "start": "1365240",
    "end": "1370960"
  },
  {
    "text": "also we have office hours and or every two weeks so if you are interested in VM beside of this event and and you can",
    "start": "1370960",
    "end": "1378559"
  },
  {
    "text": "also feel free to participate all these events to help us and and most",
    "start": "1378559",
    "end": "1384120"
  },
  {
    "text": "importantly we would like to thank our contributors so we would like to thanks to our more than 560 contributors who",
    "start": "1384120",
    "end": "1390480"
  },
  {
    "text": "have raised issues participated in discussions and submitted PRS and I would like to especially shout out to",
    "start": "1390480",
    "end": "1396200"
  },
  {
    "text": "these 20 commit committers of VM they they are actually the people who actually review your PRS and also make",
    "start": "1396200",
    "end": "1403200"
  },
  {
    "text": "sure like they pass test and has spent a lot of their efforts on maintaining the VM project and yeah I would like to a",
    "start": "1403200",
    "end": "1410279"
  },
  {
    "text": "round of applause of for [Applause]",
    "start": "1410279",
    "end": "1418799"
  },
  {
    "text": "the okay I think one thing I would like to really highlight of VM is that we have a lot of uh we have actually we are",
    "start": "1418799",
    "end": "1427440"
  },
  {
    "text": "a two Community project so first from the left hand figure is the absolute number of the major PRS from major PRS",
    "start": "1427440",
    "end": "1435559"
  },
  {
    "text": "of VM happens every month we can see that there are more than 200 PRS being merged every month and also like on the",
    "start": "1435559",
    "end": "1443240"
  },
  {
    "text": "right hand side actually the percentage of the contributors where does the percent contributor come from you can",
    "start": "1443240",
    "end": "1448679"
  },
  {
    "text": "see that at the beginning okay most of the PRS are from UC Berkeley and but like to today like actually a lot of the",
    "start": "1448679",
    "end": "1455080"
  },
  {
    "text": "pr are actually not from the UC berky and from like a lot of like active community members from different",
    "start": "1455080",
    "end": "1460200"
  },
  {
    "text": "companies so we are really happy and it's I think it's a pretty rare to see that this project has grown into a truly",
    "start": "1460200",
    "end": "1467520"
  },
  {
    "text": "true community project okay and yeah about adopters yeah I think it's",
    "start": "1467520",
    "end": "1472559"
  },
  {
    "text": "actually pretty hard for open source project to check okay who is actually using the project because you can just keep install and use and we will",
    "start": "1472559",
    "end": "1478640"
  },
  {
    "text": "actually never know but I think unfor uh fortunately we we are still getting a lot of like uh queries and a lot a lot",
    "start": "1478640",
    "end": "1486360"
  },
  {
    "text": "of people are telling us that they are actually using the using vrm and that's also always makes us very excited and",
    "start": "1486360",
    "end": "1493399"
  },
  {
    "text": "want makes us want to continue to work on this great project and I think one thing to highlight from these adopters",
    "start": "1493399",
    "end": "1500360"
  },
  {
    "text": "is that we have actually been prized by on many many clouds including the major",
    "start": "1500360",
    "end": "1505880"
  },
  {
    "text": "clouds AWS stage maker aurus AI studio and Google Cloud vertex AI model garden",
    "start": "1505880",
    "end": "1511960"
  },
  {
    "text": "and also we are also the default template for the runport and yeah and lastly I would like",
    "start": "1511960",
    "end": "1518520"
  },
  {
    "text": "to thank all of our sponsors and we mentioned that our cicd has a great cost because it's running on GPU and it's all",
    "start": "1518520",
    "end": "1525240"
  },
  {
    "text": "possible because of our uh sponsors who spons sponsors us funding and compute",
    "start": "1525240",
    "end": "1531039"
  },
  {
    "text": "yeah and yeah that's all for the recap part and I next I would like to hand over to quinti to talk about what will",
    "start": "1531039",
    "end": "1537960"
  },
  {
    "text": "happen next for VM in the coming quarter and coming years",
    "start": "1537960",
    "end": "1543760"
  },
  {
    "text": "[Applause]",
    "start": "1543760",
    "end": "1549320"
  },
  {
    "text": "thanks yeah hi everyone I'm kunai and I'm going to talk you about the path",
    "start": "1549320",
    "end": "1554360"
  },
  {
    "text": "forward for VM and there's a QR code on the top right corner of the screen feel free to scan it and uh we there there",
    "start": "1554360",
    "end": "1561600"
  },
  {
    "text": "will be a GitHub issue and we are always open for feedbacks okay so let's G let's briefly",
    "start": "1561600",
    "end": "1568279"
  },
  {
    "text": "talk about the Q4 road map of VM so in Q4 VM will continue adding support to",
    "start": "1568279",
    "end": "1574559"
  },
  {
    "text": "various type of models including the vision language models and state Spas models and also we will add API so that",
    "start": "1574559",
    "end": "1581240"
  },
  {
    "text": "people can add reward models into VM in the future and Hardware wise vrm will",
    "start": "1581240",
    "end": "1587399"
  },
  {
    "text": "keep improving the performance on various Hardware platforms like Nvidia h200 and Mi 300X we will we also work on",
    "start": "1587399",
    "end": "1596159"
  },
  {
    "text": "the page detention Cel on infia and a series of enhancements on TPU and we'll",
    "start": "1596159",
    "end": "1601279"
  },
  {
    "text": "also Upstream the Intel G soon performance wise we aim to turn on chunk",
    "start": "1601279",
    "end": "1606720"
  },
  {
    "text": "previe and prefix caching on by default also we will make structure decoding",
    "start": "1606720",
    "end": "1611760"
  },
  {
    "text": "faster and fuse Communications to reduce the overhead we will keep on enhancing the badged inference performance and and",
    "start": "1611760",
    "end": "1618320"
  },
  {
    "text": "do more kernel optimizations um production wise we will support KV cach offload into CPU so that",
    "start": "1618320",
    "end": "1625360"
  },
  {
    "text": "you have larger K cash space and uh we will also support KB cash transfer which is good for disaggregate prefilling also",
    "start": "1625360",
    "end": "1632760"
  },
  {
    "text": "we will adjust the cash in policy and the scheduler for a higher cash hit rate",
    "start": "1632760",
    "end": "1637960"
  },
  {
    "text": "and we'll also ship a series of schedule spec decoding optimizations uh and V is also going",
    "start": "1637960",
    "end": "1645559"
  },
  {
    "text": "through a major re architecture but rest assured it will be fast so we will develop a VM engine V2 which is asnc",
    "start": "1645559",
    "end": "1653120"
  },
  {
    "text": "which includes asnc scheduling and a prefix casing Centric design I'll will talk about them very soon and uh we also",
    "start": "1653120",
    "end": "1659600"
  },
  {
    "text": "implemented torch compile full support of it so that you can optimize your models and also we have new memory",
    "start": "1659600",
    "end": "1665720"
  },
  {
    "text": "manager for multimodality and last we want to focus that uh we are always there to support",
    "start": "1665720",
    "end": "1672799"
  },
  {
    "text": "our OSS Community we will further working on the performance Benchmark enhancement and also better development",
    "start": "1672799",
    "end": "1679000"
  },
  {
    "text": "DOC for contribution and research okay so uh this is the first",
    "start": "1679000",
    "end": "1684519"
  },
  {
    "text": "rearch item which is asynchron scheduling so basically it's scheduling in The Next Step while executing the",
    "start": "1684519",
    "end": "1690679"
  },
  {
    "text": "current step it res in much less GPU idle time because GPU can immediately",
    "start": "1690679",
    "end": "1696039"
  },
  {
    "text": "start executing the next step after finishing the current step so to make a",
    "start": "1696039",
    "end": "1701320"
  },
  {
    "text": "contrast before uh we will execute the step and then schedule the step which is",
    "start": "1701320",
    "end": "1707200"
  },
  {
    "text": "this the scheduling and execution are piped sequentially but after the rearch",
    "start": "1707200",
    "end": "1712600"
  },
  {
    "text": "it will be a more asynchronous architecture where the scheduling happens while the execution is happening",
    "start": "1712600",
    "end": "1719760"
  },
  {
    "text": "so that we have a much less GPU idle time and also we are also using re",
    "start": "1719760",
    "end": "1725919"
  },
  {
    "text": "prefix caching to simplify VM so this is because in current vrm there's a very",
    "start": "1725919",
    "end": "1731640"
  },
  {
    "text": "complicated logic to handle parallel sampling or preemption and in the future we will use prefix caching to",
    "start": "1731640",
    "end": "1738440"
  },
  {
    "text": "automatically identify reusable KB caches in the parallel sampling or preemption scenario so that we don't",
    "start": "1738440",
    "end": "1744640"
  },
  {
    "text": "need to handle them by ourselves to make a contrast before the scheduler directly talk to the parallel sampling and",
    "start": "1744640",
    "end": "1751640"
  },
  {
    "text": "preemption and preface caching and now after that the scheduler will directly talk to the preface caching module and",
    "start": "1751640",
    "end": "1758360"
  },
  {
    "text": "the parallel sampling KB cach reusing opportunities and also for preemption will be identified automatically by the",
    "start": "1758360",
    "end": "1764640"
  },
  {
    "text": "preface caching module and also we are working on a rearch that we re rearch the memory",
    "start": "1764640",
    "end": "1771360"
  },
  {
    "text": "allocator for different types of KV caches this is because we observing a very severe GPU memory waste due to",
    "start": "1771360",
    "end": "1778080"
  },
  {
    "text": "incompatible KV cach size so take the new llama model as an example the text",
    "start": "1778080",
    "end": "1783399"
  },
  {
    "text": "it has KV cach for all layers but the image it only has KV caches for 1/4th of",
    "start": "1783399",
    "end": "1788559"
  },
  {
    "text": "the layer and we are allocating actually allocating memory for all layers so there's a memory waste there and the",
    "start": "1788559",
    "end": "1795760"
  },
  {
    "text": "ongoing part is that we will have a new memory locator that can handle different shapes of tensors with ease the key idea",
    "start": "1795760",
    "end": "1802600"
  },
  {
    "text": "is that we will first partition the GPU memory into pages with a larger size and",
    "start": "1802600",
    "end": "1808600"
  },
  {
    "text": "then further partion each large page into different sizes to handle different KV cat size so we have a Illustrated",
    "start": "1808600",
    "end": "1816000"
  },
  {
    "text": "figure here basically we will have two level of page tables the first level we",
    "start": "1816000",
    "end": "1821159"
  },
  {
    "text": "have a larger K larger page size which is six K kilobytes in this example and",
    "start": "1821159",
    "end": "1826880"
  },
  {
    "text": "then we will have a more fine R uh level one block memory allocator for different",
    "start": "1826880",
    "end": "1832440"
  },
  {
    "text": "KB cach shapes okay and we will also enable full",
    "start": "1832440",
    "end": "1837679"
  },
  {
    "text": "support for torch compile currently vom is manually optimizing on the most popular models for example customized",
    "start": "1837679",
    "end": "1844320"
  },
  {
    "text": "Coda kernels and buffer reusing for llama models um and our ongoing effort we will",
    "start": "1844320",
    "end": "1850559"
  },
  {
    "text": "use the torch compile to optimize all models so that your customized models and architectures will run much faster",
    "start": "1850559",
    "end": "1858039"
  },
  {
    "text": "and this will also enable new research opportunities to make a contrast before",
    "start": "1858039",
    "end": "1863639"
  },
  {
    "text": "the current vom will deeply optimize on the most popular models and left some of the models unoptimized and after that",
    "start": "1863639",
    "end": "1870880"
  },
  {
    "text": "all of the models will be deeply optimized by torch compile down to the qu kernal",
    "start": "1870880",
    "end": "1876240"
  },
  {
    "text": "level and we will also shape even better speculative decoding so the current speculative decoding is still great but",
    "start": "1876240",
    "end": "1883440"
  },
  {
    "text": "when the qpi is high it may hurt performance and what coming soon is that we we will in include a include a",
    "start": "1883440",
    "end": "1889919"
  },
  {
    "text": "dynamic speculative decoding then make sure the speculative decoding always makes VM faster to make a contrast the",
    "start": "1889919",
    "end": "1896760"
  },
  {
    "text": "current vom the spectular decoding will be better until the qpi is really high",
    "start": "1896760",
    "end": "1902639"
  },
  {
    "text": "but after that the the speculative decoding will always be better regardless of the workload and uh we will also store more",
    "start": "1902639",
    "end": "1910679"
  },
  {
    "text": "K caches into CPU uh so that you have you'll have more KB caches for your multi conversation",
    "start": "1910679",
    "end": "1916639"
  },
  {
    "text": "and also uh for your system prompts this is because the current VM Only Stores KB",
    "start": "1916639",
    "end": "1922679"
  },
  {
    "text": "caches in GPU and ongoing effort is we offload the KV caches Auto into CPU and",
    "start": "1922679",
    "end": "1929159"
  },
  {
    "text": "uh you can even part vom with KV cach database like LM cach and RIS for even more KV cach storage to make a contrast",
    "start": "1929159",
    "end": "1936639"
  },
  {
    "text": "before we V will only store KV cach on GPU for future reuse but now you can",
    "start": "1936639",
    "end": "1942880"
  },
  {
    "text": "store your KB cach in GPU CPU and even optionally you can store it in the remote database",
    "start": "1942880",
    "end": "1949159"
  },
  {
    "text": "and also this will be the last feature yeah and also uh the last feature will be disagre prefilling and this is doing",
    "start": "1949159",
    "end": "1957120"
  },
  {
    "text": "prefilling and decoding on different gpus so we are going to support it because SE because of several reasons",
    "start": "1957120",
    "end": "1963360"
  },
  {
    "text": "first it allow us to configure the paraly paralyzation strategy for prefill and decode separately and also it will",
    "start": "1963360",
    "end": "1971200"
  },
  {
    "text": "result in a much lower tail into token latency and there's no need to configure any hyper parameters compared to Chun",
    "start": "1971200",
    "end": "1977200"
  },
  {
    "text": "preview of of course and uh there will be better you this will be the best use",
    "start": "1977200",
    "end": "1982240"
  },
  {
    "text": "case when you have multiple types of gpus and before the prefill and decode are running simultaneously on the GPU",
    "start": "1982240",
    "end": "1988720"
  },
  {
    "text": "but after that each GPU will only do either prefill or decod okay so let me reiterate our goal",
    "start": "1988720",
    "end": "1995679"
  },
  {
    "text": "is to build the fastest and easiest to use open source L inference and serving engine and we are confident that we are",
    "start": "1995679",
    "end": "2001880"
  },
  {
    "text": "on the right track and now we are ready for questions yeah thank you",
    "start": "2001880",
    "end": "2008600"
  },
  {
    "text": "all right so uh while the next speaker is setting up like if you have any",
    "start": "2009080",
    "end": "2014600"
  },
  {
    "text": "questions we'll take probably one or two quick questions here and overall will be",
    "start": "2014600",
    "end": "2020240"
  },
  {
    "text": "around in the track as well so any questions",
    "start": "2020240",
    "end": "2026159"
  },
  {
    "text": "comments thank thanks for the great talk I have a question about like having our own lot uh mostly mostly my question is",
    "start": "2027639",
    "end": "2034720"
  },
  {
    "text": "wrong scheduling so I know there's a scheduler inside that can do either like based on priority or based on first in",
    "start": "2034720",
    "end": "2041840"
  },
  {
    "text": "first out but imagine like uh for business use case we have like maybe",
    "start": "2041840",
    "end": "2047440"
  },
  {
    "text": "more uh complicated scheding logic maybe it will based on like customer or based on current ID like either like what is",
    "start": "2047440",
    "end": "2055040"
  },
  {
    "text": "the easy way for uh VM to sport sing like would it be easy for VM to expose",
    "start": "2055040",
    "end": "2060638"
  },
  {
    "text": "some uh some uh interface that people can part their own schedule logic or will like eer that implement is by Will",
    "start": "2060639",
    "end": "2068638"
  },
  {
    "text": "himself uh yeah I think that's a very good question I think that's also one goal of our re architecture so we want",
    "start": "2068639",
    "end": "2074760"
  },
  {
    "text": "to make sure that the scheduler interface is clean so people can easily plug in their own scheduler I think at",
    "start": "2074760",
    "end": "2081000"
  },
  {
    "text": "the high level that's is the the answer to a question but we are also actively working on that interface so stay tuned",
    "start": "2081000",
    "end": "2087000"
  },
  {
    "text": "and we will have more updates on that part eventually it will be an interface that people can Implement their own like",
    "start": "2087000",
    "end": "2092560"
  },
  {
    "text": "scheduling logic yeah I think at beginning should be something like a just a schedule class or something like",
    "start": "2092560",
    "end": "2098040"
  },
  {
    "text": "that instead of like you know direct like user interface but we will eventually make it more modular and",
    "start": "2098040",
    "end": "2103880"
  },
  {
    "text": "customizable right thank you for the good question if I have more question uh please we'll take it offline given the",
    "start": "2103880",
    "end": "2110119"
  },
  {
    "text": "time constraint but and thank you so much for joining the first talk and then now we'll set up the second",
    "start": "2110119",
    "end": "2116960"
  },
  {
    "text": "talk thank you yeah thank you every",
    "start": "2116960",
    "end": "2121880"
  }
]