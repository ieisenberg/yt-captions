[
  {
    "start": "0",
    "end": "25000"
  },
  {
    "text": "hello i'm very excited to be here and",
    "start": "1439",
    "end": "3520"
  },
  {
    "text": "talk about",
    "start": "3520",
    "end": "4319"
  },
  {
    "text": "some of the work that my colleagues and",
    "start": "4319",
    "end": "6080"
  },
  {
    "text": "i have been doing in google brains",
    "start": "6080",
    "end": "7919"
  },
  {
    "text": "machine learning course systems team",
    "start": "7919",
    "end": "11120"
  },
  {
    "text": "our motivation is that in the past",
    "start": "11120",
    "end": "13120"
  },
  {
    "text": "decade systems and hardware have truly",
    "start": "13120",
    "end": "16080"
  },
  {
    "text": "transformed machine learning and it is",
    "start": "16080",
    "end": "18640"
  },
  {
    "text": "now time for",
    "start": "18640",
    "end": "19760"
  },
  {
    "text": "machine learning to return the favor and",
    "start": "19760",
    "end": "22080"
  },
  {
    "text": "transform the way systems and hardware",
    "start": "22080",
    "end": "24080"
  },
  {
    "text": "are designed",
    "start": "24080",
    "end": "26400"
  },
  {
    "start": "25000",
    "end": "25000"
  },
  {
    "text": "uh so why do we care about compute it",
    "start": "26400",
    "end": "28720"
  },
  {
    "text": "turns out that it's actually a very",
    "start": "28720",
    "end": "30240"
  },
  {
    "text": "important problem",
    "start": "30240",
    "end": "31679"
  },
  {
    "text": "here are two motivational examples",
    "start": "31679",
    "end": "34719"
  },
  {
    "text": "on the left uh the table projects that",
    "start": "34719",
    "end": "38480"
  },
  {
    "text": "in order to achieve one person accuracy",
    "start": "38480",
    "end": "41120"
  },
  {
    "text": "and image net",
    "start": "41120",
    "end": "42079"
  },
  {
    "text": "we need 10 to the power 28 gigaflop",
    "start": "42079",
    "end": "44879"
  },
  {
    "text": "computational power",
    "start": "44879",
    "end": "46320"
  },
  {
    "text": "in comparison with 10 to the power 14",
    "start": "46320",
    "end": "49200"
  },
  {
    "text": "that we currently use",
    "start": "49200",
    "end": "50719"
  },
  {
    "text": "to achieve the state of the art and the",
    "start": "50719",
    "end": "52719"
  },
  {
    "text": "data on the right shows that since 2012",
    "start": "52719",
    "end": "55680"
  },
  {
    "text": "the amount of computational power using",
    "start": "55680",
    "end": "58000"
  },
  {
    "text": "the largest ai runs doubled",
    "start": "58000",
    "end": "59920"
  },
  {
    "text": "every 3.4 months so this growth is",
    "start": "59920",
    "end": "62719"
  },
  {
    "text": "exponential and you really need to",
    "start": "62719",
    "end": "65119"
  },
  {
    "text": "transfer",
    "start": "65119",
    "end": "66000"
  },
  {
    "text": "form the way you're designing systems",
    "start": "66000",
    "end": "68720"
  },
  {
    "text": "and chips to keep up with this demand",
    "start": "68720",
    "end": "72080"
  },
  {
    "start": "72000",
    "end": "72000"
  },
  {
    "text": "now if we look into the a lot of",
    "start": "72080",
    "end": "74159"
  },
  {
    "text": "problems in systems and hardware",
    "start": "74159",
    "end": "76320"
  },
  {
    "text": "a lot of these problems are in the form",
    "start": "76320",
    "end": "78640"
  },
  {
    "text": "of combinatorial optimization on graph",
    "start": "78640",
    "end": "81040"
  },
  {
    "text": "structure data for example compiler",
    "start": "81040",
    "end": "83840"
  },
  {
    "text": "optimization chip placement data center",
    "start": "83840",
    "end": "86000"
  },
  {
    "text": "resource allocation are three instances",
    "start": "86000",
    "end": "88720"
  },
  {
    "text": "of these problems",
    "start": "88720",
    "end": "90320"
  },
  {
    "text": "in compilers input is an xla or hlo",
    "start": "90320",
    "end": "92960"
  },
  {
    "text": "graph for example and the objective is",
    "start": "92960",
    "end": "95040"
  },
  {
    "text": "some sort of resource scheduling or",
    "start": "95040",
    "end": "97360"
  },
  {
    "text": "allocation on the ops of this graph",
    "start": "97360",
    "end": "99520"
  },
  {
    "text": "and that has been the focus of a lot of",
    "start": "99520",
    "end": "101520"
  },
  {
    "text": "the research and work we have been",
    "start": "101520",
    "end": "103840"
  },
  {
    "text": "up to and our approach is to take a",
    "start": "103840",
    "end": "106960"
  },
  {
    "start": "104000",
    "end": "104000"
  },
  {
    "text": "learning based",
    "start": "106960",
    "end": "108000"
  },
  {
    "text": "uh approach to optimizing these problems",
    "start": "108000",
    "end": "112079"
  },
  {
    "text": "and we are very motivated to do so",
    "start": "112079",
    "end": "115439"
  },
  {
    "text": "because of these fundamental properties",
    "start": "115439",
    "end": "117600"
  },
  {
    "text": "of learning",
    "start": "117600",
    "end": "118159"
  },
  {
    "text": "uh algorithms that they can learn",
    "start": "118159",
    "end": "121280"
  },
  {
    "text": "the underlying relationship between the",
    "start": "121280",
    "end": "123600"
  },
  {
    "text": "context and optimization metric",
    "start": "123600",
    "end": "125600"
  },
  {
    "text": "and implicitly optimize for the and",
    "start": "125600",
    "end": "128479"
  },
  {
    "text": "explore the trade-offs",
    "start": "128479",
    "end": "129759"
  },
  {
    "text": "optimized for the target metrics we care",
    "start": "129759",
    "end": "132080"
  },
  {
    "text": "about the other unique property compared",
    "start": "132080",
    "end": "134800"
  },
  {
    "text": "to the baseline is that they can gain",
    "start": "134800",
    "end": "136879"
  },
  {
    "text": "experience and can solve more instances",
    "start": "136879",
    "end": "140160"
  },
  {
    "text": "of the problem",
    "start": "140160",
    "end": "141280"
  },
  {
    "text": "and become experts over time this is",
    "start": "141280",
    "end": "143680"
  },
  {
    "text": "very different",
    "start": "143680",
    "end": "144560"
  },
  {
    "text": "from the baseline method such as branch",
    "start": "144560",
    "end": "146560"
  },
  {
    "text": "and bound hill climb",
    "start": "146560",
    "end": "148160"
  },
  {
    "text": "kill climbing and such and another uh",
    "start": "148160",
    "end": "151120"
  },
  {
    "text": "property",
    "start": "151120",
    "end": "152000"
  },
  {
    "text": "that we like them is that we know how to",
    "start": "152000",
    "end": "154080"
  },
  {
    "text": "scale them and how to",
    "start": "154080",
    "end": "155599"
  },
  {
    "text": "distribute them across distributed",
    "start": "155599",
    "end": "158239"
  },
  {
    "text": "platforms and train models with billions",
    "start": "158239",
    "end": "160319"
  },
  {
    "text": "of parameters",
    "start": "160319",
    "end": "162640"
  },
  {
    "text": "now the outline of this talk is as",
    "start": "162640",
    "end": "164640"
  },
  {
    "text": "follows we're i'm going to talk about",
    "start": "164640",
    "end": "166239"
  },
  {
    "text": "three",
    "start": "166239",
    "end": "166720"
  },
  {
    "text": "uh projects uh in our team on learning",
    "start": "166720",
    "end": "170000"
  },
  {
    "text": "to optimize systems and chips the first",
    "start": "170000",
    "end": "172000"
  },
  {
    "text": "one is learning to optimize device",
    "start": "172000",
    "end": "174239"
  },
  {
    "text": "placement so device placement",
    "start": "174239",
    "end": "177680"
  },
  {
    "text": "is the problem of taking a computational",
    "start": "177680",
    "end": "179920"
  },
  {
    "text": "graph for example",
    "start": "179920",
    "end": "180959"
  },
  {
    "text": "in neural net and mapping it onto the",
    "start": "180959",
    "end": "183680"
  },
  {
    "text": "hardware devices that we have such as",
    "start": "183680",
    "end": "185519"
  },
  {
    "text": "gpus tpus or cpu cores",
    "start": "185519",
    "end": "188400"
  },
  {
    "text": "and as you can imagine these problems",
    "start": "188400",
    "end": "190400"
  },
  {
    "text": "it's becoming more and more important",
    "start": "190400",
    "end": "192080"
  },
  {
    "text": "because",
    "start": "192080",
    "end": "193200"
  },
  {
    "text": "the models are getting bigger and bigger",
    "start": "193200",
    "end": "196720"
  },
  {
    "start": "196000",
    "end": "196000"
  },
  {
    "text": "so our approach to this problem was to",
    "start": "196720",
    "end": "199120"
  },
  {
    "text": "pose it as a reinforcement learning",
    "start": "199120",
    "end": "201120"
  },
  {
    "text": "problem where we train a policy",
    "start": "201120",
    "end": "203440"
  },
  {
    "text": "that takes as input a neural network and",
    "start": "203440",
    "end": "205920"
  },
  {
    "text": "a set of available devices",
    "start": "205920",
    "end": "207920"
  },
  {
    "text": "and it outputs an assignment of the ops",
    "start": "207920",
    "end": "210640"
  },
  {
    "text": "in the neural net onto the devices that",
    "start": "210640",
    "end": "212560"
  },
  {
    "text": "we have",
    "start": "212560",
    "end": "213360"
  },
  {
    "text": "once this assignment is done we can",
    "start": "213360",
    "end": "215599"
  },
  {
    "text": "evaluate we can actually",
    "start": "215599",
    "end": "217200"
  },
  {
    "text": "run the neural net according to this",
    "start": "217200",
    "end": "219599"
  },
  {
    "text": "placement",
    "start": "219599",
    "end": "220560"
  },
  {
    "text": "evaluate the runtime and use it as",
    "start": "220560",
    "end": "222239"
  },
  {
    "text": "feedback to update the policy",
    "start": "222239",
    "end": "224080"
  },
  {
    "text": "and we can iterate through this many",
    "start": "224080",
    "end": "226000"
  },
  {
    "text": "many times until the policy gets",
    "start": "226000",
    "end": "227840"
  },
  {
    "text": "optimized and outputs",
    "start": "227840",
    "end": "229519"
  },
  {
    "text": "optimized solutions um here is an",
    "start": "229519",
    "end": "232480"
  },
  {
    "text": "overview of the architecture we use in",
    "start": "232480",
    "end": "234480"
  },
  {
    "text": "the",
    "start": "234480",
    "end": "234879"
  },
  {
    "text": "as in the input we pass the operations",
    "start": "234879",
    "end": "237280"
  },
  {
    "text": "in the neural net",
    "start": "237280",
    "end": "238400"
  },
  {
    "text": "information about the type of it whether",
    "start": "238400",
    "end": "240480"
  },
  {
    "text": "it's a convolution elastium and such",
    "start": "240480",
    "end": "242879"
  },
  {
    "text": "the adjacency information um and",
    "start": "242879",
    "end": "246239"
  },
  {
    "text": "we pass it to a uh neural net",
    "start": "246239",
    "end": "249439"
  },
  {
    "text": "we that groups these ops in order to",
    "start": "249439",
    "end": "251840"
  },
  {
    "text": "handle the scale",
    "start": "251840",
    "end": "253120"
  },
  {
    "text": "and then we pass this grouping info",
    "start": "253120",
    "end": "255599"
  },
  {
    "text": "information",
    "start": "255599",
    "end": "256400"
  },
  {
    "text": "into an seek to seek layer where on the",
    "start": "256400",
    "end": "259040"
  },
  {
    "text": "output side",
    "start": "259040",
    "end": "260000"
  },
  {
    "text": "we predict the device that each group",
    "start": "260000",
    "end": "263840"
  },
  {
    "text": "should be placed onto once all these",
    "start": "263840",
    "end": "266720"
  },
  {
    "text": "devices are predicted",
    "start": "266720",
    "end": "268080"
  },
  {
    "text": "of course they're conditioned on each",
    "start": "268080",
    "end": "269600"
  },
  {
    "text": "other we can have the full placement we",
    "start": "269600",
    "end": "271919"
  },
  {
    "text": "can go ahead and run the graph measure",
    "start": "271919",
    "end": "274000"
  },
  {
    "text": "the runtime",
    "start": "274000",
    "end": "274960"
  },
  {
    "text": "and have that as our reverse reward",
    "start": "274960",
    "end": "277280"
  },
  {
    "text": "function a form of a reward function",
    "start": "277280",
    "end": "279360"
  },
  {
    "text": "in this case the inverse of the time and",
    "start": "279360",
    "end": "281919"
  },
  {
    "text": "use it to update the parameters of this",
    "start": "281919",
    "end": "283840"
  },
  {
    "text": "neural net which",
    "start": "283840",
    "end": "284960"
  },
  {
    "text": "models the policy here is uh some",
    "start": "284960",
    "end": "287759"
  },
  {
    "text": "results on",
    "start": "287759",
    "end": "288560"
  },
  {
    "text": "a neural machine translation and a seek",
    "start": "288560",
    "end": "291199"
  },
  {
    "text": "to seek",
    "start": "291199",
    "end": "292000"
  },
  {
    "text": "model and each of the five colors here",
    "start": "292000",
    "end": "295600"
  },
  {
    "text": "shows a different device four of them",
    "start": "295600",
    "end": "297360"
  },
  {
    "text": "are gpus and the white one is a cpu",
    "start": "297360",
    "end": "299680"
  },
  {
    "text": "it turns out that the policy actually",
    "start": "299680",
    "end": "302160"
  },
  {
    "text": "learns to place",
    "start": "302160",
    "end": "303680"
  },
  {
    "text": "um the ops the embedding layer on the",
    "start": "303680",
    "end": "306800"
  },
  {
    "text": "cpu",
    "start": "306800",
    "end": "307919"
  },
  {
    "text": "but the other placement are not very",
    "start": "307919",
    "end": "309680"
  },
  {
    "text": "intuitive but we were getting better",
    "start": "309680",
    "end": "311680"
  },
  {
    "text": "results than our baseline method i'm",
    "start": "311680",
    "end": "313600"
  },
  {
    "start": "312000",
    "end": "312000"
  },
  {
    "text": "showing the baseline results on the",
    "start": "313600",
    "end": "314960"
  },
  {
    "text": "right",
    "start": "314960",
    "end": "315600"
  },
  {
    "text": "uh where the it was designed by the",
    "start": "315600",
    "end": "317919"
  },
  {
    "text": "human experts",
    "start": "317919",
    "end": "319199"
  },
  {
    "text": "and the way it was done at the time was",
    "start": "319199",
    "end": "321680"
  },
  {
    "text": "to put every layer",
    "start": "321680",
    "end": "322960"
  },
  {
    "text": "of this neural net on a different gpu",
    "start": "322960",
    "end": "326000"
  },
  {
    "text": "um we try to understand why we are",
    "start": "326000",
    "end": "329199"
  },
  {
    "text": "getting better runtime results and",
    "start": "329199",
    "end": "330960"
  },
  {
    "text": "we profiled the runtime of each gpu and",
    "start": "330960",
    "end": "334080"
  },
  {
    "text": "it turned and here we were",
    "start": "334080",
    "end": "335919"
  },
  {
    "text": "placing them on four different gpus of",
    "start": "335919",
    "end": "338320"
  },
  {
    "text": "the same",
    "start": "338320",
    "end": "339120"
  },
  {
    "text": "kind and what we saw is that our the rl",
    "start": "339120",
    "end": "342080"
  },
  {
    "text": "approach",
    "start": "342080",
    "end": "342639"
  },
  {
    "text": "um is really focusing on workload",
    "start": "342639",
    "end": "345440"
  },
  {
    "text": "balancing",
    "start": "345440",
    "end": "346400"
  },
  {
    "text": "whereas the the baseline by expert even",
    "start": "346400",
    "end": "349120"
  },
  {
    "text": "though we were very balanced on the feed",
    "start": "349120",
    "end": "351039"
  },
  {
    "text": "forward pass",
    "start": "351039",
    "end": "352400"
  },
  {
    "text": "be very nice balance on the the back",
    "start": "352400",
    "end": "354639"
  },
  {
    "text": "prop which",
    "start": "354639",
    "end": "355840"
  },
  {
    "text": "consumes most of the computation and we",
    "start": "355840",
    "end": "358160"
  },
  {
    "text": "were getting this unbalanced",
    "start": "358160",
    "end": "360000"
  },
  {
    "text": "uh workload across the gpus and",
    "start": "360000",
    "end": "363199"
  },
  {
    "text": "therefore",
    "start": "363199",
    "end": "363840"
  },
  {
    "text": "the runtime was super the run time",
    "start": "363840",
    "end": "367280"
  },
  {
    "text": "for the replacement was better we did",
    "start": "367280",
    "end": "370080"
  },
  {
    "text": "the same thing on an",
    "start": "370080",
    "end": "371280"
  },
  {
    "text": "inception model an image model and each",
    "start": "371280",
    "end": "373759"
  },
  {
    "text": "of the",
    "start": "373759",
    "end": "375840"
  },
  {
    "text": "colors again different colors are",
    "start": "375840",
    "end": "377600"
  },
  {
    "text": "different gpus and the white",
    "start": "377600",
    "end": "379039"
  },
  {
    "text": "one is a cpu as you can see their policy",
    "start": "379039",
    "end": "382479"
  },
  {
    "text": "did learn",
    "start": "382479",
    "end": "383199"
  },
  {
    "text": "some sort of a parallelism across these",
    "start": "383199",
    "end": "385280"
  },
  {
    "text": "branches of the",
    "start": "385280",
    "end": "386880"
  },
  {
    "text": "and these are convolution branches in",
    "start": "386880",
    "end": "388560"
  },
  {
    "text": "the inception and without us",
    "start": "388560",
    "end": "390800"
  },
  {
    "text": "explicitly telling it anything about the",
    "start": "390800",
    "end": "393120"
  },
  {
    "text": "parallelism",
    "start": "393120",
    "end": "394400"
  },
  {
    "text": "and in this case we compared the result",
    "start": "394400",
    "end": "396560"
  },
  {
    "text": "with an inception model with a batch",
    "start": "396560",
    "end": "398639"
  },
  {
    "text": "size that was",
    "start": "398639",
    "end": "400000"
  },
  {
    "text": "uh one-fourth of what we were handling",
    "start": "400000",
    "end": "403039"
  },
  {
    "text": "but we use",
    "start": "403039",
    "end": "403840"
  },
  {
    "text": "four copies of this inception model",
    "start": "403840",
    "end": "406560"
  },
  {
    "text": "across four gpus so in the end our",
    "start": "406560",
    "end": "408720"
  },
  {
    "text": "we were having the exact same batch size",
    "start": "408720",
    "end": "410800"
  },
  {
    "text": "but on one side we were doing model",
    "start": "410800",
    "end": "412479"
  },
  {
    "text": "parallelism and on the",
    "start": "412479",
    "end": "414000"
  },
  {
    "text": "right side we were doing data parallels",
    "start": "414000",
    "end": "415840"
  },
  {
    "text": "and so in this case",
    "start": "415840",
    "end": "417039"
  },
  {
    "text": "uh we were still getting better runtimes",
    "start": "417039",
    "end": "419120"
  },
  {
    "text": "than that um",
    "start": "419120",
    "end": "420319"
  },
  {
    "text": "baseline but uh for sure we were not",
    "start": "420319",
    "end": "423599"
  },
  {
    "text": "doing as good of a job in load balancing",
    "start": "423599",
    "end": "425759"
  },
  {
    "text": "because the baseline by design was load",
    "start": "425759",
    "end": "428240"
  },
  {
    "text": "balanced",
    "start": "428240",
    "end": "429039"
  },
  {
    "text": "so we looked into memory copies the",
    "start": "429039",
    "end": "431039"
  },
  {
    "text": "amount of memory copies that the rl",
    "start": "431039",
    "end": "432880"
  },
  {
    "text": "policy does",
    "start": "432880",
    "end": "434000"
  },
  {
    "text": "and it turns out that in this this case",
    "start": "434000",
    "end": "435840"
  },
  {
    "text": "the policy learned",
    "start": "435840",
    "end": "437360"
  },
  {
    "text": "to optimize for reducing the mem copies",
    "start": "437360",
    "end": "439599"
  },
  {
    "text": "whereas the",
    "start": "439599",
    "end": "440560"
  },
  {
    "text": "the data parallel approach had to do a",
    "start": "440560",
    "end": "443680"
  },
  {
    "text": "lot of synchronization",
    "start": "443680",
    "end": "445440"
  },
  {
    "text": "uh across the gpus so it was very",
    "start": "445440",
    "end": "448800"
  },
  {
    "text": "interesting for us to",
    "start": "448800",
    "end": "450000"
  },
  {
    "text": "see that the policy learns these",
    "start": "450000",
    "end": "451440"
  },
  {
    "text": "implicit trade-offs um",
    "start": "451440",
    "end": "453360"
  },
  {
    "text": "and our next step was to uh",
    "start": "453360",
    "end": "456479"
  },
  {
    "text": "go and really try to train policies that",
    "start": "456479",
    "end": "459120"
  },
  {
    "text": "generalize",
    "start": "459120",
    "end": "460240"
  },
  {
    "text": "across different devices because so far",
    "start": "460240",
    "end": "462240"
  },
  {
    "text": "we were really optimizing",
    "start": "462240",
    "end": "464000"
  },
  {
    "text": "a policy for a single neural net so we",
    "start": "464000",
    "end": "467039"
  },
  {
    "text": "could",
    "start": "467039",
    "end": "467759"
  },
  {
    "text": "write a objective function as an",
    "start": "467759",
    "end": "470080"
  },
  {
    "text": "expected reward over a training set of",
    "start": "470080",
    "end": "473039"
  },
  {
    "text": "neuralness and we got very interesting",
    "start": "473039",
    "end": "474800"
  },
  {
    "text": "results for example here",
    "start": "474800",
    "end": "476560"
  },
  {
    "text": "um the dark the black one shows uh where",
    "start": "476560",
    "end": "480080"
  },
  {
    "text": "the",
    "start": "480080",
    "end": "481039"
  },
  {
    "text": "method is uh trained and customized for",
    "start": "481039",
    "end": "483919"
  },
  {
    "text": "a specific",
    "start": "483919",
    "end": "485199"
  },
  {
    "text": "um graph and the one that is red",
    "start": "485199",
    "end": "489199"
  },
  {
    "text": "the red one shows the generalization of",
    "start": "489199",
    "end": "491280"
  },
  {
    "text": "zero shots so we just take the",
    "start": "491280",
    "end": "492800"
  },
  {
    "text": "pre-trained policy and at",
    "start": "492800",
    "end": "494400"
  },
  {
    "text": "inference uh do model parallelism on the",
    "start": "494400",
    "end": "498000"
  },
  {
    "text": "new neural net",
    "start": "498000",
    "end": "499039"
  },
  {
    "text": "and we got very convincing results in",
    "start": "499039",
    "end": "501120"
  },
  {
    "text": "this case uh",
    "start": "501120",
    "end": "502400"
  },
  {
    "text": "and a lot of speed ups like 15 times",
    "start": "502400",
    "end": "505520"
  },
  {
    "start": "503000",
    "end": "503000"
  },
  {
    "text": "speed up",
    "start": "505520",
    "end": "506319"
  },
  {
    "text": "in search and tens of",
    "start": "506319",
    "end": "509599"
  },
  {
    "text": "percent of optimization in actual",
    "start": "509599",
    "end": "512080"
  },
  {
    "text": "runtime",
    "start": "512080",
    "end": "513039"
  },
  {
    "text": "of the of the neural net so it was",
    "start": "513039",
    "end": "514959"
  },
  {
    "text": "helping in both speeding up the search",
    "start": "514959",
    "end": "516959"
  },
  {
    "text": "and",
    "start": "516959",
    "end": "517200"
  },
  {
    "text": "also getting better quality of results",
    "start": "517200",
    "end": "520320"
  },
  {
    "text": "and if you're interested please take a",
    "start": "520320",
    "end": "521680"
  },
  {
    "text": "look at these papers that i've listed",
    "start": "521680",
    "end": "523360"
  },
  {
    "text": "here",
    "start": "523360",
    "end": "524159"
  },
  {
    "text": "now um the another project that we have",
    "start": "524159",
    "end": "527600"
  },
  {
    "text": "uh focused on",
    "start": "527600",
    "end": "529120"
  },
  {
    "text": "on was learn and we worked on was",
    "start": "529120",
    "end": "531839"
  },
  {
    "text": "learning to partition graphs and graph",
    "start": "531839",
    "end": "534320"
  },
  {
    "start": "534000",
    "end": "534000"
  },
  {
    "text": "partitioning of course is an important",
    "start": "534320",
    "end": "535920"
  },
  {
    "text": "problem",
    "start": "535920",
    "end": "536399"
  },
  {
    "text": "appears um over and over in systems and",
    "start": "536399",
    "end": "539600"
  },
  {
    "text": "chip related problems",
    "start": "539600",
    "end": "541440"
  },
  {
    "text": "and usually",
    "start": "541440",
    "end": "544560"
  },
  {
    "text": "when we deal with a graph partitioning",
    "start": "544560",
    "end": "546560"
  },
  {
    "text": "problem we want to scale down the",
    "start": "546560",
    "end": "548399"
  },
  {
    "text": "problem and work on the clusters or",
    "start": "548399",
    "end": "550320"
  },
  {
    "text": "partitions",
    "start": "550320",
    "end": "551279"
  },
  {
    "text": "rather than individual nodes of the",
    "start": "551279",
    "end": "552959"
  },
  {
    "text": "graph and a classic",
    "start": "552959",
    "end": "554560"
  },
  {
    "text": "approach to do so is by optimizing the",
    "start": "554560",
    "end": "557040"
  },
  {
    "text": "normalized cuts",
    "start": "557040",
    "end": "559120"
  },
  {
    "text": "what this means is that we want to",
    "start": "559120",
    "end": "561440"
  },
  {
    "text": "partition the graph into",
    "start": "561440",
    "end": "564080"
  },
  {
    "text": "groups that have roughly the same volume",
    "start": "564080",
    "end": "566800"
  },
  {
    "text": "or the same number of notes in them",
    "start": "566800",
    "end": "568640"
  },
  {
    "text": "and we want to minimize the edges the",
    "start": "568640",
    "end": "570720"
  },
  {
    "text": "number of cuts",
    "start": "570720",
    "end": "572720"
  },
  {
    "text": "across the edges between groups and here",
    "start": "572720",
    "end": "575680"
  },
  {
    "text": "is how",
    "start": "575680",
    "end": "576000"
  },
  {
    "text": "we can formulate this normalized cut",
    "start": "576000",
    "end": "578480"
  },
  {
    "text": "objective",
    "start": "578480",
    "end": "579440"
  },
  {
    "text": "so our uh approach was that our",
    "start": "579440",
    "end": "583040"
  },
  {
    "text": "the way we wanted to tackle this problem",
    "start": "583040",
    "end": "584880"
  },
  {
    "text": "again was with a learning based approach",
    "start": "584880",
    "end": "586800"
  },
  {
    "text": "when we can use the context to guide",
    "start": "586800",
    "end": "589200"
  },
  {
    "text": "this partitioning so we took a very",
    "start": "589200",
    "end": "591440"
  },
  {
    "text": "simple approach where we",
    "start": "591440",
    "end": "593120"
  },
  {
    "text": "uh passed the node to a neural net",
    "start": "593120",
    "end": "596160"
  },
  {
    "text": "and the output of the the neural net was",
    "start": "596160",
    "end": "598800"
  },
  {
    "text": "a prediction of where this node uh which",
    "start": "598800",
    "end": "601279"
  },
  {
    "text": "cluster or which partition this node",
    "start": "601279",
    "end": "603200"
  },
  {
    "text": "belong",
    "start": "603200",
    "end": "603760"
  },
  {
    "text": "belongs to so we have this probe once we",
    "start": "603760",
    "end": "606880"
  },
  {
    "text": "have these probabilities",
    "start": "606880",
    "end": "608640"
  },
  {
    "text": "uh we can go ahead and write the",
    "start": "608640",
    "end": "611360"
  },
  {
    "text": "probability of an",
    "start": "611360",
    "end": "612640"
  },
  {
    "text": "edge existing between these two",
    "start": "612640",
    "end": "614480"
  },
  {
    "text": "different groups and the probability of",
    "start": "614480",
    "end": "616720"
  },
  {
    "text": "the number of the of nodes or expected",
    "start": "616720",
    "end": "619120"
  },
  {
    "text": "number of nodes in a given partition",
    "start": "619120",
    "end": "621680"
  },
  {
    "text": "now in order to train these neural nets",
    "start": "621680",
    "end": "624320"
  },
  {
    "start": "622000",
    "end": "622000"
  },
  {
    "text": "we can",
    "start": "624320",
    "end": "624959"
  },
  {
    "text": "rewrite the normalized objective given",
    "start": "624959",
    "end": "628079"
  },
  {
    "text": "these probabilities",
    "start": "628079",
    "end": "629519"
  },
  {
    "text": "and we can start minimizing the",
    "start": "629519",
    "end": "631839"
  },
  {
    "text": "normalized cut",
    "start": "631839",
    "end": "633040"
  },
  {
    "text": "objective which in the end would",
    "start": "633040",
    "end": "635519"
  },
  {
    "text": "maximize the balances",
    "start": "635519",
    "end": "637200"
  },
  {
    "text": "and minimizes the cuts and here is an",
    "start": "637200",
    "end": "639680"
  },
  {
    "text": "example of how this works by minimizing",
    "start": "639680",
    "end": "642000"
  },
  {
    "text": "the cut",
    "start": "642000",
    "end": "643200"
  },
  {
    "text": "for this very simple data set of cars",
    "start": "643200",
    "end": "645680"
  },
  {
    "text": "and some birds",
    "start": "645680",
    "end": "648480"
  },
  {
    "text": "in it once the loss gets optimized",
    "start": "648640",
    "end": "652560"
  },
  {
    "text": "we can see that the neural net",
    "start": "652560",
    "end": "656000"
  },
  {
    "text": "assigns a very high probability to the",
    "start": "656000",
    "end": "658800"
  },
  {
    "text": "cars being in cluster",
    "start": "658800",
    "end": "660000"
  },
  {
    "text": "one and a very high probability to the",
    "start": "660000",
    "end": "662160"
  },
  {
    "text": "first belonging to",
    "start": "662160",
    "end": "663360"
  },
  {
    "text": "cluster two so by the end of this",
    "start": "663360",
    "end": "665440"
  },
  {
    "text": "optimization we can",
    "start": "665440",
    "end": "666959"
  },
  {
    "text": "find these clusters and the clusters are",
    "start": "666959",
    "end": "670079"
  },
  {
    "text": "being",
    "start": "670079",
    "end": "670480"
  },
  {
    "text": "formed and we took this and a further a",
    "start": "670480",
    "end": "674880"
  },
  {
    "start": "672000",
    "end": "672000"
  },
  {
    "text": "step further and thought",
    "start": "674880",
    "end": "676560"
  },
  {
    "text": "uh what if we can train these models in",
    "start": "676560",
    "end": "678720"
  },
  {
    "text": "a way that we can partition graphs at",
    "start": "678720",
    "end": "680880"
  },
  {
    "text": "inference so we train these neural nets",
    "start": "680880",
    "end": "683120"
  },
  {
    "text": "on a large",
    "start": "683120",
    "end": "684399"
  },
  {
    "text": "number of graph data sets and then use",
    "start": "684399",
    "end": "686560"
  },
  {
    "text": "it that inference to partition",
    "start": "686560",
    "end": "688480"
  },
  {
    "text": "new graphs so and for achieving that we",
    "start": "688480",
    "end": "691279"
  },
  {
    "text": "spend",
    "start": "691279",
    "end": "691680"
  },
  {
    "text": "a lot of time on representation learning",
    "start": "691680",
    "end": "694000"
  },
  {
    "text": "and really embedding the information",
    "start": "694000",
    "end": "695600"
  },
  {
    "text": "about the graph to help us with",
    "start": "695600",
    "end": "697040"
  },
  {
    "text": "generalization",
    "start": "697040",
    "end": "698320"
  },
  {
    "start": "698000",
    "end": "698000"
  },
  {
    "text": "and we got very interesting results on a",
    "start": "698320",
    "end": "700959"
  },
  {
    "text": "variety of data set our approach was",
    "start": "700959",
    "end": "702880"
  },
  {
    "text": "very different from the existing",
    "start": "702880",
    "end": "705839"
  },
  {
    "text": "baselines and we're very excited about",
    "start": "705839",
    "end": "707600"
  },
  {
    "text": "this line of work because",
    "start": "707600",
    "end": "709120"
  },
  {
    "text": "it not only can generalize it can also",
    "start": "709120",
    "end": "711279"
  },
  {
    "text": "easily scale",
    "start": "711279",
    "end": "712560"
  },
  {
    "text": "to graphs with millions of nodes and we",
    "start": "712560",
    "end": "714480"
  },
  {
    "text": "can control",
    "start": "714480",
    "end": "716399"
  },
  {
    "text": "the amount of balancedness and",
    "start": "716399",
    "end": "719519"
  },
  {
    "text": "the edges across the graphs very easily",
    "start": "719519",
    "end": "721680"
  },
  {
    "text": "if you're interested in this please",
    "start": "721680",
    "end": "723440"
  },
  {
    "text": "take a look at some of the papers that",
    "start": "723440",
    "end": "725360"
  },
  {
    "text": "i've listed here and we also have an",
    "start": "725360",
    "end": "727040"
  },
  {
    "text": "open source",
    "start": "727040",
    "end": "728320"
  },
  {
    "text": "github um repository it's open source",
    "start": "728320",
    "end": "730880"
  },
  {
    "text": "and github",
    "start": "730880",
    "end": "732639"
  },
  {
    "text": "now the third work that i'm going to",
    "start": "732639",
    "end": "734560"
  },
  {
    "text": "talk about is",
    "start": "734560",
    "end": "735680"
  },
  {
    "text": "our work on learning to optimize chip",
    "start": "735680",
    "end": "738480"
  },
  {
    "text": "placement",
    "start": "738480",
    "end": "739920"
  },
  {
    "text": "so chip placement is um",
    "start": "739920",
    "end": "743120"
  },
  {
    "text": "one of the a long pole in chip design",
    "start": "743120",
    "end": "746079"
  },
  {
    "text": "and",
    "start": "746079",
    "end": "746639"
  },
  {
    "text": "uh it has to do with taking the graph",
    "start": "746639",
    "end": "749360"
  },
  {
    "text": "that describes the chip",
    "start": "749360",
    "end": "750959"
  },
  {
    "text": "and mapping the nodes of the graph onto",
    "start": "750959",
    "end": "753760"
  },
  {
    "text": "a",
    "start": "753760",
    "end": "756000"
  },
  {
    "text": "canvas such that certain criteria is met",
    "start": "756079",
    "end": "758800"
  },
  {
    "text": "such as",
    "start": "758800",
    "end": "759360"
  },
  {
    "text": "power uh the run time and the delay",
    "start": "759360",
    "end": "763120"
  },
  {
    "text": "and we want to also meet the congestion",
    "start": "763120",
    "end": "765360"
  },
  {
    "text": "density and other properties",
    "start": "765360",
    "end": "767200"
  },
  {
    "text": "so it's basically like fitting all these",
    "start": "767200",
    "end": "769040"
  },
  {
    "text": "elements onto a canvas and making sure",
    "start": "769040",
    "end": "771440"
  },
  {
    "text": "that we are meeting the constraints that",
    "start": "771440",
    "end": "774880"
  },
  {
    "text": "otherwise we won't be able to fabricate",
    "start": "774880",
    "end": "778160"
  },
  {
    "text": "the chip",
    "start": "778160",
    "end": "779839"
  },
  {
    "start": "779000",
    "end": "779000"
  },
  {
    "text": "um it turns out that this problem is",
    "start": "779839",
    "end": "782880"
  },
  {
    "text": "really complex",
    "start": "782880",
    "end": "784000"
  },
  {
    "text": "because chips have um thousands of",
    "start": "784000",
    "end": "787760"
  },
  {
    "text": "memories and millions or even billions",
    "start": "787760",
    "end": "790560"
  },
  {
    "text": "of",
    "start": "790560",
    "end": "791040"
  },
  {
    "text": "standard cells and these are the nodes",
    "start": "791040",
    "end": "792720"
  },
  {
    "text": "of this graph that needs to be",
    "start": "792720",
    "end": "794320"
  },
  {
    "text": "that need to be placed and here even a",
    "start": "794320",
    "end": "796800"
  },
  {
    "text": "simplified version of the problem that",
    "start": "796800",
    "end": "798480"
  },
  {
    "text": "i'm going to talk about",
    "start": "798480",
    "end": "800160"
  },
  {
    "text": "requires orders of magnitude has orders",
    "start": "800160",
    "end": "803040"
  },
  {
    "text": "of magnitude more complexity",
    "start": "803040",
    "end": "804800"
  },
  {
    "text": "than chess and go which are two",
    "start": "804800",
    "end": "807120"
  },
  {
    "text": "canonical games um",
    "start": "807120",
    "end": "808720"
  },
  {
    "text": "previously solved by rl and learning",
    "start": "808720",
    "end": "811200"
  },
  {
    "text": "based methods",
    "start": "811200",
    "end": "812399"
  },
  {
    "text": "and not only that there has been five",
    "start": "812399",
    "end": "814399"
  },
  {
    "text": "decades of research",
    "start": "814399",
    "end": "815680"
  },
  {
    "text": "on uh floor planning and placement and",
    "start": "815680",
    "end": "818320"
  },
  {
    "text": "such as partitioning based methods",
    "start": "818320",
    "end": "820480"
  },
  {
    "text": "strategic hill climbing approaches and",
    "start": "820480",
    "end": "823680"
  },
  {
    "text": "analytic solvers",
    "start": "823680",
    "end": "825040"
  },
  {
    "text": "and what we proposed here was the",
    "start": "825040",
    "end": "827760"
  },
  {
    "text": "learning based uh methods",
    "start": "827760",
    "end": "829839"
  },
  {
    "text": "and the way we tackle this problem is as",
    "start": "829839",
    "end": "832800"
  },
  {
    "text": "follow",
    "start": "832800",
    "end": "833839"
  },
  {
    "text": "as following we train an rl agent",
    "start": "833839",
    "end": "837360"
  },
  {
    "text": "that um starts placing the nodes of the",
    "start": "837360",
    "end": "841279"
  },
  {
    "text": "graph",
    "start": "841279",
    "end": "842399"
  },
  {
    "text": "one at a time so in the beginning we",
    "start": "842399",
    "end": "845519"
  },
  {
    "text": "have an empty canvas",
    "start": "845519",
    "end": "846720"
  },
  {
    "text": "and in each step the rl agent places the",
    "start": "846720",
    "end": "849839"
  },
  {
    "text": "new",
    "start": "849839",
    "end": "850639"
  },
  {
    "text": "nodes and once all the memory nodes all",
    "start": "850639",
    "end": "854000"
  },
  {
    "text": "the macros of the",
    "start": "854000",
    "end": "855680"
  },
  {
    "text": "of the chip graph are placed",
    "start": "855680",
    "end": "859040"
  },
  {
    "text": "use a force directed method to place the",
    "start": "859040",
    "end": "861600"
  },
  {
    "text": "standard cells which are the",
    "start": "861600",
    "end": "863519"
  },
  {
    "text": "point and very tiny uh elements",
    "start": "863519",
    "end": "866800"
  },
  {
    "text": "of uh of the chip necklace so",
    "start": "866800",
    "end": "869519"
  },
  {
    "text": "unfortunate method is known to",
    "start": "869519",
    "end": "871920"
  },
  {
    "text": "optimize those the placement of those",
    "start": "871920",
    "end": "874000"
  },
  {
    "text": "elements well",
    "start": "874000",
    "end": "875279"
  },
  {
    "text": "so we do that and then we get some",
    "start": "875279",
    "end": "877760"
  },
  {
    "text": "estimate of the violent congestion and",
    "start": "877760",
    "end": "880240"
  },
  {
    "text": "density",
    "start": "880240",
    "end": "880959"
  },
  {
    "text": "of this placement once we have that we",
    "start": "880959",
    "end": "884560"
  },
  {
    "text": "give it back to the use it as a reward",
    "start": "884560",
    "end": "886959"
  },
  {
    "text": "function we",
    "start": "886959",
    "end": "887839"
  },
  {
    "text": "use it to update the rl agent policy so",
    "start": "887839",
    "end": "890480"
  },
  {
    "text": "the policy",
    "start": "890480",
    "end": "891600"
  },
  {
    "text": "of what it learns is that over time",
    "start": "891600",
    "end": "894720"
  },
  {
    "text": "over various iterations of this",
    "start": "894720",
    "end": "896959"
  },
  {
    "text": "placement",
    "start": "896959",
    "end": "898320"
  },
  {
    "text": "it learns to optimize its parameters in",
    "start": "898320",
    "end": "900639"
  },
  {
    "text": "a way that it produces placement",
    "start": "900639",
    "end": "902800"
  },
  {
    "text": "that optimize the final while and",
    "start": "902800",
    "end": "905440"
  },
  {
    "text": "congestion and density",
    "start": "905440",
    "end": "908480"
  },
  {
    "start": "907000",
    "end": "907000"
  },
  {
    "text": "now and our goal of course was to make",
    "start": "908480",
    "end": "911680"
  },
  {
    "text": "these policies generalized make them",
    "start": "911680",
    "end": "914399"
  },
  {
    "text": "in a way that they become better as they",
    "start": "914399",
    "end": "916800"
  },
  {
    "text": "solve on new",
    "start": "916800",
    "end": "918000"
  },
  {
    "text": "chips so we grounded the",
    "start": "918000",
    "end": "921279"
  },
  {
    "text": "approach to designing these policies",
    "start": "921279",
    "end": "924000"
  },
  {
    "text": "first",
    "start": "924000",
    "end": "924480"
  },
  {
    "text": "in an uh supervised method because our",
    "start": "924480",
    "end": "927199"
  },
  {
    "text": "our",
    "start": "927199",
    "end": "927600"
  },
  {
    "text": "intuition was that if this policy is to",
    "start": "927600",
    "end": "930240"
  },
  {
    "text": "generalize",
    "start": "930240",
    "end": "931279"
  },
  {
    "text": "it we should at least be able to solve",
    "start": "931279",
    "end": "933759"
  },
  {
    "text": "this simpler problem which is",
    "start": "933759",
    "end": "935519"
  },
  {
    "text": "uh generalization on uh on the",
    "start": "935519",
    "end": "938720"
  },
  {
    "text": "on the violin congestion and basically",
    "start": "938720",
    "end": "940959"
  },
  {
    "text": "reward prediction of a given placement",
    "start": "940959",
    "end": "943519"
  },
  {
    "text": "so we focus on the supervised uh task of",
    "start": "943519",
    "end": "946800"
  },
  {
    "text": "predicting the reward",
    "start": "946800",
    "end": "948240"
  },
  {
    "text": "given a placement we generated a data",
    "start": "948240",
    "end": "951199"
  },
  {
    "text": "set we generated",
    "start": "951199",
    "end": "953040"
  },
  {
    "text": "the labels for them all with our proxy",
    "start": "953040",
    "end": "955279"
  },
  {
    "text": "cost so it was a very cheap",
    "start": "955279",
    "end": "957040"
  },
  {
    "text": "and um affordable method once we",
    "start": "957040",
    "end": "960079"
  },
  {
    "text": "developed we developed that then",
    "start": "960079",
    "end": "961839"
  },
  {
    "text": "we could easily innovate on the",
    "start": "961839",
    "end": "965040"
  },
  {
    "text": "architecture we came up with this",
    "start": "965040",
    "end": "966959"
  },
  {
    "text": "new graph uh architecture a graph",
    "start": "966959",
    "end": "969920"
  },
  {
    "text": "convolution that could",
    "start": "969920",
    "end": "971360"
  },
  {
    "text": "basically take a graph and its placement",
    "start": "971360",
    "end": "974160"
  },
  {
    "text": "and predict the violent and conjunction",
    "start": "974160",
    "end": "976399"
  },
  {
    "text": "congestion once we had this uh we could",
    "start": "976399",
    "end": "979279"
  },
  {
    "text": "go ahead and use that",
    "start": "979279",
    "end": "981519"
  },
  {
    "text": "uh module that can really predict the",
    "start": "981519",
    "end": "985040"
  },
  {
    "text": "labels easily and generalize as the",
    "start": "985040",
    "end": "987759"
  },
  {
    "text": "encoder of our policy",
    "start": "987759",
    "end": "989519"
  },
  {
    "text": "and then on the decoder side uh we used",
    "start": "989519",
    "end": "992079"
  },
  {
    "text": "a deconvolution layer and we also use",
    "start": "992079",
    "end": "994240"
  },
  {
    "text": "the mask to kind of enforce the",
    "start": "994240",
    "end": "996000"
  },
  {
    "text": "constraints",
    "start": "996000",
    "end": "996959"
  },
  {
    "text": "so this policy was the eventually",
    "start": "996959",
    "end": "999600"
  },
  {
    "text": "allowed us",
    "start": "999600",
    "end": "1000480"
  },
  {
    "text": "to generalize and we believe this will",
    "start": "1000480",
    "end": "1002560"
  },
  {
    "text": "uh transfer to other general policy",
    "start": "1002560",
    "end": "1004959"
  },
  {
    "start": "1004000",
    "end": "1004000"
  },
  {
    "text": "generalization problems",
    "start": "1004959",
    "end": "1006560"
  },
  {
    "text": "and here too and is an example to show",
    "start": "1006560",
    "end": "1008639"
  },
  {
    "text": "you how",
    "start": "1008639",
    "end": "1009759"
  },
  {
    "text": "the generalization works on the left",
    "start": "1009759",
    "end": "1011759"
  },
  {
    "text": "policy is being trained from scratch to",
    "start": "1011759",
    "end": "1013839"
  },
  {
    "text": "do the placement of the no",
    "start": "1013839",
    "end": "1015279"
  },
  {
    "text": "these nodes that you can see here on the",
    "start": "1015279",
    "end": "1017759"
  },
  {
    "text": "right we take a pre-trained policy and",
    "start": "1017759",
    "end": "1019759"
  },
  {
    "text": "apply that at inference",
    "start": "1019759",
    "end": "1021600"
  },
  {
    "text": "on the nodes of this risk 5 processor",
    "start": "1021600",
    "end": "1024959"
  },
  {
    "text": "and as you can see the pre-trained",
    "start": "1024959",
    "end": "1026400"
  },
  {
    "text": "policy right in the beginning finds some",
    "start": "1026400",
    "end": "1029360"
  },
  {
    "text": "good structure in placing the notes here",
    "start": "1029360",
    "end": "1032160"
  },
  {
    "text": "around the canvas and leaving area for",
    "start": "1032160",
    "end": "1034079"
  },
  {
    "text": "the standard cells in the middle",
    "start": "1034079",
    "end": "1035678"
  },
  {
    "text": "but it takes the other policy some time",
    "start": "1035679",
    "end": "1038000"
  },
  {
    "text": "to get to that",
    "start": "1038000",
    "end": "1039038"
  },
  {
    "text": "and here is some data that shows that we",
    "start": "1039039",
    "end": "1041600"
  },
  {
    "text": "we need",
    "start": "1041600",
    "end": "1042798"
  },
  {
    "text": "30 more hours of training to get what",
    "start": "1042799",
    "end": "1046400"
  },
  {
    "text": "from for training a policy from scratch",
    "start": "1046400",
    "end": "1049200"
  },
  {
    "text": "to get",
    "start": "1049200",
    "end": "1049840"
  },
  {
    "text": "to what a pre-trained policy gets to",
    "start": "1049840",
    "end": "1053200"
  },
  {
    "text": "almost right away so this is very",
    "start": "1053200",
    "end": "1054880"
  },
  {
    "text": "exciting and also it shows that the",
    "start": "1054880",
    "end": "1056559"
  },
  {
    "text": "pre-term policy",
    "start": "1056559",
    "end": "1057600"
  },
  {
    "text": "achieves higher quality results um here",
    "start": "1057600",
    "end": "1060880"
  },
  {
    "start": "1059000",
    "end": "1059000"
  },
  {
    "text": "is the",
    "start": "1060880",
    "end": "1061520"
  },
  {
    "text": "effect of training on larger size so we",
    "start": "1061520",
    "end": "1064960"
  },
  {
    "text": "are",
    "start": "1064960",
    "end": "1065280"
  },
  {
    "text": "trying we are pre-training the policy on",
    "start": "1065280",
    "end": "1067840"
  },
  {
    "text": "three different datasets one with the",
    "start": "1067840",
    "end": "1069440"
  },
  {
    "text": "smaller size",
    "start": "1069440",
    "end": "1070559"
  },
  {
    "text": "the other one with the medium size on",
    "start": "1070559",
    "end": "1072160"
  },
  {
    "text": "five blocks and the larger one on 20",
    "start": "1072160",
    "end": "1074080"
  },
  {
    "text": "blocks",
    "start": "1074080",
    "end": "1074799"
  },
  {
    "text": "and it's showing that as the policy is",
    "start": "1074799",
    "end": "1077039"
  },
  {
    "text": "exposed to larger data set",
    "start": "1077039",
    "end": "1079200"
  },
  {
    "text": "it can even at zero shot generate very",
    "start": "1079200",
    "end": "1081840"
  },
  {
    "text": "high quality placement on unseen new",
    "start": "1081840",
    "end": "1084000"
  },
  {
    "text": "data set",
    "start": "1084000",
    "end": "1085120"
  },
  {
    "text": "um and here is another example when on",
    "start": "1085120",
    "end": "1087760"
  },
  {
    "text": "the",
    "start": "1087760",
    "end": "1088400"
  },
  {
    "text": "left you can see the expert placement on",
    "start": "1088400",
    "end": "1090720"
  },
  {
    "text": "a tpu block",
    "start": "1090720",
    "end": "1092160"
  },
  {
    "text": "and it's played for confidentiality on",
    "start": "1092160",
    "end": "1094480"
  },
  {
    "text": "the right you can see the ml placer",
    "start": "1094480",
    "end": "1096799"
  },
  {
    "text": "and we achieved these results on 24",
    "start": "1096799",
    "end": "1099039"
  },
  {
    "text": "hours and ever since then",
    "start": "1099039",
    "end": "1101520"
  },
  {
    "text": "we could further reduce the runtime",
    "start": "1101520",
    "end": "1104720"
  },
  {
    "text": "and uh it's very exciting that we can do",
    "start": "1104720",
    "end": "1107360"
  },
  {
    "text": "that in a short amount of time",
    "start": "1107360",
    "end": "1109280"
  },
  {
    "text": "and the baseline actually requires human",
    "start": "1109280",
    "end": "1111360"
  },
  {
    "text": "expert in the loop and take",
    "start": "1111360",
    "end": "1112880"
  },
  {
    "text": "several weeks um here are some",
    "start": "1112880",
    "end": "1115440"
  },
  {
    "text": "comparisons with the",
    "start": "1115440",
    "end": "1117679"
  },
  {
    "text": "human experts in the loop baseline and",
    "start": "1117679",
    "end": "1120320"
  },
  {
    "text": "with the sota academic baseline",
    "start": "1120320",
    "end": "1122320"
  },
  {
    "text": "and what it suggests that it's like",
    "start": "1122320",
    "end": "1124400"
  },
  {
    "text": "these learning based methods",
    "start": "1124400",
    "end": "1126160"
  },
  {
    "text": "um really can optimize the proxy costs",
    "start": "1126160",
    "end": "1128960"
  },
  {
    "text": "and actually and not only that they can",
    "start": "1128960",
    "end": "1130880"
  },
  {
    "text": "optimize the true timing area power",
    "start": "1130880",
    "end": "1134080"
  },
  {
    "text": "and congestion which are correlated well",
    "start": "1134080",
    "end": "1136559"
  },
  {
    "text": "with those proxy costs",
    "start": "1136559",
    "end": "1138160"
  },
  {
    "text": "um and this work generated a lot of",
    "start": "1138160",
    "end": "1140080"
  },
  {
    "text": "interest and we're super excited about",
    "start": "1140080",
    "end": "1141919"
  },
  {
    "text": "this and we think this is just the",
    "start": "1141919",
    "end": "1143280"
  },
  {
    "text": "beginning and",
    "start": "1143280",
    "end": "1144160"
  },
  {
    "text": "really cheap design and system design um",
    "start": "1144160",
    "end": "1147120"
  },
  {
    "text": "there are a lot of opportunities to",
    "start": "1147120",
    "end": "1148960"
  },
  {
    "text": "really transform",
    "start": "1148960",
    "end": "1150000"
  },
  {
    "text": "the way their the work around their",
    "start": "1150000",
    "end": "1153600"
  },
  {
    "text": "optimization",
    "start": "1153600",
    "end": "1154960"
  },
  {
    "text": "are done today and here are some of the",
    "start": "1154960",
    "end": "1158160"
  },
  {
    "text": "links to some of the papers on the chip",
    "start": "1158160",
    "end": "1160320"
  },
  {
    "text": "placement work",
    "start": "1160320",
    "end": "1161520"
  },
  {
    "text": "and other ml for combinator optimization",
    "start": "1161520",
    "end": "1163919"
  },
  {
    "text": "work that we have been doing",
    "start": "1163919",
    "end": "1165520"
  },
  {
    "text": "if you're interested please take a look",
    "start": "1165520",
    "end": "1167760"
  },
  {
    "text": "and",
    "start": "1167760",
    "end": "1168640"
  },
  {
    "text": "with that i can conclude my talk and",
    "start": "1168640",
    "end": "1173039"
  },
  {
    "text": "happy to take any questions",
    "start": "1173280",
    "end": "1178240"
  }
]