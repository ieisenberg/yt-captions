[
  {
    "text": "um hello everyone wait now welcome all to my Ray talk uh today",
    "start": "7859",
    "end": "16020"
  },
  {
    "text": "we're gonna talk about Merlin which is shopify's new machine learning platform",
    "start": "16020",
    "end": "21480"
  },
  {
    "text": "and this image is from a blog post we published earlier this year about this topic exactly you can find it and read",
    "start": "21480",
    "end": "29640"
  },
  {
    "text": "more about it at the Shopify engineering blog so uh I'm Isaac vidas I'm a machine",
    "start": "29640",
    "end": "37500"
  },
  {
    "text": "learning platform Tech lead at the ml platform team at Shopify I joined Shopify in March 2021 I love distributed",
    "start": "37500",
    "end": "47040"
  },
  {
    "text": "systems ml Ops and emojis and I'm also happy to connect with anyone on LinkedIn",
    "start": "47040",
    "end": "54440"
  },
  {
    "text": "so a little bit about Shopify in one sentence Shopify is an all-in-one",
    "start": "54840",
    "end": "60239"
  },
  {
    "text": "Commerce platform that enables Merchants to set up online shops easily and makes Commerce better for everyone",
    "start": "60239",
    "end": "68420"
  },
  {
    "text": "now that we know more about Shopify and me let's talk about machine learning at Shopify",
    "start": "68640",
    "end": "75500"
  },
  {
    "text": "so at Shopify we have hundreds of data scientists that do machine learning uh",
    "start": "75540",
    "end": "81659"
  },
  {
    "text": "use cases every day there are tens of teams doing ml uh and and different",
    "start": "81659",
    "end": "88140"
  },
  {
    "text": "between different requirements for machine learning use cases and many data",
    "start": "88140",
    "end": "93299"
  },
  {
    "text": "scientists at Shopify are already using Merlin on their day-to-day job",
    "start": "93299",
    "end": "98939"
  },
  {
    "text": "a migration to our new platform is ongoing but we already have uh more than",
    "start": "98939",
    "end": "104520"
  },
  {
    "text": "40 use cases running on the new platform and it still keeps happening as time",
    "start": "104520",
    "end": "110820"
  },
  {
    "text": "moves on in the in the ammo platform uh we have",
    "start": "110820",
    "end": "116880"
  },
  {
    "text": "set up the goals to enable shopify's teams to train test deploy serve and",
    "start": "116880",
    "end": "123659"
  },
  {
    "text": "monitor machine learning models efficiently and quickly so basically the whole ml workflow end to end on one",
    "start": "123659",
    "end": "131400"
  },
  {
    "text": "platform With Merlin we enable scalability with robust infrastructure that can scale up",
    "start": "131400",
    "end": "138660"
  },
  {
    "text": "our machine learning workflows we also enable fast iterations with tools that",
    "start": "138660",
    "end": "144239"
  },
  {
    "text": "reduce friction and increase productivity for our data scientists and machine learning Engineers by minimizing",
    "start": "144239",
    "end": "151379"
  },
  {
    "text": "the gap between prototyping and production and we also have flexibility so that our",
    "start": "151379",
    "end": "157500"
  },
  {
    "text": "users can use any libraries or packages that they need for their use cases and",
    "start": "157500",
    "end": "162660"
  },
  {
    "text": "their models let's talk and dive more deeply into",
    "start": "162660",
    "end": "169260"
  },
  {
    "text": "Merlin and how it was built and being used at Shopify",
    "start": "169260",
    "end": "175700"
  },
  {
    "text": "as a background the ammo platform went through two iterations in the past the first one was",
    "start": "175920",
    "end": "183239"
  },
  {
    "text": "built on top of an in-house spy spark system solution and the second iteration was built as a",
    "start": "183239",
    "end": "191220"
  },
  {
    "text": "wrapper around the Google AI platform which is these days called vertex Ai and",
    "start": "191220",
    "end": "197220"
  },
  {
    "text": "it runs as a managed service on gcp last year we re-evaluated our machine",
    "start": "197220",
    "end": "204239"
  },
  {
    "text": "learning platform based on different requirements that we got from our users and data scientists and also based on",
    "start": "204239",
    "end": "211680"
  },
  {
    "text": "the goals that we just talked about and so we decided to rebuild our machine",
    "start": "211680",
    "end": "217739"
  },
  {
    "text": "learning platform on top of Open Source tools and Technologies around the goals that we discussed such as scalability",
    "start": "217739",
    "end": "224879"
  },
  {
    "text": "fast iterations and flexibility",
    "start": "224879",
    "end": "229580"
  },
  {
    "text": "just a couple of words about the timeline we made a decision to rebuild our",
    "start": "231540",
    "end": "237060"
  },
  {
    "text": "machine learning platform in March 2021 last year then we went into a",
    "start": "237060",
    "end": "243060"
  },
  {
    "text": "prototyping phase and POC around different open source tools that we",
    "start": "243060",
    "end": "248879"
  },
  {
    "text": "wanted to test out in August we started building our production environment and",
    "start": "248879",
    "end": "254760"
  },
  {
    "text": "then earlier this year in February we released Merlin as a to General availability for all of our data",
    "start": "254760",
    "end": "261600"
  },
  {
    "text": "scientists and machine learning engineers this is a diagram of a very very early",
    "start": "261600",
    "end": "269460"
  },
  {
    "text": "version of Merlin two things about this we clearly did not have graphic",
    "start": "269460",
    "end": "275699"
  },
  {
    "text": "designers at the time and the other one is we did not have a",
    "start": "275699",
    "end": "282000"
  },
  {
    "text": "clear vision of how the ml platform would look like after the Prototype and",
    "start": "282000",
    "end": "288540"
  },
  {
    "text": "so we understood obviously that there's the machine learning workflow and we",
    "start": "288540",
    "end": "294360"
  },
  {
    "text": "start in the beginning from the input from our data lake or streaming events",
    "start": "294360",
    "end": "299460"
  },
  {
    "text": "we go through computation pre-processing training evaluation and then serving the",
    "start": "299460",
    "end": "305699"
  },
  {
    "text": "model for either bet use cases or online inference we did know that on the infrastructure",
    "start": "305699",
    "end": "312900"
  },
  {
    "text": "level we will run on top of kubernetes and then on the application layer we",
    "start": "312900",
    "end": "318180"
  },
  {
    "text": "will leverage Ray for distributed computation",
    "start": "318180",
    "end": "322580"
  },
  {
    "text": "so why um our Shopify and myself are very very",
    "start": "323880",
    "end": "329580"
  },
  {
    "text": "excited about trade first thing first it's an open source Library so it ties",
    "start": "329580",
    "end": "335100"
  },
  {
    "text": "very much into our plans for the machine learning platform also python is a first class citizen",
    "start": "335100",
    "end": "342000"
  },
  {
    "text": "which means that our data scientists that use Python every day can onboard very quickly to Rey and to",
    "start": "342000",
    "end": "349979"
  },
  {
    "text": "this new platform we can also use rate for any kind of distributed jobs with many libraries and",
    "start": "349979",
    "end": "358139"
  },
  {
    "text": "applications and tools that are dedicated for ML in addition you can also run different",
    "start": "358139",
    "end": "363720"
  },
  {
    "text": "distributed processing libraries on top of Ray such as desks spark or modem",
    "start": "363720",
    "end": "369060"
  },
  {
    "text": "they even have a new integration that they're working on for beam which we heard a talk about this yesterday from",
    "start": "369060",
    "end": "374880"
  },
  {
    "text": "Patrick and this just shows the flexibility and size of this ecosystem",
    "start": "374880",
    "end": "381860"
  },
  {
    "text": "when thinking about how we can build a machine learning platform and how we can integrate it with Ray there are",
    "start": "382740",
    "end": "389220"
  },
  {
    "text": "different ways to do it on one hand you can have a single Ray cluster where you",
    "start": "389220",
    "end": "395039"
  },
  {
    "text": "can run all of your machine learning jobs and workflows on top of on the other hand you can have array",
    "start": "395039",
    "end": "401460"
  },
  {
    "text": "cluster that is dedicated for each use case individually when looking into Ray and prototyping it",
    "start": "401460",
    "end": "408720"
  },
  {
    "text": "with it we CL we quickly understood that the driver the ray driver and workers",
    "start": "408720",
    "end": "413940"
  },
  {
    "text": "must have the same Brave version and python version which means that for us we made a",
    "start": "413940",
    "end": "420180"
  },
  {
    "text": "decision on having ephemeral weight clusters which means that each Ray cluster is created when the job starts",
    "start": "420180",
    "end": "427080"
  },
  {
    "text": "and deleted when the job ends and each rate cluster is isolated for a specific",
    "start": "427080",
    "end": "432900"
  },
  {
    "text": "machine learning use case when you build your own platform you can",
    "start": "432900",
    "end": "438539"
  },
  {
    "text": "choose the right option for you this is just what works well for us",
    "start": "438539",
    "end": "444080"
  },
  {
    "text": "and then when thinking about the dedicated and ephemeral way cluster that",
    "start": "446220",
    "end": "451860"
  },
  {
    "text": "we're using we coined the term Merlin workspace that basically wraps this",
    "start": "451860",
    "end": "457380"
  },
  {
    "text": "around and then each use case can have a dedicated Marine workspace that can be",
    "start": "457380",
    "end": "464039"
  },
  {
    "text": "used for the distributed computation that happens in that Ray cluster",
    "start": "464039",
    "end": "469500"
  },
  {
    "text": "and that Merlin workspace can contain both the resources requirements on the",
    "start": "469500",
    "end": "476340"
  },
  {
    "text": "infrastructure layer and then the dependencies and packages that are required for the use case on the",
    "start": "476340",
    "end": "482880"
  },
  {
    "text": "application layer in these few examples we have like four",
    "start": "482880",
    "end": "487919"
  },
  {
    "text": "different use cases one of them a few of them can use gpus for example on the",
    "start": "487919",
    "end": "493500"
  },
  {
    "text": "infrastructure and then on the application layer each one has array cluster with the different libraries",
    "start": "493500",
    "end": "500400"
  },
  {
    "text": "that are needed for that use case specifically which can be tensorflow pytorch",
    "start": "500400",
    "end": "506099"
  },
  {
    "text": "Etc what is important here is that workspaces is are the most important",
    "start": "506099",
    "end": "512279"
  },
  {
    "text": "part of the platform where computation is being done which can be either pre-processing training inference or any",
    "start": "512279",
    "end": "519180"
  },
  {
    "text": "other computation that can happen on one node or in a distributed fashion",
    "start": "519180",
    "end": "526880"
  },
  {
    "text": "this is an example of how we use a dedicated Merlin workspace for training",
    "start": "528420",
    "end": "533760"
  },
  {
    "text": "for our product categorization use case which uses rate train and tensorflow and",
    "start": "533760",
    "end": "539399"
  },
  {
    "text": "so on the infrastructure layer we're using gpus CPUs pods kubernetes we also",
    "start": "539399",
    "end": "545640"
  },
  {
    "text": "leverage Auto scaling so we can scale up to the amount of resources that we need and then on the application Level we",
    "start": "545640",
    "end": "552899"
  },
  {
    "text": "have array cluster that we run rate train and tensorflow on and this code is",
    "start": "552899",
    "end": "559200"
  },
  {
    "text": "basically just an example of the training script that runs on that Ray on that Merlin workspace and Ray cluster",
    "start": "559200",
    "end": "566279"
  },
  {
    "text": "behind the scenes in a different step of the machine learning workflow we perform inference",
    "start": "566279",
    "end": "573240"
  },
  {
    "text": "for product categorization and in this situation and in on that step we no",
    "start": "573240",
    "end": "579180"
  },
  {
    "text": "longer need gpus however we do leverage a different part of Ray which is the ray",
    "start": "579180",
    "end": "585300"
  },
  {
    "text": "actor pulls and it allows us to have to send different partitions through the",
    "start": "585300",
    "end": "591480"
  },
  {
    "text": "model that we just trained in order to return inference and that in our specific case here we're using actor",
    "start": "591480",
    "end": "598380"
  },
  {
    "text": "pulls but this could have been also using dusk or using great data sets Etc",
    "start": "598380",
    "end": "605779"
  },
  {
    "text": "if we take a step back let's look at the Merlin architecture as",
    "start": "608279",
    "end": "614279"
  },
  {
    "text": "we build it and so everything we talked about until now is the Merlin core",
    "start": "614279",
    "end": "620160"
  },
  {
    "text": "in the Marine Corps we have the kubernetes layer and then on top of it the auto scaling",
    "start": "620160",
    "end": "626459"
  },
  {
    "text": "and individual Merlin workspaces where each one has array cluster and the",
    "start": "626459",
    "end": "633180"
  },
  {
    "text": "different machine learning steps that are running on each Ray cluster individually each Merlin workspace has a direct",
    "start": "633180",
    "end": "640380"
  },
  {
    "text": "access to our data Lake in order to load data sets from or save models into and",
    "start": "640380",
    "end": "646200"
  },
  {
    "text": "also access to our panel feature store in addition we also created a Merlin API",
    "start": "646200",
    "end": "653040"
  },
  {
    "text": "this API helps us to perform management of the different Merlin workspaces that",
    "start": "653040",
    "end": "658920"
  },
  {
    "text": "we create on this Merlin and kubernetes environment this API is a unified API where it",
    "start": "658920",
    "end": "666720"
  },
  {
    "text": "doesn't matter if you're in like the production orchestrator that runs airflow or or through Uzi or a user",
    "start": "666720",
    "end": "674399"
  },
  {
    "text": "running their job and prototyping from a jupyter notebook they're all gonna be using this API in order to create the",
    "start": "674399",
    "end": "683040"
  },
  {
    "text": "different Merlin workspaces and leverage them for distributed machine learning",
    "start": "683040",
    "end": "688320"
  },
  {
    "text": "workflows if we look uh a little bit closer into",
    "start": "688320",
    "end": "695760"
  },
  {
    "text": "the Merlin API it's basically a rest API that allows our users to create update",
    "start": "695760",
    "end": "702240"
  },
  {
    "text": "and delete Merlin workspaces and behind the scenes array clusters",
    "start": "702240",
    "end": "707279"
  },
  {
    "text": "in this example our users can Define different parameters that they need for",
    "start": "707279",
    "end": "713279"
  },
  {
    "text": "their machine learning workflow for example the name of the of the Marlin workspace the owner so we can keep track",
    "start": "713279",
    "end": "720180"
  },
  {
    "text": "of it the base image that the Merlin workspace and Ray cluster will run with",
    "start": "720180",
    "end": "725240"
  },
  {
    "text": "the min max workers for scalability and the amount of CPUs memories they can",
    "start": "725240",
    "end": "732959"
  },
  {
    "text": "they can also specify the GPU type and number of gpus that they want to use and",
    "start": "732959",
    "end": "739200"
  },
  {
    "text": "for example they can enable jupyter lab to run on the head node so they can have",
    "start": "739200",
    "end": "744480"
  },
  {
    "text": "direct access into the array cluster immediately behind the scenes this is mostly a",
    "start": "744480",
    "end": "750779"
  },
  {
    "text": "wrapper around Cube CPL what happens is those parameters are being taken and then injected into a manifest of a",
    "start": "750779",
    "end": "759360"
  },
  {
    "text": "kubernetes manifest gml file and with the python API we basically apply it on",
    "start": "759360",
    "end": "764700"
  },
  {
    "text": "the kubernetes Clusters one thing that it helps us with is it",
    "start": "764700",
    "end": "770880"
  },
  {
    "text": "completely obstructs all of the infrastructure elements and complications from our users so they",
    "start": "770880",
    "end": "777540"
  },
  {
    "text": "don't even have to think about kubernetes when they work with Merlin what all they have to do is work through",
    "start": "777540",
    "end": "783540"
  },
  {
    "text": "this API or the clis that we create for them and that's it in addition it helps",
    "start": "783540",
    "end": "789360"
  },
  {
    "text": "us because it reduces the amount of permissions that we have to take care of right so even if we want to migrate them",
    "start": "789360",
    "end": "796380"
  },
  {
    "text": "between clusters or change the infrastructure underneath the platform we can do it very easily without",
    "start": "796380",
    "end": "802680"
  },
  {
    "text": "affecting or impacting our users in addition this API is also versioned",
    "start": "802680",
    "end": "808320"
  },
  {
    "text": "so any new changes that we incorporate cannot break clients that are using the",
    "start": "808320",
    "end": "814200"
  },
  {
    "text": "previous version once our users are creating a new Merlin",
    "start": "814200",
    "end": "819839"
  },
  {
    "text": "workspace they get back a full payload with the details for that work workspace",
    "start": "819839",
    "end": "825620"
  },
  {
    "text": "that workspace can include different URLs that will help our users to manage",
    "start": "825620",
    "end": "831360"
  },
  {
    "text": "and access the new Ray cluster with a dashboard or the ray head node address",
    "start": "831360",
    "end": "836820"
  },
  {
    "text": "or rayjob submit address so they can send jobs uh to to run on the cluster",
    "start": "836820",
    "end": "842279"
  },
  {
    "text": "access to the jupyter notebooks and also observability for their dedicated",
    "start": "842279",
    "end": "847740"
  },
  {
    "text": "environment with Splunk URLs and datadog dashboards in addition each Merlin",
    "start": "847740",
    "end": "853800"
  },
  {
    "text": "workspace gets its own time to live right this means that at the end of that",
    "start": "853800",
    "end": "859560"
  },
  {
    "text": "time frame the cluster will be will die there's a there are jobs that runs",
    "start": "859560",
    "end": "864600"
  },
  {
    "text": "periodically and make sure that expired workspaces will be deleted this means that no one will forget a lot of gpus",
    "start": "864600",
    "end": "872760"
  },
  {
    "text": "running in the background and go out to their weekend and this also allows us to kind of put in tripwires so we know of",
    "start": "872760",
    "end": "880560"
  },
  {
    "text": "large jobs that are starting to run and we can make sure that we that we plan",
    "start": "880560",
    "end": "885779"
  },
  {
    "text": "ahead for them if we need more resources in the future in addition we also included recently",
    "start": "885779",
    "end": "891959"
  },
  {
    "text": "cost estimation in the image you see that at the bottom this means that the",
    "start": "891959",
    "end": "897180"
  },
  {
    "text": "user is very very familiar with the amount of money takes to run their jobs",
    "start": "897180",
    "end": "902220"
  },
  {
    "text": "and we can also have an estimation for the max age cost because we know how",
    "start": "902220",
    "end": "907740"
  },
  {
    "text": "much time the workspace will run for with the time to live",
    "start": "907740",
    "end": "912560"
  },
  {
    "text": "once the workspace is up and running our users can start prototyping in a",
    "start": "913079",
    "end": "918120"
  },
  {
    "text": "distributed environment they have two options for them either use jupyter",
    "start": "918120",
    "end": "923519"
  },
  {
    "text": "notebooks from a central Jupiter Hub or they can work from their local machines",
    "start": "923519",
    "end": "930000"
  },
  {
    "text": "and use different Ray client or connections or apis to send their jobs",
    "start": "930000",
    "end": "936060"
  },
  {
    "text": "into the remote Ray cluster from the Jupiter notebooks they can easily use",
    "start": "936060",
    "end": "941880"
  },
  {
    "text": "grpc with the ray client API to run different jobs and from their laptop",
    "start": "941880",
    "end": "947820"
  },
  {
    "text": "they can also use the ray client or Ray job submit which will help them to package the code that they just written",
    "start": "947820",
    "end": "955500"
  },
  {
    "text": "on their local environment and then send it to the remote Ray cluster this this just allows fast iterations for our",
    "start": "955500",
    "end": "963060"
  },
  {
    "text": "users which is one of our goals for the platform from our orchestrators there are again",
    "start": "963060",
    "end": "970440"
  },
  {
    "text": "different ways to schedule and run jobs on the remoteway Clusters from airflow",
    "start": "970440",
    "end": "976500"
  },
  {
    "text": "we can spin up a pod which will run the ray driver code and that will work",
    "start": "976500",
    "end": "982380"
  },
  {
    "text": "remotely with the ray cluster through the ray client to grpc API from a legacy Pi spark system that we",
    "start": "982380",
    "end": "989820"
  },
  {
    "text": "have in place we can Leverage The Ray job submit to send the the rate jobs",
    "start": "989820",
    "end": "996600"
  },
  {
    "text": "that will run on the rate like that will run the ray driver on the ray head node and from the pi spark system we can just",
    "start": "996600",
    "end": "1005000"
  },
  {
    "text": "wait for the job to end and also tail the logs this just shows the many",
    "start": "1005000",
    "end": "1010579"
  },
  {
    "text": "different ways that we can use Ray to run remote jobs easily no matter what",
    "start": "1010579",
    "end": "1017300"
  },
  {
    "text": "the limitations are or the constraints the rate job submit specifically was",
    "start": "1017300",
    "end": "1023060"
  },
  {
    "text": "really really helpful with using Legacy systems and connecting them to the new",
    "start": "1023060",
    "end": "1028579"
  },
  {
    "text": "platform that we're building if we'll take a step back and look",
    "start": "1028579",
    "end": "1035298"
  },
  {
    "text": "specifically on the infrastructure layer so when a user creates a new Merlin",
    "start": "1035299",
    "end": "1041540"
  },
  {
    "text": "workspace We Begin by creating a new kubernetes namespace in our kubernetes",
    "start": "1041540",
    "end": "1047240"
  },
  {
    "text": "clusters we start by running the array namespaced operator which helps to create array",
    "start": "1047240",
    "end": "1054860"
  },
  {
    "text": "clusters and also Auto scale them the next part will be that the ray head",
    "start": "1054860",
    "end": "1060740"
  },
  {
    "text": "node will go up and then the workers based on the requirements from our users and if needed it can Auto scale",
    "start": "1060740",
    "end": "1067640"
  },
  {
    "text": "accordingly we create two ingresses one for HTTP access to our array cluster and another",
    "start": "1067640",
    "end": "1074660"
  },
  {
    "text": "one for the grpc access to it the ray head node and array workers are",
    "start": "1074660",
    "end": "1080660"
  },
  {
    "text": "all running based on an image that we create specifically for each individual",
    "start": "1080660",
    "end": "1085820"
  },
  {
    "text": "use case what happens from the user perspective is that they write their",
    "start": "1085820",
    "end": "1090860"
  },
  {
    "text": "code and they push it to the repo once the code is in the repo in a branch or",
    "start": "1090860",
    "end": "1096679"
  },
  {
    "text": "in the main branch automated cicd pipelines take that specific code and requirements and build",
    "start": "1096679",
    "end": "1104059"
  },
  {
    "text": "a Merlin Project image that is dedicated for a specific machine learning use case",
    "start": "1104059",
    "end": "1109220"
  },
  {
    "text": "and that image is then being pushed into the Google Cloud registry GCR and when",
    "start": "1109220",
    "end": "1115760"
  },
  {
    "text": "the ray head node and worker starts they pull that image and the ray cluster that",
    "start": "1115760",
    "end": "1121039"
  },
  {
    "text": "you get includes all the code that is dedicated for that machine learning use case",
    "start": "1121039",
    "end": "1128020"
  },
  {
    "text": "from the user's perspective again they work with a mono repo that we set up for",
    "start": "1130280",
    "end": "1135799"
  },
  {
    "text": "them they have that mono repo has a projects folder and under it under this folder",
    "start": "1135799",
    "end": "1143900"
  },
  {
    "text": "each project each Merlin project get gets its own folder that folder will",
    "start": "1143900",
    "end": "1150140"
  },
  {
    "text": "contain the source code for the use case the unit tests and also a configuration",
    "start": "1150140",
    "end": "1155660"
  },
  {
    "text": "for it the configuration sits inside of the config.tml file that you see here in",
    "start": "1155660",
    "end": "1161299"
  },
  {
    "text": "the image and once the user pushes this code to a branch or merges that Branch",
    "start": "1161299",
    "end": "1168620"
  },
  {
    "text": "into the main uh the main branch automated cicd pipelines will take all",
    "start": "1168620",
    "end": "1174620"
  },
  {
    "text": "of those artifacts and create a Docker image for that so they can use that for",
    "start": "1174620",
    "end": "1180740"
  },
  {
    "text": "creating their Merlin workspace if we take a look at the config.tml file",
    "start": "1180740",
    "end": "1188000"
  },
  {
    "text": "that is included with each Merlin project we can see different elements",
    "start": "1188000",
    "end": "1194059"
  },
  {
    "text": "that our users need to put in for example the code owners for the use case",
    "start": "1194059",
    "end": "1199220"
  },
  {
    "text": "so we'll know who is owning this thing in addition the base image that we're",
    "start": "1199220",
    "end": "1205400"
  },
  {
    "text": "building the Merlin project on top of system packages on the operating system",
    "start": "1205400",
    "end": "1211100"
  },
  {
    "text": "level and a conda environment with the python dependencies that are needed for",
    "start": "1211100",
    "end": "1216919"
  },
  {
    "text": "this use case in this example our use case is called the Boston Housing scikit-learn it uses python version 3812",
    "start": "1216919",
    "end": "1225260"
  },
  {
    "text": "with a ray version 112 one which will be upgraded to 2.0 very soon and then uses",
    "start": "1225260",
    "end": "1234200"
  },
  {
    "text": "pandas numpy and scikit-learn for the code that we're running this just shows the flexibility that we",
    "start": "1234200",
    "end": "1242360"
  },
  {
    "text": "allow our users to have because each project can use a different set of requirements and a different set of",
    "start": "1242360",
    "end": "1249919"
  },
  {
    "text": "images and dependencies based on what they need for their use case",
    "start": "1249919",
    "end": "1256419"
  },
  {
    "text": "we also added a CLI for our platform that CLI allows our users to create and",
    "start": "1257840",
    "end": "1264559"
  },
  {
    "text": "manage their Merlin projects also create and manage their Merlin workspaces",
    "start": "1264559",
    "end": "1269780"
  },
  {
    "text": "through that API that we built run remote raid jobs so again on their",
    "start": "1269780",
    "end": "1276200"
  },
  {
    "text": "laptop they can use this CLI to wrap all all the artifacts that they have on",
    "start": "1276200",
    "end": "1281780"
  },
  {
    "text": "their laptop and then send it to the remote tray cluster for fast iterations and it also allows them to create a",
    "start": "1281780",
    "end": "1288320"
  },
  {
    "text": "local Dev environment in this image we see the command that creates a numer",
    "start": "1288320",
    "end": "1293900"
  },
  {
    "text": "link project which will send the users through a wizard of use of choosing the",
    "start": "1293900",
    "end": "1300919"
  },
  {
    "text": "dependencies that they need and you know it will take them through it or give them the ability to have a completely",
    "start": "1300919",
    "end": "1307520"
  },
  {
    "text": "custom Merlin project that they can specify for themselves and this thing is easily extendable as we add more and",
    "start": "1307520",
    "end": "1314360"
  },
  {
    "text": "more features to our platform on the orchestration side we have",
    "start": "1314360",
    "end": "1319580"
  },
  {
    "text": "different orchestrators this is an example of an airflow dag where we",
    "start": "1319580",
    "end": "1324980"
  },
  {
    "text": "created different airflow operators for starting and deleting Merlin workspaces",
    "start": "1324980",
    "end": "1331159"
  },
  {
    "text": "so that our users don't have to think about it and all they need to do is to create a dag for their use case",
    "start": "1331159",
    "end": "1337360"
  },
  {
    "text": "specifically in this very very simple example we have just one step which is",
    "start": "1337360",
    "end": "1342860"
  },
  {
    "text": "the training and it always starts with creating the workspace going through the ml workflow and then deleting the",
    "start": "1342860",
    "end": "1349940"
  },
  {
    "text": "workspace at the end let's take a step back and look at the",
    "start": "1349940",
    "end": "1355880"
  },
  {
    "text": "user workflow end to end so we start with a new machine learning project by",
    "start": "1355880",
    "end": "1362059"
  },
  {
    "text": "creating a numer lean project then we go into prototype phase where our users can",
    "start": "1362059",
    "end": "1367580"
  },
  {
    "text": "create Merlin workspaces and prototype on it from their local machine or a jupyter notebook once their code is",
    "start": "1367580",
    "end": "1374659"
  },
  {
    "text": "ready they can push it into the Merlin project repo and update it with",
    "start": "1374659",
    "end": "1381020"
  },
  {
    "text": "everything that they need and then go into the orchestrators so they can periodically schedule their jobs to run",
    "start": "1381020",
    "end": "1388760"
  },
  {
    "text": "every now and then once they need to update their project they go back to the",
    "start": "1388760",
    "end": "1393860"
  },
  {
    "text": "Prototype phase and iterate on everything that they built",
    "start": "1393860",
    "end": "1398799"
  },
  {
    "text": "summary what did we learn we learned a lot from building Merlin in",
    "start": "1398900",
    "end": "1406039"
  },
  {
    "text": "our previous iterations and just mentioning a few of the main lessons that we learned when building a machine",
    "start": "1406039",
    "end": "1413240"
  },
  {
    "text": "learning platform it's important to build around real use cases in order to make sure that real user requirements",
    "start": "1413240",
    "end": "1420200"
  },
  {
    "text": "are being addressed in our case when we did the POC we chose one of the most",
    "start": "1420200",
    "end": "1425539"
  },
  {
    "text": "complicated use cases that we have at Shopify with the most amount of data",
    "start": "1425539",
    "end": "1431360"
  },
  {
    "text": "that we have to scale up to this allows us to say hey you know what Ray is actually very very good for scaling up",
    "start": "1431360",
    "end": "1438500"
  },
  {
    "text": "and it actually you know held the the amount of data that we had to use for",
    "start": "1438500",
    "end": "1444140"
  },
  {
    "text": "training and processing in addition building a platform is",
    "start": "1444140",
    "end": "1450140"
  },
  {
    "text": "completely different than maintaining one like we've been building one for the past year and a half we are slowly",
    "start": "1450140",
    "end": "1456799"
  },
  {
    "text": "transitioning into a maintaining one into a maintaining a machine learning platform and this means that making",
    "start": "1456799",
    "end": "1463100"
  },
  {
    "text": "changes is a little bit more difficult and a little bit like it requires a little bit more frictions because we",
    "start": "1463100",
    "end": "1469400"
  },
  {
    "text": "don't want to have down times and we're very careful of hurting the production jobs that are already running on top of",
    "start": "1469400",
    "end": "1475220"
  },
  {
    "text": "it in addition Rey is a really great library right like it changes frequently",
    "start": "1475220",
    "end": "1480320"
  },
  {
    "text": "and also adds a lot of features all the time but this also means that we need to",
    "start": "1480320",
    "end": "1485360"
  },
  {
    "text": "plan ahead and make sure that we can include these new features as the library changes",
    "start": "1485360",
    "end": "1491539"
  },
  {
    "text": "building is difficult maintaining is difficult and also migrating users to a new platform in the third time that you",
    "start": "1491539",
    "end": "1498980"
  },
  {
    "text": "do it is also extremely difficult all the goals that we set up allowed us",
    "start": "1498980",
    "end": "1505159"
  },
  {
    "text": "to migrate our users much easily than before because now they actually want to",
    "start": "1505159",
    "end": "1510740"
  },
  {
    "text": "migrate to the new system because they can do whatever they want with it right they can use whatever library or package",
    "start": "1510740",
    "end": "1517460"
  },
  {
    "text": "that they want to for the use case and it just helps them to move more quickly and use the right tool for the job",
    "start": "1517460",
    "end": "1525440"
  },
  {
    "text": "and the user experience while building a machine learning platform it's also important to remember who your users are",
    "start": "1525440",
    "end": "1532100"
  },
  {
    "text": "and their profile and if it requires add CLI or different tools to allow them to",
    "start": "1532100",
    "end": "1537919"
  },
  {
    "text": "to migrate to your new platform easily",
    "start": "1537919",
    "end": "1542919"
  },
  {
    "text": "so what did we cover we talk about Ray we talked about how we built a machine",
    "start": "1543260",
    "end": "1548480"
  },
  {
    "text": "learning platform on top of Ray how we isolate distributed computation environment and the lessons learned that",
    "start": "1548480",
    "end": "1555740"
  },
  {
    "text": "we have from building a machine learning platform for the third time but no the journey is not over we also",
    "start": "1555740",
    "end": "1564380"
  },
  {
    "text": "have still like we still have gaps that we need to make sure that we complete for our machine learning platform we",
    "start": "1564380",
    "end": "1570740"
  },
  {
    "text": "recently added the ability to deploy and serve machine learning models for online inference and real-time predictions we",
    "start": "1570740",
    "end": "1578179"
  },
  {
    "text": "also added a low code framework for machine learning pipelines so our users can focus only on the models and what",
    "start": "1578179",
    "end": "1585020"
  },
  {
    "text": "they need to build in the future we plan to upgrade to Ray 2.0 and Cube Ray add a model registry",
    "start": "1585020",
    "end": "1591919"
  },
  {
    "text": "for experiment tracking and have a standard for monitoring machine learning models in production",
    "start": "1591919",
    "end": "1599419"
  },
  {
    "text": "before we go into questions I just wanted to extend many acknowledgments and thanks for a huge huge shout out for",
    "start": "1599419",
    "end": "1607220"
  },
  {
    "text": "the ammo platform team that worked a lot on this in the past year and a half we have here in the in the crowd Alex K and",
    "start": "1607220",
    "end": "1615200"
  },
  {
    "text": "Vivian uh so also feel free to say hello to them uh everyone at Shopify that",
    "start": "1615200",
    "end": "1620900"
  },
  {
    "text": "helped us to build this thing from the data science teams that onboarded to the",
    "start": "1620900",
    "end": "1626419"
  },
  {
    "text": "platform the product managers devops team that helped us with the infrastructure and the user experience",
    "start": "1626419",
    "end": "1632240"
  },
  {
    "text": "test group that helped us to validate everything that we built so far and a special thank you for everyone at the",
    "start": "1632240",
    "end": "1638299"
  },
  {
    "text": "nhscale team that helped us and still support us to build this amazing new",
    "start": "1638299",
    "end": "1644059"
  },
  {
    "text": "platform and thank you all for joining this talk",
    "start": "1644059",
    "end": "1651100"
  },
  {
    "text": "questions we only have like two and a half minutes",
    "start": "1655820",
    "end": "1660278"
  },
  {
    "text": "thank you thanks for the talk um I'm curious how serving online",
    "start": "1667520",
    "end": "1672919"
  },
  {
    "text": "serving interacts with the workspace concept uh particularly",
    "start": "1672919",
    "end": "1679400"
  },
  {
    "text": "um seems like having a predefined TTL makes it difficult to have long-lived yep",
    "start": "1679400",
    "end": "1685340"
  },
  {
    "text": "blame it's interested how you resolve that okay so in online inference we",
    "start": "1685340",
    "end": "1690679"
  },
  {
    "text": "separate the workspace uh and a service right service is a long-term running",
    "start": "1690679",
    "end": "1695779"
  },
  {
    "text": "thing which we can have a whole talk about that and how we did it at Shopify but the workspace is actually being used",
    "start": "1695779",
    "end": "1703460"
  },
  {
    "text": "to test out the API and the serving layer before you deploy to production right so this is kind of like it can be",
    "start": "1703460",
    "end": "1711080"
  },
  {
    "text": "as a small Dev environment that you can use for the online predictions before you deploy it as a service so is my",
    "start": "1711080",
    "end": "1718940"
  },
  {
    "text": "would my workspace be the same environment as like one or more services",
    "start": "1718940",
    "end": "1724279"
  },
  {
    "text": "that are sort of associated with it or are the concepts totally separate you can you can set it up this way obviously",
    "start": "1724279",
    "end": "1731600"
  },
  {
    "text": "for services you might need reduced resources because you're not running like training or something like that but",
    "start": "1731600",
    "end": "1738080"
  },
  {
    "text": "you can set up a Merlin workspace with like 2 CPUs and 5 gig of memory or",
    "start": "1738080",
    "end": "1743480"
  },
  {
    "text": "something like that if you want really something slim but yeah you yeah that's",
    "start": "1743480",
    "end": "1749779"
  },
  {
    "text": "basically the the ability that that you can set up from the platform but you can also go the path of like hey I'll just",
    "start": "1749779",
    "end": "1756020"
  },
  {
    "text": "deploy it as a service and check and test that in the staging environment great thank you",
    "start": "1756020",
    "end": "1764020"
  },
  {
    "text": "hi thank you for the great talk a quick question can you share how the cost estimation feature is done",
    "start": "1766940",
    "end": "1772700"
  },
  {
    "text": "yes uh so um when we create so from the API it's",
    "start": "1772700",
    "end": "1777740"
  },
  {
    "text": "very easy to start labeling the resources that we create on kubernetes right and then on gcp you can set up",
    "start": "1777740",
    "end": "1784220"
  },
  {
    "text": "like different queries and different dashboards that you can you know then take into account as as you know even",
    "start": "1784220",
    "end": "1791059"
  },
  {
    "text": "live right for the specific resources that you're using some of it is like",
    "start": "1791059",
    "end": "1796279"
  },
  {
    "text": "hand wavy right but uh like we can give some sort of estimation it's not precise",
    "start": "1796279",
    "end": "1801980"
  },
  {
    "text": "but it's close enough hey Isaac great talk thank you a",
    "start": "1801980",
    "end": "1807080"
  },
  {
    "text": "question on the workspace uh can you talk a little bit about how you do versioning of those workspaces okay yeah",
    "start": "1807080",
    "end": "1812539"
  },
  {
    "text": "how data scientists collaborate on the workspaces yeah so good question um so the versioning is not on the",
    "start": "1812539",
    "end": "1819260"
  },
  {
    "text": "workspace level it's on the API right so like the API is basically a fast API",
    "start": "1819260",
    "end": "1824899"
  },
  {
    "text": "that we you know we we create different versions and then if we add a breaking",
    "start": "1824899",
    "end": "1830000"
  },
  {
    "text": "change we can set up it as a new version right so our so we don't have to update",
    "start": "1830000",
    "end": "1835100"
  },
  {
    "text": "all our clients at the same time for uh using the same workspace across",
    "start": "1835100",
    "end": "1840740"
  },
  {
    "text": "different teams or across different individuals so each workspace gets its own DNS record and also if they include",
    "start": "1840740",
    "end": "1849740"
  },
  {
    "text": "the ability to run Jupiter lab on top of the head node everyone can just access",
    "start": "1849740",
    "end": "1854960"
  },
  {
    "text": "that jupyter lab and collaborate we try to integrate in the past the real-time",
    "start": "1854960",
    "end": "1860539"
  },
  {
    "text": "collaboration with Jupiter lab but it's still kinda flaky and we'll wait until",
    "start": "1860539",
    "end": "1866179"
  },
  {
    "text": "Jupiter 4.0 to include that okay that's all the time we have for",
    "start": "1866179",
    "end": "1872120"
  },
  {
    "text": "questions please give a round of applause to Isaac our speaker [Applause]",
    "start": "1872120",
    "end": "1879919"
  }
]