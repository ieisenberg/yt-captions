[
  {
    "text": "uh so you know Welcome to our presentation for accelerating VM deployment through quantization my name",
    "start": "3480",
    "end": "9320"
  },
  {
    "text": "is Michael and I'll be covering your introduction into quantization within VM and then Robert Shaw will dive into some",
    "start": "9320",
    "end": "16920"
  },
  {
    "text": "performance and accuracy evaluations to show the importance of compression uh and the dirty details there so we're",
    "start": "16920",
    "end": "23720"
  },
  {
    "text": "from neural magic and our mission is to democratize AI by bringing the power of Open Source llms and VM to Enterprises",
    "start": "23720",
    "end": "31720"
  },
  {
    "text": "and users globally in the next 30 minutes we'll explore how quantization can significantly enhance the",
    "start": "31720",
    "end": "37480"
  },
  {
    "text": "performance of your VM deployments we'll cover the fundamentals of quantization its implementation in VM and how it",
    "start": "37480",
    "end": "44440"
  },
  {
    "text": "leads to faster inference times and reduced memory usage with ensuring high",
    "start": "44440",
    "end": "49960"
  },
  {
    "text": "accuracy preservation we'll give you valuable insights into both those new to",
    "start": "49960",
    "end": "55160"
  },
  {
    "text": "quantization and those looking to optimize their existing deployments so our presid presentation",
    "start": "55160",
    "end": "60280"
  },
  {
    "text": "is structured into five key sections uh that will help you help guide your understanding of quantization so first",
    "start": "60280",
    "end": "66360"
  },
  {
    "text": "we'll cover a quick quantization 101 so fundamentals on the concepts and principles there then an exploration of",
    "start": "66360",
    "end": "72560"
  },
  {
    "text": "vm's implementation of quantization techniques then we'll perform an analysis on the benefits of quantization",
    "start": "72560",
    "end": "79640"
  },
  {
    "text": "activations in addition to the traditional Weighton quantization then we'll have an",
    "start": "79640",
    "end": "85520"
  },
  {
    "text": "introduction to our framework LM compressor for creating quantized and compressed models",
    "start": "85520",
    "end": "90600"
  },
  {
    "text": "and finally we'll have an overview of future developments in quantization uh that we have planned for",
    "start": "90600",
    "end": "96920"
  },
  {
    "text": "VM so first 101 let's begin by examining the fundamental operations within an llm",
    "start": "96960",
    "end": "102840"
  },
  {
    "text": "at its core an an llm performs a series of matrix multiplication operations using learn parameters each of the",
    "start": "102840",
    "end": "109920"
  },
  {
    "text": "billions of parameters are represented numerically and stored using a specific number of bits typically High Precision",
    "start": "109920",
    "end": "116920"
  },
  {
    "text": "formats such as fp32 or bf b bf16 are used during training and and inference",
    "start": "116920",
    "end": "123719"
  },
  {
    "text": "these high Precision formats allow for a wide dynamic range and high accuracy but",
    "start": "123719",
    "end": "128959"
  },
  {
    "text": "come at the cost of increased memory usage and computation time to illustrate",
    "start": "128959",
    "end": "134040"
  },
  {
    "text": "a single fp32 number uses 32 bytes of memory in a model of just 1 billion",
    "start": "134040",
    "end": "139760"
  },
  {
    "text": "parameters this translates to 32 billion bits or 4 gigabytes of memory for single",
    "start": "139760",
    "end": "145879"
  },
  {
    "text": "for for just parameter storage alone this memory requirement presents challenges for inference for deployment",
    "start": "145879",
    "end": "151560"
  },
  {
    "text": "and inference especially as you scale up to billions and billions of parameters which quantization aims to",
    "start": "151560",
    "end": "158720"
  },
  {
    "text": "address so getting into quantization it's a technique for representing numbers using fewer bits effectively",
    "start": "158720",
    "end": "165120"
  },
  {
    "text": "mapping High Precision values to lower Precision formats for example quantization might map a range of fp32",
    "start": "165120",
    "end": "171800"
  },
  {
    "text": "or bf16 values to intake or fp8 uh ranges and and and formats this is what",
    "start": "171800",
    "end": "178840"
  },
  {
    "text": "the uh image shows so it visualizes this concept showing how a continuous range",
    "start": "178840",
    "end": "183920"
  },
  {
    "text": "of values at higher Precision can be mapped to a smaller set of discrete values within a smaller Precision while",
    "start": "183920",
    "end": "190760"
  },
  {
    "text": "quantization does involve some loss in Precision Advanced Techniques during the quantization process allow us to",
    "start": "190760",
    "end": "197840"
  },
  {
    "text": "minimize this loss to maintain model performance in some instances quantization can even act as a form of",
    "start": "197840",
    "end": "203599"
  },
  {
    "text": "regularization potentially leading to slight improvements in model performance aiding and generalization this the CH",
    "start": "203599",
    "end": "210080"
  },
  {
    "text": "lies in finding the optimal balance between reduced Precision for performance gains and maintaining model",
    "start": "210080",
    "end": "215720"
  },
  {
    "text": "capabilities so quantization offers two primary benefits and that's why you should quantize so the first one is is",
    "start": "215720",
    "end": "222680"
  },
  {
    "text": "obvious reduced uh you know Hardware accelerator Ram requirements it substantially uh you know can reduce",
    "start": "222680",
    "end": "228879"
  },
  {
    "text": "your memory usage for instance quantitizing from fp16 to in4 so from 16",
    "start": "228879",
    "end": "235400"
  },
  {
    "text": "bits to 4 bits can potentially reduce your memory requirements by 75% allowing you to host larger and larger models and",
    "start": "235400",
    "end": "242519"
  },
  {
    "text": "then there's the other aspect of accelerating linear layers through reduced data movement meaning we'll move",
    "start": "242519",
    "end": "248760"
  },
  {
    "text": "less data between memory to the compute units or tensor cores which is often a bottleneck at AI inference and a small",
    "start": "248760",
    "end": "255799"
  },
  {
    "text": "batch size workloads and then accelerating through utilization of low Precision tensor cores such that modern",
    "start": "255799",
    "end": "262919"
  },
  {
    "text": "gpus have specialized hardware for low Precision computation such as fp8 tensor",
    "start": "262919",
    "end": "268440"
  },
  {
    "text": "cores uh which offers you a you know a significant flops",
    "start": "268440",
    "end": "273680"
  },
  {
    "text": "Advantage so for instance inate or fp8 operations on Modern gpus can be two times faster than fp16 operations just",
    "start": "273680",
    "end": "281280"
  },
  {
    "text": "in terms of the flops or compute available compounding compound compounding this with memory bandwidth",
    "start": "281280",
    "end": "287759"
  },
  {
    "text": "uh effective memory bandwidth improvements means that the Improvement can be even larger sometimes uh so a key",
    "start": "287759",
    "end": "294600"
  },
  {
    "text": "thing with quantization is accuracy preservation and we work with advanced",
    "start": "294600",
    "end": "299759"
  },
  {
    "text": "techniques like fine grain quantization to allow us to achieve these benefits of quantization with minimal impact on",
    "start": "299759",
    "end": "305560"
  },
  {
    "text": "model quality uh fine grain quantization essentially means applying different scales to different parts of the weights",
    "start": "305560",
    "end": "312840"
  },
  {
    "text": "even within a single layer preserving critical local information where necessary this approach often allows",
    "start": "312840",
    "end": "318639"
  },
  {
    "text": "quantization to 8 bit or even 4-bit precisions with excellent accuracy while getting substantial speed and memory",
    "start": "318639",
    "end": "325720"
  },
  {
    "text": "benefits so now let's cover how VM supports quantis",
    "start": "325720",
    "end": "331479"
  },
  {
    "text": "so first our goal with VM and most everyone's goal with VM is to build the fastest and easy to use open source LM",
    "start": "331479",
    "end": "338440"
  },
  {
    "text": "inference and serving engine we achieved this personally by building blm's Quan quantization support on four key pillars",
    "start": "338440",
    "end": "345520"
  },
  {
    "text": "first fast inference kernels obviously for the most popular quantitized operations that researchers and the",
    "start": "345520",
    "end": "351720"
  },
  {
    "text": "community is developing ecosystem compatibility such that the popular open source quantization formats that already",
    "start": "351720",
    "end": "357600"
  },
  {
    "text": "exist and that we make are broadly supported and accepted by the open",
    "start": "357600",
    "end": "362680"
  },
  {
    "text": "source AI Community then providing pre- quantized model repositories so people have a",
    "start": "362680",
    "end": "369880"
  },
  {
    "text": "collection of ready to use off-the-shelf quantized models and then LM compressor which is a",
    "start": "369880",
    "end": "376240"
  },
  {
    "text": "UniFi framework for exploring and experimenting with combining various optimization techniques to get the best",
    "start": "376240",
    "end": "382560"
  },
  {
    "text": "performance and accuracy all these uh components work together to provide a comprehensive quantization solution",
    "start": "382560",
    "end": "389000"
  },
  {
    "text": "within VL M and make it easy for users so we'll step through each of those things so at the core of vm's",
    "start": "389000",
    "end": "395960"
  },
  {
    "text": "quantization support are the custom inference kernels will actually be running that that that we've contributed",
    "start": "395960",
    "end": "402360"
  },
  {
    "text": "uh that are optimized for quantized uh General matrix multiplication or gems",
    "start": "402360",
    "end": "407720"
  },
  {
    "text": "for Activation quantization most importantly we support fp8 and int 8 formats using cutless kernels optimized",
    "start": "407720",
    "end": "414759"
  },
  {
    "text": "for maximum efficiency on moded Nvidia gpus for weight only quantization we",
    "start": "414759",
    "end": "419960"
  },
  {
    "text": "support W8 a16 and W4 a16 formats using specialized kernels such as Marlin which",
    "start": "419960",
    "end": "426360"
  },
  {
    "text": "is optimized for Amper architectures and machete which uh is brand new and we",
    "start": "426360",
    "end": "432520"
  },
  {
    "text": "optimized for Hopper and onward uh as we'll be optimizing for the latest",
    "start": "432520",
    "end": "438759"
  },
  {
    "text": "gpus um if you look at some of the benchmarks on the right uh this some of this validates this if you look at the",
    "start": "438759",
    "end": "444800"
  },
  {
    "text": "one furthest on the right um we compare our fp8 implementation to the Baseline pip High torch implementation and we",
    "start": "444800",
    "end": "451560"
  },
  {
    "text": "show that we can get much better performance and maintain that performance while using even more",
    "start": "451560",
    "end": "457120"
  },
  {
    "text": "accurate methods like Dynamic per token activation quantization making sure that users can reach their accuracy goals",
    "start": "457120",
    "end": "464240"
  },
  {
    "text": "easily uh without much complication or leaving performance on the table so these optimized kernels translate",
    "start": "464240",
    "end": "470440"
  },
  {
    "text": "directly to faster inference times allowing you to leverage uh fully the low Precision Hardware uh",
    "start": "470440",
    "end": "477759"
  },
  {
    "text": "available so VM integrates various quantization formats used in the open source ecosystem including popular ones",
    "start": "477759",
    "end": "483960"
  },
  {
    "text": "you probably know of already like Auto awq auto gptq ggf bits and bytes with",
    "start": "483960",
    "end": "489440"
  },
  {
    "text": "many more existing formats in the community already used for weight link quantization these formats right off the",
    "start": "489440",
    "end": "495759"
  },
  {
    "text": "shelf can be natively loaded and run with our most optimized kernels that are already available inside of VM ins",
    "start": "495759",
    "end": "503000"
  },
  {
    "text": "ensuring that you can use them uh without trading off compatibility and performance a key feature we can talk",
    "start": "503000",
    "end": "508560"
  },
  {
    "text": "about specifically is the automatic conversion of Auto gptq and auto awq models to our Marlin and machete formats",
    "start": "508560",
    "end": "516159"
  },
  {
    "text": "this automatic conversion while model loading ensures Optimal Performance on your GPU regardless of the original",
    "start": "516159",
    "end": "522919"
  },
  {
    "text": "quantization method this compati compatibility layer is key such that",
    "start": "522919",
    "end": "528240"
  },
  {
    "text": "users can load their existing models and just take advantage of better kernels as",
    "start": "528240",
    "end": "533680"
  },
  {
    "text": "they come along rather than worrying about needing to re quantize to make use of the best performance",
    "start": "533680",
    "end": "541320"
  },
  {
    "text": "so as I mentioned before to facilitate rapid development we at neurom Magic maintain a repository of pre- quantized",
    "start": "541600",
    "end": "547160"
  },
  {
    "text": "model checkpoints on the hugging face model Hub this includes a catalog for popular models such as llama mistol fee",
    "start": "547160",
    "end": "554480"
  },
  {
    "text": "now Vision language models um we have multiple quantization levels for each Model 8 bit 4bit weight only 8bit inted",
    "start": "554480",
    "end": "562920"
  },
  {
    "text": "and fp8 activation quantization even fp8 KV cache quantization uh sparcity we're",
    "start": "562920",
    "end": "568920"
  },
  {
    "text": "growing this day by day and each of those come with examples of usage with VM to demonstrate how to use the models",
    "start": "568920",
    "end": "577120"
  },
  {
    "text": "uh we also regularly publish performance performance benchmarks so you can understand the use of these models under",
    "start": "577120",
    "end": "582680"
  },
  {
    "text": "various workloads and then accuracy valuations for standard tasks to report the accuracy",
    "start": "582680",
    "end": "588480"
  },
  {
    "text": "preservation these pre pre- quantized checkpoints enable quick comparison of different quation levels on the same",
    "start": "588480",
    "end": "594800"
  },
  {
    "text": "fixed model allowing you to select you know the optimal trade-off for your use case",
    "start": "594800",
    "end": "599959"
  },
  {
    "text": "uh we're continuously expanding this repo based on community feedback our latest research research findings and of",
    "start": "599959",
    "end": "607120"
  },
  {
    "text": "course the latest model of the week uh and lastly what I'll cover is LM",
    "start": "607120",
    "end": "612680"
  },
  {
    "text": "compressor which is our unified framework for creating quantized models optimized for deployment with VM it",
    "start": "612680",
    "end": "618000"
  },
  {
    "text": "supports a different range of Quant of uh compression techniques including various algorithms such as round to",
    "start": "618000",
    "end": "624720"
  },
  {
    "text": "nearest uh gptq which is uses second order information for improve proof quantization awq as mentioned before for",
    "start": "624720",
    "end": "632519"
  },
  {
    "text": "Activation a wear weight quantization smooth Quant for smoothing Activation so they can be more easily quantized sparse",
    "start": "632519",
    "end": "638839"
  },
  {
    "text": "GPT for inducing sparsity in the model weights for better compression and then",
    "start": "638839",
    "end": "643959"
  },
  {
    "text": "after you've applied each of these algorithms we can then save the compressed models in an actual compressed format on disk in many",
    "start": "643959",
    "end": "650880"
  },
  {
    "text": "different formats like I mentioned before w88 for your fp8 and inate activation quantization W4 a16 or W8 a16",
    "start": "650880",
    "end": "660320"
  },
  {
    "text": "for quantizing just the weights to lower precisions and two for sparcity for uh",
    "start": "660320",
    "end": "665720"
  },
  {
    "text": "storing uh weights and parameters that have uh inherent sparcity",
    "start": "665720",
    "end": "671680"
  },
  {
    "text": "uh uh uh like for for for more extreme model compression",
    "start": "671680",
    "end": "677040"
  },
  {
    "text": "compression the key flexibility of LM compressor and the composability of all these uh algorithms and techniques",
    "start": "677040",
    "end": "684000"
  },
  {
    "text": "allows uh you know users to experiment and determine the optimal quantization strategy",
    "start": "684000",
    "end": "689839"
  },
  {
    "text": "for their specific model and specific use cases of course alongside this we have best practices in there so you",
    "start": "689839",
    "end": "695399"
  },
  {
    "text": "don't need to experiment and you can hopefully just get on with your deployments so coming up next Rob will",
    "start": "695399",
    "end": "702320"
  },
  {
    "text": "take the lead and uh diving deep into into the impact here yep um thanks",
    "start": "702320",
    "end": "707959"
  },
  {
    "text": "Michael so I'm going to talk about some real world impacts of quantization and",
    "start": "707959",
    "end": "713680"
  },
  {
    "text": "make the case that everyone should be using activation quantization uh when they're deploying so H about Michel yeah",
    "start": "713680",
    "end": "720680"
  },
  {
    "text": "um yeah sure so um so in terms of",
    "start": "720680",
    "end": "726079"
  },
  {
    "text": "understanding why activation quantization the first thing we need to do is understand where time is spent uh",
    "start": "726079",
    "end": "733079"
  },
  {
    "text": "when a model is executing so the chart on the right uh effectively shows uh the",
    "start": "733079",
    "end": "739000"
  },
  {
    "text": "per layer or or the different operations in llama 3 a uh during an inference uh",
    "start": "739000",
    "end": "745760"
  },
  {
    "text": "pass um and the pink bars are What's called the linear layers um and what we",
    "start": "745760",
    "end": "751279"
  },
  {
    "text": "can see is that most of the time for reasonable batch sizes is spent in",
    "start": "751279",
    "end": "756560"
  },
  {
    "text": "reasonable sequence lengths is spent in these pink linear bars um and the taller",
    "start": "756560",
    "end": "762680"
  },
  {
    "text": "bars are showing the amount of time that's spent when the model is running at fp16 and the shorter bars show the",
    "start": "762680",
    "end": "770639"
  },
  {
    "text": "model running at fp8 and so the point of all this is with activation quantization",
    "start": "770639",
    "end": "776600"
  },
  {
    "text": "is to take the most expensive part of the model and cut it in",
    "start": "776600",
    "end": "782000"
  },
  {
    "text": "half and to understand why activation quantization I wanted to explain a",
    "start": "782199",
    "end": "787480"
  },
  {
    "text": "little bit of what happens in one of these linear layers you know as Michael mentioned the linear layers are Matrix",
    "start": "787480",
    "end": "794120"
  },
  {
    "text": "multiplications they're they're gems and any gem is the the time it takes to",
    "start": "794120",
    "end": "800360"
  },
  {
    "text": "execute any gem is determined by three variables uh n K and M which are the",
    "start": "800360",
    "end": "806360"
  },
  {
    "text": "shapes of the various matrices that are being multiplied together and so when we look at a large language model um the n",
    "start": "806360",
    "end": "814880"
  },
  {
    "text": "and K variables are very very large we have massive weight matrices that we",
    "start": "814880",
    "end": "820519"
  },
  {
    "text": "need to load into the uh SRAM and multiply by the activations and and",
    "start": "820519",
    "end": "826279"
  },
  {
    "text": "these are fixed but M which is the shape of the activation Matrix that we're",
    "start": "826279",
    "end": "832920"
  },
  {
    "text": "multiplying by the weight is variable and often very small so to understand",
    "start": "832920",
    "end": "838800"
  },
  {
    "text": "this most will understand that for llms we have a pre-fill phase and a decode phase",
    "start": "838800",
    "end": "844440"
  },
  {
    "text": "of inference in the prefill phase M can be very large you know we might have a prompt of length 8,000 or 4,000 and that",
    "start": "844440",
    "end": "852639"
  },
  {
    "text": "will make the X Matrix have a shape of 8,000 uh when we're when we're passing it in um but for decode and especially",
    "start": "852639",
    "end": "860440"
  },
  {
    "text": "low batch size decode M can be very small you know for batch one decode M will be length one and we'll be",
    "start": "860440",
    "end": "866600"
  },
  {
    "text": "multiplying a very large Matrix times a very skinny uh you know in in in the",
    "start": "866600",
    "end": "872399"
  },
  {
    "text": "batch one case and actually a vector not a matrix and this completely changes the",
    "start": "872399",
    "end": "878600"
  },
  {
    "text": "Dynamics of uh how the gem will respond uh in terms of the overall amount of",
    "start": "878600",
    "end": "884639"
  },
  {
    "text": "time that it takes to execute a single operation so this m variable is very important in determining um which",
    "start": "884639",
    "end": "891519"
  },
  {
    "text": "quantization scheme to use and how the gem will react and so to understand why",
    "start": "891519",
    "end": "897720"
  },
  {
    "text": "activation quantization I I first wanted to explain what drives the latency of a gem operation and to do this you have to",
    "start": "897720",
    "end": "906040"
  },
  {
    "text": "understand how a quantization how a gem kernel Works basically at a really really high level what happens is the",
    "start": "906040",
    "end": "913720"
  },
  {
    "text": "weights are sitting in the GPU hbm memory and to execute the computation we",
    "start": "913720",
    "end": "920560"
  },
  {
    "text": "move the weights from hbm into the onchip memory feed it into the tensor",
    "start": "920560",
    "end": "925839"
  },
  {
    "text": "cores and do the multiplication and then write it back out to HPM so the amount",
    "start": "925839",
    "end": "931079"
  },
  {
    "text": "of time or the latency of a gem operation is driven by a how long it",
    "start": "931079",
    "end": "936759"
  },
  {
    "text": "takes to move the weights from hbm into the SRAM as well as how long it takes to",
    "start": "936759",
    "end": "942959"
  },
  {
    "text": "actually execute the tensor core flops and these variables the bandwidth",
    "start": "942959",
    "end": "950000"
  },
  {
    "text": "as well as the tensor core number of flops are constants on the GPU so an",
    "start": "950000",
    "end": "955959"
  },
  {
    "text": "a140 gab for instance has 1.5 terabytes per second of bandwidth from hbm to SRAM",
    "start": "955959",
    "end": "962839"
  },
  {
    "text": "on an h100 this is something like 3.35 terabytes per second and you know we can",
    "start": "962839",
    "end": "968560"
  },
  {
    "text": "see one of the ways that quantization can help us is since we have less weights to move since we've compressed",
    "start": "968560",
    "end": "975519"
  },
  {
    "text": "from fp16 to fp8 there's less data that has to move from hbm into the SRAM um",
    "start": "975519",
    "end": "982480"
  },
  {
    "text": "and so when we have small M when m is really small and the memory bandwidth",
    "start": "982480",
    "end": "988240"
  },
  {
    "text": "becomes the bottle neck how long it takes to move things in and out um quantization can help to accelerate by",
    "start": "988240",
    "end": "993839"
  },
  {
    "text": "just reducing the amount of data that needs to move from hbm into the SRAM and then in addition as Michael",
    "start": "993839",
    "end": "1001079"
  },
  {
    "text": "mentioned um modern gpus in particular a100 h100 have lower Precision tensor",
    "start": "1001079",
    "end": "1008440"
  },
  {
    "text": "cores that simply offer more floating Point operations per second and so on an",
    "start": "1008440",
    "end": "1014079"
  },
  {
    "text": "h100 the fp8 and int8 tensor cores have two times the number of floating Point",
    "start": "1014079",
    "end": "1019720"
  },
  {
    "text": "operations per second that they're able to execute as compared to their fp16 counterparts and so if the activations",
    "start": "1019720",
    "end": "1026240"
  },
  {
    "text": "are quantized we're able to use these lower Precision tensor cores and increase the number of operations that",
    "start": "1026240",
    "end": "1032880"
  },
  {
    "text": "we're able to do per second so you can see how quantization can help both of",
    "start": "1032880",
    "end": "1038360"
  },
  {
    "text": "the phases of a gem kernel a reducing the amount of data that needs to move",
    "start": "1038360",
    "end": "1043600"
  },
  {
    "text": "through the memory bandwidth and then in addition increasing the number of flops that we're able to execute",
    "start": "1043600",
    "end": "1049840"
  },
  {
    "text": "now one thing Michael has kind of hinted at is um that there's two types of",
    "start": "1049840",
    "end": "1055039"
  },
  {
    "text": "quantization that we support in BLM uh one is weight only quantization which is",
    "start": "1055039",
    "end": "1060600"
  },
  {
    "text": "very very commonly used in the open source ecosystem um Simon gave me a stat",
    "start": "1060600",
    "end": "1066000"
  },
  {
    "text": "the other day when we looked at the VM usage stats which is something about 20% of total VM usage hours are running with",
    "start": "1066000",
    "end": "1074679"
  },
  {
    "text": "quantization enabled which is great however when I looked at the details of the stats most of the time is actually spent doing",
    "start": "1074679",
    "end": "1081159"
  },
  {
    "text": "weight only quantization which is not the right thing to do uh because the way that the",
    "start": "1081159",
    "end": "1087679"
  },
  {
    "text": "weight only quantization gems work is that we move the data from the hbm into",
    "start": "1087679",
    "end": "1094200"
  },
  {
    "text": "the SRAM compressed so we get take advantage of the reduced data movement but to actually do the computation we de",
    "start": "1094200",
    "end": "1101559"
  },
  {
    "text": "quantize the weights in the SRAM and then use the fp16 tensor course so we're",
    "start": "1101559",
    "end": "1106880"
  },
  {
    "text": "not actually taking advantage of the additional flops that are available on the GPU whereas with the weight and",
    "start": "1106880",
    "end": "1113760"
  },
  {
    "text": "activation quantization if we quantize both the weights and activations we're actually able to take advantage of these",
    "start": "1113760",
    "end": "1119559"
  },
  {
    "text": "lower Precision tensor cores which enables us to not only reduce the amount",
    "start": "1119559",
    "end": "1124919"
  },
  {
    "text": "of data flowing from hbm into SRAM but also allows us to Leverage The tensor",
    "start": "1124919",
    "end": "1130880"
  },
  {
    "text": "cores that have more floating Point operations per second and so we're able to improve both of the bottlenecks that",
    "start": "1130880",
    "end": "1138000"
  },
  {
    "text": "exist in a in a gem operation and so most of the ecosystem is doing weight",
    "start": "1138000",
    "end": "1143240"
  },
  {
    "text": "only quantization which I'm really happy about because we've contributed a lot of it and it's great but we really really",
    "start": "1143240",
    "end": "1149799"
  },
  {
    "text": "recommend that users take advantage of weight and activation quantization because of these fundamental reasons I",
    "start": "1149799",
    "end": "1156000"
  },
  {
    "text": "just described um and really why weight 8 A8 is specifically what I just spoke",
    "start": "1156000",
    "end": "1162679"
  },
  {
    "text": "about which is in an LM inference server we see both large m so that weight",
    "start": "1162679",
    "end": "1169720"
  },
  {
    "text": "Matrix uh that activation Matrix there's certain iterations in the batches that",
    "start": "1169720",
    "end": "1175200"
  },
  {
    "text": "will have large M which are compute bound and so most of the time when m is large will be spent doing flops",
    "start": "1175200",
    "end": "1181919"
  },
  {
    "text": "executing in the tensor course and then the low batch siiz decodes when m is small these are bandwidth bound",
    "start": "1181919",
    "end": "1188760"
  },
  {
    "text": "operations and so most of the time will be spent in this data movement from hbm",
    "start": "1188760",
    "end": "1194200"
  },
  {
    "text": "to SRAM and what we see when we do an analysis of you know a an actual llm",
    "start": "1194200",
    "end": "1200320"
  },
  {
    "text": "inference serving workload we see that you get a mix of really really low batch",
    "start": "1200320",
    "end": "1205640"
  },
  {
    "text": "sizes you know in in this chart I have on the right hand side uh we can see there's a bunch of of batch iterations",
    "start": "1205640",
    "end": "1212240"
  },
  {
    "text": "where we have you know one to five tokens that we're processing but we also have a large number of iterations where",
    "start": "1212240",
    "end": "1219520"
  },
  {
    "text": "we get high batch size High m uh which means we need to be able to accelerate",
    "start": "1219520",
    "end": "1224880"
  },
  {
    "text": "both this memory bound regime and this compute bound regime and this is exactly what activation quantization does as I",
    "start": "1224880",
    "end": "1231320"
  },
  {
    "text": "sort of spoke about before and so to give you a little bit of real world data on this um I wanted",
    "start": "1231320",
    "end": "1237559"
  },
  {
    "text": "to talk about some of the key metrics that we use to analyze the performance of an LM inference server there's really",
    "start": "1237559",
    "end": "1243559"
  },
  {
    "text": "two key variables to consider the first is time to First token which is the latency to generate the first token in",
    "start": "1243559",
    "end": "1250559"
  },
  {
    "text": "the batch uh of a request and time per output token which is the inner token latency effectively how long it takes to",
    "start": "1250559",
    "end": "1257520"
  },
  {
    "text": "generate the next token and typically we measure these variables as a function of",
    "start": "1257520",
    "end": "1262640"
  },
  {
    "text": "queries per second the amount of load on the server and these charts here demonstrate",
    "start": "1262640",
    "end": "1269400"
  },
  {
    "text": "how the various quantization schemes respond to load um so the leftand side",
    "start": "1269400",
    "end": "1276000"
  },
  {
    "text": "chart shows the time per output token of VM running llama 3.1 70b on four h100s",
    "start": "1276000",
    "end": "1285120"
  },
  {
    "text": "uh with 550 input tokens and 100 output tokens per request and what we see is on",
    "start": "1285120",
    "end": "1291120"
  },
  {
    "text": "the x-axis is the number of queries per second that the uh server is being hit",
    "start": "1291120",
    "end": "1296159"
  },
  {
    "text": "with and the y- AIS shows the mean time per output token uh per request and what",
    "start": "1296159",
    "end": "1302440"
  },
  {
    "text": "we see is at these really really low batch SI at these really low QPS ranges the mixed Precision the weit 4 a16 the",
    "start": "1302440",
    "end": "1310120"
  },
  {
    "text": "Blue Line gets the best latency when we don't have too much load on the server the mix Precision does really really",
    "start": "1310120",
    "end": "1316120"
  },
  {
    "text": "well it gives us the optimal latency and the reason for this is at low QPS we're spending a lot more time in this low",
    "start": "1316120",
    "end": "1323679"
  },
  {
    "text": "batch M decode phase um and we're not having as much time in the High m i mean",
    "start": "1323679",
    "end": "1329159"
  },
  {
    "text": "the high M uh uh pre-fill uh phase but as the amount of load on the server",
    "start": "1329159",
    "end": "1334279"
  },
  {
    "text": "increases and the queries per second that the server is uh facing goes up we can see that the latency of the weight 4",
    "start": "1334279",
    "end": "1342039"
  },
  {
    "text": "a16 model shoots up because we're not able to accelerate this compute bound regime and actually we have a per",
    "start": "1342039",
    "end": "1348520"
  },
  {
    "text": "performance penalty associated with the dequantization but the red line which is",
    "start": "1348520",
    "end": "1353679"
  },
  {
    "text": "activation quantization we can see is faster than the fp16 inference across",
    "start": "1353679",
    "end": "1359840"
  },
  {
    "text": "all of the batch across all of the QPS regimes and enables us to serve more",
    "start": "1359840",
    "end": "1365120"
  },
  {
    "text": "users on the same uh GPU infrastructure and as a rule of thumb for a 70 billion",
    "start": "1365120",
    "end": "1370960"
  },
  {
    "text": "parameter model typically what we see is the same overall time per output token",
    "start": "1370960",
    "end": "1376000"
  },
  {
    "text": "when running the fp8 model on 2G gpus as compared to running the fp16 model on",
    "start": "1376000",
    "end": "1382000"
  },
  {
    "text": "four gpus and so using activation quantization we can dramatically shrink",
    "start": "1382000",
    "end": "1387520"
  },
  {
    "text": "the overall amount of resources that we need to execute uh the",
    "start": "1387520",
    "end": "1393080"
  },
  {
    "text": "model so I hope you all understand activation quantization is where it's at int 8 for ampere fp8 for Hopper um these",
    "start": "1393080",
    "end": "1401400"
  },
  {
    "text": "are really the things that you want to use if you're running an inference server and care about reducing the",
    "start": "1401400",
    "end": "1406559"
  },
  {
    "text": "overall cost of the deployment but what about accuracy how does quantization impact the accuracy of the",
    "start": "1406559",
    "end": "1413520"
  },
  {
    "text": "models especially in this activation quantization regime that's so good for llm inference serving and offline use",
    "start": "1413520",
    "end": "1420000"
  },
  {
    "text": "cases and there's a an element in the open source ecosystem that quantize models are less good than their fb16",
    "start": "1420000",
    "end": "1427640"
  },
  {
    "text": "counterparts and I'm here to say that this is not true and importantly as",
    "start": "1427640",
    "end": "1433559"
  },
  {
    "text": "Michael mentioned one of the things we've worked really hard on in VM is to enable fine grain quantization to be",
    "start": "1433559",
    "end": "1440000"
  },
  {
    "text": "able to have accurate model checkpoints so in particular uh one of the the ways",
    "start": "1440000",
    "end": "1446559"
  },
  {
    "text": "we can have fine grain quantization is to use more scales uh basically um when",
    "start": "1446559",
    "end": "1452960"
  },
  {
    "text": "we're quantizing a weight effectively what we do is multiply The quantized Weight by a scale to convert back and",
    "start": "1452960",
    "end": "1459200"
  },
  {
    "text": "forth between the unan de quantized version and the quantise version of the weights and a simple obvious strategy is",
    "start": "1459200",
    "end": "1466840"
  },
  {
    "text": "to use one scale per weight in one scale for the uh activation Matrix this is",
    "start": "1466840",
    "end": "1473080"
  },
  {
    "text": "called per tensor quantization and is not very fine grain at all and as you",
    "start": "1473080",
    "end": "1478559"
  },
  {
    "text": "might expect it's hard to represent the full range and handle all the outliers",
    "start": "1478559",
    "end": "1483640"
  },
  {
    "text": "especially on the activation Matrix uh when we're dealing with per tensor quantization but P torch scaled mm only",
    "start": "1483640",
    "end": "1491760"
  },
  {
    "text": "supports per tensor quantization and you have to U in a fused uh way and so",
    "start": "1491760",
    "end": "1497600"
  },
  {
    "text": "you'll face a sign ific performance overhead if you try to use more uh fine",
    "start": "1497600",
    "end": "1503279"
  },
  {
    "text": "grain quantization um without our specific kernels so one of the things that we enabled in VM with our custom",
    "start": "1503279",
    "end": "1510960"
  },
  {
    "text": "cutless kernels is to have performant fine grain quantization and specifically",
    "start": "1510960",
    "end": "1516880"
  },
  {
    "text": "what we enable is channelwise scales so we can do one scale per row of the",
    "start": "1516880",
    "end": "1523000"
  },
  {
    "text": "weight Matrix and per token uh Activation quantization so we have one scale",
    "start": "1523000",
    "end": "1528840"
  },
  {
    "text": "for each individual token in a sequence and this enables much finer grain",
    "start": "1528840",
    "end": "1533880"
  },
  {
    "text": "quantization we're able to handle outliers much better and still have really really performant inference",
    "start": "1533880",
    "end": "1540399"
  },
  {
    "text": "because of the custom kernels that we've enabled that are targeted at this specific um case and so when we look at",
    "start": "1540399",
    "end": "1548559"
  },
  {
    "text": "evaluations um you know at at neural magic we have built um you know uh FP",
    "start": "1548559",
    "end": "1555279"
  },
  {
    "text": "our our model repository that Michael was talking about and our int8 and fp8",
    "start": "1555279",
    "end": "1560840"
  },
  {
    "text": "uh activation quantization um models we evaluate in in two ways the first is looking at the",
    "start": "1560840",
    "end": "1567279"
  },
  {
    "text": "open llm leaderboard which is a really simple evaluation that's very quick things like mlu GSM Etc um which are",
    "start": "1567279",
    "end": "1576039"
  },
  {
    "text": "which are fine but they're not particularly challenging tasks and so recently we've been a lot more focused",
    "start": "1576039",
    "end": "1583159"
  },
  {
    "text": "especially with some of the push back uh from the open source community on the Llama 405b fp8 model",
    "start": "1583159",
    "end": "1588799"
  },
  {
    "text": "to pick the hardest possible evaluations in in particular we've started to standardize on Arena hard Auto um Arena",
    "start": "1588799",
    "end": "1596960"
  },
  {
    "text": "hard and and chapot arena is kind of the gold standard for instruction models uh",
    "start": "1596960",
    "end": "1602200"
  },
  {
    "text": "in the open source ecosystem and Arena hard is the hardest possible tasks um",
    "start": "1602200",
    "end": "1607799"
  },
  {
    "text": "from those uh the the chapot arena and with auto there's about 500 prompts uh",
    "start": "1607799",
    "end": "1613799"
  },
  {
    "text": "that are automatically evaluated by um uh gbd4 and uh what we can see is that",
    "start": "1613799",
    "end": "1619760"
  },
  {
    "text": "the fp8 models get almost the exact same scores as the bf16 models and the int8",
    "start": "1619760",
    "end": "1626360"
  },
  {
    "text": "models are very very close uh to their uh bf16 counterparts and so we",
    "start": "1626360",
    "end": "1632640"
  },
  {
    "text": "definitely um have been really focused on these more challenging evals and have",
    "start": "1632640",
    "end": "1637880"
  },
  {
    "text": "been seeing that with our fine grain quantization we're able to keep the accuracy very very high um of these",
    "start": "1637880",
    "end": "1644960"
  },
  {
    "text": "models and really believe that this is the right way that everyone should be running inference um um and you can use",
    "start": "1644960",
    "end": "1652399"
  },
  {
    "text": "LM compressor uh to do it I'm going to skip through uh these because we're running a bit low on time um please",
    "start": "1652399",
    "end": "1658320"
  },
  {
    "text": "check out LM compressor on the VM GitHub for some examples of how to create uh",
    "start": "1658320",
    "end": "1663480"
  },
  {
    "text": "fp8 uh in intap models for your use case but I wanted to hit a little bit on our on our road map um in terms of where",
    "start": "1663480",
    "end": "1669600"
  },
  {
    "text": "we're going uh obviously we've gotten the fp8 and int8 support in vlm into a",
    "start": "1669600",
    "end": "1674679"
  },
  {
    "text": "really good place but in terms of new areas that were exploring uh two4",
    "start": "1674679",
    "end": "1680120"
  },
  {
    "text": "sparcity is another operation that we're really really excited about uh as the",
    "start": "1680120",
    "end": "1685799"
  },
  {
    "text": "next Frontier of optimization in VM just like uh the lower bit tensor",
    "start": "1685799",
    "end": "1691679"
  },
  {
    "text": "cores that we talked about uh Nvidia gpus and AMD gpus uh have two four",
    "start": "1691679",
    "end": "1698080"
  },
  {
    "text": "sparse tensor cores uh which means two out of every four weights are zero that offer a 2X compute speed up over the fp8",
    "start": "1698080",
    "end": "1706480"
  },
  {
    "text": "dense tensor course and so so we at neural magic have been working on creating two for sparse Foundation",
    "start": "1706480",
    "end": "1712679"
  },
  {
    "text": "models where we prune um you know llama in with this two4 sparse format which",
    "start": "1712679",
    "end": "1718360"
  },
  {
    "text": "should offer us additional speedups that accelerate both the you know low M case",
    "start": "1718360",
    "end": "1723960"
  },
  {
    "text": "the memory bound case and the compute bound case as our next wave of uh",
    "start": "1723960",
    "end": "1729200"
  },
  {
    "text": "optimization we're working on combining four bit uh weights with 8 bit",
    "start": "1729200",
    "end": "1734559"
  },
  {
    "text": "activations through our machete kernels and another area that we're very excited",
    "start": "1734559",
    "end": "1739760"
  },
  {
    "text": "about is the next generation of Nvidia gpus Blackwell will have fp4 and fp6",
    "start": "1739760",
    "end": "1746760"
  },
  {
    "text": "tensor course um the fp4 tensor course were offer 2x the flops of the fp8 tensor course and so we're very excited",
    "start": "1746760",
    "end": "1754080"
  },
  {
    "text": "to work on this uh for the next generation of uh um Nvidia gpus which",
    "start": "1754080",
    "end": "1759440"
  },
  {
    "text": "will allow even more uh optimization it will definitely be a challenge to quantize the activations especially to",
    "start": "1759440",
    "end": "1765640"
  },
  {
    "text": "fp4 but it's something that we're very excited to work on both from a research perspective um in terms of developing",
    "start": "1765640",
    "end": "1772399"
  },
  {
    "text": "the quantization methods and in addition from the kernel perspective uh for um getting the performance acceleration so",
    "start": "1772399",
    "end": "1779399"
  },
  {
    "text": "thank you all very much I know this was a very technical topic um but uh we're really excited about uh quantization in",
    "start": "1779399",
    "end": "1785960"
  },
  {
    "text": "VM and please feel free to chat with us after or uh reach out to us in the LM",
    "start": "1785960",
    "end": "1792080"
  },
  {
    "text": "compressor uh repository if you have any questions uh for for how to you know",
    "start": "1792080",
    "end": "1797159"
  },
  {
    "text": "take advantage of these optim optimizations thank",
    "start": "1797159",
    "end": "1801240"
  },
  {
    "text": "you all right so we should have time for some questions and please raise your",
    "start": "1804279",
    "end": "1810799"
  },
  {
    "text": "hand um I'll go down here first and then",
    "start": "1810799",
    "end": "1815480"
  },
  {
    "text": "second yeah I have a question so like um you mentioned that like you op you",
    "start": "1815960",
    "end": "1821760"
  },
  {
    "text": "quantize the linear linear layer but from my experience like when you quantize the linear layer you introduce",
    "start": "1821760",
    "end": "1829039"
  },
  {
    "text": "Quant and D Quant like uh sometimes like I find that the engine actually slows",
    "start": "1829039",
    "end": "1834840"
  },
  {
    "text": "down by introduce quantizations uh how do you deal with that or you kind of like fuse a like",
    "start": "1834840",
    "end": "1841840"
  },
  {
    "text": "linear layer with the activations after that so reduce the amount of overhead",
    "start": "1841840",
    "end": "1847679"
  },
  {
    "text": "introduced by Quant dant exactly so the kernels that we for for this is for so",
    "start": "1847679",
    "end": "1853960"
  },
  {
    "text": "you're talking about with the uh with the weight only quantity ation so so so",
    "start": "1853960",
    "end": "1859320"
  },
  {
    "text": "two things two things here um we talked a little bit about the different kernels that we have implemented in VM so the",
    "start": "1859320",
    "end": "1866880"
  },
  {
    "text": "center chart here is looking at the mix Precision so this is the weight only quantization where basically the weights",
    "start": "1866880",
    "end": "1873799"
  },
  {
    "text": "are quantized but the activations are not quantized so to do the multiplication what we have to do is um",
    "start": "1873799",
    "end": "1880960"
  },
  {
    "text": "upcon convert the weights to fp16 and then pass it into the fp16 tensor course",
    "start": "1880960",
    "end": "1886399"
  },
  {
    "text": "so what Marlin this kernel does is it moves the weight from SRAM into Damm",
    "start": "1886399",
    "end": "1893279"
  },
  {
    "text": "compressed so we're reducing the amount of data that has to move from the you",
    "start": "1893279",
    "end": "1898799"
  },
  {
    "text": "know over the from hbm into the into the SRAM and then we're",
    "start": "1898799",
    "end": "1904519"
  },
  {
    "text": "upconverting on chip and then running the fp16 tensor course so the kernels",
    "start": "1904519",
    "end": "1910519"
  },
  {
    "text": "are handling the overhead associated with de quantizing and we're doing this very efficiently in a pipelined way in",
    "start": "1910519",
    "end": "1918000"
  },
  {
    "text": "side of the kernel and there's a lot of care and Magic associated with uh making",
    "start": "1918000",
    "end": "1923840"
  },
  {
    "text": "that happen really really fast so for the for the mix Precision that's what",
    "start": "1923840",
    "end": "1929240"
  },
  {
    "text": "these kernels do is make the is hide the overhead of De quantizing by doing it on",
    "start": "1929240",
    "end": "1935519"
  },
  {
    "text": "chip and um ultimately this works out pretty well for low batch size but as",
    "start": "1935519",
    "end": "1940720"
  },
  {
    "text": "you can see from these charts like the overhead associated with de quantizing",
    "start": "1940720",
    "end": "1945760"
  },
  {
    "text": "like makes the performance worse for weight 4 a16 you know when we get to these higher QPS ranges because the",
    "start": "1945760",
    "end": "1952360"
  },
  {
    "text": "overhead associating with de quantizing the weights is really really high when we start having longer prefills um so",
    "start": "1952360",
    "end": "1960120"
  },
  {
    "text": "that's what I would say about the mix Precision for the activation quantization we have to quantize the",
    "start": "1960120",
    "end": "1966320"
  },
  {
    "text": "activations dynamically um so basically the uh I think it's the green here the",
    "start": "1966320",
    "end": "1973600"
  },
  {
    "text": "Green in this chart basically the Green in this chart I think the green um no no",
    "start": "1973600",
    "end": "1980200"
  },
  {
    "text": "no it's not it's the it's the yellow the yellow in this chart is the overhead of",
    "start": "1980200",
    "end": "1985279"
  },
  {
    "text": "associated with uh uh quantizing the activations because like in each layer",
    "start": "1985279",
    "end": "1990639"
  },
  {
    "text": "we're going to uh take the inputs in fp16 quantize them to int 8 or or to fp8",
    "start": "1990639",
    "end": "1996919"
  },
  {
    "text": "and then do the fp8 gem so there's some overhead associated with quantizing and de quantizing the activations and what",
    "start": "1996919",
    "end": "2003240"
  },
  {
    "text": "we're doing to get rid of the yellow is we're integrating",
    "start": "2003240",
    "end": "2008320"
  },
  {
    "text": "torch. compile and we're building a what's called a graph level optimization system through torch. compile which",
    "start": "2008320",
    "end": "2015360"
  },
  {
    "text": "enables us to rewrite the graphs and uh actually fuse uh those Quant and D Quant",
    "start": "2015360",
    "end": "2023240"
  },
  {
    "text": "operations onto other operations U that exist inside of the model so this is not",
    "start": "2023240",
    "end": "2028679"
  },
  {
    "text": "yet complete and has been a very long-term project associated with integrating torch. compile into VM but",
    "start": "2028679",
    "end": "2035360"
  },
  {
    "text": "that's a forward-looking optimization that we're we actually just got the first pass to do this uh working uh",
    "start": "2035360",
    "end": "2041880"
  },
  {
    "text": "earlier today but we're going to be working on putting this into production in VM over the course of uh Q4 to",
    "start": "2041880",
    "end": "2047919"
  },
  {
    "text": "eliminate the overhead eliminate more of the overheads on activation quantization which are these uh yellow uh um lines",
    "start": "2047919",
    "end": "2056240"
  },
  {
    "text": "which I guess my chart is gone but uh yeah the yellow the yellow part of those bars",
    "start": "2056240",
    "end": "2062480"
  },
  {
    "text": "do like do you let it like do automatically with compile like are you",
    "start": "2067960",
    "end": "2074480"
  },
  {
    "text": "saying you are able to do those comp no no it's with custom Kel",
    "start": "2074480",
    "end": "2082839"
  },
  {
    "text": "how about let's talk about this offline so we can uh go into more detail yeah uh thank you for your presentation",
    "start": "2082839",
    "end": "2091158"
  },
  {
    "text": "uh I have one question do you have any recommendations for fine-tuning uh for quantied models uh do you have any",
    "start": "2091159",
    "end": "2098359"
  },
  {
    "text": "methods for this uh particular um uh quantisation and how F tuning should",
    "start": "2098359",
    "end": "2105640"
  },
  {
    "text": "looks like is it better to F tune model before quantization or after and um",
    "start": "2105640",
    "end": "2112079"
  },
  {
    "text": "what's uh second question sorry uh what data sets you use for quantization same",
    "start": "2112079",
    "end": "2118119"
  },
  {
    "text": "as a model was pre-trained on or um yeah any recommendations on this yeah",
    "start": "2118119",
    "end": "2125480"
  },
  {
    "text": "typically you're going to want to apply the Quan ation after you've trained the model and so you know in terms of our",
    "start": "2125480",
    "end": "2132480"
  },
  {
    "text": "stack yeah we're not going to be able to show oh yeah I can't show the slides uh in terms of our stack like neural magic",
    "start": "2132480",
    "end": "2138320"
  },
  {
    "text": "we create a model repository of quantized models like instruction aligned versions but the point in like",
    "start": "2138320",
    "end": "2145359"
  },
  {
    "text": "why we made LM compressor and why we open sourced LM compressor was to enable users who have fine-tuned their models",
    "start": "2145359",
    "end": "2152240"
  },
  {
    "text": "onto specific use cases to use our tools to create quantized versions of the",
    "start": "2152240",
    "end": "2157760"
  },
  {
    "text": "their um model so the quantization will happen after the um model has been",
    "start": "2157760",
    "end": "2163800"
  },
  {
    "text": "fine-tuned and you can use the LM compressor framework to apply the algorithms serialize the models and then",
    "start": "2163800",
    "end": "2169359"
  },
  {
    "text": "you know load them and run them in BLM The Only Exception to this is if you wanted to for instance take advantage of",
    "start": "2169359",
    "end": "2175640"
  },
  {
    "text": "multi- Laura deployments in BLM like let's say you trained like five or six different Laura adapters um on your",
    "start": "2175640",
    "end": "2182720"
  },
  {
    "text": "model um and then when you're ult and you wanted to take advantage of the feature in v where you're going to",
    "start": "2182720",
    "end": "2188599"
  },
  {
    "text": "deploy five or six different adapters on the same model in that case you might actually want to fine-tune the lore",
    "start": "2188599",
    "end": "2195200"
  },
  {
    "text": "adapters onto the quantise model backbone and then when you go to deploy you will obviously use the fp8 model you",
    "start": "2195200",
    "end": "2202680"
  },
  {
    "text": "know as your backbone and then attach the lur adapters on top of that um so uh",
    "start": "2202680",
    "end": "2208000"
  },
  {
    "text": "but that that's kind of what I would say about the fine-tuning um setup is like if you're just fine-tuning a model as",
    "start": "2208000",
    "end": "2213920"
  },
  {
    "text": "usual apply the quantization after if you're doing Laura based approaches where you're going to have multiple",
    "start": "2213920",
    "end": "2219520"
  },
  {
    "text": "adapters on the same model then you should fine-tune the low adapters on the quantized model um there are some other",
    "start": "2219520",
    "end": "2226680"
  },
  {
    "text": "methods called like quantization aware training that um you can use to um",
    "start": "2226680",
    "end": "2232079"
  },
  {
    "text": "recover a little bit more accuracy this is in kind of like a beta stage in LM compressor we haven't really found that",
    "start": "2232079",
    "end": "2238839"
  },
  {
    "text": "we've needed it um so far because the post training techniques as I kind of showed have been recovering pretty well",
    "start": "2238839",
    "end": "2244560"
  },
  {
    "text": "but if for some reason you did want to try to do qat this is something that uh",
    "start": "2244560",
    "end": "2249720"
  },
  {
    "text": "we have nominal support for in LM compressor and we're working on productionizing a bit more then in terms",
    "start": "2249720",
    "end": "2254839"
  },
  {
    "text": "of data set um for the instruction align models we have a few generic data sets",
    "start": "2254839",
    "end": "2260040"
  },
  {
    "text": "that we use to to calibrate in particular we use um uh we use uh and I",
    "start": "2260040",
    "end": "2265920"
  },
  {
    "text": "I should probably stop uh we use um you know an ultra chat data set but if you have a fine-tuned model um that's in a",
    "start": "2265920",
    "end": "2272599"
  },
  {
    "text": "specific domain you should definitely use that data set to calibrate uh the same data set you use to train uh to",
    "start": "2272599",
    "end": "2278400"
  },
  {
    "text": "calibrate uh and you need like 2,000 samples or so so it's not too uh too big but we can we can also talk more after",
    "start": "2278400",
    "end": "2285000"
  },
  {
    "text": "so thanks everybody okay thank",
    "start": "2285000",
    "end": "2290359"
  }
]