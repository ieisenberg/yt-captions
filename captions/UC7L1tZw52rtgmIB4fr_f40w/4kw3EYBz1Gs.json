[
  {
    "start": "0",
    "end": "41000"
  },
  {
    "text": "hey everyone thanks for coming out our talk today is how Spotify evolved its ml platform to power generative AI use",
    "start": "3439",
    "end": "10400"
  },
  {
    "text": "cases and we can start so quickly some introductions my name is Grace and along with my",
    "start": "10400",
    "end": "16358"
  },
  {
    "text": "colleague Abdullah we're both Engineers on one of spotify's machine learning platform",
    "start": "16359",
    "end": "21519"
  },
  {
    "text": "teams our agenda today will be pretty straightforward I'll cover an overview of Hendrix which is spotify's ml",
    "start": "21519",
    "end": "28080"
  },
  {
    "text": "platform I'll go into the platform changes we've made under the hood so to speak in the last year then Abdullah",
    "start": "28080",
    "end": "34559"
  },
  {
    "text": "will tell us about how we've made user facing changes so we can take advantage of those infrastructure",
    "start": "34559",
    "end": "41559"
  },
  {
    "start": "41000",
    "end": "93000"
  },
  {
    "text": "improvements so Hendrick spotify's ml platform began with its first iteration",
    "start": "41559",
    "end": "46879"
  },
  {
    "text": "in 2018 it was originally built um on top of Cu flow and gcp to support",
    "start": "46879",
    "end": "52399"
  },
  {
    "text": "tensorflow use cases and an opinionated path to production but by 2022 there was",
    "start": "52399",
    "end": "58359"
  },
  {
    "text": "a large gap in serving data science ssts and researchers who didn't use tinf flow or wanted other Frameworks so we built a",
    "start": "58359",
    "end": "65000"
  },
  {
    "text": "2.0 platform called Hendrix which leverages manage TR infra on top of uh",
    "start": "65000",
    "end": "70680"
  },
  {
    "text": "Google kubernetes engine also in gcp a few things to know is that it's centrally managed multi-tenant",
    "start": "70680",
    "end": "77119"
  },
  {
    "text": "infrastructure so many teams use our platform at the same time uh they primarily interact with our platform via",
    "start": "77119",
    "end": "83640"
  },
  {
    "text": "python SDK and CLI that we manage and otherwise for platform and business logic is mostly a couple going Based",
    "start": "83640",
    "end": "90479"
  },
  {
    "text": "Services and deployments behind the scenes just to illustrate how much hrx",
    "start": "90479",
    "end": "96280"
  },
  {
    "start": "93000",
    "end": "117000"
  },
  {
    "text": "has grown in the last year since the previous Ray Summit we've grown from 80 teams using Hendrix at Spotify to about",
    "start": "96280",
    "end": "102600"
  },
  {
    "text": "120 we've gone from 100 Ray clusters a week to 1,400 it's about a 10x increase",
    "start": "102600",
    "end": "109000"
  },
  {
    "text": "and we've increased our kubernetes footprint from one cluster with 500 nodes to three clusters with around",
    "start": "109000",
    "end": "114399"
  },
  {
    "text": "4,500 nodes so it's a lot of growth and we want to highlight a couple themes",
    "start": "114399",
    "end": "119560"
  },
  {
    "start": "117000",
    "end": "160000"
  },
  {
    "text": "throughout the talk about how we've um adapted our platform to meet those needs",
    "start": "119560",
    "end": "125039"
  },
  {
    "text": "number one has been reducing friction through standardized development practices we have a lot of knobs and levers our customers like to make use of",
    "start": "125039",
    "end": "132080"
  },
  {
    "text": "with their Ray clusters and we want to give them an intuitive way to do so secondly you'll hear how we provided",
    "start": "132080",
    "end": "137640"
  },
  {
    "text": "sufficient resource capabilities I.E sheer node capacity to meet those volume",
    "start": "137640",
    "end": "142760"
  },
  {
    "text": "increases thirdly um accommodating jni on a multi-end cluster so not just",
    "start": "142760",
    "end": "148200"
  },
  {
    "text": "enough resources but fair share in of resources and finally we'll touch on a few platform and reliability",
    "start": "148200",
    "end": "155040"
  },
  {
    "text": "improvements we've made with observability and redundancy as",
    "start": "155040",
    "end": "160239"
  },
  {
    "start": "160000",
    "end": "207000"
  },
  {
    "text": "well so here's a a succinct diagram of Hendrick's architecture obviously",
    "start": "160239",
    "end": "166280"
  },
  {
    "text": "there's a whole ml Ops ecosystem around this that's not being shown like we use ml flow at Spotify and a couple",
    "start": "166280",
    "end": "172480"
  },
  {
    "text": "different storage flavors um in Google Cloud but for the compute part this is really succinct it's a user who",
    "start": "172480",
    "end": "179840"
  },
  {
    "text": "interacts with our python CLI or SDK and from there on they submit a kubernetes",
    "start": "179840",
    "end": "185680"
  },
  {
    "text": "custom resource request for array cluster that goes um to our platform managed web hook more on that in a bit",
    "start": "185680",
    "end": "193519"
  },
  {
    "text": "but from there on the web hook passes the ray cluster request to CU which we simply deploy from upstream and that",
    "start": "193519",
    "end": "201239"
  },
  {
    "text": "builds or creates and manages the ray clusters from there",
    "start": "201239",
    "end": "206440"
  },
  {
    "text": "on so that was one year ago and now we're in a multicluster universe and I think it's worth talking about the",
    "start": "206879",
    "end": "212920"
  },
  {
    "start": "207000",
    "end": "301000"
  },
  {
    "text": "evolution and how we did it and why we did it why did we do it we were running out of node IPS to provision our raid",
    "start": "212920",
    "end": "220000"
  },
  {
    "text": "clusters upon with the adoption at Spotify we were running out of um IPS in",
    "start": "220000",
    "end": "225360"
  },
  {
    "text": "our subnets to provision Ray clusters with the kubernetes autoscaler and when this happens you get ray clusters that",
    "start": "225360",
    "end": "231560"
  },
  {
    "text": "hang indefinitely in your terminal or you get a weird deadline exceeded error and it's not a good experience so we",
    "start": "231560",
    "end": "236840"
  },
  {
    "text": "made the choice to provision a second 4,000 node cluster I think will be good for a while um and we managed to do that",
    "start": "236840",
    "end": "243959"
  },
  {
    "text": "quite easily from our side because we manage everything declaratively with terraform so it's just a matter of",
    "start": "243959",
    "end": "249480"
  },
  {
    "text": "rinting and repeating a couple of resources in terraform and updating the config we cared about from the user side",
    "start": "249480",
    "end": "257000"
  },
  {
    "text": "our multicluster experience has made a lot of Headway because we've chosen for now to use our SDK as the deao Gateway",
    "start": "257000",
    "end": "264040"
  },
  {
    "text": "between the two kubernetes clusters simply put if I'm at my terminal and I want a r cluster to muck around with",
    "start": "264040",
    "end": "270919"
  },
  {
    "text": "some Jupiter notebooks we default to the original kubernetes cluster if I want to submit AR Ray job we default and send",
    "start": "270919",
    "end": "277360"
  },
  {
    "text": "you to the larger sort of batch production kubernetes cluster if you like um finally we've instantiated a",
    "start": "277360",
    "end": "284759"
  },
  {
    "text": "test cluster because we have made a lot of changes to the platform we're always doing note upgrades Q operator upgrades",
    "start": "284759",
    "end": "291039"
  },
  {
    "text": "we're testing our custom web hook so this test clusters is very very useful for validating those changes before we",
    "start": "291039",
    "end": "297800"
  },
  {
    "text": "BR them out to users and could potentially an outage so it's great that we have enough",
    "start": "297800",
    "end": "303919"
  },
  {
    "start": "301000",
    "end": "361000"
  },
  {
    "text": "nodes but that's not necessarily an optimization I want to talk a little bit about how we've improved um the network",
    "start": "303919",
    "end": "310440"
  },
  {
    "text": "bandwidth layer with our uh platform so the problem is that when you have a",
    "start": "310440",
    "end": "316199"
  },
  {
    "text": "kubernetes cluster with hundreds or thousands of nodes you have red clusters represented here by these blue circles",
    "start": "316199",
    "end": "322759"
  },
  {
    "text": "that are spanning many many nodes um they encour slower Network bandwidths because the data simply has to travel",
    "start": "322759",
    "end": "328600"
  },
  {
    "text": "further and that's not a experience when you're doing distributed training direct pointing or the like so we worked with",
    "start": "328600",
    "end": "334919"
  },
  {
    "text": "our Google support team to implement a couple best practices I'll highlight here we did all this with terraform in",
    "start": "334919",
    "end": "341080"
  },
  {
    "text": "just a few lines and it includes compact placement so putting the nodes closer together in physical space Google",
    "start": "341080",
    "end": "347720"
  },
  {
    "text": "Virtual Nick for improved io on our virtual machines and nickel fast socket",
    "start": "347720",
    "end": "352840"
  },
  {
    "text": "which is an extra optimization layer on top of some of nvidia's common low-level",
    "start": "352840",
    "end": "357880"
  },
  {
    "text": "libraries um I want to move now from like the bare",
    "start": "357880",
    "end": "364880"
  },
  {
    "start": "361000",
    "end": "456000"
  },
  {
    "text": "metal if you will the platform to some of the platform and business logic we've worked on um as a platform we have to",
    "start": "364880",
    "end": "371240"
  },
  {
    "text": "maintain a couple pieces of logic that are specific to our platform what do I mean by that sometimes or often It's",
    "start": "371240",
    "end": "377599"
  },
  {
    "text": "usually the case where we are putting red clusters on distinct node pools in our kubernetes clusters or maybe we're",
    "start": "377599",
    "end": "384319"
  },
  {
    "text": "restricting um a specific kind of Hardware accelerator to certain name spaces so up until this year all that",
    "start": "384319",
    "end": "391160"
  },
  {
    "text": "logic lived in our client side python SDK um it's easy enough to develop in",
    "start": "391160",
    "end": "396360"
  },
  {
    "text": "but the problem is it's it's difficult for our users to constantly be upgrading especially if they encounter a transient",
    "start": "396360",
    "end": "402479"
  },
  {
    "text": "dependency like a pip package changed or something else that ruin their day it uh",
    "start": "402479",
    "end": "407639"
  },
  {
    "text": "doesn't always make sense for them to take on that toil for us um so we made the decision to migrate this logic to",
    "start": "407639",
    "end": "414080"
  },
  {
    "text": "our own kubernetes web hook we were able to do this pretty pretty easily we leveraged cub which made a lot of the",
    "start": "414080",
    "end": "420759"
  },
  {
    "text": "scaffolding for us and we just put our logic into the relevant parts of the web",
    "start": "420759",
    "end": "426360"
  },
  {
    "text": "hook um you can see here in this example the web hook merely takes the incoming Ray cluster request um with this yaml on",
    "start": "426360",
    "end": "434120"
  },
  {
    "text": "the left it's minimally um it's minimally formed it takes that yaml it",
    "start": "434120",
    "end": "439160"
  },
  {
    "text": "injects the annotations that we care about and it sends it onward on its way to the cube operator as normal this is",
    "start": "439160",
    "end": "446160"
  },
  {
    "text": "really good because now our Platformers can develop all the logic that they like and release it without the customers",
    "start": "446160",
    "end": "452240"
  },
  {
    "text": "even having to know that we've upgraded our our back end um one more note when it comes to uh",
    "start": "452240",
    "end": "460400"
  },
  {
    "start": "456000",
    "end": "550000"
  },
  {
    "text": "the platform logic that we've we've improved in the last year let's say that I've provisioned for gpus and my R",
    "start": "460400",
    "end": "468560"
  },
  {
    "text": "cluster uh starts spinning up but I can only get two gpus because the rest are allocated somewhere else well now I'm",
    "start": "468560",
    "end": "475440"
  },
  {
    "text": "holding on to two gpus without using them and I'm squandering you know those resour ources being cost inefficient",
    "start": "475440",
    "end": "481599"
  },
  {
    "text": "what have you we decided that uh some of these problems with GPU and resource",
    "start": "481599",
    "end": "486720"
  },
  {
    "text": "sharing started to look like Q problems so enter this very aptly named project called Q with a K uh we use it to",
    "start": "486720",
    "end": "494400"
  },
  {
    "text": "basically uh instantiate a queue in front of our uh batch workload cluster",
    "start": "494400",
    "end": "499680"
  },
  {
    "text": "and when folks go to submit their jobs to the cluster they go to a local queue for their namespace and then that all",
    "start": "499680",
    "end": "506120"
  },
  {
    "text": "gets merged into a global cluster queue which then does gang resource scheduling",
    "start": "506120",
    "end": "511240"
  },
  {
    "text": "the really nice thing about gang resource scheduling it's all or nothing so instead of waiting half with a halfway provision R cluster my job is",
    "start": "511240",
    "end": "518080"
  },
  {
    "text": "ready once all four gpus are ready and I can just go on about with my day it",
    "start": "518080",
    "end": "523599"
  },
  {
    "text": "works really well for us because it's a natural um solution to this problem and",
    "start": "523599",
    "end": "529000"
  },
  {
    "text": "also it has a really great integration with the ray job custom resource all we have to do after we install it on our",
    "start": "529000",
    "end": "534880"
  },
  {
    "text": "cluster is add a few annotations to our Ray job CR and we're up and running",
    "start": "534880",
    "end": "540000"
  },
  {
    "text": "it's a bit of an implementation detail but worth mentioning if you're if you use Google that we use Google's Dynamic",
    "start": "540000",
    "end": "545160"
  },
  {
    "text": "workload scheduling to take care of the actual provisioning of the gpus",
    "start": "545160",
    "end": "550399"
  },
  {
    "text": "themselves here's a bit of Q and DWS in action as you can see we have a name",
    "start": "550399",
    "end": "556040"
  },
  {
    "text": "space uh or a couple different name spaces one per team typically um the job",
    "start": "556040",
    "end": "561440"
  },
  {
    "text": "goes through a workload which loads itself onto the local queue and uh those local cues all feed",
    "start": "561440",
    "end": "568040"
  },
  {
    "text": "into the global cluster cu on our kubernetes cluster and once your job um",
    "start": "568040",
    "end": "573440"
  },
  {
    "text": "is ready to go the ray job is provisioned and you can interact with it as normal via our SD or",
    "start": "573440",
    "end": "580880"
  },
  {
    "start": "581000",
    "end": "663000"
  },
  {
    "text": "CLI so one more thing I want to mention in terms of um the user experience and",
    "start": "581640",
    "end": "588279"
  },
  {
    "text": "how we've made all these changes to our platform um we try to keep the user experience in mind as we roll out all",
    "start": "588279",
    "end": "594519"
  },
  {
    "text": "these different changes and something to make note of is is three slos we've implemented this past year one is the",
    "start": "594519",
    "end": "601440"
  },
  {
    "text": "percentage of kubernetes clusters that are available for rid cluster workloads as we mentioned um when we run out of",
    "start": "601440",
    "end": "608519"
  },
  {
    "text": "nodes on our kubernetes cluster we no longer can provision rid clusters so we definitely want to keep an eye on that",
    "start": "608519",
    "end": "614440"
  },
  {
    "text": "that's a part of what this diagram is showing when we hit our Plateau the outage begins in effect um secondly we",
    "start": "614440",
    "end": "622800"
  },
  {
    "text": "would we we monitor the percentage of R clusters in a ready state for a given window of time if suddenly 80% of our r",
    "start": "622800",
    "end": "629120"
  },
  {
    "text": "clust clusters or less are not ready we know we've regressed in some way and",
    "start": "629120",
    "end": "634360"
  },
  {
    "text": "finally and this is upcoming for us we are planning to operationalize the time",
    "start": "634360",
    "end": "639440"
  },
  {
    "text": "it takes for a rid cluster to become ready in the first place a couple of our Engineers made this",
    "start": "639440",
    "end": "644839"
  },
  {
    "text": "contribution um earlier this year at to Cub and we plan to track this metric so",
    "start": "644839",
    "end": "650279"
  },
  {
    "text": "we can monitor if it suddenly is taking 10 minutes for Ray cluster to become ready so this all helps us make sure",
    "start": "650279",
    "end": "656480"
  },
  {
    "text": "that we can roll out these improvements without actually um worsening the experience for our users",
    "start": "656480",
    "end": "663200"
  },
  {
    "start": "663000",
    "end": "738000"
  },
  {
    "text": "inadvertently with that being said I'll hand it over to Abdullah and he'll take you through our user facing",
    "start": "663200",
    "end": "669639"
  },
  {
    "text": "[Applause] changes thank you Grace um so for the",
    "start": "670640",
    "end": "678040"
  },
  {
    "text": "first half Grace talked about the improvements we made at at the infrastructure level that were absolutely essential to enable the use",
    "start": "678040",
    "end": "684600"
  },
  {
    "text": "cases that as saw emerging through gen the second half we will be talking about how we take those improvements and",
    "start": "684600",
    "end": "690519"
  },
  {
    "text": "provide it to our users um and I would just like to emphasize on one thing so yesterday had the keynote Robert",
    "start": "690519",
    "end": "696000"
  },
  {
    "text": "mentioned about that complexity is the enemy in this era we we saw that live on",
    "start": "696000",
    "end": "701639"
  },
  {
    "text": "our platform a lot of tools that we introduced uh our effort on the user side was to lower the complexity and you",
    "start": "701639",
    "end": "708320"
  },
  {
    "text": "and meet users where they are in their ml Journey so a power user could uh interact with our platform at a low",
    "start": "708320",
    "end": "714200"
  },
  {
    "text": "level but we should allow or enable our users to not think too much about the",
    "start": "714200",
    "end": "719399"
  },
  {
    "text": "infrastructure level concerns and spend most of the time on the actual workflow",
    "start": "719399",
    "end": "724560"
  },
  {
    "text": "that they're building so the second half we second half we will talk about U this",
    "start": "724560",
    "end": "730079"
  },
  {
    "text": "little piece at the center which is our interfaces um that we provide to our users to interact with our",
    "start": "730079",
    "end": "736600"
  },
  {
    "text": "infrastructure um so we will talk about some features um here we show you just a",
    "start": "736600",
    "end": "743040"
  },
  {
    "start": "738000",
    "end": "763000"
  },
  {
    "text": "typical J Dev life cycle that we were able to um identify based on the the",
    "start": "743040",
    "end": "748399"
  },
  {
    "text": "user Journeys we the ones highlighted in green are the ones that we will be talking or the features that we will talk about",
    "start": "748399",
    "end": "754199"
  },
  {
    "text": "correspond to these bubbles so uh it's okay if you don't pay too much attention",
    "start": "754199",
    "end": "759240"
  },
  {
    "text": "to it CU we will be coming back to it uh with each feature that we mention so just to kickoff um how does",
    "start": "759240",
    "end": "766639"
  },
  {
    "start": "763000",
    "end": "820000"
  },
  {
    "text": "the whole how does a user who is just getting started with Hendrix how do they interact with our platform what's the",
    "start": "766639",
    "end": "772240"
  },
  {
    "text": "first entry point um Spotify built a tool called backstage which is also open",
    "start": "772240",
    "end": "777800"
  },
  {
    "text": "source right now so which means any one of you interested in it can take a look at it use it it's a software catalog",
    "start": "777800",
    "end": "784120"
  },
  {
    "text": "what we do internally is we provide a component which we call Hendrix meml component and the motivation really is",
    "start": "784120",
    "end": "790000"
  },
  {
    "text": "to bring all the golden practices or a golden path if you may in one place so",
    "start": "790000",
    "end": "795920"
  },
  {
    "text": "if you're a user you create a component on Backstage and with it you get a software development kit you get the",
    "start": "795920",
    "end": "801920"
  },
  {
    "text": "libraries that are essential to interact with Hendrix you get the infrastructure that is needed to run your workflow both",
    "start": "801920",
    "end": "809199"
  },
  {
    "text": "during the prototyping stage as well as the production stage so it really is our",
    "start": "809199",
    "end": "814320"
  },
  {
    "text": "effort to um consolidate the fragmented space that we operate",
    "start": "814320",
    "end": "819959"
  },
  {
    "text": "in and so moving on um before we talk about the feutures we would like to mention this um layer that we built",
    "start": "819959",
    "end": "826760"
  },
  {
    "start": "820000",
    "end": "870000"
  },
  {
    "text": "which we call Hendrick's config layer again like I mentioned complexity is the enemy our goal through this layer was to",
    "start": "826760",
    "end": "834440"
  },
  {
    "text": "provide users useful abstractions that they can interact with when it comes to",
    "start": "834440",
    "end": "839600"
  },
  {
    "text": "uh creating a ray cluster submitting a job so how does a config lay work with",
    "start": "839600",
    "end": "845560"
  },
  {
    "text": "Hendrix a user simply so we provide like I said abstractions that users can maintain in their yaml files Hendrix",
    "start": "845560",
    "end": "853480"
  },
  {
    "text": "config uh what it does is it loads those yaml files does the validation and",
    "start": "853480",
    "end": "859160"
  },
  {
    "text": "produces a complete and valid lower level custom resource that could be Ray cluster that could be Ray job and then",
    "start": "859160",
    "end": "865360"
  },
  {
    "text": "applies it to our gke or kubernetes infrastructure",
    "start": "865360",
    "end": "870639"
  },
  {
    "start": "870000",
    "end": "907000"
  },
  {
    "text": "so the first concept that we tried abstracting is array cluster and the template that will follow for the",
    "start": "870639",
    "end": "875880"
  },
  {
    "text": "following slides as well is on the left you can see what a user has to maintain which is their yl file which is our",
    "start": "875880",
    "end": "882279"
  },
  {
    "text": "abstraction over a ray cluster custom resource uh top right is just a snippet",
    "start": "882279",
    "end": "888120"
  },
  {
    "text": "to show that our SDK which is Hendrick's config layer sits in between does the validation and applies um applies to",
    "start": "888120",
    "end": "895320"
  },
  {
    "text": "manifest that is then produced that you can see at the bottom right um so Ray cluster is one abstraction that we",
    "start": "895320",
    "end": "901839"
  },
  {
    "text": "provide using which users can create a ray cluster on our managed infrastructure the second abstraction",
    "start": "901839",
    "end": "908639"
  },
  {
    "start": "907000",
    "end": "963000"
  },
  {
    "text": "that we provide is Ray job which is again people familiar with Cube Ray and its custom resources Ray job is another",
    "start": "908639",
    "end": "914720"
  },
  {
    "text": "custom resource um similar pattern on the left side you can see a yaml file",
    "start": "914720",
    "end": "920480"
  },
  {
    "text": "that a user has to maintain um not as low level as the actual Ray job CR the",
    "start": "920480",
    "end": "926600"
  },
  {
    "text": "SDK then does validation and producers a complete and valid Uh custom resource",
    "start": "926600",
    "end": "932480"
  },
  {
    "text": "that could then be applied to our manage infrastructure it would then create a ray job which uh again uh Ray job",
    "start": "932480",
    "end": "938920"
  },
  {
    "text": "creates a ray cluster runs a job on it and then deletes the ray cluster so now what having talked about",
    "start": "938920",
    "end": "945560"
  },
  {
    "text": "those two abstractions that we provide and we do provide other as well but those two were critical to understand the features we'll be talking about with",
    "start": "945560",
    "end": "952440"
  },
  {
    "text": "that we can now get into the actual features that we were requested to build a Spotify um recently through gen use",
    "start": "952440",
    "end": "960120"
  },
  {
    "text": "cases uh and how we went about it the first one was actually motivated",
    "start": "960120",
    "end": "965360"
  },
  {
    "start": "963000",
    "end": "1034000"
  },
  {
    "text": "by uh a lot of teams who wanted to create a rate cluster that can handle heterogeneous workloads so imagine you",
    "start": "965360",
    "end": "971959"
  },
  {
    "text": "know a single workload that has different stages with different resource requirements some requiring some",
    "start": "971959",
    "end": "977880"
  },
  {
    "text": "performing a CPU intensive data processing while another stage in that workload uh performing performing a GPU",
    "start": "977880",
    "end": "984600"
  },
  {
    "text": "intensive training so initially when HRI started we only allowed one worker growth in a ray cluster and seeing this",
    "start": "984600",
    "end": "991279"
  },
  {
    "text": "problem or this feature request from our users we move towards providing them the ability to create multiple work grubs",
    "start": "991279",
    "end": "998720"
  },
  {
    "text": "again we tried lowering the complexity how it looks like is top left a yaml file that I showed you for R cluster",
    "start": "998720",
    "end": "1006160"
  },
  {
    "text": "they simply need to make a some uh adjustments to that yaml file and our",
    "start": "1006160",
    "end": "1011720"
  },
  {
    "text": "config layer would handle the validation and would handle actually populating um",
    "start": "1011720",
    "end": "1017639"
  },
  {
    "text": "a valid R cluster resource that can then create a ray cluster with multiple worker groups each with its own set of",
    "start": "1017639",
    "end": "1025038"
  },
  {
    "text": "resource requirements um and I hope the diagram helps we tried color coding to to make",
    "start": "1025039",
    "end": "1031319"
  },
  {
    "text": "the point more obvious um secondly similar idea there was uh a distinct use",
    "start": "1031319",
    "end": "1038000"
  },
  {
    "start": "1034000",
    "end": "1132000"
  },
  {
    "text": "case at Spotify where a team wanted to host a VM on a ray cluster and so what",
    "start": "1038000",
    "end": "1043600"
  },
  {
    "text": "they really wanted to do was create a ray cluster where you have a worker grub dedicated to a VM deployment and so to",
    "start": "1043600",
    "end": "1052720"
  },
  {
    "text": "the problem really there was how do you make sure that that deployment has dedicated resources from for it and",
    "start": "1052720",
    "end": "1058480"
  },
  {
    "text": "other processes are not interrupting or are not claiming the resources on that worker grrip and so what we did uh the",
    "start": "1058480",
    "end": "1065440"
  },
  {
    "text": "feature we enabled was the ability to create a separate worker group um which we call sidecar worker group again B the",
    "start": "1065440",
    "end": "1074000"
  },
  {
    "text": "user Journey seems very similar the user yam file they have to spe specify a",
    "start": "1074000",
    "end": "1079720"
  },
  {
    "text": "sidecar worker with uh so what happens in a side car container I should actually briefly explain the main",
    "start": "1079720",
    "end": "1085880"
  },
  {
    "text": "container does not have the mo most of the resources and there's a sidecar container where VM is deployed that",
    "start": "1085880",
    "end": "1092080"
  },
  {
    "text": "actually gets allocated the resources that the user request that way we can",
    "start": "1092080",
    "end": "1097600"
  },
  {
    "text": "ensure to some degree resource isolation and that um there no other process is",
    "start": "1097600",
    "end": "1103640"
  },
  {
    "text": "claiming those resources um what a user has to do is again a yl file uh adjust it a little",
    "start": "1103640",
    "end": "1109840"
  },
  {
    "text": "bit according to our guidelines uh the schema that we provide our SDK layer our config layer would then again do the",
    "start": "1109840",
    "end": "1116480"
  },
  {
    "text": "validation make sure it's valid produce a custom resource that can actually",
    "start": "1116480",
    "end": "1122840"
  },
  {
    "text": "Implement those changes and then submitted to our cluster where Cube would then create a ray cluster with a",
    "start": "1122840",
    "end": "1128799"
  },
  {
    "text": "sidecar worker group another solution that we um",
    "start": "1128799",
    "end": "1136520"
  },
  {
    "start": "1132000",
    "end": "1244000"
  },
  {
    "text": "another problem that we saw was more on the storage optimization so the problem here is",
    "start": "1136520",
    "end": "1143440"
  },
  {
    "text": "really just one example is large scale distributed training jobs often require some storage capability to ensure",
    "start": "1143440",
    "end": "1149520"
  },
  {
    "text": "reliability so imagine there's a workload which is a long running workload uh or rate cluster that is you",
    "start": "1149520",
    "end": "1155360"
  },
  {
    "text": "know that requires to live for a long period of time and you've already done some fine-tuning on it um for some",
    "start": "1155360",
    "end": "1162840"
  },
  {
    "text": "reason it could happen that the rate cluster goes down so uh to ensure that the work that you've already done so GPU",
    "start": "1162840",
    "end": "1170840"
  },
  {
    "text": "that you've utilized in that workload doesn't go to waste uh we saw a feature",
    "start": "1170840",
    "end": "1175919"
  },
  {
    "text": "request where users wanted uh some storage capability for their long running jobs we had another solution but",
    "start": "1175919",
    "end": "1183799"
  },
  {
    "text": "then recently we moved towards GCS fuse Google Cloud Storage fuse which essentially what it does is it allows",
    "start": "1183799",
    "end": "1189400"
  },
  {
    "text": "you to create a Google Cloud Storage bucket as local file system so we've seen a lot of teams at Spotify who use",
    "start": "1189400",
    "end": "1196240"
  },
  {
    "text": "it for uh distributed model checkpointing there's also some teams who use it for other data that they need",
    "start": "1196240",
    "end": "1201880"
  },
  {
    "text": "for a long running job uh but this has really made our life easy um and again if as a user you want to use it uh we",
    "start": "1201880",
    "end": "1210320"
  },
  {
    "text": "try lowering the complexity so you can see the changes are just required on the",
    "start": "1210320",
    "end": "1215440"
  },
  {
    "text": "yl level uh what bucket you want to use the SDK then does the validation produce",
    "start": "1215440",
    "end": "1221600"
  },
  {
    "text": "array cluster custom resource that can attach um a Google Cloud Storage bucket",
    "start": "1221600",
    "end": "1227600"
  },
  {
    "text": "as local file system with it um the diagram tries to help you visualize how",
    "start": "1227600",
    "end": "1232720"
  },
  {
    "text": "it looks like there's a ray cluster where head and worker notes have access to this single uh bucket or this GCS",
    "start": "1232720",
    "end": "1238799"
  },
  {
    "text": "bucket and can work with it like it's a local file",
    "start": "1238799",
    "end": "1243679"
  },
  {
    "start": "1244000",
    "end": "1350000"
  },
  {
    "text": "system so Grace already talked briefly about uh or talked about q and DWS on",
    "start": "1244600",
    "end": "1249960"
  },
  {
    "text": "the infrastructure side things that we had to implement or how we had to move towards q and DWS uh and the problem is",
    "start": "1249960",
    "end": "1257640"
  },
  {
    "text": "uh I hope it's visible but just to reiterate on the problem it is that we needed gang scheduling for expensive",
    "start": "1257640",
    "end": "1263240"
  },
  {
    "text": "limited Hardware accelerators like gpus where you have a lot more teams um fighting for resources and you need to",
    "start": "1263240",
    "end": "1270039"
  },
  {
    "text": "ensure that the resources are not underutilized not over-provisioned and",
    "start": "1270039",
    "end": "1275279"
  },
  {
    "text": "each team gets its fair share of usage um now there's a lot of complexity as you saw earlier a lot of stuff that we",
    "start": "1275279",
    "end": "1282440"
  },
  {
    "text": "had to implement under the hood to enable q& DWS but do we expose our users",
    "start": "1282440",
    "end": "1287520"
  },
  {
    "text": "to it of course not we try to load the complexity and just uh allow them to",
    "start": "1287520",
    "end": "1293679"
  },
  {
    "text": "with a single config change as you can see top left panel if they do want to use cute provisioning they enable that",
    "start": "1293679",
    "end": "1300960"
  },
  {
    "text": "option in the config file use our interface and our config layer would",
    "start": "1300960",
    "end": "1306880"
  },
  {
    "text": "then would validate and produce a valid rate job custom resource the abstraction",
    "start": "1306880",
    "end": "1311960"
  },
  {
    "text": "that I talked about earlier so there's Ray cluster abstraction and Ray job abstraction and by simply providing this",
    "start": "1311960",
    "end": "1318159"
  },
  {
    "text": "one examp example this one config option they could opt in and use Q provisioning",
    "start": "1318159",
    "end": "1323640"
  },
  {
    "text": "which means the ray job that they create would be QED and as soon as the resources required by it could be",
    "start": "1323640",
    "end": "1330080"
  },
  {
    "text": "provisioned that's when Q would admit it and DWS would work on GPU obtainability",
    "start": "1330080",
    "end": "1336200"
  },
  {
    "text": "um so that's how it looks like on the user side again just a config change and",
    "start": "1336200",
    "end": "1341240"
  },
  {
    "text": "they don't have to worry too much about the underlying resources like local local Q Global q and other stuff that is",
    "start": "1341240",
    "end": "1347960"
  },
  {
    "text": "needed to set it up and finally to connect all the dots",
    "start": "1347960",
    "end": "1353000"
  },
  {
    "start": "1350000",
    "end": "1521000"
  },
  {
    "text": "um so most of the features that we talked about could be used are used during the prototyping or ad hoc stages",
    "start": "1353000",
    "end": "1360440"
  },
  {
    "text": "but at the same time we do realize that our users need to once they're happy U so if you talk about that diagram that I",
    "start": "1360440",
    "end": "1366960"
  },
  {
    "text": "showed you earlier where different Dev different stages in a gen Dev life cycle",
    "start": "1366960",
    "end": "1372919"
  },
  {
    "text": "let's say the key be trading on some configuration on some solution that they have eventually they would want to",
    "start": "1372919",
    "end": "1378159"
  },
  {
    "text": "deploy it or they would want to run a production job and move out of the prototyping stage and actually have um a",
    "start": "1378159",
    "end": "1384559"
  },
  {
    "text": "more robust scheduling so we need to make sure that whatever solution we have for production for",
    "start": "1384559",
    "end": "1390080"
  },
  {
    "text": "scheduling it could leverage all the features that we talked about uh all the infrastructure improvements all the user",
    "start": "1390080",
    "end": "1396000"
  },
  {
    "text": "side features uh all the config changes that we listed for our scheduling uh we",
    "start": "1396000",
    "end": "1402640"
  },
  {
    "text": "use flight which is uh a workflow orchestrator we used to be on Luigi now things are moving to flight",
    "start": "1402640",
    "end": "1409279"
  },
  {
    "text": "our key Challenge on that side was our organizational design which is usually",
    "start": "1409279",
    "end": "1415080"
  },
  {
    "text": "data workflows uh in Spotify they run on a different GK cluster which for",
    "start": "1415080",
    "end": "1420279"
  },
  {
    "text": "Simplicity we can call it data GK cluster and within that workflow could be a task that needs to run as the ray",
    "start": "1420279",
    "end": "1427840"
  },
  {
    "text": "job which means it needs to create a ray cluster and run a job on it delete the cluster but that Ray cluster needs to be",
    "start": "1427840",
    "end": "1434760"
  },
  {
    "text": "uh created on the MLK cluster so we have two GK clusters data GK cluster ml GK",
    "start": "1434760",
    "end": "1440120"
  },
  {
    "text": "cluster the problem really was how do you align this organizational design and how do you enable a task in a different",
    "start": "1440120",
    "end": "1446880"
  },
  {
    "text": "or pod task in a different GK cluster to create a ray cluster run array job on a",
    "start": "1446880",
    "end": "1452480"
  },
  {
    "text": "different GK cluster quickly for that what we did we have our internal plugin there is an",
    "start": "1452480",
    "end": "1458159"
  },
  {
    "text": "open source solution uh we did some investigation um at that time our",
    "start": "1458159",
    "end": "1463720"
  },
  {
    "text": "internal solution was more appropriate what our internal plugin does is that it does exactly what we described earlier",
    "start": "1463720",
    "end": "1470720"
  },
  {
    "text": "creates a r job manifest uh optionally use cute provisioning and then run a job",
    "start": "1470720",
    "end": "1476919"
  },
  {
    "text": "on ML GK cluster you can see U there's a quick example of how it looks or how the",
    "start": "1476919",
    "end": "1483840"
  },
  {
    "text": "workflow looks like uh in flight so this example shows two tasks there's one",
    "start": "1483840",
    "end": "1488960"
  },
  {
    "text": "regular task which would run on the data GK cluster because it doesn't require any Ray infrastructure but if you have a",
    "start": "1488960",
    "end": "1495320"
  },
  {
    "text": "task that needs to run as a ray job all you need to do is you need to initialize a rate job config and then decorate that",
    "start": "1495320",
    "end": "1502840"
  },
  {
    "text": "task with that config what would happen on the flight side is flight has its own own operator it would look at the config",
    "start": "1502840",
    "end": "1510200"
  },
  {
    "text": "uh create the look at the task create a rate cluster run that task on that Ray",
    "start": "1510200",
    "end": "1516080"
  },
  {
    "text": "cluster and then delete that Ray cluster once the task has executed so that was really just of some",
    "start": "1516080",
    "end": "1523000"
  },
  {
    "start": "1521000",
    "end": "1816000"
  },
  {
    "text": "of the some of the user side changes that we made but I would like to emphasize again uh through all the Fe",
    "start": "1523000",
    "end": "1528320"
  },
  {
    "text": "feutures there's a common on the user side there's a common theme which is that we need to bring the complexity",
    "start": "1528320",
    "end": "1533799"
  },
  {
    "text": "down and really just enable user to move at a better velocity and with that I would call Grace back up on the stage to",
    "start": "1533799",
    "end": "1540559"
  },
  {
    "text": "wrap things up cool uh thanks abdulla yeah just to",
    "start": "1540559",
    "end": "1547240"
  },
  {
    "text": "wrap up quickly wanted to highlight some of the takeaways we hope that you have from this talk uh firstly to design an",
    "start": "1547240",
    "end": "1554760"
  },
  {
    "text": "interface expressive for users and extensible for maintainers as Abdullah demonstrated there's so many features we",
    "start": "1554760",
    "end": "1561000"
  },
  {
    "text": "gave to our um users with the SDK and we managed to do this in an extensible way",
    "start": "1561000",
    "end": "1567559"
  },
  {
    "text": "using our cluster and Ray job configs secondly ml platforms require",
    "start": "1567559",
    "end": "1572600"
  },
  {
    "text": "optimization throughout the stack but be strategic about the complexity of your optimizations we had network",
    "start": "1572600",
    "end": "1578640"
  },
  {
    "text": "optimizations uh resource and storage optimizations but choose a solution that makes sense for you and what you need",
    "start": "1578640",
    "end": "1585880"
  },
  {
    "text": "and lastly maintainability and velocity don't have to be trade-offs we strongly believe having those slos the test",
    "start": "1585880",
    "end": "1592440"
  },
  {
    "text": "cluster and working directly with our users helped us deliver a lot of these changes while keeping the user",
    "start": "1592440",
    "end": "1598159"
  },
  {
    "text": "experience seamless throughout so with that being said we' love to take your questions and we are hiring by the",
    "start": "1598159",
    "end": "1606880"
  },
  {
    "text": "[Applause]",
    "start": "1607600",
    "end": "1614399"
  },
  {
    "text": "way uh feel free to use the mic in the middle aisle if you",
    "start": "1614399",
    "end": "1620520"
  },
  {
    "text": "like hi uh the presentation was really good thank you I have a question so when you mentioned the sidecar model and",
    "start": "1625399",
    "end": "1633360"
  },
  {
    "text": "multiple worker group models do they get deployed in the same cluster or can they also be deployed uh across different uh",
    "start": "1633360",
    "end": "1642600"
  },
  {
    "text": "clusters so the question was to elaborate on the side car pattern that we talked about and the multiple work",
    "start": "1642600",
    "end": "1648399"
  },
  {
    "text": "grubs um so for multiple worker grubs each worker grub gets allocated to a",
    "start": "1648399",
    "end": "1653600"
  },
  {
    "text": "note pool separately so it they could be on uh there could be different note pools um for the side car container",
    "start": "1653600",
    "end": "1661039"
  },
  {
    "text": "pattern there is um so it when we built that we called it an experimental",
    "start": "1661039",
    "end": "1666240"
  },
  {
    "text": "feature because that was something that few teams wanted to use it wasn't something that we tested and we",
    "start": "1666240",
    "end": "1672039"
  },
  {
    "text": "validated but it's just one worker group that gets allocated to one note pool so",
    "start": "1672039",
    "end": "1677320"
  },
  {
    "text": "they could be on different note tools okay thank",
    "start": "1677320",
    "end": "1683080"
  },
  {
    "text": "you hi um so my question is also about the site car worker group is that the",
    "start": "1683440",
    "end": "1689200"
  },
  {
    "text": "worker group how the ray cluster defines worker group and if yes how do you make",
    "start": "1689200",
    "end": "1694640"
  },
  {
    "text": "sure those resources are not used by um any of jobs because I understand the ray",
    "start": "1694640",
    "end": "1701120"
  },
  {
    "text": "doesn't provide anti-affinity like you can say that this you know the resources",
    "start": "1701120",
    "end": "1707519"
  },
  {
    "text": "are preferred and this worker group has these resources but you cannot say that don't go to that worker group right yeah",
    "start": "1707519",
    "end": "1713799"
  },
  {
    "text": "so the question is how do we ensure resource isolation for a sidec car um",
    "start": "1713799",
    "end": "1718960"
  },
  {
    "text": "for the VM deployment on a sidecar container I guess the solution there was to if you don't if you use a separate",
    "start": "1718960",
    "end": "1725480"
  },
  {
    "text": "sidecar container within a worker group with its own image uh and allocate the",
    "start": "1725480",
    "end": "1730919"
  },
  {
    "text": "requested resources to that worker or to that container uh not to the main container but to the sidecar container",
    "start": "1730919",
    "end": "1736720"
  },
  {
    "text": "then that would provide us some resource isolation where other resources would run on the main container but uh won't",
    "start": "1736720",
    "end": "1743600"
  },
  {
    "text": "run on the sidecar container so by doing that we hope that that would provide in",
    "start": "1743600",
    "end": "1748840"
  },
  {
    "text": "isolation for VM deployment thank you thank",
    "start": "1748840",
    "end": "1754000"
  },
  {
    "text": "you hey uh I have a question about how you're using DWS and what your cost",
    "start": "1754000",
    "end": "1759320"
  },
  {
    "text": "optimization strategy is for that because our team uses it but we have a hard time understanding how to manage",
    "start": "1759320",
    "end": "1766399"
  },
  {
    "text": "costs related to it",
    "start": "1766399",
    "end": "1770039"
  },
  {
    "text": "um yeah some of the approaches we take to managing costs we just work directly with our Google support team um we try",
    "start": "1771760",
    "end": "1778880"
  },
  {
    "text": "to take their advice with optimizing uh costs a big one for us",
    "start": "1778880",
    "end": "1784080"
  },
  {
    "text": "recently has been logging costs so we've made the decision to move to making logging optional or at least highly",
    "start": "1784080",
    "end": "1789960"
  },
  {
    "text": "configurable for R clusters with the amount of clusters we procure um that drastically helped reduce our cost for",
    "start": "1789960",
    "end": "1795559"
  },
  {
    "text": "instance so yeah thanks",
    "start": "1795559",
    "end": "1800039"
  },
  {
    "text": "it seems like that was all the questions but if you do have questions uh Grace and I will be here for a while so please feel free to approach us and ask any",
    "start": "1804519",
    "end": "1811120"
  },
  {
    "text": "question that you have thank you",
    "start": "1811120",
    "end": "1815679"
  }
]