[
  {
    "text": "hey everyone my name is emog and I'm a",
    "start": "179",
    "end": "2100"
  },
  {
    "text": "software engineer here at any skill if",
    "start": "2100",
    "end": "4080"
  },
  {
    "text": "you've ever run into problems with",
    "start": "4080",
    "end": "5460"
  },
  {
    "text": "building llm applications and in",
    "start": "5460",
    "end": "7799"
  },
  {
    "text": "particular being able to effectively and",
    "start": "7799",
    "end": "9780"
  },
  {
    "text": "quickly embed large amounts of documents",
    "start": "9780",
    "end": "12059"
  },
  {
    "text": "in this talk we'll go over how you can",
    "start": "12059",
    "end": "14040"
  },
  {
    "text": "use array to easily paralyze this across",
    "start": "14040",
    "end": "16440"
  },
  {
    "text": "multiple gpus showing how you can embed",
    "start": "16440",
    "end": "18480"
  },
  {
    "text": "thousands of PDF documents in a matter",
    "start": "18480",
    "end": "20279"
  },
  {
    "text": "of minutes",
    "start": "20279",
    "end": "21240"
  },
  {
    "text": "this talk is a second part in our series",
    "start": "21240",
    "end": "23160"
  },
  {
    "text": "of Lang chain and Rey showing how you",
    "start": "23160",
    "end": "24660"
  },
  {
    "text": "can use both together to build LM based",
    "start": "24660",
    "end": "26699"
  },
  {
    "text": "applications in our first video while we",
    "start": "26699",
    "end": "29039"
  },
  {
    "text": "talked showed how you can use a blank",
    "start": "29039",
    "end": "31439"
  },
  {
    "text": "tune and Ray together to build a search",
    "start": "31439",
    "end": "33180"
  },
  {
    "text": "engine on the ray documentation so if",
    "start": "33180",
    "end": "35760"
  },
  {
    "text": "you recall there's two steps to building",
    "start": "35760",
    "end": "37620"
  },
  {
    "text": "this type of application the first is",
    "start": "37620",
    "end": "39600"
  },
  {
    "text": "that you want to generate embeddings for",
    "start": "39600",
    "end": "41700"
  },
  {
    "text": "a document Corpus that you have then",
    "start": "41700",
    "end": "44219"
  },
  {
    "text": "that you want your large language model",
    "start": "44219",
    "end": "45600"
  },
  {
    "text": "to extract information out of so you",
    "start": "45600",
    "end": "47579"
  },
  {
    "text": "generate these embeddings for these",
    "start": "47579",
    "end": "48780"
  },
  {
    "text": "documents and stored it into a vector",
    "start": "48780",
    "end": "50280"
  },
  {
    "text": "store this happens in an offline phase",
    "start": "50280",
    "end": "52140"
  },
  {
    "text": "then the second step is an online phase",
    "start": "52140",
    "end": "54239"
  },
  {
    "text": "where you actually deploy your",
    "start": "54239",
    "end": "55320"
  },
  {
    "text": "application that can accept incoming",
    "start": "55320",
    "end": "57600"
  },
  {
    "text": "your user queries such as questions",
    "start": "57600",
    "end": "59460"
  },
  {
    "text": "about your documents that user query is",
    "start": "59460",
    "end": "62039"
  },
  {
    "text": "also embedded and that embedding is used",
    "start": "62039",
    "end": "65040"
  },
  {
    "text": "to pull similar documents from the",
    "start": "65040",
    "end": "67020"
  },
  {
    "text": "vector store and both the user query and",
    "start": "67020",
    "end": "69600"
  },
  {
    "text": "the similar documents are passed as",
    "start": "69600",
    "end": "71159"
  },
  {
    "text": "prompt into the large language model so",
    "start": "71159",
    "end": "73260"
  },
  {
    "text": "the llm can extract any relevant",
    "start": "73260",
    "end": "75540"
  },
  {
    "text": "information that you have",
    "start": "75540",
    "end": "77520"
  },
  {
    "text": "in this particular video we're going to",
    "start": "77520",
    "end": "79200"
  },
  {
    "text": "be focusing on step one specifically how",
    "start": "79200",
    "end": "82080"
  },
  {
    "text": "can we effectively quickly and easily",
    "start": "82080",
    "end": "83880"
  },
  {
    "text": "generate embeddings from a large set of",
    "start": "83880",
    "end": "86040"
  },
  {
    "text": "documents now let's dive into what the",
    "start": "86040",
    "end": "88259"
  },
  {
    "text": "flow for this particular step looks like",
    "start": "88259",
    "end": "90960"
  },
  {
    "text": "first we have 2000.pdf documents start",
    "start": "90960",
    "end": "93360"
  },
  {
    "text": "in S3 or in our and for our case these",
    "start": "93360",
    "end": "95820"
  },
  {
    "text": "are documents that have been loaded from",
    "start": "95820",
    "end": "97200"
  },
  {
    "text": "archive",
    "start": "97200",
    "end": "98880"
  },
  {
    "text": "then you want to load those files as",
    "start": "98880",
    "end": "100500"
  },
  {
    "text": "bytes and convert those bytes into a",
    "start": "100500",
    "end": "102600"
  },
  {
    "text": "string text",
    "start": "102600",
    "end": "104220"
  },
  {
    "text": "then you want to split those text into",
    "start": "104220",
    "end": "105960"
  },
  {
    "text": "chunks and each of these chunks are what",
    "start": "105960",
    "end": "108180"
  },
  {
    "text": "what's going to be converted into an",
    "start": "108180",
    "end": "109619"
  },
  {
    "text": "embedding",
    "start": "109619",
    "end": "110880"
  },
  {
    "text": "and finally we take a pre-trained",
    "start": "110880",
    "end": "112740"
  },
  {
    "text": "embedding model for example like a",
    "start": "112740",
    "end": "114899"
  },
  {
    "text": "sentence Transformer model and which",
    "start": "114899",
    "end": "116700"
  },
  {
    "text": "will take those chunks and encode it",
    "start": "116700",
    "end": "118020"
  },
  {
    "text": "into a vector such that similar pieces",
    "start": "118020",
    "end": "120960"
  },
  {
    "text": "of text also are have similar Vector",
    "start": "120960",
    "end": "123600"
  },
  {
    "text": "representations finally once we have",
    "start": "123600",
    "end": "125880"
  },
  {
    "text": "embeddings of all 30 000 Pages we can",
    "start": "125880",
    "end": "127920"
  },
  {
    "text": "store them into a vector database",
    "start": "127920",
    "end": "130679"
  },
  {
    "text": "so this all happens as an offline phase",
    "start": "130679",
    "end": "132720"
  },
  {
    "text": "and like we talked about during online",
    "start": "132720",
    "end": "134099"
  },
  {
    "text": "phase a user a user query is is used to",
    "start": "134099",
    "end": "137879"
  },
  {
    "text": "pull similar documents from this Vector",
    "start": "137879",
    "end": "139860"
  },
  {
    "text": "database to be passed this context into",
    "start": "139860",
    "end": "141780"
  },
  {
    "text": "a large language model",
    "start": "141780",
    "end": "143819"
  },
  {
    "text": "so overall like Lang chain really makes",
    "start": "143819",
    "end": "145980"
  },
  {
    "text": "this step makes this process very easy",
    "start": "145980",
    "end": "148080"
  },
  {
    "text": "and has all the abstractions uh the",
    "start": "148080",
    "end": "151080"
  },
  {
    "text": "Integrations needed to quickly get",
    "start": "151080",
    "end": "153360"
  },
  {
    "text": "started with this type of development",
    "start": "153360",
    "end": "155700"
  },
  {
    "text": "however the problem is that when you",
    "start": "155700",
    "end": "157500"
  },
  {
    "text": "have a large document Corpus in our case",
    "start": "157500",
    "end": "159420"
  },
  {
    "text": "like 2000 PDFs and like tens of",
    "start": "159420",
    "end": "161819"
  },
  {
    "text": "thousands of pages across all these",
    "start": "161819",
    "end": "163500"
  },
  {
    "text": "documents",
    "start": "163500",
    "end": "164940"
  },
  {
    "text": "um this is going to take quite a long",
    "start": "164940",
    "end": "165959"
  },
  {
    "text": "time for not paralyzing right especially",
    "start": "165959",
    "end": "168300"
  },
  {
    "text": "if you want to develop very quickly try",
    "start": "168300",
    "end": "170099"
  },
  {
    "text": "out different embedding models try out",
    "start": "170099",
    "end": "172200"
  },
  {
    "text": "different uh chunk sizes try out",
    "start": "172200",
    "end": "174480"
  },
  {
    "text": "different documents itself you want to",
    "start": "174480",
    "end": "176400"
  },
  {
    "text": "be able to do this process in in just a",
    "start": "176400",
    "end": "180060"
  },
  {
    "text": "few minutes right so you can try out all",
    "start": "180060",
    "end": "181680"
  },
  {
    "text": "these different approaches rather than",
    "start": "181680",
    "end": "183599"
  },
  {
    "text": "doing them all in like a matter of hours",
    "start": "183599",
    "end": "185640"
  },
  {
    "text": "so this is where Ray comes in by using",
    "start": "185640",
    "end": "188040"
  },
  {
    "text": "array and in particular the array data",
    "start": "188040",
    "end": "189720"
  },
  {
    "text": "sets Library we can paralyze this whole",
    "start": "189720",
    "end": "192599"
  },
  {
    "text": "process across many gpus so it would",
    "start": "192599",
    "end": "195060"
  },
  {
    "text": "normally take roughly an hour to do for",
    "start": "195060",
    "end": "197400"
  },
  {
    "text": "2000 PDFs we can do it in a matter of",
    "start": "197400",
    "end": "200099"
  },
  {
    "text": "minutes now let's take a look at the",
    "start": "200099",
    "end": "201480"
  },
  {
    "text": "actual what the actual code looks like",
    "start": "201480",
    "end": "202800"
  },
  {
    "text": "to see how you can use Ray datasets to",
    "start": "202800",
    "end": "204959"
  },
  {
    "text": "parallelize this whole process very",
    "start": "204959",
    "end": "206519"
  },
  {
    "text": "easily",
    "start": "206519",
    "end": "207659"
  },
  {
    "text": "so what I'm showing here is the any",
    "start": "207659",
    "end": "209459"
  },
  {
    "text": "skill platform which is a managed",
    "start": "209459",
    "end": "211080"
  },
  {
    "text": "version of Ray everything you see here",
    "start": "211080",
    "end": "212940"
  },
  {
    "text": "can be done fully in open source using",
    "start": "212940",
    "end": "214920"
  },
  {
    "text": "the ray open source cluster launcher",
    "start": "214920",
    "end": "216480"
  },
  {
    "text": "which we go over in the blog post but",
    "start": "216480",
    "end": "218159"
  },
  {
    "text": "for convenience here I'm going to be",
    "start": "218159",
    "end": "219239"
  },
  {
    "text": "using any scale platform and you see I",
    "start": "219239",
    "end": "221220"
  },
  {
    "text": "have a cluster consisting of five nodes",
    "start": "221220",
    "end": "223379"
  },
  {
    "text": "an overall 20 gpus in this cluster",
    "start": "223379",
    "end": "226319"
  },
  {
    "text": "so first thing you want to do is I'm",
    "start": "226319",
    "end": "227760"
  },
  {
    "text": "going to run this python script which",
    "start": "227760",
    "end": "229980"
  },
  {
    "text": "will generate the embeddings for us so",
    "start": "229980",
    "end": "231780"
  },
  {
    "text": "I'll have this running in the background",
    "start": "231780",
    "end": "232860"
  },
  {
    "text": "and then let's jump into what the actual",
    "start": "232860",
    "end": "234959"
  },
  {
    "text": "code looks like",
    "start": "234959",
    "end": "237620"
  },
  {
    "text": "so as you can see the code here overall",
    "start": "237840",
    "end": "240180"
  },
  {
    "text": "is roughly a hundred you know just a",
    "start": "240180",
    "end": "242220"
  },
  {
    "text": "little over 100 lines so even though we",
    "start": "242220",
    "end": "244500"
  },
  {
    "text": "have a very simple piece of code here we",
    "start": "244500",
    "end": "246840"
  },
  {
    "text": "can effectively paralyze across all of",
    "start": "246840",
    "end": "249120"
  },
  {
    "text": "our gpus so let's go through each of the",
    "start": "249120",
    "end": "251099"
  },
  {
    "text": "steps the first thing we have is that we",
    "start": "251099",
    "end": "253500"
  },
  {
    "text": "have our S3 bucket containing the",
    "start": "253500",
    "end": "255420"
  },
  {
    "text": "documents the PDF document so you want",
    "start": "255420",
    "end": "257160"
  },
  {
    "text": "to embed and we can use array data apis",
    "start": "257160",
    "end": "259919"
  },
  {
    "text": "to read those in as array data set so we",
    "start": "259919",
    "end": "262440"
  },
  {
    "text": "just call read binary files and we have",
    "start": "262440",
    "end": "264120"
  },
  {
    "text": "array data set here",
    "start": "264120",
    "end": "266160"
  },
  {
    "text": "now that we have this data set we can",
    "start": "266160",
    "end": "267720"
  },
  {
    "text": "apply the Transformations that we talked",
    "start": "267720",
    "end": "269280"
  },
  {
    "text": "about on this data set so take those",
    "start": "269280",
    "end": "270780"
  },
  {
    "text": "bytes convert into text take the text",
    "start": "270780",
    "end": "273000"
  },
  {
    "text": "and split them into chunks and then take",
    "start": "273000",
    "end": "275520"
  },
  {
    "text": "those chunks and use an embedding model",
    "start": "275520",
    "end": "277320"
  },
  {
    "text": "to encode them into vectors",
    "start": "277320",
    "end": "280139"
  },
  {
    "text": "so the First Transformation we want to",
    "start": "280139",
    "end": "281639"
  },
  {
    "text": "do is to take the bytes and convert them",
    "start": "281639",
    "end": "283740"
  },
  {
    "text": "into text so we Define a function to do",
    "start": "283740",
    "end": "286380"
  },
  {
    "text": "that the function takes in the bytes",
    "start": "286380",
    "end": "288960"
  },
  {
    "text": "like a bytes object and then we can use",
    "start": "288960",
    "end": "291360"
  },
  {
    "text": "a PDF Reader Library which is the same",
    "start": "291360",
    "end": "293400"
  },
  {
    "text": "library that Lang chain uses to extract",
    "start": "293400",
    "end": "295440"
  },
  {
    "text": "the text for each page in our PDF doc",
    "start": "295440",
    "end": "298860"
  },
  {
    "text": "once we have defined this function we",
    "start": "298860",
    "end": "300840"
  },
  {
    "text": "can map this across every single item in",
    "start": "300840",
    "end": "303479"
  },
  {
    "text": "our in our data set to transform the",
    "start": "303479",
    "end": "305040"
  },
  {
    "text": "data set Ray data sets are lazy by",
    "start": "305040",
    "end": "307080"
  },
  {
    "text": "default so these Transformations are not",
    "start": "307080",
    "end": "308699"
  },
  {
    "text": "applied until we trigger the entire",
    "start": "308699",
    "end": "310919"
  },
  {
    "text": "execution at the end",
    "start": "310919",
    "end": "313259"
  },
  {
    "text": "now that we've converted our bias into",
    "start": "313259",
    "end": "314820"
  },
  {
    "text": "text the next step is to split the text",
    "start": "314820",
    "end": "317460"
  },
  {
    "text": "into chunks so again we Define a",
    "start": "317460",
    "end": "319800"
  },
  {
    "text": "function this time this function takes",
    "start": "319800",
    "end": "321840"
  },
  {
    "text": "in a string for the text for each page",
    "start": "321840",
    "end": "324060"
  },
  {
    "text": "and we use Lang chains text splitter",
    "start": "324060",
    "end": "326039"
  },
  {
    "text": "abstraction passing a chunk size of a",
    "start": "326039",
    "end": "328620"
  },
  {
    "text": "thousand to convert each page of text",
    "start": "328620",
    "end": "331380"
  },
  {
    "text": "into chunks",
    "start": "331380",
    "end": "333120"
  },
  {
    "text": "so we can apply this transformation",
    "start": "333120",
    "end": "334740"
  },
  {
    "text": "again on our data set and then now we",
    "start": "334740",
    "end": "336900"
  },
  {
    "text": "have a data set containing chunks so",
    "start": "336900",
    "end": "338759"
  },
  {
    "text": "after these two Transformations we went",
    "start": "338759",
    "end": "340320"
  },
  {
    "text": "from 2000 PDF documents to 30 000 pages",
    "start": "340320",
    "end": "343620"
  },
  {
    "text": "to now roughly 150 000 chunks uh each",
    "start": "343620",
    "end": "347280"
  },
  {
    "text": "chunk is a thousand characters across",
    "start": "347280",
    "end": "349259"
  },
  {
    "text": "our entire document Corpus",
    "start": "349259",
    "end": "351600"
  },
  {
    "text": "now that we have our chunks the final",
    "start": "351600",
    "end": "353100"
  },
  {
    "text": "step the final transformation we want to",
    "start": "353100",
    "end": "354720"
  },
  {
    "text": "apply is to actually use an embedding",
    "start": "354720",
    "end": "356400"
  },
  {
    "text": "model to encode each of these chunks",
    "start": "356400",
    "end": "358320"
  },
  {
    "text": "into a vector",
    "start": "358320",
    "end": "360360"
  },
  {
    "text": "so here we do a similar thing we Define",
    "start": "360360",
    "end": "363120"
  },
  {
    "text": "a transformation but instead of defining",
    "start": "363120",
    "end": "365280"
  },
  {
    "text": "a function we define a class and in",
    "start": "365280",
    "end": "367020"
  },
  {
    "text": "particular uh we we Define an embedding",
    "start": "367020",
    "end": "370199"
  },
  {
    "text": "class so the reason we do this is",
    "start": "370199",
    "end": "372180"
  },
  {
    "text": "because uh we initializing a Transformer",
    "start": "372180",
    "end": "375180"
  },
  {
    "text": "model can be pretty expensive so you",
    "start": "375180",
    "end": "376919"
  },
  {
    "text": "only want to initialize it once per GPU",
    "start": "376919",
    "end": "379080"
  },
  {
    "text": "and reuse the same model for multiple",
    "start": "379080",
    "end": "381900"
  },
  {
    "text": "batches rather than having to initialize",
    "start": "381900",
    "end": "383940"
  },
  {
    "text": "the same Transformer model for every",
    "start": "383940",
    "end": "385500"
  },
  {
    "text": "single batch so in our class we",
    "start": "385500",
    "end": "388580"
  },
  {
    "text": "initialize our sentence Transformer",
    "start": "388580",
    "end": "390720"
  },
  {
    "text": "model using the same default model",
    "start": "390720",
    "end": "392460"
  },
  {
    "text": "pre-trained model that Lang chain uses",
    "start": "392460",
    "end": "394380"
  },
  {
    "text": "and then for each batch of data for each",
    "start": "394380",
    "end": "397740"
  },
  {
    "text": "Chunk we can use this Transformer model",
    "start": "397740",
    "end": "400199"
  },
  {
    "text": "to encode the chunk into a vector and",
    "start": "400199",
    "end": "403380"
  },
  {
    "text": "then we can return both the original",
    "start": "403380",
    "end": "405060"
  },
  {
    "text": "chunk as well as the embedding for that",
    "start": "405060",
    "end": "407639"
  },
  {
    "text": "chunk",
    "start": "407639",
    "end": "408539"
  },
  {
    "text": "and again we can map this uh on across",
    "start": "408539",
    "end": "411479"
  },
  {
    "text": "our data set so apply this",
    "start": "411479",
    "end": "412919"
  },
  {
    "text": "transformation in this case we want to",
    "start": "412919",
    "end": "415500"
  },
  {
    "text": "specify to use gpus and then we also",
    "start": "415500",
    "end": "419220"
  },
  {
    "text": "want to specify a batch size so we want",
    "start": "419220",
    "end": "421860"
  },
  {
    "text": "to use the largest batch that can fit",
    "start": "421860",
    "end": "423479"
  },
  {
    "text": "onto a single GPU to maximize our GPU",
    "start": "423479",
    "end": "425940"
  },
  {
    "text": "utilization",
    "start": "425940",
    "end": "427500"
  },
  {
    "text": "so now we've defined our three",
    "start": "427500",
    "end": "428699"
  },
  {
    "text": "Transformations uh what we can do now is",
    "start": "428699",
    "end": "431699"
  },
  {
    "text": "iterate through the results of this data",
    "start": "431699",
    "end": "434220"
  },
  {
    "text": "set so this will actually trigger",
    "start": "434220",
    "end": "435840"
  },
  {
    "text": "execution of the entire pipeline that",
    "start": "435840",
    "end": "438180"
  },
  {
    "text": "we've defined",
    "start": "438180",
    "end": "439259"
  },
  {
    "text": "and once we have the outputs uh here we",
    "start": "439259",
    "end": "442500"
  },
  {
    "text": "can then do we can then store these into",
    "start": "442500",
    "end": "444720"
  },
  {
    "text": "a vector database same way you would do",
    "start": "444720",
    "end": "446460"
  },
  {
    "text": "it with line chain normally so we Define",
    "start": "446460",
    "end": "448680"
  },
  {
    "text": "like a face Vector database store or",
    "start": "448680",
    "end": "451860"
  },
  {
    "text": "embeddings and then we can save this",
    "start": "451860",
    "end": "453780"
  },
  {
    "text": "Vector database locally",
    "start": "453780",
    "end": "456360"
  },
  {
    "text": "so let's take a look at the progress of",
    "start": "456360",
    "end": "458520"
  },
  {
    "text": "the script",
    "start": "458520",
    "end": "460800"
  },
  {
    "text": "and you can see this is almost done so",
    "start": "460800",
    "end": "462840"
  },
  {
    "text": "it's created the vector index we saved",
    "start": "462840",
    "end": "464699"
  },
  {
    "text": "it locally and overall this is going to",
    "start": "464699",
    "end": "467520"
  },
  {
    "text": "take a little bit less than uh four",
    "start": "467520",
    "end": "469680"
  },
  {
    "text": "minutes to finish",
    "start": "469680",
    "end": "471360"
  },
  {
    "text": "so just under four minutes here so in",
    "start": "471360",
    "end": "474300"
  },
  {
    "text": "just four minutes we've embedded 2 000",
    "start": "474300",
    "end": "476160"
  },
  {
    "text": "PDF documents",
    "start": "476160",
    "end": "478020"
  },
  {
    "text": "and we can also take a look at the raid",
    "start": "478020",
    "end": "479699"
  },
  {
    "text": "dashboard which can show us information",
    "start": "479699",
    "end": "481259"
  },
  {
    "text": "about our GPU utilization",
    "start": "481259",
    "end": "483720"
  },
  {
    "text": "so here as you can see",
    "start": "483720",
    "end": "485880"
  },
  {
    "text": "um during when during this time the",
    "start": "485880",
    "end": "488520"
  },
  {
    "text": "computation is actually happening our",
    "start": "488520",
    "end": "490500"
  },
  {
    "text": "gpus are nearly 100 utilized so all 20",
    "start": "490500",
    "end": "493319"
  },
  {
    "text": "gpus",
    "start": "493319",
    "end": "494819"
  },
  {
    "text": "so we can show we saw that you can just",
    "start": "494819",
    "end": "496860"
  },
  {
    "text": "100 lines of code using the array data",
    "start": "496860",
    "end": "498780"
  },
  {
    "text": "apis we can paralyze this embedding",
    "start": "498780",
    "end": "501120"
  },
  {
    "text": "generation process across 20 different",
    "start": "501120",
    "end": "503220"
  },
  {
    "text": "gpus",
    "start": "503220",
    "end": "505139"
  },
  {
    "text": "so just to recap what we talked about",
    "start": "505139",
    "end": "507780"
  },
  {
    "text": "um using Lang chain makes it very easy",
    "start": "507780",
    "end": "509400"
  },
  {
    "text": "to generate embeddings from PDF",
    "start": "509400",
    "end": "511620"
  },
  {
    "text": "documents in order to develop and move",
    "start": "511620",
    "end": "513659"
  },
  {
    "text": "quickly especially with large amounts of",
    "start": "513659",
    "end": "515459"
  },
  {
    "text": "data you need a way to scale this out",
    "start": "515459",
    "end": "517260"
  },
  {
    "text": "and to paralyze it effectively and",
    "start": "517260",
    "end": "519240"
  },
  {
    "text": "that's what we're doing with array so",
    "start": "519240",
    "end": "520740"
  },
  {
    "text": "overall with the Ray and Lang chain",
    "start": "520740",
    "end": "521940"
  },
  {
    "text": "together we can index thirty thousand",
    "start": "521940",
    "end": "523919"
  },
  {
    "text": "paid Pages uh in four minutes",
    "start": "523919",
    "end": "527040"
  },
  {
    "text": "so hope you enjoyed this talk uh we'll",
    "start": "527040",
    "end": "529500"
  },
  {
    "text": "have more upcoming videos about how you",
    "start": "529500",
    "end": "531839"
  },
  {
    "text": "can use Lang chain and raid together to",
    "start": "531839",
    "end": "533760"
  },
  {
    "text": "build uh LM applications that can be",
    "start": "533760",
    "end": "536399"
  },
  {
    "text": "used in production and for larger scale",
    "start": "536399",
    "end": "539899"
  }
]