[
  {
    "text": "all right thank you everyone",
    "start": "8420",
    "end": "12300"
  },
  {
    "text": "so uh we are going to talk about machine",
    "start": "12300",
    "end": "14880"
  },
  {
    "text": "learning on CPUs so let me introduce a",
    "start": "14880",
    "end": "17640"
  },
  {
    "text": "little bit about what we do at third eye",
    "start": "17640",
    "end": "19800"
  },
  {
    "text": "and again like we build software",
    "start": "19800",
    "end": "22439"
  },
  {
    "text": "libraries to train and fine-tune billion",
    "start": "22439",
    "end": "26039"
  },
  {
    "text": "parameters neural networks large",
    "start": "26039",
    "end": "27779"
  },
  {
    "text": "language model or retrieval augmented",
    "start": "27779",
    "end": "30060"
  },
  {
    "text": "Generation all on CPUs",
    "start": "30060",
    "end": "32220"
  },
  {
    "text": "and again I will restate we train and",
    "start": "32220",
    "end": "35340"
  },
  {
    "text": "fine tune because whenever we think",
    "start": "35340",
    "end": "36960"
  },
  {
    "text": "about AI on CPUs the common perception",
    "start": "36960",
    "end": "39660"
  },
  {
    "text": "is this is talking about inference we",
    "start": "39660",
    "end": "41820"
  },
  {
    "text": "are not talking about inference we are",
    "start": "41820",
    "end": "43320"
  },
  {
    "text": "talking about training retraining fine",
    "start": "43320",
    "end": "45239"
  },
  {
    "text": "tuning and reinforcement learning ah we",
    "start": "45239",
    "end": "48180"
  },
  {
    "text": "can do that on CPU to get started I'll",
    "start": "48180",
    "end": "50879"
  },
  {
    "text": "go over a small video that we created",
    "start": "50879",
    "end": "53940"
  },
  {
    "text": "don't wait buy your graphics card today",
    "start": "53940",
    "end": "57660"
  },
  {
    "text": "the next GPU shortage is coming and",
    "start": "57660",
    "end": "59879"
  },
  {
    "text": "honestly I'd recommend buying a GPU",
    "start": "59879",
    "end": "61559"
  },
  {
    "text": "sooner rather than later the shortage of",
    "start": "61559",
    "end": "63239"
  },
  {
    "text": "Nvidia gpus which is causing havoc on",
    "start": "63239",
    "end": "65640"
  },
  {
    "text": "the AI markets companies are buying way",
    "start": "65640",
    "end": "67860"
  },
  {
    "text": "more gpus than before thanks to the new",
    "start": "67860",
    "end": "69900"
  },
  {
    "text": "AI boom there's still industry talk of",
    "start": "69900",
    "end": "72000"
  },
  {
    "text": "yet another GPU shortage even if tsmc",
    "start": "72000",
    "end": "73979"
  },
  {
    "text": "started working on it immediately you",
    "start": "73979",
    "end": "75360"
  },
  {
    "text": "would not get a chip out for three more",
    "start": "75360",
    "end": "76799"
  },
  {
    "text": "months we are facing a GPU shortage",
    "start": "76799",
    "end": "79200"
  },
  {
    "text": "because everyone wants generative Ai and",
    "start": "79200",
    "end": "81720"
  },
  {
    "text": "generative AI can only train and",
    "start": "81720",
    "end": "83580"
  },
  {
    "text": "fine-tune on gpus",
    "start": "83580",
    "end": "85560"
  },
  {
    "text": "at least",
    "start": "85560",
    "end": "86880"
  },
  {
    "text": "it used to",
    "start": "86880",
    "end": "88740"
  },
  {
    "text": "introducing bolt 2.5 B the world's first",
    "start": "88740",
    "end": "92220"
  },
  {
    "text": "generative large language model entirely",
    "start": "92220",
    "end": "94680"
  },
  {
    "text": "pre-trained on CPUs let's see how it",
    "start": "94680",
    "end": "96900"
  },
  {
    "text": "Compares with a 1.5 billion parameter",
    "start": "96900",
    "end": "98820"
  },
  {
    "text": "gpt2 XL",
    "start": "98820",
    "end": "102140"
  },
  {
    "text": "[Music]",
    "start": "102220",
    "end": "114420"
  },
  {
    "text": "under the hood is our bolt deep learning",
    "start": "114420",
    "end": "116640"
  },
  {
    "text": "engine which uses Dynamic sparsity to",
    "start": "116640",
    "end": "118500"
  },
  {
    "text": "selectively train only the neurons that",
    "start": "118500",
    "end": "120299"
  },
  {
    "text": "are relevant to each training sample",
    "start": "120299",
    "end": "122159"
  },
  {
    "text": "this allowed us to train this 2.5",
    "start": "122159",
    "end": "124380"
  },
  {
    "text": "billion parameter model on 40 billion",
    "start": "124380",
    "end": "126780"
  },
  {
    "text": "tokens with 10 Sapphire Rapids machines",
    "start": "126780",
    "end": "129179"
  },
  {
    "text": "in just 20 days with this level of",
    "start": "129179",
    "end": "131459"
  },
  {
    "text": "efficiency fine tuning is more",
    "start": "131459",
    "end": "133080"
  },
  {
    "text": "accessible than ever before after just",
    "start": "133080",
    "end": "135599"
  },
  {
    "text": "one hour of fine tuning on the works of",
    "start": "135599",
    "end": "137459"
  },
  {
    "text": "Shakespeare using a Haswell machine we",
    "start": "137459",
    "end": "139800"
  },
  {
    "text": "were able to turn this into this",
    "start": "139800",
    "end": "144440"
  },
  {
    "text": "there was no pruning quantization or low",
    "start": "145020",
    "end": "147720"
  },
  {
    "text": "rank approximation involved",
    "start": "147720",
    "end": "149640"
  },
  {
    "text": "so think about the things you can do",
    "start": "149640",
    "end": "151260"
  },
  {
    "text": "with spare compute in your data center",
    "start": "151260",
    "end": "152940"
  },
  {
    "text": "think about what individuals can build",
    "start": "152940",
    "end": "154980"
  },
  {
    "text": "with their own laptops or desktop",
    "start": "154980",
    "end": "156720"
  },
  {
    "text": "computers think about using cheaper",
    "start": "156720",
    "end": "159360"
  },
  {
    "text": "alternatives to GPU instances for",
    "start": "159360",
    "end": "161459"
  },
  {
    "text": "regenerative AI workloads",
    "start": "161459",
    "end": "164700"
  },
  {
    "text": "so ah as as you can see we are talking",
    "start": "164700",
    "end": "167280"
  },
  {
    "text": "about training AI on CPU and for a",
    "start": "167280",
    "end": "169260"
  },
  {
    "text": "comparison we have trained uh the",
    "start": "169260",
    "end": "171660"
  },
  {
    "text": "world's first generative large language",
    "start": "171660",
    "end": "173459"
  },
  {
    "text": "model entirely on CPUs so this was",
    "start": "173459",
    "end": "176459"
  },
  {
    "text": "trained on tens of our Rapids cluster uh",
    "start": "176459",
    "end": "179280"
  },
  {
    "text": "obviously the distribution was done with",
    "start": "179280",
    "end": "182040"
  },
  {
    "text": "an interconnect and on a ray framework",
    "start": "182040",
    "end": "183860"
  },
  {
    "text": "uh it took about 20 days it has consumed",
    "start": "183860",
    "end": "186959"
  },
  {
    "text": "20 billion tokens so far and its",
    "start": "186959",
    "end": "188760"
  },
  {
    "text": "capability is at par with gpd2",
    "start": "188760",
    "end": "193519"
  },
  {
    "text": "so uh why CPUs I mean again like uh well",
    "start": "193680",
    "end": "198120"
  },
  {
    "text": "we all know about like we are all",
    "start": "198120",
    "end": "199620"
  },
  {
    "text": "talking about saving you know gpus and",
    "start": "199620",
    "end": "201900"
  },
  {
    "text": "like the GPU shortage well another way",
    "start": "201900",
    "end": "203760"
  },
  {
    "text": "to save gpus is to start using more of",
    "start": "203760",
    "end": "206400"
  },
  {
    "text": "your CPUs right and that is what we are",
    "start": "206400",
    "end": "208379"
  },
  {
    "text": "trying to do so CPUs are readily",
    "start": "208379",
    "end": "210420"
  },
  {
    "text": "available they uh you know if you are",
    "start": "210420",
    "end": "213120"
  },
  {
    "text": "doing a a better AI on CPUs it serves",
    "start": "213120",
    "end": "216480"
  },
  {
    "text": "much better latency because you don't",
    "start": "216480",
    "end": "217920"
  },
  {
    "text": "have to transfer your data from CPU to",
    "start": "217920",
    "end": "220260"
  },
  {
    "text": "GPU and back there is data residency",
    "start": "220260",
    "end": "223799"
  },
  {
    "text": "problem I'll talk a little bit more",
    "start": "223799",
    "end": "225420"
  },
  {
    "text": "about it cost is definitely a factor",
    "start": "225420",
    "end": "227760"
  },
  {
    "text": "even if we believe uh you know the cost",
    "start": "227760",
    "end": "230040"
  },
  {
    "text": "is not going to be a factor in the",
    "start": "230040",
    "end": "231720"
  },
  {
    "text": "future data movement is going to be a",
    "start": "231720",
    "end": "233519"
  },
  {
    "text": "future and we are building an AI",
    "start": "233519",
    "end": "235860"
  },
  {
    "text": "ecosystem which is about ownership as in",
    "start": "235860",
    "end": "238980"
  },
  {
    "text": "you are encouraged to fine tune daily",
    "start": "238980",
    "end": "241260"
  },
  {
    "text": "right that is kind of an AI ecosystem we",
    "start": "241260",
    "end": "244260"
  },
  {
    "text": "want to build",
    "start": "244260",
    "end": "245459"
  },
  {
    "text": "so why this is possible why now and ah",
    "start": "245459",
    "end": "249360"
  },
  {
    "text": "other things so this is a paper that we",
    "start": "249360",
    "end": "251580"
  },
  {
    "text": "presented in icml 2023 this year it was",
    "start": "251580",
    "end": "254640"
  },
  {
    "text": "taking all the like models that was",
    "start": "254640",
    "end": "256680"
  },
  {
    "text": "released by Facebook so starting from an",
    "start": "256680",
    "end": "258840"
  },
  {
    "text": "opt 350 million all the way to opt 175",
    "start": "258840",
    "end": "261660"
  },
  {
    "text": "billion what we what we can show is when",
    "start": "261660",
    "end": "264180"
  },
  {
    "text": "you run this uh model feed forward",
    "start": "264180",
    "end": "267440"
  },
  {
    "text": "99.9 or more than 99 of competitions are",
    "start": "267440",
    "end": "271259"
  },
  {
    "text": "actually zero times a number and so when",
    "start": "271259",
    "end": "273540"
  },
  {
    "text": "you do forward pass you're doing a lot",
    "start": "273540",
    "end": "275040"
  },
  {
    "text": "of zero times a number when you do",
    "start": "275040",
    "end": "276720"
  },
  {
    "text": "backward pass most of the competitions",
    "start": "276720",
    "end": "278699"
  },
  {
    "text": "or the gradients are still zero so",
    "start": "278699",
    "end": "280620"
  },
  {
    "text": "essentially we are wasting a lot of our",
    "start": "280620",
    "end": "282960"
  },
  {
    "text": "floating Point operations lot of our",
    "start": "282960",
    "end": "285540"
  },
  {
    "text": "computations and energy in calculating",
    "start": "285540",
    "end": "287580"
  },
  {
    "text": "these zeros and probably literally",
    "start": "287580",
    "end": "289620"
  },
  {
    "text": "burning like billions of dollars in",
    "start": "289620",
    "end": "291240"
  },
  {
    "text": "Computing those zeros",
    "start": "291240",
    "end": "292740"
  },
  {
    "text": "so at third eye we have found a way to",
    "start": "292740",
    "end": "297180"
  },
  {
    "text": "not compute these zeros so essentially",
    "start": "297180",
    "end": "300240"
  },
  {
    "text": "this is like I'm also a professor and",
    "start": "300240",
    "end": "302160"
  },
  {
    "text": "this is more than 10 years of research",
    "start": "302160",
    "end": "303960"
  },
  {
    "text": "that I have been doing there are dozens",
    "start": "303960",
    "end": "306360"
  },
  {
    "text": "of research paper where we showed that",
    "start": "306360",
    "end": "308759"
  },
  {
    "text": "there is and what we call it Dynamics",
    "start": "308759",
    "end": "311460"
  },
  {
    "text": "per stack there is a scientifically",
    "start": "311460",
    "end": "314040"
  },
  {
    "text": "proven way to train deep learning more",
    "start": "314040",
    "end": "315960"
  },
  {
    "text": "efficiently so just to a deep just a",
    "start": "315960",
    "end": "318360"
  },
  {
    "text": "high level overview of this deep",
    "start": "318360",
    "end": "320100"
  },
  {
    "text": "Learning Network here so imagine that",
    "start": "320100",
    "end": "322620"
  },
  {
    "text": "this is a billion parameter neural",
    "start": "322620",
    "end": "324060"
  },
  {
    "text": "network right so when we do a forward",
    "start": "324060",
    "end": "326220"
  },
  {
    "text": "pass we do billion operation we get a",
    "start": "326220",
    "end": "328320"
  },
  {
    "text": "prediction then there is an error we do",
    "start": "328320",
    "end": "330479"
  },
  {
    "text": "billion operation we get updates and",
    "start": "330479",
    "end": "333660"
  },
  {
    "text": "then we do a billion of updates at this",
    "start": "333660",
    "end": "335880"
  },
  {
    "text": "point of time we'll say look there are",
    "start": "335880",
    "end": "337320"
  },
  {
    "text": "three billion floating Point operation",
    "start": "337320",
    "end": "338759"
  },
  {
    "text": "we definitely need specialized Hardwares",
    "start": "338759",
    "end": "340979"
  },
  {
    "text": "and you would be right but if you go and",
    "start": "340979",
    "end": "342840"
  },
  {
    "text": "look into it you will find that you do",
    "start": "342840",
    "end": "344520"
  },
  {
    "text": "three billion floating Point operation",
    "start": "344520",
    "end": "345960"
  },
  {
    "text": "to only update few thousand parameters",
    "start": "345960",
    "end": "348419"
  },
  {
    "text": "now the puzzle is obviously you cannot",
    "start": "348419",
    "end": "350340"
  },
  {
    "text": "throw away parameters like if you think",
    "start": "350340",
    "end": "352380"
  },
  {
    "text": "about static sparsity right sparsity is",
    "start": "352380",
    "end": "354300"
  },
  {
    "text": "now a very overloaded word so in static",
    "start": "354300",
    "end": "356880"
  },
  {
    "text": "sparsity you may think about can I throw",
    "start": "356880",
    "end": "358380"
  },
  {
    "text": "away parameters the answer would be no",
    "start": "358380",
    "end": "360060"
  },
  {
    "text": "because every pattern will pick a very",
    "start": "360060",
    "end": "362280"
  },
  {
    "text": "different set of thousand parameters and",
    "start": "362280",
    "end": "364199"
  },
  {
    "text": "in fact if you throw away the parameter",
    "start": "364199",
    "end": "365820"
  },
  {
    "text": "that takes away L from that large",
    "start": "365820",
    "end": "367860"
  },
  {
    "text": "language model right I mean you will",
    "start": "367860",
    "end": "369360"
  },
  {
    "text": "model it will start becoming small so",
    "start": "369360",
    "end": "371520"
  },
  {
    "text": "essentially the puzzle here is how can",
    "start": "371520",
    "end": "373560"
  },
  {
    "text": "we figure out the relevant parameters",
    "start": "373560",
    "end": "375900"
  },
  {
    "text": "given a pattern and the pattern is",
    "start": "375900",
    "end": "377400"
  },
  {
    "text": "allowed to change dynamically because we",
    "start": "377400",
    "end": "378840"
  },
  {
    "text": "don't know what the input is but well we",
    "start": "378840",
    "end": "380820"
  },
  {
    "text": "have seen that problem before it's like",
    "start": "380820",
    "end": "382380"
  },
  {
    "text": "information retrieval right when you do",
    "start": "382380",
    "end": "384000"
  },
  {
    "text": "Google search when you type array submit",
    "start": "384000",
    "end": "385979"
  },
  {
    "text": "on Google search how does Google search",
    "start": "385979",
    "end": "387960"
  },
  {
    "text": "knows where to take will take you and",
    "start": "387960",
    "end": "390240"
  },
  {
    "text": "not like compare with billions of web",
    "start": "390240",
    "end": "392160"
  },
  {
    "text": "pages on the planet that is precisely a",
    "start": "392160",
    "end": "395639"
  },
  {
    "text": "technology that we have made to work so",
    "start": "395639",
    "end": "398940"
  },
  {
    "text": "basically in our bolt infrastructure",
    "start": "398940",
    "end": "401759"
  },
  {
    "text": "which is our dynamically Spar software",
    "start": "401759",
    "end": "403500"
  },
  {
    "text": "stack when you show a pattern a very",
    "start": "403500",
    "end": "405960"
  },
  {
    "text": "sparse set of parameter are queried",
    "start": "405960",
    "end": "408360"
  },
  {
    "text": "from this large neural network and only",
    "start": "408360",
    "end": "411780"
  },
  {
    "text": "those parameters are used for forward",
    "start": "411780",
    "end": "413220"
  },
  {
    "text": "propagation back propagation and update",
    "start": "413220",
    "end": "414960"
  },
  {
    "text": "so essentially instead of doing three",
    "start": "414960",
    "end": "416639"
  },
  {
    "text": "billion operation to do 1000 updates we",
    "start": "416639",
    "end": "419639"
  },
  {
    "text": "can get away with let us say 40 or 50",
    "start": "419639",
    "end": "421740"
  },
  {
    "text": "000 operations and this allows us to",
    "start": "421740",
    "end": "424139"
  },
  {
    "text": "utilize massive scale deep learning",
    "start": "424139",
    "end": "426539"
  },
  {
    "text": "training even on CPU without losing",
    "start": "426539",
    "end": "428880"
  },
  {
    "text": "accuracies",
    "start": "428880",
    "end": "429960"
  },
  {
    "text": "just to argue like why why we need this",
    "start": "429960",
    "end": "432780"
  },
  {
    "text": "now let's present a case study of why we",
    "start": "432780",
    "end": "435300"
  },
  {
    "text": "believe that current software stack is",
    "start": "435300",
    "end": "437100"
  },
  {
    "text": "actually fundamentally hard for a lot of",
    "start": "437100",
    "end": "439199"
  },
  {
    "text": "our production task so I'm just showing",
    "start": "439199",
    "end": "441120"
  },
  {
    "text": "you a retrieval augmented generation",
    "start": "441120",
    "end": "442620"
  },
  {
    "text": "framework right in which you are taking",
    "start": "442620",
    "end": "444960"
  },
  {
    "text": "a gen AI model and because the genei",
    "start": "444960",
    "end": "447060"
  },
  {
    "text": "model has a lot short context window",
    "start": "447060",
    "end": "449039"
  },
  {
    "text": "what you do is you do a semantic",
    "start": "449039",
    "end": "451259"
  },
  {
    "text": "retrieval so let's say if you want to",
    "start": "451259",
    "end": "453120"
  },
  {
    "text": "ask a question does my organization",
    "start": "453120",
    "end": "454919"
  },
  {
    "text": "provides any retirement benefit you want",
    "start": "454919",
    "end": "457080"
  },
  {
    "text": "to retrieve everything that your",
    "start": "457080",
    "end": "458340"
  },
  {
    "text": "organization has which is relevant to",
    "start": "458340",
    "end": "460020"
  },
  {
    "text": "answering this question then you give it",
    "start": "460020",
    "end": "461819"
  },
  {
    "text": "to the Gen AI model and hope the Genie",
    "start": "461819",
    "end": "463500"
  },
  {
    "text": "model will generate an answer now to be",
    "start": "463500",
    "end": "465539"
  },
  {
    "text": "able to do this retrieval people are",
    "start": "465539",
    "end": "467759"
  },
  {
    "text": "using embedding models which are now",
    "start": "467759",
    "end": "469560"
  },
  {
    "text": "going to work on gpus and then there is",
    "start": "469560",
    "end": "471599"
  },
  {
    "text": "a whole ecosystem on Vector DB which is",
    "start": "471599",
    "end": "473580"
  },
  {
    "text": "likely working on CPU now if you look at",
    "start": "473580",
    "end": "475740"
  },
  {
    "text": "this infrastructure and ask a question",
    "start": "475740",
    "end": "477120"
  },
  {
    "text": "look is this really going in production",
    "start": "477120",
    "end": "478860"
  },
  {
    "text": "as is this really going to scale so here",
    "start": "478860",
    "end": "481860"
  },
  {
    "text": "are some of the challenges first of all",
    "start": "481860",
    "end": "483180"
  },
  {
    "text": "as you can see there are two data",
    "start": "483180",
    "end": "485160"
  },
  {
    "text": "movements because all your text needs to",
    "start": "485160",
    "end": "487139"
  },
  {
    "text": "go to GPU to generate embeddings and all",
    "start": "487139",
    "end": "489960"
  },
  {
    "text": "your text needs to be in Vector DB to be",
    "start": "489960",
    "end": "492180"
  },
  {
    "text": "stored and retrieved so you have created",
    "start": "492180",
    "end": "494280"
  },
  {
    "text": "at least two copies of your text around",
    "start": "494280",
    "end": "496020"
  },
  {
    "text": "and data movement is not easy",
    "start": "496020",
    "end": "499139"
  },
  {
    "text": "I mean even if I know GPU cost may go",
    "start": "499139",
    "end": "501479"
  },
  {
    "text": "down in the future data moment will",
    "start": "501479",
    "end": "503580"
  },
  {
    "text": "still create that friction there is",
    "start": "503580",
    "end": "505259"
  },
  {
    "text": "another issue here with the embedding",
    "start": "505259",
    "end": "506940"
  },
  {
    "text": "models the the current assumption is",
    "start": "506940",
    "end": "509160"
  },
  {
    "text": "look at the text that is about like what",
    "start": "509160",
    "end": "510780"
  },
  {
    "text": "512 characters or something for a chunk",
    "start": "510780",
    "end": "513000"
  },
  {
    "text": "usually they recommend 256 or 250",
    "start": "513000",
    "end": "515820"
  },
  {
    "text": "characters for a chunk that's like what",
    "start": "515820",
    "end": "518039"
  },
  {
    "text": "500 bytes and if you use open AI Zada",
    "start": "518039",
    "end": "521099"
  },
  {
    "text": "embedding model you are generating 1500",
    "start": "521099",
    "end": "522899"
  },
  {
    "text": "numbers that's about 6000 bytes so",
    "start": "522899",
    "end": "526200"
  },
  {
    "text": "you're blowing up your data about more",
    "start": "526200",
    "end": "528000"
  },
  {
    "text": "than 10x and then moving it around",
    "start": "528000",
    "end": "531600"
  },
  {
    "text": "the other issue is let us say in this",
    "start": "531600",
    "end": "533820"
  },
  {
    "text": "system you don't like the retriever the",
    "start": "533820",
    "end": "536519"
  },
  {
    "text": "reality of everything in production is",
    "start": "536519",
    "end": "538500"
  },
  {
    "text": "there is no static AI that will make",
    "start": "538500",
    "end": "540660"
  },
  {
    "text": "everybody happy there as you scale you",
    "start": "540660",
    "end": "543660"
  },
  {
    "text": "will not like the results who will you",
    "start": "543660",
    "end": "546000"
  },
  {
    "text": "blame do you will blame it to a vector",
    "start": "546000",
    "end": "547800"
  },
  {
    "text": "DB or will you go and blame it to an",
    "start": "547800",
    "end": "549480"
  },
  {
    "text": "embedding model",
    "start": "549480",
    "end": "550680"
  },
  {
    "text": "and if you fine tune an embed model or",
    "start": "550680",
    "end": "552779"
  },
  {
    "text": "refine it you go and rebuild your vector",
    "start": "552779",
    "end": "555180"
  },
  {
    "text": "DB and Enterprises would want to do it",
    "start": "555180",
    "end": "557700"
  },
  {
    "text": "again and again and offer",
    "start": "557700",
    "end": "560640"
  },
  {
    "text": "I mean I think we are not asking hard",
    "start": "560640",
    "end": "562320"
  },
  {
    "text": "questions here is why is this friction",
    "start": "562320",
    "end": "564120"
  },
  {
    "text": "coming and we believe that part of this",
    "start": "564120",
    "end": "566100"
  },
  {
    "text": "friction is because of this",
    "start": "566100",
    "end": "568019"
  },
  {
    "text": "heterogeneity of the hardware you need a",
    "start": "568019",
    "end": "570480"
  },
  {
    "text": "different software stack to understand",
    "start": "570480",
    "end": "571680"
  },
  {
    "text": "GPU a different software stack to",
    "start": "571680",
    "end": "573779"
  },
  {
    "text": "understand a CPU and that is why you are",
    "start": "573779",
    "end": "576540"
  },
  {
    "text": "pushing these blocks and puzzles around",
    "start": "576540",
    "end": "579300"
  },
  {
    "text": "there and ah becoming the software stack",
    "start": "579300",
    "end": "582420"
  },
  {
    "text": "is becoming complicated",
    "start": "582420",
    "end": "583980"
  },
  {
    "text": "again",
    "start": "583980",
    "end": "585480"
  },
  {
    "text": "the question is let's look at",
    "start": "585480",
    "end": "588540"
  },
  {
    "text": "what happens in a vector DB right so",
    "start": "588540",
    "end": "590580"
  },
  {
    "text": "what happens is you get an embedding",
    "start": "590580",
    "end": "592140"
  },
  {
    "text": "then embedding goes into a data",
    "start": "592140",
    "end": "593640"
  },
  {
    "text": "structure some point some form of",
    "start": "593640",
    "end": "595860"
  },
  {
    "text": "pointers key or memory address that does",
    "start": "595860",
    "end": "598080"
  },
  {
    "text": "the retrieval",
    "start": "598080",
    "end": "599820"
  },
  {
    "text": "the question is why a very disconnected",
    "start": "599820",
    "end": "602580"
  },
  {
    "text": "process to do that right if we could do",
    "start": "602580",
    "end": "604560"
  },
  {
    "text": "something like",
    "start": "604560",
    "end": "606779"
  },
  {
    "text": "you know if it is assumed the reason why",
    "start": "606779",
    "end": "609959"
  },
  {
    "text": "we are doing this is because it is",
    "start": "609959",
    "end": "611339"
  },
  {
    "text": "assumed that look it's prohibitive to",
    "start": "611339",
    "end": "613380"
  },
  {
    "text": "build gen AI models so all we can do is",
    "start": "613380",
    "end": "615660"
  },
  {
    "text": "to reuse it nobody wants to build an AI",
    "start": "615660",
    "end": "618240"
  },
  {
    "text": "model because it is assumed to be",
    "start": "618240",
    "end": "619320"
  },
  {
    "text": "prohibitive and",
    "start": "619320",
    "end": "621720"
  },
  {
    "text": "we are okay with complicating the",
    "start": "621720",
    "end": "623940"
  },
  {
    "text": "software stack because right now there",
    "start": "623940",
    "end": "625800"
  },
  {
    "text": "is GPU shortage and nobody can build",
    "start": "625800",
    "end": "627480"
  },
  {
    "text": "this but let's say if you could build",
    "start": "627480",
    "end": "629880"
  },
  {
    "text": "gen AI models at ease then you would ask",
    "start": "629880",
    "end": "631740"
  },
  {
    "text": "the question why go with this",
    "start": "631740",
    "end": "633480"
  },
  {
    "text": "complicated stack why not just have a ji",
    "start": "633480",
    "end": "635580"
  },
  {
    "text": "model that take a text and predict these",
    "start": "635580",
    "end": "637800"
  },
  {
    "text": "keys and pointers and I don't have to go",
    "start": "637800",
    "end": "639420"
  },
  {
    "text": "and store the embeddings and do away",
    "start": "639420",
    "end": "641220"
  },
  {
    "text": "with like all those uh memory things",
    "start": "641220",
    "end": "643680"
  },
  {
    "text": "right I mean I can avoid data movement I",
    "start": "643680",
    "end": "646200"
  },
  {
    "text": "can do fine tuning retraining if I can",
    "start": "646200",
    "end": "648240"
  },
  {
    "text": "do ah this ah you know this direct",
    "start": "648240",
    "end": "651959"
  },
  {
    "text": "prediction which is called learning to",
    "start": "651959",
    "end": "653459"
  },
  {
    "text": "index uh in the in the in the framework",
    "start": "653459",
    "end": "655860"
  },
  {
    "text": "so this is what we offer what because we",
    "start": "655860",
    "end": "658560"
  },
  {
    "text": "are able to do this whole entire",
    "start": "658560",
    "end": "660480"
  },
  {
    "text": "ecosystem on CPU we offer a database",
    "start": "660480",
    "end": "664620"
  },
  {
    "text": "which we call Neural DB where you can",
    "start": "664620",
    "end": "666839"
  },
  {
    "text": "insert text I mean it is like a",
    "start": "666839",
    "end": "668579"
  },
  {
    "text": "retrieval augmented generation ecosystem",
    "start": "668579",
    "end": "670260"
  },
  {
    "text": "where you can insert text the text you",
    "start": "670260",
    "end": "672779"
  },
  {
    "text": "can fine tune at will there is no data",
    "start": "672779",
    "end": "675779"
  },
  {
    "text": "movement data residency is uh is a",
    "start": "675779",
    "end": "678600"
  },
  {
    "text": "problem that we solve and production the",
    "start": "678600",
    "end": "681120"
  },
  {
    "text": "system is production ready since",
    "start": "681120",
    "end": "682680"
  },
  {
    "text": "Inception",
    "start": "682680",
    "end": "685220"
  },
  {
    "text": "we can run it at about two millisecond",
    "start": "685620",
    "end": "688140"
  },
  {
    "text": "latencies right that is the kind of ah",
    "start": "688140",
    "end": "690600"
  },
  {
    "text": "thing that we can you know achieve here",
    "start": "690600",
    "end": "692760"
  },
  {
    "text": "is a Blog that was written by Wayfair",
    "start": "692760",
    "end": "694740"
  },
  {
    "text": "about how they are using third AI in",
    "start": "694740",
    "end": "696779"
  },
  {
    "text": "their ecosystem to uh to reduce the",
    "start": "696779",
    "end": "699360"
  },
  {
    "text": "latency of their retrieval systems",
    "start": "699360",
    "end": "701940"
  },
  {
    "text": "again what is what is the effect of",
    "start": "701940",
    "end": "704040"
  },
  {
    "text": "being able to pre-drain your own gen AI",
    "start": "704040",
    "end": "706320"
  },
  {
    "text": "I think we have already seen that in the",
    "start": "706320",
    "end": "707820"
  },
  {
    "text": "keynote earlier yesterday that if you",
    "start": "707820",
    "end": "709920"
  },
  {
    "text": "pre-train even simpler or inferior",
    "start": "709920",
    "end": "712140"
  },
  {
    "text": "models if you pre-train fine tune it",
    "start": "712140",
    "end": "714240"
  },
  {
    "text": "gets better than most of the",
    "start": "714240",
    "end": "715500"
  },
  {
    "text": "sophisticated model so here is an",
    "start": "715500",
    "end": "717660"
  },
  {
    "text": "experiment that what we are showing you",
    "start": "717660",
    "end": "719040"
  },
  {
    "text": "is a retrieval augmented generation we",
    "start": "719040",
    "end": "721200"
  },
  {
    "text": "are comparing it with some of the",
    "start": "721200",
    "end": "722459"
  },
  {
    "text": "popular models like T5 open AI Ada and",
    "start": "722459",
    "end": "724740"
  },
  {
    "text": "instructor large and what we are doing",
    "start": "724740",
    "end": "726899"
  },
  {
    "text": "is we are not even pre-training any",
    "start": "726899",
    "end": "728940"
  },
  {
    "text": "third eye model we just take the third",
    "start": "728940",
    "end": "730440"
  },
  {
    "text": "eye model and only pre-train it on the",
    "start": "730440",
    "end": "732959"
  },
  {
    "text": "text that was inserted",
    "start": "732959",
    "end": "734880"
  },
  {
    "text": "so you see there is a difference here",
    "start": "734880",
    "end": "736740"
  },
  {
    "text": "like we talk a lot about pre-training",
    "start": "736740",
    "end": "738779"
  },
  {
    "text": "fine tuning but you can as well",
    "start": "738779",
    "end": "740399"
  },
  {
    "text": "pre-train on the text that you are",
    "start": "740399",
    "end": "741779"
  },
  {
    "text": "inserting it's a domain specialized",
    "start": "741779",
    "end": "743100"
  },
  {
    "text": "pre-training right what is pre-trading",
    "start": "743100",
    "end": "745260"
  },
  {
    "text": "next word prediction or some cell",
    "start": "745260",
    "end": "746820"
  },
  {
    "text": "supervised learning on a raw text",
    "start": "746820",
    "end": "749040"
  },
  {
    "text": "you can as well do it on the text that",
    "start": "749040",
    "end": "750779"
  },
  {
    "text": "you are inserting what if we just do",
    "start": "750779",
    "end": "752940"
  },
  {
    "text": "that",
    "start": "752940",
    "end": "753779"
  },
  {
    "text": "what we realize is you don't need any",
    "start": "753779",
    "end": "756899"
  },
  {
    "text": "pre-trained model you can beat the",
    "start": "756899",
    "end": "758640"
  },
  {
    "text": "accuracies of some of the popular",
    "start": "758640",
    "end": "760320"
  },
  {
    "text": "foundational models that has been",
    "start": "760320",
    "end": "761880"
  },
  {
    "text": "pre-trained for for months over like a",
    "start": "761880",
    "end": "764940"
  },
  {
    "text": "very expensive infrastructure out of the",
    "start": "764940",
    "end": "766920"
  },
  {
    "text": "box and the reason why you could do is",
    "start": "766920",
    "end": "768899"
  },
  {
    "text": "because you're doing a very domain",
    "start": "768899",
    "end": "770220"
  },
  {
    "text": "specialized thing",
    "start": "770220",
    "end": "772939"
  },
  {
    "text": "here is a here is a demo of uh you know",
    "start": "773220",
    "end": "775860"
  },
  {
    "text": "a tool that we built and again it runs",
    "start": "775860",
    "end": "778139"
  },
  {
    "text": "on Windows and uh you know like it's a",
    "start": "778139",
    "end": "780540"
  },
  {
    "text": "250 MB app it can run anywhere and it is",
    "start": "780540",
    "end": "783480"
  },
  {
    "text": "actually doing this retrieval augmented",
    "start": "783480",
    "end": "785160"
  },
  {
    "text": "generation on a desktop today we'll take",
    "start": "785160",
    "end": "787320"
  },
  {
    "text": "a look at using pocket llm powered by",
    "start": "787320",
    "end": "789660"
  },
  {
    "text": "third Ice neurob as a generative AI",
    "start": "789660",
    "end": "792240"
  },
  {
    "text": "knowledge base",
    "start": "792240",
    "end": "793560"
  },
  {
    "text": "as a case study we'll load at neural DP",
    "start": "793560",
    "end": "796019"
  },
  {
    "text": "that was trained on a University's",
    "start": "796019",
    "end": "797639"
  },
  {
    "text": "knowledge base",
    "start": "797639",
    "end": "799019"
  },
  {
    "text": "if we click on this button we can see",
    "start": "799019",
    "end": "801060"
  },
  {
    "text": "that we've inserted hundreds of",
    "start": "801060",
    "end": "802500"
  },
  {
    "text": "documents too many to go through one by",
    "start": "802500",
    "end": "804600"
  },
  {
    "text": "one",
    "start": "804600",
    "end": "805380"
  },
  {
    "text": "so we'll use this tool to answer who can",
    "start": "805380",
    "end": "808260"
  },
  {
    "text": "reject a timesheet",
    "start": "808260",
    "end": "811220"
  },
  {
    "text": "or how to onboard a student",
    "start": "811920",
    "end": "814800"
  },
  {
    "text": "these answers are generated using only",
    "start": "814800",
    "end": "817019"
  },
  {
    "text": "information from our documents since",
    "start": "817019",
    "end": "819000"
  },
  {
    "text": "it's important that our knowledge base",
    "start": "819000",
    "end": "820200"
  },
  {
    "text": "presents organization specific",
    "start": "820200",
    "end": "821820"
  },
  {
    "text": "information",
    "start": "821820",
    "end": "823320"
  },
  {
    "text": "because of this pocket llm also displays",
    "start": "823320",
    "end": "826320"
  },
  {
    "text": "the specific parts of your documents",
    "start": "826320",
    "end": "827880"
  },
  {
    "text": "that are relevant to the question right",
    "start": "827880",
    "end": "829680"
  },
  {
    "text": "under the generated answer so you can go",
    "start": "829680",
    "end": "831899"
  },
  {
    "text": "ahead and read the original documents",
    "start": "831899",
    "end": "833339"
  },
  {
    "text": "too",
    "start": "833339",
    "end": "834839"
  },
  {
    "text": "now no AI is perfect and sometimes it",
    "start": "834839",
    "end": "837180"
  },
  {
    "text": "fails to understand say how to motivate",
    "start": "837180",
    "end": "839700"
  },
  {
    "text": "a faculty member to stay",
    "start": "839700",
    "end": "841440"
  },
  {
    "text": "in this case it does not hallucinate and",
    "start": "841440",
    "end": "844260"
  },
  {
    "text": "instead tells you that it doesn't know",
    "start": "844260",
    "end": "845579"
  },
  {
    "text": "the answer it's a difficult question",
    "start": "845579",
    "end": "847380"
  },
  {
    "text": "after all however if you are a knowledge",
    "start": "847380",
    "end": "849959"
  },
  {
    "text": "base administrator and you've heard of",
    "start": "849959",
    "end": "851579"
  },
  {
    "text": "Faculty retention funds you can give a",
    "start": "851579",
    "end": "853560"
  },
  {
    "text": "hint to the model by clicking this",
    "start": "853560",
    "end": "854880"
  },
  {
    "text": "button and associating the query with",
    "start": "854880",
    "end": "857100"
  },
  {
    "text": "faculty retention funds",
    "start": "857100",
    "end": "859139"
  },
  {
    "text": "now if you ask again faculty retention",
    "start": "859139",
    "end": "861899"
  },
  {
    "text": "fund comes second but that's still not",
    "start": "861899",
    "end": "864180"
  },
  {
    "text": "good enough we want the model to be",
    "start": "864180",
    "end": "865860"
  },
  {
    "text": "precise so let's give it more feedback",
    "start": "865860",
    "end": "868320"
  },
  {
    "text": "and ask again one more time this time",
    "start": "868320",
    "end": "871019"
  },
  {
    "text": "the model gets it right this is a model",
    "start": "871019",
    "end": "873779"
  },
  {
    "text": "that listens to you",
    "start": "873779",
    "end": "875339"
  },
  {
    "text": "but what if the answer is not in this",
    "start": "875339",
    "end": "876899"
  },
  {
    "text": "knowledge base at all maybe it's in a",
    "start": "876899",
    "end": "878459"
  },
  {
    "text": "new document",
    "start": "878459",
    "end": "879660"
  },
  {
    "text": "with neural DB you don't have to build a",
    "start": "879660",
    "end": "881880"
  },
  {
    "text": "completely new model just to add new",
    "start": "881880",
    "end": "883620"
  },
  {
    "text": "information all you have to do is click",
    "start": "883620",
    "end": "885899"
  },
  {
    "text": "this button and add your new documents",
    "start": "885899",
    "end": "889639"
  },
  {
    "text": "and just like that your model can now",
    "start": "890519",
    "end": "892380"
  },
  {
    "text": "generate answers using those resources",
    "start": "892380",
    "end": "894300"
  },
  {
    "text": "too",
    "start": "894300",
    "end": "895980"
  },
  {
    "text": "when you're done making changes to the",
    "start": "895980",
    "end": "897600"
  },
  {
    "text": "model you can save it to your computer",
    "start": "897600",
    "end": "899820"
  },
  {
    "text": "in fact you may remember that we also",
    "start": "899820",
    "end": "901860"
  },
  {
    "text": "loaded this model from a local file and",
    "start": "901860",
    "end": "904079"
  },
  {
    "text": "since neural DB runs completely offline",
    "start": "904079",
    "end": "906000"
  },
  {
    "text": "on your local machine your documents and",
    "start": "906000",
    "end": "908639"
  },
  {
    "text": "queries stay with you as they should",
    "start": "908639",
    "end": "911820"
  },
  {
    "text": "so in this demo what you are seeing is a",
    "start": "911820",
    "end": "914100"
  },
  {
    "text": "backend API neural DB in action in a UI",
    "start": "914100",
    "end": "916560"
  },
  {
    "text": "based demo there we uploaded more than",
    "start": "916560",
    "end": "919800"
  },
  {
    "text": "thousands of PDF it was trained on a",
    "start": "919800",
    "end": "923279"
  },
  {
    "text": "sapphire Rapids or a you know a desktop",
    "start": "923279",
    "end": "925560"
  },
  {
    "text": "box in about 20 minutes",
    "start": "925560",
    "end": "928079"
  },
  {
    "text": "then you go and query it what is",
    "start": "928079",
    "end": "930360"
  },
  {
    "text": "interesting here is if you don't like",
    "start": "930360",
    "end": "932100"
  },
  {
    "text": "the retrieval you can fix it on fly you",
    "start": "932100",
    "end": "934740"
  },
  {
    "text": "can either teach it you can do",
    "start": "934740",
    "end": "936180"
  },
  {
    "text": "reinforcement learning and the model",
    "start": "936180",
    "end": "937500"
  },
  {
    "text": "updates on the Fly",
    "start": "937500",
    "end": "939180"
  },
  {
    "text": "this is kind of a system which we",
    "start": "939180",
    "end": "941579"
  },
  {
    "text": "believe over a period of time it will",
    "start": "941579",
    "end": "943920"
  },
  {
    "text": "get better so our vision for this gen AI",
    "start": "943920",
    "end": "947760"
  },
  {
    "text": "is the is is the entire ecosystem where",
    "start": "947760",
    "end": "950760"
  },
  {
    "text": "you are utilizing all your data center",
    "start": "950760",
    "end": "952680"
  },
  {
    "text": "CPUs to pre-train a large model like you",
    "start": "952680",
    "end": "956519"
  },
  {
    "text": "know uh you know a gen AI model or an",
    "start": "956519",
    "end": "958620"
  },
  {
    "text": "embedding model then you load this model",
    "start": "958620",
    "end": "960540"
  },
  {
    "text": "on a local device all under the 30i",
    "start": "960540",
    "end": "963540"
  },
  {
    "text": "stack then you can play with the model",
    "start": "963540",
    "end": "966720"
  },
  {
    "text": "if you don't like you teach it so",
    "start": "966720",
    "end": "968760"
  },
  {
    "text": "essentially when you deploy this model",
    "start": "968760",
    "end": "970199"
  },
  {
    "text": "maybe the model is 70 good or the",
    "start": "970199",
    "end": "972420"
  },
  {
    "text": "application is only satisfying your need",
    "start": "972420",
    "end": "974220"
  },
  {
    "text": "by 70 80 percent as you grow the model",
    "start": "974220",
    "end": "977519"
  },
  {
    "text": "grows with you because as you use it you",
    "start": "977519",
    "end": "979740"
  },
  {
    "text": "give feedbacks the model grows better",
    "start": "979740",
    "end": "981720"
  },
  {
    "text": "and better and become useful we believe",
    "start": "981720",
    "end": "984300"
  },
  {
    "text": "this is a much better picture of how gen",
    "start": "984300",
    "end": "987420"
  },
  {
    "text": "AI how Enterprises would want to take",
    "start": "987420",
    "end": "989339"
  },
  {
    "text": "gen AI in in their in their ecosystem",
    "start": "989339",
    "end": "991680"
  },
  {
    "text": "and then deploy it in production",
    "start": "991680",
    "end": "994440"
  },
  {
    "text": "so that is basically what the kind of",
    "start": "994440",
    "end": "996360"
  },
  {
    "text": "stuff we do",
    "start": "996360",
    "end": "997680"
  },
  {
    "text": "now comes to the technical aspect of",
    "start": "997680",
    "end": "999660"
  },
  {
    "text": "things right I mean how do we scale up",
    "start": "999660",
    "end": "1001220"
  },
  {
    "text": "sparse trading on CPUs so again like as",
    "start": "1001220",
    "end": "1003860"
  },
  {
    "text": "as mentioned before we are talking about",
    "start": "1003860",
    "end": "1005300"
  },
  {
    "text": "training billions of parameter models on",
    "start": "1005300",
    "end": "1007339"
  },
  {
    "text": "hopefully terabytes of text",
    "start": "1007339",
    "end": "1009680"
  },
  {
    "text": "so first few things about training on",
    "start": "1009680",
    "end": "1011540"
  },
  {
    "text": "CPUs which people are not usually used",
    "start": "1011540",
    "end": "1013459"
  },
  {
    "text": "to CPU is completely eliminate more or",
    "start": "1013459",
    "end": "1016459"
  },
  {
    "text": "less the need for model parallel",
    "start": "1016459",
    "end": "1018380"
  },
  {
    "text": "training",
    "start": "1018380",
    "end": "1019339"
  },
  {
    "text": "right I mean you can assemble a six",
    "start": "1019339",
    "end": "1021019"
  },
  {
    "text": "terabyte CPU that I was trying to do",
    "start": "1021019",
    "end": "1023300"
  },
  {
    "text": "even in 2018. that can even at that time",
    "start": "1023300",
    "end": "1026360"
  },
  {
    "text": "would fit seven gpts in one machine",
    "start": "1026360",
    "end": "1029660"
  },
  {
    "text": "so we don't have to worry about model",
    "start": "1029660",
    "end": "1031160"
  },
  {
    "text": "parallel training because model will",
    "start": "1031160",
    "end": "1032839"
  },
  {
    "text": "always fit",
    "start": "1032839",
    "end": "1034160"
  },
  {
    "text": "your your node",
    "start": "1034160",
    "end": "1035900"
  },
  {
    "text": "now the question is we only need data",
    "start": "1035900",
    "end": "1038058"
  },
  {
    "text": "parallel training and we need data",
    "start": "1038059",
    "end": "1039500"
  },
  {
    "text": "parallel training because you have to",
    "start": "1039500",
    "end": "1041058"
  },
  {
    "text": "ingest data at a very fast rate right",
    "start": "1041059",
    "end": "1042740"
  },
  {
    "text": "there is a lot of data and so there are",
    "start": "1042740",
    "end": "1045558"
  },
  {
    "text": "two bottlenecks here obviously one is",
    "start": "1045559",
    "end": "1047120"
  },
  {
    "text": "the communication and the other is data",
    "start": "1047120",
    "end": "1050000"
  },
  {
    "text": "parallel training you need to be able to",
    "start": "1050000",
    "end": "1051559"
  },
  {
    "text": "do like ingest data in parallel and",
    "start": "1051559",
    "end": "1054500"
  },
  {
    "text": "aggregate the models aggregation of the",
    "start": "1054500",
    "end": "1056660"
  },
  {
    "text": "models means you need communication the",
    "start": "1056660",
    "end": "1058640"
  },
  {
    "text": "good news here is if you are doing",
    "start": "1058640",
    "end": "1060080"
  },
  {
    "text": "Dynamic sparse training like I mentioned",
    "start": "1060080",
    "end": "1062240"
  },
  {
    "text": "you before most of the gradients could",
    "start": "1062240",
    "end": "1064460"
  },
  {
    "text": "be sparse and easily compressible so we",
    "start": "1064460",
    "end": "1066919"
  },
  {
    "text": "have that in in our in our routine the",
    "start": "1066919",
    "end": "1070160"
  },
  {
    "text": "other interesting part which people have",
    "start": "1070160",
    "end": "1072320"
  },
  {
    "text": "not thought about much is if you are",
    "start": "1072320",
    "end": "1074240"
  },
  {
    "text": "doing a sparse training you can increase",
    "start": "1074240",
    "end": "1076340"
  },
  {
    "text": "your batch size significantly",
    "start": "1076340",
    "end": "1078620"
  },
  {
    "text": "right so if every parameter is not",
    "start": "1078620",
    "end": "1080900"
  },
  {
    "text": "affected by all the data set in the",
    "start": "1080900",
    "end": "1083059"
  },
  {
    "text": "batch",
    "start": "1083059",
    "end": "1083900"
  },
  {
    "text": "right so imagine if I have a billion",
    "start": "1083900",
    "end": "1085520"
  },
  {
    "text": "parameter and I am running it at one",
    "start": "1085520",
    "end": "1087919"
  },
  {
    "text": "percent sparse that means 99 of the data",
    "start": "1087919",
    "end": "1091700"
  },
  {
    "text": "is not touching every parameter that",
    "start": "1091700",
    "end": "1093980"
  },
  {
    "text": "means you can increase your batch size",
    "start": "1093980",
    "end": "1095539"
  },
  {
    "text": "by 100 x",
    "start": "1095539",
    "end": "1097340"
  },
  {
    "text": "without affecting uh the performance in",
    "start": "1097340",
    "end": "1099799"
  },
  {
    "text": "fact you will get better performance and",
    "start": "1099799",
    "end": "1101059"
  },
  {
    "text": "larger batch size is great for that data",
    "start": "1101059",
    "end": "1103520"
  },
  {
    "text": "parallel training so we have all the",
    "start": "1103520",
    "end": "1106280"
  },
  {
    "text": "grounds to to make this successful but",
    "start": "1106280",
    "end": "1109100"
  },
  {
    "text": "what we lack was a data parallel",
    "start": "1109100",
    "end": "1111020"
  },
  {
    "text": "framework which is Fault tolerant",
    "start": "1111020",
    "end": "1113020"
  },
  {
    "text": "performant Auto scaling and push button",
    "start": "1113020",
    "end": "1115760"
  },
  {
    "text": "and this is where we use Ray in fact it",
    "start": "1115760",
    "end": "1118460"
  },
  {
    "text": "was recommended To Us by some of our",
    "start": "1118460",
    "end": "1120020"
  },
  {
    "text": "VMware Partners where they told us to",
    "start": "1120020",
    "end": "1122419"
  },
  {
    "text": "start thinking about Ray we were using",
    "start": "1122419",
    "end": "1124039"
  },
  {
    "text": "ah you know we were using before MPI uh",
    "start": "1124039",
    "end": "1128179"
  },
  {
    "text": "you know and and other Alternatives but",
    "start": "1128179",
    "end": "1130160"
  },
  {
    "text": "we realized that somebody has already",
    "start": "1130160",
    "end": "1132380"
  },
  {
    "text": "solved this problem so this is basically",
    "start": "1132380",
    "end": "1134299"
  },
  {
    "text": "comes in y third Ai and Ray so we",
    "start": "1134299",
    "end": "1137480"
  },
  {
    "text": "adopted Ray core for data parallel",
    "start": "1137480",
    "end": "1139280"
  },
  {
    "text": "training after evaluating Alternatives",
    "start": "1139280",
    "end": "1140960"
  },
  {
    "text": "ah the key features that we were using",
    "start": "1140960",
    "end": "1143600"
  },
  {
    "text": "that was back when we started this in",
    "start": "1143600",
    "end": "1145340"
  },
  {
    "text": "2022 we were using actor Model Auto",
    "start": "1145340",
    "end": "1147980"
  },
  {
    "text": "scaling and glue integration we were",
    "start": "1147980",
    "end": "1149660"
  },
  {
    "text": "relying on that a lot and we were",
    "start": "1149660",
    "end": "1151820"
  },
  {
    "text": "actually surprised by the rapid",
    "start": "1151820",
    "end": "1153440"
  },
  {
    "text": "development of this data parallel system",
    "start": "1153440",
    "end": "1155140"
  },
  {
    "text": "ah then we shifted to Ray trainer in",
    "start": "1155140",
    "end": "1157880"
  },
  {
    "text": "2023 because we wanted to take ah you",
    "start": "1157880",
    "end": "1161000"
  },
  {
    "text": "know more simplified create more",
    "start": "1161000",
    "end": "1162919"
  },
  {
    "text": "simplified developer experience and also",
    "start": "1162919",
    "end": "1164360"
  },
  {
    "text": "there where enhance fault tolerance and",
    "start": "1164360",
    "end": "1166520"
  },
  {
    "text": "broader integration capabilities that we",
    "start": "1166520",
    "end": "1168140"
  },
  {
    "text": "could ah take care of",
    "start": "1168140",
    "end": "1170600"
  },
  {
    "text": "so how third eye uses Ray right well we",
    "start": "1170600",
    "end": "1173360"
  },
  {
    "text": "have created what we call bold trainer",
    "start": "1173360",
    "end": "1175400"
  },
  {
    "text": "which is ah where our distributed data",
    "start": "1175400",
    "end": "1178940"
  },
  {
    "text": "panel distributed data parallel training",
    "start": "1178940",
    "end": "1181220"
  },
  {
    "text": "happens and it is built on raised data",
    "start": "1181220",
    "end": "1184100"
  },
  {
    "text": "parallel trainer similar to Ray straw",
    "start": "1184100",
    "end": "1185960"
  },
  {
    "text": "strainer the experience is quite",
    "start": "1185960",
    "end": "1187940"
  },
  {
    "text": "simplified I will show you a snapshot of",
    "start": "1187940",
    "end": "1189679"
  },
  {
    "text": "a code we also realize that the memory",
    "start": "1189679",
    "end": "1192679"
  },
  {
    "text": "usage were reduced by 2X and you know",
    "start": "1192679",
    "end": "1196220"
  },
  {
    "text": "there were option to use more economic",
    "start": "1196220",
    "end": "1198320"
  },
  {
    "text": "CP economical CPU instances",
    "start": "1198320",
    "end": "1202220"
  },
  {
    "text": "here is actually a code of how you can",
    "start": "1202220",
    "end": "1204380"
  },
  {
    "text": "even like this is the code where we use",
    "start": "1204380",
    "end": "1206179"
  },
  {
    "text": "uh multiple CPUs to train a GPT kind of",
    "start": "1206179",
    "end": "1209120"
  },
  {
    "text": "a generative model uh all you have to do",
    "start": "1209120",
    "end": "1212000"
  },
  {
    "text": "is to you initialize this bold trainer",
    "start": "1212000",
    "end": "1213799"
  },
  {
    "text": "and in fact if you are seeing if you can",
    "start": "1213799",
    "end": "1216020"
  },
  {
    "text": "still visualize there is a scaling",
    "start": "1216020",
    "end": "1217580"
  },
  {
    "text": "config which is a typical rescaling",
    "start": "1217580",
    "end": "1219679"
  },
  {
    "text": "config and you need like a train Loop",
    "start": "1219679",
    "end": "1221539"
  },
  {
    "text": "per worker and if you write it with",
    "start": "1221539",
    "end": "1224360"
  },
  {
    "text": "third Ai and this Ray integration this",
    "start": "1224360",
    "end": "1226880"
  },
  {
    "text": "is the kind of code less than a page of",
    "start": "1226880",
    "end": "1228740"
  },
  {
    "text": "a code where you can actually run a job",
    "start": "1228740",
    "end": "1230780"
  },
  {
    "text": "on as many CPUs as you want this is how",
    "start": "1230780",
    "end": "1233120"
  },
  {
    "text": "easy it is uh against talking about some",
    "start": "1233120",
    "end": "1236539"
  },
  {
    "text": "experimental evaluation so we ah",
    "start": "1236539",
    "end": "1239000"
  },
  {
    "text": "evaluated this on critio terabyte",
    "start": "1239000",
    "end": "1241460"
  },
  {
    "text": "Benchmark which is like one of the",
    "start": "1241460",
    "end": "1242960"
  },
  {
    "text": "popular ah click through prediction",
    "start": "1242960",
    "end": "1244940"
  },
  {
    "text": "Benchmark it is the largest public data",
    "start": "1244940",
    "end": "1247400"
  },
  {
    "text": "set that is available for uh for",
    "start": "1247400",
    "end": "1249380"
  },
  {
    "text": "experimentation it has about a terabytes",
    "start": "1249380",
    "end": "1251360"
  },
  {
    "text": "of data for click through data and the",
    "start": "1251360",
    "end": "1254240"
  },
  {
    "text": "task is obviously click prediction or",
    "start": "1254240",
    "end": "1256039"
  },
  {
    "text": "not here what we are showing is we took",
    "start": "1256039",
    "end": "1258200"
  },
  {
    "text": "a million sized model and again with",
    "start": "1258200",
    "end": "1260299"
  },
  {
    "text": "different compressions so as I mentioned",
    "start": "1260299",
    "end": "1262039"
  },
  {
    "text": "about out that ah you know because of",
    "start": "1262039",
    "end": "1264140"
  },
  {
    "text": "our training being sparse we can",
    "start": "1264140",
    "end": "1265820"
  },
  {
    "text": "compress the gradients and communication",
    "start": "1265820",
    "end": "1267260"
  },
  {
    "text": "time so these are like 50 million Model",
    "start": "1267260",
    "end": "1269539"
  },
  {
    "text": "25 million model 37.5 million model ah",
    "start": "1269539",
    "end": "1272840"
  },
  {
    "text": "what you are looking on the x axis is",
    "start": "1272840",
    "end": "1274340"
  },
  {
    "text": "the number of nodes there is 12 nodes 24",
    "start": "1274340",
    "end": "1276140"
  },
  {
    "text": "nodes 48 nodes so these are nodes of",
    "start": "1276140",
    "end": "1279200"
  },
  {
    "text": "CPUs interconnected with melanox 100",
    "start": "1279200",
    "end": "1282260"
  },
  {
    "text": "gigs 10 gigs or or you know around that",
    "start": "1282260",
    "end": "1284960"
  },
  {
    "text": "I think this experiment was done with 10",
    "start": "1284960",
    "end": "1287179"
  },
  {
    "text": "gigs what we see here is this is the",
    "start": "1287179",
    "end": "1291260"
  },
  {
    "text": "time and as you can as you see the time",
    "start": "1291260",
    "end": "1293600"
  },
  {
    "text": "increases kind of linearly with the",
    "start": "1293600",
    "end": "1295580"
  },
  {
    "text": "model size which is expected but it also",
    "start": "1295580",
    "end": "1297679"
  },
  {
    "text": "kind of goes down in almost a linear",
    "start": "1297679",
    "end": "1300740"
  },
  {
    "text": "fashion as you start increasing the",
    "start": "1300740",
    "end": "1302179"
  },
  {
    "text": "number of nodes",
    "start": "1302179",
    "end": "1304460"
  },
  {
    "text": "so here is the same Benchmark but now on",
    "start": "1304460",
    "end": "1307280"
  },
  {
    "text": "a billion parameter model so previously",
    "start": "1307280",
    "end": "1309020"
  },
  {
    "text": "there was a multiple million parameter",
    "start": "1309020",
    "end": "1310580"
  },
  {
    "text": "model because we wanted to test how the",
    "start": "1310580",
    "end": "1312200"
  },
  {
    "text": "model behaves as you increase the number",
    "start": "1312200",
    "end": "1313820"
  },
  {
    "text": "of parameters now we took a billion",
    "start": "1313820",
    "end": "1315799"
  },
  {
    "text": "parameter model we see same effect right",
    "start": "1315799",
    "end": "1318380"
  },
  {
    "text": "I mean we have 12 nodes 24 nodes and 48",
    "start": "1318380",
    "end": "1320960"
  },
  {
    "text": "nodes and this is basically allowing us",
    "start": "1320960",
    "end": "1323539"
  },
  {
    "text": "to ah take this model and train it",
    "start": "1323539",
    "end": "1327080"
  },
  {
    "text": "distribute it all using Ray framework",
    "start": "1327080",
    "end": "1329659"
  },
  {
    "text": "and all you have to do is to write a",
    "start": "1329659",
    "end": "1331400"
  },
  {
    "text": "page of python code initialize array",
    "start": "1331400",
    "end": "1333559"
  },
  {
    "text": "cluster",
    "start": "1333559",
    "end": "1334659"
  },
  {
    "text": "put 30i on all of it and get going",
    "start": "1334659",
    "end": "1338240"
  },
  {
    "text": "again there is a whole bunch of details",
    "start": "1338240",
    "end": "1340640"
  },
  {
    "text": "that we have special like have we have",
    "start": "1340640",
    "end": "1342380"
  },
  {
    "text": "uh written down on a Blog with uh with",
    "start": "1342380",
    "end": "1345020"
  },
  {
    "text": "any scale so please uh read this blog",
    "start": "1345020",
    "end": "1347299"
  },
  {
    "text": "about 30i training and this is where I",
    "start": "1347299",
    "end": "1351080"
  },
  {
    "text": "would like to end with and we have time",
    "start": "1351080",
    "end": "1353360"
  },
  {
    "text": "for questions uh I would say that we",
    "start": "1353360",
    "end": "1355580"
  },
  {
    "text": "could utilize all your data center",
    "start": "1355580",
    "end": "1357860"
  },
  {
    "text": "capabilities to train AI generative Ai",
    "start": "1357860",
    "end": "1360740"
  },
  {
    "text": "and we'll be happy to Showcase you how",
    "start": "1360740",
    "end": "1363559"
  },
  {
    "text": "we can do it",
    "start": "1363559",
    "end": "1364880"
  },
  {
    "text": "thank you very much",
    "start": "1364880",
    "end": "1367780"
  },
  {
    "text": "[Applause]",
    "start": "1368040",
    "end": "1373100"
  },
  {
    "text": "yep",
    "start": "1373100",
    "end": "1375580"
  },
  {
    "text": "in the example that you showed at the",
    "start": "1377840",
    "end": "1379880"
  },
  {
    "text": "beginning how many CPUs we're using to",
    "start": "1379880",
    "end": "1382460"
  },
  {
    "text": "train uh you know the uh language model",
    "start": "1382460",
    "end": "1386059"
  },
  {
    "text": "so we were using tensor fire Rapids so",
    "start": "1386059",
    "end": "1389000"
  },
  {
    "text": "this is like the latest generation Intel",
    "start": "1389000",
    "end": "1391280"
  },
  {
    "text": "Safar Rapids with like about 100 200",
    "start": "1391280",
    "end": "1393380"
  },
  {
    "text": "cores each machine it's a dual socket so",
    "start": "1393380",
    "end": "1396080"
  },
  {
    "text": "it's 56 core each socket so 10 Duo",
    "start": "1396080",
    "end": "1398659"
  },
  {
    "text": "sockets of fire Rapids with 100 gigs",
    "start": "1398659",
    "end": "1400700"
  },
  {
    "text": "melanox connected and then it was",
    "start": "1400700",
    "end": "1403400"
  },
  {
    "text": "initialized on Ray core",
    "start": "1403400",
    "end": "1406539"
  },
  {
    "text": "so you show this with the uh the bolt AI",
    "start": "1412820",
    "end": "1415460"
  },
  {
    "text": "model does this work with does dynamic",
    "start": "1415460",
    "end": "1417260"
  },
  {
    "text": "sparsity work with any model let's say",
    "start": "1417260",
    "end": "1419120"
  },
  {
    "text": "they've developed a model in my lab can",
    "start": "1419120",
    "end": "1420740"
  },
  {
    "text": "I use the underlying infrastructure to",
    "start": "1420740",
    "end": "1422360"
  },
  {
    "text": "get the same benefits that's a great",
    "start": "1422360",
    "end": "1424280"
  },
  {
    "text": "question so generally the way this so we",
    "start": "1424280",
    "end": "1426919"
  },
  {
    "text": "have a very different approach of making",
    "start": "1426919",
    "end": "1428840"
  },
  {
    "text": "the models work right so right now if",
    "start": "1428840",
    "end": "1431000"
  },
  {
    "text": "there is a foundation model we can",
    "start": "1431000",
    "end": "1432380"
  },
  {
    "text": "distill it in our framework so our",
    "start": "1432380",
    "end": "1433880"
  },
  {
    "text": "framework uh basically initializes the",
    "start": "1433880",
    "end": "1436520"
  },
  {
    "text": "model and trains it the reason why we",
    "start": "1436520",
    "end": "1438500"
  },
  {
    "text": "get all the benefits and again like",
    "start": "1438500",
    "end": "1440059"
  },
  {
    "text": "inference with this model and fine",
    "start": "1440059",
    "end": "1441500"
  },
  {
    "text": "tuning is very fast because these models",
    "start": "1441500",
    "end": "1443600"
  },
  {
    "text": "are made for CPUs so all we can do is if",
    "start": "1443600",
    "end": "1446179"
  },
  {
    "text": "you let's say you have a Target Model in",
    "start": "1446179",
    "end": "1447679"
  },
  {
    "text": "mind we can distill it to create an",
    "start": "1447679",
    "end": "1449659"
  },
  {
    "text": "equivalent if that is what we have to do",
    "start": "1449659",
    "end": "1452900"
  },
  {
    "text": "but essentially if you think about it",
    "start": "1452900",
    "end": "1454820"
  },
  {
    "text": "any more any foundation model",
    "start": "1454820",
    "end": "1456860"
  },
  {
    "text": "is pre-trained on some data set we can",
    "start": "1456860",
    "end": "1458900"
  },
  {
    "text": "as well pre-train on the same see I mean",
    "start": "1458900",
    "end": "1460820"
  },
  {
    "text": "again like this is also like a change of",
    "start": "1460820",
    "end": "1462919"
  },
  {
    "text": "perception that we want to happen is",
    "start": "1462919",
    "end": "1465440"
  },
  {
    "text": "right now the world is believing look",
    "start": "1465440",
    "end": "1466820"
  },
  {
    "text": "fine tuning or fine tuning maybe it's",
    "start": "1466820",
    "end": "1469100"
  },
  {
    "text": "okay inference is easy fine tuning is",
    "start": "1469100",
    "end": "1471860"
  },
  {
    "text": "occasion pre-training is hard but what",
    "start": "1471860",
    "end": "1474500"
  },
  {
    "text": "if you can pre-train at well right",
    "start": "1474500",
    "end": "1476539"
  },
  {
    "text": "because what I'm talking about is tens",
    "start": "1476539",
    "end": "1478460"
  },
  {
    "text": "of fire Rapids or your data center",
    "start": "1478460",
    "end": "1480020"
  },
  {
    "text": "cluster you are training a job overnight",
    "start": "1480020",
    "end": "1482299"
  },
  {
    "text": "and and that it's done it's pre-trained",
    "start": "1482299",
    "end": "1484640"
  },
  {
    "text": "so if pre-training is easy you would be",
    "start": "1484640",
    "end": "1486980"
  },
  {
    "text": "doing it more and more often more and",
    "start": "1486980",
    "end": "1489080"
  },
  {
    "text": "more targeted pre-training more and more",
    "start": "1489080",
    "end": "1491059"
  },
  {
    "text": "targeted fine tuning and we believe that",
    "start": "1491059",
    "end": "1493760"
  },
  {
    "text": "that can happen if we start to leverage",
    "start": "1493760",
    "end": "1496220"
  },
  {
    "text": "all the cores of CPUs because right now",
    "start": "1496220",
    "end": "1498020"
  },
  {
    "text": "with GPU shortage the amount of GPU",
    "start": "1498020",
    "end": "1500179"
  },
  {
    "text": "compute and the cost that is needed to",
    "start": "1500179",
    "end": "1501860"
  },
  {
    "text": "build these models it becomes like okay",
    "start": "1501860",
    "end": "1503900"
  },
  {
    "text": "if somebody has done it let's not ah you",
    "start": "1503900",
    "end": "1506780"
  },
  {
    "text": "know burn the same amount of money to",
    "start": "1506780",
    "end": "1508400"
  },
  {
    "text": "get something that is available or even",
    "start": "1508400",
    "end": "1510559"
  },
  {
    "text": "if it's remotely available will still",
    "start": "1510559",
    "end": "1512720"
  },
  {
    "text": "reuse it but why reuse it when you can",
    "start": "1512720",
    "end": "1515179"
  },
  {
    "text": "build it essentially in no cost then",
    "start": "1515179",
    "end": "1517520"
  },
  {
    "text": "this is what our stuff is yeah so in the",
    "start": "1517520",
    "end": "1521600"
  },
  {
    "text": "last page on the experimental evaluation",
    "start": "1521600",
    "end": "1525080"
  },
  {
    "text": "I suppose a number of nodes",
    "start": "1525080",
    "end": "1528740"
  },
  {
    "text": "yeah doubled but the training time only",
    "start": "1528740",
    "end": "1531140"
  },
  {
    "text": "improved a little bit yeah I'm happy",
    "start": "1531140",
    "end": "1535159"
  },
  {
    "text": "so this is uh obviously so this was on a",
    "start": "1535159",
    "end": "1537860"
  },
  {
    "text": "uh you know an interconnect and this was",
    "start": "1537860",
    "end": "1540799"
  },
  {
    "text": "a billion parameter model so billion",
    "start": "1540799",
    "end": "1542840"
  },
  {
    "text": "parameter model when you do the",
    "start": "1542840",
    "end": "1544039"
  },
  {
    "text": "communication it is communication bound",
    "start": "1544039",
    "end": "1545539"
  },
  {
    "text": "the thing is even if you go from 24 to",
    "start": "1545539",
    "end": "1548480"
  },
  {
    "text": "48 we were actually hoping that maybe it",
    "start": "1548480",
    "end": "1550520"
  },
  {
    "text": "will not go down see this is not like a",
    "start": "1550520",
    "end": "1552740"
  },
  {
    "text": "large language model kind of a workload",
    "start": "1552740",
    "end": "1554240"
  },
  {
    "text": "but this is like a terabyte workload",
    "start": "1554240",
    "end": "1556220"
  },
  {
    "text": "with a billion parameter model so the",
    "start": "1556220",
    "end": "1558980"
  },
  {
    "text": "communication is still a barrier but",
    "start": "1558980",
    "end": "1561260"
  },
  {
    "text": "even with when we double the node size",
    "start": "1561260",
    "end": "1563059"
  },
  {
    "text": "we still get down on the training time",
    "start": "1563059",
    "end": "1566000"
  },
  {
    "text": "which was uh which was interesting all",
    "start": "1566000",
    "end": "1568520"
  },
  {
    "text": "right if there's no more questions uh",
    "start": "1568520",
    "end": "1570140"
  },
  {
    "text": "let's thank our speaker again",
    "start": "1570140",
    "end": "1573220"
  }
]