[
  {
    "text": "okay hello everyone thank you for coming to the summit and thank you for coming to my talk uh I'll be showcasing our",
    "start": "3140",
    "end": "10500"
  },
  {
    "text": "integration of hugging face with Ray air",
    "start": "10500",
    "end": "15619"
  },
  {
    "text": "so before we begin uh just quickly about myself so who am I I'm Anthony I'm a",
    "start": "15619",
    "end": "21420"
  },
  {
    "text": "software engineer at any scale I've been on the library's team for a little over a year now the library's team is working",
    "start": "21420",
    "end": "28980"
  },
  {
    "text": "mostly on Ray AI runtime now which means train tune all the different ecosystem",
    "start": "28980",
    "end": "34559"
  },
  {
    "text": "Integrations and in my spare time I'm maintaining several open source projects",
    "start": "34559",
    "end": "40559"
  },
  {
    "text": "so you probably heard about any scale but just to briefly set some context",
    "start": "40559",
    "end": "45600"
  },
  {
    "text": "here so we at any scale are original creators of Rey a unified framework for",
    "start": "45600",
    "end": "50879"
  },
  {
    "text": "scalable Computing and what we do is we want to create a scalable compute for AI and Python and we do it because scaling",
    "start": "50879",
    "end": "58739"
  },
  {
    "text": "is hard and we want to make it as easy as simple for everyone and we hope that",
    "start": "58739",
    "end": "65640"
  },
  {
    "text": "this hugging phase integration is going to be an example of that",
    "start": "65640",
    "end": "71240"
  },
  {
    "text": "so an agenda for today's talk is that I'm going to quickly introduce hugging face as a library",
    "start": "71340",
    "end": "77540"
  },
  {
    "text": "uh I will showcase the state of distributed training and why this unnecessity",
    "start": "77540",
    "end": "84320"
  },
  {
    "text": "briefly show how Ray Eric and superpower hugging face to easily run distributed",
    "start": "84320",
    "end": "90420"
  },
  {
    "text": "training on multiple gpus on a cluster do a deep dive into the internals and",
    "start": "90420",
    "end": "96900"
  },
  {
    "text": "API of our array AI runtime hugging phase trainer and we'll end with a quick",
    "start": "96900",
    "end": "101939"
  },
  {
    "text": "demo so yeah the state of machine learning as",
    "start": "101939",
    "end": "107280"
  },
  {
    "text": "we all know is that at this point deep learning is state of the art and we have been seeing some really impressive",
    "start": "107280",
    "end": "113640"
  },
  {
    "text": "foundational models so for example gpt3 so those are very large models with billions of parameters that can do some",
    "start": "113640",
    "end": "121200"
  },
  {
    "text": "quite incredible things but not only models are getting bigger the data is also growing bigger and",
    "start": "121200",
    "end": "127500"
  },
  {
    "text": "bigger which means we need more memory we need we need more compute and ml is pervasive across all domains",
    "start": "127500",
    "end": "135980"
  },
  {
    "text": "so you can see I use for machine learning in finance in Biochemistry in",
    "start": "135980",
    "end": "142080"
  },
  {
    "text": "science uh and there are multiple uses for it",
    "start": "142080",
    "end": "149360"
  },
  {
    "text": "so a hugging phase Transformers is a library for easy use of state-of-the-art",
    "start": "151200",
    "end": "158580"
  },
  {
    "text": "machine learning it is simple robust and Powerful it is a library for python",
    "start": "158580",
    "end": "163860"
  },
  {
    "text": "which provides an opinionated high-level API wrapping around libraries such as",
    "start": "163860",
    "end": "169620"
  },
  {
    "text": "pytorch tensorflow and Jacks it is mostly focused on natural language processing though there has been work on",
    "start": "169620",
    "end": "176660"
  },
  {
    "text": "computer Vision Audio processing and other modules such as that and one main selling point in my opinion",
    "start": "176660",
    "end": "185340"
  },
  {
    "text": "is that it provides access to multiple state-of-the-art pre-trained models you can very easily obtain for example GPT",
    "start": "185340",
    "end": "192140"
  },
  {
    "text": "bird and multiple other models and you can use them in whatever way you want",
    "start": "192140",
    "end": "198659"
  },
  {
    "text": "and it has a huge community and a huge social Focus the hugging phase Hub which",
    "start": "198659",
    "end": "204120"
  },
  {
    "text": "is a service hacking phase provides allows access to multiple models data sets and you can easily download and",
    "start": "204120",
    "end": "210360"
  },
  {
    "text": "upload your own so how does hugging phase make machine learning easier so first of all it",
    "start": "210360",
    "end": "217319"
  },
  {
    "text": "abstracts many complexities of deep learning with just a few lines of code you can",
    "start": "217319",
    "end": "224760"
  },
  {
    "text": "use a pre-trained model for inference you can fine tune it you can easily get and upload models it",
    "start": "224760",
    "end": "232739"
  },
  {
    "text": "has great developer velocity thanks to that and it provides the hacking face Hub which as I mentioned is a repository",
    "start": "232739",
    "end": "238739"
  },
  {
    "text": "of models and data sets where you can obtain and share your own work",
    "start": "238739",
    "end": "245879"
  },
  {
    "text": "so here's an example of a typical hacking phase workflow so we are using the hugging phase datasets library to",
    "start": "245879",
    "end": "253019"
  },
  {
    "text": "load a data set from hugging face Hub we are splitting it into train and evaluation data set we'll be doing",
    "start": "253019",
    "end": "260519"
  },
  {
    "text": "sequence classification here so we are obtaining the bird model from The Hub we",
    "start": "260519",
    "end": "265800"
  },
  {
    "text": "specify the training arguments and then initialize the hacking phase trainer and",
    "start": "265800",
    "end": "270900"
  },
  {
    "text": "we just train the trainer so as you can see in just a couple of lines of code we can quickly obtain and fine-tune a model",
    "start": "270900",
    "end": "277979"
  },
  {
    "text": "for our use case however all those state-of-the-art",
    "start": "277979",
    "end": "283259"
  },
  {
    "text": "models they need a lot of compute so as we can see the amount of",
    "start": "283259",
    "end": "290360"
  },
  {
    "text": "parameters and the amount of memory required for modus has been growing in",
    "start": "290360",
    "end": "295440"
  },
  {
    "text": "an exponential fashion and the fact of the matter is that Moore's Law and advances in Hardware",
    "start": "295440",
    "end": "303419"
  },
  {
    "text": "have not been able to keep up with that so as Jan mentioned there is no way but",
    "start": "303419",
    "end": "309240"
  },
  {
    "text": "to go distributed we simply cannot do the training of such large models on a",
    "start": "309240",
    "end": "314940"
  },
  {
    "text": "single node so for example gpt3 if we were to train it on a single V100",
    "start": "314940",
    "end": "321380"
  },
  {
    "text": "instance that would take over 350 years so even if you have a beefin styles with",
    "start": "321380",
    "end": "327900"
  },
  {
    "text": "eight gpus that is still going to take 40 years to train so there is no way but",
    "start": "327900",
    "end": "334259"
  },
  {
    "text": "to go distributed",
    "start": "334259",
    "end": "337220"
  },
  {
    "text": "yeah okay so we go distributed but now we can see new problems so first of all",
    "start": "340320",
    "end": "347520"
  },
  {
    "text": "developer velocity drops down dramatically we have to make sure that we know how to set up our model of",
    "start": "347520",
    "end": "355380"
  },
  {
    "text": "multiple nodes that we can deal with fault tolerance that we can make sure",
    "start": "355380",
    "end": "360720"
  },
  {
    "text": "that we are uh taking care of all the infrastructure",
    "start": "360720",
    "end": "365820"
  },
  {
    "text": "problems so for example AWS this is all knowledge that wasn't necessary before those are all skills that weren't",
    "start": "365820",
    "end": "371580"
  },
  {
    "text": "necessary before when we were developing on a single node but now they are bringing our developer velocity down",
    "start": "371580",
    "end": "378360"
  },
  {
    "text": "we have to start to learn how to manage complex infrastructure so we need to for",
    "start": "378360",
    "end": "384300"
  },
  {
    "text": "example make sure that we know how to set up a on-premise cluster if you want",
    "start": "384300",
    "end": "389340"
  },
  {
    "text": "to go for the for that or how to deal with AWS gcp what are the different node types",
    "start": "389340",
    "end": "395520"
  },
  {
    "text": "and in the end even if we get our training uh to our distributed state",
    "start": "395520",
    "end": "400800"
  },
  {
    "text": "where we can scale it up it doesn't end there we have to make sure that the entire machine learning pipeline can be",
    "start": "400800",
    "end": "406860"
  },
  {
    "text": "scaled up we don't want to have any bottlenecks and we want to make sure that we are not only able to train in a",
    "start": "406860",
    "end": "413940"
  },
  {
    "text": "scalable fashion but that we can make sure that our data ingest hyper parameter tuning and serving can also be",
    "start": "413940",
    "end": "420120"
  },
  {
    "text": "scaled up so what is the solution to those problems so in our opinion the solution",
    "start": "420120",
    "end": "427979"
  },
  {
    "text": "is here Ray air which is a scalable unified toolkit for both data scientists",
    "start": "427979",
    "end": "433560"
  },
  {
    "text": "and software engineers and Ray air provides a flexible pythonic framework for each step of the ml workflow so we",
    "start": "433560",
    "end": "441419"
  },
  {
    "text": "cover all the steps from data ingest through data pre-processing with free data training with rate train tuning",
    "start": "441419",
    "end": "447900"
  },
  {
    "text": "with retune and then you can easily do inference in serving at scale all the while staying entirely within the python",
    "start": "447900",
    "end": "454919"
  },
  {
    "text": "ecosystem and being able to use all your favorite libraries including hugging phase while being distributed with Ray",
    "start": "454919",
    "end": "464120"
  },
  {
    "text": "so here's an example of how the air array air ecosystem works so as you can see",
    "start": "465599",
    "end": "471020"
  },
  {
    "text": "we built on top of record the AI runtime is built on top of Ray car and provides",
    "start": "471020",
    "end": "476819"
  },
  {
    "text": "Advanced functionality and Integrations with all the popular machine learning",
    "start": "476819",
    "end": "482039"
  },
  {
    "text": "libraries and we provide apis that allow users to",
    "start": "482039",
    "end": "487740"
  },
  {
    "text": "write their own integration with whatever library or use case they may be facing",
    "start": "487740",
    "end": "494180"
  },
  {
    "text": "so why are you using Rey in the first place so first of all Ray provides an efficient data layer and distributed",
    "start": "494220",
    "end": "500400"
  },
  {
    "text": "object star meaning you don't have to worry about how to get data from one",
    "start": "500400",
    "end": "505800"
  },
  {
    "text": "node to another it provides a fully python-based API so you do not have to leave python even for",
    "start": "505800",
    "end": "513000"
  },
  {
    "text": "a moment and it provides robust scheduling and Resource Management taking away the complexities of managing",
    "start": "513000",
    "end": "518760"
  },
  {
    "text": "infrastructure so in short rape provides the compute strata infrastructure management and all the solutions for the",
    "start": "518760",
    "end": "525540"
  },
  {
    "text": "challenges of distributed computing so now we want to focus on how to how we",
    "start": "525540",
    "end": "533700"
  },
  {
    "text": "combine hugging phase and Ray and what sort of benefits we can gain from that so let me start with a quick Showcase of",
    "start": "533700",
    "end": "540959"
  },
  {
    "text": "our Ray AI runtime hugging face trainer so trainer is a concept in Ray AI",
    "start": "540959",
    "end": "546240"
  },
  {
    "text": "runtime which wraps around different libraries and allows you to do training at scale",
    "start": "546240",
    "end": "553500"
  },
  {
    "text": "so first of all we use the existing hugging face code in a function so we are not Reinventing the",
    "start": "553500",
    "end": "559620"
  },
  {
    "text": "wheel here we take your existing hacking face code or you can take code from a hugging face example all you need to do",
    "start": "559620",
    "end": "566519"
  },
  {
    "text": "is you have to put it in a function and you have to return the trainer from hugging face",
    "start": "566519",
    "end": "573360"
  },
  {
    "text": "then we need to provide the scaling config and other array AI runtime configs as needed so scaling config",
    "start": "573360",
    "end": "579980"
  },
  {
    "text": "determines on how many machines will be Distributing our training and whether we",
    "start": "579980",
    "end": "586440"
  },
  {
    "text": "are using gpus or not so in this example we'll be using three gpus and once we",
    "start": "586440",
    "end": "592320"
  },
  {
    "text": "have array clustered and all the scheduling will be taken care of by Ray we Define our hugging phase trainer",
    "start": "592320",
    "end": "600660"
  },
  {
    "text": "using great data sets for data ingest and data pre-processing if needed so Ray",
    "start": "600660",
    "end": "606420"
  },
  {
    "text": "data sets take care of transferring the data between nodes and allowing you to",
    "start": "606420",
    "end": "612240"
  },
  {
    "text": "for example use data sets that are bigger than the memory of one node or even bigger than the memory of an entire",
    "start": "612240",
    "end": "618060"
  },
  {
    "text": "cluster and at the end we simply call fit on our trainer and we obtain a results object",
    "start": "618060",
    "end": "625440"
  },
  {
    "text": "which you can inspect get the best checkpoint from and carry on over to",
    "start": "625440",
    "end": "631100"
  },
  {
    "text": "Next Step of our machine learning pipeline",
    "start": "631100",
    "end": "636019"
  },
  {
    "text": "so how is Ray air hugging phase trainer implemented so in essence it uses",
    "start": "636360",
    "end": "643500"
  },
  {
    "text": "distributed data power training on array cluster and it takes advantage of the",
    "start": "643500",
    "end": "649019"
  },
  {
    "text": "pytorch DDP training and the fact that hugging phase is a wrapper around pytorch and other libraries but we are",
    "start": "649019",
    "end": "656100"
  },
  {
    "text": "focusing on pythons here uh it allows you to run any user-defined hugging code",
    "start": "656100",
    "end": "661700"
  },
  {
    "text": "hanging face code without any changes so you do not have to make any specific",
    "start": "661700",
    "end": "669480"
  },
  {
    "text": "changes to allow for distributed training you can take the existing code and make sure that you can run it with",
    "start": "669480",
    "end": "677339"
  },
  {
    "text": "array or trainer it automatically converts the ray data",
    "start": "677339",
    "end": "683459"
  },
  {
    "text": "sets to the format expected by hugging phase so this is also another thing that is",
    "start": "683459",
    "end": "690180"
  },
  {
    "text": "abstracted and it provides built-in logging and monitoring for example if ml",
    "start": "690180",
    "end": "695700"
  },
  {
    "text": "flow tensorboard and other libraries so we parallelize the training using",
    "start": "695700",
    "end": "703140"
  },
  {
    "text": "python GDP so what that means is that we have several workers scheduled on array",
    "start": "703140",
    "end": "709019"
  },
  {
    "text": "cluster so for example each worker may be using one GPU or multiple CPUs",
    "start": "709019",
    "end": "715800"
  },
  {
    "text": "each of the workers Gets A Shard of the data and they each train the same model",
    "start": "715800",
    "end": "723060"
  },
  {
    "text": "and the gradients are synchronized during iterations the array error trainer abstracts away",
    "start": "723060",
    "end": "730860"
  },
  {
    "text": "the infrastructure so you do not have to worry about setting up those workers yourself they are all set up",
    "start": "730860",
    "end": "736500"
  },
  {
    "text": "automatically and we support both CPU and GPU workers with TPU support incoming",
    "start": "736500",
    "end": "744420"
  },
  {
    "text": "so for data ingest we are using array data sets as a common data format this",
    "start": "746100",
    "end": "751260"
  },
  {
    "text": "is consistent with the rest of Ray R this allows us to easily read from disk",
    "start": "751260",
    "end": "757860"
  },
  {
    "text": "cloud or other formats so for example if you have a pandas data frame you can easily convert it to array data set Ray",
    "start": "757860",
    "end": "765540"
  },
  {
    "text": "data sets are fully distributed and they can even handle data that is too big to",
    "start": "765540",
    "end": "771120"
  },
  {
    "text": "fit on any single node or even on an entire cluster and they can also run",
    "start": "771120",
    "end": "777079"
  },
  {
    "text": "transformations in parallel speeding up your data preprocessing",
    "start": "777079",
    "end": "782899"
  },
  {
    "text": "so for data preprocessing we have a concept of preprocessors which we",
    "start": "783180",
    "end": "789180"
  },
  {
    "text": "provide several out of box examples for you can also write your own",
    "start": "789180",
    "end": "794480"
  },
  {
    "text": "user-defined functions for map apply and they are automatically applied during training and inference if you pass them",
    "start": "794480",
    "end": "801060"
  },
  {
    "text": "as an argument to the array error trainer and here you can see an example of a tokenizing function that is passed",
    "start": "801060",
    "end": "808800"
  },
  {
    "text": "to a ray or batch mapper and used as an argument in the hacking face trainer",
    "start": "808800",
    "end": "813899"
  },
  {
    "text": "for logging and monitoring from we have multiple callbacks you can use ml flow",
    "start": "813899",
    "end": "819500"
  },
  {
    "text": "weights and biases tensor tensorboard and other such libraries and you can",
    "start": "819500",
    "end": "825300"
  },
  {
    "text": "even write your own callbacks to even simple API that we provide and you can also inspect the result object after",
    "start": "825300",
    "end": "831360"
  },
  {
    "text": "training which contains the metrics the best model and so on",
    "start": "831360",
    "end": "836399"
  },
  {
    "text": "so checkpointing for AR is fully automatic and can be configured for",
    "start": "836399",
    "end": "841740"
  },
  {
    "text": "example only keep the best checkpoint you can resume training from the checkpoint object and this system allows",
    "start": "841740",
    "end": "848579"
  },
  {
    "text": "for spot instance usage if a node goes down the array autoscaler will",
    "start": "848579",
    "end": "854160"
  },
  {
    "text": "automatically bring it back down and the training will be resumed from the latest checkpoint automatically and checkpoints",
    "start": "854160",
    "end": "861180"
  },
  {
    "text": "can also be passed to predictors and to race serve for easy inference and",
    "start": "861180",
    "end": "867720"
  },
  {
    "text": "serving so here is the typical hugging phase training workflow as I showcased before",
    "start": "867720",
    "end": "875339"
  },
  {
    "text": "so now that we want to distribute it with Ray air those are all the changes that we have made so first we need to",
    "start": "875339",
    "end": "880860"
  },
  {
    "text": "put our model and trainer initialization logic inside a function we call it",
    "start": "880860",
    "end": "886139"
  },
  {
    "text": "trainer init per worker we instantiate the ray air hugging phase trainer",
    "start": "886139",
    "end": "893480"
  },
  {
    "text": "and we simply call fit and obtain the result subject so just to summarize we",
    "start": "893519",
    "end": "898800"
  },
  {
    "text": "are using great data sets for ingest and we are using existing hacking face code and we can easily integrate with rest of",
    "start": "898800",
    "end": "906300"
  },
  {
    "text": "Ray AI runtime so for example if you want to run a hyper parameter optimization search we can simply use",
    "start": "906300",
    "end": "913440"
  },
  {
    "text": "the ray air tuner just by passing the hugging phase trainer into it and it will run multiple trials in a",
    "start": "913440",
    "end": "919500"
  },
  {
    "text": "distributed fashion and for inference and serving we pass the checkpoint object into a hugging",
    "start": "919500",
    "end": "925860"
  },
  {
    "text": "phase predictor which uses hugging phase pipelines under the hood that means that you will get the same outputs as with",
    "start": "925860",
    "end": "933000"
  },
  {
    "text": "non-distributed hugging phase but they will be returned in array data set format so that they can be used on array",
    "start": "933000",
    "end": "938220"
  },
  {
    "text": "cluster so here is an example of calling the predictor using the batch predictor so that the",
    "start": "938220",
    "end": "945360"
  },
  {
    "text": "prediction is parallelized as well okay so I have prepared a quick demo so",
    "start": "945360",
    "end": "952680"
  },
  {
    "text": "if we could quickly switch here",
    "start": "952680",
    "end": "957019"
  },
  {
    "text": "okay so this is based on a hugging face example notebook and it has been simply",
    "start": "958980",
    "end": "965639"
  },
  {
    "text": "distributed with Ray r so we'll be doing a fine tuning of a",
    "start": "965639",
    "end": "971399"
  },
  {
    "text": "model on text classification tasks and we'll be also a hyper parameter tuning",
    "start": "971399",
    "end": "977100"
  },
  {
    "text": "it and doing some predictions so first let me connect to a ray cluster this is all running in an any scale workspace so",
    "start": "977100",
    "end": "984899"
  },
  {
    "text": "let's just check what sort of resources we have so we have 16 gpus so we will run the training on four workers each",
    "start": "984899",
    "end": "992820"
  },
  {
    "text": "with one GPU so let in the interest of time let me just quickly skip over the",
    "start": "992820",
    "end": "998579"
  },
  {
    "text": "initialization here we're just defining the glue task that we'll be working on we are obtaining the tokenizer and the",
    "start": "998579",
    "end": "1005480"
  },
  {
    "text": "model from the hugging phase Hub we are using great data from hugging face to",
    "start": "1005480",
    "end": "1010699"
  },
  {
    "text": "easy convert the hagging phase data set to a data sets I'm just limiting it here",
    "start": "1010699",
    "end": "1016899"
  },
  {
    "text": "for brevity we are defining RP process function so",
    "start": "1016899",
    "end": "1021920"
  },
  {
    "text": "what it will do it will use the hugging phase tokenizer to tokenize the data",
    "start": "1021920",
    "end": "1027319"
  },
  {
    "text": "before passing it to the trainer and we are saving is as a batch multiple preprocessor",
    "start": "1027319",
    "end": "1033260"
  },
  {
    "text": "so here is our trainer in it per worker function so all we do is we are",
    "start": "1033260",
    "end": "1039079"
  },
  {
    "text": "obtaining the tokenizer the model we are defining the hugging phase training arguments uh we are defining our custom",
    "start": "1039079",
    "end": "1046160"
  },
  {
    "text": "compute metric initialize the trainer and we return it so this will be run on every array worker and the training will",
    "start": "1046160",
    "end": "1053840"
  },
  {
    "text": "be proceed in a distributed data parallel fashion so here is our array our trainer we pass",
    "start": "1053840",
    "end": "1060500"
  },
  {
    "text": "the function that we have just defined into it we specify the scaling config with the number of workers the data sets",
    "start": "1060500",
    "end": "1067880"
  },
  {
    "text": "that we'll be using the Run config so we are using the mlflow logger callback for logging and we only want to keep the",
    "start": "1067880",
    "end": "1074480"
  },
  {
    "text": "best checkpoint using the evaluation loss as a metric and we are also passing",
    "start": "1074480",
    "end": "1079580"
  },
  {
    "text": "our preprocessor which will be tokenizing the data",
    "start": "1079580",
    "end": "1085360"
  },
  {
    "text": "so all we have to do now is is called trainer.fit and we can see that here is the output",
    "start": "1085400",
    "end": "1093020"
  },
  {
    "text": "for it so every iteration of the training every Epoch you will receive a",
    "start": "1093020",
    "end": "1100580"
  },
  {
    "text": "report from the trainer with all the different metrics we are also working on",
    "start": "1100580",
    "end": "1106760"
  },
  {
    "text": "steps support so you'll be able to not only report every Epoch but every for",
    "start": "1106760",
    "end": "1112520"
  },
  {
    "text": "example 500 steps uh so let's give it a moment to run",
    "start": "1112520",
    "end": "1120460"
  },
  {
    "text": "so while it's doing its thing so we can see that it's running training running evaluation we can see that it's starting",
    "start": "1120860",
    "end": "1128000"
  },
  {
    "text": "to report the iterations and the widget is also getting updated so I've only set",
    "start": "1128000",
    "end": "1133340"
  },
  {
    "text": "it to it to to epochs so the training has completed",
    "start": "1133340",
    "end": "1138440"
  },
  {
    "text": "and we get the result object back and we can see the last reported metric",
    "start": "1138440",
    "end": "1144140"
  },
  {
    "text": "we can see the checkpoint which is saved in a local path and we can easily convert",
    "start": "1144140",
    "end": "1152059"
  },
  {
    "text": "that to for example an S3 path or an in a memory object",
    "start": "1152059",
    "end": "1157760"
  },
  {
    "text": "so now that we want to tune hyper parameters all we have to do is pass our",
    "start": "1157760",
    "end": "1163900"
  },
  {
    "text": "trainer into our Ray air tuner we Define the parameter space so we'll be",
    "start": "1163900",
    "end": "1169179"
  },
  {
    "text": "optimizing learning rate using grid search we Define the metric that we want",
    "start": "1169179",
    "end": "1174679"
  },
  {
    "text": "to use the whether we want to minimize or maximize and we are also using an",
    "start": "1174679",
    "end": "1180740"
  },
  {
    "text": "Asha scheduler here so Ray air provides several Advanced searches and schedulers that can speed up your training so the",
    "start": "1180740",
    "end": "1187820"
  },
  {
    "text": "Asha scheduler will aggressively early stop under performing",
    "start": "1187820",
    "end": "1194600"
  },
  {
    "text": "trials and this can also be combined with a resource allocating scheduler which",
    "start": "1194600",
    "end": "1200179"
  },
  {
    "text": "allows you to reallocate the resources from stopped trials into running trials",
    "start": "1200179",
    "end": "1206380"
  },
  {
    "text": "maintaining full resource utilization and thus letting you finish your hyper parameter",
    "start": "1206380",
    "end": "1212539"
  },
  {
    "text": "optimization faster so we are just letting it run here so",
    "start": "1212539",
    "end": "1218179"
  },
  {
    "text": "the widget is updating so we have four trials running in parallel so each of those four trials",
    "start": "1218179",
    "end": "1224299"
  },
  {
    "text": "is also running in a distributed fashion for gpus for a total of 16 gpus and we",
    "start": "1224299",
    "end": "1229760"
  },
  {
    "text": "can see that we are obtaining results back already",
    "start": "1229760",
    "end": "1234220"
  },
  {
    "text": "so let's let's give it a moment to run",
    "start": "1235760",
    "end": "1239740"
  },
  {
    "text": "so while it's doing its thing let me just quickly showcase The Next Step",
    "start": "1247400",
    "end": "1252799"
  },
  {
    "text": "so afterwards we can obtain a data frame of all the tuning results and we can",
    "start": "1252799",
    "end": "1259220"
  },
  {
    "text": "also get the best result by evaluation loss or any other metric winners May",
    "start": "1259220",
    "end": "1264320"
  },
  {
    "text": "specify so let's quickly check how the optimization is progressing",
    "start": "1264320",
    "end": "1269960"
  },
  {
    "text": "yeah so we can see the Asha scheduler is terminating uh trials that are that are",
    "start": "1269960",
    "end": "1275840"
  },
  {
    "text": "underperforming and we are only left with one running trial which is the most promising so I've set it to run for for",
    "start": "1275840",
    "end": "1282679"
  },
  {
    "text": "epochs and we are about to be done with the tuning",
    "start": "1282679",
    "end": "1289179"
  },
  {
    "text": "yep so here is the data frame of all the evaluation result results and of course",
    "start": "1304940",
    "end": "1311419"
  },
  {
    "text": "you can use different login callbacks with it you can use tensorboard to monitor your progress ml flow weights",
    "start": "1311419",
    "end": "1318799"
  },
  {
    "text": "and biases and any other Library you may think of so we have obtained the best result object",
    "start": "1318799",
    "end": "1324320"
  },
  {
    "text": "and now we can predict on our test data so we are instantiating the batch",
    "start": "1324320",
    "end": "1330260"
  },
  {
    "text": "predictor we are using the best result to obtain the checkpoint we'll be using",
    "start": "1330260",
    "end": "1335600"
  },
  {
    "text": "the hugging face predictor as our predictor class the task is test text classification and we'll be running the",
    "start": "1335600",
    "end": "1341240"
  },
  {
    "text": "prediction on gpus",
    "start": "1341240",
    "end": "1344679"
  },
  {
    "text": "so as you can see array automatically schedules actors to carry out the prediction in a distributed Manner and",
    "start": "1360860",
    "end": "1368000"
  },
  {
    "text": "we can see that this is an uh those are the labels that we have obtained on test",
    "start": "1368000",
    "end": "1373400"
  },
  {
    "text": "data and we can of course later we can save that prediction from array data set to a parquet file for example",
    "start": "1373400",
    "end": "1380480"
  },
  {
    "text": "or we can do online inference with reserve and this concludes the demo",
    "start": "1380480",
    "end": "1387460"
  },
  {
    "text": "yep so happy to take any questions and thank you all for listening",
    "start": "1387460",
    "end": "1392610"
  },
  {
    "text": "[Applause]",
    "start": "1392610",
    "end": "1399900"
  },
  {
    "text": "hey um I'm Tyler I was just wondering how this works with like hugging face spaces",
    "start": "1401900",
    "end": "1407659"
  },
  {
    "text": "um so if I'm trying to create like a radio or streamlit app like on top of this is it just kind of the same typical workloads",
    "start": "1407659",
    "end": "1414080"
  },
  {
    "text": "um that I normally do with spaces or is there any difference that I have to worry about so we do not support spaces just yet but",
    "start": "1414080",
    "end": "1421280"
  },
  {
    "text": "this is one of the features that we want to be evaluating for support right now the main focus is fine tuning and",
    "start": "1421280",
    "end": "1429140"
  },
  {
    "text": "prediction at scale",
    "start": "1429140",
    "end": "1432220"
  },
  {
    "text": "hi there my name is Prashant um when you showed the part where you connected to the any scale cluster how easy is it to",
    "start": "1441200",
    "end": "1448100"
  },
  {
    "text": "change that methodology to connect to let's say an external cloud like AWS or",
    "start": "1448100",
    "end": "1453380"
  },
  {
    "text": "any other yeah so it is quite easy so we are using an NSK cluster here but you",
    "start": "1453380",
    "end": "1459980"
  },
  {
    "text": "can also do the same thing on an OSS Ray cluster using the ray cluster launcher you can use AWS gcp Alibaba or on an",
    "start": "1459980",
    "end": "1469460"
  },
  {
    "text": "on-premise cluster and once you have array cluster like you can run pretty",
    "start": "1469460",
    "end": "1474860"
  },
  {
    "text": "much any rare workflow on it",
    "start": "1474860",
    "end": "1478600"
  },
  {
    "text": "hi what do you think this would look like if you used Rey for the data parallels instead of using hugging face",
    "start": "1486679",
    "end": "1493700"
  },
  {
    "text": "and um pie torch DDP under the hood like what level of surgery would it require",
    "start": "1493700",
    "end": "1501080"
  },
  {
    "text": "to do that so just I understand correctly so you just want to use like puree for everything for the training as",
    "start": "1501080",
    "end": "1507980"
  },
  {
    "text": "well so obviously you can you can create your",
    "start": "1507980",
    "end": "1513380"
  },
  {
    "text": "own training Library using our libraries Android car so I know that somebody was was working",
    "start": "1513380",
    "end": "1521480"
  },
  {
    "text": "on a fully raytonic decision tree as a toy example but in",
    "start": "1521480",
    "end": "1528559"
  },
  {
    "text": "general what we are trying to do if Ray are is that we are trying to wrap around popular libraries and handle all the",
    "start": "1528559",
    "end": "1536059"
  },
  {
    "text": "boring infrastructure and orchestration stuff so you can focus on what's important in your machine learning life",
    "start": "1536059",
    "end": "1542240"
  },
  {
    "text": "cycle",
    "start": "1542240",
    "end": "1544480"
  },
  {
    "text": "thank you um thanks for the demo um so this looks really cool am I correct in understanding that the main",
    "start": "1554059",
    "end": "1560900"
  },
  {
    "text": "difference between the raytrain API as it exists today and then um the retrain API from Air is that",
    "start": "1560900",
    "end": "1569000"
  },
  {
    "text": "um from Air it's more composable so you can like pass it into tuner and then",
    "start": "1569000",
    "end": "1574100"
  },
  {
    "text": "anything else that comes after or are there any other like key differences that you wanted to highlight as well",
    "start": "1574100",
    "end": "1580039"
  },
  {
    "text": "yeah so we pray air we are trying to provide a unified API for all of our",
    "start": "1580039",
    "end": "1585860"
  },
  {
    "text": "libraries so as you have said yes compassability is one of the new advantages of the new API but in general",
    "start": "1585860",
    "end": "1593419"
  },
  {
    "text": "we have been trying to make it simpler Hideaway some of the complexity and make",
    "start": "1593419",
    "end": "1598580"
  },
  {
    "text": "it more familiar so as you might have noticed for example we are using some of",
    "start": "1598580",
    "end": "1603679"
  },
  {
    "text": "the scikit-learn terminologies such as fitting and transforming so we are not 100 following scikit-learn but we have",
    "start": "1603679",
    "end": "1610760"
  },
  {
    "text": "obviously used that as an inspiration and we hope that uh it will allow people to just start their Journey we pray",
    "start": "1610760",
    "end": "1618500"
  },
  {
    "text": "quicker than it is currently with the old apis that weren't that great let's",
    "start": "1618500",
    "end": "1624440"
  },
  {
    "text": "be honest",
    "start": "1624440",
    "end": "1626860"
  },
  {
    "text": "thanks so I want to ask a little bit about four tolerance so I want to ask",
    "start": "1632299",
    "end": "1638840"
  },
  {
    "text": "where does the checkpoint get stored yeah if the like the for example the hand note crash where we still able to",
    "start": "1638840",
    "end": "1646340"
  },
  {
    "text": "kind of resume the proper checkpoint yeah so so by by default what we do is",
    "start": "1646340",
    "end": "1651980"
  },
  {
    "text": "we save the checkpoints on a head node so the checkpoints are transferred from worker nodes to the Head node but you",
    "start": "1651980",
    "end": "1657320"
  },
  {
    "text": "can very easily set up checkpointing on S3 on NFS and on multiple different",
    "start": "1657320",
    "end": "1666200"
  },
  {
    "text": "Cloud storages so for persistence it's just basically using array a run config",
    "start": "1666200",
    "end": "1673340"
  },
  {
    "text": "and you can do checkpoint config there and specify where you want your checkpoints to be saved",
    "start": "1673340",
    "end": "1679840"
  },
  {
    "text": "hi uh property follow-up question about for tolerance so that's the elastic tuning uh like on a schedule for like",
    "start": "1684679",
    "end": "1692360"
  },
  {
    "text": "supporting the future if so what kind of elastic training you're going to support yeah yeah so yes elastic training is",
    "start": "1692360",
    "end": "1699940"
  },
  {
    "text": "something that we would like to add uh it's hard for me to say right now what",
    "start": "1699940",
    "end": "1704960"
  },
  {
    "text": "kind of elastic training will have but definitely I can see like where I'm restarting for example so in one of our",
    "start": "1704960",
    "end": "1712940"
  },
  {
    "text": "existing libraries xgboost tray so for example it has full elastic training and this is something that we would like to",
    "start": "1712940",
    "end": "1719380"
  },
  {
    "text": "add to our other trainers but yeah so far it is still in the planning stage",
    "start": "1719380",
    "end": "1727419"
  },
  {
    "text": "sorry hi I'm nikunj ah you mentioned that we",
    "start": "1736039",
    "end": "1741980"
  },
  {
    "text": "can visualize the results on tensorboard weights and biases ml flow Etc is there any tight integration with some of these",
    "start": "1741980",
    "end": "1748760"
  },
  {
    "text": "libraries or it's on the user on what experiment tracking Library they want to use yeah so we provide several built-in",
    "start": "1748760",
    "end": "1755480"
  },
  {
    "text": "callbacks for the popular libraries but you can very easily add your own",
    "start": "1755480",
    "end": "1760940"
  },
  {
    "text": "callbacks so if you are using some library that is not built for which an",
    "start": "1760940",
    "end": "1766159"
  },
  {
    "text": "integration is not built into our you can very easily add that integration and use whatever experiment tracking Library",
    "start": "1766159",
    "end": "1773059"
  },
  {
    "text": "you want",
    "start": "1773059",
    "end": "1775600"
  },
  {
    "text": "can you talk briefly about how different kind of training parallelism must",
    "start": "1779779",
    "end": "1784880"
  },
  {
    "text": "support this yeah so we are focusing on data parallelism right now so this is",
    "start": "1784880",
    "end": "1790220"
  },
  {
    "text": "the main Paradigm that we are we want to re support really well but we are also",
    "start": "1790220",
    "end": "1796100"
  },
  {
    "text": "starting to support model parallelism with Alpa this is not yet released but",
    "start": "1796100",
    "end": "1801559"
  },
  {
    "text": "we are hoping it will get merged into Master soon and there will be a ray Air Alpha trainer for easy model parallelism",
    "start": "1801559",
    "end": "1809980"
  },
  {
    "text": "okay thank you all and yeah if you have any more questions please let me know uh",
    "start": "1821899",
    "end": "1828140"
  },
  {
    "text": "unfortunately the slide isn't displayed but yeah you can you can find me on LinkedIn and you can find me on the",
    "start": "1828140",
    "end": "1834200"
  },
  {
    "text": "speakers portal on the app so yeah happy to connect and chat me if you are interested in more",
    "start": "1834200",
    "end": "1839659"
  },
  {
    "text": "thank you",
    "start": "1839659",
    "end": "1842260"
  }
]