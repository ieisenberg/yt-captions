[
  {
    "start": "0",
    "end": "80000"
  },
  {
    "text": "all right let's get started uh good morning everyone uh thank you so much for taking your time to attend today's",
    "start": "3600",
    "end": "9599"
  },
  {
    "text": "webinar today uh we're talking about a very interesting topic if you're a machine learning",
    "start": "9599",
    "end": "15759"
  },
  {
    "text": "practitioner you will know that tuning the hyper parameters of a model is is hard it requires a lot of work",
    "start": "15759",
    "end": "22080"
  },
  {
    "text": "and the hyper parameters has a very uh critical impact on the performance of",
    "start": "22080",
    "end": "27439"
  },
  {
    "text": "the model so i'm super glad to uh introduce amok um who is an engineer on any school",
    "start": "27439",
    "end": "35440"
  },
  {
    "text": "working on the machine learning team and i'm going to talk about a a relatively new hyper parameter",
    "start": "35440",
    "end": "42480"
  },
  {
    "text": "tuning algorithm called the population-based tuning pbt so",
    "start": "42480",
    "end": "48640"
  },
  {
    "text": "yeah let's get started i will share a moog's presentation",
    "start": "48640",
    "end": "58640"
  },
  {
    "text": "hi everyone my name is emog and i'm a software engineer at any scale so i work on the team developing ray 2",
    "start": "58640",
    "end": "65040"
  },
  {
    "text": "and other training libraries on top of ray and today we will be going over",
    "start": "65040",
    "end": "70880"
  },
  {
    "text": "population-based training which is a strategy for hyper parameter optimization and how we can leverage ray tune to add",
    "start": "70880",
    "end": "76960"
  },
  {
    "text": "population-based training to our application so yeah so let's get started",
    "start": "76960",
    "end": "82320"
  },
  {
    "start": "80000",
    "end": "305000"
  },
  {
    "text": "population-based training with ray tune so a little bit of uh overview of what the",
    "start": "82320",
    "end": "87680"
  },
  {
    "text": "talk today is going to be um so we'll first go into the motivations for population-based",
    "start": "87680",
    "end": "93040"
  },
  {
    "text": "training and compare it against other hyper parameter optimization strategies and understand why population-based",
    "start": "93040",
    "end": "98880"
  },
  {
    "text": "training is the most cost-effective approach compared to other approaches that are commonly used",
    "start": "98880",
    "end": "104560"
  },
  {
    "text": "then we will dive into ray tune library which is a library built on top of the",
    "start": "104560",
    "end": "110640"
  },
  {
    "text": "array framework for distributed hyper-parameter tuning we'll we will go give an overview of",
    "start": "110640",
    "end": "116799"
  },
  {
    "text": "what rating capabilities are and how we can easily use it to run distributed hyper parameter tuning",
    "start": "116799",
    "end": "123280"
  },
  {
    "text": "experiments and then finally we'll go over some of the recent developments in population-based training that we've",
    "start": "123280",
    "end": "129200"
  },
  {
    "text": "added to tune over the past couple of months so this includes functionality for reproducible for",
    "start": "129200",
    "end": "136400"
  },
  {
    "text": "reproducing population-based training schedules as well as a synchronized implementation of",
    "start": "136400",
    "end": "142000"
  },
  {
    "text": "pbt and finally we will go over a new algorithm that we recently added in ray",
    "start": "142000",
    "end": "147840"
  },
  {
    "text": "version 1.0.1 called population based bandits which combines pbt",
    "start": "147840",
    "end": "154239"
  },
  {
    "text": "with bayesian optimization and is actually um accepted into europe's 2020 uh",
    "start": "154239",
    "end": "159680"
  },
  {
    "text": "conference so uh let's get started um let's first begin with a brief overview of hybrid",
    "start": "159680",
    "end": "166239"
  },
  {
    "text": "parameter optimization uh specifically what exactly is hpo and why does it matter so you know as",
    "start": "166239",
    "end": "173920"
  },
  {
    "text": "many of you probably know like deep learning is experiencing a rapid growth and adoption and this is not only",
    "start": "173920",
    "end": "180720"
  },
  {
    "text": "happening in academia but also in industry so a lot of you know pretty much a vast majority of applications are leveraging",
    "start": "180720",
    "end": "187519"
  },
  {
    "text": "machine learning and specifically deep learning in some sort of capacity and we can see here that",
    "start": "187519",
    "end": "192800"
  },
  {
    "text": "uh the number of machine learning papers per year has increased significantly over the past",
    "start": "192800",
    "end": "199120"
  },
  {
    "text": "few years so machine learning has really become ubiquitous in many applications that we're using",
    "start": "199120",
    "end": "204319"
  },
  {
    "text": "today but there is one little secret about machine learning so we're all familiar with convolutional",
    "start": "204319",
    "end": "211120"
  },
  {
    "text": "neural networks these are very powerful and are attributed to many of the recent",
    "start": "211120",
    "end": "216720"
  },
  {
    "text": "advances in computer vision so here we have one of the original cnn",
    "start": "216720",
    "end": "221760"
  },
  {
    "text": "designs by yan loken and on the bottom we have alexnet which was developed after 20 years of research",
    "start": "221760",
    "end": "227840"
  },
  {
    "text": "but what's interesting about recent work on deep learning is that the basic idea of convolutional neural networks have",
    "start": "227840",
    "end": "233920"
  },
  {
    "text": "largely remained unchanged but over the last 20 years beside the increased availability of our",
    "start": "233920",
    "end": "240159"
  },
  {
    "text": "computational resources we've largely modified the shape of the neural network and the size of each layer",
    "start": "240159",
    "end": "245519"
  },
  {
    "text": "right so it's the hyper parameters it can actually make a huge impact on the performance of our neural network",
    "start": "245519",
    "end": "252159"
  },
  {
    "text": "and so this includes like the shape of the network and the size of each layer and over time we've seen the many new hyper",
    "start": "252159",
    "end": "258160"
  },
  {
    "text": "parameters um uh that have been added to more recent networks that can be configured",
    "start": "258160",
    "end": "263680"
  },
  {
    "text": "and these have huge impacts in performance right so we see here that models are",
    "start": "263680",
    "end": "269199"
  },
  {
    "text": "continuously getting larger and larger with the most recent open ai model containing nearly 200 billion parameters",
    "start": "269199",
    "end": "276560"
  },
  {
    "text": "but these models are not only larger but they're more complex so there's a whole new set of hyper parameters that",
    "start": "276560",
    "end": "282240"
  },
  {
    "text": "we have to tune and which have a significant impact on our model performance and we which means that selecting the",
    "start": "282240",
    "end": "290000"
  },
  {
    "text": "right hyper parameters is going to take a lot of time and going to consume a lot of resources so with so",
    "start": "290000",
    "end": "296400"
  },
  {
    "text": "many different hyper parameters that we need to search over we need an efficient and quick method to find the most",
    "start": "296400",
    "end": "302800"
  },
  {
    "text": "optimal configuration so given the importance of hyper",
    "start": "302800",
    "end": "308080"
  },
  {
    "text": "parameter optimization what are some strategies that we can use to efficiently search over uh this",
    "start": "308080",
    "end": "313919"
  },
  {
    "text": "large search space of hyper parameter configurations so two of the most naive approaches are",
    "start": "313919",
    "end": "320320"
  },
  {
    "text": "grid search and random search so with both of these approaches we simply define",
    "start": "320320",
    "end": "326080"
  },
  {
    "text": "a search space distribution for each hyperparameter so for grid search for example we simply",
    "start": "326080",
    "end": "332240"
  },
  {
    "text": "for each hyperparameter we simply provide a list of values that we want to search over and for every point in this uh",
    "start": "332240",
    "end": "339840"
  },
  {
    "text": "multi-dimensional search space we run a few epochs of training and then at the end we select the best",
    "start": "339840",
    "end": "345759"
  },
  {
    "text": "hybrid parameter configuration so looking at this figure for grid search um for uh",
    "start": "345759",
    "end": "352240"
  },
  {
    "text": "we have basically nine different points that we try out uh and then three values for each hyper",
    "start": "352240",
    "end": "358000"
  },
  {
    "text": "parameter but the problem with grid search is that even though these training runs",
    "start": "358000",
    "end": "364400"
  },
  {
    "text": "can be run in parallel they suffer because we only search over",
    "start": "364400",
    "end": "369440"
  },
  {
    "text": "a small search space so once again looking at this image for grid search we see that uh despite running",
    "start": "369440",
    "end": "377440"
  },
  {
    "text": "nine different trials of different hyper parameter configurations the optimal hypo hyper parameter",
    "start": "377440",
    "end": "384080"
  },
  {
    "text": "configuration is not actually searched over so we need a larger search space to",
    "start": "384080",
    "end": "389520"
  },
  {
    "text": "actually get to this optimal configuration and as we increase the number of hyper parameters then we have to",
    "start": "389520",
    "end": "395360"
  },
  {
    "text": "search over a larger and larger number of different configurations and ultimately as we saw before because",
    "start": "395360",
    "end": "401600"
  },
  {
    "text": "the size of our models and the complexity of our models have increased drastically over the years searching over a large number",
    "start": "401600",
    "end": "408000"
  },
  {
    "text": "of hyper parameters is just simply very expensive and not feasible for a majority of people so we",
    "start": "408000",
    "end": "414560"
  },
  {
    "text": "can improve grid search uh slightly with random search which basically means that instead of providing a list of values",
    "start": "414560",
    "end": "421120"
  },
  {
    "text": "that we want to search over we provide distribution for example we can search over learning",
    "start": "421120",
    "end": "426319"
  },
  {
    "text": "rate with like a with a uniform distribution or a normal distribution for example so this is a little bit better than",
    "start": "426319",
    "end": "432080"
  },
  {
    "text": "group search because we can add some stochasticity to our search which potentially can lead us",
    "start": "432080",
    "end": "437919"
  },
  {
    "text": "to an optimal high parameter configuration but like grid search this still suffers for large search spaces",
    "start": "437919",
    "end": "444000"
  },
  {
    "text": "because we need to try out many different points to find the optimal configuration and both of these",
    "start": "444000",
    "end": "450800"
  },
  {
    "text": "strategies both grid search and random search are inefficient and not practical for large models with expensive training",
    "start": "450800",
    "end": "456479"
  },
  {
    "text": "time but despite their inefficiencies people still commonly use these approaches because they're easy to implement",
    "start": "456479",
    "end": "464319"
  },
  {
    "text": "so another approach to hyper parameter optimization to improve on grid and random search is bayesian",
    "start": "464319",
    "end": "470960"
  },
  {
    "text": "optimization right so with bayesian optimization we actually fit a gaussian model",
    "start": "470960",
    "end": "476479"
  },
  {
    "text": "that tries to predict the objective function given the hyper parameters so we run a set of training runs",
    "start": "476479",
    "end": "484720"
  },
  {
    "text": "that are randomly initialized with hyper parameters at the beginning and over time uh our gaussian model",
    "start": "484720",
    "end": "491199"
  },
  {
    "text": "continues to improve based on historical data and we use this model to inform future hyper parameters for",
    "start": "491199",
    "end": "497759"
  },
  {
    "text": "subsequent runs so the bayesian optimization is more optimal than random search because we",
    "start": "497759",
    "end": "504319"
  },
  {
    "text": "use prior information and as you can see in this plot if you look at the red line compared to the black line",
    "start": "504319",
    "end": "511840"
  },
  {
    "text": "over time we see that we can reach better and more optimal hyper parameter configurations because our gaussian",
    "start": "511840",
    "end": "518320"
  },
  {
    "text": "model is continuously improving based on historical data which is not the case with random search",
    "start": "518320",
    "end": "525279"
  },
  {
    "text": "however with bayesian optimization we still have the problem uh that it actually starts off similar",
    "start": "525279",
    "end": "531120"
  },
  {
    "text": "to random search and as you can see with this plot there's not much distinction between the red line the black line at the very",
    "start": "531120",
    "end": "536560"
  },
  {
    "text": "beginning so to mitigate this we can actually combine bayesian optimization with an early stopping algorithm like",
    "start": "536560",
    "end": "543120"
  },
  {
    "text": "asynchronous hyperband or uh more commonly known as asha so these early stopping algorithms",
    "start": "543120",
    "end": "549440"
  },
  {
    "text": "simply just stop bad performing trials early so we don't waste time and resources on",
    "start": "549440",
    "end": "555440"
  },
  {
    "text": "training hyperparameter configurations that are not very fruitful so combining",
    "start": "555440",
    "end": "561680"
  },
  {
    "text": "bayesian optimization with these early stopping algorithms like hyperband we can outperform other hyperparameter optimization models",
    "start": "561680",
    "end": "570080"
  },
  {
    "text": "so with bayesian optimization hyperband we use historical data to inform future hyperparameters",
    "start": "570080",
    "end": "575600"
  },
  {
    "text": "through our gaussian model and we can also stop early trials but the problem with that approach is",
    "start": "575600",
    "end": "581760"
  },
  {
    "text": "that even though our hyper parameter configurations continuously get better",
    "start": "581760",
    "end": "586800"
  },
  {
    "text": "over time subsequent training runs still have to start training from scratch what if there's a way to tune the hyper",
    "start": "586800",
    "end": "593279"
  },
  {
    "text": "parameters without needing to completely restart training we can do so with uh population based",
    "start": "593279",
    "end": "599040"
  },
  {
    "text": "training a hyperparameter strategy uh well it's created by deepmind that",
    "start": "599040",
    "end": "604399"
  },
  {
    "text": "employs an evolutionary algorithm so just like bayesian optimization we start evaluating a population",
    "start": "604399",
    "end": "610320"
  },
  {
    "text": "in parallel of random hyperparameter configs so in this example we have four",
    "start": "610320",
    "end": "615600"
  },
  {
    "text": "different trials that we try out as the initial population each with a different learning rate but",
    "start": "615600",
    "end": "623040"
  },
  {
    "text": "after some interval we can stop bad performing trials and actually copy",
    "start": "623040",
    "end": "629839"
  },
  {
    "text": "over the progress of good performing trials so here we see that a learning rate of 0.4 is not performing",
    "start": "629839",
    "end": "636800"
  },
  {
    "text": "well so we stop that we can actually copy the model weights from the best performing trial which is",
    "start": "636800",
    "end": "642959"
  },
  {
    "text": "our learning rate of 0.1 and then we can perturb the hyper parameter slightly so we perturb it from",
    "start": "642959",
    "end": "648560"
  },
  {
    "text": "point one to point one five so what this allows us to do is focus specifically on the search the points in",
    "start": "648560",
    "end": "655519"
  },
  {
    "text": "the hyper parameter search space which are providing good results and we don't have to start training from",
    "start": "655519",
    "end": "660560"
  },
  {
    "text": "scratch because we can copy over the network weights from a good performing trial into a bad performing trial",
    "start": "660560",
    "end": "667680"
  },
  {
    "text": "so population based training is about easily parallel parallelizable and can actually search over dynamic",
    "start": "667680",
    "end": "673600"
  },
  {
    "text": "schedules so certain optimal hype parameter configurations are not static but can",
    "start": "673600",
    "end": "680000"
  },
  {
    "text": "actually change over time so population-based training allows us to find these dynamic schedules",
    "start": "680000",
    "end": "685519"
  },
  {
    "text": "consisting of a training run where hyper parameters are changing during the training process",
    "start": "685519",
    "end": "693360"
  },
  {
    "text": "so here we see another diagram provided by deepmind just comparing parallel cert parallel random and grid search",
    "start": "694000",
    "end": "700399"
  },
  {
    "text": "against population-based training and yeah so we see here that for random and grid search we don't",
    "start": "700399",
    "end": "706079"
  },
  {
    "text": "use previous information to inform future hyper parameters and uh unlike in population based",
    "start": "706079",
    "end": "712560"
  },
  {
    "text": "training where we can copy over the best performing trials and perturb the hyper parameters slightly",
    "start": "712560",
    "end": "719040"
  },
  {
    "text": "to focus on the most optimal points in the search space",
    "start": "719040",
    "end": "724160"
  },
  {
    "text": "so does population-based training actually work well deepmind ran this technique over multiple published algorithms",
    "start": "724160",
    "end": "730639"
  },
  {
    "text": "and we see that from many different applications like reinforcement learning to natural language processing we see",
    "start": "730639",
    "end": "736720"
  },
  {
    "text": "that population-based training was able to provide a non-trivial performance improvement over state of",
    "start": "736720",
    "end": "742079"
  },
  {
    "text": "the art so how cost effective is population-based training",
    "start": "742079",
    "end": "748079"
  },
  {
    "text": "so here we can see comparison of various hyper parameter optimization strategies on a burke transformer model transformer",
    "start": "748079",
    "end": "755440"
  },
  {
    "text": "model and we see that population based training has the best accuracy without much",
    "start": "755440",
    "end": "760480"
  },
  {
    "text": "extra cost so even though bayesian optimization with early stopping provides the accuracy improvement uh it",
    "start": "760480",
    "end": "767440"
  },
  {
    "text": "is not cost effective because we still have to run initial random samples to train the",
    "start": "767440",
    "end": "774480"
  },
  {
    "text": "gaussian model and subsequent training runs how to start from scratch so",
    "start": "774480",
    "end": "780800"
  },
  {
    "text": "population-based training can provide the same accuracy improvements or even slightly better accuracy improvements over",
    "start": "780800",
    "end": "786399"
  },
  {
    "text": "paging optimization plus early stopping but also be cost effective because training runs can continue from",
    "start": "786399",
    "end": "793600"
  },
  {
    "text": "where previous good trials have left off all right cool so now we've established",
    "start": "793600",
    "end": "799760"
  },
  {
    "start": "798000",
    "end": "921000"
  },
  {
    "text": "that population based training is an efficient method for hyper parameter optimization but how do we actually add it to our",
    "start": "799760",
    "end": "805360"
  },
  {
    "text": "application so this is where ray tune comes in which is a library for scalable hyper",
    "start": "805360",
    "end": "810639"
  },
  {
    "text": "parameter tuning and provides an easy to use api for running distributed uh hyper",
    "start": "810639",
    "end": "816639"
  },
  {
    "text": "parameter optimization experiments so ray tune offers a variety of cutting",
    "start": "816639",
    "end": "822480"
  },
  {
    "text": "edge algorithms that we've talked about you know bayesian optimization and early stopping schedulers as well as",
    "start": "822480",
    "end": "828560"
  },
  {
    "text": "population based training and similar to other hyper parameter tuning solutions it also off",
    "start": "828560",
    "end": "834639"
  },
  {
    "text": "works with any machine learning framework that is commonly used today so tune is one of",
    "start": "834639",
    "end": "840800"
  },
  {
    "text": "the only libraries to actually offer a distributed implementation of population based training",
    "start": "840800",
    "end": "847199"
  },
  {
    "text": "so but there's two features that really make ray tune special so the first is that focus and",
    "start": "847680",
    "end": "853600"
  },
  {
    "text": "simplifying execution so with ray tune you have the ability to launch a distributed multi-gpu hyper",
    "start": "853600",
    "end": "859839"
  },
  {
    "text": "parameter sweep in less than 10 lines of code and in addition due to ray tunes",
    "start": "859839",
    "end": "864880"
  },
  {
    "text": "automatic fault tolerance you can easily leverage cheap preemptable instances in the cloud",
    "start": "864880",
    "end": "872000"
  },
  {
    "text": "also raid 2 interoperates with many other hyper parameter optimization libraries",
    "start": "872000",
    "end": "877600"
  },
  {
    "text": "so if you've already tied to one of these other other libraries that exist out there today",
    "start": "877600",
    "end": "883760"
  },
  {
    "text": "that provide algorithmic improvements ray tune can actually sit underneath the hood of these libraries and can easily distribute them",
    "start": "883760",
    "end": "890639"
  },
  {
    "text": "on a multi-node cloud cluster so you can take advantage of the algorithmic advancements of these other",
    "start": "890639",
    "end": "895680"
  },
  {
    "text": "hyper parameter optimization libraries but use ray tune for the scalability",
    "start": "895680",
    "end": "900720"
  },
  {
    "text": "portion and finally we all know that tuning and training is actually just a small part",
    "start": "900720",
    "end": "905839"
  },
  {
    "text": "of the machine learning life cycle so ray tune provides seamless one-line integrations with experiment management",
    "start": "905839",
    "end": "911279"
  },
  {
    "text": "frameworks like tensorboard or weights and biases so they can manage your experience experiments and hand off your model to",
    "start": "911279",
    "end": "917440"
  },
  {
    "text": "other tools down the pipeline so let's go over a quick overview of the",
    "start": "917440",
    "end": "923839"
  },
  {
    "text": "arraytune api to understand the flexibility and simplicity that tune provides",
    "start": "923839",
    "end": "929040"
  },
  {
    "text": "so we start off with a simple model training function so this is just a simple function",
    "start": "929040",
    "end": "934320"
  },
  {
    "text": "which creates a continent and trains it for a certain number of steps",
    "start": "934320",
    "end": "939440"
  },
  {
    "text": "and our function takes in a config argument which provides the hyper parameters for a convolutional network",
    "start": "939440",
    "end": "947279"
  },
  {
    "text": "so the first change we have to make is that we simply add a line to report our loss or whatever metric we",
    "start": "947360",
    "end": "953199"
  },
  {
    "text": "want to optimize over to tune and tune will then uh use this metric",
    "start": "953199",
    "end": "958560"
  },
  {
    "text": "for search algorithms or for an early stopping schedule for example so in this case at every step we we do",
    "start": "958560",
    "end": "965040"
  },
  {
    "text": "one step of training and then report the loss to two and then all we have to do is just call",
    "start": "965040",
    "end": "970399"
  },
  {
    "text": "tune.run so we just pass our function to tune that run specify some",
    "start": "970399",
    "end": "975920"
  },
  {
    "text": "[Music] hyper parameters that we want for our model so in this case learning rate of zero 0 0.1 and that's",
    "start": "975920",
    "end": "983120"
  },
  {
    "text": "it 2 can easily now run our hyper parameter tuning experiment",
    "start": "983120",
    "end": "988399"
  },
  {
    "text": "and can also easily scale to a large cluster so you can specify a search space um",
    "start": "988399",
    "end": "996480"
  },
  {
    "text": "so instead of a static hyper parameter of learning rate point zero of 0.1 you know we can do a uniform search",
    "start": "996480",
    "end": "1003759"
  },
  {
    "text": "across different from 0.001 to 0.1",
    "start": "1003759",
    "end": "1008800"
  },
  {
    "text": "and then we can also run 100 samples so this will sample 100 different points from the search space that we",
    "start": "1008800",
    "end": "1015199"
  },
  {
    "text": "defined and tune will automatically parallelize the evaluation of these training jobs up to the number of cores in your",
    "start": "1015199",
    "end": "1021440"
  },
  {
    "text": "cluster so you can so without any changes we can parallelize this code to our laptop and also to a multi-node cluster",
    "start": "1021440",
    "end": "1029678"
  },
  {
    "text": "through this very narrow api tune exposes a variety of features including automatic checkpointing and specifying",
    "start": "1030160",
    "end": "1036000"
  },
  {
    "text": "tuning algorithms so we can simply add our osha scheduler which we talked about before so tuna tune will automatically stop bad",
    "start": "1036000",
    "end": "1044000"
  },
  {
    "text": "performing trials early and just like that we can also add population based training to our",
    "start": "1044000",
    "end": "1051280"
  },
  {
    "text": "tune run so we swap out the asha scheduler with population based training and the only other change we have to",
    "start": "1051280",
    "end": "1057600"
  },
  {
    "text": "make to our code is that we now have to add logic for checkpointing our model so",
    "start": "1057600",
    "end": "1063520"
  },
  {
    "text": "we take in an additional argument checkpoint directory and if it's not none we can just load",
    "start": "1063520",
    "end": "1069919"
  },
  {
    "text": "that checkpoint into our model and then every step of training we save our model's current state into that",
    "start": "1069919",
    "end": "1076799"
  },
  {
    "text": "directory and the checkpoint here is necessary because as we saw before population-based training",
    "start": "1076799",
    "end": "1082559"
  },
  {
    "text": "has to uh involves moving the state from one",
    "start": "1082559",
    "end": "1087760"
  },
  {
    "text": "training run to another training run so we need to make sure we save the state into each checkpoint so that based training can work accordingly",
    "start": "1087760",
    "end": "1097440"
  },
  {
    "start": "1097000",
    "end": "1284000"
  },
  {
    "text": "great so now we've talked about how we can easily uh use tune to add population-based training to our",
    "start": "1097760",
    "end": "1103280"
  },
  {
    "text": "application and also how tune automatically scales our experiment",
    "start": "1103280",
    "end": "1109679"
  },
  {
    "text": "by running multiple parallel trials from anything on your laptop to a large multi-node cluster",
    "start": "1109679",
    "end": "1115760"
  },
  {
    "text": "so now let's go over some of the new developments with population-based training that we've",
    "start": "1115760",
    "end": "1120880"
  },
  {
    "text": "added to ray tune over the past couple of months so the first is reproducible traces",
    "start": "1120880",
    "end": "1127280"
  },
  {
    "text": "so as we talked a little bit before earlier population based training is unique",
    "start": "1127280",
    "end": "1133120"
  },
  {
    "text": "because you can actually get a dynamic schedule uh what this means is that",
    "start": "1133120",
    "end": "1138559"
  },
  {
    "text": "uh the optimal hyper parameter configuration is not just specific values but a schedule",
    "start": "1138559",
    "end": "1145760"
  },
  {
    "text": "where the hyper parameter is changing uh its value over time during a training",
    "start": "1145760",
    "end": "1151200"
  },
  {
    "text": "run so with population based training replay we can easily reproduce",
    "start": "1151200",
    "end": "1157760"
  },
  {
    "text": "the optimal dynamics schedule that we get from population based training",
    "start": "1157760",
    "end": "1163200"
  },
  {
    "text": "so another optimization that we've added is a synchronous execution mode",
    "start": "1163200",
    "end": "1168720"
  },
  {
    "text": "so um this is useful for when when we're running we want to run stable",
    "start": "1168720",
    "end": "1174160"
  },
  {
    "text": "experiments with preemptable instances so with population based training",
    "start": "1174160",
    "end": "1179520"
  },
  {
    "text": "usually what happens is that each trial will run in parallel with its own hyper parameter configuration then",
    "start": "1179520",
    "end": "1186640"
  },
  {
    "text": "at the point it reaches a certain interval it will compare its metric against the metrics of the other",
    "start": "1186640",
    "end": "1193520"
  },
  {
    "text": "current trials and if it's in the bottom percentile population based training",
    "start": "1193520",
    "end": "1198559"
  },
  {
    "text": "will carry we'll copy the state from the best performing trial into the worst performing trial and then",
    "start": "1198559",
    "end": "1206320"
  },
  {
    "text": "perturb the hyper parameter slightly and then that trial will continue training but this could be a problem when the",
    "start": "1206320",
    "end": "1213840"
  },
  {
    "text": "different trials are running in wildly different speeds so for example if you consider working",
    "start": "1213840",
    "end": "1219039"
  },
  {
    "text": "with preemptable instances and let's say one of your current trials crashes because you lose",
    "start": "1219039",
    "end": "1224960"
  },
  {
    "text": "you lose the spot instance this could lead to unstable experiments because they're the different trials are no",
    "start": "1224960",
    "end": "1231919"
  },
  {
    "text": "longer running at are running synchronously with each other so the the trial that crashed and restarted",
    "start": "1231919",
    "end": "1238480"
  },
  {
    "text": "it's always going to be behind the other trials and therefore it's always going to perform worse because they're never synchronized to the same",
    "start": "1238480",
    "end": "1243520"
  },
  {
    "text": "epoch so the way we can avoid this is through a synchronous execution mode so we basically add a synchronous",
    "start": "1243520",
    "end": "1249440"
  },
  {
    "text": "barrier at every uh perturbation and we only perturb the trials",
    "start": "1249440",
    "end": "1254880"
  },
  {
    "text": "once all once all runs have reached the same epoch so in this example even though the even",
    "start": "1254880",
    "end": "1261600"
  },
  {
    "text": "though the 0.3 hyper parameter trial crashed the other trials won't continue executing until",
    "start": "1261600",
    "end": "1268159"
  },
  {
    "text": "that crash trial restarts and catches up and then once all the trials are",
    "start": "1268159",
    "end": "1273360"
  },
  {
    "text": "synchronized the same step then we can actually perform the perturbation and continue training",
    "start": "1273360",
    "end": "1279600"
  },
  {
    "text": "so this allows us to run stable experiments with preemptable instances",
    "start": "1279600",
    "end": "1284640"
  },
  {
    "start": "1284000",
    "end": "1674000"
  },
  {
    "text": "so lastly we'll go over a new algorithm that we've added to ray tune called population based bandits",
    "start": "1285280",
    "end": "1291760"
  },
  {
    "text": "uh which is very recently added and was and is uh published in europe's 2020.",
    "start": "1291760",
    "end": "1298159"
  },
  {
    "text": "so while population based training is very efficient and provides good",
    "start": "1298159",
    "end": "1303919"
  },
  {
    "text": "accuracy benefits over other high parameter optimization approaches it still underperforms for a",
    "start": "1303919",
    "end": "1309520"
  },
  {
    "text": "low population size if you see and as we see here for a population size of 10",
    "start": "1309520",
    "end": "1315200"
  },
  {
    "text": "it's actually underperforming a random search baseline so the way we can avoid this problem is",
    "start": "1315200",
    "end": "1322159"
  },
  {
    "text": "by combining pbt with asian optimization and this is an approach that was created by",
    "start": "1322159",
    "end": "1330640"
  },
  {
    "text": "students at oxford university and has contributed to tuned by an external contributor jack who you can see here who's a phd",
    "start": "1330640",
    "end": "1337360"
  },
  {
    "text": "student in oxford so he contributed this new algorithm to ray tune and raytoon is actually the",
    "start": "1337360",
    "end": "1342960"
  },
  {
    "text": "only library that currently has population based bandits as a search algorithm",
    "start": "1342960",
    "end": "1351600"
  },
  {
    "text": "so the way that pb2 or population-based bandits works is by combining the best of both population-based training and",
    "start": "1351600",
    "end": "1357760"
  },
  {
    "text": "bayesian optimization so recall that for population-based",
    "start": "1357760",
    "end": "1363679"
  },
  {
    "text": "training when we perturb a trial we copy over the network weights from a",
    "start": "1363679",
    "end": "1370320"
  },
  {
    "text": "high performing trial into a low performing one and then we slightly modify the hyper parameters and then continue training",
    "start": "1370320",
    "end": "1377120"
  },
  {
    "text": "but pbt's acui's heal comes from its reliance on heuristics for exploring the hyperparameter space",
    "start": "1377120",
    "end": "1383280"
  },
  {
    "text": "and this essentially creates new meta parameters now the question becomes how much do we perturb or mutate the hyper",
    "start": "1383280",
    "end": "1389280"
  },
  {
    "text": "parameter when we're doing this uh uh doing this perturbation step and those meta parameters also needs to",
    "start": "1389280",
    "end": "1395679"
  },
  {
    "text": "be tuned as well and because of this it could be the case that pbt",
    "start": "1395679",
    "end": "1401120"
  },
  {
    "text": "underperforms a random baseline without with low population size since small",
    "start": "1401120",
    "end": "1407120"
  },
  {
    "text": "populations can collapse to a sub-optimal mode so instead pb2 actually uses a bayesian",
    "start": "1407120",
    "end": "1414840"
  },
  {
    "text": "optimization they used an optimizer to suggest the new hyper parameters during the",
    "start": "1414840",
    "end": "1420320"
  },
  {
    "text": "perturbation step so just like with pbt we copied over the network weights from a high performing",
    "start": "1420320",
    "end": "1425600"
  },
  {
    "text": "trial into a low performing trial but instead of using a heuristic approach uh to",
    "start": "1425600",
    "end": "1430960"
  },
  {
    "text": "modify the hyper parameters for the new trial we actually have a bayesian optimizer",
    "start": "1430960",
    "end": "1436000"
  },
  {
    "text": "which is uh which learns the objective function on all historical data just like a traditional bayesian optimizer we talked",
    "start": "1436000",
    "end": "1442559"
  },
  {
    "text": "about earlier to suggest a new hyper parameter configuration",
    "start": "1442559",
    "end": "1448158"
  },
  {
    "text": "so the key difference between pb2 and pbt is that population based bandits can select new",
    "start": "1448240",
    "end": "1453600"
  },
  {
    "text": "hyper parameters from anywhere in the search space right so the heuristic based",
    "start": "1453600",
    "end": "1459039"
  },
  {
    "text": "perturbation approach that population-based training uses means that we only",
    "start": "1459039",
    "end": "1464720"
  },
  {
    "text": "we start from a good optimal hypo parameter point but then we incrementally uh modified or perturb it",
    "start": "1464720",
    "end": "1472400"
  },
  {
    "text": "over time which means that it might take a very long time to reach the optimal configuration well for pb2 because we're using evasion",
    "start": "1472400",
    "end": "1478880"
  },
  {
    "text": "optimizer instead of just incrementally modifying hyper parameter we can actually reach the optimal configuration",
    "start": "1478880",
    "end": "1485760"
  },
  {
    "text": "potentially at a much faster rate so does pb2 actually work",
    "start": "1485760",
    "end": "1491039"
  },
  {
    "text": "well yeah it is very effective in research constrained settings specifically for low population sizes",
    "start": "1491039",
    "end": "1497520"
  },
  {
    "text": "and as you can see here for a population size of four pv2 vastly outperforms population-based training",
    "start": "1497520",
    "end": "1504559"
  },
  {
    "text": "and even for a population size of eight while pbt does better pb2 still",
    "start": "1504559",
    "end": "1511919"
  },
  {
    "text": "performs better or at the same and here we can see another experiment comparing the two",
    "start": "1511919",
    "end": "1518080"
  },
  {
    "text": "so this is a reinforcement learning bipedal walker and this is the policy behavior for ppo",
    "start": "1518080",
    "end": "1524480"
  },
  {
    "text": "at a million time steps so here we can see that pv2 has a reward of 270",
    "start": "1524480",
    "end": "1530080"
  },
  {
    "text": "while pbt has a reward of 220. so pb2 is outperforming a population-based training and this is a run with four population",
    "start": "1530080",
    "end": "1538000"
  },
  {
    "text": "size of four and what pb2 allows us to do is actually",
    "start": "1538000",
    "end": "1543840"
  },
  {
    "text": "search over an extended range for exploration right and as we saw earlier that because we're not doing incremental",
    "start": "1543840",
    "end": "1550000"
  },
  {
    "text": "perturbations and we're actually uh using a bayesian optimizer to suggest perturbations we can extend our search",
    "start": "1550000",
    "end": "1557039"
  },
  {
    "text": "ranges from uh to a much larger size and we can still get to the optimal configuration at a",
    "start": "1557039",
    "end": "1564640"
  },
  {
    "text": "faster rate so here we can ex this is copying over the experiment that we ran earlier except",
    "start": "1564640",
    "end": "1571279"
  },
  {
    "text": "we're extending the batch size range from 1k to 60k to 5k to 200k so a much larger range",
    "start": "1571279",
    "end": "1577840"
  },
  {
    "text": "and we're seeing the pbt does significantly worse here compared to pb2 so how can we use pb2",
    "start": "1577840",
    "end": "1585760"
  },
  {
    "text": "with tune well it's very simple all we do is replace the scheduler with the what was population based training with pb2 and",
    "start": "1585760",
    "end": "1592320"
  },
  {
    "text": "that's it no other changes the code needs to be made so let's take a let's do a recap of what",
    "start": "1592320",
    "end": "1598960"
  },
  {
    "text": "we will be talking about today so we went over how population-based training and population-based bandits",
    "start": "1598960",
    "end": "1604799"
  },
  {
    "text": "are two cost-effective approaches for hyperparameter tuning and they work by uh continuing training",
    "start": "1604799",
    "end": "1612559"
  },
  {
    "text": "uh by copying over the state of the network weights from what good performing trials to bad performing",
    "start": "1612559",
    "end": "1618240"
  },
  {
    "text": "trials and specifically population-based bandits outperforms pbt for a smaller population size because it",
    "start": "1618240",
    "end": "1625279"
  },
  {
    "text": "uses a bayesian optimizer to suggest perturbations rather than doing incremental updates",
    "start": "1625279",
    "end": "1631360"
  },
  {
    "text": "and we also saw how we can use ray tune to quick quickly and easily run distributed tuning experiments",
    "start": "1631360",
    "end": "1637200"
  },
  {
    "text": "and try out all the different algorithms that we talked about today so what's next yeah so try out raid 2",
    "start": "1637200",
    "end": "1644720"
  },
  {
    "text": "and some of our pbt and pb2 examples they're all located on our github repository",
    "start": "1644720",
    "end": "1650640"
  },
  {
    "text": "and you can also use ray's cluster launcher to run multi-node experiments in the cloud and for any follow-up questions you can",
    "start": "1650640",
    "end": "1658399"
  },
  {
    "text": "look into our discourse discuss radar io and we'd be happy to help you out with",
    "start": "1658399",
    "end": "1663840"
  },
  {
    "text": "any questions you have yeah so that's that's all i have for today so thank you for listening to my",
    "start": "1663840",
    "end": "1669120"
  },
  {
    "text": "talk and um we can go into some q a now thank you thank you amok that was",
    "start": "1669120",
    "end": "1676240"
  },
  {
    "start": "1674000",
    "end": "1709000"
  },
  {
    "text": "very interesting talk [Music]",
    "start": "1676240",
    "end": "1681279"
  },
  {
    "text": "hey uh any questions um yeah for people attending if you have any questions",
    "start": "1681279",
    "end": "1686480"
  },
  {
    "text": "please post down ga now post on q a thank you so uh i'm asking a question uh",
    "start": "1686480",
    "end": "1694880"
  },
  {
    "text": "so for these uh different tuning algorithms um can you share some experience of",
    "start": "1694880",
    "end": "1701919"
  },
  {
    "text": "those being used in uh you know different production environments from the users that you have talked to",
    "start": "1701919",
    "end": "1709679"
  },
  {
    "start": "1709000",
    "end": "1776000"
  },
  {
    "text": "yeah so yeah that's a good question um right so tune is used uh not only by",
    "start": "1709679",
    "end": "1715520"
  },
  {
    "text": "researchers uh uh but also in like production environments and for uh actual uh uh those kind of use cases so",
    "start": "1715520",
    "end": "1723600"
  },
  {
    "text": "one of the i mean as we talked about in the top like hyper parameter tuning is incredibly important for good model performance and",
    "start": "1723600",
    "end": "1729919"
  },
  {
    "text": "it's very very useful to make sure that the models that we're training have good accuracy and we need to make sure",
    "start": "1729919",
    "end": "1735840"
  },
  {
    "text": "that we can tune uh these type of parameters efficiently um so ray tune is perfect for you know not",
    "start": "1735840",
    "end": "1741520"
  },
  {
    "text": "only for running like small experiments for uh for like hobbyists or for researchers but also",
    "start": "1741520",
    "end": "1747919"
  },
  {
    "text": "using it as part of a production environment as well",
    "start": "1747919",
    "end": "1753840"
  },
  {
    "text": "okay thank you um any more questions from the attendees i see",
    "start": "1754960",
    "end": "1760799"
  },
  {
    "text": "one coming up so for reinforcement learning",
    "start": "1760799",
    "end": "1766240"
  },
  {
    "text": "specifically what are the sensible default um hyper parameters for ppt or pb2",
    "start": "1766240",
    "end": "1772559"
  },
  {
    "text": "oh actually two questions coming up i will let you answer them okay yeah",
    "start": "1772559",
    "end": "1778480"
  },
  {
    "start": "1776000",
    "end": "1827000"
  },
  {
    "text": "great um yeah so the first question so for reinforcement learning what are some sensible default hyper parameters for uh pvt and pb2",
    "start": "1778480",
    "end": "1785919"
  },
  {
    "text": "so actually for pv2 one of the main benefits is that there's actually less meta parameters you have to tune",
    "start": "1785919",
    "end": "1794480"
  },
  {
    "text": "so you only have to provide a bounds for your hyper parameters and as opposed to like some sort of",
    "start": "1794480",
    "end": "1800880"
  },
  {
    "text": "probabilities for mutations or things like that and specifically for what balance to provide for the hyper parameters",
    "start": "1800880",
    "end": "1807600"
  },
  {
    "text": "that depends on on the the task that you're trying to train for um usually how i do it is um going",
    "start": "1807600",
    "end": "1814720"
  },
  {
    "text": "through some research papers or some blog posts seeing what hyper parameters worked out for them",
    "start": "1814720",
    "end": "1819760"
  },
  {
    "text": "and then sort of uh doing like an extended range around those optimal hyper parameters and searching over that area um",
    "start": "1819760",
    "end": "1828080"
  },
  {
    "start": "1827000",
    "end": "1884000"
  },
  {
    "text": "yeah so hopefully that answers your question [Music] so yeah next question can pbt and pb2 be",
    "start": "1828080",
    "end": "1834799"
  },
  {
    "text": "used with cross validation do you have any examples yeah this is a good question",
    "start": "1834799",
    "end": "1840320"
  },
  {
    "text": "um so currently tune does not have built-in",
    "start": "1840320",
    "end": "1846120"
  },
  {
    "text": "cross-validation capabilities but if you're using another library like uh",
    "start": "1846120",
    "end": "1852480"
  },
  {
    "text": "scikit-learn optimize i think that has a cross-validation and you can use that with tune um so i'm not sure if",
    "start": "1852480",
    "end": "1860799"
  },
  {
    "text": "pb so i don't think pb2 pvt and pb2 are available in like scikit-learn optimize",
    "start": "1860799",
    "end": "1866320"
  },
  {
    "text": "for example uh so i'm not sure if this is possible currently but uh that is a good thing that that's definitely a good",
    "start": "1866320",
    "end": "1872159"
  },
  {
    "text": "thing that we should look into um adding like built-in cross-validation support with raytune yes thanks",
    "start": "1872159",
    "end": "1880320"
  },
  {
    "text": "roland for your question um so the next one is ray tuna search and",
    "start": "1880320",
    "end": "1888240"
  },
  {
    "start": "1884000",
    "end": "1939000"
  },
  {
    "text": "scheduling components how does pb2 separate search component and scheduling component",
    "start": "1888240",
    "end": "1894000"
  },
  {
    "text": "yes that's a good question as well um so pb2 handles both involves both the searching and the",
    "start": "1894000",
    "end": "1899679"
  },
  {
    "text": "scheduling um and then if you're interested in like the technical details it's only considered to be",
    "start": "1899679",
    "end": "1905039"
  },
  {
    "text": "a scheduler you don't use like a separate search algorithm when you're using a pbt or pb2 um",
    "start": "1905039",
    "end": "1911440"
  },
  {
    "text": "so so it involves scheduling because it not only has to uh modify like uh like stop for example",
    "start": "1911440",
    "end": "1918080"
  },
  {
    "text": "has to stop trials early or it modifies execution of the trials but then",
    "start": "1918080",
    "end": "1923200"
  },
  {
    "text": "it's also responsible for suggesting new hyper parameters um so they're both built in together as",
    "start": "1923200",
    "end": "1929600"
  },
  {
    "text": "like a scheduler but it involves yeah like both the scheduling as well as a search component",
    "start": "1929600",
    "end": "1935360"
  },
  {
    "text": "in one one library okay so next question is",
    "start": "1935360",
    "end": "1942720"
  },
  {
    "start": "1939000",
    "end": "1980000"
  },
  {
    "text": "how to work with large data sets since pb2 is not having good performance uh",
    "start": "1942720",
    "end": "1949279"
  },
  {
    "text": "what do you consider a large and a small data set um",
    "start": "1949279",
    "end": "1956640"
  },
  {
    "text": "yeah this is an uh interesting question i think for this one actually i'd suggest",
    "start": "1956640",
    "end": "1962240"
  },
  {
    "text": "uh to reach out to the pb2 authors for more specifics about uh what are some failure modes about pb2",
    "start": "1962240",
    "end": "1969760"
  },
  {
    "text": "and uh where you would uh what yeah where where pb2 would shine the most and like some",
    "start": "1969760",
    "end": "1976159"
  },
  {
    "text": "some cases where it wouldn't necessarily work that well um",
    "start": "1976159",
    "end": "1980960"
  },
  {
    "start": "1980000",
    "end": "2019000"
  },
  {
    "text": "yeah i think the the main benefit of pb2 is working with small population sizes but the",
    "start": "1981200",
    "end": "1987840"
  },
  {
    "text": "dataset sizes are uh interesting and i'm not really sh yeah i",
    "start": "1987840",
    "end": "1992960"
  },
  {
    "text": "don't have a good answer to your question unfortunately but i think the the pb2 authors would be",
    "start": "1992960",
    "end": "1998159"
  },
  {
    "text": "happy to help out here and then um yeah so if you you can email jack who's the main author on pb2 and i'm and",
    "start": "1998159",
    "end": "2005360"
  },
  {
    "text": "you'd be happy to help out by answering your question there okay so the next one is how does pbt slash pb2",
    "start": "2005360",
    "end": "2013840"
  },
  {
    "text": "uh handle cases where rollout is expensive",
    "start": "2013840",
    "end": "2019200"
  },
  {
    "start": "2019000",
    "end": "2059000"
  },
  {
    "text": "uh yeah so that's right yeah so that's another good question um",
    "start": "2019200",
    "end": "2024640"
  },
  {
    "text": "i think in cases where rollout is expensive you probably want to use a smaller population size",
    "start": "2024640",
    "end": "2030480"
  },
  {
    "text": "uh in which case like population based bandits would work the best um so they both work and regardless how",
    "start": "2030480",
    "end": "2038000"
  },
  {
    "text": "expensive the training is it's just a matter of uh sort of trying to have efficient um",
    "start": "2038000",
    "end": "2044320"
  },
  {
    "text": "hyper parameter search if if your rollout is expensive so i think uh in that case you want to use",
    "start": "2044320",
    "end": "2050000"
  },
  {
    "text": "population based bandits would be the best for your scenario um yeah hopefully that answers your and",
    "start": "2050000",
    "end": "2056800"
  },
  {
    "text": "answers your question there okay so next one is uh in cnn do you have any benchmarks for",
    "start": "2056800",
    "end": "2065200"
  },
  {
    "start": "2059000",
    "end": "2109000"
  },
  {
    "text": "the time to converge between pb2 and pvt yeah so so um if you look at the",
    "start": "2065200",
    "end": "2070240"
  },
  {
    "text": "original population based bandits paper uh which is on archive",
    "start": "2070240",
    "end": "2075280"
  },
  {
    "text": "um they mostly do experiments comparing pbt and pb2 for reinforcement learning",
    "start": "2075280",
    "end": "2080638"
  },
  {
    "text": "um so i don't think they actually have any uh benchmarks for convolutional neural networks now for",
    "start": "2080639",
    "end": "2087200"
  },
  {
    "text": "image processing examples um uh so it's mostly in reinforcement",
    "start": "2087200",
    "end": "2092878"
  },
  {
    "text": "learning experiments uh yeah i don't have any more to",
    "start": "2092879",
    "end": "2098000"
  },
  {
    "text": "to add to that i think if if you're interested in specifically for cnn's it's another good",
    "start": "2098000",
    "end": "2103520"
  },
  {
    "text": "question to reach out to jack who is the main author for uh population-based bandits so",
    "start": "2103520",
    "end": "2110400"
  },
  {
    "start": "2109000",
    "end": "2194000"
  },
  {
    "text": "okay so follow up to a previous question uh what are some sensible lower bounds on the number of population samples to",
    "start": "2110400",
    "end": "2116480"
  },
  {
    "text": "use for reinforcement learning and then when specifying the hyperparameter balance for pbt should we take care",
    "start": "2116480",
    "end": "2122400"
  },
  {
    "text": "of the underlying metrics of the space uh example using a log transform on the",
    "start": "2122400",
    "end": "2127520"
  },
  {
    "text": "bound for learning right yeah so these are good questions so a sensible lower bound um i think if you're using pv2 uh like four",
    "start": "2127520",
    "end": "2135520"
  },
  {
    "text": "trials would be a good lower bound for that um and in pbt you probably want to raise it to something more like maybe like eight",
    "start": "2135520",
    "end": "2142000"
  },
  {
    "text": "to 16 trials um but it really depends uh for pbt on the initial random and the",
    "start": "2142000",
    "end": "2148480"
  },
  {
    "text": "randomness that's involved in the in the initial uh hyperparameter configurations um for you know if one trial is randomly",
    "start": "2148480",
    "end": "2155040"
  },
  {
    "text": "initialized with a good uh high parameter config that's like near the optimal then it's you know you",
    "start": "2155040",
    "end": "2161359"
  },
  {
    "text": "can you can uh get to the most optimal configuration with a smaller population size",
    "start": "2161359",
    "end": "2166960"
  },
  {
    "text": "um but you know for specifically for lower bound uh i think",
    "start": "2166960",
    "end": "2172000"
  },
  {
    "text": "four is a good number for pv2 and for the second point about um should",
    "start": "2172000",
    "end": "2177359"
  },
  {
    "text": "we use a log transform on the bound for learning rate yeah i think it's a good idea uh i think um working in log space uh",
    "start": "2177359",
    "end": "2186160"
  },
  {
    "text": "is definitely helps for both the pvt and for pb2 as well",
    "start": "2186160",
    "end": "2191359"
  },
  {
    "text": "okay so does pbt slash pb2 now support tune resume the codebase previously",
    "start": "2193440",
    "end": "2199359"
  },
  {
    "start": "2194000",
    "end": "2227000"
  },
  {
    "text": "suggested that resume functionality may not work for uh pbt",
    "start": "2199359",
    "end": "2205599"
  },
  {
    "text": "um so i think you should tune resume should work for both pbt slash pb2 um yeah i'm not sure what",
    "start": "2205599",
    "end": "2212880"
  },
  {
    "text": "what version of ray you were using previously but um uh i think it should work and if not i",
    "start": "2212880",
    "end": "2218320"
  },
  {
    "text": "can follow up again with you uh yeah and i can follow up again with you regarding",
    "start": "2218320",
    "end": "2223920"
  },
  {
    "text": "this question what metric do you usually use for",
    "start": "2223920",
    "end": "2230240"
  },
  {
    "start": "2227000",
    "end": "2277000"
  },
  {
    "text": "training uh using the last validation loss might be misleading if the model is over trained",
    "start": "2230240",
    "end": "2236000"
  },
  {
    "text": "uh would be the best validation loss across the epochs what are your thoughts on this yeah so this is a good question um so",
    "start": "2236000",
    "end": "2242400"
  },
  {
    "text": "like you said validation the last validation loss might be misleading um so you could do the the best validation",
    "start": "2242400",
    "end": "2249280"
  },
  {
    "text": "loss across all the previous ebooks or an average of the validation loss across",
    "start": "2249280",
    "end": "2254720"
  },
  {
    "text": "the last and epochs for example uh that's another alternative you could use there um",
    "start": "2254720",
    "end": "2260560"
  },
  {
    "text": "so both so yeah all of those should should work and using average you might might",
    "start": "2260560",
    "end": "2267920"
  },
  {
    "text": "be better in cases where the last validation loss is uh is misleading in case the model is over",
    "start": "2267920",
    "end": "2273280"
  },
  {
    "text": "trained okay so i think there's a one more",
    "start": "2273280",
    "end": "2278400"
  },
  {
    "start": "2277000",
    "end": "2304000"
  },
  {
    "text": "question is there an alternative to syncing experiments during hyperparameter search uh assuming there",
    "start": "2278400",
    "end": "2284720"
  },
  {
    "text": "are no free resources in the cloud especially when preemptible instances are used the whole process would be stuck um",
    "start": "2284720",
    "end": "2294400"
  },
  {
    "text": "right so so i think i think what this is asking is basically um",
    "start": "2294880",
    "end": "2301599"
  },
  {
    "text": "so if uh yeah so correct me if i'm wrong here but i think this is asking for a",
    "start": "2303119",
    "end": "2308960"
  },
  {
    "start": "2304000",
    "end": "2370000"
  },
  {
    "text": "synchronized execution mode in case one of the trials crashes",
    "start": "2308960",
    "end": "2314160"
  },
  {
    "text": "and you can't restart that if i'm understanding correctly the whole process would be stuck yeah",
    "start": "2314160",
    "end": "2319680"
  },
  {
    "text": "and that is true if if one of the trials does crash and if you're running in synchronized execution",
    "start": "2319680",
    "end": "2325359"
  },
  {
    "text": "mode and then it can't be restarted again because uh you can't get a new up to a",
    "start": "2325359",
    "end": "2331119"
  },
  {
    "text": "new spot instance for example then the experiment would be stuck and you'd have to restart that yeah so that is one drawback of",
    "start": "2331119",
    "end": "2336800"
  },
  {
    "text": "using a synchronized execution mode but on the flip side um it will result",
    "start": "2336800",
    "end": "2342320"
  },
  {
    "text": "in a more fair experiment since all of the trials will be synchronized at the same time step before",
    "start": "2342320",
    "end": "2348960"
  },
  {
    "text": "perturbation occurs so even if they're running at different speeds you can still make sure that uh",
    "start": "2348960",
    "end": "2354320"
  },
  {
    "text": "the perturbation comparison between the performance of the trials will only occur when they're all at the",
    "start": "2354320",
    "end": "2360079"
  },
  {
    "text": "same time step um so yeah let me know yeah let me know if i and if that answer your question or",
    "start": "2360079",
    "end": "2365680"
  },
  {
    "text": "not uh if i understood your question correctly okay so there's a couple additional",
    "start": "2365680",
    "end": "2372000"
  },
  {
    "start": "2370000",
    "end": "2456000"
  },
  {
    "text": "questions um first one defined by search slash run um",
    "start": "2372000",
    "end": "2377839"
  },
  {
    "text": "i'm not sure what exactly that's referring to um and for the second question if the",
    "start": "2377839",
    "end": "2384800"
  },
  {
    "text": "rollout environment changes slightly over time so each rollout could experience experience slight out of distribution uh",
    "start": "2384800",
    "end": "2392560"
  },
  {
    "text": "does pb2 handle that well that's that's an interesting question and uh unfortunately don't have the",
    "start": "2392560",
    "end": "2398560"
  },
  {
    "text": "answer to that specifically right now um once again i think this is a good question to to um email jack about um",
    "start": "2398560",
    "end": "2405599"
  },
  {
    "text": "and uh you know i've worked with jack and be happy to answer all of these all these questions about pb2 specifics",
    "start": "2405599",
    "end": "2411839"
  },
  {
    "text": "as well um and yeah jack is the is the main author of the pb2 paper",
    "start": "2411839",
    "end": "2419760"
  },
  {
    "text": "okay sounds great um so i have two suggestions one is uh",
    "start": "2420240",
    "end": "2426480"
  },
  {
    "text": "for uh you know in-depth discussions uh we also have a slack journal i just pasted the sign up form on the chat",
    "start": "2426480",
    "end": "2435119"
  },
  {
    "text": "and also uh on nacio.com events um you can subscribe to more events",
    "start": "2435119",
    "end": "2441599"
  },
  {
    "text": "that's coming up thank you so much everyone actually",
    "start": "2441599",
    "end": "2447760"
  },
  {
    "text": "there are two [Music] thank you all right thank you have a nice one",
    "start": "2447760",
    "end": "2454400"
  },
  {
    "text": "great to have you here",
    "start": "2454400",
    "end": "2458240"
  }
]