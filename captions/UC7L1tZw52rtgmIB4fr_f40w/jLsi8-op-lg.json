[
  {
    "text": "hello everyone uh welcome to the session my name is Matthew Deng I am a software engineering manager here at anyscale and",
    "start": "2840",
    "end": "10160"
  },
  {
    "text": "in today's talk I'll be going over the ray train Library as a solution for overcoming challenges for efficient",
    "start": "10160",
    "end": "16680"
  },
  {
    "text": "training at scale so the agenda for this talk uh",
    "start": "16680",
    "end": "22880"
  },
  {
    "text": "I'll start by giving a little bit of background in some of the model training Trends in recent years and then",
    "start": "22880",
    "end": "29519"
  },
  {
    "text": "transition into talking about the challenges that we're seeing in distributed training today uh with this",
    "start": "29519",
    "end": "36120"
  },
  {
    "text": "we'll transition to an introduction of Ray train and go over how it solves the challenges in distributed training",
    "start": "36120",
    "end": "42680"
  },
  {
    "text": "namely through Ray trains Integrations observability fault tolerance and data ingestion capabilities to wrap it up",
    "start": "42680",
    "end": "50640"
  },
  {
    "text": "we'll do a quick case study and review how we did pre-training with stable diffusion with Ray train let's get",
    "start": "50640",
    "end": "58920"
  },
  {
    "text": "started so a little bit of background of model training Trends it should be obvious by now that in recent years AI",
    "start": "58920",
    "end": "66760"
  },
  {
    "text": "has grown significantly we've seen larger models with a great boom in generative AI",
    "start": "66760",
    "end": "73200"
  },
  {
    "text": "especially in the past two years similarly we've seen a massive growth in",
    "start": "73200",
    "end": "78320"
  },
  {
    "text": "data needed for training such large models and not just in the quantity of data but also in different types of data",
    "start": "78320",
    "end": "85200"
  },
  {
    "text": "modalities such as text images audio and video",
    "start": "85200",
    "end": "91079"
  },
  {
    "text": "and yes there's been a similar increase in the amount of accelerators that are created uh to support this type of large",
    "start": "91079",
    "end": "99200"
  },
  {
    "text": "scale model training with large data but one thing is inevitable even with these",
    "start": "99200",
    "end": "105079"
  },
  {
    "text": "advancements in Hardware distributed training is necessary in order to handle",
    "start": "105079",
    "end": "110360"
  },
  {
    "text": "the scale at which we need to compute model training so if we need distributed",
    "start": "110360",
    "end": "118079"
  },
  {
    "text": "training what are the common challenges that you might run into when scaling from a single instance to multiple",
    "start": "118079",
    "end": "125320"
  },
  {
    "text": "instances some of the top challenges include setting up distributed",
    "start": "125320",
    "end": "130679"
  },
  {
    "text": "environments for your particular ml framework but also in connecting with",
    "start": "130680",
    "end": "136200"
  },
  {
    "text": "your specific ml workflow you might also run into trouble",
    "start": "136200",
    "end": "141400"
  },
  {
    "text": "debugging distributed systems now as well as figuring out how do you properly",
    "start": "141400",
    "end": "147200"
  },
  {
    "text": "handle Hardware failures in addition to this there's also this",
    "start": "147200",
    "end": "152720"
  },
  {
    "text": "new challenge in which you need to manage large data complexity at such scale so how do we tackle these",
    "start": "152720",
    "end": "161200"
  },
  {
    "text": "challenges for the rest of this talk I'm going to describe Ray train a solution that we've built uh in Ray to handle",
    "start": "161200",
    "end": "168280"
  },
  {
    "text": "large scale training so the first question that you're going to ask is what is Ray",
    "start": "168280",
    "end": "174200"
  },
  {
    "text": "train Ray train is a library that makes distributed training easy it works with",
    "start": "174200",
    "end": "180400"
  },
  {
    "text": "all of your favorite ml Frameworks and enables you to do distributed training with your framework of choice in",
    "start": "180400",
    "end": "187599"
  },
  {
    "text": "addition to that it's built on top of Ray which allows you to know that you'll",
    "start": "187599",
    "end": "193159"
  },
  {
    "text": "be able to scale your distributed training across whichever compute you may be",
    "start": "193159",
    "end": "199720"
  },
  {
    "text": "using with this context let's dive into how it solves the challenges we mentioned",
    "start": "199840",
    "end": "206360"
  },
  {
    "text": "earlier first let's talk about setting the challenge of setting up your distributed environment to do this I'm",
    "start": "206360",
    "end": "213640"
  },
  {
    "text": "going to do a quick example a quick start of Ray train so let's say you have some code",
    "start": "213640",
    "end": "220959"
  },
  {
    "text": "that you need to write how do you do this with Ray training the first step is that you'll",
    "start": "220959",
    "end": "227000"
  },
  {
    "text": "want to Define your training function in which you have your model training code in this example we commented out but you",
    "start": "227000",
    "end": "235560"
  },
  {
    "text": "might have your pytorch training code your tensorflow training code or your ex boost training",
    "start": "235560",
    "end": "240920"
  },
  {
    "text": "code whatever that may be it should be familiar to you as someone who's already trained their model before in a single",
    "start": "240920",
    "end": "249040"
  },
  {
    "text": "instance the Second Step from there is that you'll want to start thinking about how do you configure",
    "start": "249040",
    "end": "254840"
  },
  {
    "text": "scale from Ray train we provide a simple interface called the scaling config",
    "start": "254840",
    "end": "260359"
  },
  {
    "text": "which allows you to define the scale at which you want to run your distributed training in just one line of",
    "start": "260359",
    "end": "266040"
  },
  {
    "text": "python in this example we have a scaling config that scales across four gpus and",
    "start": "266040",
    "end": "272240"
  },
  {
    "text": "it really is as simple as that to show you a few more examples uh of how easy",
    "start": "272240",
    "end": "278000"
  },
  {
    "text": "it is consider that you might want to scale across different types of",
    "start": "278000",
    "end": "283199"
  },
  {
    "text": "accelerators in the first example we have on this screen we're simply scaling across a few CPU instances in which you",
    "start": "283199",
    "end": "291080"
  },
  {
    "text": "can Define how many CPUs you want each instance to use the second example shows if Beyond",
    "start": "291080",
    "end": "299000"
  },
  {
    "text": "just using G gpus you want to scale across a particular GPU type in this",
    "start": "299000",
    "end": "304680"
  },
  {
    "text": "case you just want to use A1 100s and the third example shows how you",
    "start": "304680",
    "end": "310520"
  },
  {
    "text": "would configure your scaling config if you want to use a different type of accelerator in this case this is scaling",
    "start": "310520",
    "end": "317800"
  },
  {
    "text": "across tpus so we talked about the training",
    "start": "317800",
    "end": "323800"
  },
  {
    "text": "function and the scaling configuration and the next step is to just package this up with the ray Trin Library",
    "start": "323800",
    "end": "330160"
  },
  {
    "text": "and you can do so by using array train trainer interface in this case we have a",
    "start": "330160",
    "end": "335960"
  },
  {
    "text": "p torch torch trainer and this is the uh abstraction that will help set up your",
    "start": "335960",
    "end": "342319"
  },
  {
    "text": "distributed torch environment for you to tie it all together you simply",
    "start": "342319",
    "end": "348759"
  },
  {
    "text": "can launch distributed training by calling trainer. fit let's dive into this and see what's",
    "start": "348759",
    "end": "356160"
  },
  {
    "text": "actually happening under the hood the first thing that's going to happen",
    "start": "356160",
    "end": "361400"
  },
  {
    "text": "when you call trainer. fit is Ray train will take a look at your scaling config and start scaling the cluster according",
    "start": "361400",
    "end": "368160"
  },
  {
    "text": "to the scale that you specified secondly it'll launch worker",
    "start": "368160",
    "end": "373199"
  },
  {
    "text": "processes on across the cluster again based on your scaling",
    "start": "373199",
    "end": "379319"
  },
  {
    "text": "configuration the next thing that will happen is that Ray train will set up the distributed environment for your",
    "start": "379400",
    "end": "385360"
  },
  {
    "text": "particular framework of choice in this case I have a torch trainer so setting up a pytorch distributed environment",
    "start": "385360",
    "end": "392039"
  },
  {
    "text": "across the worker processes next it'll take your",
    "start": "392039",
    "end": "397280"
  },
  {
    "text": "userdefined training function and start executing it across all of the worker",
    "start": "397280",
    "end": "403319"
  },
  {
    "text": "processes while running the training function this is where the distributed training actually happens in this case",
    "start": "403319",
    "end": "409840"
  },
  {
    "text": "if you have pytorch distributed and you're running pytorch code you could use any of your favorite F uh any of",
    "start": "409840",
    "end": "416160"
  },
  {
    "text": "your favorite strategies to do the distributor training whether it's pytorch DDP pytorch fsdp or deep speed",
    "start": "416160",
    "end": "422960"
  },
  {
    "text": "zero being a few popular examples of what users are doing",
    "start": "422960",
    "end": "428280"
  },
  {
    "text": "today and yes it's really that easy so we talked really quickly through",
    "start": "428319",
    "end": "435400"
  },
  {
    "text": "a quick start to show you what the simple interface of ready train looks like and how it helps with setting up a",
    "start": "435400",
    "end": "441080"
  },
  {
    "text": "distributed environment for training the next question that you might have is well that makes sense for",
    "start": "441080",
    "end": "448400"
  },
  {
    "text": "a quick start but but what about my particular workflow I have a very specific way of doing things does it is",
    "start": "448400",
    "end": "454919"
  },
  {
    "text": "it going to work for me and to answer that question I'll quickly just Show a slide that shows a",
    "start": "454919",
    "end": "461520"
  },
  {
    "text": "few examples of some of the Integrations that we've built with Ray train the reason why we've built all these",
    "start": "461520",
    "end": "467039"
  },
  {
    "text": "Integrations is because we know that the ml ecosystem is very fragmented and you",
    "start": "467039",
    "end": "472199"
  },
  {
    "text": "might have a different training framework your own choice of experiment tracking tool or a specific compute",
    "start": "472199",
    "end": "479120"
  },
  {
    "text": "system or storage layer that you need to support but fortunately Ray train was",
    "start": "479120",
    "end": "485280"
  },
  {
    "text": "built with all of these in mind to make sure that Ray train is composable with your ml",
    "start": "485280",
    "end": "492560"
  },
  {
    "text": "workflow the third challenge that we'll dive into now is a challenge that comes",
    "start": "494919",
    "end": "500080"
  },
  {
    "text": "when scaling up to multiple instances which is how do I debug a distributed",
    "start": "500080",
    "end": "505159"
  },
  {
    "text": "system in the first place",
    "start": "505159",
    "end": "509000"
  },
  {
    "text": "luckily Ray train comes with a built-in Ray dashboard the ray dashboard is a",
    "start": "510720",
    "end": "516360"
  },
  {
    "text": "dashboard that's meant for debugging all your favorite Ray applications it comes with a variety of functionality that",
    "start": "516360",
    "end": "522518"
  },
  {
    "text": "help makes distributed computing a lot easier to deal with for example it",
    "start": "522519",
    "end": "527959"
  },
  {
    "text": "provides monitoring in which you could view the status of Ray jobs actors and",
    "start": "527959",
    "end": "533920"
  },
  {
    "text": "tasks it provides metrics which allows you to see the overview of cluster resource utilization and Ray scheduling",
    "start": "533920",
    "end": "541480"
  },
  {
    "text": "State and allows you to do profiling in which you could profile individual Ray workers across your",
    "start": "541480",
    "end": "549000"
  },
  {
    "text": "cluster but wait there's more recently on any scale we just released array",
    "start": "549320",
    "end": "555519"
  },
  {
    "text": "train dashboard this takes debugging distributed systems to the next level and focuses on ways that you could do",
    "start": "555519",
    "end": "563560"
  },
  {
    "text": "debugging for distributed training in particular here's a few examples with",
    "start": "563560",
    "end": "570760"
  },
  {
    "text": "the ray train dashboard you can monitor specifically things that you care about",
    "start": "570760",
    "end": "575920"
  },
  {
    "text": "when you're doing distributed training such as the status of training runs and training",
    "start": "575920",
    "end": "581279"
  },
  {
    "text": "workers you can also see metrics that go a little bit deeper that have insights",
    "start": "581279",
    "end": "586880"
  },
  {
    "text": "on the training throughput and training system operation time and you could also do profiling",
    "start": "586880",
    "end": "593839"
  },
  {
    "text": "that's much more specific to distributed training workloads that allow you to investigate bottlenecks hangs or errors",
    "start": "593839",
    "end": "601160"
  },
  {
    "text": "from Individual training worker",
    "start": "601160",
    "end": "604959"
  },
  {
    "text": "processes sorry let's go through a quick example of how you might debug with the",
    "start": "610320",
    "end": "615800"
  },
  {
    "text": "ray train dashboard in this example I want to debug a hanging worker if you've done",
    "start": "615800",
    "end": "623519"
  },
  {
    "text": "training you might have seen this before your training has been running for a couple of hours and suddenly it just",
    "start": "623519",
    "end": "629320"
  },
  {
    "text": "just stops you have no idea why with the ray train dashboard we make",
    "start": "629320",
    "end": "635040"
  },
  {
    "text": "it very easy to profile each and individual worker but first let's say",
    "start": "635040",
    "end": "640079"
  },
  {
    "text": "what you might do without the ray train dashboard without the rate train dashboard you might need to you know",
    "start": "640079",
    "end": "647160"
  },
  {
    "text": "terminate your script add a few print statements rerun your script SSH into",
    "start": "647160",
    "end": "653120"
  },
  {
    "text": "individual machines check the logs or profile all the python processes that",
    "start": "653120",
    "end": "659040"
  },
  {
    "text": "you have running at that point in time you have to massage all the data figure out what's going on through this",
    "start": "659040",
    "end": "665000"
  },
  {
    "text": "distributed environment however with the ray train dashboard things are a lot easier now in",
    "start": "665000",
    "end": "672120"
  },
  {
    "text": "which you could easily navigate through the dashboard to find each train worker",
    "start": "672120",
    "end": "677600"
  },
  {
    "text": "and look at its particular stack trace or logs taking the same example that we had",
    "start": "677600",
    "end": "684240"
  },
  {
    "text": "from the previous slide you could see here that I have an ongoing run and I could look at the metadata to find",
    "start": "684240",
    "end": "690839"
  },
  {
    "text": "exactly where for example my ranks my Global rank zero worker is and I could",
    "start": "690839",
    "end": "696040"
  },
  {
    "text": "click into a particular actor to see the logs for that particular actor or I",
    "start": "696040",
    "end": "702079"
  },
  {
    "text": "could do some profiling right there from the UI it's as easy as",
    "start": "702079",
    "end": "708120"
  },
  {
    "text": "that the next challenge that I'm going to talk about after debugging is what",
    "start": "709040",
    "end": "714760"
  },
  {
    "text": "happens if I actually run into an issue what happens if I run into into a hardware",
    "start": "714760",
    "end": "721079"
  },
  {
    "text": "failure this is really important in distributed training because Hardware failures become a lot more prominent",
    "start": "721079",
    "end": "727560"
  },
  {
    "text": "when you're scaling to a large cluster examples of Hardware failures might include a GPO a GPU going down or",
    "start": "727560",
    "end": "736440"
  },
  {
    "text": "you might actually have a network Interruption such that your nodes are no longer able to communicate with each",
    "start": "736440",
    "end": "741839"
  },
  {
    "text": "other or if you're training with spot instances your instance might get preempted and at this point you might be",
    "start": "741839",
    "end": "748720"
  },
  {
    "text": "asking how would I handle that situation I'll show a quick very quick example of how Ray train implements",
    "start": "748720",
    "end": "755880"
  },
  {
    "text": "fault tolerance to make it extremely easy to recover from these type of issues and more in this example assume",
    "start": "755880",
    "end": "763320"
  },
  {
    "text": "that I'm training with Ray train on a cluster of four GPU nodes the training",
    "start": "763320",
    "end": "768519"
  },
  {
    "text": "is running very steadily but at some point in time a node",
    "start": "768519",
    "end": "773920"
  },
  {
    "text": "fails what will happen here is that Ray train will enter a recovering State and",
    "start": "773920",
    "end": "779399"
  },
  {
    "text": "it will pause and remove the node from the workers until it is able to get a",
    "start": "779399",
    "end": "785000"
  },
  {
    "text": "new node it'll stay requesting the GPU node until the cluster is able to scale",
    "start": "785000",
    "end": "790399"
  },
  {
    "text": "back up and once it does have its four nodes again it'll be able to resume",
    "start": "790399",
    "end": "795600"
  },
  {
    "text": "training there next I'll show you how it is able to be implemented in Ray train as the",
    "start": "795600",
    "end": "803880"
  },
  {
    "text": "user Ray train provides quite a few apis to make this fall tolerance flow very",
    "start": "803880",
    "end": "809680"
  },
  {
    "text": "very simple to do first it provides a checkpoint",
    "start": "809680",
    "end": "814720"
  },
  {
    "text": "interface and a report utility that allows you to save checkpoints from your distributed training workers and upload",
    "start": "814720",
    "end": "821800"
  },
  {
    "text": "them to a cloud storage location the second utility it provides",
    "start": "821800",
    "end": "827680"
  },
  {
    "text": "is a get checkpoint utility that allows you to load your most recently saved checkpoint onto your distributed",
    "start": "827680",
    "end": "836480"
  },
  {
    "text": "workers and third there's a Max failures configuration that you could set in",
    "start": "837040",
    "end": "842639"
  },
  {
    "text": "order to Define how many retries you want your training application to support and with that with these few",
    "start": "842639",
    "end": "850199"
  },
  {
    "text": "apis you're able to run distributed training with fall tolerance in addition to this Ray train",
    "start": "850199",
    "end": "857320"
  },
  {
    "text": "also makes it easy to restore your training application across clusters across jobs to do this it has a restore",
    "start": "857320",
    "end": "865360"
  },
  {
    "text": "API so if your entire Ray cluster happens to terminate during a training job you could simply relaunch the",
    "start": "865360",
    "end": "871720"
  },
  {
    "text": "program on a separate cluster and call trainer. Restore to the path where the",
    "start": "871720",
    "end": "877199"
  },
  {
    "text": "experiment was saved I'm also happy to announce that",
    "start": "877199",
    "end": "882959"
  },
  {
    "text": "recently on any scale we've taken this to the next level and released an extension of fault tolerance which is",
    "start": "882959",
    "end": "889519"
  },
  {
    "text": "elastic training elastic training means that you",
    "start": "889519",
    "end": "894720"
  },
  {
    "text": "could scale your training job to a dynamic number of work workers as",
    "start": "894720",
    "end": "899759"
  },
  {
    "text": "opposed to a static number of workers and when you do this this means",
    "start": "899759",
    "end": "906360"
  },
  {
    "text": "that you might have a max number of workers where your job will typically run but your job now supports being able",
    "start": "906360",
    "end": "912199"
  },
  {
    "text": "to train on fewer resources as well this works particularly well with",
    "start": "912199",
    "end": "918120"
  },
  {
    "text": "spot instances where the capacity of spot instances might change over time and you want to continue training even",
    "start": "918120",
    "end": "924320"
  },
  {
    "text": "when you don't have a static set of spot instances available to use",
    "start": "924320",
    "end": "930800"
  },
  {
    "text": "let's take a look at the example that we previously looked like looked at in the fa tolerance page and see what happens",
    "start": "930800",
    "end": "936959"
  },
  {
    "text": "when you have elastic training enabled my job starts training with four",
    "start": "936959",
    "end": "942720"
  },
  {
    "text": "gpus again again a node fails or a spot",
    "start": "942720",
    "end": "948040"
  },
  {
    "text": "instance is preempted whereas previously I would need to request another node and wait",
    "start": "948040",
    "end": "954880"
  },
  {
    "text": "whoever knows how long in order for that node to be added back into the training job and then resume training from there",
    "start": "954880",
    "end": "961680"
  },
  {
    "text": "with elastic training the ray train job can continue training on just three GPU",
    "start": "961680",
    "end": "967000"
  },
  {
    "text": "nodes it'll continue to request a new node and continue training on three GPU",
    "start": "967000",
    "end": "972600"
  },
  {
    "text": "nodes until it's able to add that fourth node back into the training job and from that point it'll then be",
    "start": "972600",
    "end": "979160"
  },
  {
    "text": "able to continue training again on that full capacity of four nodes the best part of this is that the",
    "start": "979160",
    "end": "987040"
  },
  {
    "text": "way to enable elastic training is again simply by changing one line of code in the scaling",
    "start": "987040",
    "end": "994399"
  },
  {
    "text": "configuration the last challenge that I'm going to talk about today in distributed training is how do I deal",
    "start": "997440",
    "end": "1004440"
  },
  {
    "text": "with large data complexity we all know that when you scale up your training workload scaling",
    "start": "1004440",
    "end": "1010240"
  },
  {
    "text": "the model is one problem but scaling the data is a whole Behemoth of its",
    "start": "1010240",
    "end": "1016800"
  },
  {
    "text": "own to go over how train solves this problem I want to introduce the int the",
    "start": "1016959",
    "end": "1023279"
  },
  {
    "text": "integration between the ray train library and the ray data Library another",
    "start": "1023279",
    "end": "1028438"
  },
  {
    "text": "ray library in the ray ecosystem that's built for large scale data processing if you look at the code",
    "start": "1028439",
    "end": "1035160"
  },
  {
    "text": "snippet on the right you could see kind of a quick example of how you could integrate Ray data with Ray train to",
    "start": "1035160",
    "end": "1042079"
  },
  {
    "text": "perform distributed data ingestion first you could take a look at",
    "start": "1042079",
    "end": "1047160"
  },
  {
    "text": "the training function and and think about what data processing and ingestion",
    "start": "1047160",
    "end": "1052320"
  },
  {
    "text": "you want on the training worker itself here Ray train provides a simple get data set chard API which allows you to",
    "start": "1052320",
    "end": "1059520"
  },
  {
    "text": "get the data set chard that will be processed and read and trained on for that particular",
    "start": "1059520",
    "end": "1065320"
  },
  {
    "text": "worker the data set chart itself has a simple API called iner torch batches",
    "start": "1065320",
    "end": "1070799"
  },
  {
    "text": "which will allow you to convert this directly into a torch data loader like object similarly there are other",
    "start": "1070799",
    "end": "1077679"
  },
  {
    "text": "Integrations for other framewor works now that we've defined what the training logic looks like and the how",
    "start": "1077679",
    "end": "1084159"
  },
  {
    "text": "the training worker might consume the data we also want to plug this together",
    "start": "1084159",
    "end": "1089280"
  },
  {
    "text": "by defining exactly what data set will be fed into training as you can see here you could",
    "start": "1089280",
    "end": "1095919"
  },
  {
    "text": "easily use Ray data's apis to read a data set from paret files and perform",
    "start": "1095919",
    "end": "1101640"
  },
  {
    "text": "any pre-processing logic through a Matt batches call and lastly to tie it all",
    "start": "1101640",
    "end": "1107000"
  },
  {
    "text": "together you could pass this into your Tor trainer by passing it into the Tor trainer as an argument that is a data",
    "start": "1107000",
    "end": "1113760"
  },
  {
    "text": "set's dictionary just to summarize this integration it's fantastic because it's",
    "start": "1113760",
    "end": "1120799"
  },
  {
    "text": "pythonic it's very easy to Define your data set and pre-processing in the same",
    "start": "1120799",
    "end": "1125880"
  },
  {
    "text": "script as your training code it also supports automatic sharting in which the data set will be",
    "start": "1125880",
    "end": "1131640"
  },
  {
    "text": "dynamically split across all of your training workers so again if you want to Shard your data set from four workers to",
    "start": "1131640",
    "end": "1139120"
  },
  {
    "text": "eight workers all you need to do is modify your scaling config from Ray",
    "start": "1139120",
    "end": "1144880"
  },
  {
    "text": "train and third Ray data is extremely scalable and supports streaming data",
    "start": "1144880",
    "end": "1150440"
  },
  {
    "text": "loading and pre-processing to fit all your needs which I'll dive into more detail in the next few",
    "start": "1150440",
    "end": "1157760"
  },
  {
    "text": "slides first let's talk about",
    "start": "1158120",
    "end": "1162200"
  },
  {
    "text": "pre-processing with Ray data you can support Last Mile pre-processing in which you could form Dynamic on the-fly",
    "start": "1163440",
    "end": "1170919"
  },
  {
    "text": "online pre-processing logic without having to compute the entire data",
    "start": "1170919",
    "end": "1176280"
  },
  {
    "text": "set it also supports heterogeneous compute in which you could scale pre-processing across your array cluster",
    "start": "1176280",
    "end": "1183080"
  },
  {
    "text": "separate from your training scale in the example below you could quickly see that the ray",
    "start": "1183080",
    "end": "1190640"
  },
  {
    "text": "data operations can be executed on Purely CPU nodes while the training",
    "start": "1190640",
    "end": "1196480"
  },
  {
    "text": "function of your array train application will be running on GPU nodes all in the",
    "start": "1196480",
    "end": "1202799"
  },
  {
    "text": "same Ray cluster secondly Ray data also supports",
    "start": "1202799",
    "end": "1209760"
  },
  {
    "text": "streaming ingestion this means that you could do pipeline execution in which while your",
    "start": "1209760",
    "end": "1215400"
  },
  {
    "text": "training workers are reading and training on one batch of data Ray data",
    "start": "1215400",
    "end": "1220679"
  },
  {
    "text": "is Reading in pre-processing ingesting the second batch of data the following",
    "start": "1220679",
    "end": "1226480"
  },
  {
    "text": "batch of data and feeding it to your ray train workers it's also very memory efficient",
    "start": "1226480",
    "end": "1233159"
  },
  {
    "text": "in which data will be read in one Chuck at a time across your cluster and it'll",
    "start": "1233159",
    "end": "1238440"
  },
  {
    "text": "be discarded after training so you don't ever have to load the entire data set",
    "start": "1238440",
    "end": "1243480"
  },
  {
    "text": "into cluster memory or even cluster",
    "start": "1243480",
    "end": "1247600"
  },
  {
    "text": "disk on top of all this Ray data has a ton of optimizations on any scale that",
    "start": "1249240",
    "end": "1255440"
  },
  {
    "text": "make it that much better to run with Ray train two that I want to call out in particular for",
    "start": "1255440",
    "end": "1262080"
  },
  {
    "text": "training one is the streaming metadata fetching in which metadata",
    "start": "1262080",
    "end": "1267159"
  },
  {
    "text": "fetching previously was run in a blocking fashion as seen in the diagram at top in the top and any of the reading",
    "start": "1267159",
    "end": "1275720"
  },
  {
    "text": "or mapping or ingestion functionalities would have to wait in until the metadata",
    "start": "1275720",
    "end": "1281919"
  },
  {
    "text": "of the entire data set was fetched before proceeding however on any scale",
    "start": "1281919",
    "end": "1287520"
  },
  {
    "text": "this is now converted into a streaming operation so as soon as metadata is fetched for the first chunk of data you",
    "start": "1287520",
    "end": "1295000"
  },
  {
    "text": "could start mapping over it you could start ingesting it and you could start training on it saving a ton of time to",
    "start": "1295000",
    "end": "1302159"
  },
  {
    "text": "get those immediate results that you need another optimization is a",
    "start": "1302159",
    "end": "1308320"
  },
  {
    "text": "configuration Optimizer of its own on any scale Ray data can detect low",
    "start": "1308320",
    "end": "1313640"
  },
  {
    "text": "resource utilization in the cluster or things such as memory pressure and optimize the configurations throughout",
    "start": "1313640",
    "end": "1320159"
  },
  {
    "text": "your aray data Pipeline and increase the performance of the data processing pipeline improving the throughput of",
    "start": "1320159",
    "end": "1327320"
  },
  {
    "text": "your training job cool we've gone over some of the",
    "start": "1327320",
    "end": "1335279"
  },
  {
    "text": "challenges and how Ray train aims to solve these challenges to tie this all together I want to go over a quick case",
    "start": "1335279",
    "end": "1342000"
  },
  {
    "text": "study of how we did pre-training of stable diffusion V2 earlier this year using Ray train and Ray data together",
    "start": "1342000",
    "end": "1349880"
  },
  {
    "text": "as a primer here's a quick diagram that shows the architecture of stable",
    "start": "1349880",
    "end": "1356960"
  },
  {
    "text": "diffusion while doing training you'll take in an image and caption pair you'll",
    "start": "1356960",
    "end": "1362400"
  },
  {
    "text": "pass them through some encoders and you'll grab these latent embeddings and you'll feed them into a",
    "start": "1362400",
    "end": "1368320"
  },
  {
    "text": "unet diffusion model which is the model that you're actually trying to train a typical training pattern for",
    "start": "1368320",
    "end": "1375600"
  },
  {
    "text": "this might look like this where very simply put you're doing everything on",
    "start": "1375600",
    "end": "1381320"
  },
  {
    "text": "each individual GPU in this case I have A1 100s which will hold both the",
    "start": "1381320",
    "end": "1386400"
  },
  {
    "text": "encoders as well as the unet and I'll do both the encoding stage as well as the",
    "start": "1386400",
    "end": "1391640"
  },
  {
    "text": "training part of it uh all on one single",
    "start": "1391640",
    "end": "1396799"
  },
  {
    "text": "GPU we did some initial profiling when we ran it this way and noticed something very interesting we saw through the GPU",
    "start": "1396960",
    "end": "1404480"
  },
  {
    "text": "memory usage that the encoders were making quite a bit of time up to 30% of",
    "start": "1404480",
    "end": "1412520"
  },
  {
    "text": "the entire training time and we could also tell that it was not requiring so",
    "start": "1412520",
    "end": "1418240"
  },
  {
    "text": "much memory to do these encodings so we were looking for ways to see if we could optimize this and Reserve these a1s",
    "start": "1418240",
    "end": "1425480"
  },
  {
    "text": "simply for training the unit itself we took the original training",
    "start": "1425480",
    "end": "1432360"
  },
  {
    "text": "Pipeline and converted it so that we took the encoders out into a separate",
    "start": "1432360",
    "end": "1438080"
  },
  {
    "text": "radi data offline job in which we would run the encoders as part of the ray data",
    "start": "1438080",
    "end": "1443679"
  },
  {
    "text": "pre-processing stage and feed them directly to write into uh cloud storage",
    "start": "1443679",
    "end": "1449240"
  },
  {
    "text": "in S3 then in our training job we would use Ray data again to read from S3",
    "start": "1449240",
    "end": "1456240"
  },
  {
    "text": "directly into the training workers and in this scenario only the unet models",
    "start": "1456240",
    "end": "1462120"
  },
  {
    "text": "are on the actual A1 100s which means that they could fully utilize the GPU to themselves",
    "start": "1462120",
    "end": "1468799"
  },
  {
    "text": "the results were fantastic with the original Baseline we",
    "start": "1468799",
    "end": "1474159"
  },
  {
    "text": "saw a throughput of either 2,800 or 800 images per second based on the size of",
    "start": "1474159",
    "end": "1479799"
  },
  {
    "text": "the images however by con adding this extra offline pre-processing job we are",
    "start": "1479799",
    "end": "1485600"
  },
  {
    "text": "able to see increases of approximately 30% by simply converting to offline",
    "start": "1485600",
    "end": "1492679"
  },
  {
    "text": "pre-processing but we asked ourselves what if we need Dynamic pre-processing what if we need to make make sure that",
    "start": "1492679",
    "end": "1499399"
  },
  {
    "text": "on every Epoch we randomly crop the images a little bit differently and we need some more Dynamic handling of these",
    "start": "1499399",
    "end": "1506480"
  },
  {
    "text": "this data can I still use offline pre-processing we looked at the",
    "start": "1506480",
    "end": "1512279"
  },
  {
    "text": "architecture again and thought why not use Ray data for online",
    "start": "1512279",
    "end": "1518559"
  },
  {
    "text": "pre-processing we converted this two cluster job back into a single cluster",
    "start": "1518559",
    "end": "1524159"
  },
  {
    "text": "job in which we defined the ray data Pipeline and feed it directly into the ray train trainer as we showed earlier",
    "start": "1524159",
    "end": "1532159"
  },
  {
    "text": "in the ray data uh quick start example in this case you could see that",
    "start": "1532159",
    "end": "1538760"
  },
  {
    "text": "we're running the encoders on a1g instances and we're running the diffusion model on A1 100s and we could",
    "start": "1538760",
    "end": "1547240"
  },
  {
    "text": "scale each of these workers separately based on the compute needs the results were awesome we saw",
    "start": "1547240",
    "end": "1555640"
  },
  {
    "text": "similar results uh in that by adding a bunch of A10 GS we were able to get the",
    "start": "1555640",
    "end": "1562320"
  },
  {
    "text": "same throughput from the data pipeline that we did with offline pre-processing with an equivalent gain",
    "start": "1562320",
    "end": "1569080"
  },
  {
    "text": "over the original torch data loader workload and despite having to add more",
    "start": "1569080",
    "end": "1576039"
  },
  {
    "text": "smaller instances to support this the total cost for running this job over the",
    "start": "1576039",
    "end": "1581520"
  },
  {
    "text": "a single Epoch was still able to be reduced by",
    "start": "1581520",
    "end": "1586720"
  },
  {
    "text": "10% in addition to this after taking a look at how to optimize the data pipeline we also wanted to think how do",
    "start": "1587240",
    "end": "1594480"
  },
  {
    "text": "we optimize the training pipeline fortunately as I mentioned before Ray train works well with the ml ecosystem",
    "start": "1594480",
    "end": "1601559"
  },
  {
    "text": "so we could leverage optimizations in the existing ecosystem such as AWS EFA",
    "start": "1601559",
    "end": "1607640"
  },
  {
    "text": "pytorch fsdp and torch compile in order to further optimize our training job to",
    "start": "1607640",
    "end": "1613440"
  },
  {
    "text": "increase the throughput of the model by over 300%",
    "start": "1613440",
    "end": "1619519"
  },
  {
    "text": "to summarize this case study we use Ray train and Ray data to pre-train stable diffusion",
    "start": "1619840",
    "end": "1625559"
  },
  {
    "text": "V2 by using Ray data as part of the pre-processing pipeline we're able to increase the throughput by over 30% as",
    "start": "1625559",
    "end": "1633240"
  },
  {
    "text": "compared to the torch data loader and by enabling training optimizations from the ml ecosystem we",
    "start": "1633240",
    "end": "1639880"
  },
  {
    "text": "were able to further increase the training throughput by actually over",
    "start": "1639880",
    "end": "1645039"
  },
  {
    "text": "300% to summarize this talk we went over a few of the challenges with distributed training and how raay train aims to",
    "start": "1646880",
    "end": "1654039"
  },
  {
    "text": "address these the first challenge was about setting up distributed environments and showed how Ray train",
    "start": "1654039",
    "end": "1660600"
  },
  {
    "text": "has a very simple easy to understand API that abstracts away the distributed",
    "start": "1660600",
    "end": "1665760"
  },
  {
    "text": "complexity we talked about how you might wonder about how do I connect Ray train to my existing ml workflow and we",
    "start": "1665760",
    "end": "1674159"
  },
  {
    "text": "covered how Ray train has seamless Integrations with the very fragmented ml ecosystem to make sure that you're not",
    "start": "1674159",
    "end": "1680840"
  },
  {
    "text": "left out of the equation we talked about the challenge of distributed debugging and showcased",
    "start": "1680840",
    "end": "1687240"
  },
  {
    "text": "how Ray train has the ray dashboard and Ray train dashboard for better distributed",
    "start": "1687240",
    "end": "1692600"
  },
  {
    "text": "observability we talked about how there are more there are going to be more Hardware failures when dealing with",
    "start": "1692600",
    "end": "1698640"
  },
  {
    "text": "distributed systems and how Ray train has a nice fa tolerance mechanism as",
    "start": "1698640",
    "end": "1703799"
  },
  {
    "text": "well as elastic training to support these failures and lastly ly we talked about",
    "start": "1703799",
    "end": "1709240"
  },
  {
    "text": "dealing with large scale data complexity and how integrating with Ray data can help solve these data ingestion and",
    "start": "1709240",
    "end": "1715840"
  },
  {
    "text": "pre-processing problems with that thank you uh I'll be",
    "start": "1715840",
    "end": "1721919"
  },
  {
    "text": "around for questions if you have any [Applause]",
    "start": "1721919",
    "end": "1730569"
  },
  {
    "text": "um on the topic of uh training with complex data is it required that um all",
    "start": "1740120",
    "end": "1748440"
  },
  {
    "text": "shards of the data set be trained concurrently on on workers concurrently",
    "start": "1748440",
    "end": "1754559"
  },
  {
    "text": "or can you configure custom configure the sharding so that there's more shards than workers and let the workers pick up",
    "start": "1754559",
    "end": "1761240"
  },
  {
    "text": "the next Shard as each one becomes available to do so you can do that but",
    "start": "1761240",
    "end": "1767399"
  },
  {
    "text": "that's a more Advan configuration that doesn't come out of the box would that be done through the custom through the",
    "start": "1767399",
    "end": "1773159"
  },
  {
    "text": "data config argument yeah okay can I talk to you offline about how to do that",
    "start": "1773159",
    "end": "1778480"
  },
  {
    "text": "yeah thanks so one of the configurations I",
    "start": "1778480",
    "end": "1783760"
  },
  {
    "text": "saws uh you are able to segregate activities for uh the CPUs and the gpus",
    "start": "1783760",
    "end": "1790000"
  },
  {
    "text": "yeah there are some uh machine types let's say for example A3 in gcp which",
    "start": "1790000",
    "end": "1796399"
  },
  {
    "text": "comes with let's say eight h100s in a single machine could you specify like a",
    "start": "1796399",
    "end": "1803640"
  },
  {
    "text": "job to only have a slice of that GPU or uh you could chart it so you",
    "start": "1803640",
    "end": "1810679"
  },
  {
    "text": "logically uh get scheduled on one of those gpus um I I'm not sure if that",
    "start": "1810679",
    "end": "1815960"
  },
  {
    "text": "answers your question I mean restrict uh let's say if you have a job that requires more than one GPU but then you",
    "start": "1815960",
    "end": "1822480"
  },
  {
    "text": "want to restrict it to just one out of eight is that possible",
    "start": "1822480",
    "end": "1828480"
  },
  {
    "text": "so this would probably be more of a general Ray core question I assume uh but to answer your question you would",
    "start": "1828480",
    "end": "1835000"
  },
  {
    "text": "probably have to sh it at a higher level in which case like Ray does Ray by",
    "start": "1835000",
    "end": "1840080"
  },
  {
    "text": "itself won't Shard your machine like at the physical level okay thank",
    "start": "1840080",
    "end": "1846799"
  },
  {
    "text": "you so this is a kind of a general question so I see there's a r train as",
    "start": "1846799",
    "end": "1851960"
  },
  {
    "text": "also Rune how do they kind of collaborate together all are they yeah that yeah great question so Ray tune is",
    "start": "1851960",
    "end": "1858880"
  },
  {
    "text": "meant for hyperparameter tuning so the high level as a user what you might want to think about is if you want to scale",
    "start": "1858880",
    "end": "1865279"
  },
  {
    "text": "out uh one particular training job you'll use Ray train to support things such as data parallelism but if you want",
    "start": "1865279",
    "end": "1872159"
  },
  {
    "text": "to scale out to multiple Trials of your job you'll want to use R tune to support",
    "start": "1872159",
    "end": "1877960"
  },
  {
    "text": "multiple hyperparameter configurations and you could also utilize these together uh if you want to leverage both",
    "start": "1877960",
    "end": "1884120"
  },
  {
    "text": "at the same time okay because um we are looking at the ml flow into integration with Ray train but I couldn't found it",
    "start": "1884120",
    "end": "1890399"
  },
  {
    "text": "on official website but I see it's integrated with Ray tune so I'm not sure how that works like because we're going",
    "start": "1890399",
    "end": "1896840"
  },
  {
    "text": "to try different like not hyper parameters but also the different runs about it yeah I'm not sure if that's",
    "start": "1896840",
    "end": "1903120"
  },
  {
    "text": "something like can be done with Ray train as well yeah the the apis do work",
    "start": "1903120",
    "end": "1908200"
  },
  {
    "text": "with Ray train as well okay but depending on what your needs are you might want to use different apis if you",
    "start": "1908200",
    "end": "1913960"
  },
  {
    "text": "want things that are more granular such as uh logging from individual training workers I see thank",
    "start": "1913960",
    "end": "1921960"
  },
  {
    "text": "you can you talk a little more about um under the hood with Ray train when it's",
    "start": "1922919",
    "end": "1930039"
  },
  {
    "text": "performing its distributed training how Ray train um uh up using the backward",
    "start": "1930039",
    "end": "1936960"
  },
  {
    "text": "pass of a of a deep learning model how the various workers would receive their",
    "start": "1936960",
    "end": "1942679"
  },
  {
    "text": "their updates from each other yeah so in this case it'll just use pytorch distributed and it'll probably use like",
    "start": "1942679",
    "end": "1948000"
  },
  {
    "text": "nickel under the hood to do gradient synchronization is there uh does a",
    "start": "1948000",
    "end": "1953080"
  },
  {
    "text": "command does user have to add any command to do that or is that something that um just through the ray. tr. report",
    "start": "1953080",
    "end": "1960159"
  },
  {
    "text": "it just happens so it actually doesn't go through that path at all actually okay uh so you'll be either using a",
    "start": "1960159",
    "end": "1966519"
  },
  {
    "text": "pytorch DDP for example directly through that interface or you could use Raid trains prepare model interface which",
    "start": "1966519",
    "end": "1972480"
  },
  {
    "text": "will call distri a DDP under the hood so either rate train prepare model or just",
    "start": "1972480",
    "end": "1978760"
  },
  {
    "text": "P py TV that's",
    "start": "1978760",
    "end": "1981880"
  },
  {
    "text": "right cool thank you [Applause]",
    "start": "1988000",
    "end": "1994719"
  }
]