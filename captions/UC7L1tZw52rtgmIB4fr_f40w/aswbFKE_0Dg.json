[
  {
    "text": "all right should I",
    "start": "2440",
    "end": "6560"
  },
  {
    "text": "so let's let's start the meet up um good afternoon everyone uh my name",
    "start": "7760",
    "end": "16240"
  },
  {
    "text": "is kurash uh I'm going to be kicking off this Meetup I see a lot of excitement uh",
    "start": "16240",
    "end": "22600"
  },
  {
    "text": "a good wife here um so hopefully this content is going to be useful for you uh we have a lot of like good um I guess",
    "start": "22600",
    "end": "30759"
  },
  {
    "text": "farsite chat uh planned up after this um so my name is kuros uh I'm part of the",
    "start": "30759",
    "end": "38760"
  },
  {
    "text": "any scale team on the technical part of the any scale end point stack today I'm",
    "start": "38760",
    "end": "45600"
  },
  {
    "text": "GNA should I hold this close can everyone hear me now is it good yeah okay um I'm gonna talk about uh some",
    "start": "45600",
    "end": "54559"
  },
  {
    "text": "content that we have published today which is more about how do we deal with",
    "start": "54559",
    "end": "59600"
  },
  {
    "text": "content context limited context length uh via fine tuning language models uh",
    "start": "59600",
    "end": "65080"
  },
  {
    "text": "without this this talk is uh essentially organized in a way that I don't dive too",
    "start": "65080",
    "end": "70119"
  },
  {
    "text": "deep into the technical details of how the fine-tuning thing actually works it's more about how to set up a data set",
    "start": "70119",
    "end": "77880"
  },
  {
    "text": "for doing such a thing and how to do evaluation so that we get um essentially the the things we want for our",
    "start": "77880",
    "end": "84720"
  },
  {
    "text": "application development so um I don't know how much people are familiar with",
    "start": "84720",
    "end": "91400"
  },
  {
    "text": "like these terminologies uh but essentially um llm applications the",
    "start": "91400",
    "end": "97200"
  },
  {
    "text": "successful ones you can think about them um as like they're Feit they're starting",
    "start": "97200",
    "end": "103040"
  },
  {
    "text": "to Feit in uh either of these patterns so rack systems retrieval augmented Generations um is essentially refers to",
    "start": "103040",
    "end": "110560"
  },
  {
    "text": "this design pattern where you essentially hook up a knowledge based to your llm and uh when you're asking",
    "start": "110560",
    "end": "117000"
  },
  {
    "text": "queries the relevant documents and content are gathered um as and fed to an llm so that it can",
    "start": "117000",
    "end": "123399"
  },
  {
    "text": "essentially diffuse that information into a coherent response at the end um and on the other side uh something that",
    "start": "123399",
    "end": "130360"
  },
  {
    "text": "we are seeing a lot uh these days is this idea behind agents which is uh",
    "start": "130360",
    "end": "136120"
  },
  {
    "text": "essentially hook can up llms to all these different tools U so that they can",
    "start": "136120",
    "end": "142000"
  },
  {
    "text": "not only perceive the world but can also act on it and actually do some long-term",
    "start": "142000",
    "end": "147480"
  },
  {
    "text": "uh tasks independently um um so one of the pressing issues in building these",
    "start": "147480",
    "end": "153120"
  },
  {
    "text": "systems is the limited context length of these language models and naturally this",
    "start": "153120",
    "end": "158760"
  },
  {
    "text": "context L limitation puts an upper bound on the performance that you can get out of this with more context L on rack",
    "start": "158760",
    "end": "165040"
  },
  {
    "text": "systems you can essentially Feit more documents and let the LM reason about",
    "start": "165040",
    "end": "170239"
  },
  {
    "text": "what is relevant and what is not and with agents you can essentially Fe like",
    "start": "170239",
    "end": "175280"
  },
  {
    "text": "enable your model to do more stuff with defining more tools and things like that",
    "start": "175280",
    "end": "180959"
  },
  {
    "text": "so in this talk I want to talk about how we can address The Limited context length via fine tuning and uh I'm going",
    "start": "180959",
    "end": "188959"
  },
  {
    "text": "to focus on rack systems but what I'll be covering here is kind of generalizable you can think of it um you",
    "start": "188959",
    "end": "196120"
  },
  {
    "text": "you can essentially see and observe the same patterns in agent systems as well uh but without getting to first like",
    "start": "196120",
    "end": "203720"
  },
  {
    "text": "before getting to fine-tuning for context length um there are many benefits of doing tuning on open source",
    "start": "203720",
    "end": "210720"
  },
  {
    "text": "model so for rack um if you think about what where would fine tuning help there",
    "start": "210720",
    "end": "216480"
  },
  {
    "text": "are like several Dimensions you can think about um if you think about like you're building these rag applications",
    "start": "216480",
    "end": "223040"
  },
  {
    "text": "for a final like domain specific thing usually like there may be special terminologies involved like you're",
    "start": "223040",
    "end": "229519"
  },
  {
    "text": "building a financial statement analyzer or like a something in the health space",
    "start": "229519",
    "end": "234640"
  },
  {
    "text": "usually in these like domain specific uh applications you have terminology",
    "start": "234640",
    "end": "240319"
  },
  {
    "text": "that may not be you know apparent to um the language model from its prior",
    "start": "240319",
    "end": "246640"
  },
  {
    "text": "knowledge for example we bu like rag documentation Ray documentation you know assistant uh usually like if this LM has",
    "start": "246640",
    "end": "254319"
  },
  {
    "text": "never seen this Ray Concepts it never knows like what Ray means right it thinks it's like the fish right um so",
    "start": "254319",
    "end": "261199"
  },
  {
    "text": "fine tuning can sometimes help with M mapping what your essentially end like",
    "start": "261199",
    "end": "267000"
  },
  {
    "text": "terminologies and essentially teach uh the model what those um like specific",
    "start": "267000",
    "end": "272919"
  },
  {
    "text": "terms are and with rag you're ultimately looking at a grounded task which means",
    "start": "272919",
    "end": "280080"
  },
  {
    "text": "that you're not TR trying to create a new knowledge everything that the LM do",
    "start": "280080",
    "end": "285680"
  },
  {
    "text": "needs to do is like is captured in the context that it's uh you're feeding it",
    "start": "285680",
    "end": "291199"
  },
  {
    "text": "to um so the problem about like Hallucination is like uh not so much",
    "start": "291199",
    "end": "297560"
  },
  {
    "text": "relevant here as as long as uh it doesn't go beyond like the context that",
    "start": "297560",
    "end": "302800"
  },
  {
    "text": "is provided and it doesn't come up with answers that are not part of the context it should be fine so with fine-tuning",
    "start": "302800",
    "end": "309120"
  },
  {
    "text": "you can essentially teach a model that given a context you should just limit your knowledge based to this context and",
    "start": "309120",
    "end": "315000"
  },
  {
    "text": "if nothing is relevant um you can essentially teach it to do special answers like hey and your answer to this",
    "start": "315000",
    "end": "322680"
  },
  {
    "text": "query is not found or things like that um most of the time if you hook up like your the you know nonf find language",
    "start": "322680",
    "end": "330000"
  },
  {
    "text": "models to the lastest stage of rack systems they often tend to produce very long answers uh unless you kind of uh",
    "start": "330000",
    "end": "338400"
  },
  {
    "text": "specifically prompt them to to not do so but with like open source language modelers this kind of prompt engineering",
    "start": "338400",
    "end": "344400"
  },
  {
    "text": "is a bit like harder than like you know closed Source models like open Ai and",
    "start": "344400",
    "end": "349479"
  },
  {
    "text": "this is where like again fine-tuning can help you can essentially teach the model to not produce those long answers and",
    "start": "349479",
    "end": "356000"
  },
  {
    "text": "only uh limit itself to short ones today I'm going to talk about long context",
    "start": "356000",
    "end": "361199"
  },
  {
    "text": "extension that is something that we've found that um it's another dimension uh of capability that you can add to your",
    "start": "361199",
    "end": "367560"
  },
  {
    "text": "models um that is a bit like I guess not",
    "start": "367560",
    "end": "372840"
  },
  {
    "text": "as like as emphasized in the community as the other ones so today uh today's meet up is about",
    "start": "372840",
    "end": "380960"
  },
  {
    "text": "like this topic of needing haast stack I'm pretty sure the next speakers are going to have enough coverage on it but",
    "start": "380960",
    "end": "387280"
  },
  {
    "text": "this kind of needed for the premise of the kind of next slides so this is uh a",
    "start": "387280",
    "end": "393919"
  },
  {
    "text": "very good uh evaluation framework to think about when you want to evaluate whether an LM has like uh equal",
    "start": "393919",
    "end": "401280"
  },
  {
    "text": "understanding across all of its context so I love that picture U basically it",
    "start": "401280",
    "end": "407039"
  },
  {
    "text": "shows the problem in a high level view like you're essentially hiding um a",
    "start": "407039",
    "end": "413080"
  },
  {
    "text": "desired you know fact within like a lot of noisy uh things around it like called",
    "start": "413080",
    "end": "419160"
  },
  {
    "text": "need inh St right and you ask your llm to kind of retrieve that fact um without",
    "start": "419160",
    "end": "425160"
  },
  {
    "text": "explicit knowing where that fact is you ask a question of like where is the Apple and says like I see one apple or",
    "start": "425160",
    "end": "430440"
  },
  {
    "text": "something like that and um basically the end result um you know after you do your",
    "start": "430440",
    "end": "436039"
  },
  {
    "text": "evaluation it's something like this graph where you have two axes um on the",
    "start": "436039",
    "end": "441479"
  },
  {
    "text": "y axis you are essentially measuring like the depth at which this fact is",
    "start": "441479",
    "end": "446720"
  },
  {
    "text": "injected and you're sweeping that um across the board and then the Y on the",
    "start": "446720",
    "end": "452199"
  },
  {
    "text": "x- axis you're sweeping the context length the context of the Hy stack how much noise are around that fact um and",
    "start": "452199",
    "end": "460240"
  },
  {
    "text": "then you evaluate some sort of retrieval score or accuracy level whether um the",
    "start": "460240",
    "end": "465400"
  },
  {
    "text": "desired fact has been like retrieved when the LM sees this context and uh I'm pretty sure Greg is",
    "start": "465400",
    "end": "472120"
  },
  {
    "text": "going to cover the details here but um what we did here was we took like the",
    "start": "472120",
    "end": "479080"
  },
  {
    "text": "order original like Benchmark is essentially uses PA Graham's essays as",
    "start": "479080",
    "end": "484120"
  },
  {
    "text": "Hast stack um very like large textural Corpus and then it injects some fact",
    "start": "484120",
    "end": "490080"
  },
  {
    "text": "like what is the best like the best thing to do in SF is like to eat sandwich or something and then it ask",
    "start": "490080",
    "end": "495879"
  },
  {
    "text": "like the language model what is the best thing to do in SF and it has to respond",
    "start": "495879",
    "end": "501560"
  },
  {
    "text": "sand eating sand like I guess do sp or something is the um best thing to do and",
    "start": "501560",
    "end": "508000"
  },
  {
    "text": "uh when you when try this Benchmark on like open source models like mix draw",
    "start": "508000",
    "end": "513360"
  },
  {
    "text": "they solved it essentially like so there was not much to um kind of like you know",
    "start": "513360",
    "end": "519360"
  },
  {
    "text": "um focus on around fine tuning so we started adding a little bit of spice to",
    "start": "519360",
    "end": "524600"
  },
  {
    "text": "the problem so that it makes it more realistic um to to the scenarios that",
    "start": "524600",
    "end": "529880"
  },
  {
    "text": "could happen in real world as well so in rag applications you may also have like this problem of like hey I have like",
    "start": "529880",
    "end": "536240"
  },
  {
    "text": "bunch of documents that I retriev some of them are relevant to this question some of them are not and I have to be",
    "start": "536240",
    "end": "541320"
  },
  {
    "text": "able to kind of filter them out and like fuse them into a coherent answer so the",
    "start": "541320",
    "end": "547519"
  },
  {
    "text": "first thing that we noticed with this Benchmark was that uh the model could already have prior knowledge about what",
    "start": "547519",
    "end": "554079"
  },
  {
    "text": "is the best thing to do in San Francisco so even without like context um the likelihood that it says eating sand is",
    "start": "554079",
    "end": "561120"
  },
  {
    "text": "nonone like zero right um so we wanted to explicitly make it harder for the",
    "start": "561120",
    "end": "566640"
  },
  {
    "text": "model to kind of like Get prone to this house regime so in order to do that uh",
    "start": "566640",
    "end": "572040"
  },
  {
    "text": "we changed the kind of like the problem domain to retrieving biographies where",
    "start": "572040",
    "end": "578440"
  },
  {
    "text": "um Within These biographies we're talking about fictitious characters they don't exist in the real world and we",
    "start": "578440",
    "end": "584040"
  },
  {
    "text": "were asking questions about who is this person um and then the LM should essentially respond hey this person is",
    "start": "584040",
    "end": "590600"
  },
  {
    "text": "this guy um but this kind of modification of the problem forces the",
    "start": "590600",
    "end": "595760"
  },
  {
    "text": "llm to hallucinate if it doesn't work correctly and you can essentially easily see very quickly see how the model is",
    "start": "595760",
    "end": "604120"
  },
  {
    "text": "performing another um thing we noticed with the original problem was the needle",
    "start": "604120",
    "end": "609760"
  },
  {
    "text": "kind of stands out uh you're like the ha stack is essentially bunch of you know",
    "start": "609760",
    "end": "615720"
  },
  {
    "text": "startup ideas like those kind of essays that talks about like business plans and Etc and then you're suddenly asking like",
    "start": "615720",
    "end": "622920"
  },
  {
    "text": "what the best thing to do in San Francisco is eating San so it's it's an outlier um and this may be easier for",
    "start": "622920",
    "end": "630600"
  },
  {
    "text": "LMS to kind of distinguish from the surrounding um kind of context and we wanted to make it harder so for the",
    "start": "630600",
    "end": "637519"
  },
  {
    "text": "needle we use the same biography still you're still rearching a biography but",
    "start": "637519",
    "end": "642639"
  },
  {
    "text": "it's like from a like a I guess individual that you have um in mind so",
    "start": "642639",
    "end": "648839"
  },
  {
    "text": "it's kind of like this you go from something being too obvious or something that is like hidden and um it's harder",
    "start": "648839",
    "end": "656160"
  },
  {
    "text": "to kind of like Get Up another thing uh about the Benchmark",
    "start": "656160",
    "end": "662639"
  },
  {
    "text": "is that the answer that you want is usually copy pastable like um then",
    "start": "662639",
    "end": "667680"
  },
  {
    "text": "language model doesn't have to do a lot like higher level reasoning all it has to do is like just copy paste that fact",
    "start": "667680",
    "end": "674399"
  },
  {
    "text": "uh with some re aror phrase and we wanted to essentially see if the model can also reason across this thing so um",
    "start": "674399",
    "end": "682800"
  },
  {
    "text": "we kind of did a little bit of trick so for example like for dates where this",
    "start": "682800",
    "end": "688040"
  },
  {
    "text": "guy is born on March 10th we want the output of like the birth dat to be on",
    "start": "688040",
    "end": "693880"
  },
  {
    "text": "that particular format right like like more it the language model should learn",
    "start": "693880",
    "end": "698959"
  },
  {
    "text": "that March maps to the third month in the year so it has to do this like sort of higher level reasoning uh to be able",
    "start": "698959",
    "end": "706000"
  },
  {
    "text": "to fully solve the tasks and you're at when you're doing this benchmarking and like data set curation you're",
    "start": "706000",
    "end": "711839"
  },
  {
    "text": "effectively evaluating whether the model can do this type of things at large context as",
    "start": "711839",
    "end": "717560"
  },
  {
    "text": "well and um last thing which is I think very critical when you're developing LM",
    "start": "717560",
    "end": "723839"
  },
  {
    "text": "applications is like how do you evaluate your right like um there's this whole",
    "start": "723839",
    "end": "729320"
  },
  {
    "text": "concept of using gp4 as a judge that is kind of almost everywhere that's kind of",
    "start": "729320",
    "end": "735079"
  },
  {
    "text": "the easiest solution but like sometimes with a little bit of changing your problem statement you can um like change",
    "start": "735079",
    "end": "743120"
  },
  {
    "text": "your evaluation criteria to be non like not too hard that you need to rely on",
    "start": "743120",
    "end": "748560"
  },
  {
    "text": "like an language model so for this to give you a concrete example uh we forced",
    "start": "748560",
    "end": "754160"
  },
  {
    "text": "we want the output of the model to follow a specific Json format representing the information within that",
    "start": "754160",
    "end": "760399"
  },
  {
    "text": "biography um so something like this like what is the nationality of this guy",
    "start": "760399",
    "end": "766279"
  },
  {
    "text": "there's bunch of like uh like literal choices American like I don't know 100",
    "start": "766279",
    "end": "771720"
  },
  {
    "text": "choices uh date of birth there's a certain structure to it like uh is this",
    "start": "771720",
    "end": "776959"
  },
  {
    "text": "guy Sportsman or not said right politician or not there's aan so",
    "start": "776959",
    "end": "782760"
  },
  {
    "text": "if we have this sort of like output structure I don't need a language model as a judge I can very quickly uh write a",
    "start": "782760",
    "end": "790480"
  },
  {
    "text": "python script that checks for equality of the response to the expected format",
    "start": "790480",
    "end": "795519"
  },
  {
    "text": "and this basically solves like a lot of like when you're doing iteration on the problem you want something like this",
    "start": "795519",
    "end": "802040"
  },
  {
    "text": "framework where you can quickly evaluate and iterate and uh putting a structure",
    "start": "802040",
    "end": "807560"
  },
  {
    "text": "on your problem where can uh easily like just check for equality is one way to go",
    "start": "807560",
    "end": "812959"
  },
  {
    "text": "about this and you may ask okay how did you generate this data points um so",
    "start": "812959",
    "end": "818760"
  },
  {
    "text": "there's like a lot of tricks you can play you can actually use llm to generate this stuff um and we also like",
    "start": "818760",
    "end": "824519"
  },
  {
    "text": "he basically did this experiment with like uh even like less capable models",
    "start": "824519",
    "end": "829560"
  },
  {
    "text": "like on on a lower end like mol 70 model which is a very tin1 people are running on their laptops these days uh with a",
    "start": "829560",
    "end": "836959"
  },
  {
    "text": "constraint generation mode activate uh this is basically available on any scale end points it's an API call you",
    "start": "836959",
    "end": "843959"
  },
  {
    "text": "can kind of like feed these sleets off uh biographies to the model and ask it to produce this format out of it and uh",
    "start": "843959",
    "end": "851560"
  },
  {
    "text": "you collect you run it essentially across a lot of data points and you collect this data set then you can",
    "start": "851560",
    "end": "856720"
  },
  {
    "text": "essentially filter through them with manual like intervention sometimes um to kind of like remove the the noise the",
    "start": "856720",
    "end": "863160"
  },
  {
    "text": "noisy ones things like that don't fit the structure really well so with that",
    "start": "863160",
    "end": "869639"
  },
  {
    "text": "said uh we constructed a data set for both training and test um and uh some of",
    "start": "869639",
    "end": "876759"
  },
  {
    "text": "these kind of like results are just you know without fine tuning meaning that I'm actually just running the model on",
    "start": "876759",
    "end": "883120"
  },
  {
    "text": "these test sets and on the kind of the chart over there uh what we're seeing is",
    "start": "883120",
    "end": "888279"
  },
  {
    "text": "like the average accuracy of retrieval across these different context links and",
    "start": "888279",
    "end": "894040"
  },
  {
    "text": "we have a few baselines so as I said earlier like mixol under the right like on the original problem the without all",
    "start": "894040",
    "end": "901480"
  },
  {
    "text": "of these like um kind of spices we added sort of solves the problem it's always like almost 100% accurate across the",
    "start": "901480",
    "end": "908399"
  },
  {
    "text": "board but with this you know test data data set that we created it's hovering",
    "start": "908399",
    "end": "914680"
  },
  {
    "text": "around like 65 is% and uh despite being trained for like to support 32k it cannot solve this",
    "start": "914680",
    "end": "923240"
  },
  {
    "text": "problem out of the box and on the other hand we have like a two other non tune models which on the",
    "start": "923240",
    "end": "929839"
  },
  {
    "text": "close source side so we have GPD 3.5 uh 16k which has a you know uh degradation",
    "start": "929839",
    "end": "938600"
  },
  {
    "text": "in performance and retrieval score as you increase the context length uh but like for very like short context like 4K",
    "start": "938600",
    "end": "945600"
  },
  {
    "text": "it's almost like 99 98% accurate but as you increase the context L it kind of",
    "start": "945600",
    "end": "951240"
  },
  {
    "text": "tapers off um the other two things so these these two baselines are the things",
    "start": "951240",
    "end": "957279"
  },
  {
    "text": "that we don't train just test the LM capability on um the kind of the test",
    "start": "957279",
    "end": "964440"
  },
  {
    "text": "set like a thousand examples that we had um we then took these data sets and",
    "start": "964440",
    "end": "972240"
  },
  {
    "text": "trained like a GPD 3.5 tuo on them and then saw that GPT 3.5 after training on",
    "start": "972240",
    "end": "979720"
  },
  {
    "text": "this data set can fully you know solve this test it as good as GPD 4 uh which",
    "start": "979720",
    "end": "985560"
  },
  {
    "text": "supports up to one uh 128k and we wanted to make kind of the same story happen",
    "start": "985560",
    "end": "992399"
  },
  {
    "text": "for I guess like open source models especially for models that don't even support up 26k so here uh for our kind",
    "start": "992399",
    "end": "1001160"
  },
  {
    "text": "of like uh kind of trial we did we focused on llama 2 13B which is natively",
    "start": "1001160",
    "end": "1008240"
  },
  {
    "text": "trained up to 4K I wanted to see after fine tuning VI any scaling points on you",
    "start": "1008240",
    "end": "1014480"
  },
  {
    "text": "know these data sets that uh it's kind of proven to be good because GPD 3.5",
    "start": "1014480",
    "end": "1019880"
  },
  {
    "text": "after fine tuning kind of like nails the task we wanted to see like how much we can improve U the accuracy I mean the",
    "start": "1019880",
    "end": "1027880"
  },
  {
    "text": "beginning accuracy of the the Llama is way below like the mix roll one it's less capable but then after fine tuning",
    "start": "1027880",
    "end": "1035558"
  },
  {
    "text": "on this specific data you can get actually a better result than GPT 3.5",
    "start": "1035559",
    "end": "1041438"
  },
  {
    "text": "without fine tuning and it's kind of like up to 8K it's very close to even",
    "start": "1041439",
    "end": "1046678"
  },
  {
    "text": "close Source models in its performance um so if you take a closer",
    "start": "1046679",
    "end": "1052120"
  },
  {
    "text": "look at you know uh this this hit map chart you can see that um on the higher",
    "start": "1052120",
    "end": "1059320"
  },
  {
    "text": "end of context length there is this uh kind of phenomena of Lost in the middle",
    "start": "1059320",
    "end": "1065400"
  },
  {
    "text": "present which means that your language model um perfectly kind of retrieves the",
    "start": "1065400",
    "end": "1071480"
  },
  {
    "text": "needle that is towards the end and kind of uh in the very beginning but it it",
    "start": "1071480",
    "end": "1079000"
  },
  {
    "text": "tends to forget stuff when there like in the middle and that still pertains here",
    "start": "1079000",
    "end": "1084039"
  },
  {
    "text": "like uh this is not Sol fine tuning but the fact that this L 213b was originally",
    "start": "1084039",
    "end": "1090120"
  },
  {
    "text": "train on like 4K and then you can get this this type of performance out of for tuning um is kind of very very",
    "start": "1090120",
    "end": "1097919"
  },
  {
    "text": "interesting um more interesting than the performance is like the amount of money",
    "start": "1097919",
    "end": "1103320"
  },
  {
    "text": "that you have to pay to actually get these results right um so mix draw like this is basically showing how much time",
    "start": "1103320",
    "end": "1110200"
  },
  {
    "text": "we have to spend on like training cost for like 5,000 examples and then how",
    "start": "1110200",
    "end": "1115679"
  },
  {
    "text": "much money you have to spend for quering like across 16k Contex L for a th000",
    "start": "1115679",
    "end": "1121480"
  },
  {
    "text": "queries and uh mix is and like GPD",
    "start": "1121480",
    "end": "1127080"
  },
  {
    "text": "3.5 um end up being on the inference side almost like 2x more expensive than",
    "start": "1127080",
    "end": "1134280"
  },
  {
    "text": "the Llama 213b after you find tun it these these numbers are essentially coming from like the token based pricing",
    "start": "1134280",
    "end": "1140960"
  },
  {
    "text": "on any scale in and for the other providers like open a we get it from their",
    "start": "1140960",
    "end": "1146080"
  },
  {
    "text": "website um and like yeah let me show this this two so it's 2x um kind of",
    "start": "1146080",
    "end": "1153159"
  },
  {
    "text": "cheaper than the other things and it's almost like 10x cheaper than when you do",
    "start": "1153159",
    "end": "1158320"
  },
  {
    "text": "fine tuning with up um and I guess gp4 is out of question no one can really go",
    "start": "1158320",
    "end": "1163799"
  },
  {
    "text": "into production uh with that type of cost right um but what is interesting is",
    "start": "1163799",
    "end": "1169280"
  },
  {
    "text": "like of course you're kind of doing a trade-off between quality and cost but the question is how do you balance it",
    "start": "1169280",
    "end": "1176080"
  },
  {
    "text": "right and for different applications that answer is different and you got to try like these things to see where the",
    "start": "1176080",
    "end": "1182720"
  },
  {
    "text": "tradeoff is and like whether you can get away with like open source models by tune on your like specific domain but",
    "start": "1182720",
    "end": "1190200"
  },
  {
    "text": "the end result is like you pay a little bit off front on training cost iterations and then um during deployment",
    "start": "1190200",
    "end": "1196840"
  },
  {
    "text": "going to production you can issue like save a ton yeah so we have put out I guess the",
    "start": "1196840",
    "end": "1205280"
  },
  {
    "text": "detail of this block post out this morning you can check it out uh but in",
    "start": "1205280",
    "end": "1210320"
  },
  {
    "text": "conclusion I think I covered without diving too deep about like how we actually do this fine tuning um it's",
    "start": "1210320",
    "end": "1218000"
  },
  {
    "text": "part of like the any scale endpoint feature you can actually like uh try it out like here you can upload your data",
    "start": "1218000",
    "end": "1223840"
  },
  {
    "text": "set very quickly and be fine tuned with the right context link on your model but what you have to focus on is essentially",
    "start": "1223840",
    "end": "1230640"
  },
  {
    "text": "creating this data sets and setting up your evaluation framework so I just showcase what we did for this kind of",
    "start": "1230640",
    "end": "1237600"
  },
  {
    "text": "needle inh stack problem in the rack context and how we created the synthetic data um and how you can effectively",
    "start": "1237600",
    "end": "1245200"
  },
  {
    "text": "achieve longer context length via fine tuning and made an economical like case",
    "start": "1245200",
    "end": "1250360"
  },
  {
    "text": "study for why going for open source models fine tune is like a long-term",
    "start": "1250360",
    "end": "1256159"
  },
  {
    "text": "beneficial yeah um thanks for your attention",
    "start": "1256159",
    "end": "1261220"
  },
  {
    "text": "[Applause]",
    "start": "1261220",
    "end": "1267030"
  },
  {
    "text": "you do we have time for question yeah yeah",
    "start": "1268080",
    "end": "1275799"
  },
  {
    "text": "micop any questions from the",
    "start": "1276360",
    "end": "1280760"
  },
  {
    "text": "audience so how do you populate to the to extend the whole content",
    "start": "1283799",
    "end": "1289279"
  },
  {
    "text": "how did it with what like the conance is very huge so how do you feel feel that yeah good question so um when we",
    "start": "1289279",
    "end": "1297600"
  },
  {
    "text": "basically what we did was we went through a lot of like biographies right and then mapped them to I guess their",
    "start": "1297600",
    "end": "1304679"
  },
  {
    "text": "output desired output so you imagine you have 5,000 pairs of biographies and the",
    "start": "1304679",
    "end": "1310679"
  },
  {
    "text": "desired output of structure now what you can do is like mix them you take one needle like as a needle you randomly",
    "start": "1310679",
    "end": "1317360"
  },
  {
    "text": "sample one as needle and Sample I don't know a few thousand for ha stack and",
    "start": "1317360",
    "end": "1323120"
  },
  {
    "text": "then you sweep where that needle is and that's how we construct the data so basically you end up with like a lot of",
    "start": "1323120",
    "end": "1330400"
  },
  {
    "text": "examples that are have like one needle at different locations you have a question about who is this guy and then",
    "start": "1330400",
    "end": "1336840"
  },
  {
    "text": "you get an app so the data there's no data set limitation essentially by doing it",
    "start": "1336840",
    "end": "1343039"
  },
  {
    "text": "once no yeah basically you pick R like Bunch you have a set of biographies and then",
    "start": "1346960",
    "end": "1354240"
  },
  {
    "text": "you pick them as need and then the other ones as H and yeah like the there's no",
    "start": "1354240",
    "end": "1360520"
  },
  {
    "text": "explicit constraint to treat them",
    "start": "1360520",
    "end": "1364600"
  },
  {
    "text": "equally there bunch of questions here as well go ahead hello good evening hi how you doing I wanted to ask about fine",
    "start": "1366360",
    "end": "1373520"
  },
  {
    "text": "tuning for different languages outside of English how is it possible to get the",
    "start": "1373520",
    "end": "1379200"
  },
  {
    "text": "models as a user to understand languages that it's not really familiar with good",
    "start": "1379200",
    "end": "1386360"
  },
  {
    "text": "question um I think fine-tuning for understanding a new language you you",
    "start": "1386360",
    "end": "1392919"
  },
  {
    "text": "would I wouldn't call it fine tuning anymore I think like you need enough data representing that language like the",
    "start": "1392919",
    "end": "1398919"
  },
  {
    "text": "structure the syntax in that language um to be able to train this model and like",
    "start": "1398919",
    "end": "1405200"
  },
  {
    "text": "of course like I think pre-training a model helps but um you know like it is more it",
    "start": "1405200",
    "end": "1412520"
  },
  {
    "text": "requires more than fine tuning to kind of uh tailor the model towards that",
    "start": "1412520",
    "end": "1417640"
  },
  {
    "text": "language right so um there's a lot of like languages that have essentially are a remap between different words to",
    "start": "1417640",
    "end": "1424600"
  },
  {
    "text": "English those things can essentially get away with less amount of data to train on but there would be languages that are",
    "start": "1424600",
    "end": "1432000"
  },
  {
    "text": "totally structurally different like I don't know Arabic and English right one starts from right to left right um that",
    "start": "1432000",
    "end": "1438000"
  },
  {
    "text": "that the concepts could still exist but you need to have those languages present",
    "start": "1438000",
    "end": "1443279"
  },
  {
    "text": "your pre-training data when these models were built otherwise I don't think there's any hope to kind of fine-tune",
    "start": "1443279",
    "end": "1450440"
  },
  {
    "text": "them so that one has been trained on those languages so yeah like the training data set that they had probably",
    "start": "1454039",
    "end": "1461159"
  },
  {
    "text": "had all like all of this like there is like this um you know Mixr for example",
    "start": "1461159",
    "end": "1466640"
  },
  {
    "text": "um that it doesn't really work on like languages like farsy or Arabic like gp4 just does it very well so it means that",
    "start": "1466640",
    "end": "1473080"
  },
  {
    "text": "the pre-training data does didn't have enough coverage of these data",
    "start": "1473080",
    "end": "1479200"
  },
  {
    "text": "distributions yeah there's a lot of here um so if I",
    "start": "1479200",
    "end": "1485760"
  },
  {
    "text": "understand this correctly a needle is one distinct piece of information right in that whole data set but in reality",
    "start": "1485760",
    "end": "1493120"
  },
  {
    "text": "like we have also seen data sets might have a lot of these distinct needles",
    "start": "1493120",
    "end": "1498559"
  },
  {
    "text": "so will the evaluation change or the results change if that happens yeah so",
    "start": "1498559",
    "end": "1504039"
  },
  {
    "text": "if there are cases that you're thinking about where you need to fuse multiple information from multiple you know uh",
    "start": "1504039",
    "end": "1512640"
  },
  {
    "text": "information sources to a coherent answer then you have to think about okay I have",
    "start": "1512640",
    "end": "1518240"
  },
  {
    "text": "to have like when when I'm constructing this data I need to actually put two needles and maybe it's the same guy or",
    "start": "1518240",
    "end": "1525000"
  },
  {
    "text": "like maybe they're somehow connected and then I ask uh the language model like some question that relates those two",
    "start": "1525000",
    "end": "1532159"
  },
  {
    "text": "together so you have to think about that problem from that perspective and construct the same like similarly",
    "start": "1532159",
    "end": "1537440"
  },
  {
    "text": "construct the data set and things like that right um so I would imagine like if I want to continue this you know effort",
    "start": "1537440",
    "end": "1544240"
  },
  {
    "text": "on to address that it would be like um hey like the same biography from like",
    "start": "1544240",
    "end": "1550000"
  },
  {
    "text": "the same person but like in um two two different like distinct paragraphs and then I would ask the same question about",
    "start": "1550000",
    "end": "1556440"
  },
  {
    "text": "that person um that would require like understanding of like you know like when is this guy born and like when he died",
    "start": "1556440",
    "end": "1563600"
  },
  {
    "text": "right one piece of information is here and then another piece of information is there and I can evaluate that um because",
    "start": "1563600",
    "end": "1570159"
  },
  {
    "text": "I know the ground truth um yeah during data set",
    "start": "1570159",
    "end": "1575679"
  },
  {
    "text": "construction hi nice talk thank you for comparing the performance and the cost yeah was very delighting and very",
    "start": "1575760",
    "end": "1582679"
  },
  {
    "text": "straightforward I have two questions one is come to you mentioned uh Ceta and",
    "start": "1582679",
    "end": "1588039"
  },
  {
    "text": "that's off a lot of paino for many domains like Finance or Healthcare and",
    "start": "1588039",
    "end": "1593080"
  },
  {
    "text": "you specific giving example that like the birthday and year those are pretty",
    "start": "1593080",
    "end": "1599919"
  },
  {
    "text": "confidential you know in some field uh so if I understand correctly as user we",
    "start": "1599919",
    "end": "1605720"
  },
  {
    "text": "upload the data could be uh partial data incomplete data UNC construct data",
    "start": "1605720",
    "end": "1611159"
  },
  {
    "text": "unclean on like and fit into the system which is an scale system and so you guys",
    "start": "1611159",
    "end": "1618039"
  },
  {
    "text": "provide the um the rest right and which is linked to the second questions when",
    "start": "1618039",
    "end": "1624080"
  },
  {
    "text": "it comes to model comp comparisons you automatically to uh select the the most",
    "start": "1624080",
    "end": "1630159"
  },
  {
    "text": "high performance or most revent models to fit into customers needs is that correct and and and what it would be",
    "start": "1630159",
    "end": "1637799"
  },
  {
    "text": "fascinating if a um Google Gemini model comparison will be in crw too yeah what",
    "start": "1637799",
    "end": "1644720"
  },
  {
    "text": "was that last part yeah the Google Gemini yeah yeah but what how how does that fit into oh uh actually uh folks I",
    "start": "1644720",
    "end": "1653080"
  },
  {
    "text": "know he's here he I will Happ you to introduce to him to talk about more yeah let me let me answer the question about",
    "start": "1653080",
    "end": "1660320"
  },
  {
    "text": "the uh yeah like the fine tuning product we have is like uh you fit in your data",
    "start": "1660320",
    "end": "1666760"
  },
  {
    "text": "and there are like this metrics loss you know and evaluation loss and things like that that we kind of monitor along the",
    "start": "1666760",
    "end": "1672200"
  },
  {
    "text": "way and we pick like the if you provide a validation data set we pick the checkpoint essentially",
    "start": "1672200",
    "end": "1678039"
  },
  {
    "text": "that has the minimum loss on the validation data set and if you specify a",
    "start": "1678039",
    "end": "1683320"
  },
  {
    "text": "different criteria like number of epoch or something like that we kind of like use that as a stopping criteria but the",
    "start": "1683320",
    "end": "1690320"
  },
  {
    "text": "end result is like you get a checkpoint and it's already getting served you can start quering it using open AI",
    "start": "1690320",
    "end": "1696200"
  },
  {
    "text": "compatible API and uh essentially run your evaluation so in other words we're",
    "start": "1696200",
    "end": "1702080"
  },
  {
    "text": "taking the pain of like dealing with infrastructure and like setting up your training scripts away your just focus on",
    "start": "1702080",
    "end": "1708440"
  },
  {
    "text": "your data set curation and evaluation pipeline okay so also one of the paino I",
    "start": "1708440",
    "end": "1713919"
  },
  {
    "text": "want to follow up that's great thank you answer that um let's say company usually have a lot of Legacy data base like",
    "start": "1713919",
    "end": "1721679"
  },
  {
    "text": "sever of them you name like vet database I would say pineco coin or some sort so",
    "start": "1721679",
    "end": "1727799"
  },
  {
    "text": "uh let's say let's say there's so many Legacy datas restoring from those and",
    "start": "1727799",
    "end": "1734200"
  },
  {
    "text": "you mentioned Landa and some in one of a graph that how how you gonna be are you GNA assist in the data uh transformation",
    "start": "1734200",
    "end": "1742600"
  },
  {
    "text": "migration I mean yeah yeah so um so basically like those are application",
    "start": "1742600",
    "end": "1748679"
  },
  {
    "text": "layer logic we care about we take care of the infrastructure parts right so Pine con of course they scale things",
    "start": "1748679",
    "end": "1755279"
  },
  {
    "text": "right like vector databases um we kind of like have integration points with some of them",
    "start": "1755279",
    "end": "1760760"
  },
  {
    "text": "Pine con being one of them but these are mostly like things that fall into the platform solution like what I covered is",
    "start": "1760760",
    "end": "1767640"
  },
  {
    "text": "like the public endpoint API product similar to open AI but for open source",
    "start": "1767640",
    "end": "1772960"
  },
  {
    "text": "models um so all the integration happens at the client side like an application",
    "start": "1772960",
    "end": "1778120"
  },
  {
    "text": "layer but of course like if you hit a scaling need like any scale essentially provides the next steps like pine cone",
    "start": "1778120",
    "end": "1784880"
  },
  {
    "text": "like you move on to platform like there's pine cone integration things like that that can help scale up your application for your needs I see that's",
    "start": "1784880",
    "end": "1792159"
  },
  {
    "text": "the important step thank you so much y",
    "start": "1792159",
    "end": "1799320"
  },
  {
    "text": "um have you considered running the needle in a h stack evaluation against say like structure data like for example",
    "start": "1799600",
    "end": "1804799"
  },
  {
    "text": "the specific use Cas that I'm thinking about is like if you're looking at actions based agents who are using Code as execution actions based agents and",
    "start": "1804799",
    "end": "1811640"
  },
  {
    "text": "you're giving it like a large structured data of like API documentation for",
    "start": "1811640",
    "end": "1817039"
  },
  {
    "text": "example like how well does it do in identifying what like apis are needed to yeah we haven't tested that I think",
    "start": "1817039",
    "end": "1824000"
  },
  {
    "text": "that's a good Cas of study to kind of extend the need in that yeah",
    "start": "1824000",
    "end": "1831000"
  },
  {
    "text": "yeah thanks so much for the talk um I I thought the depth of the context was",
    "start": "1831559",
    "end": "1838080"
  },
  {
    "text": "really interesting could you talk more about like are there ways to get around lose like less accuracy when the needle",
    "start": "1838080",
    "end": "1846080"
  },
  {
    "text": "in the middle of the uh like like the documents versus at the beginning or the end or is that just a general problem",
    "start": "1846080",
    "end": "1853039"
  },
  {
    "text": "with llms yeah I guess like different LMS show different different behaviors",
    "start": "1853039",
    "end": "1858519"
  },
  {
    "text": "uh like this kind of phenomena exists also in gp4 uh Greg I guess is I haven't",
    "start": "1858519",
    "end": "1864960"
  },
  {
    "text": "seen great like he's going to talk about uh the what he observed from gp4 128k it",
    "start": "1864960",
    "end": "1870919"
  },
  {
    "text": "happens at at like a certain after a certain context length um mixol for",
    "start": "1870919",
    "end": "1877279"
  },
  {
    "text": "example shows it differently anthropic shows it differently but like there is definitely these knowledge gaps like",
    "start": "1877279",
    "end": "1883360"
  },
  {
    "text": "there's like I don't know we can call it like uh forgetting or something like that right where at certain context",
    "start": "1883360",
    "end": "1889600"
  },
  {
    "text": "links uh the model just like doesn't pay attention to that and I no one essentially Knows Why like maybe um it's",
    "start": "1889600",
    "end": "1897120"
  },
  {
    "text": "part of the training process um some of these models have different implementation of attention maybe it's",
    "start": "1897120",
    "end": "1902679"
  },
  {
    "text": "an artifact of that things like this but um for llama specifically this was also",
    "start": "1902679",
    "end": "1908799"
  },
  {
    "text": "present um this is probably like because of some architectural level details of",
    "start": "1908799",
    "end": "1914720"
  },
  {
    "text": "like how attention is implemented for these language models and that's basically shared everyone is using the same architecture there's no magic there",
    "start": "1914720",
    "end": "1920919"
  },
  {
    "text": "and that's why you see like so if you don't see this Behavior it's probably like um those models either have some",
    "start": "1920919",
    "end": "1927720"
  },
  {
    "text": "data like some figure out some like training procedure to not have that or",
    "start": "1927720",
    "end": "1933000"
  },
  {
    "text": "they have changed their architecture and someway",
    "start": "1933000",
    "end": "1937880"
  },
  {
    "text": "yeah a question over here like really good talk thank you for that um is there anything available in the open source",
    "start": "1938159",
    "end": "1944799"
  },
  {
    "text": "ecosystem that'll help you approximate how much data you need for a particular fine-tuning task and then also what's it",
    "start": "1944799",
    "end": "1952120"
  },
  {
    "text": "going to cost you to fine tune good feature ask",
    "start": "1952120",
    "end": "1957880"
  },
  {
    "text": "request um I don't think yeah I haven't seen anything uh my mental model When",
    "start": "1957880",
    "end": "1964960"
  },
  {
    "text": "developing these things is like you got to start with small amounts of data and and you got to mostly like think about",
    "start": "1964960",
    "end": "1971679"
  },
  {
    "text": "your problem right like um sometimes I start with even like prompt engineering",
    "start": "1971679",
    "end": "1977039"
  },
  {
    "text": "and I see how far I can get like if my task actually um meets that criteria like this is clearly like you can never",
    "start": "1977039",
    "end": "1984240"
  },
  {
    "text": "prompt a model and you got to try like your best like it's hard to prompt a model to Output a certain structure like",
    "start": "1984240",
    "end": "1990159"
  },
  {
    "text": "this right there's a lot of if this happened do this don't do that like a lot of like please don't output",
    "start": "1990159",
    "end": "1996480"
  },
  {
    "text": "something else right um and this demands for fun like this is perfect case for fine tuning and I would start with like",
    "start": "1996480",
    "end": "2003840"
  },
  {
    "text": "small data sets because that's the money I'm spending but you shouldn't over",
    "start": "2003840",
    "end": "2008960"
  },
  {
    "text": "index on the cost you spend on training because that's insignificant compared to what you would pay for inference and you",
    "start": "2008960",
    "end": "2016600"
  },
  {
    "text": "kind of do that analysis to yourself like how much time should I spend on like doing these iterations versus I can",
    "start": "2016600",
    "end": "2022760"
  },
  {
    "text": "essentially start with the largest data set possible and like try it out see if it works also it depends on how hard it",
    "start": "2022760",
    "end": "2030200"
  },
  {
    "text": "is to collect data from your domain if it's like something that can be scripted used other models to do this for you",
    "start": "2030200",
    "end": "2037159"
  },
  {
    "text": "right and then you do some human curation on top usually these are scale level methods I would say like start",
    "start": "2037159",
    "end": "2043399"
  },
  {
    "text": "with the largest data that you can but if it's like harder uh I would like start smaller and go from there yeah",
    "start": "2043399",
    "end": "2050320"
  },
  {
    "text": "it's hard to think about a tool to do this thing automatically essentially right thank you for the talk",
    "start": "2050320",
    "end": "2058280"
  },
  {
    "text": "oh there Sor go ahead yeah yeah so the I was wondering about the impact of the uh",
    "start": "2058280",
    "end": "2065878"
  },
  {
    "text": "the the the policy that you would use for fine tuning for example like let's",
    "start": "2065879",
    "end": "2071079"
  },
  {
    "text": "say that you know like you choose to fine tune the the early layers versus",
    "start": "2071079",
    "end": "2076358"
  },
  {
    "text": "you know the the the final layers and also how would you avoid like overfitting yeah let's say that if you",
    "start": "2076359",
    "end": "2083440"
  },
  {
    "text": "have like a small data set how would you make sure that you are not over fitting yeah so we use low rank adapters as the",
    "start": "2083440",
    "end": "2092720"
  },
  {
    "text": "fine-tuning mechanism which means you're fine-tuning every layer but you're",
    "start": "2092720",
    "end": "2098079"
  },
  {
    "text": "fine-tuning a kind of like a a yeah a low rank essentially like a extra",
    "start": "2098079",
    "end": "2104960"
  },
  {
    "text": "parameters on top of each layer um so that the whole like you kind of modify",
    "start": "2104960",
    "end": "2110200"
  },
  {
    "text": "the activations as they go through but the this thing this method has a self-regularization like you're not",
    "start": "2110200",
    "end": "2117079"
  },
  {
    "text": "changing the model too much by its definition so it's kind of like very prone to that kind of overfitting um",
    "start": "2117079",
    "end": "2124839"
  },
  {
    "text": "kind of case you mentioned and yeah this is kind of a pretty much a standard fine tuning mechanism like low rank adapters",
    "start": "2124839",
    "end": "2131160"
  },
  {
    "text": "there are variations of it of course but like uh we have like our custom version here um which kind of pretty much Works",
    "start": "2131160",
    "end": "2138800"
  },
  {
    "text": "across the board um yeah I hope that answers your question uh there's a question here",
    "start": "2138800",
    "end": "2146960"
  },
  {
    "text": "that where is the yeah uh all right that's the last question all",
    "start": "2146960",
    "end": "2152680"
  },
  {
    "text": "right uh if you could go back like a slide I noticed you have the chart that like this one um so you mentioned that",
    "start": "2152680",
    "end": "2159920"
  },
  {
    "text": "fine tuning llama 2 helps it do better and come closer to 3.5 um I noticed in",
    "start": "2159920",
    "end": "2166240"
  },
  {
    "text": "the code you ran llama 2 like the base model but you didn't plot it um so how'",
    "start": "2166240",
    "end": "2172680"
  },
  {
    "text": "it perform yeah it's basically around like 50 perish um yeah we should probably add it or something but mixw",
    "start": "2172680",
    "end": "2180079"
  },
  {
    "text": "like is perceived as the best open source model and we wanted to see how far that goes um you know two of course",
    "start": "2180079",
    "end": "2187440"
  },
  {
    "text": "it's going to be lower but like um mixol is kind of the the king in the house so",
    "start": "2187440",
    "end": "2192640"
  },
  {
    "text": "we wanted to evaluate that that's an aler bound I wanted to show okay we beat that not only that but also G do you",
    "start": "2192640",
    "end": "2198839"
  },
  {
    "text": "offer mixol fine tuning would you rerun it with mixol fine tuning if yeah got",
    "start": "2198839",
    "end": "2205720"
  },
  {
    "text": "it yeah all right I think uh that's question was it Y",
    "start": "2208000",
    "end": "2214839"
  },
  {
    "text": "Cool stop sharing next we have um Greg up",
    "start": "2214839",
    "end": "2221770"
  },
  {
    "text": "[Applause]",
    "start": "2221770",
    "end": "2230960"
  },
  {
    "text": "here thank you all right let's see",
    "start": "2232319",
    "end": "2241240"
  },
  {
    "text": "here",
    "start": "2245599",
    "end": "2248599"
  },
  {
    "text": "that is not what we",
    "start": "2260800",
    "end": "2264200"
  },
  {
    "text": "want beautiful my name is Greg and today",
    "start": "2275599",
    "end": "2282839"
  },
  {
    "text": "we're going to be talking about the origin story behind the needle in a Hy stack analysis because this all started",
    "start": "2282839",
    "end": "2289359"
  },
  {
    "text": "with a question how good are LMS at reasoning over simple facts over a long",
    "start": "2289359",
    "end": "2295160"
  },
  {
    "text": "context so imagine this I didn't get into open ai's Dev day right I'm guessing a l all did it",
    "start": "2295160",
    "end": "2302440"
  },
  {
    "text": "but that's okay so I actually partnered with the GP which is a VC firm up in North Beach and we decided well we're",
    "start": "2302440",
    "end": "2308200"
  },
  {
    "text": "probably not the only ones let's throw a watch party for people right and so I'm sitting there and then all of a sudden I",
    "start": "2308200",
    "end": "2315839"
  },
  {
    "text": "hear Sam start to say gp4 turbo supports up to 128,000 tokens of",
    "start": "2315839",
    "end": "2323680"
  },
  {
    "text": "context that's 300 pages of a standard book you'll notice that the model is much more accurate over a long",
    "start": "2323680",
    "end": "2330319"
  },
  {
    "text": "context hm interesting but before we talk about Sam first again my name is",
    "start": "2330319",
    "end": "2335440"
  },
  {
    "text": "Greg I um help businesses figure out how they're actually going to deliver customer value with AI and that sounds pretty markety but stats are cool models",
    "start": "2335440",
    "end": "2343560"
  },
  {
    "text": "are cool but I really want to know like how are end users actually going to get value from these language model",
    "start": "2343560",
    "end": "2348720"
  },
  {
    "text": "capabilities we're seeing here all right so let's get back to Sam so 300 pages",
    "start": "2348720",
    "end": "2354400"
  },
  {
    "text": "that is quite a lot that's a whole book that he says and for all of my diffuser",
    "start": "2354400",
    "end": "2361400"
  },
  {
    "text": "friends diffusion friends out there that's a whole book all right so I'm",
    "start": "2361400",
    "end": "2367400"
  },
  {
    "text": "wondering the same thing that everyone else is wondering and so I look inside my third eye my llm eye and I wonder",
    "start": "2367400",
    "end": "2375480"
  },
  {
    "text": "what is performance like though sure we have 120,000 uh tokens but what's",
    "start": "2375480",
    "end": "2381880"
  },
  {
    "text": "performance like all right so I decide I want to do a simple test right I'm a",
    "start": "2381880",
    "end": "2387160"
  },
  {
    "text": "simple person I need something really really easy I don't want any reasoning I don't want any multi hops I don't want",
    "start": "2387160",
    "end": "2392720"
  },
  {
    "text": "any agents I don't want any of that stuff I just want can it pull a simple fact from a piece of context right so",
    "start": "2392720",
    "end": "2399280"
  },
  {
    "text": "what I do is I go get one of Paul Graham's essays because that seems to be the training material for all AI",
    "start": "2399280",
    "end": "2404720"
  },
  {
    "text": "tutorial examples right and I put a um statement which I think is going to be unique in",
    "start": "2404720",
    "end": "2410960"
  },
  {
    "text": "there right the best thing to do in San Francisco is eat a sandwich and sit in Dolores Park on a sunny day and why did",
    "start": "2410960",
    "end": "2418240"
  },
  {
    "text": "I choose this well because I used to go to Ikes and go to Dolores Park and then go get a sandwich I wanted to put a",
    "start": "2418240",
    "end": "2423680"
  },
  {
    "text": "picture of this Ikes on here but it turns out it closed so that's how you know you're getting older when your favorite spots start to close down a",
    "start": "2423680",
    "end": "2428880"
  },
  {
    "text": "little bit more but either way so what we did was is we take that 1K token of",
    "start": "2428880",
    "end": "2434280"
  },
  {
    "text": "context and we turn it into a prompt so we have our context on the left and we",
    "start": "2434280",
    "end": "2439560"
  },
  {
    "text": "have our question on the right our query what is the best thing to do in San Francisco does the model get it well it",
    "start": "2439560",
    "end": "2447280"
  },
  {
    "text": "says yeah the best thing to do in San Francisco is eat a sandwich L's Park Sunday day okay whatever cool it gets it",
    "start": "2447280",
    "end": "2452720"
  },
  {
    "text": "that's pretty awesome now we go to the other side what will it do with",
    "start": "2452720",
    "end": "2459520"
  },
  {
    "text": "128,000 tokens of context we ask it the same thing we put in the same fact we ask the same question and we throw it in",
    "start": "2459520",
    "end": "2465640"
  },
  {
    "text": "there and it says I'm sorry the text does not say anything about what the",
    "start": "2465640",
    "end": "2470960"
  },
  {
    "text": "best thing to do in San Francisco is H that's interesting and I should have said",
    "start": "2470960",
    "end": "2477599"
  },
  {
    "text": "something about myself earlier but I have a background in growth and product analytics and so I'm a data guy right I I think in I think in spreadsheets I",
    "start": "2477599",
    "end": "2483960"
  },
  {
    "text": "think in gradients and so my next question is is where's it break down because we know that 1K is good to",
    "start": "2483960",
    "end": "2491640"
  },
  {
    "text": "go we know that 128k isn't good to go so",
    "start": "2491640",
    "end": "2496880"
  },
  {
    "text": "where's a breakdown in the middle right that's the question that I had and as a data person it's like well",
    "start": "2496880",
    "end": "2502480"
  },
  {
    "text": "let's just go gather some data points and let's go figure it out right but then I thought to myself wait wait a",
    "start": "2502480",
    "end": "2507720"
  },
  {
    "text": "minute there's that one lost in the middle paper and for those who are familiar with this lost in the middle",
    "start": "2507720",
    "end": "2513160"
  },
  {
    "text": "paper it was a pretty cool one that came out and it said that facts in in the middle of a long context are recalled",
    "start": "2513160",
    "end": "2519160"
  },
  {
    "text": "with less accuracy than facts on the the front end of the tail end and like any well- constructed",
    "start": "2519160",
    "end": "2526800"
  },
  {
    "text": "abstract they said that exactly in the front and they said we find that performance can degrade significantly",
    "start": "2526800",
    "end": "2532040"
  },
  {
    "text": "when changing the position of relevant information interesting so now we have a",
    "start": "2532040",
    "end": "2539160"
  },
  {
    "text": "two axi thing going on we have context length and we have where the heck it is in the document it's kind of interesting",
    "start": "2539160",
    "end": "2544640"
  },
  {
    "text": "right um I should have said a second thing about data people like myself is I constantly have the D3 example Gallery",
    "start": "2544640",
    "end": "2552280"
  },
  {
    "text": "floating through my head these data visualizations whenever I need inspiration I just go right to the D3 gallery and I go check it out to",
    "start": "2552280",
    "end": "2558920"
  },
  {
    "text": "see if anything Sparks some interest for me and so I thought to myself okay well now we have two things we have an X and",
    "start": "2558920",
    "end": "2564680"
  },
  {
    "text": "Y grid but then we want to measure performance too did it actually recall",
    "start": "2564680",
    "end": "2569960"
  },
  {
    "text": "the fact or did it not did it half did it uh call over fact uh with a 50%",
    "start": "2569960",
    "end": "2575839"
  },
  {
    "text": "accuracy maybe I don't know but let's let's go find out let's go check it out and so",
    "start": "2575839",
    "end": "2581240"
  },
  {
    "text": "what I did was I wrote up a little script the code doesn't matter here what matters is that I took my finger I",
    "start": "2581240",
    "end": "2587079"
  },
  {
    "text": "placed it over cursor's big red play button it's not red but big big button",
    "start": "2587079",
    "end": "2592920"
  },
  {
    "text": "there and we started and we kicked it off and so I wanted to find out where did the model start to break down and as",
    "start": "2592920",
    "end": "2599040"
  },
  {
    "text": "I started to run this test what we did my very first data point is I started down at the bottom left and for those in",
    "start": "2599040",
    "end": "2605440"
  },
  {
    "text": "the back if you can't see this I'm sorry we started at 1K and we started by placing the fact at the very bottom so",
    "start": "2605440",
    "end": "2610800"
  },
  {
    "text": "literally the last sentence in this prompt and we said hey what's the best thing to do in San Francisco it got it",
    "start": "2610800",
    "end": "2617280"
  },
  {
    "text": "and so green means good and it means that it was able to um accurately accurately retrieve the fact so as we",
    "start": "2617280",
    "end": "2623760"
  },
  {
    "text": "started doing it it started coming through it started coming through and then all of a sudden we have a bunch of greens for 1K and we have to let the",
    "start": "2623760",
    "end": "2630400"
  },
  {
    "text": "rest of the thing run now we'll skip to the end and we'll do without the dramatic effect but what came out the",
    "start": "2630400",
    "end": "2636920"
  },
  {
    "text": "other end was a visualization that I put together in uh Google slides and I",
    "start": "2636920",
    "end": "2643319"
  },
  {
    "text": "called it pressure testing GPT 428k via needle and a haystack because you're placing the random fact in there now we",
    "start": "2643319",
    "end": "2649400"
  },
  {
    "text": "could talk about this and go really really deep I'm only just going to say two things here I think are interesting you'll notice all of the green context",
    "start": "2649400",
    "end": "2656079"
  },
  {
    "text": "until the middle that means it was it could accurately retrieve the fact that I asked it to up until uh 64,000 tokens",
    "start": "2656079",
    "end": "2664359"
  },
  {
    "text": "of context I don't know about you but I think that's pretty cool now of course is this",
    "start": "2664359",
    "end": "2670359"
  },
  {
    "text": "the most rigorous thing in the world is it bulletproof absolutely not but the fact that it was able to do this across all these data points um shows hey yes",
    "start": "2670359",
    "end": "2677720"
  },
  {
    "text": "there is some promise here now on the second half as expected the longer the context the less accuracy you get but",
    "start": "2677720",
    "end": "2685160"
  },
  {
    "text": "was what was interesting to me is it was the top half of the document that had poor performance but not just any part",
    "start": "2685160",
    "end": "2692960"
  },
  {
    "text": "of the top half of the document the very top part you'll notice it's all 100% to top across the top that means if the",
    "start": "2692960",
    "end": "2699160"
  },
  {
    "text": "fact was the first thing in this in the essay it got it 100% of the time but the",
    "start": "2699160",
    "end": "2704480"
  },
  {
    "text": "minute that it started to creep down a little bit well that's when it started to get a problem all right um this isn't",
    "start": "2704480",
    "end": "2710520"
  },
  {
    "text": "we're going to talk more about this in a second here so I won't go too far deep into it but either way I got those results and I thought hm pretty",
    "start": "2710520",
    "end": "2718040"
  },
  {
    "text": "cool do people get the Sam meme okay good it's not directly appical cuz we're",
    "start": "2719440",
    "end": "2724960"
  },
  {
    "text": "not killing startups here but I thought that was a funny f um either way I go to Twitter and I think well I need to",
    "start": "2724960",
    "end": "2730200"
  },
  {
    "text": "publish these results and so no I'm not doing a white paper no I'm not doing a blog post I'm again I'm a simple person",
    "start": "2730200",
    "end": "2735240"
  },
  {
    "text": "so I wrote this up on Twitter here and I did a good long thread I tried to explain myself and packaged it up and",
    "start": "2735240",
    "end": "2740839"
  },
  {
    "text": "before I sent it out I was like man I really better get a stats friend to come give me some help on this so I DMD",
    "start": "2740839",
    "end": "2746480"
  },
  {
    "text": "Charles and I said hey Charles can you give me some thoughts on this and he goes well Greg I would have done a few things differently but generally I'm",
    "start": "2746480",
    "end": "2753920"
  },
  {
    "text": "Gonna Give You thumbs up on this analysis I said cool that's great so again I take my big finger and I put it",
    "start": "2753920",
    "end": "2759119"
  },
  {
    "text": "on the post button of Twitter and out the other end some really cool people started to recognize the analysis the",
    "start": "2759119",
    "end": "2766200"
  },
  {
    "text": "needle and H stock analysis some people liked it some people asked if they could donate some people weren't fans that's",
    "start": "2766200",
    "end": "2772280"
  },
  {
    "text": "okay you're not going to please everybody um my favorite response that came from the Twitter side was somebody",
    "start": "2772280",
    "end": "2777440"
  },
  {
    "text": "who posted the Will Smith iroot meme so Will Smith says you're saying you can't",
    "start": "2777440",
    "end": "2783280"
  },
  {
    "text": "even perfectly recall $18,000 tokens the robot says can",
    "start": "2783280",
    "end": "2789720"
  },
  {
    "text": "you and Will Smith and shambles yeah that was my favorite",
    "start": "2789720",
    "end": "2795520"
  },
  {
    "text": "response um but what was cool is this went out I think it was in November at some time and then you get a DM from",
    "start": "2795520",
    "end": "2801359"
  },
  {
    "text": "anthropic and they say hey Greg we want you to do this for us and I said cool they were open to",
    "start": "2801359",
    "end": "2809480"
  },
  {
    "text": "sponsoring it but I didn't want them to influence the results at all so I said hey I'm going to run this you're not just going to use me for the microphone",
    "start": "2809480",
    "end": "2815200"
  },
  {
    "text": "or anything like that and they're like no that's totally fine that's totally cool um the other thing I should have said about data people and this is the",
    "start": "2815200",
    "end": "2822400"
  },
  {
    "text": "third and final thing I'll say about data people they always want more data so I",
    "start": "2822400",
    "end": "2828440"
  },
  {
    "text": "paid for this open AI gp4 test this was out of my own pocket I was comfortable spending the couple hundred bucks that it took to go do it but if anthropics",
    "start": "2828440",
    "end": "2837040"
  },
  {
    "text": "paying I'm GNA crank it up a little bit so I did a 15 x 15 beforehand so that's 225 data points for anthropic we did a",
    "start": "2837040",
    "end": "2843640"
  },
  {
    "text": "35x 35 and so that was about five and a half times as much data and then we do",
    "start": "2843640",
    "end": "2850720"
  },
  {
    "text": "the same test for them um it it wasn't it wasn't as",
    "start": "2850720",
    "end": "2856720"
  },
  {
    "text": "positive for anthropic right out the gate and again I put this on Twitter it went out and it started to get a little",
    "start": "2856720",
    "end": "2862359"
  },
  {
    "text": "bit of publicity and some people started to rag on anthropic and um that's not what we want we're all on the same team",
    "start": "2862359",
    "end": "2868800"
  },
  {
    "text": "here we want cool lmms we want cool value for customers right and so we want to support each other through this and",
    "start": "2868800",
    "end": "2874119"
  },
  {
    "text": "so anthropic ended up posting a response blog post to this analysis and they did",
    "start": "2874119",
    "end": "2880880"
  },
  {
    "text": "their own analysis with their own heat map and it was much different right there's a lot more green on this um no",
    "start": "2880880",
    "end": "2888200"
  },
  {
    "text": "but really and so the the next natural question which is2 which they were",
    "start": "2888200",
    "end": "2893400"
  },
  {
    "text": "they've been a fabulous throughout this entire process the next natural question is how did you get so much green like",
    "start": "2893400",
    "end": "2899839"
  },
  {
    "text": "what what what happened with it right and so um they gave a few more prompt",
    "start": "2899839",
    "end": "2905040"
  },
  {
    "text": "engineering tips for working with Claude that I was not applying that open gbt",
    "start": "2905040",
    "end": "2911800"
  },
  {
    "text": "for uh it wasn't applicable towards right and so what they said in their blog post um and I know in the back here",
    "start": "2911800",
    "end": "2918200"
  },
  {
    "text": "that the screen's a little low I'll I'll skip right to the point when they end their prompt I ended it with what I had",
    "start": "2918200",
    "end": "2924800"
  },
  {
    "text": "on the left which was assistant colon because that's what they wanted me to end it with meaning hey assistant time",
    "start": "2924800",
    "end": "2930760"
  },
  {
    "text": "to do your job go do it right they added the words on the right",
    "start": "2930760",
    "end": "2935880"
  },
  {
    "text": "and the words are assistant here's the most relevant sentence in the context so",
    "start": "2935880",
    "end": "2941440"
  },
  {
    "text": "I was thinking about how to describe this and how I think about it before this it's kind of like they're on the top of the mountain and they gave the",
    "start": "2941440",
    "end": "2947720"
  },
  {
    "text": "language model just a little push to get it start rolling down so I think that there's a good discussion on whether or",
    "start": "2947720",
    "end": "2953520"
  },
  {
    "text": "not this is overfitting a prompt for a specific task at hand but that's outside the scope of this conversation either",
    "start": "2953520",
    "end": "2959160"
  },
  {
    "text": "way there is much better results with this and I also think it's another lesson in prompt engineering that if you",
    "start": "2959160",
    "end": "2964960"
  },
  {
    "text": "adjust your promp in this case by eight words or something like that you get drastically different",
    "start": "2964960",
    "end": "2970720"
  },
  {
    "text": "results so keep exploring on the prompts um either way uh what's cool is",
    "start": "2970720",
    "end": "2977839"
  },
  {
    "text": "this analysis it starts to live on without me and so some people ask hey Greg do you mind if I fork the code it's like hey I have its out of my hands at",
    "start": "2977839",
    "end": "2984119"
  },
  {
    "text": "this point go and run with it it's like a living thing at this time so it'll get shared on Twitter I'll see the visualization I don't think oh that's",
    "start": "2984119",
    "end": "2990079"
  },
  {
    "text": "pretty cool that's awesome um and then arise uh sent me an email they said hey Greg we love it it you want to take this",
    "start": "2990079",
    "end": "2996319"
  },
  {
    "text": "to the next level and I said go for it I absolutely love it and so all the work that arise is doing has been really",
    "start": "2996319",
    "end": "3001839"
  },
  {
    "text": "really awesome to see how they've uh increase the rigor of the test and how they're really um publishing more",
    "start": "3001839",
    "end": "3007720"
  },
  {
    "text": "results and really promoting llm evaluation which is all fun so thank you",
    "start": "3007720",
    "end": "3014200"
  },
  {
    "text": "for listening to the backstory behind the needle [Applause]",
    "start": "3014200",
    "end": "3024440"
  },
  {
    "text": "Anis",
    "start": "3024440",
    "end": "3027440"
  },
  {
    "text": "e wow so uh so we're going to do a",
    "start": "3054440",
    "end": "3060640"
  },
  {
    "text": "fireside chat first and if if any of you want to see the actual results we try to reproduce anthropics results we'll show",
    "start": "3060640",
    "end": "3067559"
  },
  {
    "text": "that after in the next set of research so cool is this echoing can",
    "start": "3067559",
    "end": "3075880"
  },
  {
    "text": "you cool awesome how's everyone",
    "start": "3075880",
    "end": "3082480"
  },
  {
    "text": "doing so uh so so Jason uh co founder of arise we're think of us as like a data",
    "start": "3083240",
    "end": "3090760"
  },
  {
    "text": "dog for for AI kind of observability evaluation for for AI hey I'm Robert um one of the",
    "start": "3090760",
    "end": "3097960"
  },
  {
    "text": "co-founders and CEO of any scale so we are an AI infrastructure company so we help companies businesses um scale AI",
    "start": "3097960",
    "end": "3106280"
  },
  {
    "text": "workloads like training inference embedding computations across uh a bunch",
    "start": "3106280",
    "end": "3111599"
  },
  {
    "text": "of gpus or a bunch of Computer Resources cool uh thought I just start off with um maybe just a show hands how",
    "start": "3111599",
    "end": "3119160"
  },
  {
    "text": "many of you have tested or used extrol or mistol",
    "start": "3119160",
    "end": "3124599"
  },
  {
    "text": "models wow okay I was expecting more uh how how many of you uh like don't know",
    "start": "3124720",
    "end": "3132359"
  },
  {
    "text": "what mixol or mistol models are is anyone daring to raise her",
    "start": "3132359",
    "end": "3138480"
  },
  {
    "text": "hand cool um Robert why don't you why don't you start off I I feel like that's those are kind of one of the hot or or",
    "start": "3138480",
    "end": "3144720"
  },
  {
    "text": "at least the hot over the last um yeah last B can you describe mistol",
    "start": "3144720",
    "end": "3150079"
  },
  {
    "text": "versus mixol and sure so well one of the you know big stories in 2023 was the",
    "start": "3150079",
    "end": "3156400"
  },
  {
    "text": "trajectory of Open Source llms right I think at the beginning of",
    "start": "3156400",
    "end": "3161440"
  },
  {
    "text": "2023 um the best open- source llms were not that good right and were not really",
    "start": "3161440",
    "end": "3168000"
  },
  {
    "text": "usable in real applications and that completely changed by the end of 2023 so you had um you",
    "start": "3168000",
    "end": "3176200"
  },
  {
    "text": "know the meta team the Llama team at meta released llama 2 which was a step",
    "start": "3176200",
    "end": "3182160"
  },
  {
    "text": "function above all other open source LMS up to that point um they had the you know seven they released a few different",
    "start": "3182160",
    "end": "3189680"
  },
  {
    "text": "llama 2 models in different sizes um and later that year later last",
    "start": "3189680",
    "end": "3195720"
  },
  {
    "text": "year um this this m mistl company which is a small AI startup in France released",
    "start": "3195720",
    "end": "3203599"
  },
  {
    "text": "well actually they released a couple different models so they first released their small mistl 7 billion parameter",
    "start": "3203599",
    "end": "3208680"
  },
  {
    "text": "model which was um you know better than any other 7 billion parameter models like super small super fast and that",
    "start": "3208680",
    "end": "3216720"
  },
  {
    "text": "quickly led to a few fine-tuned variants of the mistal based model which were you",
    "start": "3216720",
    "end": "3221760"
  },
  {
    "text": "know even better um but what happened toward the end of last year was that they released a slightly larger uh",
    "start": "3221760",
    "end": "3228839"
  },
  {
    "text": "mixture of experts model called mixol so it's the company name is mistol and they",
    "start": "3228839",
    "end": "3234119"
  },
  {
    "text": "called the model mix stroll because it's a mixture of experts so just kind of a cute name and this is really the first",
    "start": "3234119",
    "end": "3242040"
  },
  {
    "text": "open source model that started to U people are saying is as good as gbt 3.5",
    "start": "3242040",
    "end": "3247440"
  },
  {
    "text": "and um you know is is uh starting to really erode the lead that proprietary",
    "start": "3247440",
    "end": "3254040"
  },
  {
    "text": "models like you know open AI have so this is one of the big develops developments in open source models the",
    "start": "3254040",
    "end": "3259319"
  },
  {
    "text": "companies you know both uh meta and and mistl are have plans or are claiming",
    "start": "3259319",
    "end": "3265240"
  },
  {
    "text": "that they're going to release gp4 quality models in in 2024 so there's a lot to look forward to there yeah we run",
    "start": "3265240",
    "end": "3272599"
  },
  {
    "text": "an immense number of evaluations both model level and kind of task level um",
    "start": "3272599",
    "end": "3277680"
  },
  {
    "text": "and I would say open source models have have under underperformed uh uh have not",
    "start": "3277680",
    "end": "3283960"
  },
  {
    "text": "to date and and had not stood out to us um the the eight you know 8 x7b the the",
    "start": "3283960",
    "end": "3290200"
  },
  {
    "text": "mixtur model is one of those that like has surprised a set of return um one area we you know we've done some",
    "start": "3290200",
    "end": "3296599"
  },
  {
    "text": "evaluations on specific tasks or specific types of benchmarks and one thing that's been interesting to us so",
    "start": "3296599",
    "end": "3302640"
  },
  {
    "text": "one of the most you know commonly requested features um for us has been",
    "start": "3302640",
    "end": "3309520"
  },
  {
    "text": "function calling y basically this is a feature that open AI has had a while for a while but it's been a big gap in the",
    "start": "3309520",
    "end": "3315760"
  },
  {
    "text": "open source community and function calling you can think about the motivation here um if you're just if",
    "start": "3315760",
    "end": "3322400"
  },
  {
    "text": "you're building a chatbot then you ask a question to the llm and the response is",
    "start": "3322400",
    "end": "3327440"
  },
  {
    "text": "going to be text you're having a conversation but there going there are many applications where the output of",
    "start": "3327440",
    "end": "3334240"
  },
  {
    "text": "the llm is not going to be just read by a human it's going to be used in programmatically right maybe consumed by",
    "start": "3334240",
    "end": "3341200"
  },
  {
    "text": "another llm maybe it's used in some backend logic and some uh you know powering some product and if you are if",
    "start": "3341200",
    "end": "3349119"
  },
  {
    "text": "you know if the output of your llm is being used programmatically you often want it to be more structured and that",
    "start": "3349119",
    "end": "3354599"
  },
  {
    "text": "could mean maybe you wanted to sort of have a Json format so you can you can parse it more",
    "start": "3354599",
    "end": "3359760"
  },
  {
    "text": "easily maybe you wanted to have to call a specific function and say what the arguments to that function are like",
    "start": "3359760",
    "end": "3366039"
  },
  {
    "text": "calling some external API and so function calling is is essentially a feature that allows these you know llms",
    "start": "3366039",
    "end": "3374599"
  },
  {
    "text": "to return more structured outputs that can be used programmatically and um you",
    "start": "3374599",
    "end": "3381039"
  },
  {
    "text": "know we released so this is something that uh as part of our llm endpoint product we released function calling",
    "start": "3381039",
    "end": "3387440"
  },
  {
    "text": "which was a big gap in the open source um you know AI ecosystem and that you",
    "start": "3387440",
    "end": "3392480"
  },
  {
    "text": "know quickly became uh super widely used and one of the highlights there is we we",
    "start": "3392480",
    "end": "3398119"
  },
  {
    "text": "experimented with function calling enabling function calling for a huge variety of models you know the Llama models mistl and so on the Highlight was",
    "start": "3398119",
    "end": "3405680"
  },
  {
    "text": "that um the seven the small mistl model the 7 billion parameter model actually",
    "start": "3405680",
    "end": "3412400"
  },
  {
    "text": "did Super well so uh not as well as G GPT 4 but um as well or maybe a little",
    "start": "3412400",
    "end": "3417720"
  },
  {
    "text": "better than uh GPT 3.5 so that was a very exciting for us it's impressive I mean it's it's the first thing it's giv",
    "start": "3417720",
    "end": "3424240"
  },
  {
    "text": "me uh like like an inkling that that the open source will has a chance of",
    "start": "3424240",
    "end": "3429760"
  },
  {
    "text": "catching up um I think the you know you launched I think uh any any scale",
    "start": "3429760",
    "end": "3436240"
  },
  {
    "text": "launched mixol support quite a while ago um you must s and but and I'll say this",
    "start": "3436240",
    "end": "3442799"
  },
  {
    "text": "kind of unbiased even though I'm here uh that that any scale is actually quite quite of all the endpoints we test it",
    "start": "3442799",
    "end": "3450400"
  },
  {
    "text": "throughput speed it's very impressive and what are the CH like best of of whatever of everything I've tested what",
    "start": "3450400",
    "end": "3457599"
  },
  {
    "text": "um like what are the challenges of serving this and can someone do it themsel and like like is that biting off",
    "start": "3457599",
    "end": "3464119"
  },
  {
    "text": "a lot and like what what's the yeah yeah I mean so that's a great question um",
    "start": "3464119",
    "end": "3472839"
  },
  {
    "text": "running these types of so you can definitely run it yourself right and there are actually a lot of cool",
    "start": "3472839",
    "end": "3479359"
  },
  {
    "text": "projects that try to optimize for you know even running these models on your laptop right and actually they're um",
    "start": "3479359",
    "end": "3486160"
  },
  {
    "text": "they're projects like llama CPP which get pretty good latency for you know at very at low you know very small batch",
    "start": "3486160",
    "end": "3492680"
  },
  {
    "text": "sizes like a batch size of one but get good latency uh just on your on your laptop so you can do that kind of stuff",
    "start": "3492680",
    "end": "3500799"
  },
  {
    "text": "now there are a lot of complexities with actually running these apis I think um",
    "start": "3500799",
    "end": "3508480"
  },
  {
    "text": "one of the challenges is trading off um throughput and latency and sometimes you",
    "start": "3508480",
    "end": "3514559"
  },
  {
    "text": "can optimize for both right sometimes you can get the both Best of Both Worlds but just to give you one example there",
    "start": "3514559",
    "end": "3520520"
  },
  {
    "text": "are techniques that we invest a lot of energy into like speculative decoding right so actually can you raise your hand if you if you are familiar with",
    "start": "3520520",
    "end": "3527000"
  },
  {
    "text": "speculative decoding okay so maybe it's not a super widely known technique but",
    "start": "3527000",
    "end": "3533200"
  },
  {
    "text": "um this is well maybe a little bit related to some techniques in with um um you know like",
    "start": "3533200",
    "end": "3541359"
  },
  {
    "text": "compilers or or CPUs use which is um as you're as a processor is executing",
    "start": "3541359",
    "end": "3547319"
  },
  {
    "text": "instructions sometimes it gets to an if statement and you know that if statement",
    "start": "3547319",
    "end": "3552599"
  },
  {
    "text": "could evaluate to be true or false um and it doesn't know yet which branch",
    "start": "3552599",
    "end": "3558079"
  },
  {
    "text": "it's going to go down but sometimes it can just guess which branch it's going to go down and start executing the",
    "start": "3558079",
    "end": "3563760"
  },
  {
    "text": "subsequent instruction uh and later on when it determines which branch to go down if it guessed",
    "start": "3563760",
    "end": "3569240"
  },
  {
    "text": "correctly then it can just sort of skip ahead right if it guessed incorrectly then that extra Works gets gets thrown",
    "start": "3569240",
    "end": "3574520"
  },
  {
    "text": "away right it's roughly analogous with um speculative decoding in LM so",
    "start": "3574520",
    "end": "3581400"
  },
  {
    "text": "decoding is the process of just generating all the output tokens um the way this works is if you",
    "start": "3581400",
    "end": "3588680"
  },
  {
    "text": "have a smaller llm like a faster llm sort of guess what the output tokens are",
    "start": "3588680",
    "end": "3593720"
  },
  {
    "text": "going to be um it's you know so normally with generating these output tokens you",
    "start": "3593720",
    "end": "3599640"
  },
  {
    "text": "generate them one at a time it's sequential but if you um have a bunch of",
    "start": "3599640",
    "end": "3605000"
  },
  {
    "text": "tokens output tokens that you guessed you can check them all in parallel right so you can actually um parallelize that",
    "start": "3605000",
    "end": "3613640"
  },
  {
    "text": "that process of of checking and if you guessed correctly then you can kind of skip ahead and so you can really reduce",
    "start": "3613640",
    "end": "3620119"
  },
  {
    "text": "latency this way um of course there's a lot of work in making the smaller model guess correctly and and you can imagine",
    "start": "3620119",
    "end": "3626799"
  },
  {
    "text": "um you know how that goes but this is a a really important technique for reducing latency for llms but you can",
    "start": "3626799",
    "end": "3634240"
  },
  {
    "text": "imagine it potentially comes at the cost of of throughput because it's um you",
    "start": "3634240",
    "end": "3639319"
  },
  {
    "text": "know involves doing some extra computation right so there are trade-offs like that and maybe depending on the load on your service you want",
    "start": "3639319",
    "end": "3646599"
  },
  {
    "text": "to um you know decide whether you're optimizing for latency or throughput and and there a lot of choices like that um",
    "start": "3646599",
    "end": "3653240"
  },
  {
    "text": "so there there many different subtle that we could go into wow is speculative decoding something that you you all have",
    "start": "3653240",
    "end": "3659400"
  },
  {
    "text": "put together is it something that the mixt you know part part of the um the base model itself um so it doesn't come",
    "start": "3659400",
    "end": "3666200"
  },
  {
    "text": "with the base model we're actually uh in the process of so there many it's part",
    "start": "3666200",
    "end": "3671480"
  },
  {
    "text": "of the underlying inference engine so if you're familiar with inference engines like VM is one from Berkeley uh there's",
    "start": "3671480",
    "end": "3678119"
  },
  {
    "text": "tensorrt llm from Nvidia U there other other Frameworks like this and we're",
    "start": "3678119",
    "end": "3683319"
  },
  {
    "text": "actually in the process of open sourcing some of this the speculative decoding to and contributing that to the VM project",
    "start": "3683319",
    "end": "3689000"
  },
  {
    "text": "so um yeah and in terms of um the one",
    "start": "3689000",
    "end": "3695119"
  },
  {
    "text": "question for for those of the folks who were like uh maybe I'll I'll get a server to run this on do 8X SB run out",
    "start": "3695119",
    "end": "3702200"
  },
  {
    "text": "is it is it all multi- server is it any possibility of single server like how much memory like how what what are what",
    "start": "3702200",
    "end": "3708240"
  },
  {
    "text": "do you getting into when you're trying to take on serving that the probably the most comparable to GPT 3.5 right so um",
    "start": "3708240",
    "end": "3717119"
  },
  {
    "text": "so these models are typically reported in terms of the number of parameters they have right so you hear about seven",
    "start": "3717119",
    "end": "3722279"
  },
  {
    "text": "billion parameter models or 70 billion parameter models um the the mixt model",
    "start": "3722279",
    "end": "3728160"
  },
  {
    "text": "is is I forget the exact number 50 billion 60 billion parameters something like that now when you think about okay",
    "start": "3728160",
    "end": "3735920"
  },
  {
    "text": "how do I go from number of parameters to how much memory I need right for uh",
    "start": "3735920",
    "end": "3741559"
  },
  {
    "text": "these machine learning models or for you know llms GPU memory is often the limiting",
    "start": "3741559",
    "end": "3747880"
  },
  {
    "text": "factor okay so or one of the limiting factors you need a certain amount of memory just to hold the uh model the",
    "start": "3747880",
    "end": "3755520"
  },
  {
    "text": "model weights in GPU memory right so if I need um if I have",
    "start": "3755520",
    "end": "3761599"
  },
  {
    "text": "so so uh one parameter that is like um",
    "start": "3761599",
    "end": "3767920"
  },
  {
    "text": "say it's like 16 bits right so 16 bits is say 16 bit float that's two bytes y",
    "start": "3767920",
    "end": "3774119"
  },
  {
    "text": "so so you b because one parameter is like two bytes you need roughly double the number of of so it's 7 billion you",
    "start": "3774119",
    "end": "3781279"
  },
  {
    "text": "might need parameters you might need 14 gigabytes of GPU memory um and so for",
    "start": "3781279",
    "end": "3787680"
  },
  {
    "text": "this you know 50 billion parameter model you might need 100 gigabytes of GPU memory so um there there's there's a",
    "start": "3787680",
    "end": "3793960"
  },
  {
    "text": "caveat asterisks on all of that but um it's a lot of these models you know are",
    "start": "3793960",
    "end": "3799920"
  },
  {
    "text": "not going to fit some of the larger models are not going to fit in a single um G U memory so you're often splitting",
    "start": "3799920",
    "end": "3806920"
  },
  {
    "text": "the model across multiple gpus and um and actually you know if you split it across more gpus you can um you know",
    "start": "3806920",
    "end": "3814880"
  },
  {
    "text": "again you have latency versus throughput trade-offs there got it got it so so it's uh quite quite a bit of work if you",
    "start": "3814880",
    "end": "3821799"
  },
  {
    "text": "really want to get that you can imagine if you're getting really fancy um depending on the load on the system you",
    "start": "3821799",
    "end": "3828119"
  },
  {
    "text": "may want and the resources available and you may want to um you know split a single model over more gpus at a certain",
    "start": "3828119",
    "end": "3835559"
  },
  {
    "text": "load or fewer gpus at a different load and yeah and and then uh for the mix",
    "start": "3835559",
    "end": "3841480"
  },
  {
    "text": "mixture of expert models is there any special sauce to fine tuning or is fine tuning any more challenging or is",
    "start": "3841480",
    "end": "3847520"
  },
  {
    "text": "exactly the same as as maybe a traditional model um it's a little more challenging so in our experiments we've",
    "start": "3847520",
    "end": "3854480"
  },
  {
    "text": "done um like Laura fine tuning yeah for the mixture models um in particular you",
    "start": "3854480",
    "end": "3861200"
  },
  {
    "text": "know U using these like adapters for these low rank so actually how many of",
    "start": "3861200",
    "end": "3867200"
  },
  {
    "text": "you have heard of um Laura fine tuning okay look a lot of people yeah all right",
    "start": "3867200",
    "end": "3873119"
  },
  {
    "text": "um so Laura is a you know stands for low rank adaptation I think it's a technique",
    "start": "3873119",
    "end": "3879079"
  },
  {
    "text": "for um where instead of fine tune doing full parameter fine- tuning where you're",
    "start": "3879079",
    "end": "3886200"
  },
  {
    "text": "uh modifying every weight in the neural network every parameter instead you are",
    "start": "3886200",
    "end": "3891760"
  },
  {
    "text": "you know you sort of have a compressed um adapter that you you know add on to some of the weights to um it's both can",
    "start": "3891760",
    "end": "3900039"
  },
  {
    "text": "be more efficient to train and it can all but it the real win is that um inference time because because you have",
    "start": "3900039",
    "end": "3907160"
  },
  {
    "text": "these adapters on top of the base model you can actually serve you know you still run the base model and then you",
    "start": "3907160",
    "end": "3913480"
  },
  {
    "text": "have if you want to have a ton of different fine tuned models you just have a ton of these different adapters on top and you can kind of share you can",
    "start": "3913480",
    "end": "3921599"
  },
  {
    "text": "um batch you know requests Ross for um",
    "start": "3921599",
    "end": "3926680"
  },
  {
    "text": "um multiple different fine-tuned models so definitely more challenging it sounds like it's um it's more challenging y um",
    "start": "3926680",
    "end": "3935640"
  },
  {
    "text": "but you know so the but it still works yeah got it and um do you support that",
    "start": "3935640",
    "end": "3942000"
  },
  {
    "text": "today or is it something that's coming out for any SK for the end points um we support it today for a lot of models and",
    "start": "3942000",
    "end": "3948559"
  },
  {
    "text": "um and uh and but you know we're continually adding new models here got it um I've noticed across the ecosystem",
    "start": "3948559",
    "end": "3956119"
  },
  {
    "text": "uh a number of different you know contact sizes um and specifically for for even just the mial model um is that",
    "start": "3956119",
    "end": "3964640"
  },
  {
    "text": "you know is the one you know is the mistal endpoints different than what's an open source are you looking at like",
    "start": "3964640",
    "end": "3971640"
  },
  {
    "text": "extending extending the context window yourself like what do you yeah we actually and this is um you know I think",
    "start": "3971640",
    "end": "3978559"
  },
  {
    "text": "one of the premises of this whole events but if you're building AI applications",
    "start": "3978559",
    "end": "3984039"
  },
  {
    "text": "you were probably building a rag application because um it's hard to imagine building an application without",
    "start": "3984039",
    "end": "3989440"
  },
  {
    "text": "context and um you know if you building one of these rag applications you",
    "start": "3989440",
    "end": "3995359"
  },
  {
    "text": "probably care about context you actually how many of you have run into limitations of context",
    "start": "3995359",
    "end": "4002279"
  },
  {
    "text": "lengths okay um you know context lengths are growing longer and longer but",
    "start": "4002279",
    "end": "4009440"
  },
  {
    "text": "there's still a lot of challenges um you know with selectively choosing or",
    "start": "4009440",
    "end": "4016119"
  },
  {
    "text": "retrieving the right context to feed into um your llm call and you're you",
    "start": "4016119",
    "end": "4022079"
  },
  {
    "text": "know ultimately we're going to want to build applications that have kind of infinite context that can take advantage",
    "start": "4022079",
    "end": "4028000"
  },
  {
    "text": "of all the data you know you've ever seen or that's out there on the internet right and so uh that is necessarily",
    "start": "4028000",
    "end": "4035680"
  },
  {
    "text": "going to be a rag application but um being able to the the more limited the",
    "start": "4035680",
    "end": "4042880"
  },
  {
    "text": "context is the less margin of error you have um in",
    "start": "4042880",
    "end": "4049119"
  },
  {
    "text": "your retrieval right and right now retrieval can actually be a big bottleneck for the accuracy of our",
    "start": "4049119",
    "end": "4055000"
  },
  {
    "text": "applications like when we build llm applications you know you can sort of simplified into a retrieval stage and a",
    "start": "4055000",
    "end": "4062200"
  },
  {
    "text": "generation stage and the bottleneck could be in either place right maybe the generation quality is just low is bad",
    "start": "4062200",
    "end": "4068799"
  },
  {
    "text": "and you need um more fine tuning or you need a better um more powerful model but",
    "start": "4068799",
    "end": "4074000"
  },
  {
    "text": "but a lot of the time the bottleneck is on the retrieval step you just didn't feed in the right context in the first place and so there was no hope and when",
    "start": "4074000",
    "end": "4081680"
  },
  {
    "text": "that's the case um you know there's a lot of work that goes into improving the retrieval side of the equation maybe um",
    "start": "4081680",
    "end": "4089000"
  },
  {
    "text": "you know maybe you need to chunk you know Define the embeddings differently maybe you need to fine-tune the embeddings or chunk your data",
    "start": "4089000",
    "end": "4095920"
  },
  {
    "text": "differently um or maybe you need to and actually chunking the data correctly is",
    "start": "4095920",
    "end": "4101920"
  },
  {
    "text": "like a is an art and there's a lot of work that goes into that or maybe you need to you know have another model kind",
    "start": "4101920",
    "end": "4107758"
  },
  {
    "text": "of um after you retrieve a bunch of stuff rank the different uh pieces of context to feed in so there's a",
    "start": "4107759",
    "end": "4114318"
  },
  {
    "text": "tremendous amount of iteration that goes into that side of the equation and but you can imagine the longer your context",
    "start": "4114319",
    "end": "4120400"
  },
  {
    "text": "is the more um room you have there yeah and I think the um you know there's a",
    "start": "4120400",
    "end": "4127880"
  },
  {
    "text": "bunch of teams like I think the unstructured people unstructured IO people are here today but help help with like cleaning up the data on the injest",
    "start": "4127880",
    "end": "4134480"
  },
  {
    "text": "side which is a you know half the battle of this um and then yeah coar and and",
    "start": "4134480",
    "end": "4140318"
  },
  {
    "text": "different types of retrievers that that are more you know more advanced in your simple retrieval are common",
    "start": "4140319",
    "end": "4147159"
  },
  {
    "text": "um in terms of um in terms of both let's first off like",
    "start": "4147159",
    "end": "4153238"
  },
  {
    "text": "endpoint evaluations like maybe let's not don't want to mix the terms too much but um I know you guys have put out some",
    "start": "4153239",
    "end": "4161159"
  },
  {
    "text": "metrics on on endpoints and trying trying to help people understand what are the differences and what what do you",
    "start": "4161159",
    "end": "4166520"
  },
  {
    "text": "think are like what what what do you put out there what are some of the the stuff that um is important as teams are",
    "start": "4166520",
    "end": "4172000"
  },
  {
    "text": "looking at at testing yeah so that's a great question one of the things we",
    "start": "4172000",
    "end": "4178120"
  },
  {
    "text": "noticed there are a number of different companies that host open source llms right and when you're evaluating and",
    "start": "4178120",
    "end": "4185679"
  },
  {
    "text": "there are many different choices you have different providers um of course different models and you know",
    "start": "4185679",
    "end": "4192199"
  },
  {
    "text": "different um so when you're building your application there are different things you may care",
    "start": "4192199",
    "end": "4197840"
  },
  {
    "text": "about you may care about cost you may care about of the llm you may care about",
    "start": "4197840",
    "end": "4202880"
  },
  {
    "text": "latency of the llm um you know you care about quality but that's a function of",
    "start": "4202880",
    "end": "4208199"
  },
  {
    "text": "which llm you're using right we find that typically the top priority for",
    "start": "4208199",
    "end": "4213400"
  },
  {
    "text": "people is quality um you have to the the application has to work y right um once",
    "start": "4213400",
    "end": "4219880"
  },
  {
    "text": "you meet that threshold for working uh then the next consideration people care about is latency depends on",
    "start": "4219880",
    "end": "4227080"
  },
  {
    "text": "the application but uh for many applications latency is sort of a core",
    "start": "4227080",
    "end": "4232360"
  },
  {
    "text": "part of the user experience right so then latency comes second and then once",
    "start": "4232360",
    "end": "4238719"
  },
  {
    "text": "but that only matters to a point right there's no need to make it infinitely fast and then once you make but once",
    "start": "4238719",
    "end": "4244159"
  },
  {
    "text": "it's fast enough then people care about cost and um I'll say that for some some",
    "start": "4244159",
    "end": "4251159"
  },
  {
    "text": "applications cost comes earlier but you can imagine um especially for batch",
    "start": "4251159",
    "end": "4257480"
  },
  {
    "text": "workloads where you're processing tons of data like maybe I want to look at every web page on the world in the world",
    "start": "4257480",
    "end": "4263960"
  },
  {
    "text": "and extract structured information from that and there's just a you know just running on hundreds of millions or",
    "start": "4263960",
    "end": "4269000"
  },
  {
    "text": "billions of pages or whatever then cost really starts to matter so uh but we see",
    "start": "4269000",
    "end": "4274159"
  },
  {
    "text": "those uh happening in those order that order and when people measure things",
    "start": "4274159",
    "end": "4281159"
  },
  {
    "text": "like latency or other performance metrics it's important to make sure",
    "start": "4281159",
    "end": "4286320"
  },
  {
    "text": "you're talking about the same thing because I may say you know the latency for this thing or the throughput is X",
    "start": "4286320",
    "end": "4291640"
  },
  {
    "text": "tokens per second um and somebody that can mean a lot of different things right for instance with llm uh performance you",
    "start": "4291640",
    "end": "4300719"
  },
  {
    "text": "often want to break things down into two steps one is just how long does it take to generate the first",
    "start": "4300719",
    "end": "4305840"
  },
  {
    "text": "token um so you know how um much what's the latency and that's a function of the",
    "start": "4305840",
    "end": "4311440"
  },
  {
    "text": "number of input tokens and then and how quickly can I generate output tokens right and those those different numbers",
    "start": "4311440",
    "end": "4319320"
  },
  {
    "text": "will depend on things like the typical um input number of to number of",
    "start": "4319320",
    "end": "4325239"
  },
  {
    "text": "input tokens the typical number of output tokens and it also you know different providers may do things like",
    "start": "4325239",
    "end": "4332199"
  },
  {
    "text": "caching common prompts and and you know caching things between queries and so",
    "start": "4332199",
    "end": "4337520"
  },
  {
    "text": "you can speed things up in common cases there are all sorts of decisions you can make there but the point of and other",
    "start": "4337520",
    "end": "4342760"
  },
  {
    "text": "people have actually started publishing llm performance benchmarks as well recently because it's just a useful",
    "start": "4342760",
    "end": "4349600"
  },
  {
    "text": "thing to do these are you want to have comparisons across uh so you you can decide for my application the typical",
    "start": "4349600",
    "end": "4356600"
  },
  {
    "text": "patterns that I'm working with and the dimensions that I care about whether it's cost or latency or other things um",
    "start": "4356600",
    "end": "4362320"
  },
  {
    "text": "what are the best choices and uh I know you you have something to get to in a short bit maybe we'll get some other questions but um",
    "start": "4362320",
    "end": "4369520"
  },
  {
    "text": "maybe there's the the little bit of open AI drama today I don't know if you heard Andre kathi laugh any thoughts on on",
    "start": "4369520",
    "end": "4377320"
  },
  {
    "text": "that um sorry to drop the the random",
    "start": "4377320",
    "end": "4382719"
  },
  {
    "text": "question um he was he was building the Jarvis like apparently the Jarvis with the agan stuff there I don't think any",
    "start": "4382719",
    "end": "4389159"
  },
  {
    "text": "of us have any other other information yeah I don't I don't have any any particular insight to to add there",
    "start": "4389159",
    "end": "4394280"
  },
  {
    "text": "unfortunately I think he does great work yeah he does great work he's someone I follow closely um like both him and",
    "start": "4394280",
    "end": "4400480"
  },
  {
    "text": "opening eye a lot so um as well as the other model providers want to be even um",
    "start": "4400480",
    "end": "4405760"
  },
  {
    "text": "so any questions from from the the group out here okay",
    "start": "4405760",
    "end": "4411520"
  },
  {
    "text": "um yeah go",
    "start": "4411520",
    "end": "4414760"
  },
  {
    "text": "ahead yeah seen a lot",
    "start": "4416760",
    "end": "4422000"
  },
  {
    "text": "ofation how do we prevent uh you know the close Source providers from just",
    "start": "4424080",
    "end": "4430480"
  },
  {
    "text": "training with the you know evaluation materials because it's all public right so there's nothing preventing me from",
    "start": "4430480",
    "end": "4437840"
  },
  {
    "text": "ingesting the question and answer in there and oh I get better result you know that's something that was I hope",
    "start": "4437840",
    "end": "4445239"
  },
  {
    "text": "it's not happening but it could happen you know like I don't know jini maybe yeah um so like is there like a",
    "start": "4445239",
    "end": "4451199"
  },
  {
    "text": "movements in the evaluation way like to make some sort of you know unified",
    "start": "4451199",
    "end": "4456239"
  },
  {
    "text": "evaluation that are you know not yeah some without this problem it's",
    "start": "4456239",
    "end": "4462440"
  },
  {
    "text": "it's like that is such a good question I mean so so the base of the question if",
    "start": "4462440",
    "end": "4469040"
  },
  {
    "text": "if you all didn't hear if you um did catch it is is basically um how do we",
    "start": "4469040",
    "end": "4474480"
  },
  {
    "text": "know our tests are good if the te you know one if the if the models be Trad and closed Source because you don't know",
    "start": "4474480",
    "end": "4480560"
  },
  {
    "text": "what the data that's been trained on is it could include the test which is the big worry um and I think it could I",
    "start": "4480560",
    "end": "4488400"
  },
  {
    "text": "think the risk is too even if they don't purposely include it you can tweets can included or like like any data it could",
    "start": "4488400",
    "end": "4495000"
  },
  {
    "text": "be randomly on a website that that someone's talking about the answer to this so it's very hard when you're training on the internet worth of data",
    "start": "4495000",
    "end": "4501360"
  },
  {
    "text": "to know did I accidentally put the answer in um and I think it's a big problem and this is spe specifically for",
    "start": "4501360",
    "end": "4508040"
  },
  {
    "text": "model evals which are trying to compare model like this how the model does in this task um and and I I kind of think",
    "start": "4508040",
    "end": "4517560"
  },
  {
    "text": "my ansers is is that any that the value of an eval and a model eval",
    "start": "4517560",
    "end": "4524120"
  },
  {
    "text": "declines exponentially as the days it's in the public is kind of what it feels like to me like like the first day I put",
    "start": "4524120",
    "end": "4530000"
  },
  {
    "text": "it out I feel good about what I have and then I'm slightly worried that providers are actually you know modifying and go",
    "start": "4530000",
    "end": "4536719"
  },
  {
    "text": "and you know doing something towards it there's been some really good writeups where people have described the really good eal that that that you know is",
    "start": "4536719",
    "end": "4544040"
  },
  {
    "text": "constantly challenging the model and they can't like but but I'm I don't I",
    "start": "4544040",
    "end": "4549159"
  },
  {
    "text": "haven't seen one that exists yet I worry everyone that we put out we'll put these results out but if we rerun it in a",
    "start": "4549159",
    "end": "4554560"
  },
  {
    "text": "month or two did a Model provider you know just train to to do my test well um",
    "start": "4554560",
    "end": "4559960"
  },
  {
    "text": "so I I I think it's a challenge for the industry and it's a great question um I I I I also very suspicious like the",
    "start": "4559960",
    "end": "4566360"
  },
  {
    "text": "leaderboard evals the wellknown ones um mlu I'm gonna talk a little bit about them I don't trust them like I I I don't",
    "start": "4566360",
    "end": "4573560"
  },
  {
    "text": "trust any of them like one thing I would you know add there is in some ways it's",
    "start": "4573560",
    "end": "4579880"
  },
  {
    "text": "very different but in some ways it's actually not that different from evaluating people if you think about um",
    "start": "4579880",
    "end": "4586480"
  },
  {
    "text": "standardized testing for humans they come up with new tests every year and",
    "start": "4586480",
    "end": "4592560"
  },
  {
    "text": "you could imagine if you didn't do that if you just use the same test every year what kind of problems you would see so",
    "start": "4592560",
    "end": "4599920"
  },
  {
    "text": "um I think you'll see some similarities in in that Dimension I mean yeah yeah",
    "start": "4599920",
    "end": "4605960"
  },
  {
    "text": "and the concern is it doesn't generalize right like if it generalizes then we're all good and you can you can go use it",
    "start": "4605960",
    "end": "4611480"
  },
  {
    "text": "but um if if it's just trained to do well on the test which we're worried about that that's the problem",
    "start": "4611480",
    "end": "4619679"
  },
  {
    "text": "yeah all right so some other hands in the back of the room did you hello so",
    "start": "4619679",
    "end": "4626120"
  },
  {
    "text": "thank you so much for putting up this session so you mentioned some decoding technique right so I want so instead of",
    "start": "4626120",
    "end": "4633880"
  },
  {
    "text": "predicting one token you'll be predicting multiple tokens right so how does you change like what is changing",
    "start": "4633880",
    "end": "4640280"
  },
  {
    "text": "the architecture and then how do you fine tune this model this change architecture yeah so that's a great",
    "start": "4640280",
    "end": "4647040"
  },
  {
    "text": "question so the overall technique I mentioned is called speculative decoding right and the idea is you know you have",
    "start": "4647040",
    "end": "4654760"
  },
  {
    "text": "a llm that's normally generates one output token at a time sequentially and",
    "start": "4654760",
    "end": "4660760"
  },
  {
    "text": "if you can have a smaller model correctly guess some of the output",
    "start": "4660760",
    "end": "4666480"
  },
  {
    "text": "tokens um like the first three output tokens in a row what well it generates",
    "start": "4666480",
    "end": "4671960"
  },
  {
    "text": "some predictions and you can in parallel check that those",
    "start": "4671960",
    "end": "4677760"
  },
  {
    "text": "predictions are correct right and maybe the first two are correct and the third one is not correct so if you guess two",
    "start": "4677760",
    "end": "4683760"
  },
  {
    "text": "of them correctly then um you can output those two tokens and and kind of skip",
    "start": "4683760",
    "end": "4689719"
  },
  {
    "text": "ahead to generate from where you left off so how does this change the model",
    "start": "4689719",
    "end": "4695120"
  },
  {
    "text": "architecture um actually you can do this with pretty much any llm you don't need",
    "start": "4695120",
    "end": "4701120"
  },
  {
    "text": "to change anything about the mod in order to do this which is super cool doesn't another model I guess sorry",
    "start": "4701120",
    "end": "4708120"
  },
  {
    "text": "another small model not a change in the original model right yeah okay I got it no no you don't have to you don't have",
    "start": "4708120",
    "end": "4713239"
  },
  {
    "text": "to change the original model you have to create the small model right um now if you fine-tune the original model then",
    "start": "4713239",
    "end": "4720960"
  },
  {
    "text": "you probably are going to need to modify the small model because the small model's predictions are not going to be",
    "start": "4720960",
    "end": "4726320"
  },
  {
    "text": "accurate much of the time and thank you I I think also like a lot of models use",
    "start": "4726320",
    "end": "4731800"
  },
  {
    "text": "Beam Beam surge like there's quite a bit of things where it's not just one token at a time like most of them are built on",
    "start": "4731800",
    "end": "4738280"
  },
  {
    "text": "on doing many many tokens at once uh hey Rob I have a very quick",
    "start": "4738280",
    "end": "4745239"
  },
  {
    "text": "question uh What uh do you use do you have your own data center or you just use cloud regarding the yeah",
    "start": "4745239",
    "end": "4753600"
  },
  {
    "text": "Hardware we primarily use the clouds are you like is it confidential or like",
    "start": "4753600",
    "end": "4760040"
  },
  {
    "text": "would you do you my share like use um Microsoft or AWS or hybrid we use a",
    "start": "4760040",
    "end": "4767239"
  },
  {
    "text": "variety of different providers but um a lot of you know a lot of AWS a lot of you know g a lot of standard Cloud",
    "start": "4767239",
    "end": "4774080"
  },
  {
    "text": "providers who use Lambda Labs right oh you only choose like GPU when you choose",
    "start": "4774080",
    "end": "4779560"
  },
  {
    "text": "different like cloud services or you will consider other like and we use all different instance types in fact a lot",
    "start": "4779560",
    "end": "4785679"
  },
  {
    "text": "of the workloads that we our customers run um you know use well of course A lot",
    "start": "4785679",
    "end": "4791719"
  },
  {
    "text": "of them use gpus but a lot lot of them also use CPUs and just as an example say",
    "start": "4791719",
    "end": "4797800"
  },
  {
    "text": "you want to You're Building some big rag application you have a ton of data you want to compute embeddings for all of",
    "start": "4797800",
    "end": "4803280"
  },
  {
    "text": "your data you know billions of vectors um there's a big there's a a big GPU",
    "start": "4803280",
    "end": "4809360"
  },
  {
    "text": "component to this which is running the embedding model there's also a big CPU component which is loading the data",
    "start": "4809360",
    "end": "4814520"
  },
  {
    "text": "chunking the data pre processing the data um and or similarly with training",
    "start": "4814520",
    "end": "4820280"
  },
  {
    "text": "if you want when you have something that's both GPU intensive and data",
    "start": "4820280",
    "end": "4826199"
  },
  {
    "text": "intensive you often need to offload the the data part onto CPU machines because",
    "start": "4826199",
    "end": "4832199"
  },
  {
    "text": "they're way cheaper so this Clock Company is more like your partners right so if I'm a user I can choose oh I",
    "start": "4832199",
    "end": "4838960"
  },
  {
    "text": "prefer to Trend my stuff like on AWS so I can use AWS and you like kind of like",
    "start": "4838960",
    "end": "4847120"
  },
  {
    "text": "that's right there the way our platform works is that uh our customers you know if they want to run training workloads",
    "start": "4847120",
    "end": "4853480"
  },
  {
    "text": "or deploy models they have the option to bring their just to bring their own cloud accounts and we can manage the",
    "start": "4853480",
    "end": "4860360"
  },
  {
    "text": "model training and deployment inside of their cloud account their AWS account say and that's advantageous in a lot of",
    "start": "4860360",
    "end": "4866679"
  },
  {
    "text": "scenarios it's you know not always the thing you want to do but if it it is the thing a lot of businesses want to do",
    "start": "4866679",
    "end": "4871840"
  },
  {
    "text": "because um a they have committed spending with these Cloud providers like they've already you know signed up to",
    "start": "4871840",
    "end": "4876920"
  },
  {
    "text": "spend millions of dollars on AWS and they want to you know use that spending uh they also have negotiated you know",
    "start": "4876920",
    "end": "4882360"
  },
  {
    "text": "discount accounts with these different Cloud providers they want to take advantage of that um so there are a lot of different reasons right thank you",
    "start": "4882360",
    "end": "4888480"
  },
  {
    "text": "because I work for AI chip company so maybe you can talk more later and another factor is just um you know in a",
    "start": "4888480",
    "end": "4895440"
  },
  {
    "text": "lot of cases it makes their life simpler if the data and the models and the code",
    "start": "4895440",
    "end": "4900639"
  },
  {
    "text": "never leave their cloud account awesome thanks uh real quick question one for each of you each of you",
    "start": "4900639",
    "end": "4907719"
  },
  {
    "text": "Jason outside of mmu what are other good how should developers or thinkers or builders be thinking about evaluation uh",
    "start": "4907719",
    "end": "4915159"
  },
  {
    "text": "outside of maybe large language model uh evals uh specifically uh like MML and",
    "start": "4915159",
    "end": "4921520"
  },
  {
    "text": "then Robert in terms ofe do you see anyone fine-tuning Mes in in your space",
    "start": "4921520",
    "end": "4927120"
  },
  {
    "text": "and if they are is it early stage is it really tough do you see anyone do that in the space so one question for each so",
    "start": "4927120",
    "end": "4932960"
  },
  {
    "text": "yeah uh so so what what are what are good evals if I don't like the the public ones uh my recommendation for",
    "start": "4932960",
    "end": "4939360"
  },
  {
    "text": "most teams is is actually um is is if if you're in a business and you're building",
    "start": "4939360",
    "end": "4944760"
  },
  {
    "text": "you have some examples of what you're trying of whatever task you're on so so I think that your best eval is actually going to be internally some examples of",
    "start": "4944760",
    "end": "4951080"
  },
  {
    "text": "what you wanted to do and and build your own data set and test data set I think that's like outside and the the model",
    "start": "4951080",
    "end": "4956400"
  },
  {
    "text": "eval is out there are going to help guide you to choose a model my recommendation also there is start with",
    "start": "4956400",
    "end": "4962040"
  },
  {
    "text": "the fastest and easiest and uh most easy accessible one a lot of times if you're",
    "start": "4962040",
    "end": "4967159"
  },
  {
    "text": "you know it's the cloud provider you're on uh whatever deal they have so you know as it's going to be open AI Google",
    "start": "4967159",
    "end": "4974080"
  },
  {
    "text": "Gemini and and AWS is typically claw uh and then use that one um and and and so",
    "start": "4974080",
    "end": "4980920"
  },
  {
    "text": "so like the model you've a to choose your model A lot of times you should just ch choose the best available to you",
    "start": "4980920",
    "end": "4985960"
  },
  {
    "text": "and then see if it works on your data um and then you start to build a little data set that's your own test set I",
    "start": "4985960",
    "end": "4991880"
  },
  {
    "text": "think it's the best because it represents your task it represents what you wanted to do um you're not worrying",
    "start": "4991880",
    "end": "4997639"
  },
  {
    "text": "about cheating tests and in the end that's what you got to use it in your in your job to go do something",
    "start": "4997639",
    "end": "5003280"
  },
  {
    "text": "yeah you just to emphasize that point you have to do model evaluation inhouse you can't Outsource that part and this",
    "start": "5003280",
    "end": "5010400"
  },
  {
    "text": "is because it's very problem specific for the actual you know task you're trying to accomplish and you need to",
    "start": "5010400",
    "end": "5017520"
  },
  {
    "text": "really get into the details like look at the data look at the specific examples look at the failure modes and and",
    "start": "5017520",
    "end": "5023360"
  },
  {
    "text": "there's going to be a lot of iteration uh based on that um yeah for the question you asked me about",
    "start": "5023360",
    "end": "5029000"
  },
  {
    "text": "fine-tuning mixture of experts we do like we are fine tuning like mixol um we",
    "start": "5029000",
    "end": "5034840"
  },
  {
    "text": "our customers are fine tuning mixol and I think in general you know um Mixter",
    "start": "5034840",
    "end": "5040239"
  },
  {
    "text": "Vex models are I think are here to stay and they're going to be uh model",
    "start": "5040239",
    "end": "5045800"
  },
  {
    "text": "customization is just going to be uh used across the board for well for all for all models basically I think there's",
    "start": "5045800",
    "end": "5052560"
  },
  {
    "text": "um you know fundamentally people are going to want to improve quality for",
    "start": "5052560",
    "end": "5057960"
  },
  {
    "text": "their specific applications and you have a couple different knobs for improving quality and one of them is is fine",
    "start": "5057960",
    "end": "5064600"
  },
  {
    "text": "tuning is any learning tun different is the same I think",
    "start": "5064600",
    "end": "5073440"
  },
  {
    "text": "the um I think the biggest difference with so we've only really",
    "start": "5073440",
    "end": "5080719"
  },
  {
    "text": "done like Laura fine tuning for uh from mixol but there are big differences",
    "start": "5080719",
    "end": "5085960"
  },
  {
    "text": "between full parameter fine tuning and Laura fine tuning um certainly can be",
    "start": "5085960",
    "end": "5091639"
  },
  {
    "text": "easier to get uh full parameter fine tuning working and for things that are um especially",
    "start": "5091639",
    "end": "5098239"
  },
  {
    "text": "for like if you have a lot of data that you need to use and take advantage of um",
    "start": "5098239",
    "end": "5104560"
  },
  {
    "text": "then you're often going you know if it's starting if you have so much data that you're fine tuning on that it starts to",
    "start": "5104560",
    "end": "5109800"
  },
  {
    "text": "look more like pre-training then um you know you're better off using full parameter fine tuning um I think one of",
    "start": "5109800",
    "end": "5116760"
  },
  {
    "text": "the big advantages you know in my mind the big advantage of Laura fine tuning",
    "start": "5116760",
    "end": "5122040"
  },
  {
    "text": "is when you need to um serve a bunch of fine tuned models and you want to you",
    "start": "5122040",
    "end": "5127679"
  },
  {
    "text": "need to do that in a coste efficient",
    "start": "5127679",
    "end": "5131040"
  },
  {
    "text": "way and also sorry if you have more questions about fine tuning kro over there is the the best person to you find",
    "start": "5135040",
    "end": "5143239"
  },
  {
    "text": "him awesome uh I was curious in such a like GPU constrained world now where",
    "start": "5143239",
    "end": "5150719"
  },
  {
    "text": "like getting access to computers is difficult people like Sam Alman are trying to gather together seven trillion",
    "start": "5150719",
    "end": "5157080"
  },
  {
    "text": "to WR build AI chip foundaries and things of that nature I think one of the cool things with uh any scale is that",
    "start": "5157080",
    "end": "5165239"
  },
  {
    "text": "it's meant to be sort of you can deploy it on heterogeneous resources sort of",
    "start": "5165239",
    "end": "5170360"
  },
  {
    "text": "like uh different clusters or different instances of compute so how do you guys",
    "start": "5170360",
    "end": "5176080"
  },
  {
    "text": "thinking about serving really large models like 70 billion parameter models potentially 100 plus billion parameter",
    "start": "5176080",
    "end": "5183679"
  },
  {
    "text": "models uh by taking advantage of whatever limited compute is available",
    "start": "5183679",
    "end": "5189880"
  },
  {
    "text": "and are you guys thinking about any interesting Innovations or or sort of Technologies in that space as well yeah",
    "start": "5189880",
    "end": "5197760"
  },
  {
    "text": "I think there's a lot of you know both current stuff we're doing and future stuff we plan to do but if you think",
    "start": "5197760",
    "end": "5205440"
  },
  {
    "text": "about um say cost efficiency for llm",
    "start": "5205440",
    "end": "5210840"
  },
  {
    "text": "serving llm inference you have to really think about it holistically like many different layers",
    "start": "5210840",
    "end": "5217480"
  },
  {
    "text": "of the teex stack okay so there's everything from just um where you get",
    "start": "5217480",
    "end": "5223040"
  },
  {
    "text": "gpus and how much you're paying for them right there's things like can you take advantage of um you know different",
    "start": "5223040",
    "end": "5231159"
  },
  {
    "text": "clouds different regions within those clouds to just take advantage of the cheapest GP like can you take advantage",
    "start": "5231159",
    "end": "5237320"
  },
  {
    "text": "of uh to the extent that there is a spot Market can you take advantage of the spot Market for instances um now",
    "start": "5237320",
    "end": "5245679"
  },
  {
    "text": "slightly higher levels like um you have you know many different models you're",
    "start": "5245679",
    "end": "5251040"
  },
  {
    "text": "serving probably right some are getting high traffic some are getting low traffic right can you um can you make",
    "start": "5251040",
    "end": "5259199"
  },
  {
    "text": "can you sort of swap for the high traffic models can you autoscale the number of replicas of the model up and",
    "start": "5259199",
    "end": "5266400"
  },
  {
    "text": "down you know rapidly to account for you know variable Demand right for the low",
    "start": "5266400",
    "end": "5271760"
  },
  {
    "text": "traffic models like can you Multiplex the serving of many different models across um you know fewer gpus right um",
    "start": "5271760",
    "end": "5281480"
  },
  {
    "text": "there are things like can you know I mentioned techniques like speculative decoding but there are many other",
    "start": "5281480",
    "end": "5286800"
  },
  {
    "text": "techniques for also improving improving throughput right techniques also like uh",
    "start": "5286800",
    "end": "5292000"
  },
  {
    "text": "continuous batching is a really important one and as you as your surface",
    "start": "5292000",
    "end": "5297520"
  },
  {
    "text": "is under variable load can you um adjust whether you're optimizing",
    "start": "5297520",
    "end": "5303080"
  },
  {
    "text": "for throughput or optimizing for latency based on that load right there's and there different choices that make sense at different uh different loads uh there",
    "start": "5303080",
    "end": "5310719"
  },
  {
    "text": "are things like can you um as you Auto scaling you know you may need to load",
    "start": "5310719",
    "end": "5317719"
  },
  {
    "text": "new models say from S3 into GPU memory like can you really optimize uh the the",
    "start": "5317719",
    "end": "5325159"
  },
  {
    "text": "auto scaling time right that means is normally by default the thing you might do goes from S3 to into memory to D and",
    "start": "5325159",
    "end": "5332360"
  },
  {
    "text": "then back into memory and then into GPU memory and can you just go straight you know pipeline it all the way through um",
    "start": "5332360",
    "end": "5337719"
  },
  {
    "text": "there techniques like at a at a higher level you know you might be running your inference workloads for this inference",
    "start": "5337719",
    "end": "5343880"
  },
  {
    "text": "service but you might also have other like lower priority training jobs you're running and other like just internal R&D",
    "start": "5343880",
    "end": "5350719"
  },
  {
    "text": "workloads you're running can you kind of Multiplex these different workloads on the same pool of GPU resources or do you",
    "start": "5350719",
    "end": "5357520"
  },
  {
    "text": "have like a dedicated pool of gpus for serving and a dedicated pool of gpus for you know training and so there are um",
    "start": "5357520",
    "end": "5365440"
  },
  {
    "text": "many different things you can do um you know some of which we're doing some of which we're going to do um but there's a",
    "start": "5365440",
    "end": "5372239"
  },
  {
    "text": "lot of you have to really think about it and I'm not even I didn't even mention all of the um just like model and you",
    "start": "5372239",
    "end": "5378920"
  },
  {
    "text": "know GPU level optimizations here uh there's a lot we do there as well but there's a ton of depth",
    "start": "5378920",
    "end": "5386400"
  },
  {
    "text": "here question um you guys talked about how you like",
    "start": "5388679",
    "end": "5394080"
  },
  {
    "text": "serve a lot of different open source models a lot of companies have been popping up that are like doing model routing and kind of optimizing for Speed",
    "start": "5394080",
    "end": "5401199"
  },
  {
    "text": "performance cost all that what are your thoughts on that would you get into that curious uh we don't have a model routing",
    "start": "5401199",
    "end": "5407239"
  },
  {
    "text": "product uh I think a model router is a good idea in general like it's a useful",
    "start": "5407239",
    "end": "5412360"
  },
  {
    "text": "thing and um you know and some of these like the Martian is one of these",
    "start": "5412360",
    "end": "5418040"
  },
  {
    "text": "companies uh and they have a leaderboard and you know um we're one of the providers on on their",
    "start": "5418040",
    "end": "5424639"
  },
  {
    "text": "leaderboard um and I think you know we are if you look at these different leaderboards we're pretty competitive in",
    "start": "5424639",
    "end": "5430760"
  },
  {
    "text": "uh in in certain Dimensions right um where let's yes what do I think about it",
    "start": "5430760",
    "end": "5437040"
  },
  {
    "text": "what do I think about I mean I guess a lot of them what they do is route to different providers",
    "start": "5437040",
    "end": "5442480"
  },
  {
    "text": "as well so like any scales an option opening eyes in there but would you do routing between models and stuff too or",
    "start": "5442480",
    "end": "5447719"
  },
  {
    "text": "is that like not really on the radar um I mean it's something think that's",
    "start": "5447719",
    "end": "5452760"
  },
  {
    "text": "useful for our customers right I think the",
    "start": "5452760",
    "end": "5457960"
  },
  {
    "text": "what you know at a high level um you want customers to be able to",
    "start": "5457960",
    "end": "5463880"
  },
  {
    "text": "get the first thing they care about is quality typically um I mentioned latency",
    "start": "5463880",
    "end": "5469480"
  },
  {
    "text": "and cost and with model routing you can often get um you can do much better than",
    "start": "5469480",
    "end": "5475320"
  },
  {
    "text": "the single best model right because if you can determine that open AI is like",
    "start": "5475320",
    "end": "5480480"
  },
  {
    "text": "really good at coding questions and anthropic is really good at um you know language translation and and you know",
    "start": "5480480",
    "end": "5487679"
  },
  {
    "text": "mixt is really good at something else um you can get higher quality than any",
    "start": "5487679",
    "end": "5493520"
  },
  {
    "text": "single model and you can you know get lower cost than any single model so you",
    "start": "5493520",
    "end": "5498639"
  },
  {
    "text": "it's it's uh I mean model routing is an incredibly good idea yeah I would say it's",
    "start": "5498639",
    "end": "5505600"
  },
  {
    "text": "also um it adds complexity right and it's the kind of thing that you don't",
    "start": "5505600",
    "end": "5510920"
  },
  {
    "text": "want every company to have to solve for themselves got it",
    "start": "5510920",
    "end": "5518159"
  },
  {
    "text": "thanks I have a question for Json uh you said like your company does the",
    "start": "5518440",
    "end": "5525679"
  },
  {
    "text": "observability so are there any feedbacks or even in observ the framework you have",
    "start": "5525679",
    "end": "5531960"
  },
  {
    "text": "customer adoption metrics um it could be applicable to you too uh in your customers do you also get",
    "start": "5531960",
    "end": "5539400"
  },
  {
    "text": "back on the application how much of it is adopted how much of it is liked and that kind of",
    "start": "5539400",
    "end": "5546040"
  },
  {
    "text": "a metric is that flowing through in your system it is I mean so so we we allow people to tag um tag information tag",
    "start": "5546040",
    "end": "5554119"
  },
  {
    "text": "spans or inferences as as they're collected and or augment with like uh",
    "start": "5554119",
    "end": "5559400"
  },
  {
    "text": "could be thumbs up or like like human feedback um post uh and and lots of",
    "start": "5559400",
    "end": "5565840"
  },
  {
    "text": "companies do like like human feedback on the experience and whether they like it and whether uh you know you know was a",
    "start": "5565840",
    "end": "5572119"
  },
  {
    "text": "it was a um a good good experience or not or good generation or not is is a a common feedback you just don't get that",
    "start": "5572119",
    "end": "5578199"
  },
  {
    "text": "much of it so um so you do it it's this great signal great a good signal",
    "start": "5578199",
    "end": "5585000"
  },
  {
    "text": "good good metric but you also if it's user supplied you there might not be that much of it so um a lot of times",
    "start": "5585000",
    "end": "5592320"
  },
  {
    "text": "we're using other metrics to to generate or value success or quality um but but",
    "start": "5592320",
    "end": "5599239"
  },
  {
    "text": "yeah so I would say we absolutely do customers use it all the time um and it's one layer of of one thing you look",
    "start": "5599239",
    "end": "5605040"
  },
  {
    "text": "at",
    "start": "5605040",
    "end": "5607960"
  },
  {
    "text": "typically thank you I have a question for both of you uh what's the next step for company and let's say aside from",
    "start": "5612400",
    "end": "5620000"
  },
  {
    "text": "Models um do you guys would stick to the same business models and um and that's",
    "start": "5620000",
    "end": "5625440"
  },
  {
    "text": "SW was a competitive strategy so what's the next step for for",
    "start": "5625440",
    "end": "5631400"
  },
  {
    "text": "us you want to go first not",
    "start": "5631400",
    "end": "5638719"
  },
  {
    "text": "really um the I would say the the last year has been",
    "start": "5639400",
    "end": "5645119"
  },
  {
    "text": "um an incredible incredible year of kind of of of new stuff coming to Market um",
    "start": "5645119",
    "end": "5651600"
  },
  {
    "text": "and from a product perspective I feel like we're we're just catching up so there's a lot that I want to that that's",
    "start": "5651600",
    "end": "5657600"
  },
  {
    "text": "in the works so that we're rolling out that I would say helps so so we you know observability and evaluation helps",
    "start": "5657600",
    "end": "5663840"
  },
  {
    "text": "customers build um helps that quality of what you build turn from that nice Twitter demo that works great in Twitter",
    "start": "5663840",
    "end": "5670560"
  },
  {
    "text": "but but you know get to production actually works is is the hard part um and some of that stuff is is managing",
    "start": "5670560",
    "end": "5677920"
  },
  {
    "text": "more complex interactions so um you know we talked briefly about function calling",
    "start": "5677920",
    "end": "5684199"
  },
  {
    "text": "but but as people try to do put more complexity function call is one of those things you can do to uh to to make your",
    "start": "5684199",
    "end": "5690800"
  },
  {
    "text": "app or experience do more um the other one is retrieval and making retrieval solid and good even that you know it",
    "start": "5690800",
    "end": "5697000"
  },
  {
    "text": "feels given the number of um Tech talks on retrieval you think it's a solved",
    "start": "5697000",
    "end": "5702400"
  },
  {
    "text": "problem it's not uh so I think there's a I think there's a bunch of area there's two main areas there which is building",
    "start": "5702400",
    "end": "5708440"
  },
  {
    "text": "better tools for people to evaluate um and manage their their llm apps and",
    "start": "5708440",
    "end": "5716000"
  },
  {
    "text": "debug them in in in ways and so you're going to see an immense amount of product come out around on that this",
    "start": "5716000",
    "end": "5721960"
  },
  {
    "text": "year like stuff you haven't seen and stuff that's quite unique um so that that's we're focused on tools that help",
    "start": "5721960",
    "end": "5728480"
  },
  {
    "text": "you build these L maps and and build them well yeah for us we're trying to",
    "start": "5728480",
    "end": "5734600"
  },
  {
    "text": "make it easier for businesses to use Ai and we want companies to be able to",
    "start": "5734600",
    "end": "5740360"
  },
  {
    "text": "build AI applications and you know get AI in production without having to think about the underlying infrastructure",
    "start": "5740360",
    "end": "5747280"
  },
  {
    "text": "distributed challenges distributed systems challenges right we don't want we want you be able to get all the benefits of say training on a bunch of",
    "start": "5747280",
    "end": "5754040"
  },
  {
    "text": "gpus or deploying models deploying LMS and so on uh we don't want you to have to think about these kinds of like",
    "start": "5754040",
    "end": "5761119"
  },
  {
    "text": "scaling or fault tolerance or GPU availability or how to use spot instances or you know how to make things",
    "start": "5761119",
    "end": "5768719"
  },
  {
    "text": "cost efficient how to make things reliable right so our goal is to really take all of that work off the table and",
    "start": "5768719",
    "end": "5775040"
  },
  {
    "text": "to do that there are two main Dimensions we try to go deep on one is just cost",
    "start": "5775040",
    "end": "5780960"
  },
  {
    "text": "and per performance like really performance optimizations across the board across the full stack um and then",
    "start": "5780960",
    "end": "5786960"
  },
  {
    "text": "the second dimension is just the developer experience trying to make it a frictionless experience because distributed systems you know it's",
    "start": "5786960",
    "end": "5793600"
  },
  {
    "text": "complicated AI is complicated it's a lot for people to learn you could imagine a lot of knobs for people to tune and",
    "start": "5793600",
    "end": "5799760"
  },
  {
    "text": "stuff like that uh and that's just going to make people's lives harder so we really try to uh simplify things thank",
    "start": "5799760",
    "end": "5806920"
  },
  {
    "text": "you I appreciate the user user experience",
    "start": "5806920",
    "end": "5812159"
  },
  {
    "text": "and that's the cost is like from all sorts of CLS and as",
    "start": "5812159",
    "end": "5819840"
  },
  {
    "text": "well",
    "start": "5820280",
    "end": "5823280"
  },
  {
    "text": "right cool um thank you I should ask a simpler",
    "start": "5825600",
    "end": "5832600"
  },
  {
    "text": "question uh when you P the audience about how many developers EV the models we see about like like a third or half",
    "start": "5832600",
    "end": "5839960"
  },
  {
    "text": "of the audience raise their hand what do you think like is the barrier that prevents a lot of developers from",
    "start": "5839960",
    "end": "5845719"
  },
  {
    "text": "evaluating their models and what are some Cutting Edge techniques you've seen uh effective in the eval space okay oh",
    "start": "5845719",
    "end": "5853639"
  },
  {
    "text": "wow um so so um I think I think",
    "start": "5853639",
    "end": "5859480"
  },
  {
    "text": "depending upon the team I see uh you know some team some teams know the",
    "start": "5859480",
    "end": "5866320"
  },
  {
    "text": "product they're building and they're building it and they know what their what success looks like from from an and",
    "start": "5866320",
    "end": "5871560"
  },
  {
    "text": "and EV evaluations are really trying to evaluate the quality of of the",
    "start": "5871560",
    "end": "5876639"
  },
  {
    "text": "performance of that of your your your system and so um the the goal is to the",
    "start": "5876639",
    "end": "5883360"
  },
  {
    "text": "goal is to build evals relative to your task I think I and I think if you're if",
    "start": "5883360",
    "end": "5889520"
  },
  {
    "text": "the teams that have kind of started an open AI probably have products out already they just seem to be faster than the teams starting with open source it",
    "start": "5889520",
    "end": "5896639"
  },
  {
    "text": "just takes a long time to get all the components together um so I think depending upon where you are in the",
    "start": "5896639",
    "end": "5902080"
  },
  {
    "text": "journey evals make sense um if you're still in the early stage you don't even have a working system um you're kind of",
    "start": "5902080",
    "end": "5909040"
  },
  {
    "text": "before you know maybe just get your your first output going maybe just get your your first test cases manually going and",
    "start": "5909040",
    "end": "5915880"
  },
  {
    "text": "then I think it's it's then about repeatability it's then about like okay this works good enough to demo to my my",
    "start": "5915880",
    "end": "5922199"
  },
  {
    "text": "crew like how do I make sure it's going to work on a broader set of cases um so",
    "start": "5922199",
    "end": "5927400"
  },
  {
    "text": "I think it depends on the task um and then yeah if you're in retrieval what I would say is most teams if you're trying",
    "start": "5927400",
    "end": "5932920"
  },
  {
    "text": "to do rag come up with a set of questions come up with a set of ground truth you only need about a 100 to make",
    "start": "5932920",
    "end": "5938159"
  },
  {
    "text": "the thing The Benchmark it like it doesn't take that long it takes you three days to go do that and a lot of teams don't so my recommendation is",
    "start": "5938159",
    "end": "5945560"
  },
  {
    "text": "build a little test data first and then you know Phoenix which is our open source Library you know if you have a",
    "start": "5945560",
    "end": "5951280"
  },
  {
    "text": "question you have an answer there's a ton of evals to running a retrieval that will pinpoint your problems um you mind",
    "start": "5951280",
    "end": "5957960"
  },
  {
    "text": "if I share one slide yeah go ahead actually um you mind putting on the SL so this is um just on this topic of rag",
    "start": "5957960",
    "end": "5967159"
  },
  {
    "text": "right um wanted to mention one like upcoming event that we have so we are",
    "start": "5967159",
    "end": "5973040"
  },
  {
    "text": "hosting a boot camp two-day boot camp on building rag applications so this is",
    "start": "5973040",
    "end": "5979239"
  },
  {
    "text": "like targeted at developers it's Hands-On we're running it's essentially",
    "start": "5979239",
    "end": "5985080"
  },
  {
    "text": "like Hands-On training for uh a lot of best practices and advanced Topics in",
    "start": "5985080",
    "end": "5990719"
  },
  {
    "text": "building rag applications um this is happening",
    "start": "5990719",
    "end": "5996040"
  },
  {
    "text": "in yeah so this is happening in in um this is uh just plugging an event that",
    "start": "5996320",
    "end": "6002320"
  },
  {
    "text": "we are doing uh in March but this is uh you can imagine Advanced topics for",
    "start": "6002320",
    "end": "6008400"
  },
  {
    "text": "retrieval or how to handle multimodal data or you know how to improve quality",
    "start": "6008400",
    "end": "6014320"
  },
  {
    "text": "for your embeddings right and or how to think about eval there's a lot of uh",
    "start": "6014320",
    "end": "6020440"
  },
  {
    "text": "subtleties there and this is an in-person event San Francisco that we're running in uh in early March so you can",
    "start": "6020440",
    "end": "6027119"
  },
  {
    "text": "check that out and um we we can send out more info about that later but just wanted to uh to to uh mention that it's",
    "start": "6027119",
    "end": "6036040"
  },
  {
    "text": "also super relevant for if you're interested in Long context okay great",
    "start": "6036040",
    "end": "6041119"
  },
  {
    "text": "okay that that's all I want to share about that great um well well thank you",
    "start": "6041119",
    "end": "6046560"
  },
  {
    "text": "y'all we we have one more so thanks Robert for doing it um",
    "start": "6046560",
    "end": "6052239"
  },
  {
    "text": "um so so you saw the all the green on anthropic what we have now is the actual",
    "start": "6052239",
    "end": "6058320"
  },
  {
    "text": "results so we got we're gonna do a a real good dive into um uh we're gonna do",
    "start": "6058320",
    "end": "6065280"
  },
  {
    "text": "like a a 15 to 20 minute dive into evals which kind of brings takes Greg's original work and shows you kind of like",
    "start": "6065280",
    "end": "6073280"
  },
  {
    "text": "a rigorous next step in a large comparison with mixture models with",
    "start": "6073280",
    "end": "6078520"
  },
  {
    "text": "truly anthropic test so give us five to set that up and we'll go through it thanks so much thanks",
    "start": "6078520",
    "end": "6085280"
  },
  {
    "text": "[Applause] Robert yeah",
    "start": "6085280",
    "end": "6092280"
  },
  {
    "text": "thanks sorry good job yeah thank you all right let's do one more before I lose",
    "start": "6092760",
    "end": "6102040"
  },
  {
    "text": "everyone",
    "start": "6102040",
    "end": "6105040"
  },
  {
    "text": "yeah",
    "start": "6109840",
    "end": "6112840"
  },
  {
    "text": "how's it going yeah yeah give give me one minute I just wanted to set",
    "start": "6115239",
    "end": "6120840"
  },
  {
    "text": "up yeah yeah",
    "start": "6120840",
    "end": "6125760"
  },
  {
    "text": "thanks",
    "start": "6139760",
    "end": "6142760"
  },
  {
    "text": "all right yeah okay yeah there's another 15",
    "start": "6159040",
    "end": "6164679"
  },
  {
    "text": "minutes",
    "start": "6164679",
    "end": "6167520"
  },
  {
    "text": "yeah people I question yeah we're going to",
    "start": "6169760",
    "end": "6177760"
  },
  {
    "text": "finish up I still want to go through this yeah so hello we have one more one last",
    "start": "6177760",
    "end": "6186599"
  },
  {
    "text": "presentation hey how's it going so we're we're g to do one more",
    "start": "6186599",
    "end": "6193719"
  },
  {
    "text": "presentation for one last",
    "start": "6193719",
    "end": "6197960"
  },
  {
    "text": "thing",
    "start": "6199679",
    "end": "6202679"
  },
  {
    "text": "all right is there a is there a mic I can hold yeah just use this okay",
    "start": "6225800",
    "end": "6232679"
  },
  {
    "text": "yeah it's a little better just all right we're going to do this one quick if you",
    "start": "6234199",
    "end": "6240119"
  },
  {
    "text": "all can grab a seat we're going to finish uh five more minutes I promise I'll be fast through",
    "start": "6240119",
    "end": "6248080"
  },
  {
    "text": "this all right one more pass",
    "start": "6249000",
    "end": "6257280"
  },
  {
    "text": "through all right everyone all right so",
    "start": "6257679",
    "end": "6264800"
  },
  {
    "text": "uh so everything we've done today and what we've gone through is actually",
    "start": "6264800",
    "end": "6270280"
  },
  {
    "text": "pretty light this is going to be this is like the Deep technical part of the talk",
    "start": "6270280",
    "end": "6275960"
  },
  {
    "text": "so uh I promise I'll go fast through this so um but",
    "start": "6275960",
    "end": "6281560"
  },
  {
    "text": "but I'll um I'll just hop in here",
    "start": "6281560",
    "end": "6286440"
  },
  {
    "text": "so yeah hey can we get everyone back in in their seats and then get your food",
    "start": "6286639",
    "end": "6294400"
  },
  {
    "text": "and then could we get get a little quiet thanks so um so first off we we put this out",
    "start": "6294400",
    "end": "6302239"
  },
  {
    "text": "there so so a part of my team from Uber ex Uber put this out called Model evals versus taas task evals um and and",
    "start": "6302239",
    "end": "6311119"
  },
  {
    "text": "there're they're quite different does does anyone know what a model eval versus a task eval is any I think",
    "start": "6311119",
    "end": "6316800"
  },
  {
    "text": "there's one person in the room who knows this VI and this is the guy who knows what he doing too um so so they're quite",
    "start": "6316800",
    "end": "6323719"
  },
  {
    "text": "different and the thing you're you're actually going to use if if you use llms in production you actually get there you build your app you're actually there you",
    "start": "6323719",
    "end": "6330639"
  },
  {
    "text": "don't really care about model evals to be honest so let let like all the stuff we're talk going to tell you about here",
    "start": "6330639",
    "end": "6336880"
  },
  {
    "text": "um probably doesn't matter that much it's not that useful what you're going to use in your job or task evals first",
    "start": "6336880",
    "end": "6342280"
  },
  {
    "text": "off what's a model eval well model Val we were talking about earlier is M mlu",
    "start": "6342280",
    "end": "6347639"
  },
  {
    "text": "um it's a mixture of a lot of subjects it's like all of you remember back to SATs and college tests they're like",
    "start": "6347639",
    "end": "6355199"
  },
  {
    "text": "college test questions um but there's professional accounting there's there's some pretty complex subjects which the",
    "start": "6355199",
    "end": "6361960"
  },
  {
    "text": "models got to get right and then notice there's ground truth so a model eval has",
    "start": "6361960",
    "end": "6369280"
  },
  {
    "text": "ground truth in a question what it's good for is um is a",
    "start": "6369280",
    "end": "6376000"
  },
  {
    "text": "leaderboard so so you get you do well in your questions you're great",
    "start": "6376000",
    "end": "6381239"
  },
  {
    "text": "you get to the top spot you get to the top spot you get VC funding",
    "start": "6381239",
    "end": "6387599"
  },
  {
    "text": "so and and a lot of it sometimes so so this is highly sought-after which also",
    "start": "6387599",
    "end": "6393560"
  },
  {
    "text": "has the wrong incentives right like we talked about you know uh training to the",
    "start": "6393560",
    "end": "6398679"
  },
  {
    "text": "test well there's a huge incentive to train to the test because you make it to the top this leaderboard you're G to make a lot of money so um so those that",
    "start": "6398679",
    "end": "6406440"
  },
  {
    "text": "those are model evals there's a question a grade um and you can test your",
    "start": "6406440",
    "end": "6413360"
  },
  {
    "text": "model aask eval the other thing I want to show you is notice there's no promp",
    "start": "6413360",
    "end": "6418560"
  },
  {
    "text": "template meaning there's no variables promp template variables here a task",
    "start": "6418560",
    "end": "6425040"
  },
  {
    "text": "eval there's there's I don't know if you can see that there's a PRP template with variables and you're applying that",
    "start": "6425040",
    "end": "6431040"
  },
  {
    "text": "template throughout your data one eal might be is there is there user frustration in these conversations did",
    "start": "6431040",
    "end": "6437679"
  },
  {
    "text": "the assistant hallucinate which is one of the other FES um so so you're applying the same template to data and",
    "start": "6437679",
    "end": "6444760"
  },
  {
    "text": "this is what every one of our customers uses in business uh tasks it's not the model eval it's a template applied to",
    "start": "6444760",
    "end": "6452080"
  },
  {
    "text": "their production data so and notice also there's no there's no ground there's no",
    "start": "6452080",
    "end": "6458280"
  },
  {
    "text": "ground truth you're just applying it and it's telling you it is the ground truth",
    "start": "6458280",
    "end": "6463400"
  },
  {
    "text": "it's it's a proxy for success and I just want to give you an example what one of these task EV Bells looks like this is",
    "start": "6463400",
    "end": "6469599"
  },
  {
    "text": "from Phoenix Open open source Library completely open source and it's a hallucination template um we can't we",
    "start": "6469599",
    "end": "6477960"
  },
  {
    "text": "can't evaluate hallucination on public facts like is a model going to make up a fact on say Michael Jordan's birthday we",
    "start": "6477960",
    "end": "6483880"
  },
  {
    "text": "don't know but when you're doing retrieval you've you've got your your content your your reference data you've",
    "start": "6483880",
    "end": "6490119"
  },
  {
    "text": "got the the generated answer you can tell whether the the model has made up the answer from that data so",
    "start": "6490119",
    "end": "6497040"
  },
  {
    "text": "hallucination detection is possible on rag applications it's not possible in",
    "start": "6497040",
    "end": "6503239"
  },
  {
    "text": "general um so these evals are on a specific task and and what you on on the",
    "start": "6503239",
    "end": "6509199"
  },
  {
    "text": "right side you've got like what do tasky BS look like in Phoenix which is our open source product I'm just going to",
    "start": "6509199",
    "end": "6514760"
  },
  {
    "text": "show you like a little bit of just make this a",
    "start": "6514760",
    "end": "6520520"
  },
  {
    "text": "um just make this more like like one little product view just so you can actually see what this looks like so so",
    "start": "6520520",
    "end": "6529199"
  },
  {
    "text": "sometimes you what you can see here is there's some eval so these are this this is a r rag application um track I think",
    "start": "6529199",
    "end": "6537280"
  },
  {
    "text": "I was using llama Index this is Phoenix which is observability showing you like the call trace of what your query hit",
    "start": "6537280",
    "end": "6544560"
  },
  {
    "text": "where it did the retrieval um and there's also evales associated with this",
    "start": "6544560",
    "end": "6550080"
  },
  {
    "text": "um so it shows kind of like the trunks chunks available and just want to show you what a task Val looks like before",
    "start": "6550080",
    "end": "6555159"
  },
  {
    "text": "hopping in any deeper a task eval is just saying you know was it incorrect or correct from a Q&A perspective and then",
    "start": "6555159",
    "end": "6561440"
  },
  {
    "text": "the other thing we have in Phoenix we use a lot is explanations if you're going to use task key vs use explanations it's like my one",
    "start": "6561440",
    "end": "6568800"
  },
  {
    "text": "little hint like the industry hasn't figured it out everyone every one of our customers is using this it's a blueprint",
    "start": "6568800",
    "end": "6574040"
  },
  {
    "text": "for what's wrong in your app and it's a little flag in Phoenix that just says why it's wrong so in this case it",
    "start": "6574040",
    "end": "6580199"
  },
  {
    "text": "hallucinated because the query ask for a question and and basically it's not in the text um so a little bit of like okay",
    "start": "6580199",
    "end": "6587679"
  },
  {
    "text": "what's a task eval what's a model eval um let me go back because we're going to talk about model evals",
    "start": "6587679",
    "end": "6597520"
  },
  {
    "text": "um so so model EV vals task EV vals what we've been talking about with",
    "start": "6597520",
    "end": "6604320"
  },
  {
    "text": "hyack it's a model eval it's looking at gbd4 versus anthropic it's looking at anthropic versus mixol which I'll show",
    "start": "6604320",
    "end": "6611040"
  },
  {
    "text": "you in a sec so you're you're trying to decide what model maybe to use in your business or what model works you know",
    "start": "6611040",
    "end": "6616560"
  },
  {
    "text": "what's the best model out there what you know what's the best model on lead board um but they're they're great kind of",
    "start": "6616560",
    "end": "6622320"
  },
  {
    "text": "little they're great for understanding what what's going on under the hood um and and as we talked about the retrieval",
    "start": "6622320",
    "end": "6628280"
  },
  {
    "text": "stuff before the Greg's amazing work to get this to work in the first place and get get it out there his his first",
    "start": "6628280",
    "end": "6635080"
  },
  {
    "text": "question was a question about San Francisco you know what's the best place to what's the best thing to do in San Francisco and have a sandwich in Dolores",
    "start": "6635080",
    "end": "6641719"
  },
  {
    "text": "Park which um which which is great uh but the problem is that question could be cashed so our first concern when we",
    "start": "6641719",
    "end": "6648760"
  },
  {
    "text": "saw the question and the test was he was was the same question was being asked every time so a lot of caching occurs in",
    "start": "6648760",
    "end": "6656560"
  },
  {
    "text": "retrieval in all these systems in open AI so what we decided to do was add a random number so to really test whether",
    "start": "6656560",
    "end": "6663520"
  },
  {
    "text": "it could retrieve a fact in the context we modified the test to make a random",
    "start": "6663520",
    "end": "6669239"
  },
  {
    "text": "number we also put it into the Phoenix Library for evals which is like a 100 times faster than almost anything else",
    "start": "6669239",
    "end": "6675199"
  },
  {
    "text": "off on Market um and we also changed the city so we're not just doing San Francisco now uh that City actually",
    "start": "6675199",
    "end": "6680679"
  },
  {
    "text": "randomly changes so the question changes a little bit um and so the goal is you do you put it the top of the context the",
    "start": "6680679",
    "end": "6687560"
  },
  {
    "text": "bottom of the context also grow you also grow the context so that's kind of what",
    "start": "6687560",
    "end": "6692679"
  },
  {
    "text": "we're doing that that that's what the Hast ACT test is and we're trying to understand um again the red dots are",
    "start": "6692679",
    "end": "6699440"
  },
  {
    "text": "where it doesn't get that fact correct um and this is claw versus gp4 so this",
    "start": "6699440",
    "end": "6705960"
  },
  {
    "text": "is a repeat of Greg's experiment with more rigor um a lot you know more re",
    "start": "6705960",
    "end": "6711280"
  },
  {
    "text": "rigor and kind of an exact test um also found some more problems with anthropic",
    "start": "6711280",
    "end": "6717079"
  },
  {
    "text": "also got an email from anthropic um and uh and you know anthropic a solid model",
    "start": "6717079",
    "end": "6725000"
  },
  {
    "text": "relative to a lot of stuff we see so I don't want to be super negative on it but they gave us some guidance the same",
    "start": "6725000",
    "end": "6730639"
  },
  {
    "text": "guidance they gave uh Greg which was add this you know add this little thing into",
    "start": "6730639",
    "end": "6735800"
  },
  {
    "text": "the top to say like where um where the RO vincence and the context and this is",
    "start": "6735800",
    "end": "6741800"
  },
  {
    "text": "an example of what what Claude gives you so Claude is now pulling out the sentence first and then it's generating",
    "start": "6741800",
    "end": "6750239"
  },
  {
    "text": "the number from what it pulled out so so it's kind of a hack a little bit of a hack to like get it to retrieve this",
    "start": "6750239",
    "end": "6756119"
  },
  {
    "text": "this sentence I'm not I mean it's it's promting um and then get to get to that",
    "start": "6756119",
    "end": "6761840"
  },
  {
    "text": "that number uh and it does work you know this this was kind of prompting guidance",
    "start": "6761840",
    "end": "6768119"
  },
  {
    "text": "on the right it's better it's still not as good as gp4 but it's it's better and then this is this is the original",
    "start": "6768119",
    "end": "6774920"
  },
  {
    "text": "anthropic um and I also want to do mistol versus",
    "start": "6774920",
    "end": "6780400"
  },
  {
    "text": "mixol so there's we talked about open source and open source models damn that's that's that was good",
    "start": "6780400",
    "end": "6787400"
  },
  {
    "text": "we we when we saw it mixol was impressive uh and this is the 8 by sbat",
    "start": "6787400",
    "end": "6792440"
  },
  {
    "text": "versus that just the normal 7B that's kind of what you can run on your machine this is the one we were talking about takes a little bit more than that um but",
    "start": "6792440",
    "end": "6800560"
  },
  {
    "text": "but it's impressed Us in every way for for an open source model so so one more example here",
    "start": "6800560",
    "end": "6807280"
  },
  {
    "text": "um and then we decided we just put out something this week um to get more",
    "start": "6807280",
    "end": "6813199"
  },
  {
    "text": "complicated so the G and rag is Generation a lot of times I'm sticking a bunch of chunks and then I'm asking to",
    "start": "6813199",
    "end": "6819920"
  },
  {
    "text": "summarize or what are the four things I do or what are the in this case I want Revenue numbers for an SEC filing and I",
    "start": "6819920",
    "end": "6826400"
  },
  {
    "text": "want to know the percent increase from reeven from year 1998 to 90 you know 97",
    "start": "6826400",
    "end": "6831480"
  },
  {
    "text": "to 98 and so there's a generation task you're asking to do something cognitively at the end of finding that",
    "start": "6831480",
    "end": "6838119"
  },
  {
    "text": "fact so we so we're adding a little bit more to it um in this case you know this",
    "start": "6838119",
    "end": "6843840"
  },
  {
    "text": "example here it's the generations requiring it to number format to round",
    "start": "6843840",
    "end": "6849040"
  },
  {
    "text": "and do division and by the way the the the arithmetic makes a big difference so",
    "start": "6849040",
    "end": "6854880"
  },
  {
    "text": "when it's strings concatenations even summarizations very very you know",
    "start": "6854880",
    "end": "6860040"
  },
  {
    "text": "decently easy decently good results the arithmetic at the end actually was interesting enough um caused a lot of",
    "start": "6860040",
    "end": "6866599"
  },
  {
    "text": "the problems um so the different types of generations we tested so not I'm now",
    "start": "6866599",
    "end": "6872239"
  },
  {
    "text": "not only doing retrieval I'm not only trying to find a fact but I'm doing something with it as part of my",
    "start": "6872239",
    "end": "6877880"
  },
  {
    "text": "generation phase um we tested a mapping we tested a module a number which some",
    "start": "6877880",
    "end": "6885440"
  },
  {
    "text": "of the results are are amazing I don't have them here um which I some that that is itself a whole presentation",
    "start": "6885440",
    "end": "6892079"
  },
  {
    "text": "and fascinating story um and then there's some other stuff string concatenation and some arithmetic",
    "start": "6892079",
    "end": "6898719"
  },
  {
    "text": "um and our results were like shocking like it I have",
    "start": "6898719",
    "end": "6905840"
  },
  {
    "text": "R probably thousand plus ay stacks and tons of evaluations and like I've never seen gp4 worse than anthropic so we saw",
    "start": "6905840",
    "end": "6913079"
  },
  {
    "text": "that on like any t almost any test so we ran this for like what the hell's going on what is what is up",
    "start": "6913079",
    "end": "6920199"
  },
  {
    "text": "and um and it kind of stumped us and and so for us when we looked at the output we",
    "start": "6920199",
    "end": "6927239"
  },
  {
    "text": "noticed anthropic was like like the different like this is GPT for's response this is a formatting of a of a",
    "start": "6927239",
    "end": "6933920"
  },
  {
    "text": "of a date and a number which is what the ask is and by the way this is the same answer for anthropic but this is like",
    "start": "6933920",
    "end": "6941920"
  },
  {
    "text": "you know we joke around it's like you're talkative Uncle it's it just explains everything always no",
    "start": "6941920",
    "end": "6948199"
  },
  {
    "text": "matter how much you put like me just one answer it's just you got sentences and sentences and we noticed like it kind of",
    "start": "6948199",
    "end": "6953960"
  },
  {
    "text": "explained it and it would get it get it right more than gbd4 um so this was like a little thing we we decided to do is",
    "start": "6953960",
    "end": "6960760"
  },
  {
    "text": "like why don't we just add one line to gbd4 and just say please explain yourself and answer the question and",
    "start": "6960760",
    "end": "6967920"
  },
  {
    "text": "then answer the question and it's like perfect um which is kind of mind-blowing it's also not an like please explain",
    "start": "6967920",
    "end": "6974520"
  },
  {
    "text": "yourself and then anwers the questions like a not a natural prompt iteration you do for rag like like it's not",
    "start": "6974520",
    "end": "6980360"
  },
  {
    "text": "something I would think like to put there um but uh but but it it to be",
    "start": "6980360",
    "end": "6988199"
  },
  {
    "text": "honest that helped it work through the math so the so so for arithmetic",
    "start": "6988199",
    "end": "6993280"
  },
  {
    "text": "specifically Generations or arithmetic work putting that in made a big difference the string concatenation",
    "start": "6993280",
    "end": "6999320"
  },
  {
    "text": "didn't really matter string is great at summarization was pretty good at so so there's something about like this I I'm",
    "start": "6999320",
    "end": "7005639"
  },
  {
    "text": "slightly suspicious to that like I don't know what these clothes vendors do like do they have like a math co-processor do",
    "start": "7005639",
    "end": "7011760"
  },
  {
    "text": "they like is there any like I'm a little suspicious that there there may be some something on the side doing math work um",
    "start": "7011760",
    "end": "7018320"
  },
  {
    "text": "and as it generates it it might might be checking his answers but um that's a theory I have no nothing no substance on",
    "start": "7018320",
    "end": "7024560"
  },
  {
    "text": "it um the modulos we did do some modulo",
    "start": "7024560",
    "end": "7030199"
  },
  {
    "text": "tests which were quite hard and I'm quite surprised how well it did but long story short um you know gener these are",
    "start": "7030199",
    "end": "7039960"
  },
  {
    "text": "still big differences in generation the last thing I would I would note in just this view is anthropics on the left here",
    "start": "7039960",
    "end": "7047119"
  },
  {
    "text": "it's pretty good below like 37 so pretty you that's a pretty big You Know M the",
    "start": "7047119",
    "end": "7052719"
  },
  {
    "text": "biggest mix model is still not even 37k right now I think Contex window size I think you got 32k in their their main",
    "start": "7052719",
    "end": "7059639"
  },
  {
    "text": "API so um you know you know anthropic does pretty good on you know decently",
    "start": "7059639",
    "end": "7066520"
  },
  {
    "text": "size context um and then this is this gives you an idea of what mixol looks like and then by the way that little",
    "start": "7066520",
    "end": "7073079"
  },
  {
    "text": "trick of of please explain yourself also worked on mixol to improve the results a little bit um so uh so gives you an idea",
    "start": "7073079",
    "end": "7081599"
  },
  {
    "text": "of like Rag and just think of you know all these people telling you talking about Rag and what to do with rag Ian",
    "start": "7081599",
    "end": "7088520"
  },
  {
    "text": "you are gon like these red dots mean like mistakes in the generation like you know you have this rag app these are",
    "start": "7088520",
    "end": "7095280"
  },
  {
    "text": "your mistakes um these are customer issues so models do matter like like I",
    "start": "7095280",
    "end": "7101800"
  },
  {
    "text": "say model evals don't matter well they kind of you know you should Pro you should probably do hopefully you're",
    "start": "7101800",
    "end": "7106880"
  },
  {
    "text": "doing some of this anyway um but but this hopefully you do use some of these to actually make some decision or",
    "start": "7106880",
    "end": "7112840"
  },
  {
    "text": "hopefully you have access to these bigger better models as if you're doing rag apps they're it definitely makes",
    "start": "7112840",
    "end": "7119079"
  },
  {
    "text": "definitely makes a difference um I think that's kind of it um yeah so",
    "start": "7119079",
    "end": "7125320"
  },
  {
    "text": "hopefully that that that's thank you all for for joining in and um this is quite a bit of you know research effort we we",
    "start": "7125320",
    "end": "7132159"
  },
  {
    "text": "have going on and we'll have more just follow us on on Twitter or check out Phoenix thank",
    "start": "7132159",
    "end": "7138940"
  },
  {
    "text": "[Applause]",
    "start": "7138940",
    "end": "7142600"
  },
  {
    "text": "you",
    "start": "7158239",
    "end": "7161239"
  }
]