[
  {
    "text": "all right yeah so pyat and I will be um diving deep into some of the benchmarking and scalability testing we've been doing with rayon kubernetes",
    "start": "2800",
    "end": "10080"
  },
  {
    "text": "uh in a single GK cluster with uh 10,000 notes uh so quick intro I'm Andrew Sim",
    "start": "10080",
    "end": "16160"
  },
  {
    "text": "I'm a software engineer at Google working on gke I helped maintain CU and have been working on kubernetes since uh",
    "start": "16160",
    "end": "24560"
  },
  {
    "text": "2017 hi uh I'm py I work in the gka data team uh dealing with uh data loading",
    "start": "25240",
    "end": "32880"
  },
  {
    "text": "problems at Large Scale yeah yeah so uh first uh kind of a quick",
    "start": "32880",
    "end": "40719"
  },
  {
    "text": "intro to GK for folks that may not know uh GK is uh Google's managed kubernetes",
    "start": "40719",
    "end": "46840"
  },
  {
    "text": "Service uh one of the most uh compelling reasons why customers choose to run their workloads uh their AI workloads uh",
    "start": "46840",
    "end": "54440"
  },
  {
    "text": "on gke is that it provides a unified platform for all their UI uh all their",
    "start": "54440",
    "end": "59559"
  },
  {
    "text": "AI use cases from hosting uh interactive notebooks to training and fine tuning",
    "start": "59559",
    "end": "65239"
  },
  {
    "text": "models and uh to large scale inference with Ray and",
    "start": "65239",
    "end": "70920"
  },
  {
    "text": "VM uh gk's strength as a platform is that it's very flexible portable and",
    "start": "71000",
    "end": "77720"
  },
  {
    "text": "scalable um since the earliest days of kubernetes Google has been leading various initiatives to push the",
    "start": "77720",
    "end": "83680"
  },
  {
    "text": "boundaries of kubernetes within Google's compute infrastructure and the demand and growth",
    "start": "83680",
    "end": "88799"
  },
  {
    "text": "of AI has really put gk's scalability to the test for example uh training the",
    "start": "88799",
    "end": "95880"
  },
  {
    "text": "largest in state-of-the-art models requires massive computing power distributed across large clusters that",
    "start": "95880",
    "end": "102119"
  },
  {
    "text": "contain tens of thousands of Hardware accelerators uh GK provides the orchestration layer to seamlessly manage",
    "start": "102119",
    "end": "109000"
  },
  {
    "text": "compute with support up to 15,000 nodes in a single cluster to train the largest",
    "start": "109000",
    "end": "114200"
  },
  {
    "text": "models uh GK also supports more than 50,000 TPU chips training a single",
    "start": "114200",
    "end": "120159"
  },
  {
    "text": "machine learning model which we believe has been used to run the world's largest distributed training job for llms across",
    "start": "120159",
    "end": "127599"
  },
  {
    "text": "50,000 TPU V5 chips and in the last year alone uh gke",
    "start": "127599",
    "end": "134800"
  },
  {
    "text": "has seen explosive growth of AI with usage of gpus and tpus growing more than",
    "start": "134800",
    "end": "140560"
  },
  {
    "text": "900% in a year and it's no surprise uh at Ray",
    "start": "140560",
    "end": "146080"
  },
  {
    "text": "Summit that really popular option for large-scale distributed Compu Computing uh on gke is with Ray so Ray and gke",
    "start": "146080",
    "end": "154239"
  },
  {
    "text": "they pair really well together because Ray provides uh the libraries and abstractions for massive parallel",
    "start": "154239",
    "end": "160239"
  },
  {
    "text": "Computing and GK provides that underlying scalable orchestration and infrastructure for for",
    "start": "160239",
    "end": "168200"
  },
  {
    "text": "computing um the most common way to deploy rayon GK today is with CU um CU",
    "start": "168200",
    "end": "174080"
  },
  {
    "text": "provides declarative apis to build machine learning platforms on top of Ray and kubernetes it takes all advantage of",
    "start": "174080",
    "end": "180879"
  },
  {
    "text": "all the great scalable things we love about kubernetes like the controller P controller pattern and it provides",
    "start": "180879",
    "end": "187080"
  },
  {
    "text": "highly available and scalable deployments of Ray clusters out of the box it also provides a great amount of",
    "start": "187080",
    "end": "193319"
  },
  {
    "text": "flexibility and customization which is really important in this rapidly changing um AI",
    "start": "193319",
    "end": "200879"
  },
  {
    "text": "landscape um so yeah we're seeing significant growth of Ray and CU on gke",
    "start": "200879",
    "end": "207239"
  },
  {
    "text": "uh to support this uh growth and ensure the Optimal Performance of Ray on kubernetes we start to explore the",
    "start": "207239",
    "end": "213799"
  },
  {
    "text": "scalability limits of of Ray on gke our focus is on identifying and removing",
    "start": "213799",
    "end": "219159"
  },
  {
    "text": "bottlenecks and ensuring that Ray can fully take advantage of the scalability of of kubernetes for highly distributive",
    "start": "219159",
    "end": "226959"
  },
  {
    "text": "Computing uh in our tests we focused on uh basically two use cases one with",
    "start": "226959",
    "end": "233519"
  },
  {
    "text": "Ray data one with r ray train since these are the most um common libraries",
    "start": "233519",
    "end": "238920"
  },
  {
    "text": "enabling High highly parallel task highly parallel task execution with Ray and they can also be used for CPU",
    "start": "238920",
    "end": "245720"
  },
  {
    "text": "intensive tasks which is important for us because it would be really challenging for us to get quota approval",
    "start": "245720",
    "end": "251280"
  },
  {
    "text": "for 10,000 GPU notes be really expensive um in our testing we identified three",
    "start": "251280",
    "end": "256799"
  },
  {
    "text": "dimensions of scalability that were kind of most important for Ray first is simply the underlying compute",
    "start": "256799",
    "end": "262560"
  },
  {
    "text": "infrastructure and orchestration of that infrastructure this is where gk's ability to scale to 15,000 nodes and the",
    "start": "262560",
    "end": "269199"
  },
  {
    "text": "reliability of the underlying compute through Google compute engine is key uh next is uh efficient capacity",
    "start": "269199",
    "end": "276400"
  },
  {
    "text": "management of Ray at Large Scale uh this is where we integrate with q a very popular uh kubernetes Native Job King",
    "start": "276400",
    "end": "283520"
  },
  {
    "text": "system and then lastly it was ensuring fast parallel data access across thousands of nodes for this we leveraged",
    "start": "283520",
    "end": "289919"
  },
  {
    "text": "uh GCS views and Google Cloud Storage so yeah let's start with some scale testing we did with Ray data uh as",
    "start": "289919",
    "end": "296960"
  },
  {
    "text": "many of you know by now Ray data is a scalable data processing library for machine learning workloads it's really",
    "start": "296960",
    "end": "303080"
  },
  {
    "text": "popular right now for uh batch and frints and data processing um and highly efficient and performant at distributed",
    "start": "303080",
    "end": "309560"
  },
  {
    "text": "processing of large very large data sets uh to really stretch the limits of Ray",
    "start": "309560",
    "end": "315120"
  },
  {
    "text": "data we started searching for a sufficiently large data set to perform a",
    "start": "315120",
    "end": "320400"
  },
  {
    "text": "set of benchmarks we wanted to find a data set that is like really large um",
    "start": "320400",
    "end": "326479"
  },
  {
    "text": "and that uh the B data set needed to be public so we can kind of publish the results um any reproducible steps for",
    "start": "326479",
    "end": "333240"
  },
  {
    "text": "other people to try um one data set we discovered was the lat data set so lanat is an ongoing",
    "start": "333240",
    "end": "340160"
  },
  {
    "text": "mission of Earth observation satellites it provides the longest U continuous",
    "start": "340160",
    "end": "345319"
  },
  {
    "text": "space based record of Earth's land dating all the way back to 1972 uh the",
    "start": "345319",
    "end": "351000"
  },
  {
    "text": "lanat archive contains pedabytes of data which translates to millions of millions of individual images acquired over",
    "start": "351000",
    "end": "358000"
  },
  {
    "text": "decades and thanks to to the open data policies of the United States Geological Survey and NASA this data set is",
    "start": "358000",
    "end": "365080"
  },
  {
    "text": "available on Google Cloud Storage so the Google Cloud Storage bucket hosting this data contains approximately 4 and a half",
    "start": "365080",
    "end": "372479"
  },
  {
    "text": "pedabytes of image data uh for our use case we focus on the",
    "start": "372479",
    "end": "378919"
  },
  {
    "text": "landside 8 collection of images so the landside 8 mission is the eighth satellite uh in the lanat program uh and",
    "start": "378919",
    "end": "386560"
  },
  {
    "text": "the seventh to reach orbit successfully and for testing we chose lanet 8 because",
    "start": "386560",
    "end": "391759"
  },
  {
    "text": "it's the latest completed data set with the highest resolution of images and it actually contains most of the um data of",
    "start": "391759",
    "end": "399360"
  },
  {
    "text": "the data set which is approximately 3.3 paby of data um and the image on the right here is a example image from the",
    "start": "399360",
    "end": "406440"
  },
  {
    "text": "data set over Salt Lake City uh this is the data set structure",
    "start": "406440",
    "end": "412039"
  },
  {
    "text": "in the Google Cloud Storage bucket uh the first two parts refer to the lat satellite and collection number those",
    "start": "412039",
    "end": "418039"
  },
  {
    "text": "are fixed for our test um third and fourth Parts refer to the path and row of Earth's surface based on",
    "start": "418039",
    "end": "424800"
  },
  {
    "text": "the uh the grid system used by NASA for this data set um and the last part refers to the unique scene ID um based",
    "start": "424800",
    "end": "432840"
  },
  {
    "text": "on the time the image was taken and various other factors um and then within each scene ID is collection of images",
    "start": "432840",
    "end": "439759"
  },
  {
    "text": "And geotiff format um so our goal is basically uh to",
    "start": "439759",
    "end": "445440"
  },
  {
    "text": "process the entire late data set using Ray data and cu gray and on gke um the",
    "start": "445440",
    "end": "452759"
  },
  {
    "text": "job involves basically uh reading the images applying a series of torch transformation such as like resizing the",
    "start": "452759",
    "end": "460000"
  },
  {
    "text": "image converting the data type um and we also open source like all the source code and the ray job configuration we",
    "start": "460000",
    "end": "466080"
  },
  {
    "text": "use in our testing so it's available in the qray repo if you're interested in the",
    "start": "466080",
    "end": "471280"
  },
  {
    "text": "details so naturally the first thing we tried was we just said hey let's deploy",
    "start": "471280",
    "end": "476840"
  },
  {
    "text": "really big really big Ray cluster just like one big Ray cluster and try to have it just process the whole data set um",
    "start": "476840",
    "end": "484319"
  },
  {
    "text": "and I shared in kind of the other uh in the keynote and other talks we we know that Ray right now supports up to about",
    "start": "484319",
    "end": "490919"
  },
  {
    "text": "8,000 nodes in a single cluster so the the main downside with running such a large Ray cluster is that failures and",
    "start": "490919",
    "end": "497840"
  },
  {
    "text": "except exceptions or errors are expensive to recover from so like if the if the ray head goes down like all the",
    "start": "497840",
    "end": "504720"
  },
  {
    "text": "GCS dat is lost and then you have to sometimes like recreate the entire Ray cluster and this is really expensive to",
    "start": "504720",
    "end": "510720"
  },
  {
    "text": "do at the scale of thousands of PODS so to fully optimize for the scale",
    "start": "510720",
    "end": "516680"
  },
  {
    "text": "that gke can really support in a reliable and fault tolerant way we leveraged uh Ray job from cubay to scale",
    "start": "516680",
    "end": "523518"
  },
  {
    "text": "out to thousands of ephemeral Ray clusters and then this way we can actually Shard the data set across",
    "start": "523519",
    "end": "529800"
  },
  {
    "text": "thousands of Ray jobs and then automatically retry like individual Ray jobs if they fail for whatever",
    "start": "529800",
    "end": "535600"
  },
  {
    "text": "reason uh this is the example Ray job from our experimentation uh we use the ray runtime to configure like which",
    "start": "535600",
    "end": "542480"
  },
  {
    "text": "prefix of the data set to process um this example also shows the python dependencies um but in our actual",
    "start": "542480",
    "end": "549160"
  },
  {
    "text": "testing we just like baked all the dependencies in the image because downloading the python dependencies thousands of times would not be",
    "start": "549160",
    "end": "556959"
  },
  {
    "text": "ideal um and so due to the way the data set is structured it's actually fairly",
    "start": "556959",
    "end": "562800"
  },
  {
    "text": "easy to uh sharded out across multiple thousands of Ray jobs we basically just",
    "start": "562800",
    "end": "568519"
  },
  {
    "text": "uh give each R job a point in the grid system and then the ray job then walks through every file an image of that in",
    "start": "568519",
    "end": "575880"
  },
  {
    "text": "that path and then does the torch Transformations um so some uh numbers uh",
    "start": "575880",
    "end": "584880"
  },
  {
    "text": "if we basically uh Shard the data set for each grip Point uh that actually ends up being about",
    "start": "584880",
    "end": "590440"
  },
  {
    "text": "22,000 Ray jobs um assuming each Ray job runs a ray cluster with one rayad 10",
    "start": "590440",
    "end": "597399"
  },
  {
    "text": "workers each with four CPUs in 16 GB of memory the total capacity we need to",
    "start": "597399",
    "end": "603360"
  },
  {
    "text": "process the entire data set would be almost uh 1 million CPUs and about 3 and",
    "start": "603360",
    "end": "608959"
  },
  {
    "text": "a half terabytes of memory um in our testing we used a GK cluster with 10,000",
    "start": "608959",
    "end": "614800"
  },
  {
    "text": "E2 standard 4 nodes so that's about 40,000 CPUs and 160 terabytes of memory",
    "start": "614800",
    "end": "621440"
  },
  {
    "text": "um if we attempted to run all the ray jobs at once we would actually run into various uh scheduling issues since like",
    "start": "621440",
    "end": "627959"
  },
  {
    "text": "some Ray clusters would be left unschedulable like the ray head or like several workers wouldn't actually be",
    "start": "627959",
    "end": "633120"
  },
  {
    "text": "scheduled and then a bunch of jobs would be stuck and the the actual data processing job would never finish uh one",
    "start": "633120",
    "end": "639959"
  },
  {
    "text": "naive approach and simple way to solve this is we could just use like significantly larger instances right so",
    "start": "639959",
    "end": "645720"
  },
  {
    "text": "for example if you ran 10,000 N2 standard 96 nodes we would actually have",
    "start": "645720",
    "end": "651200"
  },
  {
    "text": "just the right amount of capacity like the 960,000 CPUs to run all the ray jobs",
    "start": "651200",
    "end": "656440"
  },
  {
    "text": "at once however that's obviously significantly more expensive and not the most cost efficient solution and so to",
    "start": "656440",
    "end": "664079"
  },
  {
    "text": "solve this uh capacity problem you leverage Q so Q is a kubernetes native",
    "start": "664079",
    "end": "669480"
  },
  {
    "text": "job queuing system it ensures that jobs that don't fit the uh existing capacity",
    "start": "669480",
    "end": "674920"
  },
  {
    "text": "should wait and it ensures proper execution of jobs by by their priority",
    "start": "674920",
    "end": "680320"
  },
  {
    "text": "order and so with Q we can kind of just deploy all the ray jobs at once and rely on Q to run a subset subset of the ray",
    "start": "680320",
    "end": "687399"
  },
  {
    "text": "jobs as capacity becomes available as jobs are kind of finishing uh Q also has um Native",
    "start": "687399",
    "end": "694320"
  },
  {
    "text": "integration with Ray job and Ray cluster uh it inspects the resource requirements",
    "start": "694320",
    "end": "699880"
  },
  {
    "text": "of the head and worker pods and ensures the resources are accurately represented in the cluster queue where we specify",
    "start": "699880",
    "end": "706519"
  },
  {
    "text": "the available quotas available uh for this cluster um and yeah you can see in the",
    "start": "706519",
    "end": "711600"
  },
  {
    "text": "slide the example cluster q that represents the total 40,000 CPUs and",
    "start": "711600",
    "end": "717800"
  },
  {
    "text": "160,000 um G gab of memory yeah so let's look at some graphs",
    "start": "717800",
    "end": "723240"
  },
  {
    "text": "um so this is just the graph of number of nodes takes a few minutes about like",
    "start": "723240",
    "end": "729200"
  },
  {
    "text": "10 minutes for GK to kind of stabilize on the 10,000 nodes um next we have like the Cub",
    "start": "729200",
    "end": "735720"
  },
  {
    "text": "object count in the JK cluster you can see like once the GK cluster has stabilized on 10,000 nodes we then",
    "start": "735720",
    "end": "742480"
  },
  {
    "text": "deploy the 22,000 Ray jobs we created and then you'll note that um Cube Ray",
    "start": "742480",
    "end": "748519"
  },
  {
    "text": "doesn't like immediately create the 22,000 clusters for the 22,000 Ray jobs",
    "start": "748519",
    "end": "753600"
  },
  {
    "text": "that's because Q will basically suspend every Ray job on creation and then as",
    "start": "753600",
    "end": "758680"
  },
  {
    "text": "capacity is available it'll it'll allow some Ray jobs to resume and so the",
    "start": "758680",
    "end": "764120"
  },
  {
    "text": "number of Ray clusters you see Pro sitting around like 1,000 is B is capped",
    "start": "764120",
    "end": "769320"
  },
  {
    "text": "based on the amount of quota available so at any point you'll have only 1,000 Ray clusters running even though the",
    "start": "769320",
    "end": "775480"
  },
  {
    "text": "cluster has like 22,000 Ray jobs that are kind of trying to complete at once and this approach is also like very",
    "start": "775480",
    "end": "781880"
  },
  {
    "text": "popular for when you have like a GPU workload a workload that needs gpus you can basically do proper gang scheduling",
    "start": "781880",
    "end": "788920"
  },
  {
    "text": "of gpus and priority scheduling um and all those Advanced use",
    "start": "788920",
    "end": "794360"
  },
  {
    "text": "cases uh next we have like the work Q depth this these are more like kubernetes controller metrics um the",
    "start": "794360",
    "end": "802120"
  },
  {
    "text": "first graph shows like the initial object ingestion when we created the 22,000 Ray jobs then you see see the red",
    "start": "802120",
    "end": "809839"
  },
  {
    "text": "line at the bottom those that's the ray cluster uh work VI depth so similar to last graph we see big spike and Ray jobs",
    "start": "809839",
    "end": "817560"
  },
  {
    "text": "and then as Q allows capacity we see a the work Q depth of Ray cluster is kind",
    "start": "817560",
    "end": "822880"
  },
  {
    "text": "of gradually increasing um same with the reconcile rate big spike in Ray jobs and then it kind of decreases as the number",
    "start": "822880",
    "end": "830320"
  },
  {
    "text": "of Ray jobs are kind of gradually completing throughout the throughout the job um the final graph here is just the",
    "start": "830320",
    "end": "838160"
  },
  {
    "text": "CRA memory CPU um you can see the cubra is fairly resource efficient and lightweight considering it's managing",
    "start": "838160",
    "end": "845279"
  },
  {
    "text": "like thousands of tens of thousands of Ray jobs um and tens of thousands of Pods at any given time it's only using",
    "start": "845279",
    "end": "852120"
  },
  {
    "text": "like four CPUs and like 8 gigabytes of RAM um really the only optimization we had to make in CU was um increasing the",
    "start": "852120",
    "end": "859920"
  },
  {
    "text": "reconcile concurrency because it's defaults to one um and we just had to make sure that there was sufficient um",
    "start": "859920",
    "end": "866279"
  },
  {
    "text": "memory and then the reason why you see multiple lines is cuz we run cubr with uh ha configuration so that if any cuay",
    "start": "866279",
    "end": "874240"
  },
  {
    "text": "dies another one can immediately take over and then the final piece of the puzzle that I haven't covered yet that",
    "start": "874240",
    "end": "880399"
  },
  {
    "text": "made all the difference is fast data access with GCS views and Cat will kind",
    "start": "880399",
    "end": "885759"
  },
  {
    "text": "of cover that in more",
    "start": "885759",
    "end": "888440"
  },
  {
    "text": "details okay so I'm going to introduce CSI drivers in kubernetes and then GK a",
    "start": "891480",
    "end": "899240"
  },
  {
    "text": "GCS fuse CSI driver that we built ground up uh to make it work seamlessly uh to",
    "start": "899240",
    "end": "905600"
  },
  {
    "text": "mount GCS buckets on kubernetes clusters so CSI is a kubernetes standard",
    "start": "905600",
    "end": "911880"
  },
  {
    "text": "specification of how you can make storage available to kubernetes pods as",
    "start": "911880",
    "end": "917000"
  },
  {
    "text": "uh mounted volumes and GK GCS F CSI driver is an extension of that uh",
    "start": "917000",
    "end": "922639"
  },
  {
    "text": "specifically focused on mounting GCS buckets so what are some of the benefits",
    "start": "922639",
    "end": "928759"
  },
  {
    "text": "of of the CSI driver uh so the first thing that i' would like to point out is portability so you get posix like file",
    "start": "928759",
    "end": "936680"
  },
  {
    "text": "system I say posix like because it's not fully posix compliant but you get the uh",
    "start": "936680",
    "end": "942519"
  },
  {
    "text": "most of the posix uh compliance and that means it's also compatible with any file",
    "start": "942519",
    "end": "947680"
  },
  {
    "text": "system apis and also the rare data apis uh reliability is a big part when you",
    "start": "947680",
    "end": "953680"
  },
  {
    "text": "want to deploy things in production uh we completely control the",
    "start": "953680",
    "end": "960759"
  },
  {
    "text": "uh upgrades of the GK CSI driver to address vulnerability fixes bug fixes",
    "start": "960759",
    "end": "967000"
  },
  {
    "text": "new features but at the same time we need to ensure that it does these kind",
    "start": "967000",
    "end": "972040"
  },
  {
    "text": "of upgrades do not impact the user pods or the ray pods for example and they",
    "start": "972040",
    "end": "978199"
  },
  {
    "text": "should not take a downtime and from the security standpoint uh we have fine grain",
    "start": "978199",
    "end": "984480"
  },
  {
    "text": "integration with uh workload identity Federation so you can set up up access control rules like this kubernetes name",
    "start": "984480",
    "end": "992240"
  },
  {
    "text": "space of this service account uh has access to read and write buckets so all",
    "start": "992240",
    "end": "998160"
  },
  {
    "text": "of that is baked into the volume apis of kubernetes uh what else so we we also",
    "start": "998160",
    "end": "1006160"
  },
  {
    "text": "ensure that you don't need to use privileged pods to mount the buckets because that's important so that you",
    "start": "1006160",
    "end": "1012199"
  },
  {
    "text": "don't need to give unnecessary elevated uh privileges to your user pods",
    "start": "1012199",
    "end": "1017800"
  },
  {
    "text": "observability is a big one where you if you want to Deep dive into performance",
    "start": "1017800",
    "end": "1023559"
  },
  {
    "text": "bottlenecks uh we want to have native uh M Prometheus uh compatible Matrix uh and",
    "start": "1023559",
    "end": "1030360"
  },
  {
    "text": "that you can slice and dice by pod and name spaces and scale is something we have",
    "start": "1030360",
    "end": "1036640"
  },
  {
    "text": "been iterating a lot over the past uh one year uh because of the growing demands of the workloads that we are",
    "start": "1036640",
    "end": "1042720"
  },
  {
    "text": "seeing in the order of thousands of nodes and tens of thousands of PODS uh",
    "start": "1042720",
    "end": "1048120"
  },
  {
    "text": "some of the improvements that I would like to highlight that we did in our CSI drivers are in the control plane of the",
    "start": "1048120",
    "end": "1055799"
  },
  {
    "text": "CSI driver we identified bottlenecks like chattiness between the driver and the cube API server so fixed all of",
    "start": "1055799",
    "end": "1063760"
  },
  {
    "text": "those uh ensure that we don't consume unnecessary quota and on the data path",
    "start": "1063760",
    "end": "1069919"
  },
  {
    "text": "side the core GCS fuse process we optimized heavily to ensure that it it",
    "start": "1069919",
    "end": "1076720"
  },
  {
    "text": "works for both the small files use case and the large file use case the small file use case could be say you have a",
    "start": "1076720",
    "end": "1083960"
  },
  {
    "text": "bucket with millions of objects uh that uh need to be read into your rate train",
    "start": "1083960",
    "end": "1090000"
  },
  {
    "text": "job or something like that uh so metadata can become a bottleneck there so we implemented uh some Advanced",
    "start": "1090000",
    "end": "1096559"
  },
  {
    "text": "caching strategies inside GCS fuse and on the larger file Spectrum uh like par",
    "start": "1096559",
    "end": "1102919"
  },
  {
    "text": "files with uh Which packs a lot of uh training data samples or model weight",
    "start": "1102919",
    "end": "1108480"
  },
  {
    "text": "files we have implemented uh features like file cache and parallel prefet so what",
    "start": "1108480",
    "end": "1114840"
  },
  {
    "text": "it does is GCS fuse will seamlessly take advantage of any available local ssds on",
    "start": "1114840",
    "end": "1120480"
  },
  {
    "text": "your GK nodes and when the user applications triggers reads on these",
    "start": "1120480",
    "end": "1125919"
  },
  {
    "text": "large files GCS fuse will asynchronously start pulling in those uh chunks of uh",
    "start": "1125919",
    "end": "1132200"
  },
  {
    "text": "sub chunks of the the large file in parallel into the uh local SSD and sub",
    "start": "1132200",
    "end": "1138880"
  },
  {
    "text": "quent reads of the uh blocks of that uh big file will be served directly from uh",
    "start": "1138880",
    "end": "1145559"
  },
  {
    "text": "the local SSD via GCS fuse so these are some of the advantages now to tie the GK",
    "start": "1145559",
    "end": "1152520"
  },
  {
    "text": "CSI with the ray ecosystem I'm going to show a short demo of of a r train job uh",
    "start": "1152520",
    "end": "1160799"
  },
  {
    "text": "where we use GK GCS CSI driver to uh run a distributed training with 500 ,000",
    "start": "1160799",
    "end": "1169200"
  },
  {
    "text": "image files uh and using rate rain and P torch lightning so this is how the uh",
    "start": "1169200",
    "end": "1175600"
  },
  {
    "text": "the volume spec looks like for a ray cluster uh you can see here we support like uh volume apis like CSI epimeral",
    "start": "1175600",
    "end": "1184600"
  },
  {
    "text": "and uh persistent volume and it's very seamless to integrate into the ray cluster",
    "start": "1184600",
    "end": "1190360"
  },
  {
    "text": "spec and now for the specific demo that I'm going to talk about uh so what we",
    "start": "1190360",
    "end": "1196960"
  },
  {
    "text": "have here is a GK cluster of thousand nodes with a Q deployed so we have two",
    "start": "1196960",
    "end": "1203760"
  },
  {
    "text": "local cues uh this could be uh like you could segregate it by teams or name",
    "start": "1203760",
    "end": "1210600"
  },
  {
    "text": "spaces so we have two local cues uh we are going to deploy simultaneously four",
    "start": "1210600",
    "end": "1216120"
  },
  {
    "text": "rate rain jobs each of these rate rain jobs uh triggers creation of epimeral",
    "start": "1216120",
    "end": "1222679"
  },
  {
    "text": "Ray clusters and each of the ray cluster has uh around 250 uh Ray train pods so",
    "start": "1222679",
    "end": "1229679"
  },
  {
    "text": "in aggregate there are thousand pods that are admitted into the cluster and all of these pods are direct directly",
    "start": "1229679",
    "end": "1236679"
  },
  {
    "text": "reading from the GCS bucket through the GCS uh like the GK CSI driver and in the",
    "start": "1236679",
    "end": "1243520"
  },
  {
    "text": "bucket we have a few artifacts so we have the pre-train model checkpoints that could be used as a starting point",
    "start": "1243520",
    "end": "1250280"
  },
  {
    "text": "to bootstrap your uh uh the training experiment you have the training data",
    "start": "1250280",
    "end": "1256080"
  },
  {
    "text": "set that contains those 500,000 uh image files and you have the uh the experiment",
    "start": "1256080",
    "end": "1263120"
  },
  {
    "text": "art output artifacts so basically as you progress through your epox you could write back uh checkpoints back to the",
    "start": "1263120",
    "end": "1269240"
  },
  {
    "text": "same GCS bucket and drilling down a bit further",
    "start": "1269240",
    "end": "1274480"
  },
  {
    "text": "uh if I double click into each Ray pod you can see that there is a g GCS fuse",
    "start": "1274480",
    "end": "1280279"
  },
  {
    "text": "sidecar container that is automatically injected by GK uh because you specified",
    "start": "1280279",
    "end": "1286640"
  },
  {
    "text": "GCS volumes in your pod spec and on in below you can see there is something called a secondary boot dis",
    "start": "1286640",
    "end": "1293760"
  },
  {
    "text": "where we basically uh bake in the ray container images because these are very large images and they are uh they are",
    "start": "1293760",
    "end": "1301919"
  },
  {
    "text": "pulled into the GK node in a streaming fashion uh which results in very fast uh",
    "start": "1301919",
    "end": "1307080"
  },
  {
    "text": "pod uh pod startup time and this is the video that I have",
    "start": "1307080",
    "end": "1313960"
  },
  {
    "text": "I'm going to uh",
    "start": "1313960",
    "end": "1318559"
  },
  {
    "text": "this is a short demo on how to deploy multiple and let me fast forward to the",
    "start": "1319200",
    "end": "1324880"
  },
  {
    "text": "actual training run because I've already talked about uh the introductory part of",
    "start": "1324880",
    "end": "1330480"
  },
  {
    "text": "the how the setup is doneo",
    "start": "1330480",
    "end": "1337080"
  },
  {
    "text": "now so we have the GK cluster set up with th000 nodes with the Ray images",
    "start": "1337080",
    "end": "1342679"
  },
  {
    "text": "cast in the secondary boot dis attached to the nodes and the GCS bucket we can see the data set and the pre-train",
    "start": "1342679",
    "end": "1351760"
  },
  {
    "text": "checkpoint now we are going to admit uh the first ROP to the uh cluster",
    "start": "1351760",
    "end": "1360120"
  },
  {
    "text": "que and this will spin up a 250 pod R cluster with one Ray head node and 2 49",
    "start": "1360120",
    "end": "1366520"
  },
  {
    "text": "worker",
    "start": "1366520",
    "end": "1368919"
  },
  {
    "text": "nodes and we should we will also take a look at the r job it's showing as an",
    "start": "1374000",
    "end": "1380240"
  },
  {
    "text": "initializing state from the r dashboard we can now see that uh the the job has",
    "start": "1380240",
    "end": "1387480"
  },
  {
    "text": "started and eventually we should see 250 R train workers uh also",
    "start": "1387480",
    "end": "1394799"
  },
  {
    "text": "started we are now spinning up three more more Ray jobs Each of which which would spin up 2 50 more pods so an",
    "start": "1405840",
    "end": "1413240"
  },
  {
    "text": "aggregate of thousand pods running in the cluster simultaneously and all of the ray jobs",
    "start": "1413240",
    "end": "1420039"
  },
  {
    "text": "are now in initializing State and one is in running as shown",
    "start": "1420039",
    "end": "1426080"
  },
  {
    "text": "before we'll set up a watch just to monitor the progress of all the ray",
    "start": "1426919",
    "end": "1433400"
  },
  {
    "text": "jobs and we can fast forward when the job completes and take a look at at one",
    "start": "1433400",
    "end": "1438760"
  },
  {
    "text": "of the jobs resonate for as to why it took slightly longer time to",
    "start": "1438760",
    "end": "1445279"
  },
  {
    "text": "complete uh when we see at the ray job logs we can see that there was a",
    "start": "1446000",
    "end": "1452760"
  },
  {
    "text": "exception due to failure to initialize the process group because of which the",
    "start": "1452760",
    "end": "1458159"
  },
  {
    "text": "ray job restarted gracefully and once it restarted it initialized the ray",
    "start": "1458159",
    "end": "1464360"
  },
  {
    "text": "workers and also resumed it training from the known",
    "start": "1464360",
    "end": "1469640"
  },
  {
    "text": "location of the checkpoints and eventually the ray train",
    "start": "1469640",
    "end": "1475240"
  },
  {
    "text": "Epoch would complete in around 88 seconds and the job wrote back the",
    "start": "1475240",
    "end": "1481240"
  },
  {
    "text": "result back to its uh job specific directory so as you can see here there",
    "start": "1481240",
    "end": "1487640"
  },
  {
    "text": "are four job specific directories with the output artifacts we'll also quickly take a look",
    "start": "1487640",
    "end": "1494600"
  },
  {
    "text": "at the observability of the bucket to see various Matrix like the read list",
    "start": "1494600",
    "end": "1501960"
  },
  {
    "text": "get request counts and the eess uh data over the network similarly",
    "start": "1501960",
    "end": "1508720"
  },
  {
    "text": "the CSI the GCS F CSI driver also emits Matrix which can be sliced and died by",
    "start": "1508720",
    "end": "1515120"
  },
  {
    "text": "various Fields like pod name name spaces and we also emit various GCS fuse",
    "start": "1515120",
    "end": "1524200"
  },
  {
    "text": "specific uh Matrix as shown here",
    "start": "1524200",
    "end": "1529760"
  },
  {
    "text": "yeah yeah so this is the demo that I had where what we what I tried to show is",
    "start": "1529840",
    "end": "1535399"
  },
  {
    "text": "like you can deploy multiple Ray jobs each reading from GCS bucket seamlessly",
    "start": "1535399",
    "end": "1541919"
  },
  {
    "text": "you could even like Mount subparts within your buckets if you if you have a",
    "start": "1541919",
    "end": "1547000"
  },
  {
    "text": "huge bucket with data sets for multiple teams uh so there there's a lot of",
    "start": "1547000",
    "end": "1552039"
  },
  {
    "text": "flexibility and also uh configuration knobs for your that is that that that's",
    "start": "1552039",
    "end": "1558000"
  },
  {
    "text": "that is attuned to your data set shapes it could be a bunch of small files it could be a collection of large files uh",
    "start": "1558000",
    "end": "1565760"
  },
  {
    "text": "we try to cover both the end of the spectrum and going back to so we have",
    "start": "1565760",
    "end": "1573000"
  },
  {
    "text": "some uh information provided here to try out the GK managed Ray Cube Ray operator",
    "start": "1573000",
    "end": "1579919"
  },
  {
    "text": "and we also have the GK GCS fuse uh CSI driver that can be uh easily enabled on",
    "start": "1579919",
    "end": "1586399"
  },
  {
    "text": "a GK cluster it's enabled by default on GK autopilot and it can be enabled",
    "start": "1586399",
    "end": "1591720"
  },
  {
    "text": "through g-cloud console and terraform on the CSI driver uh sorry on the uh",
    "start": "1591720",
    "end": "1598000"
  },
  {
    "text": "standard GK standard clusters and yeah try it out on GK uh we",
    "start": "1598000",
    "end": "1604559"
  },
  {
    "text": "have uploaded all the code base to the cube R per tests thank you",
    "start": "1604559",
    "end": "1611350"
  },
  {
    "text": "[Applause]",
    "start": "1611350",
    "end": "1618929"
  },
  {
    "text": "three minutes questions any questions",
    "start": "1624320",
    "end": "1631240"
  },
  {
    "text": "yeah so when scaling a gke cluster to like 15,000 nodes um any best practices",
    "start": "1635679",
    "end": "1644600"
  },
  {
    "text": "or things to watch out for andw you want to take them yeah um I think definitely",
    "start": "1644600",
    "end": "1653640"
  },
  {
    "text": "um doing it gradually is going to kind of help kick up like our systems to like you know scale up to the traffic um I",
    "start": "1653640",
    "end": "1661120"
  },
  {
    "text": "think the the biggest kind of gotas that we see is at really large scale it's",
    "start": "1661120",
    "end": "1666840"
  },
  {
    "text": "really easy to deploy like uh controllers or Damon set in the cluster",
    "start": "1666840",
    "end": "1672760"
  },
  {
    "text": "that make really expensive API calls so like the most common example we we see is um you deploy Damon set the Damon set",
    "start": "1672760",
    "end": "1680640"
  },
  {
    "text": "needs to look up the node information about itself but the way it does that is like it lists every node and then it",
    "start": "1680640",
    "end": "1686760"
  },
  {
    "text": "uses like a field selector to find the node that matches itself but then if you do that at 15,000 scale you have",
    "start": "1686760",
    "end": "1693000"
  },
  {
    "text": "15,000 um processes like listing 15,000 nodes so like I think the larger your JK",
    "start": "1693000",
    "end": "1699279"
  },
  {
    "text": "cluster becomes and the more pods and nodes you run you have to be a little bit more um careful about the type of uh",
    "start": "1699279",
    "end": "1706960"
  },
  {
    "text": "clients that are going to be deploy in the cluster and making those type of requests well okay so um when you're",
    "start": "1706960",
    "end": "1713720"
  },
  {
    "text": "starting nodes probably cubits have to make some uh list requests uh so is that",
    "start": "1713720",
    "end": "1720080"
  },
  {
    "text": "process uh yeah so GK GK is like when we do our scale testing we are very like we",
    "start": "1720080",
    "end": "1727000"
  },
  {
    "text": "take into account the cuet traffic because cuet is like the most uh high volume traffic client right um so in all",
    "start": "1727000",
    "end": "1733480"
  },
  {
    "text": "our benchmarks we're basically ensuring that cuet is behaving well and we've also done you know Sig node a bunch of",
    "start": "1733480",
    "end": "1740679"
  },
  {
    "text": "people from Google have kind of done a lot of uh uh investment into making like node scalable like one example is we",
    "start": "1740679",
    "end": "1747320"
  },
  {
    "text": "split out the node heartbeat into the lease API instead of doing it in the node API and that kind of improves",
    "start": "1747320",
    "end": "1752880"
  },
  {
    "text": "scalability of the C quite a bit cool thanks and I can add a couple of points there another thing that you need to be",
    "start": "1752880",
    "end": "1759600"
  },
  {
    "text": "aware of is the quotas that you have in your project you could hit quota limits",
    "start": "1759600",
    "end": "1764760"
  },
  {
    "text": "and cause retry and back off and that something to watch out for that we have hit a lot in uh trying to scale at that",
    "start": "1764760",
    "end": "1773200"
  },
  {
    "text": "that numbers another hack I'll mention maybe just for this group is if you are planning to actually run 15,000 nodes",
    "start": "1773200",
    "end": "1779399"
  },
  {
    "text": "maybe open a support ticket as well and just let Google know that you're going to do that good point all right so I had a few",
    "start": "1779399",
    "end": "1787159"
  },
  {
    "text": "questions the first one was uh well one other thing that Ray offers is uh any",
    "start": "1787159",
    "end": "1793919"
  },
  {
    "text": "scale on Raw Google Cloud when would you recommend using GK",
    "start": "1793919",
    "end": "1800000"
  },
  {
    "text": "versus just ra Google Cloud yeah I I think um well like that",
    "start": "1800000",
    "end": "1807039"
  },
  {
    "text": "was kind of like a newly announced um thing at at Ray Summit I think um you know customers that really care about",
    "start": "1807039",
    "end": "1814200"
  },
  {
    "text": "fully open- Source Solutions really flexible customizable deployments that are kind of portable across any kind of",
    "start": "1814200",
    "end": "1820200"
  },
  {
    "text": "uh kubernetes cluster um that's kind of generally where we would recommend CU",
    "start": "1820200",
    "end": "1825399"
  },
  {
    "text": "but I think you know as we saw in the Keynotes and many many other um talks if",
    "start": "1825399",
    "end": "1830480"
  },
  {
    "text": "you know any scale has uh you know really valuable proprietary features that you want we're partnering with with",
    "start": "1830480",
    "end": "1835960"
  },
  {
    "text": "them as well to make sure that any scale on GK is also going to scale really well and work well on",
    "start": "1835960",
    "end": "1841760"
  },
  {
    "text": "GK thanks another question is for that large job data processing job you were",
    "start": "1841760",
    "end": "1847960"
  },
  {
    "text": "showing um in the end it was cued on kubernetes side and you had separate",
    "start": "1847960",
    "end": "1854760"
  },
  {
    "text": "jobs that at that point what value Ray is adding cuz if you just use normal",
    "start": "1854760",
    "end": "1860240"
  },
  {
    "text": "kubernetes jobs put them on the Queue and you know the pods can discover",
    "start": "1860240",
    "end": "1866039"
  },
  {
    "text": "themselves well I mean the the main benefit is Ray providing the the libraries for that distributed computing",
    "start": "1866039",
    "end": "1873720"
  },
  {
    "text": "um the thing is you're putting the dependencies in the docker too right sorry what was that so you put the dependencies in the docker as well and",
    "start": "1873720",
    "end": "1880679"
  },
  {
    "text": "you just read the range from the bucket and process it and write back",
    "start": "1880679",
    "end": "1886000"
  },
  {
    "text": "so what's Ray adding there okay so if the question is like why did",
    "start": "1886000",
    "end": "1893919"
  },
  {
    "text": "why did we use Ray for the if you just you know put the entry point in a I",
    "start": "1893919",
    "end": "1899360"
  },
  {
    "text": "think the to put it simply like Ray data is a great um solution for for really",
    "start": "1899360",
    "end": "1904880"
  },
  {
    "text": "scalable data processing and very popular to use Ray data on kubernetes",
    "start": "1904880",
    "end": "1910679"
  },
  {
    "text": "right because Ray in the end of the day Ray needs to run on some sort of compute",
    "start": "1910679",
    "end": "1916159"
  },
  {
    "text": "right as magical as Ray feels sometimes do like ray. remote and it just runs at the end of the day like it does need to",
    "start": "1916159",
    "end": "1921360"
  },
  {
    "text": "run on something and obviously like you know Ray on kubernetes one popular option because you know we kubernetes",
    "start": "1921360",
    "end": "1929440"
  },
  {
    "text": "kind of provides that scalable compute foray so that that's kind of the where we're coming from okay and the last",
    "start": "1929440",
    "end": "1936360"
  },
  {
    "text": "question let's say whatever code I implemented uh it's not good enough to",
    "start": "1936360",
    "end": "1942240"
  },
  {
    "text": "run on a very large cluster let's say the demon set example for some reason the the cluster have deployed uh and I",
    "start": "1942240",
    "end": "1949480"
  },
  {
    "text": "would need more than one cluster so if I would want to submit jobs then I would",
    "start": "1949480",
    "end": "1956240"
  },
  {
    "text": "need as a developer need to select what queue to send to or do you offer",
    "start": "1956240",
    "end": "1961840"
  },
  {
    "text": "anything to like have like a unified Q yeah so Q basically supports um",
    "start": "1961840",
    "end": "1967120"
  },
  {
    "text": "multiple cluster cues and the way you manage what cues your jobs land on is using um one of the supported labels",
    "start": "1967120",
    "end": "1973840"
  },
  {
    "text": "that come with Q so you basically annotate the ray job with the label that matches the Q name and then that will",
    "start": "1973840",
    "end": "1979960"
  },
  {
    "text": "basically place it on one of the available cluster Keys okay thank you yeah um last question hi um I had",
    "start": "1979960",
    "end": "1988799"
  },
  {
    "text": "the question in a context of a very large uh training job um so the approach",
    "start": "1988799",
    "end": "1994799"
  },
  {
    "text": "you described before um by splitting up um and a multiple Ray job uh that you",
    "start": "1994799",
    "end": "2001360"
  },
  {
    "text": "schedule with Q um would it work out I mean would you have any advice uh to use",
    "start": "2001360",
    "end": "2007360"
  },
  {
    "text": "similar approach um in a context of a large data parel job U given that if one",
    "start": "2007360",
    "end": "2014000"
  },
  {
    "text": "fail well um because of the gradient sharing you would have to restart everything um is there like any tips or",
    "start": "2014000",
    "end": "2021919"
  },
  {
    "text": "um something you would notice that would be best to use in that case I didn't I didn't fully get the",
    "start": "2021919",
    "end": "2028679"
  },
  {
    "text": "last part of your question it was inaudible oh sorry um which F uh you",
    "start": "2028679",
    "end": "2035480"
  },
  {
    "text": "were mentioning about are you talking about sharding strategies uh um yeah I",
    "start": "2035480",
    "end": "2040760"
  },
  {
    "text": "mean um if you have um very large uh training job um but you would like to split the",
    "start": "2040760",
    "end": "2048480"
  },
  {
    "text": "maximum of nodes possible uh would you still advise to use this approach of uh",
    "start": "2048480",
    "end": "2053919"
  },
  {
    "text": "dividing in um multiple uh yeah so typically uh I think I I can talk purely",
    "start": "2053919",
    "end": "2061800"
  },
  {
    "text": "from the GCS fuse CSI perspective it's better to have like a one pod scheduled",
    "start": "2061800",
    "end": "2067398"
  },
  {
    "text": "partner node and consume all your accelerators because uh what will happen under the hood is uh we we attach a GCS",
    "start": "2067399",
    "end": "2076240"
  },
  {
    "text": "fuse side car uh on every pod now if you if you deploy let's say you have a node",
    "start": "2076240",
    "end": "2082040"
  },
  {
    "text": "with eight accelerators eight guse and you deploy eight pods to consume them",
    "start": "2082040",
    "end": "2087240"
  },
  {
    "text": "you will you'll get a GCS fuse instance for every pod and you will not be sharing like the caches uh so that and",
    "start": "2087240",
    "end": "2094599"
  },
  {
    "text": "you will you'll get significant benefit if you have a cash sharing so it's better to have one pod to consume all",
    "start": "2094599",
    "end": "2101560"
  },
  {
    "text": "accelerators rather than have uh each pod consume uh uh like one accelerator",
    "start": "2101560",
    "end": "2108560"
  },
  {
    "text": "each so that's one uh best practices pattern I can recommend okay thank you",
    "start": "2108560",
    "end": "2116880"
  }
]