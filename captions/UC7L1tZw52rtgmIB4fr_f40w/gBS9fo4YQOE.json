[
  {
    "start": "0",
    "end": "139000"
  },
  {
    "text": "all right uh welcome everyone we're going to get started uh can I get a quick check in the back can you hear me",
    "start": "3639",
    "end": "9760"
  },
  {
    "text": "okay awesome so my name is Edward Oaks and today I'll be telling you about race serve um we've had a number of talks",
    "start": "9760",
    "end": "15759"
  },
  {
    "text": "about race serve throughout the day this one is really going to be a high Lev talk where I'm going to tell you about you know where we are in Ray 2.0 and and",
    "start": "15759",
    "end": "22880"
  },
  {
    "text": "where we're headed in the future so Simon MO is also a co-author on this talk he gave the virtual version but",
    "start": "22880",
    "end": "28960"
  },
  {
    "text": "it'll just be me here today so quick outline before I get started um",
    "start": "28960",
    "end": "34719"
  },
  {
    "text": "so I'm going to start by giving you an overview of the motivation for Ray serve uh and um and then I'm going to go into",
    "start": "34719",
    "end": "43000"
  },
  {
    "text": "a few highlights that we're really excited to announce for Ray 2.0 on some of the progress that we've made um that",
    "start": "43000",
    "end": "48840"
  },
  {
    "text": "is now available with the new release so let's dive into the",
    "start": "48840",
    "end": "54640"
  },
  {
    "text": "overview so you've probably heard about Ray AI runtime or air throughout the day",
    "start": "54640",
    "end": "60199"
  },
  {
    "text": "um so race serve is part of air and air is really just a collection of libraries uh that offer basically a unified API",
    "start": "60199",
    "end": "68119"
  },
  {
    "text": "and runtime for building end to-end machine learning applications so race serve um is like I",
    "start": "68119",
    "end": "74720"
  },
  {
    "text": "said part of air uh and today we're going to focus on the online inference or going to production and really uh",
    "start": "74720",
    "end": "80799"
  },
  {
    "text": "building apis that are powered by Machine learning",
    "start": "80799",
    "end": "85078"
  },
  {
    "text": "models so race serve is flex flexible scalable and efficient compute for",
    "start": "87479",
    "end": "92880"
  },
  {
    "text": "online inference it's built on top of Ray which means that we inherit a lot of the benefits out of the box that Ray",
    "start": "92880",
    "end": "99240"
  },
  {
    "text": "provides so we inherit scalability uh low latency communication and response times uh and also efficiency which is",
    "start": "99240",
    "end": "106439"
  },
  {
    "text": "really important when serving machine learning models one of the key highlights that we really focus on in Ray serve is uh",
    "start": "106439",
    "end": "114360"
  },
  {
    "text": "multimodel inference or model composition and we'll dive more into that later and RAC serve is also python",
    "start": "114360",
    "end": "121119"
  },
  {
    "text": "native um so this means that you can easily mix business logic and machine learning and you can use all of your",
    "start": "121119",
    "end": "126840"
  },
  {
    "text": "favorite Tools in libraries uh to build online inference apis so let's dive a little bit more",
    "start": "126840",
    "end": "134080"
  },
  {
    "text": "into an example of a multimodel inference",
    "start": "134080",
    "end": "139000"
  },
  {
    "start": "139000",
    "end": "139000"
  },
  {
    "text": "application so for the rest of this section we're going to use this working example of uh building a Content",
    "start": "139519",
    "end": "145239"
  },
  {
    "text": "understanding API um so you can imagine a user passes in an image we pass it",
    "start": "145239",
    "end": "150640"
  },
  {
    "text": "through some kind of machine learning process and then as output we want to have um some kind of structured",
    "start": "150640",
    "end": "156440"
  },
  {
    "text": "information about the image that's uh powered by Machine learning so in this case we might do some object detection",
    "start": "156440",
    "end": "162760"
  },
  {
    "text": "and say oh there's a hummingbird in this picture uh we might have a confidence score uh that can be used to make",
    "start": "162760",
    "end": "168400"
  },
  {
    "text": "decisions um based on how confident we are about the the object and then we're",
    "start": "168400",
    "end": "173680"
  },
  {
    "text": "also going to Output uh like a bounding box of where in the image is this object located",
    "start": "173680",
    "end": "181040"
  },
  {
    "start": "181000",
    "end": "181000"
  },
  {
    "text": "so let's dive a little bit into what it might look like to actually build this uh as an inference",
    "start": "181040",
    "end": "186799"
  },
  {
    "text": "service so first we'll probably have to download the image fetch it from some kind of storage like Amazon S3 and then",
    "start": "186799",
    "end": "193680"
  },
  {
    "text": "we might do some basic pre-processing um so maybe crop the image to the right size or apply some basic filters or",
    "start": "193680",
    "end": "201519"
  },
  {
    "text": "transforms and then when we actually want to do the object classification uh we may have multiple different types of",
    "start": "201519",
    "end": "207280"
  },
  {
    "text": "models right this might depend on the user that you're using you might be doing an AB test um or you might use any",
    "start": "207280",
    "end": "213959"
  },
  {
    "text": "other any number of other uh factors to decide which machine learning model you actually want to pass this",
    "start": "213959",
    "end": "219799"
  },
  {
    "text": "through in parallel we also want to do object detection or image detection in order to find the bounding box that will",
    "start": "219799",
    "end": "226519"
  },
  {
    "text": "return as part of the API and then finally we need to stitch all of this together into some kind of",
    "start": "226519",
    "end": "232079"
  },
  {
    "text": "structured output format um in order to uh return the information that we want",
    "start": "232079",
    "end": "237239"
  },
  {
    "text": "from the API so this is very clearly not just you know a single pyit learn model or something like that uh to build this",
    "start": "237239",
    "end": "244159"
  },
  {
    "text": "kind of like end to-end API you end up having to stitch a lot of different uh machine learning and just business logic",
    "start": "244159",
    "end": "252000"
  },
  {
    "text": "um together so now let's imagine that we want to actually deploy this as an",
    "start": "252000",
    "end": "258320"
  },
  {
    "start": "254000",
    "end": "254000"
  },
  {
    "text": "online service right we want to have an API we can query in real time with an image and get this response so this",
    "start": "258320",
    "end": "265120"
  },
  {
    "text": "comes with a lot of of pretty strict requirements so obviously we want to minimize lat",
    "start": "265120",
    "end": "270280"
  },
  {
    "text": "because that directly impacts the user experience or maybe Downstream systems and we want to maximize throughput so",
    "start": "270280",
    "end": "276240"
  },
  {
    "text": "that we can uh uh save cost um especially when machine learning models are are very cost",
    "start": "276240",
    "end": "284199"
  },
  {
    "text": "intensive along the same vein we may also want to allocate fractional uh like fine grained resources we may want",
    "start": "284199",
    "end": "290960"
  },
  {
    "text": "multiple models to run on the same CPU or on the same GPU in order to save cost and it's really important that we",
    "start": "290960",
    "end": "297800"
  },
  {
    "text": "have a programmatic API to make sure that the data scientists or machine learning Engineers that are building",
    "start": "297800",
    "end": "303000"
  },
  {
    "text": "this application are able to uh be really productive when uh building developing and testing uh their",
    "start": "303000",
    "end": "311240"
  },
  {
    "text": "application and finally when we do go to production we need to have a rock solid story right we need to have monitoring",
    "start": "311240",
    "end": "316880"
  },
  {
    "text": "we need to have solid upgrades we need to have high availability and these things are just table Stakes so note that uh a lot of these",
    "start": "316880",
    "end": "324000"
  },
  {
    "text": "things are are not really that related to machine learning right this isn't like directly tied to your model AR",
    "start": "324000",
    "end": "329680"
  },
  {
    "text": "arure it's really about the infrastructure and the compute layer and that's what RAC serve is is uh built to",
    "start": "329680",
    "end": "335759"
  },
  {
    "text": "give you so let's look at uh what is you know",
    "start": "335759",
    "end": "341160"
  },
  {
    "start": "337000",
    "end": "337000"
  },
  {
    "text": "often kind of the naive solution to this which is let's just take all of our models and all of our business logic and",
    "start": "341160",
    "end": "346319"
  },
  {
    "text": "pack it up in a single container so you might take uh multiple models maybe from different uh different",
    "start": "346319",
    "end": "353800"
  },
  {
    "text": "libraries even like pytorch or tensorflow uh you know write a little bit of python logic to glue them",
    "start": "353800",
    "end": "360080"
  },
  {
    "text": "together put it behind a web server like Fast API or flask and then package that",
    "start": "360080",
    "end": "365199"
  },
  {
    "text": "all up into a container and deploy it on something like kubernetes I'm sure many of the people in this room have uh built",
    "start": "365199",
    "end": "371000"
  },
  {
    "text": "or seen something like this in the past and this is a really good way to get off the ground right this this looks very similar to just uh deploying a normal",
    "start": "371000",
    "end": "378639"
  },
  {
    "text": "web app or web server but you're pretty quickly going to run into problems right because you",
    "start": "378639",
    "end": "384639"
  },
  {
    "text": "can't scale these models independently um and because machine learning models are often memory and compute intensive",
    "start": "384639",
    "end": "391479"
  },
  {
    "text": "this causes big problems um you end up with high latency low throughput and it's also very expensive especially when",
    "start": "391479",
    "end": "398560"
  },
  {
    "text": "you have many different models and they aren't all used for every",
    "start": "398560",
    "end": "403759"
  },
  {
    "text": "invocation so a more complex solution that addresses the problems um in in the",
    "start": "405160",
    "end": "410400"
  },
  {
    "text": "basic kind of Monolithic solution is to split up our models and treat them more like microservices right so imagine this",
    "start": "410400",
    "end": "417840"
  },
  {
    "text": "architecture where we're still running on kubernetes but now instead of having just one container that's replicated maybe we",
    "start": "417840",
    "end": "424440"
  },
  {
    "text": "split each part of our application into its own kind of service so here we have",
    "start": "424440",
    "end": "429759"
  },
  {
    "text": "maybe a tensorflow model that has uh three containers and a pie torch model that has two and once again we're using",
    "start": "429759",
    "end": "436080"
  },
  {
    "text": "something like Fast API to uh to orchestrate everything and Stitch it all together so this solves our problem",
    "start": "436080",
    "end": "443280"
  },
  {
    "text": "right because now we can leverage kubernetes we can scale these things independently we get a lot of you know",
    "start": "443280",
    "end": "448720"
  },
  {
    "text": "power from this AR architecture but this is really complicated right every single one of",
    "start": "448720",
    "end": "455120"
  },
  {
    "text": "these boxes is a new component that you have to maintain and especially if you have data scientists and ml Engineers",
    "start": "455120",
    "end": "461319"
  },
  {
    "text": "who are trying to build and deploy like their end-to-end applications this is a huge burden to put on them uh it also",
    "start": "461319",
    "end": "468199"
  },
  {
    "text": "makes local development and testing much much harder so today I'm going to tell you",
    "start": "468199",
    "end": "474159"
  },
  {
    "start": "473000",
    "end": "473000"
  },
  {
    "text": "how Ray serve offers a better solution um uh for these types of multimodel INF",
    "start": "474159",
    "end": "479960"
  },
  {
    "text": "applications so with serve you can write a single Python program you can still use your favorite Tools in libraries and",
    "start": "479960",
    "end": "486199"
  },
  {
    "text": "your favorite um uh machine learning libraries like pytorch and tensorflow and then you can seamlessly scale up",
    "start": "486199",
    "end": "492199"
  },
  {
    "text": "across a cluster of CPUs and gpus to build a high performance and efficient inference",
    "start": "492199",
    "end": "499639"
  },
  {
    "text": "API so let's revisit the example that we had for Content understanding and just imagine what it might look like as a",
    "start": "500120",
    "end": "505560"
  },
  {
    "text": "serve application so we may still want to run on kubernetes like many of our users do um but now we",
    "start": "505560",
    "end": "512680"
  },
  {
    "text": "have pods that are managed by rerf our entire application is expressed",
    "start": "512680",
    "end": "518080"
  },
  {
    "text": "as a a single Python program and it can be flexibly deployed across our Ray",
    "start": "518080",
    "end": "523399"
  },
  {
    "text": "cluster that's running Ray serf we can allocate resources uh in a",
    "start": "523399",
    "end": "529279"
  },
  {
    "text": "very flexible way so we might have some of our nodes that have a GPU and CPUs like the top one and some of them may",
    "start": "529279",
    "end": "536040"
  },
  {
    "text": "only have CPUs and we can also have the same kind of flexible resource allocation for each",
    "start": "536040",
    "end": "541680"
  },
  {
    "text": "individual model um so in this case maybe our image classifiers require a third of a GPU and a CPU where the",
    "start": "541680",
    "end": "548880"
  },
  {
    "text": "object detector only requires a CPU and the key here is that this entire",
    "start": "548880",
    "end": "554560"
  },
  {
    "text": "picture is a single Python program you can develop it and test it locally using",
    "start": "554560",
    "end": "559720"
  },
  {
    "text": "your local IDE and just running it on your laptop and when it's time to deploy it to production you can deploy it and",
    "start": "559720",
    "end": "565760"
  },
  {
    "text": "update it as just a single application but still get all the benefits of running on a cluster uh and having all",
    "start": "565760",
    "end": "572120"
  },
  {
    "text": "the power of the flexible resource allocation and a high performance",
    "start": "572120",
    "end": "577200"
  },
  {
    "text": "API so with rate 2.0 our goal is really simple we want to make it as easy as",
    "start": "577480",
    "end": "582680"
  },
  {
    "text": "possible to put these types of scalable machine learning applications into",
    "start": "582680",
    "end": "588279"
  },
  {
    "text": "production um so the uh the first goal here is we want to have a really great",
    "start": "588399",
    "end": "595040"
  },
  {
    "text": "user experience for doing flexible model composition and building End to End",
    "start": "595040",
    "end": "600760"
  },
  {
    "text": "online inference apis uh instead of requiring you know handwriting a yaml",
    "start": "600760",
    "end": "605800"
  },
  {
    "text": "file or stitching together many different microservices or something like that we also want to make sure to uh",
    "start": "605800",
    "end": "613399"
  },
  {
    "text": "improve efficiency and save costs and we do that by uh adding Advanced autoscaling um",
    "start": "613399",
    "end": "620320"
  },
  {
    "text": "functionality and then finally we've been really really focused on production hardening and making sure that race",
    "start": "620320",
    "end": "626480"
  },
  {
    "text": "serve is Rock Solid in production and especially on kubernetes as we see more and more of our users uh are are moving",
    "start": "626480",
    "end": "633279"
  },
  {
    "text": "towards kubernetes so let's dive into the first",
    "start": "633279",
    "end": "638399"
  },
  {
    "text": "uh the first point here which is the racer model composition API so I talked a little bit about this",
    "start": "638399",
    "end": "644399"
  },
  {
    "start": "643000",
    "end": "643000"
  },
  {
    "text": "in the intro um but the requirements for a model composition API as we see it are",
    "start": "644399",
    "end": "651000"
  },
  {
    "text": "uh first of all we want to be flexible um I think machine learning is very much an uh an emerging domain and we see a",
    "start": "651000",
    "end": "658079"
  },
  {
    "text": "lot of different framework different types of models and often times a lot of custom business logic required to stitch",
    "start": "658079",
    "end": "663800"
  },
  {
    "text": "everything together we want to make sure to satisfy all of that we also think it's absolutely",
    "start": "663800",
    "end": "669519"
  },
  {
    "text": "crucial that you can uh develop test and debug your applications locally and finally and and maybe",
    "start": "669519",
    "end": "676399"
  },
  {
    "text": "obviously you have to be scalable and you have to be efficient when it's time to run in",
    "start": "676399",
    "end": "682200"
  },
  {
    "text": "production so our solution is the model composition API which we've put a lot of work into with the ray 2.0 release",
    "start": "682279",
    "end": "690079"
  },
  {
    "start": "683000",
    "end": "683000"
  },
  {
    "text": "so we've uh created this API um to build graphs which is um you know just uh",
    "start": "690079",
    "end": "696240"
  },
  {
    "text": "multiple models that can be stitched together with custom logic of sered deployments which is an existing concept",
    "start": "696240",
    "end": "701959"
  },
  {
    "text": "of uh each individual model in an application you can still use the full",
    "start": "701959",
    "end": "707079"
  },
  {
    "text": "flexibility of Ray serve so the fine grain resource allocation and uh all of the the functionality to make your",
    "start": "707079",
    "end": "713360"
  },
  {
    "text": "application efficient um so you can author configure and scale each model independently",
    "start": "713360",
    "end": "721079"
  },
  {
    "text": "and then you can orchestrate the computation and Stitch together the models just using regular python code",
    "start": "721200",
    "end": "726320"
  },
  {
    "text": "you don't have to worry about a like grpc protocol or setting up ingresses or setting up kubernetes services or",
    "start": "726320",
    "end": "732760"
  },
  {
    "text": "anything like that it's just a normal method call so let's dive into a couple of the",
    "start": "732760",
    "end": "740079"
  },
  {
    "start": "738000",
    "end": "738000"
  },
  {
    "text": "patterns that we see uh for model composition and how we can uh solve them with this API so the first one is quite",
    "start": "740079",
    "end": "746800"
  },
  {
    "text": "simple um imagine that you just have multiple models that you want to chain together so you have some kind of input",
    "start": "746800",
    "end": "752720"
  },
  {
    "text": "you want to pass that to a first model and then take the output and pass it to a",
    "start": "752720",
    "end": "758240"
  },
  {
    "text": "second this is really easy to express in the API um so we're basically just going to use this bind call to Define each one",
    "start": "758240",
    "end": "765160"
  },
  {
    "text": "of our models so here we're defining uh M1 and M2 which maybe they have",
    "start": "765160",
    "end": "770240"
  },
  {
    "text": "different uh Uris for ML flow or or the model is saved in S3 or something like",
    "start": "770240",
    "end": "775360"
  },
  {
    "text": "that and then we just write a block of ordinary python call code that defines",
    "start": "775360",
    "end": "780880"
  },
  {
    "text": "the orchestration logic so here we take the input we pass it to M1 we take the",
    "start": "780880",
    "end": "786399"
  },
  {
    "text": "output of M1 and we pass it to M2 and that's all there is to it so you can extend this to also",
    "start": "786399",
    "end": "793880"
  },
  {
    "text": "Implement something like ensembling where you want to broadcast the input to multiple models and then combine it uh",
    "start": "793880",
    "end": "799560"
  },
  {
    "text": "maybe also using custom business logic so in this case we initialize the models in just the same way uh but",
    "start": "799560",
    "end": "807079"
  },
  {
    "text": "instead of chaining the the output of M1 into M2 instead we just feed the input",
    "start": "807079",
    "end": "813160"
  },
  {
    "text": "into both of them and then we pass the output of both of them into a custom combined function and note that this",
    "start": "813160",
    "end": "820160"
  },
  {
    "text": "function uh and it could also be a class is just ordinary python code you can do anything you want in inside of it um you",
    "start": "820160",
    "end": "827040"
  },
  {
    "text": "could use another Library you could just write some business logic with if and else statements um anything you want in",
    "start": "827040",
    "end": "834199"
  },
  {
    "text": "a in a normal python function and finally uh one of the the",
    "start": "834199",
    "end": "840040"
  },
  {
    "text": "the patterns that we're really excited about because we've heard it a lot from our users is this idea of dynamic selection so sometimes based on the",
    "start": "840040",
    "end": "847480"
  },
  {
    "text": "input or based on other parameters like what user sent the request you actually want to select a different model to",
    "start": "847480",
    "end": "853839"
  },
  {
    "text": "handle the request because Ray serve lets you just use ordinary python code um in order to",
    "start": "853839",
    "end": "860600"
  },
  {
    "text": "write each node in this graph this is also really easy to express so here we're creating M1 and M2 like we did",
    "start": "860600",
    "end": "867600"
  },
  {
    "text": "earlier but we're also passing those into another node which we're calling the selector and this selector is going",
    "start": "867600",
    "end": "874360"
  },
  {
    "text": "to at runtime take the input and decide which one of those it wants to pass uh",
    "start": "874360",
    "end": "879800"
  },
  {
    "text": "which one of those it wants to call based on the input or based on the user ID or anything like that um and then you",
    "start": "879800",
    "end": "887680"
  },
  {
    "text": "uh just get the output from the selector so if we revisit this uh this",
    "start": "887680",
    "end": "894240"
  },
  {
    "text": "working example we had before of the content understanding application you can see that uh in this",
    "start": "894240",
    "end": "900000"
  },
  {
    "text": "example we actually use all three of these patterns so we chain the downloader and the pre-processor uh we",
    "start": "900000",
    "end": "905800"
  },
  {
    "text": "then do uh onsemble to uh do both classification and image detection and",
    "start": "905800",
    "end": "910920"
  },
  {
    "text": "we're also doing Dynamic selection within the classifiers and we can do this just by composing these really",
    "start": "910920",
    "end": "917199"
  },
  {
    "text": "simple uh um Primitives that I showed you in the past couple of slides and this is really expressive it allows you",
    "start": "917199",
    "end": "923800"
  },
  {
    "text": "to uh to create all sorts of really complex topologies and do it with really simple",
    "start": "923800",
    "end": "930959"
  },
  {
    "start": "931000",
    "end": "931000"
  },
  {
    "text": "code so to summarize uh the model composition API lets you write your models as ordinary classes and then you",
    "start": "931720",
    "end": "939160"
  },
  {
    "text": "can flexibly compose the models and business logic just using python code you can run test and debug the",
    "start": "939160",
    "end": "946880"
  },
  {
    "text": "entire end to-end application on your laptop and then when you deploy a",
    "start": "946880",
    "end": "952120"
  },
  {
    "text": "production you can configure and scale all of these models independently across a cluster",
    "start": "952120",
    "end": "959680"
  },
  {
    "text": "So speaking of scaling uh let's dive into the second highlight for Ray 2.0 uh which is",
    "start": "960519",
    "end": "966680"
  },
  {
    "text": "autoscaling so obviously one of the key requirements for uh for surveying machine learning models in production is",
    "start": "966680",
    "end": "972480"
  },
  {
    "start": "967000",
    "end": "967000"
  },
  {
    "text": "to reduce cost right machine learning models are very compute intensive and running gpus and CPUs 24x7 uh really",
    "start": "972480",
    "end": "979680"
  },
  {
    "text": "adds up very quickly so in addition we have some other requirements you know many times",
    "start": "979680",
    "end": "986000"
  },
  {
    "text": "not all of the models are used at all times you might have have a periodic cycle throughout the day or you might",
    "start": "986000",
    "end": "991759"
  },
  {
    "text": "have just different traffic patterns from users um that are variable throughout the",
    "start": "991759",
    "end": "997399"
  },
  {
    "text": "day it can also be really hard to tune Hardware utilization targets which is what a lot of uh autoscaling algorithms",
    "start": "997399",
    "end": "1004680"
  },
  {
    "text": "are based on uh this might require profiling your code really understanding low-level details or even like rewriting",
    "start": "1004680",
    "end": "1011160"
  },
  {
    "text": "your models to get them to work with uh with Hardware utilization based",
    "start": "1011160",
    "end": "1016199"
  },
  {
    "text": "approaches and finally it's also really crucial that uh autoscaling Works um",
    "start": "1016199",
    "end": "1022199"
  },
  {
    "text": "with multimodel or model composition applications like the ones that I just described so our solution is to add uh",
    "start": "1022199",
    "end": "1029640"
  },
  {
    "text": "Advanced autoscaling features to rerf so this supports uh scaling down to",
    "start": "1029640",
    "end": "1034959"
  },
  {
    "text": "zero which means that you can uh have models that are not occupying any resources until requests come in for",
    "start": "1034959",
    "end": "1041400"
  },
  {
    "text": "that specific model it uses request Q lengths instead of Hardware utilization which means you",
    "start": "1041400",
    "end": "1047319"
  },
  {
    "text": "don't have to do any profiling or rewriting your code at all um and it pretty much just works out of the",
    "start": "1047319",
    "end": "1052960"
  },
  {
    "text": "box and it's also fully compatible with the model composition API that I just got done explaining uh which means that",
    "start": "1052960",
    "end": "1059600"
  },
  {
    "text": "you can independently autoscale each model uh in your end to-end",
    "start": "1059600",
    "end": "1065120"
  },
  {
    "text": "application so let's see how this might work for uh the example that I've been I've been using throughout the talk so",
    "start": "1065120",
    "end": "1072039"
  },
  {
    "text": "let's focus in on this uh image classifier so you could imagine that maybe based on the user request pattern",
    "start": "1072039",
    "end": "1079200"
  },
  {
    "text": "we're sending a lot more images to classifier number one so in this case classifier number one is overloaded",
    "start": "1079200",
    "end": "1085200"
  },
  {
    "text": "there's a long CU built up which means latency is high uh and response times are",
    "start": "1085200",
    "end": "1090400"
  },
  {
    "text": "long so in this case um race serve would see that the Q size for the classifier number one is is obviously much longer",
    "start": "1090400",
    "end": "1097720"
  },
  {
    "text": "than the target we want to keep these cues short so that we can keep response times down uh oops I went one slide too much",
    "start": "1097720",
    "end": "1105679"
  },
  {
    "text": "um so in this case we might add more replicas and this would happen uh completely automatically so now we have",
    "start": "1105679",
    "end": "1111559"
  },
  {
    "text": "four replicas of IM image classifier number one and the load is is uh is",
    "start": "1111559",
    "end": "1116640"
  },
  {
    "text": "balance across the replicas at the same time notice that image classifier number three hasn't",
    "start": "1116640",
    "end": "1122120"
  },
  {
    "text": "been used so after some idle time out uh we would also scale this this classifier",
    "start": "1122120",
    "end": "1127919"
  },
  {
    "text": "number three down and at this point we have zero replicas of that model this wouldn't get Scaled up uh until we",
    "start": "1127919",
    "end": "1133960"
  },
  {
    "text": "actually have a request to use that model which is really important for applications that have uh very sparse",
    "start": "1133960",
    "end": "1139840"
  },
  {
    "text": "invocation patterns um and the key here is this is",
    "start": "1139840",
    "end": "1145440"
  },
  {
    "text": "really easy to get started all you have to do is set up a few basic parameters you don't have to set up a a metrics",
    "start": "1145440",
    "end": "1151320"
  },
  {
    "text": "inest stack you don't have to profile your code and you don't have to to change your models or anything like that",
    "start": "1151320",
    "end": "1156919"
  },
  {
    "text": "and we also support scale to zero and it integrates fully with model composition uh so you can see a sample of of an",
    "start": "1156919",
    "end": "1162799"
  },
  {
    "text": "autoscaling config at the bottom okay so let's dive into the third",
    "start": "1162799",
    "end": "1168440"
  },
  {
    "text": "highlight which is uh our efforts around production hardening so obviously putting machine",
    "start": "1168440",
    "end": "1174720"
  },
  {
    "start": "1172000",
    "end": "1172000"
  },
  {
    "text": "learning models uh into production means you have to solve some really serious operational problems that aren't related",
    "start": "1174720",
    "end": "1180760"
  },
  {
    "text": "to machine learning um so you need to handle updating your code and your models without downtime uh as you scale",
    "start": "1180760",
    "end": "1187480"
  },
  {
    "text": "up failures get more likely and you need to be able to handle them gracefully uh and you also need really good monitoring",
    "start": "1187480",
    "end": "1193480"
  },
  {
    "text": "observability and alerting for when things do go wrong so our goal uh that",
    "start": "1193480",
    "end": "1198799"
  },
  {
    "text": "we've been working towards is really to try to get the best of both worlds and get the flexibility the user experience",
    "start": "1198799",
    "end": "1205039"
  },
  {
    "text": "and the efficiency of race serve but also provide the operational benefits of a system like kubernetes so of course",
    "start": "1205039",
    "end": "1212200"
  },
  {
    "text": "racer has continue to be supported on other Stacks but we're really doubling down and trying to make it integrate as",
    "start": "1212200",
    "end": "1217799"
  },
  {
    "text": "well as we can uh with kubernetes so our first major effort is",
    "start": "1217799",
    "end": "1225159"
  },
  {
    "text": "is uh has been around building a dedicated operator for Ray serve into",
    "start": "1225159",
    "end": "1230720"
  },
  {
    "text": "Cube um which you may have heard about at other talks today so let's uh look at",
    "start": "1230720",
    "end": "1235799"
  },
  {
    "text": "an example of this so imagine that you have your python application running on your laptop so you might do the",
    "start": "1235799",
    "end": "1241600"
  },
  {
    "text": "development Loop here uh on the left and then when you're ready to deploy you'd use this new command serve build what",
    "start": "1241600",
    "end": "1248280"
  },
  {
    "text": "serve build does is takes your your application and basically just transforms it into a structured format",
    "start": "1248280",
    "end": "1255200"
  },
  {
    "text": "that can be embedded inside of a kubernetes manifest this is really important because it",
    "start": "1255200",
    "end": "1260840"
  },
  {
    "text": "means now we can apply that manifest directly to the cluster so we can deploy",
    "start": "1260840",
    "end": "1267240"
  },
  {
    "text": "upgrade and monitor our race serve application using standard kubernetes tooling uh just using Cube",
    "start": "1267240",
    "end": "1274400"
  },
  {
    "text": "CTL so what's actually happening over here is we have a ray service operator like I said that's part of Cu that will",
    "start": "1274400",
    "end": "1281120"
  },
  {
    "text": "read this manifest and it'll make changes in the cluster to uh create Ray clusters deploy your serve application",
    "start": "1281120",
    "end": "1288240"
  },
  {
    "text": "to them and then also handle updates and monitoring over time and this comes with really crucial",
    "start": "1288240",
    "end": "1294760"
  },
  {
    "text": "features like zero downtime updates uh it will automatically health check and recover from failures um and like I said",
    "start": "1294760",
    "end": "1301640"
  },
  {
    "text": "earlier it also integrates really cleanly with other kubernetes tooling uh and makes it really easy to build uh for",
    "start": "1301640",
    "end": "1307960"
  },
  {
    "text": "example cicd or automation workflows with",
    "start": "1307960",
    "end": "1312440"
  },
  {
    "text": "racer so another effort uh in order to improve production hardening has been",
    "start": "1313039",
    "end": "1318080"
  },
  {
    "text": "GCS fault tolerance um so this has been a huge effort jointly between the ray",
    "start": "1318080",
    "end": "1323919"
  },
  {
    "text": "serve and the ray core team uh if you're not intimately familiar with uh the ray architecture uh prior to Ray 2.0 it had",
    "start": "1323919",
    "end": "1332200"
  },
  {
    "text": "a single point of failure so on the ray head node we had this uh process called the GCS which is essentially uh like the",
    "start": "1332200",
    "end": "1339960"
  },
  {
    "text": "global coordinator for the cluster it handles scheduling um and health checking actors and things like that and",
    "start": "1339960",
    "end": "1346840"
  },
  {
    "text": "if this were to fail um which is unlikely because there's only a single copy of it uh but may happen over time",
    "start": "1346840",
    "end": "1353559"
  },
  {
    "text": "if this were to fail or the head node were to fail it would uh take down the entire cluster so of course you could",
    "start": "1353559",
    "end": "1360400"
  },
  {
    "text": "replicate the cluster and have multiple copies or you could um work around it by",
    "start": "1360400",
    "end": "1365880"
  },
  {
    "text": "uh Health checking the cluster and restarting it when when this did happen obviously this is not a good situation",
    "start": "1365880",
    "end": "1371400"
  },
  {
    "text": "for running a a production online inference API so in rate 2.0 we're really excited",
    "start": "1371400",
    "end": "1378919"
  },
  {
    "text": "to announce that uh we put in a ton of work and made it so that Ray can recover from GCS",
    "start": "1378919",
    "end": "1384200"
  },
  {
    "text": "failures so if the GCS fails you can actually still continue to run critical functions um like tasks and actors will",
    "start": "1384200",
    "end": "1391640"
  },
  {
    "text": "continue to run uninterrupted and uh when running on kubernetes a new GCS will automatically",
    "start": "1391640",
    "end": "1398120"
  },
  {
    "text": "be started and the cluster will recover to uh to a completely operational State and the really important part here",
    "start": "1398120",
    "end": "1405760"
  },
  {
    "text": "is that we put in a lot of work to make it so that race service applications will continue to serve traffic uh",
    "start": "1405760",
    "end": "1411880"
  },
  {
    "text": "without any disruption during this GCS downtime and to to you know show that",
    "start": "1411880",
    "end": "1419120"
  },
  {
    "start": "1417000",
    "end": "1417000"
  },
  {
    "text": "this really works um we've actually been doing a lot of uh of chaos testing um so",
    "start": "1419120",
    "end": "1424960"
  },
  {
    "text": "basically what we do is we run a sample race serve application every minute we randomly kill a node in the cluster um",
    "start": "1424960",
    "end": "1431760"
  },
  {
    "text": "so obviously this is a really stressful example I would hope that this doesn't happen in your production cluster um and",
    "start": "1431760",
    "end": "1438039"
  },
  {
    "text": "at the same time we're running a load test so here on the top you can see uh the success versus failure rate um and",
    "start": "1438039",
    "end": "1445240"
  },
  {
    "text": "you can see that even though we're uh killing nodes repeatedly as this Benchmark is running we're able to",
    "start": "1445240",
    "end": "1451360"
  },
  {
    "text": "maintain more than 99.99% uptime and by the way killing nodes also includes the head node so if",
    "start": "1451360",
    "end": "1458240"
  },
  {
    "text": "we were to run this prior to Ray 2.0 and prior to making all these fixes um you would see periods of downtime anytime",
    "start": "1458240",
    "end": "1464720"
  },
  {
    "text": "that the head node went down so we're super excited about this feature has been a long-standing request and we",
    "start": "1464720",
    "end": "1470720"
  },
  {
    "text": "really think it's going to reduce the operational burden for a lot of our",
    "start": "1470720",
    "end": "1476000"
  },
  {
    "text": "users so to summarize uh um the state of race sering 2.0 um so our goal we want",
    "start": "1476640",
    "end": "1483360"
  },
  {
    "text": "to make it as easy as possible to put scalable machine learning applications into production um so we've doubled down",
    "start": "1483360",
    "end": "1490120"
  },
  {
    "text": "and and really tried to improve our user experience for doing model composition uh We've also added Advanced",
    "start": "1490120",
    "end": "1496640"
  },
  {
    "text": "Autos scaling features to help you save cost and reduce resources and we've put",
    "start": "1496640",
    "end": "1501880"
  },
  {
    "text": "a huge amount of work into production hardening um with an increased focus on making it really easy to deploy update",
    "start": "1501880",
    "end": "1508600"
  },
  {
    "text": "and manage your applications on kubernetes so we have a number of other",
    "start": "1508600",
    "end": "1514360"
  },
  {
    "text": "talks at the summit some of which happened earlier today that are deep dives into uh some of the topics that I",
    "start": "1514360",
    "end": "1519559"
  },
  {
    "text": "talked about here um so shus will be talking about operational operationalizing Racer V kubernetes",
    "start": "1519559",
    "end": "1525679"
  },
  {
    "text": "tomorrow and if you didn't have a chance to catch the first two uh they'll be available the recordings will be",
    "start": "1525679",
    "end": "1531559"
  },
  {
    "text": "available after the summit we also had two talks uh from the community about how they've used RAC",
    "start": "1531559",
    "end": "1538279"
  },
  {
    "text": "serve um and these will also be available after the",
    "start": "1538279",
    "end": "1543840"
  },
  {
    "text": "summit uh so that's the end of the talk uh thank you and I'm happy to take",
    "start": "1544039",
    "end": "1550140"
  },
  {
    "text": "[Applause]",
    "start": "1550140",
    "end": "1555320"
  },
  {
    "text": "questions oh uh sorry if you're going to ask ask a question can you just keep your hand raised and Thomas will bring",
    "start": "1555320",
    "end": "1560520"
  },
  {
    "text": "you a microphone go ahead if you're scaling up from zero What's the cold start time for the underline compute to",
    "start": "1560520",
    "end": "1566840"
  },
  {
    "text": "warm up yeah so that's a very complicated question um because it it depends on a lot of factors um if it's",
    "start": "1566840",
    "end": "1574520"
  },
  {
    "text": "if you already have the compute and the model is sitting in the object store it can be um well under a second and",
    "start": "1574520",
    "end": "1581240"
  },
  {
    "text": "actually Fred um the community talk that I had about the zeroc copy model loading",
    "start": "1581240",
    "end": "1586279"
  },
  {
    "text": "um he talked a lot about how to optimiz this cold start time um if you've scaled",
    "start": "1586279",
    "end": "1591440"
  },
  {
    "text": "down and you have to provision a node from your cloud provider it could take much longer um so it depends a little",
    "start": "1591440",
    "end": "1596760"
  },
  {
    "text": "bit on your setup but optimizing this is has been a consistent feature request and that's something that we plan to",
    "start": "1596760",
    "end": "1602279"
  },
  {
    "text": "work on going forward uh in the",
    "start": "1602279",
    "end": "1609240"
  },
  {
    "text": "back hi um my question is around autoscaling so you mentioned that uh the",
    "start": "1610120",
    "end": "1615360"
  },
  {
    "text": "Q length is used as a urtic to make scaling decision is this customizable in the sense that",
    "start": "1615360",
    "end": "1621399"
  },
  {
    "text": "um Q lens may not always be you know very informative in terms of like how uh",
    "start": "1621399",
    "end": "1627960"
  },
  {
    "text": "input uh how inputs may vary like a large queue may not always need a",
    "start": "1627960",
    "end": "1634200"
  },
  {
    "text": "scaling up whereas like a sh or Q length may need something so I'm just wondering if that's customizable yeah yeah so it",
    "start": "1634200",
    "end": "1640640"
  },
  {
    "text": "it is uh very customizable so our goal was to really make it so that you know make simple things simple so make it so",
    "start": "1640640",
    "end": "1647039"
  },
  {
    "text": "that most applications without any customization but we do have a range of parameters that you can tune um for",
    "start": "1647039",
    "end": "1653120"
  },
  {
    "text": "different situations and yeah I encourage you to check out the documentation we have a page that details all of the the",
    "start": "1653120",
    "end": "1660240"
  },
  {
    "text": "parameters there's one in the middle here hi um so on the same subject of uh",
    "start": "1660240",
    "end": "1668200"
  },
  {
    "text": "oal scaling um is it dependent on kubernetes doing the auto scaling or is it has its",
    "start": "1668200",
    "end": "1676240"
  },
  {
    "text": "own logic that super see or how does it interact um in cand yeah so um the",
    "start": "1676240",
    "end": "1685399"
  },
  {
    "text": "autoscaler that I talked about here is at the race serve level so it'll change like it'll adjust the number of replicas",
    "start": "1685399",
    "end": "1691480"
  },
  {
    "text": "that you have for a given model uh that relies on an underlying cluster level autoscaler to actually uh provision",
    "start": "1691480",
    "end": "1698559"
  },
  {
    "text": "nodes uh that satisfy that request um that is uh like fully featured inside of",
    "start": "1698559",
    "end": "1705559"
  },
  {
    "text": "Ray and inside of cube Ray the oper operator um so yeah there a few layers",
    "start": "1705559",
    "end": "1711720"
  },
  {
    "text": "here uh but um yeah cubay and like racer",
    "start": "1711720",
    "end": "1717720"
  },
  {
    "text": "plus Cub uh provides like the full",
    "start": "1717720",
    "end": "1722039"
  },
  {
    "text": "functionality uh yeah they're independently configured you could use one without the well you could use the Cub autoscaler without serve but not VI",
    "start": "1724799",
    "end": "1733840"
  },
  {
    "text": "Versa uh on the right over here yeah hey on the first slide I saw um that you had",
    "start": "1733840",
    "end": "1741399"
  },
  {
    "text": "some Integrations with uh different data apps uh like gradio or or streamlet I",
    "start": "1741399",
    "end": "1746960"
  },
  {
    "text": "was wondering like what does that process actually look like to like develop a data app on top of um the",
    "start": "1746960",
    "end": "1753000"
  },
  {
    "text": "models that you build here yeah so uh we've actually been working we have a project right now uh collaboration with",
    "start": "1753000",
    "end": "1759399"
  },
  {
    "text": "the gradio team um to make it so that you can uh really easily take a well two",
    "start": "1759399",
    "end": "1765760"
  },
  {
    "text": "things one you can take a gradio application and you can just put it on top of race serve and just like scale it",
    "start": "1765760",
    "end": "1770960"
  },
  {
    "text": "up so if you had like a multimodel gradio application it will um you can increase the throughput reduce the",
    "start": "1770960",
    "end": "1777080"
  },
  {
    "text": "latency uh and that should basically be like a drag and drop like no changes uh",
    "start": "1777080",
    "end": "1782360"
  },
  {
    "text": "we're also trying to make it so that uh you can use gradio as a UI for an",
    "start": "1782360",
    "end": "1788120"
  },
  {
    "text": "existing race serve application um so both of those are kind of in the works like experimental stage but we do have",
    "start": "1788120",
    "end": "1794440"
  },
  {
    "text": "documentation for them and I would encourage you to check it out and and file an issue if you have",
    "start": "1794440",
    "end": "1800799"
  },
  {
    "text": "feedback uh yeah on the front left getting Thomas's exercise",
    "start": "1801600",
    "end": "1807240"
  },
  {
    "text": "today at the end of the day right so what about the eks sorry the overall",
    "start": "1807240",
    "end": "1813240"
  },
  {
    "text": "kubernetes is it going to be the defao for a or you developing your own kind of a much optimized Auto scaling yeah so um",
    "start": "1813240",
    "end": "1821399"
  },
  {
    "text": "this is kind of a bigger question than racer but in general we we are like basically doubling down on kubernetes um",
    "start": "1821399",
    "end": "1827720"
  },
  {
    "text": "we see a huge portion of our users uh that are currently on kubernetes or have plans to move to it uh so same as I said",
    "start": "1827720",
    "end": "1834840"
  },
  {
    "text": "in the talk like we're going to continue to support other Stacks because you know we can't we can't only work on",
    "start": "1834840",
    "end": "1839960"
  },
  {
    "text": "kubernetes uh but we want to make it as seamless as possible and and we don't plan to to build like our own like",
    "start": "1839960",
    "end": "1847240"
  },
  {
    "text": "kubernetes competitor cluster management layer or something like",
    "start": "1847240",
    "end": "1851840"
  },
  {
    "text": "that is that a requirement or you can have just just kubernetes is a requirement no um so Ry has a built-in",
    "start": "1853279",
    "end": "1860840"
  },
  {
    "text": "cluster launcher that works with a bunch of different backends you can use ec2 instances you can use uh Google VMS you",
    "start": "1860840",
    "end": "1868120"
  },
  {
    "text": "can also use Azure um and a lot of those are Community contributed and maintained",
    "start": "1868120",
    "end": "1873360"
  },
  {
    "text": "and if you have a back end that isn't supported then uh we'd love to work with you to get it",
    "start": "1873360",
    "end": "1879639"
  },
  {
    "text": "working okay um I think that's all the questions thank you so much",
    "start": "1882399",
    "end": "1888799"
  }
]