[
  {
    "text": "good afternoon it's really exciting to see so many reuse cases and the growth",
    "start": "2760",
    "end": "9120"
  },
  {
    "text": "of this community I'm paying retweek we're from zukes we're building Duke's",
    "start": "9120",
    "end": "15660"
  },
  {
    "text": "Auto tuning systems and this talk will be divided into two parts first I will talk a little bit",
    "start": "15660",
    "end": "22740"
  },
  {
    "text": "about dukes and the unique challenges we face when scaling self-driving algorithm",
    "start": "22740",
    "end": "28619"
  },
  {
    "text": "developments and how Ray integrates into the Duke's ecosystem",
    "start": "28619",
    "end": "33899"
  },
  {
    "text": "and following that ritwick will be elaborating how we leverage race",
    "start": "33899",
    "end": "40500"
  },
  {
    "text": "capabilities to scale our workload",
    "start": "40500",
    "end": "45079"
  },
  {
    "text": "okay so this is our Robo taxi if you were to spend just one hour on the",
    "start": "45840",
    "end": "52320"
  },
  {
    "text": "street of San Francisco it's very likely that you will come across one or",
    "start": "52320",
    "end": "57840"
  },
  {
    "text": "multiple fully autonomous vehicles you might have",
    "start": "57840",
    "end": "63140"
  },
  {
    "text": "spots some of the zoox testing vehicles but you probably haven't seen this this",
    "start": "63140",
    "end": "69360"
  },
  {
    "text": "is our robot taxi but if you spend some time in Foster City or more recently in",
    "start": "69360",
    "end": "77880"
  },
  {
    "text": "Las Vegas especially around our headquarters you will see our purpose",
    "start": "77880",
    "end": "84180"
  },
  {
    "text": "built robot taxi operating on public Street uh there is no steering wheel because",
    "start": "84180",
    "end": "91860"
  },
  {
    "text": "the vehicle was designed for passengers not drivers our vision is providing",
    "start": "91860",
    "end": "100380"
  },
  {
    "text": "Mobility as a service in dense Urban environments",
    "start": "100380",
    "end": "105720"
  },
  {
    "text": "in order to achieve that Vision we have to address a few core challenges one of",
    "start": "105720",
    "end": "112320"
  },
  {
    "text": "the challenges is scaling algorithm development to a diverse driving scenarios now let's",
    "start": "112320",
    "end": "120479"
  },
  {
    "text": "take a look at a simple example let's consider a typical driving",
    "start": "120479",
    "end": "125820"
  },
  {
    "text": "scenario you are driving Street and there is a vehicle in front and",
    "start": "125820",
    "end": "132000"
  },
  {
    "text": "there are two vehicles in the adjacent Lane what is your move in this situation",
    "start": "132000",
    "end": "138120"
  },
  {
    "text": "well it depends on a combination of factors including your immediate",
    "start": "138120",
    "end": "143459"
  },
  {
    "text": "objective and your predictions about the behaviors of other vehicles",
    "start": "143459",
    "end": "151040"
  },
  {
    "text": "to drive straight and just follow the lead vehicle or if you are looking to accelerate you",
    "start": "151260",
    "end": "158640"
  },
  {
    "text": "can switch to the adjacent Lane and follow the lead vehicle there",
    "start": "158640",
    "end": "163920"
  },
  {
    "text": "or if you're really in a hurry you can even change lanes once more and return",
    "start": "163920",
    "end": "168959"
  },
  {
    "text": "to your original Lane now which of these options is the best",
    "start": "168959",
    "end": "177180"
  },
  {
    "text": "planning algorithm at zukes is quite intricate fundamentally it is based on minimizing",
    "start": "177180",
    "end": "184560"
  },
  {
    "text": "costs this cost can take various forms",
    "start": "184560",
    "end": "189660"
  },
  {
    "text": "but for Simplicity let's consider three safety comfort and progress",
    "start": "189660",
    "end": "197159"
  },
  {
    "text": "our goal is to select the plan with the lowest overall cost",
    "start": "197159",
    "end": "203099"
  },
  {
    "text": "we take into account things like the uncertainty of the world and the hierarchy of the cost types",
    "start": "203099",
    "end": "210840"
  },
  {
    "text": "for this discussion let's focus on a basic linear combination of these three",
    "start": "210840",
    "end": "216540"
  },
  {
    "text": "costs the challenge lies in determining the",
    "start": "216540",
    "end": "223319"
  },
  {
    "text": "value of these weight parameters they don't have a direct physical",
    "start": "223319",
    "end": "228780"
  },
  {
    "text": "interpretation and heuristic rules don't scale very well",
    "start": "228780",
    "end": "233879"
  },
  {
    "text": "guessing then is not a viable solution our goal is to tune them to improve",
    "start": "233879",
    "end": "240540"
  },
  {
    "text": "overall driving performance and ensure scalability across a wide",
    "start": "240540",
    "end": "246299"
  },
  {
    "text": "range of driving scenarios one common practice is to tune them by",
    "start": "246299",
    "end": "254220"
  },
  {
    "text": "hand or monitoring we can choose a driving scenario",
    "start": "254220",
    "end": "260940"
  },
  {
    "text": "select candidate parameters run simulation",
    "start": "260940",
    "end": "266340"
  },
  {
    "text": "analyze simulated behaviors and refine the parameter iteratively until we are",
    "start": "266340",
    "end": "272639"
  },
  {
    "text": "happy with the two in the behavior however many tuning can be a cumbersome",
    "start": "272639",
    "end": "279600"
  },
  {
    "text": "process especially when dealing with the numerous parameters and the wide range",
    "start": "279600",
    "end": "285120"
  },
  {
    "text": "of driving scenarios this approach significantly increases development time",
    "start": "285120",
    "end": "291600"
  },
  {
    "text": "and it could become a never-ending cycle for Developers",
    "start": "291600",
    "end": "297259"
  },
  {
    "text": "our goal is to achieve parameter tuning that is scalable across a wide array of",
    "start": "297900",
    "end": "304680"
  },
  {
    "text": "driving scenarios all without increasing the engineering effort for Developers",
    "start": "304680",
    "end": "311940"
  },
  {
    "text": "this approach not only saves a lot of trial and error time but also allow our",
    "start": "311940",
    "end": "317340"
  },
  {
    "text": "developers to focus on refining the core algorithms",
    "start": "317340",
    "end": "323120"
  },
  {
    "text": "our strategy is closing the loop and automating the entire process",
    "start": "324840",
    "end": "331199"
  },
  {
    "text": "instead of manually observing simulation results we execute simulations across a",
    "start": "331199",
    "end": "337500"
  },
  {
    "text": "wet spectrum of driving scenarios collect relevant metrics and utilize",
    "start": "337500",
    "end": "344340"
  },
  {
    "text": "this metrics as criteria for optimizing the parameters",
    "start": "344340",
    "end": "349440"
  },
  {
    "text": "one of the challenges is simulation and metrics computation of many scenarios",
    "start": "349440",
    "end": "355440"
  },
  {
    "text": "within a limited time frame because we have limited compute resource",
    "start": "355440",
    "end": "361740"
  },
  {
    "text": "given the iterative nature of the parameter optimization process we need to simulate the same set of",
    "start": "361740",
    "end": "369180"
  },
  {
    "text": "scenarios hundreds of times with different parameters leading to millions of",
    "start": "369180",
    "end": "375600"
  },
  {
    "text": "simulations within this twin loop we will cover how to scale this workload",
    "start": "375600",
    "end": "382080"
  },
  {
    "text": "in the later portion of this talk to illustrate let's consider one",
    "start": "382080",
    "end": "390539"
  },
  {
    "text": "instance of a end-to-end auditing run we use 6000 training scenarios and the",
    "start": "390539",
    "end": "398759"
  },
  {
    "text": "process takes 200 iterations and approximately 30 hours on 800 node",
    "start": "398759",
    "end": "405180"
  },
  {
    "text": "cluster to complete because we simulate the 6000 scenarios",
    "start": "405180",
    "end": "411600"
  },
  {
    "text": "200 times in the optimization Loop so the total number of simulated",
    "start": "411600",
    "end": "417120"
  },
  {
    "text": "scenario is more than a million by using this approach we have achieved",
    "start": "417120",
    "end": "424740"
  },
  {
    "text": "a 20 boost in safety metrics 30 Improvement in Comfort metrics and a",
    "start": "424740",
    "end": "432120"
  },
  {
    "text": "5x Improvement in cost efficiency it is worth noting that the Improvement",
    "start": "432120",
    "end": "439139"
  },
  {
    "text": "in cost efficiency is a direct result of our integration of Ray and the",
    "start": "439139",
    "end": "445979"
  },
  {
    "text": "infrastructure we have built around it which will be covered in more details",
    "start": "445979",
    "end": "451139"
  },
  {
    "text": "later on",
    "start": "451139",
    "end": "453620"
  },
  {
    "text": "this video provides insights into the diverse range of scenarios we use for",
    "start": "456479",
    "end": "463199"
  },
  {
    "text": "training including some age cases and some common",
    "start": "463199",
    "end": "468360"
  },
  {
    "text": "situations we usually encounter in cities such as San Francisco",
    "start": "468360",
    "end": "474660"
  },
  {
    "text": "for example double parked cars we must account for all of these",
    "start": "474660",
    "end": "479759"
  },
  {
    "text": "scenarios in our training data set to achieve scalability",
    "start": "479759",
    "end": "485340"
  },
  {
    "text": "and we need to ensure these scenarios are well distributed in our train set",
    "start": "485340",
    "end": "492860"
  },
  {
    "text": "before we transition to the second part of this talk I'd like to give a brief overview of our platform",
    "start": "496620",
    "end": "504240"
  },
  {
    "text": "it is fully configurable allowing developers to select data sets",
    "start": "504240",
    "end": "509520"
  },
  {
    "text": "labels for training or validation specified parameters for tuning and",
    "start": "509520",
    "end": "515580"
  },
  {
    "text": "Define objective and constrained metrics result can be tracked using internal",
    "start": "515580",
    "end": "521580"
  },
  {
    "text": "tools or external platforms such as with some biases and mlflow",
    "start": "521580",
    "end": "528120"
  },
  {
    "text": "rate and retune play pivotal rules as our orchestration layer",
    "start": "528120",
    "end": "533820"
  },
  {
    "text": "and ritwick will be explaining how we leverage race capabilities to scale our",
    "start": "533820",
    "end": "539820"
  },
  {
    "text": "workload oops",
    "start": "539820",
    "end": "546800"
  },
  {
    "text": "hopefully this is working and everyone can hear me fine okay cool thank you and",
    "start": "547620",
    "end": "553440"
  },
  {
    "text": "bang uh yeah so as this mentioned to enable auto tuning of parameters for",
    "start": "553440",
    "end": "560459"
  },
  {
    "text": "this driving stack we need a fast turnaround time between iterations so in this segment I'll basically go over how",
    "start": "560459",
    "end": "567420"
  },
  {
    "text": "having a real bad platform gives you that fast turnaround time and how you can do that repeatedly in a reliable",
    "start": "567420",
    "end": "573420"
  },
  {
    "text": "manner first we'll talk about resource over execution just to phrase a lot of you",
    "start": "573420",
    "end": "580560"
  },
  {
    "text": "might think what is this so race scheduling allows us to maximize our resource utilization and you know use",
    "start": "580560",
    "end": "588180"
  },
  {
    "text": "whatever is available our workloads are extremely heterogeneous it's not just simulations we do a lot of other",
    "start": "588180",
    "end": "593880"
  },
  {
    "text": "computations with simulations and that's where race pipelining comes into the picture",
    "start": "593880",
    "end": "599880"
  },
  {
    "text": "so our jobs at zukes and most of them being CI jobs involve thousands of",
    "start": "599880",
    "end": "605760"
  },
  {
    "text": "individual tasks each task involves a driving stack being tested in a",
    "start": "605760",
    "end": "610800"
  },
  {
    "text": "simulated scenario and successfully Implement successfully evaluated for Drive quality and performance like these",
    "start": "610800",
    "end": "617700"
  },
  {
    "text": "simulation tasks often need like one CPU and one GPU to run and then Computing",
    "start": "617700",
    "end": "623040"
  },
  {
    "text": "the behavior and running a bunch of other metrics computations let's say it needs two CPUs to run",
    "start": "623040",
    "end": "629279"
  },
  {
    "text": "if we take the example of a single physical host you have two scenarios",
    "start": "629279",
    "end": "634860"
  },
  {
    "text": "you can see that the simulation task of scenario one starts running first and then as soon as it finishes the GPU is",
    "start": "634860",
    "end": "641820"
  },
  {
    "text": "freed and it begins to simulate the next scenario the scenario one simulation result is",
    "start": "641820",
    "end": "647760"
  },
  {
    "text": "then offloaded onto the background CPUs that are available and then metrics",
    "start": "647760",
    "end": "653519"
  },
  {
    "text": "computation begins on those so you're never wasting any amount of GPU time and this is pretty critical",
    "start": "653519",
    "end": "659279"
  },
  {
    "text": "because this is two scenarios but when you scale this to 6000 you save 20",
    "start": "659279",
    "end": "664800"
  },
  {
    "text": "minutes out of one R and when you have a hundred jobs running every day that",
    "start": "664800",
    "end": "669899"
  },
  {
    "text": "amounts to lower AWS spends but pipelining computation is just the",
    "start": "669899",
    "end": "675180"
  },
  {
    "text": "start we also have other Force multipliers more arrows in our quiver so to speak to increase the throughput of",
    "start": "675180",
    "end": "681779"
  },
  {
    "text": "our simulation workloads an example here shows us that we're being bound by GPU memory budgets so",
    "start": "681779",
    "end": "689459"
  },
  {
    "text": "because of the available GPU let's say it's a 12 gig graphic card and we have a",
    "start": "689459",
    "end": "695100"
  },
  {
    "text": "simulation that needs eight gigs of GPU memory to run we can't fit more than one on it so four gigs of GP Ram is",
    "start": "695100",
    "end": "701760"
  },
  {
    "text": "basically going to waste but Ray allows us to use fractional resource specifications which means that",
    "start": "701760",
    "end": "709740"
  },
  {
    "text": "we can squeeze in multiple simulation tasks per device depending on the available Hardware capabilities for",
    "start": "709740",
    "end": "716519"
  },
  {
    "text": "example if you had a beefier GPU let's say a GPU with 16 gigs of RAM you could",
    "start": "716519",
    "end": "721560"
  },
  {
    "text": "then fit your two eight gig simulations on there or if you had access to some",
    "start": "721560",
    "end": "726600"
  },
  {
    "text": "kind of custom memory allocator like unified virtual memory from Nvidia that",
    "start": "726600",
    "end": "732480"
  },
  {
    "text": "will allow you to augment your GPU Ram budget using your physical CPU RAM",
    "start": "732480",
    "end": "737640"
  },
  {
    "text": "available and then using that managed memory allocation scheme you can start to squeeze in two simulations per node",
    "start": "737640",
    "end": "746899"
  },
  {
    "text": "but it's not always about increasing throughput or just squeezing in more simulations",
    "start": "747019",
    "end": "753120"
  },
  {
    "text": "we have different kinds of execution pattern needs in RCI jobs we often need",
    "start": "753120",
    "end": "758339"
  },
  {
    "text": "to measure if a pull request opened by a software engineer at zukes is going to impact the latency the software latency",
    "start": "758339",
    "end": "765959"
  },
  {
    "text": "of our on vehicle code but measuring software latency is not super straightforward especially when",
    "start": "765959",
    "end": "772740"
  },
  {
    "text": "even a few milliseconds of variance here and there can impact the quality of code going in",
    "start": "772740",
    "end": "779519"
  },
  {
    "text": "a quick primer CPUs are essentially divided into different drones that have",
    "start": "779519",
    "end": "784800"
  },
  {
    "text": "a bunch of processors and memory banks in them when code runs on these processors it can access memory present",
    "start": "784800",
    "end": "791519"
  },
  {
    "text": "in a memory bank which is local to it or even in a memory bank that is not local to it when it goes The Not So local",
    "start": "791519",
    "end": "798839"
  },
  {
    "text": "approach it adds a few milliseconds of latency and when it does that it becomes",
    "start": "798839",
    "end": "804660"
  },
  {
    "text": "increasingly hard to determine if the regressed latency was due to a pull request opened by a software engineer or",
    "start": "804660",
    "end": "812160"
  },
  {
    "text": "whether it's just Hardware induced variants so we want to avoid that we want to",
    "start": "812160",
    "end": "818399"
  },
  {
    "text": "minimize core contention and we want to ensure that memory accesses are local we use tooling like Numa CTL to basically",
    "start": "818399",
    "end": "825899"
  },
  {
    "text": "mask out uh entire pneuma nodes from CPUs so this ensures when code runs on a",
    "start": "825899",
    "end": "832320"
  },
  {
    "text": "particular node it utilizes processors and memory banks that are local to each other",
    "start": "832320",
    "end": "837959"
  },
  {
    "text": "in this example you can see that only one pneuma node is visible to the ray task for running and then the latency",
    "start": "837959",
    "end": "844620"
  },
  {
    "text": "measurements are as accurate as possible next we're going to talk about Dynamic",
    "start": "844620",
    "end": "850800"
  },
  {
    "text": "cluster orchestration anytime a CI job is triggered we spin up a new Ray cluster on the Fly",
    "start": "850800",
    "end": "857060"
  },
  {
    "text": "resources are provided by slurm at Zeus essentially when students starts providing resources",
    "start": "857060",
    "end": "863579"
  },
  {
    "text": "we have a bunch of Docker containers that come online each Docker container has access to a",
    "start": "863579",
    "end": "870240"
  },
  {
    "text": "unique job ID which is linked to the CI job that triggers it when the first node comes online",
    "start": "870240",
    "end": "876420"
  },
  {
    "text": "it registers itself as the head node and in read this which is a key Value Store it broadcasts its IP information using a",
    "start": "876420",
    "end": "884100"
  },
  {
    "text": "set NX operation when other nodes start coming online they do a lookup at that same job ID key",
    "start": "884100",
    "end": "890699"
  },
  {
    "text": "and because of set NX it essentially becomes a read operation for them they read the head nodes IP connect to them",
    "start": "890699",
    "end": "897180"
  },
  {
    "text": "and the cluster upscales organically as time passes",
    "start": "897180",
    "end": "902420"
  },
  {
    "text": "we also try to ensure any kind of networking errors don't happen during startup and we do that by avoiding any",
    "start": "902760",
    "end": "909420"
  },
  {
    "text": "kind of race conditions as I mentioned before our nodes are essentially Docker containers spun up on physical slurm",
    "start": "909420",
    "end": "915959"
  },
  {
    "text": "hosts if we start to instantiate two Ray nodes in these Docker containers then what may",
    "start": "915959",
    "end": "922980"
  },
  {
    "text": "happen that both of them find one free port at the same time and while one of them will be able to successfully",
    "start": "922980",
    "end": "929040"
  },
  {
    "text": "instantiate race Services the other one will see that it's no longer able to do so and lead to networking errors so we",
    "start": "929040",
    "end": "935699"
  },
  {
    "text": "avoid that by just spinning up one container per host also our deployment is extremely",
    "start": "935699",
    "end": "942120"
  },
  {
    "text": "flexible all we need is a base OS installation and a GPU so we can run it",
    "start": "942120",
    "end": "947339"
  },
  {
    "text": "anywhere on-prem or any kind of cloud service provider at Zeus we extensively use bazel to",
    "start": "947339",
    "end": "953760"
  },
  {
    "text": "pre-package all our software including all array software and any kind of user source code pre-built into binaries and",
    "start": "953760",
    "end": "960839"
  },
  {
    "text": "all of that is packaged up into tar walls that are fetched at runtime you don't need any kind of custom Amis",
    "start": "960839",
    "end": "966779"
  },
  {
    "text": "Docker images none of that sort no dedicated cluster you can just run it on any computer",
    "start": "966779",
    "end": "972839"
  },
  {
    "text": "and because we utilize a general pool of compute for our nodes we try to be extremely prudent in our usage for",
    "start": "972839",
    "end": "980220"
  },
  {
    "text": "example if we don't have enough workload to fully subscribe our available compute we return anything that is no longer",
    "start": "980220",
    "end": "987000"
  },
  {
    "text": "needed in this example you'll see that the central node no longer has any rate task",
    "start": "987000",
    "end": "992220"
  },
  {
    "text": "left to run we have an idling monitor that monitors for these metrics and as soon as it",
    "start": "992220",
    "end": "998279"
  },
  {
    "text": "finds such an idle node yslurms s cancel operation it kills that node that nodes",
    "start": "998279",
    "end": "1004220"
  },
  {
    "text": "goes back to the General Pool and we use only what's needed finally I want to talk about fault",
    "start": "1004220",
    "end": "1011180"
  },
  {
    "text": "tolerance because as anyone in this room would know having a CI infrastructure",
    "start": "1011180",
    "end": "1016279"
  },
  {
    "text": "means it needs to run reliably 24 7 365. and things are not always perfect so we",
    "start": "1016279",
    "end": "1021620"
  },
  {
    "text": "need to know how we're resilient to external failures external failures come in many forms",
    "start": "1021620",
    "end": "1028400"
  },
  {
    "text": "this is one example you can see that there's a node with a bad GPU it can either be a bad Hardware problem or just",
    "start": "1028400",
    "end": "1035780"
  },
  {
    "text": "an incorrectly configured Cuda installation if our simulation tasks land on these",
    "start": "1035780",
    "end": "1041360"
  },
  {
    "text": "bad nodes then they could affect the results generated and then they would skew the results of the overall CI job",
    "start": "1041360",
    "end": "1047959"
  },
  {
    "text": "to alleviate this we run a series of health checks at the beginning of the ray job to evict any kind of cluster any",
    "start": "1047959",
    "end": "1055280"
  },
  {
    "text": "kind of node that doesn't meet our Hardware requirements uh in the cluster in this case the middle one is evicted",
    "start": "1055280",
    "end": "1063860"
  },
  {
    "text": "um we tend to download a lot of assets at runtime these include simulation assets like 3D mapping data for example",
    "start": "1063860",
    "end": "1071299"
  },
  {
    "text": "to run our Sims these come from external resources like artifactory so if we",
    "start": "1071299",
    "end": "1076580"
  },
  {
    "text": "detect that one of the node doesn't have a proper network connection and is unable to Ping a host it's evicted from",
    "start": "1076580",
    "end": "1084380"
  },
  {
    "text": "the cluster also many times things don't aren't really",
    "start": "1084380",
    "end": "1090919"
  },
  {
    "text": "detectable right at the start of the job things start going wrong while the job is in progress",
    "start": "1090919",
    "end": "1096440"
  },
  {
    "text": "our gpus run millions of simulations day in and day out and it's not unusual for",
    "start": "1096440",
    "end": "1101840"
  },
  {
    "text": "them to enter into a bad GPU state which will basically render them useless for running heavy simulations",
    "start": "1101840",
    "end": "1108980"
  },
  {
    "text": "we continuously pull them in background threads and if a GPU enters in a bad",
    "start": "1108980",
    "end": "1114320"
  },
  {
    "text": "State these tasks that were Bound for that bad GPU host they're resubmitted",
    "start": "1114320",
    "end": "1120020"
  },
  {
    "text": "and the host with the bad GP is evicted just as it is in a downscaling operation",
    "start": "1120020",
    "end": "1126640"
  },
  {
    "text": "so all of this stuff is about their execution engine itself that provides a",
    "start": "1126799",
    "end": "1132500"
  },
  {
    "text": "faster and around time for order tuning and it's also something we use in CI but how does it all come in the bigger",
    "start": "1132500",
    "end": "1138980"
  },
  {
    "text": "picture so when a software engineer at zukes opens a pull request to validate their code it triggers the build and launch",
    "start": "1138980",
    "end": "1145940"
  },
  {
    "text": "stage in our build Farm essentially their source code is built from the bar is built from the branch",
    "start": "1145940",
    "end": "1151700"
  },
  {
    "text": "using bazel and then all of that is pushed to an external uh build cache",
    "start": "1151700",
    "end": "1157220"
  },
  {
    "text": "which may be S3 or some kind of file system store at the same time we send a request to",
    "start": "1157220",
    "end": "1162980"
  },
  {
    "text": "slurm to start providing us compute resources saying hey we're about to run a job better start providing us",
    "start": "1162980",
    "end": "1168500"
  },
  {
    "text": "resources when slurm does that we have a bunch of physical hosts and Docker containers",
    "start": "1168500",
    "end": "1174679"
  },
  {
    "text": "available we start downloading our platform's software code which is basically a tarball package and all that",
    "start": "1174679",
    "end": "1181880"
  },
  {
    "text": "user code that we built when the pull request was open it gets downloaded onto each of these nodes",
    "start": "1181880",
    "end": "1187460"
  },
  {
    "text": "when this happens we're finally ready to initiate our array services on each of these nodes start up the ray cluster and",
    "start": "1187460",
    "end": "1194660"
  },
  {
    "text": "then the job begins to run when the job finishes running all the",
    "start": "1194660",
    "end": "1199940"
  },
  {
    "text": "results generated from the simulations and the metrics computation they're streamed to external endpoints these",
    "start": "1199940",
    "end": "1206600"
  },
  {
    "text": "streaming endpoints like Kafka and we also have persistent stores for when we want to introspect our simulation",
    "start": "1206600",
    "end": "1212900"
  },
  {
    "text": "results maybe 30 days later these may be S3 or some kind of file system store",
    "start": "1212900",
    "end": "1219580"
  },
  {
    "text": "in conclusion with Ray we've been able to build a very general purpose large-scale workload Runner that is",
    "start": "1219980",
    "end": "1227600"
  },
  {
    "text": "flexible in its deployment methods you can run it anywhere you don't need any kind of custom infra support or",
    "start": "1227600",
    "end": "1233419"
  },
  {
    "text": "installation uh it's super elastic job start running as",
    "start": "1233419",
    "end": "1239299"
  },
  {
    "text": "soon as they have just one node available and they run multi node multi thousand node jobs pretty easily",
    "start": "1239299",
    "end": "1246320"
  },
  {
    "text": "because of like fractional resource utilization we're super efficient we",
    "start": "1246320",
    "end": "1251419"
  },
  {
    "text": "don't let any amount of GPU memory go to waste and we try to use all available CPU cores",
    "start": "1251419",
    "end": "1257960"
  },
  {
    "text": "and like I showed in the latency example sometimes we have these weird execution pattern needs uh like constraining",
    "start": "1257960",
    "end": "1265400"
  },
  {
    "text": "things to a CPU set with this platform we can do that but we want to keep going on towards",
    "start": "1265400",
    "end": "1272960"
  },
  {
    "text": "Perfection and there are still things to be worked on uh our in-house infra ecosystem and integration with things",
    "start": "1272960",
    "end": "1279320"
  },
  {
    "text": "like GitHub Enterprise is still something we're working on for our back jobs uh we also perform a lot of Baseline",
    "start": "1279320",
    "end": "1286820"
  },
  {
    "text": "simulations when we validate pull requests at zukes which means we're repeating a lot of Baseline compute uh",
    "start": "1286820",
    "end": "1293720"
  },
  {
    "text": "which could be avoided we're looking at investigating options like real workflows to duplicate such computations",
    "start": "1293720",
    "end": "1301940"
  },
  {
    "text": "and with that that's the end of the talk thank you for listening to both of us [Applause]",
    "start": "1301940",
    "end": "1311109"
  },
  {
    "text": "oh yeah regarding questions we don't have mics so you can just like say our question I'll repeat it for everyone",
    "start": "1312559",
    "end": "1318500"
  },
  {
    "text": "else and then we'll do it that way",
    "start": "1318500",
    "end": "1322299"
  },
  {
    "text": "okay yeah",
    "start": "1325820",
    "end": "1328720"
  },
  {
    "text": "to learn that in allocate throughout when a clusters becomes available",
    "start": "1343580",
    "end": "1348860"
  },
  {
    "text": "or rather with those people the cluster and see them how does that work all right I",
    "start": "1348860",
    "end": "1355760"
  },
  {
    "text": "think it's a great question the question is uh we spin up a new cluster for every",
    "start": "1355760",
    "end": "1360980"
  },
  {
    "text": "job and essentially how do we Define scheduling those resources to be available themselves to run that job",
    "start": "1360980",
    "end": "1367840"
  },
  {
    "text": "that part is entirely handled by slur so we have a pre-packaged specification",
    "start": "1367840",
    "end": "1374840"
  },
  {
    "text": "that each of these jobs are going to be let's say 200 node jobs for these four Excel instances and then that scheduling",
    "start": "1374840",
    "end": "1381740"
  },
  {
    "text": "is just essentially handled by a centralized slum scheduler which provides those resources from that",
    "start": "1381740",
    "end": "1387559"
  },
  {
    "text": "General Pool there's no context about if it's a real job or not at that level and",
    "start": "1387559",
    "end": "1392960"
  },
  {
    "text": "then the rate portion kicks in once those resources are made Available To Us by slur",
    "start": "1392960",
    "end": "1399400"
  },
  {
    "text": "uh yeah I think this looks like that was this one",
    "start": "1401120",
    "end": "1407240"
  },
  {
    "text": "yeah sure",
    "start": "1407240",
    "end": "1409900"
  },
  {
    "text": "large cluster what's the reason that you opted for this approach rather than the",
    "start": "1425780",
    "end": "1432460"
  },
  {
    "text": "gigantic customer yeah it's a great question um why didn't we go for like a dedicated",
    "start": "1432500",
    "end": "1438919"
  },
  {
    "text": "queue break kind of service which would prevent us from having to set up and tear down array cluster every time I",
    "start": "1438919",
    "end": "1445340"
  },
  {
    "text": "think the simplest answer there was like engineering cost uh that would require support from like a lot of other teams",
    "start": "1445340",
    "end": "1452720"
  },
  {
    "text": "like uh and would require a lot of engineering manners to set up infrastructure like that and this with",
    "start": "1452720",
    "end": "1458600"
  },
  {
    "text": "this we could just reuse the existing HPC ecosystem that we already had at zoos and then all our from their point",
    "start": "1458600",
    "end": "1466159"
  },
  {
    "text": "of view it would just be a base a bunch of python executables that are running there so it was the lowest friction path",
    "start": "1466159",
    "end": "1474080"
  },
  {
    "text": "that we had at that time and uh it's been one and a half years since we've been using the system and it hasn't",
    "start": "1474080",
    "end": "1480380"
  },
  {
    "text": "caused any major issues so we're still sticking with it I think another reason for that is that",
    "start": "1480380",
    "end": "1486440"
  },
  {
    "text": "we support a lot of different types of workload so we need to configure the",
    "start": "1486440",
    "end": "1491900"
  },
  {
    "text": "cluster differently for different workloads so that's why we were now taking this type of approach",
    "start": "1491900",
    "end": "1499419"
  },
  {
    "text": "important yeah I was wondering you mentioned metrics a little bit but are you doing any kind of metric extraction",
    "start": "1499580",
    "end": "1506480"
  },
  {
    "text": "adaptive simulations front to get the results",
    "start": "1506480",
    "end": "1512080"
  },
  {
    "text": "use as well or are you having to set up different notes for that part yeah that's it that is a good question",
    "start": "1512179",
    "end": "1519200"
  },
  {
    "text": "so the Matrix calculation I think 100 of them are on CPU no not GPU",
    "start": "1519200",
    "end": "1527419"
  },
  {
    "text": "but we do multi-threading for the metrics calculations",
    "start": "1527419",
    "end": "1532179"
  },
  {
    "text": "exactly yeah we only use CPU resource to calculate the metrics",
    "start": "1534200",
    "end": "1541419"
  },
  {
    "text": "uh create question think the answer is Sloan versus kubernetes would be this",
    "start": "1544340",
    "end": "1550400"
  },
  {
    "text": "similar to one which I gave slurm is basically what powers the entire HPC ecosystem at zukes and has already done",
    "start": "1550400",
    "end": "1557059"
  },
  {
    "text": "so far like many many years uh and so that was just the easiest thing to go",
    "start": "1557059",
    "end": "1563179"
  },
  {
    "text": "with instead of trying to build out a resource a new resource provisioning mechanism uh",
    "start": "1563179",
    "end": "1570200"
  },
  {
    "text": "so yeah that's basically it",
    "start": "1570200",
    "end": "1574840"
  },
  {
    "text": "yeah so as I mentioned about results being streamed through like endpoints like S3",
    "start": "1580880",
    "end": "1586940"
  },
  {
    "text": "and uh Kafka we also use file store for persisting logs so we have Network file",
    "start": "1586940",
    "end": "1593840"
  },
  {
    "text": "systems that are mounted to each of uh our physical nodes so all those logs are",
    "start": "1593840",
    "end": "1599659"
  },
  {
    "text": "written to those persistent locations and then we have a lot of Downstream post-processing that re-cross references",
    "start": "1599659",
    "end": "1607039"
  },
  {
    "text": "everything together and so when a user sees it on a user interface everything is still available",
    "start": "1607039",
    "end": "1614200"
  },
  {
    "text": "yeah the I think so for us Ray has been more of slurm has",
    "start": "1633620",
    "end": "1641659"
  },
  {
    "text": "been a scheduler for scheduling at a more higher level like physical compute resources and Ray has been at a more",
    "start": "1641659",
    "end": "1649340"
  },
  {
    "text": "Atomic level in which we're trying to distribute scheduling of individual python functions and routines uh that is",
    "start": "1649340",
    "end": "1656659"
  },
  {
    "text": "something that we didn't have an in-house solution for already uh we could",
    "start": "1656659",
    "end": "1662360"
  },
  {
    "text": "have we had solutions for scheduling uh when to run certain executables and containers uh on when to run certain",
    "start": "1662360",
    "end": "1669500"
  },
  {
    "text": "commands in Docker but the ability to like atomically uh decide when this",
    "start": "1669500",
    "end": "1675020"
  },
  {
    "text": "particular python routine is going to get triggered or when it's going to run or on what resources it's going to run",
    "start": "1675020",
    "end": "1680539"
  },
  {
    "text": "uh we didn't have like an in-house solution for that and Ray was perfect because you basically just need to add a",
    "start": "1680539",
    "end": "1688220"
  },
  {
    "text": "decorator to existing code and then that's it uh so it's a more Atomic level",
    "start": "1688220",
    "end": "1693260"
  },
  {
    "text": "scheduler for us yeah",
    "start": "1693260",
    "end": "1697658"
  },
  {
    "text": "workers use or do you use the image",
    "start": "1698919",
    "end": "1705278"
  },
  {
    "text": "yeah so those images are basically very thin wrappers around the base Ubuntu",
    "start": "1706159",
    "end": "1711679"
  },
  {
    "text": "images uh we have integration with bazel and everything but they're built separately",
    "start": "1711679",
    "end": "1718460"
  },
  {
    "text": "with no awareness of whether a rage orb will run on them or not and they are the",
    "start": "1718460",
    "end": "1723919"
  },
  {
    "text": "same images that are universally used throughout the company for all kinds of use cases they're handled by internally",
    "start": "1723919",
    "end": "1729860"
  },
  {
    "text": "by our site reliability engineering team and I believe",
    "start": "1729860",
    "end": "1735919"
  },
  {
    "text": "I mean they'll be the best people to answer to this but they use standard Docker file practices to prep those",
    "start": "1735919",
    "end": "1741980"
  },
  {
    "text": "images but they have no no context about what job is going to run on them so",
    "start": "1741980",
    "end": "1748039"
  },
  {
    "text": "there's nothing pre-installed in those images for running radio jobs",
    "start": "1748039",
    "end": "1753278"
  },
  {
    "text": "right right yeah yeah",
    "start": "1765380",
    "end": "1770019"
  },
  {
    "text": "right uh I think right now because uh as a company zukes oh yeah we use reserved",
    "start": "1780080",
    "end": "1786919"
  },
  {
    "text": "instances but internally when those instances are at zuk's once every job finishes they might go back to another",
    "start": "1786919",
    "end": "1792500"
  },
  {
    "text": "day job but we never hold them for two consecutive jobs they go back to okay any other engineer at zooks needs them",
    "start": "1792500",
    "end": "1799100"
  },
  {
    "text": "for something",
    "start": "1799100",
    "end": "1801398"
  },
  {
    "text": "yeah exactly there's no re-download or resent and then the excellent right yeah the tear down is super quick",
    "start": "1804860",
    "end": "1812059"
  },
  {
    "text": "yeah",
    "start": "1812059",
    "end": "1814299"
  },
  {
    "text": "yeah uh yeah uh I think there are I will admit we don't have a perfect debugging",
    "start": "1823159",
    "end": "1829039"
  },
  {
    "text": "solution for a jobs but uh off late we've been trying to you know better use",
    "start": "1829039",
    "end": "1835399"
  },
  {
    "text": "Ray dashboard for live introspection and saying that oh you know you have some",
    "start": "1835399",
    "end": "1841220"
  },
  {
    "text": "tasks failing then uh what's the source of the failure are they located to a specific node is it because of a",
    "start": "1841220",
    "end": "1847039"
  },
  {
    "text": "hardware failure on that node so I think we've been trying to use Ray dashboard for like live debugging a lot whenever",
    "start": "1847039",
    "end": "1853460"
  },
  {
    "text": "we uh see an alert regarding when something's wrong uh we also have uh",
    "start": "1853460",
    "end": "1859220"
  },
  {
    "text": "kind of like hacky ways of seeing what's wrong we log if we know that there's a problem in a container we just log in",
    "start": "1859220",
    "end": "1864860"
  },
  {
    "text": "attach GDB to the ray process or the running task and see where it's stuck at",
    "start": "1864860",
    "end": "1869899"
  },
  {
    "text": "uh so there's there's no principled solution that we've developed it's just very ad",
    "start": "1869899",
    "end": "1876679"
  },
  {
    "text": "hoc but the dashboard has been super useful there",
    "start": "1876679",
    "end": "1881140"
  },
  {
    "text": "no they they get torn down",
    "start": "1881899",
    "end": "1885820"
  },
  {
    "text": "yeah so uh it so it's all yeah it's only for life",
    "start": "1888559",
    "end": "1894580"
  },
  {
    "text": "oh yeah so that's something uh kind of triggered on our end uh the fractional",
    "start": "1903080",
    "end": "1910220"
  },
  {
    "text": "resource specification is calculated by us and then provided to Ray depending we already know what type of Hardware the",
    "start": "1910220",
    "end": "1916820"
  },
  {
    "text": "job is going to run on so for example if we know that our Sims take eight gigabytes of memory and it's going to be",
    "start": "1916820",
    "end": "1923779"
  },
  {
    "text": "a 16 gig GPU that is going to be available on the nodes in the cluster then we send like okay num GP is 0.5 uh",
    "start": "1923779",
    "end": "1931039"
  },
  {
    "text": "but that calculation is predetermined and it's predetermined based on factors determined uh by us and it's all",
    "start": "1931039",
    "end": "1938480"
  },
  {
    "text": "hard-coded uh Ray obviously doesn't do that dynamically uh uh so yeah",
    "start": "1938480",
    "end": "1945140"
  },
  {
    "text": "understand oh okay thank you everyone",
    "start": "1945140",
    "end": "1953539"
  },
  {
    "text": "foreign",
    "start": "1953539",
    "end": "1956539"
  }
]