[
  {
    "text": "all right I think we should just get started um thanks everyone for being here today I have here um Edward uh",
    "start": "3760",
    "end": "12480"
  },
  {
    "text": "who's one of the software Engineers um and founding Engineers um at in and my name is Richard I'm one of the product",
    "start": "12480",
    "end": "18800"
  },
  {
    "text": "managers here um and we'll be talking about R turbo today this is one of our",
    "start": "18800",
    "end": "24119"
  },
  {
    "text": "newly announced proprietary runtime for Ray workloads uh exclusively available",
    "start": "24119",
    "end": "29359"
  },
  {
    "text": "on any scale so let's get started um I'm going to set",
    "start": "29359",
    "end": "34480"
  },
  {
    "text": "some context you might be familiar with the stack already um this is what we",
    "start": "34480",
    "end": "39600"
  },
  {
    "text": "typically show as the ray stack um and if you're familiar with Ray it has a",
    "start": "39600",
    "end": "45960"
  },
  {
    "text": "suite of AI libraries um including raay data for unstructured data processing uh",
    "start": "45960",
    "end": "52680"
  },
  {
    "text": "raay train for distributed training at scale and race serve for flexible and",
    "start": "52680",
    "end": "58160"
  },
  {
    "text": "scalable model serving so this is um this is some of the libraries that are available um and all",
    "start": "58160",
    "end": "65478"
  },
  {
    "text": "of these libraries are built on top of Ray core together these compose the open source Ray package and this entire Ray",
    "start": "65479",
    "end": "72920"
  },
  {
    "text": "stack runs on any scale which manages infrastructure and uh underlying machines on any Cloud now if you're",
    "start": "72920",
    "end": "80240"
  },
  {
    "text": "working on from the open source you might be familiar with the bottom part of the stack um looking like U you know",
    "start": "80240",
    "end": "86960"
  },
  {
    "text": "gke or eks or even just just ec2 and so on so",
    "start": "86960",
    "end": "92479"
  },
  {
    "text": "forth now um now that we have some familiarity with the stack um what what",
    "start": "92479",
    "end": "98479"
  },
  {
    "text": "we're going to talk about today is about Ray turbo so Ray turbo aims to provide the best price performance and developer",
    "start": "98479",
    "end": "105479"
  },
  {
    "text": "capabilities for AI workloads compared with other Solutions um of running Ray",
    "start": "105479",
    "end": "111840"
  },
  {
    "text": "including running Ray and open source on um an open source CU rate cluster so",
    "start": "111840",
    "end": "117799"
  },
  {
    "text": "rate tobo is a pretty broad Cate ization but it's primarily focused on four broad",
    "start": "117799",
    "end": "123320"
  },
  {
    "text": "workloads um in the AI development life cycle so um data so there is a rate",
    "start": "123320",
    "end": "129360"
  },
  {
    "text": "tural data that we'll be talking about training uh so the equivalent would be",
    "start": "129360",
    "end": "134599"
  },
  {
    "text": "rate tural train serving and LMS you might notice that LMS kind of stands out",
    "start": "134599",
    "end": "141440"
  },
  {
    "text": "in particular but the the main thing to emphasize is that we have a suite of LM",
    "start": "141440",
    "end": "149720"
  },
  {
    "text": "uh enabled uh features or sorry features to enable uh working with llms whether",
    "start": "149720",
    "end": "157560"
  },
  {
    "text": "it be um you know inferencing or training and the this sort of Suite is",
    "start": "157560",
    "end": "162920"
  },
  {
    "text": "built on top of Ray and available exclusively on any scale as well so now I'm going to Deep dive into these sort",
    "start": "162920",
    "end": "169640"
  },
  {
    "text": "of workloads and what Ray turbo has to offer for for each particular",
    "start": "169640",
    "end": "175400"
  },
  {
    "text": "component um so for rate turbo data there's a variety of features that we",
    "start": "175400",
    "end": "182120"
  },
  {
    "text": "announced um over the last couple weeks and we are going to be continuing",
    "start": "182120",
    "end": "187159"
  },
  {
    "text": "pushing on this over the next quarters so for example um one of the",
    "start": "187159",
    "end": "192440"
  },
  {
    "text": "key features that we we showcased that the keynote was accelerated metadata fetching so um R turbo uh in has has",
    "start": "192440",
    "end": "201080"
  },
  {
    "text": "included a couple of these optimizations for read intensive data workloads um so you might imagine",
    "start": "201080",
    "end": "207519"
  },
  {
    "text": "something like being ingesting data from cloud storage to feed into training this",
    "start": "207519",
    "end": "212959"
  },
  {
    "text": "is something that is fairly narrow in terms of data processing workloads but is hugely dependent um on how fast you",
    "start": "212959",
    "end": "221040"
  },
  {
    "text": "can read and what we see on raid turbo data is a up to 4.5x speed up on time to",
    "start": "221040",
    "end": "229640"
  },
  {
    "text": "First output um from the reading compared to open source",
    "start": "229640",
    "end": "234840"
  },
  {
    "text": "Ray uh resumable jobs is also something that we um we've been working on and",
    "start": "234840",
    "end": "241200"
  },
  {
    "text": "effectively this allows rata jobs to be checkpointed stopped or resumed now um this can happen within",
    "start": "241200",
    "end": "248840"
  },
  {
    "text": "the stage so for example if your if your uh pipeline only has like three stages",
    "start": "248840",
    "end": "254879"
  },
  {
    "text": "where you read um do some batch inference and you write um resumable",
    "start": "254879",
    "end": "260479"
  },
  {
    "text": "jobs allows you to be able to checkpoint intermediate progress in the batch inference stage um and and then sort of",
    "start": "260479",
    "end": "268479"
  },
  {
    "text": "uh uh resume it upon um after after after it fails so this is useful for",
    "start": "268479",
    "end": "276600"
  },
  {
    "text": "situations where you have like uh cluster head node failures you're doing bad inference and there are some edge",
    "start": "276600",
    "end": "283120"
  },
  {
    "text": "cases that you run into and fails the entire job and and the goal here is to",
    "start": "283120",
    "end": "288160"
  },
  {
    "text": "minimize the amount of ways to work screaming aggregation is one of the",
    "start": "288160",
    "end": "294680"
  },
  {
    "text": "other features that we have in rate turbo data so this implements um data",
    "start": "294680",
    "end": "299759"
  },
  {
    "text": "aggregation steps within the same key in the streaming way so um this is",
    "start": "299759",
    "end": "305280"
  },
  {
    "text": "especially useful for video batch inference um and and is is leveraged by",
    "start": "305280",
    "end": "312000"
  },
  {
    "text": "um companies that that care about sort of being able to do Advanced data processing on their",
    "start": "312000",
    "end": "317840"
  },
  {
    "text": "videos improve all scaling this is in comparison to open source um is a lot of",
    "start": "317840",
    "end": "323479"
  },
  {
    "text": "enhancements and and uh stability issue fixes um for clusters and Auto uh Factor",
    "start": "323479",
    "end": "329960"
  },
  {
    "text": "pools for autoscaling um this enables users not to need to wait for their",
    "start": "329960",
    "end": "335080"
  },
  {
    "text": "entire uh large cluster to launch in order to run the data processing jobs um",
    "start": "335080",
    "end": "340400"
  },
  {
    "text": "which allows you know jobs to scale up and scale down and continue even under node preemption and finally the the last",
    "start": "340400",
    "end": "347720"
  },
  {
    "text": "feature I want to showcase for raid turbo data um is on audio and video readers which provide uh you know",
    "start": "347720",
    "end": "354840"
  },
  {
    "text": "purpose-built uh connector modules for efficiently loading and decoding video and audio",
    "start": "354840",
    "end": "360639"
  },
  {
    "text": "data so that's the first section on rate turbo data um the the next section I",
    "start": "360639",
    "end": "367319"
  },
  {
    "text": "want to talk about is training so rate turbo train also has a couple select features um that we want to highlight as",
    "start": "367319",
    "end": "374680"
  },
  {
    "text": "part of this rate turbo release uh distribut elastic training is one of the key sort of headline features for a",
    "start": "374680",
    "end": "380599"
  },
  {
    "text": "training on top of any skill which allows training to resume even under hard Hardware failure um it enables",
    "start": "380599",
    "end": "387759"
  },
  {
    "text": "running these training workloads on spot instances uh with minimal Interruption which can thereby reduce training cost",
    "start": "387759",
    "end": "395520"
  },
  {
    "text": "significantly uh one of the things that this one of the interesting things about Ral train is that um with elastic",
    "start": "395520",
    "end": "402479"
  },
  {
    "text": "training paired with the underlying uh infrastructure improvements um our underlying any scale infrastructure also",
    "start": "402479",
    "end": "409160"
  },
  {
    "text": "enables spot um spot fall back to on demand and sort of prioritization",
    "start": "409160",
    "end": "414639"
  },
  {
    "text": "between different node groups um so the the sort of full stack approach to to",
    "start": "414639",
    "end": "420160"
  },
  {
    "text": "looking at elastic training allows for much better slas in terms of training",
    "start": "420160",
    "end": "425800"
  },
  {
    "text": "times improved training observability is also something that we've been working on very closely with our customers um",
    "start": "425800",
    "end": "433000"
  },
  {
    "text": "training oftentimes is cited as a a workload that is very iterative and very",
    "start": "433000",
    "end": "439639"
  },
  {
    "text": "experimental and therefore workloads um such as this require really good observability in add in addition to",
    "start": "439639",
    "end": "447000"
  },
  {
    "text": "existing tooling for experiment management experiment tracking um so what we've done is we spent a lot of",
    "start": "447000",
    "end": "452879"
  },
  {
    "text": "time um building a a UI and user experience uh around dashboarding and",
    "start": "452879",
    "end": "459599"
  },
  {
    "text": "being able to debug distribute training workloads which is often a very big pain when coming from an open source um so",
    "start": "459599",
    "end": "467159"
  },
  {
    "text": "this dashboard um provides insight into individual worker progresses and is",
    "start": "467159",
    "end": "472400"
  },
  {
    "text": "built to help imp point stragglers and them they ball necks within the training system um again similar to to previous",
    "start": "472400",
    "end": "480199"
  },
  {
    "text": "features we're going to be continuing to extend the feature set on this particular um R trial library and there",
    "start": "480199",
    "end": "488319"
  },
  {
    "text": "will be more announcements to come finally race serve um has an",
    "start": "488319",
    "end": "494759"
  },
  {
    "text": "equivalent uh Ray turbo serve and has a variety of features um for for optimized",
    "start": "494759",
    "end": "501360"
  },
  {
    "text": "uh model serving um one of the things that we showcased at the summit um was",
    "start": "501360",
    "end": "506720"
  },
  {
    "text": "fast Sol scaling and model loading so typically um loading a 70 bilon parameter model or",
    "start": "506720",
    "end": "512518"
  },
  {
    "text": "a 400 parameter model um can be very expensive and very costly and very timec",
    "start": "512519",
    "end": "518599"
  },
  {
    "text": "consuming and it's very hard to Autos scale and uh sort of match um underlying closer response uh underlying sort of um",
    "start": "518599",
    "end": "527440"
  },
  {
    "text": "uh requests rate responses and so um what we've done across our Stag and and",
    "start": "527440",
    "end": "535040"
  },
  {
    "text": "even within like the model loading section is we've we've improved Auto scaling and cluster start capabilities",
    "start": "535040",
    "end": "542000"
  },
  {
    "text": "um through clever engineering on our infrastructure side and now like in comparison to open source cubra end to",
    "start": "542000",
    "end": "549120"
  },
  {
    "text": "endend scaling time for for example like a 70b parameter model can be up to five",
    "start": "549120",
    "end": "554360"
  },
  {
    "text": "times faster on any scale and what what we've done here in",
    "start": "554360",
    "end": "559480"
  },
  {
    "text": "particular and we'll Showcase in the next slide is we've done a lot of work also on uh the streaming um sort of",
    "start": "559480",
    "end": "566200"
  },
  {
    "text": "streaming implementation of being able to load weights um from cloud storage and therefore that that contributes",
    "start": "566200",
    "end": "573000"
  },
  {
    "text": "significantly to to the improved runtime Edward will be demonstrating a little bit um uh about these functionalities in",
    "start": "573000",
    "end": "580760"
  },
  {
    "text": "in a later section um so uh We've also sort of done work around um High QPS",
    "start": "580760",
    "end": "588600"
  },
  {
    "text": "optimizations where we have a version of uh RAC serve that is optimized and and",
    "start": "588600",
    "end": "595040"
  },
  {
    "text": "has sort of uh parts that are reimplemented in in higher higher",
    "start": "595040",
    "end": "600760"
  },
  {
    "text": "performance components um and so what we've done what we've seen is that we",
    "start": "600760",
    "end": "606560"
  },
  {
    "text": "can sort of observe up to higher like 50% higher uh QPS under the same SLA uh",
    "start": "606560",
    "end": "613640"
  },
  {
    "text": "without sort of um incurring extra cost and so on so forth um and in terms of",
    "start": "613640",
    "end": "620519"
  },
  {
    "text": "streaming use cases we also see up to three times higher um streaming tokens per second for high traffic serving use",
    "start": "620519",
    "end": "627440"
  },
  {
    "text": "cases for example for llms um we have zero Time Zero downtime",
    "start": "627440",
    "end": "633880"
  },
  {
    "text": "incremental roll outs which um allows for incremental roll outs and Canary upgrades U without requiring like a blue",
    "start": "633880",
    "end": "641160"
  },
  {
    "text": "green deployment um uh so so especially this is in comparison to a lot of Open",
    "start": "641160",
    "end": "647240"
  },
  {
    "text": "Source tooling where um you now do not require like 2x the hardware capacity um",
    "start": "647240",
    "end": "653600"
  },
  {
    "text": "in order to do it to upgrade um it also includes roll back procedures obviously",
    "start": "653600",
    "end": "659560"
  },
  {
    "text": "so that you can sort of up upgrade your cluster deployments without extra cost",
    "start": "659560",
    "end": "665040"
  },
  {
    "text": "and um in a safe manner uh so the sort of incremental",
    "start": "665040",
    "end": "671279"
  },
  {
    "text": "rollout stuff is particularly important if you have a fixed cluster size um for",
    "start": "671279",
    "end": "676320"
  },
  {
    "text": "example if you reserved instances ahead of time um now replica compaction is",
    "start": "676320",
    "end": "682000"
  },
  {
    "text": "something that we've also invested in this is when you have to downscale and you're seeing traffic decrease and you",
    "start": "682000",
    "end": "688160"
  },
  {
    "text": "have replicas that are being deleted U across various nodes what you don't want",
    "start": "688160",
    "end": "693399"
  },
  {
    "text": "is a a sort of a fragmented cluster where your all of your cluster nodes are",
    "start": "693399",
    "end": "698639"
  },
  {
    "text": "low utilization and you have the opportunity to compact um the the the actors but you're unable to so replica",
    "start": "698639",
    "end": "706240"
  },
  {
    "text": "compaction migrates these replicas um into fewer nodes if when possible",
    "start": "706240",
    "end": "711839"
  },
  {
    "text": "reducing resource fragmentation and improving Hardware utilization there thereby reducing",
    "start": "711839",
    "end": "717480"
  },
  {
    "text": "costs uh containerized runtime environments is something that we've also uh spent time working on so",
    "start": "717480",
    "end": "723800"
  },
  {
    "text": "allowing the configuration of different container images uh for different racer deployments um this includes sort of",
    "start": "723800",
    "end": "730880"
  },
  {
    "text": "fast container optimizations and improves the security posture of um of",
    "start": "730880",
    "end": "735920"
  },
  {
    "text": "doing these containers containerized runtime environments compared to for example open source racer and finally",
    "start": "735920",
    "end": "743120"
  },
  {
    "text": "you can deploy um racer on any scale across multiple availability zones um so",
    "start": "743120",
    "end": "749399"
  },
  {
    "text": "we do have the ability to be availability Zone aware when we do the",
    "start": "749399",
    "end": "754880"
  },
  {
    "text": "scheduling of the replicas thereby providing higher redundancy to to",
    "start": "754880",
    "end": "759959"
  },
  {
    "text": "availability Zone failures um so about fast model loading",
    "start": "759959",
    "end": "766040"
  },
  {
    "text": "I want to sort of Deep dive a little bit more into this um at any scale we've",
    "start": "766040",
    "end": "771160"
  },
  {
    "text": "optimized scale up speeds across the deack leading to uh major autoscaling",
    "start": "771160",
    "end": "777000"
  },
  {
    "text": "speedups for variety of models for example we have here a example of a small Model A 7B model and also a larger",
    "start": "777000",
    "end": "783680"
  },
  {
    "text": "model a 70b model on the any scale platform when compared to running the same application using qra on Amazon eks",
    "start": "783680",
    "end": "790880"
  },
  {
    "text": "elastic community service um in addition in particular uh n skill does provide a",
    "start": "790880",
    "end": "796600"
  },
  {
    "text": "library as I mentioned earlier for for model fast model loading streaming tensors directly from cloud storage onto",
    "start": "796600",
    "end": "802760"
  },
  {
    "text": "the GPU and the graph shown here Compares open source BLM defaults with any scale fast mod loading which shows",
    "start": "802760",
    "end": "809720"
  },
  {
    "text": "up to a 5x speed up for large models again Edward will be doing a a live demo",
    "start": "809720",
    "end": "815360"
  },
  {
    "text": "of this in a couple minutes so so we can stay tuned for that um next I'm going to Showcase some",
    "start": "815360",
    "end": "822000"
  },
  {
    "text": "of the Alum features that we have on Nale and some of the exciting announcements that we've made around there we do have Alum Suite in Ray turbo",
    "start": "822000",
    "end": "829480"
  },
  {
    "text": "and this consists of three major components with seamless integration between them um through our model",
    "start": "829480",
    "end": "835199"
  },
  {
    "text": "registry and data sets features to complete the end to end LM development cycle first LM Forge is one of the most",
    "start": "835199",
    "end": "843000"
  },
  {
    "text": "comprehensive LM refinement libraries available today with extensive breath of",
    "start": "843000",
    "end": "849160"
  },
  {
    "text": "uh fine tuning techniques including puzzle language modeling instruction tuning continued pre-training and also",
    "start": "849160",
    "end": "855560"
  },
  {
    "text": "standard um lur based fine tuning we have a new uh batch inference",
    "start": "855560",
    "end": "861720"
  },
  {
    "text": "Library called BR LM batch this is a library that's built purposely for",
    "start": "861720",
    "end": "867480"
  },
  {
    "text": "optimizing executing B LM inference pipelines at scale cost improvements",
    "start": "867480",
    "end": "873120"
  },
  {
    "text": "with Ray LM batch can be up to 6X when compared to other inference providers",
    "start": "873120",
    "end": "879040"
  },
  {
    "text": "such as AIS bedrock and open AI without requiring high-end Hardware uh like A1",
    "start": "879040",
    "end": "886199"
  },
  {
    "text": "100s or h100s and finally we have raym which is we serving um it offers high performance",
    "start": "886199",
    "end": "894040"
  },
  {
    "text": "and fully configurable online serving for any open- sourced large language model as well as multimodal models",
    "start": "894040",
    "end": "901079"
  },
  {
    "text": "featuring support such as Lura multiplexing Json mode or constraint decoding and also uh Custom Performance",
    "start": "901079",
    "end": "909880"
  },
  {
    "text": "Tuning I'm going to Deep dive a little bit into uh the batch LM inference work",
    "start": "909880",
    "end": "915079"
  },
  {
    "text": "that we've been doing um so for self-hosted inference Ray and any skill",
    "start": "915079",
    "end": "921160"
  },
  {
    "text": "offer unique advantages when it comes to batch LM remance um batch Lance on in skill will",
    "start": "921160",
    "end": "928639"
  },
  {
    "text": "use key components of our stack including um the any scale proprietary",
    "start": "928639",
    "end": "934360"
  },
  {
    "text": "inference engine uh any scale rate tural data rayor and the anope",
    "start": "934360",
    "end": "939839"
  },
  {
    "text": "platform um in particular for the the inference engine which wasn't really",
    "start": "939839",
    "end": "945560"
  },
  {
    "text": "mentioned in previous slides um it's heavily based on VM and we've done some",
    "start": "945560",
    "end": "951480"
  },
  {
    "text": "extra extensions to optimize uh kernels um and and various other integration",
    "start": "951480",
    "end": "957880"
  },
  {
    "text": "steps um in particular we've also been working very closely with the VM team to",
    "start": "957880",
    "end": "963959"
  },
  {
    "text": "to make sure there are um other optimizations that are going in on the open source side um and contributing",
    "start": "963959",
    "end": "970639"
  },
  {
    "text": "some of these optimizations ourselves um in certain cases uh Ray LM",
    "start": "970639",
    "end": "976120"
  },
  {
    "text": "batch is able to reduce costs up to 2.9x compared to 8os bread Rock and open a",
    "start": "976120",
    "end": "982240"
  },
  {
    "text": "and up to 6X in share prefix situations um simply by optimizing and",
    "start": "982240",
    "end": "988079"
  },
  {
    "text": "configurating the bach infin workload much more effectively so as an example we have an",
    "start": "988079",
    "end": "994440"
  },
  {
    "text": "experiment here um so in this experiment shown we evaluate raym batch on any",
    "start": "994440",
    "end": "999839"
  },
  {
    "text": "scale on a 8B parameter model llama 3.18 and compared to Common alternative",
    "start": "999839",
    "end": "1007399"
  },
  {
    "text": "Solutions so this includes Bedrock bedrock with batch pricing and open AI",
    "start": "1007399",
    "end": "1012959"
  },
  {
    "text": "GPT 40 mini with badge pricing we evaluate the solu solution using a data",
    "start": "1012959",
    "end": "1019319"
  },
  {
    "text": "set uh with requests of 2,000 input 100 output shapes with no share prefix first",
    "start": "1019319",
    "end": "1026280"
  },
  {
    "text": "um which is a common request shape for uh summarization so we see that in this",
    "start": "1026280",
    "end": "1031918"
  },
  {
    "text": "particular workload compared to Alternatives um the anco uh",
    "start": "1031919",
    "end": "1038760"
  },
  {
    "text": "bf16 uh solution is around 2x cheaper than the Bedrock batch",
    "start": "1038760",
    "end": "1045558"
  },
  {
    "text": "solution the nkill fb8 solution is up to nearly 3x cheaper than the Amazon AWS",
    "start": "1045559",
    "end": "1053039"
  },
  {
    "text": "Bedrock batch pricing solution and if you assume a shared prefix we can see",
    "start": "1053039",
    "end": "1059039"
  },
  {
    "text": "that the a scale um solution for this work comes out to be 6X cheaper than the",
    "start": "1059039",
    "end": "1064720"
  },
  {
    "text": "a batch pricing solution and up nearly 5x cheaper um than the open AI for GPT",
    "start": "1064720",
    "end": "1072880"
  },
  {
    "text": "40 mini uh batch pricing solution so um so we're going to to be",
    "start": "1072880",
    "end": "1079080"
  },
  {
    "text": "continuing working on bad inference and and optimizing cost and so so on so",
    "start": "1079080",
    "end": "1084840"
  },
  {
    "text": "forth here um what we're going to be focused on next is on multimodal model",
    "start": "1084840",
    "end": "1090520"
  },
  {
    "text": "so if that's something that you're interested in we'd love to get in contact UM overall this is sort of",
    "start": "1090520",
    "end": "1096799"
  },
  {
    "text": "concludes the section on Ray turbo as a overview and I believe Edward you can",
    "start": "1096799",
    "end": "1103200"
  },
  {
    "text": "probably take it next to sort of um talk about uh fast small loading um and",
    "start": "1103200",
    "end": "1110039"
  },
  {
    "text": "demonstrate some of those capabilities there cool uh there are a couple of questions they might want to take from the Q&A all right okay so I can go ahead",
    "start": "1110039",
    "end": "1117679"
  },
  {
    "text": "and take a look at that right now um so yeah so soil asks will this recorded",
    "start": "1117679",
    "end": "1125159"
  },
  {
    "text": "presentation be available online um I believe it should be um yeah and second",
    "start": "1125159",
    "end": "1131480"
  },
  {
    "text": "Anonymous says are the raid turbo capabilities like data and serve available for any scale on kubernetes",
    "start": "1131480",
    "end": "1138840"
  },
  {
    "text": "for customer owned data centers H yes they should be um",
    "start": "1138840",
    "end": "1145280"
  },
  {
    "text": "Mahar says what are some of the use cases for batch reference um there's",
    "start": "1145280",
    "end": "1151159"
  },
  {
    "text": "actually a lot of different use cases for batch reference for example say um",
    "start": "1151159",
    "end": "1156559"
  },
  {
    "text": "summarization um user ticket classification um sentiment",
    "start": "1156559",
    "end": "1162400"
  },
  {
    "text": "analysis uh content understanding like if you want to if you have a large",
    "start": "1162400",
    "end": "1167679"
  },
  {
    "text": "Corpus of like uh you know images that that users are uploading being able to sort of classify",
    "start": "1167679",
    "end": "1174039"
  },
  {
    "text": "them at scale obviously is is something that doesn't Happ need to happen in real time um what are some of the current",
    "start": "1174039",
    "end": "1181760"
  },
  {
    "text": "level optimizations that need to be done for BLM otherwise bat rents so that's something some of the work that we've",
    "start": "1181760",
    "end": "1187799"
  },
  {
    "text": "done inous and in particular um we've sort of you know a lot of the kernels",
    "start": "1187799",
    "end": "1193760"
  },
  {
    "text": "are built for uh whether it be like low lency or just overall like token like",
    "start": "1193760",
    "end": "1200880"
  },
  {
    "text": "overall um like Matrix multiply throughput and um in certain situations",
    "start": "1200880",
    "end": "1207640"
  },
  {
    "text": "you can Multiplex the resources more effectively so so being able to leverage compute bandwidth um or yeah compute",
    "start": "1207640",
    "end": "1215120"
  },
  {
    "text": "capacity memory bandwidth and network bandwidth more effectively that that's something that we've we've done in house",
    "start": "1215120",
    "end": "1222159"
  },
  {
    "text": "um how well or tightly would you say R turble connects with uh pytorch hugging",
    "start": "1222159",
    "end": "1229080"
  },
  {
    "text": "face and machine learning ecosystem so the apis that rate turbo offers are um",
    "start": "1229080",
    "end": "1234880"
  },
  {
    "text": "for the large part the same as the open source Ray capabilities and so um open",
    "start": "1234880",
    "end": "1241120"
  },
  {
    "text": "source Ray is built with these sort of Integrations with Pythor Trucking face and mlflow in mind um so so it should",
    "start": "1241120",
    "end": "1249240"
  },
  {
    "text": "just work work as well as what you get in open",
    "start": "1249240",
    "end": "1253200"
  },
  {
    "text": "source cool",
    "start": "1255280",
    "end": "1259280"
  },
  {
    "text": "okay um I think that's all the questions for now so I'll take over and work and talk about the fast modelator",
    "start": "1262760",
    "end": "1269840"
  },
  {
    "text": "uh can you confirm you can see the presentation Richard yeah okay um cool so thanks so much",
    "start": "1272240",
    "end": "1278400"
  },
  {
    "text": "Richard um so I'm going to talk about a little bit more in detail about one of the optimizations that Richard mentioned",
    "start": "1278400",
    "end": "1284520"
  },
  {
    "text": "which was the fast model loading work um so Richard talked about this a little bit in the presentation but with the",
    "start": "1284520",
    "end": "1291919"
  },
  {
    "text": "rise of llms and generative AI in general um it's become you know this",
    "start": "1291919",
    "end": "1297159"
  },
  {
    "text": "this trend that models are have just gotten absolutely huge so a 7B model is",
    "start": "1297159",
    "end": "1302520"
  },
  {
    "text": "you know basically what's considered small now um but in in fp16 um a 7B",
    "start": "1302520",
    "end": "1308840"
  },
  {
    "text": "model is 14 gigabytes of weights and that means you need to fetch this from some kind of remote storage and then",
    "start": "1308840",
    "end": "1314559"
  },
  {
    "text": "load it onto the GPU before you can even start um before you can even start serving",
    "start": "1314559",
    "end": "1320760"
  },
  {
    "text": "requests a 70b model which is really common among our users and customers is 10 times larger at like 140 gigabytes",
    "start": "1320760",
    "end": "1328919"
  },
  {
    "text": "and 405b models are um starting to become popular and this is almost a terabyte uh so this is just a really big",
    "start": "1328919",
    "end": "1336120"
  },
  {
    "text": "obstacle and I'm sure a lot of people have experienced the pain of waiting for these uh weights to download and load",
    "start": "1336120",
    "end": "1341679"
  },
  {
    "text": "before you can um test things out in development or uh serve in production",
    "start": "1341679",
    "end": "1348640"
  },
  {
    "text": "so uh a lot of people are familiar with this classic XKCD comic uh you know you have two developers and they're sitting",
    "start": "1348640",
    "end": "1354520"
  },
  {
    "text": "around waiting for their code to compile but I think we need to update it for the the modern AI",
    "start": "1354520",
    "end": "1360200"
  },
  {
    "text": "practitioner and now what we're doing is the same old thing but uh just waiting for a model to load",
    "start": "1360200",
    "end": "1367240"
  },
  {
    "text": "instead and this really has pretty serious consequences so in development",
    "start": "1367240",
    "end": "1372480"
  },
  {
    "text": "um slow scaling drains productivity um anytime that you need to wait minutes and minutes for uh things to to spin up",
    "start": "1372480",
    "end": "1381200"
  },
  {
    "text": "um it's just a waste of your time and in production uh this can Al this can often lead to over-provisioning because if you",
    "start": "1381200",
    "end": "1388360"
  },
  {
    "text": "can't respond uh to you know increased user traffic by upscaling quickly then",
    "start": "1388360",
    "end": "1393799"
  },
  {
    "text": "you need to over provision and have extra U resources in steady state and",
    "start": "1393799",
    "end": "1399320"
  },
  {
    "text": "this is especially problematic when you're using gpus to serve models um it's very very expensive to have them",
    "start": "1399320",
    "end": "1404840"
  },
  {
    "text": "sitting around idle so let's talk about what kind of a",
    "start": "1404840",
    "end": "1410600"
  },
  {
    "text": "baseline solution looks like for loading these large model weights um so this is",
    "start": "1410600",
    "end": "1416279"
  },
  {
    "text": "sort of the standard thing that we see our customers and and open source users use which is um first you'll download",
    "start": "1416279",
    "end": "1422840"
  },
  {
    "text": "the weights from something like an S3 bucket um and for this you might use the the AWS S3 CP CLI um which is pretty",
    "start": "1422840",
    "end": "1431279"
  },
  {
    "text": "well tuned for for high throughput and then after you download from S3 onto",
    "start": "1431279",
    "end": "1436880"
  },
  {
    "text": "local disk um you use a library like safe tensors which is able to uh you know efficiently load from local disk",
    "start": "1436880",
    "end": "1443799"
  },
  {
    "text": "onto the GPU um so that setup looks something",
    "start": "1443799",
    "end": "1448840"
  },
  {
    "text": "like what I have in the diagram on the right but this has a couple of problems um the main one is that you have these",
    "start": "1448840",
    "end": "1455640"
  },
  {
    "text": "three synchronous load steps so you first need to download from the S3 bucket and write to the local dis you",
    "start": "1455640",
    "end": "1462480"
  },
  {
    "text": "wait for that to finish and then you load from the local dis into CPU memory and then once again you load from CPU",
    "start": "1462480",
    "end": "1468679"
  },
  {
    "text": "memory into GPU memory when the weights get really large each of these stages can take quite a while um so this end to",
    "start": "1468679",
    "end": "1475640"
  },
  {
    "text": "end process becomes quite slow so any scale offers a safe tensor",
    "start": "1475640",
    "end": "1481640"
  },
  {
    "text": "compatible client um that allows you to directly load from remote storage like",
    "start": "1481640",
    "end": "1486799"
  },
  {
    "text": "an S3 or a Google Cloud Storage bucket and um here you can see that we can pass",
    "start": "1486799",
    "end": "1492520"
  },
  {
    "text": "that that remote URI and then load directly onto a GPU",
    "start": "1492520",
    "end": "1499000"
  },
  {
    "text": "the difference under the hood is that any skill will do this in a streaming fashion and basically take advantage of",
    "start": "1499000",
    "end": "1504080"
  },
  {
    "text": "pipeline parallelism so um instead of downloading the entire file and then",
    "start": "1504080",
    "end": "1509240"
  },
  {
    "text": "loading it to the GPU we'll fetch the the model tensors chunk by chunk and then stream it onto the GPU as they're",
    "start": "1509240",
    "end": "1516679"
  },
  {
    "text": "ready and this makes a pretty big difference in end to end download times um which I will show in a quick",
    "start": "1516679",
    "end": "1526520"
  },
  {
    "text": "demo uh okay so here I have a workspace on um any scale running uh so this is",
    "start": "1526799",
    "end": "1533840"
  },
  {
    "text": "just running a um that's not too important actually",
    "start": "1533840",
    "end": "1539279"
  },
  {
    "text": "we can take a look at the dashboard and you can see that there's just one node in my cluster and this is a GPU node",
    "start": "1539279",
    "end": "1545039"
  },
  {
    "text": "it's an h10g so I'm running on a um a G5 instance on AWS and there's nothing running on it",
    "start": "1545039",
    "end": "1551600"
  },
  {
    "text": "right now um but I have some model weights stored in an S3 bucket so we can",
    "start": "1551600",
    "end": "1557720"
  },
  {
    "text": "take a look at that",
    "start": "1557720",
    "end": "1560240"
  },
  {
    "text": "oops I forgot the ls so we have a bunch of stuff in this bucket um but you can see I have a copy",
    "start": "1564480",
    "end": "1571440"
  },
  {
    "text": "of mistl 7B that's stored in safe tensor format and this is the total size in",
    "start": "1571440",
    "end": "1577279"
  },
  {
    "text": "bytes so it's about um 14 gigabytes so I have a first script here",
    "start": "1577279",
    "end": "1583240"
  },
  {
    "text": "um which is essentially implementing the the Baseline solution that I showed on the slide so it takes the the remote",
    "start": "1583240",
    "end": "1591120"
  },
  {
    "text": "model path and then it uses the AWS S3 CP command to copy it to local disk and",
    "start": "1591120",
    "end": "1598520"
  },
  {
    "text": "then once that's done it uses the safe tensor load file um command to load it",
    "start": "1598520",
    "end": "1604039"
  },
  {
    "text": "onto the GPU and I also have uh the AWS S3 CLI",
    "start": "1604039",
    "end": "1610399"
  },
  {
    "text": "tuned to give us like the maximum throughput that it",
    "start": "1610399",
    "end": "1615039"
  },
  {
    "text": "can so if we download this you can see it's starts by downloading the model from S3 and this is fairly fast it's able to",
    "start": "1615840",
    "end": "1624240"
  },
  {
    "text": "um download pretty quickly I think in most of my testing it",
    "start": "1624240",
    "end": "1629480"
  },
  {
    "text": "takes maybe like 30 seconds or so to download the model uh let's",
    "start": "1629480",
    "end": "1634720"
  },
  {
    "text": "see so it's hovering like a little bit under about half half a gigabyte to one",
    "start": "1634720",
    "end": "1640799"
  },
  {
    "text": "gigabyte per second it took about 25 seconds to download to local dis and",
    "start": "1640799",
    "end": "1645960"
  },
  {
    "text": "then it took another 3 seconds to load from disc into memory so in total it was just under 30 seconds um so not too bad",
    "start": "1645960",
    "end": "1653720"
  },
  {
    "text": "but if you use a 70b model this would be you know almost 10 times as long and in",
    "start": "1653720",
    "end": "1660399"
  },
  {
    "text": "production waiting 30 seconds for your model to load is already you know a pretty big problem so let's take a look",
    "start": "1660399",
    "end": "1666600"
  },
  {
    "text": "at the any scale version which I have in this script called fast. piy so it looks",
    "start": "1666600",
    "end": "1672440"
  },
  {
    "text": "pretty similar uh but the difference is that we're using this any scale version of the load file command and we're",
    "start": "1672440",
    "end": "1678320"
  },
  {
    "text": "directly passing the remote URI in here um and then we're telling it that we want to load these weights onto the",
    "start": "1678320",
    "end": "1686360"
  },
  {
    "text": "GPU so let's try running this one now and remember this is going to do the",
    "start": "1686360",
    "end": "1691760"
  },
  {
    "text": "download and stream to the GPU all at once so when this uh command finishes it will already be loaded onto the",
    "start": "1691760",
    "end": "1698960"
  },
  {
    "text": "GPU and just like that it finished so it finished in about uh just under N9 seconds versus almost 30 for the",
    "start": "1698960",
    "end": "1705960"
  },
  {
    "text": "Baseline and if we take a look over here we can see that uh the entire 14",
    "start": "1705960",
    "end": "1712320"
  },
  {
    "text": "gigabyte model is already loaded onto the GPU so this is a a pretty significant",
    "start": "1712320",
    "end": "1717720"
  },
  {
    "text": "speed up just for the 7 7B model um you can imagine if you're using a 70b or an",
    "start": "1717720",
    "end": "1723519"
  },
  {
    "text": "uh you know even larger model that it would be even more dramatic and so the other thing I want",
    "start": "1723519",
    "end": "1729200"
  },
  {
    "text": "to show is that on um so on any scale we have Ray llm which makes it really easy",
    "start": "1729200",
    "end": "1734880"
  },
  {
    "text": "to deploy llms as you know Production service so here I have um a config file for",
    "start": "1734880",
    "end": "1741640"
  },
  {
    "text": "deploying mistol 7B uh and inside of here I've I've",
    "start": "1741640",
    "end": "1748159"
  },
  {
    "text": "configured it by passing that same um model path to use fast loading uh when",
    "start": "1748159",
    "end": "1754919"
  },
  {
    "text": "I'm using Ray llm so between any scales optimizations",
    "start": "1754919",
    "end": "1760159"
  },
  {
    "text": "on node startup and container pulling and this fast model loading um for a",
    "start": "1760159",
    "end": "1765320"
  },
  {
    "text": "production service we should be able to spin up new copies of the model really",
    "start": "1765320",
    "end": "1770679"
  },
  {
    "text": "fast so here is the the service um I just showed the config for and I have it",
    "start": "1770679",
    "end": "1776120"
  },
  {
    "text": "running I started it um just before the the webinar began and you can see I have the mistol",
    "start": "1776120",
    "end": "1782360"
  },
  {
    "text": "7B instruct deployment uh but it currently has zero replicas so it's scaled all the way down to zero and um",
    "start": "1782360",
    "end": "1789120"
  },
  {
    "text": "there are no copies of the model ready to serve if we take a look at the cluster we can see that uh there's only",
    "start": "1789120",
    "end": "1795279"
  },
  {
    "text": "a single node and there's no GPU at all this is only the head node um so what",
    "start": "1795279",
    "end": "1802039"
  },
  {
    "text": "I'm going to do next is uh send a query to my service which is going to need to",
    "start": "1802039",
    "end": "1807120"
  },
  {
    "text": "spin up a new copy of the model uh in order to service it so this is going to start a new node with a GPU on it it's",
    "start": "1807120",
    "end": "1814120"
  },
  {
    "text": "going to pull the pretty large container images with all of my dependencies like VM and pytorch and then it's going to",
    "start": "1814120",
    "end": "1820760"
  },
  {
    "text": "load the 14 gigabytes of model weights um and we should be able to do all of this relatively quickly",
    "start": "1820760",
    "end": "1829000"
  },
  {
    "text": "um so I need to give it a prompt what is the weather like in San",
    "start": "1829000",
    "end": "1834600"
  },
  {
    "text": "Francisco in October so this query is not going to",
    "start": "1834600",
    "end": "1841120"
  },
  {
    "text": "return immediately because um we didn't have any replicas running but if I return over here we should see um uh",
    "start": "1841120",
    "end": "1850480"
  },
  {
    "text": "here this is the event log for the service and you can see here that the request came in so uh we're now",
    "start": "1850480",
    "end": "1856919"
  },
  {
    "text": "upscaling from 0 to one replicas uh and the N scale autoscaler",
    "start": "1856919",
    "end": "1862360"
  },
  {
    "text": "has launched uh a g68 x large instance so this has an Nvidia L4 GPU on it um",
    "start": "1862360",
    "end": "1870639"
  },
  {
    "text": "and if we go back up to the top we can see that that deployment that had zero replicas now says that it's upscaling",
    "start": "1870639",
    "end": "1877760"
  },
  {
    "text": "from 0 to one uh so it looks like the node was uh",
    "start": "1877760",
    "end": "1885720"
  },
  {
    "text": "already booted and added to the cluster so that that took uh around 30 seconds",
    "start": "1885720",
    "end": "1890960"
  },
  {
    "text": "so now we should be waiting for the container image to get pulled um which should be relatively fast as",
    "start": "1890960",
    "end": "1898080"
  },
  {
    "text": "well I think if we go over to this log page we should be able to see it let's",
    "start": "1898080",
    "end": "1905799"
  },
  {
    "text": "see um oh okay this was this didn't print one of the logs that I expected but the upscaling has already finished",
    "start": "1912559",
    "end": "1919919"
  },
  {
    "text": "end to end you can see we got this message deployment upscaling completed and if we go back we can see that um the",
    "start": "1919919",
    "end": "1927639"
  },
  {
    "text": "the query returned so it says in October San Francisco typically experiences mild damp weather uh and remember this when I",
    "start": "1927639",
    "end": "1934080"
  },
  {
    "text": "sent this query there was nothing running at all no machine no container and no model loaded and end to end we",
    "start": "1934080",
    "end": "1941120"
  },
  {
    "text": "were able to uh scale it up in uh it looks like about 45 seconds um so",
    "start": "1941120",
    "end": "1949279"
  },
  {
    "text": "running the comparable setup on cub in our testing takes over five minutes so this is like a more than a five times",
    "start": "1949279",
    "end": "1957080"
  },
  {
    "text": "Improvement um cool okay so that concludes the demo portion looks like we",
    "start": "1957080",
    "end": "1963760"
  },
  {
    "text": "have a couple of uh questions in the chat that I can answer now and if anyone",
    "start": "1963760",
    "end": "1970080"
  },
  {
    "text": "else has any questions feel free to put them in the Q&A um okay so raate turbo is VM only",
    "start": "1970080",
    "end": "1976960"
  },
  {
    "text": "like any skill platform form or it is going to support GK as well and be able to offer live uh yeah so good question",
    "start": "1976960",
    "end": "1984039"
  },
  {
    "text": "so nkll actually recently launched support for running the any scale platform on kubernetes including gke uh",
    "start": "1984039",
    "end": "1992000"
  },
  {
    "text": "and all of the ray turbo features yeah I believe all of the features that Richard mentioned in the presentation are also",
    "start": "1992000",
    "end": "1998279"
  },
  {
    "text": "supported on um any scale on kubernetes and then wayo says how to set",
    "start": "1998279",
    "end": "2005919"
  },
  {
    "text": "up the config file for serving any best practices um so actually let me just do",
    "start": "2005919",
    "end": "2012559"
  },
  {
    "text": "like a impromptu live demo here so on any scale there's actually something",
    "start": "2012559",
    "end": "2018039"
  },
  {
    "text": "really cool so if you use like Ray llm you can use this J gen config command um",
    "start": "2018039",
    "end": "2024159"
  },
  {
    "text": "and you know there are a ton of knobs that you need to tune oh I have my I have my environment",
    "start": "2024159",
    "end": "2029480"
  },
  {
    "text": "messed up so I I won't be able to show it but there are a ton of knobs that you need to tune for",
    "start": "2029480",
    "end": "2035760"
  },
  {
    "text": "um for things like uh like VM there're just like a lot of",
    "start": "2035760",
    "end": "2041600"
  },
  {
    "text": "different configurations and with Ray LM on any scale we try to like configure a lot of them out of the box for you so",
    "start": "2041600",
    "end": "2047039"
  },
  {
    "text": "that you don't need to worry about it and then raym exposes like kind of only a subset of that so that really helps",
    "start": "2047039",
    "end": "2053960"
  },
  {
    "text": "with you know getting good performance out of the box and then we have a bunch of tutorials and documentation for um",
    "start": "2053960",
    "end": "2059839"
  },
  {
    "text": "how to configure things beyond that um okay how different it is from",
    "start": "2059839",
    "end": "2067720"
  },
  {
    "text": "from some of the fast model loading optimization that Nvidia is doing with tensor RT and NS platform any benchmarks",
    "start": "2067720",
    "end": "2074440"
  },
  {
    "text": "available uh I'm I'm not familiar with nms um and as far as I know the Nvidia",
    "start": "2074440",
    "end": "2082638"
  },
  {
    "text": "model loading that I've seen in tens RT is only from local disk uh but I I might",
    "start": "2082639",
    "end": "2088480"
  },
  {
    "text": "be I might have missed something um so the main difference is this ability to load from remote storage um but don't",
    "start": "2088480",
    "end": "2095118"
  },
  {
    "text": "quote me on this because I my knowledge might be out of date the other thing is that this client that I showed um is not",
    "start": "2095119",
    "end": "2102160"
  },
  {
    "text": "really specific to like raym or VM it can be used with any torch model um so",
    "start": "2102160",
    "end": "2107960"
  },
  {
    "text": "it's very general purpose um am I able to use an on-prem",
    "start": "2107960",
    "end": "2115320"
  },
  {
    "text": "infra to help serve predictions with Ray turbo uh so if you use any scale we do",
    "start": "2115320",
    "end": "2120800"
  },
  {
    "text": "have support for something called machine pools where you can connect um like some on-prem instances and be able",
    "start": "2120800",
    "end": "2126800"
  },
  {
    "text": "to use them with the any platform and in that case you would be able to use it with raate",
    "start": "2126800",
    "end": "2132720"
  },
  {
    "text": "turbo and does rate turbo use any specific tools like Carpenter or some",
    "start": "2133040",
    "end": "2138440"
  },
  {
    "text": "other tool for scheduling the nodes um no so uh any skill has",
    "start": "2138440",
    "end": "2144839"
  },
  {
    "text": "basically a a full custom control plane that that handles the scheduling um so even when you're",
    "start": "2144839",
    "end": "2151560"
  },
  {
    "text": "running on top of kubernetes it will talk back to the the any scale control plane including the autoscaler in order",
    "start": "2151560",
    "end": "2157440"
  },
  {
    "text": "to to decide what node types need to be added and then do the",
    "start": "2157440",
    "end": "2162599"
  },
  {
    "text": "scheduling cool um yeah thanks so much everyone for participating today that's all the questions that we have right now",
    "start": "2169160",
    "end": "2175240"
  },
  {
    "text": "um we can wait another 30 seconds or or one minute oh there's another question from",
    "start": "2175240",
    "end": "2181760"
  },
  {
    "text": "Brian Dennis I can yeah just to be clear raay turbo is only within any scale platform there's no open source SL",
    "start": "2181760",
    "end": "2187880"
  },
  {
    "text": "self-hosted version yeah that's right so currently Ray turbo is only available um",
    "start": "2187880",
    "end": "2192960"
  },
  {
    "text": "if you use it with the N scale platform well what on the self-hosted I think the clarification there is it will run in",
    "start": "2192960",
    "end": "2199960"
  },
  {
    "text": "your control plane um or sorry it will run in your data plane so like the you",
    "start": "2199960",
    "end": "2205920"
  },
  {
    "text": "know the machines and the infrastructure um can be configured to your liking but there will not be an",
    "start": "2205920",
    "end": "2212040"
  },
  {
    "text": "open source ver",
    "start": "2212040",
    "end": "2215960"
  },
  {
    "text": "Edward do you want to press done on the the questions that you've answered oh uh yeah sorry I didn't realize I could do",
    "start": "2218200",
    "end": "2226000"
  },
  {
    "text": "that okay all right um with that I think we're basically done um thanks everyone",
    "start": "2226000",
    "end": "2231880"
  },
  {
    "text": "for attending and uh appreciate the audience um if you have any questions",
    "start": "2231880",
    "end": "2238000"
  },
  {
    "text": "please feel free to follow up with the any skill folks um or like you know contact us through the the website and",
    "start": "2238000",
    "end": "2246560"
  },
  {
    "text": "um well I guess there's one last question sundip asks how are we keeping up to date with VM support stack and",
    "start": "2246560",
    "end": "2256040"
  },
  {
    "text": "support realm um well uh so essentially if I understand correctly it's like how",
    "start": "2256040",
    "end": "2262119"
  },
  {
    "text": "do we sort of make sure our LM stack is sort of receiving the best um sort of",
    "start": "2262119",
    "end": "2270280"
  },
  {
    "text": "improvements from the open source um so RM is sort of at a lower layer than or a",
    "start": "2270280",
    "end": "2276160"
  },
  {
    "text": "different layer than VM so they don't interact and uh and VM is uh uh like we",
    "start": "2276160",
    "end": "2285000"
  },
  {
    "text": "do sort of have our proprietary inference engine that that is closely maintained with the Upstream so we try",
    "start": "2285000",
    "end": "2291800"
  },
  {
    "text": "to sync it every every week or so so it's like the plan will be to sort of",
    "start": "2291800",
    "end": "2299359"
  },
  {
    "text": "continue maintaining it and in terms of its interactions with raym that shouldn't be a problem since the",
    "start": "2299359",
    "end": "2305520"
  },
  {
    "text": "interfaces we leverage there are pretty St",
    "start": "2305520",
    "end": "2309440"
  },
  {
    "text": "great cool okay I oh were there any talks on Ray Summit AT Ray Summit on Ray",
    "start": "2313680",
    "end": "2320280"
  },
  {
    "text": "turbo um so there",
    "start": "2320280",
    "end": "2325760"
  },
  {
    "text": "were a couple there wasn't one specific talk um on R turbo there were a variety",
    "start": "2325760",
    "end": "2332880"
  },
  {
    "text": "of different talks that covered topics that we covered um so for for example Edward gave a talk on Race serve um",
    "start": "2332880",
    "end": "2340920"
  },
  {
    "text": "which covered a allot of the bat uh the sorry not the batch difference the um",
    "start": "2340920",
    "end": "2346119"
  },
  {
    "text": "the serving work that and the mall loading work that he just showed and I",
    "start": "2346119",
    "end": "2351680"
  },
  {
    "text": "gave a talk on batch of FRS which goes into much more detail on like the bach and friends optimizations we've done um",
    "start": "2351680",
    "end": "2357680"
  },
  {
    "text": "I believe there was also some some talks on like LM Suites and like the all them",
    "start": "2357680",
    "end": "2362880"
  },
  {
    "text": "capabilities in on any scale so those were yeah there were I think you have to",
    "start": "2362880",
    "end": "2368280"
  },
  {
    "text": "dig a little bit but they were definitely",
    "start": "2368280",
    "end": "2371799"
  },
  {
    "text": "there all right okay we're gonna call it and um thank you everyone thank you",
    "start": "2374880",
    "end": "2383839"
  }
]