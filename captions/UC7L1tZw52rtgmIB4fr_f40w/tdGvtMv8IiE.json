[
  {
    "text": "uh and uh today I will be talking about scaling race serve uh from zero to",
    "start": "299",
    "end": "6839"
  },
  {
    "text": "infinity and back down to zero which is what the use case actually is uh so just",
    "start": "6839",
    "end": "13740"
  },
  {
    "text": "two sentences about me currently I'm working as a senior data scientist at outbreak uh but this was very recent it",
    "start": "13740",
    "end": "21000"
  },
  {
    "text": "doesn't really matter for this talk um however I will be talking about my uh best experience as a machine learning",
    "start": "21000",
    "end": "27300"
  },
  {
    "text": "engineer at hereka group which is a company in central eastern Europe uh",
    "start": "27300",
    "end": "32880"
  },
  {
    "text": "they do e-commerce and so on so um let's just get to it",
    "start": "32880",
    "end": "38880"
  },
  {
    "text": "um in this talk I will present a use case and an implementation of a custom",
    "start": "38880",
    "end": "44960"
  },
  {
    "text": "serve scaling controller uh you will see this is just an if else statement and",
    "start": "44960",
    "end": "50700"
  },
  {
    "text": "nothing else just because I promised to my followers",
    "start": "50700",
    "end": "56520"
  },
  {
    "text": "on LinkedIn I will touch a little bit on Ray finops or just how to",
    "start": "56520",
    "end": "62699"
  },
  {
    "text": "you know just how to think about um financials uh when adopting Ray",
    "start": "62699",
    "end": "68600"
  },
  {
    "text": "because that's I think that's quite important given how complex uh the array",
    "start": "68600",
    "end": "74400"
  },
  {
    "text": "tooling is um and and the last part will be my",
    "start": "74400",
    "end": "79560"
  },
  {
    "text": "experience with the ray developer Community which helped us out a lot during these past two years that we've",
    "start": "79560",
    "end": "87360"
  },
  {
    "text": "been using Ray so uh just a little bit of context",
    "start": "87360",
    "end": "92400"
  },
  {
    "text": "um we used uh Ray's ecosystem of libraries and tools as like this Central",
    "start": "92400",
    "end": "98159"
  },
  {
    "text": "Planet the central pillar upon which everything stands and we used it for",
    "start": "98159",
    "end": "103799"
  },
  {
    "text": "model inference so predictions model training as well as",
    "start": "103799",
    "end": "109640"
  },
  {
    "text": "and you will see this in the implementation this kind of provisioning of cloud compute so this is this is like",
    "start": "109640",
    "end": "117000"
  },
  {
    "text": "one big reason why we why I personally loved Rey because you",
    "start": "117000",
    "end": "122700"
  },
  {
    "text": "can do this kind of scaling um very easily and by by doing that we can be saving a",
    "start": "122700",
    "end": "131280"
  },
  {
    "text": "lot of costs um so um just like a quick project of you what",
    "start": "131280",
    "end": "136440"
  },
  {
    "text": "we were doing so uh Eureka group does e-commerce and there's a few use cases",
    "start": "136440",
    "end": "143220"
  },
  {
    "text": "there uh it these are typical classical e-commerce use cases one is product classification so you have",
    "start": "143220",
    "end": "150599"
  },
  {
    "text": "um information about the product and you need to put it in some kind of category uh just so that you have a nice category",
    "start": "150599",
    "end": "157260"
  },
  {
    "text": "a nice catalog of products presentable and available on your on your websites",
    "start": "157260",
    "end": "163980"
  },
  {
    "text": "and there's also product matching which is which is kind of like product classification but it's product to",
    "start": "163980",
    "end": "169739"
  },
  {
    "text": "product matching so you know offers from various Merchants",
    "start": "169739",
    "end": "177360"
  },
  {
    "text": "um you have offers of various merchants and you try to match them together under the same product entity so to speak uh",
    "start": "177360",
    "end": "185040"
  },
  {
    "text": "so I won't be going into that at all uh I just want to more briefly look at you",
    "start": "185040",
    "end": "191040"
  },
  {
    "text": "know how we used right in in these use cases so our goals were",
    "start": "191040",
    "end": "197640"
  },
  {
    "text": "basically when I was starting out as a machine learning engineer I was just very new to the whole discipline to the",
    "start": "197640",
    "end": "204780"
  },
  {
    "text": "I don't think machine learning operations was even a word back then um it's been thrown out around a lot",
    "start": "204780",
    "end": "211860"
  },
  {
    "text": "these days but I never really found it anywhere on the internet back then uh",
    "start": "211860",
    "end": "217260"
  },
  {
    "text": "and my my biggest concern was just okay we're moving to the cloud",
    "start": "217260",
    "end": "222299"
  },
  {
    "text": "we we got to keep inference costs as low as possible so just provisioning a few",
    "start": "222299",
    "end": "228180"
  },
  {
    "text": "computes here and there it was just quite scary to me and I was really",
    "start": "228180",
    "end": "234060"
  },
  {
    "text": "trying not to Skyrocket costs at all so that that was my biggest",
    "start": "234060",
    "end": "239519"
  },
  {
    "text": "priority um of course we wanted to kind of simplify Cloud Jupiter provisioning and management uh there's",
    "start": "239519",
    "end": "246420"
  },
  {
    "text": "um you know uh we are we were running gray on kubernetes and kubernetes",
    "start": "246420",
    "end": "252840"
  },
  {
    "text": "um it it's got a nice Primitives for scaling up and down but it doesn't have Primitives for scaling down to zero and",
    "start": "252840",
    "end": "259560"
  },
  {
    "text": "kubernetes 1.23 there is actually an alpha Feature Feature uh that's",
    "start": "259560",
    "end": "265820"
  },
  {
    "text": "basically you for the horizontal pod Auto scalar and you can actually query",
    "start": "265820",
    "end": "271440"
  },
  {
    "text": "external metrics and see uh by those metrics you can make a decision whether to scale down the replicas down to zero",
    "start": "271440",
    "end": "277699"
  },
  {
    "text": "so there's a few there's like some options outside of Ray for that but I",
    "start": "277699",
    "end": "283259"
  },
  {
    "text": "really haven't seen it at the time so that's why I wanted to use rain just because um it looked like we could be doing",
    "start": "283259",
    "end": "291060"
  },
  {
    "text": "um downscaling all the way down to zero and spending no money on on on compute",
    "start": "291060",
    "end": "297000"
  },
  {
    "text": "when we don't need it of course uh and of course uh I wanted to keep the",
    "start": "297000",
    "end": "303960"
  },
  {
    "text": "project called clean and we we had two use cases that you will see and we",
    "start": "303960",
    "end": "309660"
  },
  {
    "text": "wanted to use the same code for both um and by keeping everything under the",
    "start": "309660",
    "end": "315000"
  },
  {
    "text": "array ecosystem we wanted to reduce the tooling surface so that when new people",
    "start": "315000",
    "end": "320220"
  },
  {
    "text": "joined the company new people joined the team we they wouldn't be having you know onboarding that just lasts forever",
    "start": "320220",
    "end": "328500"
  },
  {
    "text": "um those were kind of like a few of my concerns but there were others as well",
    "start": "328500",
    "end": "335460"
  },
  {
    "text": "so let's get to the implementation right away um I think you can already see this is",
    "start": "335460",
    "end": "342120"
  },
  {
    "text": "something very familiar from the ray documentation we started out with a fast",
    "start": "342120",
    "end": "348300"
  },
  {
    "text": "API app uh it just wrapped up the model and put it to Docker and it worked for",
    "start": "348300",
    "end": "355740"
  },
  {
    "text": "us quite well actually I would say uh we didn't have really any problems with",
    "start": "355740",
    "end": "361380"
  },
  {
    "text": "that it was in production for I don't know at least a half a year and it just",
    "start": "361380",
    "end": "366660"
  },
  {
    "text": "worked um so but then we started thinking okay uh we",
    "start": "366660",
    "end": "373080"
  },
  {
    "text": "we obviously want to improve this model we want to take it into the future we wanted to grow we we saw ourselves using",
    "start": "373080",
    "end": "380580"
  },
  {
    "text": "multimodal inference so we'd have text and images and all",
    "start": "380580",
    "end": "385680"
  },
  {
    "text": "sorts of other features that we need to calculate and you know have different process or deployments and all of that",
    "start": "385680",
    "end": "392940"
  },
  {
    "text": "all of that was somewhere up there in inside our heads we really wanted to do it then we wanted to do scaling correctly because we saw that either our",
    "start": "392940",
    "end": "401039"
  },
  {
    "text": "code will get very messy or I don't know we'll be spending a lot of on cloud compute if we don't really build the",
    "start": "401039",
    "end": "407520"
  },
  {
    "text": "correct tools so uh the first thing was of course to just wrap our app into or",
    "start": "407520",
    "end": "413819"
  },
  {
    "text": "our model into a self-deployment um and we started from that",
    "start": "413819",
    "end": "420300"
  },
  {
    "text": "so our first use case was on-demand inference so",
    "start": "420300",
    "end": "426060"
  },
  {
    "text": "uh we were supporting our internal um internal departments uh for the product",
    "start": "426060",
    "end": "433800"
  },
  {
    "text": "classification case and our models needed to be there serving people from",
    "start": "433800",
    "end": "439740"
  },
  {
    "text": "7am to 5 PM that's roughly 50 hours a week so it was just a quality of service",
    "start": "439740",
    "end": "445560"
  },
  {
    "text": "guarantee of course these models were idle most of the time because humans just don't work as fast as possible they",
    "start": "445560",
    "end": "452400"
  },
  {
    "text": "need to get predictions they needed to get suggestions for the product categories and uh yeah this is something",
    "start": "452400",
    "end": "460199"
  },
  {
    "text": "that just our stakeholders required of us so we were serving these models from 7am to 5 PM but of course when you're",
    "start": "460199",
    "end": "467900"
  },
  {
    "text": "when you have so much compute being wasted you're starting to think okay maybe we can",
    "start": "467900",
    "end": "474240"
  },
  {
    "text": "scale this down a little bit I mean can we just spin down the instances while people are not working that seems",
    "start": "474240",
    "end": "481440"
  },
  {
    "text": "reasonable right and uh the Second Use case uh was the",
    "start": "481440",
    "end": "487080"
  },
  {
    "text": "batch inference jobs this this was for quite a different Tool uh it it's got",
    "start": "487080",
    "end": "492539"
  },
  {
    "text": "more to do on with automation of classification of classification of",
    "start": "492539",
    "end": "498900"
  },
  {
    "text": "products and there were just some very specific let's",
    "start": "498900",
    "end": "504539"
  },
  {
    "text": "say um architectural constraints basically these bad jobs they had to",
    "start": "504539",
    "end": "510660"
  },
  {
    "text": "fully process all their workloads before the start of the work week and there was",
    "start": "510660",
    "end": "515880"
  },
  {
    "text": "just let's say a relatively short window for that and we just couldn't use the CPUs for that",
    "start": "515880",
    "end": "521640"
  },
  {
    "text": "so the CPUs were just too slow and that's why we started thinking about GPU",
    "start": "521640",
    "end": "526680"
  },
  {
    "text": "instances of course gpus are very expensive and so we needed to",
    "start": "526680",
    "end": "533060"
  },
  {
    "text": "have the list time of this gpus very low and we just couldn't place this on gpus these models on gpus",
    "start": "533060",
    "end": "541740"
  },
  {
    "text": "and just let them run and waste carbon like just increase our carbon footprint",
    "start": "541740",
    "end": "547560"
  },
  {
    "text": "and all of that right and we really needed to scale these these models down at least the GPU",
    "start": "547560",
    "end": "554339"
  },
  {
    "text": "instances so we came up with this",
    "start": "554339",
    "end": "560100"
  },
  {
    "text": "um Ray um surf you uses this um Ray serve API",
    "start": "560100",
    "end": "566760"
  },
  {
    "text": "or at the handle API which uh we we used uh to to to control the the the model",
    "start": "566760",
    "end": "575279"
  },
  {
    "text": "deployments themselves so my model was from the previous slide and what you can do with this is you can actually",
    "start": "575279",
    "end": "582660"
  },
  {
    "text": "um just just query Ray surf objects uh inside surf",
    "start": "582660",
    "end": "589440"
  },
  {
    "text": "serve deployments like normal python objects and what we did was we defined a",
    "start": "589440",
    "end": "595500"
  },
  {
    "text": "simple controller uh there's uh you you basically make a HTTP put request to uh",
    "start": "595500",
    "end": "602580"
  },
  {
    "text": "an HTTP handle and um what we wanted was to be able to",
    "start": "602580",
    "end": "610860"
  },
  {
    "text": "um increase the number of replicas for a specific model which we referred to by",
    "start": "610860",
    "end": "616860"
  },
  {
    "text": "the name and of course specify the number of replicas that we did which",
    "start": "616860",
    "end": "622500"
  },
  {
    "text": "usually was either zero or one and also whether this model",
    "start": "622500",
    "end": "628740"
  },
  {
    "text": "was going to be placed on a regular instance or on an instance with a GPU and we just let Ray do all the magic of",
    "start": "628740",
    "end": "637620"
  },
  {
    "text": "provisioning under the hood and that would be it so if this controller",
    "start": "637620",
    "end": "643019"
  },
  {
    "text": "got a valid name for a model and",
    "start": "643019",
    "end": "649380"
  },
  {
    "text": "yeah said Okay I want to spin it down then we just simply called the delete method on the deployment",
    "start": "649380",
    "end": "657240"
  },
  {
    "text": "object if it received something more than that we we just said okay we just",
    "start": "657240",
    "end": "663420"
  },
  {
    "text": "deployed the model um as this and and I think I think we",
    "start": "663420",
    "end": "669480"
  },
  {
    "text": "didn't really needed to care whether the model was already deployed or not because um Ray just does everything under the",
    "start": "669480",
    "end": "675360"
  },
  {
    "text": "hood so this is a super low code solution to",
    "start": "675360",
    "end": "680579"
  },
  {
    "text": "um model scaling um why did we need to do this because at",
    "start": "680579",
    "end": "687480"
  },
  {
    "text": "the time I think it was early 2021 uh array did not yet include any any sort",
    "start": "687480",
    "end": "694320"
  },
  {
    "text": "of Auto scaling in Surf and I'll get to this uh we we had a short collaboration",
    "start": "694320",
    "end": "700980"
  },
  {
    "text": "with the uh Ray serve team uh we we told them our use case what we wanted and so",
    "start": "700980",
    "end": "706740"
  },
  {
    "text": "on but it was I think I think there was some issues with the autoscaler back then so uh we just developed this and it",
    "start": "706740",
    "end": "715380"
  },
  {
    "text": "was validated by the surf team and we just went along with this one",
    "start": "715380",
    "end": "720660"
  },
  {
    "text": "um okay so what did we gain from this okay as far as on-demand influence goes",
    "start": "720660",
    "end": "726600"
  },
  {
    "text": "uh we basically achieved the uses reduction factor of of uh 0.3 which",
    "start": "726600",
    "end": "733260"
  },
  {
    "text": "means that we were serving the models um for 50 hours per week and also",
    "start": "733260",
    "end": "740579"
  },
  {
    "text": "um and the rest of the 168 minus 50 hours",
    "start": "740579",
    "end": "745620"
  },
  {
    "text": "um those hours were simply those over the hours that we saved however this is not",
    "start": "745620",
    "end": "752160"
  },
  {
    "text": "like a huge saving because it's just CPU right so um",
    "start": "752160",
    "end": "757200"
  },
  {
    "text": "okay so it's not a big saving but in in terms of money but but it is okay and",
    "start": "757200",
    "end": "762959"
  },
  {
    "text": "then the second one was this batch Increase jobs if we if we kept these gpus up and running all all the time",
    "start": "762959",
    "end": "769860"
  },
  {
    "text": "um it just it just wouldn't be financially feasible for us so we only use them for 30 minutes per model per",
    "start": "769860",
    "end": "778139"
  },
  {
    "text": "week and with this uh uh we achieved the reduction factor of you know this one",
    "start": "778139",
    "end": "785700"
  },
  {
    "text": "one 1000 uh of what we'd actually spent and this was like quite a big saving and",
    "start": "785700",
    "end": "792180"
  },
  {
    "text": "this this actually was kind of um this was like the big reason why uh like this",
    "start": "792180",
    "end": "798899"
  },
  {
    "text": "was the big result that kind of Justified going the the race sort of uh way",
    "start": "798899",
    "end": "805019"
  },
  {
    "text": "okay um of course um you you might be thinking okay",
    "start": "805019",
    "end": "811980"
  },
  {
    "text": "um but you know people might not be working from seven to five or nine to five",
    "start": "811980",
    "end": "819600"
  },
  {
    "text": "um the some of them might be working from I don't know in in early morning or",
    "start": "819600",
    "end": "826440"
  },
  {
    "text": "or they're more of an evening working person and uh what we'd actually do is",
    "start": "826440",
    "end": "832380"
  },
  {
    "text": "uh you you notice that the deployment controller was wrapped in a and",
    "start": "832380",
    "end": "837660"
  },
  {
    "text": "what what we actually did was we um we uh used this uh deployment controller",
    "start": "837660",
    "end": "845160"
  },
  {
    "text": "as as a router so we so every request was uh evaluated uh on which model it",
    "start": "845160",
    "end": "853680"
  },
  {
    "text": "was supposed to go to actually this if statement is missing but basically what we do is just we just forward these",
    "start": "853680",
    "end": "860839"
  },
  {
    "text": "these requests to the actual models that that were being used and so",
    "start": "860839",
    "end": "866820"
  },
  {
    "text": "um uh this magic right here what it does is it just gets the handle of the model",
    "start": "866820",
    "end": "872279"
  },
  {
    "text": "that we are supposed to use and these handles are cached uh and then just",
    "start": "872279",
    "end": "879480"
  },
  {
    "text": "creates another HTTP request to that to that model instance and just Returns the",
    "start": "879480",
    "end": "886740"
  },
  {
    "text": "result back through this through this deployment controller and what this does actually is if the if the model replica",
    "start": "886740",
    "end": "894360"
  },
  {
    "text": "is down to zero uh this code actually re-initializes uh the the model uh",
    "start": "894360",
    "end": "902220"
  },
  {
    "text": "deployment through the serve handle API and unfortunately this um hand this",
    "start": "902220",
    "end": "909300"
  },
  {
    "text": "serve handle API is being deprecated um but I think",
    "start": "909300",
    "end": "915120"
  },
  {
    "text": "uh the the racer team is aware that users want this this sort of functionality",
    "start": "915120",
    "end": "921720"
  },
  {
    "text": "um in in future versions of Ray so um yeah kind of uh hoping that we'd get",
    "start": "921720",
    "end": "929160"
  },
  {
    "text": "something similar in the future so what happens uh when a user is you know using",
    "start": "929160",
    "end": "935639"
  },
  {
    "text": "the the API uh they get they want to get predictions uh what happens is on the",
    "start": "935639",
    "end": "942180"
  },
  {
    "text": "first prediction when the server replicas are down to zero this code autoagically calls the I mean it",
    "start": "942180",
    "end": "950160"
  },
  {
    "text": "triggers the serve uh not the serve but the ray cluster autoscaler to request a",
    "start": "950160",
    "end": "957180"
  },
  {
    "text": "new pod and then kubray or Ray operator then request resources to to be able to",
    "start": "957180",
    "end": "964019"
  },
  {
    "text": "to get that up and running and if you set the health checks uh High",
    "start": "964019",
    "end": "970199"
  },
  {
    "text": "Enough uh in a few minutes time these requests will start being served again so",
    "start": "970199",
    "end": "979320"
  },
  {
    "text": "um yeah so um actually going back to this",
    "start": "979320",
    "end": "984480"
  },
  {
    "text": "um we still needed to make HTTP requests to to the to this uh to this uh",
    "start": "984480",
    "end": "991440"
  },
  {
    "text": "controller and what what we did with that was actually we used a schedule for that",
    "start": "991440",
    "end": "997380"
  },
  {
    "text": "um so um we just had a crown uh definition that said okay make an HTTP request to",
    "start": "997380",
    "end": "1005060"
  },
  {
    "text": "this to this uh deployment controller at this time with instruct with these",
    "start": "1005060",
    "end": "1011899"
  },
  {
    "text": "parameters that we needed for either scaling up or scaling down and that's pretty much it",
    "start": "1011899",
    "end": "1019100"
  },
  {
    "text": "so um now this solution that you saw um is just very low code like even what I",
    "start": "1019100",
    "end": "1027678"
  },
  {
    "text": "showed here on the slides it's not that different from the actual production um however",
    "start": "1027679",
    "end": "1033620"
  },
  {
    "text": "um we have to realize that Ray on kubernetes well what we were",
    "start": "1033620",
    "end": "1038660"
  },
  {
    "text": "using uh actually requires multiple layers of resource managers and the first one is the ray cluster autoscaler",
    "start": "1038660",
    "end": "1044480"
  },
  {
    "text": "of course there's also the surf Auto scaler which is a different kind of mechanism but direct cluster autoscaler",
    "start": "1044480",
    "end": "1051440"
  },
  {
    "text": "uh works on the ray cluster level and it just manages survectors then there's",
    "start": "1051440",
    "end": "1058640"
  },
  {
    "text": "qbre or Ray operator which manages kubernetes spots and whole ring cluster",
    "start": "1058640",
    "end": "1064820"
  },
  {
    "text": "um uh resources themselves as well as you need some kind of resource manager",
    "start": "1064820",
    "end": "1071720"
  },
  {
    "text": "outside of that which would provision you compete instances uh in our case it was Google kubernetes engine but you",
    "start": "1071720",
    "end": "1078860"
  },
  {
    "text": "know if you're on AWS or or Azure or any other sort of cloud provider I think Ray",
    "start": "1078860",
    "end": "1085220"
  },
  {
    "text": "has support for that too now all of this is nice because all of the complexity is",
    "start": "1085220",
    "end": "1091160"
  },
  {
    "text": "tucked away on their array but this does this complexity City doesn't really go away you still need to kind of manage it",
    "start": "1091160",
    "end": "1098240"
  },
  {
    "text": "and so uh what we found out was that you know simple box can really throw a wrench",
    "start": "1098240",
    "end": "1105140"
  },
  {
    "text": "into your deliverables and um we we Face some bugs that were",
    "start": "1105140",
    "end": "1111200"
  },
  {
    "text": "actually quite quite frustrating to deal with because we just simply didn't have the expertise to be dealing with",
    "start": "1111200",
    "end": "1116419"
  },
  {
    "text": "deploying great clusters ourselves on that on that level so I think this level",
    "start": "1116419",
    "end": "1123200"
  },
  {
    "text": "of complexities must be somehow Justified so either you'd have costly Hardware so gpus or something or you",
    "start": "1123200",
    "end": "1131179"
  },
  {
    "text": "have deep inference grabs or you expect the inference graphs to grow as time",
    "start": "1131179",
    "end": "1136220"
  },
  {
    "text": "goes on um or you'd have like a multiplicity of models to serve so you'd have many",
    "start": "1136220",
    "end": "1143480"
  },
  {
    "text": "models that you need to to serve to different stakeholders and customers and basically the rate cluster needs to be",
    "start": "1143480",
    "end": "1149780"
  },
  {
    "text": "quite big it needs to cost a lot for for these kind of mechanisms um or just deployment in general to to be worth it",
    "start": "1149780",
    "end": "1156860"
  },
  {
    "text": "on right um so that's just my promise to my to my LinkedIn followers that I will talk a",
    "start": "1156860",
    "end": "1162440"
  },
  {
    "text": "little bit a little bit about this so it's not all in in Cloud compute it's",
    "start": "1162440",
    "end": "1168620"
  },
  {
    "text": "also about managing this complexity okay now",
    "start": "1168620",
    "end": "1175160"
  },
  {
    "text": "this is like the big reason why why I was invited to talk here it's just my",
    "start": "1175160",
    "end": "1181100"
  },
  {
    "text": "experience with the ray Community um so here's how it goes uh it this will",
    "start": "1181100",
    "end": "1186200"
  },
  {
    "text": "be just one slide but if I get too long just cut me off please okay um so and of course you know you can",
    "start": "1186200",
    "end": "1193880"
  },
  {
    "text": "maybe ask questions um maybe in the end if if you want so",
    "start": "1193880",
    "end": "1200480"
  },
  {
    "text": "so my first uh uh yeah the first time I kind of came across Rey was actually for",
    "start": "1200480",
    "end": "1206660"
  },
  {
    "text": "the hyper parameter search Library Ray tune um so we needed to to do the hyper",
    "start": "1206660",
    "end": "1213140"
  },
  {
    "text": "parameter search for for a few parameters I don't think it was really Justified to to to really go into retune",
    "start": "1213140",
    "end": "1220520"
  },
  {
    "text": "just Just For Those few parameters but we we were evaluating quite large",
    "start": "1220520",
    "end": "1227539"
  },
  {
    "text": "um models a slow models and we wanted to parallelize that somehow so that's how I",
    "start": "1227539",
    "end": "1233240"
  },
  {
    "text": "got to know the ray ecosystem and um shortener like uh shortly after uh we",
    "start": "1233240",
    "end": "1240860"
  },
  {
    "text": "figured out that actually we want to do multimodal model deployments and",
    "start": "1240860",
    "end": "1247100"
  },
  {
    "text": "we started thinking really hard about scalability and uh shortly after that we",
    "start": "1247100",
    "end": "1253340"
  },
  {
    "text": "decided to put Ray cluster into an on-prem kubernetes setup",
    "start": "1253340",
    "end": "1258799"
  },
  {
    "text": "um this worked quite well for a lot of time actually um it was a little bit harder after we we",
    "start": "1258799",
    "end": "1266240"
  },
  {
    "text": "started the migration to the cloud it was a little bit harder to just to get to know the specifics of our",
    "start": "1266240",
    "end": "1272720"
  },
  {
    "text": "Google kubernetes engine setup and how Ray would work with that like there's a bunch of different uh",
    "start": "1272720",
    "end": "1279620"
  },
  {
    "text": "constraints there for example security constraints secret pod security settings",
    "start": "1279620",
    "end": "1284780"
  },
  {
    "text": "and so on that were really not um expected I think by the the rate team",
    "start": "1284780",
    "end": "1291380"
  },
  {
    "text": "and the documentation uh you really needed additional expertise uh on that",
    "start": "1291380",
    "end": "1296720"
  },
  {
    "text": "just so that you you know things like okay where can I write Ray logs",
    "start": "1296720",
    "end": "1302720"
  },
  {
    "text": "um how can I do that when you can't really you know things like that you know if you have a strict security uh",
    "start": "1302720",
    "end": "1309620"
  },
  {
    "text": "context that means that you need to mount a a volume somewhere so that you",
    "start": "1309620",
    "end": "1316100"
  },
  {
    "text": "can ride the locks and things like that so um it was kind of challenging just to",
    "start": "1316100",
    "end": "1322039"
  },
  {
    "text": "get ray uh up and running um because there was no really guidance",
    "start": "1322039",
    "end": "1327080"
  },
  {
    "text": "anywhere um after that we started uh talking um",
    "start": "1327080",
    "end": "1332659"
  },
  {
    "text": "with the surf team uh regarding Auto scaling on the serve level",
    "start": "1332659",
    "end": "1339080"
  },
  {
    "text": "um and what we wanted to do was we wanted to get rid of the controller for",
    "start": "1339080",
    "end": "1345799"
  },
  {
    "text": "scaling and just use something that would be even more low code and that would be something baked into",
    "start": "1345799",
    "end": "1351980"
  },
  {
    "text": "serve itself and we we really wanted to use the auto scaler but as it turns out",
    "start": "1351980",
    "end": "1358159"
  },
  {
    "text": "uh the initial Auto scaler did not have support for scaling down to zero I'm not",
    "start": "1358159",
    "end": "1363500"
  },
  {
    "text": "sure about these days I think it it does support that but I'm not 100 sure in the",
    "start": "1363500",
    "end": "1369860"
  },
  {
    "text": "end I think we can just kind of ran out of time and we actually didn't use it uh",
    "start": "1369860",
    "end": "1375620"
  },
  {
    "text": "but um we we took a second look at it uh just before I left the company and we",
    "start": "1375620",
    "end": "1381980"
  },
  {
    "text": "noticed that um it's still not like it's it's quite configurable if you're making HTTP",
    "start": "1381980",
    "end": "1388640"
  },
  {
    "text": "requests and that's all you need but I think the autoscaler still needs a little bit of work uh like we opened a",
    "start": "1388640",
    "end": "1394700"
  },
  {
    "text": "feature request for for maybe diversifying the metrics uh upon which the server to scaler does decisions",
    "start": "1394700",
    "end": "1401360"
  },
  {
    "text": "whether to scale down or up or not and I'm not sure about the status of that but",
    "start": "1401360",
    "end": "1407480"
  },
  {
    "text": "um yeah there is like basic I would say there is now basic support um as we were collaborating with surf",
    "start": "1407480",
    "end": "1414620"
  },
  {
    "text": "team so I think that collaboration was all in all quite fruitful and I learned",
    "start": "1414620",
    "end": "1419720"
  },
  {
    "text": "a lot about a rate just because that that channel of communication was always available to us",
    "start": "1419720",
    "end": "1426679"
  },
  {
    "text": "uh with any skills so um really props for that",
    "start": "1426679",
    "end": "1432620"
  },
  {
    "text": "and the fourth chapter was uh there were some like dormant memories in in the ray",
    "start": "1432620",
    "end": "1440179"
  },
  {
    "text": "cluster itself and did these were addressed in various uh by various pull",
    "start": "1440179",
    "end": "1447080"
  },
  {
    "text": "requests not just hours uh I mean not just our our race issues but other",
    "start": "1447080",
    "end": "1452419"
  },
  {
    "text": "people as well were raising them and did this actually quite this is actually quite a big topic I'm not sure if I have",
    "start": "1452419",
    "end": "1458600"
  },
  {
    "text": "time for it but basically uh that there were some memory leaks related to the",
    "start": "1458600",
    "end": "1463940"
  },
  {
    "text": "dashboard processes and we we wanted to keep the ray head note",
    "start": "1463940",
    "end": "1470539"
  },
  {
    "text": "uh on low uh memory and what happened was in two three weeks",
    "start": "1470539",
    "end": "1478460"
  },
  {
    "text": "um the the three gigabyte or four gigabyte uh memory that that we had for",
    "start": "1478460",
    "end": "1483740"
  },
  {
    "text": "the ray had was simply filled out and the the whole cluster crashed and",
    "start": "1483740",
    "end": "1489620"
  },
  {
    "text": "because we were routing all the requests through through the through the Ingress which was placed on the ray head node uh",
    "start": "1489620",
    "end": "1497299"
  },
  {
    "text": "you know uh the the whole basically the whole functionality just kind of crashed so",
    "start": "1497299",
    "end": "1502580"
  },
  {
    "text": "um this was something like this was like a really big Saga for us just because uh we we we thought we fixed it or we",
    "start": "1502580",
    "end": "1509480"
  },
  {
    "text": "thought we made some kind of um you know um some kind of uh",
    "start": "1509480",
    "end": "1516860"
  },
  {
    "text": "um I don't know some kind of roundabout way of fixing it or whatever and uh in",
    "start": "1516860",
    "end": "1522440"
  },
  {
    "text": "the editors out actually no it's something completely different and we really had to like iterate we had this kind of slow iteration Cycles so that we",
    "start": "1522440",
    "end": "1529580"
  },
  {
    "text": "finally got to the end of what was actually happening um anyway again thanks for Ray for",
    "start": "1529580",
    "end": "1536360"
  },
  {
    "text": "fixing it uh it was like a really tough challenge I think for for even them so I'm really glad we we sorted that out uh",
    "start": "1536360",
    "end": "1544220"
  },
  {
    "text": "and the final one was uh the ray 2.0 migration uh these fixes for the",
    "start": "1544220",
    "end": "1551120"
  },
  {
    "text": "memory leaks were placed into the ray two branch and yeah there was some other",
    "start": "1551120",
    "end": "1557539"
  },
  {
    "text": "introductions there for example kubrey was bumped up to be the recommended way",
    "start": "1557539",
    "end": "1562760"
  },
  {
    "text": "of um serving gray on kubernetes and due to",
    "start": "1562760",
    "end": "1568039"
  },
  {
    "text": "our setup it was really quite challenging to to find the time to to do",
    "start": "1568039",
    "end": "1574640"
  },
  {
    "text": "the migration but in the end we did it and I think now the the red cluster as I left the company I think it's in really",
    "start": "1574640",
    "end": "1580880"
  },
  {
    "text": "good shape hopefully and um yeah that's that's our experience",
    "start": "1580880",
    "end": "1585980"
  },
  {
    "text": "with right now if anyone has any questions I'm happy otherwise thank you",
    "start": "1585980",
    "end": "1591740"
  },
  {
    "text": "for uh listening can you guys hear me now yes yes okay",
    "start": "1591740",
    "end": "1597260"
  },
  {
    "text": "well thanks a lot for the um for the questions uh thanks a lot for the presentation",
    "start": "1597260",
    "end": "1603260"
  },
  {
    "text": "um I think I might be able to actually set the context for some of the issues which were actually raised by mayhouse",
    "start": "1603260",
    "end": "1610400"
  },
  {
    "text": "so if you guys have any questions we have the researching board please put your questions on the Q a panel so we",
    "start": "1610400",
    "end": "1617240"
  },
  {
    "text": "can address them now that my mic is working I might be able to take maybe another 15 or 20 minutes just to give",
    "start": "1617240",
    "end": "1623120"
  },
  {
    "text": "you an overview of rare which sort of sets the context because I was supposed to talk first to set the context award",
    "start": "1623120",
    "end": "1628700"
  },
  {
    "text": "me I was going to talk about and what David is going to talk about so if you guys don't have any questions I can just",
    "start": "1628700",
    "end": "1634279"
  },
  {
    "text": "jump into my presentation it'll take me about 15 minutes just to give you a context of what race is all about and",
    "start": "1634279",
    "end": "1640100"
  },
  {
    "text": "what are some of the things that we actually done in 2.0 to mitigate some of the issues that Mia had actually",
    "start": "1640100",
    "end": "1645919"
  },
  {
    "text": "experienced and then since they have gone on the path of migrating 2.x people",
    "start": "1645919",
    "end": "1651260"
  },
  {
    "text": "will see a huge huge benefits that we actually have put in in Rio 2.0 as as",
    "start": "1651260",
    "end": "1657020"
  },
  {
    "text": "that so I don't see any questions coming on the qns I'm going to go ahead and share my screen and I'll present",
    "start": "1657020",
    "end": "1664279"
  },
  {
    "text": "um the overview of what the state of race service is today and then we'll get into uh final the the most important",
    "start": "1664279",
    "end": "1670220"
  },
  {
    "text": "talk about about David so let me go ahead and share my screen over here skip in minutes",
    "start": "1670220",
    "end": "1676840"
  },
  {
    "text": "and we should start momentum okay brilliant this was supposed to be the",
    "start": "1676840",
    "end": "1682220"
  },
  {
    "text": "first slide this was supposed to be the introduction I was supposed to give on unfortunately that we had second couple programs so I'm just going to jump into",
    "start": "1682220",
    "end": "1688039"
  },
  {
    "text": "the state of race of we have a couple of Finance I wanted to make we have a race Summit which is coming up",
    "start": "1688039",
    "end": "1694539"
  },
  {
    "text": "in in September um 18th through the 19th and it's going",
    "start": "1694539",
    "end": "1699860"
  },
  {
    "text": "to be in San Francisco all in person those of you who are interested to share your race stories is as to our Board of",
    "start": "1699860",
    "end": "1707059"
  },
  {
    "text": "speakers indeed talk about the cfps are open till March the 6th it used to be uh open until February 6th but not February",
    "start": "1707059",
    "end": "1714260"
  },
  {
    "text": "17 but now we are extended by a couple of weeks um so go ahead and and and submit in the",
    "start": "1714260",
    "end": "1719539"
  },
  {
    "text": "cap that you actually might want and that's a URL I'm going to share the slides with the community so you actually have that but it is open till",
    "start": "1719539",
    "end": "1726200"
  },
  {
    "text": "March the 6th and we would love to hear about your experiences with Ray in general and in particular any use cases",
    "start": "1726200",
    "end": "1731720"
  },
  {
    "text": "you're actually using with respect to raise so any of the components that you actually might have a couple of things",
    "start": "1731720",
    "end": "1737419"
  },
  {
    "text": "that I wanted to mention we're actually doing a rain workshop and this is mainly for people who are in the Bay Area and a",
    "start": "1737419",
    "end": "1743779"
  },
  {
    "text": "few of you might have joined us from the Bay Area this is in person we're giving a two-day eight-hour workshops on Ray",
    "start": "1743779",
    "end": "1749779"
  },
  {
    "text": "about everything you actually want to know about array those are the instructors and if you actually want to join us it is on May the 11th and the",
    "start": "1749779",
    "end": "1757039"
  },
  {
    "text": "12th we call that Ray Saturday but I think it's going to be very Saturday and Sunday it's going to be loads of fun so if you're in the Bay Area",
    "start": "1757039",
    "end": "1763580"
  },
  {
    "text": "um feel free to to join us we have done the race Saturday before it was really good success and we will repeat and and",
    "start": "1763580",
    "end": "1769820"
  },
  {
    "text": "double down on that a couple of meetups are coming up um a few new raid books are on the uh on",
    "start": "1769820",
    "end": "1778340"
  },
  {
    "text": "the publishing already we have learning Ray which is coming up from uh from from the the the original members of The Ray",
    "start": "1778340",
    "end": "1785659"
  },
  {
    "text": "team and then there's another one from the community by Holden Crow they're both available for for early release on",
    "start": "1785659",
    "end": "1793100"
  },
  {
    "text": "O'Reilly's they have affiliate and go for it now I'm just going to go straight into the state of racism because I was",
    "start": "1793100",
    "end": "1798919"
  },
  {
    "text": "supposed to give this talk to set the context okay here we go",
    "start": "1798919",
    "end": "1804799"
  },
  {
    "text": "so what I'm going to do is very quickly give you an overview of of where we are",
    "start": "1804799",
    "end": "1811159"
  },
  {
    "text": "in respective rate what are the actual motivations going from 1.x to 2.x and then double down on a couple of the",
    "start": "1811159",
    "end": "1816980"
  },
  {
    "text": "highlights that we actually talked about one is the motor composition which has been a new API that actually has been",
    "start": "1816980",
    "end": "1822380"
  },
  {
    "text": "introduced in 2.0 which is a developer review and some of the auto scaling that we are actually implemented as part of",
    "start": "1822380",
    "end": "1828380"
  },
  {
    "text": "the race of 2.0 which addresses some of the issues that we actually had before such as zero scaling and making sure",
    "start": "1828380",
    "end": "1835100"
  },
  {
    "text": "that when when traffic is not very upskill and scale down and then the production hardening about what is it",
    "start": "1835100",
    "end": "1841640"
  },
  {
    "text": "for racer to be uh uh deployed in production and in particular the focus that we hear was on on on kubernetes in",
    "start": "1841640",
    "end": "1849320"
  },
  {
    "text": "kubray and that's where most of our users are so we actually focused on that particular part unfortunately because of",
    "start": "1849320",
    "end": "1854720"
  },
  {
    "text": "the time constraint I really won't get back into into too much of the kubernetes but I have two talks and I'll",
    "start": "1854720",
    "end": "1859760"
  },
  {
    "text": "arrange another talk about productionizing Ray uh race on kubernetes so stay tuned for that so let",
    "start": "1859760",
    "end": "1866659"
  },
  {
    "text": "me just get into the outline those of you who actually have been following the history of rape from the early days",
    "start": "1866659",
    "end": "1872000"
  },
  {
    "text": "might have heard about the rare AI runtime and the air runtime is really our end-to-end toolkit that was",
    "start": "1872000",
    "end": "1878779"
  },
  {
    "text": "announced as part of the radar 2.0 and it's it's a unified set of apis that allow you to and build an end-to-end",
    "start": "1878779",
    "end": "1884840"
  },
  {
    "text": "application and Ray so is an integral part of it and today we want to focus on how we can",
    "start": "1884840",
    "end": "1890600"
  },
  {
    "text": "actually use race serve the 2.0 apis to build an end-to-end uh inference for Ray",
    "start": "1890600",
    "end": "1896440"
  },
  {
    "text": "and and just to sort of give you a high level overview I can use three",
    "start": "1896440",
    "end": "1901940"
  },
  {
    "text": "adjectives to actually describe raids very flexible it's very scalable and it's efficient for compute online",
    "start": "1901940",
    "end": "1907760"
  },
  {
    "text": "influencer what do I mean by that now the fact that Ray is actually Race Service is built on top of Ray we",
    "start": "1907760",
    "end": "1914720"
  },
  {
    "text": "inherit inherently all the scalability benefits that you can actually get from Ray uh",
    "start": "1914720",
    "end": "1921320"
  },
  {
    "text": "from from scaling a broadcaster you get the low latency because of all the network efficiency we actually have",
    "start": "1921320",
    "end": "1926659"
  },
  {
    "text": "using all the grpc calls they may actually have and finally uh you get the efficiency uh Straight Out of the Box",
    "start": "1926659",
    "end": "1932600"
  },
  {
    "text": "because Ray we continue to make performance Improvement as we continue to fix issues and that sort of gets uh",
    "start": "1932600",
    "end": "1939860"
  },
  {
    "text": "um uh bubbled up to whatever whatever the applications that built on top of that which is actually great so the",
    "start": "1939860",
    "end": "1945620"
  },
  {
    "text": "second thing I wanted to concentrate today is on the composition API which is really really Central to be able to",
    "start": "1945620",
    "end": "1951919"
  },
  {
    "text": "scale and deploy models because if you look at machine learning models in general they don't exist in isolation",
    "start": "1951919",
    "end": "1957740"
  },
  {
    "text": "they exists exist in cohesiveness with other models so I want to double down on that one and then everybody loves python",
    "start": "1957740",
    "end": "1964520"
  },
  {
    "text": "right we want to make sure that the the race of apis are python specific and today you're going to be building mixing",
    "start": "1964520",
    "end": "1970640"
  },
  {
    "text": "models with your business logic written in Python along with the latest and greatest machine learning so those were",
    "start": "1970640",
    "end": "1976640"
  },
  {
    "text": "the three pillars those are the three ingredients that sort of what the motivation behind actually making sure",
    "start": "1976640",
    "end": "1981980"
  },
  {
    "text": "that racer delivered those and in order to understand how those manifest itself in in racial I wanted to sort of walk",
    "start": "1981980",
    "end": "1990500"
  },
  {
    "text": "through a couple of applications that that are very prevalent and and will use",
    "start": "1990500",
    "end": "1995840"
  },
  {
    "text": "this particular application as an inference engine so I'm going to keep that depending on mine now here we",
    "start": "1995840",
    "end": "2001299"
  },
  {
    "text": "actually have an image uh where you want to feed into some machine learning model right or machine learning functions and",
    "start": "2001299",
    "end": "2007960"
  },
  {
    "text": "out out comes some sort of uh uh uh a structured output that says what was",
    "start": "2007960",
    "end": "2013779"
  },
  {
    "text": "detected on that particular image in this case it's a hummingbird what was the confidence score uh that you feel",
    "start": "2013779",
    "end": "2020919"
  },
  {
    "text": "the good their creation is good and then finally some sort of pigmentation uh pixel so we have a segmentation or a",
    "start": "2020919",
    "end": "2027640"
  },
  {
    "text": "drawing box around the around the image so this is a very typical inference",
    "start": "2027640",
    "end": "2032919"
  },
  {
    "text": "engine that you could actually do that knew that now how How would how would this actually benefit itself in",
    "start": "2032919",
    "end": "2039279"
  },
  {
    "text": "if you were to build that as an end-to-end infant serving uh in your in your application so you probably would",
    "start": "2039279",
    "end": "2045640"
  },
  {
    "text": "have some sort of an application that's actually downloading the image from an S3 bucket or from from some uh data",
    "start": "2045640",
    "end": "2052118"
  },
  {
    "text": "store you're probably going to feed it to a preprocessor to put some filters into transform the images to to do some",
    "start": "2052119",
    "end": "2058540"
  },
  {
    "text": "sort of cropping of the image and then finally you might actually have a set of image classifiers that you dispatch to",
    "start": "2058540",
    "end": "2064839"
  },
  {
    "text": "and then based on the processor output it might be based on the user ID it might be based on an SQ it might be",
    "start": "2064839",
    "end": "2071200"
  },
  {
    "text": "based on a geographic location such as gpus I mean ZIP code or it might be on a longitude and latitude and in Nintendo",
    "start": "2071200",
    "end": "2078520"
  },
  {
    "text": "you actually might want to feed the the image process to image sector that actually sort of does the the",
    "start": "2078520",
    "end": "2084700"
  },
  {
    "text": "identification and this and the object segmentation and then you want to combine those results into a structured",
    "start": "2084700",
    "end": "2090460"
  },
  {
    "text": "output that you can actually somehow use it and you've actually were to",
    "start": "2090460",
    "end": "2095580"
  },
  {
    "text": "implement this as as an end-to-end application there are some strict requirements you actually want to make",
    "start": "2095580",
    "end": "2101740"
  },
  {
    "text": "sure that you actually encounter or you fulfill them one is that you do want to minimize the latency right because",
    "start": "2101740",
    "end": "2107619"
  },
  {
    "text": "latency is going to affect the user experience and it's going to also affect how the throughput is so you want to",
    "start": "2107619",
    "end": "2113740"
  },
  {
    "text": "minimize the X you want to minimize the latency you want to maximize the throughput so you can handle more queries per second the second thing is",
    "start": "2113740",
    "end": "2120160"
  },
  {
    "text": "that because you're going to be using both CPUs and gpus to do some sort of inference into a large images uh you",
    "start": "2120160",
    "end": "2127240"
  },
  {
    "text": "want to have the ability to somehow use fine-grained resources and allocate fractional resources for those",
    "start": "2127240",
    "end": "2133599"
  },
  {
    "text": "particular Hardware accelerators so that's an important part of requirement and then most important you want to have",
    "start": "2133599",
    "end": "2141040"
  },
  {
    "text": "a programmatic API so then you as a developer can actually double this locally test it locally debug it locally",
    "start": "2141040",
    "end": "2148180"
  },
  {
    "text": "and then easily deploy it on on in production that's an important parity because today most of the people do work",
    "start": "2148180",
    "end": "2154119"
  },
  {
    "text": "on the laptop and when they're ready that can actually do the staging in the production and finally when you're",
    "start": "2154119",
    "end": "2160060"
  },
  {
    "text": "putting it in production you actually want to make sure that it's actually rock steady but I mean by Rook stadium is that it has high availability it",
    "start": "2160060",
    "end": "2166839"
  },
  {
    "text": "actually has monitoring capabilities it has ability to update your your your",
    "start": "2166839",
    "end": "2172240"
  },
  {
    "text": "your models or your end-to-end inference service without any downtime so these",
    "start": "2172240",
    "end": "2177820"
  },
  {
    "text": "are the high table Stakes right without those things um it's it's it's not possible and you",
    "start": "2177820",
    "end": "2183460"
  },
  {
    "text": "realize that none of these things that I'm talked about are really part of your",
    "start": "2183460",
    "end": "2188740"
  },
  {
    "text": "your ml code right this is the this is the technical debt that Google talked",
    "start": "2188740",
    "end": "2193780"
  },
  {
    "text": "about in 2003 when they received the paper of 2013-14 when they actually had this all these little boxes and the ml",
    "start": "2193780",
    "end": "2200260"
  },
  {
    "text": "code was this little square this is the technical debt that that is not really part of the ml code but you have to",
    "start": "2200260",
    "end": "2206560"
  },
  {
    "text": "somehow deal with it and and one way to solve this particular problem",
    "start": "2206560",
    "end": "2212140"
  },
  {
    "text": "for other end-to-end inference service that we actually talked about is to somehow use a single container that has",
    "start": "2212140",
    "end": "2220000"
  },
  {
    "text": "all your multiple models right so you have your tensorflow model you have your pytorch model you put it",
    "start": "2220000",
    "end": "2225099"
  },
  {
    "text": "um some python logic you stick it together and you put it behind a flask or a fast API and then you develop and",
    "start": "2225099",
    "end": "2233079"
  },
  {
    "text": "you replicate those into a couple of containers you put in a large container and deploying kubernetes",
    "start": "2233079",
    "end": "2238119"
  },
  {
    "text": "this is Simple Solution it actually works straight out of the box but pretty soon sooner or later you're going to run into your problems and latency those",
    "start": "2238119",
    "end": "2246579"
  },
  {
    "text": "reasons are because you can't independently take each of these models and then scale them independently so",
    "start": "2246579",
    "end": "2253180"
  },
  {
    "text": "sooner or later you won't be able to actually scale those independently and those problems will manifest itself in",
    "start": "2253180",
    "end": "2259119"
  },
  {
    "text": "high latency and low throughput and becomes costly over time because each of these are different containers you have",
    "start": "2259119",
    "end": "2264880"
  },
  {
    "text": "to manage and you have to upgrade and you have to do that while this simple solution has been used before and I'm sure some of you might have encountered",
    "start": "2264880",
    "end": "2271420"
  },
  {
    "text": "and might have actually used it but it's not very efficient it has some shortcomings there might be another",
    "start": "2271420",
    "end": "2276880"
  },
  {
    "text": "solution right which is sort of more complex but mitigates and resolve addresses some of the issues or the",
    "start": "2276880",
    "end": "2283839"
  },
  {
    "text": "first one is then to deploy this as micro services and not people actually do that so what you do is you have one container that has only a transfer model",
    "start": "2283839",
    "end": "2291700"
  },
  {
    "text": "you might have another container that has only ux models and you might have a third one that has at least Pi torque",
    "start": "2291700",
    "end": "2297400"
  },
  {
    "text": "and you put your your python logic as an orchestrator behind fast API and then",
    "start": "2297400",
    "end": "2303099"
  },
  {
    "text": "deploy that in the kubernetes well this actually works right it can scale if you",
    "start": "2303099",
    "end": "2308260"
  },
  {
    "text": "actually wanted to scale another scale the utensil flow model because more traffic is going over there you go ahead",
    "start": "2308260",
    "end": "2313359"
  },
  {
    "text": "and create another container but there are some problems even though it actually works and you can you can have",
    "start": "2313359",
    "end": "2319020"
  },
  {
    "text": "independently deploy the resources the problem is this is a bit quite complicated because now you have bespoke",
    "start": "2319020",
    "end": "2325420"
  },
  {
    "text": "Micro Services you've got to manage you've got to worry and then think about if your data scientist and machine learning who actually wants to deploy",
    "start": "2325420",
    "end": "2331180"
  },
  {
    "text": "those things you are putting this cognitive burden on them to understand that and most importantly is that it",
    "start": "2331180",
    "end": "2337540"
  },
  {
    "text": "does not meet some of the requirements that you actually have to be able to do that you imagine trying to replicate",
    "start": "2337540",
    "end": "2343660"
  },
  {
    "text": "this particular environment on your laptop can be quite cumbersome so that is not the best solution and so you ask",
    "start": "2343660",
    "end": "2350619"
  },
  {
    "text": "your question Jules is there a better way to do it right and we have an opinion way of saying this yes you can",
    "start": "2350619",
    "end": "2356260"
  },
  {
    "text": "actually do it and razor we sort of build it to address some of the issues to be able to actually use this as a",
    "start": "2356260",
    "end": "2362440"
  },
  {
    "text": "modal entrance and the reason you can do that is because we want to make sure that you as a developer can drive a",
    "start": "2362440",
    "end": "2368500"
  },
  {
    "text": "single Python program that allows you to be able to stitch all these different modules together and use your favorite",
    "start": "2368500",
    "end": "2374200"
  },
  {
    "text": "tools right you want to use your favorite ml libraries you want to use your family biodata ecosystem libraries",
    "start": "2374200",
    "end": "2379240"
  },
  {
    "text": "do that and the important thing is you should be able to scale any of these models uh with the appropriate amount of",
    "start": "2379240",
    "end": "2386680"
  },
  {
    "text": "resources Hardware resources that they need across the cluster I think this is an important part of it right this is an",
    "start": "2386680",
    "end": "2392560"
  },
  {
    "text": "important part of it so let's revisit it our example and how we can actually use these ingredients that we actually have",
    "start": "2392560",
    "end": "2399099"
  },
  {
    "text": "met that part of radio.o or Ray sub 2.0 to ensure that we can actually do this",
    "start": "2399099",
    "end": "2404440"
  },
  {
    "text": "this inference uh example that that I refer to so in that example we can still actually",
    "start": "2404440",
    "end": "2411220"
  },
  {
    "text": "use um uh kubernetes as two different set ports so you can actually have you can",
    "start": "2411220",
    "end": "2417760"
  },
  {
    "text": "have a one pause and both these pods are actually managed by by race of and",
    "start": "2417760",
    "end": "2424560"
  },
  {
    "text": "independently for each point you can say okay for this particular Port I'm going to use one GPU and for the other Port",
    "start": "2424560",
    "end": "2430900"
  },
  {
    "text": "I'm going to use four and for the second Port where I might not require GPU I'm just going to use two CPUs and then",
    "start": "2430900",
    "end": "2437020"
  },
  {
    "text": "independently and that's an important part because you won't be able to somehow fractionalize allocate resources",
    "start": "2437020",
    "end": "2442540"
  },
  {
    "text": "for your model so over here I can say for my image classifier I want to use a fraction of the GPU because it can I",
    "start": "2442540",
    "end": "2450160"
  },
  {
    "text": "don't need to use an entire GP I'll just use a fraction of it and I can just use uh for the for the image detector I",
    "start": "2450160",
    "end": "2456220"
  },
  {
    "text": "don't need that so this whole idea about being able to write your end-to-end application using a single python script",
    "start": "2456220",
    "end": "2463180"
  },
  {
    "text": "and being able to scale it flexibly and independently because each of these models can be scaled is actually the",
    "start": "2463180",
    "end": "2469540"
  },
  {
    "text": "part of what we think is a solution to address some of these problems that actually we have and so",
    "start": "2469540",
    "end": "2477040"
  },
  {
    "text": "the takeaway over here is that with this open-ended solution that we actually propose that addresses some of the",
    "start": "2477040",
    "end": "2483400"
  },
  {
    "text": "challenges that we indicated earlier single python script to be able to write the program you develop locally you test",
    "start": "2483400",
    "end": "2489640"
  },
  {
    "text": "it locally because all you're writing is python code you don't have to worry about IP addresses you don't have to worry about the RPC code you don't have",
    "start": "2489640",
    "end": "2496300"
  },
  {
    "text": "everybody ingest you deploy it uh in a single app and I think that's sort of a takeaway for this and just to highlight",
    "start": "2496300",
    "end": "2503920"
  },
  {
    "text": "we wanted to make sure that for Ray 2.0 and an ongoing effort they'd be actually",
    "start": "2503920",
    "end": "2508960"
  },
  {
    "text": "making it to that X is to ensure that we can actually make sure that that we can actually use ml in production and",
    "start": "2508960",
    "end": "2515380"
  },
  {
    "text": "importantly we're going to give you flexible API so you can actually do these motor composition in a very",
    "start": "2515380",
    "end": "2521260"
  },
  {
    "text": "friendly and intuitive way and I think that's an important part because we start addressing and introducing a lot",
    "start": "2521260",
    "end": "2526660"
  },
  {
    "text": "of friction you're going to have a lot of friction and and this is this is a huge problem the second question was",
    "start": "2526660",
    "end": "2532480"
  },
  {
    "text": "obviously mihad talked about was autoscaling right we have sort of introduced Advanced Auto scaling as part",
    "start": "2532480",
    "end": "2539200"
  },
  {
    "text": "of the racer so we can actually minimize code so you're only going to use the models which actually have traffic and",
    "start": "2539200",
    "end": "2544720"
  },
  {
    "text": "you can scale down appropriately and then finally um we decided to sort of double down on",
    "start": "2544720",
    "end": "2550119"
  },
  {
    "text": "kubernetes because a large number of people who are actually using a rate serve in production are using kubernetes",
    "start": "2550119",
    "end": "2556119"
  },
  {
    "text": "there's no mean to say that we are not addressing those other people the kubernetes seems to be a common platform",
    "start": "2556119",
    "end": "2562060"
  },
  {
    "text": "where people actually use that and so we have created array operator for working",
    "start": "2562060",
    "end": "2567760"
  },
  {
    "text": "with kubridge we actually do that in working conjunction with that so let's talk about model composition",
    "start": "2567760",
    "end": "2573760"
  },
  {
    "text": "you know how do I address this motor composition apis that allow you to use Python to create this topologies as a",
    "start": "2573760",
    "end": "2581680"
  },
  {
    "text": "single graph and obviously the requirements are no different from the general requirements of of uh race of",
    "start": "2581680",
    "end": "2589900"
  },
  {
    "text": "one is we want to make sure that those apis are flexible that is to say that you can use different Frameworks and we",
    "start": "2589900",
    "end": "2597339"
  },
  {
    "text": "are sort of in the Zeiss guys the machine learning where a lot of people are using different Frameworks so we won't be able to support the latest and",
    "start": "2597339",
    "end": "2603160"
  },
  {
    "text": "the greatest one you want to combine the business logic with that and we want to make sure that you can actually develop",
    "start": "2603160",
    "end": "2608380"
  },
  {
    "text": "and debug those locally I think that's an important part of it because a lot of people actually use their laptop to do most of the development and then they",
    "start": "2608380",
    "end": "2614619"
  },
  {
    "text": "want to be able to transition over the staging or production and then finally because of the auto scaling you would",
    "start": "2614619",
    "end": "2620680"
  },
  {
    "text": "actually make sure that whatever you actually produce or whatever you where you deploy in production is is efficient",
    "start": "2620680",
    "end": "2627040"
  },
  {
    "text": "and scalable and so our solution is composition apis that allow you to build",
    "start": "2627040",
    "end": "2633880"
  },
  {
    "text": "this inference graph this topologies which are an ensemble of different actually models as part of the sub",
    "start": "2633880",
    "end": "2639579"
  },
  {
    "text": "deployment and you saw me how talking about how they actually use a particular class to use those self-deployment as a",
    "start": "2639579",
    "end": "2646060"
  },
  {
    "text": "decorator and that that gets deployed as as a single unit deployment using sub",
    "start": "2646060",
    "end": "2651880"
  },
  {
    "text": "deploy that sort of all API the new apis you use actually just you use that sub Depot run and it gets actually deployed",
    "start": "2651880",
    "end": "2659680"
  },
  {
    "text": "and we want to make sure that it's fully flexible and takes advantage of all the ray uh uh efficiencies and Facilities",
    "start": "2659680",
    "end": "2667480"
  },
  {
    "text": "they provide in other words you can alter in your individual classes as server deployments you can configure them independently to scale",
    "start": "2667480",
    "end": "2674740"
  },
  {
    "text": "appropriately and and the important part is that you can just write it in Python I think that's an important part and",
    "start": "2674740",
    "end": "2681099"
  },
  {
    "text": "then one thing good about is that because it's actually written in Python you can actually orchestrate all the logic that you actually need to be able",
    "start": "2681099",
    "end": "2687160"
  },
  {
    "text": "to do that so what do I mean by that what do I mean by modal composition and I think the",
    "start": "2687160",
    "end": "2693640"
  },
  {
    "text": "thing that we have actually discovered from talking to the community uh and even including including uh talking to",
    "start": "2693640",
    "end": "2699880"
  },
  {
    "text": "to mija was that that we saw a few patents with which how people actually",
    "start": "2699880",
    "end": "2706060"
  },
  {
    "text": "deploy and build actually practical models one was called patent Cheney which is an important model where you",
    "start": "2706060",
    "end": "2711640"
  },
  {
    "text": "have certain input that's coming in and you're going to give it to one model that's going to do some sort of prediction and then takes it output and",
    "start": "2711640",
    "end": "2717760"
  },
  {
    "text": "put it in second model and then you finally get the output this is very common chaining where you're actually doing that and how we express this in",
    "start": "2717760",
    "end": "2725079"
  },
  {
    "text": "the new composition model API is very quite intuitive and very pytonic right here we have two models M1 model V1 and",
    "start": "2725079",
    "end": "2732700"
  },
  {
    "text": "model V2 and this could just be Uris or this could be just a model sitting on on your S3 and you create this instances of",
    "start": "2732700",
    "end": "2740560"
  },
  {
    "text": "this model abstract um um primitive M1 and M2",
    "start": "2740560",
    "end": "2746980"
  },
  {
    "text": "and then just using python code you know the context manager you're going to take the input of first one and you give it",
    "start": "2746980",
    "end": "2753460"
  },
  {
    "text": "to the first the first particular model and that's going to produce an output and you take the model output on the",
    "start": "2753460",
    "end": "2758740"
  },
  {
    "text": "second one and you forward it to the to the second model and that gives you the output very simple pythonic API you",
    "start": "2758740",
    "end": "2765579"
  },
  {
    "text": "don't have to worry about ingest you just do that very easily with using this composition Primitives to do a chaining",
    "start": "2765579",
    "end": "2772839"
  },
  {
    "text": "a bunch of particular models and this is a very common use case where people will change things together whether you're",
    "start": "2772839",
    "end": "2778119"
  },
  {
    "text": "actually doing transformation whether you're doing pre-processing whether you're actually applying some sort of a",
    "start": "2778119",
    "end": "2783220"
  },
  {
    "text": "business logic where you're looking at a particular feature in say a feature store and combine it together the output",
    "start": "2783220",
    "end": "2788859"
  },
  {
    "text": "and then give it to the next model so this is this is no different and then finally there is The Ensemble uh",
    "start": "2788859",
    "end": "2795640"
  },
  {
    "text": "patent where you actually have an input where you broadcasting to multiple models and then each will produce a",
    "start": "2795640",
    "end": "2802300"
  },
  {
    "text": "particular output which is then somehow combined to a combiner and the combiner could just be a business logic or a",
    "start": "2802300",
    "end": "2808420"
  },
  {
    "text": "class that you actually write and that's going to produce the final output and this can be expressed quite equally and",
    "start": "2808420",
    "end": "2814540"
  },
  {
    "text": "quite intuitively with with this particular composite API not very not",
    "start": "2814540",
    "end": "2819579"
  },
  {
    "text": "very dissimilar to the previous one we actually create model one model two and over here we just and wrap that into of",
    "start": "2819579",
    "end": "2826599"
  },
  {
    "text": "a python um Orchestra engine where we take we send inputs to both the particular",
    "start": "2826599",
    "end": "2832060"
  },
  {
    "text": "models and then the output of those can actually go to the combined and then combine will will combine those two",
    "start": "2832060",
    "end": "2838599"
  },
  {
    "text": "outputs and then produce the final result and last but not so listen this is a",
    "start": "2838599",
    "end": "2843760"
  },
  {
    "text": "very this is a very very important part of this because the dynamic selection where you take the end code and you",
    "start": "2843760",
    "end": "2849400"
  },
  {
    "text": "broadcast it to multiple models that you actually have and then based on certain input criteria it could be a user ID it",
    "start": "2849400",
    "end": "2855640"
  },
  {
    "text": "could be a geographical location it could be longitude latitude it could be a color whatever you want to whatever you want to have the particular criteria",
    "start": "2855640",
    "end": "2862060"
  },
  {
    "text": "this particular Dynamic selector will choose a particular model we'll use that and then create create the final output",
    "start": "2862060",
    "end": "2868480"
  },
  {
    "text": "it's not both output it's one or the other and then over here same logic very simple to express yourself the apology",
    "start": "2868480",
    "end": "2875440"
  },
  {
    "text": "you actually have those two models and instead of sending the output what you're going to do is you're going to send the models to this particular",
    "start": "2875440",
    "end": "2882400"
  },
  {
    "text": "selector and then in the in the in the orchestration you're going to forward",
    "start": "2882400",
    "end": "2887440"
  },
  {
    "text": "the input to both the the dynamic selector and it will actually decide based on the input which model to use",
    "start": "2887440",
    "end": "2894579"
  },
  {
    "text": "for prediction and you'll get the final Dynamic output so I talked about these patterns over here how do we actually",
    "start": "2894579",
    "end": "2900460"
  },
  {
    "text": "apply to the application that we have been talking about which is the understanding of our inference engine so",
    "start": "2900460",
    "end": "2906700"
  },
  {
    "text": "we're here if you want to check this particular patterns you'll see that they can be used quite easily over here so",
    "start": "2906700",
    "end": "2912400"
  },
  {
    "text": "for my chaining I would actually use my downloader and pre-processor chain together for dynamic selection actually",
    "start": "2912400",
    "end": "2918520"
  },
  {
    "text": "I would use any of the three classifiers that I actually have and intend them I can use ensemble",
    "start": "2918520",
    "end": "2925060"
  },
  {
    "text": "with my Dynamic dispatch and my image detector and then the output of that could be given to a combiner and again",
    "start": "2925060",
    "end": "2931119"
  },
  {
    "text": "by structured output so that's sort of quite an easy way to to to build this",
    "start": "2931119",
    "end": "2936220"
  },
  {
    "text": "particular model as an end-to-end application using this motor composite and API and this is just not one single",
    "start": "2936220",
    "end": "2942880"
  },
  {
    "text": "uh model this could be a multiple actually models that you actually create a topology of that",
    "start": "2942880",
    "end": "2948119"
  },
  {
    "text": "so just overview red composition FB allows you to write very simple ordinary python classes that you can actually do",
    "start": "2948119",
    "end": "2955119"
  },
  {
    "text": "that and this is part of the array 2.2.x currently is in developer preview but we're getting a lot of feedback a lot of",
    "start": "2955119",
    "end": "2961240"
  },
  {
    "text": "people actually using it and they just love the patterns that allows them to do that it flexibility has the ability to",
    "start": "2961240",
    "end": "2967359"
  },
  {
    "text": "actually compose those modems and you can actually see you can actually mix and mix and mix mix",
    "start": "2967359",
    "end": "2972819"
  },
  {
    "text": "um both your ml model code as well as your business logic in Python code and then finally you can run and test these",
    "start": "2972819",
    "end": "2979359"
  },
  {
    "text": "because everything is a single python script you can do it on your laptop and more importantly is is when you want to",
    "start": "2979359",
    "end": "2985540"
  },
  {
    "text": "deploy into production you can independently scale these models independently and then that's an imperative for for racer to be able to",
    "start": "2985540",
    "end": "2993460"
  },
  {
    "text": "take this modal composition they have an individual motives and compose them and then scale them independently and I look",
    "start": "2993460",
    "end": "3000119"
  },
  {
    "text": "at the appropriate resources to those when when it's actually needed now talking about scaling uh what does it",
    "start": "3000119",
    "end": "3006960"
  },
  {
    "text": "actually mean to scale in in the new world that we actually have introduced into that X now",
    "start": "3006960",
    "end": "3012900"
  },
  {
    "text": "one of the problems that auto scaling will actually will have to address is because model machine learnings are are",
    "start": "3012900",
    "end": "3019140"
  },
  {
    "text": "expensive they're very compute intensive they take a lot of resources you have to make sure that that the models are used",
    "start": "3019140",
    "end": "3024960"
  },
  {
    "text": "efficiently to actually save the cost and so not all the models are going to be used at the same time so if you have",
    "start": "3024960",
    "end": "3030960"
  },
  {
    "text": "a large cluster you have a lot of models they have deployed at some point you might have a various huge traffic only",
    "start": "3030960",
    "end": "3036839"
  },
  {
    "text": "going to certain models and those could be Auto scale automatically but what are the other ones which are not being used",
    "start": "3036839",
    "end": "3042240"
  },
  {
    "text": "so somehow you want to bring them down and that way they don't take the resources that's an important part that's an important table stack holder",
    "start": "3042240",
    "end": "3048900"
  },
  {
    "text": "some of these models can actually be hard to to to use appropriate tuning for for Hardware",
    "start": "3048900",
    "end": "3055920"
  },
  {
    "text": "utilization you might have to profile them you have to make sure that you actually do some sort of vector prediction so those are our problems",
    "start": "3055920",
    "end": "3063900"
  },
  {
    "text": "hard to solve and then they have to work with with multi-modal that's an important part that it actually works",
    "start": "3063900",
    "end": "3069180"
  },
  {
    "text": "more and more and we believe that the Advanced Auto scaling will raise so would you introduce into that X actually",
    "start": "3069180",
    "end": "3075599"
  },
  {
    "text": "helps a lot because it addresses some of the issues that mihad talked about which is how to actually scale it down to zero",
    "start": "3075599",
    "end": "3082440"
  },
  {
    "text": "because I have this model if you're sitting there and they're not being used I don't want to use my Hardware resources I want to be able to actually",
    "start": "3082440",
    "end": "3088380"
  },
  {
    "text": "reschedule the resources everywhere so we want to be able to scale them down to zero and that's an important part of it",
    "start": "3088380",
    "end": "3094579"
  },
  {
    "text": "and then I don't want to go to the profiling and I don't want to go through optimizing my particular code uh why",
    "start": "3094579",
    "end": "3100859"
  },
  {
    "text": "don't we just use something to be heuristic where I say if I start getting my request queuing up for a long time",
    "start": "3100859",
    "end": "3106920"
  },
  {
    "text": "I'm going to be able to use the profile I want to Auto scale or or put some more",
    "start": "3106920",
    "end": "3114359"
  },
  {
    "text": "um replicas on the Fly and then finally this has to be completely compatible",
    "start": "3114359",
    "end": "3120780"
  },
  {
    "text": "with my composite API they have to work in conjunction so how does it actually work with our application let's go back",
    "start": "3120780",
    "end": "3126599"
  },
  {
    "text": "to the inference application we've been talking about over a period of time you realize that",
    "start": "3126599",
    "end": "3133500"
  },
  {
    "text": "all the all the cues are now being directed to the image classify number one and maybe some of the images are",
    "start": "3133500",
    "end": "3140339"
  },
  {
    "text": "going to classify too and all of a sudden now we begin to actually see the latency because of the fact that the",
    "start": "3140339",
    "end": "3145559"
  },
  {
    "text": "image classifies the only one that's servicing this large backlog of Q so",
    "start": "3145559",
    "end": "3151619"
  },
  {
    "text": "what the what the auto scaler will actually do with with respect to race so is that if we look at the particular",
    "start": "3151619",
    "end": "3157319"
  },
  {
    "text": "input request coming in from the from the queue and it says oh my Q size is really huge so I'm going to go ahead and",
    "start": "3157319",
    "end": "3163980"
  },
  {
    "text": "replicate my image classifier and create three different instances of those and",
    "start": "3163980",
    "end": "3169079"
  },
  {
    "text": "I'll redistribute the load over a period of time so what you have done is you've Auto scaled that to minimize and lower",
    "start": "3169079",
    "end": "3175500"
  },
  {
    "text": "and minimize the latency and it can get sort of more of a high throughput so now you have four instances of four replicas",
    "start": "3175500",
    "end": "3181079"
  },
  {
    "text": "of image classifier which are now serving those particular requests in Tandem and this is automatically done",
    "start": "3181079",
    "end": "3187200"
  },
  {
    "text": "for you you don't have to worry about it and the second thing you might actually noticed that there's no request going to",
    "start": "3187200",
    "end": "3192960"
  },
  {
    "text": "email classifier three so what do we do at this point over a period of time our certain minutes configurable we can",
    "start": "3192960",
    "end": "3199800"
  },
  {
    "text": "remove that particular replica which is coming down to what MIA was talking about scaling down to zero because you",
    "start": "3199800",
    "end": "3205140"
  },
  {
    "text": "have you know you have a motor sitting there uh and occupying your Hardware",
    "start": "3205140",
    "end": "3210240"
  },
  {
    "text": "resources you want to scale it down to zero so that's how sort of the the auto scaling actually works and it's actually",
    "start": "3210240",
    "end": "3215880"
  },
  {
    "text": "really quite simple you don't have to sort of think about writing uh manifest you don't have to worry about you know",
    "start": "3215880",
    "end": "3221460"
  },
  {
    "text": "mucking around with kubernetes all you do is is in your server deployment in",
    "start": "3221460",
    "end": "3227040"
  },
  {
    "text": "the new API decorator you provide the auto scaling config they're saying these are the minimum replicas I want I'm",
    "start": "3227040",
    "end": "3232619"
  },
  {
    "text": "going to scale it down to zero uh the maximum replica of Ramon is 10 if the queue size is very good and then these",
    "start": "3232619",
    "end": "3238559"
  },
  {
    "text": "are the number of the size of the QC if I have anything more than two I want you to start thinking about Auto scaling so",
    "start": "3238559",
    "end": "3244500"
  },
  {
    "text": "that's in a nutshell uh with the with the for the scaling is now because of",
    "start": "3244500",
    "end": "3249839"
  },
  {
    "text": "the time constraint and I'm going to give my esteeming colleague or David to give a talk I'm gonna I'm gonna create",
    "start": "3249839",
    "end": "3256079"
  },
  {
    "text": "another talk that talks about how we actually have doubled down on kubernetes because we have a make sure that we take",
    "start": "3256079",
    "end": "3262319"
  },
  {
    "text": "the advantage of race so it's flexibility it's user experience that I talked about in terms of doing composition apis and the efficiency",
    "start": "3262319",
    "end": "3268680"
  },
  {
    "text": "comes naturally because of the ray and combine that uh with the operational",
    "start": "3268680",
    "end": "3273839"
  },
  {
    "text": "benefits you actually get from kubernetes to see how you can actually use uh in race of in production and this",
    "start": "3273839",
    "end": "3280680"
  },
  {
    "text": "is going to be a separate talk but if you're curious to know exactly how we actually have done that there is an",
    "start": "3280680",
    "end": "3286680"
  },
  {
    "text": "in-depth talked about high availability architecture for online so on the race Summit you actually go to that",
    "start": "3286680",
    "end": "3291720"
  },
  {
    "text": "particular Ray Summit page operation is increased someone kubernetes is another good talk and those are the talks that",
    "start": "3291720",
    "end": "3299280"
  },
  {
    "text": "we can actually recommend because the time constraints I can't get into it but we'll arrange it",
    "start": "3299280",
    "end": "3304440"
  },
  {
    "text": "once a regular talk and there's another couple of talks that you actually might be interested where the community to",
    "start": "3304440",
    "end": "3309660"
  },
  {
    "text": "talk about how they're actually scaling uh Race So using zero copy model using the object store because now you can",
    "start": "3309660",
    "end": "3315780"
  },
  {
    "text": "actually do a zero copy because the models can actually be stored in on on",
    "start": "3315780",
    "end": "3320819"
  },
  {
    "text": "Object Store and if you have a model which is deployed on the same node where the object store you can actually just",
    "start": "3320819",
    "end": "3326579"
  },
  {
    "text": "do a zero copy from the from the from the array restore and then",
    "start": "3326579",
    "end": "3333540"
  },
  {
    "text": "um another company called Dipset they actually implemented racer as part of the",
    "start": "3333540",
    "end": "3339300"
  },
  {
    "text": "question answering certain music some of the NLP which we're gonna have the next talk today we talk about that so you",
    "start": "3339300",
    "end": "3345780"
  },
  {
    "text": "guys are welcome to do that but stay tuned for a more in-depth version of race of on kubernetes coming up soon so",
    "start": "3345780",
    "end": "3353880"
  },
  {
    "text": "with that I'm going to just recap as I say well we're going was to make sure that that uh ml in production is easier",
    "start": "3353880",
    "end": "3360900"
  },
  {
    "text": "which way so we do that by making sure that the user experience for the developer is is quite easy in the due to",
    "start": "3360900",
    "end": "3367140"
  },
  {
    "text": "using the flexible composition apis for you to be able to build these topologies and then use them in one single python",
    "start": "3367140",
    "end": "3373740"
  },
  {
    "text": "script I talked about how we can actually Auto scale independently any of those particular models and and have a",
    "start": "3373740",
    "end": "3380460"
  },
  {
    "text": "fine grain use of the hardware resources and this is actually done quite efficiently uh and automatically you",
    "start": "3380460",
    "end": "3386520"
  },
  {
    "text": "don't have to worry about that just through those configuration parameters and then productionizing and hardening",
    "start": "3386520",
    "end": "3391800"
  },
  {
    "text": "on kubernetes as the best way or one of the ways to do that uh to do that so if",
    "start": "3391800",
    "end": "3398040"
  },
  {
    "text": "you have any questions uh let me know and I'm going to stop sharing my screen I don't see any questions",
    "start": "3398040",
    "end": "3405359"
  },
  {
    "text": "foreign",
    "start": "3405359",
    "end": "3407539"
  },
  {
    "text": "over there go away David yeah thanks for the",
    "start": "3415520",
    "end": "3420960"
  },
  {
    "text": "introduction so um I'm David I'm a developer Advocate engineer at Argilla and I'm gonna talk about shortcuts for",
    "start": "3420960",
    "end": "3428160"
  },
  {
    "text": "bootstrapping NLP pipelines before actually being able to serve them with with racer where I'll be going a bit",
    "start": "3428160",
    "end": "3435180"
  },
  {
    "text": "more into depth on how you could potentially use way data to actually distribute data processing for like",
    "start": "3435180",
    "end": "3442200"
  },
  {
    "text": "preposition for NLP and also give a brief overview of using like great train",
    "start": "3442200",
    "end": "3447839"
  },
  {
    "text": "and wait tune to actually create a a model for a arbitrary NOP class so this",
    "start": "3447839",
    "end": "3453900"
  },
  {
    "text": "is uh the agenda for today initially we'll start with our favorite beef into into",
    "start": "3453900",
    "end": "3459839"
  },
  {
    "text": "NLP then eventually I'll give an intro about our Gala and what we do I'll get",
    "start": "3459839",
    "end": "3465180"
  },
  {
    "text": "back to you with these cool shortcuts uh week supervision future learning and active learning and eventually we'll",
    "start": "3465180",
    "end": "3471119"
  },
  {
    "text": "wrap up so we've entered into NLP initially if you said that you did something with text and AI everyone's",
    "start": "3471119",
    "end": "3476940"
  },
  {
    "text": "like okay cool but now with gbtt you're gonna have to be actually explain it to",
    "start": "3476940",
    "end": "3482520"
  },
  {
    "text": "my friends and they are like oh cool we are doing something with like these cool AI models so that's nice",
    "start": "3482520",
    "end": "3489180"
  },
  {
    "text": "um what I'll be focusing on on this talk is actually a use case where we'll like bootstrap text classification model from",
    "start": "3489180",
    "end": "3496260"
  },
  {
    "text": "scratch so you have a bare data set without enemy labels and how to work towards a data set that you can actually",
    "start": "3496260",
    "end": "3502920"
  },
  {
    "text": "train with these these smart shortcuts for actually labeling data",
    "start": "3502920",
    "end": "3508740"
  },
  {
    "text": "um another good thing to notice the difference between lexical and semantic similarity within text while lexical",
    "start": "3508740",
    "end": "3514920"
  },
  {
    "text": "similarities really focused on exact keyword matches and these semantic similarities more conceptual equality",
    "start": "3514920",
    "end": "3521880"
  },
  {
    "text": "like if you talk about football sports and soccer they aren't the exact let's go same words but they do kind of uh",
    "start": "3521880",
    "end": "3530099"
  },
  {
    "text": "coincide in the same conceptual meaning space for for humans uh so our Gala what do we do we actually",
    "start": "3530099",
    "end": "3536880"
  },
  {
    "text": "are open source labeling platform for data Centric NLP and what we mean by that is actually uh one of that we want",
    "start": "3536880",
    "end": "3544140"
  },
  {
    "text": "to focus on high quality trading data instead of like having these big huge uh",
    "start": "3544140",
    "end": "3549420"
  },
  {
    "text": "data sets that maybe also contain this less yeah less structures and less",
    "start": "3549420",
    "end": "3555660"
  },
  {
    "text": "organized data because we actually improve believe that if you improve these models over time right you can",
    "start": "3555660",
    "end": "3561180"
  },
  {
    "text": "eventually get way better results uh and we do that by actually allowing",
    "start": "3561180",
    "end": "3566819"
  },
  {
    "text": "like both data scientists ml engineers and the just human experts to be able to",
    "start": "3566819",
    "end": "3571880"
  },
  {
    "text": "work with these data sets and work with Dr Gala ecosystem",
    "start": "3571880",
    "end": "3577260"
  },
  {
    "text": "so our glass like three main components and the on the left hand side you",
    "start": "3577260",
    "end": "3582480"
  },
  {
    "text": "actually see the arguella pipe clients along with potential models that you might want to use to bootstrap your your",
    "start": "3582480",
    "end": "3588480"
  },
  {
    "text": "project because there are like a lot of Open Source models out there and a lot of Open Source data sets and benchmarks",
    "start": "3588480",
    "end": "3594299"
  },
  {
    "text": "data sets and uh to get started with with uh model you'll probably want to",
    "start": "3594299",
    "end": "3599520"
  },
  {
    "text": "use some of them to initially load your data into orgilla and then log it into",
    "start": "3599520",
    "end": "3605880"
  },
  {
    "text": "the server this Arcola server actually stores and manage to see data sets and computes some metrics in terms of like",
    "start": "3605880",
    "end": "3612059"
  },
  {
    "text": "the accuracy and the text length and some other text specific metrics before",
    "start": "3612059",
    "end": "3617520"
  },
  {
    "text": "logging the data into elasticsearch which is like the the database backends against you can can switch you can query",
    "start": "3617520",
    "end": "3624900"
  },
  {
    "text": "and nowadays also do this semantic search queries based on gain and",
    "start": "3624900",
    "end": "3629940"
  },
  {
    "text": "similarity um [Music] and she was already mentioned deep set",
    "start": "3629940",
    "end": "3636240"
  },
  {
    "text": "and they have actually have also this cool ecosystem to do the semantic search and Factor search based on text",
    "start": "3636240",
    "end": "3642960"
  },
  {
    "text": "similarity instead of just focusing on this lexical keyword similarity and since a couple of years Google has also",
    "start": "3642960",
    "end": "3650099"
  },
  {
    "text": "introduced these kind of things into their Google search engine so it's pretty cool and then finally what you",
    "start": "3650099",
    "end": "3655980"
  },
  {
    "text": "can do within the ocular UI as a non-technical users actually level data it's reverse data filter query",
    "start": "3655980",
    "end": "3662040"
  },
  {
    "text": "everything you might want to do to actually get to know your data instead of just passing through records and",
    "start": "3662040",
    "end": "3668339"
  },
  {
    "text": "records and Records which might be boring and less intuitive so for us it's really about getting to know your data",
    "start": "3668339",
    "end": "3674640"
  },
  {
    "text": "and getting a high quality data and iterating over time uh within our Gala we've got like users",
    "start": "3674640",
    "end": "3681299"
  },
  {
    "text": "and workspaces so you can either work together as or as a team um and this is kind of what it looks",
    "start": "3681299",
    "end": "3687119"
  },
  {
    "text": "like where you have your data set name the workspace assigned to either a specific user or group and then the",
    "start": "3687119",
    "end": "3693540"
  },
  {
    "text": "specific tasks for now we will focus on text classification but you can also focus on like token classification ndr",
    "start": "3693540",
    "end": "3700619"
  },
  {
    "text": "named entity recognition where you might want to extract specific persons or locations or other entities from text",
    "start": "3700619",
    "end": "3707119"
  },
  {
    "text": "and text Generations or text to text we also like working a lot with the",
    "start": "3707119",
    "end": "3713400"
  },
  {
    "text": "community so like these ways in which activity 2 is trained with reinforcement",
    "start": "3713400",
    "end": "3718859"
  },
  {
    "text": "learning human feedback we're also looking for ways to actually integrate that within our default tasks and",
    "start": "3718859",
    "end": "3725099"
  },
  {
    "text": "default workflows because yeah a lot of users have of course been asking for that",
    "start": "3725099",
    "end": "3730200"
  },
  {
    "text": "normally what we see as a default training workflow is that you really start with a bare empty data source you",
    "start": "3730200",
    "end": "3738299"
  },
  {
    "text": "want to like read that data source create records within Argyle and potentially infer some model or",
    "start": "3738299",
    "end": "3744299"
  },
  {
    "text": "realistics so that you have some initial predictions these predictions are not going to be good with this video about",
    "start": "3744299",
    "end": "3750440"
  },
  {
    "text": "bootstrapping and getting some initial estimations for your predictions",
    "start": "3750440",
    "end": "3756260"
  },
  {
    "text": "eventually then if you look at the UI you will like refresh the data explore",
    "start": "3756260",
    "end": "3762299"
  },
  {
    "text": "and label the data um after that you'll need to like load some records prepare for training and",
    "start": "3762299",
    "end": "3768540"
  },
  {
    "text": "actually train with your favorite NOP framework which might be Transformers space c spark and op and we actually",
    "start": "3768540",
    "end": "3774660"
  },
  {
    "text": "have some integrated methods to directly export our data via one API call within",
    "start": "3774660",
    "end": "3779819"
  },
  {
    "text": "python to be able to train use it directly for training and then you don't",
    "start": "3779819",
    "end": "3785339"
  },
  {
    "text": "need to worry about this data formatting stuff that's really annoying because we've handled that for you",
    "start": "3785339",
    "end": "3791880"
  },
  {
    "text": "and then once you have your trained model what you're going to do is like monitor these model work fast maybe",
    "start": "3791880",
    "end": "3797160"
  },
  {
    "text": "you're like hosting it within the serve and actually uh use like a St middleware",
    "start": "3797160",
    "end": "3802500"
  },
  {
    "text": "or monitoring or async blogging so as to not block the actual inference time uh",
    "start": "3802500",
    "end": "3808799"
  },
  {
    "text": "to Monitor and lock these predictions into RK Lagan because eventually if you're a model you will want to keep on",
    "start": "3808799",
    "end": "3816140"
  },
  {
    "text": "monitoring and improving on the data for example chat gbt didn't exist a year ago",
    "start": "3816140",
    "end": "3821520"
  },
  {
    "text": "nowadays it's a cool cool hip thing so you actually need to retrain your model",
    "start": "3821520",
    "end": "3827579"
  },
  {
    "text": "to pick up on these kind of words as well similar to kovitz maybe other things that might have happened in",
    "start": "3827579",
    "end": "3833460"
  },
  {
    "text": "recent years and uh yeah this kind of data shifts data drift thingies that",
    "start": "3833460",
    "end": "3838619"
  },
  {
    "text": "that tend to happen you will want to like keep keep track of your data set improve it and give it higher quality as",
    "start": "3838619",
    "end": "3845160"
  },
  {
    "text": "well uh I want you to like labeled and explore your data we've got some built-in metrics like a once for an",
    "start": "3845160",
    "end": "3851339"
  },
  {
    "text": "accuracy now you can actually load your records again and prepare for training train a new model and this is kind of",
    "start": "3851339",
    "end": "3856799"
  },
  {
    "text": "how we close the envelopes Loop so these are some like snapshots from",
    "start": "3856799",
    "end": "3862980"
  },
  {
    "text": "the python clients where you can see that's relatively straightforward to create a record",
    "start": "3862980",
    "end": "3869839"
  },
  {
    "text": "in this case a text classification record with predictions and prediction",
    "start": "3869839",
    "end": "3874980"
  },
  {
    "text": "and annotation but initially you make an assumption of what a model what the label might be given a score and then",
    "start": "3874980",
    "end": "3882540"
  },
  {
    "text": "eventually there's one golden standards label that you actually assign which is",
    "start": "3882540",
    "end": "3887579"
  },
  {
    "text": "quality annotation and we also provide like these operations for loading data from into pandas and to actually log",
    "start": "3887579",
    "end": "3895200"
  },
  {
    "text": "these records relatively easily when you have like a created records or a list of records that you then log into the data",
    "start": "3895200",
    "end": "3901859"
  },
  {
    "text": "sets um similarly for loading you can actually pass a query where you and a",
    "start": "3901859",
    "end": "3907980"
  },
  {
    "text": "data set name and then eventually actually uh call the prepare for Training Method and this is like a",
    "start": "3907980",
    "end": "3913380"
  },
  {
    "text": "default Universal uh prepare for training for Transformers so for the",
    "start": "3913380",
    "end": "3918540"
  },
  {
    "text": "like major uh NLP framework for this use case I will actually be",
    "start": "3918540",
    "end": "3924900"
  },
  {
    "text": "covering like a data sets which we host on the hugging face Hub which is like a",
    "start": "3924900",
    "end": "3929940"
  },
  {
    "text": "model data set and nowadays also a UI Library where you can really easily get like these Transformer models Birds",
    "start": "3929940",
    "end": "3936780"
  },
  {
    "text": "texture text text classification and nowadays also image image",
    "start": "3936780",
    "end": "3942119"
  },
  {
    "text": "support and computer vision support and for this we will focus on the task of",
    "start": "3942119",
    "end": "3948359"
  },
  {
    "text": "classifying uh uh each of the input entries to other science attack business",
    "start": "3948359",
    "end": "3954720"
  },
  {
    "text": "sports or worlds and this is actually a snapshot of how to Via load data from",
    "start": "3954720",
    "end": "3961740"
  },
  {
    "text": "dragonface app and actually quite easily within three lines of code loaded into our Gala so that you can actually get",
    "start": "3961740",
    "end": "3967680"
  },
  {
    "text": "started with with labeling and create creating your high quality data",
    "start": "3967680",
    "end": "3973380"
  },
  {
    "text": "set um normally when you're actually like upload your data into our Gala there's",
    "start": "3973380",
    "end": "3979859"
  },
  {
    "text": "some ways in which you can explore within the UI you have got like the keyword search bar that you can see on",
    "start": "3979859",
    "end": "3985920"
  },
  {
    "text": "the right hand side there's this built-in text queries for for uh which",
    "start": "3985920",
    "end": "3991140"
  },
  {
    "text": "use the listening query language they have some default operators like and or",
    "start": "3991140",
    "end": "3996260"
  },
  {
    "text": "wildcard queries are supported and also these WebEx queries and we've also got",
    "start": "3996260",
    "end": "4002420"
  },
  {
    "text": "filters built in to either filter on metadata like um the some custom",
    "start": "4002420",
    "end": "4008059"
  },
  {
    "text": "metadata that you add and some actual data that we add by default like it's the annotated as some less updated",
    "start": "4008059",
    "end": "4015099"
  },
  {
    "text": "timestamps and these kind of things and in the needs you can actually see a record here that is actually matched by",
    "start": "4015099",
    "end": "4023119"
  },
  {
    "text": "the filter politics and precedent and you can see the highlights words the assigned label and these kind of things",
    "start": "4023119",
    "end": "4029900"
  },
  {
    "text": "another thing that you can do is actually add highlighted text so that during like this uh labeling process",
    "start": "4029900",
    "end": "4036319"
  },
  {
    "text": "annotators might be more nudge towards looking at these specific nouns so that during The annotation we're doing for",
    "start": "4036319",
    "end": "4043400"
  },
  {
    "text": "plus a text classification you were inclined to look at like the word gains",
    "start": "4043400",
    "end": "4048559"
  },
  {
    "text": "Euro economy dollar euro and then it's a bit more easy to actually start labeling",
    "start": "4048559",
    "end": "4054440"
  },
  {
    "text": "and go from that the the bare minimum blank data set to a high quality data set",
    "start": "4054440",
    "end": "4060319"
  },
  {
    "text": "and another cool thing might be where we've added support for is like this",
    "start": "4060319",
    "end": "4065599"
  },
  {
    "text": "similarity search based on Vector embeddings where if uh like input text is similar then the",
    "start": "4065599",
    "end": "4073700"
  },
  {
    "text": "embedding or the embedding presence in the embedding space is also going to be similar where the example again with",
    "start": "4073700",
    "end": "4080720"
  },
  {
    "text": "football soccer and these words they'll end up in the same space within the",
    "start": "4080720",
    "end": "4086720"
  },
  {
    "text": "vector space um and then search these workers can actually create it by uh by the example",
    "start": "4086720",
    "end": "4092960"
  },
  {
    "text": "on the right so if you actually go into a brief demo of like how to query uh Aguila",
    "start": "4092960",
    "end": "4101440"
  },
  {
    "text": "uh you can actually see like the query about politics and presidents and what",
    "start": "4104900",
    "end": "4110000"
  },
  {
    "text": "you can for example also do is politics for president and the nice thing is that when you actually have these hits for",
    "start": "4110000",
    "end": "4116540"
  },
  {
    "text": "the queries you can actually see these keywords updates uh on the side as well so for politics and president there are",
    "start": "4116540",
    "end": "4123199"
  },
  {
    "text": "going to be a lot more records so the keywords actually update so this means that you also are aware of what data",
    "start": "4123199",
    "end": "4128960"
  },
  {
    "text": "might be in your in your query or might be hit by your query additionally what",
    "start": "4128960",
    "end": "4134179"
  },
  {
    "text": "you can do is like filter on predictions or filter on annotations the metadata I",
    "start": "4134179",
    "end": "4140000"
  },
  {
    "text": "mentioned the status of it being either validated or not and similarly what's",
    "start": "4140000",
    "end": "4145520"
  },
  {
    "text": "cool is that you have this find similar pattern which really covers the semantic",
    "start": "4145520",
    "end": "4150798"
  },
  {
    "text": "search part of everything and as you can see this is a record and",
    "start": "4150799",
    "end": "4157758"
  },
  {
    "text": "then eventually end up with the most similar records and these are all worlds",
    "start": "4157759",
    "end": "4163040"
  },
  {
    "text": "uh cover these all these records go for World topics so what you can do is",
    "start": "4163040",
    "end": "4168080"
  },
  {
    "text": "actually select them and all annotate them as a specific example so by doing like these filtering querying and these",
    "start": "4168080",
    "end": "4175040"
  },
  {
    "text": "kind of things you can really like Traverse your data set a big group a lot quicker than you would normally do",
    "start": "4175040",
    "end": "4182380"
  },
  {
    "text": "um what you want to do before you actually need to before you actually have the",
    "start": "4182719",
    "end": "4189140"
  },
  {
    "text": "capability to do semantic search is actually do these inference Loops where you want to have your like your data set",
    "start": "4189140",
    "end": "4196040"
  },
  {
    "text": "you want to upload it into your Gala and uh for doing that I actually played around a bit with great data to do like",
    "start": "4196040",
    "end": "4202219"
  },
  {
    "text": "this distributed pre-processing wherever you initialize the sentence sentence Transformer model",
    "start": "4202219",
    "end": "4208640"
  },
  {
    "text": "in this case a very default one and actually distribute the data over these",
    "start": "4208640",
    "end": "4215600"
  },
  {
    "text": "four actors or two to four actors that you might want to Auto skill to and you",
    "start": "4215600",
    "end": "4221719"
  },
  {
    "text": "actually assign a number of resources that each one of these actors can use and due to the fact that like these",
    "start": "4221719",
    "end": "4227179"
  },
  {
    "text": "models are actually quite the these models can be quite large language models you actually distribute the the",
    "start": "4227179",
    "end": "4233600"
  },
  {
    "text": "task load over over your cluster and therefore it can can be done more easily than uh you would do it like",
    "start": "4233600",
    "end": "4240920"
  },
  {
    "text": "individually and recently uh a user as one one of our users who's quite active",
    "start": "4240920",
    "end": "4247820"
  },
  {
    "text": "in slack that also had the same issue where he wanted to um embed I think the factors for 30",
    "start": "4247820",
    "end": "4255199"
  },
  {
    "text": "million records and use this similar use case along with a package that actually transforms the census Transformer model",
    "start": "4255199",
    "end": "4263120"
  },
  {
    "text": "in Onyx representation so that you actually dialed down like the the model memory",
    "start": "4263120",
    "end": "4269960"
  },
  {
    "text": "and also this increase the speed by doing that I actually have a package for this myself and I will also forward the",
    "start": "4269960",
    "end": "4277219"
  },
  {
    "text": "the references to that later one so then we will actually get to the",
    "start": "4277219",
    "end": "4283880"
  },
  {
    "text": "first search shortcuts which is a week's supervision where you actually have this raw data set and intuitively start to",
    "start": "4283880",
    "end": "4291260"
  },
  {
    "text": "query the data set and Define rules based on these queries where for example you have these uh query that we had with",
    "start": "4291260",
    "end": "4298520"
  },
  {
    "text": "politics and the presidents and then eventually assign a label to that",
    "start": "4298520",
    "end": "4303800"
  },
  {
    "text": "specific query if the records hit or match that query then you end up with a weak labeling set and you actually",
    "start": "4303800",
    "end": "4311780"
  },
  {
    "text": "compose a label model where this is like a decision tree kind of",
    "start": "4311780",
    "end": "4317420"
  },
  {
    "text": "if uh we'll mention so query then you actually assign a label and then you can",
    "start": "4317420",
    "end": "4322520"
  },
  {
    "text": "use this labeling model to actually uh train a data set as well",
    "start": "4322520",
    "end": "4329140"
  },
  {
    "text": "so you yeah end up querying our Gala choose relevant keywords based on the keyword sidebar and Define rules that",
    "start": "4329480",
    "end": "4337280"
  },
  {
    "text": "are human readable quick and easy and even no programming knowledge is required even though you can also on the",
    "start": "4337280",
    "end": "4343880"
  },
  {
    "text": "right hand side see that you can actually Define these rules programmatically as well so if you have",
    "start": "4343880",
    "end": "4349100"
  },
  {
    "text": "like a knowledge base on the side which on default uh key terminology that you expect to see in certain records or for",
    "start": "4349100",
    "end": "4356480"
  },
  {
    "text": "certain labels you can actually choose to to include those and what happens is",
    "start": "4356480",
    "end": "4361880"
  },
  {
    "text": "that you actually can directly export this manual this label model and actually apply it to a data set and",
    "start": "4361880",
    "end": "4368420"
  },
  {
    "text": "prepare the data set for training and then you end up having like a default like model which you can use for for",
    "start": "4368420",
    "end": "4375560"
  },
  {
    "text": "training uh Transformer model that is a bit more more like a bit more robust",
    "start": "4375560",
    "end": "4383840"
  },
  {
    "text": "and then you'll have like a decent Baseline to get started so within the UI you can also like verify these rules",
    "start": "4383840",
    "end": "4390739"
  },
  {
    "text": "like this rule would for example be the word Minister if the word Minister occurs within one of these records as",
    "start": "4390739",
    "end": "4396860"
  },
  {
    "text": "you can see I want to assign the the label words to it you can actually see for each one of the rules that you",
    "start": "4396860",
    "end": "4403040"
  },
  {
    "text": "define what the corporates is for the rules so how many of the actual records are are present uh how many uh Records",
    "start": "4403040",
    "end": "4411500"
  },
  {
    "text": "it hits from other records that are present and you can see how many of the annotated records it hits so for now I",
    "start": "4411500",
    "end": "4418040"
  },
  {
    "text": "only annotated 32 records so 2 out of 32 actually have a match and out of those",
    "start": "4418040",
    "end": "4423080"
  },
  {
    "text": "two two are correct so for this rule you actually know that the Precision given the annotated data is 100",
    "start": "4423080",
    "end": "4430219"
  },
  {
    "text": "and then you end up like with our entire rule set that's a human can actually",
    "start": "4430219",
    "end": "4435340"
  },
  {
    "text": "interpret and you can also verify against the annotated data",
    "start": "4435340",
    "end": "4440540"
  },
  {
    "text": "and you can actually use to training class of Fire",
    "start": "4440540",
    "end": "4445239"
  },
  {
    "text": "another week supervision approach that I was playing around with is actually based on these inspectors that we",
    "start": "4446900",
    "end": "4453159"
  },
  {
    "text": "obtained via the ray data integration and so if you have your data set with",
    "start": "4453159",
    "end": "4458420"
  },
  {
    "text": "all these factors what you can do is actually load the annotated samples and actually average out the embeddings of",
    "start": "4458420",
    "end": "4464960"
  },
  {
    "text": "these samples into semantic examples as you might say where for example within",
    "start": "4464960",
    "end": "4471679"
  },
  {
    "text": "the blue cluster you would end up with like grabbing eight of the annotated",
    "start": "4471679",
    "end": "4476780"
  },
  {
    "text": "examples written in the blue cluster averaging them out and then from the average look at the most similar records",
    "start": "4476780",
    "end": "4483260"
  },
  {
    "text": "and actually apply predictions based on that given the assumption that like things are going to be semantically",
    "start": "4483260",
    "end": "4489500"
  },
  {
    "text": "similar so they're going to be semantically equal cement fee in the same space within the vector space you",
    "start": "4489500",
    "end": "4497239"
  },
  {
    "text": "might use that to actually apply predictions and by doing this you actually have these like pre-computed embeddings you don't need to train any",
    "start": "4497239",
    "end": "4503840"
  },
  {
    "text": "classifier and you can safely assume that uh uh when you have these",
    "start": "4503840",
    "end": "4509060"
  },
  {
    "text": "annotations already set up that you can relatively easy make these predictions that the most similar records are also",
    "start": "4509060",
    "end": "4517400"
  },
  {
    "text": "going to be records of the same class within text application",
    "start": "4517400",
    "end": "4523178"
  },
  {
    "text": "then another example of a cool shortcut to get for your data set from no any",
    "start": "4523659",
    "end": "4530600"
  },
  {
    "text": "annotated data to decently annotate that data as future learning where you will",
    "start": "4530600",
    "end": "4535940"
  },
  {
    "text": "have like these couple of examples per labels class and actually start your training process with these couple of",
    "start": "4535940",
    "end": "4542600"
  },
  {
    "text": "examples in my case as you might have seen within the UI we ended up starting",
    "start": "4542600",
    "end": "4547699"
  },
  {
    "text": "with eight annotated examples per class and uh these annotated examples we",
    "start": "4547699",
    "end": "4552800"
  },
  {
    "text": "actually use for a package that I created which is a Class C classification which uses like a space c",
    "start": "4552800",
    "end": "4559040"
  },
  {
    "text": "or a sentence Transformers embeddings and a supports Vector classifier to actually train a classifier from scratch",
    "start": "4559040",
    "end": "4566679"
  },
  {
    "text": "and use the user Onyx model representation to also make the",
    "start": "4566679",
    "end": "4573260"
  },
  {
    "text": "inference a lot faster because normally if you like need to train a custom birth",
    "start": "4573260",
    "end": "4578540"
  },
  {
    "text": "model or any other Transformer you you'll end up needing like thousands of examples and GPU to actually train it",
    "start": "4578540",
    "end": "4585679"
  },
  {
    "text": "but this is actually done on CPU and the training and inference for like 500",
    "start": "4585679",
    "end": "4591199"
  },
  {
    "text": "samples within the data set just to get the most certain samples uh yeah once",
    "start": "4591199",
    "end": "4596600"
  },
  {
    "text": "within under one minute and on the right hand side you can actually see some some code that like loads the data from our",
    "start": "4596600",
    "end": "4603020"
  },
  {
    "text": "Gala for all of the records that are validated um a train apply a crazy training data",
    "start": "4603020",
    "end": "4608840"
  },
  {
    "text": "sets for classic classification and then eventually assigns the uh the Dos like",
    "start": "4608840",
    "end": "4615980"
  },
  {
    "text": "inference for an annotated data and updates that data specifically within the data sets and that's also another",
    "start": "4615980",
    "end": "4622940"
  },
  {
    "text": "way to get from like no Zero from eight examples per class to like 500 labeled",
    "start": "4622940",
    "end": "4629840"
  },
  {
    "text": "examples you know based on some heuristics and assumptions and the bonus is that you can actually use like",
    "start": "4629840",
    "end": "4635840"
  },
  {
    "text": "multilingual models as well which ends up giving you multilingual predictions uh where for example low research",
    "start": "4635840",
    "end": "4643040"
  },
  {
    "text": "languages this might not always have data available and you might want to like move from one of your like um",
    "start": "4643040",
    "end": "4650080"
  },
  {
    "text": "English data sets to a Dutch data set and this is also a approach that I used",
    "start": "4650080",
    "end": "4655100"
  },
  {
    "text": "for another package Crosslink local reference that I that I made at my previous employer where the where core",
    "start": "4655100",
    "end": "4662600"
  },
  {
    "text": "references are resolved within NLP and finally there's Active Learning",
    "start": "4662600",
    "end": "4670600"
  },
  {
    "text": "where you really focus on Side by Side Learning and of a human and a machine",
    "start": "4670600",
    "end": "4678140"
  },
  {
    "text": "learning algorithm or AI where initially you pass the Oracle like a label start",
    "start": "4678140",
    "end": "4683659"
  },
  {
    "text": "labeling data you have your label data on the rights which you like pass to a",
    "start": "4683659",
    "end": "4689300"
  },
  {
    "text": "learner that learner actually evaluates the unlabeled data and the unlabeled",
    "start": "4689300",
    "end": "4694640"
  },
  {
    "text": "data actually gets either filtered or selected based on the certainty of the",
    "start": "4694640",
    "end": "4700640"
  },
  {
    "text": "predictions for example so if like this future learner actually picks up on",
    "start": "4700640",
    "end": "4706100"
  },
  {
    "text": "predictions that are very certain you might make the assumption that the they are correct and then you can as a",
    "start": "4706100",
    "end": "4712640"
  },
  {
    "text": "annotator actually give preference to looking at these uh like certainty more certain predictions to make the",
    "start": "4712640",
    "end": "4719000"
  },
  {
    "text": "annotations uh annotation process go a little bit more smoother um what the nice thing is from classic",
    "start": "4719000",
    "end": "4726199"
  },
  {
    "text": "classification or the future learning approach I just showed you is that it actually can run uh yeah quite fast and",
    "start": "4726199",
    "end": "4733580"
  },
  {
    "text": "side by side on CPU so normally for like doing Active Learning with birds or the",
    "start": "4733580",
    "end": "4739219"
  },
  {
    "text": "small text package you will end up needing a GPU and uh still it takes some",
    "start": "4739219",
    "end": "4744380"
  },
  {
    "text": "time to actually process the uh newly labeled data and to do one like a",
    "start": "4744380",
    "end": "4750380"
  },
  {
    "text": "training step but with a classic classification you can actually do that side by side so you can actually works more smarter",
    "start": "4750380",
    "end": "4757580"
  },
  {
    "text": "listen to newly annotated examples so the most recent analytic samples a data",
    "start": "4757580",
    "end": "4763100"
  },
  {
    "text": "model and then make new predictions and we actually wrapped around this like workflow within like a more easy",
    "start": "4763100",
    "end": "4771080"
  },
  {
    "text": "interpretation where you just Imports classy classification as an active learner plugin then direct it to a",
    "start": "4771080",
    "end": "4778040"
  },
  {
    "text": "specific data set within Argilla and actually start the plugin and then in",
    "start": "4778040",
    "end": "4783260"
  },
  {
    "text": "the background this actually starts labeling and that's also a brief demo that I wanted to show so here you'll end",
    "start": "4783260",
    "end": "4789739"
  },
  {
    "text": "up with the importing the class C learner The Classy learner is for the",
    "start": "4789739",
    "end": "4795080"
  },
  {
    "text": "data software Summits the execution interval by which it's going to check the data set for newly annotated",
    "start": "4795080",
    "end": "4801080"
  },
  {
    "text": "examples in the background is one and a batch size bar which is going to make prediction system so there's some",
    "start": "4801080",
    "end": "4808000"
  },
  {
    "text": "variables that you might configure but in the end it's also going to be a lot",
    "start": "4808000",
    "end": "4813140"
  },
  {
    "text": "easier than coming up with all these assumptions yourself",
    "start": "4813140",
    "end": "4817480"
  },
  {
    "text": "so what happens now is that it actually starts like this background process and due to the assumption that I made that",
    "start": "4818420",
    "end": "4825199"
  },
  {
    "text": "they'd be want at least eight samples per class it now states that we don't",
    "start": "4825199",
    "end": "4831140"
  },
  {
    "text": "have enough of enough samples so if we were to go back to like the UI you'll",
    "start": "4831140",
    "end": "4837020"
  },
  {
    "text": "end up with uh labeling this as Sports as an annotation and then what is going to happen is",
    "start": "4837020",
    "end": "4843860"
  },
  {
    "text": "actually classic classification so the active learner is actually going to load to data and in the background lock new",
    "start": "4843860",
    "end": "4850460"
  },
  {
    "text": "predictions into our Gala uh going back to IPL live you go to sorting then go to",
    "start": "4850460",
    "end": "4858679"
  },
  {
    "text": "last updated most recent what happens is that you can actually see like these most recent",
    "start": "4858679",
    "end": "4865219"
  },
  {
    "text": "predictions with a prediction score and this will really help when you are annotating and going over the newly",
    "start": "4865219",
    "end": "4872900"
  },
  {
    "text": "annotated records these records and these predictions are going to be more certain and it's relatively easy to kind",
    "start": "4872900",
    "end": "4879440"
  },
  {
    "text": "of say okay this is correct this is incorrect and it will improve over time and you'll like do the active learning",
    "start": "4879440",
    "end": "4885800"
  },
  {
    "text": "workflow side by side while also improving your model",
    "start": "4885800",
    "end": "4890860"
  },
  {
    "text": "[Music] then eventually so when you've got all these uh applied all these cool",
    "start": "4891620",
    "end": "4897679"
  },
  {
    "text": "shortcuts for bootstrapping uh NOP pipeline you can start training a formal",
    "start": "4897679",
    "end": "4902780"
  },
  {
    "text": "model and I actually created some yeah code Snippets to Showcase how this would be done by integrating uh both way June",
    "start": "4902780",
    "end": "4912199"
  },
  {
    "text": "and our gala where you initially on the left hand side with like load some data from arcella prepare the data for",
    "start": "4912199",
    "end": "4918860"
  },
  {
    "text": "training and then on the right hand side actually use a way tune for a hyper parameter optimization and eventually",
    "start": "4918860",
    "end": "4926060"
  },
  {
    "text": "you might also use like a red train to do",
    "start": "4926060",
    "end": "4931420"
  },
  {
    "text": "training process if the data ice turns out to be too large or if you just want",
    "start": "4932380",
    "end": "4937699"
  },
  {
    "text": "to speed up your your training process and the the nice thing is that this approach also directly integrates with",
    "start": "4937699",
    "end": "4943580"
  },
  {
    "text": "weights and biases so that you can actually directly see like the the output of T20 processes",
    "start": "4943580",
    "end": "4950480"
  },
  {
    "text": "um where like and for all of the ones that you did and similarly I think that",
    "start": "4950480",
    "end": "4956060"
  },
  {
    "text": "I do really like is that nowadays they've also got like this system GPU what the what usage and these kind of",
    "start": "4956060",
    "end": "4962120"
  },
  {
    "text": "things because this has been really discussed for for like these larger",
    "start": "4962120",
    "end": "4967640"
  },
  {
    "text": "language models what the uh uh sustainability impacts might be for for",
    "start": "4967640",
    "end": "4973400"
  },
  {
    "text": "these kind of things foreign",
    "start": "4973400",
    "end": "4977440"
  },
  {
    "text": "what I generally like to do is think about the resources that you do have so for like these weeks of provision rules",
    "start": "4981080",
    "end": "4987860"
  },
  {
    "text": "you end up with uh some rules that's the human human annotators are aware of be",
    "start": "4987860",
    "end": "4995179"
  },
  {
    "text": "creative and iterate of your data and that's uh kind of what I recommend doing for these NLP approaches where you don't",
    "start": "4995179",
    "end": "5002500"
  },
  {
    "text": "have any data and you really want to get started and boost up your process before starting to iterate over your data and",
    "start": "5002500",
    "end": "5009940"
  },
  {
    "text": "your data set then eventually this is a use case about or this is an example of like the",
    "start": "5009940",
    "end": "5016060"
  },
  {
    "text": "package I mentioned before costly Google reference where there wasn't any data available in Dutch I'm from the",
    "start": "5016060",
    "end": "5022360"
  },
  {
    "text": "Netherlands and we actually needed to get a cross cross-lingual or sorry a",
    "start": "5022360",
    "end": "5028600"
  },
  {
    "text": "co-reference model up and running in Dutch and we actually decided to use a cross-lingual model as a default model",
    "start": "5028600",
    "end": "5035020"
  },
  {
    "text": "to train but using our English data sets and eventually it also proves to work on",
    "start": "5035020",
    "end": "5040239"
  },
  {
    "text": "like Dutch German French and nowadays it's I think one of the only models that's like multilingually available for",
    "start": "5040239",
    "end": "5047260"
  },
  {
    "text": "cover reference resolution so that's pretty cool and this is another example where I",
    "start": "5047260",
    "end": "5054100"
  },
  {
    "text": "found a cute cool way to work on like a few shots entity recognition where you have like these labels and only a couple",
    "start": "5054100",
    "end": "5060400"
  },
  {
    "text": "of examples I really want to get from having No Label data for entity recognition to some default data to to",
    "start": "5060400",
    "end": "5068020"
  },
  {
    "text": "get you started and to actually iterate and work towards a decent data set and training model and eventually serving it",
    "start": "5068020",
    "end": "5074560"
  },
  {
    "text": "of course with everybody serve these are some sources that I use so",
    "start": "5074560",
    "end": "5080020"
  },
  {
    "text": "agila people we have some cool data sets on the Hub nowadays we also have the option to actually deploy our Gala along",
    "start": "5080020",
    "end": "5088179"
  },
  {
    "text": "with elasticsearch to end some test training data sets within our within",
    "start": "5088179",
    "end": "5093760"
  },
  {
    "text": "hurricane phase spaces uh something about B and the packages I created can",
    "start": "5093760",
    "end": "5099340"
  },
  {
    "text": "be found on their Pandora intelligence I do some volunteering as well and some other packages that are cool for a few",
    "start": "5099340",
    "end": "5106600"
  },
  {
    "text": "soap learning set fits weak supervision SK week and active learning small text",
    "start": "5106600",
    "end": "5113380"
  },
  {
    "text": "and that's it uh thank you David for that broad introduction to the NLP",
    "start": "5113380",
    "end": "5120699"
  },
  {
    "text": "um use cases a couple of couple of curious questions I saw you actually use Ray data excuse me losing my voice",
    "start": "5120699",
    "end": "5129719"
  },
  {
    "text": "no problem to do your to to do your batch training so you're actually using",
    "start": "5130480",
    "end": "5136020"
  },
  {
    "text": "records active pool strategy to create minimal number of actors and maximum",
    "start": "5136020",
    "end": "5141040"
  },
  {
    "text": "number of actors what did you see the benefits in there I mean why did you choose to do that compared to say",
    "start": "5141040",
    "end": "5146560"
  },
  {
    "text": "something else was it something that that give you the the easy that you could actually train",
    "start": "5146560",
    "end": "5153219"
  },
  {
    "text": "and distribute your data using the record strategy that was like the the easy out",
    "start": "5153219",
    "end": "5159159"
  },
  {
    "text": "of the box uh availability because otherwise you will need to send up a set",
    "start": "5159159",
    "end": "5164380"
  },
  {
    "text": "up something like host different models separate or like you could use racer for example but then you would need to",
    "start": "5164380",
    "end": "5170199"
  },
  {
    "text": "handle all this logic in terms of making requests and these kind of things if you want to be able to scale this",
    "start": "5170199",
    "end": "5175960"
  },
  {
    "text": "automatically and now you can use the easy like python API whilst they were using like pandas and things that I at",
    "start": "5175960",
    "end": "5183639"
  },
  {
    "text": "least intuitively I'm more familiar with and similarly this",
    "start": "5183639",
    "end": "5189580"
  },
  {
    "text": "like pipeline that you set up also works for like only using one data set and",
    "start": "5189580",
    "end": "5195280"
  },
  {
    "text": "that's uh over sorry only using one model or just using it locally",
    "start": "5195280",
    "end": "5202000"
  },
  {
    "text": "so for the for the rate Tunes hyper parameter tuning I mean did you did you take an existing Transformer hugging",
    "start": "5202000",
    "end": "5208360"
  },
  {
    "text": "Trace model and fine-tuned it um yeah to do to do that and this was",
    "start": "5208360",
    "end": "5213400"
  },
  {
    "text": "what the text classification yeah exactly so for the model I actually used",
    "start": "5213400",
    "end": "5219040"
  },
  {
    "text": "a tiny Bird model because okay the the demo purposes uh so it uses a tiny Bird",
    "start": "5219040",
    "end": "5225699"
  },
  {
    "text": "model which you can quite easily test and train on a local device or a bit",
    "start": "5225699",
    "end": "5230860"
  },
  {
    "text": "more easier than a this default Bird model but yeah you can you can plug and play any arbitrary model that you want",
    "start": "5230860",
    "end": "5236860"
  },
  {
    "text": "so it's in arguing phase as I mentioned you'll have all of all these uh default",
    "start": "5236860",
    "end": "5241960"
  },
  {
    "text": "language models that you can just call Via right right using a string yeah as a",
    "start": "5241960",
    "end": "5248679"
  },
  {
    "text": "matter of fact I don't know if you're actually aware I mean you know we're in our rig array API of the the retrain we",
    "start": "5248679",
    "end": "5256179"
  },
  {
    "text": "actually have added added the ray train trainer of hugging face you can actually take any hugging face and wrap it around",
    "start": "5256179",
    "end": "5261940"
  },
  {
    "text": "your python trainer and then do the tuning did you actually use that uh API or you just use the ray Trend so yeah I",
    "start": "5261940",
    "end": "5270159"
  },
  {
    "text": "use the way tune in initially and then eventually I wanted to use the uh",
    "start": "5270159",
    "end": "5275560"
  },
  {
    "text": "hugging phase trailer okay in China yes yeah yeah I got this uh uh error which",
    "start": "5275560",
    "end": "5282699"
  },
  {
    "text": "was uh which I wasn't managed to wasn't able to resolve in like uh easy short",
    "start": "5282699",
    "end": "5289659"
  },
  {
    "text": "time but it might be something with apple silicon yeah I think I think",
    "start": "5289659",
    "end": "5295900"
  },
  {
    "text": "sort of a good addition because you actually have people actually",
    "start": "5295900",
    "end": "5301900"
  },
  {
    "text": "replace trainer by just as you said providing the the pre-trained model and then we actually tune in to do the",
    "start": "5301900",
    "end": "5308020"
  },
  {
    "text": "executive do the specific tasks where they're doing summarization where they were doing entertain and then using the",
    "start": "5308020",
    "end": "5314199"
  },
  {
    "text": "using the the array trainer model on top of pycho which allows you actually to squel quite a bit what I was curious was",
    "start": "5314199",
    "end": "5321280"
  },
  {
    "text": "was when you were actually doing this NLP inferences um did you actually use raso uh opening",
    "start": "5321280",
    "end": "5329020"
  },
  {
    "text": "up the apis to deploy the model using some of the things I talked about earlier or was it something that's still under investigation no it's still under",
    "start": "5329020",
    "end": "5336100"
  },
  {
    "text": "investigation so for for nive uh it's our Gillan not actively used Tracer but by our previous company we did so we had",
    "start": "5336100",
    "end": "5343659"
  },
  {
    "text": "like all kinds of models for English French uh Spanish and Dutch and then uh",
    "start": "5343659",
    "end": "5351219"
  },
  {
    "text": "yeah for for some clients the certain models were used more often and then we would use it for for these kind of use",
    "start": "5351219",
    "end": "5357760"
  },
  {
    "text": "cases for now I was also thinking for Reserve potentially looking into it to for example host these active Learners",
    "start": "5357760",
    "end": "5364679"
  },
  {
    "text": "scaling to zero and whenever someone like starts Active Learning Loop that you could potentially like boot up the",
    "start": "5364679",
    "end": "5371080"
  },
  {
    "text": "active learner and make sure that he like the inference uh can be done in terms of the the embedding that you",
    "start": "5371080",
    "end": "5377020"
  },
  {
    "text": "might want to do so I wanted to play around with that a bit more but yeah for now the the plugins is like a toy use",
    "start": "5377020",
    "end": "5384520"
  },
  {
    "text": "case and we are like looking into to how we can extend it there um I've been working with with fee for",
    "start": "5384520",
    "end": "5391360"
  },
  {
    "text": "example uh your your colleague to move into like the SP middleware integration that we offer to extract so",
    "start": "5391360",
    "end": "5399340"
  },
  {
    "text": "um like monetary serves Ray serve models and actually uh make sure that all of",
    "start": "5399340",
    "end": "5405820"
  },
  {
    "text": "the predictions from each one of these models actually come into our Gala correctly so that you don't lose any of",
    "start": "5405820",
    "end": "5411460"
  },
  {
    "text": "the valuable data that or predictions that you might have done yeah I mean we we certainly would be",
    "start": "5411460",
    "end": "5416980"
  },
  {
    "text": "interested to see how actually earlier can actually use risk so uh at scale especially some of the things that",
    "start": "5416980",
    "end": "5422139"
  },
  {
    "text": "you're actually doing in some of the use cases that that you talked about and I think it lends it itself very easily to",
    "start": "5422139",
    "end": "5429400"
  },
  {
    "text": "some of the scaling that you actually talked about editing some of the issues that Mia Miha ran into",
    "start": "5429400",
    "end": "5435040"
  },
  {
    "text": "um yeah a question for you did you feel the the the 2.0",
    "start": "5435040",
    "end": "5440199"
  },
  {
    "text": "um apis for model composition and the problem that you actually rendered at",
    "start": "5440199",
    "end": "5447159"
  },
  {
    "text": "the earlier compliance I think it was you actually started the initiation by filing an issue in PR saying we actually",
    "start": "5447159",
    "end": "5452199"
  },
  {
    "text": "want to do zero zero scaling so we did Implement Implement uh uh uh scaling",
    "start": "5452199",
    "end": "5459400"
  },
  {
    "text": "down to zero yeah um thanks uh yeah actually um I remember what it was uh why we",
    "start": "5459400",
    "end": "5466120"
  },
  {
    "text": "haven't gone through with the auto scaler for surf uh it was like a",
    "start": "5466120",
    "end": "5471880"
  },
  {
    "text": "combination of time and just other you know priorities",
    "start": "5471880",
    "end": "5477159"
  },
  {
    "text": "um but also there was like I think this is a big issue for um serving uh race serve on uh",
    "start": "5477159",
    "end": "5484659"
  },
  {
    "text": "kubernetes is that um we were like one key element of",
    "start": "5484659",
    "end": "5490000"
  },
  {
    "text": "making a race surf uh work on on kubernetes is the fact that",
    "start": "5490000",
    "end": "5496060"
  },
  {
    "text": "um we needed to optimize our array Docker images um and we we already did actually",
    "start": "5496060",
    "end": "5503699"
  },
  {
    "text": "into GPU and CPU and stripped down a lot of libraries but there was more work to",
    "start": "5503699",
    "end": "5509800"
  },
  {
    "text": "be done and the problem was at the beginning when we when we started we had",
    "start": "5509800",
    "end": "5515260"
  },
  {
    "text": "quite uh let's say quite a slow connection between the the docker repository and the the kubernetes",
    "start": "5515260",
    "end": "5523120"
  },
  {
    "text": "cluster and this took uh several minutes actually around five minutes when we first did it uh to actually download the",
    "start": "5523120",
    "end": "5530500"
  },
  {
    "text": "the docker image to the Pod start it up um your bootstrap bootstrap the pot uh",
    "start": "5530500",
    "end": "5536500"
  },
  {
    "text": "and then also load the models which transfer the model from let's say a",
    "start": "5536500",
    "end": "5541540"
  },
  {
    "text": "bucket and then um and then load it uh that that the last part took just a few seconds still",
    "start": "5541540",
    "end": "5547840"
  },
  {
    "text": "but um all of this was quite a I would say quite a bottleneck when when it came",
    "start": "5547840",
    "end": "5553360"
  },
  {
    "text": "to you know quickly provisioning pods for auto scale for for you know this kind of fast Dynamic Auto scaling and",
    "start": "5553360",
    "end": "5560860"
  },
  {
    "text": "those tasks uh needed to be done first I think um we also were on onyx and uh we we",
    "start": "5560860",
    "end": "5567760"
  },
  {
    "text": "tried like I already had like um stories written out we just couldn't get to it",
    "start": "5567760",
    "end": "5573820"
  },
  {
    "text": "yeah yeah so I mean we like like I said I think the continuing effort and and",
    "start": "5573820",
    "end": "5579639"
  },
  {
    "text": "Akshay was the engineering manager racer will will attest to that they've been putting a lot of effort in making sure",
    "start": "5579639",
    "end": "5585159"
  },
  {
    "text": "that the the the doubling downward production for race so especially the array operator that we have actually",
    "start": "5585159",
    "end": "5591520"
  },
  {
    "text": "specifically built on top of cube Ray to be able to vary seamlessly and easily",
    "start": "5591520",
    "end": "5597520"
  },
  {
    "text": "um uh update scale uh the new race of application the year you actually don't have to worry about uh writing a",
    "start": "5597520",
    "end": "5604480"
  },
  {
    "text": "manifest you just use this server command that we actually have you take your your your entire python application",
    "start": "5604480",
    "end": "5610960"
  },
  {
    "text": "you give it to so you actually go in and build and it actually builds and manifest and then you can actually use Cube CTL to deploy that manifest and we",
    "start": "5610960",
    "end": "5618820"
  },
  {
    "text": "will actually deploy it through the ray airport operator to the cube rate go ahead and update create the new cluster",
    "start": "5618820",
    "end": "5624760"
  },
  {
    "text": "and update in replicas so if you wanted to change your configuration there's almost zero time so look into that and I",
    "start": "5624760",
    "end": "5630820"
  },
  {
    "text": "think you actually might see that it is addresses some of the questions yes David have you you have your hand raised",
    "start": "5630820",
    "end": "5636820"
  },
  {
    "text": "go ahead yeah yeah so but I was I was curious what what's your view because now you're mostly talking about scaling",
    "start": "5636820",
    "end": "5643000"
  },
  {
    "text": "up and down but what's your view on uh like making sure that these models are as efficient as possible like like",
    "start": "5643000",
    "end": "5649780"
  },
  {
    "text": "either using specification or next um these kind of things quantization",
    "start": "5649780",
    "end": "5656920"
  },
  {
    "text": "I mean obviously the quantization like I said remember when when you when you when you when you're thinking about",
    "start": "5656920",
    "end": "5662199"
  },
  {
    "text": "about a modal efficiency uh there are two ways to do that one is either to",
    "start": "5662199",
    "end": "5668860"
  },
  {
    "text": "profile the model or you actually quantize the model the breaking down into into all these different uh",
    "start": "5668860",
    "end": "5674620"
  },
  {
    "text": "different smaller parts in where you can actually um uh do the prediction easily by",
    "start": "5674620",
    "end": "5679659"
  },
  {
    "text": "quantizing that I think all that work is actually going on to be able to do that especially when you're dealing with",
    "start": "5679659",
    "end": "5685719"
  },
  {
    "text": "large models right when you're dealing with large ml models some of them don't fit in the members here they're breaking down the equation to make sure so all",
    "start": "5685719",
    "end": "5692260"
  },
  {
    "text": "these efforts are on our roadmap is anything else you uh or see how you want",
    "start": "5692260",
    "end": "5697540"
  },
  {
    "text": "to talk about in terms of of my efforts for for quantization but I'm not sure",
    "start": "5697540",
    "end": "5702699"
  },
  {
    "text": "about I'm not sure about Onyx yet I don't know if that's on on the roadmap Can it can either of you uh speak to it",
    "start": "5702699",
    "end": "5711179"
  },
  {
    "text": "uh not really too much for for model itself um I still rely on any uh third party uh",
    "start": "5711179",
    "end": "5718540"
  },
  {
    "text": "library to optimizing your model and on the serve side we are supporting any",
    "start": "5718540",
    "end": "5723880"
  },
  {
    "text": "hardware acceleration uh for example if you want to go for uh Triton for your",
    "start": "5723880",
    "end": "5729580"
  },
  {
    "text": "GPU uh or you want to go to more deeper or using the information on the right",
    "start": "5729580",
    "end": "5735040"
  },
  {
    "text": "side so I think both of this can be supported on the resource side but on the model itself if you want to use Onyx",
    "start": "5735040",
    "end": "5741820"
  },
  {
    "text": "or using any uh third-party library or deepseat to directly optimize your model",
    "start": "5741820",
    "end": "5746920"
  },
  {
    "text": "that totally uh free read out on the client side or engineer to do any magic",
    "start": "5746920",
    "end": "5752320"
  },
  {
    "text": "thing yeah in terms of the quantization of what",
    "start": "5752320",
    "end": "5759520"
  },
  {
    "text": "sort of you know whether you want to do is fp16 or whether you want to scale it down to an integer I think that also depends on the pie torch underlying uh",
    "start": "5759520",
    "end": "5766960"
  },
  {
    "text": "model so when you actually send it to to a particular Cuda device you can actually specify you know how you",
    "start": "5766960",
    "end": "5772120"
  },
  {
    "text": "actually want to quantize that so I think that is not so much part of the race of itself but it's sort of more",
    "start": "5772120",
    "end": "5778060"
  },
  {
    "text": "part of the underlying underlying implementation of of python but it was more like uh yes yes or uh yeah",
    "start": "5778060",
    "end": "5785380"
  },
  {
    "text": "landscape question yeah yeah so like I said the quantize was actually a bit about about breaking in breaking into or",
    "start": "5785380",
    "end": "5792280"
  },
  {
    "text": "or making sure that the inferences is quite those are the techniques that that the underlying framework actually gives",
    "start": "5792280",
    "end": "5799239"
  },
  {
    "text": "you and we can sort of you know your your python code is just going to say okay I'm going to send my I'm using a",
    "start": "5799239",
    "end": "5805659"
  },
  {
    "text": "Cuda device and I'm going to quantize that and scale it down to uh fp16 so yeah those are those are the things",
    "start": "5805659",
    "end": "5811480"
  },
  {
    "text": "which are sort of more Library dependent any other questions I think we are coming close to almost",
    "start": "5811480",
    "end": "5818860"
  },
  {
    "text": "there I do want to thank David I do want to thank Mia and I don't want to pin uh what I really want from both of you is",
    "start": "5818860",
    "end": "5825699"
  },
  {
    "text": "that if you can actually send me your PDF versions of your slides so I can",
    "start": "5825699",
    "end": "5830980"
  },
  {
    "text": "make it available to people who actually couldn't attend so thank you very much have a good evening",
    "start": "5830980",
    "end": "5839400"
  },
  {
    "text": "okay yeah all right thanks for the opportunity thanks for the invites",
    "start": "5840520",
    "end": "5846360"
  }
]