[
  {
    "text": "okay well let's get started uh welcome to the Bay Area rare Community Meetup",
    "start": "1620",
    "end": "8400"
  },
  {
    "text": "folks the the first Meetup of the new year uh my name is Jules damji I am part of the ray advocacy team over here I'm",
    "start": "8400",
    "end": "15960"
  },
  {
    "text": "delighted to see you so many of you actually joined us in different time zones always happy to have all these",
    "start": "15960",
    "end": "21660"
  },
  {
    "text": "people who actually take time to join us Dimitri welcome thanks a lot for joining us a couple of logistics I would just",
    "start": "21660",
    "end": "28380"
  },
  {
    "text": "want to make you sort out so that we actually can run this thing tonight smoothly audio mics are unfortunately",
    "start": "28380",
    "end": "34320"
  },
  {
    "text": "muted so we can't hear you nor can we actually see you uh if you have any",
    "start": "34320",
    "end": "39660"
  },
  {
    "text": "problems regarding the slideshow or regarding the notebook then you can't really see the font",
    "start": "39660",
    "end": "45719"
  },
  {
    "text": "um do please put it on the chat and we'll try to sort it out I do have done",
    "start": "45719",
    "end": "53039"
  },
  {
    "text": "it now I have to change settings to",
    "start": "53039",
    "end": "57020"
  },
  {
    "text": "yeah it is actually enabled for everyone so everyone should be able to actually put things on the chat um if you have any questions for the Q a",
    "start": "58500",
    "end": "64920"
  },
  {
    "text": "or the speaker please put it in the qns section because it's easier for me to manage and run it rather than scrolling",
    "start": "64920",
    "end": "71520"
  },
  {
    "text": "up and down the chat window because because I can actually lose that quite easily so put all your questions for the",
    "start": "71520",
    "end": "77100"
  },
  {
    "text": "speakers in the Q a sections and towards the end of the talk I'll try to moderate and then put those questions to the",
    "start": "77100",
    "end": "82979"
  },
  {
    "text": "speaker who can answer it so so much for the logistics um let's get really started with the uh",
    "start": "82979",
    "end": "88860"
  },
  {
    "text": "with the session today and here's the agenda normally for the first few minutes I'll give an",
    "start": "88860",
    "end": "95220"
  },
  {
    "text": "announcements or some of the upcoming events which are coming up and I think it's nice to actually reflect back on",
    "start": "95220",
    "end": "100740"
  },
  {
    "text": "how the ray Community invaded in 2022 so I'm going to briefly about for about 10 minutes give a high level view of some",
    "start": "100740",
    "end": "108000"
  },
  {
    "text": "of the accomplishments that the ray team and the community together have achieved in 2022 and then we have two good uh",
    "start": "108000",
    "end": "114840"
  },
  {
    "text": "talks uh how to actually prevent your pernicious error um error using the",
    "start": "114840",
    "end": "121140"
  },
  {
    "text": "array monitor by Clarence is part of the great team focusing on Race stability and observability",
    "start": "121140",
    "end": "127320"
  },
  {
    "text": "so he's going to share that with us and then we have a ray Community user from Shopify",
    "start": "127320",
    "end": "133560"
  },
  {
    "text": "um raghavan who's going to tell us how they actually use tensorflow and Rey",
    "start": "133560",
    "end": "139260"
  },
  {
    "text": "um building hierarchical models with billions of products so I think that's going to be a good evening of",
    "start": "139260",
    "end": "144360"
  },
  {
    "text": "re-illumination pun intended over here and so a couple of announcements as you",
    "start": "144360",
    "end": "150360"
  },
  {
    "text": "know we actually had a very successful race Summit last year and the cfps are",
    "start": "150360",
    "end": "155400"
  },
  {
    "text": "open for race Summit again on we started about a month ago and so they opened",
    "start": "155400",
    "end": "161400"
  },
  {
    "text": "till the February the 17th I think about three weeks from today the conference is",
    "start": "161400",
    "end": "166440"
  },
  {
    "text": "going to be held in person so if you decide to submit the cfp and if it gets",
    "start": "166440",
    "end": "171900"
  },
  {
    "text": "accepted do make sure that you can actually travel wherever you're submitting from it's going to be held at",
    "start": "171900",
    "end": "177540"
  },
  {
    "text": "the Marriott Marquis in San Francisco new venue and here's a URL for the cfp it's open",
    "start": "177540",
    "end": "184140"
  },
  {
    "text": "till the 17th so do make sure that uh you submit the speakers the good thing",
    "start": "184140",
    "end": "189239"
  },
  {
    "text": "about any conference is that if it has a good sessions it has a good speaker it actually draws a good crowd to be",
    "start": "189239",
    "end": "195540"
  },
  {
    "text": "depending on the rare Community to share uh your user Journey adopting Ray uh for",
    "start": "195540",
    "end": "201300"
  },
  {
    "text": "the next year okay a couple of announcements we're actually having our first Ray inaugural European exclusive",
    "start": "201300",
    "end": "209040"
  },
  {
    "text": "mini a month from today so if you're in Europe and if you have friends in Europe",
    "start": "209040",
    "end": "214400"
  },
  {
    "text": "please join us a month from today we're going to be talking about race serve and how people in Europe especially two",
    "start": "214400",
    "end": "220620"
  },
  {
    "text": "companies actually using race so and what they're actually doing to build on top of that so that's going to be our first ever exclusive European Meetup",
    "start": "220620",
    "end": "228000"
  },
  {
    "text": "it's going to be 8 30 a.m in Pacific time which is about 4 30 in East Coast I mean it's Central Europe time uh another",
    "start": "228000",
    "end": "235560"
  },
  {
    "text": "couple of amounts and if you're new to rain this is your first Meetup day actually having we have various",
    "start": "235560",
    "end": "241860"
  },
  {
    "text": "platforms and various avenues that you can actually join the community so feel free to go to this particular URL",
    "start": "241860",
    "end": "248400"
  },
  {
    "text": "and sign up for the platform of choice of Engagement we want to hear from you and we'd love to hear what your problems",
    "start": "248400",
    "end": "254459"
  },
  {
    "text": "are and try to incorporate that in our releases",
    "start": "254459",
    "end": "258799"
  },
  {
    "text": "like any other public in-person gathering and um virtual event we are very much",
    "start": "259680",
    "end": "266699"
  },
  {
    "text": "steadfast about our code of conduct and civil obedience and so while I'm not going to read all this to you the upshot",
    "start": "266699",
    "end": "273780"
  },
  {
    "text": "is that we want to make sure that of a community Gatherings such as this one and conferences are harassment free so",
    "start": "273780",
    "end": "280199"
  },
  {
    "text": "have a quick read and I'll leave this up here for about a couple of minutes before we go on to the next part of the",
    "start": "280199",
    "end": "287040"
  },
  {
    "text": "presentation so every okay now it's quite customary really",
    "start": "287040",
    "end": "296400"
  },
  {
    "text": "um towards the end of the year you know you you see social media being quite suffused with you know how people have",
    "start": "296400",
    "end": "302160"
  },
  {
    "text": "been sort of achieved things in the last year there's always a reflection about new things that actually the planning to",
    "start": "302160",
    "end": "307320"
  },
  {
    "text": "do in the year and I thought it was actually fitting for me to recap you know some of the accomplishments and",
    "start": "307320",
    "end": "313199"
  },
  {
    "text": "some of the achievements that the ray Community along with the ray team at any scale have achieved in 2022 so here are",
    "start": "313199",
    "end": "320220"
  },
  {
    "text": "some of the high level achievement at a very high level that I dimmed them as pivotal to recount and recap on and so",
    "start": "320220",
    "end": "327240"
  },
  {
    "text": "there are five things that I thought were were worth mentioning and recounting them just to get you know the",
    "start": "327240",
    "end": "333660"
  },
  {
    "text": "year started for for a review so one is is Ray 2.x or Ray 2.0 along",
    "start": "333660",
    "end": "340620"
  },
  {
    "text": "with Ray air and these are the two Milestones they are kind of related to each other because they really prepare",
    "start": "340620",
    "end": "347100"
  },
  {
    "text": "Ray inevitably in the direction that was wanting it was about to come",
    "start": "347100",
    "end": "352320"
  },
  {
    "text": "and and there were two things there's rare air and there's there's Ray 2.0 and",
    "start": "352320",
    "end": "357720"
  },
  {
    "text": "Ray Ray 2.0 was sort of built with the with the principles that we want to make sure it's it's proponent we want to make",
    "start": "357720",
    "end": "363900"
  },
  {
    "text": "sure we simplify the apis you want to make sure he's stable we make sure we we provide a lot of debugging and array of",
    "start": "363900",
    "end": "370020"
  },
  {
    "text": "zlp uh tools that that make Ray easy to use and then built on top of that that",
    "start": "370020",
    "end": "376139"
  },
  {
    "text": "principle we're going to stop with the stability and Reliance is is the gray Air now the question is what is Ray air",
    "start": "376139",
    "end": "383160"
  },
  {
    "text": "what is Ray air runtime and the Nemesis really the Nemesis of the project really",
    "start": "383160",
    "end": "389460"
  },
  {
    "text": "culminated uh when over the years the great team has been talking with a lot of infrastructure group and Android",
    "start": "389460",
    "end": "395940"
  },
  {
    "text": "practitioners with groups like ants and Uber and Shopify and Crews uh and open",
    "start": "395940",
    "end": "401280"
  },
  {
    "text": "AI which had been the race Summit that actually used Rey to train the gp3 and",
    "start": "401280",
    "end": "406800"
  },
  {
    "text": "and chat GPT model which was actually quite uh uh useful for us to know even",
    "start": "406800",
    "end": "411900"
  },
  {
    "text": "though they're very security but now they're made it public do you understand what the pain points are what were the pain points for them",
    "start": "411900",
    "end": "418319"
  },
  {
    "text": "while adopting particular array and I think this particular project called Ray air was really a culmination of lessons",
    "start": "418319",
    "end": "423960"
  },
  {
    "text": "that we actually learned and I can articulate those lessons into sort of um categories one was to synthesize these",
    "start": "423960",
    "end": "430919"
  },
  {
    "text": "efforts these painful efforts to make sure that that we incorporate this into the next release of our libraries and",
    "start": "430919",
    "end": "437819"
  },
  {
    "text": "this was to provide a community with what we call a a simple toolkit called",
    "start": "437819",
    "end": "443819"
  },
  {
    "text": "Ray air which is very cohesive and it actually works together and the second bit was to make sure that we simplify",
    "start": "443819",
    "end": "449400"
  },
  {
    "text": "the array apis so that some of the artifacts that these particular libraries which are part of",
    "start": "449400",
    "end": "456180"
  },
  {
    "text": "the ray air work on whether the input of one can be the output of another one or or something that you actually do a",
    "start": "456180",
    "end": "462960"
  },
  {
    "text": "checkpoint on your tuning and training you can actually use that in serving and so the API is a very very expressive and",
    "start": "462960",
    "end": "469800"
  },
  {
    "text": "they're very composable and we just make sure that they work uniformly and the goal were three things one was that make",
    "start": "469800",
    "end": "476340"
  },
  {
    "text": "sure we have a unified apis that ml practitioners can write into an application from data ingestion all the",
    "start": "476340",
    "end": "482520"
  },
  {
    "text": "way to to serving and scoring we just make sure that the apis were simple and intuitive",
    "start": "482520",
    "end": "488580"
  },
  {
    "text": "and the reliable and more important thing was that we want to make sure that that practitioners can actually scale",
    "start": "488580",
    "end": "495660"
  },
  {
    "text": "the individual workloads whether they're doing training or whether they're doing hyper parameter tuning or they're doing Serving batch scoring those were",
    "start": "495660",
    "end": "501840"
  },
  {
    "text": "actually the important things so that was generally the essence of gray Air and if I had to express those",
    "start": "501840",
    "end": "507720"
  },
  {
    "text": "pictorially what I really meant by those three things were that if you were a data scientist if you're a machine",
    "start": "507720",
    "end": "513539"
  },
  {
    "text": "learning and you're using Ray air you should be able to scale individual workloads whether you're just doing data processing by using Ray data to do",
    "start": "513539",
    "end": "520860"
  },
  {
    "text": "pre-processing so you can do your last mile ingestion into your training uh set in the distributed manner uh the second",
    "start": "520860",
    "end": "527459"
  },
  {
    "text": "thing was we want to make sure that if you commit yourself you use Ray air that is future proof and what I mean by",
    "start": "527459",
    "end": "533760"
  },
  {
    "text": "Future proof what I mean by that is that we want to make sure that out of the box we support some of the common ml",
    "start": "533760",
    "end": "540480"
  },
  {
    "text": "platforms libraries that most people actually use and if for some reason",
    "start": "540480",
    "end": "545940"
  },
  {
    "text": "given that the pace at which the technology is changing and the pace it with the ml Frameworks are changing",
    "start": "545940",
    "end": "551040"
  },
  {
    "text": "there's a new extension or tensorflow extension or pytory we want to be able to make sure that those apis are",
    "start": "551040",
    "end": "557100"
  },
  {
    "text": "extensible and you can actually use them quite easily without having to wait for another release",
    "start": "557100",
    "end": "562920"
  },
  {
    "text": "so that was the second part of make the array extensible and easy to integrate and the third bit is is",
    "start": "562920",
    "end": "571019"
  },
  {
    "text": "use the toolkit in a very holistic manner so if you're actually using uh we are building an ml model you can",
    "start": "571019",
    "end": "577620"
  },
  {
    "text": "actually use it from ingestion all the way to scoring in a very unified manner rather than in a very siled manner the",
    "start": "577620",
    "end": "583140"
  },
  {
    "text": "problem with our libraries prior to Ray air that they were very siled and they didn't work well together they weren't",
    "start": "583140",
    "end": "589080"
  },
  {
    "text": "really cohesive so the idea about Ray air using raid 2.0 apis made us",
    "start": "589080",
    "end": "594860"
  },
  {
    "text": "easy to sort of use the artifacts can that can be used from one to the other so that was you know the the the reason",
    "start": "594860",
    "end": "603060"
  },
  {
    "text": "behind it and if I could you know bear with me with this Marquee architecture slide if I could use that as a layer",
    "start": "603060",
    "end": "609720"
  },
  {
    "text": "kicker functionality you got rare core sitting at the bottom running on top of of your uh your common cloud and as you",
    "start": "609720",
    "end": "616680"
  },
  {
    "text": "go up you got the this cohesive libraries that actually work together and then if you go up at the top you",
    "start": "616680",
    "end": "622680"
  },
  {
    "text": "actually have all these integration points some of them are maintained by the community and others come straight out of the box so you can actually use",
    "start": "622680",
    "end": "629100"
  },
  {
    "text": "Ray air to do your end-to-end application issue and that's sort of the vertical layer of functionality and if I",
    "start": "629100",
    "end": "634920"
  },
  {
    "text": "were to sort of look at it from a horizontal point of view out of the box where it comes with these common",
    "start": "634920",
    "end": "640320"
  },
  {
    "text": "Frameworks that you actually use and because the rear apis the base apis for trainers and Tuners are are extensible",
    "start": "640320",
    "end": "648000"
  },
  {
    "text": "you can actually extend those classes and and incorporate the new ones and finally if you have custom",
    "start": "648000",
    "end": "653600"
  },
  {
    "text": "customable components that you want to use let's say you're in a new data store that you will read from mongodb or even",
    "start": "653600",
    "end": "658980"
  },
  {
    "text": "you're from Delta lake or do you want to use a snowflake you can actually write the extensions to read data and and use",
    "start": "658980",
    "end": "666060"
  },
  {
    "text": "real data sets so that primarily is is one of the things that gray Air actually accomplishes and obviously is actually",
    "start": "666060",
    "end": "673500"
  },
  {
    "text": "built on top of Ray 2.0 which was a massive achievement for us to to get",
    "start": "673500",
    "end": "678660"
  },
  {
    "text": "that going as I say Ray air is is one of the things that was built on top of that another Monumental thing that actually",
    "start": "678660",
    "end": "685320"
  },
  {
    "text": "was quite important was the the introduction of Ray data sets and the work that actually started in the",
    "start": "685320",
    "end": "691740"
  },
  {
    "text": "towards the end of 2022 was where we were able to actually Shuffle 100 terabytes of data using raid header now",
    "start": "691740",
    "end": "699000"
  },
  {
    "text": "shuffling when you're actually doing training is a quite important part because when you're doing distributed training and we're doing e-port per",
    "start": "699000",
    "end": "704700"
  },
  {
    "text": "shuffling you won't be able to make sure that your data is well randomly distributed so shuffling is an important",
    "start": "704700",
    "end": "710279"
  },
  {
    "text": "part of an operation and radiating asset was able to actually do that and the other thing the the efforts that started",
    "start": "710279",
    "end": "717360"
  },
  {
    "text": "in 2002 LED yesterday where we actually published of a first uh world record",
    "start": "717360",
    "end": "723480"
  },
  {
    "text": "that we actually set which was a Cloud store Benchmark that we actually achieved last record was actually set in 2016 and",
    "start": "723480",
    "end": "731160"
  },
  {
    "text": "I recall it quite vividly because I was at databricks at that time uh um and in 2016 we actually released this",
    "start": "731160",
    "end": "737820"
  },
  {
    "text": "particular Cloud mark and it actually shifted the perception of people it shifted the perception of how people",
    "start": "737820",
    "end": "743160"
  },
  {
    "text": "actually looked at spark at that time rather than looking at research project that emerged out of UC Berkeley from the",
    "start": "743160",
    "end": "749880"
  },
  {
    "text": "from from the amp Lab at that time ampstead for algorithm machines and people uh into into the real world where",
    "start": "749880",
    "end": "756660"
  },
  {
    "text": "people started seeing that you know spark is is here for us to actually use that scalable in production in use cases",
    "start": "756660",
    "end": "763560"
  },
  {
    "text": "and there was a milestone for for spark in 2016. fast forward you know 10 10 10",
    "start": "763560",
    "end": "768839"
  },
  {
    "text": "years ahead we set the record yesterday as being the most performant and most",
    "start": "768839",
    "end": "774060"
  },
  {
    "text": "cost effective way to actually use Ray and this again sets a milestone for us to to to to the people who are actually",
    "start": "774060",
    "end": "781019"
  },
  {
    "text": "using Ray serious use cases and production that Ray has is not only a research project that emerged out of",
    "start": "781019",
    "end": "786899"
  },
  {
    "text": "rice lab which was a successful a student implant it is not only an academic exercise for researchers but it",
    "start": "786899",
    "end": "793740"
  },
  {
    "text": "is actually a product that actually being used by industry and we'll see some of those people are actually using it another thing that emerged out of",
    "start": "793740",
    "end": "800880"
  },
  {
    "text": "cube Ray another thing in the image out of 2.0 was a cube Ray which is a toolkit for Ray to run on kubernetes and this is",
    "start": "800880",
    "end": "808320"
  },
  {
    "text": "in beta it's a replacement for the all python-based version of the operator I think one of the writers of the uh",
    "start": "808320",
    "end": "814980"
  },
  {
    "text": "contributors to Cuba Dimitri is on on call at the Meetup so thanks a lot Dimitri and the group that you actually",
    "start": "814980",
    "end": "821459"
  },
  {
    "text": "work with and the other bit was the erasers diploma graph apis now those of you who",
    "start": "821459",
    "end": "828240"
  },
  {
    "text": "actually have worked on deploying models you do realize that models in deployment",
    "start": "828240",
    "end": "834600"
  },
  {
    "text": "or more than the production don't exist in isolation what I mean by they don't exist in isolation is that models are",
    "start": "834600",
    "end": "841500"
  },
  {
    "text": "work together they add samples of model you might be doing some inference in in the input that actually comes in which",
    "start": "841500",
    "end": "848579"
  },
  {
    "text": "is going to be predicting something that might be used as an input for the second model so you might have a chain of model you might have an ensemble of models and",
    "start": "848579",
    "end": "855839"
  },
  {
    "text": "these deployment inference graphs allows you to create a graph of all these different models that you actually do",
    "start": "855839",
    "end": "861720"
  },
  {
    "text": "and they're going to be linked together and be used as an inference graph and that way you can actually deploy that as",
    "start": "861720",
    "end": "868139"
  },
  {
    "text": "a single unit of execution and each and individual models that you actually have can be scaled as replicas in the",
    "start": "868139",
    "end": "874980"
  },
  {
    "text": "particular graph so if certain part of the model that requires more traffic you can actually you can you can have more",
    "start": "874980",
    "end": "880680"
  },
  {
    "text": "CPUs you can have more gpus or you can actually have more resources dedicated to that so traffic can actually goes to",
    "start": "880680",
    "end": "886079"
  },
  {
    "text": "that so that has been sort of a huge Improvement in 2.0 and based on to that array air was actually built on top of",
    "start": "886079",
    "end": "892440"
  },
  {
    "text": "that to solidify that so if you want to try it out if install array default and",
    "start": "892440",
    "end": "897480"
  },
  {
    "text": "have a go with it the second bit that actually started that I think it's worth mentioning is is",
    "start": "897480",
    "end": "904860"
  },
  {
    "text": "it's one thing really we're ready to be able to easily make you write distributed",
    "start": "904860",
    "end": "911579"
  },
  {
    "text": "application it is a whole different ball game if you had to debug something right if you really don't know what's going on",
    "start": "911579",
    "end": "916740"
  },
  {
    "text": "if you have to scour to the blogs and you have to figure out you know what what is what the hell is going on why",
    "start": "916740",
    "end": "923220"
  },
  {
    "text": "why why my my resources are sluggish so we had to introduce this whole idea about Ray observing which is actually",
    "start": "923220",
    "end": "929459"
  },
  {
    "text": "starting in Ray 2.0 and it's continuing to to uh to progress in 2.1 and 2.2 and",
    "start": "929459",
    "end": "936060"
  },
  {
    "text": "last month we actually had our first part of the meter where we had people from the record team talk about all",
    "start": "936060",
    "end": "942120"
  },
  {
    "text": "these particular metrics and we have given people uh the ability through this very visual real-time metrics of all the",
    "start": "942120",
    "end": "949380"
  },
  {
    "text": "resources that your rate has are using that the array actors are using how much memory they're using CPU GPU hip memory",
    "start": "949380",
    "end": "956519"
  },
  {
    "text": "you got all this actually happening in real time so you can actually Zoom me into those particular metrics you can go into particular tasks you can go into",
    "start": "956519",
    "end": "962820"
  },
  {
    "text": "particular actor you can particularly go into a work part of the node is actually using memory and this has been a huge",
    "start": "962820",
    "end": "969480"
  },
  {
    "text": "huge Monumental Improvement in in the 2. series and today you're going to hear about how the um monitor that Alliance",
    "start": "969480",
    "end": "977279"
  },
  {
    "text": "is going to talk about which is part of this continual effort that we started in 2022 and we're continuing as part of our",
    "start": "977279",
    "end": "984660"
  },
  {
    "text": "principles to make sure the ray 2.0 is stable it's performance is Reliant number three",
    "start": "984660",
    "end": "992399"
  },
  {
    "text": "structured documentation now I'm going to repeat something over here I'm going",
    "start": "992399",
    "end": "997440"
  },
  {
    "text": "to declare my my tenant good code begets",
    "start": "997440",
    "end": "1002720"
  },
  {
    "text": "good documentation good code begets good documentation if",
    "start": "1002720",
    "end": "1007880"
  },
  {
    "text": "you agree with me put a thumbs up if you disagree with me put the thumbs down I'm okay but good",
    "start": "1007880",
    "end": "1013639"
  },
  {
    "text": "documentation always begets good good code orders begets good documentation those of you who actually have been",
    "start": "1013639",
    "end": "1020000"
  },
  {
    "text": "working on the open source projects will appreciate immensely if",
    "start": "1020000",
    "end": "1025520"
  },
  {
    "text": "your documentation was complete and comprehensive if your documentation was discoverable",
    "start": "1025520",
    "end": "1031339"
  },
  {
    "text": "in searchable you know you type in something and it gives you a list of things if your documentation was suffused or",
    "start": "1031339",
    "end": "1037640"
  },
  {
    "text": "had rosedly late with examples and code Snippets and and user guides and good",
    "start": "1037640",
    "end": "1043160"
  },
  {
    "text": "runnable code that you can actually take it in a notebook and then run it as an example and take some of the code and",
    "start": "1043160",
    "end": "1048679"
  },
  {
    "text": "use it or your use cases those are immensely valuable for you for you as a",
    "start": "1048679",
    "end": "1054080"
  },
  {
    "text": "programmer to get to get your productivity up and if the documentation is well organized it is well structured",
    "start": "1054080",
    "end": "1059299"
  },
  {
    "text": "there is a nice Toc it's easy to navigate and it's quite intuitive",
    "start": "1059299",
    "end": "1064460"
  },
  {
    "text": "and those are high standards those are high standards the metrics to actually achieve and one of the efforts that",
    "start": "1064460",
    "end": "1070820"
  },
  {
    "text": "started in 2.0 and it continues to do that is is to make sure that the documentation is consumable and just",
    "start": "1070820",
    "end": "1077600"
  },
  {
    "text": "like any good writing just like any other good code uh it's never complete",
    "start": "1077600",
    "end": "1082880"
  },
  {
    "text": "right documentation is never complete it doesn't stand on its own as a finished product there's always room for",
    "start": "1082880",
    "end": "1088580"
  },
  {
    "text": "improvement and we are steadfast in our belief in another commitment in our conviction to make sure that this",
    "start": "1088580",
    "end": "1094940"
  },
  {
    "text": "continues in our efforts and if you look at some of the documentation that has improved since the early releases you",
    "start": "1094940",
    "end": "1101059"
  },
  {
    "text": "got years ago a gallery of user guides that actually attack each and every different workload that tell you how to",
    "start": "1101059",
    "end": "1106640"
  },
  {
    "text": "actually use it both for each and every component we have user guides we have examples the code Snippets are replete",
    "start": "1106640",
    "end": "1113419"
  },
  {
    "text": "with the apis are completely with runnable code examples that actually explain and illustrate how you can",
    "start": "1113419",
    "end": "1119960"
  },
  {
    "text": "actually use that particular API so that's that's that's the the third big",
    "start": "1119960",
    "end": "1126320"
  },
  {
    "text": "now whenever you reflect back uh some of the times that Ray actually has gone",
    "start": "1126320",
    "end": "1131780"
  },
  {
    "text": "through you tend to look at early days with a soft sentiment of nostalgia and",
    "start": "1131780",
    "end": "1136880"
  },
  {
    "text": "here we got Robert and Philip the original creators of Ray and co-founders of any scale giving their first",
    "start": "1136880",
    "end": "1143240"
  },
  {
    "text": "presentation at the AI conference I think this was around 2000 even late 2017 with Circa early 2018 and on the",
    "start": "1143240",
    "end": "1150679"
  },
  {
    "text": "right you got Richard Lou Who was one of the first contributors uh talking about Ray tune and RL lib we didn't have any",
    "start": "1150679",
    "end": "1156679"
  },
  {
    "text": "other other libraries that was one of the first few libraries that Rhea started with and so we have come a long",
    "start": "1156679",
    "end": "1162320"
  },
  {
    "text": "way you know we have come a long way from the early days of Rey to today and it's quite heartening actually to see",
    "start": "1162320",
    "end": "1168620"
  },
  {
    "text": "how the community has actually gone at the Fort and the fifth beat is the red community and the rare Summit and we",
    "start": "1168620",
    "end": "1174260"
  },
  {
    "text": "have seen meetups in Far Corners of the world from Madeleine in Colombia to all",
    "start": "1174260",
    "end": "1179299"
  },
  {
    "text": "the way in Toronto I was in Toronto the envelopes conference doing a tutorial on Race serving in Ray and there were three",
    "start": "1179299",
    "end": "1185720"
  },
  {
    "text": "other talks on three which are completely not from any skill and I was quite hardening one of them was was from",
    "start": "1185720",
    "end": "1190760"
  },
  {
    "text": "Shopify Isaac who actually gave a talk about how they actually use Merlin um and and I was in New York uh at the",
    "start": "1190760",
    "end": "1198440"
  },
  {
    "text": "Pioneer doing a tutorial and there were six different talks from Ray and not for many scale these were from the community",
    "start": "1198440",
    "end": "1204020"
  },
  {
    "text": "and it was quite heartening actually to go to a political conference and and realizing that the other people talking",
    "start": "1204020",
    "end": "1210320"
  },
  {
    "text": "about Ray and that really is a testament that Ray is actually really really taking off in in various ways",
    "start": "1210320",
    "end": "1217700"
  },
  {
    "text": "and here is here is some of the companies who are actually using Ray at a massive scale right and all these",
    "start": "1217700",
    "end": "1224299"
  },
  {
    "text": "companies actually presented at the race Summit how their use cases are actually going to use what scale",
    "start": "1224299",
    "end": "1230240"
  },
  {
    "text": "they're actually using what part of Ray are they actually using and some of the stats that you actually have at the bottom are really vanity stats I agree",
    "start": "1230240",
    "end": "1237740"
  },
  {
    "text": "you can actually make a case of that but I think those are important signals I would 10 these are important signals for",
    "start": "1237740",
    "end": "1244940"
  },
  {
    "text": "you to understand how popular your your source is how many gitup styles you actually have but important thing is how",
    "start": "1244940",
    "end": "1251299"
  },
  {
    "text": "many contributors you actually have which is really a good indicator of the adoption of Ray we have about 770",
    "start": "1251299",
    "end": "1257240"
  },
  {
    "text": "contributors who have started just in the last three or four years five thousand different reported raises depend on where they have include",
    "start": "1257240",
    "end": "1263240"
  },
  {
    "text": "statements in Ray that allowed that over a thousand plus organizations are using gray All These Little Things actually",
    "start": "1263240",
    "end": "1269600"
  },
  {
    "text": "add up the power of small things is enormous when you actually looked at them in in collectively",
    "start": "1269600",
    "end": "1275840"
  },
  {
    "text": "um and if I were to compare those in terms of the activities actually happening on GitHub let's say compared to tasks we actually see the ray is is",
    "start": "1275840",
    "end": "1282500"
  },
  {
    "text": "is really really taking off in its in its popularity and if I were to compare that with the",
    "start": "1282500",
    "end": "1289039"
  },
  {
    "text": "common distributed system we actually see today in Array kind of matches them",
    "start": "1289039",
    "end": "1294080"
  },
  {
    "text": "or or it it also um sort of beginning to to approach them",
    "start": "1294080",
    "end": "1299720"
  },
  {
    "text": "uh in in in terms of how Ray is being popular so these are actually good indications that in the last couple of",
    "start": "1299720",
    "end": "1306260"
  },
  {
    "text": "years starting from uh 2022 we're beginning to see more emotions of of Rey",
    "start": "1306260",
    "end": "1311840"
  },
  {
    "text": "and the community is an important part you guys play a huge role in in",
    "start": "1311840",
    "end": "1317120"
  },
  {
    "text": "advocating Rey and being part of the community and I think communities growing communities create values and",
    "start": "1317120",
    "end": "1323659"
  },
  {
    "text": "what I mean by value is that they create assets they create blogs they talk at conferences they share the vision these",
    "start": "1323659",
    "end": "1330860"
  },
  {
    "text": "are important part of how Community actually creates value these two books were actually written in 2022 and",
    "start": "1330860",
    "end": "1336620"
  },
  {
    "text": "they've submitted for for publication or one from the community is a big advocate of open source project",
    "start": "1336620",
    "end": "1343340"
  },
  {
    "text": "some of you actually might know her she's Canadian and uh learning raise another book that was written by the ray",
    "start": "1343340",
    "end": "1349520"
  },
  {
    "text": "team which is now available I think we're having it might be available in a couple of months but the scaling with",
    "start": "1349520",
    "end": "1356419"
  },
  {
    "text": "python is available today through pre-order so these are the assets that the community actually creates and look",
    "start": "1356419",
    "end": "1361640"
  },
  {
    "text": "at the blogs that people are actually writing and last but not least Ray Summit I don't know how many of you",
    "start": "1361640",
    "end": "1367880"
  },
  {
    "text": "attended if you ended up with your thumbs up you know it was a huge you bloody success I mean you know we had",
    "start": "1367880",
    "end": "1374000"
  },
  {
    "text": "exceeded our expectations you know it exceeded not only the the caliber of the Keynotes but also the sessions and the",
    "start": "1374000",
    "end": "1381140"
  },
  {
    "text": "training the the training session that we actually had but the energy the camaraderie you know the the that comes",
    "start": "1381140",
    "end": "1388520"
  },
  {
    "text": "with when you actually go to an open source conference you know you meet people from around the globe that you've interact with them with the slack you",
    "start": "1388520",
    "end": "1394520"
  },
  {
    "text": "meet people and have a chat with them that you have communicated that when on on the social media you meet with the",
    "start": "1394520",
    "end": "1399919"
  },
  {
    "text": "people with whom you're actually either worked on the pr and have a pint you know and then have a laugh with them and",
    "start": "1399919",
    "end": "1406039"
  },
  {
    "text": "these are Priceless moments at any open source Gathering that you actually have and we're planning to actually repeat",
    "start": "1406039",
    "end": "1411440"
  },
  {
    "text": "that in 2023 coming up in a few months so hope to actually see you there so",
    "start": "1411440",
    "end": "1416780"
  },
  {
    "text": "enough of this recounting and enough sort of nostalgia I'm gonna now hand it over to Clarence",
    "start": "1416780",
    "end": "1422900"
  },
  {
    "text": "um to so start of our first talk so Clarence",
    "start": "1422900",
    "end": "1429440"
  },
  {
    "text": "um go for it balances is a software engineer with the ray team specializing",
    "start": "1429440",
    "end": "1434539"
  },
  {
    "text": "on Ray observatively and and race stability and core and he's going to talk about uh the the second bit of our",
    "start": "1434539",
    "end": "1441980"
  },
  {
    "text": "record which is the uh um monitor so go ahead Clarence",
    "start": "1441980",
    "end": "1447320"
  },
  {
    "text": "yeah thanks uh Jules for uh the introduction uh so maybe just give me a second I'll try to sure sure",
    "start": "1447320",
    "end": "1455539"
  },
  {
    "text": "foreign yeah thanks for uh the patience and um",
    "start": "1455539",
    "end": "1462140"
  },
  {
    "text": "again I'm Clarence I'm a software engineer I'm at any scale working on record",
    "start": "1462140",
    "end": "1467780"
  },
  {
    "text": "um as you probably know record is the part of Ray that provides the basic",
    "start": "1467780",
    "end": "1472880"
  },
  {
    "text": "Primitives like tasks actors and objects today I wanted to share the recent work",
    "start": "1472880",
    "end": "1480020"
  },
  {
    "text": "on the ray memory monitor or aka the unkiller so it monitors the memory usage",
    "start": "1480020",
    "end": "1487400"
  },
  {
    "text": "on the Node and when the node usage exceeds a certain threshold",
    "start": "1487400",
    "end": "1492500"
  },
  {
    "text": "it'll try to free up memory by killing a task so this feature is already enabled by",
    "start": "1492500",
    "end": "1498260"
  },
  {
    "text": "default in the latest Ray release which is Ray 2.2 plus and it is uh built to",
    "start": "1498260",
    "end": "1505880"
  },
  {
    "text": "improve the stability of a grade um so just to very quickly go over the",
    "start": "1505880",
    "end": "1512600"
  },
  {
    "text": "agenda so we'll first give a very brief introduction to the memory issues that",
    "start": "1512600",
    "end": "1517760"
  },
  {
    "text": "someone may encounter when they're using rate and then why the existing solution does",
    "start": "1517760",
    "end": "1523400"
  },
  {
    "text": "not work well um and how the rate memory monitor might help",
    "start": "1523400",
    "end": "1529100"
  },
  {
    "text": "after that we will go give an overview of how the memory monitor works we will",
    "start": "1529100",
    "end": "1535580"
  },
  {
    "text": "Deep dive into the task preemption policy that is used to decide what tasks to",
    "start": "1535580",
    "end": "1542659"
  },
  {
    "text": "kill in order to obtain memory finally we will also walk through some",
    "start": "1542659",
    "end": "1547820"
  },
  {
    "text": "demos to see the memory monitoring action",
    "start": "1547820",
    "end": "1553299"
  },
  {
    "text": "so one of the key values of gray is that it can help us advertis lead scale our",
    "start": "1554059",
    "end": "1559220"
  },
  {
    "text": "workplace so here we have a workload that let's say we need to run a bunch of tasks uh",
    "start": "1559220",
    "end": "1566720"
  },
  {
    "text": "say we want to process a bunch of images where each task processes a subset of",
    "start": "1566720",
    "end": "1572419"
  },
  {
    "text": "those images and we want to do it you know obviously you know as quick as possible in the",
    "start": "1572419",
    "end": "1578179"
  },
  {
    "text": "most efficient as possible so in this example right we use Ray to distribute a bunch of tasks across a",
    "start": "1578179",
    "end": "1585740"
  },
  {
    "text": "cluster of three nodes where each node has four CPU",
    "start": "1585740",
    "end": "1591799"
  },
  {
    "text": "and in this example each task is requesting one CPU",
    "start": "1591799",
    "end": "1597700"
  },
  {
    "text": "So based on that we can see that each node would have four tasks running",
    "start": "1598520",
    "end": "1605659"
  },
  {
    "text": "so but what if the four tasks combined you know consumes too much memory that it causes the node to run out of memory",
    "start": "1605659",
    "end": "1613520"
  },
  {
    "text": "so when the node runs over memory which in this case includes a swap space",
    "start": "1613520",
    "end": "1618980"
  },
  {
    "text": "so on Linux this will trigger the um the OS open killer which will pick a process",
    "start": "1618980",
    "end": "1625520"
  },
  {
    "text": "and to kill and free up the memory so that um it'll give some room to the kernel in",
    "start": "1625520",
    "end": "1632000"
  },
  {
    "text": "other processes that are running",
    "start": "1632000",
    "end": "1635440"
  },
  {
    "text": "so so now back to um you know the second part right so if there's already a umkiller you know in",
    "start": "1638360",
    "end": "1644960"
  },
  {
    "text": "the operating system why do we still need to build one inside Ray you know at that application Level so before we",
    "start": "1644960",
    "end": "1652100"
  },
  {
    "text": "answer that to recap right the OS um killer triggers when the host or the node runs",
    "start": "1652100",
    "end": "1659539"
  },
  {
    "text": "out of free memory pages and when that happens it'll prioritize",
    "start": "1659539",
    "end": "1664760"
  },
  {
    "text": "killing uh memory hungry processes first uh the frequency of that also have that",
    "start": "1664760",
    "end": "1672559"
  },
  {
    "text": "happens is also Limited meaning that it is still possible for a node to be in a",
    "start": "1672559",
    "end": "1678020"
  },
  {
    "text": "state where it is running out of memory and a bunch of processes are stalled",
    "start": "1678020",
    "end": "1683419"
  },
  {
    "text": "waiting for memory to be allocated so what we've seen in the past is that",
    "start": "1683419",
    "end": "1689240"
  },
  {
    "text": "when this happens the no and array itself freezes and it becomes unresponsive",
    "start": "1689240",
    "end": "1695840"
  },
  {
    "text": "so in addition to uh prioritizing memory hungry processes right the OS umkiller",
    "start": "1695840",
    "end": "1701679"
  },
  {
    "text": "calculates a score for each process and uses that to determine uh which process",
    "start": "1701679",
    "end": "1708200"
  },
  {
    "text": "to kill there are configurations that allow certain processes to be marked as higher",
    "start": "1708200",
    "end": "1713419"
  },
  {
    "text": "priority so that it is less likely to be killed when the umkiller triggers",
    "start": "1713419",
    "end": "1720799"
  },
  {
    "text": "so rate by default configures its processes so that the tasks and actors",
    "start": "1720799",
    "end": "1725840"
  },
  {
    "text": "are more likely to be killed before it goes and kills other critical",
    "start": "1725840",
    "end": "1731299"
  },
  {
    "text": "processes like the raila GCS the agents dashboards things like that",
    "start": "1731299",
    "end": "1738220"
  },
  {
    "text": "so there are a couple of issues as we uh will go into with why the osm Killer is",
    "start": "1739659",
    "end": "1747380"
  },
  {
    "text": "not sufficient as stated before it is possible that the node can stall right when the tasks are",
    "start": "1747380",
    "end": "1754360"
  },
  {
    "text": "very aggressive at consuming memory",
    "start": "1754360",
    "end": "1759340"
  },
  {
    "text": "and when Ray stalls on the Node it becomes unresponsive what we have seen is that the node can",
    "start": "1759799",
    "end": "1766880"
  },
  {
    "text": "sometimes solve for over half a minute and that would trigger the health check",
    "start": "1766880",
    "end": "1771919"
  },
  {
    "text": "to fail so when Ray fails a health check the node it assumes that it's dead and",
    "start": "1771919",
    "end": "1777200"
  },
  {
    "text": "then it will remove it from the cluster another situation that could happen is",
    "start": "1777200",
    "end": "1782299"
  },
  {
    "text": "that other critical processes like the ray dashboard agents it installed um it'll won't be functioning",
    "start": "1782299",
    "end": "1791080"
  },
  {
    "text": "so here we have a screenshot of a workload that is running uh using the",
    "start": "1791539",
    "end": "1799520"
  },
  {
    "text": "osm Killer um and these are diagrams of the ray",
    "start": "1799520",
    "end": "1805220"
  },
  {
    "text": "dashboard so this is showing well that's running on a single node cluster that's having",
    "start": "1805220",
    "end": "1810860"
  },
  {
    "text": "memory issues so on the right diagram the green uh the green diagram it shows that the memory",
    "start": "1810860",
    "end": "1818899"
  },
  {
    "text": "usage of the node and that the dashed line is indicating the the max node",
    "start": "1818899",
    "end": "1824299"
  },
  {
    "text": "memory what we're seeing here is that the usage is running very close to the Limit and it's very likely that it is",
    "start": "1824299",
    "end": "1830960"
  },
  {
    "text": "running out of memory or booming and on the left we can see the number of",
    "start": "1830960",
    "end": "1836000"
  },
  {
    "text": "tasks that are running in the cluster broken back down by the states what we",
    "start": "1836000",
    "end": "1841880"
  },
  {
    "text": "see here are gaps in the metrics all right that is being rendered by the array dashboard",
    "start": "1841880",
    "end": "1847460"
  },
  {
    "text": "so when the node is running out of memory the processes stall and in this case the metrics collection is failing",
    "start": "1847460",
    "end": "1854539"
  },
  {
    "text": "to record the metrics and therefore it produces these gaps when the dashboard",
    "start": "1854539",
    "end": "1860480"
  },
  {
    "text": "tries to render and one last thing to note in the",
    "start": "1860480",
    "end": "1865580"
  },
  {
    "text": "diagram to the left is that the the number of tasks that are running it stays relatively fast",
    "start": "1865580",
    "end": "1872419"
  },
  {
    "text": "so now we're going to look at the same dashboard and for the same workload but",
    "start": "1872419",
    "end": "1877460"
  },
  {
    "text": "after we turn on the ray memory monitor what we're seeing here is that there are",
    "start": "1877460",
    "end": "1883940"
  },
  {
    "text": "hardly any gaps and that there's very little to no stalling to on the Node",
    "start": "1883940",
    "end": "1891260"
  },
  {
    "text": "we also can see that there's also a peak in the number of tasks that are running which shows that the node is actually",
    "start": "1891260",
    "end": "1898039"
  },
  {
    "text": "being responsive and the tasks are being scheduled",
    "start": "1898039",
    "end": "1903220"
  },
  {
    "text": "so another problem with relying on the boom killer on the the operating system",
    "start": "1904640",
    "end": "1910399"
  },
  {
    "text": "is that it is possible for tasks to be refreshing so let's recall that the osm",
    "start": "1910399",
    "end": "1917960"
  },
  {
    "text": "killer right it prioritizes killing processes that use the most memory",
    "start": "1917960",
    "end": "1923120"
  },
  {
    "text": "so to illustrate fraction here we have an example with uh two identical tasks",
    "start": "1923120",
    "end": "1929360"
  },
  {
    "text": "that needs to run uh where we assume the memory usage of the task is uh",
    "start": "1929360",
    "end": "1935840"
  },
  {
    "text": "increasing monotonically so the left task here it got scheduled",
    "start": "1935840",
    "end": "1941899"
  },
  {
    "text": "first and it's running at 90 of its intended memory usage it's almost about",
    "start": "1941899",
    "end": "1949220"
  },
  {
    "text": "to finish whereas the the right task here it just got scheduled started running and it's",
    "start": "1949220",
    "end": "1955279"
  },
  {
    "text": "only using around about 30 of what it needs to use so let's assume at this time the node",
    "start": "1955279",
    "end": "1961880"
  },
  {
    "text": "runs sort of memory the OS in killer kickstand is more",
    "start": "1961880",
    "end": "1967039"
  },
  {
    "text": "likely that that has on the left that got scheduled first and is about to finish is the one that is being Chosen",
    "start": "1967039",
    "end": "1974179"
  },
  {
    "text": "and showed by uh the boom killer",
    "start": "1974179",
    "end": "1978398"
  },
  {
    "text": "so let's say we kill the the task that was about to finish",
    "start": "1980419",
    "end": "1985940"
  },
  {
    "text": "so now that the task is killed right Ray will detect that the process was gone",
    "start": "1985940",
    "end": "1991520"
  },
  {
    "text": "and it'll try to reschedule and rerun that same task",
    "start": "1991520",
    "end": "1997059"
  },
  {
    "text": "so now let's say that um the task that was not killed before it keeps making",
    "start": "1997220",
    "end": "2003399"
  },
  {
    "text": "progress It's almost done now it's using 90 of its memory but at this moment it is possible that",
    "start": "2003399",
    "end": "2010960"
  },
  {
    "text": "the right the node is running out of memory again and it just might repeat what just",
    "start": "2010960",
    "end": "2017140"
  },
  {
    "text": "happened which is it will try to kill the tasks that ran first that was about to finish",
    "start": "2017140",
    "end": "2024640"
  },
  {
    "text": "so in short with the osm killer it is possible for a passive thrash where it",
    "start": "2024640",
    "end": "2030880"
  },
  {
    "text": "is more likely to kill um tasks that are almost finished but ideally what we want to do is to",
    "start": "2030880",
    "end": "2038140"
  },
  {
    "text": "minimize uh wasted work right in this case perhaps it should kill the task",
    "start": "2038140",
    "end": "2044380"
  },
  {
    "text": "that was that's newer that has made less progress",
    "start": "2044380",
    "end": "2049380"
  },
  {
    "text": "um another disadvantage of uh relying on the osm Killer is that there's a lack of",
    "start": "2050379",
    "end": "2055419"
  },
  {
    "text": "visibility when something is going wrong so for example here we have a single task",
    "start": "2055419",
    "end": "2061780"
  },
  {
    "text": "that is leaking memory it's causing the node right to run out of memory it'll be then it'll be killed",
    "start": "2061780",
    "end": "2068138"
  },
  {
    "text": "by the Osborne killer um so in this case let's say that has is configured to just you know retry",
    "start": "2068139",
    "end": "2075760"
  },
  {
    "text": "forever and what will happen is that the task because it's running out of memory",
    "start": "2075760",
    "end": "2081158"
  },
  {
    "text": "always it'll just keep retrying and failing and it appear to be hanging",
    "start": "2081159",
    "end": "2086320"
  },
  {
    "text": "right from the uh from the user's perspective and someone at that point would then",
    "start": "2086320",
    "end": "2092020"
  },
  {
    "text": "have to go and debug why the task is hanging right why is it keep it failing forever",
    "start": "2092020",
    "end": "2097480"
  },
  {
    "text": "so ideally Ray should be able to detect this condition and fail and produce some",
    "start": "2097480",
    "end": "2104200"
  },
  {
    "text": "useful message uh to indicate that there's something going wrong",
    "start": "2104200",
    "end": "2110700"
  },
  {
    "text": "um so to quickly summarize right the osm killer doesn't work really well",
    "start": "2112480",
    "end": "2117820"
  },
  {
    "text": "um because the nose could stall um task of refreshing",
    "start": "2117820",
    "end": "2123099"
  },
  {
    "text": "or just hang so here uh we will briefly go over how",
    "start": "2123099",
    "end": "2130180"
  },
  {
    "text": "the uh the ray memory monitor works and how it fits into the raid architecture",
    "start": "2130180",
    "end": "2138820"
  },
  {
    "text": "uh the memory monitor is part of the Rayleigh which manages a bunch of tasks or actors each belonging to its process",
    "start": "2138820",
    "end": "2148599"
  },
  {
    "text": "the process publishes stats like the memory usage to the operating system",
    "start": "2148599",
    "end": "2154599"
  },
  {
    "text": "and the memory monitor will periodically pull the memory usage from the operating system",
    "start": "2154599",
    "end": "2160420"
  },
  {
    "text": "to determine the memory utilization on the Node which would include",
    "start": "2160420",
    "end": "2166560"
  },
  {
    "text": "non-ray processes the ray Object Store and also obviously the Heap usage of the",
    "start": "2166560",
    "end": "2174339"
  },
  {
    "text": "tasking actors so let's say the memory mods for uh",
    "start": "2174339",
    "end": "2182400"
  },
  {
    "text": "reads the memory usage if the combined memory usage is above",
    "start": "2182400",
    "end": "2188079"
  },
  {
    "text": "some threshold let's say 95 then it will then look for a task or actor to kill",
    "start": "2188079",
    "end": "2196980"
  },
  {
    "text": "so here the memory monitor applies some kind of policy to determine what or",
    "start": "2197140",
    "end": "2202180"
  },
  {
    "text": "which task to kill and then it goes and kill it so after it is killed the the same",
    "start": "2202180",
    "end": "2209020"
  },
  {
    "text": "process repeats if it detects that the memory usage is still above the threshold after killing the previous one",
    "start": "2209020",
    "end": "2216280"
  },
  {
    "text": "uh it will trigger and kill the second one so when the note runs out of memory and",
    "start": "2216280",
    "end": "2222099"
  },
  {
    "text": "Rey needs to kill a task we apply the preemption policy and so",
    "start": "2222099",
    "end": "2227800"
  },
  {
    "text": "now we're talking about what policy does Ray use to choose the task to kill when",
    "start": "2227800",
    "end": "2235720"
  },
  {
    "text": "the node is running out of memory so we think that the ideal policy is one",
    "start": "2235720",
    "end": "2242200"
  },
  {
    "text": "where uh it would fail if the workload cannot complete and notify and provide some useful information on how it could",
    "start": "2242200",
    "end": "2248920"
  },
  {
    "text": "be fixed to simplify debugging we also expect parallel tasks to run in",
    "start": "2248920",
    "end": "2255520"
  },
  {
    "text": "some degraded mode when the node is running out of memory it may run slightly slower than you know",
    "start": "2255520",
    "end": "2262180"
  },
  {
    "text": "if the tasks were to run steerly uh but the world should still complete",
    "start": "2262180",
    "end": "2267640"
  },
  {
    "text": "without failing and it shouldn't be stuck or hanging",
    "start": "2267640",
    "end": "2272760"
  },
  {
    "text": "so in the latest release of Rey which is Ray 2.2 the The Raid memory monitor is",
    "start": "2274839",
    "end": "2281740"
  },
  {
    "text": "enabled by default uh and the preemption policy that is is using to is",
    "start": "2281740",
    "end": "2288640"
  },
  {
    "text": "to find a task to kill is um fairly straightforward so it'll prefer",
    "start": "2288640",
    "end": "2294160"
  },
  {
    "text": "killing tasks a task that is reach viable um because we if the application specify",
    "start": "2294160",
    "end": "2301240"
  },
  {
    "text": "the task is retrievable then the system would know that it is safe to retry this task",
    "start": "2301240",
    "end": "2307119"
  },
  {
    "text": "so we will prefer killing a task that as we travel and within those tasks that every tribal",
    "start": "2307119",
    "end": "2314079"
  },
  {
    "text": "it will find the newest executed task to minimize flashing",
    "start": "2314079",
    "end": "2321299"
  },
  {
    "text": "so in Ray write a task by default um is we tried up to three times",
    "start": "2322420",
    "end": "2329680"
  },
  {
    "text": "save it crashes due to an error in a system so when a task is set to be retrievable",
    "start": "2329680",
    "end": "2336220"
  },
  {
    "text": "the memory monitor will prioritize on killing it as it knows it could be safe",
    "start": "2336220",
    "end": "2341380"
  },
  {
    "text": "to be re-executed later and by default an actor is not",
    "start": "2341380",
    "end": "2346420"
  },
  {
    "text": "retrievable and therefore by default the memory monitor will prioritize killing tasks overactive",
    "start": "2346420",
    "end": "2354359"
  },
  {
    "text": "and then the memory monitor will then sort these tasks by its execution start",
    "start": "2357579",
    "end": "2363400"
  },
  {
    "text": "time and then it picks the last one that started executing",
    "start": "2363400",
    "end": "2368440"
  },
  {
    "text": "as we said before it'll help minimize wasted work",
    "start": "2368440",
    "end": "2375540"
  },
  {
    "text": "so to illustrate uh this policy in action um we'll use a real workload in this",
    "start": "2377040",
    "end": "2383980"
  },
  {
    "text": "case uh hyper parameter tuning um here we run two trials in parallel",
    "start": "2383980",
    "end": "2391900"
  },
  {
    "text": "um to tune a let's say a machine learning model using Ray air and to",
    "start": "2391900",
    "end": "2397720"
  },
  {
    "text": "recap right air stands for AI runtime which is the rate toolkit for ML",
    "start": "2397720",
    "end": "2402940"
  },
  {
    "text": "applications each trial here is training the same model but on different parameters",
    "start": "2402940",
    "end": "2409420"
  },
  {
    "text": "and Ray will help us schedule this onto a single node cluster so in this diagram the driver script",
    "start": "2409420",
    "end": "2416200"
  },
  {
    "text": "first creates the the trial actor which is then used to coordinate the execution",
    "start": "2416200",
    "end": "2422079"
  },
  {
    "text": "of the entire trial the trial actor then spawns a bunch of data tasks that will pre-process the",
    "start": "2422079",
    "end": "2429339"
  },
  {
    "text": "data where the result solves are then stored into the ray Object Store",
    "start": "2429339",
    "end": "2436420"
  },
  {
    "text": "the the trial actor also spawns the trainer or the training actor which",
    "start": "2436420",
    "end": "2441820"
  },
  {
    "text": "reads the data that was produced by the data tasks in order to train the model",
    "start": "2441820",
    "end": "2449220"
  },
  {
    "text": "so now let's say we are running this workload on a single node and the node is running out of memory",
    "start": "2449220",
    "end": "2455980"
  },
  {
    "text": "according to the preemption policy right we'll first prioritize killing a task so",
    "start": "2455980",
    "end": "2461140"
  },
  {
    "text": "the data tasks that are pre-processing data are highlighted as one of them that will be killed here",
    "start": "2461140",
    "end": "2468359"
  },
  {
    "text": "among the data tasks we didn't pick the one that was last executed so let's say the one to the right was the last one",
    "start": "2468579",
    "end": "2475540"
  },
  {
    "text": "um then the policy will go and kill that so one of the problem uh with this",
    "start": "2475540",
    "end": "2481359"
  },
  {
    "text": "policy is that it may lead to starvation so it is possible that a trial can get",
    "start": "2481359",
    "end": "2487780"
  },
  {
    "text": "stuck where all of the data tasks are preempted in favor of a different trial",
    "start": "2487780",
    "end": "2493599"
  },
  {
    "text": "as shown in this diagram the the trial on the right side has no data",
    "start": "2493599",
    "end": "2500520"
  },
  {
    "text": "pre-processing task to make progress foreign ideally we would keep some tasks in each",
    "start": "2500520",
    "end": "2509260"
  },
  {
    "text": "execution Branch to avoid this situation",
    "start": "2509260",
    "end": "2513900"
  },
  {
    "text": "another problem with this policy is that it cannot detect when a task is hanging",
    "start": "2514540",
    "end": "2520720"
  },
  {
    "text": "because it is using too much memory so if the task is set here to be you know",
    "start": "2520720",
    "end": "2526300"
  },
  {
    "text": "retrying forever it will appear as if the task is hanging and it never completes because it just keeps running",
    "start": "2526300",
    "end": "2531940"
  },
  {
    "text": "out of memory and then it just retries again um so in rate 2.3 the upcoming release",
    "start": "2531940",
    "end": "2539320"
  },
  {
    "text": "we are making some improvements to how um the task is being selected",
    "start": "2539320",
    "end": "2546160"
  },
  {
    "text": "uh the key change is that now we will group the task from the same parent",
    "start": "2546160",
    "end": "2551200"
  },
  {
    "text": "together which will illustrate um and then try to kill uh the the",
    "start": "2551200",
    "end": "2557500"
  },
  {
    "text": "largest group to ensure liveness",
    "start": "2557500",
    "end": "2561960"
  },
  {
    "text": "so let's go back to the same workload that we do hyper parameter tuning but",
    "start": "2563800",
    "end": "2569500"
  },
  {
    "text": "using the new policy uh like before right the actors are last to be killed",
    "start": "2569500",
    "end": "2574660"
  },
  {
    "text": "um so we form it's a group that is only for actors that are highlighted here in purple",
    "start": "2574660",
    "end": "2583119"
  },
  {
    "text": "and then we also form a group uh for each parent so from this example we can",
    "start": "2583119",
    "end": "2589420"
  },
  {
    "text": "see that um there's a group on the left that contains the data tasks",
    "start": "2589420",
    "end": "2595240"
  },
  {
    "text": "for the left trial and then the same thing there's another group of data",
    "start": "2595240",
    "end": "2601000"
  },
  {
    "text": "tasks for the right trial So based on the new policy it creates",
    "start": "2601000",
    "end": "2606460"
  },
  {
    "text": "three different groups",
    "start": "2606460",
    "end": "2610859"
  },
  {
    "text": "so when now when the node is running out of memory it'll pick um the largest group so in this case",
    "start": "2612040",
    "end": "2619540"
  },
  {
    "text": "it'll just pick the left group and it kills one that has to",
    "start": "2619540",
    "end": "2626500"
  },
  {
    "text": "so let's assume the node is still running out of memory then the process repeats it takes the the group",
    "start": "2626500",
    "end": "2633339"
  },
  {
    "text": "the largest group so in this case the data group on the right is larger than the beta group on the",
    "start": "2633339",
    "end": "2639880"
  },
  {
    "text": "left so it kills a task on the right group so now you have two uh in each of",
    "start": "2639880",
    "end": "2645700"
  },
  {
    "text": "the data groups so let's say the process keeps continuing if the node is still running",
    "start": "2645700",
    "end": "2651579"
  },
  {
    "text": "a little memory after a certain point there's only one task left",
    "start": "2651579",
    "end": "2657280"
  },
  {
    "text": "for each of the trials",
    "start": "2657280",
    "end": "2661740"
  },
  {
    "text": "so now let's say the node is still running out of memory right then well it",
    "start": "2662500",
    "end": "2668500"
  },
  {
    "text": "will have to kill uh in this case it will decide to kill a data task in the left group",
    "start": "2668500",
    "end": "2676180"
  },
  {
    "text": "and the preemption policy right the new one in 2.3 it states that it'll throw an",
    "start": "2676180",
    "end": "2681339"
  },
  {
    "text": "error if it kills the last task of a group so in this case",
    "start": "2681339",
    "end": "2687579"
  },
  {
    "text": "it kills the last data task of the left trial and that and what will happen is that it",
    "start": "2687579",
    "end": "2693819"
  },
  {
    "text": "would then throw an error and provide some message indicating that the work will run out of memory and it",
    "start": "2693819",
    "end": "2701680"
  },
  {
    "text": "also provides some more information on how to fix the problem foreign",
    "start": "2701680",
    "end": "2707280"
  },
  {
    "text": "when we talk about resolving or fixing the problem is is to either you know increase the memory capacity in the node",
    "start": "2707280",
    "end": "2713859"
  },
  {
    "text": "add more nodes or just reduce the parallelism so that there are fewer trials running in parallel",
    "start": "2713859",
    "end": "2721318"
  },
  {
    "text": "so going back to um the previous example where you know there's a task that's running in met leaking memory and and",
    "start": "2722200",
    "end": "2731140"
  },
  {
    "text": "running forever because of the infinite retry and hanging so with the of the new policy it'll see",
    "start": "2731140",
    "end": "2738579"
  },
  {
    "text": "that this is the last task for the group and it'll just feel immediately instead",
    "start": "2738579",
    "end": "2745599"
  },
  {
    "text": "of hanging um so now I'll quickly go into uh the",
    "start": "2745599",
    "end": "2754300"
  },
  {
    "text": "the demo where we show um the memory monitor that is running",
    "start": "2754300",
    "end": "2762000"
  },
  {
    "text": "um so the first thing I'll show here is",
    "start": "2763780",
    "end": "2768880"
  },
  {
    "text": "um the the second example we just talked about which is we have a single",
    "start": "2768880",
    "end": "2774880"
  },
  {
    "text": "task um that is leaking memory um but is uh trying to uh retrying you",
    "start": "2774880",
    "end": "2782680"
  },
  {
    "text": "know infinitely and hanging so here we set we can see that",
    "start": "2782680",
    "end": "2787960"
  },
  {
    "text": "um uh it sets the uh the retry of the task to be a",
    "start": "2787960",
    "end": "2793839"
  },
  {
    "text": "negative one which means that this task will just keep retrying forever um if it's scratching",
    "start": "2793839",
    "end": "2801400"
  },
  {
    "text": "and um so what we see here is that now I just run the script so it'll take some",
    "start": "2801400",
    "end": "2807220"
  },
  {
    "text": "time to run um we can come back to this afterwards",
    "start": "2807220",
    "end": "2812380"
  },
  {
    "text": "and then the other uh demo uh what was going to show is that we will have um",
    "start": "2812380",
    "end": "2819460"
  },
  {
    "text": "the hyper parameter tuning uh workload um so here I have actually two",
    "start": "2819460",
    "end": "2826359"
  },
  {
    "text": "um two more uh demos environments",
    "start": "2826359",
    "end": "2832300"
  },
  {
    "text": "um here I'm actually using the any scale workspace which uh which gives me the",
    "start": "2832300",
    "end": "2838780"
  },
  {
    "text": "IDE direct access to the cluster and so um here I just what I did is I",
    "start": "2838780",
    "end": "2845440"
  },
  {
    "text": "created an environment and I cloned it um so create an identical environment so",
    "start": "2845440",
    "end": "2851560"
  },
  {
    "text": "that we can just do like an A B comparison so the the two things that we're comparing here is uh uh hyper",
    "start": "2851560",
    "end": "2859480"
  },
  {
    "text": "parameter tuning with uh one trial as you can see here it says two trials",
    "start": "2859480",
    "end": "2864819"
  },
  {
    "text": "here that's running parallel and then the the other uh cluster is running uh a single trial",
    "start": "2864819",
    "end": "2874740"
  },
  {
    "text": "um so it's kind of uh so it takes a couple minutes to run these things so",
    "start": "2876220",
    "end": "2881380"
  },
  {
    "text": "I'll just run them and you might be able to see the results but if not I'll show you the the results",
    "start": "2881380",
    "end": "2887500"
  },
  {
    "text": "from the previous one um so so let's say uh going back to the",
    "start": "2887500",
    "end": "2894819"
  },
  {
    "text": "um uh the infinite retry task so we can see if it's finished",
    "start": "2894819",
    "end": "2900460"
  },
  {
    "text": "looks like it's already finished here um you can see that um instead of running forever because of",
    "start": "2900460",
    "end": "2907420"
  },
  {
    "text": "the max free trial that's set to infinite it actually fails right away um the program already finished with the",
    "start": "2907420",
    "end": "2913540"
  },
  {
    "text": "outer memory error here um it also prints out",
    "start": "2913540",
    "end": "2919180"
  },
  {
    "text": "um the offending uh processes that is using too much memory so in this case uh",
    "start": "2919180",
    "end": "2925480"
  },
  {
    "text": "the leaker uh task that I just wrote above is using 16 gig memory and then and then",
    "start": "2925480",
    "end": "2933700"
  },
  {
    "text": "it got killed um also here it shows a link to",
    "start": "2933700",
    "end": "2939119"
  },
  {
    "text": "uh our documentation which describes how it may help resolve the issue so I",
    "start": "2939119",
    "end": "2946000"
  },
  {
    "text": "already opened it here um so has a page containing uh some information about",
    "start": "2946000",
    "end": "2951880"
  },
  {
    "text": "memory Monitor and kind of describes how it can be uh fixed so so in this case it",
    "start": "2951880",
    "end": "2959980"
  },
  {
    "text": "you know it would suggest maybe increasing the memory capacity to the node um or just yeah reducing the concurrency",
    "start": "2959980",
    "end": "2966520"
  },
  {
    "text": "but in this case because you know the script is a it's a it's a you know designed to be a bad task so um it",
    "start": "2966520",
    "end": "2974140"
  },
  {
    "text": "probably won't help but if it's a normal task uh those suggestions should help a little bit",
    "start": "2974140",
    "end": "2981480"
  },
  {
    "text": "um so yeah I would assume these are still running so maybe we can just look at some of the uh the uh one of my",
    "start": "2982119",
    "end": "2987640"
  },
  {
    "text": "previous runs here so um so the the trial with a",
    "start": "2987640",
    "end": "2993640"
  },
  {
    "text": "um or or the cluster of a single trial it took about eight minutes to run",
    "start": "2993640",
    "end": "2999339"
  },
  {
    "text": "and then we look at um a cluster that was running two trials in parallel",
    "start": "2999339",
    "end": "3006619"
  },
  {
    "text": "it took yeah it took about 13 minutes so actually it was actually faster uh to run them in parallel",
    "start": "3006619",
    "end": "3014760"
  },
  {
    "text": "and then with the any scale workspace I'm also uh using it to uh",
    "start": "3014760",
    "end": "3021599"
  },
  {
    "text": "to access the re-dashboard in grafana um so we can see like what was actually",
    "start": "3021599",
    "end": "3028319"
  },
  {
    "text": "actually happening to the workload so now I'm in the um the ray dashboard for uh the cluster",
    "start": "3028319",
    "end": "3036000"
  },
  {
    "text": "that was running to concurrent trials and you can see that well first of all",
    "start": "3036000",
    "end": "3042000"
  },
  {
    "text": "like the memory usage is kind of flat which indicates that it is um it is",
    "start": "3042000",
    "end": "3048240"
  },
  {
    "text": "being uh you know being uh the memory monitors is doing some work to prevent",
    "start": "3048240",
    "end": "3054420"
  },
  {
    "text": "it from going above uh 20 gig around and it eventually finishes the workload",
    "start": "3054420",
    "end": "3062599"
  },
  {
    "text": "and then we look at um the tasks uh that are running uh we",
    "start": "3062880",
    "end": "3068700"
  },
  {
    "text": "can see um the the big purple at the very bottom is uh the the number of",
    "start": "3068700",
    "end": "3075420"
  },
  {
    "text": "running tasks at any given time so you you can kind of see that at the beginning it tries to run a bunch of",
    "start": "3075420",
    "end": "3081300"
  },
  {
    "text": "tasks and then if they get killed then they get restarted but it smears out a little bit because",
    "start": "3081300",
    "end": "3088220"
  },
  {
    "text": "uh the uh the umkiller would try to do some exponential back off to reduce some",
    "start": "3088220",
    "end": "3093960"
  },
  {
    "text": "of the pressure so that's why you see it's uh not that aggressive over time",
    "start": "3093960",
    "end": "3101180"
  },
  {
    "text": "another thing to look at is uh using the grafana uh dashboard that is available",
    "start": "3102420",
    "end": "3109500"
  },
  {
    "text": "on any scale is uh to look at the the counter so this records like the",
    "start": "3109500",
    "end": "3116099"
  },
  {
    "text": "instance of uh or how many times the umkiller went and",
    "start": "3116099",
    "end": "3122460"
  },
  {
    "text": "uh went into achilla task so we can see like it triggered about 100 times",
    "start": "3122460",
    "end": "3129740"
  },
  {
    "text": "and then if we go look at compare it with um the uh the cluster that ran a single trial",
    "start": "3129900",
    "end": "3137000"
  },
  {
    "text": "it actually also has some other memory so I guess it's around 49 so you can",
    "start": "3137000",
    "end": "3142920"
  },
  {
    "text": "kind of see that the even though when we're running in uh the two trials in parallel",
    "start": "3142920",
    "end": "3148339"
  },
  {
    "text": "the amount of uh preemption was not as uh was not as bad as they were trying to",
    "start": "3148339",
    "end": "3156119"
  },
  {
    "text": "run and run them together so this uh concludes",
    "start": "3156119",
    "end": "3163319"
  },
  {
    "text": "um the demo so I'm going to go back to our slides",
    "start": "3163319",
    "end": "3169740"
  },
  {
    "text": "um so just a very quick recap of you know what we talked about um so in",
    "start": "3171240",
    "end": "3176640"
  },
  {
    "text": "uh grade 2.2 and above the ray memory monitor is enabled by",
    "start": "3176640",
    "end": "3181980"
  },
  {
    "text": "default and it's helping us to improve the cluster stability",
    "start": "3181980",
    "end": "3188940"
  },
  {
    "text": "um it also hasn't improved observability in terms of logging um",
    "start": "3188940",
    "end": "3194520"
  },
  {
    "text": "dashboards and then in the upcoming reviews we also have some tweets so the algorithms we",
    "start": "3194520",
    "end": "3200280"
  },
  {
    "text": "make it even more robust so that concludes uh my talk about uh",
    "start": "3200280",
    "end": "3207240"
  },
  {
    "text": "the ray memory monitor um now our canvas packaging tools well thanks a lot Clarence for that",
    "start": "3207240",
    "end": "3213180"
  },
  {
    "text": "insightful uh observability lens through which we actually prevent and understand",
    "start": "3213180",
    "end": "3218760"
  },
  {
    "text": "what's actually happening behind the scenes uh if you guys have any questions for Clarence please put it in the Q a",
    "start": "3218760",
    "end": "3224700"
  },
  {
    "text": "but while you're class while you were mentioning about when you were talking about things few things kind of property to my mind as you were going about doing",
    "start": "3224700",
    "end": "3231720"
  },
  {
    "text": "that you mentioned there is a certain threshold uh when the gray when the ohms",
    "start": "3231720",
    "end": "3238440"
  },
  {
    "text": "ohm killer starts looking at it what is that particular threshold you said it's enabled by default but how does some go",
    "start": "3238440",
    "end": "3244980"
  },
  {
    "text": "go how does some go about and change it is it something that is pre-configured when you when you run 2.2 or can you",
    "start": "3244980",
    "end": "3251880"
  },
  {
    "text": "actually tweak it to make it higher to make it lower yeah I know that's a great question um so yeah by default",
    "start": "3251880",
    "end": "3259200"
  },
  {
    "text": "um it looks at the the node memory or the container memory and um if it's",
    "start": "3259200",
    "end": "3265140"
  },
  {
    "text": "above 95 um then it'll trigger and start preempting tasks",
    "start": "3265140",
    "end": "3271200"
  },
  {
    "text": "um there's a environment variable that can be set to adjust that number",
    "start": "3271200",
    "end": "3277500"
  },
  {
    "text": "um and um there's our documentation page contains those details on how to do that",
    "start": "3277500",
    "end": "3284160"
  },
  {
    "text": "now is that 90 or whatever the threshold is that collectively across the entire cluster or is it per node",
    "start": "3284160",
    "end": "3290099"
  },
  {
    "text": "uh yes that's a good question yeah it's per note so um one example of of one of",
    "start": "3290099",
    "end": "3295800"
  },
  {
    "text": "our users is they they would like to have a higher threshold for their worker",
    "start": "3295800",
    "end": "3301680"
  },
  {
    "text": "nodes um so they were able to do that just by setting the environment variable differently",
    "start": "3301680",
    "end": "3308160"
  },
  {
    "text": "a couple of times you actually use the the the word tribal and non-tribal right",
    "start": "3308160",
    "end": "3314280"
  },
  {
    "text": "certain tasks which are tribal and certain types we said non-tribal could you explain what's the difference",
    "start": "3314280",
    "end": "3319440"
  },
  {
    "text": "between what's tribal and what's known tribal and and what determines what is one what is the other",
    "start": "3319440",
    "end": "3326160"
  },
  {
    "text": "yeah yeah um yeah so for uh task and access so by default tasks are",
    "start": "3326160",
    "end": "3331440"
  },
  {
    "text": "rechriable meaning that if it for some reason it crashes right or you know for",
    "start": "3331440",
    "end": "3337380"
  },
  {
    "text": "let's say if it's running on a node that node disappears it failed so so the task",
    "start": "3337380",
    "end": "3342960"
  },
  {
    "text": "would be retried we submitted you know in into the cluster and by default actors they don't retry",
    "start": "3342960",
    "end": "3349319"
  },
  {
    "text": "and um so they both have uh options or configurations that you can adjust uh",
    "start": "3349319",
    "end": "3356339"
  },
  {
    "text": "the the retry count just through the annotate the python annotation you can do that",
    "start": "3356339",
    "end": "3362700"
  },
  {
    "text": "and uh we actually have a question from rajia from um he's answering great work well thanks",
    "start": "3362700",
    "end": "3369180"
  },
  {
    "text": "a lot Richard um thanks a lot Clarence and and the team as a very developer should we or can we change the way we",
    "start": "3369180",
    "end": "3376859"
  },
  {
    "text": "develop and design applications to better leverage the new Ray umkiller in",
    "start": "3376859",
    "end": "3382920"
  },
  {
    "text": "other words do they actually have to change the behavior do they actually have to adopt certain strategies uh to",
    "start": "3382920",
    "end": "3389339"
  },
  {
    "text": "take advantage of this or the home killer with the thresholds they actually maintains give them a good good starting",
    "start": "3389339",
    "end": "3396000"
  },
  {
    "text": "point and they can start tuning the parameters to do that so what would what would be your advice how would they",
    "start": "3396000",
    "end": "3401700"
  },
  {
    "text": "actually take advantage of the home killer yeah um I I think like the uh the",
    "start": "3401700",
    "end": "3407220"
  },
  {
    "text": "umkiller it's on by default so like someone wouldn't have to like go and and",
    "start": "3407220",
    "end": "3412980"
  },
  {
    "text": "and try to look at it I mean and and I think the best way to to observe is is",
    "start": "3412980",
    "end": "3418740"
  },
  {
    "text": "when if the workload is running you know it may be skewed and is triggering a lot",
    "start": "3418740",
    "end": "3424680"
  },
  {
    "text": "of memory issues now you know before this memory monitor like we don't even",
    "start": "3424680",
    "end": "3430020"
  },
  {
    "text": "know why it like the task crash now it will actually print um the error message it'll print uh like",
    "start": "3430020",
    "end": "3437940"
  },
  {
    "text": "this is actually happening because it's because of memory issue now we have um like dashboards for final like to to",
    "start": "3437940",
    "end": "3445380"
  },
  {
    "text": "tell us this is actually happening um so I think when these situation happens which typically you know if",
    "start": "3445380",
    "end": "3452579"
  },
  {
    "text": "hopefully it doesn't always happen and it depends on the workload then it's a good way is to look at like look at um",
    "start": "3452579",
    "end": "3460500"
  },
  {
    "text": "like how to resolve it for example at that point like maybe you know it maybe it should reduce the the concurrency or",
    "start": "3460500",
    "end": "3467640"
  },
  {
    "text": "maybe it needs a a node that has the higher memory ratio because of the",
    "start": "3467640",
    "end": "3473819"
  },
  {
    "text": "nature of the workload at that point so I think I think it's it's it helps make it simpler to debug and find these",
    "start": "3473819",
    "end": "3480059"
  },
  {
    "text": "issues as before um it's even like it's it's like a shot",
    "start": "3480059",
    "end": "3486240"
  },
  {
    "text": "in the dark we don't know even though like okay so this is a memory uh ski you know racial problem or or whether",
    "start": "3486240",
    "end": "3493440"
  },
  {
    "text": "whether there's other things going on now when you were actually showing that offensive task that was just too hungry",
    "start": "3493440",
    "end": "3501180"
  },
  {
    "text": "to to take all the memory off the Heap and immediately actually failed with the um killer where you actually say this is",
    "start": "3501180",
    "end": "3507420"
  },
  {
    "text": "memory function that actually happened now suppose I'm I'm a developer and I'm I'm I'm having I'm doing the second bit",
    "start": "3507420",
    "end": "3513720"
  },
  {
    "text": "which was doing my hyper parameter tuning and and doing all these trials but I'm not going to sit there right I'm",
    "start": "3513720",
    "end": "3520079"
  },
  {
    "text": "gonna launch the particular job and I'm gonna go out for a cup of coffee and come back and I'm obviously I'm not",
    "start": "3520079",
    "end": "3525240"
  },
  {
    "text": "watching the the the the the events or I'm not watching the dashboard so I come after two hours and all of a sudden you",
    "start": "3525240",
    "end": "3531599"
  },
  {
    "text": "know everything is is finished or this thing has been killed um I go to the dashboard and time has",
    "start": "3531599",
    "end": "3537780"
  },
  {
    "text": "gone by because the two hours is gone by how and where",
    "start": "3537780",
    "end": "3543540"
  },
  {
    "text": "are events or actions that the home killer took recorded that I can actually look back is it",
    "start": "3543540",
    "end": "3550799"
  },
  {
    "text": "recorded in the in the log to somewhere yeah um the the quickest and easiest way is",
    "start": "3550799",
    "end": "3556260"
  },
  {
    "text": "to uh go to uh the grafana and so in this case I'm assuming that you know",
    "start": "3556260",
    "end": "3561540"
  },
  {
    "text": "like someone deployed Rey and then they also connected their their you know monitoring stack like you know grafana",
    "start": "3561540",
    "end": "3568079"
  },
  {
    "text": "for example um and then because we actually emit those counters like uh so we have a",
    "start": "3568079",
    "end": "3574500"
  },
  {
    "text": "cluster-wide counter of how many um events has triggered that would be the quickest",
    "start": "3574500",
    "end": "3583200"
  },
  {
    "text": "way to see you know let's say I run I finish a workload it's slower than what I would expect it could be because you",
    "start": "3583200",
    "end": "3589740"
  },
  {
    "text": "know I suspect that it's you know because there's a lot of you know out of memory issues",
    "start": "3589740",
    "end": "3595680"
  },
  {
    "text": "um so that would be the very quick way to look okay is there the line chain that going up and so that in that case then",
    "start": "3595680",
    "end": "3601740"
  },
  {
    "text": "um that would indicate there's an issue and then the documentation would be a good place to uh go to to like look at",
    "start": "3601740",
    "end": "3608579"
  },
  {
    "text": "how to uh you know resolve or reduce that in order to run the world faster",
    "start": "3608579",
    "end": "3615119"
  },
  {
    "text": "now if I was an envelopes guy and I would be begging the question how can I",
    "start": "3615119",
    "end": "3620460"
  },
  {
    "text": "be notified about the home killer taking a lot of actions right I know that it's",
    "start": "3620460",
    "end": "3625920"
  },
  {
    "text": "not on our roadmap where I want to be able to be notified and I think this is something that when people do have large",
    "start": "3625920",
    "end": "3632520"
  },
  {
    "text": "jobs that actually run over time where you're doing you know distributed training for a large data sets computer",
    "start": "3632520",
    "end": "3638579"
  },
  {
    "text": "vision it does take a lot and they take a lot of memory in tensors or if you're actually doing a huge hyper parameter",
    "start": "3638579",
    "end": "3645540"
  },
  {
    "text": "sweep over a large space this is going to take a long time and obviously they're going to be some some some home",
    "start": "3645540",
    "end": "3651780"
  },
  {
    "text": "events they actually kill but I do want to be notified what do you think is the best way for people to to approach that",
    "start": "3651780",
    "end": "3659940"
  },
  {
    "text": "yeah I mean definitely like we we can extend the the metric so I mean if it's",
    "start": "3659940",
    "end": "3665040"
  },
  {
    "text": "uh already connected to Cortana I mean adding alarms now at that point right if",
    "start": "3665040",
    "end": "3671940"
  },
  {
    "text": "there's excessive uh happening will obviously slow down the workload uh",
    "start": "3671940",
    "end": "3678420"
  },
  {
    "text": "yeah yeah yeah uh then like or maybe it's just uh slightly misconfigured and in that",
    "start": "3678420",
    "end": "3684599"
  },
  {
    "text": "case then setting an alarm just to measure the you know the product the the growth of that counter would help detect",
    "start": "3684599",
    "end": "3692059"
  },
  {
    "text": "and these these these home killer events the the actions that the home killer",
    "start": "3692059",
    "end": "3697619"
  },
  {
    "text": "take those are considered as Ray events right so we also if you go to the event space you will actually see a history of",
    "start": "3697619",
    "end": "3704640"
  },
  {
    "text": "all the events that happen just like the auto scaler you know loads and and brings down a particular node likewise",
    "start": "3704640",
    "end": "3711780"
  },
  {
    "text": "you would actually have these events being being part of the node so you can actually also look at that as a story of",
    "start": "3711780",
    "end": "3717180"
  },
  {
    "text": "of how the home killer reacted exactly so that's part of the benefits uh",
    "start": "3717180",
    "end": "3722900"
  },
  {
    "text": "emission that that we are showing",
    "start": "3722900",
    "end": "3728040"
  },
  {
    "text": "earlier like that line is is the same metrics reporting mechanism that we are",
    "start": "3728040",
    "end": "3733799"
  },
  {
    "text": "using to report all the other metrics okay well I think we don't have any more",
    "start": "3733799",
    "end": "3739020"
  },
  {
    "text": "questions and I don't want to be uh hugging the mic for too long because we have uh and I was actually waiting in in",
    "start": "3739020",
    "end": "3745920"
  },
  {
    "text": "in patiently to to tell us all about how they're actually using red Shopify so uh",
    "start": "3745920",
    "end": "3753260"
  },
  {
    "text": "thank you thanks a lot for taking the time to come from ShopRite tell us how you guys are actually using it so take",
    "start": "3753260",
    "end": "3759240"
  },
  {
    "text": "it away and just stay on if if um some questions come up in the Q a feel free to answer them yeah thank you",
    "start": "3759240",
    "end": "3767579"
  },
  {
    "text": "thanks Jules um can everyone hear me yes okay all right let me share screen",
    "start": "3767579",
    "end": "3774720"
  },
  {
    "text": "so",
    "start": "3774720",
    "end": "3777720"
  },
  {
    "text": "um people see my screen awesome um so first of all",
    "start": "3780299",
    "end": "3785700"
  },
  {
    "text": "um hello everyone my name is it's not an easy name most people at",
    "start": "3785700",
    "end": "3791880"
  },
  {
    "text": "Shopify called BK um so I'm very excited to be here today or to",
    "start": "3791880",
    "end": "3797700"
  },
  {
    "text": "talk about some of the work that we're doing over at Shopify um and so for those of you who are not familiar with Shopify",
    "start": "3797700",
    "end": "3804480"
  },
  {
    "text": "um we're a leading Global Commerce company or we provide trusted tools to start grow manage and Market or retail",
    "start": "3804480",
    "end": "3812640"
  },
  {
    "text": "business of any size or um our goal is to make Commerce better",
    "start": "3812640",
    "end": "3817799"
  },
  {
    "text": "for everyone with platforms and services that are engineered for reliability um while delivering better shopping",
    "start": "3817799",
    "end": "3823920"
  },
  {
    "text": "experience for consumers everywhere right uh Shopify Powers over 2 billion",
    "start": "3823920",
    "end": "3829500"
  },
  {
    "text": "businesses the last time we uh publicly announced that uh across 175 countries",
    "start": "3829500",
    "end": "3835740"
  },
  {
    "text": "um and I'm not going to list all of the various Brands we uh we power but",
    "start": "3835740",
    "end": "3841200"
  },
  {
    "text": "um almost everyone would have at some point in time purchase something from a Shopify Merchant or whether you realize",
    "start": "3841200",
    "end": "3848400"
  },
  {
    "text": "it or not uh in most cases you probably didn't realize it and that's by Design uh because we we consider ourselves",
    "start": "3848400",
    "end": "3854400"
  },
  {
    "text": "almost like the OS of farmers and so we we stay in the background right um So within Shopify I specifically",
    "start": "3854400",
    "end": "3861420"
  },
  {
    "text": "today uh work for um a team called Capital uh but what I'm",
    "start": "3861420",
    "end": "3866520"
  },
  {
    "text": "going to talk to you about today um is some work that I did with a different team at Shopify called",
    "start": "3866520",
    "end": "3871680"
  },
  {
    "text": "Commerce algorithms um where we essentially built something we call the product understanding engine",
    "start": "3871680",
    "end": "3877260"
  },
  {
    "text": "what that basically means is we built an ml application um to extract structured metadata around",
    "start": "3877260",
    "end": "3885059"
  },
  {
    "text": "uh products that or inherently the data we get for products are unstructured in nature",
    "start": "3885059",
    "end": "3890640"
  },
  {
    "text": "right so that's the topic and we're going to talk about how we built it and what kind of infrastructure and Ray is a",
    "start": "3890640",
    "end": "3896400"
  },
  {
    "text": "big part of that uh that enabled us to do it at the scale that we needed to be done right",
    "start": "3896400",
    "end": "3901500"
  },
  {
    "text": "um so having said all of that um I wanted to quickly go over some of",
    "start": "3901500",
    "end": "3906720"
  },
  {
    "text": "the areas that I'll be discussing today um I've tried to touch over a lot of different components of the system and I",
    "start": "3906720",
    "end": "3912359"
  },
  {
    "text": "would be more than happy at the end of this presentation to dig deeper into specific areas or components uh as uh",
    "start": "3912359",
    "end": "3918540"
  },
  {
    "text": "interest pops up um we'll start by discussing what product understanding itself means at Shopify and why we think it's important",
    "start": "3918540",
    "end": "3925020"
  },
  {
    "text": "for our Merchant success once that is defined and understood um I'll talk to you about the way we",
    "start": "3925020",
    "end": "3930480"
  },
  {
    "text": "formulated this as a machine learning problem and the specific modeling techniques that we took to solve this",
    "start": "3930480",
    "end": "3935819"
  },
  {
    "text": "problem I'll then go over some of the scaling requirements and this is where Ray and rml platform comes into the",
    "start": "3935819",
    "end": "3941339"
  },
  {
    "text": "picture and what the infrastructure is that we had to build to support this and then finally we'll talk about the most",
    "start": "3941339",
    "end": "3946799"
  },
  {
    "text": "fun of one part of this project for me which is how we surfaced up all of this to our Merchants were our end customers",
    "start": "3946799",
    "end": "3952859"
  },
  {
    "text": "and then kind of build workflows that help our Merchants uh and how this actually makes a difference in the real",
    "start": "3952859",
    "end": "3958740"
  },
  {
    "text": "world right uh and then finally we'll talk about where this is going to go from here",
    "start": "3958740",
    "end": "3964079"
  },
  {
    "text": "um so what is product understanding right um essentially we want like I'll let you",
    "start": "3964079",
    "end": "3970680"
  },
  {
    "text": "read what's on the screen but essentially we want to build a service that extracts useful metadata around",
    "start": "3970680",
    "end": "3976200"
  },
  {
    "text": "products right our Merchants are selling billions and billions of products in fact right we have millions of merchants",
    "start": "3976200",
    "end": "3981960"
  },
  {
    "text": "selling billions of products and they upload all sorts of information about the product that they're selling right",
    "start": "3981960",
    "end": "3988140"
  },
  {
    "text": "uh but we need a structured way in order to understand what our Merchants are select",
    "start": "3988140",
    "end": "3993900"
  },
  {
    "text": "right uh and the usage of this can be from anywhere from internal teams who",
    "start": "3993900",
    "end": "3999480"
  },
  {
    "text": "are using this these predictions for various analytical purposes all the way through external Partners like Google",
    "start": "3999480",
    "end": "4006440"
  },
  {
    "text": "Facebook Tech talk in order to kind of do things like search and relevancy trust and safety legal tax purposes",
    "start": "4006440",
    "end": "4012559"
  },
  {
    "text": "there's so many other unique consumers we have of this product that it just touches every aspect of Commerce right",
    "start": "4012559",
    "end": "4018859"
  },
  {
    "text": "having structured metadata our own products basically is a prerequisite to do Commerce well uh so but you can very",
    "start": "4018859",
    "end": "4026599"
  },
  {
    "text": "justifiably now ask me at this point why do you need machine learning to do this right why can't we just collect",
    "start": "4026599",
    "end": "4031940"
  },
  {
    "text": "structured metadata just like we collect or everything else from our merchants and that would be a great question right",
    "start": "4031940",
    "end": "4037700"
  },
  {
    "text": "so let's actually take a trip down memory lane right uh I don't know how many of our uh how many of the people on",
    "start": "4037700",
    "end": "4045140"
  },
  {
    "text": "the call today have you Shopify before but the screen you see in front of you would have been very familiar if you use",
    "start": "4045140",
    "end": "4050960"
  },
  {
    "text": "Shopify a year ago right this is basically the product admin page at Shopify this is where Merchants would",
    "start": "4050960",
    "end": "4056420"
  },
  {
    "text": "have come about a year ago and uh they would apply to update their products right in this instance I'm going to try to",
    "start": "4056420",
    "end": "4062539"
  },
  {
    "text": "create a product I'm selling a dog bed and those are my dogs that you see in front of you right and I'm going to use",
    "start": "4062539",
    "end": "4067579"
  },
  {
    "text": "that uh help to Market this product as a merchant I have a lot of flexibility",
    "start": "4067579",
    "end": "4072619"
  },
  {
    "text": "here to add feeds that are useful to me right I can add titles descriptions vendors product type collections tags",
    "start": "4072619",
    "end": "4079099"
  },
  {
    "text": "all sorts of things right but note is that none of this is structured like I can put any free text I want in any of",
    "start": "4079099",
    "end": "4085339"
  },
  {
    "text": "those fields I can add an image if I want I can in fact I can add multiple images if I want right but none of this",
    "start": "4085339",
    "end": "4091039"
  },
  {
    "text": "is structured in any sort or any way shape or sound right it's all unstructured data",
    "start": "4091039",
    "end": "4096080"
  },
  {
    "text": "what then so what our Merchants do is they normally use all of these fields to organize their products and catalog",
    "start": "4096080",
    "end": "4102560"
  },
  {
    "text": "internally right it's super important to realize that the way every Merchant organizes their store is very different",
    "start": "4102560",
    "end": "4109699"
  },
  {
    "text": "and it's almost unique in a way to them and so it doesn't really work right so take",
    "start": "4109699",
    "end": "4116060"
  },
  {
    "text": "take for example that field there that's called Product type right I have just added there that it's called my custom",
    "start": "4116060",
    "end": "4121758"
  },
  {
    "text": "document right if Jules were selling the same thing he would probably call it super duper bad or something like it",
    "start": "4121759",
    "end": "4128120"
  },
  {
    "text": "could be called anything you want it's completely free text right so that doesn't really help us do metadata",
    "start": "4128120",
    "end": "4134359"
  },
  {
    "text": "extraction very well we need to have a better way in order to kind of extract metadata from our products from this",
    "start": "4134359",
    "end": "4140060"
  },
  {
    "text": "very unstructured world that our Merchants are operating in right um so this this creates a problem right",
    "start": "4140060",
    "end": "4146298"
  },
  {
    "text": "because fundamentally we are what our Merchants are selling and partners like Facebook and Google rely",
    "start": "4146299",
    "end": "4152359"
  },
  {
    "text": "heavily on these kind of information not just for organizing their catalogs and",
    "start": "4152359",
    "end": "4157699"
  },
  {
    "text": "their shopping in their shopping ecosystem but also things like search and relevancy in fact there are legal",
    "start": "4157699",
    "end": "4163758"
  },
  {
    "text": "ramifications to this we have to for example taxation is handled differently based on what you're selling right if",
    "start": "4163759",
    "end": "4169338"
  },
  {
    "text": "you're selling food I need to tax you differently than if you were selling heavy machinery right so it becomes very",
    "start": "4169339",
    "end": "4174798"
  },
  {
    "text": "critical for us to kind of know what the merchants are selling and more structured data right uh so that's the",
    "start": "4174799",
    "end": "4181219"
  },
  {
    "text": "problem right we have to go from this world where our Merchants are giving us unstructured data in a world where it's",
    "start": "4181219",
    "end": "4187219"
  },
  {
    "text": "local to them to something that's more Global and uh standardized across all",
    "start": "4187219",
    "end": "4192920"
  },
  {
    "text": "Merchants right um so what do I actually mean by that let's go back to that same dog bed that",
    "start": "4192920",
    "end": "4198140"
  },
  {
    "text": "I was trying to sell and let's look at it right what kind of metadata could we extract from this well",
    "start": "4198140",
    "end": "4204739"
  },
  {
    "text": "I can tell you that it's a dog bed immediately that's the category of this product it's a dog bed I know that its",
    "start": "4204739",
    "end": "4210500"
  },
  {
    "text": "color is gray I know that it's the memory of like the the film material of it is memory foam and so on and so forth",
    "start": "4210500",
    "end": "4216380"
  },
  {
    "text": "right there's many many attributes that I can now extract for that image right a lot of us are very familiar with all of",
    "start": "4216380",
    "end": "4222620"
  },
  {
    "text": "these different attributes because we use them all all the time when we do online shopping right whether it's",
    "start": "4222620",
    "end": "4227780"
  },
  {
    "text": "through search or whether it's filters we use all of this to get relevant results and these attributes are going",
    "start": "4227780",
    "end": "4233960"
  },
  {
    "text": "to help us Define and organize products the key thing to remember here of course",
    "start": "4233960",
    "end": "4239179"
  },
  {
    "text": "is that all of those values you see whether it's a dog bed whether it's gray all of those are coming from a control",
    "start": "4239179",
    "end": "4246199"
  },
  {
    "text": "list of finite values right as an example by the shape of my docket can",
    "start": "4246199",
    "end": "4251600"
  },
  {
    "text": "possibly only take one of three let's say right oval rectangular and semicircular there are no other values",
    "start": "4251600",
    "end": "4257060"
  },
  {
    "text": "that it can take it is coming from a control list of categories that can go in right so having said all of this the",
    "start": "4257060",
    "end": "4262880"
  },
  {
    "text": "focus of the rest of this discussion is going to be narrowed down to just one of those attributes which is category and",
    "start": "4262880",
    "end": "4268040"
  },
  {
    "text": "how we built an ml model using Ray and tensorflow to predict product category",
    "start": "4268040",
    "end": "4273679"
  },
  {
    "text": "so with that all said let's focus on the specific task which our initial model",
    "start": "4273679",
    "end": "4279260"
  },
  {
    "text": "was for right like I mentioned we wanted to infer the product category for all products that our Merchants are selling",
    "start": "4279260",
    "end": "4285020"
  },
  {
    "text": "in reality this problem is who is twofold right first we need to come up with a taxonomy or a control list of",
    "start": "4285020",
    "end": "4292040"
  },
  {
    "text": "products a quick like we did a quick count we went to our database and we're like hey we have that product category",
    "start": "4292040",
    "end": "4297260"
  },
  {
    "text": "that our Merchants are putting free text for let's look at how many unique categories there are a quick count of",
    "start": "4297260",
    "end": "4302480"
  },
  {
    "text": "that showed us millions and millions of product categories now that's not sustainable like we cannot have millions",
    "start": "4302480",
    "end": "4307580"
  },
  {
    "text": "of categories it needs to be more controlled it needs to be something that we can actually use right so we needed",
    "start": "4307580",
    "end": "4313400"
  },
  {
    "text": "to come up with the taxonomy of product of product categories that are manageable induced right let's say we",
    "start": "4313400",
    "end": "4318980"
  },
  {
    "text": "did that then I need a machine learning service to help me categorize each product and remember our margins are",
    "start": "4318980",
    "end": "4325460"
  },
  {
    "text": "selling billions of products with multiple tens of billions of images to categorize",
    "start": "4325460",
    "end": "4332420"
  },
  {
    "text": "every one of them into one of these categories and then kind of go from there with all our Downstream consumers",
    "start": "4332420",
    "end": "4338420"
  },
  {
    "text": "so to solve the first problem which is how do I come up with the taxonomy well we did right we said this is not a",
    "start": "4338420",
    "end": "4344600"
  },
  {
    "text": "problem that we want to solve by ourselves let's see if somebody else has solved it for us luckily there exists",
    "start": "4344600",
    "end": "4350000"
  },
  {
    "text": "something called the Google product taxonomy this is open source it's maintained by Google of course as the name suggests and you will immediately",
    "start": "4350000",
    "end": "4357739"
  },
  {
    "text": "notice that this isn't a flat list right rather it ended up being a hierarchical tree-like structure",
    "start": "4357739",
    "end": "4363320"
  },
  {
    "text": "what that means is you'll see that on the top level there the first thing you see is animals in pet supplies now",
    "start": "4363320",
    "end": "4368840"
  },
  {
    "text": "within animals and pet supplies you see two separate sub subcategories right live animals and then pet supplies and",
    "start": "4368840",
    "end": "4375140"
  },
  {
    "text": "this goes on and on right so what and this makes a lot of sense if you actually think about it right because",
    "start": "4375140",
    "end": "4380960"
  },
  {
    "text": "what I'm wearing is a t-shirt but it also belongs to the clothing uh the clothing industry right and there's and",
    "start": "4380960",
    "end": "4386840"
  },
  {
    "text": "there's probably specific types of t-shirts and you can go more and more granular right uh and maybe different",
    "start": "4386840",
    "end": "4392300"
  },
  {
    "text": "use cases want to want classifications at different levels right so maybe I want to report on Revenue growth across",
    "start": "4392300",
    "end": "4399860"
  },
  {
    "text": "the top level Industries right or maybe I want to go a little deeper and say hey just show me the revenue or the clothing",
    "start": "4399860",
    "end": "4405380"
  },
  {
    "text": "vertical right or maybe I want to go super granular and say hey is this person likely to buy dog food versus cat",
    "start": "4405380",
    "end": "4411140"
  },
  {
    "text": "food right so we have the need to go down to different levels of the taxonomy for different use cases right and so",
    "start": "4411140",
    "end": "4418159"
  },
  {
    "text": "it's important for our model to be able to predict across those different levels right",
    "start": "4418159",
    "end": "4423320"
  },
  {
    "text": "so this is an example of one single branch of the Google product actually right so you start all the way up there",
    "start": "4423320",
    "end": "4429140"
  },
  {
    "text": "I think I believe it starts off at arts and entertainment and then if you come down all the way to the left it'll",
    "start": "4429140",
    "end": "4435440"
  },
  {
    "text": "probably end somewhere with clarinet crickets like that's how granular this thing gets there are over 6000 nodes in",
    "start": "4435440",
    "end": "4441800"
  },
  {
    "text": "the street right and so every product can belong to one of these branches right so essentially the prod the",
    "start": "4441800",
    "end": "4448040"
  },
  {
    "text": "problem we're trying to solve is given a product given features of a product which is the text and the image we want",
    "start": "4448040",
    "end": "4454340"
  },
  {
    "text": "to find which branch within this taxonomy does that product belong to like that is essentially the problem",
    "start": "4454340",
    "end": "4459920"
  },
  {
    "text": "right no the taxonomy structure works great from a categorization perspective but it makes our machine learning",
    "start": "4459920",
    "end": "4466760"
  },
  {
    "text": "problem so much harder to Define and train right because normally we when I say classification most people assume a",
    "start": "4466760",
    "end": "4473300"
  },
  {
    "text": "flat a flat classification where you have n number of classes and I need to pick one this is not that this is I have",
    "start": "4473300",
    "end": "4479780"
  },
  {
    "text": "different branches and I have to pick one branch that best describes that product",
    "start": "4479780",
    "end": "4486140"
  },
  {
    "text": "so before I jump into the model itself and the architecture and how we use today",
    "start": "4486140",
    "end": "4492020"
  },
  {
    "text": "um a couple of requirements that we had right um our vision in the end was to not just",
    "start": "4492020",
    "end": "4497179"
  },
  {
    "text": "stick to product categories we wanted to build all those other things right so shape size color right like all of those",
    "start": "4497179",
    "end": "4502760"
  },
  {
    "text": "other things right so we wanted to start building essentially a knowledge graph that we could then reuse for other uh",
    "start": "4502760",
    "end": "4510920"
  },
  {
    "text": "other tasks down the line right so we first of all started uh framing this as a multi-task multi-class problem the",
    "start": "4510920",
    "end": "4518360"
  },
  {
    "text": "idea is that we would have a central neural network graph with a lot of layers and nodes and then a subset of",
    "start": "4518360",
    "end": "4523820"
  },
  {
    "text": "each of those layers could be reused for different tasks by making the right connections between each of those layers",
    "start": "4523820",
    "end": "4529460"
  },
  {
    "text": "right so what what did that mean for us remember I said the Google product taxonomy has different levels to it in",
    "start": "4529460",
    "end": "4536360"
  },
  {
    "text": "fact it has seven different levels you could start at the very top arts and entertainment and then go all the way",
    "start": "4536360",
    "end": "4542600"
  },
  {
    "text": "down to level seven clarinet care kits so essentially each level of the taxonomy is treated as a separate",
    "start": "4542600",
    "end": "4549040"
  },
  {
    "text": "classification problem right so I basically say hey at level one what do you think the prediction is find great",
    "start": "4549040",
    "end": "4554420"
  },
  {
    "text": "like then using that information I come down to level two ask the next so we have essentially these different",
    "start": "4554420",
    "end": "4559699"
  },
  {
    "text": "tasks we still train a single model but the single model is now trying to do multiple tasks together and each task is",
    "start": "4559699",
    "end": "4566239"
  },
  {
    "text": "predicting one level of the taxonomy right um and then obviously each each task is",
    "start": "4566239",
    "end": "4571820"
  },
  {
    "text": "a multiple multi-class because you have multiple classes in each level right uh",
    "start": "4571820",
    "end": "4577280"
  },
  {
    "text": "one key thing to remember here is not all levels are equal right like every level has a different number of",
    "start": "4577280",
    "end": "4582860"
  },
  {
    "text": "categories belonging to that level for example at the very beginning there are a few nodes level when level one",
    "start": "4582860",
    "end": "4588440"
  },
  {
    "text": "probably has 21 categories or something by the time you get to level three or four you're now dealing with thousands",
    "start": "4588440",
    "end": "4594080"
  },
  {
    "text": "of categories in that in that level right so not all levels are",
    "start": "4594080",
    "end": "4598960"
  },
  {
    "text": "um if you remember uh if you remember our product detail page before all of the data was unstructured right we had",
    "start": "4599840",
    "end": "4604940"
  },
  {
    "text": "text and we had some images right uh we didn't want to reinvent the wheel right so there are a lot of pre-trained models",
    "start": "4604940",
    "end": "4610159"
  },
  {
    "text": "already existing or that we could kind of use to kick start the learning process so we said hey let's go and see",
    "start": "4610159",
    "end": "4616219"
  },
  {
    "text": "if we can use some of these pre-trained models uh to help us do better specifically we use multilingual word",
    "start": "4616219",
    "end": "4622760"
  },
  {
    "text": "for text and then we use mobile net we do for images right the choices of both",
    "start": "4622760",
    "end": "4628460"
  },
  {
    "text": "of these models were essentially done after exhaustive testing it essentially boils down to",
    "start": "4628460",
    "end": "4633980"
  },
  {
    "text": "you use bigger models that will give you gains and performance but it comes at the cost there is a cost associated with",
    "start": "4633980",
    "end": "4640580"
  },
  {
    "text": "it and so we had to kind of find this Balancing Act between cost and performance right and so we ended up",
    "start": "4640580",
    "end": "4645860"
  },
  {
    "text": "with these two right uh and then we had different consumers like I said",
    "start": "4645860",
    "end": "4651800"
  },
  {
    "text": "like there are so many different consumers both internally and externally and all of them had slightly different access patterns right maybe somebody",
    "start": "4651800",
    "end": "4658280"
  },
  {
    "text": "wants to access this in a database through batch prediction right maybe someone wants to have real-time",
    "start": "4658280",
    "end": "4663679"
  },
  {
    "text": "inference or streaming right and so then we realized the model has to be actually deployed in different flavors to",
    "start": "4663679",
    "end": "4670159"
  },
  {
    "text": "accommodate all kinds of this so we went with the subclass model architecture and I'll I'll dig deeper into what that",
    "start": "4670159",
    "end": "4676760"
  },
  {
    "text": "means in a few minutes right uh but basically you can either deploy the entire model as a standalone model or",
    "start": "4676760",
    "end": "4684500"
  },
  {
    "text": "you can chunk it up into pieces and Jules actually touched upon this a little bit where he spoke about inference graphs a little earlier you",
    "start": "4684500",
    "end": "4690800"
  },
  {
    "text": "could essentially have this graph of models one that enables us is to kind of then do things like caching of",
    "start": "4690800",
    "end": "4696320"
  },
  {
    "text": "embeddings for example right it actually gives you efficiency gains in boosts if you kind of split that out and for",
    "start": "4696320",
    "end": "4702920"
  },
  {
    "text": "different use cases we've actually deployed the model slightly differently uh so these are some of our decisions that",
    "start": "4702920",
    "end": "4710360"
  },
  {
    "text": "we made initially right uh so having and then we there is of course",
    "start": "4710360",
    "end": "4715940"
  },
  {
    "text": "there's one last thing and this is something that used to trip up all of us before which is uh there were a lot of",
    "start": "4715940",
    "end": "4721480"
  },
  {
    "text": "pre-processing that is specific to the model itself right in this like things like oh how do I have one Hot Import",
    "start": "4721480",
    "end": "4727640"
  },
  {
    "text": "something because and I want to do that in real time so we we decided to go tensorflow transform to kind of help us",
    "start": "4727640",
    "end": "4733520"
  },
  {
    "text": "do the stateful transformation what that does is we use tensorflow transform during the training to kind of do all of",
    "start": "4733520",
    "end": "4739580"
  },
  {
    "text": "the pre-processing and then that pre-processing step is now essentially saved as a tensorflow graph which I can",
    "start": "4739580",
    "end": "4747020"
  },
  {
    "text": "then use during inference time to do the exact same uh transformation so that there is no difference in pre-processing",
    "start": "4747020",
    "end": "4753679"
  },
  {
    "text": "between um training and inference time so",
    "start": "4753679",
    "end": "4759199"
  },
  {
    "text": "um having said all of that now let's actually look at the model architecture I've spoken quite a bit about all of the things we wanted to do",
    "start": "4759199",
    "end": "4765739"
  },
  {
    "text": "um I'm not going to get too much into the specifics here's a very rough schematic of the model for the first two levels of tasks right remember there are",
    "start": "4765739",
    "end": "4772699"
  },
  {
    "text": "seven layers of tasks one for each level of the taxonomy so what we do initially",
    "start": "4772699",
    "end": "4778280"
  },
  {
    "text": "is we take the raw text and we take the raw image and separate inputs or pass them into their corresponding embedding",
    "start": "4778280",
    "end": "4784159"
  },
  {
    "text": "layer so Burton image or mobile that we do we then pass the combination of these",
    "start": "4784159",
    "end": "4789199"
  },
  {
    "text": "embeddings through a multi-layer perceptron to finally predict the first layer right like the first level right",
    "start": "4789199",
    "end": "4794659"
  },
  {
    "text": "so you'll pass the image you'll pass the text and then you kind of go hey I think this belongs to animals and pet supplies",
    "start": "4794659",
    "end": "4801620"
  },
  {
    "text": "uh you essentially get that score uh at level one we then take that output",
    "start": "4801620",
    "end": "4806840"
  },
  {
    "text": "combine it back with the original embedding so we take the level one output combine it back to the original embedding and then pass that through a",
    "start": "4806840",
    "end": "4814219"
  },
  {
    "text": "second multi-layer perceptron right and this this second layer the second multi",
    "start": "4814219",
    "end": "4819380"
  },
  {
    "text": "multi layer perceptron essentially that it is focused on the second task which is predicting the second level of the",
    "start": "4819380",
    "end": "4825679"
  },
  {
    "text": "Google product taxonomy and you do this over and over and over again until you hit Level Seven right so you predict",
    "start": "4825679",
    "end": "4830960"
  },
  {
    "text": "level one take the predictions or you try to predict level one take the output of that concatenate it back with the original",
    "start": "4830960",
    "end": "4837199"
  },
  {
    "text": "inputs pass it through another set of layers try to predict level two take the output of that concatenate it back with",
    "start": "4837199",
    "end": "4842960"
  },
  {
    "text": "the original on and on and on until you hit Level Seven right uh so while",
    "start": "4842960",
    "end": "4848900"
  },
  {
    "text": "remember like so while each um so what that did what that essentially",
    "start": "4848900",
    "end": "4855500"
  },
  {
    "text": "did is the parent node helps to predict the child mode and that makes sense right because if I told you if I",
    "start": "4855500",
    "end": "4861560"
  },
  {
    "text": "inherently told you hey something is I think the first part the first level of this is clothing then the second level",
    "start": "4861560",
    "end": "4868280"
  },
  {
    "text": "should have a higher likelihood of being a shirt or a pant versus a drill bit right so the the parent prediction",
    "start": "4868280",
    "end": "4873800"
  },
  {
    "text": "should affect the child protection right uh and so that's why these connections made sense right uh",
    "start": "4873800",
    "end": "4880100"
  },
  {
    "text": "the second thing and we already spoke about this right like the seven level seven tasks right uh",
    "start": "4880100",
    "end": "4886760"
  },
  {
    "text": "what we also then did is we have massive amounts of training data right we have",
    "start": "4886760",
    "end": "4892280"
  },
  {
    "text": "maximum amounts of data that we want to predict on as well but in terms of just the sheer data we have to train on",
    "start": "4892280",
    "end": "4898460"
  },
  {
    "text": "it runs well into the hundreds of millions of products tens and tens of millions in fact",
    "start": "4898460",
    "end": "4905120"
  },
  {
    "text": "hundreds of millions of uh images right a lot of text it can actually get very exhaustive the training data set can get",
    "start": "4905120",
    "end": "4911179"
  },
  {
    "text": "very very big right uh so we had to do this in a distributed fashion right so we ended up using distributed tensorflow",
    "start": "4911179",
    "end": "4917360"
  },
  {
    "text": "with multiple multiple gpus multiple machines and this is where raytrain came",
    "start": "4917360",
    "end": "4923060"
  },
  {
    "text": "now we did this over a year ago and at that point Ray 2.0 wasn't a thing so",
    "start": "4923060",
    "end": "4928880"
  },
  {
    "text": "this was all built on Ray one point x so raytrain was what we ended up with that at that time we're using uh distributed",
    "start": "4928880",
    "end": "4935480"
  },
  {
    "text": "tensorflow um and so we did that and then",
    "start": "4935480",
    "end": "4941480"
  },
  {
    "text": "we built a shop like so we we use shopify's ml platform I it says that",
    "start": "4941480",
    "end": "4947000"
  },
  {
    "text": "it's built on Google Cloud platform but Google Cloud platform is our infrastructure provider right like that's who gives us our machines there's",
    "start": "4947000",
    "end": "4953780"
  },
  {
    "text": "Google Cloud platform there's rare sitting on top of it and then there's me right so I'm essentially the user of our",
    "start": "4953780",
    "end": "4959960"
  },
  {
    "text": "ml platform which is called Berlin there is a slide later where I'll talk to you about the architecture of how Merlin and",
    "start": "4959960",
    "end": "4966020"
  },
  {
    "text": "Ray is being used but essentially our infrastructure layer is Google Cloud platform with resetting over on top of",
    "start": "4966020",
    "end": "4971420"
  },
  {
    "text": "it uh and so you might be wondering I said something",
    "start": "4971420",
    "end": "4976460"
  },
  {
    "text": "like a minute ago where I said well if I predicted something is clothing and apparel at level one it should have a",
    "start": "4976460",
    "end": "4982460"
  },
  {
    "text": "higher likelihood of being the clock or a shirt or a pant at level two and not a",
    "start": "4982460",
    "end": "4988280"
  },
  {
    "text": "drill bit right like you can very justifiably at this point ask me but okay it can never ever be a drill but it",
    "start": "4988280",
    "end": "4994820"
  },
  {
    "text": "should always be either a shirt or a bat right and you're right like eventually when we expose this to our consumers like if I predict clothing at level one",
    "start": "4994820",
    "end": "5002260"
  },
  {
    "text": "I should never ever predict a drill bit at level two but during training at least the training is completely",
    "start": "5002260",
    "end": "5008920"
  },
  {
    "text": "taxonomy unaware as in we allow the model the flexibility to say hey if you if you think it's drill bit at level two",
    "start": "5008920",
    "end": "5016060"
  },
  {
    "text": "and if you're confident you can do it and there's actually a reason for this because if you if you enforce the condition at",
    "start": "5016060",
    "end": "5023140"
  },
  {
    "text": "the very beginning then the model will never recover from uh from a from a catastrophic failure right as in if it",
    "start": "5023140",
    "end": "5029800"
  },
  {
    "text": "gets the level one prediction wrong it's never going to recover below it'll always be long below right whereas if",
    "start": "5029800",
    "end": "5035140"
  },
  {
    "text": "you're kind of during training letter kind of say give it the flexibility to make deviate from the parent prediction",
    "start": "5035140",
    "end": "5041140"
  },
  {
    "text": "a little bit it allows the pair and prediction to catch up due to the back propagation right uh so it's so we did",
    "start": "5041140",
    "end": "5048460"
  },
  {
    "text": "all of that for training right for inference these were the requirements we had right uh I already mentioned we have",
    "start": "5048460",
    "end": "5054179"
  },
  {
    "text": "multiple billions of products historically we've connected tens of billions of images for these products",
    "start": "5054179",
    "end": "5059800"
  },
  {
    "text": "I'm not giving you the exact number because I can't talk about it but we have a massive large-scale data problem",
    "start": "5059800",
    "end": "5066340"
  },
  {
    "text": "right uh and every time we train a new model one of the requirements we had is we have to do a historical backfill of",
    "start": "5066340",
    "end": "5073060"
  },
  {
    "text": "all products right so every product that every any Merchant has ever created a Shopify I have to go back and basically",
    "start": "5073060",
    "end": "5079960"
  },
  {
    "text": "predict on all of them right uh and then furthermore we have tens of millions of products that are either being created",
    "start": "5079960",
    "end": "5085780"
  },
  {
    "text": "or updated on a daily basis all of those I'm basically telling you because dealing with this many images",
    "start": "5085780",
    "end": "5091540"
  },
  {
    "text": "and products is non-trivial like it's extremely hard in fact uh furthermore like I said we have many consumers who",
    "start": "5091540",
    "end": "5097960"
  },
  {
    "text": "want to consume us from different access patterns it's basically spans the whole range right we have consumers who are in",
    "start": "5097960",
    "end": "5104380"
  },
  {
    "text": "batch consumers and streaming consumers in real time we basically have to support all of those different use cases",
    "start": "5104380",
    "end": "5110140"
  },
  {
    "text": "right um so those were our scaling requirements and so I remember I said that's how we",
    "start": "5110140",
    "end": "5117520"
  },
  {
    "text": "train the model how we train the model is slightly different from how we actually use it for making prediction so",
    "start": "5117520",
    "end": "5124600"
  },
  {
    "text": "essentially what we do is uh we chunk the model right we train this one big model and then we chunk it into smaller",
    "start": "5124600",
    "end": "5131140"
  },
  {
    "text": "pieces essentially almost like a Lego right so we want to make like we take this big big model and chunk it into smaller Lego",
    "start": "5131140",
    "end": "5137320"
  },
  {
    "text": "pieces and then we want the idea behind is each chunk can now be deployed as a",
    "start": "5137320",
    "end": "5142360"
  },
  {
    "text": "standalone model because then you can kind of go hey I've already seen this image before so maybe I don't have to",
    "start": "5142360",
    "end": "5148120"
  },
  {
    "text": "create a new image embedding for it I can just reuse it from the image embedding cache right or I've seen this text before so let's not re-embed that",
    "start": "5148120",
    "end": "5155800"
  },
  {
    "text": "text right so it gives us that kind of benefit or in the real-time case where you don't want to make these calls to go",
    "start": "5155800",
    "end": "5161080"
  },
  {
    "text": "and fetch things from the cache and you want to do it right away then you also have this and this big the combined",
    "start": "5161080",
    "end": "5166600"
  },
  {
    "text": "model that's the combination of all these chunks that can also be deployed as a standalone service right uh so we",
    "start": "5166600",
    "end": "5172540"
  },
  {
    "text": "start off initially with the two embedding layers those are deployed separately as Standalone models",
    "start": "5172540",
    "end": "5177820"
  },
  {
    "text": "then we have what we essentially call the embedding to probability this is the mod this is the chunk of it right so",
    "start": "5177820",
    "end": "5183280"
  },
  {
    "text": "this is going from okay you've given me the text embedding and the image embedding and from these I'm going to essentially give you logic scores of all",
    "start": "5183280",
    "end": "5191020"
  },
  {
    "text": "of the categories right and from there remember I told you that the model is",
    "start": "5191020",
    "end": "5196060"
  },
  {
    "text": "taxonomy unaware we then impose business rules on top of it right we essentially say hey if you predict at level one",
    "start": "5196060",
    "end": "5203080"
  },
  {
    "text": "clothing and accessories you cannot predict level two as uh uh screwdriver",
    "start": "5203080",
    "end": "5208840"
  },
  {
    "text": "right so that that's where we kind of come back and we impose the taxonomy on it and so we have all of these pieces",
    "start": "5208840",
    "end": "5213940"
  },
  {
    "text": "right so the first two pieces are embeddingly embedding models essentially second one is take the embeddings make",
    "start": "5213940",
    "end": "5219820"
  },
  {
    "text": "core predictions and then the third ones take the scores apply business Logic on it to give the final refined protection",
    "start": "5219820",
    "end": "5225639"
  },
  {
    "text": "that we expose to our consumers right now each of this you can basically take any combination of this and deploy it as",
    "start": "5225639",
    "end": "5232000"
  },
  {
    "text": "and when we need and we've actually deployed multiple combination permutations and combinations of this as",
    "start": "5232000",
    "end": "5237159"
  },
  {
    "text": "and when need it for the use case so not gonna stick",
    "start": "5237159",
    "end": "5242679"
  },
  {
    "text": "um yeah we I'm not going to jump too much into this we've already spoken about it this is where we kind of come and say during",
    "start": "5242679",
    "end": "5249520"
  },
  {
    "text": "inference we basically say hey give me the prediction at level one with the highest probability let's say that's",
    "start": "5249520",
    "end": "5254980"
  },
  {
    "text": "clothing and accessories then I go back to level two and I say out of all of this or you I have scores",
    "start": "5254980",
    "end": "5261460"
  },
  {
    "text": "for every node at level two but I basically then say if I predicted clothing at level one give me only the",
    "start": "5261460",
    "end": "5268239"
  },
  {
    "text": "scores of the children of clothing and then take the max of that right so that's how I impose the taxonomy during",
    "start": "5268239",
    "end": "5273580"
  },
  {
    "text": "uh inference and this is the slide that talks about that I'm I'm jumping a little bit here just",
    "start": "5273580",
    "end": "5279639"
  },
  {
    "text": "to kind of get to the interesting Ray Brit in a second right uh and then we also have some thresholding built into",
    "start": "5279639",
    "end": "5285040"
  },
  {
    "text": "this right so because at the end of the day it's a stochastic model it can make mistakes we we want to kind of limit we",
    "start": "5285040",
    "end": "5291400"
  },
  {
    "text": "would rather not make a prediction than make a wrong prediction especially in certain use cases like I don't want to",
    "start": "5291400",
    "end": "5297340"
  },
  {
    "text": "wrongly predict something if I'm using it for taxation right uh so we do have some thresholding built in where we kind",
    "start": "5297340",
    "end": "5302679"
  },
  {
    "text": "of go okay I'll go level one level two level three but at level four I'm not confident enough and I'll kind of say",
    "start": "5302679",
    "end": "5308199"
  },
  {
    "text": "that's all I'm exposing so you for in this in this example you see that we're actually fairly confident about the",
    "start": "5308199",
    "end": "5315040"
  },
  {
    "text": "first three levels and then when we get to Dog apparel we're not confident and we kind of go I'm not exposing that to",
    "start": "5315040",
    "end": "5320739"
  },
  {
    "text": "the consumer uh so let's get to the interesting part right so what tools did we use Rey is actually a",
    "start": "5320739",
    "end": "5328060"
  },
  {
    "text": "very Central component of what we did because the mo the ml platform for Shopify which is called Merlin is",
    "start": "5328060",
    "end": "5334120"
  },
  {
    "text": "essentially built on top of Ray and as a consumer I interact with Merlin right but that also means I'm using Ray in the",
    "start": "5334120",
    "end": "5340600"
  },
  {
    "text": "background right uh we use a few other tools as well uh spark was essentially",
    "start": "5340600",
    "end": "5345880"
  },
  {
    "text": "the Charlie spark is the choice of uh tool for any kind of data wrangling a Shopify so a lot of our pre-processing",
    "start": "5345880",
    "end": "5352719"
  },
  {
    "text": "was done at uh feature engineering is done in spark we already spoke about tensorflow and tensorflow transforms and",
    "start": "5352719",
    "end": "5359199"
  },
  {
    "text": "the tensorflow extended ecosystem um our infrastructure layer is Google",
    "start": "5359199",
    "end": "5364360"
  },
  {
    "text": "Cloud uh we also used a little bit of Pub sub and Cloud functions essentially for us to make an image pipeline right",
    "start": "5364360",
    "end": "5371080"
  },
  {
    "text": "so remember this this thing consumes images so we needed an ability to kind of move images efficiently from our",
    "start": "5371080",
    "end": "5378040"
  },
  {
    "text": "cloud storage location to something that's accessible by the model so we use the we built a com we built a custom",
    "start": "5378040",
    "end": "5384580"
  },
  {
    "text": "image embedding image ingestion pipeline right um so here's the interesting part",
    "start": "5384580",
    "end": "5391840"
  },
  {
    "text": "um so Isaac who was from Shopify is actually given a great talk in last year's array",
    "start": "5391840",
    "end": "5397420"
  },
  {
    "text": "Summit about how we built the Merlin uh platform on top of Ray and if you haven't already watched it I hope",
    "start": "5397420",
    "end": "5403300"
  },
  {
    "text": "there's a recording of it um you should uh this is a very high level overview of",
    "start": "5403300",
    "end": "5409659"
  },
  {
    "text": "what are data infrastructure looks like at Shopify right uh so all of data like",
    "start": "5409659",
    "end": "5415120"
  },
  {
    "text": "I mentioned already it's built on top of the Google Cloud platform right and we've had for a few years now a mature",
    "start": "5415120",
    "end": "5420820"
  },
  {
    "text": "data platform which has been tried and tested for many many years right and this is usually enabled our data",
    "start": "5420820",
    "end": "5425920"
  },
  {
    "text": "scientists to consume a wide range of data sources right uh We've enabled our",
    "start": "5425920",
    "end": "5431739"
  },
  {
    "text": "data scientist to process data using either spark SQL or Flink based on specific needs of that team and that",
    "start": "5431739",
    "end": "5436840"
  },
  {
    "text": "project right but over the last few years we've realized that ml use cases have different needs that are in",
    "start": "5436840",
    "end": "5442420"
  },
  {
    "text": "addition to what was already being provided by our existing data platform right and this is what led us to",
    "start": "5442420",
    "end": "5448179"
  },
  {
    "text": "building our new ml platform which is called Berlin that's focused on solving these specific needs to complement the",
    "start": "5448179",
    "end": "5454540"
  },
  {
    "text": "existing data platform and so this is where Merlin comes in right uh Merlin's",
    "start": "5454540",
    "end": "5460420"
  },
  {
    "text": "built again it's built on our gcp system there is a kubernetes like we have a kubernetes platform and Ray sits on top",
    "start": "5460420",
    "end": "5466719"
  },
  {
    "text": "of the that current of what Ray enables us to do is as a data scientist I",
    "start": "5466719",
    "end": "5472300"
  },
  {
    "text": "basically go to our ml platform and I say hey I want to I want to either prototype something or I either want to",
    "start": "5472300",
    "end": "5477699"
  },
  {
    "text": "run something in production and I essentially give a configuration file that says hey this",
    "start": "5477699",
    "end": "5483219"
  },
  {
    "text": "many workers this money gpus this much memory essentially it's a config driven system where I give a config file and",
    "start": "5483219",
    "end": "5489280"
  },
  {
    "text": "say these are the hardware requirements here is the code here are the software requirements I essentially package all",
    "start": "5489280",
    "end": "5495520"
  },
  {
    "text": "of that put that up in a repo send it over to Merlin and Merlin can then run",
    "start": "5495520",
    "end": "5500739"
  },
  {
    "text": "that in the hardware with the hardware and the software that I have specified",
    "start": "5500739",
    "end": "5505900"
  },
  {
    "text": "right the the concept of that is called a Merlin workspace or array workspace for us right uh and the benefit of doing",
    "start": "5505900",
    "end": "5512920"
  },
  {
    "text": "it this way is that whatever I whatever environment my thing is going to run in in production is the",
    "start": "5512920",
    "end": "5520719"
  },
  {
    "text": "exact same environment I'm going to develop or prototype any of this right so when I'm prototyping I'm confident",
    "start": "5520719",
    "end": "5526540"
  },
  {
    "text": "that the the libraries and the hardware that it's running on is exactly the same thing as I'm what I'm going to run in",
    "start": "5526540",
    "end": "5532060"
  },
  {
    "text": "production right uh and so we have Ray and then we have Ray workspaces right uh and on and Ray workspaces on top of that",
    "start": "5532060",
    "end": "5538960"
  },
  {
    "text": "so it's our code and our training and our prediction all sits there right uh outside of the way workspaces we also",
    "start": "5538960",
    "end": "5546520"
  },
  {
    "text": "have complementary products within Merlin so we have a model registry which uses Comet we have an online inference",
    "start": "5546520",
    "end": "5553900"
  },
  {
    "text": "service today the online influence Services Bare Bones and it's a micro Services uh architecture based but for",
    "start": "5553900",
    "end": "5560320"
  },
  {
    "text": "some of our more complex use cases like product classification we are currently investigating we're actually prototyping",
    "start": "5560320",
    "end": "5567040"
  },
  {
    "text": "to see if we can use Reserve to do things like inference crafts that Jules mentioned uh and then we also have a",
    "start": "5567040",
    "end": "5574360"
  },
  {
    "text": "panel which is our feature store this is built on top of feast right so we have model registry we have online influence",
    "start": "5574360",
    "end": "5581620"
  },
  {
    "text": "service we have Pano but all of these revolve around the Merlin core system which is built on Ray and then we have",
    "start": "5581620",
    "end": "5588159"
  },
  {
    "text": "the workspaces um so this is kind of the rough idea of how",
    "start": "5588159",
    "end": "5593199"
  },
  {
    "text": "our data infrastructure looks like again for more details I am I am not a platform engineer or a data engineer I",
    "start": "5593199",
    "end": "5599440"
  },
  {
    "text": "am a consumer of this for more details on this I highly encourage you guys to look at the talk we also have a blog",
    "start": "5599440",
    "end": "5605560"
  },
  {
    "text": "post about it the Riser gave uh in last year's array suffered",
    "start": "5605560",
    "end": "5610780"
  },
  {
    "text": "um so we started like I mentioned we started working on this project over a year ago and at that point I do not",
    "start": "5610780",
    "end": "5617020"
  },
  {
    "text": "believe that Ray 2.0 was released yet right we do have plans to kind of migrate over to Ray 2.0 at some point",
    "start": "5617020",
    "end": "5623219"
  },
  {
    "text": "but uh as of today uh we use raytrain for training uh reactive pools for batch",
    "start": "5623219",
    "end": "5629500"
  },
  {
    "text": "batch inference and then uh for online inference we use our microservices based",
    "start": "5629500",
    "end": "5634960"
  },
  {
    "text": "architecture so this this is actually my favorite part",
    "start": "5634960",
    "end": "5640120"
  },
  {
    "text": "of the presentation right like typically at the end of a presentation about machine learning people will kind of say",
    "start": "5640120",
    "end": "5646060"
  },
  {
    "text": "oh I increased accuracy by this much or I I just might recall is so much my Precision so much right",
    "start": "5646060",
    "end": "5652179"
  },
  {
    "text": "and they did all of that like I'm not going to say we didn't we did all of that but to me that's not actually the",
    "start": "5652179",
    "end": "5657340"
  },
  {
    "text": "most important part of why you would want to do ml at the scale in Shopify right remember that um",
    "start": "5657340",
    "end": "5664000"
  },
  {
    "text": "admin pages that we saw before right the one we saw earlier where we had all of that unstructured data let's go",
    "start": "5664000",
    "end": "5669639"
  },
  {
    "text": "reacquaint ourselves right like nothing has changed right it's the same image there's a lot of unstructured data at a",
    "start": "5669639",
    "end": "5674739"
  },
  {
    "text": "high level it looks the same right and then we made a change right all the",
    "start": "5674739",
    "end": "5680260"
  },
  {
    "text": "change you see here it looks kind of the same you the one small difference is you we've introduced this new field there",
    "start": "5680260",
    "end": "5686800"
  },
  {
    "text": "called product category right uh and you'll see that it's actually blue whatever is filled there is in blue what",
    "start": "5686800",
    "end": "5693219"
  },
  {
    "text": "that means is actually that's our prediction that's the model actually predicting something and it's put in",
    "start": "5693219",
    "end": "5698440"
  },
  {
    "text": "front of the merchant right so as the merch is typing as you're creating something let's say you type up and say dog bed immediately there's a prediction",
    "start": "5698440",
    "end": "5705520"
  },
  {
    "text": "there that says we think it's this right and as the merchant gives us more and more information in real time we keep",
    "start": "5705520",
    "end": "5711100"
  },
  {
    "text": "updating our prediction now the merchant has a choice right like they can do one of three things they can either accept",
    "start": "5711100",
    "end": "5717280"
  },
  {
    "text": "what we're what we're giving them which is great like that's the best case uh thing that's the best case scenario they",
    "start": "5717280",
    "end": "5723460"
  },
  {
    "text": "can if they don't accept they can come and edit what we've said right so if I've said dog bed here they can come",
    "start": "5723460",
    "end": "5728860"
  },
  {
    "text": "click on it and go through the taxonomy and then they can correct me and say no I don't think it's a dog bed I think it's a cat bed instead which is also",
    "start": "5728860",
    "end": "5736060"
  },
  {
    "text": "great because a they've engaged it doesn't have given us what is truly what the true category of it is and we can",
    "start": "5736060",
    "end": "5742540"
  },
  {
    "text": "use that for use cases down the line B it also gives us new training data right and so our model continuously now gets",
    "start": "5742540",
    "end": "5749080"
  },
  {
    "text": "better as Merchants interact with us right and finally they can ignore us right we hope they don't do that but",
    "start": "5749080",
    "end": "5755440"
  },
  {
    "text": "there are certain use cases for which we cannot legally use the prediction and we need the merchants either correcting us",
    "start": "5755440",
    "end": "5761620"
  },
  {
    "text": "or accepting our prediction right uh but this actually happened and so this immediately now as of today this gets",
    "start": "5761620",
    "end": "5767980"
  },
  {
    "text": "used for surge it gets used for uh recommender systems it gets used for automatic tax calculations it gets used",
    "start": "5767980",
    "end": "5774940"
  },
  {
    "text": "for channel things there's a bunch of like automations and workflows that we immediately built on top of product",
    "start": "5774940",
    "end": "5780159"
  },
  {
    "text": "classification that benefits our Merchants right away right uh and there's also remember that feedback loop",
    "start": "5780159",
    "end": "5785320"
  },
  {
    "text": "and this is automatically getting better as Merchants use it more and more right uh and all of this couldn't have",
    "start": "5785320",
    "end": "5791980"
  },
  {
    "text": "happened without Merlin and Ray sitting underneath that today uh so again this",
    "start": "5791980",
    "end": "5797860"
  },
  {
    "text": "is the honestly the least important slide for me right which is how do you measure the ml metrics",
    "start": "5797860",
    "end": "5803199"
  },
  {
    "text": "we have the usual uh metrics that you would use for a classification problem the only Wrinkle in this is It's a",
    "start": "5803199",
    "end": "5809199"
  },
  {
    "text": "hierarchical classification right so normally people talk Precision recall uh",
    "start": "5809199",
    "end": "5814600"
  },
  {
    "text": "Precision recall we talk hierarchical Precision hierarchical recall we have coverage lowest common ancestor all of",
    "start": "5814600",
    "end": "5821139"
  },
  {
    "text": "those are fairly self-explanatory again more important product metrics adoption and Merchant acceptance right",
    "start": "5821139",
    "end": "5827440"
  },
  {
    "text": "like how many Merchants actually use us and how is that then Downstream uh being consumed right uh",
    "start": "5827440",
    "end": "5833980"
  },
  {
    "text": "so what comes next well we have to evolve and adapt the talks on taxonomy like the taxonomy isn't like the Google",
    "start": "5833980",
    "end": "5840520"
  },
  {
    "text": "product taxonomy was a great starting point for us right like it starts off but it was built for it was built by",
    "start": "5840520",
    "end": "5846400"
  },
  {
    "text": "Google for Google right and we quickly realized our needs are a little bit more different than the health right and so",
    "start": "5846400",
    "end": "5853179"
  },
  {
    "text": "we've actually already moved away from the Google taxonomy we've used them as a starting point and then kind of evolved",
    "start": "5853179",
    "end": "5859179"
  },
  {
    "text": "from there right uh um one of the other things is not all",
    "start": "5859179",
    "end": "5864940"
  },
  {
    "text": "consumers are made the same right for example taxation will say okay tell me exactly",
    "start": "5864940",
    "end": "5871060"
  },
  {
    "text": "what this product is because I need to know right whereas uh trust and safety will come and tell me I don't really care about the difference between a",
    "start": "5871060",
    "end": "5878139"
  },
  {
    "text": "shirt and a pant but I really care if they're selling guns right like they only care about certain specific",
    "start": "5878139",
    "end": "5883659"
  },
  {
    "text": "categories so different consumers have different uh use uh requirements so we",
    "start": "5883659",
    "end": "5888760"
  },
  {
    "text": "instead of having this one model that fits everything we could potentially have one core model and then multiple",
    "start": "5888760",
    "end": "5894219"
  },
  {
    "text": "flavors of head sitting on top of that to kind of uh service many of those consumers and then",
    "start": "5894219",
    "end": "5899980"
  },
  {
    "text": "yeah the last point we want to kind of there is the tuning in the Middle where we kind of say remember when I said we",
    "start": "5899980",
    "end": "5905920"
  },
  {
    "text": "stop we do early stopping that threshold tuning is currently manual we want to kind of use Merchant feedback to feed",
    "start": "5905920",
    "end": "5912040"
  },
  {
    "text": "feed into it right uh and so these are things we want to do right ah so I did want to stop a little early to kind of",
    "start": "5912040",
    "end": "5918159"
  },
  {
    "text": "give time for questions so yeah if you have questions I would be more than willing to answer them today",
    "start": "5918159",
    "end": "5924159"
  },
  {
    "text": "well thanks a lot k for this fascinating Journey uh how you actually sort of decouple you know the data science group",
    "start": "5924159",
    "end": "5930880"
  },
  {
    "text": "we're actually working on the actual building of the actual model and the infrastructure which is completely",
    "start": "5930880",
    "end": "5937120"
  },
  {
    "text": "abstract in a way so you as a data scientist when you're actually building this particular models do you actually",
    "start": "5937120",
    "end": "5943239"
  },
  {
    "text": "see any do you write any rate code like actors or anything or is that completely abstracted away in in the modeling",
    "start": "5943239",
    "end": "5948580"
  },
  {
    "text": "workspace uh we do write summary code um not that race collect because we",
    "start": "5948580",
    "end": "5953739"
  },
  {
    "text": "still have to kind of like I said Ray train right so we enactment pools we still have to write that code to kind of",
    "start": "5953739",
    "end": "5959880"
  },
  {
    "text": "use some Ray core so it doesn't completely abstract it away is the Ops part of it okay okay so I",
    "start": "5959880",
    "end": "5969460"
  },
  {
    "text": "don't have to man like I don't have to create a cluster or I don't have to kind of like make sure that it auto scales like all of that was taken care of by",
    "start": "5969460",
    "end": "5975880"
  },
  {
    "text": "the the infra team and the platform team my thing is okay I know I need to use",
    "start": "5975880",
    "end": "5980920"
  },
  {
    "text": "rare in a way that's minimally invasive to the rest of my code right so on this",
    "start": "5980920",
    "end": "5986080"
  },
  {
    "text": "model predates us using Ray right like right before we use today and in fact it",
    "start": "5986080",
    "end": "5991600"
  },
  {
    "text": "was the first Model that moved to Merlin when we used today well actually a piece of cake to move it today and which is a",
    "start": "5991600",
    "end": "5997420"
  },
  {
    "text": "great uh which is great feedback that I can give for you because the migration for as a user for me to use rare wasn't",
    "start": "5997420",
    "end": "6004440"
  },
  {
    "text": "actually right it was like a few lines of code that we changed and boom it works now in rare right um so yeah like",
    "start": "6004440",
    "end": "6010800"
  },
  {
    "text": "it's not that we don't use red it's minimal remember you you didn't mention about",
    "start": "6010800",
    "end": "6016199"
  },
  {
    "text": "how you're actually doing batch inference at scale you're using Ray tasks to be able to do that and then you're using Ray actor pools and I think",
    "start": "6016199",
    "end": "6022739"
  },
  {
    "text": "these are some of the things that we also have have even not only in 2.0 but it also is part of the gray Air there",
    "start": "6022739",
    "end": "6029460"
  },
  {
    "text": "are two ways to so you can actually do imprints it you can actually use batch imprints in a master scale using the",
    "start": "6029460",
    "end": "6034739"
  },
  {
    "text": "record apis that you guys are using right you're using rate tasks to detaching printer using red pool actors",
    "start": "6034739",
    "end": "6041280"
  },
  {
    "text": "to be able to do the batch inference or you can actually use the you know Ray data sets and you can use the array air",
    "start": "6041280",
    "end": "6046920"
  },
  {
    "text": "predictor to do badge in prints so there are sort of two ways to actually do that and as you say you guys predated right",
    "start": "6046920",
    "end": "6053100"
  },
  {
    "text": "here so you didn't have the the the the the ability to to take advantage of that",
    "start": "6053100",
    "end": "6058880"
  },
  {
    "text": "I'm delighted to hear that you guys are thinking of of of uh migrating to what",
    "start": "6058880",
    "end": "6064980"
  },
  {
    "text": "uh the ray air and and 2.0 right and exactly and also remember uh we are one",
    "start": "6064980",
    "end": "6070800"
  },
  {
    "text": "level away from Ray right as in Merlin support like our platform has to support",
    "start": "6070800",
    "end": "6076860"
  },
  {
    "text": "specific versions of Ray so the day my platform team comes to me and says hey uh where uh you have to move to Ray 2.0",
    "start": "6076860",
    "end": "6084239"
  },
  {
    "text": "that's when I'm going to have the discussion of migrating it over because otherwise there is no product benefit to",
    "start": "6084239",
    "end": "6089280"
  },
  {
    "text": "moving to data point right so as of today it works with wave one point x so I'm like yeah this works great uh having",
    "start": "6089280",
    "end": "6096900"
  },
  {
    "text": "said that for newer projects that we've tried like we have moved on to Ray 2.0",
    "start": "6096900",
    "end": "6102840"
  },
  {
    "text": "right 2.0 but again this is the the one of the pain points that I always",
    "start": "6102840",
    "end": "6108840"
  },
  {
    "text": "complain about when there are breaking changes like all of those changes between Ray one point x to two point x is kind of",
    "start": "6108840",
    "end": "6115020"
  },
  {
    "text": "what happens to all of those things where we built it now that becomes a migration task over which right right",
    "start": "6115020",
    "end": "6120060"
  },
  {
    "text": "inherently have product value associated with it right right right and I think one of the things we have we are doing",
    "start": "6120060",
    "end": "6126060"
  },
  {
    "text": "diligently is to make sure that we actually provide migration paths uh so the API is to set an extent work uh",
    "start": "6126060",
    "end": "6132780"
  },
  {
    "text": "Backward Compatible but at some point obviously they're going to be deprecated uh but this is actually a a good good",
    "start": "6132780",
    "end": "6140159"
  },
  {
    "text": "use case that you actually um telling us and sharing with us there's a question from swarup who says",
    "start": "6140159",
    "end": "6146760"
  },
  {
    "text": "what would you say is difference in velocity of model development time with",
    "start": "6146760",
    "end": "6152100"
  },
  {
    "text": "or without a new experience since you're the consumer of that um",
    "start": "6152100",
    "end": "6157619"
  },
  {
    "text": "I mean I don't have a I don't have a quantitative number yeah yeah but uh",
    "start": "6157619",
    "end": "6165179"
  },
  {
    "text": "anecdotally I can tell you for me and my team there is an increase like just that one point where I uh that I mentioned",
    "start": "6165179",
    "end": "6171360"
  },
  {
    "text": "where hey like at least the way Merlin is using Rey where it kind of uh",
    "start": "6171360",
    "end": "6177420"
  },
  {
    "text": "where I I'm saying hey I have the exact same like the library the software and",
    "start": "6177420",
    "end": "6182820"
  },
  {
    "text": "the hardware equal like environment uh in production as I will have in uh prototyping that is massive like I",
    "start": "6182820",
    "end": "6189540"
  },
  {
    "text": "cannot understand how massive uh difference that is right because typically Dev and staging environments",
    "start": "6189540",
    "end": "6195420"
  },
  {
    "text": "are less public you probably see less of the data it has less resources and then you kind of have to move that over to",
    "start": "6195420",
    "end": "6201239"
  },
  {
    "text": "production and then there's a like there is some translational involved but I have no such problems right like in fact",
    "start": "6201239",
    "end": "6206639"
  },
  {
    "text": "we even have uh examples where we kind of go hey some data scientists train something in some notebook ad hoc just",
    "start": "6206639",
    "end": "6214320"
  },
  {
    "text": "know it'll work right we can even find like even copy it over and say like use it through the registry in production right so the ability to kind of have the",
    "start": "6214320",
    "end": "6222420"
  },
  {
    "text": "same environment is massive and I think created like array enables a lot of that and I also find",
    "start": "6222420",
    "end": "6228840"
  },
  {
    "text": "myself using Ray quite a bit for things like monitoring like I use the dashboards like all of that is like a big win not to say it didn't exist",
    "start": "6228840",
    "end": "6235440"
  },
  {
    "text": "before but that one single place where it all exists and is God said to be honest",
    "start": "6235440",
    "end": "6241440"
  },
  {
    "text": "right and so have I Quantified it no but has it has it increased our velocity 100",
    "start": "6241440",
    "end": "6247880"
  },
  {
    "text": "and just just just to uh add an adductory which you which you said and",
    "start": "6247880",
    "end": "6253139"
  },
  {
    "text": "just to to give a my perspective on what swarup is asking is that I think you",
    "start": "6253139",
    "end": "6258780"
  },
  {
    "text": "know the the goal and the mission of Ray is is twofold one is to make distributed",
    "start": "6258780",
    "end": "6263820"
  },
  {
    "text": "computing programming not easy and very pythonic and anybody can actually do that distributed campaigning is not easy",
    "start": "6263820",
    "end": "6270179"
  },
  {
    "text": "right in the old days you actually had to write MPI code and actually had to write very complicated code to actually",
    "start": "6270179",
    "end": "6275760"
  },
  {
    "text": "understand you need to worry about full tolerance you have to worry about memory management as as clearance actually",
    "start": "6275760",
    "end": "6281040"
  },
  {
    "text": "mentioned all these are different aspects of it today with Ray all the heartbeat is actually taken care of it",
    "start": "6281040",
    "end": "6287580"
  },
  {
    "text": "and so what we provide you is with these Primitives that allows you to actually write distributed applications very",
    "start": "6287580",
    "end": "6293820"
  },
  {
    "text": "easily with with rare tasks and reactors so in that respect the velocity is just",
    "start": "6293820",
    "end": "6298860"
  },
  {
    "text": "astronomically faster compared to eventually we're doing writing we're competing in in the older way because of",
    "start": "6298860",
    "end": "6306300"
  },
  {
    "text": "the fact that we have extracted away with Primitives that that allow you to do that and then we take uh uh we do all",
    "start": "6306300",
    "end": "6313260"
  },
  {
    "text": "the heartbeat about scheduling the task about about um making sure there's a fault tolerance",
    "start": "6313260",
    "end": "6319500"
  },
  {
    "text": "about retrying but giving you the ability to actually look into as Clarence mentioned about so yes there is",
    "start": "6319500",
    "end": "6324540"
  },
  {
    "text": "there is a hip Improvement and velocity in terms of how fast you can actually write distributed applications using Ray",
    "start": "6324540",
    "end": "6330960"
  },
  {
    "text": "and and it's very pytonic I think of it is really rages for the compute is an",
    "start": "6330960",
    "end": "6336119"
  },
  {
    "text": "extension of python really you can actually yeah very easily if you come from that particular background so",
    "start": "6336119",
    "end": "6342119"
  },
  {
    "text": "this is only half a joke I actually joke to my platform Team all the time that they're actually making it too easy",
    "start": "6342119",
    "end": "6348119"
  },
  {
    "text": "right uh yeah actually it's half a joke because if you're not very careful it's very",
    "start": "6348119",
    "end": "6354960"
  },
  {
    "text": "easy to write non-performant python code and then we're able to distribute it across and then you're just",
    "start": "6354960",
    "end": "6361639"
  },
  {
    "text": "broadcasting your mistakes so it's it's only half a joke but uh you you see what I mean",
    "start": "6361639",
    "end": "6367940"
  },
  {
    "text": "friends did you actually have any any input on um on on is there a difference",
    "start": "6367940",
    "end": "6373199"
  },
  {
    "text": "in velocity between using rare not using right",
    "start": "6373199",
    "end": "6379080"
  },
  {
    "text": "I think you're muted we can't hear you uh yeah I mean definitely uh I think",
    "start": "6379080",
    "end": "6385320"
  },
  {
    "text": "like one of the the big um uh improvements uh you know uh we've",
    "start": "6385320",
    "end": "6391440"
  },
  {
    "text": "been making in the last at least in the last quarter or or more is um like",
    "start": "6391440",
    "end": "6397560"
  },
  {
    "text": "around observability and um it has been one of the the bigger pain points at least",
    "start": "6397560",
    "end": "6404540"
  },
  {
    "text": "learning from users around like because a lot of the development work you know",
    "start": "6404540",
    "end": "6410219"
  },
  {
    "text": "part of it is you know writing code but also a lot of it is around um debugging uh so you know we've",
    "start": "6410219",
    "end": "6416820"
  },
  {
    "text": "actually made a lot of improvements in in that area I think that's I I think that's how like especially",
    "start": "6416820",
    "end": "6424560"
  },
  {
    "text": "like one one of the critical features for example will raise like we have like distributed log collection and and",
    "start": "6424560",
    "end": "6431580"
  },
  {
    "text": "filtering that you know it's not an easy thing to build like you know if I were to just build a",
    "start": "6431580",
    "end": "6437159"
  },
  {
    "text": "framework um that's not using rape it's yeah it's quite a bit of effort to build in so I",
    "start": "6437159",
    "end": "6443639"
  },
  {
    "text": "think one of the nice thing about Ray is that like these like observability things are coming for free and and for",
    "start": "6443639",
    "end": "6450300"
  },
  {
    "text": "the distributed application if if it's not for Ray you know someone will have to re-engineer that",
    "start": "6450300",
    "end": "6456420"
  },
  {
    "text": "uh another question from uh he said have you explored using modem slash disk",
    "start": "6456420",
    "end": "6462600"
  },
  {
    "text": "instead of instead of Pi Spark uh yeah um have I exploded no uh there's a",
    "start": "6462600",
    "end": "6468960"
  },
  {
    "text": "reason for that because uh spark was just a natural choice because the rest of the Shopify data platform is built on",
    "start": "6468960",
    "end": "6474600"
  },
  {
    "text": "spark we have an in-house tool that's an abstraction of spark so 99 of all of the",
    "start": "6474600",
    "end": "6481260"
  },
  {
    "text": "data transformations in Shopify happen on that platform and so spark was just the Natural Choice for us because we had",
    "start": "6481260",
    "end": "6487920"
  },
  {
    "text": "that pool uh having said that there are I know there are other teams who are using uh",
    "start": "6487920",
    "end": "6494159"
  },
  {
    "text": "I think task task on Ray is something that I know there are teams at Shopify using uh have I personally use it no but",
    "start": "6494159",
    "end": "6501540"
  },
  {
    "text": "I know there is a use case for them a question from alexandros uh sharonton",
    "start": "6501540",
    "end": "6507360"
  },
  {
    "text": "great talk uh if you don't mind me asking you how does the model work for different levels of taxonomy depth",
    "start": "6507360",
    "end": "6514380"
  },
  {
    "text": "during training yeah um yeah so if I understand that question",
    "start": "6514380",
    "end": "6519780"
  },
  {
    "text": "correctly you're saying not every branch is equal right so for example t-shirts is only level three but lateral net care",
    "start": "6519780",
    "end": "6526619"
  },
  {
    "text": "accessories is level seven how do I deal with it um actually it was very simple",
    "start": "6526619",
    "end": "6531960"
  },
  {
    "text": "we just introduced dummy variable so if if the label ends at level three then we introduce a dummy variable at level four",
    "start": "6531960",
    "end": "6538619"
  },
  {
    "text": "through seven so the model will always predict all seven levels except if it",
    "start": "6538619",
    "end": "6543659"
  },
  {
    "text": "ends at level three it will then say everything it will predict the dummy variable for every other level after that right uh does that this does",
    "start": "6543659",
    "end": "6550260"
  },
  {
    "text": "introduce some imbalance data set problems right uh we already have imbalanced data set problems this",
    "start": "6550260",
    "end": "6556440"
  },
  {
    "text": "amplifies it a little bit um but we there are techniques to kind of overcome that right but essentially",
    "start": "6556440",
    "end": "6562199"
  },
  {
    "text": "the model will always predict seven levels of prediction but it would kind of say hey this is clothing shirts",
    "start": "6562199",
    "end": "6569940"
  },
  {
    "text": "t-shirts and then it will say dummy dummy dummy after that um yeah I think there's a it's a",
    "start": "6569940",
    "end": "6575159"
  },
  {
    "text": "multi-part question I guess that I just asked you the first part if you actually go to the Q a question you can actually see it where do you only consider the",
    "start": "6575159",
    "end": "6583199"
  },
  {
    "text": "top layers that correspond to the level indicated by your labels example fashion goes to dresses only use out code and if",
    "start": "6583199",
    "end": "6592679"
  },
  {
    "text": "so how does the fact that different parts of the model are trained for a different number of iterations affect",
    "start": "6592679",
    "end": "6598260"
  },
  {
    "text": "the overall performance yeah you actually want to get your feet are you are you on the Q a you can actually look at the panel just click on the queue in",
    "start": "6598260",
    "end": "6604920"
  },
  {
    "text": "the panel yeah I see it I see that okay you see it all right yeah yeah um yeah I think it's that I think fundamentally like I said",
    "start": "6604920",
    "end": "6612540"
  },
  {
    "text": "because we introduced the dummy variable um it always it's the same number of steps and iterations for every product",
    "start": "6612540",
    "end": "6619500"
  },
  {
    "text": "in the pairing and the prediction set right uh so the number of iterations is the same",
    "start": "6619500",
    "end": "6624540"
  },
  {
    "text": "we do feed them right so during training we'll kind of feed the image to the first level get whatever logic scores at",
    "start": "6624540",
    "end": "6631260"
  },
  {
    "text": "level one feed that into level two level three we will continue to feed it into level four five and six right except the",
    "start": "6631260",
    "end": "6638400"
  },
  {
    "text": "target variable is now a dummy variable that we've introduced and it says hey this is almost think about it as unknown",
    "start": "6638400",
    "end": "6643980"
  },
  {
    "text": "class right so we've introduced the spake unknown class and so the model will just that's a legitimate",
    "start": "6643980",
    "end": "6649800"
  },
  {
    "text": "predictions that the model can make right so it can say hey uh it's unknown class you know it does end up going",
    "start": "6649800",
    "end": "6655260"
  },
  {
    "text": "there um the other thing that actually popped into my mind when you were talking about the merchant classification and you're",
    "start": "6655260",
    "end": "6662699"
  },
  {
    "text": "getting daily images on you know thousands and hundreds of images daily so you actually how often then do you",
    "start": "6662699",
    "end": "6668580"
  },
  {
    "text": "actually retrain your model is it like a nightly basis or because when you actually have new images they need to be",
    "start": "6668580",
    "end": "6674580"
  },
  {
    "text": "classified and they have to go through the entire taxonomy hierarchical to be put in the classification how do you how",
    "start": "6674580",
    "end": "6680280"
  },
  {
    "text": "do you account for the um yeah so we don't retrain the model very",
    "start": "6680280",
    "end": "6686100"
  },
  {
    "text": "often because what used to be a t-shirt before is probably facilities yes yes so in terms",
    "start": "6686100",
    "end": "6692639"
  },
  {
    "text": "of the learning capacity like we probably retrained the model once every quarter like that stuff because it is an",
    "start": "6692639",
    "end": "6699119"
  },
  {
    "text": "expensive model to train right like because it is the whole thing trains for days we have close to half a billion uh",
    "start": "6699119",
    "end": "6706679"
  },
  {
    "text": "pre-labeled data right like products that are labeled so it actually is very intense prediction on the other hand is",
    "start": "6706679",
    "end": "6712320"
  },
  {
    "text": "different right because every time the merchant updates the product we might want to predict on it right so there are",
    "start": "6712320",
    "end": "6718500"
  },
  {
    "text": "both real times like I said real-time and streaming uh real-time streaming and Patch processes so there are consumers",
    "start": "6718500",
    "end": "6724920"
  },
  {
    "text": "where as you are typing like every time you move from one field to another it actually makes a request to the model to",
    "start": "6724920",
    "end": "6732119"
  },
  {
    "text": "say hey here's an image and here's the text give me the prediction there's also a thing where when you hit the save button you've made all of your",
    "start": "6732119",
    "end": "6738300"
  },
  {
    "text": "changes and you hit that save button that goes into a streaming Pipeline and it says Hey the merchants finished saving everything tell me what the final",
    "start": "6738300",
    "end": "6744719"
  },
  {
    "text": "prediction is and then we also have a batch process at the end of it because both real-time and streaming have",
    "start": "6744719",
    "end": "6751199"
  },
  {
    "text": "failures like it's basically clean up act at that end right if we missed this",
    "start": "6751199",
    "end": "6756540"
  },
  {
    "text": "many thousand predictions yesterday and the batch process is going to clean it up right uh so we do all of that right",
    "start": "6756540",
    "end": "6762000"
  },
  {
    "text": "uh but prediction happens almost instantaneously when the merchant makes",
    "start": "6762000",
    "end": "6767100"
  },
  {
    "text": "the change training on the other hand we only retrain the model once every uh you can order also so you you don't worry",
    "start": "6767100",
    "end": "6774179"
  },
  {
    "text": "about data ships you're not worrying about any of the uh I mean we monitor it we want it but uh it doesn't ship that",
    "start": "6774179",
    "end": "6781080"
  },
  {
    "text": "much on a day-to-day basis right uh well we have we have one minute and I",
    "start": "6781080",
    "end": "6786900"
  },
  {
    "text": "don't see any last questions since I have the mic I'm not going to ask you a tongue-in chick question okay so I I see",
    "start": "6786900",
    "end": "6792480"
  },
  {
    "text": "those two guitars in the back did you buy them off Shopify do you buy them at Amazon uh these actually printed my timer in",
    "start": "6792480",
    "end": "6800400"
  },
  {
    "text": "fact some of my guitars I build myself right so lovely well that that begs another question do you play the guitar",
    "start": "6800400",
    "end": "6806639"
  },
  {
    "text": "or are they just for display oh lovely well maybe next time we'll have the 10 minutes extra that you can",
    "start": "6806639",
    "end": "6812760"
  },
  {
    "text": "actually entertain us with your with your skill on guitar not only your data scientist you can write code in Python",
    "start": "6812760",
    "end": "6819000"
  },
  {
    "text": "and Ray but you can also play play good music that's a good combination so thanks a lot on on that note will",
    "start": "6819000",
    "end": "6826320"
  },
  {
    "text": "actually ended I really appreciate you uh taking the time to share with you how you're actually using Ray at scale at",
    "start": "6826320",
    "end": "6831659"
  },
  {
    "text": "Shopify and what are some of the challenges and and problems and we hope fully looking forward for you to making",
    "start": "6831659",
    "end": "6837119"
  },
  {
    "text": "migration to Rey air into that though and I won't take uh Clarence as well for giving us an insight and providing our",
    "start": "6837119",
    "end": "6844020"
  },
  {
    "text": "lenses to figure out how to actually debug and and detect our own monitor for",
    "start": "6844020",
    "end": "6849179"
  },
  {
    "text": "those of you who hope to see you again next month again we have an all-exclusive European uh race Meetup",
    "start": "6849179",
    "end": "6855900"
  },
  {
    "text": "that's going to be early in the morning for those of you on this particular time zone but I'm sure we'll have this",
    "start": "6855900",
    "end": "6861659"
  },
  {
    "text": "Cadence on monthly basis and if you have a story to tell us on Ray make sure you",
    "start": "6861659",
    "end": "6867840"
  },
  {
    "text": "actually uh submit your cfp so see you next month have a good night and thanks a lot for all of you",
    "start": "6867840",
    "end": "6875300"
  }
]