[
  {
    "start": "0",
    "end": "170000"
  },
  {
    "text": "welcome to the second race summit",
    "start": "160",
    "end": "2399"
  },
  {
    "text": "connect",
    "start": "2399",
    "end": "3280"
  },
  {
    "text": "my name is dean walper i'm your host",
    "start": "3280",
    "end": "4880"
  },
  {
    "text": "today the subject",
    "start": "4880",
    "end": "6799"
  },
  {
    "text": "is the road to auto ml we've got three",
    "start": "6799",
    "end": "9280"
  },
  {
    "text": "talks lined up for you on specific",
    "start": "9280",
    "end": "11440"
  },
  {
    "text": "topics of hyper parameter tuning",
    "start": "11440",
    "end": "14080"
  },
  {
    "text": "optimizing burt which is a popular",
    "start": "14080",
    "end": "16160"
  },
  {
    "text": "natural language processing model",
    "start": "16160",
    "end": "18400"
  },
  {
    "text": "and neural architecture search",
    "start": "18400",
    "end": "22240"
  },
  {
    "text": "i want to thank our community partners",
    "start": "22800",
    "end": "25039"
  },
  {
    "text": "two of our speakers are from sigopt",
    "start": "25039",
    "end": "27039"
  },
  {
    "text": "and determined ai so we appreciate their",
    "start": "27039",
    "end": "29279"
  },
  {
    "text": "help in promoting this event",
    "start": "29279",
    "end": "31359"
  },
  {
    "text": "the sf big analytics speed up in san",
    "start": "31359",
    "end": "33520"
  },
  {
    "text": "francisco",
    "start": "33520",
    "end": "34480"
  },
  {
    "text": "helped a lot as well as tech stacks in",
    "start": "34480",
    "end": "36960"
  },
  {
    "text": "chicago",
    "start": "36960",
    "end": "37680"
  },
  {
    "text": "slacks and also rise lab which is uh",
    "start": "37680",
    "end": "40879"
  },
  {
    "text": "the uc berkeley pro project that was",
    "start": "40879",
    "end": "43360"
  },
  {
    "text": "actually the",
    "start": "43360",
    "end": "44800"
  },
  {
    "text": "place where rey was born",
    "start": "44800",
    "end": "48079"
  },
  {
    "text": "let me also mention two upcoming events",
    "start": "48719",
    "end": "50800"
  },
  {
    "text": "that we have",
    "start": "50800",
    "end": "52079"
  },
  {
    "text": "uh one is we're going to do some live",
    "start": "52079",
    "end": "54239"
  },
  {
    "text": "training",
    "start": "54239",
    "end": "55199"
  },
  {
    "text": "next week actually a week from today on",
    "start": "55199",
    "end": "56960"
  },
  {
    "text": "reinforcement learning with ray r",
    "start": "56960",
    "end": "58640"
  },
  {
    "text": "liv i'll be teaching that also our next",
    "start": "58640",
    "end": "61199"
  },
  {
    "text": "connect event will also be on",
    "start": "61199",
    "end": "62640"
  },
  {
    "text": "reinforcement learning that will be july",
    "start": "62640",
    "end": "64478"
  },
  {
    "text": "8th",
    "start": "64479",
    "end": "65280"
  },
  {
    "text": "now you can find out more information",
    "start": "65280",
    "end": "67119"
  },
  {
    "text": "about both events and register for them",
    "start": "67119",
    "end": "69200"
  },
  {
    "text": "at anyscale.com",
    "start": "69200",
    "end": "71439"
  },
  {
    "text": "events",
    "start": "71439",
    "end": "73840"
  },
  {
    "text": "a popular question we will post the",
    "start": "74720",
    "end": "77119"
  },
  {
    "text": "slides and videos afterwards probably in",
    "start": "77119",
    "end": "79040"
  },
  {
    "text": "a day or so",
    "start": "79040",
    "end": "79920"
  },
  {
    "text": "uh check the any skill.com blog uh with",
    "start": "79920",
    "end": "83280"
  },
  {
    "text": "uh we'll post uh all the information",
    "start": "83280",
    "end": "85439"
  },
  {
    "text": "there and on that subject actually if",
    "start": "85439",
    "end": "87600"
  },
  {
    "text": "you missed last month's connect event",
    "start": "87600",
    "end": "89600"
  },
  {
    "text": "with",
    "start": "89600",
    "end": "90000"
  },
  {
    "text": "michael jordan and jan stoica uh you can",
    "start": "90000",
    "end": "92960"
  },
  {
    "text": "find the videos",
    "start": "92960",
    "end": "94000"
  },
  {
    "text": "and slides from that event at a blog",
    "start": "94000",
    "end": "97280"
  },
  {
    "text": "post that you'll find",
    "start": "97280",
    "end": "98240"
  },
  {
    "text": "if you go to the anyscale.com blog",
    "start": "98240",
    "end": "102960"
  },
  {
    "text": "okay so the way it's going to work today",
    "start": "103439",
    "end": "105040"
  },
  {
    "text": "is our speakers actually pre-recorded",
    "start": "105040",
    "end": "106799"
  },
  {
    "text": "their talks",
    "start": "106799",
    "end": "108079"
  },
  {
    "text": "and i'm going to play them one at a time",
    "start": "108079",
    "end": "110159"
  },
  {
    "text": "and if you have a question go ahead and",
    "start": "110159",
    "end": "111759"
  },
  {
    "text": "post it in the q",
    "start": "111759",
    "end": "112799"
  },
  {
    "text": "a feature uh in the zoom app uh the",
    "start": "112799",
    "end": "115759"
  },
  {
    "text": "speakers are standing by",
    "start": "115759",
    "end": "117600"
  },
  {
    "text": "uh they'll answer your questions um you",
    "start": "117600",
    "end": "120479"
  },
  {
    "text": "know or some of them will just wait and",
    "start": "120479",
    "end": "122079"
  },
  {
    "text": "answer with",
    "start": "122079",
    "end": "122799"
  },
  {
    "text": "the live panel discussion at the end so",
    "start": "122799",
    "end": "124640"
  },
  {
    "text": "be three talks then the speakers will",
    "start": "124640",
    "end": "126560"
  },
  {
    "text": "join me for a panel",
    "start": "126560",
    "end": "127680"
  },
  {
    "text": "discussion and siri can be quiet now",
    "start": "127680",
    "end": "130479"
  },
  {
    "text": "thank you very much",
    "start": "130479",
    "end": "131760"
  },
  {
    "text": "uh also we'd love to hear feedback about",
    "start": "131760",
    "end": "133760"
  },
  {
    "text": "what you like about this suggestions for",
    "start": "133760",
    "end": "135760"
  },
  {
    "text": "topics we might cover in the future and",
    "start": "135760",
    "end": "137520"
  },
  {
    "text": "so forth and you can send us email at",
    "start": "137520",
    "end": "139280"
  },
  {
    "text": "events",
    "start": "139280",
    "end": "139760"
  },
  {
    "text": "at anyscale.com and if you'd like to",
    "start": "139760",
    "end": "142319"
  },
  {
    "text": "tweet about the event today",
    "start": "142319",
    "end": "144000"
  },
  {
    "text": "you can use the hashtag raysummitconnect",
    "start": "144000",
    "end": "147040"
  },
  {
    "text": "or you could mention one of our twitter",
    "start": "147040",
    "end": "148480"
  },
  {
    "text": "accounts any scale compute or ray",
    "start": "148480",
    "end": "150239"
  },
  {
    "text": "distributed",
    "start": "150239",
    "end": "152879"
  },
  {
    "text": "okay enough administration let me",
    "start": "153120",
    "end": "155360"
  },
  {
    "text": "introduce our first speaker",
    "start": "155360",
    "end": "157120"
  },
  {
    "text": "richard lau is a software engineer at",
    "start": "157120",
    "end": "159040"
  },
  {
    "text": "any scale and the primary developer of",
    "start": "159040",
    "end": "161280"
  },
  {
    "text": "the tune library",
    "start": "161280",
    "end": "162480"
  },
  {
    "text": "and he's going to talk about distributed",
    "start": "162480",
    "end": "164319"
  },
  {
    "text": "hyper parameter tuning",
    "start": "164319",
    "end": "171840"
  },
  {
    "start": "170000",
    "end": "380000"
  },
  {
    "text": "hi my name is richard i'm a software",
    "start": "174879",
    "end": "177360"
  },
  {
    "text": "engineer at nsl",
    "start": "177360",
    "end": "178480"
  },
  {
    "text": "and i've been working on ray for a",
    "start": "178480",
    "end": "179760"
  },
  {
    "text": "couple of years now",
    "start": "179760",
    "end": "181519"
  },
  {
    "text": "today i'm going to be talking to you",
    "start": "181519",
    "end": "183040"
  },
  {
    "text": "about hyperparameter tuning",
    "start": "183040",
    "end": "185200"
  },
  {
    "text": "which i think is machine learning's most",
    "start": "185200",
    "end": "187360"
  },
  {
    "text": "expensive problem",
    "start": "187360",
    "end": "190239"
  },
  {
    "text": "today we'll begin with this talk by",
    "start": "190879",
    "end": "194400"
  },
  {
    "text": "motivating the",
    "start": "194400",
    "end": "195440"
  },
  {
    "text": "importance in highlighting the",
    "start": "195440",
    "end": "197040"
  },
  {
    "text": "complexity of tuning model height",
    "start": "197040",
    "end": "198879"
  },
  {
    "text": "parameters",
    "start": "198879",
    "end": "200560"
  },
  {
    "text": "then we'll overview some of the",
    "start": "200560",
    "end": "201840"
  },
  {
    "text": "state-of-the-art techniques for handling",
    "start": "201840",
    "end": "203280"
  },
  {
    "text": "this complexity",
    "start": "203280",
    "end": "205120"
  },
  {
    "text": "and finally we'll talk about retuning",
    "start": "205120",
    "end": "206879"
  },
  {
    "text": "library built on top of ray for",
    "start": "206879",
    "end": "208239"
  },
  {
    "text": "simplifying",
    "start": "208239",
    "end": "209200"
  },
  {
    "text": "and scaling hyperparameter tuning at the",
    "start": "209200",
    "end": "211680"
  },
  {
    "text": "end of this talk",
    "start": "211680",
    "end": "212560"
  },
  {
    "text": "you should be able to leverage new",
    "start": "212560",
    "end": "214000"
  },
  {
    "text": "algorithms and new frameworks to",
    "start": "214000",
    "end": "216640"
  },
  {
    "text": "accelerate your machine learning",
    "start": "216640",
    "end": "217760"
  },
  {
    "text": "workflow",
    "start": "217760",
    "end": "219920"
  },
  {
    "text": "so let's start by motivating the",
    "start": "219920",
    "end": "221760"
  },
  {
    "text": "importance of hyperparameter tuning",
    "start": "221760",
    "end": "223840"
  },
  {
    "text": "we all know that deep learning is",
    "start": "223840",
    "end": "225360"
  },
  {
    "text": "experiencing rapid growth and adoption",
    "start": "225360",
    "end": "227840"
  },
  {
    "text": "and this is not only happening in",
    "start": "227840",
    "end": "229280"
  },
  {
    "text": "academia which i've presented on the",
    "start": "229280",
    "end": "231040"
  },
  {
    "text": "graph",
    "start": "231040",
    "end": "231599"
  },
  {
    "text": "but also an industry where more and more",
    "start": "231599",
    "end": "233599"
  },
  {
    "text": "applications such as speech recognition",
    "start": "233599",
    "end": "236000"
  },
  {
    "text": "and autonomous vehicles are diverging",
    "start": "236000",
    "end": "238000"
  },
  {
    "text": "deep learning",
    "start": "238000",
    "end": "240000"
  },
  {
    "text": "but let me tell you something there's a",
    "start": "240000",
    "end": "241519"
  },
  {
    "text": "deer dirty secret here",
    "start": "241519",
    "end": "243519"
  },
  {
    "text": "there's a very secret in that there's",
    "start": "243519",
    "end": "246720"
  },
  {
    "text": "actually a lot of hyper hammer tuning",
    "start": "246720",
    "end": "248400"
  },
  {
    "text": "that goes into achieving these",
    "start": "248400",
    "end": "250720"
  },
  {
    "text": "state-of-the-art performances",
    "start": "250720",
    "end": "252879"
  },
  {
    "text": "let's take for example convolutional",
    "start": "252879",
    "end": "254640"
  },
  {
    "text": "neural networks these are very powerful",
    "start": "254640",
    "end": "256720"
  },
  {
    "text": "machine learning techniques and are",
    "start": "256720",
    "end": "258079"
  },
  {
    "text": "attributed to many of the recent",
    "start": "258079",
    "end": "259759"
  },
  {
    "text": "advances",
    "start": "259759",
    "end": "261040"
  },
  {
    "text": "in computer vision on the screen here",
    "start": "261040",
    "end": "263840"
  },
  {
    "text": "i've shown you actually one of the",
    "start": "263840",
    "end": "265120"
  },
  {
    "text": "original designs of convolutional neural",
    "start": "265120",
    "end": "266720"
  },
  {
    "text": "networks by",
    "start": "266720",
    "end": "267520"
  },
  {
    "text": "young likun after 20 years of research",
    "start": "267520",
    "end": "271360"
  },
  {
    "text": "we end up with",
    "start": "271360",
    "end": "272080"
  },
  {
    "text": "alexnet which sparked this new wave of",
    "start": "272080",
    "end": "274479"
  },
  {
    "text": "deep learning that we have now today",
    "start": "274479",
    "end": "278160"
  },
  {
    "text": "what's interesting is that if you",
    "start": "278160",
    "end": "280240"
  },
  {
    "text": "compare the two the basic",
    "start": "280240",
    "end": "282000"
  },
  {
    "text": "idea of convolutional neural networks",
    "start": "282000",
    "end": "284160"
  },
  {
    "text": "hasn't really changed",
    "start": "284160",
    "end": "286160"
  },
  {
    "text": "but what has changed over the last 20",
    "start": "286160",
    "end": "288240"
  },
  {
    "text": "years is that we've largely modified the",
    "start": "288240",
    "end": "290560"
  },
  {
    "text": "shape of the neural networks",
    "start": "290560",
    "end": "292080"
  },
  {
    "text": "and also the size of each layer and",
    "start": "292080",
    "end": "294479"
  },
  {
    "text": "these are essentially what we call hyper",
    "start": "294479",
    "end": "296080"
  },
  {
    "text": "hammers and these",
    "start": "296080",
    "end": "296960"
  },
  {
    "text": "hyper grammars obviously make a huge",
    "start": "296960",
    "end": "298639"
  },
  {
    "text": "difference in",
    "start": "298639",
    "end": "301520"
  },
  {
    "text": "but what's making making matters worse",
    "start": "302960",
    "end": "305360"
  },
  {
    "text": "is is actually two trends",
    "start": "305360",
    "end": "307759"
  },
  {
    "text": "we see that models are getting larger",
    "start": "307759",
    "end": "309440"
  },
  {
    "text": "and larger with",
    "start": "309440",
    "end": "311039"
  },
  {
    "text": "the most recent open ai model containing",
    "start": "311039",
    "end": "313280"
  },
  {
    "text": "nearly 200 billion parameters",
    "start": "313280",
    "end": "317360"
  },
  {
    "text": "also we see that these models are not",
    "start": "317520",
    "end": "320160"
  },
  {
    "text": "only getting larger",
    "start": "320160",
    "end": "321199"
  },
  {
    "text": "but more and more complex so to the",
    "start": "321199",
    "end": "323759"
  },
  {
    "text": "right we see a table of roberta",
    "start": "323759",
    "end": "326400"
  },
  {
    "text": "which has you know 20 different hyper",
    "start": "326400",
    "end": "329039"
  },
  {
    "text": "cameras that they had to tune",
    "start": "329039",
    "end": "331600"
  },
  {
    "text": "just to get a well-performed model",
    "start": "331600",
    "end": "335039"
  },
  {
    "text": "what this means for us is that selecting",
    "start": "335039",
    "end": "336960"
  },
  {
    "text": "the right hyper parameters",
    "start": "336960",
    "end": "338320"
  },
  {
    "text": "in order to achieve state of our",
    "start": "338320",
    "end": "339680"
  },
  {
    "text": "performance is going to take",
    "start": "339680",
    "end": "342160"
  },
  {
    "text": "a long time and consume a lot of",
    "start": "342160",
    "end": "343919"
  },
  {
    "text": "resources",
    "start": "343919",
    "end": "346479"
  },
  {
    "text": "but what's making matters worse is two",
    "start": "348639",
    "end": "350880"
  },
  {
    "text": "trends that we see today",
    "start": "350880",
    "end": "352800"
  },
  {
    "text": "first mods are getting larger and larger",
    "start": "352800",
    "end": "355600"
  },
  {
    "text": "the most recent",
    "start": "355600",
    "end": "356479"
  },
  {
    "text": "open ai model containing nearly 200",
    "start": "356479",
    "end": "358800"
  },
  {
    "text": "building parameters",
    "start": "358800",
    "end": "361440"
  },
  {
    "text": "also these state-of-the-art models are",
    "start": "361440",
    "end": "364000"
  },
  {
    "text": "not only getting larger but also more",
    "start": "364000",
    "end": "365759"
  },
  {
    "text": "complex",
    "start": "365759",
    "end": "367840"
  },
  {
    "text": "what this means for us is that selecting",
    "start": "367840",
    "end": "371360"
  },
  {
    "text": "the right hyper parameters is simply",
    "start": "371360",
    "end": "373759"
  },
  {
    "text": "going to take a long time",
    "start": "373759",
    "end": "375840"
  },
  {
    "text": "and also consume the law of resources",
    "start": "375840",
    "end": "380240"
  },
  {
    "start": "380000",
    "end": "890000"
  },
  {
    "text": "so we arrive at the main problem of",
    "start": "380240",
    "end": "382800"
  },
  {
    "text": "hyperparametering today",
    "start": "382800",
    "end": "384639"
  },
  {
    "text": "it's not simply that how do we choose",
    "start": "384639",
    "end": "387199"
  },
  {
    "text": "these hyperparameters",
    "start": "387199",
    "end": "388560"
  },
  {
    "text": "but rather how do we choose these",
    "start": "388560",
    "end": "390800"
  },
  {
    "text": "hyperparameters",
    "start": "390800",
    "end": "391840"
  },
  {
    "text": "in a way that is both efficient and fast",
    "start": "391840",
    "end": "396720"
  },
  {
    "text": "and today in this talk we'll cover a",
    "start": "396880",
    "end": "399840"
  },
  {
    "text": "two-pronged approach to this problem",
    "start": "399840",
    "end": "403360"
  },
  {
    "text": "first we'll leverage advanced algorithms",
    "start": "403360",
    "end": "407680"
  },
  {
    "text": "to navigate navigate the search space",
    "start": "407680",
    "end": "410960"
  },
  {
    "text": "and second we'll find out how to use",
    "start": "410960",
    "end": "412960"
  },
  {
    "text": "cutting edge software to better leverage",
    "start": "412960",
    "end": "414880"
  },
  {
    "text": "our compute resources",
    "start": "414880",
    "end": "418000"
  },
  {
    "text": "so let's cover three different tuning",
    "start": "418479",
    "end": "421280"
  },
  {
    "text": "techniques",
    "start": "421280",
    "end": "422080"
  },
  {
    "text": "on how we can efficiently navigate their",
    "start": "422080",
    "end": "424720"
  },
  {
    "text": "search space",
    "start": "424720",
    "end": "426000"
  },
  {
    "text": "we'll cover random search we'll cover",
    "start": "426000",
    "end": "428720"
  },
  {
    "text": "bandit based approaches",
    "start": "428720",
    "end": "430479"
  },
  {
    "text": "and then finally we'll cover population",
    "start": "430479",
    "end": "432160"
  },
  {
    "text": "based training",
    "start": "432160",
    "end": "433599"
  },
  {
    "text": "some of these techniques might be",
    "start": "433599",
    "end": "435120"
  },
  {
    "text": "applicable for across all machine",
    "start": "435120",
    "end": "436880"
  },
  {
    "text": "learning including non-deep learning",
    "start": "436880",
    "end": "438400"
  },
  {
    "text": "regimes",
    "start": "438400",
    "end": "439520"
  },
  {
    "text": "but some of these techniques will have",
    "start": "439520",
    "end": "440880"
  },
  {
    "text": "much more significance and exploit",
    "start": "440880",
    "end": "443280"
  },
  {
    "text": "different characteristics allowing to be",
    "start": "443280",
    "end": "445919"
  },
  {
    "text": "much more impactful",
    "start": "445919",
    "end": "447120"
  },
  {
    "text": "in the deep learning problems",
    "start": "447120",
    "end": "451840"
  },
  {
    "text": "so let's start off before we start",
    "start": "452319",
    "end": "456160"
  },
  {
    "text": "i want to make sure that we're on the",
    "start": "456160",
    "end": "458080"
  },
  {
    "text": "same page about",
    "start": "458080",
    "end": "460160"
  },
  {
    "text": "definitions and i'm going to be using",
    "start": "460160",
    "end": "461840"
  },
  {
    "text": "the word trial quite often",
    "start": "461840",
    "end": "463759"
  },
  {
    "text": "and that's just simply going to mean one",
    "start": "463759",
    "end": "466000"
  },
  {
    "text": "high parameter evaluation so one set of",
    "start": "466000",
    "end": "468400"
  },
  {
    "text": "hyperparameters that we're going to",
    "start": "468400",
    "end": "469599"
  },
  {
    "text": "sample from",
    "start": "469599",
    "end": "472160"
  },
  {
    "text": "so random search is just what it sounds",
    "start": "472960",
    "end": "475120"
  },
  {
    "text": "like",
    "start": "475120",
    "end": "477120"
  },
  {
    "text": "given different distributions of each",
    "start": "477120",
    "end": "479840"
  },
  {
    "text": "hyperparameter value",
    "start": "479840",
    "end": "481199"
  },
  {
    "text": "we're going to sample uh parameters from",
    "start": "481199",
    "end": "483680"
  },
  {
    "text": "these distributions",
    "start": "483680",
    "end": "485680"
  },
  {
    "text": "all of them at once but over and over",
    "start": "485680",
    "end": "487680"
  },
  {
    "text": "again eventually",
    "start": "487680",
    "end": "489120"
  },
  {
    "text": "aiming to find the best model so",
    "start": "489120",
    "end": "492560"
  },
  {
    "text": "when i say trial here i mean i'm going",
    "start": "492560",
    "end": "495199"
  },
  {
    "text": "to sample once from",
    "start": "495199",
    "end": "496720"
  },
  {
    "text": "alpha once from the distribution given",
    "start": "496720",
    "end": "500560"
  },
  {
    "text": "by to hidden layers and once from the",
    "start": "500560",
    "end": "503520"
  },
  {
    "text": "distribution given to params",
    "start": "503520",
    "end": "505039"
  },
  {
    "text": "as you see on the pseudocode so",
    "start": "505039",
    "end": "508319"
  },
  {
    "text": "now that i have sampled values from all",
    "start": "508319",
    "end": "510639"
  },
  {
    "text": "three different",
    "start": "510639",
    "end": "511599"
  },
  {
    "text": "hyper hyperparameter distributions i",
    "start": "511599",
    "end": "514320"
  },
  {
    "text": "have what we call a trial",
    "start": "514320",
    "end": "516560"
  },
  {
    "text": "now there's a couple depth benefits for",
    "start": "516560",
    "end": "519200"
  },
  {
    "text": "for doing hyperparameter search",
    "start": "519200",
    "end": "521039"
  },
  {
    "text": "first all the trials are easily",
    "start": "521039",
    "end": "522880"
  },
  {
    "text": "paralyzable since",
    "start": "522880",
    "end": "524480"
  },
  {
    "text": "since all of them are independent from",
    "start": "524480",
    "end": "526080"
  },
  {
    "text": "each other and also turns out in high",
    "start": "526080",
    "end": "528080"
  },
  {
    "text": "dimensions",
    "start": "528080",
    "end": "528800"
  },
  {
    "text": "random search is just very hard to beat",
    "start": "528800",
    "end": "532480"
  },
  {
    "text": "but what's wrong with random search well",
    "start": "532480",
    "end": "534480"
  },
  {
    "text": "it turns out there's a lot of things",
    "start": "534480",
    "end": "535680"
  },
  {
    "text": "that we can do better",
    "start": "535680",
    "end": "537120"
  },
  {
    "text": "after all you're only trying things on",
    "start": "537120",
    "end": "538640"
  },
  {
    "text": "random",
    "start": "538640",
    "end": "540399"
  },
  {
    "text": "so i'll show you on the next slide how",
    "start": "540399",
    "end": "542720"
  },
  {
    "text": "we can actually achieve",
    "start": "542720",
    "end": "544480"
  },
  {
    "text": "um better performance by limiting the",
    "start": "544480",
    "end": "547440"
  },
  {
    "text": "amount",
    "start": "547440",
    "end": "548480"
  },
  {
    "text": "of of resources that we spend on bad",
    "start": "548480",
    "end": "551120"
  },
  {
    "text": "trials",
    "start": "551120",
    "end": "553519"
  },
  {
    "text": "so for example let's take this graph of",
    "start": "553519",
    "end": "556240"
  },
  {
    "text": "training run accuracies",
    "start": "556240",
    "end": "558800"
  },
  {
    "text": "what we see on the graph is that there's",
    "start": "558800",
    "end": "560399"
  },
  {
    "text": "simply a lot of models that are bad",
    "start": "560399",
    "end": "561839"
  },
  {
    "text": "performers",
    "start": "561839",
    "end": "563200"
  },
  {
    "text": "naturally you can ask well why bother",
    "start": "563200",
    "end": "566080"
  },
  {
    "text": "wasting",
    "start": "566080",
    "end": "566720"
  },
  {
    "text": "resources on the trials that aren't",
    "start": "566720",
    "end": "568320"
  },
  {
    "text": "simply going to be good",
    "start": "568320",
    "end": "571440"
  },
  {
    "text": "and so there's a hyperparametric",
    "start": "572560",
    "end": "574560"
  },
  {
    "text": "technique called band-based algorithms",
    "start": "574560",
    "end": "578720"
  },
  {
    "text": "that addresses this precisely",
    "start": "578720",
    "end": "582319"
  },
  {
    "text": "so hyperband and osha are um",
    "start": "583279",
    "end": "586480"
  },
  {
    "text": "are early stopping algorithms or",
    "start": "586480",
    "end": "588399"
  },
  {
    "text": "abandon-based algorithms that",
    "start": "588399",
    "end": "590320"
  },
  {
    "text": "aim to allocate resources to better",
    "start": "590320",
    "end": "592000"
  },
  {
    "text": "performing trials",
    "start": "592000",
    "end": "594320"
  },
  {
    "text": "the intuition behind these algorithms is",
    "start": "594320",
    "end": "596160"
  },
  {
    "text": "that each trial is going to",
    "start": "596160",
    "end": "598880"
  },
  {
    "text": "compare is going to be evaluated and",
    "start": "598880",
    "end": "602640"
  },
  {
    "text": "we're going to compare the relative",
    "start": "602640",
    "end": "604160"
  },
  {
    "text": "performance among different trials",
    "start": "604160",
    "end": "606880"
  },
  {
    "text": "at different points in time at any point",
    "start": "606880",
    "end": "610320"
  },
  {
    "text": "of this",
    "start": "610320",
    "end": "611040"
  },
  {
    "text": "any point of evaluation we're going to",
    "start": "611040",
    "end": "613440"
  },
  {
    "text": "terminate the bad performing trials and",
    "start": "613440",
    "end": "615040"
  },
  {
    "text": "then we're going to let the better",
    "start": "615040",
    "end": "616399"
  },
  {
    "text": "performing trials",
    "start": "616399",
    "end": "617279"
  },
  {
    "text": "run for a longer period of time",
    "start": "617279",
    "end": "620399"
  },
  {
    "text": "here's what this looks like in",
    "start": "620399",
    "end": "622320"
  },
  {
    "text": "pseudocode we're going to start with the",
    "start": "622320",
    "end": "624000"
  },
  {
    "text": "trial",
    "start": "624000",
    "end": "624640"
  },
  {
    "text": "we're just going to sample from which is",
    "start": "624640",
    "end": "626399"
  },
  {
    "text": "going to be you know sample from some",
    "start": "626399",
    "end": "627680"
  },
  {
    "text": "hyper hammer space",
    "start": "627680",
    "end": "630000"
  },
  {
    "text": "then the trial is going to run for a",
    "start": "630000",
    "end": "633279"
  },
  {
    "text": "number of epics and at a user designated",
    "start": "633279",
    "end": "636800"
  },
  {
    "text": "cutoff",
    "start": "636800",
    "end": "637760"
  },
  {
    "text": "for you know period of time we're going",
    "start": "637760",
    "end": "640000"
  },
  {
    "text": "to evaluate whether or not it's one of",
    "start": "640000",
    "end": "641600"
  },
  {
    "text": "the better performing trials",
    "start": "641600",
    "end": "643519"
  },
  {
    "text": "if it is we're going to let it run for a",
    "start": "643519",
    "end": "645360"
  },
  {
    "text": "longer period of time",
    "start": "645360",
    "end": "646959"
  },
  {
    "text": "otherwise we're going to pause it and",
    "start": "646959",
    "end": "648640"
  },
  {
    "text": "allow other trials to start",
    "start": "648640",
    "end": "651760"
  },
  {
    "text": "i watched the one note that there's has",
    "start": "651760",
    "end": "653440"
  },
  {
    "text": "there has been many recent advances",
    "start": "653440",
    "end": "655760"
  },
  {
    "text": "that have been made um that made",
    "start": "655760",
    "end": "658079"
  },
  {
    "text": "hyperband",
    "start": "658079",
    "end": "658800"
  },
  {
    "text": "capable of being combined with",
    "start": "658800",
    "end": "662000"
  },
  {
    "text": "smarter techniques such as management",
    "start": "662000",
    "end": "664000"
  },
  {
    "text": "optimization",
    "start": "664000",
    "end": "665920"
  },
  {
    "text": "another benefit of hyperband is that",
    "start": "665920",
    "end": "668880"
  },
  {
    "text": "it's easily parallelizable",
    "start": "668880",
    "end": "670640"
  },
  {
    "text": "meaning that you can easily increase its",
    "start": "670640",
    "end": "672880"
  },
  {
    "text": "efficiency by leveraging",
    "start": "672880",
    "end": "674560"
  },
  {
    "text": "parallel resources but i want to also",
    "start": "674560",
    "end": "677839"
  },
  {
    "text": "point out there's also much more room",
    "start": "677839",
    "end": "679519"
  },
  {
    "text": "for improvement here",
    "start": "679519",
    "end": "682240"
  },
  {
    "text": "turns out in deep learning",
    "start": "682240",
    "end": "683920"
  },
  {
    "text": "hyperparameter schedules matter a lot",
    "start": "683920",
    "end": "686560"
  },
  {
    "text": "this means that you can change the hyper",
    "start": "686560",
    "end": "689040"
  },
  {
    "text": "parameter",
    "start": "689040",
    "end": "689680"
  },
  {
    "text": "value during training and this type of",
    "start": "689680",
    "end": "692399"
  },
  {
    "text": "value can be something like momentum or",
    "start": "692399",
    "end": "694160"
  },
  {
    "text": "learning rate",
    "start": "694160",
    "end": "695360"
  },
  {
    "text": "and you can actually affect the",
    "start": "695360",
    "end": "697279"
  },
  {
    "text": "performance of the model",
    "start": "697279",
    "end": "698720"
  },
  {
    "text": "simply by perturbing the hyperparameters",
    "start": "698720",
    "end": "701360"
  },
  {
    "text": "during training",
    "start": "701360",
    "end": "702959"
  },
  {
    "text": "this is actually required in the law of",
    "start": "702959",
    "end": "704640"
  },
  {
    "text": "models to achieve state of our results",
    "start": "704640",
    "end": "708480"
  },
  {
    "text": "so there's a technique called",
    "start": "709040",
    "end": "711279"
  },
  {
    "text": "population-based training",
    "start": "711279",
    "end": "712639"
  },
  {
    "text": "that addresses this issue this is a",
    "start": "712639",
    "end": "715760"
  },
  {
    "text": "technique",
    "start": "715760",
    "end": "716800"
  },
  {
    "text": "that was recent that was published a",
    "start": "716800",
    "end": "718639"
  },
  {
    "text": "couple years ago by google deep mind",
    "start": "718639",
    "end": "722639"
  },
  {
    "text": "so the main idea of population based",
    "start": "722880",
    "end": "724800"
  },
  {
    "text": "training is to evaluate multiple trials",
    "start": "724800",
    "end": "726639"
  },
  {
    "text": "in parallel",
    "start": "726639",
    "end": "728480"
  },
  {
    "text": "and at a certain frequency similar to",
    "start": "728480",
    "end": "731839"
  },
  {
    "text": "a hyperband terminate the lowest",
    "start": "731839",
    "end": "733839"
  },
  {
    "text": "performers",
    "start": "733839",
    "end": "735839"
  },
  {
    "text": "but when we terminate those performers",
    "start": "735839",
    "end": "738399"
  },
  {
    "text": "instead of starting a new trial we're",
    "start": "738399",
    "end": "740000"
  },
  {
    "text": "going to copy the weights of the best",
    "start": "740000",
    "end": "741440"
  },
  {
    "text": "performer",
    "start": "741440",
    "end": "742320"
  },
  {
    "text": "and mutate its hyper parameters so that",
    "start": "742320",
    "end": "744480"
  },
  {
    "text": "we can",
    "start": "744480",
    "end": "745839"
  },
  {
    "text": "search across different schedules",
    "start": "745839",
    "end": "749120"
  },
  {
    "text": "a quick walkthrough of this code shows",
    "start": "749600",
    "end": "751200"
  },
  {
    "text": "that or let's let's take a quick walk",
    "start": "751200",
    "end": "753440"
  },
  {
    "text": "through over the example",
    "start": "753440",
    "end": "756079"
  },
  {
    "text": "so we'll start off with four different",
    "start": "756079",
    "end": "757920"
  },
  {
    "text": "hyperparameter values",
    "start": "757920",
    "end": "759600"
  },
  {
    "text": "say 0.1 0.2 0.3 0.4",
    "start": "759600",
    "end": "763680"
  },
  {
    "text": "and we're going to run each of these",
    "start": "763680",
    "end": "765519"
  },
  {
    "text": "different trials",
    "start": "765519",
    "end": "767360"
  },
  {
    "text": "for for just the except period of time",
    "start": "767360",
    "end": "772639"
  },
  {
    "text": "after the set period of time we're going",
    "start": "772639",
    "end": "774240"
  },
  {
    "text": "to evaluate",
    "start": "774240",
    "end": "775680"
  },
  {
    "text": "all these uh these trials and we'll find",
    "start": "775680",
    "end": "779040"
  },
  {
    "text": "that",
    "start": "779040",
    "end": "779360"
  },
  {
    "text": "0.4 is actually returning uh performing",
    "start": "779360",
    "end": "782160"
  },
  {
    "text": "very",
    "start": "782160",
    "end": "782480"
  },
  {
    "text": "poorly so let's terminate that and",
    "start": "782480",
    "end": "785360"
  },
  {
    "text": "instead",
    "start": "785360",
    "end": "786160"
  },
  {
    "text": "let's um take the best performing trial",
    "start": "786160",
    "end": "789040"
  },
  {
    "text": "and",
    "start": "789040",
    "end": "789519"
  },
  {
    "text": "copy its weights and mutate its hyper",
    "start": "789519",
    "end": "791760"
  },
  {
    "text": "parameters",
    "start": "791760",
    "end": "793680"
  },
  {
    "text": "so 0.110 what turns out was the best",
    "start": "793680",
    "end": "796399"
  },
  {
    "text": "private parameter",
    "start": "796399",
    "end": "798000"
  },
  {
    "text": "or the best performing trial so we",
    "start": "798000",
    "end": "799920"
  },
  {
    "text": "perturbed its hype parameters to 0.15",
    "start": "799920",
    "end": "803040"
  },
  {
    "text": "and we copied its weights and and",
    "start": "803040",
    "end": "806240"
  },
  {
    "text": "effectively cloned a copy of 0.1",
    "start": "806240",
    "end": "809839"
  },
  {
    "text": "um to take the place of 0.4",
    "start": "809839",
    "end": "814399"
  },
  {
    "text": "and we'll do this over and over again so",
    "start": "814399",
    "end": "816399"
  },
  {
    "text": "in this case let's say 0.2 is the",
    "start": "816399",
    "end": "818800"
  },
  {
    "text": "worst performer and 0.3 is the best",
    "start": "818800",
    "end": "822399"
  },
  {
    "text": "performer",
    "start": "822399",
    "end": "823680"
  },
  {
    "text": "so in this case we'll terminate 0.2 and",
    "start": "823680",
    "end": "826959"
  },
  {
    "text": "we'll perturb the weights of zero point",
    "start": "826959",
    "end": "829040"
  },
  {
    "text": "the perturbed of high parameters of 0.3",
    "start": "829040",
    "end": "832639"
  },
  {
    "text": "copying it to to take the place of",
    "start": "832639",
    "end": "836000"
  },
  {
    "text": "of the terminated trial",
    "start": "836000",
    "end": "839760"
  },
  {
    "text": "so the benefits of population-based",
    "start": "840079",
    "end": "841839"
  },
  {
    "text": "training is that it's actually easily",
    "start": "841839",
    "end": "843839"
  },
  {
    "text": "paralyzable",
    "start": "843839",
    "end": "845040"
  },
  {
    "text": "it also allows you to search over",
    "start": "845040",
    "end": "846480"
  },
  {
    "text": "different high pressure tuning schedules",
    "start": "846480",
    "end": "848240"
  },
  {
    "text": "which is important for",
    "start": "848240",
    "end": "849360"
  },
  {
    "text": "achieving state-of-the-art results and",
    "start": "849360",
    "end": "851519"
  },
  {
    "text": "it is also efficient in that it",
    "start": "851519",
    "end": "853360"
  },
  {
    "text": "terminates bad performance",
    "start": "853360",
    "end": "856560"
  },
  {
    "text": "now you might be asking okay does this",
    "start": "856560",
    "end": "858320"
  },
  {
    "text": "really work well",
    "start": "858320",
    "end": "860160"
  },
  {
    "text": "google deepmind ran this technique over",
    "start": "860160",
    "end": "862880"
  },
  {
    "text": "multiple published algorithms",
    "start": "862880",
    "end": "864399"
  },
  {
    "text": "when they first released this algorithm",
    "start": "864399",
    "end": "866480"
  },
  {
    "text": "and they found that across the board",
    "start": "866480",
    "end": "868240"
  },
  {
    "text": "population-based training was able to",
    "start": "868240",
    "end": "869839"
  },
  {
    "text": "provide non-trivial performance",
    "start": "869839",
    "end": "871839"
  },
  {
    "text": "increases over the state of art",
    "start": "871839",
    "end": "875279"
  },
  {
    "text": "so now that we know about the",
    "start": "876000",
    "end": "878160"
  },
  {
    "text": "state-of-the-art and hypergram tuning",
    "start": "878160",
    "end": "879920"
  },
  {
    "text": "algorithms how do we efficiently",
    "start": "879920",
    "end": "883600"
  },
  {
    "text": "leverage our resources so that we can",
    "start": "883600",
    "end": "885760"
  },
  {
    "text": "tune our hyper-members quickly",
    "start": "885760",
    "end": "889440"
  },
  {
    "text": "well raytune is a library that addresses",
    "start": "889440",
    "end": "893199"
  },
  {
    "start": "890000",
    "end": "1187000"
  },
  {
    "text": "this problem",
    "start": "893199",
    "end": "894399"
  },
  {
    "text": "it is a scalable hyperparameter tuning",
    "start": "894399",
    "end": "896399"
  },
  {
    "text": "library built on top of red",
    "start": "896399",
    "end": "899360"
  },
  {
    "text": "so to give some context let's quickly",
    "start": "899360",
    "end": "901360"
  },
  {
    "text": "talk about rey",
    "start": "901360",
    "end": "902399"
  },
  {
    "text": "reyes ray is both a framework for",
    "start": "902399",
    "end": "905760"
  },
  {
    "text": "distributed python",
    "start": "905760",
    "end": "906959"
  },
  {
    "text": "but also contains an ecosystem of",
    "start": "906959",
    "end": "909199"
  },
  {
    "text": "libraries for specific use cases such as",
    "start": "909199",
    "end": "911199"
  },
  {
    "text": "ray2",
    "start": "911199",
    "end": "913760"
  },
  {
    "text": "how did these two interact well ray tune",
    "start": "913760",
    "end": "916399"
  },
  {
    "text": "actually",
    "start": "916399",
    "end": "916880"
  },
  {
    "text": "leverages the primitives that ray",
    "start": "916880",
    "end": "920480"
  },
  {
    "text": "the framework exposes so that ray tune",
    "start": "920480",
    "end": "923839"
  },
  {
    "text": "can",
    "start": "923839",
    "end": "924320"
  },
  {
    "text": "easily parallelize and distribute its",
    "start": "924320",
    "end": "926800"
  },
  {
    "text": "work",
    "start": "926800",
    "end": "928079"
  },
  {
    "text": "across multiple nodes and and chords",
    "start": "928079",
    "end": "932959"
  },
  {
    "text": "so tune specifically is a library that",
    "start": "933040",
    "end": "935199"
  },
  {
    "text": "handles the execution",
    "start": "935199",
    "end": "936800"
  },
  {
    "text": "of high parameter search it provides",
    "start": "936800",
    "end": "939440"
  },
  {
    "text": "hooks",
    "start": "939440",
    "end": "940480"
  },
  {
    "text": "and plug-ins for different",
    "start": "940480",
    "end": "942480"
  },
  {
    "text": "high-performer tuning algorithms",
    "start": "942480",
    "end": "944880"
  },
  {
    "text": "while automatically handling the",
    "start": "944880",
    "end": "946800"
  },
  {
    "text": "parallelism for you",
    "start": "946800",
    "end": "948959"
  },
  {
    "text": "so why is tuned special well",
    "start": "948959",
    "end": "952639"
  },
  {
    "text": "tune is actually built with deep",
    "start": "952639",
    "end": "954800"
  },
  {
    "text": "learning as a priority",
    "start": "954800",
    "end": "957839"
  },
  {
    "text": "this means that tune is uh tunes built",
    "start": "957839",
    "end": "960399"
  },
  {
    "text": "so that you can",
    "start": "960399",
    "end": "961279"
  },
  {
    "text": "effectively utilize and spread your",
    "start": "961279",
    "end": "963120"
  },
  {
    "text": "training across multiple gpus",
    "start": "963120",
    "end": "966240"
  },
  {
    "text": "you can also tune different models um",
    "start": "966240",
    "end": "969199"
  },
  {
    "text": "built with",
    "start": "969199",
    "end": "970000"
  },
  {
    "text": "any machine learning framework",
    "start": "970000",
    "end": "973199"
  },
  {
    "text": "and finally tune allows you to run",
    "start": "973199",
    "end": "975279"
  },
  {
    "text": "hyperion tuning at any scale",
    "start": "975279",
    "end": "977839"
  },
  {
    "text": "and you can go from running a single",
    "start": "977839",
    "end": "979279"
  },
  {
    "text": "process to running multiple gpus to",
    "start": "979279",
    "end": "981120"
  },
  {
    "text": "running across multiple nodes",
    "start": "981120",
    "end": "982720"
  },
  {
    "text": "all without changing your code",
    "start": "982720",
    "end": "986160"
  },
  {
    "text": "tune also offers many algorithms to",
    "start": "986320",
    "end": "989199"
  },
  {
    "text": "optimize your hyperparameter search",
    "start": "989199",
    "end": "990959"
  },
  {
    "text": "including all the hyperparameter",
    "start": "990959",
    "end": "992560"
  },
  {
    "text": "algorithm search algorithms mentioned",
    "start": "992560",
    "end": "994240"
  },
  {
    "text": "today",
    "start": "994240",
    "end": "995519"
  },
  {
    "text": "can also integrates with many",
    "start": "995519",
    "end": "997759"
  },
  {
    "text": "hyperparameter search",
    "start": "997759",
    "end": "998880"
  },
  {
    "text": "libraries such as hyperopt",
    "start": "998880",
    "end": "1002079"
  },
  {
    "text": "axe or sigopt",
    "start": "1002079",
    "end": "1005120"
  },
  {
    "text": "allowing you to transparently scale",
    "start": "1005120",
    "end": "1006880"
  },
  {
    "text": "these libraries across multiple cores",
    "start": "1006880",
    "end": "1008480"
  },
  {
    "text": "and nodes",
    "start": "1008480",
    "end": "1011600"
  },
  {
    "text": "now tune is actually quite easy to use",
    "start": "1011600",
    "end": "1013440"
  },
  {
    "text": "and let's walk through our api demo real",
    "start": "1013440",
    "end": "1015360"
  },
  {
    "text": "quickly",
    "start": "1015360",
    "end": "1017120"
  },
  {
    "text": "so let's start off with a simple",
    "start": "1017120",
    "end": "1020079"
  },
  {
    "text": "training function",
    "start": "1020079",
    "end": "1021279"
  },
  {
    "text": "this is very similar code that to",
    "start": "1021279",
    "end": "1023600"
  },
  {
    "text": "something that you might",
    "start": "1023600",
    "end": "1024480"
  },
  {
    "text": "see in keras or or pytorch",
    "start": "1024480",
    "end": "1028079"
  },
  {
    "text": "and all we have to do to convert this to",
    "start": "1028079",
    "end": "1030240"
  },
  {
    "text": "using tune",
    "start": "1030240",
    "end": "1031760"
  },
  {
    "text": "is by adding a report a one-line call",
    "start": "1031760",
    "end": "1035600"
  },
  {
    "text": "to report the loss or report some",
    "start": "1035600",
    "end": "1039038"
  },
  {
    "text": "accuracy metric",
    "start": "1039039",
    "end": "1041438"
  },
  {
    "text": "and this notifies tune to inform",
    "start": "1041439",
    "end": "1044640"
  },
  {
    "text": "a tune of the current intermediate",
    "start": "1044640",
    "end": "1046640"
  },
  {
    "text": "result the intermediate training",
    "start": "1046640",
    "end": "1048640"
  },
  {
    "text": "progress",
    "start": "1048640",
    "end": "1049520"
  },
  {
    "text": "of this training function",
    "start": "1049520",
    "end": "1052720"
  },
  {
    "text": "to run a search or to use tune you can",
    "start": "1052720",
    "end": "1055919"
  },
  {
    "text": "actually just simply call to not run",
    "start": "1055919",
    "end": "1058160"
  },
  {
    "text": "and if you want to run a search of",
    "start": "1058160",
    "end": "1060720"
  },
  {
    "text": "different hive parameters",
    "start": "1060720",
    "end": "1062000"
  },
  {
    "text": "you can easily specify a search space",
    "start": "1062000",
    "end": "1065360"
  },
  {
    "text": "and add a single argument which uh",
    "start": "1065360",
    "end": "1068559"
  },
  {
    "text": "here is num samples and it allows you to",
    "start": "1068559",
    "end": "1071120"
  },
  {
    "text": "run",
    "start": "1071120",
    "end": "1071679"
  },
  {
    "text": "up to a hundred different uh samples",
    "start": "1071679",
    "end": "1073760"
  },
  {
    "text": "from this hyperparameter space",
    "start": "1073760",
    "end": "1076640"
  },
  {
    "text": "parallelizing them to leverage all the",
    "start": "1076640",
    "end": "1078720"
  },
  {
    "text": "cores in your cluster",
    "start": "1078720",
    "end": "1081440"
  },
  {
    "text": "q also provides a lot of other features",
    "start": "1081440",
    "end": "1083200"
  },
  {
    "text": "through this api sessions checkpointing",
    "start": "1083200",
    "end": "1085520"
  },
  {
    "text": "and different search algorithms",
    "start": "1085520",
    "end": "1089039"
  },
  {
    "text": "finally if you want to tune something",
    "start": "1089440",
    "end": "1091679"
  },
  {
    "text": "like burt which is",
    "start": "1091679",
    "end": "1093200"
  },
  {
    "text": "as we know very large and very complex",
    "start": "1093200",
    "end": "1096640"
  },
  {
    "text": "you can easily speed up the tuning",
    "start": "1096640",
    "end": "1099600"
  },
  {
    "text": "process by enabling the trials",
    "start": "1099600",
    "end": "1101520"
  },
  {
    "text": "themselves to be distributed",
    "start": "1101520",
    "end": "1103840"
  },
  {
    "text": "this requires the usage of an underlying",
    "start": "1103840",
    "end": "1105840"
  },
  {
    "text": "framework and",
    "start": "1105840",
    "end": "1107679"
  },
  {
    "text": "ray actually provides many of these",
    "start": "1107679",
    "end": "1109280"
  },
  {
    "text": "frameworks for you to leverage",
    "start": "1109280",
    "end": "1111200"
  },
  {
    "text": "in um in different use cases such as",
    "start": "1111200",
    "end": "1114480"
  },
  {
    "text": "all live for distributed reinforcement",
    "start": "1114480",
    "end": "1116400"
  },
  {
    "text": "learning or ray sgd for distributed",
    "start": "1116400",
    "end": "1119440"
  },
  {
    "text": "deep learning so",
    "start": "1119440",
    "end": "1123520"
  },
  {
    "text": "by using tune you can simplify the",
    "start": "1123520",
    "end": "1126320"
  },
  {
    "text": "execution of your hyperparameter search",
    "start": "1126320",
    "end": "1128880"
  },
  {
    "text": "ultimately accelerating your machine",
    "start": "1128880",
    "end": "1130559"
  },
  {
    "text": "learning workflow",
    "start": "1130559",
    "end": "1132720"
  },
  {
    "text": "to summarize in this talk we",
    "start": "1132720",
    "end": "1136080"
  },
  {
    "text": "motivated the importance",
    "start": "1136080",
    "end": "1139280"
  },
  {
    "text": "and highlighted the complexity of",
    "start": "1139280",
    "end": "1140799"
  },
  {
    "text": "hyper-frame returning",
    "start": "1140799",
    "end": "1143200"
  },
  {
    "text": "we overviewed some of the",
    "start": "1143200",
    "end": "1144320"
  },
  {
    "text": "state-of-the-art techniques",
    "start": "1144320",
    "end": "1146559"
  },
  {
    "text": "for tuning parameters and finally we",
    "start": "1146559",
    "end": "1149919"
  },
  {
    "text": "talked about ray 2",
    "start": "1149919",
    "end": "1151600"
  },
  {
    "text": "which is the library built on top of ray",
    "start": "1151600",
    "end": "1153360"
  },
  {
    "text": "for simplifying and scaling hypergraph",
    "start": "1153360",
    "end": "1155120"
  },
  {
    "text": "training",
    "start": "1155120",
    "end": "1156799"
  },
  {
    "text": "i hope that the information presented in",
    "start": "1156799",
    "end": "1159200"
  },
  {
    "text": "this talk",
    "start": "1159200",
    "end": "1160240"
  },
  {
    "text": "can be now you can actually take this",
    "start": "1160240",
    "end": "1162240"
  },
  {
    "text": "information",
    "start": "1162240",
    "end": "1163679"
  },
  {
    "text": "accelerate your existing machine",
    "start": "1163679",
    "end": "1165200"
  },
  {
    "text": "learning workflow",
    "start": "1165200",
    "end": "1168000"
  },
  {
    "text": "so thanks for listening and if you have",
    "start": "1168720",
    "end": "1170799"
  },
  {
    "text": "any more questions you can feel free to",
    "start": "1170799",
    "end": "1172559"
  },
  {
    "text": "reach out",
    "start": "1172559",
    "end": "1173360"
  },
  {
    "text": "to me on twitter or at my email there",
    "start": "1173360",
    "end": "1176080"
  },
  {
    "text": "will also be a",
    "start": "1176080",
    "end": "1177200"
  },
  {
    "text": "panel section later on and and",
    "start": "1177200",
    "end": "1180240"
  },
  {
    "text": "happy to take any questions there",
    "start": "1180240",
    "end": "1189039"
  }
]