[
  {
    "text": "thank you everyone uh super excited to",
    "start": "4940",
    "end": "7859"
  },
  {
    "text": "be here today it's been a minute since",
    "start": "7859",
    "end": "10440"
  },
  {
    "text": "I've been at a conference in person it's",
    "start": "10440",
    "end": "12540"
  },
  {
    "text": "actually been three years",
    "start": "12540",
    "end": "14280"
  },
  {
    "text": "so it feels good I forgot what the heart",
    "start": "14280",
    "end": "16619"
  },
  {
    "text": "flutter is like when you're presenting",
    "start": "16619",
    "end": "18300"
  },
  {
    "text": "live right",
    "start": "18300",
    "end": "19980"
  },
  {
    "text": "we got a lot to cover so I'm gonna jump",
    "start": "19980",
    "end": "21480"
  },
  {
    "text": "straight into it",
    "start": "21480",
    "end": "22800"
  },
  {
    "text": "um okay there are three themes that I",
    "start": "22800",
    "end": "26640"
  },
  {
    "text": "want you to keep in mind as I go through",
    "start": "26640",
    "end": "27599"
  },
  {
    "text": "this presentation but also represent why",
    "start": "27599",
    "end": "30180"
  },
  {
    "text": "we're here and why we're partnering with",
    "start": "30180",
    "end": "32099"
  },
  {
    "text": "any scale",
    "start": "32099",
    "end": "33420"
  },
  {
    "text": "the first is accelerated Computing meets",
    "start": "33420",
    "end": "35820"
  },
  {
    "text": "orchestration efficiency through",
    "start": "35820",
    "end": "37739"
  },
  {
    "text": "distributed computing",
    "start": "37739",
    "end": "39300"
  },
  {
    "text": "if you think of the value prop of",
    "start": "39300",
    "end": "41399"
  },
  {
    "text": "accelerated Computing that's the ability",
    "start": "41399",
    "end": "43379"
  },
  {
    "text": "for you to do more right for a fraction",
    "start": "43379",
    "end": "45480"
  },
  {
    "text": "on the cost",
    "start": "45480",
    "end": "46680"
  },
  {
    "text": "the recurrent themes throughout this",
    "start": "46680",
    "end": "48180"
  },
  {
    "text": "presentation throughout this Summit are",
    "start": "48180",
    "end": "50879"
  },
  {
    "text": "all about doing things faster at a",
    "start": "50879",
    "end": "52800"
  },
  {
    "text": "fraction of the cost",
    "start": "52800",
    "end": "54059"
  },
  {
    "text": "so value prop of doing more for Less is",
    "start": "54059",
    "end": "56699"
  },
  {
    "text": "fundamental to everything we're doing",
    "start": "56699",
    "end": "58860"
  },
  {
    "text": "the second piece is right A lot of the",
    "start": "58860",
    "end": "61500"
  },
  {
    "text": "stuff is happening open source world",
    "start": "61500",
    "end": "64198"
  },
  {
    "text": "um but I think it's super important that",
    "start": "64199",
    "end": "65700"
  },
  {
    "text": "we give you guys a path to get into",
    "start": "65700",
    "end": "67500"
  },
  {
    "text": "production right to get into production",
    "start": "67500",
    "end": "69060"
  },
  {
    "text": "AI I thought it was really interesting",
    "start": "69060",
    "end": "71100"
  },
  {
    "text": "that when when Ben Horowitz was asked",
    "start": "71100",
    "end": "73380"
  },
  {
    "text": "what are going to be some of the",
    "start": "73380",
    "end": "74820"
  },
  {
    "text": "pitfalls right for large language models",
    "start": "74820",
    "end": "77820"
  },
  {
    "text": "he started talking about role-based",
    "start": "77820",
    "end": "80159"
  },
  {
    "text": "access control",
    "start": "80159",
    "end": "81479"
  },
  {
    "text": "right so security and compliance and cve",
    "start": "81479",
    "end": "84360"
  },
  {
    "text": "patching right giving you a path",
    "start": "84360",
    "end": "85920"
  },
  {
    "text": "Enterprise grade and the final piece",
    "start": "85920",
    "end": "87900"
  },
  {
    "text": "about being Cloud native right",
    "start": "87900",
    "end": "90119"
  },
  {
    "text": "um and yes A lot's happening in the",
    "start": "90119",
    "end": "92400"
  },
  {
    "text": "cloud but I think cloud native is about",
    "start": "92400",
    "end": "94080"
  },
  {
    "text": "meeting you where you guys are right so",
    "start": "94080",
    "end": "96060"
  },
  {
    "text": "for sure there's a lot of our our our",
    "start": "96060",
    "end": "97860"
  },
  {
    "text": "stack in the cloud but you might have a",
    "start": "97860",
    "end": "99840"
  },
  {
    "text": "GPU in your local workstation giving you",
    "start": "99840",
    "end": "101939"
  },
  {
    "text": "a path to start locally and then scaling",
    "start": "101939",
    "end": "103380"
  },
  {
    "text": "to the cloud is part of that value prop",
    "start": "103380",
    "end": "106979"
  },
  {
    "text": "okay yes we do gpus",
    "start": "106979",
    "end": "110579"
  },
  {
    "text": "but Nvidia is a full stack company",
    "start": "110579",
    "end": "113220"
  },
  {
    "text": "right as a full stack company and I",
    "start": "113220",
    "end": "115380"
  },
  {
    "text": "don't think people realize or appreciate",
    "start": "115380",
    "end": "116820"
  },
  {
    "text": "the breadth of the software that we have",
    "start": "116820",
    "end": "118200"
  },
  {
    "text": "to offer we have four million developers",
    "start": "118200",
    "end": "120420"
  },
  {
    "text": "that are enrolled in our developer",
    "start": "120420",
    "end": "121560"
  },
  {
    "text": "program there are more than 400 sdks and",
    "start": "121560",
    "end": "125219"
  },
  {
    "text": "pre-trained models that we make",
    "start": "125219",
    "end": "126360"
  },
  {
    "text": "available through ngcr container",
    "start": "126360",
    "end": "128280"
  },
  {
    "text": "registry",
    "start": "128280",
    "end": "129420"
  },
  {
    "text": "right 40 000 Enterprises have deployed",
    "start": "129420",
    "end": "133459"
  },
  {
    "text": "their applications in production on",
    "start": "133459",
    "end": "135780"
  },
  {
    "text": "video AI so just a vast ecosystem of",
    "start": "135780",
    "end": "138239"
  },
  {
    "text": "software that we provide again I go back",
    "start": "138239",
    "end": "140099"
  },
  {
    "text": "to saying like we are an accelerated",
    "start": "140099",
    "end": "141300"
  },
  {
    "text": "Computing company the value comes from",
    "start": "141300",
    "end": "144120"
  },
  {
    "text": "the software in fact the majority of the",
    "start": "144120",
    "end": "146280"
  },
  {
    "text": "engineers working at Nvidia today work",
    "start": "146280",
    "end": "148319"
  },
  {
    "text": "on on software",
    "start": "148319",
    "end": "150780"
  },
  {
    "text": "and the software that we create right is",
    "start": "150780",
    "end": "153480"
  },
  {
    "text": "across the workflow right these are",
    "start": "153480",
    "end": "155940"
  },
  {
    "text": "Baseline libraries that we built domain",
    "start": "155940",
    "end": "158220"
  },
  {
    "text": "specific sdks on top of Rapids is great",
    "start": "158220",
    "end": "161459"
  },
  {
    "text": "for ETL right prepping tabular data",
    "start": "161459",
    "end": "164640"
  },
  {
    "text": "again the viprop is always we can do",
    "start": "164640",
    "end": "166980"
  },
  {
    "text": "things faster for a fraction of the cost",
    "start": "166980",
    "end": "169920"
  },
  {
    "text": "um I was really happy to hear about",
    "start": "169920",
    "end": "171420"
  },
  {
    "text": "thiano yesterday I grew up in Montreal",
    "start": "171420",
    "end": "173459"
  },
  {
    "text": "but for as long as I remember we've been",
    "start": "173459",
    "end": "175319"
  },
  {
    "text": "contributing to open source Frameworks",
    "start": "175319",
    "end": "177599"
  },
  {
    "text": "we Upstream all of our optimizations",
    "start": "177599",
    "end": "179940"
  },
  {
    "text": "so you know people you know no longer do",
    "start": "179940",
    "end": "182340"
  },
  {
    "text": "training without accelerators right so",
    "start": "182340",
    "end": "184440"
  },
  {
    "text": "pytorch and tensorflow are two of the",
    "start": "184440",
    "end": "186720"
  },
  {
    "text": "most popular open source Frameworks uh",
    "start": "186720",
    "end": "189120"
  },
  {
    "text": "tensor RT is our Optimizer and our model",
    "start": "189120",
    "end": "191940"
  },
  {
    "text": "runtime right we'll be talking about trt",
    "start": "191940",
    "end": "194280"
  },
  {
    "text": "LM and China is our entrance server",
    "start": "194280",
    "end": "196860"
  },
  {
    "text": "right and the ability for us to do",
    "start": "196860",
    "end": "198239"
  },
  {
    "text": "inference at magnitudes greater uh on",
    "start": "198239",
    "end": "201480"
  },
  {
    "text": "our accelerators versus GPU loans so",
    "start": "201480",
    "end": "204000"
  },
  {
    "text": "again we provide uh optimizations across",
    "start": "204000",
    "end": "206760"
  },
  {
    "text": "the end-to-end framework and then we",
    "start": "206760",
    "end": "208680"
  },
  {
    "text": "provide domain-specific sdks that we",
    "start": "208680",
    "end": "211200"
  },
  {
    "text": "build on top of these libraries",
    "start": "211200",
    "end": "213900"
  },
  {
    "text": "and that's what we're here to talk about",
    "start": "213900",
    "end": "215220"
  },
  {
    "text": "today we're here to talk about Nemo Nemo",
    "start": "215220",
    "end": "218459"
  },
  {
    "text": "is uh our end-to-end Cloud native",
    "start": "218459",
    "end": "220680"
  },
  {
    "text": "framework for building",
    "start": "220680",
    "end": "222739"
  },
  {
    "text": "customizing and deploying generative AI",
    "start": "222739",
    "end": "225540"
  },
  {
    "text": "it's comprised of a training container",
    "start": "225540",
    "end": "227640"
  },
  {
    "text": "comprised of an inference container and",
    "start": "227640",
    "end": "230400"
  },
  {
    "text": "it's got tooling for data prep or",
    "start": "230400",
    "end": "232140"
  },
  {
    "text": "curation and also guard rails right to",
    "start": "232140",
    "end": "234720"
  },
  {
    "text": "make sure your model stays in check",
    "start": "234720",
    "end": "236580"
  },
  {
    "text": "I'm gonna go into some of the the",
    "start": "236580",
    "end": "238019"
  },
  {
    "text": "specifics across some of these",
    "start": "238019",
    "end": "240599"
  },
  {
    "text": "capabilities so as as the expression",
    "start": "240599",
    "end": "242940"
  },
  {
    "text": "goes garbage into an llm the LM is going",
    "start": "242940",
    "end": "245760"
  },
  {
    "text": "to spew some garbage out right and so",
    "start": "245760",
    "end": "248159"
  },
  {
    "text": "the data curator is unique to Nemo it",
    "start": "248159",
    "end": "250560"
  },
  {
    "text": "allows you to create high quality data",
    "start": "250560",
    "end": "252599"
  },
  {
    "text": "sets right to train your",
    "start": "252599",
    "end": "254519"
  },
  {
    "text": "state-of-the-art foundation models it",
    "start": "254519",
    "end": "257519"
  },
  {
    "text": "removes deduplication right to ensure",
    "start": "257519",
    "end": "259979"
  },
  {
    "text": "there isn't bias in your data set right",
    "start": "259979",
    "end": "261919"
  },
  {
    "text": "allows you to do blending and filtering",
    "start": "261919",
    "end": "264419"
  },
  {
    "text": "so I'm Lebanese if you were to change a",
    "start": "264419",
    "end": "267720"
  },
  {
    "text": "large language model right 70 of that's",
    "start": "267720",
    "end": "270360"
  },
  {
    "text": "got to be English but then the other 30",
    "start": "270360",
    "end": "272340"
  },
  {
    "text": "has got to be in Arabic because when my",
    "start": "272340",
    "end": "273960"
  },
  {
    "text": "wife and I fight it's typically in",
    "start": "273960",
    "end": "275340"
  },
  {
    "text": "Arabic right so you got to make sure you",
    "start": "275340",
    "end": "276479"
  },
  {
    "text": "have that the right blend right other",
    "start": "276479",
    "end": "279360"
  },
  {
    "text": "things are done it does is it can read",
    "start": "279360",
    "end": "282240"
  },
  {
    "text": "HTML JavaScript",
    "start": "282240",
    "end": "284240"
  },
  {
    "text": "we use a data curator I think one of the",
    "start": "284240",
    "end": "287100"
  },
  {
    "text": "main value props as well is we use a",
    "start": "287100",
    "end": "288900"
  },
  {
    "text": "data curator to train one of our own",
    "start": "288900",
    "end": "290400"
  },
  {
    "text": "models at Nvidia I think we started with",
    "start": "290400",
    "end": "292560"
  },
  {
    "text": "3.3 trillion tokens after cleaning",
    "start": "292560",
    "end": "296340"
  },
  {
    "text": "deduplicating filtering the data set we",
    "start": "296340",
    "end": "298620"
  },
  {
    "text": "ended up with 1.2 trillion uh tokens",
    "start": "298620",
    "end": "301680"
  },
  {
    "text": "what that meant what that meant actually",
    "start": "301680",
    "end": "304080"
  },
  {
    "text": "reduce our training Time by half",
    "start": "304080",
    "end": "306479"
  },
  {
    "text": "right so higher quality data set reduce",
    "start": "306479",
    "end": "308580"
  },
  {
    "text": "our training time again value of of",
    "start": "308580",
    "end": "311100"
  },
  {
    "text": "accelerated Computing a couple of things",
    "start": "311100",
    "end": "313620"
  },
  {
    "text": "on the roadmap that we keep getting",
    "start": "313620",
    "end": "315000"
  },
  {
    "text": "asked about is we will support mix",
    "start": "315000",
    "end": "317660"
  },
  {
    "text": "models soon so you can you know inject",
    "start": "317660",
    "end": "320460"
  },
  {
    "text": "documents such as PDFs and then data",
    "start": "320460",
    "end": "323220"
  },
  {
    "text": "creation is also uh on the roadmap and",
    "start": "323220",
    "end": "325740"
  },
  {
    "text": "something that we get a lot of asks from",
    "start": "325740",
    "end": "328320"
  },
  {
    "text": "I'm going real fast because there's a",
    "start": "328320",
    "end": "330000"
  },
  {
    "text": "demo that I want to show at the end I",
    "start": "330000",
    "end": "331860"
  },
  {
    "text": "got jealous from the Adobe guys so we",
    "start": "331860",
    "end": "333600"
  },
  {
    "text": "want to make sure we show you our our",
    "start": "333600",
    "end": "335039"
  },
  {
    "text": "demo or not jealous I should say",
    "start": "335039",
    "end": "337259"
  },
  {
    "text": "inspired right inspired",
    "start": "337259",
    "end": "340560"
  },
  {
    "text": "okay waste the customize did anybody see",
    "start": "340560",
    "end": "343259"
  },
  {
    "text": "Lang chains uh Harrison's event",
    "start": "343259",
    "end": "345539"
  },
  {
    "text": "yesterday I mean I thought he did a",
    "start": "345539",
    "end": "346919"
  },
  {
    "text": "masterful job by explaining like how to",
    "start": "346919",
    "end": "348780"
  },
  {
    "text": "provide context so there's prop",
    "start": "348780",
    "end": "350580"
  },
  {
    "text": "engineering right that's a few short",
    "start": "350580",
    "end": "352199"
  },
  {
    "text": "learning or Chain of Thought reasoning",
    "start": "352199",
    "end": "354600"
  },
  {
    "text": "right like the the way you you prompt it",
    "start": "354600",
    "end": "357960"
  },
  {
    "text": "the way you you customize it through the",
    "start": "357960",
    "end": "359520"
  },
  {
    "text": "spoken language of English uh prompt",
    "start": "359520",
    "end": "362100"
  },
  {
    "text": "learning or P tuning is when you",
    "start": "362100",
    "end": "364139"
  },
  {
    "text": "actually put an lstm in front of the",
    "start": "364139",
    "end": "366600"
  },
  {
    "text": "large language model right so you're not",
    "start": "366600",
    "end": "368160"
  },
  {
    "text": "actually changing the parameters of the",
    "start": "368160",
    "end": "369419"
  },
  {
    "text": "longest large language model itself",
    "start": "369419",
    "end": "372360"
  },
  {
    "text": "um P EFT or parameter efficient fine",
    "start": "372360",
    "end": "375000"
  },
  {
    "text": "tuning is when you only alter a subset",
    "start": "375000",
    "end": "377940"
  },
  {
    "text": "or you fine-tune a subset of the",
    "start": "377940",
    "end": "379500"
  },
  {
    "text": "parameters in the large language model",
    "start": "379500",
    "end": "381000"
  },
  {
    "text": "right again with with tasks or domain",
    "start": "381000",
    "end": "384300"
  },
  {
    "text": "specific data and then obviously fine",
    "start": "384300",
    "end": "386340"
  },
  {
    "text": "tuning is you know RL HF or supervised",
    "start": "386340",
    "end": "390240"
  },
  {
    "text": "fine tuning again here it's more data",
    "start": "390240",
    "end": "392759"
  },
  {
    "text": "intensive more compute intensive right",
    "start": "392759",
    "end": "394919"
  },
  {
    "text": "and you're essentially you're changing",
    "start": "394919",
    "end": "396300"
  },
  {
    "text": "all the parameters of the model so the",
    "start": "396300",
    "end": "398460"
  },
  {
    "text": "gist of this slide is you know the",
    "start": "398460",
    "end": "400620"
  },
  {
    "text": "further right you go the more complex it",
    "start": "400620",
    "end": "402840"
  },
  {
    "text": "gets in terms of training but the more",
    "start": "402840",
    "end": "404400"
  },
  {
    "text": "accurate gets but that comes at a cost",
    "start": "404400",
    "end": "406440"
  },
  {
    "text": "of potential increased compute you know",
    "start": "406440",
    "end": "408479"
  },
  {
    "text": "data and resources",
    "start": "408479",
    "end": "411979"
  },
  {
    "text": "tensor RT llm right um",
    "start": "412259",
    "end": "416580"
  },
  {
    "text": "hopefully you guys caught this on stage",
    "start": "416580",
    "end": "418199"
  },
  {
    "text": "yesterday uh this is all about making",
    "start": "418199",
    "end": "421680"
  },
  {
    "text": "inference compute as efficient as",
    "start": "421680",
    "end": "423960"
  },
  {
    "text": "possible we just released this or",
    "start": "423960",
    "end": "426120"
  },
  {
    "text": "announced this I want to say 10 days ago",
    "start": "426120",
    "end": "428280"
  },
  {
    "text": "a week ago right and already our",
    "start": "428280",
    "end": "430919"
  },
  {
    "text": "Engineers are working very hard with the",
    "start": "430919",
    "end": "433020"
  },
  {
    "text": "ray teams Engineers right the Eddie",
    "start": "433020",
    "end": "434639"
  },
  {
    "text": "scale Engineers to integrate this in the",
    "start": "434639",
    "end": "436500"
  },
  {
    "text": "next release of race serve it's an open",
    "start": "436500",
    "end": "438720"
  },
  {
    "text": "source python API for defining",
    "start": "438720",
    "end": "441479"
  },
  {
    "text": "optimizing and executing models it",
    "start": "441479",
    "end": "444120"
  },
  {
    "text": "builds on top of our trt compiler and",
    "start": "444120",
    "end": "447479"
  },
  {
    "text": "faster Transformer consists of you know",
    "start": "447479",
    "end": "450000"
  },
  {
    "text": "pre-processing post-processing libraries",
    "start": "450000",
    "end": "452759"
  },
  {
    "text": "it's got optimized Fusion kernels that",
    "start": "452759",
    "end": "455520"
  },
  {
    "text": "includes flash attention",
    "start": "455520",
    "end": "457020"
  },
  {
    "text": "right it",
    "start": "457020",
    "end": "458940"
  },
  {
    "text": "um what else",
    "start": "458940",
    "end": "460259"
  },
  {
    "text": "uh I think I caught pretty much",
    "start": "460259",
    "end": "462479"
  },
  {
    "text": "everything oh yeah benchmarks why why",
    "start": "462479",
    "end": "464940"
  },
  {
    "text": "why",
    "start": "464940",
    "end": "466020"
  },
  {
    "text": "so with the CNN daily mail data set for",
    "start": "466020",
    "end": "468960"
  },
  {
    "text": "article summarization all right where we",
    "start": "468960",
    "end": "471599"
  },
  {
    "text": "had a variable i o length",
    "start": "471599",
    "end": "474000"
  },
  {
    "text": "compared to an a100 right with gptj",
    "start": "474000",
    "end": "477419"
  },
  {
    "text": "which is a 6 billion parameter model we",
    "start": "477419",
    "end": "479639"
  },
  {
    "text": "saw eight X improvements in latency",
    "start": "479639",
    "end": "482699"
  },
  {
    "text": "right on a100 versus h100 with trt llm",
    "start": "482699",
    "end": "486479"
  },
  {
    "text": "from a a TCO perspective that was a",
    "start": "486479",
    "end": "488639"
  },
  {
    "text": "reduction of 5.3 x",
    "start": "488639",
    "end": "490620"
  },
  {
    "text": "and from just a power efficiency",
    "start": "490620",
    "end": "492300"
  },
  {
    "text": "perspective like we're talking about",
    "start": "492300",
    "end": "493500"
  },
  {
    "text": "five you know between five and six five",
    "start": "493500",
    "end": "495599"
  },
  {
    "text": "and a half X you know improvements",
    "start": "495599",
    "end": "499020"
  },
  {
    "text": "um",
    "start": "499020",
    "end": "499979"
  },
  {
    "text": "but everyone's talking about llama 270b",
    "start": "499979",
    "end": "502199"
  },
  {
    "text": "right so I think it's only fair that I",
    "start": "502199",
    "end": "503759"
  },
  {
    "text": "share what kind of optimizations or",
    "start": "503759",
    "end": "505199"
  },
  {
    "text": "performance games we saw it was",
    "start": "505199",
    "end": "507599"
  },
  {
    "text": "4.6 x with llama 270b from a performance",
    "start": "507599",
    "end": "511379"
  },
  {
    "text": "perspective TCO was about 3x Improvement",
    "start": "511379",
    "end": "515099"
  },
  {
    "text": "power efficiency about 3.2 x",
    "start": "515099",
    "end": "517200"
  },
  {
    "text": "improvements",
    "start": "517200",
    "end": "518459"
  },
  {
    "text": "right super important",
    "start": "518459",
    "end": "520740"
  },
  {
    "text": "um LMS are highly dynamic",
    "start": "520740",
    "end": "523080"
  },
  {
    "text": "which means that output length very is",
    "start": "523080",
    "end": "526980"
  },
  {
    "text": "it like varies right like chatbot q a is",
    "start": "526980",
    "end": "529980"
  },
  {
    "text": "different to uh llm generating long",
    "start": "529980",
    "end": "531779"
  },
  {
    "text": "chunks of code which is different to",
    "start": "531779",
    "end": "533820"
  },
  {
    "text": "document summarization or document",
    "start": "533820",
    "end": "535680"
  },
  {
    "text": "creation right and so what happens is",
    "start": "535680",
    "end": "537779"
  },
  {
    "text": "versatility makes a difficult to like",
    "start": "537779",
    "end": "540180"
  },
  {
    "text": "bash all these requests and execute them",
    "start": "540180",
    "end": "541860"
  },
  {
    "text": "at once one of the main features and why",
    "start": "541860",
    "end": "544320"
  },
  {
    "text": "a lot of these performance gains are",
    "start": "544320",
    "end": "545580"
  },
  {
    "text": "seen is because of in-flight batching",
    "start": "545580",
    "end": "547560"
  },
  {
    "text": "and that's an optimization scheduling",
    "start": "547560",
    "end": "550019"
  },
  {
    "text": "technique what essentially does is it",
    "start": "550019",
    "end": "551880"
  },
  {
    "text": "affects a sequence that are complete and",
    "start": "551880",
    "end": "554640"
  },
  {
    "text": "begins executing the next sequence right",
    "start": "554640",
    "end": "556920"
  },
  {
    "text": "while the new ones are coming in right",
    "start": "556920",
    "end": "559320"
  },
  {
    "text": "so it really drives that concurrency and",
    "start": "559320",
    "end": "561480"
  },
  {
    "text": "the last thing I want to talk about here",
    "start": "561480",
    "end": "562620"
  },
  {
    "text": "is quantization",
    "start": "562620",
    "end": "564120"
  },
  {
    "text": "what quantization does is it allows you",
    "start": "564120",
    "end": "566100"
  },
  {
    "text": "go from a you know reducing Precision so",
    "start": "566100",
    "end": "568440"
  },
  {
    "text": "from fp16 to fp8 without compromising on",
    "start": "568440",
    "end": "573120"
  },
  {
    "text": "accuracy right so it reduces the",
    "start": "573120",
    "end": "574620"
  },
  {
    "text": "footprint of the model allows you to fit",
    "start": "574620",
    "end": "576000"
  },
  {
    "text": "it in in a smaller memory footprints",
    "start": "576000",
    "end": "580760"
  },
  {
    "text": "uh talking about Triton how am I doing",
    "start": "581459",
    "end": "584160"
  },
  {
    "text": "on time I'm okay one time okay so uh",
    "start": "584160",
    "end": "586560"
  },
  {
    "text": "China is our inference server uh we are",
    "start": "586560",
    "end": "589980"
  },
  {
    "text": "in discussions with the ray team in",
    "start": "589980",
    "end": "592680"
  },
  {
    "text": "terms of how we're going to figure out",
    "start": "592680",
    "end": "593880"
  },
  {
    "text": "what the right integration points are",
    "start": "593880",
    "end": "595500"
  },
  {
    "text": "here right but one of the things we",
    "start": "595500",
    "end": "598740"
  },
  {
    "text": "bring to the table is we support all you",
    "start": "598740",
    "end": "600839"
  },
  {
    "text": "know the backends from all major",
    "start": "600839",
    "end": "601800"
  },
  {
    "text": "Frameworks",
    "start": "601800",
    "end": "603000"
  },
  {
    "text": "you know your favorite training",
    "start": "603000",
    "end": "604260"
  },
  {
    "text": "Frameworks to you know trt LMS obviously",
    "start": "604260",
    "end": "607740"
  },
  {
    "text": "um we support different communication",
    "start": "607740",
    "end": "609000"
  },
  {
    "text": "protocols HTTP grpc C plus plus it works",
    "start": "609000",
    "end": "613680"
  },
  {
    "text": "on GPU and CPUs crazy right crazy uh",
    "start": "613680",
    "end": "617940"
  },
  {
    "text": "works on arm x86 uh Jetson works on",
    "start": "617940",
    "end": "622200"
  },
  {
    "text": "Windows supports different type of",
    "start": "622200",
    "end": "624839"
  },
  {
    "text": "queries right whether that's real time",
    "start": "624839",
    "end": "627300"
  },
  {
    "text": "batch or streaming",
    "start": "627300",
    "end": "629940"
  },
  {
    "text": "and one of the things that I think is",
    "start": "629940",
    "end": "631980"
  },
  {
    "text": "super important or critical about uh",
    "start": "631980",
    "end": "633779"
  },
  {
    "text": "Triton and again this is one of those",
    "start": "633779",
    "end": "635040"
  },
  {
    "text": "things we're collaborating on is the",
    "start": "635040",
    "end": "636660"
  },
  {
    "text": "model analyzer and what it does is it",
    "start": "636660",
    "end": "638519"
  },
  {
    "text": "tool defines optimal model",
    "start": "638519",
    "end": "640500"
  },
  {
    "text": "configurations",
    "start": "640500",
    "end": "642660"
  },
  {
    "text": "um to maximize performance by looking at",
    "start": "642660",
    "end": "645300"
  },
  {
    "text": "different constraints so you specify",
    "start": "645300",
    "end": "647100"
  },
  {
    "text": "like your latency your throughput your",
    "start": "647100",
    "end": "648779"
  },
  {
    "text": "GPU memory and what it does is",
    "start": "648779",
    "end": "650700"
  },
  {
    "text": "automatically sweeps through different",
    "start": "650700",
    "end": "651779"
  },
  {
    "text": "configurations and gives you back right",
    "start": "651779",
    "end": "654240"
  },
  {
    "text": "a an optimal route based on those",
    "start": "654240",
    "end": "657000"
  },
  {
    "text": "constraints right it's called the model",
    "start": "657000",
    "end": "658740"
  },
  {
    "text": "optimizer",
    "start": "658740",
    "end": "661740"
  },
  {
    "text": "okay guardrails",
    "start": "663420",
    "end": "665579"
  },
  {
    "text": "what are guard rails guardrails are an",
    "start": "665579",
    "end": "668220"
  },
  {
    "text": "open source toolkit to basically not",
    "start": "668220",
    "end": "671339"
  },
  {
    "text": "enable jailbreak policies",
    "start": "671339",
    "end": "673680"
  },
  {
    "text": "so developers can use guardrails to",
    "start": "673680",
    "end": "676440"
  },
  {
    "text": "program like filters or rails it's",
    "start": "676440",
    "end": "679980"
  },
  {
    "text": "almost like a dialogue manager for llms",
    "start": "679980",
    "end": "682440"
  },
  {
    "text": "right",
    "start": "682440",
    "end": "683660"
  },
  {
    "text": "it uses fuzzy logic and large language",
    "start": "683660",
    "end": "687060"
  },
  {
    "text": "models under the hood to understand the",
    "start": "687060",
    "end": "689160"
  },
  {
    "text": "prompts and like generate the responses",
    "start": "689160",
    "end": "690540"
  },
  {
    "text": "as well",
    "start": "690540",
    "end": "691500"
  },
  {
    "text": "and you can programmatically add topical",
    "start": "691500",
    "end": "694760"
  },
  {
    "text": "Safety and Security rails",
    "start": "694760",
    "end": "697920"
  },
  {
    "text": "what's an example of each",
    "start": "697920",
    "end": "699600"
  },
  {
    "text": "topical could be something like hey",
    "start": "699600",
    "end": "701600"
  },
  {
    "text": "stick to the subject matter at hand",
    "start": "701600",
    "end": "703800"
  },
  {
    "text": "don't talk about sports",
    "start": "703800",
    "end": "705779"
  },
  {
    "text": "don't talk about you know Financial",
    "start": "705779",
    "end": "707579"
  },
  {
    "text": "disclosures don't talk about competitors",
    "start": "707579",
    "end": "710100"
  },
  {
    "text": "right that's a nice thing to do",
    "start": "710100",
    "end": "712459"
  },
  {
    "text": "safety can be about uh you know don't be",
    "start": "712459",
    "end": "716459"
  },
  {
    "text": "don't don't spew any toxic content right",
    "start": "716459",
    "end": "718980"
  },
  {
    "text": "don't be harmful",
    "start": "718980",
    "end": "720360"
  },
  {
    "text": "you can actually use model to be harmful",
    "start": "720360",
    "end": "721800"
  },
  {
    "text": "like prevent hallucinations exclude",
    "start": "721800",
    "end": "723899"
  },
  {
    "text": "personal information right in your",
    "start": "723899",
    "end": "725640"
  },
  {
    "text": "responses could be another one from a",
    "start": "725640",
    "end": "727200"
  },
  {
    "text": "from a safety perspective and security",
    "start": "727200",
    "end": "729660"
  },
  {
    "text": "is all about like restricting the app",
    "start": "729660",
    "end": "731519"
  },
  {
    "text": "from Making Connections to just like",
    "start": "731519",
    "end": "733680"
  },
  {
    "text": "verified or certified apis right so you",
    "start": "733680",
    "end": "736500"
  },
  {
    "text": "can actually dictate what it can speak",
    "start": "736500",
    "end": "737760"
  },
  {
    "text": "to",
    "start": "737760",
    "end": "738480"
  },
  {
    "text": "uh what's awesome about the guardrails",
    "start": "738480",
    "end": "740459"
  },
  {
    "text": "is they sit between the user interaction",
    "start": "740459",
    "end": "742019"
  },
  {
    "text": "and your chatbot and you know you can",
    "start": "742019",
    "end": "744839"
  },
  {
    "text": "connect it via toolkits like macchate",
    "start": "744839",
    "end": "747779"
  },
  {
    "text": "among others",
    "start": "747779",
    "end": "750560"
  },
  {
    "text": "okay so I I know I went through a lot",
    "start": "751079",
    "end": "754620"
  },
  {
    "text": "but there's also a lot for us to cover",
    "start": "754620",
    "end": "757380"
  },
  {
    "text": "um so we talked about the value prop of",
    "start": "757380",
    "end": "759060"
  },
  {
    "text": "accelerated Computing the context of",
    "start": "759060",
    "end": "760440"
  },
  {
    "text": "Nemo different ways to customize it",
    "start": "760440",
    "end": "762839"
  },
  {
    "text": "different ways to ensure that the data",
    "start": "762839",
    "end": "764639"
  },
  {
    "text": "that you're bringing into the model is",
    "start": "764639",
    "end": "766560"
  },
  {
    "text": "curated balanced unbiased right all the",
    "start": "766560",
    "end": "769500"
  },
  {
    "text": "way through inferencing to make sure",
    "start": "769500",
    "end": "770760"
  },
  {
    "text": "your model stays uh you know in check",
    "start": "770760",
    "end": "773160"
  },
  {
    "text": "right now let's pivot real quick and",
    "start": "773160",
    "end": "776160"
  },
  {
    "text": "think about Enterprise software what it",
    "start": "776160",
    "end": "778500"
  },
  {
    "text": "means to be Enterprise software",
    "start": "778500",
    "end": "780180"
  },
  {
    "text": "as I as I told you guys earlier",
    "start": "780180",
    "end": "783060"
  },
  {
    "text": "um",
    "start": "783060",
    "end": "783720"
  },
  {
    "text": "uh we contenderize a lot of our software",
    "start": "783720",
    "end": "785820"
  },
  {
    "text": "what people don't realize is there are",
    "start": "785820",
    "end": "789180"
  },
  {
    "text": "four thousand I know these numbers",
    "start": "789180",
    "end": "791339"
  },
  {
    "text": "um you know accurately because I have",
    "start": "791339",
    "end": "792959"
  },
  {
    "text": "the countdown for our CEO 4471",
    "start": "792959",
    "end": "796500"
  },
  {
    "text": "Nvidia packages open source packages and",
    "start": "796500",
    "end": "800100"
  },
  {
    "text": "third-party packages creating",
    "start": "800100",
    "end": "802760"
  },
  {
    "text": "9256 dependencies",
    "start": "802760",
    "end": "805260"
  },
  {
    "text": "right across these base containers it's",
    "start": "805260",
    "end": "808740"
  },
  {
    "text": "it's a gigantic gigantic body of work",
    "start": "808740",
    "end": "811620"
  },
  {
    "text": "that we have to maintain",
    "start": "811620",
    "end": "813720"
  },
  {
    "text": "um but we do this on a monthly basis we",
    "start": "813720",
    "end": "816240"
  },
  {
    "text": "do this on a monthly basis I want to",
    "start": "816240",
    "end": "817500"
  },
  {
    "text": "spend a bit of time on this slide",
    "start": "817500",
    "end": "819720"
  },
  {
    "text": "so what do we do every month",
    "start": "819720",
    "end": "821760"
  },
  {
    "text": "for for tensor RT for trying for pi",
    "start": "821760",
    "end": "825120"
  },
  {
    "text": "torch we figure out what are the right",
    "start": "825120",
    "end": "827160"
  },
  {
    "text": "set of 500 packages for a given",
    "start": "827160",
    "end": "830399"
  },
  {
    "text": "container figure out what the right",
    "start": "830399",
    "end": "832320"
  },
  {
    "text": "knobs are to tune it so we can get the",
    "start": "832320",
    "end": "834420"
  },
  {
    "text": "best Peak Performance on our gpus",
    "start": "834420",
    "end": "836700"
  },
  {
    "text": "before we make the container available",
    "start": "836700",
    "end": "838139"
  },
  {
    "text": "on our container registry",
    "start": "838139",
    "end": "839820"
  },
  {
    "text": "right we scan it",
    "start": "839820",
    "end": "841860"
  },
  {
    "text": "we address critical and high",
    "start": "841860",
    "end": "843720"
  },
  {
    "text": "vulnerabilities anybody know what",
    "start": "843720",
    "end": "844800"
  },
  {
    "text": "vulnerabilities are",
    "start": "844800",
    "end": "846839"
  },
  {
    "text": "right when you're when you're dealing",
    "start": "846839",
    "end": "848040"
  },
  {
    "text": "with open source software right attack",
    "start": "848040",
    "end": "850620"
  },
  {
    "text": "surface for exploiting grows",
    "start": "850620",
    "end": "852480"
  },
  {
    "text": "considerably right when you're you know",
    "start": "852480",
    "end": "855120"
  },
  {
    "text": "absorbing a cloud service your cloud",
    "start": "855120",
    "end": "857399"
  },
  {
    "text": "provider is usually doing that patching",
    "start": "857399",
    "end": "858839"
  },
  {
    "text": "for you right but if you're doing",
    "start": "858839",
    "end": "860220"
  },
  {
    "text": "deploying containers right the vendor or",
    "start": "860220",
    "end": "862920"
  },
  {
    "text": "as the Enterprise or the user you want",
    "start": "862920",
    "end": "864480"
  },
  {
    "text": "to be patching those containers as well",
    "start": "864480",
    "end": "865800"
  },
  {
    "text": "so before we make any of our software",
    "start": "865800",
    "end": "867480"
  },
  {
    "text": "available we patch it for critical and",
    "start": "867480",
    "end": "869160"
  },
  {
    "text": "high vulnerabilities but we never go",
    "start": "869160",
    "end": "871019"
  },
  {
    "text": "back and touch that container next month",
    "start": "871019",
    "end": "873060"
  },
  {
    "text": "we go through the exact same process",
    "start": "873060",
    "end": "874860"
  },
  {
    "text": "right we picked the right set of",
    "start": "874860",
    "end": "876240"
  },
  {
    "text": "libraries the top of three latest",
    "start": "876240",
    "end": "877860"
  },
  {
    "text": "pytorch release return all the knobs we",
    "start": "877860",
    "end": "880019"
  },
  {
    "text": "squeeze the best performance out of it",
    "start": "880019",
    "end": "881820"
  },
  {
    "text": "and we publish it on NGC right we scan",
    "start": "881820",
    "end": "884279"
  },
  {
    "text": "it we publish our NGC one important",
    "start": "884279",
    "end": "886260"
  },
  {
    "text": "point to call out",
    "start": "886260",
    "end": "887519"
  },
  {
    "text": "vulnerabilities are a point in time",
    "start": "887519",
    "end": "889019"
  },
  {
    "text": "statement",
    "start": "889019",
    "end": "890279"
  },
  {
    "text": "that means because a container is free",
    "start": "890279",
    "end": "892560"
  },
  {
    "text": "of vulnerabilities today if you see",
    "start": "892560",
    "end": "894000"
  },
  {
    "text": "rescan that container two weeks from now",
    "start": "894000",
    "end": "895620"
  },
  {
    "text": "you're gonna find new critical and high",
    "start": "895620",
    "end": "897120"
  },
  {
    "text": "vulnerabilities",
    "start": "897120",
    "end": "898500"
  },
  {
    "text": "and our posture to date spend don't",
    "start": "898500",
    "end": "900600"
  },
  {
    "text": "worry Mr customer",
    "start": "900600",
    "end": "901920"
  },
  {
    "text": "right next month I'm going to publish a",
    "start": "901920",
    "end": "903240"
  },
  {
    "text": "new container new set of libraries but",
    "start": "903240",
    "end": "905339"
  },
  {
    "text": "it's going to be new containers there",
    "start": "905339",
    "end": "906600"
  },
  {
    "text": "publish it it's not going to have any",
    "start": "906600",
    "end": "907800"
  },
  {
    "text": "critical High vulnerabilities",
    "start": "907800",
    "end": "909720"
  },
  {
    "text": "the problem with that is right by",
    "start": "909720",
    "end": "912180"
  },
  {
    "text": "changing libraries you introduce",
    "start": "912180",
    "end": "913440"
  },
  {
    "text": "breaking changes into the container",
    "start": "913440",
    "end": "916019"
  },
  {
    "text": "right change of libraries can't",
    "start": "916019",
    "end": "917579"
  },
  {
    "text": "introduce breaking changes to containers",
    "start": "917579",
    "end": "918899"
  },
  {
    "text": "and that might be okay if you're doing",
    "start": "918899",
    "end": "920579"
  },
  {
    "text": "development",
    "start": "920579",
    "end": "921779"
  },
  {
    "text": "right but would be problematic if you're",
    "start": "921779",
    "end": "923639"
  },
  {
    "text": "doing inferencing",
    "start": "923639",
    "end": "925440"
  },
  {
    "text": "right and so what I've described is what",
    "start": "925440",
    "end": "928019"
  },
  {
    "text": "we call our development branch",
    "start": "928019",
    "end": "930000"
  },
  {
    "text": "with Nvidia AI Enterprise which is",
    "start": "930000",
    "end": "931860"
  },
  {
    "text": "basically taking an open source software",
    "start": "931860",
    "end": "933540"
  },
  {
    "text": "we're introducing production branches",
    "start": "933540",
    "end": "935639"
  },
  {
    "text": "next month right and long-term supported",
    "start": "935639",
    "end": "937920"
  },
  {
    "text": "branches what that means is for the",
    "start": "937920",
    "end": "940560"
  },
  {
    "text": "production brushes for a duration of",
    "start": "940560",
    "end": "942060"
  },
  {
    "text": "nine months two branches per year we're",
    "start": "942060",
    "end": "944160"
  },
  {
    "text": "gonna do all that patching without",
    "start": "944160",
    "end": "945600"
  },
  {
    "text": "changing the underlying libraries",
    "start": "945600",
    "end": "948000"
  },
  {
    "text": "right without changing the versions of",
    "start": "948000",
    "end": "949620"
  },
  {
    "text": "the underlying libraries and so what",
    "start": "949620",
    "end": "951420"
  },
  {
    "text": "that provides you is guarantees slas",
    "start": "951420",
    "end": "953519"
  },
  {
    "text": "that Nvidia will commit to fit critical",
    "start": "953519",
    "end": "955019"
  },
  {
    "text": "and high vulnerabilities without",
    "start": "955019",
    "end": "956459"
  },
  {
    "text": "changing apis on you",
    "start": "956459",
    "end": "958860"
  },
  {
    "text": "and the bottom row is even more",
    "start": "958860",
    "end": "960660"
  },
  {
    "text": "Draconian than that it's for long-term",
    "start": "960660",
    "end": "962459"
  },
  {
    "text": "support of branches where for a duration",
    "start": "962459",
    "end": "964139"
  },
  {
    "text": "of 36 months right we undertake that",
    "start": "964139",
    "end": "967380"
  },
  {
    "text": "patching and maintenance right so you",
    "start": "967380",
    "end": "969660"
  },
  {
    "text": "know that for three years a minimum of",
    "start": "969660",
    "end": "971279"
  },
  {
    "text": "three years you're gonna have the same",
    "start": "971279",
    "end": "972720"
  },
  {
    "text": "stack that you could Deploy on now some",
    "start": "972720",
    "end": "974639"
  },
  {
    "text": "of you might be wondering like who would",
    "start": "974639",
    "end": "976019"
  },
  {
    "text": "in what world would you do this",
    "start": "976019",
    "end": "978060"
  },
  {
    "text": "plenty of domains Auto",
    "start": "978060",
    "end": "980100"
  },
  {
    "text": "Medical Imaging",
    "start": "980100",
    "end": "981660"
  },
  {
    "text": "right Telco right we're doing a lot of",
    "start": "981660",
    "end": "983579"
  },
  {
    "text": "deployments you know uh in remote areas",
    "start": "983579",
    "end": "986880"
  },
  {
    "text": "and so I think the important piece here",
    "start": "986880",
    "end": "988320"
  },
  {
    "text": "to call out is that these aren't",
    "start": "988320",
    "end": "989639"
  },
  {
    "text": "mutually exclusive we see Enterprise and",
    "start": "989639",
    "end": "992040"
  },
  {
    "text": "organizations using development branches",
    "start": "992040",
    "end": "993480"
  },
  {
    "text": "for the latest features I'm gonna do",
    "start": "993480",
    "end": "995339"
  },
  {
    "text": "sparseity XYZ right with the latest",
    "start": "995339",
    "end": "996899"
  },
  {
    "text": "python container but hey from my",
    "start": "996899",
    "end": "999000"
  },
  {
    "text": "production deployment I want the trying",
    "start": "999000",
    "end": "1000740"
  },
  {
    "text": "inference server trt llm server right I",
    "start": "1000740",
    "end": "1003259"
  },
  {
    "text": "need it to be stable right and so again",
    "start": "1003259",
    "end": "1006019"
  },
  {
    "text": "just a gigantic task when you consider",
    "start": "1006019",
    "end": "1007759"
  },
  {
    "text": "the amount of packages that are part of",
    "start": "1007759",
    "end": "1009800"
  },
  {
    "text": "these different containers",
    "start": "1009800",
    "end": "1012680"
  },
  {
    "text": "um thanks to my good friends and",
    "start": "1012680",
    "end": "1014480"
  },
  {
    "text": "colleagues over here this is a an any",
    "start": "1014480",
    "end": "1018440"
  },
  {
    "text": "scale platform uh value prop slide and",
    "start": "1018440",
    "end": "1022459"
  },
  {
    "text": "they could probably speak more to it",
    "start": "1022459",
    "end": "1023600"
  },
  {
    "text": "than I can but I just thought it was you",
    "start": "1023600",
    "end": "1026959"
  },
  {
    "text": "know given what the the three Vibe props",
    "start": "1026959",
    "end": "1029839"
  },
  {
    "text": "I've been talking about right like",
    "start": "1029839",
    "end": "1031100"
  },
  {
    "text": "accelerated Computing TCO make sure a",
    "start": "1031100",
    "end": "1034040"
  },
  {
    "text": "production grade and and meeting",
    "start": "1034040",
    "end": "1036260"
  },
  {
    "text": "developers where they are right I didn't",
    "start": "1036260",
    "end": "1038540"
  },
  {
    "text": "alter this slide",
    "start": "1038540",
    "end": "1039860"
  },
  {
    "text": "but that message is is",
    "start": "1039860",
    "end": "1042199"
  },
  {
    "text": "pretty consistent with what we're",
    "start": "1042199",
    "end": "1044120"
  },
  {
    "text": "talking about right and so that's why I",
    "start": "1044120",
    "end": "1046220"
  },
  {
    "text": "think this this marriage this",
    "start": "1046220",
    "end": "1047660"
  },
  {
    "text": "relationship between any scale and video",
    "start": "1047660",
    "end": "1049820"
  },
  {
    "text": "and that's just beginning now is is",
    "start": "1049820",
    "end": "1052160"
  },
  {
    "text": "going to grow and improve",
    "start": "1052160",
    "end": "1054940"
  },
  {
    "text": "uh real quick uh",
    "start": "1055700",
    "end": "1059059"
  },
  {
    "text": "rags and and this is a pretty uh you",
    "start": "1059059",
    "end": "1061580"
  },
  {
    "text": "know",
    "start": "1061580",
    "end": "1062360"
  },
  {
    "text": "straightforward example right and again",
    "start": "1062360",
    "end": "1064160"
  },
  {
    "text": "kind of goes back to a lot of the use",
    "start": "1064160",
    "end": "1066200"
  },
  {
    "text": "cases we're seeing are about using",
    "start": "1066200",
    "end": "1067460"
  },
  {
    "text": "retrieval augmented generation and so",
    "start": "1067460",
    "end": "1069919"
  },
  {
    "text": "what that means is just making sure that",
    "start": "1069919",
    "end": "1071360"
  },
  {
    "text": "your model's got context right uh has",
    "start": "1071360",
    "end": "1074900"
  },
  {
    "text": "access to your knowledge base right it's",
    "start": "1074900",
    "end": "1077120"
  },
  {
    "text": "pretty lame an example here but uh you",
    "start": "1077120",
    "end": "1079340"
  },
  {
    "text": "know I was trained a couple months ago",
    "start": "1079340",
    "end": "1080780"
  },
  {
    "text": "it's like I have no idea what the you",
    "start": "1080780",
    "end": "1083120"
  },
  {
    "text": "know what was charged on your credit",
    "start": "1083120",
    "end": "1084320"
  },
  {
    "text": "card",
    "start": "1084320",
    "end": "1085280"
  },
  {
    "text": "and so uh the demo that we're going to",
    "start": "1085280",
    "end": "1088220"
  },
  {
    "text": "show is a rack based demo we're going to",
    "start": "1088220",
    "end": "1090620"
  },
  {
    "text": "show uh basically how we use Lang chain",
    "start": "1090620",
    "end": "1094460"
  },
  {
    "text": "we use Ray we use any scale to build uh",
    "start": "1094460",
    "end": "1098840"
  },
  {
    "text": "you know a a rag based co-pilot and to",
    "start": "1098840",
    "end": "1102380"
  },
  {
    "text": "be honest with you right as",
    "start": "1102380",
    "end": "1104360"
  },
  {
    "text": "the infrastructure you know full stack",
    "start": "1104360",
    "end": "1106340"
  },
  {
    "text": "accelerated Computing company",
    "start": "1106340",
    "end": "1108140"
  },
  {
    "text": "right I'm acting like a trusted advisor",
    "start": "1108140",
    "end": "1110900"
  },
  {
    "text": "here right I'm not telling you to go",
    "start": "1110900",
    "end": "1112760"
  },
  {
    "text": "customize models",
    "start": "1112760",
    "end": "1114679"
  },
  {
    "text": "right we're not asking to go customize",
    "start": "1114679",
    "end": "1116360"
  },
  {
    "text": "models more often than not you know a a",
    "start": "1116360",
    "end": "1119240"
  },
  {
    "text": "copilot with an Enterprise's I.T data",
    "start": "1119240",
    "end": "1122240"
  },
  {
    "text": "their HR data their travel and expense",
    "start": "1122240",
    "end": "1124880"
  },
  {
    "text": "data right that's the use case that",
    "start": "1124880",
    "end": "1126919"
  },
  {
    "text": "every Enterprise can benefit from and",
    "start": "1126919",
    "end": "1128960"
  },
  {
    "text": "frankly that's what all the Enterprise",
    "start": "1128960",
    "end": "1130220"
  },
  {
    "text": "that we're engaged with are trying first",
    "start": "1130220",
    "end": "1131900"
  },
  {
    "text": "and foremost right rack based approach",
    "start": "1131900",
    "end": "1135160"
  },
  {
    "text": "so a couple of software pieces that",
    "start": "1135160",
    "end": "1138740"
  },
  {
    "text": "we're going to leverage as part of this",
    "start": "1138740",
    "end": "1140179"
  },
  {
    "text": "workflow uh number one is is workbench",
    "start": "1140179",
    "end": "1144260"
  },
  {
    "text": "what workbench does is essentially does",
    "start": "1144260",
    "end": "1146900"
  },
  {
    "text": "a couple things",
    "start": "1146900",
    "end": "1148100"
  },
  {
    "text": "if you have a GPU it provides it enables",
    "start": "1148100",
    "end": "1151520"
  },
  {
    "text": "anybody with a GPU on a local system to",
    "start": "1151520",
    "end": "1153260"
  },
  {
    "text": "become a gen AI creator",
    "start": "1153260",
    "end": "1155299"
  },
  {
    "text": "through this concept called project it",
    "start": "1155299",
    "end": "1157460"
  },
  {
    "text": "sets up all the drivers the stack on",
    "start": "1157460",
    "end": "1159500"
  },
  {
    "text": "your given machine",
    "start": "1159500",
    "end": "1160880"
  },
  {
    "text": "makes it really easy for you really easy",
    "start": "1160880",
    "end": "1162500"
  },
  {
    "text": "for you to spin up a Jupiter notebook",
    "start": "1162500",
    "end": "1163700"
  },
  {
    "text": "pip install whatever you need and as",
    "start": "1163700",
    "end": "1166820"
  },
  {
    "text": "your compute needs grow you can stick it",
    "start": "1166820",
    "end": "1169460"
  },
  {
    "text": "back to the cloud and then deploy it on",
    "start": "1169460",
    "end": "1171679"
  },
  {
    "text": "you know on on a cloud service provider",
    "start": "1171679",
    "end": "1174559"
  },
  {
    "text": "of your choice it just makes it super",
    "start": "1174559",
    "end": "1176299"
  },
  {
    "text": "easy it's an early access",
    "start": "1176299",
    "end": "1179480"
  },
  {
    "text": "um I think we're going to start",
    "start": "1179480",
    "end": "1180320"
  },
  {
    "text": "admitting folks next month's I'll share",
    "start": "1180320",
    "end": "1182600"
  },
  {
    "text": "the link towards the end of this to make",
    "start": "1182600",
    "end": "1184640"
  },
  {
    "text": "it available so why did I introduce",
    "start": "1184640",
    "end": "1186080"
  },
  {
    "text": "workbench well this is the demo we're",
    "start": "1186080",
    "end": "1187400"
  },
  {
    "text": "going to go through",
    "start": "1187400",
    "end": "1188480"
  },
  {
    "text": "right we're gonna pick a llama 2 model",
    "start": "1188480",
    "end": "1191480"
  },
  {
    "text": "we're gonna use the tensor RT llm",
    "start": "1191480",
    "end": "1195100"
  },
  {
    "text": "container that's embedded as part of the",
    "start": "1195100",
    "end": "1197059"
  },
  {
    "text": "Nemo inference container that I spoke",
    "start": "1197059",
    "end": "1198380"
  },
  {
    "text": "about we're going to use workbench we're",
    "start": "1198380",
    "end": "1200539"
  },
  {
    "text": "going to create a ray compatible",
    "start": "1200539",
    "end": "1201679"
  },
  {
    "text": "environment locally",
    "start": "1201679",
    "end": "1203419"
  },
  {
    "text": "right then we're going to scale to the",
    "start": "1203419",
    "end": "1204919"
  },
  {
    "text": "cloud as our compute needs grow",
    "start": "1204919",
    "end": "1209480"
  },
  {
    "text": "and so without further Ado hopefully",
    "start": "1209480",
    "end": "1212120"
  },
  {
    "text": "it's not going to muck up on me",
    "start": "1212120",
    "end": "1215320"
  },
  {
    "text": "again inspired by the Adobe demo",
    "start": "1220760",
    "end": "1224860"
  },
  {
    "text": "Nvidia and any scale are partnering to",
    "start": "1230919",
    "end": "1233600"
  },
  {
    "text": "provide developers an easy way to build",
    "start": "1233600",
    "end": "1235700"
  },
  {
    "text": "and deploy large language models faster",
    "start": "1235700",
    "end": "1238760"
  },
  {
    "text": "Rey any scale's open source platform",
    "start": "1238760",
    "end": "1241340"
  },
  {
    "text": "offers the flexibility unification and",
    "start": "1241340",
    "end": "1244760"
  },
  {
    "text": "orchestration efficiencies developers",
    "start": "1244760",
    "end": "1246679"
  },
  {
    "text": "need to easily distribute and",
    "start": "1246679",
    "end": "1248900"
  },
  {
    "text": "parallelize their code while Nvidia",
    "start": "1248900",
    "end": "1251360"
  },
  {
    "text": "delivers a performance and optimizations",
    "start": "1251360",
    "end": "1253340"
  },
  {
    "text": "needed to run large language models or",
    "start": "1253340",
    "end": "1255919"
  },
  {
    "text": "llms at scale since taking llm based",
    "start": "1255919",
    "end": "1260299"
  },
  {
    "text": "code from open source software to",
    "start": "1260299",
    "end": "1261980"
  },
  {
    "text": "production is new for many developers",
    "start": "1261980",
    "end": "1263840"
  },
  {
    "text": "we'll show how easy it is to start from",
    "start": "1263840",
    "end": "1266480"
  },
  {
    "text": "open source to build a custom llama 2",
    "start": "1266480",
    "end": "1268880"
  },
  {
    "text": "based co-pilot and then deployed into",
    "start": "1268880",
    "end": "1271340"
  },
  {
    "text": "production with any scale and Nvidia AI",
    "start": "1271340",
    "end": "1273860"
  },
  {
    "text": "Enterprise software",
    "start": "1273860",
    "end": "1275600"
  },
  {
    "text": "co-pilots provide a conversation",
    "start": "1275600",
    "end": "1277520"
  },
  {
    "text": "interface that uses an llm so anyone",
    "start": "1277520",
    "end": "1280520"
  },
  {
    "text": "within an Enterprise can create new",
    "start": "1280520",
    "end": "1282559"
  },
  {
    "text": "content generate ideas and automate",
    "start": "1282559",
    "end": "1285559"
  },
  {
    "text": "tasks",
    "start": "1285559",
    "end": "1286700"
  },
  {
    "text": "here's the end-to-end workflow for",
    "start": "1286700",
    "end": "1288679"
  },
  {
    "text": "developing a co-pilot it starts with a",
    "start": "1288679",
    "end": "1291260"
  },
  {
    "text": "developer working on a GPU accelerated",
    "start": "1291260",
    "end": "1293360"
  },
  {
    "text": "local workstation the Nvidia AI software",
    "start": "1293360",
    "end": "1296419"
  },
  {
    "text": "needed to create the co-pilot is pulled",
    "start": "1296419",
    "end": "1298580"
  },
  {
    "text": "from the Nvidia NGC code repository and",
    "start": "1298580",
    "end": "1301520"
  },
  {
    "text": "is built to be compatible with any",
    "start": "1301520",
    "end": "1303260"
  },
  {
    "text": "scales rate Nvidia AI workbench is used",
    "start": "1303260",
    "end": "1306559"
  },
  {
    "text": "to launch Jupiter and pull the Llama 2",
    "start": "1306559",
    "end": "1308720"
  },
  {
    "text": "language model from hugging face",
    "start": "1308720",
    "end": "1311240"
  },
  {
    "text": "we ask the model a question about Nvidia",
    "start": "1311240",
    "end": "1313820"
  },
  {
    "text": "and anyscales partnership but since the",
    "start": "1313820",
    "end": "1316220"
  },
  {
    "text": "model hasn't been augmented it provides",
    "start": "1316220",
    "end": "1318380"
  },
  {
    "text": "a generic response with no mention of",
    "start": "1318380",
    "end": "1320659"
  },
  {
    "text": "Nvidia AI software",
    "start": "1320659",
    "end": "1323179"
  },
  {
    "text": "to fix this we'll augment the llama2",
    "start": "1323179",
    "end": "1325820"
  },
  {
    "text": "model with a vector database and Lang",
    "start": "1325820",
    "end": "1327860"
  },
  {
    "text": "chain using a retrieval augmented",
    "start": "1327860",
    "end": "1329780"
  },
  {
    "text": "generation also called a rag pipeline",
    "start": "1329780",
    "end": "1333799"
  },
  {
    "text": "documents related to Nvidia and any",
    "start": "1333799",
    "end": "1336020"
  },
  {
    "text": "scale can be uploaded using local",
    "start": "1336020",
    "end": "1337880"
  },
  {
    "text": "resources or scaled using the cloud",
    "start": "1337880",
    "end": "1340480"
  },
  {
    "text": "since we have thousands of documents we",
    "start": "1340480",
    "end": "1343340"
  },
  {
    "text": "need more compute resources it's easy to",
    "start": "1343340",
    "end": "1346280"
  },
  {
    "text": "create embeddings and prepare for",
    "start": "1346280",
    "end": "1348080"
  },
  {
    "text": "production deployment in the cloud since",
    "start": "1348080",
    "end": "1350059"
  },
  {
    "text": "we already set up array compatible",
    "start": "1350059",
    "end": "1351799"
  },
  {
    "text": "environment",
    "start": "1351799",
    "end": "1352940"
  },
  {
    "text": "by leveraging any scale's command line",
    "start": "1352940",
    "end": "1355039"
  },
  {
    "text": "interface and Nvidia AI workbench the",
    "start": "1355039",
    "end": "1358640"
  },
  {
    "text": "whole project is uploaded to an any",
    "start": "1358640",
    "end": "1360440"
  },
  {
    "text": "scale workspace",
    "start": "1360440",
    "end": "1362480"
  },
  {
    "text": "before deploying the model we need to",
    "start": "1362480",
    "end": "1364580"
  },
  {
    "text": "optimize the llama2 model with Nvidia",
    "start": "1364580",
    "end": "1366860"
  },
  {
    "text": "trt llm which is Now supported in",
    "start": "1366860",
    "end": "1369799"
  },
  {
    "text": "raceserv",
    "start": "1369799",
    "end": "1371240"
  },
  {
    "text": "this provides massive latency",
    "start": "1371240",
    "end": "1372980"
  },
  {
    "text": "improvements deploying the rag improved",
    "start": "1372980",
    "end": "1375740"
  },
  {
    "text": "model across multiple nodes as an any",
    "start": "1375740",
    "end": "1378140"
  },
  {
    "text": "scale service is achieved with one",
    "start": "1378140",
    "end": "1380000"
  },
  {
    "text": "command next we grab the endpoint",
    "start": "1380000",
    "end": "1382940"
  },
  {
    "text": "information and easily integrate it into",
    "start": "1382940",
    "end": "1385159"
  },
  {
    "text": "our Enterprise application via python or",
    "start": "1385159",
    "end": "1387740"
  },
  {
    "text": "curl",
    "start": "1387740",
    "end": "1389780"
  },
  {
    "text": "since the model is augmented with the",
    "start": "1389780",
    "end": "1391640"
  },
  {
    "text": "latest information it delivers a more",
    "start": "1391640",
    "end": "1393740"
  },
  {
    "text": "accurate response to the question about",
    "start": "1393740",
    "end": "1395600"
  },
  {
    "text": "Nvidia and any scales partnership",
    "start": "1395600",
    "end": "1397539"
  },
  {
    "text": "together Nvidia and any scale are making",
    "start": "1397539",
    "end": "1400760"
  },
  {
    "text": "sure that developers can quickly and",
    "start": "1400760",
    "end": "1402620"
  },
  {
    "text": "efficiently create language-based",
    "start": "1402620",
    "end": "1404419"
  },
  {
    "text": "generative AI applications and Nvidia AI",
    "start": "1404419",
    "end": "1407600"
  },
  {
    "text": "software ensures compatibility low",
    "start": "1407600",
    "end": "1410000"
  },
  {
    "text": "latency and high throughput inference",
    "start": "1410000",
    "end": "1412340"
  },
  {
    "text": "sign up for a free Nvidia AI Enterprise",
    "start": "1412340",
    "end": "1415179"
  },
  {
    "text": "90-day eval to get started today",
    "start": "1415179",
    "end": "1418400"
  },
  {
    "text": "[Music]",
    "start": "1418400",
    "end": "1422019"
  },
  {
    "text": "thank you",
    "start": "1422720",
    "end": "1425500"
  },
  {
    "text": "yeah so hopefully you guys uh found that",
    "start": "1431480",
    "end": "1433940"
  },
  {
    "text": "exciting",
    "start": "1433940",
    "end": "1435980"
  },
  {
    "text": "right uh creating a ray compatible",
    "start": "1435980",
    "end": "1438679"
  },
  {
    "text": "environment was as easy as like a pip",
    "start": "1438679",
    "end": "1440240"
  },
  {
    "text": "install and a Docker file",
    "start": "1440240",
    "end": "1442340"
  },
  {
    "text": "super easy then we spawned up a Jupiter",
    "start": "1442340",
    "end": "1444500"
  },
  {
    "text": "notebook using workbench which set up",
    "start": "1444500",
    "end": "1446659"
  },
  {
    "text": "all the infrastructure you needed for",
    "start": "1446659",
    "end": "1448880"
  },
  {
    "text": "the GPU like locally",
    "start": "1448880",
    "end": "1450620"
  },
  {
    "text": "we then pulled a llama 2 model",
    "start": "1450620",
    "end": "1453080"
  },
  {
    "text": "then using the any scale CLI it was",
    "start": "1453080",
    "end": "1455120"
  },
  {
    "text": "super easy to farm back out to the to",
    "start": "1455120",
    "end": "1457039"
  },
  {
    "text": "your platform",
    "start": "1457039",
    "end": "1458360"
  },
  {
    "text": "where right because we had a lot more",
    "start": "1458360",
    "end": "1460900"
  },
  {
    "text": "documents to store you know to create",
    "start": "1460900",
    "end": "1463460"
  },
  {
    "text": "the embeddings installment of vector",
    "start": "1463460",
    "end": "1464720"
  },
  {
    "text": "database right we used more compute",
    "start": "1464720",
    "end": "1466220"
  },
  {
    "text": "there again always was super easy but I",
    "start": "1466220",
    "end": "1469640"
  },
  {
    "text": "didn't do any of the work as the team",
    "start": "1469640",
    "end": "1471380"
  },
  {
    "text": "that's sitting here up front so you guys",
    "start": "1471380",
    "end": "1472640"
  },
  {
    "text": "have any technical questions today AC",
    "start": "1472640",
    "end": "1475580"
  },
  {
    "text": "Julie put up your hands",
    "start": "1475580",
    "end": "1477919"
  },
  {
    "text": "don't be shy now right take the credit",
    "start": "1477919",
    "end": "1481580"
  },
  {
    "text": "they're actually looking like no we",
    "start": "1481580",
    "end": "1482900"
  },
  {
    "text": "didn't do this you got their credit as",
    "start": "1482900",
    "end": "1484940"
  },
  {
    "text": "well",
    "start": "1484940",
    "end": "1486260"
  },
  {
    "text": "um okay so so kind of bringing all this",
    "start": "1486260",
    "end": "1488960"
  },
  {
    "text": "together right and then we'll have some",
    "start": "1488960",
    "end": "1490700"
  },
  {
    "text": "some time for questions",
    "start": "1490700",
    "end": "1492679"
  },
  {
    "text": "um",
    "start": "1492679",
    "end": "1494080"
  },
  {
    "text": "we talk about presented a lot right and",
    "start": "1494080",
    "end": "1496880"
  },
  {
    "text": "I think the idea is you can start",
    "start": "1496880",
    "end": "1498799"
  },
  {
    "text": "anywhere you can start with open source",
    "start": "1498799",
    "end": "1500480"
  },
  {
    "text": "and know that you have a path uh to",
    "start": "1500480",
    "end": "1502940"
  },
  {
    "text": "production or Enterprise grade software",
    "start": "1502940",
    "end": "1504799"
  },
  {
    "text": "uh starting with open source right we're",
    "start": "1504799",
    "end": "1507679"
  },
  {
    "text": "an accelerated Computing company that's",
    "start": "1507679",
    "end": "1509000"
  },
  {
    "text": "a full stack value prop that means right",
    "start": "1509000",
    "end": "1511640"
  },
  {
    "text": "you could do more for Less higher",
    "start": "1511640",
    "end": "1513080"
  },
  {
    "text": "throughput for a fraction of the cost I",
    "start": "1513080",
    "end": "1515059"
  },
  {
    "text": "think when you talk about or you know",
    "start": "1515059",
    "end": "1516260"
  },
  {
    "text": "distributed computing and orchestration",
    "start": "1516260",
    "end": "1518299"
  },
  {
    "text": "efficiency that's the same kpis no",
    "start": "1518299",
    "end": "1520700"
  },
  {
    "text": "that's not for fundamentally the same",
    "start": "1520700",
    "end": "1521960"
  },
  {
    "text": "kpis second piece is hey how do we bring",
    "start": "1521960",
    "end": "1524900"
  },
  {
    "text": "Enterprise grade software and I think",
    "start": "1524900",
    "end": "1527480"
  },
  {
    "text": "between the any scale platform and the",
    "start": "1527480",
    "end": "1529400"
  },
  {
    "text": "Nvidia AI Enterprise production branches",
    "start": "1529400",
    "end": "1531620"
  },
  {
    "text": "right you have that confidence that",
    "start": "1531620",
    "end": "1534799"
  },
  {
    "text": "security that not only you know your",
    "start": "1534799",
    "end": "1537020"
  },
  {
    "text": "software is going to be patched you're",
    "start": "1537020",
    "end": "1538039"
  },
  {
    "text": "going to meet you know compliance needs",
    "start": "1538039",
    "end": "1539659"
  },
  {
    "text": "security Etc but you can actually call",
    "start": "1539659",
    "end": "1541760"
  },
  {
    "text": "1-800 in video right from a support",
    "start": "1541760",
    "end": "1543620"
  },
  {
    "text": "perspective if needed that is true by",
    "start": "1543620",
    "end": "1546260"
  },
  {
    "text": "the way right I forgot to mention that",
    "start": "1546260",
    "end": "1547580"
  },
  {
    "text": "but the Enterprise offering there are",
    "start": "1547580",
    "end": "1549320"
  },
  {
    "text": "slas for response times directly",
    "start": "1549320",
    "end": "1553039"
  },
  {
    "text": "year 10 video and the final piece is I",
    "start": "1553039",
    "end": "1555559"
  },
  {
    "text": "think right any scale as a platform AWS",
    "start": "1555559",
    "end": "1558500"
  },
  {
    "text": "my Google Cloud right NVA Enterprise is",
    "start": "1558500",
    "end": "1561500"
  },
  {
    "text": "available on those CSP marketplaces",
    "start": "1561500",
    "end": "1563659"
  },
  {
    "text": "we've got vmis we're starting",
    "start": "1563659",
    "end": "1565700"
  },
  {
    "text": "integrating their platforms as well and",
    "start": "1565700",
    "end": "1568100"
  },
  {
    "text": "so we can meet you as needed in the",
    "start": "1568100",
    "end": "1570260"
  },
  {
    "text": "cloud",
    "start": "1570260",
    "end": "1571880"
  },
  {
    "text": "I think that is my final slide uh next",
    "start": "1571880",
    "end": "1574940"
  },
  {
    "text": "steps is yeah you can screen that QR",
    "start": "1574940",
    "end": "1577640"
  },
  {
    "text": "code gives you access to a 90-day eval",
    "start": "1577640",
    "end": "1580340"
  },
  {
    "text": "for 90 days you get free access to",
    "start": "1580340",
    "end": "1581960"
  },
  {
    "text": "Nvidia AI enterprise software that's",
    "start": "1581960",
    "end": "1583880"
  },
  {
    "text": "crazy talk",
    "start": "1583880",
    "end": "1585740"
  },
  {
    "text": "that's worth a lot right",
    "start": "1585740",
    "end": "1588679"
  },
  {
    "text": "um",
    "start": "1588679",
    "end": "1589400"
  },
  {
    "text": "yeah go check it out it'll also I think",
    "start": "1589400",
    "end": "1591980"
  },
  {
    "text": "keep me on a studio right it also gives",
    "start": "1591980",
    "end": "1594679"
  },
  {
    "text": "them the opportunity to say hey as",
    "start": "1594679",
    "end": "1596299"
  },
  {
    "text": "you're any scale Nvidia Integrations are",
    "start": "1596299",
    "end": "1599299"
  },
  {
    "text": "in place right you can sign up and we'll",
    "start": "1599299",
    "end": "1601880"
  },
  {
    "text": "reach out as we make those libraries",
    "start": "1601880",
    "end": "1603919"
  },
  {
    "text": "available",
    "start": "1603919",
    "end": "1605360"
  },
  {
    "text": "and with that I think that concludes my",
    "start": "1605360",
    "end": "1608000"
  },
  {
    "text": "session",
    "start": "1608000",
    "end": "1609200"
  },
  {
    "text": "that is correct",
    "start": "1609200",
    "end": "1611059"
  },
  {
    "text": "any questions",
    "start": "1611059",
    "end": "1613899"
  },
  {
    "text": "yes sir",
    "start": "1614059",
    "end": "1616460"
  },
  {
    "text": "oh yeah sure",
    "start": "1616460",
    "end": "1619240"
  },
  {
    "text": "thank you so early on you mentioned",
    "start": "1621919",
    "end": "1624919"
  },
  {
    "text": "tensor RTM for inference you give us",
    "start": "1624919",
    "end": "1628940"
  },
  {
    "text": "some examples for acceleration multiple",
    "start": "1628940",
    "end": "1630980"
  },
  {
    "text": "X I'll also available for even hundreds",
    "start": "1630980",
    "end": "1635539"
  },
  {
    "text": "or only for h100 now trt llm works on",
    "start": "1635539",
    "end": "1639440"
  },
  {
    "text": "both a100 and h100 in fact what I was",
    "start": "1639440",
    "end": "1642440"
  },
  {
    "text": "comparing was how much better it gets",
    "start": "1642440",
    "end": "1644840"
  },
  {
    "text": "with h100 versus an a100 so when you",
    "start": "1644840",
    "end": "1647539"
  },
  {
    "text": "talk about GPT J and I'm saying 8X",
    "start": "1647539",
    "end": "1649760"
  },
  {
    "text": "performance improvements I was just",
    "start": "1649760",
    "end": "1652100"
  },
  {
    "text": "showing you that trt llm running on an",
    "start": "1652100",
    "end": "1654200"
  },
  {
    "text": "h100 drives 8X Improvement versus an",
    "start": "1654200",
    "end": "1656960"
  },
  {
    "text": "a100",
    "start": "1656960",
    "end": "1658640"
  },
  {
    "text": "but it is supported on on trt it is",
    "start": "1658640",
    "end": "1661159"
  },
  {
    "text": "support on a100 as well okay all right",
    "start": "1661159",
    "end": "1663860"
  },
  {
    "text": "but those numbers you showed us like",
    "start": "1663860",
    "end": "1666020"
  },
  {
    "text": "multiple X for example",
    "start": "1666020",
    "end": "1668720"
  },
  {
    "text": "there is like uh",
    "start": "1668720",
    "end": "1672559"
  },
  {
    "text": "it was here",
    "start": "1672559",
    "end": "1674120"
  },
  {
    "text": "um",
    "start": "1674120",
    "end": "1675200"
  },
  {
    "text": "8X speed up of foundational logical",
    "start": "1675200",
    "end": "1677840"
  },
  {
    "text": "models those like 8x",
    "start": "1677840",
    "end": "1680059"
  },
  {
    "text": "8X over even 100 correct my h100 yeah",
    "start": "1680059",
    "end": "1683539"
  },
  {
    "text": "actually I think there's a Blog that we",
    "start": "1683539",
    "end": "1685039"
  },
  {
    "text": "published about this make sure that",
    "start": "1685039",
    "end": "1689200"
  },
  {
    "text": "oh never mind I can't bring it up here I",
    "start": "1695720",
    "end": "1698480"
  },
  {
    "text": "do have a table to show for this there",
    "start": "1698480",
    "end": "1699919"
  },
  {
    "text": "is a Blog that we publish a developer",
    "start": "1699919",
    "end": "1701659"
  },
  {
    "text": "blog that goes into all the details that",
    "start": "1701659",
    "end": "1703640"
  },
  {
    "text": "we have for those benchmarks but yeah",
    "start": "1703640",
    "end": "1706039"
  },
  {
    "text": "we're not comparing against other",
    "start": "1706039",
    "end": "1707120"
  },
  {
    "text": "accelerators we're comparing versus",
    "start": "1707120",
    "end": "1708740"
  },
  {
    "text": "older generation over accelerators but",
    "start": "1708740",
    "end": "1711320"
  },
  {
    "text": "yeah 8X was what we saw from a for gptj",
    "start": "1711320",
    "end": "1714799"
  },
  {
    "text": "for the CNN daily mail",
    "start": "1714799",
    "end": "1717200"
  },
  {
    "text": "uh summarization uh",
    "start": "1717200",
    "end": "1720559"
  },
  {
    "text": "um data sets",
    "start": "1720559",
    "end": "1723860"
  },
  {
    "text": "yeah no problem",
    "start": "1723860",
    "end": "1726580"
  },
  {
    "text": "any other questions",
    "start": "1727520",
    "end": "1729080"
  },
  {
    "text": "hi",
    "start": "1729080",
    "end": "1731620"
  },
  {
    "text": "everybody",
    "start": "1734419",
    "end": "1735919"
  },
  {
    "text": "could you elaborate a little more on the",
    "start": "1735919",
    "end": "1737720"
  },
  {
    "text": "integration point if it's okay to share",
    "start": "1737720",
    "end": "1739820"
  },
  {
    "text": "without breaking these closures of",
    "start": "1739820",
    "end": "1742880"
  },
  {
    "text": "faster Transformer back-end plus array",
    "start": "1742880",
    "end": "1744980"
  },
  {
    "text": "especially in the context of serving",
    "start": "1744980",
    "end": "1746720"
  },
  {
    "text": "multiple Laura fine-tuned models yeah uh",
    "start": "1746720",
    "end": "1750200"
  },
  {
    "text": "what can I provide so it's not faster",
    "start": "1750200",
    "end": "1751460"
  },
  {
    "text": "Transformer we're doing trt llm",
    "start": "1751460",
    "end": "1753260"
  },
  {
    "text": "integration",
    "start": "1753260",
    "end": "1754640"
  },
  {
    "text": "trt LM Builds on faster Transformer",
    "start": "1754640",
    "end": "1757039"
  },
  {
    "text": "right",
    "start": "1757039",
    "end": "1758539"
  },
  {
    "text": "um",
    "start": "1758539",
    "end": "1759440"
  },
  {
    "text": "I mean I don't know what additional",
    "start": "1759440",
    "end": "1762020"
  },
  {
    "text": "detail I could share other than I think",
    "start": "1762020",
    "end": "1765080"
  },
  {
    "text": "in the next release of race serve 2.8",
    "start": "1765080",
    "end": "1768260"
  },
  {
    "text": "which happened six weeks from now we",
    "start": "1768260",
    "end": "1769760"
  },
  {
    "text": "should expect some",
    "start": "1769760",
    "end": "1771980"
  },
  {
    "text": "of the first Integrations coming to",
    "start": "1771980",
    "end": "1773600"
  },
  {
    "text": "Market",
    "start": "1773600",
    "end": "1775778"
  },
  {
    "text": "I think that's it",
    "start": "1786140",
    "end": "1789039"
  },
  {
    "text": "thanks guys thanks for having us sir",
    "start": "1789320",
    "end": "1793240"
  }
]