[
  {
    "text": "all right thanks Matthew hey everybody",
    "start": "5899",
    "end": "11160"
  },
  {
    "text": "um yeah so that's right we're going to be talking about a pretty crazy production use case for Ray at Amazon",
    "start": "11160",
    "end": "16740"
  },
  {
    "text": "today this has been years in the making so I'm excited to get up here and talk to you about it there's some other",
    "start": "16740",
    "end": "21840"
  },
  {
    "text": "people I see in the audience back here who have played a huge role in making this happen looking at raghavendra",
    "start": "21840",
    "end": "28140"
  },
  {
    "text": "sitting right back there so um there's people around here that have also played a big role in this",
    "start": "28140",
    "end": "33840"
  },
  {
    "text": "definitely far from a one-person effort lots of teams collaborated across Amazon",
    "start": "33840",
    "end": "40200"
  },
  {
    "text": "to make this happen but yeah I'm Patrick Ames I'm from Amazon I'm a principal",
    "start": "40200",
    "end": "45420"
  },
  {
    "text": "engineer in the business intelligence organization at Amazon and we are going to tell you about an exabyte scale",
    "start": "45420",
    "end": "53100"
  },
  {
    "text": "production migration that we did and is still ongoing",
    "start": "53100",
    "end": "60440"
  },
  {
    "text": "from uh Apache spark over to Ray and uh this migration is pretty unique",
    "start": "60440",
    "end": "68760"
  },
  {
    "text": "we'll start with an overview of the kind of problem space that we're in introduce you to it and",
    "start": "68760",
    "end": "74580"
  },
  {
    "text": "um talk about this so-called compaction problem that we have with our data",
    "start": "74580",
    "end": "80400"
  },
  {
    "text": "residing in our business intelligence data catalogs at Amazon uh why we decided to migrate this particular",
    "start": "80400",
    "end": "87420"
  },
  {
    "text": "workload from spark to array even though this is fundamentally what we're going to be talking about as a big data processing workload so you may look at",
    "start": "87420",
    "end": "94439"
  },
  {
    "text": "and say oh why exactly did you choose Rey for it we'll we'll get into that as we go through the talk and talk about",
    "start": "94439",
    "end": "100140"
  },
  {
    "text": "our results which I've been very happy with so far and future work that we're",
    "start": "100140",
    "end": "105780"
  },
  {
    "text": "planning on doing in this space in open source and internally and then",
    "start": "105780",
    "end": "110820"
  },
  {
    "text": "we'll close up with ways for you to get involved and help out with the project yourselves going forward",
    "start": "110820",
    "end": "116640"
  },
  {
    "text": "so without further Ado getting into the introduction um let's start with talking about the",
    "start": "116640",
    "end": "122939"
  },
  {
    "text": "journey from data Lakes to lake houses over the years so those of you that are maybe involved in this space a little",
    "start": "122939",
    "end": "129060"
  },
  {
    "text": "bit more some of these terminologies that I put up here are familiar to you I think back in the 2000s and 2010s data",
    "start": "129060",
    "end": "135540"
  },
  {
    "text": "warehousing was a very popular thing that we were all doing and here on this slide I'm kind of using it to refer to",
    "start": "135540",
    "end": "140660"
  },
  {
    "text": "this historic method of using especially relational databases to run coupled storage and compute over structured data",
    "start": "140660",
    "end": "147239"
  },
  {
    "text": "and at Amazon we were doing the same thing we were running one of the largest Oracle data warehouses on the planet",
    "start": "147239",
    "end": "156420"
  },
  {
    "text": "and that's I think a kind of a direct Twitter quote from uh Werner vogels on",
    "start": "156420",
    "end": "162120"
  },
  {
    "text": "our side like it it was huge it was at the time 50 petabytes of replicated",
    "start": "162120",
    "end": "167760"
  },
  {
    "text": "Oracle data across all kinds of servers and we eventually Federated that into an",
    "start": "167760",
    "end": "173340"
  },
  {
    "text": "S3 powered data Lake all right and so at the time a lot of people were migrating away from this coupled storage and compute model uh",
    "start": "173340",
    "end": "180720"
  },
  {
    "text": "into uh decoupled storage and compute and that's where data Lakes kind of come into play things like hdfs and S3 can",
    "start": "180720",
    "end": "188940"
  },
  {
    "text": "really help you with that but they don't bring a lot of structure to your workloads when we start migrating these workloads in desk 3 into Data Lakes",
    "start": "188940",
    "end": "195300"
  },
  {
    "text": "you're starting to sacrifice things like acid compliance if that term rings a bell for any of you atomicity",
    "start": "195300",
    "end": "201239"
  },
  {
    "text": "consistency isolation and durability right these things that make sure that it doesn't become a data swamp as the",
    "start": "201239",
    "end": "208620"
  },
  {
    "text": "terminology is sometimes applied to data Lakes you lose a lot of that when you go to data Lakes right so how do you get it",
    "start": "208620",
    "end": "214440"
  },
  {
    "text": "back well that's where data cataloging starts to come into play and you Hive was one of the earliest things there to",
    "start": "214440",
    "end": "220319"
  },
  {
    "text": "say Hey you know we're going to use like partitions inside of directory paths and",
    "start": "220319",
    "end": "225420"
  },
  {
    "text": "allow you to use your directory paths by itself just to bring some structure to your data to group it into partitions",
    "start": "225420",
    "end": "231239"
  },
  {
    "text": "and tables and stuff like that and then Iceberg and hoodie and Delta Lake and see these other projects have become",
    "start": "231239",
    "end": "236459"
  },
  {
    "text": "more sophisticated with their metadata grouping and they start to give you some of the Primitives you need to get that acid compliance back they start to give",
    "start": "236459",
    "end": "243060"
  },
  {
    "text": "you transactionality Primitives and stuff like that and then the lake house model came along I think in 2021 or so",
    "start": "243060",
    "end": "249420"
  },
  {
    "text": "from databricks um where basically you're saying you know it doesn't have to be one or the",
    "start": "249420",
    "end": "254640"
  },
  {
    "text": "other you can have decoupled storage and compute and still have acid compliance but the two need to kind of cooperate",
    "start": "254640",
    "end": "260760"
  },
  {
    "text": "with each other and get along and it's been a very interesting journey and where we're at more in the business",
    "start": "260760",
    "end": "267600"
  },
  {
    "text": "intelligence space on Amazon's side it's kind of more towards that lake house model where we have decoupled storage",
    "start": "267600",
    "end": "272940"
  },
  {
    "text": "and compute but we still go for acid compliance and um we're very focused on kind of",
    "start": "272940",
    "end": "279360"
  },
  {
    "text": "spinning the business intelligence flywheel here at Amazon it's kind of a centralized hub for data analytics and",
    "start": "279360",
    "end": "285660"
  },
  {
    "text": "ml pipeline data i o at Amazon and um this flywheel basically focused",
    "start": "285660",
    "end": "292380"
  },
  {
    "text": "on collecting business insights making data-driven decisions from those insights improving operational efficiency and then that leads to",
    "start": "292380",
    "end": "299340"
  },
  {
    "text": "collecting even more business insights and at a certain level it gets kind of meta we're collecting business insights uh business intelligence insights about",
    "start": "299340",
    "end": "306360"
  },
  {
    "text": "the business intelligence organization right and using those to improve our own efficiency as well",
    "start": "306360",
    "end": "311580"
  },
  {
    "text": "and then there's this kind of bi decision Evolution and general decision Evolution it doesn't just apply to bi",
    "start": "311580",
    "end": "317580"
  },
  {
    "text": "where we're gradually migrating more away from just purely manual human in",
    "start": "317580",
    "end": "322860"
  },
  {
    "text": "the loop decisions to supervise decisions where maybe there's a little bit of ml involved in it but somebody's",
    "start": "322860",
    "end": "329460"
  },
  {
    "text": "kind of babysitting it and making sure it doesn't make any critical mistakes to the business and eventually we'd like to",
    "start": "329460",
    "end": "335160"
  },
  {
    "text": "kind of get to this whole unsupervised decision realm at least you know for some subset of decisions you know I",
    "start": "335160",
    "end": "342240"
  },
  {
    "text": "don't think you're going to have like Auto CEO anytime soon but um Ray is kind of helping us transition",
    "start": "342240",
    "end": "349020"
  },
  {
    "text": "into more supervised and where it's safe unsupervised decisions and these business intelligence Pipelines",
    "start": "349020",
    "end": "356220"
  },
  {
    "text": "and a brief history of Amazon bi touched on some of this a little bit but yeah 2016-2018 we migrated this enormous 50",
    "start": "356220",
    "end": "364380"
  },
  {
    "text": "petabyte Oracle data warehouse to our S3 based data catalog at the time it's just kind of a data Lake it became a data",
    "start": "364380",
    "end": "370919"
  },
  {
    "text": "catalog as we put on metadata and got organization around it and then we",
    "start": "370919",
    "end": "376020"
  },
  {
    "text": "started to kind of form these underpinnings of what we might call a lake house now where we learned how to",
    "start": "376020",
    "end": "382139"
  },
  {
    "text": "do schema and partition Evolution data repair and time travel operations and then LSM based change data capture so",
    "start": "382139",
    "end": "389580"
  },
  {
    "text": "log structure merge change data capture compaction as we call it applying inserts updates and deletes that are",
    "start": "389580",
    "end": "395699"
  },
  {
    "text": "written to a table over time using spark on EMR then we did uh some data",
    "start": "395699",
    "end": "401220"
  },
  {
    "text": "pipelines as code where we kind of built some applications that helped us take a multi-system",
    "start": "401220",
    "end": "409080"
  },
  {
    "text": "pipeline consisting of spark and Ray and redshift and all these other applications and Define them as a single",
    "start": "409080",
    "end": "414479"
  },
  {
    "text": "logical application 2019 to 2023 we started getting into reintegration for",
    "start": "414479",
    "end": "421080"
  },
  {
    "text": "uh data quality analysis first off which kind of helped us vet the stability and capabilities of Rey at exabyte scale and",
    "start": "421080",
    "end": "429720"
  },
  {
    "text": "it also prepped us for the sparked array compaction migration which was a really critical workload we're creating read",
    "start": "429720",
    "end": "435120"
  },
  {
    "text": "optimized versions of these tables and everybody in their business",
    "start": "435120",
    "end": "440220"
  },
  {
    "text": "intelligence pipelines are going to depend on this so data quality analysis was going to help us get this over the",
    "start": "440220",
    "end": "446160"
  },
  {
    "text": "bar by comparing Ray and Spark results and it was also the first big use case where we vetted Ray at scale like can",
    "start": "446160",
    "end": "451740"
  },
  {
    "text": "Ray actually run data processing operations at exabyte scale",
    "start": "451740",
    "end": "458099"
  },
  {
    "text": "and um so a quick aside about why exactly we chose Rey here",
    "start": "458099",
    "end": "464280"
  },
  {
    "text": "um is you know initially we weren't looking at it strictly for compaction we were looking at",
    "start": "464280",
    "end": "470479"
  },
  {
    "text": "an API that would help our data scientists and ml engineers",
    "start": "470479",
    "end": "475740"
  },
  {
    "text": "um scale their workloads on python better we found it of course you know it's pythonic it fit the bill there we",
    "start": "475740",
    "end": "482280"
  },
  {
    "text": "found it to have very intuitive apis those of you that have worked with it it's pretty much plain python with remotes dot remotes added in and Ray",
    "start": "482280",
    "end": "489240"
  },
  {
    "text": "remote annotation stuff like that uh and it was very scalable that's what you're",
    "start": "489240",
    "end": "494819"
  },
  {
    "text": "looking for performance efficient and potentially unified we saw a potential here that we could do more than just ml",
    "start": "494819",
    "end": "500340"
  },
  {
    "text": "with it that we could start to run large segments of our data processing",
    "start": "500340",
    "end": "505620"
  },
  {
    "text": "pipelines and and potentially using a single distributed compute fabric",
    "start": "505620",
    "end": "512419"
  },
  {
    "text": "and if we get into our particular problem that we're solving here and talking about it is this fundamental",
    "start": "512760",
    "end": "519300"
  },
  {
    "text": "compaction efficiency problem so we'll start out with a simple definition of efficiency I think the oldest",
    "start": "519300",
    "end": "524700"
  },
  {
    "text": "definitions of efficiency go back to physics which just says hey it's useful energy output divided by energy input",
    "start": "524700",
    "end": "530339"
  },
  {
    "text": "another general definition Computing I'd say well it's useful Computing output divided by the resources you input into",
    "start": "530339",
    "end": "536040"
  },
  {
    "text": "it so basically the more work you can do with the same resources the more efficient you are so it's a simple",
    "start": "536040",
    "end": "542820"
  },
  {
    "text": "example if you have two equivalent computers reading the same two gigabytes of data uh in one",
    "start": "542820",
    "end": "550740"
  },
  {
    "text": "minute if system a can do two gigabytes and system B can do four gigabytes a minute of input tabular data we say",
    "start": "550740",
    "end": "556320"
  },
  {
    "text": "system B is twice as efficient a system a right so pretty uh straightforward",
    "start": "556320",
    "end": "561660"
  },
  {
    "text": "when we apply that to a very specific use case all other variables being equal",
    "start": "561660",
    "end": "567920"
  },
  {
    "text": "and then let's back up a little bit if all for those of you either ml Engineers I'd like to give you a holistic view of",
    "start": "568200",
    "end": "575880"
  },
  {
    "text": "your machine learning pipelines efficiency uh first off I want you to know that of",
    "start": "575880",
    "end": "581459"
  },
  {
    "text": "course you inherit any cost that your Upstream data providers have if you're just reading strictly from S3 you're",
    "start": "581459",
    "end": "587580"
  },
  {
    "text": "going to get some of S3's costs to store your data handed back down to you that's part of what goes into your bill right",
    "start": "587580",
    "end": "593959"
  },
  {
    "text": "and then whatever costs you incur if you're building a service of course you're going to pass those down to your",
    "start": "593959",
    "end": "599459"
  },
  {
    "text": "Downstream consumers as well so this compaction use case that we're talking about is a very fundamental Upstream use",
    "start": "599459",
    "end": "606120"
  },
  {
    "text": "case for probably a lot of your machine learning pipelines either implicitly or",
    "start": "606120",
    "end": "611580"
  },
  {
    "text": "explicitly from where you're sourcing your data and would thus have an impact",
    "start": "611580",
    "end": "617040"
  },
  {
    "text": "on the cost of your Pipelines so think about for all your pipelines where",
    "start": "617040",
    "end": "622620"
  },
  {
    "text": "exactly your data sets come from and at what cost and frequency do they arrive and I think we'd all like it to",
    "start": "622620",
    "end": "629459"
  },
  {
    "text": "be cheaper and quicker right that nobody ever argues about getting things cheaper",
    "start": "629459",
    "end": "635760"
  },
  {
    "text": "and quicker and that's fundamentally what we're after here",
    "start": "635760",
    "end": "640940"
  },
  {
    "text": "and that goes into understanding Amazon business intelligence pricing we charge internal pricing to other Amazon",
    "start": "641279",
    "end": "647940"
  },
  {
    "text": "internal customers to use business intelligence so we basically have a set of administrative costs that we divide",
    "start": "647940",
    "end": "654360"
  },
  {
    "text": "evenly amongst thousands of distinct data consumers where everyone's charged the same normalized amount per byte that",
    "start": "654360",
    "end": "661019"
  },
  {
    "text": "they read from our catalog and our biggest administrative cost arguably is compaction which does just take this",
    "start": "661019",
    "end": "667920"
  },
  {
    "text": "input list of structured S3 files that just say hey these are records to insert update and or delete from this running",
    "start": "667920",
    "end": "676019"
  },
  {
    "text": "table of change history right and the output is a set of S3 files with all inserts updates and deletes applied it's",
    "start": "676019",
    "end": "682140"
  },
  {
    "text": "a merge fundamentally right we've merged it in said hey I did it for you here's",
    "start": "682140",
    "end": "687180"
  },
  {
    "text": "the final result here's what the table should look like and why is it so expensive well it runs",
    "start": "687180",
    "end": "692820"
  },
  {
    "text": "after any right to actively read tables in our X byte scale data catalog so that all the reads coming at it can be",
    "start": "692820",
    "end": "698519"
  },
  {
    "text": "optimized it's producing a read optimized view fundamentally and of course this incurs a very large",
    "start": "698519",
    "end": "703980"
  },
  {
    "text": "compute and storage cost you're storing a lot of data and you're Computing a lot over a lot of data in perpetuity",
    "start": "703980",
    "end": "709860"
  },
  {
    "text": "and yeah we're doing it so that reads can be optimized because we do reads a lot more than writes for most of these",
    "start": "709860",
    "end": "716339"
  },
  {
    "text": "tables right we do a bit of you know at read time compaction as well but the",
    "start": "716339",
    "end": "722220"
  },
  {
    "text": "vast majority of our use cases and cost falls under the copy on right compaction",
    "start": "722220",
    "end": "727740"
  },
  {
    "text": "model right if you're into hoodie or Iceberg you'll see the terms copy on write and merge on read thrown around we",
    "start": "727740",
    "end": "733800"
  },
  {
    "text": "do both we're talking more about copy on right here in this presentation with compaction",
    "start": "733800",
    "end": "739680"
  },
  {
    "text": "and also we use it to enforce security and compliance policies like gdpr we fundamentally rely on this to delete",
    "start": "739680",
    "end": "745820"
  },
  {
    "text": "sensitive customer data so that it's not visible to any of our catalog consumers",
    "start": "745820",
    "end": "754100"
  },
  {
    "text": "so I'll give you a quick example here using our open source Delta cat apis which is Ray ecosystem project which is",
    "start": "754320",
    "end": "761220"
  },
  {
    "text": "currently experimental but we are using it in production internally we just haven't ported it completely to Iceberg",
    "start": "761220",
    "end": "768240"
  },
  {
    "text": "and hoodie and other catalogs yet but if we give you an example here of like a sample S3 catalog you can initialize",
    "start": "768240",
    "end": "774899"
  },
  {
    "text": "let's say you have a production catalog here and we just initialize it saying Delta",
    "start": "774899",
    "end": "782459"
  },
  {
    "text": "cat initializes catalog and we give it an end point say that's where the catalog resides in S3",
    "start": "782459",
    "end": "788760"
  },
  {
    "text": "and then we load a pandas data frame into it pretty simple column one is one two three column two is ABC we're going",
    "start": "788760",
    "end": "795120"
  },
  {
    "text": "to say create this table store the content as parquet and then we are going to write another",
    "start": "795120",
    "end": "805760"
  },
  {
    "text": "pandas data frame to this table after we created it we said hey just append this Panda's data frame that I gave you",
    "start": "805760",
    "end": "812700"
  },
  {
    "text": "and its content type is again parquet it's a simple insert of the two so we should expect the two tables the two",
    "start": "812700",
    "end": "820019"
  },
  {
    "text": "pans data frames that we inserted to be simply appended to one another and if we look at under the hood how",
    "start": "820019",
    "end": "825600"
  },
  {
    "text": "this whole thing happens and how the compaction problem comes into play for this very simple example you started",
    "start": "825600",
    "end": "831540"
  },
  {
    "text": "with this pandas data frame one uh right there we create a Delta which",
    "start": "831540",
    "end": "838019"
  },
  {
    "text": "is a metadata layer that's going to store references to your S3 files right",
    "start": "838019",
    "end": "843240"
  },
  {
    "text": "we take the Panda's data frame we write a parquet file to S3 for it",
    "start": "843240",
    "end": "848459"
  },
  {
    "text": "we reference that parquet file inside the delta right and we append it it's a Delta to",
    "start": "848459",
    "end": "854459"
  },
  {
    "text": "the sample table right so this table in the metadata layer and the data catalog metadata earlier you have two Deltas",
    "start": "854459",
    "end": "859500"
  },
  {
    "text": "right the first Panda's data frame the second pan is data frame and then we're going to send an implicit",
    "start": "859500",
    "end": "865980"
  },
  {
    "text": "compact operation hey a new Delta arrived let's do a copy on write compaction of it that's going to launch",
    "start": "865980",
    "end": "871200"
  },
  {
    "text": "another array cluster which is going to read both of those Deltas the contents of the pandas data",
    "start": "871200",
    "end": "878160"
  },
  {
    "text": "frames we saw earlier it's going to create a new revision of the table it's going to store the results later of compaction",
    "start": "878160",
    "end": "884220"
  },
  {
    "text": "and it's going to Simply concatenate the two in this case pretty straightforward",
    "start": "884220",
    "end": "890339"
  },
  {
    "text": "and the concatenated result is going to go into a new Delta so this Delta the",
    "start": "890339",
    "end": "895860"
  },
  {
    "text": "file that's going to go inside of it and be referenced inside of it by URI is written to S3",
    "start": "895860",
    "end": "901199"
  },
  {
    "text": "you reference it in the Delta we put the delta in the new compacted table we commit the table once it's committed",
    "start": "901199",
    "end": "907260"
  },
  {
    "text": "other people can be like okay you're good to go you can read the table now you don't have to do that concatenation every time you read it yourself",
    "start": "907260",
    "end": "914940"
  },
  {
    "text": "now more interesting is a merge operation that's specifically what we're more focused on optimizing here",
    "start": "914940",
    "end": "922620"
  },
  {
    "text": "so say we inserted that same 124 def second pandas data frame there but we",
    "start": "922620",
    "end": "929040"
  },
  {
    "text": "said do a table write mode merge instead of a just an append",
    "start": "929040",
    "end": "934680"
  },
  {
    "text": "well in that case the end-to-hand workflow changes to this and the key part of it is the far right side where",
    "start": "934680",
    "end": "940620"
  },
  {
    "text": "you're seeing the merged column one column two there wherever column one if",
    "start": "940620",
    "end": "945959"
  },
  {
    "text": "that was our merge key wherever those values matched they would be overwritten with the later",
    "start": "945959",
    "end": "951420"
  },
  {
    "text": "Deltas values in column two so you see one and two were updated to D and E From A and B",
    "start": "951420",
    "end": "957199"
  },
  {
    "text": "3 and C was never updated so it stays the same 4 F was inserted right so this",
    "start": "957199",
    "end": "963240"
  },
  {
    "text": "is fundamentally a deduplication problem that's very large deduplication problem finding matches",
    "start": "963240",
    "end": "969420"
  },
  {
    "text": "and then inserting things or matches don't exist updating things where they do",
    "start": "969420",
    "end": "975000"
  },
  {
    "text": "so when we're looking at our spark terrain migration in this context we're going to look at some key Milestones",
    "start": "975000",
    "end": "980760"
  },
  {
    "text": "challenges concessions we made and uh lessons we learned so laying out kind of a timeline here",
    "start": "980760",
    "end": "989220"
  },
  {
    "text": "back in 2019 we launched our initial Ray investigation",
    "start": "989220",
    "end": "995579"
  },
  {
    "text": "and did benchmarking and Analysis of could Ray be a good fit for this and any",
    "start": "995579",
    "end": "1002240"
  },
  {
    "text": "other arbitrary distributed compute problems that Amazon has we thought yeah it could",
    "start": "1002240",
    "end": "1008839"
  },
  {
    "text": "um and specifically we think we could even make a much better compactor with Ray so let's try it out so in 2020",
    "start": "1008839",
    "end": "1015740"
  },
  {
    "text": "we came up with a design that we called the flash compactor and we made a proof",
    "start": "1015740",
    "end": "1021800"
  },
  {
    "text": "of concept code implementation of it ran it directly against a S3 alongside spark and said oh looks pretty good",
    "start": "1021800",
    "end": "1028360"
  },
  {
    "text": "then we came up with the next step if it looks good how are you going to run it in production well we needed some",
    "start": "1028360",
    "end": "1034040"
  },
  {
    "text": "serverless job manager which is a big problem in and of itself so we came up with a raid job management system design and",
    "start": "1034040",
    "end": "1040220"
  },
  {
    "text": "implementation in 2021 and then 2022 we did exabyte scale",
    "start": "1040220",
    "end": "1046040"
  },
  {
    "text": "production data quality analysis which we talked about earlier with Ray to vet that everything was working correctly job management works correctly rake and",
    "start": "1046040",
    "end": "1053000"
  },
  {
    "text": "process exabytes of data it all looks good even in prod and then we started shadowing spark with",
    "start": "1053000",
    "end": "1060260"
  },
  {
    "text": "a compactor okay keep spark on start running race side by side see how they actually do when they're both off to the",
    "start": "1060260",
    "end": "1068179"
  },
  {
    "text": "Off to the Races at the same starting point and compare their results to each other using the data quality framework",
    "start": "1068179",
    "end": "1074600"
  },
  {
    "text": "and then we did production migration from spark to the flash compactor on Ray",
    "start": "1074600",
    "end": "1080179"
  },
  {
    "text": "here in 2023 . and um there have been a lot of",
    "start": "1080179",
    "end": "1086840"
  },
  {
    "text": "challenges along the way here that we've tried to tackle um",
    "start": "1086840",
    "end": "1092000"
  },
  {
    "text": "one of the biggest challenges here is of course correctness and backwards compatibility with spark",
    "start": "1092000",
    "end": "1099260"
  },
  {
    "text": "um we correctness is an interesting thing because you could get in a lot of long",
    "start": "1099260",
    "end": "1104660"
  },
  {
    "text": "debates about what exactly is correct results even to a simple problem like deduplication you get into type systems",
    "start": "1104660",
    "end": "1111200"
  },
  {
    "text": "and differences between Type systems and they don't always agree about one value being equal to another you know even",
    "start": "1111200",
    "end": "1117020"
  },
  {
    "text": "simple things like integers some type systems have a concept of negative zero and other type systems don't and time",
    "start": "1117020",
    "end": "1124039"
  },
  {
    "text": "stamps are particularly tricky so basically we said well we want the end results of this produced by Ray to look",
    "start": "1124039",
    "end": "1130160"
  },
  {
    "text": "as close to spark as possible and then your other problems are okay how do you actually make sure that",
    "start": "1130160",
    "end": "1135860"
  },
  {
    "text": "they're backwards compatible with spark equivalent to whatever spark is producing on a strict budget because you",
    "start": "1135860",
    "end": "1142520"
  },
  {
    "text": "can't just you know you're you're processing exabytes of data you're not going to also compare exhaustively",
    "start": "1142520",
    "end": "1148340"
  },
  {
    "text": "exabytes of data without blowing up a huge budget",
    "start": "1148340",
    "end": "1153400"
  },
  {
    "text": "um and then automating those workflows right and budgeting for deferred results is a",
    "start": "1153559",
    "end": "1158720"
  },
  {
    "text": "big thing too right we're running them in Shadow mode costs are going to go way up before they go down so you have to",
    "start": "1158720",
    "end": "1164720"
  },
  {
    "text": "have a budget that says and buy-in from your Business Leaders to say hey do you want to spend a lot more money this year",
    "start": "1164720",
    "end": "1171260"
  },
  {
    "text": "so that you can save money next year that can be a hard proposition to push through unless you have really",
    "start": "1171260",
    "end": "1176840"
  },
  {
    "text": "convincing data that says I promise you you'll save money and here's by when you'll break even",
    "start": "1176840",
    "end": "1183200"
  },
  {
    "text": "um and we also found out that you know what do what do we want to Target we",
    "start": "1183200",
    "end": "1188539"
  },
  {
    "text": "found out the largest one percent of tables or about 50 of our cost so we said okay we'll go after those first",
    "start": "1188539",
    "end": "1194780"
  },
  {
    "text": "small number of tables a lot of cost sunk there um but they're also hard hard to work",
    "start": "1194780",
    "end": "1200960"
  },
  {
    "text": "with and then operational sustainability was super hard cluster management is a hard",
    "start": "1200960",
    "end": "1206780"
  },
  {
    "text": "issue by itself just infrastructure the infrastructure we needed from ec2 we wanted more",
    "start": "1206780",
    "end": "1212419"
  },
  {
    "text": "infrastructure than they had available at times right there we're already sucking up a huge pool of ec2 instances",
    "start": "1212419",
    "end": "1219200"
  },
  {
    "text": "in various regions for spark compaction now we want it even more to run Ray Side",
    "start": "1219200",
    "end": "1224600"
  },
  {
    "text": "by Side so we had to get into war and pooling discussions and all kinds of crazy stuff and a separate team is",
    "start": "1224600",
    "end": "1231080"
  },
  {
    "text": "fundamentally dedicated to the sole problem of cluster management at Amazon today and making sure clusters",
    "start": "1231080",
    "end": "1237140"
  },
  {
    "text": "can launch reasonably fast and distributed across various availability zones and their heterogeneous clusters",
    "start": "1237140",
    "end": "1244039"
  },
  {
    "text": "that consist of multiple instance types we go fishing for the quickest instance type we can grab basically for the",
    "start": "1244039",
    "end": "1250460"
  },
  {
    "text": "cheapest cost um and then you know all the stuff that",
    "start": "1250460",
    "end": "1256160"
  },
  {
    "text": "goes along with that's pretty hard reducing cluster startup times cluster management out of memory errors was a",
    "start": "1256160",
    "end": "1261380"
  },
  {
    "text": "was a big problem too that we are facing and regressive tables some of our tables",
    "start": "1261380",
    "end": "1268880"
  },
  {
    "text": "consist of millions of tiny files others a few huge files there's a lot of challenges there and",
    "start": "1268880",
    "end": "1276860"
  },
  {
    "text": "some of the fundamental lessons we learn some big ones yeah your customers",
    "start": "1276860",
    "end": "1282080"
  },
  {
    "text": "ultimately Define correctness so don't waste your time too much getting into debates with each other about what is a",
    "start": "1282080",
    "end": "1287960"
  },
  {
    "text": "correct result to a particular query ultimately you can't break your",
    "start": "1287960",
    "end": "1294200"
  },
  {
    "text": "customers right if your result is more correct but if you have some customer on redshift who needs to consume it but",
    "start": "1294200",
    "end": "1299720"
  },
  {
    "text": "redshift then crashes because your more correct result with like I don't know 100 points of decimal Precision it's",
    "start": "1299720",
    "end": "1306080"
  },
  {
    "text": "more correct and they can't consume it well it doesn't matter um you need to listen to your customers and what they expect",
    "start": "1306080",
    "end": "1312799"
  },
  {
    "text": "and uh Murphy's Law of course that when you're dealing a thing with things that exabyte",
    "start": "1312799",
    "end": "1318740"
  },
  {
    "text": "scale if there's like a oh well there's a one in a billion chance that that'll happen it's going to happen many many",
    "start": "1318740",
    "end": "1324020"
  },
  {
    "text": "times a day if that's like a per record chance so you can't get away from fixing all",
    "start": "1324020",
    "end": "1330620"
  },
  {
    "text": "your tiny little bugs when choosing infrastructure we found ec2 in general it's what we turn to for",
    "start": "1330620",
    "end": "1337520"
  },
  {
    "text": "running like greater than 10 terabyte jobs and if you're dealing with less than 10 terabytes for this type of",
    "start": "1337520",
    "end": "1342740"
  },
  {
    "text": "workload we'll turn to AWS glue easier to run and better cost because you're",
    "start": "1342740",
    "end": "1349220"
  },
  {
    "text": "spending less time spinning up a cluster AWS glue is going to spin up your",
    "start": "1349220",
    "end": "1354380"
  },
  {
    "text": "cluster very very quickly there's next to no provisioning time for your cluster unlike ec2 so we'll get",
    "start": "1354380",
    "end": "1361159"
  },
  {
    "text": "into and so some of the concessions made was we conceded to backwards compatibility",
    "start": "1361159",
    "end": "1367280"
  },
  {
    "text": "as we said some specifically that meant we did things like we regress back to",
    "start": "1367280",
    "end": "1372799"
  },
  {
    "text": "parquet 1.0 from parquet to point from the latest two point x parquet version",
    "start": "1372799",
    "end": "1378740"
  },
  {
    "text": "so that we could be compatible with spark um and",
    "start": "1378740",
    "end": "1384799"
  },
  {
    "text": "over time we found different tables present different challenges so we started favoring this this pattern that",
    "start": "1384799",
    "end": "1391280"
  },
  {
    "text": "says well you might actually have multiple disparate implementations of something like compaction depending on what table",
    "start": "1391280",
    "end": "1396919"
  },
  {
    "text": "it's going up against and depending on what Hardware you can give it there's not maybe one implementation to",
    "start": "1396919",
    "end": "1402679"
  },
  {
    "text": "rule them all but you have to now balance the ROI maintaining those and then we got more",
    "start": "1402679",
    "end": "1408740"
  },
  {
    "text": "into manual memory management right like automated garbage collection is great as long as you don't care a whole lot about",
    "start": "1408740",
    "end": "1415159"
  },
  {
    "text": "efficiency the more you care about efficiency the more you get into managing your own memory it seems to be",
    "start": "1415159",
    "end": "1420860"
  },
  {
    "text": "a universal law of computing almost and that was certainly true in this case too",
    "start": "1420860",
    "end": "1427280"
  },
  {
    "text": "okay so some some fun facts about the migration here",
    "start": "1427280",
    "end": "1433419"
  },
  {
    "text": "um so uh some some things that you might not have guessed uh going into this is",
    "start": "1433460",
    "end": "1440299"
  },
  {
    "text": "all of our production code actually calls into Java apis via Pi for J so",
    "start": "1440299",
    "end": "1446059"
  },
  {
    "text": "anybody out there thinks you can't use Pi for J in production um well it actually is possible I don't",
    "start": "1446059",
    "end": "1452299"
  },
  {
    "text": "know if it's advisable don't I mean don't don't do it don't put in Python to Java shims if you don't have to but if",
    "start": "1452299",
    "end": "1458960"
  },
  {
    "text": "you have to and we kind of had to because Amazon has such an enormous Java code base",
    "start": "1458960",
    "end": "1464360"
  },
  {
    "text": "that we had to tie into under the hood well you can actually make it super stable if you work on it and you're",
    "start": "1464360",
    "end": "1470179"
  },
  {
    "text": "careful about how you do it and all of our all the code we actually",
    "start": "1470179",
    "end": "1475280"
  },
  {
    "text": "run in production is legitimately checked into open source we don't have a secret fork or anything like that that",
    "start": "1475280",
    "end": "1480320"
  },
  {
    "text": "we run in production and then keep the open source stuff separate so you can review it in open source and we still",
    "start": "1480320",
    "end": "1487640"
  },
  {
    "text": "have some work to do before it works for Iceberg but we're trying to bring it to other catalogs as well",
    "start": "1487640",
    "end": "1493900"
  },
  {
    "text": "and so let's get into some of the results that we picked up here this is kind of",
    "start": "1495799",
    "end": "1501200"
  },
  {
    "text": "the interesting stuff so some of our key production statistics here",
    "start": "1501200",
    "end": "1506720"
  },
  {
    "text": "are so our largest cluster sizes that we're running in production right now",
    "start": "1506720",
    "end": "1511820"
  },
  {
    "text": "the P99 is about 27 000 vcpus per cluster so pretty huge P99 of data",
    "start": "1511820",
    "end": "1518900"
  },
  {
    "text": "process per job is about 130 terabytes and P99 of performance is about 763",
    "start": "1518900",
    "end": "1525020"
  },
  {
    "text": "terabytes per hour and our efficiency for spot instance pricing which is similar to the pricing",
    "start": "1525020",
    "end": "1530419"
  },
  {
    "text": "we get internally with our kind of warm pooling and ec2 instance selection algorithm is a mere about 12 cents a",
    "start": "1530419",
    "end": "1537320"
  },
  {
    "text": "terabyte even if you do on demand it's not too bad about two dollars a terabyte average",
    "start": "1537320",
    "end": "1544700"
  },
  {
    "text": "um and keep in mind like things like the sort record are about 97 cents per terabyte right now for cloud sort which",
    "start": "1544700",
    "end": "1551779"
  },
  {
    "text": "is actually also powered by Ray that record and the EXO Shuffle paper and we",
    "start": "1551779",
    "end": "1557120"
  },
  {
    "text": "run about 570 000 uh vcpus of arraycla uh across all of our array clusters",
    "start": "1557120",
    "end": "1563240"
  },
  {
    "text": "every day process about 1.5 petabytes a day have over 300 000 consumer queries running against these jobs every day and",
    "start": "1563240",
    "end": "1570080"
  },
  {
    "text": "overall we're reducing our cost versus spark at most these are fully automated",
    "start": "1570080",
    "end": "1575200"
  },
  {
    "text": "clusters that are just being provisioned without any Hands-On intervention intervention we're still getting about",
    "start": "1575200",
    "end": "1580340"
  },
  {
    "text": "76 Improvement on our P99 about 30 percent with our average Improvement",
    "start": "1580340",
    "end": "1585740"
  },
  {
    "text": "focusing on the largest tables moving from spark to Ray and another upside surprise is we were",
    "start": "1585740",
    "end": "1592700"
  },
  {
    "text": "able to tune our algorithm to avoid unnecessary copies of S3 files versus",
    "start": "1592700",
    "end": "1599360"
  },
  {
    "text": "spark so basically we would group a compacted data set file by the date each record was last updated and then we",
    "start": "1599360",
    "end": "1605240"
  },
  {
    "text": "would copy each S3 file that was Untouched by the merge operation by reference instead of copying the file",
    "start": "1605240",
    "end": "1610880"
  },
  {
    "text": "contents and then we'd repeat that initial grouping as basically the untouched file ratio to grades over time",
    "start": "1610880",
    "end": "1617120"
  },
  {
    "text": "and this put our cost reduction versus spark since we are initially focused not only reducing compute cost this actually",
    "start": "1617120",
    "end": "1623360"
  },
  {
    "text": "improved the cost above and beyond the cost of compute to about 114 percent of",
    "start": "1623360",
    "end": "1628520"
  },
  {
    "text": "just the cost of compute and this is assuming that you have a garbage collector in S3 it's worse if you don't",
    "start": "1628520",
    "end": "1633679"
  },
  {
    "text": "our garbage collector maybe leaves old files sitting there for two days so we're saying if it sits there for two",
    "start": "1633679",
    "end": "1640159"
  },
  {
    "text": "days that's the cost reduction we're getting and some interesting graphs here to",
    "start": "1640159",
    "end": "1646580"
  },
  {
    "text": "quickly review there's a lot of insights and and one Insight from the whole",
    "start": "1646580",
    "end": "1652100"
  },
  {
    "text": "migration as you know at scale basically QA and Telemetry in production become",
    "start": "1652100",
    "end": "1657260"
  },
  {
    "text": "the same thing you can't test every case so your quality assurance is only as good as your metrics",
    "start": "1657260",
    "end": "1662480"
  },
  {
    "text": "and this shows the Orange Line there is an important one when you're going left to right across this graph you're going",
    "start": "1662480",
    "end": "1668600"
  },
  {
    "text": "increasing cluster size from 1000 vcpus on the far left to about 27 000 CPUs on",
    "start": "1668600",
    "end": "1674299"
  },
  {
    "text": "the far right you see some problems with or some interesting use cases of tables",
    "start": "1674299",
    "end": "1679340"
  },
  {
    "text": "that have a lot of tiny rows but not a lot of data some a few huge rows which is where we get our highest data",
    "start": "1679340",
    "end": "1685100"
  },
  {
    "text": "processing rates and we're processing you know on average",
    "start": "1685100",
    "end": "1690880"
  },
  {
    "text": "182 terabytes an hour on those job runs and some poor efficiency data points",
    "start": "1690880",
    "end": "1698419"
  },
  {
    "text": "and then we can dig in with some other graphs and find out okay why why did we have poor efficiency well it was slow",
    "start": "1698419",
    "end": "1703760"
  },
  {
    "text": "materialized time which is way of us saying uh the slowness was due to us spending too much time writing back to",
    "start": "1703760",
    "end": "1709520"
  },
  {
    "text": "S3 and find other data points that are pointing to high overhead where you're doing other things besides the actual",
    "start": "1709520",
    "end": "1714740"
  },
  {
    "text": "work on your cluster and some slow cluster setup times as you're getting into the larger clusters",
    "start": "1714740",
    "end": "1721279"
  },
  {
    "text": "and to show that with our smaller clusters less than 10 000 CPUs you have less than 20 overhead provisioning ec2",
    "start": "1721279",
    "end": "1727220"
  },
  {
    "text": "instances but then that goes up to 30 to 50 overhead provisioning ec2 instances above 10 000 CPUs",
    "start": "1727220",
    "end": "1734480"
  },
  {
    "text": "going back to benefits of using Something Like Glue if you don't have huge data volumes",
    "start": "1734480",
    "end": "1739520"
  },
  {
    "text": "and again you see here the 60 materialized time in that regressive data set we saw earlier going to S3",
    "start": "1739520",
    "end": "1745520"
  },
  {
    "text": "writes and then smaller smaller clusters generally have",
    "start": "1745520",
    "end": "1751279"
  },
  {
    "text": "less overhead overall larger clusters 40 to 70 percent overhead overall greater than 10 000",
    "start": "1751279",
    "end": "1757400"
  },
  {
    "text": "CPUs and this overhead if we break it apart the overhead from just the tasks like",
    "start": "1757400",
    "end": "1764720"
  },
  {
    "text": "scheduling tasks and cleaning them up and the raid driver grows larger the larger the cluster gets as well compared",
    "start": "1764720",
    "end": "1770120"
  },
  {
    "text": "to just provisioning time which dominates on smaller clusters",
    "start": "1770120",
    "end": "1775299"
  },
  {
    "text": "and also cluster sizing is hard this graph just shows we're not very",
    "start": "1775940",
    "end": "1781039"
  },
  {
    "text": "good at utilizing all of our objects to our memory we need to get better at tuning and choosing the memory sizes",
    "start": "1781039",
    "end": "1786740"
  },
  {
    "text": "that we have on our clusters we're we're overly cautious right now we don't use enough",
    "start": "1786740",
    "end": "1792559"
  },
  {
    "text": "and so in conclusion here if you're thinking should you start migrating all your spark jobs to Ray I don't think",
    "start": "1792559",
    "end": "1799399"
  },
  {
    "text": "these results are really a green light to say all right you all rush out and start migrating all your spark jobs to",
    "start": "1799399",
    "end": "1804919"
  },
  {
    "text": "Ray because there's no paved road to translate them all from spark to Ray Spark's still more General and feature",
    "start": "1804919",
    "end": "1810380"
  },
  {
    "text": "Rich for data processing than Ray and if you just run your existing spark jobs on something like radp you shouldn't expect",
    "start": "1810380",
    "end": "1817159"
  },
  {
    "text": "comparable improvements here right so if you want to have some key takeaways I",
    "start": "1817159",
    "end": "1823039"
  },
  {
    "text": "would say the flexibility of Ray core is what we've Illustrated here lets you craft Optimal Solutions to very specific",
    "start": "1823039",
    "end": "1829100"
  },
  {
    "text": "problems and we've given an example of a very specific problem we've crafted a more optimal solution for with Ray here",
    "start": "1829100",
    "end": "1835039"
  },
  {
    "text": "and the EXO Shuffle paper is an interesting use case where you can see more General examples of how Ray can improve the data processing space and in",
    "start": "1835039",
    "end": "1841940"
  },
  {
    "text": "general it has by setting the 100 terabyte Cloud sort cost record with 97 dollars",
    "start": "1841940",
    "end": "1847460"
  },
  {
    "text": "so in conclusion I think Ray has the potential to be a world-class big data processing framework but realizing that",
    "start": "1847460",
    "end": "1852799"
  },
  {
    "text": "potential takes a lot of work today and very quickly some future work we",
    "start": "1852799",
    "end": "1858080"
  },
  {
    "text": "didn't do um I want to call out the Daft project you guys if you haven't heard of it that's its link there on GitHub it",
    "start": "1858080",
    "end": "1863899"
  },
  {
    "text": "provides fast scalable pythonic data frames it can run on Ray and convert to and from Ray data sets right now we're",
    "start": "1863899",
    "end": "1869419"
  },
  {
    "text": "working with it as you saw earlier we have that materialize regression that's all writing to S3 reading and writing",
    "start": "1869419",
    "end": "1875179"
  },
  {
    "text": "from S3 are key bottlenecks for our workflow for tons of workflows file i o in general we're working on solving that",
    "start": "1875179",
    "end": "1881360"
  },
  {
    "text": "problem first with daf our long-term goal with Daft is to convert our whole compactor from",
    "start": "1881360",
    "end": "1887299"
  },
  {
    "text": "thousands of lines of code down to a few elegant lines of daf data frame code without sacrificing",
    "start": "1887299",
    "end": "1892480"
  },
  {
    "text": "efficiency scalability reliability and we're doing ongoing tests with it in",
    "start": "1892480",
    "end": "1897740"
  },
  {
    "text": "production right now and some S3 benchmarks are pretty interesting with it basically you're",
    "start": "1897740",
    "end": "1903020"
  },
  {
    "text": "getting for single column reads more than 200 Improvement median versus Pi Arrow over",
    "start": "1903020",
    "end": "1909320"
  },
  {
    "text": "one thousand percent versus s3fs for single column parquet reads that",
    "start": "1909320",
    "end": "1914419"
  },
  {
    "text": "we're doing with our production data from with Daft and with all columns you're still getting 23 Improvement",
    "start": "1914419",
    "end": "1919820"
  },
  {
    "text": "versus Pi arrow and over 400 percent versus s3fs and some future work that",
    "start": "1919820",
    "end": "1926480"
  },
  {
    "text": "we're looking to do is bring this Apache Iceberg compaction over to AWS glue",
    "start": "1926480",
    "end": "1932059"
  },
  {
    "text": "using the Delta cap project so you can all use it to do merge on read copy on",
    "start": "1932059",
    "end": "1938179"
  },
  {
    "text": "write operations in your Iceberg catalogs as well and benefit from these same efficiency improvements that we've",
    "start": "1938179",
    "end": "1944120"
  },
  {
    "text": "demonstrated here so thank you all if you want to reach",
    "start": "1944120",
    "end": "1951500"
  },
  {
    "text": "out to me I'll be here after this talk but also ping me on the ray Community slack or visit us at the Delta Cat",
    "start": "1951500",
    "end": "1958460"
  },
  {
    "text": "Project to review what we just talked about at that link",
    "start": "1958460",
    "end": "1963760"
  }
]