[
  {
    "start": "0",
    "end": "233000"
  },
  {
    "text": "hi everyone uh thank you for coming to our talk yeah as we as uh Matthew said today we'll be talking about how to use",
    "start": "3000",
    "end": "8440"
  },
  {
    "text": "Ray data and any skill to optimize your batch inference workloads so I'm Scott a software",
    "start": "8440",
    "end": "14519"
  },
  {
    "text": "engineer at any skill I'm working on R data I'm joined by Richard uh who's a product",
    "start": "14519",
    "end": "20680"
  },
  {
    "text": "manager cool so quick overview of today's talk first I'll go over the basics of bash inference and then I'll",
    "start": "20680",
    "end": "26640"
  },
  {
    "text": "talk about uh why you should use Ray and any skill for bash inference then we'll compare uh it to other Solutions out",
    "start": "26640",
    "end": "32840"
  },
  {
    "text": "there and finally uh we'll introduce Ray LM batch which is a framework for cost",
    "start": "32840",
    "end": "38000"
  },
  {
    "text": "efficient batch LM inference so what is batch inference",
    "start": "38000",
    "end": "45320"
  },
  {
    "text": "batch inference workloads typically uh are workloads which generate model outputs from a large input data set and",
    "start": "45320",
    "end": "52399"
  },
  {
    "text": "these are often uh referred to as offline workloads and some defining characteristics are that they have a very large data scale so often hundreds",
    "start": "52399",
    "end": "59320"
  },
  {
    "text": "of gigabytes uh terabytes or even larger uh and they have a big focus on high throughput and low cost uh and this",
    "start": "59320",
    "end": "66080"
  },
  {
    "text": "means that they often have a loose latency requirement um and it uh typically involves deep learning or",
    "start": "66080",
    "end": "72439"
  },
  {
    "text": "other models that have heavy GPU computations uh and below this is what a",
    "start": "72439",
    "end": "77520"
  },
  {
    "text": "typical bat inference workload would look like you would start off by reading in some files maybe doing some pre-processing on the data before",
    "start": "77520",
    "end": "84280"
  },
  {
    "text": "passing it to your uh GPU model uh after that you might do some postprocessing and then write out the results to some",
    "start": "84280",
    "end": "90079"
  },
  {
    "text": "cloud storage uh and batch inference workloads typically permeate uh many many business",
    "start": "90079",
    "end": "96680"
  },
  {
    "text": "use cases right they're very very common uh some example workloads uh might be for like text and image sorry text and",
    "start": "96680",
    "end": "103920"
  },
  {
    "text": "image embedding generation uh image model batch inference and llm batch inference uh and some common alternative",
    "start": "103920",
    "end": "110320"
  },
  {
    "text": "Solutions out there for running these workloads include open AI stagemaker spark and",
    "start": "110320",
    "end": "117680"
  },
  {
    "text": "Bedrock now so far we we've discussed a lot about offline bash inference um and now I'll take a second to compare it to",
    "start": "118280",
    "end": "124360"
  },
  {
    "text": "online inference so online or real-time serving workloads are a little different in that they want to minimize uh latency",
    "start": "124360",
    "end": "132120"
  },
  {
    "text": "um and these workloads are very different and so they require different uh configuration of like code uh",
    "start": "132120",
    "end": "137959"
  },
  {
    "text": "optimization and compute and specifically for llm workloads this means that we're going to have to use",
    "start": "137959",
    "end": "143680"
  },
  {
    "text": "different optimization techniques and compute to really uh optimize our objective so for offline workloads uh we",
    "start": "143680",
    "end": "150120"
  },
  {
    "text": "want to maximize throughput and minimize the cost and so we'll use optimization techniques like pipeline parallelism and",
    "start": "150120",
    "end": "156080"
  },
  {
    "text": "KV caching for these types of workloads we can often uh use cost efficient gpus",
    "start": "156080",
    "end": "161400"
  },
  {
    "text": "that are more widely available like A10 GS and l4s for online uh real-time inference workloads we want to maximize",
    "start": "161400",
    "end": "168400"
  },
  {
    "text": "reliability and minimize latency and so we'll use techniques like speculative decoding and tensor pism in order to",
    "start": "168400",
    "end": "174519"
  },
  {
    "text": "achieve them uh and these typically require the use of higher end gpus like h100s and 800s to really minimize",
    "start": "174519",
    "end": "183239"
  },
  {
    "text": "aeny now let's compare what these workloads look like at a high level so taking a glance um the online real-time",
    "start": "184120",
    "end": "191799"
  },
  {
    "text": "workloads look pretty similar to the offline batch inference workload that we discussed uh they need to take the user",
    "start": "191799",
    "end": "196920"
  },
  {
    "text": "query do some pre-processing set pass it through the model and then maybe do some postprocessing before returning the",
    "start": "196920",
    "end": "202640"
  },
  {
    "text": "response um but because of these uh different techniques and optimizations that we discussed they actually have uh",
    "start": "202640",
    "end": "209040"
  },
  {
    "text": "they run into different problems that we'll need to uh figure out and so for offline batch inference uh we run into",
    "start": "209040",
    "end": "214599"
  },
  {
    "text": "problems around like GP utilization fault tolerance autoscaling for these really large workloads that involve like",
    "start": "214599",
    "end": "221159"
  },
  {
    "text": "terabytes of data running for days or weeks uh whereas for online realtime inference we'll have to tackle problems",
    "start": "221159",
    "end": "227760"
  },
  {
    "text": "around request routing load balancing and L multiplexing Cool next I'll turn it over",
    "start": "227760",
    "end": "234319"
  },
  {
    "start": "233000",
    "end": "682000"
  },
  {
    "text": "to Richard who will introduce uh Ray and any skill stack for basion FRS",
    "start": "234319",
    "end": "240319"
  },
  {
    "text": "hi everyone thanks Scott so what I'm going to talk about now is some of the sort of core problems from a technical",
    "start": "240319",
    "end": "247159"
  },
  {
    "text": "perspective on how we think about uh batch Alum inference or batch inference",
    "start": "247159",
    "end": "252400"
  },
  {
    "text": "and then batch Alum inference and we'll also then talk about how the r/ scale",
    "start": "252400",
    "end": "257600"
  },
  {
    "text": "solution sort of addresses these problems from a technical perspective so again as Scott mentioned",
    "start": "257600",
    "end": "263320"
  },
  {
    "text": "there's sort of two standard ways of doing batch inference especially with deep learning models nowadays one is to",
    "start": "263320",
    "end": "269080"
  },
  {
    "text": "re re use an existing data system such as spark or Flink or to repurpose an",
    "start": "269080",
    "end": "274360"
  },
  {
    "text": "online deployment as something running on something like kubernetes and both of these sort of techniques actually come",
    "start": "274360",
    "end": "280840"
  },
  {
    "text": "with some major limitations which I'll expand upon next um for existing Data Systems from a",
    "start": "280840",
    "end": "288199"
  },
  {
    "text": "technical perspective there's a couple limitations when it comes to doing batch inference with deep learning first of",
    "start": "288199",
    "end": "294000"
  },
  {
    "text": "all you need really great GPU support especially when you're working with deep learning models and you need to utilize",
    "start": "294000",
    "end": "299400"
  },
  {
    "text": "these gpus well and therefore you don't really want to have a bunch of gpus sitting idle waiting while the CPUs run",
    "start": "299400",
    "end": "307440"
  },
  {
    "text": "or as sort of pre-processing or post-processing steps run so the second part is that you need",
    "start": "307440",
    "end": "313720"
  },
  {
    "text": "uh support for great parallelism uh especially in a sort of expressive way these models are often very large and",
    "start": "313720",
    "end": "320240"
  },
  {
    "text": "therefore it's actually quite necessary to have some sort of hierarchical parallelism for example being able to do",
    "start": "320240",
    "end": "326520"
  },
  {
    "text": "data parallelism which is very standard in these data processing systems and within that having some support for tensor parallelism or pipeline",
    "start": "326520",
    "end": "333759"
  },
  {
    "text": "parallelism and finally similar to other systems it needs to be reliable needs to work at scale needs to have all the",
    "start": "333759",
    "end": "339520"
  },
  {
    "text": "standard data connector supports compared to online serving",
    "start": "339520",
    "end": "344600"
  },
  {
    "text": "systems which is the other sort of alternative for deploying uh Bach FR Solutions you'll often see these sort of",
    "start": "344600",
    "end": "350560"
  },
  {
    "text": "three limitations the first is having some overhead from other components such as low bouncing components or proxies",
    "start": "350560",
    "end": "357360"
  },
  {
    "text": "and these typical things are typically again built and constructed in a way that are are architected for um good",
    "start": "357360",
    "end": "364400"
  },
  {
    "text": "latencies but not like massive uh gigabyte throughput the other thing is uh you",
    "start": "364400",
    "end": "370560"
  },
  {
    "text": "have to manage the online service separately from uh the the actual batch system which leads to sort of a",
    "start": "370560",
    "end": "376919"
  },
  {
    "text": "two-tiered uh deployment model um and having sort of observability that doesn't work across the stacks and",
    "start": "376919",
    "end": "383759"
  },
  {
    "text": "finally uh for larger models again as Scott mentioned you'll compare things differently right you'll you might",
    "start": "383759",
    "end": "389599"
  },
  {
    "text": "Target sort of slas for online deployments in which case you will sacrifice some throughput overall what",
    "start": "389599",
    "end": "396120"
  },
  {
    "text": "we often see is that like especially for NVIDIA gpus you can max out throughput much more effectively than you can uh",
    "start": "396120",
    "end": "403039"
  },
  {
    "text": "and um and achieve a much greater uh sort of token throughput or you know uh image or input throughput um per second",
    "start": "403039",
    "end": "410400"
  },
  {
    "text": "compared to when you optimize things for thec so talking a little bit about the",
    "start": "410400",
    "end": "416720"
  },
  {
    "text": "sort of the any scale um sort of philosophy and approach to the solution is we we sort of look at this as a full",
    "start": "416720",
    "end": "422879"
  },
  {
    "text": "stack problem right we start off with a ray data uh which we've had numerous",
    "start": "422879",
    "end": "427960"
  },
  {
    "text": "talks on uh over the course of the summit Raya is the distributed data system for unstructured data processing",
    "start": "427960",
    "end": "434280"
  },
  {
    "text": "uh this is built on top of Ray core which is a Distributive runtime for scaling Ai and data workloads and and",
    "start": "434280",
    "end": "441960"
  },
  {
    "text": "underneath Um this can so Ray data and Ray core can run on any Cloud but in particular any scale provides",
    "start": "441960",
    "end": "448160"
  },
  {
    "text": "capabilities for scaling across clouds so talking a little bit more",
    "start": "448160",
    "end": "453639"
  },
  {
    "text": "about each layer just as a primer um Ray data comes with uh sort of skillable and",
    "start": "453639",
    "end": "458800"
  },
  {
    "text": "Fator apis for data processing especially for unstructured data um it",
    "start": "458800",
    "end": "463840"
  },
  {
    "text": "integrates very closely and is built as a python first sort of system uh so pandas numpy pyo these things are core",
    "start": "463840",
    "end": "471440"
  },
  {
    "text": "components of Ray data and interoperate nicely as compared to other systems that might be built from the Java",
    "start": "471440",
    "end": "477360"
  },
  {
    "text": "world uh this allows for inter operability between the machine learning ecosystem and also the data ecosystem",
    "start": "477360",
    "end": "483759"
  },
  {
    "text": "and finally there's a sort of streaming execution which allow for really efficient mixed CPU and GPU execution",
    "start": "483759",
    "end": "490039"
  },
  {
    "text": "and I'll talk a little bit more about this next as an example it's typically",
    "start": "490039",
    "end": "495479"
  },
  {
    "text": "really important to have CPUs and gpus fully utilized together at the same time",
    "start": "495479",
    "end": "501479"
  },
  {
    "text": "however this is sort of a core limitation in existing systems like spark where scheduling is fused uh as",
    "start": "501479",
    "end": "507800"
  },
  {
    "text": "all the stages within scheduling are fused together regardless of the resource requirements this means that in a",
    "start": "507800",
    "end": "514760"
  },
  {
    "text": "typical uh single cluster setup the total parallelism is limited by um your rate limiting factor which is probably",
    "start": "514760",
    "end": "521080"
  },
  {
    "text": "the number of gpus that you have leading to underutilization CPUs we've also",
    "start": "521080",
    "end": "526320"
  },
  {
    "text": "tried doing things like stage scheduling but that unfortunately has run to limitations when using gpus uh so but this fundamental",
    "start": "526320",
    "end": "533480"
  },
  {
    "text": "limitation means that gbus are idle uh when um or there's sort of",
    "start": "533480",
    "end": "538519"
  },
  {
    "text": "underutilization of the resources and also gpus are completely idle when there is other sort of proess processing steps",
    "start": "538519",
    "end": "545040"
  },
  {
    "text": "that are happening on the other hand ray data is able to sort of stream the data more",
    "start": "545040",
    "end": "550279"
  },
  {
    "text": "effectively and execute the the CPU and uh GPU steps simultaneously if written",
    "start": "550279",
    "end": "556000"
  },
  {
    "text": "in a uh sort of streaming fashion and with a streaming executor leading to a much more efficient uh sort of batch",
    "start": "556000",
    "end": "562760"
  },
  {
    "text": "batch inference step so we do have a blog post that talks a little bit more about this and we'll have a QR link uh",
    "start": "562760",
    "end": "568160"
  },
  {
    "text": "later later in the talk Ray data is built um on top of Ray core",
    "start": "568160",
    "end": "574880"
  },
  {
    "text": "so Ray core provides this scalable Foundation First of all with um with",
    "start": "574880",
    "end": "580240"
  },
  {
    "text": "Primitives to express very flexible GPU scheduling uh specifically for sort of fine grain compute task that may maybe",
    "start": "580240",
    "end": "587160"
  },
  {
    "text": "asynchronous and devish the GPU um this is sort of unique in the sense of there's not many other distributive",
    "start": "587160",
    "end": "593440"
  },
  {
    "text": "systems that are able to uh sort of offer this functionality other thing is stateful",
    "start": "593440",
    "end": "598760"
  },
  {
    "text": "actors which allow to uh allow the system and sort of programs to hold",
    "start": "598760",
    "end": "603800"
  },
  {
    "text": "expensive models uh in memory allowing for sort of um the the you know caching",
    "start": "603800",
    "end": "610519"
  },
  {
    "text": "of these these objects and not needing to uh sort of rematerialize and redownload these things which can be",
    "start": "610519",
    "end": "616360"
  },
  {
    "text": "very expensive and finally as mentioned before the ray cor does come with nested",
    "start": "616360",
    "end": "621959"
  },
  {
    "text": "parallelism which makes it really easy to express data Paralis and model plalism um and and sort of construct a",
    "start": "621959",
    "end": "629040"
  },
  {
    "text": "batch system that's uniquely suited for models that are getting bigger and bigger and finally uh just to quickly",
    "start": "629040",
    "end": "636800"
  },
  {
    "text": "sort of talk about nisco infrastructure uh NCO infrastructure comes with a variety of different sort of functionalities that are really relevant",
    "start": "636800",
    "end": "642800"
  },
  {
    "text": "for this type of workload uh so you know crossone instant searching to find gpus",
    "start": "642800",
    "end": "648000"
  },
  {
    "text": "out the low rates uh being able to support spa inance spa back onto on demand to best average uh Spa instances",
    "start": "648000",
    "end": "654320"
  },
  {
    "text": "for cost savings um also comes with accelerated model loading um as you as",
    "start": "654320",
    "end": "659440"
  },
  {
    "text": "we have multiple demos of um to sort of demonstrate and have the capability of",
    "start": "659440",
    "end": "664839"
  },
  {
    "text": "quick startup of large model replicas and also extensive observability across the stack so now I'll hand it over back",
    "start": "664839",
    "end": "672160"
  },
  {
    "text": "to St uh Scott to talk a little bit about the code examples and results that we have for standard batch",
    "start": "672160",
    "end": "680320"
  },
  {
    "text": "inference thanks Richard yeah so let's take a look at um how we might Implement one of these batch inference workloads",
    "start": "681200",
    "end": "687279"
  },
  {
    "text": "with red data so this is kind of the layout that we've seen before um and just to give a quick",
    "start": "687279",
    "end": "693519"
  },
  {
    "text": "example of how you might implement this you'll start off with one of the many read methods that R data provides uh so",
    "start": "693519",
    "end": "699040"
  },
  {
    "text": "we're very flexible in in the data input and output formats uh we support very common formats like Json CSV images um",
    "start": "699040",
    "end": "707360"
  },
  {
    "text": "videos and and as well as like SQL based databases uh and much more uh you can check out more on our",
    "start": "707360",
    "end": "714279"
  },
  {
    "text": "docs um so yeah you would do that read and then the next step would be doing this uh pre-processing Stu step which we",
    "start": "714279",
    "end": "719760"
  },
  {
    "text": "can express with like a map operation and so in this case uh we're just applying kind of the stateless",
    "start": "719760",
    "end": "725000"
  },
  {
    "text": "transformation to each row of the data set so we can use uh this task based or stateless uh map um in order to express",
    "start": "725000",
    "end": "733320"
  },
  {
    "text": "that the next step is going to be applying the GPU model uh the GPU based model and so for this step uh what we",
    "start": "733320",
    "end": "740320"
  },
  {
    "text": "want to do is actually use this actor-based or stateful map function to do it and so this means that you just",
    "start": "740320",
    "end": "745839"
  },
  {
    "text": "need to load the expensive model once on your GPU and then you can continue ously repeatedly call uh with whatever batches",
    "start": "745839",
    "end": "752399"
  },
  {
    "text": "that you're passing to it uh Ray data makes it really easy for you to configure uh like the concurrency as",
    "start": "752399",
    "end": "758120"
  },
  {
    "text": "well as number of gpus that you're using on per worker uh so you can test with very small amounts of data um locally or",
    "start": "758120",
    "end": "765399"
  },
  {
    "text": "just on a small cluster and then you can scale up uh very flexibly and easily to uh whatever production workflows you",
    "start": "765399",
    "end": "771040"
  },
  {
    "text": "have in mind finally you might do the same kind of like postprocessing per each row uh",
    "start": "771040",
    "end": "777760"
  },
  {
    "text": "which you can do with a map function and then finally you can uh use any of our WR methods uh to write out the",
    "start": "777760",
    "end": "784480"
  },
  {
    "text": "results to whatever cloud storage local disk",
    "start": "784480",
    "end": "789839"
  },
  {
    "text": "Etc cool now let's take a look at a few case studies uh on how we actually apply these workloads so this is an example of",
    "start": "790320",
    "end": "797600"
  },
  {
    "text": "uh taking some input PDFs and generating a bunch of text embeddings um and so for this example uh we took about 2,000 PDFs",
    "start": "797600",
    "end": "805279"
  },
  {
    "text": "which has like 30,000 Pages uh and we were able to generate about 40,000 encodings so for this workload we used",
    "start": "805279",
    "end": "812120"
  },
  {
    "text": "20 gpus uh and it took about 4 minutes to run uh and the estimated cost uh if",
    "start": "812120",
    "end": "817279"
  },
  {
    "text": "we use AWS spot instances is around 95 cents right so you can see from this graph here that we have basically 100%",
    "start": "817279",
    "end": "823760"
  },
  {
    "text": "GP utilization making sure uh taking advantage of that streaming execution to really maximize how much uh GP",
    "start": "823760",
    "end": "830079"
  },
  {
    "text": "utilization we can get this is another example of using uh",
    "start": "830079",
    "end": "835880"
  },
  {
    "text": "generating text embedding so we use the Falcon refined web data set for this uh which is a 1.68 terab data set um and",
    "start": "835880",
    "end": "844040"
  },
  {
    "text": "using 320 gpus for this large workload uh this took about 18 hours to run in",
    "start": "844040",
    "end": "849160"
  },
  {
    "text": "total uh and the estimated cost on a scale was around $6,000 to run this",
    "start": "849160",
    "end": "855440"
  },
  {
    "text": "workload finally here's an example of uh batch image inference so just comparing",
    "start": "856560",
    "end": "861880"
  },
  {
    "text": "uh Ray to kind of these other Solutions uh Ray is up to two times faster than spark uh on this particular workload uh",
    "start": "861880",
    "end": "868800"
  },
  {
    "text": "and to 17 fast times faster than Sage maker next I'll turn it back over to",
    "start": "868800",
    "end": "875560"
  },
  {
    "start": "874000",
    "end": "1297000"
  },
  {
    "text": "Richard who will talk more uh death about LM infants thanks Scott so this is one of",
    "start": "875560",
    "end": "881560"
  },
  {
    "text": "the more exciting sort of rate turbo announcements which is um some of the work that we've been doing for batch LM",
    "start": "881560",
    "end": "887040"
  },
  {
    "text": "inference in particular on Ray skill um so just as a refresher batch LM",
    "start": "887040",
    "end": "893240"
  },
  {
    "text": "inference is basically very similar to a lot of the graphs that um that Scott has been showing except the LM refence stage",
    "start": "893240",
    "end": "900079"
  },
  {
    "text": "has models that are themselves uh possibly distributed much larger so we're excited to announce RM",
    "start": "900079",
    "end": "907680"
  },
  {
    "text": "batch which is a uh proprietary Library built for a large scale cost efficient batch LM inference it's built on top of",
    "start": "907680",
    "end": "915120"
  },
  {
    "text": "an any scale inference engine um and uh Ray turbo data uh with Ray core and and",
    "start": "915120",
    "end": "921600"
  },
  {
    "text": "any scale um so uh this allows you to bring any sort of Open Source model",
    "start": "921600",
    "end": "927560"
  },
  {
    "text": "that's available on V M because our inscope inference entr uh is built off",
    "start": "927560",
    "end": "932880"
  },
  {
    "text": "of VM it's fault tolerant allowing you to leverage spot inces that scale and uh",
    "start": "932880",
    "end": "938399"
  },
  {
    "text": "achieve massive cost reductions and then it sort of uh also comes with a Optimizer for custom workloads to",
    "start": "938399",
    "end": "945040"
  },
  {
    "text": "configure and optimize your engine settings properly um so uh let's dive into a",
    "start": "945040",
    "end": "950959"
  },
  {
    "text": "quick example of what this uh new library looks like so it's actually quite simple uh it allows you to express",
    "start": "950959",
    "end": "957680"
  },
  {
    "text": "a workload um and we'll cover what a workload means and we we'll be able to",
    "start": "957680",
    "end": "963160"
  },
  {
    "text": "pass that into the batch um sort of this uh batch LM Pipeline and you can just ex",
    "start": "963160",
    "end": "968920"
  },
  {
    "text": "execute the pipeline with run and what this returns is a ray data set which then you can do more postup uh",
    "start": "968920",
    "end": "975240"
  },
  {
    "text": "post post uh um operations as well now um let's dive into what this",
    "start": "975240",
    "end": "982880"
  },
  {
    "text": "workload um API actually looks like so it's quite simple um so you effectively",
    "start": "982880",
    "end": "989120"
  },
  {
    "text": "you should be able to express um where your data set is located and um by and",
    "start": "989120",
    "end": "995199"
  },
  {
    "text": "have sort of the ability to do some pre-processing of the data set within this API within the load API uh load",
    "start": "995199",
    "end": "1000920"
  },
  {
    "text": "data set sort of abstraction um there's some specific pars row um sort of",
    "start": "1000920",
    "end": "1006000"
  },
  {
    "text": "operators so that you know you can express like how exactly you want to massage the data so that maybe for",
    "start": "1006000",
    "end": "1012040"
  },
  {
    "text": "example you want to map it into like open the eye chat um sort of API so that you you know you can do batch inference",
    "start": "1012040",
    "end": "1018079"
  },
  {
    "text": "in that that setting um also expresses uh the ability to you know provide sampling parameters which",
    "start": "1018079",
    "end": "1024160"
  },
  {
    "text": "also in here you can plug in um sort of you know various things like temperature max token so on so forth um and uh the",
    "start": "1024160",
    "end": "1033079"
  },
  {
    "text": "the abstraction also allows you to do some some sampling from a data set perspective so that you can operate on a",
    "start": "1033079",
    "end": "1038600"
  },
  {
    "text": "fraction of the data set at once so um so again this workload is plugged back",
    "start": "1038600",
    "end": "1044400"
  },
  {
    "text": "into um this Ray LM batch which takes in two things the workload API and also a",
    "start": "1044400",
    "end": "1050600"
  },
  {
    "text": "configuration file now the configuration file is quite straightforward you can express which",
    "start": "1050600",
    "end": "1056160"
  },
  {
    "text": "model you want to run and then also some number of engine um keyword arcs uh and",
    "start": "1056160",
    "end": "1061960"
  },
  {
    "text": "accelerator type now a couple nice things here is that um uh like we will",
    "start": "1061960",
    "end": "1067600"
  },
  {
    "text": "be we also have an Optimizer which will sort of Auto automatically tune uh some",
    "start": "1067600",
    "end": "1072720"
  },
  {
    "text": "of these or most of these engine um configurations for you given the workload and also so um right now we",
    "start": "1072720",
    "end": "1080400"
  },
  {
    "text": "have to specify a particular accelerator type but we're planning to expand this to feel a little bit more serve for this",
    "start": "1080400",
    "end": "1086039"
  },
  {
    "text": "allowing to express a class of accelerated types and we'll be able to sort of mix and match over that um what's what's really interesting",
    "start": "1086039",
    "end": "1093320"
  },
  {
    "text": "about this is that given the accelerator type the configurations will be different so we will also um the",
    "start": "1093320",
    "end": "1098840"
  },
  {
    "text": "optimizer should be able to take advantage of that so again putting it out all together this is what like a",
    "start": "1098840",
    "end": "1104840"
  },
  {
    "text": "typical Ray LM batch um abstraction will look like and it should be quite easy to sort of Express these um these batch",
    "start": "1104840",
    "end": "1112080"
  },
  {
    "text": "inference workloads very easily um and when you run pipeline. run this is like",
    "start": "1112080",
    "end": "1118120"
  },
  {
    "text": "it will run like a typical sort of um execution of like the read uh tokenization um inference and so on so",
    "start": "1118120",
    "end": "1126720"
  },
  {
    "text": "forth so in comparison um to Alternative Solutions uh Ray LM batch with its",
    "start": "1126720",
    "end": "1134960"
  },
  {
    "text": "optimized configurations is able to achieve up to 2X sort of lower price",
    "start": "1134960",
    "end": "1140280"
  },
  {
    "text": "than bedrock and this is bedrock with batch pricing right so it's bedrock with 50% off um is with in the fp8 model",
    "start": "1140280",
    "end": "1149159"
  },
  {
    "text": "we're able to achieve up to 3x and this is again for a llama llama 38b",
    "start": "1149159",
    "end": "1154320"
  },
  {
    "text": "model and then with prefix caching which when you have sort of shared prefixes",
    "start": "1154320",
    "end": "1159559"
  },
  {
    "text": "we're able to achieve up to sixa a spreadrock with batch pricing um so on larger models um for",
    "start": "1159559",
    "end": "1168720"
  },
  {
    "text": "for fp8 we're able to achieve 20% off um Bedrock batch pricing and uh for prefix",
    "start": "1168720",
    "end": "1176200"
  },
  {
    "text": "cached workloads we can achieve up to 2x cheaper than batch pricing on AOS",
    "start": "1176200",
    "end": "1181720"
  },
  {
    "text": "Bedrock um I think the other thing that's not noted on the slides here is that these were all achieved using l4s",
    "start": "1181720",
    "end": "1188360"
  },
  {
    "text": "and l4s um so not needing A1 100s or h100s pre-reserved instances and so on",
    "start": "1188360",
    "end": "1194320"
  },
  {
    "text": "so forth um yeah so uh the other Maybe the other thing I would note is that um",
    "start": "1194320",
    "end": "1201760"
  },
  {
    "text": "we compare if for fb8 it's actually not really clear um what Bedrock itself is using they may be using fp16 and fp8 so",
    "start": "1201760",
    "end": "1209200"
  },
  {
    "text": "that's why we kind of try to demonstrate both numbers and the last thing is that there are limitations on the Bed Rock",
    "start": "1209200",
    "end": "1214960"
  },
  {
    "text": "side in the sense that if you use batch pricing um it will sort of take up to 24 hours to return especially for open AA",
    "start": "1214960",
    "end": "1221559"
  },
  {
    "text": "as well and whereas for for any scale raym batch um the solution is able to",
    "start": "1221559",
    "end": "1226919"
  },
  {
    "text": "start executing as soon as you acquire instances and typically for l4s and l4s uh those are quite instant to to",
    "start": "1226919",
    "end": "1234960"
  },
  {
    "text": "acquire um so the road map here moving forward is to start focusing on optimizing these and expanding our",
    "start": "1234960",
    "end": "1242200"
  },
  {
    "text": "Optimizer to sort of um leverage these multimodal models uh pixol 3.2 uh Momo",
    "start": "1242200",
    "end": "1249280"
  },
  {
    "text": "is also sort of on our radar we'll we'll pick a couple and sort of focus on large and small as well and we believe that",
    "start": "1249280",
    "end": "1255240"
  },
  {
    "text": "there's huge rooms for cost optimizations there um on the proprietary side we will sort of be",
    "start": "1255240",
    "end": "1261640"
  },
  {
    "text": "releasing our um or engine Optimizer API for public preview so that there's U",
    "start": "1261640",
    "end": "1267640"
  },
  {
    "text": "anyone on the any scale platform is able to leverage it and we obviously have sort of Engineers continuing to burn",
    "start": "1267640",
    "end": "1274120"
  },
  {
    "text": "down uh konel optimizations to improve throughput um so to recap we sort of",
    "start": "1274120",
    "end": "1279919"
  },
  {
    "text": "talked about um as a very high level what Bach inference is about where why",
    "start": "1279919",
    "end": "1285200"
  },
  {
    "text": "Ray and any scale are a great fit um what's the story of results 10 we also introduced RM batch um which is a cost",
    "start": "1285200",
    "end": "1292480"
  },
  {
    "text": "cost effective batch LM reference um Library so I think that's that's",
    "start": "1292480",
    "end": "1297760"
  },
  {
    "start": "1297000",
    "end": "1790000"
  },
  {
    "text": "basically it for this talk and thank you all for for attending and happy to take any questions at this",
    "start": "1297760",
    "end": "1304210"
  },
  {
    "text": "[Applause]",
    "start": "1304210",
    "end": "1309130"
  },
  {
    "text": "point is there a mic no we don't have a mic okay did you investigate the reasons why",
    "start": "1309960",
    "end": "1316520"
  },
  {
    "text": "Sage maker was 17 time slower uh any thoughts on",
    "start": "1316520",
    "end": "1321840"
  },
  {
    "text": "why for the image for the image example I don't I don't recall the um I don't",
    "start": "1321840",
    "end": "1329039"
  },
  {
    "text": "recall the exact reasons why stage maker in particular was slow um I think it's just a combination of like all of the",
    "start": "1329039",
    "end": "1336000"
  },
  {
    "text": "Ray Ray data and any scale optimizations that kind of combine all together as a stack yeah I think the the API itself",
    "start": "1336000",
    "end": "1343640"
  },
  {
    "text": "was a little like we the sagemaker API is a little bit transparent or like you know is it's not very or opaque is the",
    "start": "1343640",
    "end": "1350320"
  },
  {
    "text": "right word it's like not very clear where exactly is the like the the way to optimize it and there's also four four",
    "start": "1350320",
    "end": "1357480"
  },
  {
    "text": "or five different offerings for uh batch Transformations on stagemaker so um we chose the one that was marketed more",
    "start": "1357480",
    "end": "1363880"
  },
  {
    "text": "clearly towards this particular use case it may be like the results may be different at this point but we you know",
    "start": "1363880",
    "end": "1370720"
  },
  {
    "text": "we would be happy to update these numbers if that was the case so in the example that you showed",
    "start": "1370720",
    "end": "1377960"
  },
  {
    "text": "you had Cod strippers on the pipeline doing preprocessing in the model U is",
    "start": "1377960",
    "end": "1384080"
  },
  {
    "text": "there do you guys support prefetching onto the",
    "start": "1384080",
    "end": "1388200"
  },
  {
    "text": "gpus yes do you guys support yeah so the question was do we",
    "start": "1389799",
    "end": "1395480"
  },
  {
    "text": "support prefetching onto the gpus when we like kind of run these uh GPU heavy um map batches operations and um I think",
    "start": "1395480",
    "end": "1403039"
  },
  {
    "text": "for map batches we currently do support um like prefetching of of batches and",
    "start": "1403039",
    "end": "1408200"
  },
  {
    "text": "you can configure that um when you use your data yeah Derek yeah can results be",
    "start": "1408200",
    "end": "1416799"
  },
  {
    "text": "materialized like periodically during a long running bch J can uh results be",
    "start": "1416799",
    "end": "1422720"
  },
  {
    "text": "materialized yeah so um do you want to take this one yeah so um I guess by",
    "start": "1422720",
    "end": "1428360"
  },
  {
    "text": "materialize uh if you're referring to just like writing out the results somewhere um and not like in memory um",
    "start": "1428360",
    "end": "1433880"
  },
  {
    "text": "this kind of works as like our R data has streaming execution which means that like whenever it's done processing some",
    "start": "1433880",
    "end": "1440240"
  },
  {
    "text": "amount of data it's able to kind of send it to the next task so the entire like Ray data job doesn't need to run in",
    "start": "1440240",
    "end": "1445679"
  },
  {
    "text": "order to write all the data it'll kind of write things as they're being finished",
    "start": "1445679",
    "end": "1451279"
  },
  {
    "text": "yeah which embeding model did you use uh for the uh I believe it GT large yeah",
    "start": "1451279",
    "end": "1459000"
  },
  {
    "text": "which is a open source model by Alibaba and like how long would it take",
    "start": "1459000",
    "end": "1464799"
  },
  {
    "text": "like I say somebody starts this process and how long would it take before result start to",
    "start": "1464799",
    "end": "1470760"
  },
  {
    "text": "appear like the basically the first results to come out right it should be like maybe a like seconds or minutes",
    "start": "1473200",
    "end": "1479559"
  },
  {
    "text": "basically yeah um you how do you get any",
    "start": "1479559",
    "end": "1485640"
  },
  {
    "text": "observability on uh being able to see the intermediate Object Store values for",
    "start": "1485640",
    "end": "1492679"
  },
  {
    "text": "intermediate results that are being consumed by subsequent steps um so the",
    "start": "1492679",
    "end": "1497760"
  },
  {
    "text": "question is do you get to see intermediate representations of the objects that are being s stored and",
    "start": "1497760",
    "end": "1502799"
  },
  {
    "text": "saved and right now by default I I don't think we have that sort of yeah um you",
    "start": "1502799",
    "end": "1509240"
  },
  {
    "text": "could kind of get into the weeds of things by like using some debugging like debugger like AR Ray debugger that we have to like inspect individual objects",
    "start": "1509240",
    "end": "1515760"
  },
  {
    "text": "um but I would say for most use cases it's t not really typical for like there's no real um use case for like",
    "start": "1515760",
    "end": "1522399"
  },
  {
    "text": "looking at individual objects usually they're just like represented kind of as these like object references um and",
    "start": "1522399",
    "end": "1528480"
  },
  {
    "text": "that's what gets passed between all these operators have a question uh in your",
    "start": "1528480",
    "end": "1536559"
  },
  {
    "text": "engineering realization uh do you have any special optimizations for the TPU",
    "start": "1536559",
    "end": "1542960"
  },
  {
    "text": "memory management in order to improve data loading or read right efficiency",
    "start": "1542960",
    "end": "1550120"
  },
  {
    "text": "something like this so this is for regular GP regular or LMS",
    "start": "1550120",
    "end": "1556679"
  },
  {
    "text": "um Regular and llms uh yes oh for llms we have spent a lot of time sort of um",
    "start": "1556679",
    "end": "1563399"
  },
  {
    "text": "optimizing sort of the KV cach um pattern and right now we're exploring some ways to sort of uh you know um have",
    "start": "1563399",
    "end": "1571799"
  },
  {
    "text": "better sort of uh coordination between the prefill and decoding steps so that there is a little bit better sort",
    "start": "1571799",
    "end": "1577960"
  },
  {
    "text": "utilization through put uh trading off LC okay uh another question is uh in",
    "start": "1577960",
    "end": "1586200"
  },
  {
    "text": "your daily work uh you have any best practice of how to do the profiling for",
    "start": "1586200",
    "end": "1592200"
  },
  {
    "text": "the online serving system because you know it's a synchronous system right uh",
    "start": "1592200",
    "end": "1597760"
  },
  {
    "text": "do we have some best practice also uh for online serving systems serving it's",
    "start": "1597760",
    "end": "1602960"
  },
  {
    "text": "a How Could You monitor the system performance and BR the bottom neck um so",
    "start": "1602960",
    "end": "1610200"
  },
  {
    "text": "I'm happy to refer you to the folks that work on the online serving systems but this is more of a batch system talk yeah",
    "start": "1610200",
    "end": "1618399"
  },
  {
    "text": "the LM bad is that be open sour or is that only proprietary So currently the plan is to",
    "start": "1618399",
    "end": "1625159"
  },
  {
    "text": "keep it as proprietary um but we will have sort of like examples in the open",
    "start": "1625159",
    "end": "1630840"
  },
  {
    "text": "source uh especially in the VM repository for how to do some B basic versions of batch imprints obviously",
    "start": "1630840",
    "end": "1637760"
  },
  {
    "text": "like the tuning and stuff like that will be much more optimized on any",
    "start": "1637760",
    "end": "1642880"
  },
  {
    "text": "scale yeah Derek I think you mentioned previously that checkpoint and restart would be",
    "start": "1642880",
    "end": "1648760"
  },
  {
    "text": "available eventually is it available in this preview release yeah so um checkpointing so actually SC I'm working",
    "start": "1648760",
    "end": "1656039"
  },
  {
    "text": "on this right now we we're basically like in the preview stage and we're trying to run it on like larger workloads uh and then soon we'll make it",
    "start": "1656039",
    "end": "1661799"
  },
  {
    "text": "available to like all customers yeah follow on this so is that Che",
    "start": "1661799",
    "end": "1668200"
  },
  {
    "text": "pointing and tolerance for the entire data that's going to be launched that's right yeah just for like Ray data in",
    "start": "1668200",
    "end": "1674240"
  },
  {
    "text": "general yeah I thought that's a feure yeah for Ray it works on R data on scale",
    "start": "1674240",
    "end": "1679679"
  },
  {
    "text": "yeah so like R turbo data yeah so you're not making no yeah",
    "start": "1679679",
    "end": "1686480"
  },
  {
    "text": "got it so the question I think was with uh Ray data when you're writing files can you specify like the output file",
    "start": "1708519",
    "end": "1713880"
  },
  {
    "text": "size",
    "start": "1713880",
    "end": "1716279"
  },
  {
    "text": "right got it got it yeah so for the first question uh we the right apis don't have a direct like uh like file",
    "start": "1735799",
    "end": "1743039"
  },
  {
    "text": "size Target but we we have a way to specify like the number of rows in the output file so that might be that's like",
    "start": "1743039",
    "end": "1749159"
  },
  {
    "text": "a kind of a proxy of of doing that um and then for the second question uh because Ray data uses streaming",
    "start": "1749159",
    "end": "1755080"
  },
  {
    "text": "execution it will essentially like not keep too many files like or too much",
    "start": "1755080",
    "end": "1760399"
  },
  {
    "text": "like data that is to be written out like in memory as soon as the results are available to be written out it'll basically do the writing out so you",
    "start": "1760399",
    "end": "1767159"
  },
  {
    "text": "won't run into s where you have like a bunch of intermediate blocks that are like waiting to be written and they're just like holding a",
    "start": "1767159",
    "end": "1773600"
  },
  {
    "text": "memory yeah any question yeah so you had a",
    "start": "1773600",
    "end": "1781720"
  },
  {
    "text": "slide on the future road map for",
    "start": "1781720",
    "end": "1788720"
  },
  {
    "text": "you so it'll be this will be a to Hot Topic in Q4 and given the s eration of",
    "start": "1789960",
    "end": "1798799"
  },
  {
    "start": "1790000",
    "end": "1894000"
  },
  {
    "text": "new models uh we plan on sort of having you know announcements by the end of the",
    "start": "1798799",
    "end": "1805200"
  },
  {
    "text": "year do you have a decision of whats that you so one of the big differences",
    "start": "1805200",
    "end": "1811200"
  },
  {
    "text": "for uh you know multimodal models is that the data size becomes a little bit",
    "start": "1811200",
    "end": "1817480"
  },
  {
    "text": "larger and you have to embed the data into the text so we'll be sort of tackling that challenge the second one",
    "start": "1817480",
    "end": "1825399"
  },
  {
    "text": "um is that um there are sort of optimizations that we can do in order to",
    "start": "1825399",
    "end": "1831559"
  },
  {
    "text": "take advantage of these architectures that are a little bit more split not so homogeneous and so we will be taking",
    "start": "1831559",
    "end": "1837919"
  },
  {
    "text": "advantage of that for that as well which models do you support so",
    "start": "1837919",
    "end": "1844840"
  },
  {
    "text": "right now um like we support I think you'll get the best",
    "start": "1844840",
    "end": "1850120"
  },
  {
    "text": "performance for large like large LM models uh on that are available on VM uh",
    "start": "1850120",
    "end": "1858440"
  },
  {
    "text": "we will probably be doing some extra tuning uh for again for multimodal models because we believe there's",
    "start": "1858440",
    "end": "1864399"
  },
  {
    "text": "opportunity there and also for like very very large models like 405b um those",
    "start": "1864399",
    "end": "1869480"
  },
  {
    "text": "things I think you will see a little bit less performance optimization um but but",
    "start": "1869480",
    "end": "1874519"
  },
  {
    "text": "that's you know at currently but that we believe that's some room for us to improve",
    "start": "1874519",
    "end": "1880919"
  },
  {
    "text": "on cool I think we'll we'll uh you know finish up here and then we will take",
    "start": "1884039",
    "end": "1889679"
  },
  {
    "text": "more questions on the side thank you everyone",
    "start": "1889679",
    "end": "1894399"
  }
]