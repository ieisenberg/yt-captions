[
  {
    "text": "hi folks my name is Malik kadis I'm the",
    "start": "659",
    "end": "3300"
  },
  {
    "text": "head of engineering here at any scale",
    "start": "3300",
    "end": "4680"
  },
  {
    "text": "and I'm really excited to share with you",
    "start": "4680",
    "end": "7020"
  },
  {
    "text": "uh news of an open source project that",
    "start": "7020",
    "end": "9300"
  },
  {
    "text": "we're releasing today it's called Aviary",
    "start": "9300",
    "end": "11400"
  },
  {
    "text": "and the idea is Avery is a place where",
    "start": "11400",
    "end": "13799"
  },
  {
    "text": "you can manage and run lots of different",
    "start": "13799",
    "end": "16379"
  },
  {
    "text": "open source llm models this comes from",
    "start": "16379",
    "end": "18960"
  },
  {
    "text": "our experiences we have huge fans of",
    "start": "18960",
    "end": "21300"
  },
  {
    "text": "Open Source large language models but",
    "start": "21300",
    "end": "23340"
  },
  {
    "text": "you know we were trying to like work out",
    "start": "23340",
    "end": "25380"
  },
  {
    "text": "what are the best models for particular",
    "start": "25380",
    "end": "27000"
  },
  {
    "text": "situations what are the different pros",
    "start": "27000",
    "end": "28859"
  },
  {
    "text": "and cons and as we started to develop",
    "start": "28859",
    "end": "30900"
  },
  {
    "text": "that tool we came up with this interface",
    "start": "30900",
    "end": "32880"
  },
  {
    "text": "that I'm just about to show you now but",
    "start": "32880",
    "end": "34620"
  },
  {
    "text": "it turned out that building the",
    "start": "34620",
    "end": "35880"
  },
  {
    "text": "interface was kind of the easy part what",
    "start": "35880",
    "end": "37920"
  },
  {
    "text": "was really the hard part was the back",
    "start": "37920",
    "end": "39480"
  },
  {
    "text": "end you know it's not like you can just",
    "start": "39480",
    "end": "41399"
  },
  {
    "text": "download a model of hagging face and go",
    "start": "41399",
    "end": "43379"
  },
  {
    "text": "bloop it's ready for production there's",
    "start": "43379",
    "end": "45000"
  },
  {
    "text": "much more to it than that are there deep",
    "start": "45000",
    "end": "46920"
  },
  {
    "text": "speed optimizations for these particular",
    "start": "46920",
    "end": "48660"
  },
  {
    "text": "models do they work in this case is",
    "start": "48660",
    "end": "50219"
  },
  {
    "text": "there a bug in deep speed that stops IT",
    "start": "50219",
    "end": "51719"
  },
  {
    "text": "from working what are the stop tokens",
    "start": "51719",
    "end": "53219"
  },
  {
    "text": "that this particular model uses what's",
    "start": "53219",
    "end": "55199"
  },
  {
    "text": "its model of system prompts and user",
    "start": "55199",
    "end": "56879"
  },
  {
    "text": "prompts do we need to give it an initial",
    "start": "56879",
    "end": "58320"
  },
  {
    "text": "prompt there's all this kind of",
    "start": "58320",
    "end": "59699"
  },
  {
    "text": "additional data so so what I'm going to",
    "start": "59699",
    "end": "61440"
  },
  {
    "text": "show you now is an interface that shows",
    "start": "61440",
    "end": "63120"
  },
  {
    "text": "you what you can build on top of Aviary",
    "start": "63120",
    "end": "65540"
  },
  {
    "text": "this is just a demo front end that we've",
    "start": "65540",
    "end": "68100"
  },
  {
    "text": "been using to experiment with internally",
    "start": "68100",
    "end": "70020"
  },
  {
    "text": "and what it allows you to do is we've",
    "start": "70020",
    "end": "72180"
  },
  {
    "text": "got a host of different open source",
    "start": "72180",
    "end": "73920"
  },
  {
    "text": "models here we have about 10 here right",
    "start": "73920",
    "end": "75600"
  },
  {
    "text": "now and we also have sometimes different",
    "start": "75600",
    "end": "78299"
  },
  {
    "text": "types so for example for the Mosaic ml",
    "start": "78299",
    "end": "80580"
  },
  {
    "text": "types we have the instruct fine-tuned",
    "start": "80580",
    "end": "82619"
  },
  {
    "text": "version and the chat fine-tuned version",
    "start": "82619",
    "end": "85200"
  },
  {
    "text": "and what we what this model allows us to",
    "start": "85200",
    "end": "87299"
  },
  {
    "text": "do is to very easily click on say a",
    "start": "87299",
    "end": "90060"
  },
  {
    "text": "prompt like how do I make fried rice and",
    "start": "90060",
    "end": "92040"
  },
  {
    "text": "see how these different llms respond to",
    "start": "92040",
    "end": "94080"
  },
  {
    "text": "that particular query now the really",
    "start": "94080",
    "end": "96060"
  },
  {
    "text": "interesting thing about Amazon light GPT",
    "start": "96060",
    "end": "98040"
  },
  {
    "text": "is today we're filming this on Thursday",
    "start": "98040",
    "end": "100380"
  },
  {
    "text": "May 25th Amazon light GPT was released",
    "start": "100380",
    "end": "103560"
  },
  {
    "text": "yesterday on Wednesday May 24th as",
    "start": "103560",
    "end": "105900"
  },
  {
    "text": "you'll see we were able to get this into",
    "start": "105900",
    "end": "108240"
  },
  {
    "text": "Aviary in the space of five minutes it",
    "start": "108240",
    "end": "110759"
  },
  {
    "text": "didn't take us very long light GPT has a",
    "start": "110759",
    "end": "113340"
  },
  {
    "text": "very standard architecture so it was",
    "start": "113340",
    "end": "116040"
  },
  {
    "text": "nothing more than a config file that we",
    "start": "116040",
    "end": "117600"
  },
  {
    "text": "had to put together so as we've seen",
    "start": "117600",
    "end": "119100"
  },
  {
    "text": "we've now hit this model against these",
    "start": "119100",
    "end": "121259"
  },
  {
    "text": "three different llms all of them open",
    "start": "121259",
    "end": "123360"
  },
  {
    "text": "source and then what we can do here and",
    "start": "123360",
    "end": "125280"
  },
  {
    "text": "what we've been using this to evaluate",
    "start": "125280",
    "end": "126600"
  },
  {
    "text": "is you can see that this tool gives you",
    "start": "126600",
    "end": "128099"
  },
  {
    "text": "the text but it also gives you",
    "start": "128099",
    "end": "130580"
  },
  {
    "text": "statistics like which one of these was",
    "start": "130580",
    "end": "132780"
  },
  {
    "text": "the fastest in this particular case",
    "start": "132780",
    "end": "134340"
  },
  {
    "text": "we've had really great experiences with",
    "start": "134340",
    "end": "136620"
  },
  {
    "text": "Amazon low GPT it's very fast but the",
    "start": "136620",
    "end": "138959"
  },
  {
    "text": "answers are pretty good for given how",
    "start": "138959",
    "end": "140879"
  },
  {
    "text": "small the model is right but maybe you",
    "start": "140879",
    "end": "143160"
  },
  {
    "text": "know what we want is you know a more",
    "start": "143160",
    "end": "144840"
  },
  {
    "text": "tradition final recipe style thing and",
    "start": "144840",
    "end": "147480"
  },
  {
    "text": "so in this particular case we might",
    "start": "147480",
    "end": "148980"
  },
  {
    "text": "actually give it to number three and so",
    "start": "148980",
    "end": "150599"
  },
  {
    "text": "what we're doing here is in the",
    "start": "150599",
    "end": "151860"
  },
  {
    "text": "background we're collecting statistics",
    "start": "151860",
    "end": "153599"
  },
  {
    "text": "every time someone runs this and what",
    "start": "153599",
    "end": "155580"
  },
  {
    "text": "we've been doing is like trying to",
    "start": "155580",
    "end": "157080"
  },
  {
    "text": "understand which models are the best and",
    "start": "157080",
    "end": "159420"
  },
  {
    "text": "so what we have is we're Computing",
    "start": "159420",
    "end": "160739"
  },
  {
    "text": "something like a win ratio anything over",
    "start": "160739",
    "end": "163019"
  },
  {
    "text": "1000 is better than random and what you",
    "start": "163019",
    "end": "165540"
  },
  {
    "text": "can see here is that our users",
    "start": "165540",
    "end": "167220"
  },
  {
    "text": "internally within any scale have been",
    "start": "167220",
    "end": "169260"
  },
  {
    "text": "voting and giving us a good signal",
    "start": "169260",
    "end": "171540"
  },
  {
    "text": "unlike what are the best models not",
    "start": "171540",
    "end": "173220"
  },
  {
    "text": "surprisingly here the open assistance",
    "start": "173220",
    "end": "175019"
  },
  {
    "text": "model is doing very well that's not",
    "start": "175019",
    "end": "176580"
  },
  {
    "text": "completely surprising because it has a",
    "start": "176580",
    "end": "178800"
  },
  {
    "text": "13 billion parameter model but maybe the",
    "start": "178800",
    "end": "180840"
  },
  {
    "text": "surprise in our particular case for",
    "start": "180840",
    "end": "182400"
  },
  {
    "text": "example is just how well the Mosaic ml",
    "start": "182400",
    "end": "184319"
  },
  {
    "text": "chat model performs so that's one set of",
    "start": "184319",
    "end": "186540"
  },
  {
    "text": "metrics that you care about with llms is",
    "start": "186540",
    "end": "189060"
  },
  {
    "text": "the quality how good are they and of",
    "start": "189060",
    "end": "190680"
  },
  {
    "text": "course you know in this particular case",
    "start": "190680",
    "end": "191819"
  },
  {
    "text": "we're comparing to open source models",
    "start": "191819",
    "end": "193379"
  },
  {
    "text": "but it would be very very easy to",
    "start": "193379",
    "end": "195300"
  },
  {
    "text": "compare this as well with commercial",
    "start": "195300",
    "end": "196800"
  },
  {
    "text": "models like open AIS the other side of",
    "start": "196800",
    "end": "199620"
  },
  {
    "text": "performance though is of course cost and",
    "start": "199620",
    "end": "201480"
  },
  {
    "text": "latency this is where you know we've",
    "start": "201480",
    "end": "203099"
  },
  {
    "text": "been compiling these statistics as well",
    "start": "203099",
    "end": "204659"
  },
  {
    "text": "and we see here is that you know Amazon",
    "start": "204659",
    "end": "207000"
  },
  {
    "text": "light GPT is just like Rocket Fast",
    "start": "207000",
    "end": "208980"
  },
  {
    "text": "answers in like 1.4 seconds it's really",
    "start": "208980",
    "end": "211379"
  },
  {
    "text": "a rocket ship and as a result its cost",
    "start": "211379",
    "end": "213300"
  },
  {
    "text": "per answer is very high now let's have a",
    "start": "213300",
    "end": "215400"
  },
  {
    "text": "look at the most expensive models the",
    "start": "215400",
    "end": "217140"
  },
  {
    "text": "most expensive model is the Mosaic story",
    "start": "217140",
    "end": "219120"
  },
  {
    "text": "writer model and that's not because the",
    "start": "219120",
    "end": "221040"
  },
  {
    "text": "story writer is more expensive per token",
    "start": "221040",
    "end": "223379"
  },
  {
    "text": "it's actually pretty reasonable it",
    "start": "223379",
    "end": "224580"
  },
  {
    "text": "produces 30 tokens a second the issue is",
    "start": "224580",
    "end": "226620"
  },
  {
    "text": "it produces really large articles and",
    "start": "226620",
    "end": "228599"
  },
  {
    "text": "that's because that's what it's designed",
    "start": "228599",
    "end": "229799"
  },
  {
    "text": "for we can try it later it's one of the",
    "start": "229799",
    "end": "231420"
  },
  {
    "text": "slower models on average it takes 20",
    "start": "231420",
    "end": "233099"
  },
  {
    "text": "seconds to produce an output what it",
    "start": "233099",
    "end": "234780"
  },
  {
    "text": "produces is like very very readable",
    "start": "234780",
    "end": "236760"
  },
  {
    "text": "story and of course it is a bit more",
    "start": "236760",
    "end": "238500"
  },
  {
    "text": "expensive at one cent per article these",
    "start": "238500",
    "end": "240480"
  },
  {
    "text": "are based on statistics we look at how",
    "start": "240480",
    "end": "242159"
  },
  {
    "text": "much it costs the machine and so on we",
    "start": "242159",
    "end": "243659"
  },
  {
    "text": "built this and we thought hey this is a",
    "start": "243659",
    "end": "245099"
  },
  {
    "text": "really cool tool but as we started to",
    "start": "245099",
    "end": "247260"
  },
  {
    "text": "build it we started to realize that the",
    "start": "247260",
    "end": "249239"
  },
  {
    "text": "interesting thing was not the front end",
    "start": "249239",
    "end": "250620"
  },
  {
    "text": "the front end is interesting it gives us",
    "start": "250620",
    "end": "252720"
  },
  {
    "text": "all of these insights but really the",
    "start": "252720",
    "end": "255060"
  },
  {
    "text": "back end was complicated because again",
    "start": "255060",
    "end": "256859"
  },
  {
    "text": "we found it was very hard to actually",
    "start": "256859",
    "end": "259199"
  },
  {
    "text": "get these llms in production in a stable",
    "start": "259199",
    "end": "262500"
  },
  {
    "text": "way not producing Chinese or any any",
    "start": "262500",
    "end": "265440"
  },
  {
    "text": "other language there's a lot of fine",
    "start": "265440",
    "end": "267000"
  },
  {
    "text": "tuning that we had to do this what we've",
    "start": "267000",
    "end": "268740"
  },
  {
    "text": "done is we've focused all our energies",
    "start": "268740",
    "end": "270419"
  },
  {
    "text": "on making the back end really good and",
    "start": "270419",
    "end": "273419"
  },
  {
    "text": "making very easy for you to add new",
    "start": "273419",
    "end": "275220"
  },
  {
    "text": "models as well as to build scalable",
    "start": "275220",
    "end": "277320"
  },
  {
    "text": "systems on top of it so what backs this",
    "start": "277320",
    "end": "279660"
  },
  {
    "text": "is array service you can see here that",
    "start": "279660",
    "end": "282120"
  },
  {
    "text": "you know we have something like 11",
    "start": "282120",
    "end": "283620"
  },
  {
    "text": "models running right now it's using",
    "start": "283620",
    "end": "285900"
  },
  {
    "text": "something like five or six different",
    "start": "285900",
    "end": "287880"
  },
  {
    "text": "machines you're seeing that the models",
    "start": "287880",
    "end": "290040"
  },
  {
    "text": "are already pre-loaded into the memory",
    "start": "290040",
    "end": "292320"
  },
  {
    "text": "of the different notes and that we're",
    "start": "292320",
    "end": "294479"
  },
  {
    "text": "using machines with four gpus each right",
    "start": "294479",
    "end": "296400"
  },
  {
    "text": "now obviously we're not sending any",
    "start": "296400",
    "end": "297780"
  },
  {
    "text": "queries simply as the traffic increases",
    "start": "297780",
    "end": "299580"
  },
  {
    "text": "that's when you can do some some more",
    "start": "299580",
    "end": "302400"
  },
  {
    "text": "interesting things and when this system",
    "start": "302400",
    "end": "303960"
  },
  {
    "text": "is really at its best let me now just",
    "start": "303960",
    "end": "305880"
  },
  {
    "text": "take a few moments to walk you through",
    "start": "305880",
    "end": "307500"
  },
  {
    "text": "what the code looks like for Avery",
    "start": "307500",
    "end": "309900"
  },
  {
    "text": "because we're really excited we really",
    "start": "309900",
    "end": "311400"
  },
  {
    "text": "hope that you folks will be able to",
    "start": "311400",
    "end": "312660"
  },
  {
    "text": "contribute code to it too our hope is",
    "start": "312660",
    "end": "314880"
  },
  {
    "text": "that Avia will be the repository that",
    "start": "314880",
    "end": "317400"
  },
  {
    "text": "makes llms from all over the place",
    "start": "317400",
    "end": "319860"
  },
  {
    "text": "production ready with the appropriate",
    "start": "319860",
    "end": "321419"
  },
  {
    "text": "settings but we make it very easy for",
    "start": "321419",
    "end": "323100"
  },
  {
    "text": "you to contribute",
    "start": "323100",
    "end": "324360"
  },
  {
    "text": "so what I'm going to go now to is an",
    "start": "324360",
    "end": "326520"
  },
  {
    "text": "instance where we have opened Visual",
    "start": "326520",
    "end": "328919"
  },
  {
    "text": "Studio code and what I'm going to spend",
    "start": "328919",
    "end": "330600"
  },
  {
    "text": "the most time on is not the code itself",
    "start": "330600",
    "end": "332220"
  },
  {
    "text": "but just how easy we've tried to make it",
    "start": "332220",
    "end": "334680"
  },
  {
    "text": "to add different models and so for",
    "start": "334680",
    "end": "337020"
  },
  {
    "text": "example I mentioned that the Amazon",
    "start": "337020",
    "end": "338880"
  },
  {
    "text": "light GPT ammo is here it's actually",
    "start": "338880",
    "end": "341340"
  },
  {
    "text": "derived from uh you know one of these",
    "start": "341340",
    "end": "343800"
  },
  {
    "text": "other models and you can see that this",
    "start": "343800",
    "end": "345419"
  },
  {
    "text": "config file is not very very different",
    "start": "345419",
    "end": "346979"
  },
  {
    "text": "to the other ones and so most of the",
    "start": "346979",
    "end": "348900"
  },
  {
    "text": "time you can just copy one yaml file to",
    "start": "348900",
    "end": "350460"
  },
  {
    "text": "another but you know we give the",
    "start": "350460",
    "end": "352020"
  },
  {
    "text": "specifications in terms of the questions",
    "start": "352020",
    "end": "354120"
  },
  {
    "text": "of how do we want this thing to be",
    "start": "354120",
    "end": "356100"
  },
  {
    "text": "deployed what's its maximum scale how do",
    "start": "356100",
    "end": "358680"
  },
  {
    "text": "we upscale and downscales what types of",
    "start": "358680",
    "end": "360960"
  },
  {
    "text": "gpus it's optimized for whether we can",
    "start": "360960",
    "end": "363120"
  },
  {
    "text": "use optimized algorithms like deep speed",
    "start": "363120",
    "end": "365820"
  },
  {
    "text": "not all llms can be optimized with deep",
    "start": "365820",
    "end": "369000"
  },
  {
    "text": "speed so rwkv for example one of these",
    "start": "369000",
    "end": "372120"
  },
  {
    "text": "models which is actually a current",
    "start": "372120",
    "end": "373800"
  },
  {
    "text": "neural network model not a Transformer",
    "start": "373800",
    "end": "375479"
  },
  {
    "text": "model can't be done in this way for",
    "start": "375479",
    "end": "376919"
  },
  {
    "text": "example but we've also configured like",
    "start": "376919",
    "end": "378840"
  },
  {
    "text": "what we think are reasonable parameters",
    "start": "378840",
    "end": "380580"
  },
  {
    "text": "but of course you can modify these",
    "start": "380580",
    "end": "382080"
  },
  {
    "text": "parameters all that you do now is just",
    "start": "382080",
    "end": "383819"
  },
  {
    "text": "run it so over here I have a workspace",
    "start": "383819",
    "end": "387240"
  },
  {
    "text": "workspaces are what we use in any scale",
    "start": "387240",
    "end": "390000"
  },
  {
    "text": "to kind of do our development but you",
    "start": "390000",
    "end": "392460"
  },
  {
    "text": "can just imagine this this is your",
    "start": "392460",
    "end": "394080"
  },
  {
    "text": "favorite Ray cluster whatever way you",
    "start": "394080",
    "end": "396419"
  },
  {
    "text": "like to launch or a cluster whether it's",
    "start": "396419",
    "end": "397800"
  },
  {
    "text": "any scale or open source all you have to",
    "start": "397800",
    "end": "399660"
  },
  {
    "text": "do to launch these new models I won't do",
    "start": "399660",
    "end": "401340"
  },
  {
    "text": "it now because it takes like five or six",
    "start": "401340",
    "end": "403020"
  },
  {
    "text": "minutes and I don't want to bore you but",
    "start": "403020",
    "end": "404580"
  },
  {
    "text": "literally all you do is you specify",
    "start": "404580",
    "end": "406319"
  },
  {
    "text": "which one of those yaml files that we",
    "start": "406319",
    "end": "408539"
  },
  {
    "text": "put together you want to load and it",
    "start": "408539",
    "end": "410880"
  },
  {
    "text": "loads those models up so now let's start",
    "start": "410880",
    "end": "412800"
  },
  {
    "text": "to I'm going to show you now the the CLI",
    "start": "412800",
    "end": "415319"
  },
  {
    "text": "that we have available just to show you",
    "start": "415319",
    "end": "417419"
  },
  {
    "text": "a power of something like this so you",
    "start": "417419",
    "end": "419639"
  },
  {
    "text": "saw that one option is that we have a",
    "start": "419639",
    "end": "422400"
  },
  {
    "text": "company-wide service like this one where",
    "start": "422400",
    "end": "425639"
  },
  {
    "text": "you might have you know you want to",
    "start": "425639",
    "end": "427560"
  },
  {
    "text": "share different models between everyone",
    "start": "427560",
    "end": "429539"
  },
  {
    "text": "in an organization and that's one way to",
    "start": "429539",
    "end": "431880"
  },
  {
    "text": "do it but sometimes you just want to run",
    "start": "431880",
    "end": "433560"
  },
  {
    "text": "things in your own local workspace or",
    "start": "433560",
    "end": "436319"
  },
  {
    "text": "Ray cluster for efficiency and just",
    "start": "436319",
    "end": "438840"
  },
  {
    "text": "because you want to experiment so let's",
    "start": "438840",
    "end": "440699"
  },
  {
    "text": "start by seeing what models this llm",
    "start": "440699",
    "end": "443699"
  },
  {
    "text": "supports okay so it's got three",
    "start": "443699",
    "end": "445680"
  },
  {
    "text": "pre-loaded models in it the one that we",
    "start": "445680",
    "end": "447720"
  },
  {
    "text": "were looking at earlier had quite a few",
    "start": "447720",
    "end": "449340"
  },
  {
    "text": "more but maybe for these experiments we",
    "start": "449340",
    "end": "451139"
  },
  {
    "text": "only need these three what we can do now",
    "start": "451139",
    "end": "452699"
  },
  {
    "text": "is of course we can query these models",
    "start": "452699",
    "end": "454979"
  },
  {
    "text": "we can query one at a time oops what did",
    "start": "454979",
    "end": "457500"
  },
  {
    "text": "I forget oh I forgot to say query",
    "start": "457500",
    "end": "460740"
  },
  {
    "text": "there we go and so now it's hitting",
    "start": "460740",
    "end": "463319"
  },
  {
    "text": "Amazon light GPT and it's telling us the",
    "start": "463319",
    "end": "465720"
  },
  {
    "text": "output from that",
    "start": "465720",
    "end": "467160"
  },
  {
    "text": "um but of course we can also print out",
    "start": "467160",
    "end": "468780"
  },
  {
    "text": "the statistics about like how much time",
    "start": "468780",
    "end": "471060"
  },
  {
    "text": "it used how many different tokens it was",
    "start": "471060",
    "end": "473039"
  },
  {
    "text": "so that after this we can compute like",
    "start": "473039",
    "end": "475380"
  },
  {
    "text": "different statistics find out which",
    "start": "475380",
    "end": "477300"
  },
  {
    "text": "models we like find out you know the",
    "start": "477300",
    "end": "479220"
  },
  {
    "text": "different latencies of the different",
    "start": "479220",
    "end": "480360"
  },
  {
    "text": "models",
    "start": "480360",
    "end": "481800"
  },
  {
    "text": "um and of course we can also do things",
    "start": "481800",
    "end": "484319"
  },
  {
    "text": "like add more models and see how it",
    "start": "484319",
    "end": "486360"
  },
  {
    "text": "responds so we're going to add the other",
    "start": "486360",
    "end": "488639"
  },
  {
    "text": "two models",
    "start": "488639",
    "end": "490560"
  },
  {
    "text": "and",
    "start": "490560",
    "end": "492360"
  },
  {
    "text": "the instruct one and see how they",
    "start": "492360",
    "end": "494819"
  },
  {
    "text": "perform",
    "start": "494819",
    "end": "495780"
  },
  {
    "text": "and you can see here that again you know",
    "start": "495780",
    "end": "497819"
  },
  {
    "text": "it's finished taking those three queries",
    "start": "497819",
    "end": "499919"
  },
  {
    "text": "sending them to all those three",
    "start": "499919",
    "end": "502080"
  },
  {
    "text": "different agents and seeing what the",
    "start": "502080",
    "end": "504300"
  },
  {
    "text": "results of those queries are",
    "start": "504300",
    "end": "506099"
  },
  {
    "text": "which is kind of awesome because now we",
    "start": "506099",
    "end": "507840"
  },
  {
    "text": "can start to think about comparison and",
    "start": "507840",
    "end": "509819"
  },
  {
    "text": "what we've also done is we've created",
    "start": "509819",
    "end": "511379"
  },
  {
    "text": "another system called multi-query so",
    "start": "511379",
    "end": "514500"
  },
  {
    "text": "let's imagine before I go there let me",
    "start": "514500",
    "end": "516659"
  },
  {
    "text": "just show you",
    "start": "516659",
    "end": "517860"
  },
  {
    "text": "I've created a file here called QA",
    "start": "517860",
    "end": "520800"
  },
  {
    "text": "prompts which has just four prompts that",
    "start": "520800",
    "end": "523020"
  },
  {
    "text": "we're going to use for testing",
    "start": "523020",
    "end": "524940"
  },
  {
    "text": "um and um what what we can do is just go",
    "start": "524940",
    "end": "529440"
  },
  {
    "text": "give those four prompts instead now",
    "start": "529440",
    "end": "531839"
  },
  {
    "text": "instead of that we don't need this",
    "start": "531839",
    "end": "533220"
  },
  {
    "text": "anymore we're going to specify a file",
    "start": "533220",
    "end": "535380"
  },
  {
    "text": "which is the file that I just showed you",
    "start": "535380",
    "end": "536880"
  },
  {
    "text": "and and uh you can see that it's now",
    "start": "536880",
    "end": "539100"
  },
  {
    "text": "starting to um send these queries",
    "start": "539100",
    "end": "541560"
  },
  {
    "text": "through and contact all of the different",
    "start": "541560",
    "end": "544019"
  },
  {
    "text": "uh agents right now we're in the middle",
    "start": "544019",
    "end": "546660"
  },
  {
    "text": "of implementing batching which we think",
    "start": "546660",
    "end": "548519"
  },
  {
    "text": "will give us huge speed UPS so hopefully",
    "start": "548519",
    "end": "551100"
  },
  {
    "text": "by the time we release this this will be",
    "start": "551100",
    "end": "552779"
  },
  {
    "text": "even faster it's really important to",
    "start": "552779",
    "end": "554640"
  },
  {
    "text": "understand that this thing is backed by",
    "start": "554640",
    "end": "556560"
  },
  {
    "text": "racer racer has been used extensively",
    "start": "556560",
    "end": "560720"
  },
  {
    "text": "for serving models of all kinds but is",
    "start": "560720",
    "end": "564300"
  },
  {
    "text": "especially well suited to serving these",
    "start": "564300",
    "end": "566220"
  },
  {
    "text": "other lamps because of how large they",
    "start": "566220",
    "end": "567839"
  },
  {
    "text": "are and the careful things you need to",
    "start": "567839",
    "end": "570300"
  },
  {
    "text": "do around batching and coding so we've",
    "start": "570300",
    "end": "573000"
  },
  {
    "text": "really enjoyed being able to build this",
    "start": "573000",
    "end": "574620"
  },
  {
    "text": "on top of array surf as you can see it's",
    "start": "574620",
    "end": "576720"
  },
  {
    "text": "almost finished",
    "start": "576720",
    "end": "578100"
  },
  {
    "text": "and it's just processing that last",
    "start": "578100",
    "end": "580080"
  },
  {
    "text": "question and in fact it's generated a",
    "start": "580080",
    "end": "582420"
  },
  {
    "text": "file with like the ovary output and this",
    "start": "582420",
    "end": "585180"
  },
  {
    "text": "is basically just a file that has the",
    "start": "585180",
    "end": "586920"
  },
  {
    "text": "output from each of the different llms",
    "start": "586920",
    "end": "590640"
  },
  {
    "text": "and what we can do now that we have this",
    "start": "590640",
    "end": "593940"
  },
  {
    "text": "is we can start to do an evaluation of",
    "start": "593940",
    "end": "595980"
  },
  {
    "text": "it so have a look at a command that",
    "start": "595980",
    "end": "598140"
  },
  {
    "text": "we've included called Avery evaluate",
    "start": "598140",
    "end": "601019"
  },
  {
    "text": "and you can see that it can take the",
    "start": "601019",
    "end": "602760"
  },
  {
    "text": "input",
    "start": "602760",
    "end": "603600"
  },
  {
    "text": "um and where to save the output and",
    "start": "603600",
    "end": "605640"
  },
  {
    "text": "everything else in this particular case",
    "start": "605640",
    "end": "607800"
  },
  {
    "text": "what we've done is we've created the",
    "start": "607800",
    "end": "610380"
  },
  {
    "text": "default one is an evaluator that uses a",
    "start": "610380",
    "end": "612600"
  },
  {
    "text": "carefully crafted prompt that we're",
    "start": "612600",
    "end": "614519"
  },
  {
    "text": "still fine tuning because there are some",
    "start": "614519",
    "end": "615720"
  },
  {
    "text": "odd bits in it that actually runs uh the",
    "start": "615720",
    "end": "619200"
  },
  {
    "text": "evaluation by asking gpt4 to rank which",
    "start": "619200",
    "end": "621660"
  },
  {
    "text": "of the answers are best",
    "start": "621660",
    "end": "623279"
  },
  {
    "text": "so as you'll see there's a little bit of",
    "start": "623279",
    "end": "625260"
  },
  {
    "text": "bias in the way that",
    "start": "625260",
    "end": "626880"
  },
  {
    "text": "um that Avery that gpt4 does this and",
    "start": "626880",
    "end": "630120"
  },
  {
    "text": "we're working to improve the prompt but",
    "start": "630120",
    "end": "632160"
  },
  {
    "text": "we hope to make this tool available to",
    "start": "632160",
    "end": "634500"
  },
  {
    "text": "everyone to experiment with so while",
    "start": "634500",
    "end": "636240"
  },
  {
    "text": "this is finishing",
    "start": "636240",
    "end": "637440"
  },
  {
    "text": "again I want to emphasize this is built",
    "start": "637440",
    "end": "639959"
  },
  {
    "text": "on top of Ray which comes with all of",
    "start": "639959",
    "end": "642899"
  },
  {
    "text": "those superpowers around being able to",
    "start": "642899",
    "end": "645360"
  },
  {
    "text": "deal with high loads automatically scale",
    "start": "645360",
    "end": "648060"
  },
  {
    "text": "you saw it as managing like 16 gpus",
    "start": "648060",
    "end": "650640"
  },
  {
    "text": "before like it was nothing it really",
    "start": "650640",
    "end": "652320"
  },
  {
    "text": "does have a lot of capabilities and so",
    "start": "652320",
    "end": "654120"
  },
  {
    "text": "now it's just finalizing the result over",
    "start": "654120",
    "end": "655920"
  },
  {
    "text": "here and we'll soon see the result it's",
    "start": "655920",
    "end": "657660"
  },
  {
    "text": "left as an exercise for the user to",
    "start": "657660",
    "end": "659279"
  },
  {
    "text": "detect what the bias is but as you can",
    "start": "659279",
    "end": "661500"
  },
  {
    "text": "see it's ranked each of the different",
    "start": "661500",
    "end": "663839"
  },
  {
    "text": "models said which is the best answer and",
    "start": "663839",
    "end": "666420"
  },
  {
    "text": "the idea here now is that you can see",
    "start": "666420",
    "end": "667920"
  },
  {
    "text": "pretty well that you know if if gpt4 is",
    "start": "667920",
    "end": "670320"
  },
  {
    "text": "to be believed Amazon light250 is the",
    "start": "670320",
    "end": "672959"
  },
  {
    "text": "best answer so obviously we're still",
    "start": "672959",
    "end": "674579"
  },
  {
    "text": "working on this but we're really excited",
    "start": "674579",
    "end": "676680"
  },
  {
    "text": "about the possibilities of what we and",
    "start": "676680",
    "end": "679140"
  },
  {
    "text": "the community will be able to do",
    "start": "679140",
    "end": "680459"
  },
  {
    "text": "together you know this comes from our",
    "start": "680459",
    "end": "683040"
  },
  {
    "text": "own experiences and building this and we",
    "start": "683040",
    "end": "685740"
  },
  {
    "text": "thought well if this was something that",
    "start": "685740",
    "end": "687240"
  },
  {
    "text": "was useful to us maybe it would be",
    "start": "687240",
    "end": "688700"
  },
  {
    "text": "useful to the rest of the llm community",
    "start": "688700",
    "end": "691019"
  },
  {
    "text": "so we're open sourcing everything that",
    "start": "691019",
    "end": "693240"
  },
  {
    "text": "you've seen and we're really excited we",
    "start": "693240",
    "end": "694980"
  },
  {
    "text": "hope that people will do things like add",
    "start": "694980",
    "end": "697440"
  },
  {
    "text": "new evaluation algorithms to",
    "start": "697440",
    "end": "699779"
  },
  {
    "text": "automatically evaluate and also add",
    "start": "699779",
    "end": "702000"
  },
  {
    "text": "support for many new llms so as soon as",
    "start": "702000",
    "end": "704640"
  },
  {
    "text": "an llm comes out we hope to within hours",
    "start": "704640",
    "end": "707880"
  },
  {
    "text": "have that available through Aviary and",
    "start": "707880",
    "end": "710579"
  },
  {
    "text": "have it through the the front end that",
    "start": "710579",
    "end": "713279"
  },
  {
    "text": "we demoed to you today then you can can",
    "start": "713279",
    "end": "715440"
  },
  {
    "text": "interact with it so what our plan is",
    "start": "715440",
    "end": "717839"
  },
  {
    "text": "when every every time someone adds a",
    "start": "717839",
    "end": "719760"
  },
  {
    "text": "model to the open source repo we will",
    "start": "719760",
    "end": "722100"
  },
  {
    "text": "basically deploy and very soon after",
    "start": "722100",
    "end": "724079"
  },
  {
    "text": "that that model will appear in this list",
    "start": "724079",
    "end": "726360"
  },
  {
    "text": "so we're really excited about what we",
    "start": "726360",
    "end": "728820"
  },
  {
    "text": "and you were built together in terms of",
    "start": "728820",
    "end": "730800"
  },
  {
    "text": "making it easier to evaluate understand",
    "start": "730800",
    "end": "733380"
  },
  {
    "text": "the properties of these open source llms",
    "start": "733380",
    "end": "736079"
  },
  {
    "text": "and we're just really excited about the",
    "start": "736079",
    "end": "737640"
  },
  {
    "text": "future but we also understand that you",
    "start": "737640",
    "end": "739860"
  },
  {
    "text": "know setting up a rate cluster can be",
    "start": "739860",
    "end": "741360"
  },
  {
    "text": "tricky we have a managed hosted service",
    "start": "741360",
    "end": "743940"
  },
  {
    "text": "called any scale but that makes it",
    "start": "743940",
    "end": "746339"
  },
  {
    "text": "absolutely trivial for you to bring up",
    "start": "746339",
    "end": "748079"
  },
  {
    "text": "your own right cluster and in on top of",
    "start": "748079",
    "end": "750600"
  },
  {
    "text": "the existing things that this system can",
    "start": "750600",
    "end": "752459"
  },
  {
    "text": "do on Ray by itself Avery adds a few",
    "start": "752459",
    "end": "755880"
  },
  {
    "text": "additional benefits one of those is the",
    "start": "755880",
    "end": "758399"
  },
  {
    "text": "ability to use spot instances with",
    "start": "758399",
    "end": "760980"
  },
  {
    "text": "fallback to On Demand which can save you",
    "start": "760980",
    "end": "763139"
  },
  {
    "text": "50 to 75 percent on your serving costs",
    "start": "763139",
    "end": "766079"
  },
  {
    "text": "another one is we have zero downtime",
    "start": "766079",
    "end": "768180"
  },
  {
    "text": "upgrades so you can switch from an old",
    "start": "768180",
    "end": "770700"
  },
  {
    "text": "version to a new version very very",
    "start": "770700",
    "end": "772260"
  },
  {
    "text": "quickly with no loss of traffic and we",
    "start": "772260",
    "end": "774180"
  },
  {
    "text": "also have some very nice features like",
    "start": "774180",
    "end": "776160"
  },
  {
    "text": "scale to zero so if a model isn't used",
    "start": "776160",
    "end": "778019"
  },
  {
    "text": "for a few hours it gets flushed out all",
    "start": "778019",
    "end": "780660"
  },
  {
    "text": "of that of course is configurable and so",
    "start": "780660",
    "end": "782519"
  },
  {
    "text": "we're looking for people who are",
    "start": "782519",
    "end": "783720"
  },
  {
    "text": "interested in becoming testers for us as",
    "start": "783720",
    "end": "786180"
  },
  {
    "text": "we prepare Aviary running on any scale",
    "start": "786180",
    "end": "788579"
  },
  {
    "text": "perhaps you're a business that wants to",
    "start": "788579",
    "end": "790139"
  },
  {
    "text": "kind of manage that Central repository",
    "start": "790139",
    "end": "791760"
  },
  {
    "text": "like we showed earlier or maybe you're",
    "start": "791760",
    "end": "793860"
  },
  {
    "text": "just a scientist that's just trying to",
    "start": "793860",
    "end": "795899"
  },
  {
    "text": "evaluate the different characteristics",
    "start": "795899",
    "end": "797880"
  },
  {
    "text": "of different llms but we're really",
    "start": "797880",
    "end": "800220"
  },
  {
    "text": "really excited just to kind of release",
    "start": "800220",
    "end": "801779"
  },
  {
    "text": "this see how you folks like it or not",
    "start": "801779",
    "end": "804480"
  },
  {
    "text": "and we are just really curious what your",
    "start": "804480",
    "end": "807779"
  },
  {
    "text": "reactions to this project would be thank",
    "start": "807779",
    "end": "809519"
  },
  {
    "text": "you very much for your time and we can't",
    "start": "809519",
    "end": "810899"
  },
  {
    "text": "wait to see what you think",
    "start": "810899",
    "end": "813860"
  }
]