[
  {
    "start": "0",
    "end": "314000"
  },
  {
    "text": "hello I'm Eric here to tell you about Ray data streaming um so this talk will be a bit more of a",
    "start": "3780",
    "end": "9420"
  },
  {
    "text": "deep dive into this streaming feature of Ray data some of you might have seen Stephanie and Scott's talk yesterday about how to kind of uh streaming in its",
    "start": "9420",
    "end": "16800"
  },
  {
    "text": "performance for training um here I'm going to talk both about ml inference and training um the streaming feature and kind of",
    "start": "16800",
    "end": "23220"
  },
  {
    "text": "dive into kind of all the code examples of how it works how to debug this thing and I'm also going to talk a bit about",
    "start": "23220",
    "end": "28500"
  },
  {
    "text": "the back end how this works in ratecore so about me I'm the tech lead for array",
    "start": "28500",
    "end": "34680"
  },
  {
    "text": "at any skill my focus these days is a lot on raid data and also uh Ray core",
    "start": "34680",
    "end": "41100"
  },
  {
    "text": "so a bit about ML workloads and data so clearly data is kind of ubiquitous",
    "start": "41100",
    "end": "46800"
  },
  {
    "text": "across ml workloads but the way I like to think about it is for like the ml life cycle there are kind of three steps",
    "start": "46800",
    "end": "52020"
  },
  {
    "text": "there's the ETL phase and then pre-processing and training and this is how you get your data to your ML",
    "start": "52020",
    "end": "58140"
  },
  {
    "text": "workflows and concretely the ETL step involves things like ingesting the data",
    "start": "58140",
    "end": "63480"
  },
  {
    "text": "your company joining data tables and so on pre-processing involves preparing your data for training which it might",
    "start": "63480",
    "end": "69659"
  },
  {
    "text": "involve resizing images video decoding data augmentation things like that and",
    "start": "69659",
    "end": "74760"
  },
  {
    "text": "then finally you have to do your actual training which might involve leveraging your favorite ml framework like pytorch",
    "start": "74760",
    "end": "79820"
  },
  {
    "text": "LMS and and so on right kind of in very general terms the first",
    "start": "79820",
    "end": "86400"
  },
  {
    "text": "two steps are typically run on a lot of CPUs they might be CPU intensive and the latter step is often running on gpus",
    "start": "86400",
    "end": "94860"
  },
  {
    "text": "and when you think about the MLT or ml organization the latter two steps are what we care about most of the times in",
    "start": "94860",
    "end": "102180"
  },
  {
    "text": "the first step might be it might be a different organ like a data team or something like that",
    "start": "102180",
    "end": "107640"
  },
  {
    "text": "so we're going to focus on uh kind of challenges area and how to solve them and kind of in a nutshell like from kind",
    "start": "107640",
    "end": "113100"
  },
  {
    "text": "of the right perspective the challenges we see here are infra silos and fragmentation so there are many",
    "start": "113100",
    "end": "119700"
  },
  {
    "text": "different systems being used for this different clusters for pre-processing training different systems for inference",
    "start": "119700",
    "end": "125280"
  },
  {
    "text": "versus training and this leads to a lot of overheads so their performance overheads moving data between systems",
    "start": "125280",
    "end": "132060"
  },
  {
    "text": "between different clusters and operational overheads like the development life cycle can be much more",
    "start": "132060",
    "end": "137879"
  },
  {
    "text": "you know Annoying you have to work with a lot of different different Technologies so we're kind of vision for simplifying",
    "start": "137879",
    "end": "144540"
  },
  {
    "text": "this with Ray is to kind of simplify those latter two steps so for ETL this",
    "start": "144540",
    "end": "150239"
  },
  {
    "text": "is not something we're really focusing on with Ray because there's already a mature ecosystem of solutions out there",
    "start": "150239",
    "end": "155520"
  },
  {
    "text": "you know data warehouses spark so on but we kind of see pain points uh with with",
    "start": "155520",
    "end": "160920"
  },
  {
    "text": "Ray we're trying to simplify where pre-processing and training can be just done very simply in one python script uh",
    "start": "160920",
    "end": "166620"
  },
  {
    "text": "no kind of no matter the the complexity of your workload and uh",
    "start": "166620",
    "end": "172980"
  },
  {
    "text": "early users of Ray data are kind of echoing um kind of this Vision like they're seeing like improvements and iteration",
    "start": "172980",
    "end": "179519"
  },
  {
    "text": "Speed and Performance using great data with fortrain so what is Ray data uh Ray data is one",
    "start": "179519",
    "end": "187440"
  },
  {
    "text": "of several rate libraries alongside train uh tune sort of an R lib and all",
    "start": "187440",
    "end": "192900"
  },
  {
    "text": "these libraries are kind of built on Ray core and Ray core provides you know task actors and objects that do all the",
    "start": "192900",
    "end": "198480"
  },
  {
    "text": "distributed heavy lifting for these libraries rated is a relatively new library in the",
    "start": "198480",
    "end": "204540"
  },
  {
    "text": "ray ecosystem so when Ray was first created there was just the low level apis",
    "start": "204540",
    "end": "210019"
  },
  {
    "text": "we started with a couple initial libraries for reinforcement learning hyper parameter tuning and it's only got",
    "start": "210019",
    "end": "215879"
  },
  {
    "text": "relatively recently we try to complete the entire ml life cycle with the data data train libraries and so on",
    "start": "215879",
    "end": "224180"
  },
  {
    "text": "kind of if you look at the tech wise rated it's unsurprisingly distributed data set so how that means it has to you",
    "start": "225180",
    "end": "231180"
  },
  {
    "text": "know distribute its data across the Clusters so you can kind of think about array data data set as a number of",
    "start": "231180",
    "end": "236940"
  },
  {
    "text": "blocks spread across the cluster and every block is represented as array object so in this way Ray data can scale",
    "start": "236940",
    "end": "242940"
  },
  {
    "text": "to ingesting or processing data across a system",
    "start": "242940",
    "end": "248280"
  },
  {
    "text": "and Ray data is using Apache arrow is a standard under the hood so that's to interoperate with many other systems for",
    "start": "248280",
    "end": "254819"
  },
  {
    "text": "example it can very easily ingest data from S3 um you know a spark and so on using kind",
    "start": "254819",
    "end": "260160"
  },
  {
    "text": "of Arrow arrows i o apis and out goes and can also efficiently uh send data to other systems like tensorflow pytorch",
    "start": "260160",
    "end": "266880"
  },
  {
    "text": "and so on uh radiate the radiated apis that you",
    "start": "266880",
    "end": "272340"
  },
  {
    "text": "kind of need to know for this talk are those for i o for transformation and consuming data so this is probably not",
    "start": "272340",
    "end": "279660"
  },
  {
    "text": "unfamiliar if you're used to working with data set apis we have apis like read parquet read CSV you can transform",
    "start": "279660",
    "end": "286380"
  },
  {
    "text": "your data with a UDF that you put in and you can also consume data so one thing I do want to point out is",
    "start": "286380",
    "end": "292560"
  },
  {
    "text": "Ray data is a little bit different from other data set apis in that it's iteration API is designed for high",
    "start": "292560",
    "end": "297720"
  },
  {
    "text": "performance so you can iterate over batches and raid data add you know multiple gigabytes a second",
    "start": "297720",
    "end": "303360"
  },
  {
    "text": "which is not a typical design point and the reason for this is Ray data is designed to be used for distributor",
    "start": "303360",
    "end": "308820"
  },
  {
    "text": "trainings where you want to want to feed data into your gpus at high throw book",
    "start": "308820",
    "end": "314720"
  },
  {
    "start": "314000",
    "end": "379000"
  },
  {
    "text": "okay so what's this a streaming feature I'm talking about um so previous versions of rate data",
    "start": "315780",
    "end": "321740"
  },
  {
    "text": "basically last year I used a kind of bulk execution strategy so this is a very simple naive execution strategy of",
    "start": "321740",
    "end": "327539"
  },
  {
    "text": "when you try to execute a data job you're loading all the data into memory then apply any Transformations onto it",
    "start": "327539",
    "end": "333780"
  },
  {
    "text": "and um and then send it to the destination so this is this works very simply but there's some limitations for",
    "start": "333780",
    "end": "340320"
  },
  {
    "text": "example if you're trying to load data that's much bigger than your cluster memory you have to Spill the disk and and this is often and introduce a lot of",
    "start": "340320",
    "end": "346919"
  },
  {
    "text": "performance overheads not to mention if your data set is just bigger than your cluster disk you just your job cannot",
    "start": "346919",
    "end": "352139"
  },
  {
    "text": "complete at all so in the new kind of streaming execution Paradigm the API doesn't",
    "start": "352139",
    "end": "359699"
  },
  {
    "text": "really change but under the hood raid data is instead creating a series of operators for your array data job and",
    "start": "359699",
    "end": "365580"
  },
  {
    "text": "stream the data from the data source through those operators and kind of to your your final data sync without",
    "start": "365580",
    "end": "371400"
  },
  {
    "text": "needing to Spill the disk",
    "start": "371400",
    "end": "374840"
  },
  {
    "start": "379000",
    "end": "793000"
  },
  {
    "text": "so why did we uh why did we do this um",
    "start": "380340",
    "end": "385560"
  },
  {
    "text": "so there are some obvious benefits but I like to think about it like this is kind of two by two Matrix",
    "start": "385560",
    "end": "390600"
  },
  {
    "text": "um so there are two kind of workloads that we address with Ray data first thing I think of CPU only",
    "start": "390600",
    "end": "396720"
  },
  {
    "text": "pipelines like say you're just you just want to apply at UDF and then consume your result it's a",
    "start": "396720",
    "end": "402240"
  },
  {
    "text": "pretty straightforward Pipeline and then we have heterogeneous pipelines where some stages your pipelines might run on",
    "start": "402240",
    "end": "407280"
  },
  {
    "text": "CPUs and some might run on gpus so the kind of pros and cons of streaming and",
    "start": "407280",
    "end": "413639"
  },
  {
    "text": "bulk are different across these workloads so that's let's look at the very simple workload first and uh let me walk you through example",
    "start": "413639",
    "end": "422039"
  },
  {
    "text": "so a very simple CPU only pipeline might look like this and this here I'm using a batch inference as an example you might",
    "start": "422039",
    "end": "428880"
  },
  {
    "text": "want to read data pre-process data do inference and then save your data",
    "start": "428880",
    "end": "434340"
  },
  {
    "text": "and in code it would look something like this so you define your model and pre-processing function and some",
    "start": "434340",
    "end": "440340"
  },
  {
    "text": "inference function here I'm going to flesh this one out in Ray these kind of udfs take dig some non-pirates and",
    "start": "440340",
    "end": "446759"
  },
  {
    "text": "produce sticks of non-pirates so all this this function is doing is calling a model and returning the outputs",
    "start": "446759",
    "end": "452039"
  },
  {
    "text": "and I can build my my data pipeline so I could use Ray data read parquet to load the data I map I use a map",
    "start": "452039",
    "end": "459240"
  },
  {
    "text": "transformation with that pre-processing function and then do that inference function before saving my data",
    "start": "459240",
    "end": "466979"
  },
  {
    "text": "so the way this pipeline executes is as a single stage so what so this means",
    "start": "466979",
    "end": "472620"
  },
  {
    "text": "that all these all these functions are can run on CPUs therefore they can be executed kind of you know in one CPU",
    "start": "472620",
    "end": "478680"
  },
  {
    "text": "task you think about just one thing to for Ray to run and when you execute it in a bulk way",
    "start": "478680",
    "end": "484860"
  },
  {
    "text": "it's very straightforward so in this example say you have three input partitions we had our our stage we just",
    "start": "484860",
    "end": "491639"
  },
  {
    "text": "compute all of them and then get all the outputs all at once and you can't get any output until the everything's done",
    "start": "491639",
    "end": "499940"
  },
  {
    "text": "so what are the pros and cons of this um uh one kind of not obvious thing is",
    "start": "501120",
    "end": "506699"
  },
  {
    "text": "actually this is memory optimal so there's no advantage of streaming in this situation from memory usage that's",
    "start": "506699",
    "end": "513719"
  },
  {
    "text": "because there's no there's no multiple stages to stream data through there's just one stage as soon as the data goes in it's done there's no intermediate",
    "start": "513719",
    "end": "520020"
  },
  {
    "text": "data so this is actually pretty great for inference and uh you know that's why spark is great for inference in these",
    "start": "520020",
    "end": "525480"
  },
  {
    "text": "settings as well it's what it's not good for is distributor training because when you're you know you're trying to consume like a",
    "start": "525480",
    "end": "532019"
  },
  {
    "text": "terabyte data set you don't want to process the terrier Data before you start your pytorch job you want to process data as soon as as soon as",
    "start": "532019",
    "end": "537959"
  },
  {
    "text": "possible now streaming solves this problem because with streaming you you create",
    "start": "537959",
    "end": "543779"
  },
  {
    "text": "that operator and for each data partition you can send it to the operator and as soon as that operator is",
    "start": "543779",
    "end": "549060"
  },
  {
    "text": "finished computer data you can send it to your Downstream sync and so on so forth in an incremental way so that",
    "start": "549060",
    "end": "554820"
  },
  {
    "text": "solves our problem for distributed training so the streaming model is kind of better in this case",
    "start": "554820",
    "end": "560279"
  },
  {
    "text": "so you see for these simple pipelines the the main benefit of streaming is for distributor training",
    "start": "560279",
    "end": "566519"
  },
  {
    "text": "now for heterogeneous Pipelines there are more benefits so here's an",
    "start": "566519",
    "end": "572580"
  },
  {
    "text": "example of a heterogeneous pipeline where I have that read pre-process",
    "start": "572580",
    "end": "577800"
  },
  {
    "text": "inference save kind of job but one one change the inference is running on a GPU",
    "start": "577800",
    "end": "584700"
  },
  {
    "text": "I might have a you know a GPU model and in Rey uh encode this my looks like",
    "start": "584700",
    "end": "590519"
  },
  {
    "text": "this for GPU powered udfs typically you write a class instead of a function and",
    "start": "590519",
    "end": "595560"
  },
  {
    "text": "in the Constructor of the function you can set up your model that like loads it into you know initializes the GPU loads",
    "start": "595560",
    "end": "601080"
  },
  {
    "text": "the model in GP memory and then you have a call function to process a batch of of data with that initialized model and the",
    "start": "601080",
    "end": "608820"
  },
  {
    "text": "pipeline is a little bit modified uh for that GPU stage we can we we map batches",
    "start": "608820",
    "end": "615240"
  },
  {
    "text": "that class we can tell Ray to use one GPU per actor to run these",
    "start": "615240",
    "end": "620519"
  },
  {
    "text": "Transformations and we can tell Ray how many actors to use uh to to execute that",
    "start": "620519",
    "end": "626279"
  },
  {
    "text": "operator if you uh if you look at the physical",
    "start": "626279",
    "end": "631740"
  },
  {
    "text": "execution for this now there's multiple stages or multiple operators to run you have your CPU pre-processed stage",
    "start": "631740",
    "end": "639180"
  },
  {
    "text": "you have your GPU inference stage and you have a final CPU saving stage and and why are these separate stages it's",
    "start": "639180",
    "end": "645300"
  },
  {
    "text": "because you might want to scale them independently say you have say you have a lot of CPU machines you want those to run tasks from stage one then runs tasks",
    "start": "645300",
    "end": "653100"
  },
  {
    "text": "from stage two onto GPU machines and and then you can run the stage three on CP machines again",
    "start": "653100",
    "end": "659120"
  },
  {
    "text": "so if you look at the bulk execution plan for this this is where you start running into problems if you run stage",
    "start": "659399",
    "end": "665279"
  },
  {
    "text": "one you have you have to put the output data for this somewhere probably into disk or materialize into storage",
    "start": "665279",
    "end": "672779"
  },
  {
    "text": "and then if you're on stage sheet read that data back again and then station either read the",
    "start": "672779",
    "end": "678360"
  },
  {
    "text": "data so this introduces a lot of blocking calls you can't fully utilize the resources of your cluster and for these type of Pipelines",
    "start": "678360",
    "end": "685160"
  },
  {
    "text": "um so we see kind of for this this these types of pipelines um a bulk execution is really really not",
    "start": "685620",
    "end": "691740"
  },
  {
    "text": "great um and streaming uh is actually solve this to solve this problem because for",
    "start": "691740",
    "end": "697740"
  },
  {
    "text": "every operator you can uh run that data send it to the next operator and start processing um",
    "start": "697740",
    "end": "703680"
  },
  {
    "text": "uh data for the next partition and so on and so you see here we never have to materialize the intermediate data in",
    "start": "703680",
    "end": "709620"
  },
  {
    "text": "bulk and spill it to disk so we kind of can avoid that overhead yeah so kind of in summary",
    "start": "709620",
    "end": "717240"
  },
  {
    "text": "um there are some advantages of streaming execution even for a simple uh CPU only pipelines but it really shines",
    "start": "717240",
    "end": "724380"
  },
  {
    "text": "for heterogeneous use cases another interesting comparison is how",
    "start": "724380",
    "end": "732180"
  },
  {
    "text": "does streaming compare to other streaming data loaders out there like you might be familiar with pi torch data",
    "start": "732180",
    "end": "737220"
  },
  {
    "text": "or TF data there there are many of these things out there so one way of looking at this is along",
    "start": "737220",
    "end": "742560"
  },
  {
    "text": "these Dimensions so uh multi-processing based data loaders um can support some",
    "start": "742560",
    "end": "748140"
  },
  {
    "text": "of these functions they usually have some sort of caching and fault tolerance but are a little bit more limited for",
    "start": "748140",
    "end": "754200"
  },
  {
    "text": "things like partitioning data across the cluster or supporting heterogeneous clusters or anything that involves the",
    "start": "754200",
    "end": "761160"
  },
  {
    "text": "work nodes of the cluster working together now TF data is kind of an interesting",
    "start": "761160",
    "end": "766860"
  },
  {
    "text": "system it's a little bit more General than the other data loaders um the TF data is actually a fully",
    "start": "766860",
    "end": "772440"
  },
  {
    "text": "distributed system it supports automatic data partitioning and caching uh and then you are at the Netflix talk",
    "start": "772440",
    "end": "778079"
  },
  {
    "text": "yesterday you see also how they combine it with Ray and also supported heterogeneous clusters um uh which is",
    "start": "778079",
    "end": "784019"
  },
  {
    "text": "something also Ray data supports natively along with supporting inference",
    "start": "784019",
    "end": "789360"
  },
  {
    "text": "Pipelines okay so next I want to walk through uh",
    "start": "789360",
    "end": "796680"
  },
  {
    "start": "793000",
    "end": "938000"
  },
  {
    "text": "an end-to-end code example for for this Pipelines so let's look at a video",
    "start": "796680",
    "end": "802139"
  },
  {
    "text": "inference pipeline so in this example we have four logical steps you have you have a bunch of video",
    "start": "802139",
    "end": "808440"
  },
  {
    "text": "files to start with the first step is you would decode the video into frames you want to apply one model to annotate",
    "start": "808440",
    "end": "814079"
  },
  {
    "text": "the frames the second model to classify the frames and then you're running the output",
    "start": "814079",
    "end": "819120"
  },
  {
    "text": "so we have to write a kind of functions for all these stages and here's another",
    "start": "819120",
    "end": "824820"
  },
  {
    "text": "those familiar udfs that takes in a numpy daked and returns in the numpy dict",
    "start": "824820",
    "end": "830639"
  },
  {
    "text": "um that I can define something like this for a decode for frame annotation suppose I have a model I want to use to",
    "start": "830639",
    "end": "837180"
  },
  {
    "text": "annotate my frames um I can Define that model here for classification I can have another",
    "start": "837180",
    "end": "843240"
  },
  {
    "text": "model and then for the final step I don't need any custom code for that",
    "start": "843240",
    "end": "850040"
  },
  {
    "text": "so in code it will look something like this um I'll highlight some Ray features",
    "start": "850680",
    "end": "856079"
  },
  {
    "text": "um so first you can read a read video binary files so raid provides that kind of read binary files API to read files",
    "start": "856079",
    "end": "863100"
  },
  {
    "text": "from disk if I want to decode frames from that um uh file I can use a map batches",
    "start": "863100",
    "end": "869519"
  },
  {
    "text": "function to format decoder UDF Ray has this interesting feature where you can customize the resource request per stage",
    "start": "869519",
    "end": "875700"
  },
  {
    "text": "for example suppose my decoder was a multi-threaded you know video decoder already I can tell Ray I don't want you",
    "start": "875700",
    "end": "882540"
  },
  {
    "text": "to over paralyze this I want you to allocate four CPUs in that machine per task that way I can support",
    "start": "882540",
    "end": "887880"
  },
  {
    "text": "multi-threaded computations then I can run the next the model using",
    "start": "887880",
    "end": "894899"
  },
  {
    "text": "a map transformation here with a active pool of size five and then I can run the next the",
    "start": "894899",
    "end": "901680"
  },
  {
    "text": "classification step with a different strategy if I want for example I can set GPU one I can set the batch size here so",
    "start": "901680",
    "end": "909240"
  },
  {
    "text": "so tell Ray hey make sure to batch the data into you know transfer 64 for for this GPU and I can say oh here I just",
    "start": "909240",
    "end": "916740"
  },
  {
    "text": "want to use two two gpus for for this um this stuff and I can save the output",
    "start": "916740",
    "end": "924540"
  },
  {
    "text": "so uh this is kind of the most General case of running Ray data streaming with multiple operators see every operator is",
    "start": "924540",
    "end": "931860"
  },
  {
    "text": "running on a different type of resource like CPU tasks CPU actors and GPU GPU",
    "start": "931860",
    "end": "936959"
  },
  {
    "text": "actors as well and what do you get if you actually try to run this script if you run that",
    "start": "936959",
    "end": "942360"
  },
  {
    "start": "938000",
    "end": "1067000"
  },
  {
    "text": "script I kind of overviewed before you'll see that Ray data will print out the operators it creates so and you can",
    "start": "942360",
    "end": "950639"
  },
  {
    "text": "kind of see like what they're running on you can see some are running on a task pool some are running actor pool and the",
    "start": "950639",
    "end": "955740"
  },
  {
    "text": "final step is running a task pool and uh Ray data will also provide a kind",
    "start": "955740",
    "end": "961260"
  },
  {
    "text": "of a nice progress bar um and uh I think some interesting things to note here from the kind of",
    "start": "961260",
    "end": "967019"
  },
  {
    "text": "debugging standpoint is it prints out the CPU resources being used the GPU resources and memory right",
    "start": "967019",
    "end": "972959"
  },
  {
    "text": "so you can see here um that it's only using 25 CPUs in a cluster it's using",
    "start": "972959",
    "end": "978300"
  },
  {
    "text": "all the gpus and it's using all the the memory given to this job uh you know if",
    "start": "978300",
    "end": "983519"
  },
  {
    "text": "you're wondering why it's why it's not using all the CPUs it's because this pipeline happens to be Memory limited so",
    "start": "983519",
    "end": "988980"
  },
  {
    "text": "in order to stream the data through the cluster we only need 25 CPUs if we use more than more CPUs than that we might",
    "start": "988980",
    "end": "995459"
  },
  {
    "text": "create too many intermediate objects and start stability disk so that that's what this is showing",
    "start": "995459",
    "end": "1001339"
  },
  {
    "text": "and uh the job will eventually finish and you can kind of view",
    "start": "1001339",
    "end": "1007300"
  },
  {
    "text": "diagnose the stuff using the ray dashboard so the ray dashboard has some nice metric views for this so this is",
    "start": "1007300",
    "end": "1014180"
  },
  {
    "text": "just showing Rey's a breakdown of tasks in a cluster by name if you haven't seen this uh it's pretty useful so you can",
    "start": "1014180",
    "end": "1021259"
  },
  {
    "text": "have a Time series view of what actor task or what normal tasks are running and so here you can see it seems like",
    "start": "1021259",
    "end": "1028220"
  },
  {
    "text": "most of the the tasks the active tasks in the pipeline are my frame classification tasks those orange tasks",
    "start": "1028220",
    "end": "1034040"
  },
  {
    "text": "and a few were frame annotation tasks and and seems like the rest of the tasks weren't actually using that much time in",
    "start": "1034040",
    "end": "1039918"
  },
  {
    "text": "my cluster you can also look at the state of actors so here I guess there were uh I think",
    "start": "1039919",
    "end": "1047120"
  },
  {
    "text": "seven actors and maybe one utility actor you can see that for the entire job run these actors were kept 100 busy so they",
    "start": "1047120",
    "end": "1053120"
  },
  {
    "text": "weren't installed on anything you can also check physical usage of course like Network utilization so you",
    "start": "1053120",
    "end": "1059419"
  },
  {
    "text": "can see here this pipeline was pushing about two two Giga gigabytes per second I guess of of network usage",
    "start": "1059419",
    "end": "1066940"
  },
  {
    "start": "1067000",
    "end": "1192000"
  },
  {
    "text": "uh so that was an inference pipeline uh for distributor training uh um the story is not that much different",
    "start": "1067940",
    "end": "1074240"
  },
  {
    "text": "actually so um because Ray data is kind of fully General you can use those inference",
    "start": "1074240",
    "end": "1079940"
  },
  {
    "text": "pipelines for training very well the main difference is that when for inference you the you output to a uh to",
    "start": "1079940",
    "end": "1087919"
  },
  {
    "text": "a data sync at the very end but for training you instead want to split your data into multiple sub kind of sub",
    "start": "1087919",
    "end": "1095419"
  },
  {
    "text": "streams and each of those sub streams you can send to a worker for example a pie torch worker and in that work or",
    "start": "1095419",
    "end": "1101900"
  },
  {
    "text": "code you can use an iterator API to kind of pull batches from the Stream",
    "start": "1101900",
    "end": "1108160"
  },
  {
    "text": "um so there are kind of many interesting advantages using streaming for training",
    "start": "1110360",
    "end": "1116780"
  },
  {
    "text": "um one uh one uh very common use case is is when to accelerate the reader",
    "start": "1116780",
    "end": "1122419"
  },
  {
    "text": "pre-processing operation um for example if your training is actually bottlenecked by by reading i o",
    "start": "1122419",
    "end": "1128660"
  },
  {
    "text": "from S3 or pre-processing your data so just a very simple Benchmark I put together here um of this I I think I set",
    "start": "1128660",
    "end": "1136760"
  },
  {
    "text": "up a pytorch training cluster with 10 10g for the end nodes so what I think",
    "start": "1136760",
    "end": "1142220"
  },
  {
    "text": "one one GPU each and I first tried uh running the job with with just those",
    "start": "1142220",
    "end": "1148820"
  },
  {
    "text": "Jeep units and it turns out in this case that I was only able to uh uh in just",
    "start": "1148820",
    "end": "1155780"
  },
  {
    "text": "about one gigabyte per second because of the limited network and and CPU resource on those GP nodes and these are very",
    "start": "1155780",
    "end": "1161840"
  },
  {
    "text": "expensive GPU nodes but by adding uh just a few relatively cheap CPU nodes I was able to greatly",
    "start": "1161840",
    "end": "1168799"
  },
  {
    "text": "increase my ingest throughput and hence my GPU utilization um making this much more cost effective",
    "start": "1168799",
    "end": "1177100"
  },
  {
    "text": "um similarly for uh for training you can monitor the the the the the GP",
    "start": "1177919",
    "end": "1184100"
  },
  {
    "text": "utilization things like that in the training dashboard storage bandwidth and and also the memory usage of your your",
    "start": "1184100",
    "end": "1189740"
  },
  {
    "text": "training jobs um so finally I want to talk about how",
    "start": "1189740",
    "end": "1196220"
  },
  {
    "start": "1192000",
    "end": "1265000"
  },
  {
    "text": "data streaming Works in using Ray um so at a very high level uh Ray data",
    "start": "1196220",
    "end": "1203780"
  },
  {
    "text": "streaming works by implementing the transformations in that data API as operators and the execution of these operators is",
    "start": "1203780",
    "end": "1211520"
  },
  {
    "text": "actually a pretty straightforwardly decomposes into Ray core Primitives for example if you uh if you're using the",
    "start": "1211520",
    "end": "1218120"
  },
  {
    "text": "default um you know execution strategy for for an operator is just",
    "start": "1218120",
    "end": "1223400"
  },
  {
    "text": "going to use raytast to execute execute that you're a uef for Transformations if you specify actor pool of course it's",
    "start": "1223400",
    "end": "1229820"
  },
  {
    "text": "going to use array actors to to run run your code so intermediate data between these kind",
    "start": "1229820",
    "end": "1236840"
  },
  {
    "text": "of stages between these tasks and actors are data blocks in the ray Object Store",
    "start": "1236840",
    "end": "1242500"
  },
  {
    "text": "and the memory usage of these operators is limited also known as back pressure",
    "start": "1242500",
    "end": "1247700"
  },
  {
    "text": "to kind of enable efficient streaming without spilling to disk so this part is interesting because if you think about",
    "start": "1247700",
    "end": "1253900"
  },
  {
    "text": "not having BRAC pressure what happens is streaming actually just degrades to bulk",
    "start": "1253900",
    "end": "1259520"
  },
  {
    "text": "execution so so basically the back pressure is the main difference between streaming and bulk",
    "start": "1259520",
    "end": "1266360"
  },
  {
    "start": "1265000",
    "end": "1360000"
  },
  {
    "text": "so just an example of how array data schedules the execution of each stage under the hood let's look at an example",
    "start": "1266360",
    "end": "1273559"
  },
  {
    "text": "of that four stage Pipeline and look at a snapshot of what's happening in the cluster at this point in time so I have",
    "start": "1273559",
    "end": "1279140"
  },
  {
    "text": "those four operators and here I'm showing logically those those color diagrams those clutter blocks are the",
    "start": "1279140",
    "end": "1285440"
  },
  {
    "text": "intermediate data produced by one operator to be consumed by an X operator so those for example those purple lower",
    "start": "1285440",
    "end": "1290900"
  },
  {
    "text": "blocks are produced by decode frames they're about to be consumed by frame annotations okay so as the job runs you have these cues",
    "start": "1290900",
    "end": "1298280"
  },
  {
    "text": "between the operators the various sizes and radiator needs to decide like what what am I going to do to schedule a next",
    "start": "1298280",
    "end": "1304580"
  },
  {
    "text": "unit work give them the current state well looking at this pipeline you can",
    "start": "1304580",
    "end": "1309620"
  },
  {
    "text": "see that the queue between the decode and frame annotation is the largest so",
    "start": "1309620",
    "end": "1316580"
  },
  {
    "text": "this is probably telling you something that the decode frames operator is",
    "start": "1316580",
    "end": "1321740"
  },
  {
    "text": "probably producing output faster than the downstream pipeline can consume so the right action for array data is to",
    "start": "1321740",
    "end": "1327679"
  },
  {
    "text": "throttle this this this operator which means reducing a number of active tasks running here",
    "start": "1327679",
    "end": "1335000"
  },
  {
    "text": "if we look at the next operator however its output queue looks pretty small relative to to the rest so in this case",
    "start": "1335000",
    "end": "1341960"
  },
  {
    "text": "we might actually want to increase the the parallelism of this annotation thing if it was on a task a task operator",
    "start": "1341960",
    "end": "1348320"
  },
  {
    "text": "maybe increase number of tasks if it was an actor if it was an actor pool we'd try to increase the number of actors up",
    "start": "1348320",
    "end": "1353780"
  },
  {
    "text": "to the size aloud and for some operators maybe we just keep the same same parallelism",
    "start": "1353780",
    "end": "1360799"
  },
  {
    "start": "1360000",
    "end": "1424000"
  },
  {
    "text": "so building a record um is pretty interesting so a lot of the",
    "start": "1360799",
    "end": "1366860"
  },
  {
    "text": "advantages of Ray data just come purely from being built on record for example the support for heterogeneous clusters",
    "start": "1366860",
    "end": "1371900"
  },
  {
    "text": "and and fault tolerance we can kind of Leverage a lot of features of rate just just for free for example the uh the",
    "start": "1371900",
    "end": "1379760"
  },
  {
    "text": "lineage based reconstruction of Ray Thompson actors when a node fails kind of just just works for raid data we",
    "start": "1379760",
    "end": "1385520"
  },
  {
    "text": "didn't really have to do anything special for that and uh kind of the object store allows",
    "start": "1385520",
    "end": "1391760"
  },
  {
    "text": "many features such as a caching and object locality that rated a does not",
    "start": "1391760",
    "end": "1396919"
  },
  {
    "text": "really need to kind of re-implement the final point I want to make is that",
    "start": "1396919",
    "end": "1402679"
  },
  {
    "text": "Ray core also lets Ray data scale um very well so um uh when we ran a",
    "start": "1402679",
    "end": "1409400"
  },
  {
    "text": "scale test of how much data we could push just using the array data plane we were able to push more than a terabyte a",
    "start": "1409400",
    "end": "1415700"
  },
  {
    "text": "second so you can see that this doesn't really this lets Ray data scale very",
    "start": "1415700",
    "end": "1420919"
  },
  {
    "text": "well and we don't we don't think this this will be a monologue so just to summarize Ray data streaming",
    "start": "1420919",
    "end": "1427940"
  },
  {
    "start": "1424000",
    "end": "1546000"
  },
  {
    "text": "skills batch inference and training workloads it's a much more efficient computational model than the Bold",
    "start": "1427940",
    "end": "1433340"
  },
  {
    "text": "processing back in the past and the the API it doesn't doesn't really change the same same simple API",
    "start": "1433340",
    "end": "1440120"
  },
  {
    "text": "so if you're interested in using radiator for streaming reference or documentation uh is available at Doc",
    "start": "1440120",
    "end": "1446659"
  },
  {
    "text": "story.io there's also integration with the raytrain library and starting with 2006 so you don't need to so you can use",
    "start": "1446659",
    "end": "1453500"
  },
  {
    "text": "a train API and pass the data sets to it and it will be streamed so what the data team is currently",
    "start": "1453500",
    "end": "1460039"
  },
  {
    "text": "working on is a kind of hardening streaming to robust to work robustly in many edge cases for example very large",
    "start": "1460039",
    "end": "1465080"
  },
  {
    "text": "clusters or many millions of files and there are many challenges there so if you have use cases there we'd be interested in talking",
    "start": "1465080",
    "end": "1471919"
  },
  {
    "text": "okay thank you [Applause]",
    "start": "1471919",
    "end": "1480380"
  },
  {
    "text": "questions",
    "start": "1480380",
    "end": "1482860"
  },
  {
    "text": "built to show um of data mode operation as part of the",
    "start": "1489159",
    "end": "1494720"
  },
  {
    "text": "stream for example between data from a copy queue uh great question so the question was if we want to read from a",
    "start": "1494720",
    "end": "1501740"
  },
  {
    "text": "Kafka queue or something like that like a true stream Source do we support that so right now we do not support reading",
    "start": "1501740",
    "end": "1508159"
  },
  {
    "text": "from kind of unbounded input sources only like bounded input sources such as like S3 bucket or things like that but",
    "start": "1508159",
    "end": "1515000"
  },
  {
    "text": "that's that's the feature that's been asked for we might we might add that in the future",
    "start": "1515000",
    "end": "1520480"
  },
  {
    "text": "far like",
    "start": "1524120",
    "end": "1527140"
  },
  {
    "text": "yeah a great question so the question was how does this compare to spark rdds um",
    "start": "1535700",
    "end": "1541580"
  },
  {
    "text": "so I think one slide that's useful is this one so actually so Ray data bulk execution",
    "start": "1541580",
    "end": "1552140"
  },
  {
    "start": "1546000",
    "end": "1818000"
  },
  {
    "text": "was almost exactly the same as rdds with the same kind of pros and cons so I",
    "start": "1552140",
    "end": "1558260"
  },
  {
    "text": "would think of rdds as bulk executed data sets and then Ray data the the new version of Ray data is implements",
    "start": "1558260",
    "end": "1564980"
  },
  {
    "text": "streaming and pipeline execution for data sets",
    "start": "1564980",
    "end": "1569019"
  },
  {
    "text": "sorry could you repeat the question there's some clapping",
    "start": "1586520",
    "end": "1590919"
  },
  {
    "text": "very good",
    "start": "1592900",
    "end": "1596020"
  },
  {
    "text": "got it um so the question was what how do we handle shuffling data between epochs um so yeah that's a good question so",
    "start": "1603620",
    "end": "1610760"
  },
  {
    "text": "there are a couple strategies first we support like TF data and systems like other that we support a local Shuffle so",
    "start": "1610760",
    "end": "1616880"
  },
  {
    "text": "you can specify like a shuffle window like say I want a window of you know 100 000 records to mix records and we",
    "start": "1616880",
    "end": "1623000"
  },
  {
    "text": "support that um with Ray data you can also do a true Global Shuffle of the data using a kind",
    "start": "1623000",
    "end": "1630320"
  },
  {
    "text": "of dot random shuffle operation though this is pretty expensive um we also have a something where you",
    "start": "1630320",
    "end": "1636980"
  },
  {
    "text": "can Shuffle Just the files so you can for every Epoch you can randomize the order of files that you read which which",
    "start": "1636980",
    "end": "1644240"
  },
  {
    "text": "some users have I found to be the kind of best trade-off",
    "start": "1644240",
    "end": "1648639"
  },
  {
    "text": "s",
    "start": "1657020",
    "end": "1659020"
  },
  {
    "text": "uh yeah a great question so the question was um uh does the parallelism for each",
    "start": "1667340",
    "end": "1672380"
  },
  {
    "text": "operator is a fix or can it dynamically change over time and does that cause any uh problems with data skew",
    "start": "1672380",
    "end": "1678080"
  },
  {
    "text": "um so the so uh it is fully Dynamic so for",
    "start": "1678080",
    "end": "1683179"
  },
  {
    "text": "example for stages that run on tasks it we can run any number of tasks from zero to the full cluster of parallelism and",
    "start": "1683179",
    "end": "1690440"
  },
  {
    "text": "for actors we kind of constrain it to your specifications so if you say use exactly Five actors we'll use exactly",
    "start": "1690440",
    "end": "1696620"
  },
  {
    "text": "Five actors but if you say it let this Auto scale between like two and ten we can we can do that",
    "start": "1696620",
    "end": "1702080"
  },
  {
    "text": "um and as for how it impacts the um the data skew we can dynamically",
    "start": "1702080",
    "end": "1708500"
  },
  {
    "text": "adjust that um because we're just doing data loading typically we don't have problems about like you know one key",
    "start": "1708500",
    "end": "1713600"
  },
  {
    "text": "getting too big because there's no there's no like group by operations or anything like that",
    "start": "1713600",
    "end": "1718960"
  },
  {
    "text": "good question um whether we support sampling we do have a random sample operator uh I don't",
    "start": "1730700",
    "end": "1737240"
  },
  {
    "text": "I don't believe it lets you it might allow you to combine the two data sets but I'm not sure if it can do that in a",
    "start": "1737240",
    "end": "1743360"
  },
  {
    "text": "fully streaming way at the get back to you on that one yeah but you can check out like the random sample function",
    "start": "1743360",
    "end": "1750700"
  },
  {
    "text": "when you say streaming is there any ordering guarantee uh great so the question is is there any",
    "start": "1751039",
    "end": "1756380"
  },
  {
    "text": "order or guarantee for streaming so by default no we have a determined deterministic execution mode that we",
    "start": "1756380",
    "end": "1763700"
  },
  {
    "text": "that can guarantee order um uh yeah that you can you can enable that",
    "start": "1763700",
    "end": "1768860"
  },
  {
    "text": "mode",
    "start": "1768860",
    "end": "1771039"
  },
  {
    "text": "uh so there's a question can you chain any operators",
    "start": "1778240",
    "end": "1783580"
  },
  {
    "text": "oh yes um so yeah so we you can chain any number of operators in raid data and",
    "start": "1786620",
    "end": "1792440"
  },
  {
    "text": "we'll fuse together operators that require the same resources for example they both use CPUs they can get fused",
    "start": "1792440",
    "end": "1797720"
  },
  {
    "text": "together into one task",
    "start": "1797720",
    "end": "1800500"
  },
  {
    "text": "well thanks again [Applause]",
    "start": "1808159",
    "end": "1817849"
  }
]