[
  {
    "text": "uh Hey everybody thanks for coming out today my name is David Scott and I'm a machine learning engineer on the Caper",
    "start": "5279",
    "end": "11280"
  },
  {
    "text": "team with an instacart and today I want to share with you guys one of the things we're doing within Caper right now and",
    "start": "11280",
    "end": "17160"
  },
  {
    "text": "this is specifically something that we were building on Ray uh and it's basically scaling computer vision models with raid",
    "start": "17160",
    "end": "23220"
  },
  {
    "text": "and so today I'm going to walk through first table of contents right now so first I'm going to give you a little bit",
    "start": "23220",
    "end": "29039"
  },
  {
    "text": "of an introduction about instacart and Caper afterwards um we'll chat a bit about the computer",
    "start": "29039",
    "end": "34559"
  },
  {
    "text": "vision and AI teams within caper as well two is scaling our computer vision workloads and finally we'll talk a",
    "start": "34559",
    "end": "41100"
  },
  {
    "text": "little about the results evaluation and the conclusion so first is Caper what is Caper um with",
    "start": "41100",
    "end": "48539"
  },
  {
    "text": "an instacart so Caper was originally a startup that was uh went through YC program quite a few years ago and",
    "start": "48539",
    "end": "55260"
  },
  {
    "text": "Instagram hired Caper at the end of 2021. so some of the original versions of the Caper cart that you might see and",
    "start": "55260",
    "end": "61920"
  },
  {
    "text": "this is a actual Hardware product that you can utilize in grocery stores is basically like an intelligent bars code",
    "start": "61920",
    "end": "67500"
  },
  {
    "text": "scanner with an UI that you see on the right hand side maybe similar to an Amazon go-kart if you've seen an Amazon",
    "start": "67500",
    "end": "73020"
  },
  {
    "text": "go-kart before and then now we're moving on to some of our future generations of that that are actually utilizing AI at",
    "start": "73020",
    "end": "80400"
  },
  {
    "text": "scale in order to scan products in real time so what does a seamless shopping",
    "start": "80400",
    "end": "86159"
  },
  {
    "text": "experience look like for our customers essentially like with the car right now as you can see probably from the initial",
    "start": "86159",
    "end": "91560"
  },
  {
    "text": "viewing is we have quite a few sensors integrated into the cart so you can probably see on the top of",
    "start": "91560",
    "end": "97200"
  },
  {
    "text": "the cart right there is we actually have cameras that are embedded within the card and as well things like a barcode",
    "start": "97200",
    "end": "102600"
  },
  {
    "text": "scanner too one thing that's quite interesting as well is actually the bottom of the cart has a scale on it so",
    "start": "102600",
    "end": "108659"
  },
  {
    "text": "as you do things such as like adding produce products for example is that we can actually weigh the produce products in real time as you're adding them into",
    "start": "108659",
    "end": "115320"
  },
  {
    "text": "the card and then finally to one thing that we're working on is actually a real embedding",
    "start": "115320",
    "end": "120420"
  },
  {
    "text": "information of where the card is located in the store because one thing that we'd like to do in the future is basically give our users like intelligent ability",
    "start": "120420",
    "end": "126719"
  },
  {
    "text": "to like give advertisements to our users or like coupons or discounts in real time so we're also looking at the live",
    "start": "126719",
    "end": "133739"
  },
  {
    "text": "location of where the cards are in store too",
    "start": "133739",
    "end": "137660"
  },
  {
    "text": "and then next is basically talking about like how is computer vision and AI actually like integrated into our",
    "start": "138780",
    "end": "144300"
  },
  {
    "text": "products so the first thing is the hardware itself um and then the next thing that we have to do with that is basically the",
    "start": "144300",
    "end": "151140"
  },
  {
    "text": "computer vision portion of it right so within the computer vision portion is one of the things that we're looking at",
    "start": "151140",
    "end": "156780"
  },
  {
    "text": "is such as like object detection so object detection for example if you're not familiar with it is basically you",
    "start": "156780",
    "end": "163260"
  },
  {
    "text": "have a item within a image and essentially you want to determine where is the item located within that image",
    "start": "163260",
    "end": "170220"
  },
  {
    "text": "and finally how are we doing this at scale and in real time because as we're adding items into the cart is we don't",
    "start": "170220",
    "end": "176340"
  },
  {
    "text": "want any lag or feedback for our users so we train with things like Python and kind of more simple training Frameworks",
    "start": "176340",
    "end": "182760"
  },
  {
    "text": "but actually at scale when we're deploying our systems is we're utilizing things like C plus to basically give us",
    "start": "182760",
    "end": "188040"
  },
  {
    "text": "the best inference possible in real time and so in order to unlock a magical",
    "start": "188040",
    "end": "195239"
  },
  {
    "text": "shopping experience for our users is we essentially enable seamless AI that can detect products and recognize them in",
    "start": "195239",
    "end": "201000"
  },
  {
    "text": "less than 0.5 seconds so let me just try to play a quick example for you guys of",
    "start": "201000",
    "end": "207180"
  },
  {
    "text": "say how a user would utilize our product in real time and I'll play it back over and talk a little about the video",
    "start": "207180",
    "end": "214260"
  },
  {
    "text": "so oh my apologies let's go back so essentially with the user adding the",
    "start": "214260",
    "end": "220260"
  },
  {
    "text": "product into the cart is so you can kind of see uh sorry about that you can kind of see how the object detection problem",
    "start": "220260",
    "end": "226319"
  },
  {
    "text": "would work here is that from the video is we essentially have uh let me just",
    "start": "226319",
    "end": "231780"
  },
  {
    "text": "exit out of this more videos is we have the cameras in the back and we have cameras in the front of the cart and as",
    "start": "231780",
    "end": "237420"
  },
  {
    "text": "the product is coming into the cart is essentially that object recognition portion of the pipeline starts so we are",
    "start": "237420",
    "end": "243599"
  },
  {
    "text": "essentially say like the user's adding the product in and the views that we have from the cameras is immediately already detecting that object coming",
    "start": "243599",
    "end": "250319"
  },
  {
    "text": "into the card at that time and that's basically where our entire machine line machine learning pipeline starts at that",
    "start": "250319",
    "end": "256440"
  },
  {
    "text": "particular instance and so I'll just kind of quickly talk about the computer vision portion um and",
    "start": "256440",
    "end": "262440"
  },
  {
    "text": "then as well too afterwards uh start getting into kind of the meat of this presentation which is basically the",
    "start": "262440",
    "end": "267540"
  },
  {
    "text": "training uh and whatnot so for the computer vision like I was mentioning is um the basic problem is",
    "start": "267540",
    "end": "274199"
  },
  {
    "text": "object detection so we're trying to identify that problem or a product within the frame second",
    "start": "274199",
    "end": "280860"
  },
  {
    "text": "um pretty much object detection is like one of the most important portions of any computer vision pipeline because if",
    "start": "280860",
    "end": "286259"
  },
  {
    "text": "you can't get object detection to work properly every other part of your machine learning pipeline down down the",
    "start": "286259",
    "end": "292440"
  },
  {
    "text": "line essentially is going to suffer because you're not getting the object detection right at the first part of the problem",
    "start": "292440",
    "end": "297780"
  },
  {
    "text": "and then how are the computer vision models trained so basically we train with labeled images so we'll do some",
    "start": "297780",
    "end": "304199"
  },
  {
    "text": "types of like data collection and then we have to do data annotation and understand like on the bounding box of where that product is and then as well",
    "start": "304199",
    "end": "311280"
  },
  {
    "text": "too is make this robust for like real world conditions where every store is going to be quite different from one",
    "start": "311280",
    "end": "316500"
  },
  {
    "text": "another is we have to augment our images with say like different noises or different variations and order them in",
    "start": "316500",
    "end": "321960"
  },
  {
    "text": "order to get them to basically like have more variability and more understanding across different stores",
    "start": "321960",
    "end": "328199"
  },
  {
    "text": "and so let's get into more kind of like our traditional training framework of what we were doing beforehand and then",
    "start": "328199",
    "end": "334620"
  },
  {
    "text": "we'll start moving more into say some of the comparison between Ray and kubeflow so for beforehand how this would look",
    "start": "334620",
    "end": "340800"
  },
  {
    "text": "like for us is basically utilizing like Docker containers across different servers in order to make make sure we",
    "start": "340800",
    "end": "346620"
  },
  {
    "text": "have like reproducible environments if any of our gpus were different between the different types of servers that we",
    "start": "346620",
    "end": "352020"
  },
  {
    "text": "have we can easily uh basically work horizontally with those and get them to integrate with each other without any",
    "start": "352020",
    "end": "358380"
  },
  {
    "text": "issues secondly was actually the use of gpus um as you can imagine basically in like day",
    "start": "358380",
    "end": "364800"
  },
  {
    "text": "and age right now with llms and computer vision models is if you want to train really large models on a lot of data is",
    "start": "364800",
    "end": "370620"
  },
  {
    "text": "CPU is essentially like very unable to basically work up that scale and get it",
    "start": "370620",
    "end": "375720"
  },
  {
    "text": "to train quickly so for us is essentially critical that we're utilizing gpus in order to train our",
    "start": "375720",
    "end": "381240"
  },
  {
    "text": "system and then third is the distributed training component is that we basically",
    "start": "381240",
    "end": "386639"
  },
  {
    "text": "have to like in our traditional case of not using cloud is like we would have to essentially get our servers to connect",
    "start": "386639",
    "end": "392220"
  },
  {
    "text": "with each other's on our local server setup to essentially get like our models",
    "start": "392220",
    "end": "397380"
  },
  {
    "text": "to scale and train well in that instance otherwise you know we're not going to be able to train things in a very quick",
    "start": "397380",
    "end": "403020"
  },
  {
    "text": "timeline and so uh looking a little bit back at say like what are kind of like previous",
    "start": "403020",
    "end": "409560"
  },
  {
    "text": "uh generation like training system would look like as something like this is so an engineer would basically kick off a",
    "start": "409560",
    "end": "415740"
  },
  {
    "text": "training job and um essentially like we would have a server set up in like uh",
    "start": "415740",
    "end": "420900"
  },
  {
    "text": "you know one of our office locations across the world and basically like the engineer would already move say like",
    "start": "420900",
    "end": "426300"
  },
  {
    "text": "these Docker containers onto that particular server or sets of servers and then we would have something like a",
    "start": "426300",
    "end": "432240"
  },
  {
    "text": "network file system like NFS setup in order to get this to All Connect properly and then essentially the",
    "start": "432240",
    "end": "438840"
  },
  {
    "text": "training process would kick off so the training process would look a little bit like this is basically we fragment a",
    "start": "438840",
    "end": "444419"
  },
  {
    "text": "model across the different servers we'll also fragment like shards of our data across those particular servers too",
    "start": "444419",
    "end": "451080"
  },
  {
    "text": "um through the training process is we'll have some type of like gradient differentiation on each server that",
    "start": "451080",
    "end": "456120"
  },
  {
    "text": "we're working with and then at the end of a particular step is we'll basically go and average our gradients from that",
    "start": "456120",
    "end": "461520"
  },
  {
    "text": "particular uh epic and then you know once the training is done we'll save the model somewhere locally and I'm sure you",
    "start": "461520",
    "end": "468660"
  },
  {
    "text": "know as many people know is like doing this by yourself is difficult and you will run into problems so this was one",
    "start": "468660",
    "end": "474360"
  },
  {
    "text": "of the things like we wanted to kind of think about like as we were looking to basically like maybe go to more of like",
    "start": "474360",
    "end": "480060"
  },
  {
    "text": "a cloud-based framework for example so for us one of the things is we want to",
    "start": "480060",
    "end": "485160"
  },
  {
    "text": "have scalable distributed training for our cloud-based system and kind of the reason for that is we just wanted to",
    "start": "485160",
    "end": "491639"
  },
  {
    "text": "have more ability to basically you know if we easily needed to bring instances up and put instances down just have that",
    "start": "491639",
    "end": "499740"
  },
  {
    "text": "kind of at the tip of our fingers in order to do so like some of the things that we looked at for this was kupler",
    "start": "499740",
    "end": "505319"
  },
  {
    "text": "array to basically speed up our training jobs second was optimizing our training speed",
    "start": "505319",
    "end": "510660"
  },
  {
    "text": "so um one thing I'll talk a little bit about is for computer vision models in particular especially if you're working",
    "start": "510660",
    "end": "516240"
  },
  {
    "text": "with very small image sizes is you're basically going to be opening you know in the ranges of anywhere from tens of",
    "start": "516240",
    "end": "521279"
  },
  {
    "text": "thousands to hundreds of thousands or millions of images quickly and basically writing and reading on them so by",
    "start": "521279",
    "end": "527160"
  },
  {
    "text": "frequent opening and loading of numerous small image files as you run into a lot of i o bottlenecks so how this could",
    "start": "527160",
    "end": "532920"
  },
  {
    "text": "actually happen afterwards is your GP utilization is low because you're wasting all your time basically processing your data in order to feed it",
    "start": "532920",
    "end": "539580"
  },
  {
    "text": "into your gpus so and as well too obviously using additional gpus for training is nice so",
    "start": "539580",
    "end": "545580"
  },
  {
    "text": "if we don't have to use our local gpus and we can use cloud-based gpus that would help us too",
    "start": "545580",
    "end": "551700"
  },
  {
    "text": "um and then finally two is also the ease of use um onboarding for us is interesting",
    "start": "551700",
    "end": "556860"
  },
  {
    "text": "because not everybody in the team has like super strong expertise and say like machine learning infrastructure and like",
    "start": "556860",
    "end": "563040"
  },
  {
    "text": "basically running systems like on the cloud so we wanted to just give like the other Engineers on our team like a very",
    "start": "563040",
    "end": "568380"
  },
  {
    "text": "like simple way to basically get them like into like a cloud-based like training environment and allow them to",
    "start": "568380",
    "end": "573720"
  },
  {
    "text": "like scale their systems more easily um and obviously to minimizing cost is great so if we can use spot instances as",
    "start": "573720",
    "end": "580380"
  },
  {
    "text": "possible that was obviously of interest to us so kind of going back into the training",
    "start": "580380",
    "end": "586320"
  },
  {
    "text": "requirements like I was talking about in the last slide is the high throughput was essentially critical for us just",
    "start": "586320",
    "end": "592200"
  },
  {
    "text": "given the small size or image files so like actually um one of the images here is an example",
    "start": "592200",
    "end": "598680"
  },
  {
    "text": "image from one of our training setups beforehand and you know you can think like we'll have like tens of thousands hundreds of thousands millions of images",
    "start": "598680",
    "end": "604740"
  },
  {
    "text": "just like this that were basically opening and closing um all the time during our training setup",
    "start": "604740",
    "end": "610800"
  },
  {
    "text": "two is the training speed and the scalability so like I was mentioning the GPU utilization is uh critical in order",
    "start": "610800",
    "end": "617640"
  },
  {
    "text": "to basically like maximize your training speed and as well get the most out of your training setup",
    "start": "617640",
    "end": "622680"
  },
  {
    "text": "um and as well Dynamic scaling so basically like depending on how we were going to train our job if we needed more",
    "start": "622680",
    "end": "628200"
  },
  {
    "text": "GPU instances we wanted to be able to dynamically scale our GPU instances if we need more RAM based instances we want",
    "start": "628200",
    "end": "634800"
  },
  {
    "text": "to be able to dynamically scale like our Ram based instances um and finally is the operational",
    "start": "634800",
    "end": "641220"
  },
  {
    "text": "efficiency so essentially as um you know I think a lot of teams right now are trying to do more with less so like we",
    "start": "641220",
    "end": "647519"
  },
  {
    "text": "wanted to have like easy maintenance for example we didn't want like our Engineers to have to go through and you know do a lot of debugging like in the",
    "start": "647519",
    "end": "654060"
  },
  {
    "text": "cloud in order to get the system to work uh and finally is the integration flexibility so basically like being able",
    "start": "654060",
    "end": "661019"
  },
  {
    "text": "to easily integrate with our existing tools so things like Docker containers um you know if Engineers want to write a",
    "start": "661019",
    "end": "667560"
  },
  {
    "text": "script in Python things like that just giving them the easy ability in order to basically like integrate with this",
    "start": "667560",
    "end": "672899"
  },
  {
    "text": "tooling and so what did this look like for our experimental setup um and how were we",
    "start": "672899",
    "end": "679500"
  },
  {
    "text": "basically evaluating some of these like training setups so for us is um we were training a model called delpheris uh",
    "start": "679500",
    "end": "686100"
  },
  {
    "text": "which basically is more like an object recognition model um so that's why like beforehand I showed you the bag of the peanuts",
    "start": "686100",
    "end": "692040"
  },
  {
    "text": "because we're actually like recognizing like lots of images at scale um in real time and we utilize like a",
    "start": "692040",
    "end": "698339"
  },
  {
    "text": "AWS for our cloud-based partner in this I mean you know you can kind of utilize like whoever you like but we utilize AWS",
    "start": "698339",
    "end": "704399"
  },
  {
    "text": "and we basically were comparing Rey and kubeflow um in this particular",
    "start": "704399",
    "end": "709440"
  },
  {
    "text": "experiment and seeing how well they were basically going to work against one another for our criterias",
    "start": "709440",
    "end": "715560"
  },
  {
    "text": "so for our method of comparison is we basically set up like a different um or",
    "start": "715560",
    "end": "721019"
  },
  {
    "text": "had quite a few things that we were looking at so I'll start with three and I'll move over to the other three afterwards so for the first one was basically the",
    "start": "721019",
    "end": "727800"
  },
  {
    "text": "time required for setup understanding of the tool and as well the configuration of the tool",
    "start": "727800",
    "end": "733560"
  },
  {
    "text": "second was how well each systems responds to scalability and performance in real time",
    "start": "733560",
    "end": "739800"
  },
  {
    "text": "third would be the cost analysis so this would include things for us such as like storage because you have to keep your",
    "start": "739800",
    "end": "745200"
  },
  {
    "text": "data somewhere and as well instance pricing and just any additional overheads that you have with that",
    "start": "745200",
    "end": "750300"
  },
  {
    "text": "particular system so finally would be the performance metrics around the training time the GPU",
    "start": "750300",
    "end": "757680"
  },
  {
    "text": "utilization and as well our i o throughput uh fifth was kind of the integration in our compatibility with",
    "start": "757680",
    "end": "763440"
  },
  {
    "text": "existing systems and basically like onboarding our Engineers into that system and then six was basically",
    "start": "763440",
    "end": "769019"
  },
  {
    "text": "reliability and fault tolerance because we don't want to lose any training jobs while they're being kicked off um just",
    "start": "769019",
    "end": "774240"
  },
  {
    "text": "because it's going to be costly and basically slow down our engineering teams so what did this look like for us kind",
    "start": "774240",
    "end": "780899"
  },
  {
    "text": "of like as a proposed solution so depending on the um like type of framework that you utilize for example",
    "start": "780899",
    "end": "786660"
  },
  {
    "text": "like if it was going to be a ray for us is we would basically like kick off our training jobs like utilizing like a",
    "start": "786660",
    "end": "793019"
  },
  {
    "text": "traditional like training script for example versus kubeflow is we were basically utilizing like yaml files to",
    "start": "793019",
    "end": "799260"
  },
  {
    "text": "basically do all the um initial like work at the beginning and pretty much how this would work for us is we would",
    "start": "799260",
    "end": "805620"
  },
  {
    "text": "have an eks cluster and eks is a Alaska kubernetes service on AWS for anybody that's not familiar but basically we",
    "start": "805620",
    "end": "812399"
  },
  {
    "text": "would essentially kick off a training job send it into our eks cluster have a head node that would basically start",
    "start": "812399",
    "end": "818760"
  },
  {
    "text": "kicking off the training job to any of the worker nodes from our data store we",
    "start": "818760",
    "end": "824579"
  },
  {
    "text": "would essentially like Shard portions of our data in order for it to basically like go into our worker nodes which",
    "start": "824579",
    "end": "829740"
  },
  {
    "text": "would be the gpus and then finally is we would have some type of like monitoring",
    "start": "829740",
    "end": "834899"
  },
  {
    "text": "um or type of services like that during our training job that would be able to to like look into it so we you know look",
    "start": "834899",
    "end": "841740"
  },
  {
    "text": "at things such as like grafana or like ml flow for like saving some of our like particular training um pieces and as",
    "start": "841740",
    "end": "847620"
  },
  {
    "text": "well datadog 2 for like you know monitoring any of the log lines and like any issues during our jobs that we might",
    "start": "847620",
    "end": "853139"
  },
  {
    "text": "run into um and then one other portion of this uh",
    "start": "853139",
    "end": "859019"
  },
  {
    "text": "that was quite interesting for us as well is actually like being able to optimize our data loader within uh raid",
    "start": "859019",
    "end": "864600"
  },
  {
    "text": "data So for anybody that's not super familiar with raid data and I'll just bring up all the pieces right now",
    "start": "864600",
    "end": "870120"
  },
  {
    "text": "um anybody that's super not super familiar with raid data is it essentially allows you to actually like kind of recreate some of your data",
    "start": "870120",
    "end": "877380"
  },
  {
    "text": "pre-processing steps and allow it to kind of like scale within like Ray train or like any of the other Ray services so",
    "start": "877380",
    "end": "883560"
  },
  {
    "text": "some of the things that we did is basically like um worked on parallelization because uh we really",
    "start": "883560",
    "end": "889560"
  },
  {
    "text": "wanted to actually utilize like ram-based instances to basically pre-process like all of our data and",
    "start": "889560",
    "end": "894600"
  },
  {
    "text": "then quickly feed it into our GPU instances to basically like saturate our gpus as quickly and easily as possible",
    "start": "894600",
    "end": "901860"
  },
  {
    "text": "um as well too is like Ray data does offer uh built-in support for streaming execution um so we did notice like that was quite",
    "start": "901860",
    "end": "908940"
  },
  {
    "text": "beneficial for kind of like large-scale uh training setups because like the more gpus that you have like you want to be",
    "start": "908940",
    "end": "914279"
  },
  {
    "text": "able to like easily stream data in um and finally two is another nice thing that we notice with Ray data is you can",
    "start": "914279",
    "end": "920579"
  },
  {
    "text": "actually like Implement your own like user-defined functions like udfs inside raid data so if you have any like kind",
    "start": "920579",
    "end": "927000"
  },
  {
    "text": "of like pre-processing criterias for like the type of like machine learning pipelines that you're working with is",
    "start": "927000",
    "end": "932160"
  },
  {
    "text": "you can actually kind of like create those within Ray data to work effectively",
    "start": "932160",
    "end": "937639"
  },
  {
    "text": "um and so like an example of like our particular case of like how this looked like is like in terms of like rebuilding our data loader is we have some",
    "start": "937740",
    "end": "944639"
  },
  {
    "text": "user-defined functions which would be like parse file names resize and Pad images adding keys and then stack",
    "start": "944639",
    "end": "950339"
  },
  {
    "text": "emulated is along Channel axes um and we can actually do like mapping sorting and group buys like quite",
    "start": "950339",
    "end": "956699"
  },
  {
    "text": "effectively with that just using the built-in API that raid data has which is just read images and then within the",
    "start": "956699",
    "end": "963660"
  },
  {
    "text": "tray Loop config is it might just be particular things such as like your image size or any any other types of",
    "start": "963660",
    "end": "969480"
  },
  {
    "text": "config information that you need for your training job and then going into the results and",
    "start": "969480",
    "end": "977100"
  },
  {
    "text": "evaluation so for rate or for kubeflow is um one of the things we want to look",
    "start": "977100",
    "end": "982980"
  },
  {
    "text": "at uh was the time required to utilize the tool for kubeflow what we found at the beginning is basically um it's kind",
    "start": "982980",
    "end": "989459"
  },
  {
    "text": "of similar in Ray is that you have to manually set up your elastic kubernetes clusters and your auto scalers",
    "start": "989459",
    "end": "994740"
  },
  {
    "text": "um but actually like getting everything to get set up out of the box because there's so many like moving components is it did require like several hours to",
    "start": "994740",
    "end": "1001459"
  },
  {
    "text": "basically like set it up and get it to work like well for our team another thing is too is that because we took the",
    "start": "1001459",
    "end": "1007759"
  },
  {
    "text": "path of basically like allowing our Engineers to like create jobs from yaml files is we had to write a lot of documentation for our team for like you",
    "start": "1007759",
    "end": "1015259"
  },
  {
    "text": "know our Engineers basically understand okay if you're going to set up a yaml Based training job uh these are all the",
    "start": "1015259",
    "end": "1020300"
  },
  {
    "text": "steps you need to do you need to bring your Docker image on to elastic container registry ECR within AWS so",
    "start": "1020300",
    "end": "1026900"
  },
  {
    "text": "there was a little bit more Headway needed that we found um and Ray for example is because all of",
    "start": "1026900",
    "end": "1033020"
  },
  {
    "text": "our Engineers are super familiar earlier with python is that we're you know we'll always say is all you have to do is like import your tensorflow trainer and then",
    "start": "1033020",
    "end": "1039260"
  },
  {
    "text": "maybe make a couple of small changes like in your training setup like recreate your training Loop and",
    "start": "1039260",
    "end": "1044959"
  },
  {
    "text": "basically you can start kicking off your job and also allocate some particular pieces like the",
    "start": "1044959",
    "end": "1050540"
  },
  {
    "text": "number of workers that you have or like the use of GPU um and whatnot and uh yeah we found like having the",
    "start": "1050540",
    "end": "1057039"
  },
  {
    "text": "integration on raid there was quite effective for our team so in terms of performance metrics say",
    "start": "1057039",
    "end": "1063260"
  },
  {
    "text": "between both Ray and kubeflow is like in terms of scalability we found that like each system does leverage Auto scaling",
    "start": "1063260",
    "end": "1069740"
  },
  {
    "text": "and Cloud instances so the scalability we could say between like Ray and kubeflow is quite similar actually",
    "start": "1069740",
    "end": "1075200"
  },
  {
    "text": "similar with like reliability and fault tolerance uh like both systems do have some ability for like fault tolerance",
    "start": "1075200",
    "end": "1081020"
  },
  {
    "text": "and reliability so like we didn't really find like a huge difference between like Ray and kubeflow for either of those",
    "start": "1081020",
    "end": "1087260"
  },
  {
    "text": "um and one of the things I'll chat a little bit more about afterwards but we did actually find we had some cost analysis for Rey",
    "start": "1087260",
    "end": "1093860"
  },
  {
    "text": "um mainly because we were actually able to store our data in like S3 instead of like elastic file storage like EFS",
    "start": "1093860",
    "end": "1099500"
  },
  {
    "text": "within AWS and S3 is actually a lot cheaper than uh at ew or sorry EFS for",
    "start": "1099500",
    "end": "1105500"
  },
  {
    "text": "storing data so we actually noticed like quite a considerable cost savings just from the storage side",
    "start": "1105500",
    "end": "1111820"
  },
  {
    "text": "um and then getting a little bit more into kind of like the meat of the performance metrics is um we did look at",
    "start": "1111919",
    "end": "1117500"
  },
  {
    "text": "like the GP utilization between both kubeflow and Ray um what we did notice is like you know",
    "start": "1117500",
    "end": "1122780"
  },
  {
    "text": "GPU utilization will vary for sure during your training jobs but like I think on average we kind of notice like",
    "start": "1122780",
    "end": "1127880"
  },
  {
    "text": "maybe about five percent more saturation on Ray um compared to kubeflow um and I should",
    "start": "1127880",
    "end": "1134179"
  },
  {
    "text": "also probably just introduce the graph sides a little bit so basically the left hand side is average GPU utilization",
    "start": "1134179",
    "end": "1139820"
  },
  {
    "text": "over like a one minute period um and then the bottom is just like a 20 minute period for that particular",
    "start": "1139820",
    "end": "1146240"
  },
  {
    "text": "um like training instance two is the performance metrics and I O",
    "start": "1146240",
    "end": "1152600"
  },
  {
    "text": "so um the first thing to kind of talk about here is the basically the left hand side the left axis which is red is",
    "start": "1152600",
    "end": "1158299"
  },
  {
    "text": "read speed right hand side is write speed so for this particular case the reason why you'll actually see like",
    "start": "1158299",
    "end": "1164600"
  },
  {
    "text": "kubeflow kind of have like more of like a declining linear uh pattern here is because we're actually able to load like",
    "start": "1164600",
    "end": "1170059"
  },
  {
    "text": "all the memory into Data like on that particular GPU because it's actually a smaller size for this particular experiment",
    "start": "1170059",
    "end": "1176660"
  },
  {
    "text": "um and real like in a real kind of like training environment is I don't think you would expect to see a pattern exactly like this Ray for example",
    "start": "1176660",
    "end": "1184340"
  },
  {
    "text": "actually the reason why we do see kind of like more of like a batch like this is actually like the raid data API I",
    "start": "1184340",
    "end": "1190760"
  },
  {
    "text": "believe it's more of like a batch um like batch loading for like the data that you're working with at that particular point in time so that's why",
    "start": "1190760",
    "end": "1197120"
  },
  {
    "text": "like in this case you'll see more of kind of like at each epic Point like why you're basically getting your read speed",
    "start": "1197120",
    "end": "1202460"
  },
  {
    "text": "and your write speed to kind of like work like that um so then kind of like moving a bit",
    "start": "1202460",
    "end": "1208700"
  },
  {
    "text": "more into the insights pros and cons and the conclusion so for the pros of each setup I think with kubeflow um one thing",
    "start": "1208700",
    "end": "1215360"
  },
  {
    "text": "that was really nice for us and our Engineers is like it did have like built-in Docker support right off the bat um so this was able you know great for",
    "start": "1215360",
    "end": "1221840"
  },
  {
    "text": "us to basically like easily utilize like our existing like Docker images and ensure like we can basically have consistency between our like um on in",
    "start": "1221840",
    "end": "1228860"
  },
  {
    "text": "Prem or like on-prem setup and our Cloud setup because our Engineers are used to that um and then as well too easy job",
    "start": "1228860",
    "end": "1234919"
  },
  {
    "text": "launching is like as soon as you basically figured out how to set up your yaml files or you had like everything set up is like it was really easy to",
    "start": "1234919",
    "end": "1241400"
  },
  {
    "text": "basically like kick off your training jobs afterwards um with Ray so I think like one of the",
    "start": "1241400",
    "end": "1246620"
  },
  {
    "text": "things that I mentioned before is that integration was S3 um and it was able to allow like our gpus to actually achieve",
    "start": "1246620",
    "end": "1252620"
  },
  {
    "text": "like higher saturation using commodity storage so basically utilizing like cheap Ram instances we were actually",
    "start": "1252620",
    "end": "1257840"
  },
  {
    "text": "able to basically like pre-process all our data beforehand and then get like better GPU saturation afterwards",
    "start": "1257840",
    "end": "1265700"
  },
  {
    "text": "um as well too kind of in line with that is like the streaming execution so Ray data is built in um like memory or sorry support for",
    "start": "1265700",
    "end": "1272539"
  },
  {
    "text": "streaming uh did lower like memory and compute requirements for our large scale training jobs",
    "start": "1272539",
    "end": "1278480"
  },
  {
    "text": "cons of each setup um kind of like I think I mentioned before is kubeflow it does have like a lot of setup",
    "start": "1278480",
    "end": "1283580"
  },
  {
    "text": "complexibility at the start so it has like you know a decent amount of time required and depending on your team's um",
    "start": "1283580",
    "end": "1288980"
  },
  {
    "text": "maturity for like machine learning infrastructures you might have to write like more documentations and things like that and then integration constraints is",
    "start": "1288980",
    "end": "1296059"
  },
  {
    "text": "what we did notice is that like when kubeflow for example maybe different portions of the kubeflow cluster ran",
    "start": "1296059",
    "end": "1302299"
  },
  {
    "text": "into issues it was basically hinder your entire kubeflow's cluster performance so you might have to go into like",
    "start": "1302299",
    "end": "1307460"
  },
  {
    "text": "additional pods that aren't super critical to your training job to basically like make sure or understand like why you're running into issues",
    "start": "1307460",
    "end": "1314059"
  },
  {
    "text": "there and then Ray for example is that well it does have a built-in integration with S3 is there were challenges when",
    "start": "1314059",
    "end": "1320960"
  },
  {
    "text": "optimizing i o so that's pretty much like why we had to utilize like raid data so we could kind of see this as",
    "start": "1320960",
    "end": "1326240"
  },
  {
    "text": "like a bit of a pro and a con I think it just gives you like more ability to basically like optimize your i o because",
    "start": "1326240",
    "end": "1331640"
  },
  {
    "text": "it's all within the ray ecosystem um as well dependency management is I think sometimes is that in order to get",
    "start": "1331640",
    "end": "1338000"
  },
  {
    "text": "like the ray cluster to operate like the way that you want it to is you do have to pay a lot of attention to your dependencies but as soon as you get them",
    "start": "1338000",
    "end": "1344240"
  },
  {
    "text": "right everything will work the way that you expect it would couple of unexpected insights that",
    "start": "1344240",
    "end": "1350059"
  },
  {
    "text": "things that we didn't kind of like realize when we were getting into this particular investigation is that",
    "start": "1350059",
    "end": "1356000"
  },
  {
    "text": "um we didn't expect to get better I O using S3 than EFS because if you actually do look at like how AWS pitches",
    "start": "1356000",
    "end": "1362120"
  },
  {
    "text": "these products is AWS will essentially pitch these products as that S3 would be like an inferior i o system to an EFS",
    "start": "1362120",
    "end": "1369140"
  },
  {
    "text": "system so that was one of the unexpected insights that we found uh Second and I think this was kind of",
    "start": "1369140",
    "end": "1374960"
  },
  {
    "text": "like good to know for now and in the future too is that in order to optimize our training is actually like the data",
    "start": "1374960",
    "end": "1380539"
  },
  {
    "text": "loader and how you feed using Ram instances to GPU instances can be super critical in order to basically like",
    "start": "1380539",
    "end": "1386659"
  },
  {
    "text": "speeding up your training jobs reducing your overall costs and getting better GPU utilization",
    "start": "1386659",
    "end": "1392120"
  },
  {
    "text": "three is using those cheap Ram instances instead of expensive gpus for pre-processing also can be an optimal",
    "start": "1392120",
    "end": "1398360"
  },
  {
    "text": "route to take which is one of the things that we found through this um and just in terms of acknowledgments",
    "start": "1398360",
    "end": "1404659"
  },
  {
    "text": "uh just like to thank Balaji and Xiao and as well the any scale team because they were super helpful with this",
    "start": "1404659",
    "end": "1410059"
  },
  {
    "text": "particular setup and all the collaboration work that we had and that is also super happy to answer any of",
    "start": "1410059",
    "end": "1416960"
  },
  {
    "text": "your questions now or you're also more than welcome to find me around here or offline and I'll be happy to",
    "start": "1416960",
    "end": "1422120"
  },
  {
    "text": "um yeah answer any more questions thanks foreign [Applause]",
    "start": "1422120",
    "end": "1429799"
  },
  {
    "text": "oh yeah and um there's no mic in here so I'll get the questions and I'll repeat them back to you and then I'll answer",
    "start": "1429799",
    "end": "1434900"
  },
  {
    "text": "afterwards yeah",
    "start": "1434900",
    "end": "1437620"
  },
  {
    "text": "um so I don't think we've ever actually seen like GP utilization get like close to 100 um I think if you average it out over",
    "start": "1454760",
    "end": "1461179"
  },
  {
    "text": "like say like a Long training run like honestly kind of like even for an on-prem server setup it could be like I",
    "start": "1461179",
    "end": "1467059"
  },
  {
    "text": "would say 25 to 45 and anybody in here if you have a different experience feel",
    "start": "1467059",
    "end": "1472340"
  },
  {
    "text": "free to mention as well but I've never seen like us even on like an on-pre like on-prem with like super fast uh like",
    "start": "1472340",
    "end": "1478820"
  },
  {
    "text": "memory like ssds and like using like a network file system like being able to kind of get over like you know 40 45",
    "start": "1478820",
    "end": "1484760"
  },
  {
    "text": "over like a long period of time",
    "start": "1484760",
    "end": "1488620"
  },
  {
    "text": "which one is",
    "start": "1493760",
    "end": "1496539"
  },
  {
    "text": "that's a good question um to be honest I'm not 100 sure off the top of my head between those two which",
    "start": "1500360",
    "end": "1506360"
  },
  {
    "text": "the one with this but if you'd like like you know feel free to email me or connect with me and like I can go and double check and you know provide the",
    "start": "1506360",
    "end": "1512600"
  },
  {
    "text": "response to offline afterwards yes",
    "start": "1512600",
    "end": "1516100"
  },
  {
    "text": "you'll have to actually",
    "start": "1537500",
    "end": "1540100"
  },
  {
    "text": "because you think that makes a difference",
    "start": "1543860",
    "end": "1549440"
  },
  {
    "text": "got it okay so I'll kind of try to summarize this question so I think the question is that basically",
    "start": "1549440",
    "end": "1554480"
  },
  {
    "text": "um you have kuplo and Rey and correct me if I'm wrong so you have kuplo and Rey and essentially the question is that uh",
    "start": "1554480",
    "end": "1559820"
  },
  {
    "text": "because kuplo is like a kubernetes based um like deploying processes once Google set up like you don't have to like",
    "start": "1559820",
    "end": "1564980"
  },
  {
    "text": "basically manage any of the cluster and whatnot versus because Rey is essentially like you bring up the cluster at that particular point in time",
    "start": "1564980",
    "end": "1570980"
  },
  {
    "text": "is like you would have to essentially like have like more Management on the cluster does that sound about right okay",
    "start": "1570980",
    "end": "1576320"
  },
  {
    "text": "so I I guess to some extent is true um it's it would be like this is that",
    "start": "1576320",
    "end": "1583039"
  },
  {
    "text": "like with the kuplo side for example is you'll set up your cluster at the beginning but like after you set up your cluster is like you still do to actually",
    "start": "1583039",
    "end": "1589279"
  },
  {
    "text": "kind of have to manage it to some extent is like in our case for example is that like we weren't actually training jobs",
    "start": "1589279",
    "end": "1594620"
  },
  {
    "text": "on kubeflow all the time so actually what we would have to do before we wanted to put jobs into kubeflows we would actually go into like our eec2",
    "start": "1594620",
    "end": "1602000"
  },
  {
    "text": "um basically and then we would turn on our Auto scaler again so we basically bring up our like Ram based instances for ec2 to basically like spin up our",
    "start": "1602000",
    "end": "1609020"
  },
  {
    "text": "jobs um so I guess to some extent is it does still actually require some like upkeep to some extent like it would require",
    "start": "1609020",
    "end": "1614779"
  },
  {
    "text": "Engineers to basically understand like how to deploy it um and then as well too is like we did",
    "start": "1614779",
    "end": "1620480"
  },
  {
    "text": "actually run into like some issues too with kubeflow and the sense of that like you know we would have certain pods that",
    "start": "1620480",
    "end": "1625940"
  },
  {
    "text": "would go down during deployment and then like we would have to basically go into like these subplot pods and basically understand like why those are not",
    "start": "1625940",
    "end": "1632360"
  },
  {
    "text": "working the way that we expected them to versus like with Ray for example is I mean based on my experience um and we",
    "start": "1632360",
    "end": "1638419"
  },
  {
    "text": "were actually like utilizing like the any scale uh managed solution like while we were like investigating Rey is it",
    "start": "1638419",
    "end": "1644059"
  },
  {
    "text": "actually seemed like pretty low maintenance from like an engineer side because like we would basically just",
    "start": "1644059",
    "end": "1650120"
  },
  {
    "text": "have like our code already built and then we would basically just submit our code and like it would already spin up a",
    "start": "1650120",
    "end": "1655220"
  },
  {
    "text": "cluster for us already so I guess based on this comparison is that my finding would be is they both do",
    "start": "1655220",
    "end": "1662179"
  },
  {
    "text": "require upkeep to some extent but I feel is that like the kubeflow is like you might actually depending on it always",
    "start": "1662179",
    "end": "1667880"
  },
  {
    "text": "depends on how you're deploying these things but the kuplo actually could still require like maybe a little bit more upkeep than the raid I guess to my",
    "start": "1667880",
    "end": "1673520"
  },
  {
    "text": "my user experience is that it seemed like to me Ray required a little bit less upkeeping good question thank you",
    "start": "1673520",
    "end": "1680799"
  },
  {
    "text": "you'll have to change your like some of the import ance",
    "start": "1686419",
    "end": "1692679"
  },
  {
    "text": "between",
    "start": "1694700",
    "end": "1696940"
  },
  {
    "text": "so sorry uh",
    "start": "1703760",
    "end": "1707320"
  },
  {
    "text": "yeah so um just kind of repeating the question really quickly is that um so basically in kubeflow is that",
    "start": "1716120",
    "end": "1722059"
  },
  {
    "text": "because everything is like in our case we use yaml basically to set up everything so um the question is essentially is like",
    "start": "1722059",
    "end": "1728480"
  },
  {
    "text": "you know is there probably a lot of overhead in changing your code base like if you're going to use Rey to start your",
    "start": "1728480",
    "end": "1733820"
  },
  {
    "text": "like computer vision training jobs compared to kubeflow um and then so for us within kubeflow is that basically so the requirements are",
    "start": "1733820",
    "end": "1740120"
  },
  {
    "text": "get your Docker images into elastic uh container registry so ECR then basically",
    "start": "1740120",
    "end": "1746059"
  },
  {
    "text": "like create your yaml file with like how you need to start your job and then three is basically just submit your yaml",
    "start": "1746059",
    "end": "1751700"
  },
  {
    "text": "file so that um Ray like I mean as you can kind of see based off the like right hand side",
    "start": "1751700",
    "end": "1757159"
  },
  {
    "text": "here is because all our training jobs were like natively like based out of python even though that we would create Docker images from those Python scripts",
    "start": "1757159",
    "end": "1764120"
  },
  {
    "text": "but I mean even if you were just like doing experimentation like this is like for us so all we would have to do is",
    "start": "1764120",
    "end": "1770360"
  },
  {
    "text": "basically import the tensorflow trainer and then basically like you would just create this train Loop per worker so",
    "start": "1770360",
    "end": "1776059"
  },
  {
    "text": "like instead of us maybe beforehand having like a main function is you would just create like another function that",
    "start": "1776059",
    "end": "1782240"
  },
  {
    "text": "basically would run this like per epic type thing so instead of maybe setting up your job for like however many epics",
    "start": "1782240",
    "end": "1787399"
  },
  {
    "text": "you need you just like Define like how that Loop works like per epic and so I actually think it like it doesn't require a lot of work for us maybe but",
    "start": "1787399",
    "end": "1794600"
  },
  {
    "text": "it's like any tooling right any tooling will have like a little bit of like learning at the beginning but like once you get over that learning hump like it",
    "start": "1794600",
    "end": "1800240"
  },
  {
    "text": "actually is quite straightforward so I feel like for actually like many of the engineers on our team that are comfortable with python is like this",
    "start": "1800240",
    "end": "1806419"
  },
  {
    "text": "would be like doable for them for sure so maybe not too much um like upkeep",
    "start": "1806419",
    "end": "1811460"
  },
  {
    "text": "yeah and I think you had a question back there too yeah my experience that",
    "start": "1811460",
    "end": "1817460"
  },
  {
    "text": "that um S3 is better it still seems a little",
    "start": "1817460",
    "end": "1824419"
  },
  {
    "text": "ridiculous I'm curious if you've tried with any other Reserve Network",
    "start": "1824419",
    "end": "1829760"
  },
  {
    "text": "maybe a better yeah totally um so just I think everybody heard it but basically just",
    "start": "1829760",
    "end": "1835039"
  },
  {
    "text": "the I think summary of the question is um for E EFS basically has very poor",
    "start": "1835039",
    "end": "1840200"
  },
  {
    "text": "kind of small files um support uh S3 also isn't great",
    "start": "1840200",
    "end": "1845840"
  },
  {
    "text": "um and then like is there any other options available uh so for us what we did actually look at too um we did",
    "start": "1845840",
    "end": "1851720"
  },
  {
    "text": "actually Benchmark like most of the E8 AWS storage systems for us uh gp3",
    "start": "1851720",
    "end": "1857059"
  },
  {
    "text": "actually seemed to work really really well um so if anybody is doing like any trainings like you can look at that",
    "start": "1857059",
    "end": "1863059"
  },
  {
    "text": "um I think based off our experiences we found like for all the i o particular throughput things that we were looking at actually gp3 has the best even though",
    "start": "1863059",
    "end": "1869840"
  },
  {
    "text": "I think GP actually probably not 100 sure I thought my head but I think gp2",
    "start": "1869840",
    "end": "1875000"
  },
  {
    "text": "is actually advertised to be faster than gp3 but yeah so for us what we found with basically small file uh support is",
    "start": "1875000",
    "end": "1881840"
  },
  {
    "text": "gp3 did have like very good ability um so yeah that could be something interesting to look at in any any of the",
    "start": "1881840",
    "end": "1888380"
  },
  {
    "text": "distributed training cases",
    "start": "1888380",
    "end": "1891580"
  },
  {
    "text": "yeah good question um so our our data sorry just a quick",
    "start": "1899260",
    "end": "1904399"
  },
  {
    "text": "repeat basically it's um how is the data stored and like fed into the system uh for us so data store right now is like",
    "start": "1904399",
    "end": "1912140"
  },
  {
    "text": "it would basically be a um like image file that either would exist like locally on like a SSD or like in the",
    "start": "1912140",
    "end": "1918679"
  },
  {
    "text": "cloud on like S3 EFS gp3 or whatnot um basically as it's fed into our system",
    "start": "1918679",
    "end": "1923960"
  },
  {
    "text": "is we are doing like numpy basically uh modifications on that particular system",
    "start": "1923960",
    "end": "1929000"
  },
  {
    "text": "so we're not actually doing um any parquet reading um for like our normal like training setup that we do",
    "start": "1929000",
    "end": "1934760"
  },
  {
    "text": "right now it's all basically like numpy modifications and operations yeah good question",
    "start": "1934760",
    "end": "1941260"
  },
  {
    "text": "down there yeah um so I think the question is basically so",
    "start": "1951140",
    "end": "1956779"
  },
  {
    "text": "when we're pre-processing the images and like feeding them into our system is are we basically taking like the background to or like adding more into the",
    "start": "1956779",
    "end": "1962720"
  },
  {
    "text": "background sorry maybe I'm under misunderstanding the question I mean all the image",
    "start": "1962720",
    "end": "1969580"
  },
  {
    "text": "oh I understand",
    "start": "1973279",
    "end": "1976120"
  },
  {
    "text": "I yeah I think uh yeah so I think the question is basically um and once again correct me if I'm",
    "start": "1989860",
    "end": "1995899"
  },
  {
    "text": "wrong is uh essentially it's like and I think I actually have uh maybe here this is a good slide to go on is so basically",
    "start": "1995899",
    "end": "2002860"
  },
  {
    "text": "like are we also including some of the pre-processing steps like during the training rather than like basically all",
    "start": "2002860",
    "end": "2008799"
  },
  {
    "text": "at the beginning or all at the end um so in the ray case is uh like actually because we're utilizing like",
    "start": "2008799",
    "end": "2014559"
  },
  {
    "text": "the cheaper um Ram based instances to basically do all the pre-processing on like our data at the beginning is we actually find",
    "start": "2014559",
    "end": "2020860"
  },
  {
    "text": "like we can do pretty much like most of those like mapping and like group eyes like all within like those Ram based instances and then just like feed it",
    "start": "2020860",
    "end": "2027100"
  },
  {
    "text": "into our gpus afterwards so for like this case yes um for our traditional case of like say",
    "start": "2027100",
    "end": "2033340"
  },
  {
    "text": "if we were just like using like a commodity GPU and utilizing this is like we would basically do that all like on",
    "start": "2033340",
    "end": "2038559"
  },
  {
    "text": "each epic so basically at the beginning of each epic is like we would do all that data pre-processing and then basically like once that is ready at",
    "start": "2038559",
    "end": "2045940"
  },
  {
    "text": "like the initial epic then like we we would feed the data in once it's all been pre-processed so in this case you",
    "start": "2045940",
    "end": "2053260"
  },
  {
    "text": "would do it basically all the beginning on the ram based instances on our if we're just using our GPU based instances like on like a local setup or whatnot we",
    "start": "2053260",
    "end": "2060220"
  },
  {
    "text": "would actually do all the uh pre-processing like on that GPU machine at that point in time the training",
    "start": "2060220",
    "end": "2067358"
  },
  {
    "text": "yeah thank you",
    "start": "2067359",
    "end": "2070200"
  }
]