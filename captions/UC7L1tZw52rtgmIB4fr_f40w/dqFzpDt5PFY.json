[
  {
    "text": "hey everyone my name is Karthik I'm a machine learning engineer at Arthur AI hello everyone I am sahil I'm a PhD",
    "start": "3120",
    "end": "10500"
  },
  {
    "text": "student at the University of Washington and today we'll be talking about fast CFE which is a distributed reinforcement",
    "start": "10500",
    "end": "16740"
  },
  {
    "text": "learning based kind of factual explanation tool and this work was done while I was interning at r3i",
    "start": "16740",
    "end": "24840"
  },
  {
    "text": "so in the stock we'll be going over the background the problem statement followed by the implementation system",
    "start": "24840",
    "end": "30900"
  },
  {
    "text": "design and a quick glance at the results and let's Dive Right In",
    "start": "30900",
    "end": "39000"
  },
  {
    "text": "all right so so consider a case where you go to a",
    "start": "39000",
    "end": "44100"
  },
  {
    "text": "bank to ask for a loan to purchase a new house or a car and you go to a banker",
    "start": "44100",
    "end": "49140"
  },
  {
    "text": "and they're taking all your features like your income your age your education level and stuff like that and feed it",
    "start": "49140",
    "end": "54660"
  },
  {
    "text": "into the computer that's in front of you which is running up back in machine learning model",
    "start": "54660",
    "end": "60660"
  },
  {
    "text": "and after consuming all your features it spits out that you are rejected for the loan as someone who needs money at that",
    "start": "60660",
    "end": "67140"
  },
  {
    "text": "point you would be interested in asking the question why was my loan rejected or",
    "start": "67140",
    "end": "72240"
  },
  {
    "text": "specifically what can I do in order to get the loan and this is the specific question that count of actual",
    "start": "72240",
    "end": "78240"
  },
  {
    "text": "explanations answer so for example it could tell you that had your income been income being forty",
    "start": "78240",
    "end": "84540"
  },
  {
    "text": "thousand dollars rather than thirty thousand dollars you would have gotten the loan or if you had been a masters instead of a bachelor's you would have",
    "start": "84540",
    "end": "90840"
  },
  {
    "text": "gotten the loan so there are some desirable properties which ideally counter fascial",
    "start": "90840",
    "end": "97380"
  },
  {
    "text": "explanations should follow and the first and foremost that a modified data point which is denoted by X Prime over here",
    "start": "97380",
    "end": "103619"
  },
  {
    "text": "should lead you across the decision boundary which is in this case getting you the loan approved",
    "start": "103619",
    "end": "109680"
  },
  {
    "text": "apart from that in order to be in order to in order for the changes to be",
    "start": "109680",
    "end": "114780"
  },
  {
    "text": "realistic we should be close to the training data manifold that was used to train the machine learning model which",
    "start": "114780",
    "end": "120360"
  },
  {
    "text": "the bank is using uh there are certain features that have constraints on themselves and therefore",
    "start": "120360",
    "end": "126540"
  },
  {
    "text": "we should not modify them for example your race or your suggesting that you need to decrease your age",
    "start": "126540",
    "end": "133560"
  },
  {
    "text": "and another constraint to be realistic is that we should follow some real world causal constraints like if if a model is",
    "start": "133560",
    "end": "140580"
  },
  {
    "text": "suggesting you to increase your education level it also needs to take into the account that your age will increase",
    "start": "140580",
    "end": "147420"
  },
  {
    "text": "so these are the properties of a count of actual explanation that are desirable there are also certain properties that",
    "start": "147420",
    "end": "153180"
  },
  {
    "text": "are desirable from the count the approach that is generating the count of actual explanation and we have",
    "start": "153180",
    "end": "158640"
  },
  {
    "text": "enumerated them here so model agnosticity is the first one of them which is basically if a if an approach",
    "start": "158640",
    "end": "165540"
  },
  {
    "text": "can work is model agnostic then it can work for multiple machine learning models",
    "start": "165540",
    "end": "170640"
  },
  {
    "text": "Black Box access is useful if a approach is going to be deployed in a privacy",
    "start": "170640",
    "end": "176160"
  },
  {
    "text": "restricted setting in which the machine learning model cannot be exposed to the to the vendor and if the approach can",
    "start": "176160",
    "end": "184379"
  },
  {
    "text": "only generate counter factual explanation by accessing the predict function that's great and the third one is about being fast so",
    "start": "184379",
    "end": "191940"
  },
  {
    "text": "when these systems are deployed at industrial scale they need to consume a lot of points and generate counterfactual explanations for them and",
    "start": "191940",
    "end": "198300"
  },
  {
    "text": "therefore we want them to be amortized and fast so we went over the properties of counterfactual explanations and the",
    "start": "198300",
    "end": "204840"
  },
  {
    "text": "approaches that are generating them and now let's look at how are the approaches",
    "start": "204840",
    "end": "209879"
  },
  {
    "text": "that are present in these in the explainability literature perform on these qualitative attributes",
    "start": "209879",
    "end": "215940"
  },
  {
    "text": "so here in the leftmost column I have enlisted one of some of the popular and",
    "start": "215940",
    "end": "221700"
  },
  {
    "text": "like the most cited approaches in this in this research area as you can see",
    "start": "221700",
    "end": "228000"
  },
  {
    "text": "fast CFE was the first and only one that could satisfy all of these qualitative properties",
    "start": "228000",
    "end": "235319"
  },
  {
    "text": "and now let's talk about how did Fast CF achieve this so the main Insight here was to train a",
    "start": "235319",
    "end": "242760"
  },
  {
    "text": "reinforcement learning agent that could generate account of actual explanation for a data point from a given data",
    "start": "242760",
    "end": "249480"
  },
  {
    "text": "distribution and this Insight can satisfy all the desirable properties that were mentioned",
    "start": "249480",
    "end": "256199"
  },
  {
    "text": "in the slides before because to train this reinforcement learning agent you only need access to",
    "start": "256199",
    "end": "262440"
  },
  {
    "text": "the predict function of a model and hits and hence its model agnostic and black box",
    "start": "262440",
    "end": "267479"
  },
  {
    "text": "and once this reinforcement learning agent has learned a policy to produce",
    "start": "267479",
    "end": "272940"
  },
  {
    "text": "account of actual explanation for a data for a data point belonging to a data distribution you can deploy it to",
    "start": "272940",
    "end": "279240"
  },
  {
    "text": "produce counterfactual explanation for many data points and has hence it's amortized",
    "start": "279240",
    "end": "285060"
  },
  {
    "text": "and since we are also designing the transition function for this reinforcement learning agent it can be",
    "start": "285060",
    "end": "291419"
  },
  {
    "text": "modified to account for feature-wise constraints and the causal constraints which are other properties that we",
    "start": "291419",
    "end": "296880"
  },
  {
    "text": "talked about so what is reinforcement learning agent",
    "start": "296880",
    "end": "302220"
  },
  {
    "text": "because we are going to use it here so it's one of the main branches in machine learning where there's this this is",
    "start": "302220",
    "end": "307740"
  },
  {
    "text": "Agent which is in this case a mouse which is gonna take some actions in the",
    "start": "307740",
    "end": "312900"
  },
  {
    "text": "environment it's present and when it takes some action it modifies its state the environment responds to this action",
    "start": "312900",
    "end": "320460"
  },
  {
    "text": "by giving a positive or A negative reward and the goal of a reinforcement learning agent is to learn a policy that can",
    "start": "320460",
    "end": "327419"
  },
  {
    "text": "maximize the sum of positive rewards over the length of an episode",
    "start": "327419",
    "end": "332600"
  },
  {
    "text": "so how would this look for count of actual explanations so consider the case",
    "start": "333180",
    "end": "338400"
  },
  {
    "text": "you have a binary classifier similar to the example that we were previously talking about the loan example",
    "start": "338400",
    "end": "344340"
  },
  {
    "text": "and your features right now are in the blue side because they are not being approved for the loan and the goal is",
    "start": "344340",
    "end": "349740"
  },
  {
    "text": "for them to cross the decision boundary and get approved for the loan",
    "start": "349740",
    "end": "355919"
  },
  {
    "text": "and how would uh the the setup for reinforcement learning in this case would be an action would be to modify a",
    "start": "355919",
    "end": "364979"
  },
  {
    "text": "feature either increasing or decrease it the environment is the machine learning model that is giving you a reward if you",
    "start": "364979",
    "end": "371699"
  },
  {
    "text": "indeed cross the decision boundary otherwise if you are still on the negative side you will not get any reward and an agent in this case is the",
    "start": "371699",
    "end": "379139"
  },
  {
    "text": "reinforcement learning agent that's trying a bunch of actions by modifying features and trying to cross the",
    "start": "379139",
    "end": "385500"
  },
  {
    "text": "decision boundary and now my friend Karthik is gonna talk about the implementation the system",
    "start": "385500",
    "end": "391979"
  },
  {
    "text": "design and about the results thank you Soho so yeah now we're going",
    "start": "391979",
    "end": "397020"
  },
  {
    "text": "to talk a little bit about the implementation and details of fast CFE and what Frameworks are used to build",
    "start": "397020",
    "end": "402479"
  },
  {
    "text": "the algorithm to scale it into our platform as well as build out the algorithm",
    "start": "402479",
    "end": "407819"
  },
  {
    "text": "so the two Frameworks we focused on were openai gym and Ray plus RL lib each of",
    "start": "407819",
    "end": "413280"
  },
  {
    "text": "these Frameworks have a different purpose in designing fastier feed we've utilize open aigim to Define our",
    "start": "413280",
    "end": "419580"
  },
  {
    "text": "environment and how much reward an agent will get when it's at any given State taking some type of action we use Rey",
    "start": "419580",
    "end": "426360"
  },
  {
    "text": "and rlib to do the distributed training and distributed prediction after we've",
    "start": "426360",
    "end": "432060"
  },
  {
    "text": "defined this custom open AI gym environment in order to build a custom open AGM environment we need the",
    "start": "432060",
    "end": "438060"
  },
  {
    "text": "following four things the first is the observation space which can also be considered the full possible set of",
    "start": "438060",
    "end": "444000"
  },
  {
    "text": "States this can be defined using a variety of different types of data types and we're going to see an example of it",
    "start": "444000",
    "end": "449340"
  },
  {
    "text": "in a couple of slides the next thing we need to Define is the action space the action space is also the full set of",
    "start": "449340",
    "end": "455340"
  },
  {
    "text": "possible actions an agent can take this is similarly defined by the same types of data types that you can use to define",
    "start": "455340",
    "end": "460860"
  },
  {
    "text": "the observation space next we have to define a reset function which is a policy for starting once an",
    "start": "460860",
    "end": "467099"
  },
  {
    "text": "episode is completed and finally we need to define a step function which is how much a reward will the environment",
    "start": "467099",
    "end": "473340"
  },
  {
    "text": "produce if an agent takes an action a at State s it will also produce the next",
    "start": "473340",
    "end": "478620"
  },
  {
    "text": "state that the agent will go to once it has completed that action so for",
    "start": "478620",
    "end": "485039"
  },
  {
    "text": "fast CFE this is actually one of our most descriptive functions and we're going to take a look at it in a couple of slides",
    "start": "485039",
    "end": "491479"
  },
  {
    "text": "so in this next section I'm going to be providing a couple of code Snippets about how we Define the different parts",
    "start": "492180",
    "end": "497340"
  },
  {
    "text": "of the openai gym environment in the context of fast CFE so for Simplicity",
    "start": "497340",
    "end": "502379"
  },
  {
    "text": "I'll also be using a toy Credit Data set which has three generic features income credit score and education",
    "start": "502379",
    "end": "508979"
  },
  {
    "text": "so to Define our observation space we are going to first of all normalize",
    "start": "508979",
    "end": "516000"
  },
  {
    "text": "all our features to be between negative one and one and we found that this is very useful for training and kind of",
    "start": "516000",
    "end": "521339"
  },
  {
    "text": "containing the ranges of these variables um and we Define it using this data type",
    "start": "521339",
    "end": "527399"
  },
  {
    "text": "called a box which allows us to Define continuous State spaces and we defined it to be between negative one and one",
    "start": "527399",
    "end": "533100"
  },
  {
    "text": "for our three feature dimensions um next we can Define our action space",
    "start": "533100",
    "end": "539279"
  },
  {
    "text": "um and for our action space we kind of utilize two different data types we utilize the Tuple to combine two",
    "start": "539279",
    "end": "545820"
  },
  {
    "text": "discrete actions so in our case we are the first part of our action will Define",
    "start": "545820",
    "end": "550860"
  },
  {
    "text": "which feature index we want to move along so for example if we select one or two and that's the index for a credit",
    "start": "550860",
    "end": "557160"
  },
  {
    "text": "credit score or income or education the second part is simply binary do we want",
    "start": "557160",
    "end": "562380"
  },
  {
    "text": "to increase along that Axis or decrease along that axis cool so moving on we need to also Define",
    "start": "562380",
    "end": "570060"
  },
  {
    "text": "our reset function which is kind of the policy for how do we Define an initial point during training and prediction",
    "start": "570060",
    "end": "576779"
  },
  {
    "text": "so to do this we have it done in separate ways for predict and train for train we'll simply sample our initial",
    "start": "576779",
    "end": "583500"
  },
  {
    "text": "training data set for the negatively classified data points while for prediction we can simply input that",
    "start": "583500",
    "end": "589260"
  },
  {
    "text": "starting point and then initialize the custom environment with that data point as the starting point",
    "start": "589260",
    "end": "595940"
  },
  {
    "text": "finally we move on to the step function which is again the most descriptive function for fast CFE so we want to",
    "start": "596519",
    "end": "602339"
  },
  {
    "text": "define a function that when given an action and a current state it will be able to produce the reward that the",
    "start": "602339",
    "end": "608820"
  },
  {
    "text": "environment the environment will be able to produce the award and the next state the agent will be in so let's walk",
    "start": "608820",
    "end": "613920"
  },
  {
    "text": "through this code a little slowly on the left side here we are first going to get from the action and the feature index",
    "start": "613920",
    "end": "619740"
  },
  {
    "text": "that we talked about before and the second part is going to be are we going to increase along that Axis or decrease",
    "start": "619740",
    "end": "625140"
  },
  {
    "text": "along that axis next we quickly Define some constants for the reward and if",
    "start": "625140",
    "end": "630180"
  },
  {
    "text": "we've completed the episode on the right side we see that we want to Define what our new state will be so",
    "start": "630180",
    "end": "636420"
  },
  {
    "text": "we'll simply index into that feature index that we got before and we will increment it or decrement it based on",
    "start": "636420",
    "end": "643200"
  },
  {
    "text": "what the second part was I would also like to note that we have put negative 0.05 or 0.05 here this can",
    "start": "643200",
    "end": "649700"
  },
  {
    "text": "arbitrarily be defined as anything we just chose it because we have normalized all our features to be between negative",
    "start": "649700",
    "end": "655140"
  },
  {
    "text": "one and one next we're going to get our classifier reward which is kind of um an",
    "start": "655140",
    "end": "660540"
  },
  {
    "text": "approximation for how close we are to our decision boundary we will next get the manifold distance loss which",
    "start": "660540",
    "end": "665640"
  },
  {
    "text": "represents how this data point looks compared to the training data set and we'll combine these two return that as",
    "start": "665640",
    "end": "671220"
  },
  {
    "text": "the reward to the client as well as the next state that the agent will be in",
    "start": "671220",
    "end": "678320"
  },
  {
    "text": "so um now that we've um defined what the define what the",
    "start": "679380",
    "end": "685320"
  },
  {
    "text": "environment custom open AGM environment is we want to define a model that is able to generate counterfactual",
    "start": "685320",
    "end": "691019"
  },
  {
    "text": "explanations so what this would look like again for our toy Credit Data set example may be increasing income by ten",
    "start": "691019",
    "end": "696839"
  },
  {
    "text": "thousand dollars and increase in education by one year in order to do so we want to produce an",
    "start": "696839",
    "end": "702959"
  },
  {
    "text": "approximately optimal policy which would return the best action at State s that",
    "start": "702959",
    "end": "708000"
  },
  {
    "text": "maximizes our future expected reward and our model will and in order to do so",
    "start": "708000",
    "end": "714060"
  },
  {
    "text": "we kind of utilize deep reinforcement learning and deep reinforcement learning has some benefits where first of all",
    "start": "714060",
    "end": "719519"
  },
  {
    "text": "it's faster training times because instead of trying to learn this policy um the exact optimal policy we kind of",
    "start": "719519",
    "end": "725399"
  },
  {
    "text": "approximate it using a neural network second we have Easy distribution we are able to distribute these training jobs",
    "start": "725399",
    "end": "730800"
  },
  {
    "text": "and prediction jobs across many different nodes in our cluster and lastly we want to have near real",
    "start": "730800",
    "end": "736320"
  },
  {
    "text": "inference time so once we've finished our training the prediction part is very very fast",
    "start": "736320",
    "end": "742160"
  },
  {
    "text": "so we obviously used Rey and rlib as our tools and Frameworks to build out fast",
    "start": "742200",
    "end": "748800"
  },
  {
    "text": "CFE and in general Ray and rlib provide a very easy API in Python to distribute",
    "start": "748800",
    "end": "755040"
  },
  {
    "text": "machine learning tasks and these reinforcement learning tasks in a kubernetes cluster and on a single node",
    "start": "755040",
    "end": "760200"
  },
  {
    "text": "so our Liv has a variety of different optimization algorithms right out of the box and provides configurations that you",
    "start": "760200",
    "end": "767040"
  },
  {
    "text": "can use for the specific Optimizer as well as the generic generic RL based tasks",
    "start": "767040",
    "end": "772380"
  },
  {
    "text": "and for our use case we decided to use the proximal policy optimization which is a new gradient based technique uh we",
    "start": "772380",
    "end": "779279"
  },
  {
    "text": "used it because it has a nice trade-off of ease of use scalability and wall-time efficiency",
    "start": "779279",
    "end": "785760"
  },
  {
    "text": "yeah and now we want to talk a little bit about our RL lib wrapper can we wrap some of the code around the native",
    "start": "785760",
    "end": "792240"
  },
  {
    "text": "libraries that RL lib provides and we're going to start with the train side here again sorry for the small font but the",
    "start": "792240",
    "end": "798660"
  },
  {
    "text": "first thing we need to do is Define and register our custom open AI gym environment inside rlbib and this allows",
    "start": "798660",
    "end": "805440"
  },
  {
    "text": "our lib to find the environment and be able to create an agent that can interact in this environment uh the next",
    "start": "805440",
    "end": "810779"
  },
  {
    "text": "part is we're going to kind of set these hyper parameters and these hyper parameters are useful because it decides",
    "start": "810779",
    "end": "815880"
  },
  {
    "text": "how many episodes the model should use before batching them together to train the neural network we Define what our",
    "start": "815880",
    "end": "822060"
  },
  {
    "text": "Optimizer OB we use PPO for this case and again we can use any kind of Optimizer from the RL lib standard and",
    "start": "822060",
    "end": "828180"
  },
  {
    "text": "now we can go through our training Loop and through every training Loop we'll train the model and we want to see that",
    "start": "828180",
    "end": "833940"
  },
  {
    "text": "our mean reward is increasing and the maximum amount is 100 is just defined by",
    "start": "833940",
    "end": "838980"
  },
  {
    "text": "our custom environment and we also want to see the number of steps it takes to actually achieve",
    "start": "838980",
    "end": "845100"
  },
  {
    "text": "achieve that as decreasing over the training loop as well so new now moving on to the",
    "start": "845100",
    "end": "850560"
  },
  {
    "text": "predictrapper side um basically we want to be able to get a",
    "start": "850560",
    "end": "855660"
  },
  {
    "text": "counter factual explanation given any initial starting point so in order to do so we can reset the custom environment",
    "start": "855660",
    "end": "861779"
  },
  {
    "text": "using that initial starting point and for every single step it takes we can compute that best action through the",
    "start": "861779",
    "end": "867959"
  },
  {
    "text": "saved and trained model note we have a maximum number of steps so after 50 steps we decide that we",
    "start": "867959",
    "end": "874200"
  },
  {
    "text": "cannot find a counter factual explanation and terminate the loop",
    "start": "874200",
    "end": "879720"
  },
  {
    "text": "cool so now we can show a brief demo about how some of these Concepts look in",
    "start": "879720",
    "end": "885420"
  },
  {
    "text": "an actual jupyter notebook and how we can kind of utilize this out of the box",
    "start": "885420",
    "end": "890600"
  },
  {
    "text": "perfect okay yeah so let's do our basic Imports",
    "start": "899699",
    "end": "905220"
  },
  {
    "text": "um so I think generically what we're trying to show here in this demo is how we can use fast CFE for a simple German",
    "start": "905220",
    "end": "912300"
  },
  {
    "text": "credit data set so here we're going to do our Basics we're going to load in our CSV file",
    "start": "912300",
    "end": "918120"
  },
  {
    "text": "we're going to break up train test split and we have also previously created a",
    "start": "918120",
    "end": "923399"
  },
  {
    "text": "binary classifier which is a random forest model um so this is all here",
    "start": "923399",
    "end": "929639"
  },
  {
    "text": "so now with just these artifacts and a cup a little bit of extra metadata which",
    "start": "929639",
    "end": "935699"
  },
  {
    "text": "we talked about with the feature constraints as well as feature relationships we can specify what these",
    "start": "935699",
    "end": "942480"
  },
  {
    "text": "are while inputting inputting into fast CFE so for example for our German Credit",
    "start": "942480",
    "end": "947820"
  },
  {
    "text": "Data set some of the immutable features are you know personal status number of people being liable Etc",
    "start": "947820",
    "end": "954560"
  },
  {
    "text": "features that cannot decrease or something like age and we have a bunch of categorical features as well and this",
    "start": "954560",
    "end": "960300"
  },
  {
    "text": "allows us to be more prescriptive instead of just kind of cheating everything as continuous",
    "start": "960300",
    "end": "966959"
  },
  {
    "text": "uh we have to input our predict function as uh well and we can now create an",
    "start": "966959",
    "end": "972060"
  },
  {
    "text": "Optimizer and this Optimizer what it'll do is basically create that RL lib wrapper that we talked about in the",
    "start": "972060",
    "end": "977100"
  },
  {
    "text": "presentation and with that we can train um I don't want to run it live it takes a few minutes to boot up Ray if you've",
    "start": "977100",
    "end": "983699"
  },
  {
    "text": "used it right before you definitely know that and um uh yeah so you just want to kind of look",
    "start": "983699",
    "end": "988920"
  },
  {
    "text": "through kind of these epochs and how training is going and we can see that we want this mean reward to increase over",
    "start": "988920",
    "end": "995040"
  },
  {
    "text": "time and we can kind of see that pattern of course with some Randomness and similarly we want to see with the number",
    "start": "995040",
    "end": "1001339"
  },
  {
    "text": "of steps to be decreasing over time and we again start to see that again with some randomness",
    "start": "1001339",
    "end": "1008420"
  },
  {
    "text": "um we have some code to use cyber parameter tuning using Ray tune as well we're going to skip this part not really",
    "start": "1008420",
    "end": "1014480"
  },
  {
    "text": "relevant um yeah we have we can evaluate how well our model did with some baseline metrics",
    "start": "1014480",
    "end": "1020959"
  },
  {
    "text": "we talk about validity and we're going to talk about that more in a few slides",
    "start": "1020959",
    "end": "1026298"
  },
  {
    "text": "um yeah so here I want to showcase like how a counter factual explanation will actually look for you know a data",
    "start": "1026299",
    "end": "1033438"
  },
  {
    "text": "scientist or a machine learning engineer who's trying to evaluate some type of machine learning model so what we'll see",
    "start": "1033439",
    "end": "1039438"
  },
  {
    "text": "is that what are the variables that actually change that need to be changed in order to flip your decision and we",
    "start": "1039439",
    "end": "1045500"
  },
  {
    "text": "want to see what the new value is versus the old value and we can see this for a variety of different counterfactual",
    "start": "1045500",
    "end": "1052100"
  },
  {
    "text": "um or a variety of different inference points I would also also like to note that these are categorical variables so",
    "start": "1052100",
    "end": "1058640"
  },
  {
    "text": "this numeric difference is doesn't really mean much it's just that this is a different kind of category",
    "start": "1058640",
    "end": "1065480"
  },
  {
    "text": "awesome and now we can go back to the slides and talk a little bit more about our system design and how we in used",
    "start": "1065480",
    "end": "1072799"
  },
  {
    "text": "array and rlib to actually incorporate into our Arthur platform",
    "start": "1072799",
    "end": "1078340"
  },
  {
    "text": "awesome yeah so we had three major goals when trying",
    "start": "1079580",
    "end": "1084980"
  },
  {
    "text": "to figure out what tool to use to deploy this algorithm and the first one was to be able to deploy a service that",
    "start": "1084980",
    "end": "1091160"
  },
  {
    "text": "computes these counterfactual explanations in real time in the Arthur platform users send their send us their",
    "start": "1091160",
    "end": "1097700"
  },
  {
    "text": "inference data and we want to be able to stand up a service in our back end that when sending a new inference data point",
    "start": "1097700",
    "end": "1103940"
  },
  {
    "text": "it can return that counterfectionation very fast second we want to be able to deploy a",
    "start": "1103940",
    "end": "1109520"
  },
  {
    "text": "service that is able to train fast CFE and this training should include training on a single node with multiple",
    "start": "1109520",
    "end": "1115220"
  },
  {
    "text": "CPUs as well as on a kubernetes cluster as we are a kubernetes native application and then thirdly we want to reduce the",
    "start": "1115220",
    "end": "1121700"
  },
  {
    "text": "runtime for using Ray and rlib through distribution over kind of a simple Stables Baseline implementation",
    "start": "1121700",
    "end": "1129080"
  },
  {
    "text": "so we break up our design into two phases training and inference and in this slide we're really going to discuss",
    "start": "1129500",
    "end": "1135140"
  },
  {
    "text": "our training architecture and in the first step our user sends us their reference data and model artifact",
    "start": "1135140",
    "end": "1141500"
  },
  {
    "text": "just how kind of how we showcased on our demo and both those things will be sent into the Arthur platform and they'll be",
    "start": "1141500",
    "end": "1147500"
  },
  {
    "text": "bundled together to kick off a kubernetes native scheduler for our fast CFG training job with this training job",
    "start": "1147500",
    "end": "1153679"
  },
  {
    "text": "we are able to utilize Rey and actually do the training on a cluster a kubernetes cluster using multiple nodes",
    "start": "1153679",
    "end": "1160700"
  },
  {
    "text": "and we also like to note that this is a one-time training job so we're able to utilize as many nodes as possible to",
    "start": "1160700",
    "end": "1166640"
  },
  {
    "text": "really speed up that training so once training is done we save that model artifact to S3 and deploy our fast",
    "start": "1166640",
    "end": "1173059"
  },
  {
    "text": "CFE service using that model artifact now this makes inference very easy",
    "start": "1173059",
    "end": "1178460"
  },
  {
    "text": "because we already have our deployed fast CFE service our clients send us all their inference data that gets ingested",
    "start": "1178460",
    "end": "1184100"
  },
  {
    "text": "again into the Arthur platform and now since we've already stood up our fast CFE service any data scientist or ml",
    "start": "1184100",
    "end": "1190520"
  },
  {
    "text": "engineer who is using Arthur can simply click on compute counterfactual explanation and get back that explanation in real time",
    "start": "1190520",
    "end": "1198400"
  },
  {
    "text": "so now we can talk a little bit about the results and the two major results are kind of",
    "start": "1199340",
    "end": "1204799"
  },
  {
    "text": "metrics we wanted to look at were performance metrics and namely how did Fast CFE compare against other",
    "start": "1204799",
    "end": "1209900"
  },
  {
    "text": "state-of-the-art counterfactual explainers and the second is we want to look at the training time what benefit",
    "start": "1209900",
    "end": "1215120"
  },
  {
    "text": "did we gain from using Ray and rlib for and how it helped us distribute across",
    "start": "1215120",
    "end": "1220520"
  },
  {
    "text": "our cluster for this we kind of looked at five major five data sets three were open source",
    "start": "1220520",
    "end": "1225679"
  },
  {
    "text": "and two were company sensitive um the three that were open source for German credit adult credit and credit",
    "start": "1225679",
    "end": "1231440"
  },
  {
    "text": "default and the other two were kind of Inc with encoded with several different types of variables uh I'd like to also",
    "start": "1231440",
    "end": "1238160"
  },
  {
    "text": "say that these data sets represented a wider range of number of data points as well as number of features",
    "start": "1238160",
    "end": "1245240"
  },
  {
    "text": "so the first thing we look at is our performance metrics and we look at validity and mean inference time",
    "start": "1245240",
    "end": "1250840"
  },
  {
    "text": "validity is the total number of counterfactual explanations actually found divided by the total number of",
    "start": "1250840",
    "end": "1257120"
  },
  {
    "text": "negatively label cost data points so this number is actually represented as a percentage the second thing is we wanted",
    "start": "1257120",
    "end": "1262820"
  },
  {
    "text": "to look at that mean inference time which is the average time it takes to compute a single counterfactual explanation",
    "start": "1262820",
    "end": "1267980"
  },
  {
    "text": "so looking across our three open source data sets we see that fast CFE gets a",
    "start": "1267980",
    "end": "1273500"
  },
  {
    "text": "validity of at least 97.3 percent and actually achieves a perfect score in the",
    "start": "1273500",
    "end": "1278960"
  },
  {
    "text": "German Credit Data set at 99.9 in the credit default we also see that our mean",
    "start": "1278960",
    "end": "1284059"
  },
  {
    "text": "inference time is an order of magnitude faster across all the different methods",
    "start": "1284059",
    "end": "1290260"
  },
  {
    "text": "next part is we wanted to look at the kind of training time and how the training time of using rain are a loop",
    "start": "1291260",
    "end": "1297620"
  },
  {
    "text": "compared against a simple Stables Baseline implementation we see again Across The Stables Baseline",
    "start": "1297620",
    "end": "1303620"
  },
  {
    "text": "implementation in some cases took very long and did not even finish on data sets and we noticed that when using Ray",
    "start": "1303620",
    "end": "1309500"
  },
  {
    "text": "and rlib on a single node a core machine we were able to achieve 6X time and",
    "start": "1309500",
    "end": "1315380"
  },
  {
    "text": "train times savings so in conclusion we hope this presentation",
    "start": "1315380",
    "end": "1322220"
  },
  {
    "text": "provided you a good overview of the following concepts one counterfactual explanations what they are and how are",
    "start": "1322220",
    "end": "1327919"
  },
  {
    "text": "they useful in industrial and explainability applications to reinforcement learning how do we",
    "start": "1327919",
    "end": "1333260"
  },
  {
    "text": "Implement a production level reinforcement learning model into an existing system and the third is the",
    "start": "1333260",
    "end": "1338780"
  },
  {
    "text": "power of distribution how we can achieve tremendous savings by using scalable and distributed reinforcement learning",
    "start": "1338780",
    "end": "1344000"
  },
  {
    "text": "Frameworks such as RL lib for more details we have a couple of blog posts",
    "start": "1344000",
    "end": "1349940"
  },
  {
    "text": "um you can QR code if you want the first one talks about more of the algorithmic side of counterfactual explanations and",
    "start": "1349940",
    "end": "1356240"
  },
  {
    "text": "fast CFE well the second one is more of a practitioner guide of how to define a custom opening gym environment and",
    "start": "1356240",
    "end": "1363080"
  },
  {
    "text": "Define some basic RL wrappers um I can leave this up also but thank you",
    "start": "1363080",
    "end": "1369380"
  },
  {
    "text": "these are emails any questions",
    "start": "1369380",
    "end": "1374440"
  },
  {
    "text": "um all right thank you can you talk a little bit more about how your",
    "start": "1377179",
    "end": "1382940"
  },
  {
    "text": "classifier uh relates to the reward signal you",
    "start": "1382940",
    "end": "1389480"
  },
  {
    "text": "mentioned that was tree based um How do you derive the reward from the distance in",
    "start": "1389480",
    "end": "1395780"
  },
  {
    "text": "that case and also um your RL lab agent's performance will be",
    "start": "1395780",
    "end": "1403039"
  },
  {
    "text": "dependent on the accuracy of that classifier have you separated those benchmarks and",
    "start": "1403039",
    "end": "1409299"
  },
  {
    "text": "uh sort of done an analysis of of those independently thank you okay yeah so",
    "start": "1409299",
    "end": "1415940"
  },
  {
    "text": "I'll get the first part you get the second part okay I can answer the first word about the reward system so uh like",
    "start": "1415940",
    "end": "1423380"
  },
  {
    "text": "uh you can train like for example if you're using a neural network you could simply if the if the data point is",
    "start": "1423380",
    "end": "1430940"
  },
  {
    "text": "indeed a crossing the decision boundary then you can give a reward of say plus one or whatever and if it's not crossing",
    "start": "1430940",
    "end": "1437000"
  },
  {
    "text": "the decision boundary then you can penalize a small a small amount for",
    "start": "1437000",
    "end": "1442100"
  },
  {
    "text": "taking a step but not give any any other reward and that's a generic reward thing",
    "start": "1442100",
    "end": "1447620"
  },
  {
    "text": "you can use with any model so that makes it a model agnostic you can use trees or",
    "start": "1447620",
    "end": "1453620"
  },
  {
    "text": "any any um like model for that case uh and",
    "start": "1453620",
    "end": "1460700"
  },
  {
    "text": "rewards I can again you don't have a varying magnitude of rewards you just have a plus one minus one type of reward",
    "start": "1460700",
    "end": "1467299"
  },
  {
    "text": "uh plus one and uh like minus 0.05 something like that for like I see a few",
    "start": "1467299",
    "end": "1473659"
  },
  {
    "text": "just like just penalize for a step so that we also wanted to encourage to cross the distance around as soon as",
    "start": "1473659",
    "end": "1479840"
  },
  {
    "text": "possible so you want to minimize that steps so there's a small penalty for taking a step and not crossing the",
    "start": "1479840",
    "end": "1484940"
  },
  {
    "text": "decision boundary and if you indeed cross the written body there's a large positive word so yeah that's not a huge",
    "start": "1484940",
    "end": "1491059"
  },
  {
    "text": "disparity over there yeah okay thank you one one more question I'll tack on here",
    "start": "1491059",
    "end": "1496520"
  },
  {
    "text": "um immutable features how does that uh work with the agents in terms of reward",
    "start": "1496520",
    "end": "1503360"
  },
  {
    "text": "if if an intermutable feature is selected by the agents uh how do you",
    "start": "1503360",
    "end": "1508520"
  },
  {
    "text": "account for that it's the exact same thing like if you if if the agent chooses immune immutable features there",
    "start": "1508520",
    "end": "1514640"
  },
  {
    "text": "will be no change in the state and there will be a small negative reward okay uh",
    "start": "1514640",
    "end": "1519679"
  },
  {
    "text": "and you had another question about RL which I think Karthik can answer how does the performance of the classifier",
    "start": "1519679",
    "end": "1525500"
  },
  {
    "text": "relate to your benchmarks oh oh yeah I uh I don't think I have an answer for",
    "start": "1525500",
    "end": "1533720"
  },
  {
    "text": "that on the top of my mind uh I think the better the model is the more uh",
    "start": "1533720",
    "end": "1539179"
  },
  {
    "text": "performant would be the reinforcement learning agent but even if the accuracy of this model is like low it the",
    "start": "1539179",
    "end": "1547220"
  },
  {
    "text": "reinforcement learning agent can still learn how to cross the decision boundary so ah the accuracy of the machine learning",
    "start": "1547220",
    "end": "1554600"
  },
  {
    "text": "the binary classifier would not affect the efficacy of the reinforcement",
    "start": "1554600",
    "end": "1560360"
  },
  {
    "text": "launching agent much",
    "start": "1560360",
    "end": "1563380"
  },
  {
    "text": "sorry just for the question before the question is do you train a different policy per classifier yeah okay so do",
    "start": "1567200",
    "end": "1575779"
  },
  {
    "text": "you um have you played it all with warm starting policies like doing some sort of transfer learning so you can cut down your training time we haven't actually",
    "start": "1575779",
    "end": "1582740"
  },
  {
    "text": "tried that it's a good uh suggestion good to know yeah sorry one other question um I was",
    "start": "1582740",
    "end": "1589400"
  },
  {
    "text": "wondering also about algorithm choice so ppos generates a stochastic policy I was",
    "start": "1589400",
    "end": "1595940"
  },
  {
    "text": "wondering if you have a sense of how undecided the policy is at any given stage for instance what's the relative",
    "start": "1595940",
    "end": "1602179"
  },
  {
    "text": "weight on the logits uh in one step like would it rather is it like very",
    "start": "1602179",
    "end": "1608299"
  },
  {
    "text": "preferentially going One Direction or is it kind of undecided yeah so I when I was kind of running a lot of experiments",
    "start": "1608299",
    "end": "1614779"
  },
  {
    "text": "with this um earlier on is we noticed that like it is very it directly does relate to",
    "start": "1614779",
    "end": "1622220"
  },
  {
    "text": "the Logics that are being outputted from the classifier right so I think that was one of the things that we talked about is in practice like",
    "start": "1622220",
    "end": "1629179"
  },
  {
    "text": "using the Logics from the classifier allow the agent to slightly understand where it's moving relatively to this",
    "start": "1629179",
    "end": "1634700"
  },
  {
    "text": "decision boundary because you know you get the positive and negative class um predicted probabilities so we're",
    "start": "1634700",
    "end": "1640700"
  },
  {
    "text": "testing a lot with that I think um warm starting would be a great suggestion for like speeding up",
    "start": "1640700",
    "end": "1645860"
  },
  {
    "text": "especially if you're using like a same class type of problem like credit uh risk or something",
    "start": "1645860",
    "end": "1651620"
  },
  {
    "text": "thank you uh there's I think one more question there yeah",
    "start": "1651620",
    "end": "1658419"
  },
  {
    "text": "hi thanks for a great talk uh so I'm not super familiar with counterfactuals so please correct me if",
    "start": "1659059",
    "end": "1665840"
  },
  {
    "text": "I'm misrepresenting something um but at least at a high level the goal",
    "start": "1665840",
    "end": "1671240"
  },
  {
    "text": "of changing the input as little as possible to change the classifier's",
    "start": "1671240",
    "end": "1676460"
  },
  {
    "text": "output feels very similar to me to adversarial training so I was wondering",
    "start": "1676460",
    "end": "1681919"
  },
  {
    "text": "what you see as the benefits of training this reinforcement learning model to do",
    "start": "1681919",
    "end": "1687140"
  },
  {
    "text": "something similar which feels like a more complex approach to me at least on",
    "start": "1687140",
    "end": "1692179"
  },
  {
    "text": "the face of things yeah I can uh I can answer that question so uh that's a very good point that it's",
    "start": "1692179",
    "end": "1699140"
  },
  {
    "text": "uh it has very uh a lot of similarities with adversary learning and that comes often in the literature and on Twitter",
    "start": "1699140",
    "end": "1705500"
  },
  {
    "text": "all the time uh the main difference lies in how the features are being altered so",
    "start": "1705500",
    "end": "1712159"
  },
  {
    "text": "in adversary learning you would not consider which feature is immutable or something in this case you would want to",
    "start": "1712159",
    "end": "1718220"
  },
  {
    "text": "give that advice to a person so you have to play some real world constraints so that's one thing but to answer the more",
    "start": "1718220",
    "end": "1724760"
  },
  {
    "text": "in-depth question which you ask like why did you choose reinforcement learning and not something let's say just like fgsm like agreed and descent the the",
    "start": "1724760",
    "end": "1731900"
  },
  {
    "text": "answer for that question is that using such a there have been many approaches like if you remember the table that I",
    "start": "1731900",
    "end": "1737480"
  },
  {
    "text": "presented those most previous approaches use such a thing like a gradient descent based approach it's just that it's not",
    "start": "1737480",
    "end": "1743240"
  },
  {
    "text": "amortized so you have to run the grid descent for every new data point whereas in this case you do not need to so",
    "start": "1743240",
    "end": "1749299"
  },
  {
    "text": "that's the motivation behind using our does it answer your question awesome",
    "start": "1749299",
    "end": "1756880"
  },
  {
    "text": "um this might be this is more speculative can you um do you guys have an uncertainty estimate or a",
    "start": "1761899",
    "end": "1767059"
  },
  {
    "text": "distribution of outcomes that um or have thought of a way of doing that so you know for example in your credit",
    "start": "1767059",
    "end": "1773779"
  },
  {
    "text": "score assignment you say it could be you know instead of thirty thousand it was forty thousand dollars was the income but there's also I guess a distribution",
    "start": "1773779",
    "end": "1780020"
  },
  {
    "text": "of that plus other features um have you thought of a way to do that uh yeah they could be like obviously the",
    "start": "1780020",
    "end": "1786740"
  },
  {
    "text": "multiple counter faster explanation that could exist for a data point so you could weigh them according to",
    "start": "1786740",
    "end": "1792919"
  },
  {
    "text": "different uh Norms or something we just choose the one that had the minimum L2 Norm I believe",
    "start": "1792919",
    "end": "1800240"
  },
  {
    "text": "from the data point but they could be you could produce multiple kind of actual explanation and present all of",
    "start": "1800240",
    "end": "1805580"
  },
  {
    "text": "them and give the user the freedom to choose whatever they prefer uh yeah",
    "start": "1805580",
    "end": "1812860"
  },
  {
    "text": "okay thank you everyone thank you so much [Applause]",
    "start": "1824059",
    "end": "1831399"
  }
]