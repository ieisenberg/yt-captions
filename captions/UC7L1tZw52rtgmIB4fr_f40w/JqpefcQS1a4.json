[
  {
    "text": "hello and welcome to our talk today this is a talk by me",
    "start": "5160",
    "end": "12000"
  },
  {
    "text": "my name is Ali and Sasha my partner here we are from LinkedIn where our mission",
    "start": "12000",
    "end": "17520"
  },
  {
    "text": "is to create an equal opportunity for every member of the global workspace essentially that's all of you",
    "start": "17520",
    "end": "24960"
  },
  {
    "text": "um and in this in this Mission we we are driven by Ai and with today we're going",
    "start": "24960",
    "end": "30180"
  },
  {
    "text": "to present uh how inference graphs are being used at LinkedIn",
    "start": "30180",
    "end": "35340"
  },
  {
    "text": "um yeah you want to introduce yourself quickly my name is Sasha uh yeah we're here",
    "start": "35340",
    "end": "41219"
  },
  {
    "text": "we'll work on uh AI platform at LinkedIn Welcome to our talk",
    "start": "41219",
    "end": "47640"
  },
  {
    "text": "all right so we're going to walk through this agenda today we're going to talk about some of the challenges that we are",
    "start": "47640",
    "end": "53760"
  },
  {
    "text": "we see um that we want to solve for how racer",
    "start": "53760",
    "end": "58920"
  },
  {
    "text": "is helping us achieve and solve those problems uh we're also going to be talking about uh as we build on top of",
    "start": "58920",
    "end": "65640"
  },
  {
    "text": "Reserve where we see that we can contribute to a growing rate",
    "start": "65640",
    "end": "70799"
  },
  {
    "text": "um and ecosystem and then we're going to be just highlighting some solutions and success",
    "start": "70799",
    "end": "77400"
  },
  {
    "text": "stories ah just to start off this is our",
    "start": "77400",
    "end": "82740"
  },
  {
    "text": "inference stack um very standard infrastack you would see probably find anywhere",
    "start": "82740",
    "end": "89100"
  },
  {
    "text": "um you know if you go to any any company you will probably find this you basically have a serving system a",
    "start": "89100",
    "end": "96060"
  },
  {
    "text": "serving uh you know service which is uh which is capable of serving in models uh you know there is",
    "start": "96060",
    "end": "103860"
  },
  {
    "text": "it it has a bunch of features it gets a",
    "start": "103860",
    "end": "109140"
  },
  {
    "text": "request inference request you know does a bunch of feature producing hand it over to inference server like",
    "start": "109140",
    "end": "116579"
  },
  {
    "text": "Triton and um in tensorflow serving is what we use as a back end and the inference is done on that stack so the",
    "start": "116579",
    "end": "124140"
  },
  {
    "text": "middleware service is really doing feature massaging and feature fetching and then handing it over to infinite server",
    "start": "124140",
    "end": "130140"
  },
  {
    "text": "um we do have multi-tenancies we have many users uh you know our multi-tenancy",
    "start": "130140",
    "end": "136500"
  },
  {
    "text": "story today is literally making a copy of a stateless sort of a service so each",
    "start": "136500",
    "end": "142440"
  },
  {
    "text": "each server each pod or each each instance of the services replicated n number of times for multiple tenants ah",
    "start": "142440",
    "end": "150660"
  },
  {
    "text": "we do have various use cases running on top of these there are LM use cases that run in this configuration but there are",
    "start": "150660",
    "end": "156900"
  },
  {
    "text": "also personalization models which are also a significant part of our infrastructure which are on top of this",
    "start": "156900",
    "end": "161940"
  },
  {
    "text": "infrastructure we also have a lot of experimentation",
    "start": "161940",
    "end": "167640"
  },
  {
    "text": "and Ensemble inference use cases where you know we want to do a b testing and",
    "start": "167640",
    "end": "173099"
  },
  {
    "text": "various other type of experimentation on top of on top of the infrastructure that",
    "start": "173099",
    "end": "178379"
  },
  {
    "text": "we provide and this story is pretty straightforward which is again you know",
    "start": "178379",
    "end": "183420"
  },
  {
    "text": "create multiple tenants of one experiment type and then another experiment type in you",
    "start": "183420",
    "end": "189480"
  },
  {
    "text": "know manage this Army of multi-tenants even the experimentation universe and",
    "start": "189480",
    "end": "195959"
  },
  {
    "text": "then the client system switches between these tenants depending on the experiment configuration",
    "start": "195959",
    "end": "201620"
  },
  {
    "text": "on The Ensemble side we have a similar story where you know we host various",
    "start": "201620",
    "end": "206940"
  },
  {
    "text": "type of models in their own tenants and then switch between them on the client",
    "start": "206940",
    "end": "212519"
  },
  {
    "text": "side which means that you if you want to do model one and then do some business logic and then do model two you would",
    "start": "212519",
    "end": "217980"
  },
  {
    "text": "make round trips to the inference stack ah our inference ecosystem is made up of",
    "start": "217980",
    "end": "225299"
  },
  {
    "text": "online near line and offline use cases and these are very important um for us there are a lot of in addition",
    "start": "225299",
    "end": "232739"
  },
  {
    "text": "to just pure online use cases the nearline use cases add a lot of value to LinkedIn in terms of computing",
    "start": "232739",
    "end": "238440"
  },
  {
    "text": "embeddings and features that are then used for various other use cases uh same",
    "start": "238440",
    "end": "243599"
  },
  {
    "text": "thing with offline offline has two applications you can think of one is to validate your model so once you've",
    "start": "243599",
    "end": "249180"
  },
  {
    "text": "trained a model you want to validate it and see if it if it is behaving the way you expect it to behave so that's one use case of offline inference the other",
    "start": "249180",
    "end": "255659"
  },
  {
    "text": "use case of offline inferences actually generating uh you know you know running",
    "start": "255659",
    "end": "261120"
  },
  {
    "text": "a large compute on on data and then generating embeddings for that",
    "start": "261120",
    "end": "266840"
  },
  {
    "text": "ah some of the limitations that we see in our current stack is basically you know obviously this ml in space is is",
    "start": "267240",
    "end": "275520"
  },
  {
    "text": "constantly changing and you know having a custom stack adds uh you know friction",
    "start": "275520",
    "end": "281639"
  },
  {
    "text": "in that path to adopt to new changes so we want to build something that is uh more fluid and we can adopt faster uh to",
    "start": "281639",
    "end": "289139"
  },
  {
    "text": "upcoming Trends we do have this homogeneous versus heterogeneous we are",
    "start": "289139",
    "end": "294600"
  },
  {
    "text": "moving to a world where gpus become very important and there are different type of gpus that that come into play and we",
    "start": "294600",
    "end": "301320"
  },
  {
    "text": "want to build infrastructure that can play over a heterogeneous infrastructure and not just have like homogeneous",
    "start": "301320",
    "end": "308240"
  },
  {
    "text": "multi-tenance replicated a number of times uh we want to solve for auto scaling we",
    "start": "308240",
    "end": "313919"
  },
  {
    "text": "want to have better use resource utilization and then you know we want a story where",
    "start": "313919",
    "end": "319800"
  },
  {
    "text": "you could move from your offline inference world to an online inference seamlessly so today if you have to if",
    "start": "319800",
    "end": "328139"
  },
  {
    "text": "just scroll back a slide if you have to move from your offline inference stack to an online inference stack it's a",
    "start": "328139",
    "end": "334080"
  },
  {
    "text": "significant uplift you have to rewrite a lot of your code to to work in the online stack and and differently because",
    "start": "334080",
    "end": "340979"
  },
  {
    "text": "it works very differently in the offline stack and this is where we started",
    "start": "340979",
    "end": "346860"
  },
  {
    "text": "experimenting with racer and started to play around an idea and how racer can help us solve these problems so yes so",
    "start": "346860",
    "end": "353820"
  },
  {
    "text": "basically you want to simplify this this management of this complex architecture heterogeneous architecture we want to uh",
    "start": "353820",
    "end": "360240"
  },
  {
    "text": "you know have better integration with open source so that we can get new and upcoming things faster into the stack",
    "start": "360240",
    "end": "366900"
  },
  {
    "text": "and we're also looking for cross-language interoperability because a lot of LinkedIn infrastructure is",
    "start": "366900",
    "end": "372419"
  },
  {
    "text": "built on top of java whereas the new and upcoming things in the machine learning world are mostly happening on the python",
    "start": "372419",
    "end": "378960"
  },
  {
    "text": "side so we want to have an ability to interoperate between these these languages",
    "start": "378960",
    "end": "385680"
  },
  {
    "text": "ah so what we have designed is this um reserved based inference runtime and",
    "start": "385680",
    "end": "394259"
  },
  {
    "text": "essentially what you can see here is a heterogeneous infrastructure where uh",
    "start": "394259",
    "end": "400500"
  },
  {
    "text": "you know each graph or each inference graph is deployed as an independent unit",
    "start": "400500",
    "end": "408440"
  },
  {
    "text": "you can replicate them in differently uh you know for example if you're running a",
    "start": "408440",
    "end": "414660"
  },
  {
    "text": "b experiments and you want your control to be scaled out at 50 percent in your your other experiments scale out a",
    "start": "414660",
    "end": "422039"
  },
  {
    "text": "different other percentages you could deploy all of that in in a single infrastructure and scale them up and",
    "start": "422039",
    "end": "427740"
  },
  {
    "text": "down at will we are integrating this with our our you know experimentation",
    "start": "427740",
    "end": "433259"
  },
  {
    "text": "framework so that you can do seamless and you know without any significant",
    "start": "433259",
    "end": "439800"
  },
  {
    "text": "issues roll out on production so this graph basically shows you that nodes are",
    "start": "439800",
    "end": "446220"
  },
  {
    "text": "split across multiple different inference graphs",
    "start": "446220",
    "end": "451680"
  },
  {
    "text": "um and um and then each individual graph is also split into multiple nodes and",
    "start": "451680",
    "end": "457680"
  },
  {
    "text": "these nodes could be different based on whether you are running a CPU computer a GPU compute this helps us isolate",
    "start": "457680",
    "end": "465900"
  },
  {
    "text": "um you know workloads that are data intensive let's say on CPU but on um and",
    "start": "465900",
    "end": "471479"
  },
  {
    "text": "and run only the inference on gpus which helps uh in better utilization uh we",
    "start": "471479",
    "end": "477000"
  },
  {
    "text": "obviously have integrated all of this with our load balancing and rest of the stack",
    "start": "477000",
    "end": "482819"
  },
  {
    "text": "um we have also ported the same design over to our offline inference runtime so",
    "start": "482819",
    "end": "489120"
  },
  {
    "text": "basically you can take the same inference graph that you would Define in an online world and run it and offline",
    "start": "489120",
    "end": "494880"
  },
  {
    "text": "or you could take it an offline graph that you define it offline and then you can run it online and it works exactly",
    "start": "494880",
    "end": "501479"
  },
  {
    "text": "the same way it's the only difference is that you are deploying one in the serve uh reserve and the other in in record",
    "start": "501479",
    "end": "510120"
  },
  {
    "text": "uh some of the challenges that we see that we are trying to figure out how to solve this basically",
    "start": "510120",
    "end": "516380"
  },
  {
    "text": "adopting Ray at LinkedIn ecosystem is done in a very special way so we're",
    "start": "516380",
    "end": "522300"
  },
  {
    "text": "adopting Ray into that TLS configs um you know how do you run Java and python together logging crash reporting",
    "start": "522300",
    "end": "529140"
  },
  {
    "text": "all of that into linked infrastructure code rollout and fault tolerance is another thing that we are looking at",
    "start": "529140",
    "end": "535040"
  },
  {
    "text": "very significantly um Auto scaling and dynamic actor",
    "start": "535040",
    "end": "540660"
  },
  {
    "text": "pooling is another thing that we're looking at so basically we want to see if we can build uh build actor pooling",
    "start": "540660",
    "end": "546600"
  },
  {
    "text": "in a way that you know it can it can be shared across multiple multiple",
    "start": "546600",
    "end": "552240"
  },
  {
    "text": "deployments and and you could have like more more advanced routing between them",
    "start": "552240",
    "end": "557760"
  },
  {
    "text": "uh we're also looking for In-Place graph updates and rate limiting is another thing that we're looking at with that I will hand it over to my",
    "start": "557760",
    "end": "566339"
  },
  {
    "text": "partner Sasha here who will talk about the rest of the concept of an inference graph right",
    "start": "566339",
    "end": "571500"
  },
  {
    "text": "um again my name is Sasha uh I should say it's uh it feels really amazing to participate in uh you know face-to-face",
    "start": "571500",
    "end": "578700"
  },
  {
    "text": "event I think it's my first in like four years it's it's great to have interaction uh so yeah so uh we",
    "start": "578700",
    "end": "587580"
  },
  {
    "text": "are uh AI platform at LinkedIn and our",
    "start": "587580",
    "end": "592980"
  },
  {
    "text": "users are AI Engineers who train models and create those workflows based on",
    "start": "592980",
    "end": "598920"
  },
  {
    "text": "models so you might be wondering during the talk like what is inference graph so we want to get a little bit into that so",
    "start": "598920",
    "end": "606779"
  },
  {
    "text": "this is what you see here is the simplest uh interest graph basically",
    "start": "606779",
    "end": "612779"
  },
  {
    "text": "just inputs that are fed to a model and then outputs are sent back to the users that's so that's",
    "start": "612779",
    "end": "620220"
  },
  {
    "text": "the simplest possible thing now one one other example is for uh when when you",
    "start": "620220",
    "end": "626940"
  },
  {
    "text": "need to like pre-process some data and do some post processing for example for",
    "start": "626940",
    "end": "632100"
  },
  {
    "text": "image processing right so you you would want to do some some scaling I don't know color management",
    "start": "632100",
    "end": "638240"
  },
  {
    "text": "or some some quantization so that that would be pre-processing so then you do",
    "start": "638240",
    "end": "643440"
  },
  {
    "text": "inference then you do post processing that that'll be output so that's another example of influence graph",
    "start": "643440",
    "end": "649260"
  },
  {
    "text": "and then the uh yet another example with features",
    "start": "649260",
    "end": "655440"
  },
  {
    "text": "where before even calling the model you just go into feature store retrieve the",
    "start": "655440",
    "end": "661620"
  },
  {
    "text": "featured data and hand those features to the model and then after after you did",
    "start": "661620",
    "end": "667140"
  },
  {
    "text": "inference you you want to record what features were uh were there for this",
    "start": "667140",
    "end": "673260"
  },
  {
    "text": "particular inference uh and maybe you're doing Logan wait or or for for debugging",
    "start": "673260",
    "end": "679620"
  },
  {
    "text": "uh you need this information so this is yet another way of doing this and uh uh",
    "start": "679620",
    "end": "689279"
  },
  {
    "text": "uh like last example in this series uh multiple models uh it's like Ensemble",
    "start": "689279",
    "end": "697019"
  },
  {
    "text": "Ensemble models multiple models uh using the same Feature Feature fetching uh",
    "start": "697019",
    "end": "705180"
  },
  {
    "text": "process to kind of save time and then blending the model results so these are",
    "start": "705180",
    "end": "710880"
  },
  {
    "text": "all examples of things that we want to do beyond our users I want to do Beyond",
    "start": "710880",
    "end": "717839"
  },
  {
    "text": "Simple model inference right and uh we need we need something to to do that",
    "start": "717839",
    "end": "725339"
  },
  {
    "text": "and as we talked as Ali talked about we use array",
    "start": "725339",
    "end": "730500"
  },
  {
    "text": "framework for that and he went into a lot of like low level detail of how this is uh this is",
    "start": "730500",
    "end": "737880"
  },
  {
    "text": "happening but uh our goal is to make uh not only",
    "start": "737880",
    "end": "743399"
  },
  {
    "text": "the the global Workforce but our uh AI Engineers uh productive and for that we",
    "start": "743399",
    "end": "750360"
  },
  {
    "text": "want them to work at the higher level and without like getting into low-level details of how uh how it is implemented",
    "start": "750360",
    "end": "758459"
  },
  {
    "text": "so that's uh that's why we created a platform or framework that we call",
    "start": "758459",
    "end": "765180"
  },
  {
    "text": "Proxima that's an internal name uh this is a platform for high level",
    "start": "765180",
    "end": "772920"
  },
  {
    "text": "authoring of inference graphs so basically the uh it allows them to",
    "start": "772920",
    "end": "778760"
  },
  {
    "text": "independently author and uh deploy and run the whole process Monitor and",
    "start": "778760",
    "end": "784860"
  },
  {
    "text": "everything um of of this inference graphs so so the graphs I showed you it's it's a still",
    "start": "784860",
    "end": "791519"
  },
  {
    "text": "simple like in in real world examples there will be much more complicated than that",
    "start": "791519",
    "end": "797700"
  },
  {
    "text": "um so of course we want this to be dual integrated with all the development",
    "start": "797700",
    "end": "803339"
  },
  {
    "text": "ecosystem use the CI CD pipelines and experimentation systems and feature",
    "start": "803339",
    "end": "808920"
  },
  {
    "text": "infrastructure and all the pieces of the infrastructure that that we need for me to make it work uh we need to scale on",
    "start": "808920",
    "end": "815820"
  },
  {
    "text": "CPUs and gpus and uh gpus as as we will",
    "start": "815820",
    "end": "822300"
  },
  {
    "text": "know uh really hard to get an expensive so we want to utilize them to to its",
    "start": "822300",
    "end": "827399"
  },
  {
    "text": "full power and we need a lot of technical understanding how to do that",
    "start": "827399",
    "end": "833459"
  },
  {
    "text": "um again that's that's for platform to handle and uh kind of unique or maybe",
    "start": "833459",
    "end": "838980"
  },
  {
    "text": "not unique but but special situation for LinkedIn is that a lot of services and infra is written in Java language and uh",
    "start": "838980",
    "end": "847620"
  },
  {
    "text": "a lot of ml is written in Python and Native like C plus plus and we need we",
    "start": "847620",
    "end": "853079"
  },
  {
    "text": "need the easy way to uh to integrate those parts and Ray is very helpful with",
    "start": "853079",
    "end": "860220"
  },
  {
    "text": "that so this is uh this is the problems how we uh we were trying to sell so this is",
    "start": "860220",
    "end": "866880"
  },
  {
    "text": "just an idea of uh how this whole stack looks like uh there's a kubernetes at the bottom",
    "start": "866880",
    "end": "874279"
  },
  {
    "text": "and there's a control plane something like parallel to I would say good Cube",
    "start": "874279",
    "end": "881519"
  },
  {
    "text": "rate but it is a specific for kind of LinkedIn applications and the",
    "start": "881519",
    "end": "886680"
  },
  {
    "text": "microservice architectures and things like that so that's uh that's our custom control play and we run a reserve on top",
    "start": "886680",
    "end": "895320"
  },
  {
    "text": "of this control plane and then we run our uh like a framework",
    "start": "895320",
    "end": "902760"
  },
  {
    "text": "runtime uh infrastruct runtime on top of that and deploy using our deployment",
    "start": "902760",
    "end": "909720"
  },
  {
    "text": "platform so we in order to configure those dags",
    "start": "909720",
    "end": "915000"
  },
  {
    "text": "we chose to use Python which we feel is intuitive for AI engineers",
    "start": "915000",
    "end": "921019"
  },
  {
    "text": "and then it is it is integrated into cicd process and that allows AI",
    "start": "921019",
    "end": "929579"
  },
  {
    "text": "Engineers to uh to do their work independently and don't depend on like production application Engineers to uh",
    "start": "929579",
    "end": "936839"
  },
  {
    "text": "to deploy the the work or their model so there we go so basically everybody does",
    "start": "936839",
    "end": "942300"
  },
  {
    "text": "their own like things and everybody's Uncle um so and then obviously a credibility",
    "start": "942300",
    "end": "949139"
  },
  {
    "text": "tools profiling monitoring configuration uh it all it is",
    "start": "949139",
    "end": "955320"
  },
  {
    "text": "all taken care of by the platform so this is an example of what what do we",
    "start": "955320",
    "end": "962820"
  },
  {
    "text": "mean what do we mean by intuitive uh python API for the graph so this is",
    "start": "962820",
    "end": "968459"
  },
  {
    "text": "the graph that we already talked about and this is uh basically how this graph",
    "start": "968459",
    "end": "973920"
  },
  {
    "text": "will be written in our uh python API uh you can see that you just create a",
    "start": "973920",
    "end": "981779"
  },
  {
    "text": "bunch of objects which represent models that needs to be run inference and then",
    "start": "981779",
    "end": "988800"
  },
  {
    "text": "then you combine uh features of those models so that you",
    "start": "988800",
    "end": "994800"
  },
  {
    "text": "can run the feature fetch operation once to save time",
    "start": "994800",
    "end": "1000019"
  },
  {
    "text": "and then you run so this I I don't know",
    "start": "1000019",
    "end": "1005899"
  },
  {
    "text": "pointer here but in the last line Nia M1 you see like in in the brackets that's",
    "start": "1005899",
    "end": "1011000"
  },
  {
    "text": "that's the inference of the first model and then M2 with the brackets is uh is",
    "start": "1011000",
    "end": "1016579"
  },
  {
    "text": "the inference of the second model and then this mosa blended with the weights",
    "start": "1016579",
    "end": "1024160"
  },
  {
    "text": "so that's that's basically looks like python uh it is basically it it",
    "start": "1024160",
    "end": "1031660"
  },
  {
    "text": "implements the some very limited subset of python uh and I'll I'll talk a little bit uh",
    "start": "1031660",
    "end": "1039740"
  },
  {
    "text": "why right so so there's a process that this whole",
    "start": "1039740",
    "end": "1045079"
  },
  {
    "text": "system kind of goes through so AI engineer authors their graph and this",
    "start": "1045079",
    "end": "1050320"
  },
  {
    "text": "myservice.pi and then there's a build process that builds two things uh it",
    "start": "1050320",
    "end": "1056600"
  },
  {
    "text": "builds this deployment artifact so basically that it takes this graph definition from Python and converts it",
    "start": "1056600",
    "end": "1063919"
  },
  {
    "text": "to some some like high level definition or internal representation uh what I'd say that as a protobar file",
    "start": "1063919",
    "end": "1071660"
  },
  {
    "text": "and that file could be loaded in a different system why do we need to convert python into something because we",
    "start": "1071660",
    "end": "1078020"
  },
  {
    "text": "have Java runtime that wants to load this and execute this and Java can",
    "start": "1078020",
    "end": "1084200"
  },
  {
    "text": "execute python um so and and or or it could be python runtime",
    "start": "1084200",
    "end": "1090559"
  },
  {
    "text": "so that's one thing we were creating during the build the other part is kind",
    "start": "1090559",
    "end": "1095720"
  },
  {
    "text": "of straightforward it is uh Proto like grpc Proto file that represents the",
    "start": "1095720",
    "end": "1103220"
  },
  {
    "text": "service so if it is if it is this function definition this",
    "start": "1103220",
    "end": "1108260"
  },
  {
    "text": "uh function signature that takes these two two inputs uh there will be equivalent",
    "start": "1108260",
    "end": "1114740"
  },
  {
    "text": "uh product definition uh for that for the signature that is just created and",
    "start": "1114740",
    "end": "1120080"
  },
  {
    "text": "then we go through the normal grpc process of compiling this into stops and in any chosen language and application",
    "start": "1120080",
    "end": "1127220"
  },
  {
    "text": "Engineers can call our service using those stops so that's the process how this system",
    "start": "1127220",
    "end": "1133220"
  },
  {
    "text": "works so here's our status it's uh uh it's",
    "start": "1133220",
    "end": "1139820"
  },
  {
    "text": "under octave development for for a few months but we already have production use cases and we'll talk about it a",
    "start": "1139820",
    "end": "1145880"
  },
  {
    "text": "little bit and uh one of the question to to the communities",
    "start": "1145880",
    "end": "1152260"
  },
  {
    "text": "uh how much interest is is how how interesting is this uh a whole thing",
    "start": "1152260",
    "end": "1158419"
  },
  {
    "text": "that allows you to uh to express the graphs and run them deploy them and run",
    "start": "1158419",
    "end": "1164960"
  },
  {
    "text": "them and whether we should be trying to publish this code as an open source so",
    "start": "1164960",
    "end": "1171260"
  },
  {
    "text": "that's that's kind of open question for us so we'll appreciate any feedback on that",
    "start": "1171260",
    "end": "1176600"
  },
  {
    "text": "uh and uh this is one of the examples of production use cases that uh will run",
    "start": "1176600",
    "end": "1182900"
  },
  {
    "text": "with this stack uh this application that",
    "start": "1182900",
    "end": "1188000"
  },
  {
    "text": "allows recruiters uh basically",
    "start": "1188000",
    "end": "1193240"
  },
  {
    "text": "become more productive save time and improve the engagement uh with uh well",
    "start": "1193240",
    "end": "1200480"
  },
  {
    "text": "over there uh um with their user or what people they",
    "start": "1200480",
    "end": "1205760"
  },
  {
    "text": "communicate with uh using using AI models that are running on the the stack",
    "start": "1205760",
    "end": "1213260"
  },
  {
    "text": "on the ray basically that's all we wanted to talk about",
    "start": "1213260",
    "end": "1219260"
  },
  {
    "text": "any questions",
    "start": "1219260",
    "end": "1222039"
  },
  {
    "text": "okay can you repeat the question I'm sorry",
    "start": "1242539",
    "end": "1246640"
  },
  {
    "text": "uh the question is do we support pre-processing steps in the inference graph and the in the API that we just",
    "start": "1259419",
    "end": "1266419"
  },
  {
    "text": "showed and how do we handle serialization deserialization for that you want to answer them sure uh so uh so",
    "start": "1266419",
    "end": "1274660"
  },
  {
    "text": "pre-processing post-processing steps all of that is supported via what we call",
    "start": "1274660",
    "end": "1280160"
  },
  {
    "text": "proximal operators so basically this is a one these are functions that are built",
    "start": "1280160",
    "end": "1285440"
  },
  {
    "text": "into the system and uh in the graph in in the graph definition",
    "start": "1285440",
    "end": "1292039"
  },
  {
    "text": "that I showed you it looks like uh just python call to a function right but uh",
    "start": "1292039",
    "end": "1297559"
  },
  {
    "text": "but behind the scenes it's a little bit more more involved than function but not much and uh you can have those operators",
    "start": "1297559",
    "end": "1305780"
  },
  {
    "text": "for any types uh types of uh uh functions including pre-processing pre-processing like for images for",
    "start": "1305780",
    "end": "1313340"
  },
  {
    "text": "example there's there's a bunch of like more or less standard methods right so so I think it could be covered with uh",
    "start": "1313340",
    "end": "1319880"
  },
  {
    "text": "with the limited number of operators and uh that that can be done uh this way uh",
    "start": "1319880",
    "end": "1327380"
  },
  {
    "text": "but uh add to what Sasha said is basically we support two type of use cases one is these generalized",
    "start": "1327380",
    "end": "1333740"
  },
  {
    "text": "pre-processing we mentioned uh image based processing which can be generalized we can have a library",
    "start": "1333740",
    "end": "1339500"
  },
  {
    "text": "already integrated into the runtime and therefore you don't have to really code you just have to point to us saying this",
    "start": "1339500",
    "end": "1345320"
  },
  {
    "text": "is what I want to use right so that's one use case the other use case is like more specialized when you you want to do some really ah low level processing",
    "start": "1345320",
    "end": "1353000"
  },
  {
    "text": "which is only specific to your use case we allow we support that as well where you can ah you know contribute sort of",
    "start": "1353000",
    "end": "1359179"
  },
  {
    "text": "uh your own operator and and that can only be run for you uh in them so",
    "start": "1359179",
    "end": "1364400"
  },
  {
    "text": "there's a inference graph which is like a pointer to functions and then there is this operator code which is the code",
    "start": "1364400",
    "end": "1369620"
  },
  {
    "text": "that houses the logic for running the pre-processing post processing how do we handle serialization deserialization",
    "start": "1369620",
    "end": "1375140"
  },
  {
    "text": "that's a very good question um that's one of the things that we are trying to figure out how to juice out",
    "start": "1375140",
    "end": "1380840"
  },
  {
    "text": "the juice out Rays Primitives so we at to start with",
    "start": "1380840",
    "end": "1387679"
  },
  {
    "text": "we're using obviously object stores so we just store Ray uh you know use Ray",
    "start": "1387679",
    "end": "1392960"
  },
  {
    "text": "Object Store to store and get the value now the the uh the money basically is in",
    "start": "1392960",
    "end": "1398600"
  },
  {
    "text": "the place like co-locating these things so that you Object Store copy over the",
    "start": "1398600",
    "end": "1403640"
  },
  {
    "text": "network can be saved and all those things so we're working on how we can co-locate operators and how we can collocate the graph parts of the graph",
    "start": "1403640",
    "end": "1410480"
  },
  {
    "text": "so that we can save serialization cost but there are cases where you will have to pay the serialization cost and we",
    "start": "1410480",
    "end": "1416659"
  },
  {
    "text": "will have to work on like figuring out the fastest way to get things out",
    "start": "1416659",
    "end": "1421760"
  },
  {
    "text": "yeah there was some somebody behind oh yeah go ahead",
    "start": "1421760",
    "end": "1426820"
  },
  {
    "text": "General solution it might not be very specialized like that yeah I'll just repeat the question so that I think the",
    "start": "1450020",
    "end": "1456620"
  },
  {
    "text": "question is that when we've moved to using Ray did it help us in speeding up or slowing down",
    "start": "1456620",
    "end": "1463700"
  },
  {
    "text": "whatever whichever it is right the basically overhead of the the platform what what are the additional overhead is",
    "start": "1463700",
    "end": "1470299"
  },
  {
    "text": "that what you're asking for yeah",
    "start": "1470299",
    "end": "1473440"
  },
  {
    "text": "match your previous QPS yes ah so the second part of the",
    "start": "1475640",
    "end": "1483020"
  },
  {
    "text": "question is were we able to match our latency requirements and uh in the old system versus the new system especially",
    "start": "1483020",
    "end": "1488419"
  },
  {
    "text": "right ah so there are uh you know there are two use cases here one is the llm use case we started building the llm use",
    "start": "1488419",
    "end": "1494900"
  },
  {
    "text": "case on top of the new infrastructure so we really don't have a baseline there but the Baseline that we're trying to",
    "start": "1494900",
    "end": "1500840"
  },
  {
    "text": "measure against is that overhead of the platform right so most of your llm time is spent inside the GPU doing the",
    "start": "1500840",
    "end": "1507799"
  },
  {
    "text": "inference so we want to like figure out what what is that additional overhead that Ray is or the stack that we have",
    "start": "1507799",
    "end": "1515900"
  },
  {
    "text": "built on top of Ray is creating and we don't have those numbers here but that is minimalistic right now like it's it's",
    "start": "1515900",
    "end": "1522140"
  },
  {
    "text": "in milliseconds so it's like really tiny amount of overhead compared to the llm use case now the other use case that we",
    "start": "1522140",
    "end": "1528919"
  },
  {
    "text": "have which is uh you know the personalization models which are different than llam sorry so the",
    "start": "1528919",
    "end": "1534380"
  },
  {
    "text": "personalization models uh latency is super important and the model the the",
    "start": "1534380",
    "end": "1539720"
  },
  {
    "text": "overhead from the model itself is not that much in that space yes we are we",
    "start": "1539720",
    "end": "1546620"
  },
  {
    "text": "are removing some of the layers that we have created for the llm hosting to to",
    "start": "1546620",
    "end": "1552200"
  },
  {
    "text": "run the LPM or the the personalization models more intricately so that we don't",
    "start": "1552200",
    "end": "1558140"
  },
  {
    "text": "pay the cost of serialization decentralization I think that's where the cost really is to juice out the",
    "start": "1558140",
    "end": "1564260"
  },
  {
    "text": "survey cost um it's most most likely it will go into into Java runtime well I did that yeah",
    "start": "1564260",
    "end": "1572360"
  },
  {
    "text": "go ahead yes",
    "start": "1572360",
    "end": "1574960"
  },
  {
    "text": "got it so ah repeating the question I think you're asking about what what's the",
    "start": "1602059",
    "end": "1607940"
  },
  {
    "text": "what's the first part of your question again latency for what",
    "start": "1607940",
    "end": "1612100"
  },
  {
    "text": "yeah so the latency difference between this infrastructure for llm and AI again",
    "start": "1616640",
    "end": "1622640"
  },
  {
    "text": "I think I'll just repeat what I just said before you know it is really about saving the the cost of survey uh then",
    "start": "1622640",
    "end": "1630320"
  },
  {
    "text": "the network trips and and stuff like that which will uh which will you know really impact your performance right so",
    "start": "1630320",
    "end": "1637520"
  },
  {
    "text": "uh as long as you can minimize that that is what the where the game is and uh we",
    "start": "1637520",
    "end": "1644900"
  },
  {
    "text": "are like our new stack like the way we are uh using Ray Ray offers you Primitives around uh you know Affinity",
    "start": "1644900",
    "end": "1652580"
  },
  {
    "text": "based placements and various other ways to minimize the cost of ah transmitting",
    "start": "1652580",
    "end": "1658400"
  },
  {
    "text": "data over uh you know over the network for a long period of",
    "start": "1658400",
    "end": "1664220"
  },
  {
    "text": "time you can localize actors and you can use uh Object Store that way to to to minimize the cost right so those are The",
    "start": "1664220",
    "end": "1671000"
  },
  {
    "text": "Primitives that that we are looking at the the actual cost of as I said the the llm hosting at least right the cost that",
    "start": "1671000",
    "end": "1677900"
  },
  {
    "text": "we have seen is minimal uh from the overhead that we have seen so far uh I think it I think there's more color to",
    "start": "1677900",
    "end": "1683960"
  },
  {
    "text": "this question because it gets complex when you have like an ensemble models with multiple models running one after",
    "start": "1683960",
    "end": "1690320"
  },
  {
    "text": "the other or we have large amount of pre-processing post processing running before and after the model inference and",
    "start": "1690320",
    "end": "1695539"
  },
  {
    "text": "then I think the measurements will be different but for the vanilla use cases we do not see a lot of ah overhead from",
    "start": "1695539",
    "end": "1702380"
  },
  {
    "text": "the from the framework hmm",
    "start": "1702380",
    "end": "1706539"
  },
  {
    "text": "first do Therapies",
    "start": "1712960",
    "end": "1717220"
  },
  {
    "text": "let's say the excitement so you don't care about things yes",
    "start": "1725200",
    "end": "1732080"
  },
  {
    "text": "yes so the question is that um for large models um the millisecond overhead is probably",
    "start": "1733039",
    "end": "1739640"
  },
  {
    "text": "not that significant but for models that care about performance a lot millisecond differences would matter a lot uh or",
    "start": "1739640",
    "end": "1746779"
  },
  {
    "text": "mentally second overhead would matter a lot that is absolutely right and that's why we have like two different Stacks if",
    "start": "1746779",
    "end": "1752240"
  },
  {
    "text": "you saw a lot of that is coming from python because survey in Python is costly",
    "start": "1752240",
    "end": "1757779"
  },
  {
    "text": "and you know we are looking at different solutions one of the things that we already have uh in the in the works is a",
    "start": "1757779",
    "end": "1764240"
  },
  {
    "text": "Java based runtime on top of Ray which is which is not no python in the in the call Path it is all Java in Java out",
    "start": "1764240",
    "end": "1771380"
  },
  {
    "text": "like it's just working on top of java we're also trying to see if we can invest in Native C plus plus base actors",
    "start": "1771380",
    "end": "1779299"
  },
  {
    "text": "and all that to reduce the the overhead of survey from that point of view so",
    "start": "1779299",
    "end": "1785299"
  },
  {
    "text": "that's our play there we haven't solved all the problems yet but we have seen the difference between Java and python",
    "start": "1785299",
    "end": "1791840"
  },
  {
    "text": "definitely as a significant cost saving that happens and also like just to add",
    "start": "1791840",
    "end": "1797899"
  },
  {
    "text": "here that you know most of LinkedIn stack is in Java so uh and any overhead for example feature fetching is a sort",
    "start": "1797899",
    "end": "1804500"
  },
  {
    "text": "of an overhead for us because if Pi if python if we do inference in Python but feature fetching in Java then it becomes",
    "start": "1804500",
    "end": "1811220"
  },
  {
    "text": "automatically very costly so we are investing in Java side a lot to basically reduce that cost as well",
    "start": "1811220",
    "end": "1818860"
  },
  {
    "text": "yeah um so the question is how are we scheduling offline because we are using",
    "start": "1829100",
    "end": "1834260"
  },
  {
    "text": "the same grapher of online and offline so how is scheduling offline that's the question and answer the question is basically what we do is on the offline",
    "start": "1834260",
    "end": "1841520"
  },
  {
    "text": "world we use Ray data sets to scale out the workload what you are deploying is",
    "start": "1841520",
    "end": "1846919"
  },
  {
    "text": "an actor pool basically and in the online world you are deploying a raid",
    "start": "1846919",
    "end": "1852320"
  },
  {
    "text": "serve deployment which is an equivalent of the actor pool uh in the in the online world so you scale out based on",
    "start": "1852320",
    "end": "1858320"
  },
  {
    "text": "the actor pool strategy so scale out scaling out configurations are different for online and offline but the inference",
    "start": "1858320",
    "end": "1864380"
  },
  {
    "text": "graph remains exactly the same so the the user experience if you think about the user experiences literally they go",
    "start": "1864380",
    "end": "1870080"
  },
  {
    "text": "and write the inference graph once and then they're able to just because of our framework they're able to run it both in",
    "start": "1870080",
    "end": "1875419"
  },
  {
    "text": "on online and offline well go ahead last question go ahead",
    "start": "1875419",
    "end": "1884140"
  },
  {
    "text": "uh I think it's more to do with the um that just how how the languages work",
    "start": "1898640",
    "end": "1904880"
  },
  {
    "text": "uh I I'm pretty sure the global interpreter GI also plays a role there",
    "start": "1904880",
    "end": "1910640"
  },
  {
    "text": "uh we haven't really measured uh removing Gil what is the cost saving we haven't measured that if you were asking",
    "start": "1910640",
    "end": "1916880"
  },
  {
    "text": "that question um but I think it's even if you remove that the overhead is still going to be",
    "start": "1916880",
    "end": "1922399"
  },
  {
    "text": "still significant yes is serialization in Python uh is you know it goes through uh does a lot",
    "start": "1922399",
    "end": "1929659"
  },
  {
    "text": "in Python and uh does a lot of like lookups and field lookups and all that",
    "start": "1929659",
    "end": "1934779"
  },
  {
    "text": "and that could be very costly",
    "start": "1934779",
    "end": "1939279"
  }
]