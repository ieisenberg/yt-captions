[
  {
    "start": "0",
    "end": "107000"
  },
  {
    "text": "[Music]",
    "start": "170",
    "end": "14799"
  },
  {
    "text": "hi",
    "start": "14799",
    "end": "15200"
  },
  {
    "text": "my name is clark zenzo and i'm going to",
    "start": "15200",
    "end": "17039"
  },
  {
    "text": "be talking to you today about",
    "start": "17039",
    "end": "18880"
  },
  {
    "text": "per epoch shuffling data loaders",
    "start": "18880",
    "end": "22160"
  },
  {
    "text": "and specifically the one that ray has",
    "start": "22160",
    "end": "24160"
  },
  {
    "text": "developed",
    "start": "24160",
    "end": "26560"
  },
  {
    "text": "uh first what's this talk about i had a",
    "start": "28480",
    "end": "30880"
  },
  {
    "text": "high level quick overview",
    "start": "30880",
    "end": "33440"
  },
  {
    "text": "we're going to both motivate and talk",
    "start": "33440",
    "end": "35200"
  },
  {
    "text": "about the specific challenges of",
    "start": "35200",
    "end": "37040"
  },
  {
    "text": "creating a shuffling data loader that",
    "start": "37040",
    "end": "39520"
  },
  {
    "text": "can handle",
    "start": "39520",
    "end": "40160"
  },
  {
    "text": "large data scales and distributed model",
    "start": "40160",
    "end": "42320"
  },
  {
    "text": "training",
    "start": "42320",
    "end": "43520"
  },
  {
    "text": "and we're also going to describe uh what",
    "start": "43520",
    "end": "45680"
  },
  {
    "text": "we feel is a compelling rape based",
    "start": "45680",
    "end": "47520"
  },
  {
    "text": "solution",
    "start": "47520",
    "end": "49200"
  },
  {
    "text": "first a little bit about me i'm a",
    "start": "49200",
    "end": "51920"
  },
  {
    "text": "software engineer at any scale i've been",
    "start": "51920",
    "end": "53520"
  },
  {
    "text": "here for",
    "start": "53520",
    "end": "54079"
  },
  {
    "text": "a little bit over three months i've been",
    "start": "54079",
    "end": "56800"
  },
  {
    "text": "working on the core array system",
    "start": "56800",
    "end": "58320"
  },
  {
    "text": "focusing on",
    "start": "58320",
    "end": "59280"
  },
  {
    "text": "large-scale data processing and then",
    "start": "59280",
    "end": "61440"
  },
  {
    "text": "also doing some work in the integration",
    "start": "61440",
    "end": "63120"
  },
  {
    "text": "space",
    "start": "63120",
    "end": "63920"
  },
  {
    "text": "like this shuffling data loader as well",
    "start": "63920",
    "end": "66159"
  },
  {
    "text": "as desk on ray",
    "start": "66159",
    "end": "68240"
  },
  {
    "text": "and before any scale i was a ray user so",
    "start": "68240",
    "end": "70960"
  },
  {
    "text": "i will",
    "start": "70960",
    "end": "71520"
  },
  {
    "text": "always fight for the user because i used",
    "start": "71520",
    "end": "73520"
  },
  {
    "text": "to be one",
    "start": "73520",
    "end": "75920"
  },
  {
    "text": "so first what exactly is a shuffling",
    "start": "75920",
    "end": "78640"
  },
  {
    "text": "data loader",
    "start": "78640",
    "end": "79439"
  },
  {
    "text": "and why do we need it a brief overview",
    "start": "79439",
    "end": "83119"
  },
  {
    "text": "of your typical ml pipeline",
    "start": "83119",
    "end": "85200"
  },
  {
    "text": "you typically have a data pre-processing",
    "start": "85200",
    "end": "87360"
  },
  {
    "text": "stage",
    "start": "87360",
    "end": "88240"
  },
  {
    "text": "a model training stage and a model",
    "start": "88240",
    "end": "90799"
  },
  {
    "text": "deployment stage",
    "start": "90799",
    "end": "91920"
  },
  {
    "text": "with the last also being known as model",
    "start": "91920",
    "end": "94320"
  },
  {
    "text": "serving or",
    "start": "94320",
    "end": "95119"
  },
  {
    "text": "inference and what we're interested in",
    "start": "95119",
    "end": "98240"
  },
  {
    "text": "this talk",
    "start": "98240",
    "end": "98880"
  },
  {
    "text": "since we're talking about a shuffling",
    "start": "98880",
    "end": "100479"
  },
  {
    "text": "data loader for model training we're",
    "start": "100479",
    "end": "101920"
  },
  {
    "text": "interested in this",
    "start": "101920",
    "end": "103200"
  },
  {
    "text": "data pre-processing and model training",
    "start": "103200",
    "end": "105200"
  },
  {
    "text": "stage interaction",
    "start": "105200",
    "end": "107600"
  },
  {
    "start": "107000",
    "end": "107000"
  },
  {
    "text": "uh first to dive in a little bit more",
    "start": "107600",
    "end": "110960"
  },
  {
    "text": "into this interaction typically you",
    "start": "110960",
    "end": "113840"
  },
  {
    "text": "store your data",
    "start": "113840",
    "end": "114960"
  },
  {
    "text": "in some intermediate storage such as",
    "start": "114960",
    "end": "116799"
  },
  {
    "text": "like s3 or hdfs",
    "start": "116799",
    "end": "118880"
  },
  {
    "text": "and then you load it from that store uh",
    "start": "118880",
    "end": "121680"
  },
  {
    "text": "when you are starting to train your",
    "start": "121680",
    "end": "123040"
  },
  {
    "text": "model",
    "start": "123040",
    "end": "123600"
  },
  {
    "text": "and you are going to load it from that",
    "start": "123600",
    "end": "124960"
  },
  {
    "text": "store often",
    "start": "124960",
    "end": "126880"
  },
  {
    "text": "at the start of every epoch so",
    "start": "126880",
    "end": "130160"
  },
  {
    "text": "what's the big deal why do we need like",
    "start": "130160",
    "end": "131920"
  },
  {
    "text": "a shuffling data loader or whatever",
    "start": "131920",
    "end": "133760"
  },
  {
    "text": "like loading data from a story super",
    "start": "133760",
    "end": "135920"
  },
  {
    "text": "simple right you just",
    "start": "135920",
    "end": "137360"
  },
  {
    "text": "read it that's it right uh not so fast",
    "start": "137360",
    "end": "140879"
  },
  {
    "text": "not really",
    "start": "140879",
    "end": "141920"
  },
  {
    "text": "especially when you start approaching",
    "start": "141920",
    "end": "144000"
  },
  {
    "text": "large data scales",
    "start": "144000",
    "end": "145680"
  },
  {
    "text": "uh and coupling that with wanting to do",
    "start": "145680",
    "end": "148160"
  },
  {
    "text": "distributed data parallel training",
    "start": "148160",
    "end": "150239"
  },
  {
    "text": "uh such as essentially to increase like",
    "start": "150239",
    "end": "153040"
  },
  {
    "text": "the",
    "start": "153040",
    "end": "153680"
  },
  {
    "text": "scale of data that you can handle where",
    "start": "153680",
    "end": "155519"
  },
  {
    "text": "the data maybe can't fit into a single",
    "start": "155519",
    "end": "157599"
  },
  {
    "text": "trainer node",
    "start": "157599",
    "end": "159200"
  },
  {
    "text": "or potentially to speed up training by",
    "start": "159200",
    "end": "161280"
  },
  {
    "text": "parallelizing it",
    "start": "161280",
    "end": "163519"
  },
  {
    "text": "and also a few things that make this",
    "start": "163519",
    "end": "166239"
  },
  {
    "text": "extra tricky",
    "start": "166239",
    "end": "167680"
  },
  {
    "text": "we're not loading the data just once",
    "start": "167680",
    "end": "170640"
  },
  {
    "text": "we're actually loading the data at the",
    "start": "170640",
    "end": "172000"
  },
  {
    "text": "start of every single epoch",
    "start": "172000",
    "end": "174239"
  },
  {
    "text": "so if data loading ends up becoming",
    "start": "174239",
    "end": "176640"
  },
  {
    "text": "expensive",
    "start": "176640",
    "end": "177760"
  },
  {
    "text": "you're not going to just have to absorb",
    "start": "177760",
    "end": "180159"
  },
  {
    "text": "absorb that expense",
    "start": "180159",
    "end": "181360"
  },
  {
    "text": "once at the beginning of training you're",
    "start": "181360",
    "end": "183200"
  },
  {
    "text": "going to be hitting",
    "start": "183200",
    "end": "185200"
  },
  {
    "text": "you're going to be hitting that expense",
    "start": "185200",
    "end": "186879"
  },
  {
    "text": "at the beginning of every epoch and it",
    "start": "186879",
    "end": "188560"
  },
  {
    "text": "can",
    "start": "188560",
    "end": "188959"
  },
  {
    "text": "really slow down your training so there",
    "start": "188959",
    "end": "191519"
  },
  {
    "text": "is like a tiny bit of pseudo code of",
    "start": "191519",
    "end": "193920"
  },
  {
    "text": "what usually the loop structure looks",
    "start": "193920",
    "end": "196000"
  },
  {
    "text": "like",
    "start": "196000",
    "end": "196959"
  },
  {
    "text": "for every epoch you're loading your data",
    "start": "196959",
    "end": "198640"
  },
  {
    "text": "from the store",
    "start": "198640",
    "end": "201280"
  },
  {
    "text": "we're also not just loading data into a",
    "start": "201280",
    "end": "203120"
  },
  {
    "text": "single trainer",
    "start": "203120",
    "end": "204720"
  },
  {
    "text": "we're loading partitions of the input",
    "start": "204720",
    "end": "207120"
  },
  {
    "text": "data",
    "start": "207120",
    "end": "208159"
  },
  {
    "text": "partitions of that data in the store",
    "start": "208159",
    "end": "210480"
  },
  {
    "text": "into multiple trainers",
    "start": "210480",
    "end": "212080"
  },
  {
    "text": "uh often spanning multiple machines once",
    "start": "212080",
    "end": "214640"
  },
  {
    "text": "you start getting into",
    "start": "214640",
    "end": "215760"
  },
  {
    "text": "distributed data parallel training",
    "start": "215760",
    "end": "219200"
  },
  {
    "text": "and we're also not just loading the data",
    "start": "219200",
    "end": "221519"
  },
  {
    "text": "after all",
    "start": "221519",
    "end": "222879"
  },
  {
    "text": "we're also shuffling the data at the",
    "start": "222879",
    "end": "224720"
  },
  {
    "text": "start of every epoch",
    "start": "224720",
    "end": "226080"
  },
  {
    "text": "or this is often desirable and i'll talk",
    "start": "226080",
    "end": "227920"
  },
  {
    "text": "about why that is",
    "start": "227920",
    "end": "229760"
  },
  {
    "text": "in a little bit so typically the data",
    "start": "229760",
    "end": "232400"
  },
  {
    "text": "comes through",
    "start": "232400",
    "end": "233200"
  },
  {
    "text": "a shuffler this could often be a the",
    "start": "233200",
    "end": "236319"
  },
  {
    "text": "shuffler process could be",
    "start": "236319",
    "end": "238239"
  },
  {
    "text": "its own service the shuffler process",
    "start": "238239",
    "end": "240239"
  },
  {
    "text": "could be on each of the trainers it",
    "start": "240239",
    "end": "242159"
  },
  {
    "text": "depends on the",
    "start": "242159",
    "end": "243599"
  },
  {
    "text": "data loader implementation and the",
    "start": "243599",
    "end": "246799"
  },
  {
    "text": "uh shuffled input partitions are then",
    "start": "246799",
    "end": "249840"
  },
  {
    "text": "sent off to the individual trainers",
    "start": "249840",
    "end": "251840"
  },
  {
    "text": "so a trainer will just operate on a",
    "start": "251840",
    "end": "253519"
  },
  {
    "text": "subset of the data",
    "start": "253519",
    "end": "256320"
  },
  {
    "text": "so why do we want to shuffle the data at",
    "start": "256320",
    "end": "258799"
  },
  {
    "text": "the start of every epoch",
    "start": "258799",
    "end": "259919"
  },
  {
    "text": "you know you're used to a model training",
    "start": "259919",
    "end": "261519"
  },
  {
    "text": "you shuffle your data once before",
    "start": "261519",
    "end": "263040"
  },
  {
    "text": "starting to train",
    "start": "263040",
    "end": "264240"
  },
  {
    "text": "perhaps in spark and then you load it um",
    "start": "264240",
    "end": "267520"
  },
  {
    "text": "and you know you're like cool i shuffled",
    "start": "267520",
    "end": "269360"
  },
  {
    "text": "my data i decorated the samples",
    "start": "269360",
    "end": "271520"
  },
  {
    "text": "uh so why do you want to shuffle at the",
    "start": "271520",
    "end": "273600"
  },
  {
    "text": "start of every epoch how does the model",
    "start": "273600",
    "end": "275360"
  },
  {
    "text": "training actually benefit",
    "start": "275360",
    "end": "278080"
  },
  {
    "text": "well at a high level shuffling decorates",
    "start": "278080",
    "end": "280400"
  },
  {
    "text": "samples within and across batches",
    "start": "280400",
    "end": "282240"
  },
  {
    "text": "and this is both true when shuffling",
    "start": "282240",
    "end": "284400"
  },
  {
    "text": "data before",
    "start": "284400",
    "end": "285600"
  },
  {
    "text": "starting a training run like doing a",
    "start": "285600",
    "end": "287680"
  },
  {
    "text": "single global shuffle to decorate all of",
    "start": "287680",
    "end": "289680"
  },
  {
    "text": "your samples",
    "start": "289680",
    "end": "290639"
  },
  {
    "text": "and this also stays true",
    "start": "290639",
    "end": "294000"
  },
  {
    "text": "when talking about batches across epochs",
    "start": "294000",
    "end": "298720"
  },
  {
    "text": "uh specifically for the per epoch",
    "start": "298720",
    "end": "300320"
  },
  {
    "text": "shuffling case uh",
    "start": "300320",
    "end": "301919"
  },
  {
    "text": "per epoch shuffling reduces variance",
    "start": "301919",
    "end": "303840"
  },
  {
    "text": "improves generalization",
    "start": "303840",
    "end": "305280"
  },
  {
    "text": "and prevents overfitting for batch",
    "start": "305280",
    "end": "308400"
  },
  {
    "text": "gradient descent which is obviously very",
    "start": "308400",
    "end": "310080"
  },
  {
    "text": "popular also known as mini batch",
    "start": "310080",
    "end": "311520"
  },
  {
    "text": "gradient descent",
    "start": "311520",
    "end": "313039"
  },
  {
    "text": "and many people just call it stochastic",
    "start": "313039",
    "end": "314639"
  },
  {
    "text": "gradient descent nowadays",
    "start": "314639",
    "end": "317039"
  },
  {
    "text": "by shuffling uh in between epochs",
    "start": "317039",
    "end": "320160"
  },
  {
    "text": "the batches are more representative of",
    "start": "320160",
    "end": "322639"
  },
  {
    "text": "the entire data set",
    "start": "322639",
    "end": "324320"
  },
  {
    "text": "and fundamentally improves the estimate",
    "start": "324320",
    "end": "326400"
  },
  {
    "text": "of the true full data set gradient which",
    "start": "326400",
    "end": "328160"
  },
  {
    "text": "after all is the entire point",
    "start": "328160",
    "end": "329919"
  },
  {
    "text": "of batch gradient descent",
    "start": "329919",
    "end": "333120"
  },
  {
    "text": "and this applies for both batch and",
    "start": "333120",
    "end": "334479"
  },
  {
    "text": "stochastic grainy descent the",
    "start": "334479",
    "end": "336400"
  },
  {
    "text": "grading updates on individual samples",
    "start": "336400",
    "end": "339440"
  },
  {
    "text": "are independent of the sample ordering",
    "start": "339440",
    "end": "342479"
  },
  {
    "text": "and this can be very uh very very",
    "start": "342479",
    "end": "345039"
  },
  {
    "text": "important for example if",
    "start": "345039",
    "end": "347039"
  },
  {
    "text": "if your samples end up being ordered by",
    "start": "347039",
    "end": "350560"
  },
  {
    "text": "time or label you could have some really",
    "start": "350560",
    "end": "353360"
  },
  {
    "text": "nasty hidden correlations",
    "start": "353360",
    "end": "355440"
  },
  {
    "text": "that end up uh making your uh",
    "start": "355440",
    "end": "358639"
  },
  {
    "text": "model overfit to weird esoteric",
    "start": "358639",
    "end": "362160"
  },
  {
    "text": "properties of the data set which isn't",
    "start": "362160",
    "end": "363840"
  },
  {
    "text": "great",
    "start": "363840",
    "end": "364400"
  },
  {
    "text": "and this holds as well for every single",
    "start": "364400",
    "end": "366240"
  },
  {
    "text": "epoch",
    "start": "366240",
    "end": "367759"
  },
  {
    "text": "you don't want your model getting uh",
    "start": "367759",
    "end": "370240"
  },
  {
    "text": "like overfitting on the ordering of that",
    "start": "370240",
    "end": "372479"
  },
  {
    "text": "initial shuffle",
    "start": "372479",
    "end": "373520"
  },
  {
    "text": "you want to continuously see newly",
    "start": "373520",
    "end": "375919"
  },
  {
    "text": "ordered",
    "start": "375919",
    "end": "376639"
  },
  {
    "text": "uh batches from sampled from the global",
    "start": "376639",
    "end": "379520"
  },
  {
    "text": "data set",
    "start": "379520",
    "end": "381120"
  },
  {
    "text": "and the end result of this is that you",
    "start": "381120",
    "end": "383280"
  },
  {
    "text": "achieve better model accuracy after a",
    "start": "383280",
    "end": "385120"
  },
  {
    "text": "set number of epochs",
    "start": "385120",
    "end": "386560"
  },
  {
    "text": "um so in a given set of epochs and you",
    "start": "386560",
    "end": "389600"
  },
  {
    "text": "could think of it as",
    "start": "389600",
    "end": "391759"
  },
  {
    "text": "like per epoch shuffling improving the",
    "start": "391759",
    "end": "393440"
  },
  {
    "text": "statistical gain of each step in the",
    "start": "393440",
    "end": "395039"
  },
  {
    "text": "training process",
    "start": "395039",
    "end": "396720"
  },
  {
    "text": "there's a lot more details on why this",
    "start": "396720",
    "end": "399919"
  },
  {
    "text": "is the case",
    "start": "399919",
    "end": "400560"
  },
  {
    "text": "and why it's nice to shuffle at the",
    "start": "400560",
    "end": "402319"
  },
  {
    "text": "start of every epoch",
    "start": "402319",
    "end": "403600"
  },
  {
    "text": "a lot of interesting recent papers uh",
    "start": "403600",
    "end": "406160"
  },
  {
    "text": "also proving like",
    "start": "406160",
    "end": "407440"
  },
  {
    "text": "also going through the convergence",
    "start": "407440",
    "end": "408720"
  },
  {
    "text": "analysis of like stochastic gradient",
    "start": "408720",
    "end": "410160"
  },
  {
    "text": "descent for example",
    "start": "410160",
    "end": "412560"
  },
  {
    "text": "so uh why is this hard like what are the",
    "start": "412560",
    "end": "415039"
  },
  {
    "text": "challenges in creating a shuffling data",
    "start": "415039",
    "end": "416720"
  },
  {
    "text": "loader",
    "start": "416720",
    "end": "417199"
  },
  {
    "text": "can't you just read it all into memory",
    "start": "417199",
    "end": "419599"
  },
  {
    "text": "and then",
    "start": "419599",
    "end": "420560"
  },
  {
    "text": "like you know use random.shuffle and",
    "start": "420560",
    "end": "422400"
  },
  {
    "text": "shuffle a list of data of a list of",
    "start": "422400",
    "end": "424319"
  },
  {
    "text": "samples",
    "start": "424319",
    "end": "425919"
  },
  {
    "text": "well as i mentioned before once you get",
    "start": "425919",
    "end": "428080"
  },
  {
    "text": "start getting to a certain data scale",
    "start": "428080",
    "end": "429759"
  },
  {
    "text": "where that becomes infeasible to load it",
    "start": "429759",
    "end": "431599"
  },
  {
    "text": "all into memory on a single machine",
    "start": "431599",
    "end": "433759"
  },
  {
    "text": "which is also the motivator for doing uh",
    "start": "433759",
    "end": "435840"
  },
  {
    "text": "distributed data parallel training or",
    "start": "435840",
    "end": "437360"
  },
  {
    "text": "one of the motivators",
    "start": "437360",
    "end": "438639"
  },
  {
    "text": "things get way more difficult and also",
    "start": "438639",
    "end": "440400"
  },
  {
    "text": "way more fun",
    "start": "440400",
    "end": "442800"
  },
  {
    "text": "at a high level your requirements for",
    "start": "442800",
    "end": "444560"
  },
  {
    "text": "the shuffling data loader is",
    "start": "444560",
    "end": "446560"
  },
  {
    "text": "you want a high quality global shuffle",
    "start": "446560",
    "end": "449680"
  },
  {
    "text": "you want no significant increases in",
    "start": "449680",
    "end": "451520"
  },
  {
    "text": "training time you know due to the",
    "start": "451520",
    "end": "453120"
  },
  {
    "text": "trainers",
    "start": "453120",
    "end": "453919"
  },
  {
    "text": "sitting there waiting for the shuffle to",
    "start": "453919",
    "end": "455440"
  },
  {
    "text": "complete you don't want that to happen",
    "start": "455440",
    "end": "457520"
  },
  {
    "text": "and then you want very convenient",
    "start": "457520",
    "end": "458880"
  },
  {
    "text": "integration with machine learning",
    "start": "458880",
    "end": "460080"
  },
  {
    "text": "frameworks",
    "start": "460080",
    "end": "460800"
  },
  {
    "text": "like nice ux nice apis to integrate with",
    "start": "460800",
    "end": "464240"
  },
  {
    "text": "frameworks such as torch and tensorflow",
    "start": "464240",
    "end": "467039"
  },
  {
    "text": "so what solutions to this already exist",
    "start": "467039",
    "end": "469599"
  },
  {
    "text": "uh",
    "start": "469599",
    "end": "470080"
  },
  {
    "text": "and how exactly do they fall short",
    "start": "470080",
    "end": "472560"
  },
  {
    "text": "because",
    "start": "472560",
    "end": "473120"
  },
  {
    "text": "we wouldn't be here at this talk if",
    "start": "473120",
    "end": "475919"
  },
  {
    "text": "there is currently a solution that was",
    "start": "475919",
    "end": "477680"
  },
  {
    "text": "awesome",
    "start": "477680",
    "end": "478000"
  },
  {
    "text": "for all use cases there are a couple of",
    "start": "478000",
    "end": "481840"
  },
  {
    "start": "480000",
    "end": "480000"
  },
  {
    "text": "uh",
    "start": "481840",
    "end": "482160"
  },
  {
    "text": "key pitfalls that we've identified at",
    "start": "482160",
    "end": "484000"
  },
  {
    "text": "the existing solutions",
    "start": "484000",
    "end": "485280"
  },
  {
    "text": "there are of course the framework",
    "start": "485280",
    "end": "488160"
  },
  {
    "text": "specific solutions by pi torch and",
    "start": "488160",
    "end": "490000"
  },
  {
    "text": "tensorflow",
    "start": "490000",
    "end": "491199"
  },
  {
    "text": "and that ends up being a bit of a con",
    "start": "491199",
    "end": "493360"
  },
  {
    "text": "because you have to learn",
    "start": "493360",
    "end": "494479"
  },
  {
    "text": "the specific apis for each of these data",
    "start": "494479",
    "end": "497680"
  },
  {
    "text": "loaders and then you're locked in",
    "start": "497680",
    "end": "499599"
  },
  {
    "text": "to that framework and you're you often",
    "start": "499599",
    "end": "502560"
  },
  {
    "text": "have to juggle",
    "start": "502560",
    "end": "503440"
  },
  {
    "text": "like unique performance characteristics",
    "start": "503440",
    "end": "506160"
  },
  {
    "text": "uh",
    "start": "506160",
    "end": "506560"
  },
  {
    "text": "for any fancy shuffling that might be",
    "start": "506560",
    "end": "508479"
  },
  {
    "text": "going on under the hood and stuff like",
    "start": "508479",
    "end": "510000"
  },
  {
    "text": "prefetching",
    "start": "510000",
    "end": "511520"
  },
  {
    "text": "uh the shuffle is not global and this is",
    "start": "511520",
    "end": "513680"
  },
  {
    "text": "true for pi torch tensorflow",
    "start": "513680",
    "end": "515518"
  },
  {
    "text": "and also petastorm uh petastorm is",
    "start": "515519",
    "end": "518880"
  },
  {
    "text": "uh is a a data loader",
    "start": "518880",
    "end": "521919"
  },
  {
    "text": "out of uber an open source data loader",
    "start": "521919",
    "end": "525040"
  },
  {
    "text": "and although it has a pseudo global",
    "start": "525040",
    "end": "527279"
  },
  {
    "text": "shuffle",
    "start": "527279",
    "end": "528080"
  },
  {
    "text": "it only shuffles at the granularity of",
    "start": "528080",
    "end": "529839"
  },
  {
    "text": "row groups and uh",
    "start": "529839",
    "end": "531440"
  },
  {
    "text": "in files so it's not uh fully global",
    "start": "531440",
    "end": "534480"
  },
  {
    "text": "like at",
    "start": "534480",
    "end": "535040"
  },
  {
    "text": "uh sample level granularity",
    "start": "535040",
    "end": "538160"
  },
  {
    "text": "uh also we've heard reports of",
    "start": "538160",
    "end": "540399"
  },
  {
    "text": "performance issues from both pi torch",
    "start": "540399",
    "end": "542480"
  },
  {
    "text": "and peta storm data loader users uh",
    "start": "542480",
    "end": "545760"
  },
  {
    "text": "another option is to try and do a global",
    "start": "545760",
    "end": "548160"
  },
  {
    "text": "shuffle",
    "start": "548160",
    "end": "549200"
  },
  {
    "text": "with an external service that's running",
    "start": "549200",
    "end": "551040"
  },
  {
    "text": "on separate infrastructure like spark",
    "start": "551040",
    "end": "552800"
  },
  {
    "text": "and to try and interleave that with",
    "start": "552800",
    "end": "554240"
  },
  {
    "text": "training of course that has the issue of",
    "start": "554240",
    "end": "556880"
  },
  {
    "text": "it has to run on separate infrastructure",
    "start": "556880",
    "end": "558480"
  },
  {
    "text": "compared to your distributed training",
    "start": "558480",
    "end": "560800"
  },
  {
    "text": "infrastructure which is never fun for",
    "start": "560800",
    "end": "563120"
  },
  {
    "text": "people in charge of that ops",
    "start": "563120",
    "end": "565600"
  },
  {
    "text": "not to mention also the cost a worse",
    "start": "565600",
    "end": "568480"
  },
  {
    "text": "resource utilization if those resources",
    "start": "568480",
    "end": "570160"
  },
  {
    "text": "can't be shared",
    "start": "570160",
    "end": "571120"
  },
  {
    "text": "and it's also very difficult to",
    "start": "571120",
    "end": "572480"
  },
  {
    "text": "integrate with ml frameworks",
    "start": "572480",
    "end": "574640"
  },
  {
    "text": "that's like separate spark shuffling",
    "start": "574640",
    "end": "576320"
  },
  {
    "text": "service it's uh it's a little bit tough",
    "start": "576320",
    "end": "578480"
  },
  {
    "text": "to interleave it with training and to do",
    "start": "578480",
    "end": "580720"
  },
  {
    "text": "stuff like pipelining shuffling with",
    "start": "580720",
    "end": "582959"
  },
  {
    "text": "training in a nice way",
    "start": "582959",
    "end": "585600"
  },
  {
    "text": "um so some uh we've heard from a couple",
    "start": "585600",
    "end": "587760"
  },
  {
    "text": "of users that this is",
    "start": "587760",
    "end": "589040"
  },
  {
    "text": "an issue uh the one of the biggest users",
    "start": "589040",
    "end": "591440"
  },
  {
    "text": "that we've been working with",
    "start": "591440",
    "end": "592800"
  },
  {
    "text": "is uh uber's deep learning teams",
    "start": "592800",
    "end": "595920"
  },
  {
    "text": "and so they were experiencing a couple",
    "start": "595920",
    "end": "597440"
  },
  {
    "text": "of key issues uh first",
    "start": "597440",
    "end": "599600"
  },
  {
    "text": "with when global shuffling was turned on",
    "start": "599600",
    "end": "601440"
  },
  {
    "text": "within petastorm",
    "start": "601440",
    "end": "602720"
  },
  {
    "text": "they were experiencing latency spikes",
    "start": "602720",
    "end": "604320"
  },
  {
    "text": "and throughput instability",
    "start": "604320",
    "end": "606880"
  },
  {
    "text": "and as you can see from this graph over",
    "start": "606880",
    "end": "608399"
  },
  {
    "text": "here the baseline shuffler",
    "start": "608399",
    "end": "610800"
  },
  {
    "text": "was synchronous not doing any pipelining",
    "start": "610800",
    "end": "613120"
  },
  {
    "text": "and the async shuffler was doing",
    "start": "613120",
    "end": "614880"
  },
  {
    "text": "pipelining of shuffling with training",
    "start": "614880",
    "end": "617279"
  },
  {
    "text": "and even with the pipelining async",
    "start": "617279",
    "end": "620240"
  },
  {
    "text": "shuffler",
    "start": "620240",
    "end": "621200"
  },
  {
    "text": "uh shuffling data loader they were still",
    "start": "621200",
    "end": "623120"
  },
  {
    "text": "seeing these uh random like up to 10",
    "start": "623120",
    "end": "625279"
  },
  {
    "text": "second",
    "start": "625279",
    "end": "626240"
  },
  {
    "text": "uh latency spikes causing uh trainer",
    "start": "626240",
    "end": "629519"
  },
  {
    "text": "stragglers and slowing down training",
    "start": "629519",
    "end": "631200"
  },
  {
    "text": "overall",
    "start": "631200",
    "end": "633839"
  },
  {
    "text": "and then when they tried to mitigate",
    "start": "633839",
    "end": "636320"
  },
  {
    "text": "those latency spikes and throughput",
    "start": "636320",
    "end": "638079"
  },
  {
    "text": "instability",
    "start": "638079",
    "end": "638800"
  },
  {
    "text": "by turning off the global shuffling and",
    "start": "638800",
    "end": "641279"
  },
  {
    "text": "just doing",
    "start": "641279",
    "end": "642000"
  },
  {
    "text": "um per uh input partition per",
    "start": "642000",
    "end": "645040"
  },
  {
    "text": "trainer partition shuffling they found",
    "start": "645040",
    "end": "648399"
  },
  {
    "text": "that",
    "start": "648399",
    "end": "648959"
  },
  {
    "text": "due to the statistical sampling of the",
    "start": "648959",
    "end": "650800"
  },
  {
    "text": "training data being degraded",
    "start": "650800",
    "end": "652959"
  },
  {
    "text": "by not doing a full global shuffle at",
    "start": "652959",
    "end": "655120"
  },
  {
    "text": "the beginning of each epoch",
    "start": "655120",
    "end": "657040"
  },
  {
    "text": "they saw a large training loss",
    "start": "657040",
    "end": "660240"
  },
  {
    "text": "oscillation",
    "start": "660240",
    "end": "661600"
  },
  {
    "text": "um so ultimately as you can see here",
    "start": "661600",
    "end": "663920"
  },
  {
    "text": "their training loss",
    "start": "663920",
    "end": "664959"
  },
  {
    "text": "oscillated uh like crazy and so",
    "start": "664959",
    "end": "668160"
  },
  {
    "text": "basically it took them",
    "start": "668160",
    "end": "669360"
  },
  {
    "text": "longer to converge to a nice accuracy",
    "start": "669360",
    "end": "673920"
  },
  {
    "text": "and so",
    "start": "673920",
    "end": "674399"
  },
  {
    "text": "yeah the fundamental result of this is",
    "start": "674399",
    "end": "677200"
  },
  {
    "text": "that model accuracy took a big hit",
    "start": "677200",
    "end": "679440"
  },
  {
    "text": "uh both of these issues severely",
    "start": "679440",
    "end": "681200"
  },
  {
    "text": "affected their model accuracy within a",
    "start": "681200",
    "end": "682880"
  },
  {
    "text": "given set time window",
    "start": "682880",
    "end": "684160"
  },
  {
    "text": "and their particular use case was a",
    "start": "684160",
    "end": "686640"
  },
  {
    "text": "concrete like 12 hour time window where",
    "start": "686640",
    "end": "688320"
  },
  {
    "text": "they want to get the model as accurate",
    "start": "688320",
    "end": "689680"
  },
  {
    "text": "as possible",
    "start": "689680",
    "end": "691839"
  },
  {
    "text": "and in addition to uber looking for a",
    "start": "691839",
    "end": "694560"
  },
  {
    "text": "solution",
    "start": "694560",
    "end": "695360"
  },
  {
    "text": "even some other users are actually using",
    "start": "695360",
    "end": "697680"
  },
  {
    "text": "ray to build their own data loaders",
    "start": "697680",
    "end": "699920"
  },
  {
    "text": "one example of that is some people at",
    "start": "699920",
    "end": "701839"
  },
  {
    "text": "nvidia they were experiencing issues",
    "start": "701839",
    "end": "704160"
  },
  {
    "text": "with the pi torch data loader",
    "start": "704160",
    "end": "705600"
  },
  {
    "text": "particularly",
    "start": "705600",
    "end": "706160"
  },
  {
    "text": "around hanging a little bit of detail in",
    "start": "706160",
    "end": "708640"
  },
  {
    "text": "their use case",
    "start": "708640",
    "end": "709360"
  },
  {
    "text": "their models were small and i o bound",
    "start": "709360",
    "end": "712000"
  },
  {
    "text": "very few model parameters",
    "start": "712000",
    "end": "713600"
  },
  {
    "text": "so data loading performance is very",
    "start": "713600",
    "end": "715120"
  },
  {
    "text": "important it's it was their most",
    "start": "715120",
    "end": "716480"
  },
  {
    "text": "significant training bottleneck",
    "start": "716480",
    "end": "718639"
  },
  {
    "text": "uh pytorch's multi-processing data",
    "start": "718639",
    "end": "720639"
  },
  {
    "text": "loader occasionally was hanging which",
    "start": "720639",
    "end": "722560"
  },
  {
    "text": "was hurting their training times you",
    "start": "722560",
    "end": "724160"
  },
  {
    "text": "know that's no fun",
    "start": "724160",
    "end": "725760"
  },
  {
    "text": "and they built a simple ray-based data",
    "start": "725760",
    "end": "727680"
  },
  {
    "text": "loader which is basically a drop-in",
    "start": "727680",
    "end": "729040"
  },
  {
    "text": "placement for the pi",
    "start": "729040",
    "end": "730079"
  },
  {
    "text": "uh the pi torch multiprocessing data",
    "start": "730079",
    "end": "731839"
  },
  {
    "text": "loader and they achieved higher",
    "start": "731839",
    "end": "733200"
  },
  {
    "text": "throughput than tensorflow's data loader",
    "start": "733200",
    "end": "735360"
  },
  {
    "text": "and also they uh matched pi torch's data",
    "start": "735360",
    "end": "738000"
  },
  {
    "text": "loader throughput",
    "start": "738000",
    "end": "739120"
  },
  {
    "text": "uh but without those occasional hangs",
    "start": "739120",
    "end": "741360"
  },
  {
    "text": "and so this was a very like you know",
    "start": "741360",
    "end": "742720"
  },
  {
    "text": "simple straightforward drop in",
    "start": "742720",
    "end": "743839"
  },
  {
    "text": "replacement",
    "start": "743839",
    "end": "744800"
  },
  {
    "text": "nothing super fancy and race specific uh",
    "start": "744800",
    "end": "747680"
  },
  {
    "text": "and they still were able to meet pi",
    "start": "747680",
    "end": "749440"
  },
  {
    "text": "torches like a multi-processing",
    "start": "749440",
    "end": "751440"
  },
  {
    "text": "uh data loader throughput uh despite ray",
    "start": "751440",
    "end": "754240"
  },
  {
    "text": "having more overhead",
    "start": "754240",
    "end": "755440"
  },
  {
    "text": "than python's multi-processing",
    "start": "755440",
    "end": "759360"
  },
  {
    "text": "so our specific solution um",
    "start": "759360",
    "end": "762800"
  },
  {
    "text": "this ray-based shuffling data loader",
    "start": "762800",
    "end": "764240"
  },
  {
    "text": "we've been working on why exactly is ray",
    "start": "764240",
    "end": "766880"
  },
  {
    "text": "a good fit",
    "start": "766880",
    "end": "767519"
  },
  {
    "text": "why would you want to use ray for this",
    "start": "767519",
    "end": "769920"
  },
  {
    "text": "first",
    "start": "769920",
    "end": "770639"
  },
  {
    "text": "great system design ray has some awesome",
    "start": "770639",
    "end": "773600"
  },
  {
    "text": "system characteristics i'm going to talk",
    "start": "773600",
    "end": "775040"
  },
  {
    "text": "about in a bit",
    "start": "775040",
    "end": "776160"
  },
  {
    "text": "and they also have a large ecosystem",
    "start": "776160",
    "end": "778320"
  },
  {
    "text": "around",
    "start": "778320",
    "end": "779200"
  },
  {
    "text": "machine learning around that entire",
    "start": "779200",
    "end": "780800"
  },
  {
    "text": "machine learning pipeline",
    "start": "780800",
    "end": "782800"
  },
  {
    "text": "so first ray system design uh three big",
    "start": "782800",
    "end": "786160"
  },
  {
    "text": "aspects",
    "start": "786160",
    "end": "786720"
  },
  {
    "text": "uh header the support for heterogeneous",
    "start": "786720",
    "end": "788560"
  },
  {
    "text": "resources",
    "start": "788560",
    "end": "789839"
  },
  {
    "text": "um the distributed in memory object",
    "start": "789839",
    "end": "792079"
  },
  {
    "text": "store",
    "start": "792079",
    "end": "792959"
  },
  {
    "text": "and really fast smart decentralized",
    "start": "792959",
    "end": "794880"
  },
  {
    "text": "peer-to-peer scheduling",
    "start": "794880",
    "end": "797040"
  },
  {
    "text": "uh first header we um the array cluster",
    "start": "797040",
    "end": "800240"
  },
  {
    "text": "can support",
    "start": "800240",
    "end": "801360"
  },
  {
    "text": "node types with pretty uh disparate uh",
    "start": "801360",
    "end": "804959"
  },
  {
    "text": "resources so for example it could have",
    "start": "804959",
    "end": "807760"
  },
  {
    "text": "different hardware like cpus and gpus",
    "start": "807760",
    "end": "810320"
  },
  {
    "text": "it can have a differing number of cpu",
    "start": "810320",
    "end": "812000"
  },
  {
    "text": "cores and each node can have a differing",
    "start": "812000",
    "end": "814160"
  },
  {
    "text": "amount",
    "start": "814160",
    "end": "814639"
  },
  {
    "text": "of cpu memory and each of these",
    "start": "814639",
    "end": "817360"
  },
  {
    "text": "heterogeneous resources are schedulable",
    "start": "817360",
    "end": "819519"
  },
  {
    "text": "at a very granular level at the task and",
    "start": "819519",
    "end": "822240"
  },
  {
    "text": "actor",
    "start": "822240",
    "end": "822800"
  },
  {
    "text": "level which enables fine-grained",
    "start": "822800",
    "end": "825199"
  },
  {
    "text": "resource provisioning",
    "start": "825199",
    "end": "828160"
  },
  {
    "start": "828000",
    "end": "828000"
  },
  {
    "text": "and then the distributed in-memory",
    "start": "828160",
    "end": "829680"
  },
  {
    "text": "object store which is raised",
    "start": "829680",
    "end": "831360"
  },
  {
    "text": "probably one of rey's uh biggest and",
    "start": "831360",
    "end": "833279"
  },
  {
    "text": "best features",
    "start": "833279",
    "end": "834720"
  },
  {
    "text": "it has uh two core dimensions um",
    "start": "834720",
    "end": "837920"
  },
  {
    "text": "that make uh ray awesome uh it's",
    "start": "837920",
    "end": "841199"
  },
  {
    "text": "it's uh its optimizations around",
    "start": "841199",
    "end": "843279"
  },
  {
    "text": "performance and its insurances of",
    "start": "843279",
    "end": "845440"
  },
  {
    "text": "reliability",
    "start": "845440",
    "end": "846800"
  },
  {
    "text": "uh for performance particularly for",
    "start": "846800",
    "end": "848560"
  },
  {
    "text": "shuffling intermediate results and",
    "start": "848560",
    "end": "850079"
  },
  {
    "text": "shuffled outputs",
    "start": "850079",
    "end": "851040"
  },
  {
    "text": "are transparently cached in memory and",
    "start": "851040",
    "end": "852720"
  },
  {
    "text": "transferred over the network",
    "start": "852720",
    "end": "854160"
  },
  {
    "text": "um yeah uh very transparent to the user",
    "start": "854160",
    "end": "857440"
  },
  {
    "text": "level api",
    "start": "857440",
    "end": "858639"
  },
  {
    "text": "even in this case the user being the",
    "start": "858639",
    "end": "860560"
  },
  {
    "text": "implementation of the shuffling data",
    "start": "860560",
    "end": "861920"
  },
  {
    "text": "loader",
    "start": "861920",
    "end": "862560"
  },
  {
    "text": "uh we don't have to manually deal with",
    "start": "862560",
    "end": "864560"
  },
  {
    "text": "like when to transfer data over the",
    "start": "864560",
    "end": "866000"
  },
  {
    "text": "network",
    "start": "866000",
    "end": "866800"
  },
  {
    "text": "or how to optimize data being cached in",
    "start": "866800",
    "end": "869040"
  },
  {
    "text": "memory that's done by ray",
    "start": "869040",
    "end": "870480"
  },
  {
    "text": "automatically and then also for",
    "start": "870480",
    "end": "873680"
  },
  {
    "text": "uh reading data that's on the same node",
    "start": "873680",
    "end": "875920"
  },
  {
    "text": "uh due to",
    "start": "875920",
    "end": "876800"
  },
  {
    "text": "the distributed memory in memory object",
    "start": "876800",
    "end": "878800"
  },
  {
    "text": "store using shared memory",
    "start": "878800",
    "end": "880399"
  },
  {
    "text": "we get zero copy reads which decreases",
    "start": "880399",
    "end": "882800"
  },
  {
    "text": "the read time",
    "start": "882800",
    "end": "884000"
  },
  {
    "text": "really nice when it's large data and",
    "start": "884000",
    "end": "886880"
  },
  {
    "text": "then as far as reliability goes the ray",
    "start": "886880",
    "end": "888720"
  },
  {
    "text": "scheduler",
    "start": "888720",
    "end": "889440"
  },
  {
    "text": "limits the total memory that can be used",
    "start": "889440",
    "end": "891680"
  },
  {
    "text": "by objects in a single node and this",
    "start": "891680",
    "end": "893600"
  },
  {
    "text": "uh prevents spurious out of memory",
    "start": "893600",
    "end": "895839"
  },
  {
    "text": "errors which is great",
    "start": "895839",
    "end": "897760"
  },
  {
    "text": "and then beyond that if that fails uh",
    "start": "897760",
    "end": "900079"
  },
  {
    "text": "when the object store",
    "start": "900079",
    "end": "901279"
  },
  {
    "text": "ultimately becomes full objects are",
    "start": "901279",
    "end": "903199"
  },
  {
    "text": "spilled to external storage",
    "start": "903199",
    "end": "905519"
  },
  {
    "text": "so essentially your working data set can",
    "start": "905519",
    "end": "907680"
  },
  {
    "text": "exceed the cluster memory",
    "start": "907680",
    "end": "909920"
  },
  {
    "text": "which is which is really nice that used",
    "start": "909920",
    "end": "912639"
  },
  {
    "text": "to be hard cap and we're continuing to",
    "start": "912639",
    "end": "914160"
  },
  {
    "text": "work on this too",
    "start": "914160",
    "end": "915839"
  },
  {
    "text": "essentially with the goal of providing",
    "start": "915839",
    "end": "918079"
  },
  {
    "text": "like uh",
    "start": "918079",
    "end": "919120"
  },
  {
    "text": "infinite object space uh that will just",
    "start": "919120",
    "end": "922079"
  },
  {
    "text": "smartly",
    "start": "922079",
    "end": "923040"
  },
  {
    "text": "cache things in memory and then spill",
    "start": "923040",
    "end": "924800"
  },
  {
    "text": "the disc when needed",
    "start": "924800",
    "end": "926240"
  },
  {
    "text": "um we're continuing to optimize that all",
    "start": "926240",
    "end": "928240"
  },
  {
    "text": "the time that layer",
    "start": "928240",
    "end": "930639"
  },
  {
    "text": "and then uh scheduling as i mentioned",
    "start": "930639",
    "end": "932880"
  },
  {
    "text": "it's a decentralized peer-to-peer",
    "start": "932880",
    "end": "934399"
  },
  {
    "text": "scheduling there's no global scheduler",
    "start": "934399",
    "end": "936160"
  },
  {
    "text": "bottleneck",
    "start": "936160",
    "end": "937199"
  },
  {
    "text": "this allows much larger cluster scales",
    "start": "937199",
    "end": "939839"
  },
  {
    "text": "and",
    "start": "939839",
    "end": "940320"
  },
  {
    "text": "very high throughput because you don't",
    "start": "940320",
    "end": "942560"
  },
  {
    "text": "have that central",
    "start": "942560",
    "end": "943519"
  },
  {
    "text": "uh bottleneck we also have like some",
    "start": "943519",
    "end": "946800"
  },
  {
    "text": "things like hot node mitigation where",
    "start": "946800",
    "end": "948240"
  },
  {
    "text": "we'll attempt to schedule tasks on the",
    "start": "948240",
    "end": "949759"
  },
  {
    "text": "nose nodes with that low memory",
    "start": "949759",
    "end": "951199"
  },
  {
    "text": "utilization",
    "start": "951199",
    "end": "952399"
  },
  {
    "text": "and this prevents for example spurious",
    "start": "952399",
    "end": "955279"
  },
  {
    "text": "out of memory errors",
    "start": "955279",
    "end": "956320"
  },
  {
    "text": "which are never fun uh we also have",
    "start": "956320",
    "end": "959199"
  },
  {
    "text": "locality aware scheduling where we'll",
    "start": "959199",
    "end": "960959"
  },
  {
    "text": "try to schedule tasks onto",
    "start": "960959",
    "end": "963120"
  },
  {
    "text": "the node with most of the needed",
    "start": "963120",
    "end": "966240"
  },
  {
    "text": "task dependencies most bytes already",
    "start": "966240",
    "end": "968560"
  },
  {
    "text": "local to that node",
    "start": "968560",
    "end": "969839"
  },
  {
    "text": "and this minimizes uh object transfer uh",
    "start": "969839",
    "end": "972959"
  },
  {
    "text": "which",
    "start": "972959",
    "end": "973519"
  },
  {
    "text": "essentially provides like faster task",
    "start": "973519",
    "end": "975360"
  },
  {
    "text": "execution and also",
    "start": "975360",
    "end": "976959"
  },
  {
    "text": "uh minimizes like network saturation",
    "start": "976959",
    "end": "979120"
  },
  {
    "text": "stuff making the clusters a whole",
    "start": "979120",
    "end": "980800"
  },
  {
    "text": "more reliable the ray ecosystem too",
    "start": "980800",
    "end": "984639"
  },
  {
    "text": "big a big pro of ray we have",
    "start": "984639",
    "end": "988399"
  },
  {
    "text": "integrations around data pre-processing",
    "start": "988399",
    "end": "990880"
  },
  {
    "text": "model training",
    "start": "990880",
    "end": "991680"
  },
  {
    "text": "model serving all on a single ray",
    "start": "991680",
    "end": "994160"
  },
  {
    "text": "cluster infrastructure and so this is a",
    "start": "994160",
    "end": "996560"
  },
  {
    "text": "brief little overview",
    "start": "996560",
    "end": "997920"
  },
  {
    "text": "we've got some rain native libraries for",
    "start": "997920",
    "end": "1000240"
  },
  {
    "text": "machine learning such as",
    "start": "1000240",
    "end": "1001600"
  },
  {
    "text": "tune ro lib ray serve and then we also",
    "start": "1001600",
    "end": "1004000"
  },
  {
    "text": "have some third-party integrations",
    "start": "1004000",
    "end": "1005360"
  },
  {
    "text": "including distributed training optimize",
    "start": "1005360",
    "end": "1007440"
  },
  {
    "text": "options such as horvat and it's all",
    "start": "1007440",
    "end": "1009759"
  },
  {
    "text": "running on top of ray this is like",
    "start": "1009759",
    "end": "1011440"
  },
  {
    "text": "universal like compute substrate",
    "start": "1011440",
    "end": "1013440"
  },
  {
    "text": "and then ray can run on top of your",
    "start": "1013440",
    "end": "1015680"
  },
  {
    "text": "favorite uh",
    "start": "1015680",
    "end": "1016639"
  },
  {
    "text": "cloud it could run on kubernetes and it",
    "start": "1016639",
    "end": "1019120"
  },
  {
    "text": "can run on-prem",
    "start": "1019120",
    "end": "1020800"
  },
  {
    "text": "now we finally got into the design of",
    "start": "1020800",
    "end": "1023360"
  },
  {
    "text": "the shuffling data loader",
    "start": "1023360",
    "end": "1024880"
  },
  {
    "text": "the actual the big meat of the talk uh",
    "start": "1024880",
    "end": "1027678"
  },
  {
    "text": "so i'm gonna",
    "start": "1027679",
    "end": "1028480"
  },
  {
    "text": "talk briefly about the api for the data",
    "start": "1028480",
    "end": "1030480"
  },
  {
    "text": "loader and its architecture",
    "start": "1030480",
    "end": "1032480"
  },
  {
    "text": "uh first if you're familiar with the",
    "start": "1032480",
    "end": "1035438"
  },
  {
    "text": "like",
    "start": "1035439",
    "end": "1035839"
  },
  {
    "text": "pi torch data loaders or with the",
    "start": "1035839",
    "end": "1037839"
  },
  {
    "text": "tensorflow data loaders",
    "start": "1037839",
    "end": "1039520"
  },
  {
    "text": "it offers a similar data set api um",
    "start": "1039520",
    "end": "1042720"
  },
  {
    "text": "particularly an iterable api that yields",
    "start": "1042720",
    "end": "1045918"
  },
  {
    "text": "these globally shuffled gpu batches",
    "start": "1045919",
    "end": "1047839"
  },
  {
    "text": "hiding all of the shuffler like",
    "start": "1047839",
    "end": "1050320"
  },
  {
    "text": "implementation and all that complexity",
    "start": "1050320",
    "end": "1053039"
  },
  {
    "text": "that distributed shuffle is hidden",
    "start": "1053039",
    "end": "1054559"
  },
  {
    "text": "underneath a simple iterator api",
    "start": "1054559",
    "end": "1057520"
  },
  {
    "text": "um so here's an example where you",
    "start": "1057520",
    "end": "1059120"
  },
  {
    "text": "configure the shuffling data set",
    "start": "1059120",
    "end": "1060880"
  },
  {
    "text": "um with uh with different parameters",
    "start": "1060880",
    "end": "1062960"
  },
  {
    "text": "such as like file names pointing to the",
    "start": "1062960",
    "end": "1064480"
  },
  {
    "text": "input data",
    "start": "1064480",
    "end": "1065360"
  },
  {
    "text": "number of epochs and trainers your gpu",
    "start": "1065360",
    "end": "1067600"
  },
  {
    "text": "batch size the",
    "start": "1067600",
    "end": "1068720"
  },
  {
    "text": "rank of the current training worker and",
    "start": "1068720",
    "end": "1070799"
  },
  {
    "text": "then some shuffling specific",
    "start": "1070799",
    "end": "1072480"
  },
  {
    "text": "uh little tweak parameters",
    "start": "1072480",
    "end": "1075679"
  },
  {
    "text": "that allows you to tweak the shuffling",
    "start": "1075679",
    "end": "1077039"
  },
  {
    "text": "implementation and then there you see",
    "start": "1077039",
    "end": "1079280"
  },
  {
    "text": "the usage where you essentially just",
    "start": "1079280",
    "end": "1081520"
  },
  {
    "text": "iterate through the data set and get",
    "start": "1081520",
    "end": "1083360"
  },
  {
    "text": "back your batches",
    "start": "1083360",
    "end": "1085360"
  },
  {
    "text": "um the data set will kick off the",
    "start": "1085360",
    "end": "1086799"
  },
  {
    "text": "shuffling in the background as soon as",
    "start": "1086799",
    "end": "1088240"
  },
  {
    "text": "that shuffling data set is constructed",
    "start": "1088240",
    "end": "1090480"
  },
  {
    "text": "it's uh that's so that's done upon",
    "start": "1090480",
    "end": "1092880"
  },
  {
    "text": "instantiation",
    "start": "1092880",
    "end": "1094080"
  },
  {
    "text": "and then in the background it will use",
    "start": "1094080",
    "end": "1096799"
  },
  {
    "text": "the raycluster to pipeline shuffling",
    "start": "1096799",
    "end": "1098559"
  },
  {
    "text": "with model training",
    "start": "1098559",
    "end": "1100000"
  },
  {
    "text": "up to the max concurrent epochs",
    "start": "1100000",
    "end": "1101760"
  },
  {
    "text": "throttling limit that you give it",
    "start": "1101760",
    "end": "1103440"
  },
  {
    "text": "so for example at most two epoch",
    "start": "1103440",
    "end": "1106320"
  },
  {
    "text": "shuffles going on at once",
    "start": "1106320",
    "end": "1108160"
  },
  {
    "text": "and the training data can be read from",
    "start": "1108160",
    "end": "1110400"
  },
  {
    "text": "local disk s3",
    "start": "1110400",
    "end": "1111520"
  },
  {
    "text": "hdfs if you check out fs spec everything",
    "start": "1111520",
    "end": "1114400"
  },
  {
    "text": "that fs spec",
    "start": "1114400",
    "end": "1115360"
  },
  {
    "text": "supports the shuffling data loader will",
    "start": "1115360",
    "end": "1118400"
  },
  {
    "text": "be able to read from",
    "start": "1118400",
    "end": "1120720"
  },
  {
    "text": "and then specifically for uh torch uh",
    "start": "1120720",
    "end": "1123600"
  },
  {
    "text": "distributed data parallel training",
    "start": "1123600",
    "end": "1125520"
  },
  {
    "text": "uh we have a torch dataset api which is",
    "start": "1125520",
    "end": "1127760"
  },
  {
    "text": "very a very",
    "start": "1127760",
    "end": "1128799"
  },
  {
    "text": "light wrapper around that last data set",
    "start": "1128799",
    "end": "1132240"
  },
  {
    "text": "as you can see",
    "start": "1132240",
    "end": "1133600"
  },
  {
    "text": "most of the construction parameters are",
    "start": "1133600",
    "end": "1136559"
  },
  {
    "text": "the same except for these feature",
    "start": "1136559",
    "end": "1138559"
  },
  {
    "text": "and label specs and these are used to",
    "start": "1138559",
    "end": "1141760"
  },
  {
    "text": "transform",
    "start": "1141760",
    "end": "1142559"
  },
  {
    "text": "the gpu batches from the pandas data",
    "start": "1142559",
    "end": "1144960"
  },
  {
    "text": "frames",
    "start": "1144960",
    "end": "1145679"
  },
  {
    "text": "that the base data set that i just",
    "start": "1145679",
    "end": "1147520"
  },
  {
    "text": "showed",
    "start": "1147520",
    "end": "1148960"
  },
  {
    "text": "operates on um to pi torch tensors and",
    "start": "1148960",
    "end": "1151760"
  },
  {
    "text": "it uh",
    "start": "1151760",
    "end": "1152320"
  },
  {
    "text": "transparently does that using the",
    "start": "1152320",
    "end": "1153520"
  },
  {
    "text": "provided feature in label spec and it'll",
    "start": "1153520",
    "end": "1155440"
  },
  {
    "text": "try to smartly infer",
    "start": "1155440",
    "end": "1156880"
  },
  {
    "text": "that spec as well and we might add like",
    "start": "1156880",
    "end": "1159120"
  },
  {
    "text": "auto detection based upon",
    "start": "1159120",
    "end": "1160880"
  },
  {
    "text": "um uh introspecting your data to try to",
    "start": "1160880",
    "end": "1163520"
  },
  {
    "text": "auto detect that spec",
    "start": "1163520",
    "end": "1164880"
  },
  {
    "text": "in the future for example from the",
    "start": "1164880",
    "end": "1166799"
  },
  {
    "text": "parquet metadata",
    "start": "1166799",
    "end": "1168400"
  },
  {
    "text": "and this will seamlessly integrate with",
    "start": "1168400",
    "end": "1170640"
  },
  {
    "text": "torch training on",
    "start": "1170640",
    "end": "1172160"
  },
  {
    "text": "horovod on ray as well so distributed",
    "start": "1172160",
    "end": "1175200"
  },
  {
    "text": "training all",
    "start": "1175200",
    "end": "1175840"
  },
  {
    "text": "on top of ray using the horvat",
    "start": "1175840",
    "end": "1178320"
  },
  {
    "text": "integration",
    "start": "1178320",
    "end": "1179600"
  },
  {
    "text": "to briefly go over that horvat is a",
    "start": "1179600",
    "end": "1181679"
  },
  {
    "text": "distributed deep learning training",
    "start": "1181679",
    "end": "1182799"
  },
  {
    "text": "framework that supports",
    "start": "1182799",
    "end": "1184160"
  },
  {
    "text": "all of your favorite machine learning",
    "start": "1184160",
    "end": "1186240"
  },
  {
    "text": "frameworks",
    "start": "1186240",
    "end": "1187840"
  },
  {
    "text": "particularly you know tensorflow and",
    "start": "1187840",
    "end": "1189200"
  },
  {
    "text": "pytorch very popular",
    "start": "1189200",
    "end": "1191520"
  },
  {
    "text": "and you can actually run horvaad on",
    "start": "1191520",
    "end": "1193360"
  },
  {
    "text": "array cluster",
    "start": "1193360",
    "end": "1194720"
  },
  {
    "text": "this kind of shows the an example api of",
    "start": "1194720",
    "end": "1197600"
  },
  {
    "text": "how to start it up",
    "start": "1197600",
    "end": "1199120"
  },
  {
    "text": "um i won't go into that too much but",
    "start": "1199120",
    "end": "1201039"
  },
  {
    "text": "within that training function",
    "start": "1201039",
    "end": "1202880"
  },
  {
    "text": "you can actually use our data set uh",
    "start": "1202880",
    "end": "1206000"
  },
  {
    "text": "uh like that will run uh on each",
    "start": "1206000",
    "end": "1209440"
  },
  {
    "text": "uh training worker and uh just yield",
    "start": "1209440",
    "end": "1212000"
  },
  {
    "text": "those gpu batches as you would with your",
    "start": "1212000",
    "end": "1213840"
  },
  {
    "text": "normal like torch flow when iterating",
    "start": "1213840",
    "end": "1216000"
  },
  {
    "start": "1216000",
    "end": "1216000"
  },
  {
    "text": "over a data loader",
    "start": "1216000",
    "end": "1217679"
  },
  {
    "text": "so the architecture of the shuffling",
    "start": "1217679",
    "end": "1220480"
  },
  {
    "text": "data loader is a",
    "start": "1220480",
    "end": "1221440"
  },
  {
    "text": "is i think interesting thing to talk",
    "start": "1221440",
    "end": "1222960"
  },
  {
    "text": "about first you have some",
    "start": "1222960",
    "end": "1224640"
  },
  {
    "text": "input data in your store you",
    "start": "1224640",
    "end": "1227840"
  },
  {
    "text": "partition that data and send it out to",
    "start": "1227840",
    "end": "1229760"
  },
  {
    "text": "some number of mappers",
    "start": "1229760",
    "end": "1231360"
  },
  {
    "text": "those mappers will then further",
    "start": "1231360",
    "end": "1233440"
  },
  {
    "text": "partition or each mapper will",
    "start": "1233440",
    "end": "1235360"
  },
  {
    "text": "partition into your number of redo your",
    "start": "1235360",
    "end": "1238080"
  },
  {
    "text": "number reducers",
    "start": "1238080",
    "end": "1239200"
  },
  {
    "text": "uh like partitions so mapper one will",
    "start": "1239200",
    "end": "1242400"
  },
  {
    "text": "send",
    "start": "1242400",
    "end": "1242799"
  },
  {
    "text": "num reducer partitions out um",
    "start": "1242799",
    "end": "1245840"
  },
  {
    "text": "so further chunking of the data and then",
    "start": "1245840",
    "end": "1248480"
  },
  {
    "text": "the reducer is in charge of shuffling",
    "start": "1248480",
    "end": "1250640"
  },
  {
    "text": "that data",
    "start": "1250640",
    "end": "1252080"
  },
  {
    "text": "and then sending that data to a queue to",
    "start": "1252080",
    "end": "1254080"
  },
  {
    "text": "this global queue",
    "start": "1254080",
    "end": "1255440"
  },
  {
    "text": "and internally that global queue will",
    "start": "1255440",
    "end": "1257120"
  },
  {
    "text": "have a couple of local cues that each",
    "start": "1257120",
    "end": "1259200"
  },
  {
    "text": "trainer",
    "start": "1259200",
    "end": "1259840"
  },
  {
    "text": "will pull from individually and",
    "start": "1259840",
    "end": "1262880"
  },
  {
    "text": "so that's essentially that's really just",
    "start": "1262880",
    "end": "1264559"
  },
  {
    "text": "the shuffling implementation and the",
    "start": "1264559",
    "end": "1265840"
  },
  {
    "text": "reducer will do like",
    "start": "1265840",
    "end": "1267200"
  },
  {
    "text": "sample level shuffling like it'll take",
    "start": "1267200",
    "end": "1269520"
  },
  {
    "text": "all of its mapper partitions",
    "start": "1269520",
    "end": "1271200"
  },
  {
    "text": "and shuffle up those samples and",
    "start": "1271200",
    "end": "1274480"
  },
  {
    "text": "also this queue is streaming it just",
    "start": "1274480",
    "end": "1276480"
  },
  {
    "text": "works with uh",
    "start": "1276480",
    "end": "1277919"
  },
  {
    "text": "with references to the data not the",
    "start": "1277919",
    "end": "1280480"
  },
  {
    "text": "actual data",
    "start": "1280480",
    "end": "1281440"
  },
  {
    "text": "so the trainers will pull directly from",
    "start": "1281440",
    "end": "1283360"
  },
  {
    "text": "the reducers the queue is just in charge",
    "start": "1283360",
    "end": "1285440"
  },
  {
    "text": "of",
    "start": "1285440",
    "end": "1286240"
  },
  {
    "text": "uh queuing up those references and also",
    "start": "1286240",
    "end": "1289120"
  },
  {
    "text": "applying back pressure on the shuffler",
    "start": "1289120",
    "end": "1290640"
  },
  {
    "text": "which i'll talk about in a bit",
    "start": "1290640",
    "end": "1292640"
  },
  {
    "text": "we have a couple of key optimizations um",
    "start": "1292640",
    "end": "1295200"
  },
  {
    "text": "we have",
    "start": "1295200",
    "end": "1296080"
  },
  {
    "text": "uh pipelining pre-fetching",
    "start": "1296080",
    "end": "1299760"
  },
  {
    "text": "and uh i'll hint at data locality i'm",
    "start": "1299760",
    "end": "1302799"
  },
  {
    "text": "not going to talk about that in this",
    "start": "1302799",
    "end": "1303919"
  },
  {
    "text": "talk but we're currently working on",
    "start": "1303919",
    "end": "1305840"
  },
  {
    "text": "co-locating reducers with trainers to",
    "start": "1305840",
    "end": "1307520"
  },
  {
    "text": "minimize data transfer",
    "start": "1307520",
    "end": "1309679"
  },
  {
    "text": "first pipelining pipelining",
    "start": "1309679",
    "end": "1313039"
  },
  {
    "text": "what we do is we pipeline data shuffling",
    "start": "1313039",
    "end": "1315679"
  },
  {
    "text": "with",
    "start": "1315679",
    "end": "1317120"
  },
  {
    "text": "your data your model training um so the",
    "start": "1317120",
    "end": "1319760"
  },
  {
    "text": "shuffling data for future epochs is done",
    "start": "1319760",
    "end": "1321679"
  },
  {
    "text": "concurrently with training for your",
    "start": "1321679",
    "end": "1323200"
  },
  {
    "text": "current epoch",
    "start": "1323200",
    "end": "1324240"
  },
  {
    "text": "and to briefly illustrate this um let's",
    "start": "1324240",
    "end": "1326559"
  },
  {
    "text": "say we kick off shuffling for epoch 1",
    "start": "1326559",
    "end": "1328960"
  },
  {
    "text": "and then we can also concurrently kick",
    "start": "1328960",
    "end": "1330960"
  },
  {
    "text": "off shuffling for epoch",
    "start": "1330960",
    "end": "1332000"
  },
  {
    "text": "2. in this case you can see these two",
    "start": "1332000",
    "end": "1333600"
  },
  {
    "text": "shuffling lines as like the two slots",
    "start": "1333600",
    "end": "1336080"
  },
  {
    "text": "because we allow at most two concurrent",
    "start": "1336080",
    "end": "1337919"
  },
  {
    "text": "epoch shuffles",
    "start": "1337919",
    "end": "1339360"
  },
  {
    "text": "at once um so those two are kicked off",
    "start": "1339360",
    "end": "1341600"
  },
  {
    "text": "concurrently",
    "start": "1341600",
    "end": "1342480"
  },
  {
    "text": "and then once the first epoch worth of",
    "start": "1342480",
    "end": "1345039"
  },
  {
    "text": "data",
    "start": "1345039",
    "end": "1345520"
  },
  {
    "text": "is done being shuffled we can send it",
    "start": "1345520",
    "end": "1347520"
  },
  {
    "text": "off to a trainer",
    "start": "1347520",
    "end": "1348799"
  },
  {
    "text": "um and then this trainer will say uh we",
    "start": "1348799",
    "end": "1351520"
  },
  {
    "text": "don't consider",
    "start": "1351520",
    "end": "1352559"
  },
  {
    "text": "that like a shuffled data for that epoch",
    "start": "1352559",
    "end": "1354640"
  },
  {
    "text": "released until that trainer",
    "start": "1354640",
    "end": "1356480"
  },
  {
    "text": "is done with uh training on the first",
    "start": "1356480",
    "end": "1358559"
  },
  {
    "text": "epoch so that's what the yellow",
    "start": "1358559",
    "end": "1360080"
  },
  {
    "text": "indicates there",
    "start": "1360080",
    "end": "1360880"
  },
  {
    "text": "like we can't uh we can't uh we",
    "start": "1360880",
    "end": "1363679"
  },
  {
    "text": "essentially can't put any",
    "start": "1363679",
    "end": "1365280"
  },
  {
    "text": "start any new shuffles there that's just",
    "start": "1365280",
    "end": "1366960"
  },
  {
    "text": "an illustration of our throttling on the",
    "start": "1366960",
    "end": "1369039"
  },
  {
    "text": "shuffling",
    "start": "1369039",
    "end": "1369919"
  },
  {
    "text": "and then once uh training for epoch 1 is",
    "start": "1369919",
    "end": "1372559"
  },
  {
    "text": "done we kick off the shuffling for epoch",
    "start": "1372559",
    "end": "1374320"
  },
  {
    "text": "3.",
    "start": "1374320",
    "end": "1375679"
  },
  {
    "text": "and then we can also immediately kick",
    "start": "1375679",
    "end": "1377440"
  },
  {
    "text": "off the training for epoch 2 because we",
    "start": "1377440",
    "end": "1379360"
  },
  {
    "text": "can currently shuffled",
    "start": "1379360",
    "end": "1380960"
  },
  {
    "text": "in advance the data for epoch 2.",
    "start": "1380960",
    "end": "1384400"
  },
  {
    "text": "and then obviously i'm showing here how",
    "start": "1384400",
    "end": "1386960"
  },
  {
    "text": "slot number two is blocked until",
    "start": "1386960",
    "end": "1388799"
  },
  {
    "text": "training on epoch two is done and then",
    "start": "1388799",
    "end": "1391280"
  },
  {
    "text": "as soon as uh training on epoch two is",
    "start": "1391280",
    "end": "1393120"
  },
  {
    "text": "done we can start like another",
    "start": "1393120",
    "end": "1395200"
  },
  {
    "text": "uh another um epoch shuffle we could",
    "start": "1395200",
    "end": "1397679"
  },
  {
    "text": "kick that off",
    "start": "1397679",
    "end": "1398640"
  },
  {
    "text": "and so on and so forth um so that's",
    "start": "1398640",
    "end": "1401039"
  },
  {
    "text": "essentially uh",
    "start": "1401039",
    "end": "1402799"
  },
  {
    "text": "how we do pipelining and the end result",
    "start": "1402799",
    "end": "1406720"
  },
  {
    "text": "is that if the throughput of the",
    "start": "1406720",
    "end": "1407840"
  },
  {
    "text": "shuffler is greater than or equal to the",
    "start": "1407840",
    "end": "1409840"
  },
  {
    "text": "throughput of the training um after the",
    "start": "1409840",
    "end": "1412400"
  },
  {
    "text": "first epoch",
    "start": "1412400",
    "end": "1413520"
  },
  {
    "text": "uh all the trainers will always see hot",
    "start": "1413520",
    "end": "1415919"
  },
  {
    "text": "gpu batches",
    "start": "1415919",
    "end": "1417120"
  },
  {
    "text": "after that first epoch um so they should",
    "start": "1417120",
    "end": "1419840"
  },
  {
    "text": "never be sitting there waiting for a",
    "start": "1419840",
    "end": "1421120"
  },
  {
    "text": "shuffle to complete",
    "start": "1421120",
    "end": "1422400"
  },
  {
    "text": "and then the back pressure on the",
    "start": "1422400",
    "end": "1423760"
  },
  {
    "text": "shuffle is enforced by the queue",
    "start": "1423760",
    "end": "1425840"
  },
  {
    "text": "adhering to that throttle limit that we",
    "start": "1425840",
    "end": "1427440"
  },
  {
    "text": "mentioned before and as illustrated in",
    "start": "1427440",
    "end": "1429760"
  },
  {
    "text": "this diagram",
    "start": "1429760",
    "end": "1431600"
  },
  {
    "text": "in addition to that we also have",
    "start": "1431600",
    "end": "1432960"
  },
  {
    "text": "pre-fetching from the queue where",
    "start": "1432960",
    "end": "1434400"
  },
  {
    "text": "trainers will pre-fetch",
    "start": "1434400",
    "end": "1435840"
  },
  {
    "text": "the batches of shuffle outputs from the",
    "start": "1435840",
    "end": "1437760"
  },
  {
    "text": "queue and this",
    "start": "1437760",
    "end": "1438880"
  },
  {
    "text": "pipeline is basically data transfer",
    "start": "1438880",
    "end": "1440559"
  },
  {
    "text": "across nodes",
    "start": "1440559",
    "end": "1442000"
  },
  {
    "text": "uh with uh training on the model and so",
    "start": "1442000",
    "end": "1444799"
  },
  {
    "text": "this is just a small further",
    "start": "1444799",
    "end": "1446080"
  },
  {
    "text": "optimization",
    "start": "1446080",
    "end": "1447120"
  },
  {
    "text": "so there's an example of synchronous",
    "start": "1447120",
    "end": "1448640"
  },
  {
    "text": "fetching from the queue uh versus here",
    "start": "1448640",
    "end": "1450799"
  },
  {
    "text": "where we say give us back for example",
    "start": "1450799",
    "end": "1452880"
  },
  {
    "text": "like uh up to three gpu batches",
    "start": "1452880",
    "end": "1455919"
  },
  {
    "text": "at once and that just it's just a",
    "start": "1455919",
    "end": "1458480"
  },
  {
    "text": "smaller little optimization to",
    "start": "1458480",
    "end": "1460480"
  },
  {
    "text": "mitigate um the number of times that",
    "start": "1460480",
    "end": "1462240"
  },
  {
    "text": "we're sitting there waiting for data to",
    "start": "1462240",
    "end": "1463520"
  },
  {
    "text": "be transferred across",
    "start": "1463520",
    "end": "1464400"
  },
  {
    "text": "nodes and so that also is a big help to",
    "start": "1464400",
    "end": "1467279"
  },
  {
    "text": "mitigate uh",
    "start": "1467279",
    "end": "1468400"
  },
  {
    "text": "stragglers when we're sitting there",
    "start": "1468400",
    "end": "1469919"
  },
  {
    "text": "waiting for data to be transferred",
    "start": "1469919",
    "end": "1472080"
  },
  {
    "text": "and then if there's a gpu batch",
    "start": "1472080",
    "end": "1473679"
  },
  {
    "text": "available trainers will always see that",
    "start": "1473679",
    "end": "1476000"
  },
  {
    "text": "as soon as possible and so we've found",
    "start": "1476000",
    "end": "1478000"
  },
  {
    "text": "in practice that this",
    "start": "1478000",
    "end": "1479520"
  },
  {
    "text": "decreases average and max batch wait",
    "start": "1479520",
    "end": "1481279"
  },
  {
    "text": "time on trainers which is great",
    "start": "1481279",
    "end": "1482799"
  },
  {
    "text": "training goes faster and then i'll",
    "start": "1482799",
    "end": "1485679"
  },
  {
    "text": "briefly talk about data locality but",
    "start": "1485679",
    "end": "1487200"
  },
  {
    "text": "i'll go into this more probably in",
    "start": "1487200",
    "end": "1488960"
  },
  {
    "text": "in a future talk um we're currently",
    "start": "1488960",
    "end": "1490799"
  },
  {
    "text": "exploring placing shuffle reducers on",
    "start": "1490799",
    "end": "1492400"
  },
  {
    "text": "the same note as trainers as i mentioned",
    "start": "1492400",
    "end": "1493919"
  },
  {
    "text": "before",
    "start": "1493919",
    "end": "1494880"
  },
  {
    "text": "and then this should virtually eliminate",
    "start": "1494880",
    "end": "1496880"
  },
  {
    "text": "the communication between reducers and",
    "start": "1496880",
    "end": "1498480"
  },
  {
    "text": "trainers",
    "start": "1498480",
    "end": "1499039"
  },
  {
    "text": "which is great higher throughput less",
    "start": "1499039",
    "end": "1500799"
  },
  {
    "text": "batch weight time spikes",
    "start": "1500799",
    "end": "1502880"
  },
  {
    "start": "1502000",
    "end": "1502000"
  },
  {
    "text": "and so this rate-based solution",
    "start": "1502880",
    "end": "1504880"
  },
  {
    "text": "addresses uh",
    "start": "1504880",
    "end": "1506159"
  },
  {
    "text": "the following pitfalls that we talked",
    "start": "1506159",
    "end": "1507360"
  },
  {
    "text": "about before it's framework agnostic",
    "start": "1507360",
    "end": "1509760"
  },
  {
    "text": "the shuffle is truly global it's a truly",
    "start": "1509760",
    "end": "1512159"
  },
  {
    "text": "global shuffle",
    "start": "1512159",
    "end": "1513440"
  },
  {
    "text": "it mitigates the performance issues that",
    "start": "1513440",
    "end": "1515919"
  },
  {
    "text": "the pi torch data loader and petastorm",
    "start": "1515919",
    "end": "1518400"
  },
  {
    "text": "data loaders have experienced via",
    "start": "1518400",
    "end": "1520159"
  },
  {
    "text": "pipelining pre-fetching and then also",
    "start": "1520159",
    "end": "1521919"
  },
  {
    "text": "raised system characteristics",
    "start": "1521919",
    "end": "1523760"
  },
  {
    "text": "it runs on a shared ray cluster with",
    "start": "1523760",
    "end": "1525679"
  },
  {
    "text": "your trainers",
    "start": "1525679",
    "end": "1526880"
  },
  {
    "text": "it's easy to integrate with all of your",
    "start": "1526880",
    "end": "1528799"
  },
  {
    "text": "favorite machine learning frameworks",
    "start": "1528799",
    "end": "1530159"
  },
  {
    "text": "it's essentially just an iterator api",
    "start": "1530159",
    "end": "1532080"
  },
  {
    "text": "which",
    "start": "1532080",
    "end": "1532640"
  },
  {
    "text": "super easy to you know match any ml",
    "start": "1532640",
    "end": "1535120"
  },
  {
    "text": "frameworks dataset api",
    "start": "1535120",
    "end": "1537440"
  },
  {
    "text": "a brief discussion of some benchmarks",
    "start": "1537440",
    "end": "1540640"
  },
  {
    "text": "when scaling the cluster for example um",
    "start": "1540640",
    "end": "1543919"
  },
  {
    "text": "the benchmark config we're reading this",
    "start": "1543919",
    "end": "1545360"
  },
  {
    "text": "data from s3",
    "start": "1545360",
    "end": "1546880"
  },
  {
    "text": "those are the ec2 nope types and then we",
    "start": "1546880",
    "end": "1549120"
  },
  {
    "text": "have 110 gigabytes object store per node",
    "start": "1549120",
    "end": "1551760"
  },
  {
    "text": "and as you can see here when scaling",
    "start": "1551760",
    "end": "1554000"
  },
  {
    "text": "from",
    "start": "1554000",
    "end": "1555120"
  },
  {
    "text": "one node which is at uh four million",
    "start": "1555120",
    "end": "1557200"
  },
  {
    "text": "records per second throughput",
    "start": "1557200",
    "end": "1558799"
  },
  {
    "text": "uh we achieve 5.5 million records per uh",
    "start": "1558799",
    "end": "1561440"
  },
  {
    "text": "per second",
    "start": "1561440",
    "end": "1562159"
  },
  {
    "text": "uh throughput and this is uh not",
    "start": "1562159",
    "end": "1564960"
  },
  {
    "text": "adhering to linear scaling because",
    "start": "1564960",
    "end": "1566640"
  },
  {
    "text": "there's some",
    "start": "1566640",
    "end": "1567279"
  },
  {
    "text": "added overhead when going from one node",
    "start": "1567279",
    "end": "1569039"
  },
  {
    "text": "to four nodes um",
    "start": "1569039",
    "end": "1570559"
  },
  {
    "text": "that's all that you're always going to",
    "start": "1570559",
    "end": "1571919"
  },
  {
    "text": "hit and then also in addition",
    "start": "1571919",
    "end": "1574000"
  },
  {
    "text": "um shuffling is an all to all dependency",
    "start": "1574000",
    "end": "1576640"
  },
  {
    "text": "so",
    "start": "1576640",
    "end": "1577039"
  },
  {
    "text": "shuffling uh you're not going to be able",
    "start": "1577039",
    "end": "1578720"
  },
  {
    "text": "to get linear scaling with something",
    "start": "1578720",
    "end": "1580080"
  },
  {
    "text": "like shuffling so what we're looking at",
    "start": "1580080",
    "end": "1581440"
  },
  {
    "text": "here",
    "start": "1581440",
    "end": "1582080"
  },
  {
    "text": "is we don't we want to see a little bit",
    "start": "1582080",
    "end": "1583919"
  },
  {
    "text": "of uptick",
    "start": "1583919",
    "end": "1585120"
  },
  {
    "text": "in the throughput ideally because we",
    "start": "1585120",
    "end": "1586640"
  },
  {
    "text": "have more compute resources available",
    "start": "1586640",
    "end": "1588320"
  },
  {
    "text": "but we",
    "start": "1588320",
    "end": "1588799"
  },
  {
    "text": "really just don't want to see a",
    "start": "1588799",
    "end": "1589840"
  },
  {
    "text": "degradation and throughput",
    "start": "1589840",
    "end": "1592159"
  },
  {
    "text": "and then as far as for scaling the data",
    "start": "1592159",
    "end": "1593919"
  },
  {
    "text": "set size um",
    "start": "1593919",
    "end": "1596080"
  },
  {
    "text": "uh those last two data points there with",
    "start": "1596080",
    "end": "1597919"
  },
  {
    "text": "200 million records",
    "start": "1597919",
    "end": "1599520"
  },
  {
    "text": "uh this one is with uh five uh 400",
    "start": "1599520",
    "end": "1602720"
  },
  {
    "text": "million records",
    "start": "1602720",
    "end": "1603600"
  },
  {
    "text": "and we see that we still stay around uh",
    "start": "1603600",
    "end": "1606640"
  },
  {
    "text": "that like five million records per",
    "start": "1606640",
    "end": "1608080"
  },
  {
    "text": "second throughput and we also have some",
    "start": "1608080",
    "end": "1609679"
  },
  {
    "text": "recent optimizations that were probably",
    "start": "1609679",
    "end": "1612240"
  },
  {
    "text": "greatly improve this baseline throughput",
    "start": "1612240",
    "end": "1614400"
  },
  {
    "text": "this is a rather naive implementation",
    "start": "1614400",
    "end": "1616400"
  },
  {
    "text": "from",
    "start": "1616400",
    "end": "1616799"
  },
  {
    "text": "earlier user stories so",
    "start": "1616799",
    "end": "1619919"
  },
  {
    "text": "how happy is uber with this for example",
    "start": "1619919",
    "end": "1622080"
  },
  {
    "text": "whose user we're looking we're working",
    "start": "1622080",
    "end": "1623760"
  },
  {
    "text": "with closely",
    "start": "1623760",
    "end": "1624880"
  },
  {
    "text": "uber is currently experimenting with",
    "start": "1624880",
    "end": "1626400"
  },
  {
    "text": "integrating the shuffling data loader",
    "start": "1626400",
    "end": "1627840"
  },
  {
    "text": "into their deep learning pipelines",
    "start": "1627840",
    "end": "1630000"
  },
  {
    "text": "and at their current data scale we were",
    "start": "1630000",
    "end": "1631760"
  },
  {
    "text": "able to consistently deliver these",
    "start": "1631760",
    "end": "1633679"
  },
  {
    "text": "statistically great globally shuffled",
    "start": "1633679",
    "end": "1635520"
  },
  {
    "text": "gpu batches to their trainers in 100",
    "start": "1635520",
    "end": "1637520"
  },
  {
    "text": "milliseconds after the first epoch",
    "start": "1637520",
    "end": "1640000"
  },
  {
    "text": "so the trainers really never have to",
    "start": "1640000",
    "end": "1641919"
  },
  {
    "text": "wait like at all",
    "start": "1641919",
    "end": "1644799"
  },
  {
    "text": "the key takeaways from this talk per",
    "start": "1644799",
    "end": "1647679"
  },
  {
    "text": "epoch shuffling is needed to avoid",
    "start": "1647679",
    "end": "1649440"
  },
  {
    "text": "overfitting from many deep learning",
    "start": "1649440",
    "end": "1650799"
  },
  {
    "text": "workloads",
    "start": "1650799",
    "end": "1651600"
  },
  {
    "text": "as we talked about earlier ray has a",
    "start": "1651600",
    "end": "1653760"
  },
  {
    "text": "compelling data loader that seamlessly",
    "start": "1653760",
    "end": "1655279"
  },
  {
    "text": "integrates with torch training on horvat",
    "start": "1655279",
    "end": "1657039"
  },
  {
    "text": "on ray",
    "start": "1657039",
    "end": "1658159"
  },
  {
    "text": "and ray as a common substrate for your",
    "start": "1658159",
    "end": "1660480"
  },
  {
    "text": "machine learning pipeline gives you best",
    "start": "1660480",
    "end": "1662159"
  },
  {
    "text": "in class ux performance",
    "start": "1662159",
    "end": "1663760"
  },
  {
    "text": "and ops if you",
    "start": "1663760",
    "end": "1667200"
  },
  {
    "text": "if you want please connect with us i",
    "start": "1667200",
    "end": "1669279"
  },
  {
    "text": "have some links here the slides will be",
    "start": "1669279",
    "end": "1670880"
  },
  {
    "text": "shared",
    "start": "1670880",
    "end": "1671440"
  },
  {
    "text": "you can check out the data loader repo",
    "start": "1671440",
    "end": "1673120"
  },
  {
    "text": "you can come talk to us on the ray",
    "start": "1673120",
    "end": "1674559"
  },
  {
    "text": "discussion forum",
    "start": "1674559",
    "end": "1675600"
  },
  {
    "text": "and also as always we're hiring so if",
    "start": "1675600",
    "end": "1677600"
  },
  {
    "text": "any of these problems are interesting to",
    "start": "1677600",
    "end": "1678960"
  },
  {
    "text": "you please",
    "start": "1678960",
    "end": "1679520"
  },
  {
    "text": "come talk with us thank you and enjoy",
    "start": "1679520",
    "end": "1682399"
  },
  {
    "text": "the rest of the summit",
    "start": "1682399",
    "end": "1688399"
  }
]