[
  {
    "start": "0",
    "end": "17000"
  },
  {
    "text": "hi good morning everyone thanks for stopping in this virtual presentation my name is anna luo i'm an applied",
    "start": "2879",
    "end": "9760"
  },
  {
    "text": "scientist at aws sagemaker it's my great pleasure to present here today",
    "start": "9760",
    "end": "15280"
  },
  {
    "text": "at resubmit let's start with the content of today's presentation",
    "start": "15280",
    "end": "20800"
  },
  {
    "start": "17000",
    "end": "105000"
  },
  {
    "text": "in principle we will have two components first we will talk about what is amazon sage making and will",
    "start": "20800",
    "end": "28240"
  },
  {
    "text": "use reinforcement learning as an example to demonstrate how to build a typical reinforcement",
    "start": "28240",
    "end": "34320"
  },
  {
    "text": "slash machine learning workflow in practice today and then i will introduce you how you",
    "start": "34320",
    "end": "40000"
  },
  {
    "text": "can run rey and earl lips seamlessly on amazon sagemaker together with the",
    "start": "40000",
    "end": "45520"
  },
  {
    "text": "ability to scale and save your costs with amazon sagemaker plus are all lit",
    "start": "45520",
    "end": "51840"
  },
  {
    "text": "in the second part of the presentation today we will focus on a real-world reinforcement learning use",
    "start": "51840",
    "end": "57120"
  },
  {
    "text": "case specifically we'll talk about inventory management problem i will walk",
    "start": "57120",
    "end": "62879"
  },
  {
    "text": "you through the problem dynamics as well as the formulation and i'll introduce you how can we use",
    "start": "62879",
    "end": "70080"
  },
  {
    "text": "historical data to construct a simulator so that the ro agent can use to develop its policy",
    "start": "70080",
    "end": "77439"
  },
  {
    "text": "most importantly i will introduce what are the state's actions and rewards in the formulation",
    "start": "77439",
    "end": "83439"
  },
  {
    "text": "of the corresponding markov decision process last but not least i will demonstrate",
    "start": "83439",
    "end": "88560"
  },
  {
    "text": "some preliminary experiments results that we got using both oral reinforcement learning",
    "start": "88560",
    "end": "94720"
  },
  {
    "text": "agents as well as the traditional operations research algorithms in the end we will finish this",
    "start": "94720",
    "end": "101920"
  },
  {
    "text": "presentation with a quick recap so let's talk about amazon sagemaker",
    "start": "101920",
    "end": "108399"
  },
  {
    "start": "105000",
    "end": "154000"
  },
  {
    "text": "first what is amazon sagemaker it is a fully managed service that provides every",
    "start": "108399",
    "end": "114000"
  },
  {
    "text": "developer and scientist with the ability to build train and deploy machine learning models",
    "start": "114000",
    "end": "119119"
  },
  {
    "text": "quickly it provides an integrated reader notebook interface for easy access to",
    "start": "119119",
    "end": "124159"
  },
  {
    "text": "the data source for exploration and analysis so you don't have to manage service it also provides common",
    "start": "124159",
    "end": "131200"
  },
  {
    "text": "machine learning algorithms as well as ray and dora lip for reinforcement learning and i will elaborate that a little bit",
    "start": "131200",
    "end": "137360"
  },
  {
    "text": "more later on you can deploy an rl model into a secure and scalable environment by launching it with",
    "start": "137360",
    "end": "143360"
  },
  {
    "text": "a few clicks so in a nutshell we are making it easy for you to build skill and apply url model impractical",
    "start": "143360",
    "end": "152879"
  },
  {
    "start": "154000",
    "end": "399000"
  },
  {
    "text": "so before we dive into the service itself let's just look at what it actually means to build a",
    "start": "154720",
    "end": "159760"
  },
  {
    "text": "typical machine learning workflow in practice today unfortunately it's pretty difficult task",
    "start": "159760",
    "end": "165040"
  },
  {
    "text": "and you have to go through a number of different steps there are so many tools that you can use for each of them and everything needs to",
    "start": "165040",
    "end": "171120"
  },
  {
    "text": "line up perfectly if you want to build a successful model and deploy it so of course first you need to prepare",
    "start": "171120",
    "end": "177760"
  },
  {
    "text": "data set you need to annotate and clean and transform data and then get your training set ready",
    "start": "177760",
    "end": "183519"
  },
  {
    "text": "for reinforcement learning this usually corresponds to the process of developing a simulated environment",
    "start": "183519",
    "end": "190000"
  },
  {
    "text": "once you have that you can start with experimentation you can start with trying out different algorithms in this case you can use the",
    "start": "190000",
    "end": "196959"
  },
  {
    "text": "suite of state-of-the-art algorithms that are only provides then once you start to figure out which",
    "start": "196959",
    "end": "202959"
  },
  {
    "text": "organism is going to work you need to try different primary combinations that requires a bunch of infrastructure",
    "start": "202959",
    "end": "209840"
  },
  {
    "text": "no one actually wants to manage that but you have to do it to power those training jobs",
    "start": "209840",
    "end": "215120"
  },
  {
    "text": "then you need to tune them you need to extract every bit of accuracy from those training jobs in order to get the best",
    "start": "215120",
    "end": "221040"
  },
  {
    "text": "possible model and the best business outcome over the time of the project you're going to end up with",
    "start": "221040",
    "end": "227519"
  },
  {
    "text": "training numbers of thousands of jobs and it's not easy to recall what you did last week once you have had a model",
    "start": "227519",
    "end": "236239"
  },
  {
    "text": "with satisfying performance it comes to the hardest component which is deploying that model",
    "start": "236239",
    "end": "241280"
  },
  {
    "text": "in production where it can start making the actual decisions from the request that your",
    "start": "241280",
    "end": "246799"
  },
  {
    "text": "business apps send to it so as you can see although people tend to focus a lot on",
    "start": "246799",
    "end": "252560"
  },
  {
    "text": "algorithms and neural networks and distributed training there is quite a lot of stuff that needs to be done if you want your oral",
    "start": "252560",
    "end": "258560"
  },
  {
    "text": "proctor to be successful this stands in the way of achieving your expected outcome which is the purpose of amazon",
    "start": "258560",
    "end": "264880"
  },
  {
    "text": "sagemaker in last weekend we expanded on stage maker and added a bunch of new",
    "start": "264880",
    "end": "270160"
  },
  {
    "text": "capabilities in addition to reinforcement specific offerings we will look into the spot in",
    "start": "270160",
    "end": "275840"
  },
  {
    "text": "suit today as an rl practitioner you may wonder why",
    "start": "275840",
    "end": "281199"
  },
  {
    "text": "amazon stagemaker can be a good place for you to hold your workflow compared to other cloud-based",
    "start": "281199",
    "end": "286560"
  },
  {
    "text": "machine learning services such as self-managed amazon ec2 and aws managed amazon eks you must",
    "start": "286560",
    "end": "293440"
  },
  {
    "text": "consider the infrastructure operational and security costs for each step of your rr workflow",
    "start": "293440",
    "end": "298800"
  },
  {
    "text": "as well as the size and expertise of your rl science teams the total cost of ownership is often the",
    "start": "298800",
    "end": "304960"
  },
  {
    "text": "financial metric that you use to estimate and compare machine learning cost and the detailed total cost of ownership",
    "start": "304960",
    "end": "311440"
  },
  {
    "text": "analysis conducted earlier showed that amazon sagemaker provides a better total cost ownership across teams",
    "start": "311440",
    "end": "318000"
  },
  {
    "text": "of all sizes specifically that's 54 lower than ec2 and eks over three years",
    "start": "318000",
    "end": "325440"
  },
  {
    "text": "with self-managed machine learning with ec2 you take on the responsibility of provisioning and managing ec2 instances",
    "start": "325440",
    "end": "332160"
  },
  {
    "text": "including instance failure recovery patching automatic scaling and building and maintaining request",
    "start": "332160",
    "end": "337680"
  },
  {
    "text": "security and compliance you can use the aws deep learning armies with the machine learning frameworks and",
    "start": "337680",
    "end": "343919"
  },
  {
    "text": "libraries previewed which is also provided in ray but you still need to optimize data access to get high",
    "start": "343919",
    "end": "350160"
  },
  {
    "text": "throughput and also optimize your setup to scale up in contrast amazon savemaker is fully",
    "start": "350160",
    "end": "355759"
  },
  {
    "text": "managed you don't need to build manage or maintain any infrastructure or two to support rl amazon sagemaker also runs",
    "start": "355759",
    "end": "363120"
  },
  {
    "text": "your model on auto scaling clusters that are spread around multiple availability zones to deliver",
    "start": "363120",
    "end": "368479"
  },
  {
    "text": "both high performance and high availability because you pay for storage network",
    "start": "368479",
    "end": "373759"
  },
  {
    "text": "based on your usage costs are controlled amazon sagemaker has built in security and compliance for",
    "start": "373759",
    "end": "379600"
  },
  {
    "text": "machine learning models so you don't need to invest in additional security",
    "start": "379600",
    "end": "384880"
  },
  {
    "text": "training and hosting are built by minutes of usage with no minimum fees and no upfront commitments",
    "start": "384880",
    "end": "391199"
  },
  {
    "text": "you can also use by instance to take advantage of unused compute capacity in aws cloud just like what you can do with",
    "start": "391199",
    "end": "397759"
  },
  {
    "text": "midi ect now let's move to reinforcement learning specific part",
    "start": "397759",
    "end": "403360"
  },
  {
    "start": "399000",
    "end": "500000"
  },
  {
    "text": "amazon stage maker rl built on top of amazon sagemaker adding pre-packaged reinforcement toolkits and making it easy for you to",
    "start": "403360",
    "end": "410160"
  },
  {
    "text": "integrate various simulation environment as you would expect training and production infrastructure is fully managed",
    "start": "410160",
    "end": "416960"
  },
  {
    "text": "so that you can focus on the ro research problem and not on managing service today you can use containers provided by",
    "start": "416960",
    "end": "423919"
  },
  {
    "text": "sagemaker for both tensorflow and pytorch that includes open hygiene and oral lip as euro with",
    "start": "423919",
    "end": "430319"
  },
  {
    "text": "samsung sagemaker you can easily create our own custom container environment using other rl",
    "start": "430319",
    "end": "435919"
  },
  {
    "text": "tokens if necessary in terms of the oral algorithms we",
    "start": "435919",
    "end": "440960"
  },
  {
    "text": "support code from intel ray or elite from ending skill and opennight based lines users found rey",
    "start": "440960",
    "end": "446639"
  },
  {
    "text": "are left handy for large-scale reinforcement learning experiments when it comes to simulation",
    "start": "446639",
    "end": "453280"
  },
  {
    "text": "environments amazon sagemaker aura enables you to connect to various types of simulators",
    "start": "453280",
    "end": "458639"
  },
  {
    "text": "first it supports first party simulators such as aws robomaker as well as other open source simulation",
    "start": "458639",
    "end": "465599"
  },
  {
    "text": "environments that are developed using openidge interfaces for example energy plus for hvac and",
    "start": "465599",
    "end": "472160"
  },
  {
    "text": "pipeline for robotics application you can also build your own custom environment so that you can code this up",
    "start": "472160",
    "end": "478720"
  },
  {
    "text": "and bring that into sagemakaria as a matter of fact this will be the case in the second part of our",
    "start": "478720",
    "end": "484319"
  },
  {
    "text": "presentation today where we build our inventory management simulator and bring it to amazon",
    "start": "484319",
    "end": "489680"
  },
  {
    "text": "sagemaker lastly you can also connect to commercial simulation environment",
    "start": "489680",
    "end": "495199"
  },
  {
    "text": "such as mass work simulants if you're doing manufacturing type of use cases",
    "start": "495199",
    "end": "500240"
  },
  {
    "start": "500000",
    "end": "612000"
  },
  {
    "text": "as a current ray or a lot user you may wonder what you can do with your auto models slash algorithm on amazon stage",
    "start": "500240",
    "end": "506960"
  },
  {
    "text": "make url so first of all uh you can train your models with many spot instances simply",
    "start": "506960",
    "end": "512719"
  },
  {
    "text": "as what i mentioned before let me explain this a little bit so many spot instances uses amazon ec2 for",
    "start": "512719",
    "end": "519039"
  },
  {
    "text": "instance to run training jobs instead of on demand instance you can specify which training jobs you",
    "start": "519039",
    "end": "524720"
  },
  {
    "text": "spawn instances as well as a stopping condition that specifies how long stage maker waits for job to",
    "start": "524720",
    "end": "530640"
  },
  {
    "text": "run metrics and locks generated during training runs are available in college",
    "start": "530640",
    "end": "536320"
  },
  {
    "text": "spot instance can be interrupted causing jobs to take longer to start or finish you can configure your manufac instance",
    "start": "536320",
    "end": "542720"
  },
  {
    "text": "jobs to use checkpoints sagemaker copied checkpoints data from the local pass to amazon s3",
    "start": "542720",
    "end": "548480"
  },
  {
    "text": "when the job is restarted sagemaker copies the data from amazon stream back to the local pass",
    "start": "548480",
    "end": "553920"
  },
  {
    "text": "the training can then resume from the latest checkpoint instead of restarting amazon sagemaker automatically",
    "start": "553920",
    "end": "560240"
  },
  {
    "text": "configures your manual spot instance jobs to use those checkpoints the training can then resume from the latest",
    "start": "560240",
    "end": "566399"
  },
  {
    "text": "ray oral checkpoint instead of restarting in terms of scaling you can scale up",
    "start": "566399",
    "end": "572080"
  },
  {
    "text": "your experiments with either homogeneous scaling or heterogeneous skilling by homogeneous scaling you can use",
    "start": "572080",
    "end": "577760"
  },
  {
    "text": "multiple instances with the same type for a single amazon stage in the future whereas for",
    "start": "577760",
    "end": "583519"
  },
  {
    "text": "heterogeneous scaling you can mix and match different types of instances for example cpu plus gpu",
    "start": "583519",
    "end": "589760"
  },
  {
    "text": "and they can run in multiple amazon stage record jobs so this multiple amazon sitemaker jobs will talk to each",
    "start": "589760",
    "end": "595600"
  },
  {
    "text": "other and share resources that builds up this rate cluster in total you have the freedom to set up the cluster according",
    "start": "595600",
    "end": "602399"
  },
  {
    "text": "to a specific use case for example whether you want to use multiple cpus for raw workers",
    "start": "602399",
    "end": "607920"
  },
  {
    "text": "collections and the gpu instance for example to policy update we also come with a collection of",
    "start": "607920",
    "end": "614160"
  },
  {
    "start": "612000",
    "end": "645000"
  },
  {
    "text": "jupyter notebooks just like amazon stagemaker does they are available on github featuring both simple examples like card pool",
    "start": "614160",
    "end": "621360"
  },
  {
    "text": "as well as the advanced ones in a variety of domains such as robotics operations research finance and more",
    "start": "621360",
    "end": "627360"
  },
  {
    "text": "as you can see we have different types listed here and we have the github repo listed here so feel free to check out",
    "start": "627360",
    "end": "634800"
  },
  {
    "text": "aws labs amazon sagemaker examples slash reinforcement learning",
    "start": "634800",
    "end": "640320"
  },
  {
    "text": "so you can easily extend these notebooks and customize them for your own business problem",
    "start": "640320",
    "end": "645360"
  },
  {
    "start": "645000",
    "end": "671000"
  },
  {
    "text": "we will start with this specific folder called rr resource allocation great customer",
    "start": "645360",
    "end": "650640"
  },
  {
    "text": "environments so it focuses on operations research and analog sensory canonical problems",
    "start": "650640",
    "end": "656560"
  },
  {
    "text": "impacting vehicle routing and use vendor problem we will select one the news finder problem as the starting",
    "start": "656560",
    "end": "662800"
  },
  {
    "text": "point and in the second part of the presentation today we focus on a specific branch on top of it",
    "start": "662800",
    "end": "669040"
  },
  {
    "text": "that's inventory management okay mr let's move on to the second part",
    "start": "669040",
    "end": "674640"
  },
  {
    "start": "671000",
    "end": "890000"
  },
  {
    "text": "of the presentation today real world reinforcement learning inventory management managing inventory levels is critical to",
    "start": "674640",
    "end": "681279"
  },
  {
    "text": "supply chain sustainability a clear relation exists between inventory levels and other fulfillment service levels",
    "start": "681279",
    "end": "687839"
  },
  {
    "text": "the role of an inventory planning system is to determine the optimal inventory level that should be held for different",
    "start": "687839",
    "end": "693279"
  },
  {
    "text": "products the systems attempt to balance the cost of meeting customer demands with the cost of holding too much",
    "start": "693279",
    "end": "698880"
  },
  {
    "text": "inventory inventory level is reviewed periodically and adjustments are made by either adding or removing the existing",
    "start": "698880",
    "end": "705040"
  },
  {
    "text": "inventory through different means most retailers fix this problem either in their stores or warehouses",
    "start": "705040",
    "end": "711519"
  },
  {
    "text": "at each time point the main drains inventory and extra inventories carried over to the next period if the retailer fails",
    "start": "711519",
    "end": "718720"
  },
  {
    "text": "to meet that demand it will either be marked as a backlog order that gets fulfilled at a later",
    "start": "718720",
    "end": "724079"
  },
  {
    "text": "date or simply chalked up as a lost sale with zero profit and this typically describes the state",
    "start": "724079",
    "end": "730240"
  },
  {
    "text": "evolution of a dynamic problem handling such dynamic program is challenging due to the curse of",
    "start": "730240",
    "end": "736480"
  },
  {
    "text": "dimensionality for example online demands that comes in at each period is stochastic",
    "start": "736480",
    "end": "741600"
  },
  {
    "text": "and there exists clear cross cross-product correlation decisions are really isolated to a",
    "start": "741600",
    "end": "747120"
  },
  {
    "text": "single period and they are repeatedly and periodically taken and does have a downstream impact",
    "start": "747120",
    "end": "752959"
  },
  {
    "text": "this makes the problem non-trivial for large retailers in competitive markets price matching causes abrupt and random",
    "start": "752959",
    "end": "759680"
  },
  {
    "text": "changes in the price of a product during the decision besides it is estimated through analysis",
    "start": "759680",
    "end": "765360"
  },
  {
    "text": "a large skill survey that only 15 to 23 of customers are willing to delay a",
    "start": "765360",
    "end": "770959"
  },
  {
    "text": "purchase when confronted by an out of stock page this indicates that a lost sale",
    "start": "770959",
    "end": "776880"
  },
  {
    "text": "assumption is more realistic in competitive environments rather than a full backlogging assumption",
    "start": "776880",
    "end": "782880"
  },
  {
    "text": "and again by lost sale will mean that customer they decide not to purchase anything when they say",
    "start": "782880",
    "end": "788639"
  },
  {
    "text": "when they found that the product is out of stock where backlog consumption assumes that a",
    "start": "788639",
    "end": "793760"
  },
  {
    "text": "customer will wait for the product to come back last but not least in reality there",
    "start": "793760",
    "end": "798800"
  },
  {
    "text": "often exists constrained across different products such as maximum number of units that you",
    "start": "798800",
    "end": "803839"
  },
  {
    "text": "can have at one time and also the volume of similar capacity constraints",
    "start": "803839",
    "end": "809760"
  },
  {
    "text": "due to other complexities inherited in inventory models researchers in operational research typically put",
    "start": "809760",
    "end": "815360"
  },
  {
    "text": "certain assumptions for this problem a classic formulation is the news finder problem which tries to decide on the",
    "start": "815360",
    "end": "821199"
  },
  {
    "text": "ultimate decision of a single product to cover a single period of uncertain demand",
    "start": "821199",
    "end": "826560"
  },
  {
    "text": "current studied an inventory system with continuous demand density with lost sales in a constant time of 1.",
    "start": "826560",
    "end": "833040"
  },
  {
    "text": "they show that an order of policy which it means a policy that orders up to a target inventory level",
    "start": "833040",
    "end": "839279"
  },
  {
    "text": "is sub-optimal in this case even with linear cost structures general kiraman in ram they developed a",
    "start": "839279",
    "end": "846000"
  },
  {
    "text": "convexity result for an inventory management system with lost sales constant price non-crossing lead",
    "start": "846000",
    "end": "852160"
  },
  {
    "text": "times and order up policies full backlogging is another commonly used assumption which we assume any",
    "start": "852160",
    "end": "859199"
  },
  {
    "text": "sales that means due to lack of inventory are backlogged and filled up in the next period",
    "start": "859199",
    "end": "864320"
  },
  {
    "text": "as discussed in the previous slide it is more realistic is to assume only a certain percent of the customers are",
    "start": "864320",
    "end": "870079"
  },
  {
    "text": "willing to wait for the product to get back besides nahim is formulate myopic policies for inventory system where",
    "start": "870079",
    "end": "876880"
  },
  {
    "text": "excess sales are lost and the leak time is random so in summary um the money period and you spend a problem with lead times under",
    "start": "876880",
    "end": "883519"
  },
  {
    "text": "the lawsuit model is known not to meet a simple solution such as in order up to policy in order to apply",
    "start": "883519",
    "end": "891040"
  },
  {
    "start": "890000",
    "end": "955000"
  },
  {
    "text": "reinforcement learning into this inventory managed problem we need to dive a little bit into the problem dynamics itself",
    "start": "891040",
    "end": "897199"
  },
  {
    "text": "so let's look at what would happen inside this inventory management process",
    "start": "897199",
    "end": "902399"
  },
  {
    "text": "so at each time period the following sequence of events would occur first the features of inventory for each",
    "start": "902399",
    "end": "908480"
  },
  {
    "text": "product is observed and replenishment orders are taken and filled according to available capacities the underlying",
    "start": "908480",
    "end": "915120"
  },
  {
    "text": "decision-making system can be either reinforcing learning or non-reinforcement learning then the retailer receives the in-flight",
    "start": "915120",
    "end": "921600"
  },
  {
    "text": "inventory from the previous orders and also the current orders with zero lead time then the customer demand is",
    "start": "921600",
    "end": "927600"
  },
  {
    "text": "observed and filled using available inventory on hand after that the extra inventory is held",
    "start": "927600",
    "end": "933279"
  },
  {
    "text": "at a holding cost of course there are other price related terms as well for example",
    "start": "933279",
    "end": "938560"
  },
  {
    "text": "revenue the purchasing cost the holding costs as listed here and of course underage cost if we",
    "start": "938560",
    "end": "944480"
  },
  {
    "text": "consider that so this workflow is adjusted from this paper from hubsp in 2020",
    "start": "944480",
    "end": "951440"
  },
  {
    "text": "origin a reinforcement learning library for operations research problem once we understand the transition",
    "start": "951440",
    "end": "957279"
  },
  {
    "start": "955000",
    "end": "1074000"
  },
  {
    "text": "inherited in the problem we are ready to formulate the inventory manage problem as a markup decision process",
    "start": "957279",
    "end": "962639"
  },
  {
    "text": "so it's time for some mathematical notation now first of all we have the state sti that",
    "start": "962639",
    "end": "968959"
  },
  {
    "text": "is the state for the markup decision process for product i at decision epoc t",
    "start": "968959",
    "end": "975360"
  },
  {
    "text": "and ati is a one-dimensional root member and that's the action taken by the agent and it makes the",
    "start": "975360",
    "end": "981759"
  },
  {
    "text": "decision at epoch t for product i so the superscript i is for a product",
    "start": "981759",
    "end": "987440"
  },
  {
    "text": "and the subscript t is for the epoch or timestamp and we use capital t to denote the",
    "start": "987440",
    "end": "994560"
  },
  {
    "text": "demand from the decision epoch s to decision epoch t and note here that s happens always",
    "start": "994560",
    "end": "1001040"
  },
  {
    "text": "before t so s is less than t and we also find this conventional notation that",
    "start": "1001040",
    "end": "1006399"
  },
  {
    "text": "when we talk about d t i we mean the demand that counts between t and t",
    "start": "1006399",
    "end": "1011519"
  },
  {
    "text": "plus one last but not least we have i t i",
    "start": "1011519",
    "end": "1016639"
  },
  {
    "text": "denoting the inventory for product i at position e part t and more mathematical",
    "start": "1016639",
    "end": "1022160"
  },
  {
    "text": "notation to come we have the vendor lead times denoting the time you will need to wait for the product to arrive to the",
    "start": "1022160",
    "end": "1028720"
  },
  {
    "text": "retailer therefore it's a non-negative value we have the price",
    "start": "1028720",
    "end": "1034079"
  },
  {
    "text": "revenue at time t for product i once you sell that product to the customer",
    "start": "1034079",
    "end": "1040000"
  },
  {
    "text": "we also have this cost term say ti denoting the cost at each time t for",
    "start": "1040000",
    "end": "1045600"
  },
  {
    "text": "product i and we have this notion of f t i it is",
    "start": "1045600",
    "end": "1051120"
  },
  {
    "text": "a fuel rate for product i at decision epoch t it denotes the number of units that the",
    "start": "1051120",
    "end": "1056799"
  },
  {
    "text": "vendor has available at decision epoc so it is intuitive that the action larger than the fuel rate level",
    "start": "1056799",
    "end": "1063360"
  },
  {
    "text": "will be capped at the field rate level and this one you may be already familiar",
    "start": "1063360",
    "end": "1069039"
  },
  {
    "text": "with that pi is the inventory management policy with all those notations what is the",
    "start": "1069039",
    "end": "1075200"
  },
  {
    "start": "1074000",
    "end": "1378000"
  },
  {
    "text": "actual objective function here and that corresponds to reward function in the formulation of a markov decision",
    "start": "1075200",
    "end": "1081039"
  },
  {
    "text": "process you can see this long reward function it consists of three different components",
    "start": "1081039",
    "end": "1088400"
  },
  {
    "text": "for the ease of understanding we if we look at the first part we can understand it as the net revenue",
    "start": "1088400",
    "end": "1094240"
  },
  {
    "text": "how much the retailer has earned by selling those products to the customers therefore does mean value between the",
    "start": "1094240",
    "end": "1101440"
  },
  {
    "text": "demands and inventory decides how much actual units you can sell to customers",
    "start": "1101440",
    "end": "1107440"
  },
  {
    "text": "the second part of the right hand side corresponds to the cost of goods salt and cti again is the cost term and we",
    "start": "1107440",
    "end": "1114880"
  },
  {
    "text": "also have a minimum between the action taken by the agent and the actual fill rate from the vendor and the",
    "start": "1114880",
    "end": "1122160"
  },
  {
    "text": "last term corresponds to the cost of arrival for orders place and you can see here we also have this",
    "start": "1122160",
    "end": "1127520"
  },
  {
    "text": "minimum between the agent section and the actual action coming from the fuel rate",
    "start": "1127520",
    "end": "1133679"
  },
  {
    "text": "so to summarize the objective is to trade off the various costs incurred and revenues achieved during the period",
    "start": "1133679",
    "end": "1139919"
  },
  {
    "text": "it generally consists of sales revenue purchasing and coding cost as we see here in this",
    "start": "1139919",
    "end": "1145440"
  },
  {
    "text": "reward function in more complex formulations cost including loss of goodwill in the case",
    "start": "1145440",
    "end": "1151039"
  },
  {
    "text": "of missed sales and the terminal salvage value of the unsold items are also included",
    "start": "1151039",
    "end": "1157600"
  },
  {
    "text": "once the reward function is defined in md it's natural to have the objective function as the expected cumulative reward and",
    "start": "1157760",
    "end": "1165440"
  },
  {
    "text": "just to have a quick recap p here is the net revenue ct as the cost of goods sold",
    "start": "1165440",
    "end": "1171280"
  },
  {
    "text": "and qt as the cost of arrival and also we do have certain constraints here",
    "start": "1171280",
    "end": "1176559"
  },
  {
    "text": "first of all ati that corresponds to action taken by ro agent at time t for product i and in the current",
    "start": "1176559",
    "end": "1183280"
  },
  {
    "text": "simulation study we only consider the case where a ti is greater or equal to zero meaning that we",
    "start": "1183280",
    "end": "1189600"
  },
  {
    "text": "only decide whether to buy rather than considering whether we want to remove the inventory",
    "start": "1189600",
    "end": "1195120"
  },
  {
    "text": "and here at ti as the actual action for that goes into the system again it's the minimum value of the",
    "start": "1195120",
    "end": "1201760"
  },
  {
    "text": "action taken by the agent and the and the fuel rate let's look at the transition of inventory from",
    "start": "1201760",
    "end": "1207760"
  },
  {
    "text": "t to t plus 1 as well so on the left hand side it denotes the inventory at t",
    "start": "1207760",
    "end": "1212880"
  },
  {
    "text": "plus 1 for product i it is the maximum of two components",
    "start": "1212880",
    "end": "1217919"
  },
  {
    "text": "the first component and the first part is the inventory at t it denotes the inventory level when you",
    "start": "1217919",
    "end": "1223280"
  },
  {
    "text": "start the current epoch and then the second component is the",
    "start": "1223280",
    "end": "1228559"
  },
  {
    "text": "units that just arrived the summation is over k where k starts",
    "start": "1228559",
    "end": "1234400"
  },
  {
    "text": "from zero meaning that the product have zero lead time up to t as the maximum value meaning that the",
    "start": "1234400",
    "end": "1240480"
  },
  {
    "text": "order was placed at the beginning of the round and d here is the demand at time t for",
    "start": "1240480",
    "end": "1246480"
  },
  {
    "text": "for product i therefore the inventory at t plus 1 is the maximum value of all the products",
    "start": "1246480",
    "end": "1252159"
  },
  {
    "text": "that you have hand minus the demand at t we also demonstrate a sample constraint here meaning the",
    "start": "1252159",
    "end": "1258880"
  },
  {
    "text": "arrival constraint at time t the left hand side denotes all units that arrive at t",
    "start": "1258880",
    "end": "1264159"
  },
  {
    "text": "so if we do have a rival constraint at t here is how you can formulate the problem as you can see there are so many",
    "start": "1264159",
    "end": "1270480"
  },
  {
    "text": "constraints so many terms in the objective function so to start our experiments we decided to",
    "start": "1270480",
    "end": "1276000"
  },
  {
    "text": "simplify the problem a little bit specifically and we revisit the objective function here instead of",
    "start": "1276000",
    "end": "1282400"
  },
  {
    "text": "having both cost of goods sold and cost of arrival here we only consider the revenue term",
    "start": "1282400",
    "end": "1289039"
  },
  {
    "text": "the actual demand if stock is adequate i will explain that a little bit more and the custom c hat so essentially we",
    "start": "1289039",
    "end": "1296880"
  },
  {
    "text": "are simplify the problem by setting q t equals to zero also here again a t is actually taken by",
    "start": "1296880",
    "end": "1304799"
  },
  {
    "text": "rl agent within an i t minus i up in the inventory at time t after the order arrives and before the",
    "start": "1304799",
    "end": "1312559"
  },
  {
    "text": "demand comes and i t i as the inventory at time t after order arrives and after",
    "start": "1312559",
    "end": "1319760"
  },
  {
    "text": "the amount comes note that this implicitly assumes that the fuel rate is always",
    "start": "1319760",
    "end": "1325200"
  },
  {
    "text": "infinity meaning that the order amount decided by the oral agent can always be placed and",
    "start": "1325200",
    "end": "1331360"
  },
  {
    "text": "received by the vendor and qti is zero meaning that the cost of",
    "start": "1331360",
    "end": "1336640"
  },
  {
    "text": "arrival is zero and lastly as i mentioned we relax all the constraints volume",
    "start": "1336640",
    "end": "1342400"
  },
  {
    "text": "constraint is infinity capital constraints arrival constraints they are all infinity",
    "start": "1342400",
    "end": "1348320"
  },
  {
    "text": "with all the simplifications the goal now here is to build a policy pi such that objective function is",
    "start": "1348320",
    "end": "1354640"
  },
  {
    "text": "maximized our policy obtains data by acting in a real or simulated environment",
    "start": "1354640",
    "end": "1361679"
  },
  {
    "text": "however what we have is a set of historical data from a retailer that has been generated by some policy",
    "start": "1361679",
    "end": "1368080"
  },
  {
    "text": "acting on the words at the time of collection therefore a natural question is how can we make usage of those historical data",
    "start": "1368080",
    "end": "1374640"
  },
  {
    "text": "to construct a simulator and develop an rl policy to answer that question let's first look at some",
    "start": "1374640",
    "end": "1380799"
  },
  {
    "start": "1378000",
    "end": "1424000"
  },
  {
    "text": "characteristics of the historical data that we have at hand historical data tends to be dependent on",
    "start": "1380799",
    "end": "1386480"
  },
  {
    "text": "the policy that was actually managing the inventory for example demand sensor for those time where the policy",
    "start": "1386480",
    "end": "1393039"
  },
  {
    "text": "failed to meet customer demand therefore sales becomes the minimum value of demand and inventory at that",
    "start": "1393039",
    "end": "1398880"
  },
  {
    "text": "time further policy dependent data issues might be that the vendor lead time depends on the size of order placed",
    "start": "1398880",
    "end": "1404720"
  },
  {
    "text": "unit cost also depends on that size of other place and price typically vary within the decision epoc",
    "start": "1404720",
    "end": "1411679"
  },
  {
    "text": "in a sense his target data is thus on policy only for the old policy that was managing",
    "start": "1411679",
    "end": "1417200"
  },
  {
    "text": "inventory and it is of policy for any other algorithm that tries to learn a new policy",
    "start": "1417200",
    "end": "1424240"
  },
  {
    "text": "so how can we turn historical data into a simulator the answer is that we can try to correct",
    "start": "1424320",
    "end": "1430080"
  },
  {
    "text": "different historical on policy data by performing different operations and that corresponds to",
    "start": "1430080",
    "end": "1435679"
  },
  {
    "text": "the actual physics of the words meaning that we build a simulator by performing statistical operations on the historical",
    "start": "1435679",
    "end": "1442240"
  },
  {
    "text": "data an online retailer may see signals of a demand other than cells even when an item is out of stock and this is",
    "start": "1442240",
    "end": "1449360"
  },
  {
    "text": "mostly done by looking at the page wheels we can therefore make usage of this view",
    "start": "1449360",
    "end": "1454720"
  },
  {
    "text": "and construct a common factual demand signal denoted as d tilde t",
    "start": "1454720",
    "end": "1459840"
  },
  {
    "text": "and that's actually part of the real function we do this by correcting for availability making use of the",
    "start": "1459840",
    "end": "1466000"
  },
  {
    "text": "web page view now that we have corrected some of the",
    "start": "1466000",
    "end": "1471520"
  },
  {
    "start": "1468000",
    "end": "1497000"
  },
  {
    "text": "historical data we can use it to construct a simulator and that simulator can be used for training our rl",
    "start": "1471520",
    "end": "1477520"
  },
  {
    "text": "operations we treat each product's demand as a path from some joint stochastic process",
    "start": "1477520",
    "end": "1483679"
  },
  {
    "text": "with the appropriate contacts and features i will elaborate those features in the next slide and then we roll out a single policy for",
    "start": "1483679",
    "end": "1490400"
  },
  {
    "text": "all products and collect the rewards and use all of those episodes to train our rl policy",
    "start": "1490400",
    "end": "1496880"
  },
  {
    "text": "so before we look at the actual training results let's do a quick summary of what the state",
    "start": "1496880",
    "end": "1502559"
  },
  {
    "text": "space and action space are so state space is the n-dimensional space at time t",
    "start": "1502559",
    "end": "1507840"
  },
  {
    "text": "for product i and it contains a subset of other features listed here the current",
    "start": "1507840",
    "end": "1513760"
  },
  {
    "text": "inventory level as well as previous actions from at the agent in terms of the main time series",
    "start": "1513760",
    "end": "1520240"
  },
  {
    "text": "features we have included multiple different features for example historical availability corrected demand",
    "start": "1520240",
    "end": "1526240"
  },
  {
    "text": "distance to public holidays historical website viewer data and as for the static product features",
    "start": "1526240",
    "end": "1531520"
  },
  {
    "text": "we considered product group product description letter density and brand we also make usage of",
    "start": "1531520",
    "end": "1537360"
  },
  {
    "text": "other features for example the economics of the product as well as the outputs of another baseline policy in terms of the",
    "start": "1537360",
    "end": "1545039"
  },
  {
    "start": "1544000",
    "end": "1595000"
  },
  {
    "text": "action space at each time t for product i denotes the order quantity directly rather than a target inventory position",
    "start": "1545039",
    "end": "1552080"
  },
  {
    "text": "this prevents unnecessary dependency on other out policies which are known to be suboptimal for a",
    "start": "1552080",
    "end": "1558240"
  },
  {
    "text": "periodic review inventory system with stochastic lead times and lost sales the action can be stochastic or",
    "start": "1558240",
    "end": "1564880"
  },
  {
    "text": "deterministic depending on the learning agent to represent a scorecast action a policy",
    "start": "1564880",
    "end": "1570240"
  },
  {
    "text": "net produces the mean and standard deviation of a gaussian or the scalar action for deterministic",
    "start": "1570240",
    "end": "1575279"
  },
  {
    "text": "algorithm products can have orders of magnitude differences in their demand per period",
    "start": "1575279",
    "end": "1580559"
  },
  {
    "text": "thus the policy may have vastly different skills of orders for different products which becomes a",
    "start": "1580559",
    "end": "1585679"
  },
  {
    "text": "potential problem to mitigate that issue we use a baseline policy and that's actually a version of",
    "start": "1585679",
    "end": "1591679"
  },
  {
    "text": "the news vendor policy to provide a scaling factor for the current action we consider the following policies for",
    "start": "1591679",
    "end": "1598320"
  },
  {
    "text": "the purpose of simulation study first benchmark against an article policy which presents an idolized",
    "start": "1598320",
    "end": "1604559"
  },
  {
    "text": "scenario where the policy knows at time t what are the future demands for all the products in terms of reinforcement",
    "start": "1604559",
    "end": "1611279"
  },
  {
    "text": "learning algorithms we choose augmented random research soft active critic and hrc we turn off agents exploration during",
    "start": "1611279",
    "end": "1618159"
  },
  {
    "text": "evaluation and those three algorithms are the top three performing operations of cross-over experiments",
    "start": "1618159",
    "end": "1624799"
  },
  {
    "text": "we consider news vendor problem which optimizes over lead time and demand and stationary condition as",
    "start": "1624799",
    "end": "1630080"
  },
  {
    "text": "determination this produces a classical operations research baseline where the target inventory level is the",
    "start": "1630080",
    "end": "1636799"
  },
  {
    "text": "outcome other app policies implicitly assume that the first vendor lead time period sales",
    "start": "1636799",
    "end": "1642240"
  },
  {
    "text": "are fully backlogged in order to mitigate that effect we improve the news vendor based stock policy by absorbing extra",
    "start": "1642240",
    "end": "1648880"
  },
  {
    "text": "information of the vendor lead time distribution often the planning horizon is represented as the sum of a reveal",
    "start": "1648880",
    "end": "1654480"
  },
  {
    "text": "period and the percentile of the ventilated time distribution we normalize the planning horizon to get",
    "start": "1654480",
    "end": "1659919"
  },
  {
    "text": "the buying amount at each time step and manually multiply it by the median of the vendor lead time distributions",
    "start": "1659919",
    "end": "1666399"
  },
  {
    "text": "of all the products in order to approximate the amount required on hand to fulfill demand for single lead time",
    "start": "1666399",
    "end": "1673120"
  },
  {
    "text": "note that all the policy were trained using two years of historical data and 19 weeks for testing",
    "start": "1673120",
    "end": "1680159"
  },
  {
    "start": "1679000",
    "end": "1753000"
  },
  {
    "text": "so let's look at some results here please note that all those results shown are still from simulation study although",
    "start": "1680159",
    "end": "1686480"
  },
  {
    "text": "we use historical data to build the simulator first and foremost the ultimate goal of",
    "start": "1686480",
    "end": "1691679"
  },
  {
    "text": "an rl agent is to maximize the expected discounted cumulative reward and the first panel figure there",
    "start": "1691679",
    "end": "1698480"
  },
  {
    "text": "shows the evaluation curves in which the first value given by oracle policy is said to be 1",
    "start": "1698480",
    "end": "1704480"
  },
  {
    "text": "such that all the rest values can be interpreted as the percentage as expected oracle achieved highest",
    "start": "1704480",
    "end": "1710960"
  },
  {
    "text": "cumulative reward at the end of the testing period and all our policies are competitive with or superior to use vendor problem",
    "start": "1710960",
    "end": "1718799"
  },
  {
    "text": "this verifies that our indeed can be a good fit in our problem we also present the sales of rl and",
    "start": "1718799",
    "end": "1725440"
  },
  {
    "text": "baseline policies in the last figure across the testing period or numbers again are normalized by the value of",
    "start": "1725440",
    "end": "1731520"
  },
  {
    "text": "oracle at the first time point it is observed that news vendor in myopic policies tends to",
    "start": "1731520",
    "end": "1737919"
  },
  {
    "text": "buy more leading to the highest sales value the significant jump at time",
    "start": "1737919",
    "end": "1743200"
  },
  {
    "text": "0.16 in the sale is due to a seasonal behavior expected by the demand",
    "start": "1743200",
    "end": "1748640"
  },
  {
    "text": "and that corresponds to the sharp decrease in the inventory level at the same time we also present inventory level over",
    "start": "1748640",
    "end": "1755360"
  },
  {
    "start": "1753000",
    "end": "1793000"
  },
  {
    "text": "time of two random selected products here against the demand adjusted by the web",
    "start": "1755360",
    "end": "1760640"
  },
  {
    "text": "page views the inventory position is measured after supply has arrived and before demand has",
    "start": "1760640",
    "end": "1766480"
  },
  {
    "text": "fulfilled the trends here are consistent with observations from previous figures",
    "start": "1766480",
    "end": "1771679"
  },
  {
    "text": "it is worth noting that in the left panel where normalized planning horizon music and the problem",
    "start": "1771679",
    "end": "1777360"
  },
  {
    "text": "fails to meet the demand corrected by the webpage when the event comes",
    "start": "1777360",
    "end": "1782720"
  },
  {
    "text": "h3c and sac have enough in stock and the right panel indicates that the aggregate behavior of a3c",
    "start": "1782720",
    "end": "1788880"
  },
  {
    "text": "and sac are indeed similar to that of the news vendor from the proper formulation to the",
    "start": "1788880",
    "end": "1795200"
  },
  {
    "start": "1793000",
    "end": "1838000"
  },
  {
    "text": "simulator construction as well as the experiments results we can reach to you some of the conclusions now",
    "start": "1795200",
    "end": "1801679"
  },
  {
    "text": "first of all we have demonstrated that deep reinforcement is indeed a candidate solution for solving historical",
    "start": "1801679",
    "end": "1807440"
  },
  {
    "text": "intractable dynamic programs we developed a novel approach to turning the historical data into a",
    "start": "1807440",
    "end": "1813600"
  },
  {
    "text": "simulator which allows us to bypass the intermediate system as our work simulator at the same time",
    "start": "1813600",
    "end": "1820399"
  },
  {
    "text": "other problems with generous complex conflictuals may still require more simple efficient methods",
    "start": "1820399",
    "end": "1825679"
  },
  {
    "text": "and the application of model based rl into this problem remains an exciting avenue of research",
    "start": "1825679",
    "end": "1832559"
  },
  {
    "text": "great so this serves as the end of the second part of the presentation let's do a quick recap recall that this",
    "start": "1832559",
    "end": "1839520"
  },
  {
    "start": "1838000",
    "end": "1884000"
  },
  {
    "text": "presentation only contains two parts the first part focuses on amazon stage maker arrow whereas the second part focuses on the",
    "start": "1839520",
    "end": "1846320"
  },
  {
    "text": "inventory management problem please note that while the two components seems orthogonal they are",
    "start": "1846320",
    "end": "1851520"
  },
  {
    "text": "related in a sense that you can always bring in the creative simulator for inventory problem into safe make rl to train your",
    "start": "1851520",
    "end": "1858720"
  },
  {
    "text": "rr algorithms with that being said amazon sagemaker removes the heavy lifting from each step",
    "start": "1858720",
    "end": "1864640"
  },
  {
    "text": "of the rr process and you can easily run the skill experiments so hope you folks",
    "start": "1864640",
    "end": "1869760"
  },
  {
    "text": "enjoy the presentation today and always remember to bring your own oral problem applied to the real world again here's a",
    "start": "1869760",
    "end": "1876880"
  },
  {
    "text": "github link to our public report feel free to check out the examples there and i'm happy to take the questions in",
    "start": "1876880",
    "end": "1882799"
  },
  {
    "text": "the q a session",
    "start": "1882799",
    "end": "1886480"
  }
]