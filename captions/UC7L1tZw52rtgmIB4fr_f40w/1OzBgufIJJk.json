[
  {
    "text": "how's everyone doing it's getting late uh but thank you for showing up here you",
    "start": "2800",
    "end": "7839"
  },
  {
    "text": "show your dedication um I hope our presentation will worth your time uh my",
    "start": "7839",
    "end": "13280"
  },
  {
    "text": "name is Ivan I have my colleague saish today we are going to talk about our efforts uh P training and deploying the",
    "start": "13280",
    "end": "19920"
  },
  {
    "text": "multimodel foundation model for document Automation in Uber um it is about",
    "start": "19920",
    "end": "25320"
  },
  {
    "text": "document automation but at the same time I'm sure our presentation can be generalized to",
    "start": "25320",
    "end": "30480"
  },
  {
    "text": "uh a lot more use cases so let's get it started the agenda for today is we will",
    "start": "30480",
    "end": "36000"
  },
  {
    "text": "start with motivation and then it will be the juicy part about how we build a model architecture and pining and how we",
    "start": "36000",
    "end": "42480"
  },
  {
    "text": "fine tuning for downstreaming applications uh and also we will show the evaluation comparison with GPT and",
    "start": "42480",
    "end": "48840"
  },
  {
    "text": "also latest llama 3.2 Vision model uh and also uh in the end we will do a quick",
    "start": "48840",
    "end": "54840"
  },
  {
    "text": "demo um so before we talk about the patrini and model it's important to share more T about why we are building",
    "start": "54840",
    "end": "61320"
  },
  {
    "text": "this uh inside Uber we are processing 100 million plus of documents annually",
    "start": "61320",
    "end": "67439"
  },
  {
    "text": "if you think Uber's business as Iceberg these are the huge part under the water service is invisible but they are very",
    "start": "67439",
    "end": "74119"
  },
  {
    "text": "essential for the normal operations and uh the functioning of business uh so",
    "start": "74119",
    "end": "79320"
  },
  {
    "text": "what happen is if I'm a driver I want to unboard to the Uber platform I need to upload my driver license and vehicle",
    "start": "79320",
    "end": "86159"
  },
  {
    "text": "insurance uh registration and all kind of forms and they need to be transcribe to extract all key informations at the",
    "start": "86159",
    "end": "93200"
  },
  {
    "text": "same time we have grocery delivery and then we will get receipts from our Shoppers and we need to do a lot of",
    "start": "93200",
    "end": "99399"
  },
  {
    "text": "checks on there so lot of documents also a lot of document types and not to",
    "start": "99399",
    "end": "104640"
  },
  {
    "text": "mention a lot of different languages it POS all kind of challenges so the main M",
    "start": "104640",
    "end": "110399"
  },
  {
    "text": "the first motivation to build a foundation model is it's very slow and expensive to transcribe this amount of",
    "start": "110399",
    "end": "116840"
  },
  {
    "text": "documents manually uh and slow down the process onb new drivers uh to validate",
    "start": "116840",
    "end": "122600"
  },
  {
    "text": "grocery receipts and process invoice uh a lot of different um um business",
    "start": "122600",
    "end": "127880"
  },
  {
    "text": "process uh and also very expensive to transcribe each one of them manually but",
    "start": "127880",
    "end": "133319"
  },
  {
    "text": "at the same time it's very difficult to scale to thousands of document types um with one model at a time um because",
    "start": "133319",
    "end": "141200"
  },
  {
    "text": "every document type has their own uh Target fields to extract so you need to",
    "start": "141200",
    "end": "146519"
  },
  {
    "text": "um kind of develop a new model for it but uh we we have find empirically training a model from scratch need at",
    "start": "146519",
    "end": "153319"
  },
  {
    "text": "least 1 million to two million um docs examples just to meet launch C criteria",
    "start": "153319",
    "end": "158560"
  },
  {
    "text": "um but this is huge longtail problem means that many documents just don't have um a lot of examples train uh so we",
    "start": "158560",
    "end": "165319"
  },
  {
    "text": "need a different approach and that's why we want to build a strong Foundation model to automate the whole process uh",
    "start": "165319",
    "end": "172480"
  },
  {
    "text": "leveraging all the in-house document data and speak uh other uh languages about key markets uh and leveraging all",
    "start": "172480",
    "end": "180040"
  },
  {
    "text": "the in modalities we can have tax layouts IM measures um so overall that's",
    "start": "180040",
    "end": "185879"
  },
  {
    "text": "the motivations for developing this in-house Foundation model and next I will have my colleague saes talking",
    "start": "185879",
    "end": "192760"
  },
  {
    "text": "about the juicy part of the pretraining and the model architecture uh hello uh I'm sabish uh I",
    "start": "192760",
    "end": "200200"
  },
  {
    "text": "will talk about the model as well as some pre-training task we do to make this good enough for deployment um so",
    "start": "200200",
    "end": "209120"
  },
  {
    "text": "our model is an encoder decoder Transformer model and we want to encode all the information we have uh initially",
    "start": "209120",
    "end": "217040"
  },
  {
    "text": "and then send it to the decoder so um when we get an image uh so uh we extract",
    "start": "217040",
    "end": "224000"
  },
  {
    "text": "OCR from it for people who don't know OC gives you both text spatial coordinates",
    "start": "224000",
    "end": "229879"
  },
  {
    "text": "as well as confidence of each extracted text from it now we want to encode all this information uh using our encoder so",
    "start": "229879",
    "end": "237519"
  },
  {
    "text": "for the image part alone uh we use a lava style uh system where we split the image into four quadrant and then send",
    "start": "237519",
    "end": "244760"
  },
  {
    "text": "it through an image encoder concatenate all the uh to embeddings of the patches",
    "start": "244760",
    "end": "250560"
  },
  {
    "text": "and with respect to the OCR we first uh encode the text using our tokenizer and",
    "start": "250560",
    "end": "257120"
  },
  {
    "text": "then send it to our encoder so in the encoder is where we also induce a",
    "start": "257120",
    "end": "262759"
  },
  {
    "text": "spatial information of all the text as well as images uh so we have modified our self attention block which will",
    "start": "262759",
    "end": "270039"
  },
  {
    "text": "compute attention not only with text and text or image and image patch but it",
    "start": "270039",
    "end": "275120"
  },
  {
    "text": "also uh computes uh attention between spatial and spatial coordinates and we",
    "start": "275120",
    "end": "280440"
  },
  {
    "text": "add it together so in this way the model uh each token has some kind of spatial",
    "start": "280440",
    "end": "285919"
  },
  {
    "text": "awareness um and then we send this encoder uh to the decoder along with the",
    "start": "285919",
    "end": "292720"
  },
  {
    "text": "prompt um the next part is pre-training this model so we have few task which we",
    "start": "293320",
    "end": "300080"
  },
  {
    "text": "do to make this model learn natural language which is present in the document as well as you know uh have",
    "start": "300080",
    "end": "306520"
  },
  {
    "text": "strong understanding of what extract for each entity we want to uh so the",
    "start": "306520",
    "end": "312000"
  },
  {
    "text": "pre-training task itself can be split into both encoded decoder based as well as encoder only task so for the encoder",
    "start": "312000",
    "end": "319120"
  },
  {
    "text": "decoder we have two main task one is the entity extraction the other is a phrase localization for entity extraction",
    "start": "319120",
    "end": "326560"
  },
  {
    "text": "basically we give bunch of entities to be extracted at time and the decoder will generate that values in that order",
    "start": "326560",
    "end": "333880"
  },
  {
    "text": "and for phrase localization we randomly pick a phrase and ask the model predicts is coordinates so the model can learn",
    "start": "333880",
    "end": "340840"
  },
  {
    "text": "where this text is from and utilize the spatial information which we gave in our",
    "start": "340840",
    "end": "346319"
  },
  {
    "text": "encoder and for the encoder only task we have MLM and we also want the model to",
    "start": "346319",
    "end": "353000"
  },
  {
    "text": "learn the characters in the image so for each image patch we ask the model to",
    "start": "353000",
    "end": "358880"
  },
  {
    "text": "count number of character characters in it um and then we all have other encoded task like document classification image",
    "start": "358880",
    "end": "365240"
  },
  {
    "text": "quality Etc so what it takes to create a strong",
    "start": "365240",
    "end": "371520"
  },
  {
    "text": "Foundation model so first we have like 50 million documents across 120",
    "start": "371520",
    "end": "376680"
  },
  {
    "text": "different document types so we utilize both all the modalities we have like text layout images and everything and we",
    "start": "376680",
    "end": "383599"
  },
  {
    "text": "have both uh lots of infra using Ray uh which can help to stream this data set",
    "start": "383599",
    "end": "389280"
  },
  {
    "text": "as well trrain as fast as possible and we not only that we have also realized",
    "start": "389280",
    "end": "395160"
  },
  {
    "text": "uh building our own tokenizer has pretty good Advantage so it can support all the languages as well as we uh you know can",
    "start": "395160",
    "end": "402960"
  },
  {
    "text": "each digit can be treated separately um and our training pipeline is very",
    "start": "402960",
    "end": "408919"
  },
  {
    "text": "configurable so it is easy to add any kind of task we have as well as we can",
    "start": "408919",
    "end": "414039"
  },
  {
    "text": "extend to add any kind of augmentation on the data and we have a pretty good",
    "start": "414039",
    "end": "419120"
  },
  {
    "text": "monitor system uh I'll hand over to Ian to talk about the train infra okay training",
    "start": "419120",
    "end": "425039"
  },
  {
    "text": "infra I can spend one hour on here uh but for the sake of time and just talk about some highlights uh we build the",
    "start": "425039",
    "end": "431759"
  },
  {
    "text": "training infra very tightly using Ray framework for scalability but at the same time uh we put a lot of emphasis on",
    "start": "431759",
    "end": "439400"
  },
  {
    "text": "Fast sitations um uh and also training throughputs so on the high level uh we",
    "start": "439400",
    "end": "445960"
  },
  {
    "text": "have a bunch of documents uh inside the company but we need to source and curate data set for training so we do a lot of",
    "start": "445960",
    "end": "453199"
  },
  {
    "text": "we spend a lot of efforts curating the data sets uh and do a lot of samplings to end up having a pining data sets um",
    "start": "453199",
    "end": "461400"
  },
  {
    "text": "and then we will load data sets into AR rate data sets to shot across workers so",
    "start": "461400",
    "end": "466520"
  },
  {
    "text": "each worker will get their portion of training data uh and inside it we implement the streaming data loader of",
    "start": "466520",
    "end": "473680"
  },
  {
    "text": "um uh which will load load data from the object store uh and then do a lot of pre-processing on both OCR and image and",
    "start": "473680",
    "end": "481280"
  },
  {
    "text": "cach it locally into into the each of host uh and then we will uh feed the",
    "start": "481280",
    "end": "487759"
  },
  {
    "text": "data loader to the trainer inside the trainer we have P python GDP as well as deep speed for speed up the training and",
    "start": "487759",
    "end": "495240"
  },
  {
    "text": "just a few uh quick points here um because we run a lot of experimentation",
    "start": "495240",
    "end": "500560"
  },
  {
    "text": "so it's important for us to um speed up the data pre-processing so that's why we build up the caching so that within a",
    "start": "500560",
    "end": "506800"
  },
  {
    "text": "training job we can reuse the uh cache build from first Epoch but just beyond that we also run a lot of the experiment",
    "start": "506800",
    "end": "513479"
  },
  {
    "text": "in the same clusters so we want to reuse the cach across training job R data set",
    "start": "513479",
    "end": "518959"
  },
  {
    "text": "doesn't uh by default aut box it doesn't provide determin shutting so we also spend efforts to implement the shotting",
    "start": "518959",
    "end": "525920"
  },
  {
    "text": "logic so that we can making sure that uh when uh every time we launch a new job on the same host uh in the same cluster",
    "start": "525920",
    "end": "533399"
  },
  {
    "text": "uh the shoting will be deterministic so that the previous uh cash can be reused here uh to save some time we spend a lot",
    "start": "533399",
    "end": "540160"
  },
  {
    "text": "of efforts here and there to optimize those details uh so that we can speed up",
    "start": "540160",
    "end": "545200"
  },
  {
    "text": "the overall iteration process um okay so those are about model architecture and",
    "start": "545200",
    "end": "551760"
  },
  {
    "text": "training now let's talk about some applications um so first of all uh document transcription as say name says",
    "start": "551760",
    "end": "558480"
  },
  {
    "text": "that whenever we got a new document whether it's a driver license from us or from friends uh or it's uh vehicle",
    "start": "558480",
    "end": "566000"
  },
  {
    "text": "registration forms um we will uh extract it the target entities from the documents like here uh we have a list of",
    "start": "566000",
    "end": "573399"
  },
  {
    "text": "entities from the driver license and essentially we will uh feed the document image and OCR to the cvfm model which is",
    "start": "573399",
    "end": "581600"
  },
  {
    "text": "the foundation model um and we will prompt the model with the target Fields uh we are expecting and it give us the",
    "start": "581600",
    "end": "588920"
  },
  {
    "text": "uh uh Target values um in outputs um we will have we will show this in more",
    "start": "588920",
    "end": "595279"
  },
  {
    "text": "detail in a demo um and we uh because this application is very accur uh uh",
    "start": "595279",
    "end": "603640"
  },
  {
    "text": "heavy uh we spend a lot of efforts to prevent hallucination and this is one approach that we uh look into the models",
    "start": "603640",
    "end": "611959"
  },
  {
    "text": "cross attention uh and Trace back the uh predictions back to the input OCR and",
    "start": "611959",
    "end": "617959"
  },
  {
    "text": "use the OCR confidence to help justify whether the prediction is uh hallucinated or not and use that to uh",
    "start": "617959",
    "end": "625079"
  },
  {
    "text": "boost the model accuracy and also capture some Corner cases",
    "start": "625079",
    "end": "630279"
  },
  {
    "text": "application number two is document classification this is one of the reason we want to include a encoder in our",
    "start": "630279",
    "end": "635800"
  },
  {
    "text": "model architecture is that once we pretr the model we can use the both encoder decoder architecture as well as the",
    "start": "635800",
    "end": "642079"
  },
  {
    "text": "encoder only part to do the document classification here we are showing an example of a multi-label classification",
    "start": "642079",
    "end": "649680"
  },
  {
    "text": "where that we have a document uh come in and then the encoder can uh run a",
    "start": "649680",
    "end": "655399"
  },
  {
    "text": "classification and tell okay uh this document fall into the class three and Doc class uh two um and give a score for",
    "start": "655399",
    "end": "663760"
  },
  {
    "text": "each document type independently so will talk about uh the other two use",
    "start": "663760",
    "end": "670360"
  },
  {
    "text": "cases so the other um use cases the name matching it's pretty unique to uh our uh",
    "start": "670360",
    "end": "676839"
  },
  {
    "text": "Uber so uh this is to match any kind of nicknames as well as what is the name in",
    "start": "676839",
    "end": "682440"
  },
  {
    "text": "the document um for example like um so you can see that there are various uh",
    "start": "682440",
    "end": "688000"
  },
  {
    "text": "names like there is orinal like variation types I mean so there is virginal which is the Alexander J sample",
    "start": "688000",
    "end": "694240"
  },
  {
    "text": "and then we have different names which of course you shouldn't match to then there is nickname there is initials and",
    "start": "694240",
    "end": "701079"
  },
  {
    "text": "whatnot so we if just by using a string matcher you won't be able to go through",
    "start": "701079",
    "end": "706880"
  },
  {
    "text": "and correctly match to all these use cases so we uh train a model to learn",
    "start": "706880",
    "end": "713600"
  },
  {
    "text": "associate like learn various variations the name associate nickname with the names as well as you initials and things",
    "start": "713600",
    "end": "720160"
  },
  {
    "text": "like that so for this um instead of trying to",
    "start": "720160",
    "end": "726040"
  },
  {
    "text": "extract name from each and every document what we do is we give a document image and we also give the name",
    "start": "726040",
    "end": "731800"
  },
  {
    "text": "which we want to match it with so basically our model uh takes that uh encodes the input data and we provide",
    "start": "731800",
    "end": "739519"
  },
  {
    "text": "the profile name or any kind of name to our decoder so and the model will",
    "start": "739519",
    "end": "744760"
  },
  {
    "text": "predict whether a name exist in some form in the document or not",
    "start": "744760",
    "end": "751160"
  },
  {
    "text": "and then we have receipt transcription so basically um this is a so uh receipt",
    "start": "751199",
    "end": "757680"
  },
  {
    "text": "are of various forms we have various items we have to extract the like quantity as well as the weight of each",
    "start": "757680",
    "end": "764320"
  },
  {
    "text": "uh items and we want to um we also have what was ordered so when user orderers",
    "start": "764320",
    "end": "770920"
  },
  {
    "text": "something from the Uber Eats they also have uh like we have the digital copy of it so we'll have to match what the",
    "start": "770920",
    "end": "778000"
  },
  {
    "text": "physical copy of the receip with the digital copy and our CFM model helps in extracting various entities from the",
    "start": "778000",
    "end": "785560"
  },
  {
    "text": "receipt and then we have some matching criteria which will match it back to the digital",
    "start": "785560",
    "end": "791519"
  },
  {
    "text": "receipt uh next to the evaluation so so here we have compared our model with GPD",
    "start": "791880",
    "end": "798399"
  },
  {
    "text": "human as well as uh and and you can see um so the yellow is our model's",
    "start": "798399",
    "end": "805120"
  },
  {
    "text": "performance uh red is the human performance and the blue is the gpd's for performance and so our model",
    "start": "805120",
    "end": "813519"
  },
  {
    "text": "actually beats both GPD and human considerably and so one of the main",
    "start": "813519",
    "end": "820320"
  },
  {
    "text": "error source is the OCR error so on the left uh there is a driving license like",
    "start": "820320",
    "end": "827079"
  },
  {
    "text": "we have masked many of the parts of the image just for privacy reasons but so uh",
    "start": "827079",
    "end": "832680"
  },
  {
    "text": "it's an O and then bunch of characters so both GPD and Lama has like extracted",
    "start": "832680",
    "end": "838360"
  },
  {
    "text": "that it has zero instead of O and our model has learned to correct this by looking at various nuances in the image",
    "start": "838360",
    "end": "845560"
  },
  {
    "text": "Associates with the state and everything and similarly uh there's a handwritten document on the right and then the model",
    "start": "845560",
    "end": "854480"
  },
  {
    "text": "and I think GPD has done a very good job on it as well as a model has and then the Lama has still some ocrs in",
    "start": "854480",
    "end": "861680"
  },
  {
    "text": "it and we'll go to the demo previously record a demo but now we",
    "start": "861680",
    "end": "868480"
  },
  {
    "text": "will do a live demo here uh this is a dashboard we prepared for this uh",
    "start": "868480",
    "end": "874040"
  },
  {
    "text": "presentation and here we are going to show the first example of doing document transcription where I'm going to upload",
    "start": "874040",
    "end": "880480"
  },
  {
    "text": "a driver license image um and I will run the models uh and then uh let me",
    "start": "880480",
    "end": "888560"
  },
  {
    "text": "see let's see yes so this is the driver license",
    "start": "888560",
    "end": "894759"
  },
  {
    "text": "image uh and um it the the task is is to extract all the the target informations",
    "start": "894759",
    "end": "902160"
  },
  {
    "text": "like driver license ID expiration date uh and [Music]",
    "start": "902160",
    "end": "908440"
  },
  {
    "text": "names let's see for some reason it's not showing but",
    "start": "908440",
    "end": "915560"
  },
  {
    "text": "maybe we can go back to oh here um so uh again uh for each of the",
    "start": "915560",
    "end": "922680"
  },
  {
    "text": "documents uh it has the target um uh fields to extract so for this documents",
    "start": "922680",
    "end": "928000"
  },
  {
    "text": "it has uh handful of documents uh handful of fields and here is showing the model is extracting all the target",
    "start": "928000",
    "end": "934639"
  },
  {
    "text": "informations together with their scores and the score can be used to indicate uh how confident the model is and can be",
    "start": "934639",
    "end": "940759"
  },
  {
    "text": "used to um decide whether we want to drop this predictions and for back to human or not so this is an example of",
    "start": "940759",
    "end": "947759"
  },
  {
    "text": "document transcription and second example document classification which I'm going to give a uh this one",
    "start": "947759",
    "end": "958360"
  },
  {
    "text": "so um given this documents the models will predict the score for each of the",
    "start": "959800",
    "end": "965319"
  },
  {
    "text": "uh intended uh classes and here it says that there are three classes that is likely to fall into US driver license ID",
    "start": "965319",
    "end": "973240"
  },
  {
    "text": "as well as the US national ID and US license history which are all correct",
    "start": "973240",
    "end": "979800"
  },
  {
    "text": "um and also image quality um this is uh also important uh to provide um the the",
    "start": "979800",
    "end": "987920"
  },
  {
    "text": "feedback to not only um whether the image has good quality or not uh whether it has any cusions",
    "start": "987920",
    "end": "996880"
  },
  {
    "text": "blurrier and it does uh correctly capture that this is a relevant document",
    "start": "1002279",
    "end": "1007480"
  },
  {
    "text": "and also not a photo of the original document which is true um lastly name",
    "start": "1007480",
    "end": "1013759"
  },
  {
    "text": "matching U little bit complex uh to explain um but I think sear has done a",
    "start": "1013759",
    "end": "1019759"
  },
  {
    "text": "good job just to explain uh that and let's see this",
    "start": "1019759",
    "end": "1025520"
  },
  {
    "text": "one so U the the goal of name matching is to ID to to just matching whether",
    "start": "1025520",
    "end": "1032000"
  },
  {
    "text": "this document is the right document to that is expected for a uh driver's profile and say let's say the if the",
    "start": "1032000",
    "end": "1038959"
  },
  {
    "text": "profile name is uh let's go",
    "start": "1038959",
    "end": "1044319"
  },
  {
    "text": "here Mac loving",
    "start": "1044319",
    "end": "1049360"
  },
  {
    "text": "it will give a very high confidence school but if we just say",
    "start": "1050679",
    "end": "1055720"
  },
  {
    "text": "loving and it will dramatically decrease um if we give something different like",
    "start": "1055720",
    "end": "1061880"
  },
  {
    "text": "but it's inside the document like Hawaii um the model will also correctly tell",
    "start": "1061880",
    "end": "1067919"
  },
  {
    "text": "this is not the right name uh for this profile so just to show this as a uh",
    "start": "1067919",
    "end": "1074760"
  },
  {
    "text": "example of how we use uh these models and um for all those use cases we have",
    "start": "1074760",
    "end": "1080960"
  },
  {
    "text": "developed models based on the foundation model and deploy into",
    "start": "1080960",
    "end": "1085679"
  },
  {
    "text": "production okay uh and in the very end just to summarize uh we build this",
    "start": "1086000",
    "end": "1091679"
  },
  {
    "text": "Foundation model because Weber have a very significant number of documents to process uh of different document types",
    "start": "1091679",
    "end": "1098679"
  },
  {
    "text": "um and at the same time it has a very high emphasis on accuracy so we have to train a very strong Foundation model uh",
    "start": "1098679",
    "end": "1106000"
  },
  {
    "text": "using all inhouse data uh that we accumulate across years uh to build uh",
    "start": "1106000",
    "end": "1111120"
  },
  {
    "text": "very uh strong adaptive models that we can use for fine-tuning uh um variety of",
    "start": "1111120",
    "end": "1117200"
  },
  {
    "text": "Downstream applications um and R distributed framework definitely helps us in both the training as well as the",
    "start": "1117200",
    "end": "1124039"
  },
  {
    "text": "evaluation uh in this work um this work is also um not only uh by us but also by",
    "start": "1124039",
    "end": "1131720"
  },
  {
    "text": "a huge number of members inside uh our teams um and at the end we are also",
    "start": "1131720",
    "end": "1138120"
  },
  {
    "text": "hiring so if you're interested uh please reach out to us um that brings to end to",
    "start": "1138120",
    "end": "1144320"
  },
  {
    "text": "our presentation uh if you there any questions feel free to come to pick out mic and we can um have a",
    "start": "1144320",
    "end": "1152279"
  },
  {
    "text": "discussion all uh thank you for your uh presentation I have a quick question so for the for your uh uh in-house",
    "start": "1155520",
    "end": "1163280"
  },
  {
    "text": "Foundation model how many parameters you have and the also for evaluation part you mention you can beat the human right",
    "start": "1163280",
    "end": "1171919"
  },
  {
    "text": "and what's evaluation data set you have and uh and uh what's a ground choose",
    "start": "1171919",
    "end": "1176960"
  },
  {
    "text": "label yeah for that one yeah uh let me go backwards so first evaluations uh",
    "start": "1176960",
    "end": "1183480"
  },
  {
    "text": "evaluation is huge part for uh this Foundation model in a few aspect one is we want to use the we we want to have a",
    "start": "1183480",
    "end": "1190559"
  },
  {
    "text": "very comprehensive evaluation Benchmark framework to tell us uh if we throw different type of treatment uh which one",
    "start": "1190559",
    "end": "1197120"
  },
  {
    "text": "work how do we pick up the right architecture uh and uh and and also do a lot of experiments as indications so",
    "start": "1197120",
    "end": "1204320"
  },
  {
    "text": "that's important at the same time uh for all the models that we are going to deploy in production it's very critical",
    "start": "1204320",
    "end": "1209960"
  },
  {
    "text": "to meet meet or re exceed human accuracy otherwise we cannot deploy this model so",
    "start": "1209960",
    "end": "1215720"
  },
  {
    "text": "that's why uh the accuracy is very important uh and how do we build those evaluations so internally we build uh",
    "start": "1215720",
    "end": "1223159"
  },
  {
    "text": "for each documents a golden data sets which we accumulate we sample uh uh",
    "start": "1223159",
    "end": "1229120"
  },
  {
    "text": "carefully documents throughout the Productions uh and uh apply multiple labelers to do cross label check uh to",
    "start": "1229120",
    "end": "1237080"
  },
  {
    "text": "making sure the high quality label and use those to evaluate the model the",
    "start": "1237080",
    "end": "1242400"
  },
  {
    "text": "scale is could varies but usually they are kind of at a scale of tens of thousands of documents uh per document",
    "start": "1242400",
    "end": "1249919"
  },
  {
    "text": "type so that's for the evaluation in terms of what's the size of model we have been exploring different size of",
    "start": "1249919",
    "end": "1256039"
  },
  {
    "text": "the model we find The Sweet Spot is around um say",
    "start": "1256039",
    "end": "1262159"
  },
  {
    "text": "uh5 billion to 1 billion size uh which is very easy to iterate but and also",
    "start": "1262159",
    "end": "1268320"
  },
  {
    "text": "same time very very fast to to serve uh so that's the scale of our model yeah",
    "start": "1268320",
    "end": "1273679"
  },
  {
    "text": "hope this answer your question yeah thank you uh quick clarification on that last",
    "start": "1273679",
    "end": "1280559"
  },
  {
    "text": "one before I ask my question is it uh the decoder specifically that's the half",
    "start": "1280559",
    "end": "1285600"
  },
  {
    "text": "billion to 1 billion or is that including the encoder as well include the encoder decoder as well as the image",
    "start": "1285600",
    "end": "1291919"
  },
  {
    "text": "uh component okay um uh follow-up question I guess would be uh so on the",
    "start": "1291919",
    "end": "1297279"
  },
  {
    "text": "OCR side um could you speak to sort of the decisioning around uh using that over just sort of a purely Vision",
    "start": "1297279",
    "end": "1303600"
  },
  {
    "text": "approach uh as well as do you have uh sort of an internal OCR Library that's",
    "start": "1303600",
    "end": "1309039"
  },
  {
    "text": "Specialized or using something more off the shelf like a tesseract okay um so we are not so we don't train our own OCR",
    "start": "1309039",
    "end": "1317120"
  },
  {
    "text": "model so we do have third party vendors who provide OCR for us um and the reason",
    "start": "1317120",
    "end": "1323159"
  },
  {
    "text": "why we don't just use images is because the just the image alone uh trying to",
    "start": "1323159",
    "end": "1328679"
  },
  {
    "text": "extract information and giving a very accurate result um I don't think we have got good results just by using Vision",
    "start": "1328679",
    "end": "1335919"
  },
  {
    "text": "only so we definitely need the text extracted and also um if you want the",
    "start": "1335919",
    "end": "1341559"
  },
  {
    "text": "text in the image can very like from really big to really small so we'll have to account for a lot of things if you",
    "start": "1341559",
    "end": "1347279"
  },
  {
    "text": "want to Extra ract highly accurate transcription just by using image so",
    "start": "1347279",
    "end": "1353400"
  },
  {
    "text": "having a text along with the image helps just to add two uh two points to that um",
    "start": "1353400",
    "end": "1361200"
  },
  {
    "text": "so oci itself could also have error so we kind of using OCR and vision to check each other uh I think here we are",
    "start": "1361200",
    "end": "1368240"
  },
  {
    "text": "showing the example of when you only have a vision uh you have a vision only model um it could make an error on like",
    "start": "1368240",
    "end": "1375279"
  },
  {
    "text": "o versus zero and five versus S um but at the same time um I think one",
    "start": "1375279",
    "end": "1381400"
  },
  {
    "text": "of the thing that we didn't emphasize a lot is the tokenization we also spent a lot of emphasis on",
    "start": "1381400",
    "end": "1388279"
  },
  {
    "text": "tokenization uh in both uh petring the tokenizer on our own vocabulary but at",
    "start": "1388279",
    "end": "1393919"
  },
  {
    "text": "the same time spending a lot of um time deciding uh how to how to treat like",
    "start": "1393919",
    "end": "1399360"
  },
  {
    "text": "digits how to treat the leading and the middle digits and also uh how to come up with special tokens to for the document",
    "start": "1399360",
    "end": "1406640"
  },
  {
    "text": "uh so those are also very critical for our um performance in the end and are",
    "start": "1406640",
    "end": "1412279"
  },
  {
    "text": "you using bpe or we are using BP okay all right thank you so",
    "start": "1412279",
    "end": "1418159"
  },
  {
    "text": "much hi wonderful work um thank I think you may possibly answered my question",
    "start": "1418200",
    "end": "1423760"
  },
  {
    "text": "but what what area or what kind of Roadblock or hurdle that you encountered that you didn't expect and took up a lot",
    "start": "1423760",
    "end": "1430559"
  },
  {
    "text": "more time than you originally expected like for example tokenization or sort some aspect of the project because",
    "start": "1430559",
    "end": "1435919"
  },
  {
    "text": "individually you know all the components are I would say pretty standard you know so is there something when you try to do",
    "start": "1435919",
    "end": "1442559"
  },
  {
    "text": "this in scale and all these different things that you ran into the hurdle you didn't expect you have",
    "start": "1442559",
    "end": "1448799"
  },
  {
    "text": "anything it was scaling you have anything you want to",
    "start": "1448799",
    "end": "1454679"
  },
  {
    "text": "say not specifically okay and there are lot of different things uh that we",
    "start": "1454679",
    "end": "1460240"
  },
  {
    "text": "encounter uh in the middle um just to give um example like uh in when we when",
    "start": "1460240",
    "end": "1468600"
  },
  {
    "text": "when we handle both OCR and images they are both um non structured and at the same time they are also pretty big in",
    "start": "1468600",
    "end": "1475159"
  },
  {
    "text": "size so we need to do a lot of optimizations inside uh inside each workers how we process download and also",
    "start": "1475159",
    "end": "1482320"
  },
  {
    "text": "catch them if we don't handle them carefully the downloading speed could become the bottom neck the uh GPU could",
    "start": "1482320",
    "end": "1488480"
  },
  {
    "text": "go out of disk because of the cache uh so there are a lot of things that didn't we didn't anticipate much but once we",
    "start": "1488480",
    "end": "1495320"
  },
  {
    "text": "reach the scale of data uh that's what happen and the same time um this data",
    "start": "1495320",
    "end": "1500760"
  },
  {
    "text": "wasn't just lying there ready for us we need to uh fash them through some internal API uh and we also hit a lot of",
    "start": "1500760",
    "end": "1507960"
  },
  {
    "text": "like R limit because of the bursty nature of pretraining that you will have",
    "start": "1507960",
    "end": "1513399"
  },
  {
    "text": "a massive RPS uh for shorter period of time but in other times it's stay flat",
    "start": "1513399",
    "end": "1520440"
  },
  {
    "text": "uh so all of those would become a technical challenge uh in order to have this pre-training at a scale and um yeah",
    "start": "1520440",
    "end": "1528320"
  },
  {
    "text": "I just remember so one important thing is the data noise so we accumulate so we",
    "start": "1528320",
    "end": "1534480"
  },
  {
    "text": "train on 50 million documents and we have like humans who have labeled this so that's not like the standard way of",
    "start": "1534480",
    "end": "1541279"
  },
  {
    "text": "say transcribing some of the text free form text and then we'll have to format it we'll have to look for spelling",
    "start": "1541279",
    "end": "1547039"
  },
  {
    "text": "errors so there are certain fields which um the humans are really baded like",
    "start": "1547039",
    "end": "1553200"
  },
  {
    "text": "maybe because of changing uh instructions or period of time so we'll have to stand all those things so that",
    "start": "1553200",
    "end": "1559480"
  },
  {
    "text": "was one of the main uh important things to consider when we scaled it up I question uh is your model uh mainly",
    "start": "1559480",
    "end": "1568240"
  },
  {
    "text": "used for online serving or offline serving yeah good question uh our model is primary used for online serving",
    "start": "1568240",
    "end": "1574600"
  },
  {
    "text": "that's why we also mentioned the uh the size is very Size Matters uh to choose",
    "start": "1574600",
    "end": "1579760"
  },
  {
    "text": "the right size to for uh serving scale yeah um so if it's online serving like",
    "start": "1579760",
    "end": "1586320"
  },
  {
    "text": "do you have like the strict SLO like for example latency requirement for for your",
    "start": "1586320",
    "end": "1592159"
  },
  {
    "text": "online services and how do you like balance like latency with um with",
    "start": "1592159",
    "end": "1597880"
  },
  {
    "text": "throughput and GPU utilizations I think it's similar to all",
    "start": "1597880",
    "end": "1603919"
  },
  {
    "text": "other real time predictions and that you will have you you will get an SLA",
    "start": "1603919",
    "end": "1609159"
  },
  {
    "text": "requirements from your partner or uh the downstream use cases and then uh it's",
    "start": "1609159",
    "end": "1615600"
  },
  {
    "text": "then we we need to come up with the decision in both the model architecture as well as the optimizations on the",
    "start": "1615600",
    "end": "1621200"
  },
  {
    "text": "serving side to meet that bar um I think that's all I can disclose yeah uh so do",
    "start": "1621200",
    "end": "1628559"
  },
  {
    "text": "you uh did you use like uh do you do quantization for your model or do you do",
    "start": "1628559",
    "end": "1633679"
  },
  {
    "text": "quantization through I already in the training or to optimize for your model",
    "start": "1633679",
    "end": "1639480"
  },
  {
    "text": "in training we use mixed Precision but in in the serving uh we didn't use",
    "start": "1639480",
    "end": "1644559"
  },
  {
    "text": "quantization uh right now but it's definitely one of the real map uh if we",
    "start": "1644559",
    "end": "1649919"
  },
  {
    "text": "encounter more real time uh like more strict SLA requirements all okay",
    "start": "1649919",
    "end": "1657399"
  },
  {
    "text": "that's yeah just uh had a quick question on how you would like handle edge cases and like failure cases say like your",
    "start": "1657399",
    "end": "1662640"
  },
  {
    "text": "receipts like a little crumpled or like you know your driver license isn't like super like clear like how would you handle that like yeah yeah yeah this is",
    "start": "1662640",
    "end": "1670200"
  },
  {
    "text": "why we have that um image quality model um that uh that partially help us to",
    "start": "1670200",
    "end": "1677080"
  },
  {
    "text": "detect and also give feedback to uh the user who upload those images uh to give",
    "start": "1677080",
    "end": "1682760"
  },
  {
    "text": "give them instructions oh you should adjust your angles or you should adjust the lighting um I think this is",
    "start": "1682760",
    "end": "1689480"
  },
  {
    "text": "definitely the most effective way but at the same time in training and Al also serving we have other techniques to help",
    "start": "1689480",
    "end": "1695760"
  },
  {
    "text": "us to sample the right data uh as well as to uh prevent those cases to affect",
    "start": "1695760",
    "end": "1700960"
  },
  {
    "text": "our model performance so is there like a particular like quality threshold of for Quality that like you kind of look for",
    "start": "1700960",
    "end": "1706600"
  },
  {
    "text": "before you you know tell them to the user to you know go take a better picture or something like that yeah um",
    "start": "1706600",
    "end": "1712240"
  },
  {
    "text": "yeah so we have models which will basically tell you whether it's a good enough for not so uh that's also",
    "start": "1712240",
    "end": "1718640"
  },
  {
    "text": "calibrated based on some kind of golden data set all right cool great thank",
    "start": "1718640",
    "end": "1723679"
  },
  {
    "text": "you a great work uh can you speak a little bit about the adversarial examples um into the training set like",
    "start": "1723679",
    "end": "1730760"
  },
  {
    "text": "how far do you go in terms of getting the different types of aerial examples",
    "start": "1730760",
    "end": "1736640"
  },
  {
    "text": "uh or even to the extent of uh synthesize",
    "start": "1736640",
    "end": "1742360"
  },
  {
    "text": "it uh so missing dat um so we don't synthesize any data we have a lot of",
    "start": "1742360",
    "end": "1749159"
  },
  {
    "text": "data for we process them so with respect to sampling we just sample based on",
    "start": "1749159",
    "end": "1754559"
  },
  {
    "text": "document types if that's what you're asking and but to give some example um I",
    "start": "1754559",
    "end": "1761519"
  },
  {
    "text": "think there are a lot of uh so our data was labeled by uh human operators who",
    "start": "1761519",
    "end": "1768159"
  },
  {
    "text": "goal was to uh transcribe those fields and they will have a lot of different logic or implicit logic like if for",
    "start": "1768159",
    "end": "1776159"
  },
  {
    "text": "example if a start date is January 1st and there's no expiration date on the document they will intend to add one",
    "start": "1776159",
    "end": "1782320"
  },
  {
    "text": "year uh as expiration date so we also spend a lot efforts trying to identify",
    "start": "1782320",
    "end": "1787440"
  },
  {
    "text": "those samples because you don't want to teach the model to hallucinate um there are certain biases that we want to",
    "start": "1787440",
    "end": "1793480"
  },
  {
    "text": "prevent the model to learn from so we also need to spend time to uh identify from opportunity data and exclude them",
    "start": "1793480",
    "end": "1800440"
  },
  {
    "text": "uh from the data set and I hope that can serve as one example of the ADV serial",
    "start": "1800440",
    "end": "1806720"
  },
  {
    "text": "even though it's not intended to be serial but in the end it definitely posa some challenges or mistakes to the model",
    "start": "1806720",
    "end": "1814000"
  },
  {
    "text": "it definitely help with the out of example yeah scenario thank you uh I have three quick question first",
    "start": "1814000",
    "end": "1821840"
  },
  {
    "text": "the question is about the um uh the multitask you are mentioning in the",
    "start": "1821840",
    "end": "1828399"
  },
  {
    "text": "slack yeah yeah so for that part could you share more uh how your model handles",
    "start": "1828399",
    "end": "1833799"
  },
  {
    "text": "that different multitask in the same model R in the same same same model run",
    "start": "1833799",
    "end": "1839679"
  },
  {
    "text": "same model run yeah yeah sorry can you clarify the question",
    "start": "1839679",
    "end": "1846399"
  },
  {
    "text": "I didn't how how do you uh handle the multitask in the same model Rong you you",
    "start": "1846399",
    "end": "1852159"
  },
  {
    "text": "use different header right MTI head and how you so here you never Shar the",
    "start": "1852159",
    "end": "1858639"
  },
  {
    "text": "architecture about that part yeah so we have one single decoder which does all the decoder related tasks and then we",
    "start": "1858639",
    "end": "1865559"
  },
  {
    "text": "have incoder for which we have multiple heads based on the task we have so um",
    "start": "1865559",
    "end": "1870600"
  },
  {
    "text": "our training pipeline is configured in such a way that you can assign probabilities to each task and based on the probab each data sample we have gets",
    "start": "1870600",
    "end": "1877840"
  },
  {
    "text": "assigned to these particular tasks I see so that is for the case when",
    "start": "1877840",
    "end": "1883000"
  },
  {
    "text": "there's only one hat but uh for example we have other heads uh to that is",
    "start": "1883000",
    "end": "1889600"
  },
  {
    "text": "intended for different task like for example uh we have another hat for document classification so in that case",
    "start": "1889600",
    "end": "1895039"
  },
  {
    "text": "uh the example will feed into the encoder and it will go to the multiple hats and each of them will generate",
    "start": "1895039",
    "end": "1901000"
  },
  {
    "text": "their own loss so we will uh add the LW together to back propagate uh uh to",
    "start": "1901000",
    "end": "1906799"
  },
  {
    "text": "optimize the model at the same time when we do monitoring which is the part we haven't talked much about in a",
    "start": "1906799",
    "end": "1912639"
  },
  {
    "text": "monitoring we'll also monitor those training laws separately uh to monitor progress and we also have callback to",
    "start": "1912639",
    "end": "1919880"
  },
  {
    "text": "print out the examples uh of each task in the training so that to do sanity",
    "start": "1919880",
    "end": "1925039"
  },
  {
    "text": "check as well as do qualitative check how do you design different weight for different",
    "start": "1925039",
    "end": "1930279"
  },
  {
    "text": "task it's a basically an experimentation and for uh for us the field extraction",
    "start": "1930279",
    "end": "1936440"
  },
  {
    "text": "is the most important task so that's where we have most weight on here okay thank you second question is about the",
    "start": "1936440",
    "end": "1941480"
  },
  {
    "text": "MTI uh language support so you have 10 languages uh 10 plus yeah yeah 10 plus",
    "start": "1941480",
    "end": "1947519"
  },
  {
    "text": "language yeah how do you support multilingual here so we train our tokenizer on our",
    "start": "1947519",
    "end": "1953960"
  },
  {
    "text": "data and the tokenizer learns the tokens for all the languages so we don't do anything more specific to each language",
    "start": "1953960",
    "end": "1961000"
  },
  {
    "text": "for now yeah oh I see tokenization I see so you okay the next",
    "start": "1961000",
    "end": "1967639"
  },
  {
    "text": "question is about the um you have 120 different tabs yeah that is the yeah",
    "start": "1967639",
    "end": "1974440"
  },
  {
    "text": "yeah so that's the um that's a one 120 classification model like multiple",
    "start": "1974440",
    "end": "1982320"
  },
  {
    "text": "classification model for this uh Foundation model right um both the classification part but also the decoder",
    "start": "1982320",
    "end": "1989039"
  },
  {
    "text": "part uh on the classification part yes uh it's going to be 120 different class for classification",
    "start": "1989039",
    "end": "1996240"
  },
  {
    "text": "but at the same time it also means that uh there are these diversity of data in the P training uh which is very",
    "start": "1996240",
    "end": "2003159"
  },
  {
    "text": "important for model to learn how to extract the key information from those different from both variety of the types",
    "start": "2003159",
    "end": "2010120"
  },
  {
    "text": "yeah so it's intended for both purposes mhm I see yeah thank you thank",
    "start": "2010120",
    "end": "2017960"
  },
  {
    "text": "you hi um so interesting talk I just want to ask a little bit about I walked",
    "start": "2018080",
    "end": "2023320"
  },
  {
    "text": "halfway through so I probably might have missed the first piece but uh you mentioned something about um an idea of",
    "start": "2023320",
    "end": "2028840"
  },
  {
    "text": "a person being uh able to identify if that was a generated ID or not um so the",
    "start": "2028840",
    "end": "2034639"
  },
  {
    "text": "question I had is around first of all like the data set that you how how did you come up with the data set that you",
    "start": "2034639",
    "end": "2040039"
  },
  {
    "text": "have for sorry can you repeat the specifically the data scenario you want to capture um so I want to add",
    "start": "2040039",
    "end": "2047519"
  },
  {
    "text": "understand that like let's say um let's say I have a driver's license and how can you detect if this is actually",
    "start": "2047519",
    "end": "2055760"
  },
  {
    "text": "valid is that something that your platform can do or is this not not like necessarily so right now the we just",
    "start": "2055760",
    "end": "2062560"
  },
  {
    "text": "analyze the image of the uh picture the are not uploaded based on that our image",
    "start": "2062560",
    "end": "2068320"
  },
  {
    "text": "quality model will identify whether it's a valid or not valid image of the document yeah uh sorry I didn't get the",
    "start": "2068320",
    "end": "2075520"
  },
  {
    "text": "last part what you mean so basically based on image classification we can identify whether it's a valid image or",
    "start": "2075520",
    "end": "2080760"
  },
  {
    "text": "valid document I mean yeah okay and to be able to do that what is",
    "start": "2080760",
    "end": "2086960"
  },
  {
    "text": "the yeah thank you thank you thanks everyone",
    "start": "2087200",
    "end": "2093800"
  },
  {
    "text": "[Applause]",
    "start": "2093800",
    "end": "2097329"
  }
]