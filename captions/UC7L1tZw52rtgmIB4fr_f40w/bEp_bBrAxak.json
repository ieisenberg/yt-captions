[
  {
    "text": "all right yeah so uh welcome everyone my name is Edward and I'm joined by my teammate Reyes",
    "start": "5000",
    "end": "11340"
  },
  {
    "text": "um we're both software Engineers working on Race serve and today we're going to be talking about building production AI applications with racer",
    "start": "11340",
    "end": "19020"
  },
  {
    "text": "so high level overview of this talk um so first I'm going to talk a little bit about the motivation behind race",
    "start": "19020",
    "end": "24539"
  },
  {
    "text": "serve and how racer helps address some common problems that we see in serving",
    "start": "24539",
    "end": "30359"
  },
  {
    "text": "online applications then we're going to dive into a few trends that we've noticed among the companies and open",
    "start": "30359",
    "end": "36960"
  },
  {
    "text": "source users that we've been working with in 2023 and how we've been addressing them and some exciting updates that we've launched this year",
    "start": "36960",
    "end": "45000"
  },
  {
    "text": "and then for the second half of the talk shreyas is going to be giving a demo of running a production AI application on",
    "start": "45000",
    "end": "51719"
  },
  {
    "text": "Race serve on the any scale platform so that will show off a bunch of the features that I'm highlighting in the talk",
    "start": "51719",
    "end": "58500"
  },
  {
    "text": "so let's dive into the overview so before I talk about racer I first",
    "start": "58500",
    "end": "64080"
  },
  {
    "text": "want to talk about some of the problems that we're trying to solve with it so many of the customers and users that",
    "start": "64080",
    "end": "70020"
  },
  {
    "text": "we've worked with come to us with an architecture that looks something like it is on the screen right now which we call model microservices",
    "start": "70020",
    "end": "76799"
  },
  {
    "text": "so this is um an architecture diagram of an application that has two models you",
    "start": "76799",
    "end": "82140"
  },
  {
    "text": "know one tensorflow and one Pi torch each of those is a separate application",
    "start": "82140",
    "end": "87600"
  },
  {
    "text": "with a separate Docker file a separate configuration and is deployed as a separate kubernetes deployment those are",
    "start": "87600",
    "end": "95100"
  },
  {
    "text": "then stitched together with you know some kind of business logic running in a python web server like Fast API",
    "start": "95100",
    "end": "102299"
  },
  {
    "text": "and as you can see this works for deploying you know multi-model AI applications but it's very complicated",
    "start": "102299",
    "end": "109200"
  },
  {
    "text": "there are many different configurations there are many different code bases and",
    "start": "109200",
    "end": "114240"
  },
  {
    "text": "images that you need to maintain and this comes with some drawbacks so the first is that it can be really",
    "start": "114240",
    "end": "119520"
  },
  {
    "text": "hard to innovate because you have this single logical application split across many different components it requires a",
    "start": "119520",
    "end": "126600"
  },
  {
    "text": "lot of expertise and there are a lot of moving parts that you need to update in tandem",
    "start": "126600",
    "end": "131959"
  },
  {
    "text": "it can also be really hard if you want to efficiently use and share resources among multiple different models for",
    "start": "132360",
    "end": "138180"
  },
  {
    "text": "example if some models are used infrequently and you want to have them share the same GPU this can be really",
    "start": "138180",
    "end": "144540"
  },
  {
    "text": "complicated to achieve with kubernetes maybe requiring you to understand the internals and a scheduler or use some",
    "start": "144540",
    "end": "150180"
  },
  {
    "text": "kind of plug-in for the cluster so with race serve we're trying to build",
    "start": "150180",
    "end": "155879"
  },
  {
    "text": "a better way to build these type of applications so race serve is flexible scalable and efficient compute for",
    "start": "155879",
    "end": "162239"
  },
  {
    "text": "online inference one of the highlights of rayserv is that like the rest of Ray it's python native",
    "start": "162239",
    "end": "167879"
  },
  {
    "text": "this means that you can express complex applications like the one that I had in the previous diagram in just a single",
    "start": "167879",
    "end": "173760"
  },
  {
    "text": "python file you can mix business logic with multiple different types of models and you can iterate quickly testing",
    "start": "173760",
    "end": "180360"
  },
  {
    "text": "locally and deploying to production all as a single program we also have first class support for",
    "start": "180360",
    "end": "187200"
  },
  {
    "text": "multi-model inference so this includes model composition running many different independent models on the same hardware",
    "start": "187200",
    "end": "193500"
  },
  {
    "text": "and even use cases like model multiplexing where you might have tens hundreds thousands even tens of",
    "start": "193500",
    "end": "199500"
  },
  {
    "text": "thousands of models that you want to run on a small amount of hardware and overall because race serve is built",
    "start": "199500",
    "end": "205379"
  },
  {
    "text": "on top of Ray it supports flexible scaling and resource allocation which is crucial for reducing costs especially as",
    "start": "205379",
    "end": "212099"
  },
  {
    "text": "the cost of Hardware like GPU gpus Rises and of course we're building this for",
    "start": "212099",
    "end": "218099"
  },
  {
    "text": "online use cases which means that it has to be production ready and we've we've focused a lot on production Readiness",
    "start": "218099",
    "end": "223799"
  },
  {
    "text": "with the two best ways to run racer being on top of kubernetes or on top of the any scale platform like you'll see",
    "start": "223799",
    "end": "229440"
  },
  {
    "text": "in the demo later so let's dive into a real world example",
    "start": "229440",
    "end": "235260"
  },
  {
    "text": "of these benefits so here I'm talking about um a user samsara that we've worked with",
    "start": "235260",
    "end": "241620"
  },
  {
    "text": "over the past year that changed their model serving solution to use racerv",
    "start": "241620",
    "end": "247500"
  },
  {
    "text": "so before racer they had separate microservices for this crash detection application",
    "start": "247500",
    "end": "252840"
  },
  {
    "text": "so the exact details of what's happening within each of these boxes aren't too important the important part is that",
    "start": "252840",
    "end": "259079"
  },
  {
    "text": "this Anatomy looks similar to that architecture diagram that I showed two slides ago they had multiple",
    "start": "259079",
    "end": "265680"
  },
  {
    "text": "microservices in multiple languages each with their own configuration deployed",
    "start": "265680",
    "end": "271080"
  },
  {
    "text": "independently and they were stitching them together with http this came with some drawbacks",
    "start": "271080",
    "end": "276600"
  },
  {
    "text": "so the first is that it was inefficient and they had a really hard time changing their parallelization and scaling",
    "start": "276600",
    "end": "282540"
  },
  {
    "text": "strategy without rewriting a large chunk of modeling infrastructure and devops",
    "start": "282540",
    "end": "288720"
  },
  {
    "text": "code any kind of in Innovation was difficult because they had to coordinate across",
    "start": "288720",
    "end": "294360"
  },
  {
    "text": "these multiple teams if they wanted to change anything about their architecture",
    "start": "294360",
    "end": "299759"
  },
  {
    "text": "and this also meant that they had a high operational cost each of these was slightly different and was deployed",
    "start": "299759",
    "end": "306000"
  },
  {
    "text": "upgraded and monitored both separately and in a slightly different way so they",
    "start": "306000",
    "end": "311400"
  },
  {
    "text": "ended up having a very high operational cost from all of the Manpower they had to put into running this as a production",
    "start": "311400",
    "end": "316560"
  },
  {
    "text": "system so they decided to change to using Rey and Ray serve for their next Generation",
    "start": "316560",
    "end": "322080"
  },
  {
    "text": "ml platform they were able to express this entire application as uh just essentially one",
    "start": "322080",
    "end": "329460"
  },
  {
    "text": "Python program but with all of the flexible scaling and resource allocation benefits that racer gives you",
    "start": "329460",
    "end": "337680"
  },
  {
    "text": "this meant that they could make their application much more efficient they could independently scale each step they",
    "start": "337680",
    "end": "342960"
  },
  {
    "text": "could paralyze within the existing steps and they could share resources for different models and pre-processing",
    "start": "342960",
    "end": "348419"
  },
  {
    "text": "stages they were also able to iterate much more quickly in order to add new models",
    "start": "348419",
    "end": "354539"
  },
  {
    "text": "update business logic and this was mostly because they were able to update their architecture and",
    "start": "354539",
    "end": "361979"
  },
  {
    "text": "test it locally just as a Python program and then deployed to production easily",
    "start": "361979",
    "end": "368300"
  },
  {
    "text": "and finally this solution was production ready and gave them a unified story for deploying upgraded upgrading and",
    "start": "368699",
    "end": "375419"
  },
  {
    "text": "monitoring the entire application together instead of having a piecemeal solution with many different moving",
    "start": "375419",
    "end": "380580"
  },
  {
    "text": "parts so overall moving from this model micro",
    "start": "380580",
    "end": "385620"
  },
  {
    "text": "Services solution to race serve netted them over a 50 reduction in total ML",
    "start": "385620",
    "end": "391080"
  },
  {
    "text": "inferencing cost per year and this is not even including the benefits in",
    "start": "391080",
    "end": "396240"
  },
  {
    "text": "operational overhead that I mentioned this is just the hardware cost",
    "start": "396240",
    "end": "401300"
  },
  {
    "text": "so now let's dive into a few of the trends that we've been seeing in 2023 and how we've been addressing them",
    "start": "402419",
    "end": "408660"
  },
  {
    "text": "so the first I think is is probably the most obvious you've been hearing about this uh not only today but probably ad",
    "start": "408660",
    "end": "414479"
  },
  {
    "text": "nauseam for the past few months which is 2023 has been the year of the rise of llms just a year ago trace and I were on",
    "start": "414479",
    "end": "422160"
  },
  {
    "text": "stage at the previous Ray Summit giving a talk not too different from this one and I don't think we said those three",
    "start": "422160",
    "end": "428100"
  },
  {
    "text": "letters in a row and today you can't go more than like five minutes without hearing it so the the industry has",
    "start": "428100",
    "end": "434039"
  },
  {
    "text": "definitely changed so every week or two weeks there's a new model or new techniques coming out and",
    "start": "434039",
    "end": "440819"
  },
  {
    "text": "the specifics of those are important and there's a ton of business value being driven around llms but to me the more",
    "start": "440819",
    "end": "446699"
  },
  {
    "text": "important point is this meta point which is with technology changing this fast it's just absolutely critical that you",
    "start": "446699",
    "end": "453720"
  },
  {
    "text": "can iterate fast and keep up with the trends you can't be locked into a framework you can't be locked into an",
    "start": "453720",
    "end": "459240"
  },
  {
    "text": "architecture you have to be ready to actually be able to drive business value",
    "start": "459240",
    "end": "464520"
  },
  {
    "text": "from these new developments quickly if you want to keep up so in race serve we've been keeping up",
    "start": "464520",
    "end": "470759"
  },
  {
    "text": "with the llm trend um and the flexibility and the developer",
    "start": "470759",
    "end": "476099"
  },
  {
    "text": "productivity of race serve has allowed us to do that we've recently launched the ray llm sub project which is",
    "start": "476099",
    "end": "483060"
  },
  {
    "text": "optimized infrastructure for serving llms and this is actually what powers any scale endpoints which is the",
    "start": "483060",
    "end": "489960"
  },
  {
    "text": "offering that you heard about in the keynote earlier today this integrates with state-of-the-art Solutions like vllm for optimizations",
    "start": "489960",
    "end": "497819"
  },
  {
    "text": "and we'll soon integrate with nvidia's tensor RT as well",
    "start": "497819",
    "end": "503280"
  },
  {
    "text": "we've also added a number of llm specific features and optimizations basically keeping up with the state of",
    "start": "503280",
    "end": "509340"
  },
  {
    "text": "our state of the art as quickly as we can this includes streaming responses continuous batching and tensor",
    "start": "509340",
    "end": "514680"
  },
  {
    "text": "parallelism and for a deep dive into this topic and some of the really cool stuff that we've been working on I",
    "start": "514680",
    "end": "520320"
  },
  {
    "text": "encourage you to check out tanmay and kade's talk tomorrow afternoon",
    "start": "520320",
    "end": "525380"
  },
  {
    "text": "so the second Trend that's very closely related to this rise of llms is the increase in cost of hardware",
    "start": "526380",
    "end": "532920"
  },
  {
    "text": "so not only are models getting larger and more expensive but they're also more and more use cases using machine",
    "start": "532920",
    "end": "538860"
  },
  {
    "text": "learning so as the hardware demands have gone up GPU availability has gone down",
    "start": "538860",
    "end": "544200"
  },
  {
    "text": "I'm sure many of the people in this room have run into this problem where it's hard to even get your hands on a GPU to",
    "start": "544200",
    "end": "549660"
  },
  {
    "text": "do inference or training this means that more than ever it's crucial to be able to efficiently use",
    "start": "549660",
    "end": "555779"
  },
  {
    "text": "the hardware that you can get and make the most of flexible resource allocation and scaling and avoid wasted resources",
    "start": "555779",
    "end": "564360"
  },
  {
    "text": "we've been doing a lot of performance focused work to address this this year",
    "start": "564360",
    "end": "569820"
  },
  {
    "text": "one example of this is we recently launched support for model multiplexing which is this case that is for this many",
    "start": "569820",
    "end": "577440"
  },
  {
    "text": "model workload where you have hundreds or thousands of models maybe one for each product line or one for each user",
    "start": "577440",
    "end": "583920"
  },
  {
    "text": "which many of our customers have come to us with and the model multiplexing support we've added to race serve can more than double",
    "start": "583920",
    "end": "590640"
  },
  {
    "text": "throughput on the same hardware for a typical many model workloads if you want",
    "start": "590640",
    "end": "595860"
  },
  {
    "text": "to hear more about more in depth about that I encourage you to check out our teammate Cindy and sihan's talk tomorrow",
    "start": "595860",
    "end": "601500"
  },
  {
    "text": "afternoon we've also been working on some Advanced Auto scaling capabilities to make sure",
    "start": "601500",
    "end": "607800"
  },
  {
    "text": "that you can get gpus as much as possible and at the lowest price possible so on any scale we now have",
    "start": "607800",
    "end": "614220"
  },
  {
    "text": "support for cross Zone and cross cluster or cross region Auto scaling so you can get gpus wherever they're available and",
    "start": "614220",
    "end": "621360"
  },
  {
    "text": "we've also added support for spot instances for online services this means",
    "start": "621360",
    "end": "626399"
  },
  {
    "text": "you can use spot instances and gracefully fall back to On Demand when they get preempted and will also",
    "start": "626399",
    "end": "632820"
  },
  {
    "text": "automatically fall back from on-demand to spot when they become available this means you can get instances for as cheap",
    "start": "632820",
    "end": "639300"
  },
  {
    "text": "as possible and this is one of the things that shreyas will show in the demo in a few minutes",
    "start": "639300",
    "end": "644779"
  },
  {
    "text": "so with these techniques and a few more optimizations such as the ones that I mentioned around",
    "start": "645420",
    "end": "650540"
  },
  {
    "text": "vllm and LMS in general any scale endpoints which runs on top of race serve has been able to offer",
    "start": "650540",
    "end": "656940"
  },
  {
    "text": "state-of-the-art models like Lama 2 for 50 cheaper than open ai's GPT 3.5",
    "start": "656940",
    "end": "662940"
  },
  {
    "text": "and I also want to highlight some of the developer productivity benefits the any scale endpoints team has been able to",
    "start": "662940",
    "end": "668940"
  },
  {
    "text": "add new models on the the same day that they're publicly released uh which is",
    "start": "668940",
    "end": "674579"
  },
  {
    "text": "just really cool and it's a testament to um how beneficial it is to have uh to be",
    "start": "674579",
    "end": "680220"
  },
  {
    "text": "able to locally test code and push to production quickly",
    "start": "680220",
    "end": "685160"
  },
  {
    "text": "so the final Trend that I want to mention is that 2023 has really been the year that a lot of companies have gone",
    "start": "685980",
    "end": "692459"
  },
  {
    "text": "to production with ML in previous years we'd seen a lot of pses and ideation but",
    "start": "692459",
    "end": "697800"
  },
  {
    "text": "we hadn't seen people like really driving business value in production but that's definitely changed this year and",
    "start": "697800",
    "end": "703980"
  },
  {
    "text": "what it means is that having a really solid deployment and observability story is just absolutely critical you need to",
    "start": "703980",
    "end": "709200"
  },
  {
    "text": "be able to sleep well at night when you're running these services so we've been putting a lot of work into",
    "start": "709200",
    "end": "714300"
  },
  {
    "text": "production Readiness for race serve and any scale Services we've added a lot of observability features over the past few",
    "start": "714300",
    "end": "720480"
  },
  {
    "text": "releases including the new Serv tab to the ray dashboard and improved metrics",
    "start": "720480",
    "end": "725700"
  },
  {
    "text": "and logging to help you figure out where your bottleneck is and what happened when things go wrong",
    "start": "725700",
    "end": "731760"
  },
  {
    "text": "we've also been doing a lot of continuous chaos and scale testing to make sure that we catch any stability",
    "start": "731760",
    "end": "737579"
  },
  {
    "text": "issues before customers could even notice them and we've been working closely with",
    "start": "737579",
    "end": "743880"
  },
  {
    "text": "large-scale users to make sure that we improve the upgrade and stability story for both Cube Ray and the open source",
    "start": "743880",
    "end": "750660"
  },
  {
    "text": "and the managed any scale service offering and this hasn't just been the year of",
    "start": "750660",
    "end": "757200"
  },
  {
    "text": "machine learning going to production it's also been the year of race serve going to production",
    "start": "757200",
    "end": "762839"
  },
  {
    "text": "so as I mentioned earlier samsara was able to save 50 on their total annual inference costs by migrating their",
    "start": "762839",
    "end": "768779"
  },
  {
    "text": "platform to use Reserve and financial is also running a 4 000",
    "start": "768779",
    "end": "775079"
  },
  {
    "text": "GPU online inference cluster the largest one in their company on top of rayserv",
    "start": "775079",
    "end": "781500"
  },
  {
    "text": "and any scale was able to launch their llm endpoint offering and offer",
    "start": "781500",
    "end": "786959"
  },
  {
    "text": "state-of-the-art models for 50 percent less than openai's chat GPT 3.5",
    "start": "786959",
    "end": "793160"
  },
  {
    "text": "we've also seen wildly increased usage across the board so across many",
    "start": "793680",
    "end": "799500"
  },
  {
    "text": "different use cases and Company sizes we've seen over 600 percent increase in the total usage of Ray serve in the last",
    "start": "799500",
    "end": "806399"
  },
  {
    "text": "year so with all of the exciting use cases that we've seen people using racer going to production for and all of the",
    "start": "806399",
    "end": "813420"
  },
  {
    "text": "stability and feature improvements we've put in in the last year we're excited to announce that in 2.7 race serve is now",
    "start": "813420",
    "end": "819839"
  },
  {
    "text": "generally available",
    "start": "819839",
    "end": "822500"
  },
  {
    "text": "thank you um so now I'm going to hand it off to shreyas who's going to give a demo of a lot of these cool features in action",
    "start": "825420",
    "end": "832880"
  },
  {
    "text": "nice awesome thank you Ed for walking us through some of the exciting trends that are happening in the ml industry and",
    "start": "838260",
    "end": "843720"
  },
  {
    "text": "talking a little bit about how racer has helped Propel those Trends forward now let's jump into a demo that showcases",
    "start": "843720",
    "end": "849240"
  },
  {
    "text": "some of the features of Ray serve So today we're going to be demoing a photographic calculator app the way that",
    "start": "849240",
    "end": "855720"
  },
  {
    "text": "this application works is you take a picture of an algebra problem like 5x equals 10. and then you get an answer",
    "start": "855720",
    "end": "861360"
  },
  {
    "text": "like x equals 2. in between we're going to be running two steps on top of racer first we're going to be running a reader",
    "start": "861360",
    "end": "867959"
  },
  {
    "text": "deployment this serve deployment is going to be running a pytorch model which takes in an image and then",
    "start": "867959",
    "end": "873240"
  },
  {
    "text": "extracts text from it it's going to send that text over to a solver deployment the solver is going to",
    "start": "873240",
    "end": "878940"
  },
  {
    "text": "be running Senpai Senpai is python symbolic solving Library it can take in text of an algebra expression and",
    "start": "878940",
    "end": "884519"
  },
  {
    "text": "actually solve that problem and give us our final answer this is a pretty interesting application",
    "start": "884519",
    "end": "889560"
  },
  {
    "text": "because the reader deployment and the solver deployment have very different Hardware requirements the reader is",
    "start": "889560",
    "end": "895440"
  },
  {
    "text": "going to be using a CPU in order to load the image and do some pre-processing but it also needs a GPU in order to run the",
    "start": "895440",
    "end": "901560"
  },
  {
    "text": "model efficiently on the other hand our solver is going to be doing symbolic solving this is a very CPU intensive",
    "start": "901560",
    "end": "907620"
  },
  {
    "text": "process and our solver is not going to be using a GPU because of these different Hardware requirements it makes",
    "start": "907620",
    "end": "912660"
  },
  {
    "text": "a lot of sense to split our application into two separate deployments and as we'll see later in the demo surf can",
    "start": "912660",
    "end": "918060"
  },
  {
    "text": "independently scale each of these deployments automatically in order to best meet your user's demand",
    "start": "918060",
    "end": "924120"
  },
  {
    "text": "our demo is going to be running on the any seal platform any skill is going to give us access to a cluster of Ray nodes",
    "start": "924120",
    "end": "930480"
  },
  {
    "text": "on top of these nodes we're going to be running replicas of the deployments that we just talked about",
    "start": "930480",
    "end": "936060"
  },
  {
    "text": "any skill is going to provide access to that Ray cluster through a load balancer so requests will come into the load",
    "start": "936060",
    "end": "941100"
  },
  {
    "text": "balancer get forwarded to the reader deployment which will extract text send that text over to the solver the solver",
    "start": "941100",
    "end": "947940"
  },
  {
    "text": "will solve the problem and send a response back to the user we're going to be covering four",
    "start": "947940",
    "end": "954180"
  },
  {
    "text": "different steps during our demo today first we're going to talk a little bit about the any scale service UI which",
    "start": "954180",
    "end": "959459"
  },
  {
    "text": "gives us nice visualizations for our serve application then we'll go over serves new observability capabilities that give you",
    "start": "959459",
    "end": "965699"
  },
  {
    "text": "information on how serve is reacting to current changes in traffic then we're going to talk a little bit about serve's new spot instant support",
    "start": "965699",
    "end": "972120"
  },
  {
    "text": "which lets you run your models and your workloads on top of spot instances to save cost and last we're going to cover",
    "start": "972120",
    "end": "977339"
  },
  {
    "text": "Auto scaling and talk about how served can scale the number of replicas and nodes in its system up or down in order",
    "start": "977339",
    "end": "982620"
  },
  {
    "text": "to meet user demand with that out of the way let's jump into the demo",
    "start": "982620",
    "end": "988279"
  },
  {
    "text": "all right so we're going to be starting our demo here on the any scale platform the page we're looking at is the home",
    "start": "988320",
    "end": "993540"
  },
  {
    "text": "page of our race serve application at the top here we have the name of our application our photographic calculator",
    "start": "993540",
    "end": "999360"
  },
  {
    "text": "we have some high level information like the fact that the application is currently running in production and some other metadata",
    "start": "999360",
    "end": "1005240"
  },
  {
    "text": "and right below we have all the versions of our application that are running we're currently running one version of the application with two deployments the",
    "start": "1005240",
    "end": "1012199"
  },
  {
    "text": "reader and the solver let's click into the docs for the reader and take a look at what they look like",
    "start": "1012199",
    "end": "1020320"
  },
  {
    "text": "all right so the docs give us some information about the requests and responses that are going to flow through our application but they'll also let us",
    "start": "1024260",
    "end": "1030438"
  },
  {
    "text": "send a test request to our application just to make sure that it's working so let's try it out as I mentioned we",
    "start": "1030439",
    "end": "1036140"
  },
  {
    "text": "have a photographic calculator so I need a photo of an algebra problem I Googled some algebra problems and I'm",
    "start": "1036140",
    "end": "1041360"
  },
  {
    "text": "going to take a screenshot of one of them and try to get served to solve it we're going to screenshot this problem 5x plus 4 is equal to 3x plus 16. so I",
    "start": "1041360",
    "end": "1049220"
  },
  {
    "text": "take my screenshot go back to the UI and then I submit it",
    "start": "1049220",
    "end": "1054559"
  },
  {
    "text": "and then we let serve execute and down here in the responses it looks like serve says that X is equal to 6.",
    "start": "1054559",
    "end": "1060620"
  },
  {
    "text": "Let's double check that just to make sure that we're right all right so again the problem was 5x plus 4 equals 3x Plus 16. on the left",
    "start": "1060620",
    "end": "1067340"
  },
  {
    "text": "hand side 5 times 6 is 30 30 plus 4 is 34. on the right hand side 3 times 6 is 18. 18 plus 16 is 34 looks like both",
    "start": "1067340",
    "end": "1075320"
  },
  {
    "text": "sides balance serve as getting that number right all right whoo",
    "start": "1075320",
    "end": "1082419"
  },
  {
    "text": "okay so that was pretty cool but now let's try to see what this would look like in production what I'm going to do",
    "start": "1084140",
    "end": "1089480"
  },
  {
    "text": "is start a burst of traffic against this application then we can use serves observability to see what that burst of traffic looks like",
    "start": "1089480",
    "end": "1097059"
  },
  {
    "text": "this is a terminal that contains a command which will start a tool that can generate load let's run this command",
    "start": "1097160",
    "end": "1104860"
  },
  {
    "text": "all right we get a web interface we're going to go over there to actually start the load test",
    "start": "1105440",
    "end": "1110679"
  },
  {
    "text": "all right so this command is running a load test on the First Column there you can see the number of requests that it's",
    "start": "1114440",
    "end": "1119960"
  },
  {
    "text": "sending it's sending requests just like that test request that we sent and we can also see the number of failures we see that the number of requests is going",
    "start": "1119960",
    "end": "1125900"
  },
  {
    "text": "up the number of failures is staying at zero that's great it means our server application is successfully serving traffic in production",
    "start": "1125900",
    "end": "1132679"
  },
  {
    "text": "we can also take a look at the requests per second that our load test tool is sending as well as the response times this corresponds to the throughput and",
    "start": "1132679",
    "end": "1139520"
  },
  {
    "text": "the latency of our application we're going to check back in on this tool in just a little bit for now let's",
    "start": "1139520",
    "end": "1144980"
  },
  {
    "text": "jump over to serve's observability and see how we might manage this application in production",
    "start": "1144980",
    "end": "1150440"
  },
  {
    "text": "we're back on the page where we sent the test request now if I were managing this application in production one useful",
    "start": "1150440",
    "end": "1156140"
  },
  {
    "text": "feature that I might want is to understand how this request flowed through the application for example how",
    "start": "1156140",
    "end": "1161299"
  },
  {
    "text": "much time did it spend on each deployment so we can see in the response headers a request ID for This Server Quest",
    "start": "1161299",
    "end": "1168620"
  },
  {
    "text": "I'm going to copy this request ID and because I'm running serve on top of the any scale platform I can take",
    "start": "1168620",
    "end": "1174740"
  },
  {
    "text": "advantage of a native integration between race serve and AWS cloudwatch to quickly search through all of the logs",
    "start": "1174740",
    "end": "1180440"
  },
  {
    "text": "that serve as emitting and find the ones that are related to just this request let's go to AWS cloudwatch",
    "start": "1180440",
    "end": "1186919"
  },
  {
    "text": "all right so we're in Cloud watch cloudwatch is a tool that's going to aggregate all of my logs and let me quickly search through them I'm going to",
    "start": "1186919",
    "end": "1193400"
  },
  {
    "text": "search for any messages that contain the request ID for that test request that we sent",
    "start": "1193400",
    "end": "1198679"
  },
  {
    "text": "let's hit enter let cloudwatch look for the messages looks like no events were found on this one so my mistake I think I was looking",
    "start": "1198679",
    "end": "1205460"
  },
  {
    "text": "at the wrong Cloud watch uh Trace there so what I'm going to do is run the same command",
    "start": "1205460",
    "end": "1212500"
  },
  {
    "text": "over here ah okay looks like cloudwatch still isn't showing anything what could be happening is that the logs are simply",
    "start": "1219620",
    "end": "1225440"
  },
  {
    "text": "delayed as the request is flowing through the system but basically what would be what we would what we would be",
    "start": "1225440",
    "end": "1231200"
  },
  {
    "text": "able to see is um any messages related to the request that we just sent I'm going to try one",
    "start": "1231200",
    "end": "1237140"
  },
  {
    "text": "more time",
    "start": "1237140",
    "end": "1239440"
  },
  {
    "text": "oh I see",
    "start": "1242900",
    "end": "1245679"
  },
  {
    "text": "yeah okay we're not seeing any uh any events from that particular message but what we would have seen is basically",
    "start": "1250160",
    "end": "1255679"
  },
  {
    "text": "logs that tell us the latency for the request at each deployment as it flowed through the system um now something interesting that we",
    "start": "1255679",
    "end": "1261740"
  },
  {
    "text": "would have seen in those logs is that the request spent a lot more time on the reader deployment than it did on the solver deployment uh this makes a lot of",
    "start": "1261740",
    "end": "1268280"
  },
  {
    "text": "sense because generally speaking uh extracting text from an image is a much",
    "start": "1268280",
    "end": "1273620"
  },
  {
    "text": "more lengthy process than actually solving the algebra expression and this is also interesting as someone who's",
    "start": "1273620",
    "end": "1278840"
  },
  {
    "text": "actually managing the application because what that tells me is that the reader is the bottleneck of the",
    "start": "1278840",
    "end": "1283880"
  },
  {
    "text": "application rather than the solver and because of that as serve Auto scales the number of replicas and nodes in the system up or down I'd expect to serve to",
    "start": "1283880",
    "end": "1290720"
  },
  {
    "text": "create more problem reader replicas than Problem Solver replicas in order to best utilize resources and serve our users",
    "start": "1290720",
    "end": "1296539"
  },
  {
    "text": "the best so now let's jump over to some of the observability tools we have for surf to",
    "start": "1296539",
    "end": "1302120"
  },
  {
    "text": "take a look at how the application looks like from a higher level all right so I'm going to go back to the",
    "start": "1302120",
    "end": "1307220"
  },
  {
    "text": "any scale console and I'm going to go to the ray dashboard",
    "start": "1307220",
    "end": "1311919"
  },
  {
    "text": "the ray dashboard contains some useful information for array cluster and this year we've added a new Serv tab that",
    "start": "1312380",
    "end": "1318679"
  },
  {
    "text": "contains observability specifically for racer at the top level here we can see obser observability information for race serve",
    "start": "1318679",
    "end": "1325820"
  },
  {
    "text": "at a high level for example the healthiness of important serve actors like the serve controller or the serve proxies as well as the application",
    "start": "1325820",
    "end": "1331820"
  },
  {
    "text": "status we can also see logs in our application that gives us important information for",
    "start": "1331820",
    "end": "1338120"
  },
  {
    "text": "what serve is doing at any given point in time and at the bottom we can see metrics",
    "start": "1338120",
    "end": "1343880"
  },
  {
    "text": "that give us high level information about what's happening inside of our serve application on the left hand side here we see a metric for the QPS per",
    "start": "1343880",
    "end": "1350659"
  },
  {
    "text": "application and we can see that this has been steadily growing this makes a lot of sense because we've just sent a quick",
    "start": "1350659",
    "end": "1355700"
  },
  {
    "text": "burst of traffic and that traffic is continuing to be sent to that serve application and we're seeing that reflected here in this metric",
    "start": "1355700",
    "end": "1362120"
  },
  {
    "text": "we also have any errors that are showing up in our application we see no data because no errors are happening our",
    "start": "1362120",
    "end": "1367820"
  },
  {
    "text": "application is successfully serving traffic we can also take a look at the P90 latency for our application",
    "start": "1367820",
    "end": "1373340"
  },
  {
    "text": "now serve also emits additional metrics on top of this and if we want to take a look at those we can click view in",
    "start": "1373340",
    "end": "1379220"
  },
  {
    "text": "grafana grafana gives us useful information for",
    "start": "1379220",
    "end": "1384860"
  },
  {
    "text": "any metrics that serve as emitting it gives us dashboards that visualize those metrics and help us understand what's happening in the application",
    "start": "1384860",
    "end": "1391520"
  },
  {
    "text": "we can see some of the same metrics that we saw in the last tab like the QPS per application as well as the errors",
    "start": "1391520",
    "end": "1397580"
  },
  {
    "text": "and we can also see additional information like the number of replicas for each deployment like I mentioned earlier generally extracting text from",
    "start": "1397580",
    "end": "1404720"
  },
  {
    "text": "an image is going to take longer than actually solving an algebra problem so we expect to see the uh serve the serve",
    "start": "1404720",
    "end": "1410360"
  },
  {
    "text": "application scale up the number of reader replicas versus the number of solver replicas we can see that when we",
    "start": "1410360",
    "end": "1415400"
  },
  {
    "text": "started the demo we had one replica for each problem reader deployment and one replica for the problem solver deployment but over time we see that the",
    "start": "1415400",
    "end": "1422600"
  },
  {
    "text": "problem reader scaled up to six replicas while the problem solver stayed at one replica this lets serve process more requests in",
    "start": "1422600",
    "end": "1429799"
  },
  {
    "text": "parallel during the reading step and have and save uh save cost on the CPUs that are used for the problem solver",
    "start": "1429799",
    "end": "1435679"
  },
  {
    "text": "deployment by solving it using only one replica we can also see at the bottom here the",
    "start": "1435679",
    "end": "1441380"
  },
  {
    "text": "number of nodes in the application we can see that we started with only about three nodes but then we Auto scaled up",
    "start": "1441380",
    "end": "1446960"
  },
  {
    "text": "the number of nodes in order to provide more resources for the new problem reader replicas now to step back for a second",
    "start": "1446960",
    "end": "1453320"
  },
  {
    "text": "observability is a really important component of any production grade system like racer but another important feature",
    "start": "1453320",
    "end": "1459380"
  },
  {
    "text": "for production systems especially this year when Hardware is becoming more and more scarce and more and more expensive",
    "start": "1459380",
    "end": "1464659"
  },
  {
    "text": "is cost effectiveness So to that end we've introduced spot instant support to racer you can run",
    "start": "1464659",
    "end": "1470360"
  },
  {
    "text": "your serve applications on spot instances and as Ed mentioned earlier in the talk uh any the racer will be able",
    "start": "1470360",
    "end": "1476600"
  },
  {
    "text": "to adapt to any preemptions for those spot instances and shift traffic so that there are no failures it'll be able to",
    "start": "1476600",
    "end": "1482780"
  },
  {
    "text": "shift traffic onto on-demand nodes and when spot nodes become available again it'll be able to shift traffic back to spawn instances to continue saving cost",
    "start": "1482780",
    "end": "1489679"
  },
  {
    "text": "so let's take a look at that in action all right so we're back on the",
    "start": "1489679",
    "end": "1495200"
  },
  {
    "text": "photographic calculator page I'm going to navigate to the cluster that's running our serve application",
    "start": "1495200",
    "end": "1501940"
  },
  {
    "text": "all right so we're still in the any scale platform but now we're on the page for the cluster that's running the serve application this cluster gives us access",
    "start": "1506720",
    "end": "1513559"
  },
  {
    "text": "to a terminal that lets me run commands on the head node of array cluster now for demo purposes it's a little hard to",
    "start": "1513559",
    "end": "1519140"
  },
  {
    "text": "guarantee that we're going to have a spot instance at exactly this moment so what I've done is write a python script that's going to simulate what happens",
    "start": "1519140",
    "end": "1524960"
  },
  {
    "text": "when a spot instance gets preempted it's going to send a signal to our serve application and that signal is going to",
    "start": "1524960",
    "end": "1531080"
  },
  {
    "text": "tell the serve application that a particular node is about to be preempted and give it an opportunity to shift traffic so that no failures occur",
    "start": "1531080",
    "end": "1537559"
  },
  {
    "text": "let's go ahead and set the script up go back to the dashboard and grab a node that we want to preempt",
    "start": "1537559",
    "end": "1546140"
  },
  {
    "text": "let's take this node it looks like this node is running one of our problem reader deployment replicas and a proxy",
    "start": "1546140",
    "end": "1551960"
  },
  {
    "text": "actor I'm going to copy the node ID go back to the cluster",
    "start": "1551960",
    "end": "1558440"
  },
  {
    "text": "paste it in and we're going to preempt that node it looks like the command is run let's",
    "start": "1558440",
    "end": "1563659"
  },
  {
    "text": "go back to the serve tab on our Ray dashboard we can see that all of the proxies are no longer healthy one of the proxies has",
    "start": "1563659",
    "end": "1570440"
  },
  {
    "text": "entered the draining phase draining is a status that indicates that that proxy is no longer accepting new traffic it's",
    "start": "1570440",
    "end": "1576200"
  },
  {
    "text": "just finishing up any requests that are still in progress because when spawn instances are spawn instances are preempted you get a two",
    "start": "1576200",
    "end": "1582200"
  },
  {
    "text": "minute grace period before the instance is actually terminated this gives the proxy enough time to finish serving all",
    "start": "1582200",
    "end": "1587720"
  },
  {
    "text": "of its requests before the traffic needs to get shifted over to other nodes let's go back to the load test tool that",
    "start": "1587720",
    "end": "1594500"
  },
  {
    "text": "we started at the beginning of this demo and see if see how the users would uh see how see what the users would see",
    "start": "1594500",
    "end": "1600860"
  },
  {
    "text": "when the spot instance gets terminated I'm going to go back to that graph to the table that shows the number of",
    "start": "1600860",
    "end": "1606559"
  },
  {
    "text": "requests and the failures that we're seeing we can see that we're still not seeing any failures and the requests are",
    "start": "1606559",
    "end": "1612380"
  },
  {
    "text": "continuing to be sent to the serve application so in other words no users are seeing any failures when the spot instance got terminated but requests are",
    "start": "1612380",
    "end": "1618799"
  },
  {
    "text": "continuing to be served if we go back to the graph on the right hand side we can see that just like we",
    "start": "1618799",
    "end": "1624799"
  },
  {
    "text": "saw on the chart there are no failures but there's going to be a temporary dip in throughput until we've spun up a new",
    "start": "1624799",
    "end": "1630260"
  },
  {
    "text": "node in order to continue to serve requests I'd also like to point out one other pattern on this graph on the left hand",
    "start": "1630260",
    "end": "1636740"
  },
  {
    "text": "side we can see that initially we had a throughput of only about six requests per second but we see that throughput has steadily gone up to around 22 to",
    "start": "1636740",
    "end": "1644480"
  },
  {
    "text": "maybe 26 requests per second this is because serve is auto scaling in the beginning of the demo we only had about",
    "start": "1644480",
    "end": "1650600"
  },
  {
    "text": "three nodes that were serving requests but as we saw in the observability we got more nodes and more replicas to",
    "start": "1650600",
    "end": "1655640"
  },
  {
    "text": "serve requests which increase the throughput we can see the same pattern with the latency we see that the latency the 50th",
    "start": "1655640",
    "end": "1662539"
  },
  {
    "text": "percentile for the latency was about 750 milliseconds when we started the demo but once servato scaled that latency",
    "start": "1662539",
    "end": "1668360"
  },
  {
    "text": "went down to about 160 milliseconds so in other words serves Auto scaling was able to increase the capacity of",
    "start": "1668360",
    "end": "1674960"
  },
  {
    "text": "serve and be able to service more traffic and do it in a cost-effective manner so this brings us to the end of",
    "start": "1674960",
    "end": "1681799"
  },
  {
    "text": "our demo and just to recap what we covered we talked a little bit about the service",
    "start": "1681799",
    "end": "1687919"
  },
  {
    "text": "UI that the any scale platform provides for you to visualize the race serve application we went over some of the",
    "start": "1687919",
    "end": "1693140"
  },
  {
    "text": "serv's new observability capabilities that tells you exactly what's happening in the race serve application while it's in production",
    "start": "1693140",
    "end": "1699200"
  },
  {
    "text": "we also covered spawn instant support and how you can run your models and your server application on top of spot instances to save cost in production",
    "start": "1699200",
    "end": "1706460"
  },
  {
    "text": "and last we touched on we touched on serve's auto scaling Auto scaling lets serve create new replicas and new nodes",
    "start": "1706460",
    "end": "1712880"
  },
  {
    "text": "in order to service bursts of traffic without any failures overall we were glad to talk to we're",
    "start": "1712880",
    "end": "1721039"
  },
  {
    "text": "glad to answer any questions that you have about racer and we'd love to talk a",
    "start": "1721039",
    "end": "1726320"
  },
  {
    "text": "little bit more about how raceserve is GA and Ray 2.7 and we'd love for you all to get in touch with us on slack discuss",
    "start": "1726320",
    "end": "1732860"
  },
  {
    "text": "or GitHub so yeah Ed if you want to come on up we'd love to answer any questions looks like we have about 30 seconds to",
    "start": "1732860",
    "end": "1738740"
  },
  {
    "text": "do so yeah uniella",
    "start": "1738740",
    "end": "1743900"
  },
  {
    "text": "hi in the previous presentation we saw an example of how samsara was able to migrate their go python go framework",
    "start": "1744500",
    "end": "1752120"
  },
  {
    "text": "into all python built on top of record and race serve but not all times that",
    "start": "1752120",
    "end": "1757760"
  },
  {
    "text": "the orgs is able to or can't afford to migrate the whole system into another language so in that case what would you",
    "start": "1757760",
    "end": "1765020"
  },
  {
    "text": "suggest be a best a better option or is there any roadmap in Ray to support",
    "start": "1765020",
    "end": "1771279"
  },
  {
    "text": "other languages is is my mic live can you hear me okay",
    "start": "1771279",
    "end": "1776720"
  },
  {
    "text": "um yeah so race serve actually does have experimental support for Java and we're looking at adding more languages in the",
    "start": "1776720",
    "end": "1783020"
  },
  {
    "text": "future but we've also seen users that have just like piecemeal migrated part",
    "start": "1783020",
    "end": "1788360"
  },
  {
    "text": "of their architecture to raceserve it doesn't have to be all or nothing for example in that case they could have taken just the python parts and put that",
    "start": "1788360",
    "end": "1795440"
  },
  {
    "text": "in a ray cluster so yeah we don't have support for every language in the world but those are two options",
    "start": "1795440",
    "end": "1803620"
  }
]