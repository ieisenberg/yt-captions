[
  {
    "text": "so thanks everybody my name is Simon welcome to the talk model composition with straight deployment graph I'm a",
    "start": "3440",
    "end": "10559"
  },
  {
    "text": "software engineer at any scale and I lead the reserve team and in particular what we're going to do today is going to",
    "start": "10559",
    "end": "16500"
  },
  {
    "text": "be we're gonna I'm gonna give you a slow introduction about what is reserved what problem do we solve Where Do We Stand",
    "start": "16500",
    "end": "22800"
  },
  {
    "text": "sort of in the model serving landscape and then I'll talk a little bit about the context and motivation for us to",
    "start": "22800",
    "end": "28619"
  },
  {
    "text": "build a brand new API as well as go over a little bit of Deep dive into what the",
    "start": "28619",
    "end": "33840"
  },
  {
    "text": "API looks like in the new generation for us to optimize for the inference graph multi model serving scenario and in the",
    "start": "33840",
    "end": "41100"
  },
  {
    "text": "end I'm going to give you a preview of what's coming up with a reserve deployment graph cool let's start with",
    "start": "41100",
    "end": "47219"
  },
  {
    "text": "what is racer so Reserve is part of Ray AR runtime you have heard a lot about",
    "start": "47219",
    "end": "52260"
  },
  {
    "text": "today announcing the keynote so in particular Reserve is a components in our Ray AI runtimes That's responsible",
    "start": "52260",
    "end": "58680"
  },
  {
    "text": "for online inference and by online influence I mean this kind of once you have trained a model you want to put in",
    "start": "58680",
    "end": "64920"
  },
  {
    "text": "production for live interactive querying that where you have this kind of low latency High strooper kind of targets",
    "start": "64920",
    "end": "71640"
  },
  {
    "text": "you want the response to be delivered almost instantly so race serve in general is a framework",
    "start": "71640",
    "end": "78780"
  },
  {
    "text": "optimized for a flexible scalable and efficient online inference and by",
    "start": "78780",
    "end": "84420"
  },
  {
    "text": "building on top of a common ground Ray we have all the scalability benefits",
    "start": "84420",
    "end": "89640"
  },
  {
    "text": "also have the low latency RPC call inherited from Ray as well as the efficiency where you're going to see a",
    "start": "89640",
    "end": "95820"
  },
  {
    "text": "lot later where you can pack a lot of model into same set of Hardware to improve realization and save cost",
    "start": "95820",
    "end": "103079"
  },
  {
    "text": "and we actually have also have first class support for multimodo inference and by Mochi model influence I mean the",
    "start": "103079",
    "end": "110040"
  },
  {
    "text": "cases where you are forming a sequential pipeline where it's pre-processing model",
    "start": "110040",
    "end": "115140"
  },
  {
    "text": "logic handling and post-processing as well as broadcasting the input to multiple model to format Ensemble that's",
    "start": "115140",
    "end": "121740"
  },
  {
    "text": "what I mean by multimodal inference and then by building on top of array and",
    "start": "121740",
    "end": "127320"
  },
  {
    "text": "we're having this kind of native API you can write arbitrary python code and integrate arbitrary python libraries to",
    "start": "127320",
    "end": "135239"
  },
  {
    "text": "serve model and sort of provide you a perfect mix of writing business logic with your machine learning model",
    "start": "135239",
    "end": "141780"
  },
  {
    "text": "inference code and in particular is a deployment graph API I'm going to introduce today is going to be focused",
    "start": "141780",
    "end": "148080"
  },
  {
    "text": "on this kind of Mochi model inference support cool throughout this talk let's we're",
    "start": "148080",
    "end": "153239"
  },
  {
    "text": "going to be working through this example of a Content understanding simple contestantia a country understanding",
    "start": "153239",
    "end": "158459"
  },
  {
    "text": "application so let's say if we want to take an image as given an input image we",
    "start": "158459",
    "end": "163680"
  },
  {
    "text": "want to say put us through a machine learning application that will give us some combined feature outputs of say for",
    "start": "163680",
    "end": "170459"
  },
  {
    "text": "example what is a category what are the objects what are the bounding boxes that can give you some tags about this image",
    "start": "170459",
    "end": "177540"
  },
  {
    "text": "so a high level this might looks like a simple application but if you if we want",
    "start": "177540",
    "end": "183120"
  },
  {
    "text": "to really want to make sure to see what a simple app could look like in production it actually turns out to be",
    "start": "183120",
    "end": "188220"
  },
  {
    "text": "have a lot of moving pieces so the first piece we typically see is the image are",
    "start": "188220",
    "end": "193680"
  },
  {
    "text": "sometimes are like big enough you don't want to just send them around sometimes you might want to just put them in an",
    "start": "193680",
    "end": "199560"
  },
  {
    "text": "object store and retrieve it later so that's easy for download and like easy for fall tolerance so when image come in",
    "start": "199560",
    "end": "206760"
  },
  {
    "text": "we might need a downloader to retrieve a funnel object store and then after we get an image JPEG we typically sometimes",
    "start": "206760",
    "end": "213060"
  },
  {
    "text": "need a pre-processing logic to turn the jpeg into a tensor format that model our",
    "start": "213060",
    "end": "219180"
  },
  {
    "text": "Trend and understood and after the preprocessor we probably want to send it to a lot of image classifiers and the",
    "start": "219180",
    "end": "225900"
  },
  {
    "text": "reason being is typically you don't want just one image classifier for the engine application sometimes you want to",
    "start": "225900",
    "end": "232200"
  },
  {
    "text": "fine-tune each classifier for each category or sometimes you just want an ensemble of them to improve the output",
    "start": "232200",
    "end": "238760"
  },
  {
    "text": "and of course as I shown before maybe our application doesn't just want a",
    "start": "238760",
    "end": "243900"
  },
  {
    "text": "simple classification rather we also want some sort of image detection with bounding boxes and sort of pulling Cloud",
    "start": "243900",
    "end": "250140"
  },
  {
    "text": "that kind of outputs to help us tag the image furthermore so the pre-process image will also want to send it to a",
    "start": "250140",
    "end": "256859"
  },
  {
    "text": "different model and in the end combines it together for our final output so this example kind of my Sims can try but it's",
    "start": "256859",
    "end": "263280"
  },
  {
    "text": "our best attempt at capturing a lot of real machine learning end-to-end application when they're running",
    "start": "263280",
    "end": "269880"
  },
  {
    "text": "production what cases they might run into what they could look like and let's think about how would you build this",
    "start": "269880",
    "end": "276300"
  },
  {
    "text": "today right and as a preview if you just want to build this today there can be a",
    "start": "276300",
    "end": "281340"
  },
  {
    "text": "very simple solution you can just pack all the logic together and put them into a single container and this container",
    "start": "281340",
    "end": "288419"
  },
  {
    "text": "will essentially run the for loop around the if statement run all the logic in like a single sequence and then you can",
    "start": "288419",
    "end": "295199"
  },
  {
    "text": "use Python to write it and then you can pack all the application into a single container and scale that as a standalone",
    "start": "295199",
    "end": "300840"
  },
  {
    "text": "scalability units of course this approach is actually fairly easy to write and develop and operate however it",
    "start": "300840",
    "end": "308759"
  },
  {
    "text": "is unable to scale each module a model independently essentially each container here is doing the first thing and the",
    "start": "308759",
    "end": "315600"
  },
  {
    "text": "second scene and last thing like all together in sequence and additionally because of this architecture it has high",
    "start": "315600",
    "end": "322139"
  },
  {
    "text": "latency and as well as low stripper and is fairly costly because when it's",
    "start": "322139",
    "end": "327419"
  },
  {
    "text": "processing one thing in a container actually all other parts are idle and additionally you don't have all the",
    "start": "327419",
    "end": "332940"
  },
  {
    "text": "right cash efficiency C to help you to optimize the latency as well so a lot of people look at this it's",
    "start": "332940",
    "end": "339180"
  },
  {
    "text": "like well we can do better a better approach here is to split it each individual pieces into their own",
    "start": "339180",
    "end": "345479"
  },
  {
    "text": "component into their own container so you can imagine you can have any of you can write this kind of scenario where",
    "start": "345479",
    "end": "351539"
  },
  {
    "text": "you can put the attention for model in their own container Pi Touch model in their own container maybe you have",
    "start": "351539",
    "end": "356940"
  },
  {
    "text": "another web application fast API to orchestrate the request in like essentially the microservices style but",
    "start": "356940",
    "end": "363180"
  },
  {
    "text": "this approach of course requires a lot of care in operating and deploying them in production so so rest",
    "start": "363180",
    "end": "370860"
  },
  {
    "text": "white right here we're putting the kubernetes icon as well as a lot of yaml icons right here because we will have to",
    "start": "370860",
    "end": "376560"
  },
  {
    "text": "write all that and this approach gives you the advantage of scale each individual models independently using",
    "start": "376560",
    "end": "383340"
  },
  {
    "text": "different resource it's very easy to configure and typically can reach an optimal configuration however it does",
    "start": "383340",
    "end": "389100"
  },
  {
    "text": "require you and your team to have deep understanding of how to orchestrate and operate this kind of application on",
    "start": "389100",
    "end": "394979"
  },
  {
    "text": "kubernetes so Reserve is coming as a sort of intersection of these two approaches and",
    "start": "394979",
    "end": "402240"
  },
  {
    "text": "we are a framework built for motion model inference what kind of in between here in the way that if you look at",
    "start": "402240",
    "end": "408600"
  },
  {
    "text": "where racer fits you can put all the models in their own container but racer",
    "start": "408600",
    "end": "415080"
  },
  {
    "text": "help you to pack say both tensorflow and the pi torch model or it sounds a business logic in one node and since all",
    "start": "415080",
    "end": "423060"
  },
  {
    "text": "the other in other nodes or however you want to scale them and flexibly place them and just to compare it for this",
    "start": "423060",
    "end": "430380"
  },
  {
    "text": "like uh image if you compare this one with the previous one you can clearly see the efficiency again we're providing",
    "start": "430380",
    "end": "437280"
  },
  {
    "text": "here now let's take a look at how with the previous app to further enforce this",
    "start": "437280",
    "end": "442620"
  },
  {
    "text": "idea how would the previous app to be placed on reserve for example typically race serve can be run on top of",
    "start": "442620",
    "end": "449039"
  },
  {
    "text": "kubernetes on top of kubernetes there can be different racer pods essentially you can imagine this as a standalone",
    "start": "449039",
    "end": "454620"
  },
  {
    "text": "units of scaling and within racer pod we can maybe put the uh the downloader in",
    "start": "454620",
    "end": "460139"
  },
  {
    "text": "one of the Pod and the preprocessor on the other part to balance out or maybe",
    "start": "460139",
    "end": "465300"
  },
  {
    "text": "the first part can fit or the image image classifier well and then therefore",
    "start": "465300",
    "end": "470580"
  },
  {
    "text": "we put the other model into another party balance it out and in the end we can run combined logic somewhere and the",
    "start": "470580",
    "end": "477479"
  },
  {
    "text": "key here is the flexibility and efficiency as a placement for example if we say each pod has one GPU and four CPU",
    "start": "477479",
    "end": "485000"
  },
  {
    "text": "and we can actually we're actually assigning say each image classifier to only use 0.3 of GPU and in this case",
    "start": "485000",
    "end": "493380"
  },
  {
    "text": "racer will help you to only use one GPU for all the three models when you are",
    "start": "493380",
    "end": "499319"
  },
  {
    "text": "running in production so that's sort of a picture about what race serve is what problem are we",
    "start": "499319",
    "end": "505620"
  },
  {
    "text": "solving and where do we sort of stand in different possible solutions for multimodal inference now I'm going to",
    "start": "505620",
    "end": "510960"
  },
  {
    "text": "give you a little bit more about the context and motivation about the API so again take a look at this",
    "start": "510960",
    "end": "517680"
  },
  {
    "text": "architecture imagine how would you write it in code right so let me just quickly",
    "start": "517680",
    "end": "523560"
  },
  {
    "text": "go here so if you want to write in code it's a little bit tricky right we get the architecture right but API to",
    "start": "523560",
    "end": "531060"
  },
  {
    "text": "compose everything together is going to be a little it requires a lot of under deep understanding and expressiveness so",
    "start": "531060",
    "end": "539279"
  },
  {
    "text": "there are two before I dive into the API there's actually a lot of different trade-off you want to think about here do we want a decorative API or do we",
    "start": "539279",
    "end": "547680"
  },
  {
    "text": "want an imperative API so declarative API for those uh who don't know is essentially like some sort of apis that",
    "start": "547680",
    "end": "554220"
  },
  {
    "text": "provide you what's the desired end state or go state will look like so uh example",
    "start": "554220",
    "end": "559260"
  },
  {
    "text": "of declare the API are like SQL essentially what you're describing what the output data do you want it to look",
    "start": "559260",
    "end": "564839"
  },
  {
    "text": "like but you are now writing the for Loop the if statement the threading or other things to make to make to make it",
    "start": "564839",
    "end": "571680"
  },
  {
    "text": "happen so typically when we're describing the decorative API for for this kind of use case we find the",
    "start": "571680",
    "end": "578040"
  },
  {
    "text": "creative API is very good for production because it's very clear to see what's going to happen actually run in the",
    "start": "578040",
    "end": "584339"
  },
  {
    "text": "final architecture but then also there's a but the downside is is sometimes not",
    "start": "584339",
    "end": "589500"
  },
  {
    "text": "very flexible it's hard to express what you want to do with a sequence of decorative API and yaml",
    "start": "589500",
    "end": "596100"
  },
  {
    "text": "on the other side there's imperative API it's just like a programming language where you can provide step to step",
    "start": "596100",
    "end": "602399"
  },
  {
    "text": "instruction to show how to reach the angle like python Java any programming",
    "start": "602399",
    "end": "608100"
  },
  {
    "text": "language is imperative as in you can tell us exactly what to do so they are",
    "start": "608100",
    "end": "613620"
  },
  {
    "text": "easy to use but a lot of cases are hard to program because they're so flexible and a lot of cases they're aeroprone so",
    "start": "613620",
    "end": "620040"
  },
  {
    "text": "judging between these two we can take a look at some of the clear examples we don't need to read the code here so",
    "start": "620040",
    "end": "625620"
  },
  {
    "text": "sagemaker has this kind of API that's uh give you sort of like the trainer",
    "start": "625620",
    "end": "630899"
  },
  {
    "text": "interface the inference endpoint interface and then you can just orchestrate them in user python DSL so",
    "start": "630899",
    "end": "637500"
  },
  {
    "text": "sagemaker is decorative there's also a cases called there's a framework called",
    "start": "637500",
    "end": "642779"
  },
  {
    "text": "k-serv which allows you to write yaml configuration format in like kind of a",
    "start": "642779",
    "end": "647820"
  },
  {
    "text": "kubernetes native Style again right here if you want to compose different models together you're just writing like",
    "start": "647820",
    "end": "653339"
  },
  {
    "text": "configuration lines uh in the sort of in the yaml format in zero DSL and DSL",
    "start": "653339",
    "end": "659940"
  },
  {
    "text": "stands for domain specific language and in this case again you can see it is very declarative on the other hand a",
    "start": "659940",
    "end": "667440"
  },
  {
    "text": "great framework for model serving typically is like Ben what we found is Bento ml so you might have heard of",
    "start": "667440",
    "end": "673560"
  },
  {
    "text": "intro ml ventral MLS model compensation API is very flexible and very powerful you can see essentially you are defining",
    "start": "673560",
    "end": "680760"
  },
  {
    "text": "your python logic in there using code on orchestrate the computation and communication yourself so we can say",
    "start": "680760",
    "end": "687420"
  },
  {
    "text": "that ventral ML on the other hand is on the imperative land so and let me just",
    "start": "687420",
    "end": "693240"
  },
  {
    "text": "put them in the sort of map right here we have declarative imperative and then we know Sage maker ventral ML and where",
    "start": "693240",
    "end": "699180"
  },
  {
    "text": "does other scene stands right race serve 1.0 which is the version that",
    "start": "699180",
    "end": "705600"
  },
  {
    "text": "will have been out there for the past two years are actually on the imperative side we are allowing you to have the",
    "start": "705600",
    "end": "711779"
  },
  {
    "text": "flexibility of composing different models together using python code however this comes with several",
    "start": "711779",
    "end": "717839"
  },
  {
    "text": "challenges but before before I dive in let's do a quick review of The Reserve concept for those who are not familiar",
    "start": "717839",
    "end": "723540"
  },
  {
    "text": "with research so in reserve our core concept is called deployment a deployment essentially is a",
    "start": "723540",
    "end": "729540"
  },
  {
    "text": "collection of what we call replica where each replica is a process of instantiation of your model and in this",
    "start": "729540",
    "end": "737160"
  },
  {
    "text": "case you can if you know Ray essentially each replica is also a reactor that can communicate with each other",
    "start": "737160",
    "end": "743399"
  },
  {
    "text": "so as you can see right here we're instantiating a model class decorated with serve that deployment turning the",
    "start": "743399",
    "end": "750899"
  },
  {
    "text": "model into a service deployments which can now be scaled into different different numbers of replicas so",
    "start": "750899",
    "end": "757920"
  },
  {
    "text": "and the other Concept in race serve is called serve handle serve handle essentially is a client that allows you",
    "start": "757920",
    "end": "763560"
  },
  {
    "text": "to call deployment in Python so as I mentioned before we have deployments that have replica serve handle is like",
    "start": "763560",
    "end": "770339"
  },
  {
    "text": "an object that lets you basically getting the input and send it to another",
    "start": "770339",
    "end": "775800"
  },
  {
    "text": "model get the output back and send it to yet another model to perform this kind of composition or calling them in Python",
    "start": "775800",
    "end": "782880"
  },
  {
    "text": "essentially it's a client-side load balancer that will direct the request and load balancing across different",
    "start": "782880",
    "end": "788339"
  },
  {
    "text": "replicas and the important thing here is where's deployment and waste serve handle you can build arbitrary complex",
    "start": "788339",
    "end": "795480"
  },
  {
    "text": "application for example right here you can have orchestrator that called the first model and the second model or",
    "start": "795480",
    "end": "801180"
  },
  {
    "text": "using these two simple API so uh right here I'm trying to walk",
    "start": "801180",
    "end": "806220"
  },
  {
    "text": "through the uh how would you actually implement it in one dial API essentially",
    "start": "806220",
    "end": "811800"
  },
  {
    "text": "uh we can have Oxford deployment that will get that serve handle to the first model and second model and in a block",
    "start": "811800",
    "end": "818639"
  },
  {
    "text": "essentially you will be able to uh perform this kind of chaining operation using a handle.remel API or a Ensemble",
    "start": "818639",
    "end": "826560"
  },
  {
    "text": "operation using handle that remotes API as well so it's infinitely flexible it's",
    "start": "826560",
    "end": "831779"
  },
  {
    "text": "very imperative now it does come with a lot of challenges because as I mentioned as you",
    "start": "831779",
    "end": "837959"
  },
  {
    "text": "might have seen before because we're creating the serve handle within using our python code in our init function so",
    "start": "837959",
    "end": "846060"
  },
  {
    "text": "serve doesn't know which deployment to start first and how do we manage the dependency between the two there's also",
    "start": "846060",
    "end": "852360"
  },
  {
    "text": "implicit data flow across deployments in this case it's hard to read and debug and understand what's going on because",
    "start": "852360",
    "end": "858839"
  },
  {
    "text": "everything is reaching code in this imperative API and the complexity will just grow as your application is getting",
    "start": "858839",
    "end": "865320"
  },
  {
    "text": "more and more complex and you start adding more and more stages and lastly the the core of the problem",
    "start": "865320",
    "end": "872220"
  },
  {
    "text": "here is that we have a no high level graph obstruction we haven't been able to tell you well was the inference call",
    "start": "872220",
    "end": "878579"
  },
  {
    "text": "graph actually looks like what components that they have because everything just serve handle and",
    "start": "878579",
    "end": "883980"
  },
  {
    "text": "deployments to us right so we cannot deploy these as a unit now so these are all challenges come",
    "start": "883980",
    "end": "891240"
  },
  {
    "text": "from a purely imperative API that's why in racer 2.0 we want to make sure to",
    "start": "891240",
    "end": "896699"
  },
  {
    "text": "build an API that's essentially the best of both world that essentially gives you the expressiveness of imperative API as",
    "start": "896699",
    "end": "904980"
  },
  {
    "text": "well as give you sort of that good for production side of the declarative side so let's take a look at exactly what is",
    "start": "904980",
    "end": "912660"
  },
  {
    "text": "a content of so what again just to recall we want to",
    "start": "912660",
    "end": "917959"
  },
  {
    "text": "involve evolve our imperative API to the creative side and we have two main problems to solve basically implicit",
    "start": "917959",
    "end": "924240"
  },
  {
    "text": "dependencies and implicit data flow to solve the implicit dependency and data flow we have introduced a new API and",
    "start": "924240",
    "end": "930240"
  },
  {
    "text": "deployment called bind bind essentially you can be caught it on the deployment class Constructor as well as as well as",
    "start": "930240",
    "end": "938100"
  },
  {
    "text": "calling on the deployment class method so when you call it on the Constructor you are using it to define the",
    "start": "938100",
    "end": "943800"
  },
  {
    "text": "dependency among deployments I'm going to show you more example later and when you call a message you are kind of",
    "start": "943800",
    "end": "949500"
  },
  {
    "text": "declaring the data for dependency of what would it look like so on the right I'm showing an example of essentially",
    "start": "949500",
    "end": "956279"
  },
  {
    "text": "the class deployment a which you can call the bind on the a itself as well as",
    "start": "956279",
    "end": "961320"
  },
  {
    "text": "answer sort of the predict method itself so these will be the true main concept",
    "start": "961320",
    "end": "967079"
  },
  {
    "text": "where the single API help us to build end-to-end deployment graph so let's take a look at bind and Constructor",
    "start": "967079",
    "end": "973920"
  },
  {
    "text": "bind and Constructor help you declare dependency among the deployments in particular will inject the serve handle",
    "start": "973920",
    "end": "980339"
  },
  {
    "text": "at runtime so take a look at this example where the orchestrator depends on Model A a deployment a and deployment",
    "start": "980339",
    "end": "986940"
  },
  {
    "text": "B in our API you can just specify orchestrator.bind a DOT bind and B dot",
    "start": "986940",
    "end": "993420"
  },
  {
    "text": "bind in this case in our actual init function of the orchestrator we're not",
    "start": "993420",
    "end": "998699"
  },
  {
    "text": "giving you the deployment of the name anymore it's not something else rather just a handle that you can directly use",
    "start": "998699",
    "end": "1004940"
  },
  {
    "text": "to call the models so the innate mesobind helps you to create this kind",
    "start": "1004940",
    "end": "1010339"
  },
  {
    "text": "of dependencies and tell and be very much easy to see and visible",
    "start": "1010339",
    "end": "1015440"
  },
  {
    "text": "bind out Mexico help you to make the deployment uh basically the data flow",
    "start": "1015440",
    "end": "1021339"
  },
  {
    "text": "explicit and in particular way of we're using a menu for you to declare the data",
    "start": "1021339",
    "end": "1027380"
  },
  {
    "text": "for what would it look like also to avoid this kind of async await uh messiness in your code so example here",
    "start": "1027380",
    "end": "1034400"
  },
  {
    "text": "is you can have model 1. by model 2.bound to create them and then decrease this using this input node context",
    "start": "1034400",
    "end": "1040760"
  },
  {
    "text": "manager to essentially build your graph like a decorative way like this",
    "start": "1040760",
    "end": "1046819"
  },
  {
    "text": "and inside you can surround the whole thing just like how would you run all the deployment together",
    "start": "1046819",
    "end": "1051980"
  },
  {
    "text": "and we're going to start showing you a sequence of example of building a deployment graph from ways this kind of",
    "start": "1051980",
    "end": "1058400"
  },
  {
    "text": "data flow API so the first one of course is chaining you can declare the models",
    "start": "1058400",
    "end": "1063559"
  },
  {
    "text": "model one and model two and then our graph are essentially three lines you declare the input node you pass the",
    "start": "1063559",
    "end": "1068720"
  },
  {
    "text": "input called m1.4.bine and then you pass the input to M2 called",
    "start": "1068720",
    "end": "1075220"
  },
  {
    "text": "m2.4.bine essentially each call declares the address and help us to understand",
    "start": "1075220",
    "end": "1080360"
  },
  {
    "text": "the data flow relationship and just like Channing Ensemble is not too different essentially the input is",
    "start": "1080360",
    "end": "1088039"
  },
  {
    "text": "getting passed in from the Imp from the internal context manager and getting to",
    "start": "1088039",
    "end": "1093500"
  },
  {
    "text": "pass into M1 M2 at the same time and once these get into the m1mg at the same",
    "start": "1093500",
    "end": "1099020"
  },
  {
    "text": "time we can pass the output to our average combined node that we essentially declare the last Edge in our",
    "start": "1099020",
    "end": "1104720"
  },
  {
    "text": "data flow graph and then of course with these two privileges you can very easily just",
    "start": "1104720",
    "end": "1109880"
  },
  {
    "text": "start building arbitrary graph together so right here is an example of getting our Ensemble together with our chaining",
    "start": "1109880",
    "end": "1117799"
  },
  {
    "text": "and then combine everything together at the end right and then this does scale to the",
    "start": "1117799",
    "end": "1123980"
  },
  {
    "text": "the cases of hundreds of models change together or Ensemble together or arbitrary complex graph",
    "start": "1123980",
    "end": "1129980"
  },
  {
    "text": "and then of course I'm just here showing you as an illustration example you can build a tree of models uh in very simply",
    "start": "1129980",
    "end": "1137900"
  },
  {
    "text": "just like uh four or five lines of code I'm not sure who actually do this since they're entering business app but just",
    "start": "1137900",
    "end": "1143900"
  },
  {
    "text": "here as a demonstration of you can use this API to build arbitrary graph that's",
    "start": "1143900",
    "end": "1149600"
  },
  {
    "text": "kind of uh it's a very expressive API and then but however we typically this",
    "start": "1149600",
    "end": "1155840"
  },
  {
    "text": "is sort of gave us a base case of what the data will look like but what happens if you have Dynamic selection in",
    "start": "1155840",
    "end": "1161720"
  },
  {
    "text": "particular called the dynamic dispatch node uh in in your use case so example right here is we want the input to come",
    "start": "1161720",
    "end": "1168320"
  },
  {
    "text": "in and then we want to make sure the input goes to model one or model 2 given",
    "start": "1168320",
    "end": "1173480"
  },
  {
    "text": "a random number or like given some random selection of course it can also be given some uh it can be used to",
    "start": "1173480",
    "end": "1179059"
  },
  {
    "text": "select based on the input attributes or the input categories or any other features like that so right here we're",
    "start": "1179059",
    "end": "1186200"
  },
  {
    "text": "not doing anything different all you need to do here is instead of declaring the data flow you can fall back to the",
    "start": "1186200",
    "end": "1191600"
  },
  {
    "text": "deployment unconstructor approach where you can start using the code to express the data per a data flow program where",
    "start": "1191600",
    "end": "1198740"
  },
  {
    "text": "for example right here we Implement a random selection deployment we take the other model as dependency and",
    "start": "1198740",
    "end": "1204740"
  },
  {
    "text": "Constructor and in the call method you can now start essentially choosing exactly what you do",
    "start": "1204740",
    "end": "1210980"
  },
  {
    "text": "so this is how we have this Dynamic selection mode where it allow you to",
    "start": "1210980",
    "end": "1216020"
  },
  {
    "text": "embed the imperative API call to as part of the declarative data flow",
    "start": "1216020",
    "end": "1222620"
  },
  {
    "text": "now this is really goes with the principle of Ray where we want to make sure the simple things are very simple",
    "start": "1222620",
    "end": "1228740"
  },
  {
    "text": "and we are making the complex scenes possible so you can start Simple Start easy and then go beyond that to make",
    "start": "1228740",
    "end": "1236660"
  },
  {
    "text": "your actual business application more and more complex so this is essentially the two API we",
    "start": "1236660",
    "end": "1242179"
  },
  {
    "text": "had in mind to help you to build the engine application with both the declarative font as well as the",
    "start": "1242179",
    "end": "1248000"
  },
  {
    "text": "imperative front and will help you to uh what what helps this API can help you to do that",
    "start": "1248000",
    "end": "1253820"
  },
  {
    "text": "so in the end let's Wrap This Together by taking taking a look at our content standing understanding application again",
    "start": "1253820",
    "end": "1260299"
  },
  {
    "text": "so where is the content and understanding architecture it's very simple that we just need to declare",
    "start": "1260299",
    "end": "1265640"
  },
  {
    "text": "every single component as your own deployment and then sort of start",
    "start": "1265640",
    "end": "1271160"
  },
  {
    "text": "calling uh binding them creating the Constructor right so for example our",
    "start": "1271160",
    "end": "1276380"
  },
  {
    "text": "Dynamic dispatch node depends on the classifier so we had this Dynamic dispatch.bind star classifier to inject",
    "start": "1276380",
    "end": "1284120"
  },
  {
    "text": "the dependency and then we can declare that declarative data flow was was input node program",
    "start": "1284120",
    "end": "1290539"
  },
  {
    "text": "where we pass the input into the downloader pass the input output from the downloader to the preprocessor pass",
    "start": "1290539",
    "end": "1296960"
  },
  {
    "text": "the output from the pre-processor to both the dynamic classifier dispatch as",
    "start": "1296960",
    "end": "1302120"
  },
  {
    "text": "well as the image detector at the same time sharing the same input and in the end getting the Bose output while piping",
    "start": "1302120",
    "end": "1309080"
  },
  {
    "text": "them to the combined node so this is really how you can build a kind of this kind of real world looking engine",
    "start": "1309080",
    "end": "1315919"
  },
  {
    "text": "machine learning application with its few lines of code and in the end we have this new API to",
    "start": "1315919",
    "end": "1321980"
  },
  {
    "text": "help you essentially run up run a application uh very easily a whole",
    "start": "1321980",
    "end": "1328220"
  },
  {
    "text": "graph application very easily because with a bind API we now have a concept by understanding of what a whole graph",
    "start": "1328220",
    "end": "1334340"
  },
  {
    "text": "looks like you can use Serve that run which is a brand new API we introduced to help you to essentially deploy",
    "start": "1334340",
    "end": "1340220"
  },
  {
    "text": "everything at the same time and start orchestrating the data flow and the DAC driver you see here essentially will",
    "start": "1340220",
    "end": "1345500"
  },
  {
    "text": "help YouTube most efficiently orchestrate the data FL with high through platin level latency",
    "start": "1345500",
    "end": "1351500"
  },
  {
    "text": "cool so that's the sort of the walkthrough of the API I understand there's a lot of code and a little bit a",
    "start": "1351500",
    "end": "1357740"
  },
  {
    "text": "preview of what's coming up next so on our roadmap we're gonna make sure that with the deploying graph we can have",
    "start": "1357740",
    "end": "1364100"
  },
  {
    "text": "much better Auto scaling and the reason I'm mentioning here is because we serve already supports really really good Auto",
    "start": "1364100",
    "end": "1370820"
  },
  {
    "text": "scaling based on the incoming request load as well as scaling to zero where the traffic when you your model is not",
    "start": "1370820",
    "end": "1377059"
  },
  {
    "text": "receiving any traffic for some duration of time what can bring we make sure to de-allocate the model properly and only",
    "start": "1377059",
    "end": "1383720"
  },
  {
    "text": "bring it back up when the model comes however this Auto scaling algorithm of individual Auto scaling in particular",
    "start": "1383720",
    "end": "1389659"
  },
  {
    "text": "can run into inefficiency or sort of uh unoptimized scenario when it's running",
    "start": "1389659",
    "end": "1394880"
  },
  {
    "text": "in a graph for example when requests come in we want to make sure it's Auto scaling both the node in the beginning",
    "start": "1394880",
    "end": "1401240"
  },
  {
    "text": "as well as informing the auto scaling for the notes that's going to be ready later on so we are going to be actively",
    "start": "1401240",
    "end": "1408140"
  },
  {
    "text": "working on that as well as making sure to add more graph Primitives to help you to write sequential node right Ensemble",
    "start": "1408140",
    "end": "1415760"
  },
  {
    "text": "combining node as well as if you squint at it this is exactly the perimeter we",
    "start": "1415760",
    "end": "1420919"
  },
  {
    "text": "need to write a b testing and perform live model inference a b testing and switching as a runtime",
    "start": "1420919",
    "end": "1427220"
  },
  {
    "text": "and additionally as you can see with this declarative API it can help us to do a lot of performance of optimization",
    "start": "1427220",
    "end": "1433760"
  },
  {
    "text": "under the hood for example if we want to combine preprocessor and the downloader together where will immediately avoid a",
    "start": "1433760",
    "end": "1441320"
  },
  {
    "text": "copy or we want to scale out since differently or code placings to optimize both the downloading Network bandwidth",
    "start": "1441320",
    "end": "1447919"
  },
  {
    "text": "as well as a resource CPU and memory utilization and uh we're also actually working on",
    "start": "1447919",
    "end": "1453860"
  },
  {
    "text": "visualizing the graph to help you understand what's happening in the application and this is really the next",
    "start": "1453860",
    "end": "1459020"
  },
  {
    "text": "step here to make sure that you can debug observe and what's going on with this kind of very complex messy pipeline",
    "start": "1459020",
    "end": "1467720"
  },
  {
    "text": "and lastly in the end we're more than a year and happy to talk to you about your suggestion on the sort of the multi",
    "start": "1467720",
    "end": "1474620"
  },
  {
    "text": "model inference use case and in conclusion this talk I covered the how the serve model composition",
    "start": "1474620",
    "end": "1480860"
  },
  {
    "text": "ability is really built to be a power in the most crucial part of your business",
    "start": "1480860",
    "end": "1486440"
  },
  {
    "text": "application and our server is uniquely positioned as this kind of framework to",
    "start": "1486440",
    "end": "1492020"
  },
  {
    "text": "do that kind of job as well as in the rate 2.0 our sort of API is trying to",
    "start": "1492020",
    "end": "1497780"
  },
  {
    "text": "make sure it is both has an imperative front as well as the clear the front to make sure that you have the right API to",
    "start": "1497780",
    "end": "1504559"
  },
  {
    "text": "build complex application and you can try it out today there's a documentation page it's a rate 2.0 has been released",
    "start": "1504559",
    "end": "1511400"
  },
  {
    "text": "so even if you just open a laptop right now you should be able to try it all right with that I'm happy to take any",
    "start": "1511400",
    "end": "1517820"
  },
  {
    "text": "questions thank you and uh Dimitri will be running the mic",
    "start": "1517820",
    "end": "1525020"
  },
  {
    "text": "so",
    "start": "1525020",
    "end": "1527620"
  },
  {
    "text": "hello yeah thank you yeah very exciting uh um development so I'm thinking um I know",
    "start": "1531799",
    "end": "1538220"
  },
  {
    "text": "this might be like a little outside of this but I noticed that when you you you can you have like dynamic graphs but I'm",
    "start": "1538220",
    "end": "1545299"
  },
  {
    "text": "wondering if let's say in the for Loop uh you kind of can control how many",
    "start": "1545299",
    "end": "1550940"
  },
  {
    "text": "nodes you create on the Fly it's like it depends maybe on another note is it is it Dynamic is it that's my dynamic as",
    "start": "1550940",
    "end": "1558320"
  },
  {
    "text": "that yeah exactly so there's two uh sort of a pro uh both are programmable right",
    "start": "1558320",
    "end": "1564740"
  },
  {
    "text": "like in the in the graph data for the correlation layer you can use for loop as I show in the tree example you can",
    "start": "1564740",
    "end": "1570860"
  },
  {
    "text": "use for Loop you can use if statement to dynamically sort of trace a data flow once and then deploy it in but also to",
    "start": "1570860",
    "end": "1579020"
  },
  {
    "text": "actually massage the actual data flow you can use a dynamic dispatch node which we will cover the rest of the",
    "start": "1579020",
    "end": "1584960"
  },
  {
    "text": "scenes that the current of data flow that doesn't cover and in there again you can use a for Loop and all other",
    "start": "1584960",
    "end": "1590179"
  },
  {
    "text": "things and essentially as a fallback to or other things that occur the data flow cannot do",
    "start": "1590179",
    "end": "1597278"
  },
  {
    "text": "uh I know you mentioned that uh for auto scaling there's like work to be done but",
    "start": "1603559",
    "end": "1608900"
  },
  {
    "text": "uh meanwhile is there tooling or like is there an API exposed to kind of look",
    "start": "1608900",
    "end": "1615440"
  },
  {
    "text": "into the data flow like for example there's an inference graph which is composed of multiple nodes and you know",
    "start": "1615440",
    "end": "1622480"
  },
  {
    "text": "there you need to as a developer you're trying to look for optimizations of you",
    "start": "1622480",
    "end": "1627500"
  },
  {
    "text": "know maybe there's like data backup in one part of the node and not in the other one",
    "start": "1627500",
    "end": "1632659"
  },
  {
    "text": "um yeah so uh this is a great question so essentially in Rey sir would provide various tooling for you to observe the",
    "start": "1632659",
    "end": "1638900"
  },
  {
    "text": "performance as well as observes the shape of your program so observing the",
    "start": "1638900",
    "end": "1644720"
  },
  {
    "text": "performance we have uh build chain Metra collection uh Prometheus metric Exposition as well as there's a way for",
    "start": "1644720",
    "end": "1652700"
  },
  {
    "text": "example you can Benchmark against your program on a chromatic cluster to to see what the uh how your application perform",
    "start": "1652700",
    "end": "1659179"
  },
  {
    "text": "and drill into the bottleneck as well as a way to essentially visualize the graph",
    "start": "1659179",
    "end": "1666020"
  },
  {
    "text": "right so we we said we're already working on visualization but like um we're already possible Auto scanning",
    "start": "1666020",
    "end": "1671900"
  },
  {
    "text": "and visualization there's already primitive support in raid 2.0 so you can already get started with that and we",
    "start": "1671900",
    "end": "1677600"
  },
  {
    "text": "want to make sure they experience the best class",
    "start": "1677600",
    "end": "1681340"
  },
  {
    "text": "uh any thoughts about coupling these kind of workflows back in the train really you can make a dynamic selection and say like hey you know I have my",
    "start": "1687020",
    "end": "1694580"
  },
  {
    "text": "control I have my future Futures now below control I want to kick off a retraining pipeline like stuff like that",
    "start": "1694580",
    "end": "1700159"
  },
  {
    "text": "so yeah so there's two parts one is that the uh Ray Serve by running on top of",
    "start": "1700159",
    "end": "1707059"
  },
  {
    "text": "Ray we actually have native integration and you can totally just kick off a training job from whizzing a racer",
    "start": "1707059",
    "end": "1713120"
  },
  {
    "text": "Pipeline and actually completely Loop that way but and yeah another thing is",
    "start": "1713120",
    "end": "1718460"
  },
  {
    "text": "this kind of the creative API is actually not just a reserve API it's",
    "start": "1718460",
    "end": "1723500"
  },
  {
    "text": "actually a really core layer API if you some of you if I've been to the workflow talk about the workflow engine it's",
    "start": "1723500",
    "end": "1730580"
  },
  {
    "text": "actually already we're using exactly the same backbone and we're most excited to bring that kind of this kind of",
    "start": "1730580",
    "end": "1736100"
  },
  {
    "text": "declarative construction API through more Library like train and data set and others",
    "start": "1736100",
    "end": "1742539"
  },
  {
    "text": "quick question on so in the example you presumably the end point you expose just like one to do the",
    "start": "1750740",
    "end": "1757220"
  },
  {
    "text": "whole thing what if I also wanted to expose a sub part of that graph like could I do that how would I do that like",
    "start": "1757220",
    "end": "1763580"
  },
  {
    "text": "so so that's also uh very much possible we have the perimeter essentially to support different entry points to the",
    "start": "1763580",
    "end": "1770899"
  },
  {
    "text": "graph and expose them under different routes as well as go beyonza where you",
    "start": "1770899",
    "end": "1776000"
  },
  {
    "text": "can bring different dag to form if you want to think about it essentially a different entry point just a matter of",
    "start": "1776000",
    "end": "1782240"
  },
  {
    "text": "bringing a bigger graph and creating more edges uh for for for for your",
    "start": "1782240",
    "end": "1787520"
  },
  {
    "text": "application so we definitely have support for that we'll have publish pattern for that and more than excited to work with everybody to see what you",
    "start": "1787520",
    "end": "1793880"
  },
  {
    "text": "build with it cool I think we're out of time for",
    "start": "1793880",
    "end": "1799460"
  },
  {
    "text": "questions um but we can continue uh the the discussion and the lobby and elsewhere",
    "start": "1799460",
    "end": "1804860"
  },
  {
    "text": "yeah thanks everyone thanks everybody [Applause]",
    "start": "1804860",
    "end": "1811119"
  }
]