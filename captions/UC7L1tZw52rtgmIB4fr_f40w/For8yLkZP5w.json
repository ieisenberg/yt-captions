[
  {
    "text": "hey everyone uh welcome to training large language models with Ray Jax and",
    "start": "6980",
    "end": "12059"
  },
  {
    "text": "tpovs uh I'm Sid shizjana and we both work at cohere so let's get started uh",
    "start": "12059",
    "end": "19080"
  },
  {
    "text": "if I seem a little nervous it's because my Docker build was failing and I'm reeling back from the trauma",
    "start": "19080",
    "end": "24180"
  },
  {
    "text": "uh so the overview is I'm just going to talk about what cohere is what we do and",
    "start": "24180",
    "end": "30599"
  },
  {
    "text": "then talk about our training framework which is called facts and and how Fox interfaces with Ray and our experience",
    "start": "30599",
    "end": "36480"
  },
  {
    "text": "using Gray maybe a little bit about the future work and we'll leave some time for questions this is the last Talk of the day so I",
    "start": "36480",
    "end": "42780"
  },
  {
    "text": "apparently have like a one minute leeway so yeah about us uh cohero is a language AI",
    "start": "42780",
    "end": "48600"
  },
  {
    "text": "company our goal is to make Cutting Edge NLP accessible to every developer in the world so if you're thinking of an NLP",
    "start": "48600",
    "end": "56399"
  },
  {
    "text": "use case we won't go here to be the default NLP toolkit for you we want you",
    "start": "56399",
    "end": "61920"
  },
  {
    "text": "to go to the website we want you to try out our product and we basically aim to",
    "start": "61920",
    "end": "67080"
  },
  {
    "text": "be yeah the default and how we do it today is by training extremely large language models on a data set we've",
    "start": "67080",
    "end": "73380"
  },
  {
    "text": "curated with care I know there are a lot of concerns around uh scraping web data and all of that so we really put a lot",
    "start": "73380",
    "end": "79260"
  },
  {
    "text": "of thought and effort into filtering out data that is that that could be toxic and we really want our language models",
    "start": "79260",
    "end": "85320"
  },
  {
    "text": "to be representative of uh how we see the world or how we would like to see the world so between these large",
    "start": "85320",
    "end": "92159"
  },
  {
    "text": "language models and we build endpoints around these models that cater to specific use cases in the real world so",
    "start": "92159",
    "end": "99420"
  },
  {
    "text": "that's what we do and our core offering is a selection of generative and embedding models and",
    "start": "99420",
    "end": "105299"
  },
  {
    "text": "um by selection I mean that the variant size and if some of you are not familiar with scaling laws scaling laws basically",
    "start": "105299",
    "end": "111540"
  },
  {
    "text": "say that as you scale up in your model size your capabilities kind of seem to increase uh there are lots of caveats",
    "start": "111540",
    "end": "117960"
  },
  {
    "text": "around that but it seems to from our experience it seems to work decently well so we offer small medium",
    "start": "117960",
    "end": "126119"
  },
  {
    "text": "large generative and embedding models and for generative model we also serve an extra large model and uh yeah you can",
    "start": "126119",
    "end": "133860"
  },
  {
    "text": "try it out and see if you could use that for like any of your use cases and we also offer the",
    "start": "133860",
    "end": "139080"
  },
  {
    "text": "ability to fine-tune our core models which are like small medium large on your custom data set so if out of the",
    "start": "139080",
    "end": "146280"
  },
  {
    "text": "box few short performance is not good enough for you you can just kick off a fine tune and then we'll serve it for",
    "start": "146280",
    "end": "151920"
  },
  {
    "text": "you and it's very cheap I think we have the cheapest out there right now uh so yeah",
    "start": "151920",
    "end": "158099"
  },
  {
    "text": "our product as uh basically on the right side is a generative snapshot so all you",
    "start": "158099",
    "end": "164519"
  },
  {
    "text": "need to do if you're new to language models is that you just need to show the model a few examples of what you're",
    "start": "164519",
    "end": "170040"
  },
  {
    "text": "trying to do and here it's a movie review sentiment classifier so you give it a review and say oh this review is",
    "start": "170040",
    "end": "176040"
  },
  {
    "text": "positive and you give another one you say it's neutral you say another one say it's negative and ultimately what you",
    "start": "176040",
    "end": "181739"
  },
  {
    "text": "want to do is as soon as you receive a new review you want it to you want the language model to classify whether it's",
    "start": "181739",
    "end": "186959"
  },
  {
    "text": "positive negative or neutral and here like the darker shade of blue positive",
    "start": "186959",
    "end": "192120"
  },
  {
    "text": "is the generator token which means the language model is trying to tell you based on the context it has seen so far",
    "start": "192120",
    "end": "197580"
  },
  {
    "text": "that the review is positive and this is like out of the box performance and it seems to work pretty well based on our",
    "start": "197580",
    "end": "203159"
  },
  {
    "text": "internal benchmarks and a few real world data sets and on the left side there are two two rows there the top row is",
    "start": "203159",
    "end": "209580"
  },
  {
    "text": "basically a visualization of the embedding space so you can just upload a bunch of text to our product and it's",
    "start": "209580",
    "end": "214680"
  },
  {
    "text": "going to embed those uh all of those text data using the model of your choice",
    "start": "214680",
    "end": "220440"
  },
  {
    "text": "and it's going to embed them it's going to plot them on a 2d space and that's you can use",
    "start": "220440",
    "end": "225540"
  },
  {
    "text": "that however you want to judge how how close sentences are together auto drive to use that information to drive",
    "start": "225540",
    "end": "232519"
  },
  {
    "text": "decisions that and products that you want to build off of cohere",
    "start": "232519",
    "end": "238560"
  },
  {
    "text": "on the bottom left is basically our new endpoint relatively new it's called co-classify and we believe that the not",
    "start": "238560",
    "end": "247620"
  },
  {
    "text": "there aren't a lot of modalities of like text problems in the world and we want",
    "start": "247620",
    "end": "252659"
  },
  {
    "text": "to kind of like build endpoints that are specialized to these modalities and one of them is classification you can",
    "start": "252659",
    "end": "258419"
  },
  {
    "text": "specify you can frame many many text problems as like classification problems",
    "start": "258419",
    "end": "264540"
  },
  {
    "text": "and so without classify is perfect for that and you can basically upload a data",
    "start": "264540",
    "end": "270540"
  },
  {
    "text": "set and say label them on the on the OS which is our product and it's going to train a classifier for you you can it's",
    "start": "270540",
    "end": "277440"
  },
  {
    "text": "going to serve it so you can use it as an API so yeah uh that's an overview of the product and",
    "start": "277440",
    "end": "284340"
  },
  {
    "text": "our team's role at cohere it's basically we created and maintain a cohes proprietary training framework which is",
    "start": "284340",
    "end": "290520"
  },
  {
    "text": "called facts it's built with Ray Jax and tpuv force we used to be on TPU v3s now",
    "start": "290520",
    "end": "296639"
  },
  {
    "text": "we have switched over to TPU V force that pre-gender availability I think the v-force so they're exclusive to kind of",
    "start": "296639",
    "end": "301919"
  },
  {
    "text": "go here for now but uh yeah that's that's what fox uses to train large language models",
    "start": "301919",
    "end": "307860"
  },
  {
    "text": "and so what is Fox used for internally first one is our core model offering which is what I was mentioning earlier",
    "start": "307860",
    "end": "313740"
  },
  {
    "text": "which is generated when embedding modules of all sizes so facts Powers facts lies at the core at the center of",
    "start": "313740",
    "end": "320100"
  },
  {
    "text": "kohio's tech stack because everything is based around language models so yeah Fox Powers Baseline model free",
    "start": "320100",
    "end": "327060"
  },
  {
    "text": "training it's also about it also Powers customer fine tuning so if you go to OS Dot",
    "start": "327060",
    "end": "332220"
  },
  {
    "text": "cohere.ai and kick off a fine tune on your data set uh the models are actually trained on tpus and fax and we use drugs",
    "start": "332220",
    "end": "339120"
  },
  {
    "text": "so it's also where internal research experiments happen so all of uh cohero's",
    "start": "339120",
    "end": "346259"
  },
  {
    "text": "big focus is also advancing the state of NLP and not just training large language models and we also want to be efficient",
    "start": "346259",
    "end": "352979"
  },
  {
    "text": "we also want to be on the Forefront of discovering new capabilities and adding new capabilities to language models and",
    "start": "352979",
    "end": "358680"
  },
  {
    "text": "that's how we do it uh all of internal research happens at facts so a very quick dive into facts John I",
    "start": "358680",
    "end": "366300"
  },
  {
    "text": "will talk much more about how Fox uses astray and all of that but a very high level overview we have a technical paper",
    "start": "366300",
    "end": "372479"
  },
  {
    "text": "out it's an archive uh if the slide deck is shared later you can actually access",
    "start": "372479",
    "end": "377880"
  },
  {
    "text": "the link somewhere and but that's the title we wrote about it if you're interested please read it",
    "start": "377880",
    "end": "385259"
  },
  {
    "text": "yeah um the code of facts is Jax's speeget which is partitioned jit",
    "start": "385259",
    "end": "391319"
  },
  {
    "text": "and it relies on an spmd Paradigm for those who are like have a CS background or worked with computers a lot there's a",
    "start": "391319",
    "end": "397919"
  },
  {
    "text": "single program multiple data Paradigm so when we have multiple devices what Jax's",
    "start": "397919",
    "end": "403380"
  },
  {
    "text": "speech it enables you to do is it enables you to write code as if you're writing for a data parallel regime or",
    "start": "403380",
    "end": "408840"
  },
  {
    "text": "like as if you're writing code for a single device without knowledge of multi multiple host setup and you can write",
    "start": "408840",
    "end": "415380"
  },
  {
    "text": "this code you can wrap that function in a piece it and what it's going to do is perform automatic sharding inference",
    "start": "415380",
    "end": "421620"
  },
  {
    "text": "based on your input sharding specification and the output shotting specification and so PG does all of that",
    "start": "421620",
    "end": "428280"
  },
  {
    "text": "automatically so that's what's so great about it the mles don't need to worry about how sharding is happening as long",
    "start": "428280",
    "end": "433979"
  },
  {
    "text": "as they just specify how the input needs to be sharded and output needs to be sharded everything in between is automatic and powered by jux's speedrit",
    "start": "433979",
    "end": "440759"
  },
  {
    "text": "and juxta speech it also works with it requires a few things so with V4 pods",
    "start": "440759",
    "end": "447000"
  },
  {
    "text": "they're actually cubic topologies they're trips connected to each other in this 3D Taurus and",
    "start": "447000",
    "end": "453840"
  },
  {
    "text": "we basically reordered this physical device mesh into a logical mesh of two",
    "start": "453840",
    "end": "458940"
  },
  {
    "text": "Dimensions the First Dimension has a tensor parallel Dimension and the second dimension is a data parallel Dimension which is what you see in the grid next",
    "start": "458940",
    "end": "465599"
  },
  {
    "text": "to that and uh yeah we Define this user defined mesh",
    "start": "465599",
    "end": "471000"
  },
  {
    "text": "logical mesh representing tensor and data parallelism and we perform all of",
    "start": "471000",
    "end": "477240"
  },
  {
    "text": "the pjted operations within this context of this mesh that we have established so yeah that's what that's PJ",
    "start": "477240",
    "end": "484919"
  },
  {
    "text": "that are very uh I don't know but bird's eye view I guess yeah",
    "start": "484919",
    "end": "490740"
  },
  {
    "text": "and fox Hardware so a little bit about up TPU VMS and multi-host setups",
    "start": "490740",
    "end": "497580"
  },
  {
    "text": "okay um there are mult so TPU pods are basically groups of TPU hosts connected to each",
    "start": "497580",
    "end": "504000"
  },
  {
    "text": "other TPU host is basically just you can think of it as a GCE VM or something so it's really a VM and each VM is actually",
    "start": "504000",
    "end": "509699"
  },
  {
    "text": "as uh connected to four accelerator ships four accelerator chips forms the",
    "start": "509699",
    "end": "514979"
  },
  {
    "text": "fundamental unit of a TPU V4 which is a v48 so eight codes four chips and uh and",
    "start": "514979",
    "end": "521159"
  },
  {
    "text": "here you have four hosts because each four uh each host is connected to four trips you're actually looking at a v416",
    "start": "521159",
    "end": "526680"
  },
  {
    "text": "pod slice here uh I mean before 32 32 core 16 chips that's",
    "start": "526680",
    "end": "532080"
  },
  {
    "text": "a little confusing at times and a little bit about model parallelism as well just a background on the left",
    "start": "532080",
    "end": "540240"
  },
  {
    "text": "side you see pipeline parallelism which is traditionally how cohere used to train large language models before uh",
    "start": "540240",
    "end": "545459"
  },
  {
    "text": "TPU pods and like even GPU pods came into existence or like were really prevalent",
    "start": "545459",
    "end": "551120"
  },
  {
    "text": "so let's say you have three devices in three layers so on the left you see F0",
    "start": "551120",
    "end": "556260"
  },
  {
    "text": "F1 F2 which is basically forward propagation through layer one on device one layer two on device two and layer 3",
    "start": "556260",
    "end": "562019"
  },
  {
    "text": "on device three and so on you do the backward pass and so on so there's a communication after every device because",
    "start": "562019",
    "end": "568140"
  },
  {
    "text": "you need to communicate activations and gradients uh wherever applicable when",
    "start": "568140",
    "end": "573600"
  },
  {
    "text": "you're going from device to device and if they're not in high bandwidth interconnect these communications are",
    "start": "573600",
    "end": "579240"
  },
  {
    "text": "expensive and also you see that when the second device is processing its",
    "start": "579240",
    "end": "584399"
  },
  {
    "text": "activations the first device is Idle it's doing nothing and this is a classic problem in pipeline patelism so you",
    "start": "584399",
    "end": "590820"
  },
  {
    "text": "basically want to uh minimize the overhead of the bubble and the bubble represents the sum of all the idle times",
    "start": "590820",
    "end": "597420"
  },
  {
    "text": "of the devices so you want to minimize that there are techniques to minimize that but it's it's still not perfect",
    "start": "597420",
    "end": "602880"
  },
  {
    "text": "device utilization on the right side is uh what you see as spmd Paradigm all",
    "start": "602880",
    "end": "608760"
  },
  {
    "text": "layers exist on all devices the only difference is that those layers are sharded but that that basically means that after",
    "start": "608760",
    "end": "615300"
  },
  {
    "text": "every layer you have a communication overhead because you need to all gather those activations before you go to the",
    "start": "615300",
    "end": "620880"
  },
  {
    "text": "next layer and ah this traditionally if you do not have high bandwidth interconnect this is",
    "start": "620880",
    "end": "626399"
  },
  {
    "text": "a much worse idea compared to pipeline parallelism because you're introducing a communication bottleneck after every",
    "start": "626399",
    "end": "631680"
  },
  {
    "text": "layer but because of uh TPU pods or even GPU pods uh you have these high",
    "start": "631680",
    "end": "638160"
  },
  {
    "text": "bandwidth interconnects in tpus there ICI connects and uh this this communication is super super fast",
    "start": "638160",
    "end": "644399"
  },
  {
    "text": "actually and on the right side you basically see the step time for train batch size as a",
    "start": "644399",
    "end": "651060"
  },
  {
    "text": "function of train batch size so as you scale up you see that the slope of the tensor parallelism with",
    "start": "651060",
    "end": "658140"
  },
  {
    "text": "Jax and P jet is much lower which means it's much more scale friendly as opposed to pipeline",
    "start": "658140",
    "end": "664079"
  },
  {
    "text": "parallelism where your step time just goes up as you increase your token count yeah",
    "start": "664079",
    "end": "670260"
  },
  {
    "text": "and some numbers here on a V4 512 are at",
    "start": "670260",
    "end": "676019"
  },
  {
    "text": "about size of 256 which means one example per trip we can actually achieve a Max maximum",
    "start": "676019",
    "end": "681660"
  },
  {
    "text": "module size of 340 billion parameters and a step time of 6.21 seconds which is",
    "start": "681660",
    "end": "687140"
  },
  {
    "text": "honestly from where we were two years back it's absolutely insane what these",
    "start": "687140",
    "end": "692640"
  },
  {
    "text": "Port topologies can do and how fast they can train in large language models and the same for V4 128 which is 64 chips",
    "start": "692640",
    "end": "698820"
  },
  {
    "text": "you can train 84 86.6 billion parameter models without going out of memory and",
    "start": "698820",
    "end": "703860"
  },
  {
    "text": "it's a 1.5 to Second Step time and this relies on a few xla enhancements and",
    "start": "703860",
    "end": "709380"
  },
  {
    "text": "some compilation tricks but but those tricks don't work in a way that it's",
    "start": "709380",
    "end": "715260"
  },
  {
    "text": "practicality at like risk or anything they're just really smart compilation optimizations which enable us to train",
    "start": "715260",
    "end": "721560"
  },
  {
    "text": "these models at scale really fast yeah um I'll leave it to Joanna to talk about",
    "start": "721560",
    "end": "728640"
  },
  {
    "text": "Ray and facts cool um so let's dive in how we used rate in facts",
    "start": "728640",
    "end": "735300"
  },
  {
    "text": "um yeah so let me go over by the abstraction that we have in facts and where they reside so by Nature we need",
    "start": "735300",
    "end": "743399"
  },
  {
    "text": "to instantiate a cluster and we call our abstractions in two ways one is a",
    "start": "743399",
    "end": "749279"
  },
  {
    "text": "Megazord the other one's worker the name is pre-self explanatory in that megasword coordinates the the workers",
    "start": "749279",
    "end": "756120"
  },
  {
    "text": "together so let's talk about Vegas first so Megazord is the sort of like the head",
    "start": "756120",
    "end": "761220"
  },
  {
    "text": "of the training and it handles the data loading patched splitting and job",
    "start": "761220",
    "end": "766740"
  },
  {
    "text": "coordination across worker and we have a host VM aside from the TPU VMS that are",
    "start": "766740",
    "end": "773399"
  },
  {
    "text": "spot that spawn specifically for this and this is where the main train Loop lies so if you are you know familiar",
    "start": "773399",
    "end": "779700"
  },
  {
    "text": "with training machine learning model like this is like where you know while true train Loop all happens",
    "start": "779700",
    "end": "785060"
  },
  {
    "text": "and for worker on the other end worker resides in the TPU host VMS so as it",
    "start": "785060",
    "end": "792480"
  },
  {
    "text": "explained there are multiple TPU host VMS in a given TPU size and Ray remote",
    "start": "792480",
    "end": "799019"
  },
  {
    "text": "worker handles a processing of the job dispatch from the Megazord and it resides in the the individual TPU",
    "start": "799019",
    "end": "807120"
  },
  {
    "text": "VMS and since the TPU TPU chips are interconnected with the the faster interconnect",
    "start": "807120",
    "end": "813060"
  },
  {
    "text": "um this um this system of you know one uh host VM and the TPU VMS make up a",
    "start": "813060",
    "end": "819779"
  },
  {
    "text": "custom cluster for us and on the initialization",
    "start": "819779",
    "end": "826019"
  },
  {
    "text": "um um so let me start by clarifying the word Ray cluster I'll be saying this a",
    "start": "826019",
    "end": "831420"
  },
  {
    "text": "lot and this is going to be a bit different from the ray cluster that's in the dock because we view the system of a",
    "start": "831420",
    "end": "837360"
  },
  {
    "text": "host VM multiple tpvms as a cluster created by Ray but we do not use the ray",
    "start": "837360",
    "end": "842399"
  },
  {
    "text": "cluster created by Ray because our use cases are a bit different that I'll go over all later and this manual cluster",
    "start": "842399",
    "end": "850560"
  },
  {
    "text": "is instantiated by the ray headnote um instantiated in the Megazord and TPU",
    "start": "850560",
    "end": "857940"
  },
  {
    "text": "VMS connect manually to the Head node uh the the address the head node is created for and Megazords are during the",
    "start": "857940",
    "end": "865019"
  },
  {
    "text": "initialization Megazord sends the information such as the head no address and runtime environment so one thing I",
    "start": "865019",
    "end": "871079"
  },
  {
    "text": "really want to note about how like a how great the radius is",
    "start": "871079",
    "end": "876779"
  },
  {
    "text": "um that we send our fax code-based artifact through a runtime so it gets instantiated through reworking directory",
    "start": "876779",
    "end": "884279"
  },
  {
    "text": "and this really makes uh sure that fax remote code base and the head node code",
    "start": "884279",
    "end": "890220"
  },
  {
    "text": "base is in sync all the time and honestly I don't think I've ever had trouble um with this module really cool",
    "start": "890220",
    "end": "896279"
  },
  {
    "text": "and um so let's let me move on to the train steps so um this is like a you know any",
    "start": "896279",
    "end": "903480"
  },
  {
    "text": "kind of runtime setup that we have and a good example that I can give is a train",
    "start": "903480",
    "end": "908820"
  },
  {
    "text": "step so during a train step Ray had no spins of the batch it splits the the batch in the according to the TPU",
    "start": "908820",
    "end": "915480"
  },
  {
    "text": "topology and it feeds into the remote function worker train feeding the batch",
    "start": "915480",
    "end": "920579"
  },
  {
    "text": "needs to be synchronized um uh it doesn't have to be like precise but it kind of needs to be on this on",
    "start": "920579",
    "end": "926160"
  },
  {
    "text": "those even the same um around the same time to make sure that this is uh there's a low latency uh and this is",
    "start": "926160",
    "end": "932760"
  },
  {
    "text": "because of the spmd Paradigm that described earlier and then each worker fees is split a batch into the picture",
    "start": "932760",
    "end": "939779"
  },
  {
    "text": "train function which spans across uh globally across the TPU accelerators and",
    "start": "939779",
    "end": "945420"
  },
  {
    "text": "this picture function does the train train step uh in a distributed fashion",
    "start": "945420",
    "end": "950820"
  },
  {
    "text": "returns a loss and then the loss gets threaded to the the host VM so this all sounds really crazy and this sounds",
    "start": "950820",
    "end": "957240"
  },
  {
    "text": "really complicated but with Ray it is handled by one line the W uh in blue",
    "start": "957240",
    "end": "962639"
  },
  {
    "text": "this is just a gloss is equal to where you get worker train remote batch",
    "start": "962639",
    "end": "967800"
  },
  {
    "text": "um so it's um the it like really really uh lifts off the heavy lifting that we",
    "start": "967800",
    "end": "973199"
  },
  {
    "text": "do on the back end um so let's talk about how we use Ray",
    "start": "973199",
    "end": "979139"
  },
  {
    "text": "and what we require from right um so um for us we need to have we need to be",
    "start": "979139",
    "end": "985560"
  },
  {
    "text": "able to use a fixed set of identical workers instead of you know using Auto scaler or Ray cluster thereof and we",
    "start": "985560",
    "end": "992519"
  },
  {
    "text": "need to be able to synchronize the workers to do around the remote function um um in a timely manner and we need to be",
    "start": "992519",
    "end": "1000019"
  },
  {
    "text": "able to easily Implement remote function calls we will only showed you the training call but we have a lot of",
    "start": "1000019",
    "end": "1005420"
  },
  {
    "text": "different remote calls that we want to run with the TPU and all of those needs to be implemented through the remote",
    "start": "1005420",
    "end": "1011240"
  },
  {
    "text": "calls and of course we would want low latency instability we wouldn't want our cluster to die",
    "start": "1011240",
    "end": "1017360"
  },
  {
    "text": "um easily and we want to be able to schedule the train function on the on",
    "start": "1017360",
    "end": "1023060"
  },
  {
    "text": "the TPU when we call train on the host VM um the I already talked about this",
    "start": "1023060",
    "end": "1029178"
  },
  {
    "text": "briefly but we want to be able to do to set up the worker environments um easily the one alternative one we are",
    "start": "1029179",
    "end": "1038058"
  },
  {
    "text": "well we worse alternative for this is like you know saving the environment through SSH and setting the effects uh",
    "start": "1038059",
    "end": "1043938"
  },
  {
    "text": "effects directory with this and it you know Ray work uh Ray Ray runtime environment proved to be so much better",
    "start": "1043939",
    "end": "1052240"
  },
  {
    "text": "um let me briefly touch about the other use cases of Ray at cohere so we use Ray",
    "start": "1052760",
    "end": "1059960"
  },
  {
    "text": "Tunes obstructions for our parameter sweeps uh we use partial of it like the",
    "start": "1059960",
    "end": "1065299"
  },
  {
    "text": "the distribution functions and we um tune and we use the array cluster so",
    "start": "1065299",
    "end": "1071720"
  },
  {
    "text": "this is the arraycluster that um um that rate provides and we use this for for the products and finding",
    "start": "1071720",
    "end": "1077900"
  },
  {
    "text": "environment because with the pre-provisioned resources so already created tpus for example",
    "start": "1077900",
    "end": "1083720"
  },
  {
    "text": "um and this really helps us minimize the initialization overhead",
    "start": "1083720",
    "end": "1088480"
  },
  {
    "text": "perfect and so for like so far experience with radio",
    "start": "1092240",
    "end": "1097940"
  },
  {
    "text": "was like great I was really reliable it has never been the source of our runtime errors and um the ease of you know",
    "start": "1097940",
    "end": "1105679"
  },
  {
    "text": "implementing the the remote calls was uh was very easy and we sometimes forget like a lot of the code bases like based",
    "start": "1105679",
    "end": "1112940"
  },
  {
    "text": "on the remote call and Ray dashboard was very helpful so when we started building this out we",
    "start": "1112940",
    "end": "1118100"
  },
  {
    "text": "need to make sure that you know the nose are healthy and then the communication that we see uh between the head node and",
    "start": "1118100",
    "end": "1124700"
  },
  {
    "text": "worker needed to make sense and really dashboard played a big role and you know checking all of these",
    "start": "1124700",
    "end": "1132820"
  },
  {
    "text": "that's right and what's up okay awesome so Sid and I are going to",
    "start": "1133520",
    "end": "1138860"
  },
  {
    "text": "dive into the core examples um okay",
    "start": "1138860",
    "end": "1144260"
  },
  {
    "text": "uh yeah this is a very simplified example uh I'll start by giving a little bit of",
    "start": "1144260",
    "end": "1149840"
  },
  {
    "text": "context around this before I actually talk about the code because there's honestly not a lot of things going on",
    "start": "1149840",
    "end": "1155840"
  },
  {
    "text": "here um well yeah large language models are large so everything needs to be kind of",
    "start": "1155840",
    "end": "1163039"
  },
  {
    "text": "distributed and scale friendly and this is this could be a big ask in theory because",
    "start": "1163039",
    "end": "1168559"
  },
  {
    "text": "most of our mle team even Joanna and I don't have a systems background we have never worked with distributed systems",
    "start": "1168559",
    "end": "1174020"
  },
  {
    "text": "before actually dabbling with facts and the framework before that so",
    "start": "1174020",
    "end": "1179179"
  },
  {
    "text": "asking an mle team with mostly ml research experience and not a lot of systems experience to write distributed",
    "start": "1179179",
    "end": "1184220"
  },
  {
    "text": "programs is a big ask and but we don't want it to be a big ask we want the",
    "start": "1184220",
    "end": "1190160"
  },
  {
    "text": "barrier to entry to be as low as possible because to make fast progress iterations need to be fast and the",
    "start": "1190160",
    "end": "1196520"
  },
  {
    "text": "framework needs to be really friendly both for like production environments and research environments so that's a challenge we have",
    "start": "1196520",
    "end": "1203000"
  },
  {
    "text": "uh tried to like I don't know tackle so here if and Emily wanted to the",
    "start": "1203000",
    "end": "1210620"
  },
  {
    "text": "specific example if anybody wanted to like average checkpoints and if you're talking about like let's say a previous",
    "start": "1210620",
    "end": "1216919"
  },
  {
    "text": "example was a 340 billion parameter model on a v4512 so if you wanted average checkpoints uh stochastic weight",
    "start": "1216919",
    "end": "1223880"
  },
  {
    "text": "averaging is a thing uh in the field and it's been shown to like it's been shown",
    "start": "1223880",
    "end": "1229160"
  },
  {
    "text": "to improve performance so if someone wanted to implement that something like that the naive version of it is this uh",
    "start": "1229160",
    "end": "1235580"
  },
  {
    "text": "this code so let's walk through the code and see what's happening there and here's how an",
    "start": "1235580",
    "end": "1241160"
  },
  {
    "text": "Emily would do this they would just import a bunch of utils from fax one is basically just getting the config with",
    "start": "1241160",
    "end": "1247460"
  },
  {
    "text": "the batch size and the TPU name and all of that and you initialize three head node by just calling read.inet and then",
    "start": "1247460",
    "end": "1253220"
  },
  {
    "text": "you initialize your TPU which basically creates it and sets up the environment uh necessary and it also sets up the",
    "start": "1253220",
    "end": "1260240"
  },
  {
    "text": "rate cluster by taking the head information and the devices that are contained in the multi-host TPU pod",
    "start": "1260240",
    "end": "1265400"
  },
  {
    "text": "slice and then this is a one-liner function which is jax.numpy dot mean it",
    "start": "1265400",
    "end": "1271820"
  },
  {
    "text": "takes in a list of tensors stock substances takes the average of those tensors along the Stacked access that's",
    "start": "1271820",
    "end": "1278120"
  },
  {
    "text": "all it's doing and now the actual distributed part so all you need to all",
    "start": "1278120",
    "end": "1284660"
  },
  {
    "text": "we need to do is our checkpoints are all distributed we cannot expect 340 billion parameters to be in one NPC file or a",
    "start": "1284660",
    "end": "1292159"
  },
  {
    "text": "pickle file or in something it's going to break file systems so uh what we want is basically have",
    "start": "1292159",
    "end": "1299059"
  },
  {
    "text": "distribute these checkpoints into multiple shards and that's what I mean by short index there so if we split it",
    "start": "1299059",
    "end": "1305000"
  },
  {
    "text": "10 ways there are like 34 billion parameters per shot so let's say there",
    "start": "1305000",
    "end": "1310460"
  },
  {
    "text": "are 10 shards and short index could go from zero to nine and this average checkpoints from short function is",
    "start": "1310460",
    "end": "1316580"
  },
  {
    "text": "basically dealing one shot at a time because there are disjoint sets of parameters so you can just average each",
    "start": "1316580",
    "end": "1322940"
  },
  {
    "text": "chart and get away with it and the result is going to be an average of the entire parameter Pi tree",
    "start": "1322940",
    "end": "1328760"
  },
  {
    "text": "and like I said before everything needs to happen in a mesh context which is The Logical mesh I was referring to when",
    "start": "1328760",
    "end": "1334159"
  },
  {
    "text": "talking about pjt it takes a list of devices that is a logical device array and we name that as we attach these",
    "start": "1334159",
    "end": "1341480"
  },
  {
    "text": "Concepts tensor parallelism and data parallelism to it uh to piece it itself they mean nothing",
    "start": "1341480",
    "end": "1346880"
  },
  {
    "text": "they just they're just strings but the way we short things kind of represent tensor and data parallelism there and",
    "start": "1346880",
    "end": "1354679"
  },
  {
    "text": "the read shots function basically reads The Shard from all of these checkpoint paths that we want to update from so it",
    "start": "1354679",
    "end": "1361280"
  },
  {
    "text": "could just be a bunch of uh numpy's numpy reads or something and it's going to give you a list of",
    "start": "1361280",
    "end": "1367520"
  },
  {
    "text": "dictionaries and the great thing about it is jacks.3 multimap works with arbitrary containers and it's going to",
    "start": "1367520",
    "end": "1374600"
  },
  {
    "text": "automatically map average checkpoint function to all the leaves contained in the shards list of batteries which is",
    "start": "1374600",
    "end": "1382460"
  },
  {
    "text": "the shards thing here and once you get the average chart back that's what the jax.3 multi-map is going",
    "start": "1382460",
    "end": "1388940"
  },
  {
    "text": "to give you you're just going to write that chart out in whatever way we want so the result is that we end up with 10",
    "start": "1388940",
    "end": "1396440"
  },
  {
    "text": "shards of average checkpoints each and we can load that model and evaluate it and whatever we want to do with it and",
    "start": "1396440",
    "end": "1402380"
  },
  {
    "text": "the main function is basically just a bunch of remote calls based on how many hosts we have in the TPU bot slice and",
    "start": "1402380",
    "end": "1408559"
  },
  {
    "text": "array.get of that so that we actually don't exit the program early and uh",
    "start": "1408559",
    "end": "1414320"
  },
  {
    "text": "yeah so that's really that simple to write checkpoint operation code",
    "start": "1414320",
    "end": "1419720"
  },
  {
    "text": "yeah and one more thing is like like looking at this code it's kind of",
    "start": "1419720",
    "end": "1425120"
  },
  {
    "text": "like a really good experience for me because like the Consciousness that you get with Jack's Ray and tpus and PJ is",
    "start": "1425120",
    "end": "1434059"
  },
  {
    "text": "it's hard to overstate the powers of that because you can really write simple functions that that work with python",
    "start": "1434059",
    "end": "1440480"
  },
  {
    "text": "containers out of the box so well",
    "start": "1440480",
    "end": "1444520"
  },
  {
    "text": "cool um yes and let me just go over very hugely simplified version of our training",
    "start": "1446600",
    "end": "1453799"
  },
  {
    "text": "workflow and uh please keep in mind that this code will not run this is a very",
    "start": "1453799",
    "end": "1458900"
  },
  {
    "text": "stripped on code um so yeah so we introduced you know like the remote uh class worker and",
    "start": "1458900",
    "end": "1465559"
  },
  {
    "text": "there's like a main Loop that runs in the host PMW earlier explained so really",
    "start": "1465559",
    "end": "1472039"
  },
  {
    "text": "this is this is it for for us for for training the the um large language model so let me go",
    "start": "1472039",
    "end": "1478700"
  },
  {
    "text": "over each um like a functions that and what we what they do so on the worker side when you instantiate it you",
    "start": "1478700",
    "end": "1485120"
  },
  {
    "text": "instantiate the match that reshapes the the Jax devices or TPU devices in the",
    "start": "1485120",
    "end": "1491059"
  },
  {
    "text": "order that you want and for the Simplicity we are keeping the tensor parallelism as one and that we are using",
    "start": "1491059",
    "end": "1497480"
  },
  {
    "text": "data parallelism for the rest of them and for and given the mesh context we are going to build the model State and",
    "start": "1497480",
    "end": "1503720"
  },
  {
    "text": "train function so this model functions Constructor will be building fidget function and when you run train function",
    "start": "1503720",
    "end": "1510140"
  },
  {
    "text": "it's just as easy as you know training writing or training function which is just it just dreads down to the train",
    "start": "1510140",
    "end": "1516500"
  },
  {
    "text": "function that you have uh under the under the mesh context and on the main Loop so this is now running in host VM",
    "start": "1516500",
    "end": "1523940"
  },
  {
    "text": "um so let's see what's going on so when we when we initialize the TPU all it does is like we make sure the TP is",
    "start": "1523940",
    "end": "1529460"
  },
  {
    "text": "healthy and you know we get the host addresses of the TPU or VMS and we",
    "start": "1529460",
    "end": "1535279"
  },
  {
    "text": "instantiate a ray um restart rate and within the Megazord and",
    "start": "1535279",
    "end": "1541400"
  },
  {
    "text": "we get the way ahead info and we're going to set up the cluster and this functional it does is you know starting",
    "start": "1541400",
    "end": "1546860"
  },
  {
    "text": "way with the header that's given from the host VM and then we specify the",
    "start": "1546860",
    "end": "1552200"
  },
  {
    "text": "custom resource called TPU um and we make sure that we require one problem so you know I really like the",
    "start": "1552200",
    "end": "1559580"
  },
  {
    "text": "way we can set up the the custom resources and this is how we schedule uh and how we validate our cluster we need",
    "start": "1559580",
    "end": "1566960"
  },
  {
    "text": "to make sure worker is instantiated once and only once on each host BM of TPU so",
    "start": "1566960",
    "end": "1574760"
  },
  {
    "text": "this you know TPU equals one is the key that throws down and in the rate cluster and that's the one that guarantees us to",
    "start": "1574760",
    "end": "1581240"
  },
  {
    "text": "have one worker per TPU so yeah after you have that you instantiate the remote",
    "start": "1581240",
    "end": "1586460"
  },
  {
    "text": "workers and you know that's really just one liner of you know creating the worker of number of the hosts we have",
    "start": "1586460",
    "end": "1593960"
  },
  {
    "text": "right now and when you run the train uh when the train Loop this is uh where the the you",
    "start": "1593960",
    "end": "1600860"
  },
  {
    "text": "know satisfaction comes from um all of these heavy lifting or all of the all of the distributed you know",
    "start": "1600860",
    "end": "1606500"
  },
  {
    "text": "training setup is really uh like lifted off it's one liner like the one that I introduced earlier you just",
    "start": "1606500",
    "end": "1613520"
  },
  {
    "text": "you know you just uh feed down the the batch you gather the loss from it and",
    "start": "1613520",
    "end": "1618860"
  },
  {
    "text": "you just get uh run Ray get to make sure they are coordinated and yeah this is",
    "start": "1618860",
    "end": "1625279"
  },
  {
    "text": "the so I just um there's not much to see and I think that's the beauty of it like like the using Ray and and you know",
    "start": "1625279",
    "end": "1631940"
  },
  {
    "text": "Jackson TPU would just mean everything very simpler for us in terms of like designing the cluster and you know",
    "start": "1631940",
    "end": "1637760"
  },
  {
    "text": "training the distributed and large language models um yeah and with that being said",
    "start": "1637760",
    "end": "1644419"
  },
  {
    "text": "um thank you uh for Ray for you know hosting us and I and yeah please let me know if you have",
    "start": "1644419",
    "end": "1650360"
  },
  {
    "text": "any questions [Applause]",
    "start": "1650360",
    "end": "1659600"
  },
  {
    "text": "I was just curious so you mentioned like the heavy lifting is done by Ray but then like when you're talking about that",
    "start": "1659600",
    "end": "1665720"
  },
  {
    "text": "is like this architecture of like the tpus the workers and everything that you created is that like like in a specific",
    "start": "1665720",
    "end": "1672620"
  },
  {
    "text": "way so that the ray can do this heavy lifting or you can make a new architecture and Ray can handle that and",
    "start": "1672620",
    "end": "1677659"
  },
  {
    "text": "like uh also I was curious about the initialized TPU that you talked about like I think both the both of the",
    "start": "1677659",
    "end": "1684320"
  },
  {
    "text": "questions kind of relate to each other like is the TPU like local or is it also",
    "start": "1684320",
    "end": "1689480"
  },
  {
    "text": "like array TPU I'm confused about that so in the cloud or is it like coheres",
    "start": "1689480",
    "end": "1694580"
  },
  {
    "text": "TPU versus race TPU I'm not sure about that in the cluster",
    "start": "1694580",
    "end": "1699799"
  },
  {
    "text": "TPO is a resource managed by Google Cloud itself and we work with gcp VMS but they could work with your MacBook as",
    "start": "1699799",
    "end": "1706100"
  },
  {
    "text": "well as long as you connect to those tpus and tpus are basically for all we care or for all raycares tpus",
    "start": "1706100",
    "end": "1713240"
  },
  {
    "text": "are basically just a bunch of hosts that are connected to each other and the actual placement of the model and",
    "start": "1713240",
    "end": "1719840"
  },
  {
    "text": "training and architecture they're all handled by widget and so our usage of",
    "start": "1719840",
    "end": "1726320"
  },
  {
    "text": "Ray is kind of limited to establishing that cluster and orchestrating the placement of code and invoking those",
    "start": "1726320",
    "end": "1732500"
  },
  {
    "text": "remote calls and once once that kicks in then it's all Jax's speech it and our",
    "start": "1732500",
    "end": "1737539"
  },
  {
    "text": "model Constructors and like Fox utilities right the architecture description is still like made by like",
    "start": "1737539",
    "end": "1743840"
  },
  {
    "text": "it's not made keeping in mind that you're going to be using rain in the future it's just made and then rage just",
    "start": "1743840",
    "end": "1749900"
  },
  {
    "text": "adds on top of that I think um it could you could pretty",
    "start": "1749900",
    "end": "1755240"
  },
  {
    "text": "much generalize to any spmd multi-host setup I'm pretty sure like this any any",
    "start": "1755240",
    "end": "1760399"
  },
  {
    "text": "you know gpus for example that have like multiple like uh host VMS the ray could",
    "start": "1760399",
    "end": "1766520"
  },
  {
    "text": "definitely play a role in there um it doesn't have to be you know even like accelerators it could be it could",
    "start": "1766520",
    "end": "1771620"
  },
  {
    "text": "be a CPU like a job distributor across a CPU which is you know like a race definitely and traditional use case",
    "start": "1771620",
    "end": "1778940"
  },
  {
    "text": "um but yeah um the use of synchronizing the work and you know synchronizing the train",
    "start": "1778940",
    "end": "1784100"
  },
  {
    "text": "function for example really could could scale up to any kind of devices with a similar setup",
    "start": "1784100",
    "end": "1791620"
  },
  {
    "text": "hi can you show this slide about the uh the recall in the very last the very last slide yeah the very last line so",
    "start": "1797659",
    "end": "1804740"
  },
  {
    "text": "that's um we said we are passing train which is a one we are passing one batch to",
    "start": "1804740",
    "end": "1811159"
  },
  {
    "text": "different train function of a different workers right for all the workers as far as I understand from this talk I think",
    "start": "1811159",
    "end": "1817940"
  },
  {
    "text": "say uh 250 TPU chips each worker will be at several chips right so how does this",
    "start": "1817940",
    "end": "1824720"
  },
  {
    "text": "batch uh how does it work with like say one worker corresponds to four chips but I'm",
    "start": "1824720",
    "end": "1830120"
  },
  {
    "text": "passing a single batch to all workers really really great eyes uh I wanted to add like a you know like",
    "start": "1830120",
    "end": "1837620"
  },
  {
    "text": "a simple function that called split batch but it was just running too long we have like a custom in real life",
    "start": "1837620",
    "end": "1843740"
  },
  {
    "text": "you'll be zipping like a split about your cost of 20 like a 256 workers and",
    "start": "1843740",
    "end": "1849140"
  },
  {
    "text": "you'll be passing those part of the batches especially the one that I showed earlier is the Qatar apparel training so",
    "start": "1849140",
    "end": "1855140"
  },
  {
    "text": "you should be splitting the batch into 256 um sword batches and you should be trading them like individually to each",
    "start": "1855140",
    "end": "1861620"
  },
  {
    "text": "worker but that was a good catch yes this is why this will not run",
    "start": "1861620",
    "end": "1868220"
  },
  {
    "text": "yeah if it helps you can imagine jax.device put calls inside self.train function",
    "start": "1868220",
    "end": "1874600"
  },
  {
    "text": "let me add one more like in in real life we used like a Jax's newer um",
    "start": "1875059",
    "end": "1880340"
  },
  {
    "text": "like um your abstraction called Global device array so that like um they handle",
    "start": "1880340",
    "end": "1885559"
  },
  {
    "text": "the splitting of the batch by themselves so they're like theoretically it kind of makes sense for us to just throw it down",
    "start": "1885559",
    "end": "1891679"
  },
  {
    "text": "the patch and they should be able to figure this out within the like a picture function so yeah",
    "start": "1891679",
    "end": "1898520"
  },
  {
    "text": "we'll take one more question yeah quick question so",
    "start": "1898520",
    "end": "1904159"
  },
  {
    "text": "um in your programming model and in your infrastructure Where is the kind of pipelining and parallel where",
    "start": "1904159",
    "end": "1910460"
  },
  {
    "text": "it's like the fertilization decision made um is the mle him or herself making that",
    "start": "1910460",
    "end": "1917299"
  },
  {
    "text": "decision or is it handled by Ray and then also are you using kind of any scale hosting are you kind of working directly with the VMS and was that kind",
    "start": "1917299",
    "end": "1924080"
  },
  {
    "text": "of interface look like we're working with uh VMS directly uh",
    "start": "1924080",
    "end": "1930080"
  },
  {
    "text": "not any skill hosting and uh for the tpus itself the parallelization that's",
    "start": "1930080",
    "end": "1935899"
  },
  {
    "text": "taken care of by feature there are really two ways of going about it digit also is basically an algorithm which is",
    "start": "1935899",
    "end": "1941539"
  },
  {
    "text": "powered by gspmd uh it's a paper by Google you can read it I guess uh it has automatic",
    "start": "1941539",
    "end": "1948260"
  },
  {
    "text": "charting inference built in you can say that you're in my input is shot at this way and my output is going to be shorted this way and everything in between it",
    "start": "1948260",
    "end": "1955340"
  },
  {
    "text": "has the capability of figuring it out of like sharding propagation it's got that built-in but if we know for like for at",
    "start": "1955340",
    "end": "1962840"
  },
  {
    "text": "least for Transformer architectures there are definite ways we know that like to minimize the communication overhead we know how to short things so",
    "start": "1962840",
    "end": "1969919"
  },
  {
    "text": "gspmd also provides this thing called sharding constraints where it treats them as hard constraints that it needs",
    "start": "1969919",
    "end": "1975500"
  },
  {
    "text": "to obey for the sharding propagation to complete uh to finish going through so we use sharding constraints for like",
    "start": "1975500",
    "end": "1981799"
  },
  {
    "text": "vanilla Transformer architectures and for any custom architectures we have we just rely on automatic starting",
    "start": "1981799",
    "end": "1987020"
  },
  {
    "text": "propagation sometimes it works sometimes it doesn't in the cases when it doesn't work we look at tensorflow profiles and",
    "start": "1987020",
    "end": "1992480"
  },
  {
    "text": "figure route where the breakage happens or where the sub-optimality is and we kind of fix that by changing the",
    "start": "1992480",
    "end": "1997760"
  },
  {
    "text": "sharding specification or adding a constraint so I'm just trying to roll back as said",
    "start": "1997760",
    "end": "2004000"
  },
  {
    "text": "so just about like pipeline parallelism specifically though like we found out internally it's just maybe enough to use",
    "start": "2004000",
    "end": "2011320"
  },
  {
    "text": "tensor parallelism and maybe it's more optimal just to use um [Music]",
    "start": "2011320",
    "end": "2016539"
  },
  {
    "text": "the tensor Parliament server Pipeline and we've been just defaulting to the tensor parallelism overall like this is",
    "start": "2016539",
    "end": "2022899"
  },
  {
    "text": "because we have big TPU pods are available but if it didn't we would have had to have logic to",
    "start": "2022899",
    "end": "2030059"
  },
  {
    "text": "either choose pipeline or tensor parallelism and you know build Logics around that",
    "start": "2030059",
    "end": "2036419"
  }
]