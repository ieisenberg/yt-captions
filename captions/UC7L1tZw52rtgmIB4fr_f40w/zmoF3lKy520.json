[
  {
    "start": "0",
    "end": "18000"
  },
  {
    "text": "like Ray core uh to facilitate scalable high performance data processing such as",
    "start": "659",
    "end": "6240"
  },
  {
    "text": "working on the data plane and then working on some libraries like desk on Ray and Ray data sets and I really love",
    "start": "6240",
    "end": "13500"
  },
  {
    "text": "working on data intensive distributed computing uh it's a lot of fun so first quick overview of the talk",
    "start": "13500",
    "end": "20359"
  },
  {
    "start": "18000",
    "end": "18000"
  },
  {
    "text": "scaling machine learning tends to be pretty hard and there are also some non-intuitive bottlenecks that I'll get",
    "start": "20359",
    "end": "26699"
  },
  {
    "text": "a little bit into Ray error in general makes this scaling pretty easy it's usually like a one line",
    "start": "26699",
    "end": "32640"
  },
  {
    "text": "config change to go from a single node to multi-node training and batch inference",
    "start": "32640",
    "end": "37940"
  },
  {
    "text": "ingest and I'll Define ingests in a little bit is a pretty common bottleneck",
    "start": "37940",
    "end": "42960"
  },
  {
    "text": "once you start moving to larger data and larger cluster scales and raid data sets solve some of these",
    "start": "42960",
    "end": "49079"
  },
  {
    "text": "ingest bottleneck problems and we'll do a little bit of a deep dive into how we solve those problems",
    "start": "49079",
    "end": "54899"
  },
  {
    "text": "so first scaling ml is pretty hard your typical ml pipeline might look",
    "start": "54899",
    "end": "60660"
  },
  {
    "text": "something like this you've got some data loading and pre-processing with some of these familiar single node libraries such as pandas and numpy or distributed",
    "start": "60660",
    "end": "68100"
  },
  {
    "text": "libraries like spark and tensorflow and then you basically take that",
    "start": "68100",
    "end": "73200"
  },
  {
    "text": "pre-processed data and ingest it into a trainer or take that pre-processed data and move it into your batch inference",
    "start": "73200",
    "end": "80460"
  },
  {
    "text": "call and when it's on a single node this ends up being pretty efficient the data is",
    "start": "80460",
    "end": "87240"
  },
  {
    "text": "the data is all just transferred within memory uh that exchange is usually not much of a problem you don't have to",
    "start": "87240",
    "end": "93360"
  },
  {
    "text": "worry about it unfortunately we can't always stick to a single node eventually your machine",
    "start": "93360",
    "end": "99780"
  },
  {
    "start": "94000",
    "end": "94000"
  },
  {
    "text": "learning pipelines must scale as your data scales and there are two big drivers either",
    "start": "99780",
    "end": "105600"
  },
  {
    "text": "your data doesn't fit into the CPU or GPU memory of a single node or it your machine learning pipeline",
    "start": "105600",
    "end": "113100"
  },
  {
    "text": "ends up being too slow if you just constrain it to the hardware resources of a single node or if it's just to",
    "start": "113100",
    "end": "119220"
  },
  {
    "text": "churn through a large data set sequentially on a single node can just as a baseline take too long",
    "start": "119220",
    "end": "125700"
  },
  {
    "start": "125000",
    "end": "125000"
  },
  {
    "text": "so one of the most common Solutions here probably pretty much like the econonical solution is distributed data parallel",
    "start": "125700",
    "end": "132180"
  },
  {
    "text": "training and batch inference and so for training you paralyze training across multiple GPU workers",
    "start": "132180",
    "end": "138599"
  },
  {
    "text": "and how that works is you have a couple of trainers usually one trainer per accelerator and you might have multiple",
    "start": "138599",
    "end": "145319"
  },
  {
    "text": "accelerators per node and you then assign A Shard a disjoint Shard one",
    "start": "145319",
    "end": "151500"
  },
  {
    "text": "disjoint Shard per trainer and then you do a gradient all reduce at",
    "start": "151500",
    "end": "156540"
  },
  {
    "text": "the end of each training step to synchronize the gradients and so this is like the classic like data parallel uh",
    "start": "156540",
    "end": "162480"
  },
  {
    "text": "distributed training paradigm and for batch inference it's kind of a very similar model much simpler thankfully uh",
    "start": "162480",
    "end": "168840"
  },
  {
    "text": "you can paralyze batch inference across multiple GPU workers here it's the same sort of like sharding or partitioning",
    "start": "168840",
    "end": "175019"
  },
  {
    "text": "scheme but this is embarrassingly parallel you don't need to have a gradient sync it's all just a forward",
    "start": "175019",
    "end": "180180"
  },
  {
    "text": "pass through your model perform batch inference in parallel across a bunch of workers and then usually like store the",
    "start": "180180",
    "end": "185700"
  },
  {
    "text": "output to persistent storage or feed it into another system but in either case distributed data parallel is kind of the",
    "start": "185700",
    "end": "192420"
  },
  {
    "text": "Paradigm that solves this data scale problem and for your typical ml pipeline that we",
    "start": "192420",
    "end": "198659"
  },
  {
    "start": "196000",
    "end": "196000"
  },
  {
    "text": "talked about um trying to go from single node to multi-node usually it would end up looking something like this where you",
    "start": "198659",
    "end": "204720"
  },
  {
    "text": "have a spark cluster doing some pre-processing you have something like horovod and tensorflow to do distributed",
    "start": "204720",
    "end": "211379"
  },
  {
    "text": "training and then you usually have something like tensorflow like a pandas like a python UDF and Spark to do",
    "start": "211379",
    "end": "219239"
  },
  {
    "text": "tensorflow batch inference or maybe you're using something like desk or or sagemaker if you're on AWS and that's",
    "start": "219239",
    "end": "226200"
  },
  {
    "text": "how you try and scale out your batch inference there are a few uh difficulties with this it's uh it's",
    "start": "226200",
    "end": "232920"
  },
  {
    "text": "multiple disparate systems that you're stitching together so you're having to",
    "start": "232920",
    "end": "238200"
  },
  {
    "text": "manage multiple separate types of clusters and it's also difficult to extend especially once you start hitting",
    "start": "238200",
    "end": "245099"
  },
  {
    "text": "performance bottlenecks and we'll get to that in a little bit there's inefficient data exchange between these systems because you're",
    "start": "245099",
    "end": "251400"
  },
  {
    "text": "having to persist data to storage when going between pre-processing and training there's no shared data plane in",
    "start": "251400",
    "end": "259139"
  },
  {
    "text": "memory data planning plane like you had on a single node and it's also difficult to keep those",
    "start": "259139",
    "end": "264540"
  },
  {
    "text": "accelerators saturated as data scales and as these accelerators continue to",
    "start": "264540",
    "end": "269580"
  },
  {
    "text": "get faster they're getting hungry and hungrier for more data they are very hard to satiate",
    "start": "269580",
    "end": "275460"
  },
  {
    "start": "275000",
    "end": "275000"
  },
  {
    "text": "um yeah so one possible solution to a few of these problems in particular that keeping your accelerator saturated is",
    "start": "275460",
    "end": "281759"
  },
  {
    "text": "you try and pipeline that spark based pre-processing with training and this usually looks something like this where",
    "start": "281759",
    "end": "288000"
  },
  {
    "text": "you have this separate coordinator microservice that is in charge of coordinating the pipelining where you're",
    "start": "288000",
    "end": "294360"
  },
  {
    "text": "trying to overlap that spark based pre-processing for example for a future Epoch with the training on a current",
    "start": "294360",
    "end": "301500"
  },
  {
    "text": "Epoch or maybe even like within an Epoch you're doing Spar the spark based pre-processing on a subset of data while",
    "start": "301500",
    "end": "308460"
  },
  {
    "text": "like uh we're already training on an earlier subset of data for that Epoch and this is generally the protocol here",
    "start": "308460",
    "end": "315259"
  },
  {
    "text": "and you usually have to use external storage to transfer data between pre-processing and training and uh there",
    "start": "315259",
    "end": "322259"
  },
  {
    "text": "are a few issues with this uh well first of all it's pretty difficult to orchestrate you've now gone from two uh",
    "start": "322259",
    "end": "328199"
  },
  {
    "text": "disparate systems stitching together to now your own bespoke coordinator microservice uh to try and orchestrate",
    "start": "328199",
    "end": "335039"
  },
  {
    "text": "this pipelining so that's a little bit difficult um and then you also have to still",
    "start": "335039",
    "end": "340800"
  },
  {
    "text": "materialize data to external storage so you're always having to write out to external like remote store like S3 or",
    "start": "340800",
    "end": "347280"
  },
  {
    "text": "maybe you're using something like hdfs in either case it's both expensive to write to that storage and then you have",
    "start": "347280",
    "end": "354360"
  },
  {
    "text": "to serialize and deserialize it which ultimately both takes time and also",
    "start": "354360",
    "end": "359940"
  },
  {
    "text": "takes resources so uh how are we trying to solve this problem in Rey how can you scale ml",
    "start": "359940",
    "end": "366479"
  },
  {
    "text": "pipelines using Ray we have uh this high level unified ml toolkit called Ray air",
    "start": "366479",
    "end": "372780"
  },
  {
    "text": "which I'm sure that a lot of y'all have heard a good bit about over the last few days",
    "start": "372780",
    "end": "377960"
  },
  {
    "text": "brief little overview since there's already like several other talks in rare it's a really nice scalable and unified",
    "start": "377960",
    "end": "384360"
  },
  {
    "text": "toolkit for ML applications involving each of these different use cases and in",
    "start": "384360",
    "end": "390060"
  },
  {
    "text": "particular for what's relevant to this talk is it allows you to scale from a single node to multiple nodes with",
    "start": "390060",
    "end": "395699"
  },
  {
    "text": "basically just like a single line config change it's basically just like how many workers do you want for your training or",
    "start": "395699",
    "end": "401759"
  },
  {
    "text": "batch inference workload and then that's the extent of the change to scale up",
    "start": "401759",
    "end": "407639"
  },
  {
    "start": "406000",
    "end": "406000"
  },
  {
    "text": "um in this talk in particular focuses on errors data processing engine which is a library called Ray datasets and it's",
    "start": "407639",
    "end": "414720"
  },
  {
    "text": "involved in these three aspects of Ray air it's actually technically involved",
    "start": "414720",
    "end": "420120"
  },
  {
    "text": "like under the hood and tuning as well but that's just because tuning is running multiple hyper parameter tuning",
    "start": "420120",
    "end": "426900"
  },
  {
    "text": "trials which uses raytrain which uses Ray data sets for like pre-processing and data ingest yeah so data sets is",
    "start": "426900",
    "end": "433979"
  },
  {
    "text": "used here for all data loading it's used as the engine for Distributing parallel",
    "start": "433979",
    "end": "439440"
  },
  {
    "text": "pre-processing it's used for efficient data transfer into the GPU workers for training and it's the actual engine that",
    "start": "439440",
    "end": "446639"
  },
  {
    "text": "performs the distributed data parallel batch inference as well so it's used for a lot of stuff",
    "start": "446639",
    "end": "452460"
  },
  {
    "text": "and then one key thing to emphasize here we were talking about how there's no",
    "start": "452460",
    "end": "457620"
  },
  {
    "text": "shared data plane between like a spark cluster and a tensorflow cluster which makes trying to transfer data between",
    "start": "457620",
    "end": "463520"
  },
  {
    "text": "those pre-processing and training stages a little expensive data sets is serving",
    "start": "463520",
    "end": "469680"
  },
  {
    "text": "as errors in memory distributed data Bridge so this is a shared in-memory",
    "start": "469680",
    "end": "475319"
  },
  {
    "text": "data plane which obviates the need for those for that persistence to external",
    "start": "475319",
    "end": "480660"
  },
  {
    "text": "storage in order to exchange data between the pre-processing and training stages or between pre-processing and",
    "start": "480660",
    "end": "487380"
  },
  {
    "text": "batch inference stages and so this ends up being a pretty big performance win for a couple of case studies which we'll",
    "start": "487380",
    "end": "493259"
  },
  {
    "text": "talk about later so first I've been talking about raid data sets in the abstract you know like",
    "start": "493259",
    "end": "499259"
  },
  {
    "text": "what the heck is it uh so it's a distributed data processing Library very",
    "start": "499259",
    "end": "504300"
  },
  {
    "text": "focused on a few machine learning use cases it's not a generic like ETL or like distributed data frame Library like",
    "start": "504300",
    "end": "510599"
  },
  {
    "text": "you might be familiar with like Das data frames or like you know spark for",
    "start": "510599",
    "end": "515700"
  },
  {
    "text": "example spark data frames uh it's very geared towards these problems of data",
    "start": "515700",
    "end": "521039"
  },
  {
    "text": "loading pre-processing and ingest into trainers and then data loading pre-processing and parallel batch",
    "start": "521039",
    "end": "527160"
  },
  {
    "text": "inference and a little bit more about what it actually is it has a really nice featureful performant I O layer a lot of",
    "start": "527160",
    "end": "534779"
  },
  {
    "text": "exchange with different distributed Frameworks such as spark and desk very efficient exchange and then also a lot",
    "start": "534779",
    "end": "542519"
  },
  {
    "text": "of like performant parallel i o uh Integrations with your favorite file",
    "start": "542519",
    "end": "547620"
  },
  {
    "text": "formats like Json CSV parquet imagery and then also very efficient data",
    "start": "547620",
    "end": "553680"
  },
  {
    "text": "exchange with your favorite distributed training and distributed inference Frameworks such as tensorflow torch and",
    "start": "553680",
    "end": "559260"
  },
  {
    "text": "like distributed xgboost the underlying compute Paradigm is",
    "start": "559260",
    "end": "564839"
  },
  {
    "text": "pretty simple it's kind of like you're you're famous like mapreduce Paradigm we've uh we've got basic parallel and",
    "start": "564839",
    "end": "571860"
  },
  {
    "text": "distributed Ops like map filter and Shuffle and with those Primitives you can express like a whole lot of of",
    "start": "571860",
    "end": "578580"
  },
  {
    "text": "different computation and then finally under the hood raid datasets is built on top of Ray and uh a",
    "start": "578580",
    "end": "586860"
  },
  {
    "text": "raid data set just consists of Ray objects so you can easily pass this Ray",
    "start": "586860",
    "end": "592140"
  },
  {
    "text": "data set that you've built up on your client to other Ray tasks other Ray actors to a ray actor that's like your",
    "start": "592140",
    "end": "599700"
  },
  {
    "text": "trainer and this allows for really easy exchange of a handle to this distributed",
    "start": "599700",
    "end": "605459"
  },
  {
    "text": "data sets data without actually having to move the data you're basically just passing these like distributed pointers",
    "start": "605459",
    "end": "611100"
  },
  {
    "text": "around so that allows us to make ingest and data exchange very simple in terms",
    "start": "611100",
    "end": "617459"
  },
  {
    "text": "of user code so um I've been talking about that there are some bottlenecks once you reach a",
    "start": "617459",
    "end": "623399"
  },
  {
    "text": "certain data or compute scale we uh we probably all immediately think that the most common bottleneck is uh the actual",
    "start": "623399",
    "end": "631320"
  },
  {
    "text": "distributed training like you're having to do that very complex gradient all reduce but there have been essentially",
    "start": "631320",
    "end": "636899"
  },
  {
    "text": "like you know hundreds if not thousands of engineering hours devoted to systems such as like uh distributed data",
    "start": "636899",
    "end": "643560"
  },
  {
    "text": "parallel torch tensorflow horvod to make that extremely efficient and also the",
    "start": "643560",
    "end": "649680"
  },
  {
    "text": "accelerator interconnects between for example like tpus and a TPU pod or uh",
    "start": "649680",
    "end": "656339"
  },
  {
    "text": "like some of the very fast interconnects uh provided by your favorite you know",
    "start": "656339",
    "end": "661620"
  },
  {
    "text": "cloud provider as well as like nvidia's Tech you have extremely high bandwidth",
    "start": "661620",
    "end": "667560"
  },
  {
    "text": "and extremely fast exchange when doing that gradient all reduce and so a very common bottleneck ends up being ingest",
    "start": "667560",
    "end": "674279"
  },
  {
    "text": "and what I mean by ingest here is basically everything Upstream from",
    "start": "674279",
    "end": "679680"
  },
  {
    "text": "training your data loading pre-processing and then the actual data transferred to the GPU workers",
    "start": "679680",
    "end": "685740"
  },
  {
    "text": "so to go a little bit more into some of these ingest scaling challenges first you need to be able to Shard the data if",
    "start": "685740",
    "end": "691860"
  },
  {
    "text": "you're doing distributed data parallel training you need to be able to take a large data set partition it into shards",
    "start": "691860",
    "end": "697980"
  },
  {
    "text": "and then efficiently move those shards to the trainers you need to avoid these uh pesky GPU",
    "start": "697980",
    "end": "705360"
  },
  {
    "text": "data prep stalls if you're familiar with this this has been getting a lot of attention in both Academia and Industry",
    "start": "705360",
    "end": "711120"
  },
  {
    "text": "recently where the chief the chief offender for poor GP utilization and",
    "start": "711120",
    "end": "716880"
  },
  {
    "text": "these little GP utilization bubbles end up it ends up being actually ingest your",
    "start": "716880",
    "end": "721920"
  },
  {
    "text": "data loading and pre-processing where you're just not able to feed the gpus fast enough and then finally per Epoch shuffling is",
    "start": "721920",
    "end": "729240"
  },
  {
    "text": "pretty difficult to do in a distributed data parallel training scenario especially if your model is very",
    "start": "729240",
    "end": "734519"
  },
  {
    "text": "sensitive to the quality of your data Shuffle on every Epoch which we've encountered from multiple people in",
    "start": "734519",
    "end": "741180"
  },
  {
    "text": "industry it's tough to do in a distributed setting if you're trying to do a fully Global Shuffle so we offer a",
    "start": "741180",
    "end": "746339"
  },
  {
    "text": "few options there going into each of these a little bit more data sharding we need to be able to",
    "start": "746339",
    "end": "753060"
  },
  {
    "text": "efficiently assign data Shard partitions to each trainer pretty straightforward",
    "start": "753060",
    "end": "758519"
  },
  {
    "start": "758000",
    "end": "758000"
  },
  {
    "text": "aborting these GPU data prep stalls to give a little bit more color to this uh",
    "start": "758519",
    "end": "763620"
  },
  {
    "text": "if your pipeline ends up being ingest bound and you have these two stages",
    "start": "763620",
    "end": "769079"
  },
  {
    "text": "ingest and training you might be basically doing data loading and pre-processing AKA ingest",
    "start": "769079",
    "end": "775740"
  },
  {
    "text": "while your GPU is sitting idle and for the first Epoch it's tough to avoid that your GPU is going to be idle until the",
    "start": "775740",
    "end": "782100"
  },
  {
    "text": "data loading and pre-processing is done um but then now if you're just doing this synchronously training and uh",
    "start": "782100",
    "end": "788880"
  },
  {
    "text": "ingest entirely serially synchronously you are going to see that GPU idle pause",
    "start": "788880",
    "end": "794760"
  },
  {
    "text": "that data prep stall on every single Epoch where your trainers are just sitting there waiting for data to be",
    "start": "794760",
    "end": "801120"
  },
  {
    "text": "loaded pre-processed and transferred to the GPU workers and this can even if you're when you're when the data is",
    "start": "801120",
    "end": "807120"
  },
  {
    "text": "actually ready and the trainers are able to actually like do training on the data you might see 100 GB utilization really",
    "start": "807120",
    "end": "813480"
  },
  {
    "text": "high GP utilization uh during that batch of time but if you're having to wait for data",
    "start": "813480",
    "end": "819240"
  },
  {
    "text": "loading and ingest uh like before the trainers can actually see the data you might basically be seeing like an",
    "start": "819240",
    "end": "825000"
  },
  {
    "text": "aggregate like 50 cheap utilization or Worse we've seen users actually come to us with like 20 to 30 percent uh GP",
    "start": "825000",
    "end": "831600"
  },
  {
    "text": "utilization even though the training when the data was actually available was going pretty quickly",
    "start": "831600",
    "end": "836760"
  },
  {
    "text": "um so yeah that can really hurt the idle gpus cost money and we don't want to burn money",
    "start": "836760",
    "end": "842760"
  },
  {
    "text": "even uh for batch inference although it's less of a commonly studied issue",
    "start": "842760",
    "end": "848360"
  },
  {
    "text": "we've also seen users that are doing the same synchronous data loading of the",
    "start": "848360",
    "end": "853380"
  },
  {
    "text": "entire data set pre-process the entire data set and only then do they actually send data to the inference stage and if",
    "start": "853380",
    "end": "861180"
  },
  {
    "text": "you have like a large like uh tens of gigabytes hundreds of gigabytes multi-terabyte data set you are just",
    "start": "861180",
    "end": "867920"
  },
  {
    "text": "burning money by letting those those GPU inference workers just sit there idle so",
    "start": "867920",
    "end": "874320"
  },
  {
    "text": "we would really love to be able to overlap these stages to try and hide some of that ingest uh latency",
    "start": "874320",
    "end": "881339"
  },
  {
    "text": "and then finally per Epoch shuffling um the general motivation for this is some models are Shuffle sensitive and",
    "start": "881339",
    "end": "887100"
  },
  {
    "text": "you'll get a much better model accuracy after a fixed number of epochs this can",
    "start": "887100",
    "end": "892260"
  },
  {
    "text": "kind of be there's a lot of extra information on this both in industry and also some recent research on like the",
    "start": "892260",
    "end": "898680"
  },
  {
    "text": "convergence of stochastic gradient descent and everything which I wish I could just spend an entire talk talking about that because it's pretty",
    "start": "898680",
    "end": "904860"
  },
  {
    "text": "interesting but the the general idea is uh per Epoch shuffling matters as well",
    "start": "904860",
    "end": "910740"
  },
  {
    "text": "as just general data shuffling before you start training because if you see a bad GPU batch that can that can make you",
    "start": "910740",
    "end": "918060"
  },
  {
    "text": "get stuck in a local Minima when you're uh when your solver is trying to optimize uh like when your solver is",
    "start": "918060",
    "end": "926100"
  },
  {
    "text": "trying to actually like reach that Global minimum you might get stuck in a local Minima and then if you see that same batch in the same order on every",
    "start": "926100",
    "end": "933779"
  },
  {
    "text": "single Epoch you will keep hitting that local Minima and by shuffling on every Epoch it helps you bounce out of that",
    "start": "933779",
    "end": "939480"
  },
  {
    "text": "local Minima and actually find find a more optimal solution and but unfortunately like we would love",
    "start": "939480",
    "end": "946620"
  },
  {
    "text": "to do fully Global shuffling fully random shuffling as if it was like perfect like without replacement popping",
    "start": "946620",
    "end": "953519"
  },
  {
    "text": "a single record at a time but unfortunately while that's very easy on a single node it's hard to do in a distributed setting and the typical",
    "start": "953519",
    "end": "959760"
  },
  {
    "text": "solution is an all to all communication mapreduce style Shuffle where you mix",
    "start": "959760",
    "end": "964800"
  },
  {
    "text": "shards as uh shown in that day in that diagram so we've talked about these General",
    "start": "964800",
    "end": "971820"
  },
  {
    "text": "ingest scaling challenges how exactly does raid data sets meet these challenges",
    "start": "971820",
    "end": "977279"
  },
  {
    "text": "first let's talk about data sharding this is a pretty straightforward one basically sharding data sets across",
    "start": "977279",
    "end": "985199"
  },
  {
    "start": "982000",
    "end": "982000"
  },
  {
    "text": "workers using air and using Ray data sets is basically a single API call like",
    "start": "985199",
    "end": "991800"
  },
  {
    "text": "that that's essentially it you call Dot split on a data set and you give it the",
    "start": "991800",
    "end": "997199"
  },
  {
    "text": "number of shards you want and then you specify whether or not you want to guarantee that the shards are equal in",
    "start": "997199",
    "end": "1002899"
  },
  {
    "text": "which case it will basically do like an optimized rebalancing of shards and then it will drop like at most number of",
    "start": "1002899",
    "end": "1009800"
  },
  {
    "text": "shards rows to make sure that they're equal so it'll do a pretty like basically a minimal amount of truncating",
    "start": "1009800",
    "end": "1017360"
  },
  {
    "text": "um yeah and under the hood you know it's pretty pretty straightforward the basic sharding algorithm you just basically uh",
    "start": "1017360",
    "end": "1024319"
  },
  {
    "text": "yeah you partition the distributed data set and then you create a mapping from each Shard to the appropriate trainer",
    "start": "1024319",
    "end": "1031640"
  },
  {
    "text": "we also have some interesting optimizations here such as locality aware assignment of shards to trainers",
    "start": "1031640",
    "end": "1037540"
  },
  {
    "text": "so for example if you take the handles to the training workers which in Ray air",
    "start": "1037540",
    "end": "1044660"
  },
  {
    "text": "and Ray data sets that refers to like handles to the right actor that is",
    "start": "1044660",
    "end": "1049820"
  },
  {
    "text": "sitting on the GPU and you give those to the split API uh data sets will then do a locality",
    "start": "1049820",
    "end": "1057320"
  },
  {
    "text": "where assignment of those shards uh to those trainers and what that looks like",
    "start": "1057320",
    "end": "1062660"
  },
  {
    "text": "is let's say this data set already lives on your GPU nodes and uh each Shard is",
    "start": "1062660",
    "end": "1068240"
  },
  {
    "text": "already sitting in the object store of one of these GPU nodes all that this does is it makes sure that",
    "start": "1068240",
    "end": "1074840"
  },
  {
    "text": "we assign The Shard to the local trainer and this applies even when they're like",
    "start": "1074840",
    "end": "1080299"
  },
  {
    "text": "small little little chunks of A Shard sitting on a node we will try to",
    "start": "1080299",
    "end": "1086600"
  },
  {
    "text": "basically ensure that though as much data that's already local to the trainer is assigned",
    "start": "1086600",
    "end": "1092720"
  },
  {
    "text": "to that trainer Shard as possible and this really reduces data movement if you don't have this you could have that the",
    "start": "1092720",
    "end": "1099500"
  },
  {
    "text": "like for example the blue Shard and node one might end up being assigned to the trainer 4 on node four which would",
    "start": "1099500",
    "end": "1106340"
  },
  {
    "text": "involve having to move that entire Shard to that trainer so this ends up really reducing data movement and actually ends",
    "start": "1106340",
    "end": "1112940"
  },
  {
    "text": "up improving the ingest throughput a good bit so next challenge how do we avoid those",
    "start": "1112940",
    "end": "1119360"
  },
  {
    "text": "pesky GPU data prep stalls there are three core aspects to how we address this first if we speed up pre-processing",
    "start": "1119360",
    "end": "1127240"
  },
  {
    "text": "awesome ingest is now faster and you are able to at least decrease the amount of",
    "start": "1127240",
    "end": "1133700"
  },
  {
    "text": "time your GPU is Idle if you're able to just ingest faster then your gpus are",
    "start": "1133700",
    "end": "1138860"
  },
  {
    "text": "just going to be idle for less time even without any fancy overlapping or pipelining uh number two uh we've got a",
    "start": "1138860",
    "end": "1145340"
  },
  {
    "text": "very easy way that you can pipeline ingest with GPU compute and the number three we have some",
    "start": "1145340",
    "end": "1151100"
  },
  {
    "text": "options for overloading gpus so that involves assigning uh more than one batch worker to a single GPU and then",
    "start": "1151100",
    "end": "1158720"
  },
  {
    "text": "although that incurs some like Cuda kernel context switches and that can be like a little bit inefficient we've seen",
    "start": "1158720",
    "end": "1165440"
  },
  {
    "text": "in practice that if you like overload a single GPU with four batch inference workers you can typically achieve a",
    "start": "1165440",
    "end": "1171140"
  },
  {
    "text": "pretty good GP utilization like in the 90 percentile in the 90s so first how do",
    "start": "1171140",
    "end": "1176960"
  },
  {
    "start": "1176000",
    "end": "1176000"
  },
  {
    "text": "we speed up pre-processing a few core aspects parallel i o and Transformations we've got all this CPU compute available",
    "start": "1176960",
    "end": "1184220"
  },
  {
    "text": "across the cluster why don't we just do all of these data parallel Transformations uh in parallel easy",
    "start": "1184220",
    "end": "1191360"
  },
  {
    "text": "first a little background on what array data set is a little on its actual data model a raid data set is very similar to",
    "start": "1191360",
    "end": "1198799"
  },
  {
    "text": "other distributed data collection abstractions you might be familiar with such as spark or dasc data frames those",
    "start": "1198799",
    "end": "1204860"
  },
  {
    "text": "are like the very common ones people know basically a data set just consists of",
    "start": "1204860",
    "end": "1209900"
  },
  {
    "text": "those distributed pointers that I was talking about where it has a block reference per underlying data partition",
    "start": "1209900",
    "end": "1217100"
  },
  {
    "text": "or we call them blocks and these blocks could actually live on a bunch of",
    "start": "1217100",
    "end": "1222380"
  },
  {
    "text": "different nodes on your client machine all that you have is this very lightweight data set object that just",
    "start": "1222380",
    "end": "1228860"
  },
  {
    "text": "contains these distributed data pointers so and then you can very quickly do like orchestrate large cluster scale",
    "start": "1228860",
    "end": "1236440"
  },
  {
    "text": "compute that will all be done and distributed over these blocks that live on other nodes but like in actuality on",
    "start": "1236440",
    "end": "1242780"
  },
  {
    "text": "your machine which can even be sitting outside of the cluster using Ray client um yeah you're you're not really pulling",
    "start": "1242780",
    "end": "1249140"
  },
  {
    "text": "any data to that local machine unless you actually want to we also have a bunch of quick introspection apis like",
    "start": "1249140",
    "end": "1256100"
  },
  {
    "text": "checking out the metadata for the data set and then also just getting doing like a head call for example like",
    "start": "1256100",
    "end": "1261860"
  },
  {
    "text": "getting the first 10 rows to make sure that everything is looking good and how this actually translates to",
    "start": "1261860",
    "end": "1269240"
  },
  {
    "text": "parallel i o you can probably parallel i o and transformation so you can probably already see how this distributed",
    "start": "1269240",
    "end": "1275179"
  },
  {
    "text": "representation is nice for doing parallel operations for example here's like the API for reading a bunch of",
    "start": "1275179",
    "end": "1281720"
  },
  {
    "text": "parquet files from a particular directory and that can by the way be in S3 Google Cloud Storage Azure blob store",
    "start": "1281720",
    "end": "1288080"
  },
  {
    "text": "local disk hdfs we support a bunch of different storage backends if you",
    "start": "1288080",
    "end": "1293480"
  },
  {
    "text": "specify a parallelism of two what this will look like is if you have four parquet files we will launch two read",
    "start": "1293480",
    "end": "1299659"
  },
  {
    "text": "tasks and each read task will create one data block and those read tasks can be",
    "start": "1299659",
    "end": "1305659"
  },
  {
    "text": "executed anywhere on the cluster and we actually have a specific optimization to make sure that data is load balanced as",
    "start": "1305659",
    "end": "1313100"
  },
  {
    "text": "best as possible across the cluster for both maximal Downstream parallelism and also General stability of memory",
    "start": "1313100",
    "end": "1319640"
  },
  {
    "text": "intensive operations and I'll get to that in a little bit but yeah cool now we've got these data blocks after",
    "start": "1319640",
    "end": "1324740"
  },
  {
    "text": "reading the data from storage and these read tasks were executed in parallel across the cluster now let's say you",
    "start": "1324740",
    "end": "1330799"
  },
  {
    "text": "then do a transformation like a like a batch mapping function so you just want to apply this function across batches",
    "start": "1330799",
    "end": "1338960"
  },
  {
    "text": "multiple batches per block all that this amounts to is let's say you have four blocks we will launch a map task per",
    "start": "1338960",
    "end": "1346700"
  },
  {
    "text": "block running somewhere on the cluster and then within that map task that Ray",
    "start": "1346700",
    "end": "1353059"
  },
  {
    "text": "task we will then apply your user provided function on sub batches on",
    "start": "1353059",
    "end": "1358520"
  },
  {
    "text": "batches of that block and then the output is another block so from your",
    "start": "1358520",
    "end": "1363640"
  },
  {
    "text": "client-side data set perspective you didn't have to pull any data local or anything like that all of these",
    "start": "1363640",
    "end": "1369860"
  },
  {
    "text": "operations were run wherever the data is living in the cluster and then basically",
    "start": "1369860",
    "end": "1376100"
  },
  {
    "text": "once the computation completes you then have a handle to the new output blocks",
    "start": "1376100",
    "end": "1381140"
  },
  {
    "text": "in your data set and ultimately what this results in",
    "start": "1381140",
    "end": "1386179"
  },
  {
    "text": "instead of having synchronous single node you know uh basically row by row or batch by batch uh reading and",
    "start": "1386179",
    "end": "1393740"
  },
  {
    "text": "Transformations which can be slow by parallelizing this across all of your uh CPU memory and network bandwidth in your",
    "start": "1393740",
    "end": "1400880"
  },
  {
    "text": "cluster you're able to often achieve much better reading throughput and much better pre-processing throughput just by",
    "start": "1400880",
    "end": "1407000"
  },
  {
    "text": "doing this parallel execution and that should decrease the time to arrival for GPU batches on your trainers",
    "start": "1407000",
    "end": "1413000"
  },
  {
    "text": "um so we're speeding up pre-processing that's great and there are also a few data plane optimizations I want to",
    "start": "1413000",
    "end": "1418700"
  },
  {
    "start": "1415000",
    "end": "1415000"
  },
  {
    "text": "quickly talk about there are these particularly once again like improve the the execution time of",
    "start": "1418700",
    "end": "1426320"
  },
  {
    "text": "reading and pre-processing the first is the load balancing of read tasks that I talked about where we do a best effort",
    "start": "1426320",
    "end": "1432740"
  },
  {
    "text": "of spreading those initial read tasks that are reading data from like external storage across the cluster and so that",
    "start": "1432740",
    "end": "1439760"
  },
  {
    "text": "should end up uh better utilizing the cluster's aggregate Network bandwidth and also better utilizing uh the",
    "start": "1439760",
    "end": "1447620"
  },
  {
    "text": "Clusters like entire aggregate like uh available Object Store memory which",
    "start": "1447620",
    "end": "1452840"
  },
  {
    "text": "should result in more stable pre-processing Downstream it it basically prevents like those classic",
    "start": "1452840",
    "end": "1458419"
  },
  {
    "text": "Hot node issues where if you scheduled all of your read tasks on a single node totally uh number one uh exhaust and",
    "start": "1458419",
    "end": "1465740"
  },
  {
    "text": "saturate the network band with that node and then number two totally max out its memory allocation you might then in",
    "start": "1465740",
    "end": "1472520"
  },
  {
    "text": "Downstream operations encounter out of memory errors or a lot of spilling to disk so that's nice number two uh locality",
    "start": "1472520",
    "end": "1480380"
  },
  {
    "text": "where scheduling of those transformation tasks Ray core Ray will do a best effort",
    "start": "1480380",
    "end": "1485480"
  },
  {
    "text": "scheduling of those map tasks to where the data already lives so this is this",
    "start": "1485480",
    "end": "1491720"
  },
  {
    "text": "should basically reduce like unnecessary data movement where this is the concept of like moving compute to the data and",
    "start": "1491720",
    "end": "1498799"
  },
  {
    "text": "so this results in Faster execution and less data transfer which is basically more stable for the Clusters Object",
    "start": "1498799",
    "end": "1506299"
  },
  {
    "text": "Store and then finally transparent object spilling to disk this is more a reliability aspect of uh of your",
    "start": "1506299",
    "end": "1514100"
  },
  {
    "text": "ingestion pipeline um but you know I I would say that I the ingestion pipeline that's slowest is one",
    "start": "1514100",
    "end": "1520039"
  },
  {
    "text": "that never completes because it fails and this will prevent those classic like out of memory errors when it's like oop",
    "start": "1520039",
    "end": "1526820"
  },
  {
    "text": "oops I've exhausted either a single notes memory or my cluster's aggregate memory",
    "start": "1526820",
    "end": "1532520"
  },
  {
    "text": "um what this will do is it'll just transparently uh do fallback allocation to disk when first creating the object",
    "start": "1532520",
    "end": "1538520"
  },
  {
    "text": "in the object store or it will actually do like Progressive spilling to disk uh",
    "start": "1538520",
    "end": "1545299"
  },
  {
    "text": "if basically uh your node is currently under a good bit of memory pressure so",
    "start": "1545299",
    "end": "1550940"
  },
  {
    "text": "that basically allows you to have disk as like a uh like a less optimal but",
    "start": "1550940",
    "end": "1557000"
  },
  {
    "text": "still available Object Store backup and that should prevent those out of memory errors",
    "start": "1557000",
    "end": "1562220"
  },
  {
    "text": "so it makes data sets faster more scalable or faster and more stable and",
    "start": "1562220",
    "end": "1568279"
  },
  {
    "text": "ultimately it allows that pre-processing and that ingest to complete faster so that was uh the first uh in the first",
    "start": "1568279",
    "end": "1574580"
  },
  {
    "text": "like little way that we address these data prep stalls number two is pipelining ingest with GPU compute first",
    "start": "1574580",
    "end": "1581539"
  },
  {
    "start": "1579000",
    "end": "1579000"
  },
  {
    "text": "when it comes to trading if you remember we previously had these issues of where",
    "start": "1581539",
    "end": "1587679"
  },
  {
    "text": "pre-processing and training is happening uh synchronously so you're having all of",
    "start": "1587679",
    "end": "1593419"
  },
  {
    "text": "these idle GPU stalls now with this very easy basically you",
    "start": "1593419",
    "end": "1599539"
  },
  {
    "text": "don't actually this actually happens automatically you don't really have to think about it as a user but once you called dot repeat on a data set which",
    "start": "1599539",
    "end": "1607400"
  },
  {
    "text": "will by the way repeat this uh this data set infinitely any Downstream operations such as random",
    "start": "1607400",
    "end": "1614900"
  },
  {
    "text": "shuffling will be pipelined with the actual uh the actual training and so",
    "start": "1614900",
    "end": "1622820"
  },
  {
    "text": "what this ends up looking like is you have to take that hit for ingest for the",
    "start": "1622820",
    "end": "1627919"
  },
  {
    "text": "first Epoch that's kind of unavoidable but then as soon as you start uh training on the first Epoch we actually",
    "start": "1627919",
    "end": "1635059"
  },
  {
    "text": "start the ingest for the second Epoch and this basically allows you to at",
    "start": "1635059",
    "end": "1640460"
  },
  {
    "text": "least mitigate and at best completely eliminate those idle GPU Bubbles and so",
    "start": "1640460",
    "end": "1647779"
  },
  {
    "text": "if your ingest throughput is at least as",
    "start": "1647779",
    "end": "1653240"
  },
  {
    "text": "high as your training throughput you should basically not see any of those Ido GPU stalls your GPU trainers should",
    "start": "1653240",
    "end": "1661400"
  },
  {
    "text": "only just see hot in-memory GPU batches waiting for them by the time they",
    "start": "1661400",
    "end": "1666679"
  },
  {
    "text": "actually get to training on that next epochs data and so we've had several",
    "start": "1666679",
    "end": "1672260"
  },
  {
    "text": "users that have enjoyed this feature and this is also a very common solution in industry for example like uh like a lot",
    "start": "1672260",
    "end": "1680059"
  },
  {
    "text": "of your favorite uh torch data loaders or like tensorflow data sets they typically do this like on a local scale",
    "start": "1680059",
    "end": "1686480"
  },
  {
    "text": "such that uh your data loading or maybe a few pre-processing operations is",
    "start": "1686480",
    "end": "1692059"
  },
  {
    "text": "pipelined with like an asynchronous like background thread or background process we actually do this in an entirely",
    "start": "1692059",
    "end": "1697880"
  },
  {
    "text": "distributed fashion and we even have a solution called disaggregated CPU and",
    "start": "1697880",
    "end": "1704600"
  },
  {
    "text": "GPU compute which means you can have a bunch of CPU workers that do all of your ingests and pre-processing and your GPU",
    "start": "1704600",
    "end": "1710900"
  },
  {
    "text": "workers are just reserved for training and you can scale those independently so scale your CPU workers up to meet your",
    "start": "1710900",
    "end": "1717620"
  },
  {
    "text": "pre-processing demand and then uh basically doing this pipelining and distributed fashion not just with like",
    "start": "1717620",
    "end": "1723679"
  },
  {
    "text": "an asynchronous like background thread or process this uh this also applies for inference",
    "start": "1723679",
    "end": "1729799"
  },
  {
    "start": "1727000",
    "end": "1727000"
  },
  {
    "text": "as well if you remember these three stages for uh for doing a batch",
    "start": "1729799",
    "end": "1734960"
  },
  {
    "text": "inference uh for for doing a uh for executing a batch inference pipeline um what we can do is we can window the",
    "start": "1734960",
    "end": "1741799"
  },
  {
    "text": "data set this window call after the read will actually result in creating a",
    "start": "1741799",
    "end": "1748480"
  },
  {
    "text": "Windows of sub of like disjoint subsets of the data set so you can think of it",
    "start": "1748480",
    "end": "1753620"
  },
  {
    "text": "as like chunking or windowing the data set and uh all subsequent operations",
    "start": "1753620",
    "end": "1759039"
  },
  {
    "text": "will then be pipelined using those disjoint windows so as you're loading",
    "start": "1759039",
    "end": "1764659"
  },
  {
    "text": "the data for example for the second window you will be doing pre-processing on the first window and then uh",
    "start": "1764659",
    "end": "1771740"
  },
  {
    "text": "depending on the number of stages of compute that you have after the window call in this case we actually have that",
    "start": "1771740",
    "end": "1778279"
  },
  {
    "text": "the read is transparently moved to after the window calls so that allows you to basically get fully streaming ingest the",
    "start": "1778279",
    "end": "1785539"
  },
  {
    "text": "this is a double nicety of of you are getting a parallel execution of the loading pre-processing and inference",
    "start": "1785539",
    "end": "1791779"
  },
  {
    "text": "stages and then you can ultimately hope to get a really good GPU saturation but",
    "start": "1791779",
    "end": "1798320"
  },
  {
    "text": "you're also like your total uh cluster memory utilization is kept pretty low because you're actually able to do uh",
    "start": "1798320",
    "end": "1804799"
  },
  {
    "text": "essentially like streaming um batch inference uh at the window level so for example as soon as batch",
    "start": "1804799",
    "end": "1812240"
  },
  {
    "text": "inference for the first window completes that data is then ditched that data is",
    "start": "1812240",
    "end": "1818419"
  },
  {
    "text": "released and you can move on to the subsequent windows in the data set without having to worry about that first",
    "start": "1818419",
    "end": "1824419"
  },
  {
    "text": "window taking up your cluster Object Store and so this can actually be down be done right now down to the file level",
    "start": "1824419",
    "end": "1830799"
  },
  {
    "text": "and we are exploring even doing this down to the uh per batch or per row",
    "start": "1830799",
    "end": "1836240"
  },
  {
    "text": "level to basically how you could have like a streaming pipeline that could",
    "start": "1836240",
    "end": "1841399"
  },
  {
    "text": "only just like read and process and do inference on a single row at a time if you want it so that could help like",
    "start": "1841399",
    "end": "1847640"
  },
  {
    "text": "super like memory limited clusters and then finally GPU overloading",
    "start": "1847640",
    "end": "1853279"
  },
  {
    "start": "1852000",
    "end": "1852000"
  },
  {
    "text": "um so this is addressed in a few different ways uh we've got this Auto scaling actor pool for inference which",
    "start": "1853279",
    "end": "1860539"
  },
  {
    "text": "first of all it scales dynamically with the size of the work queue and",
    "start": "1860539",
    "end": "1866000"
  },
  {
    "text": "ultimately that should uh that should try to ensure that your gpus aren't wasted during inference jobs if you've",
    "start": "1866000",
    "end": "1871580"
  },
  {
    "text": "ever done like let me statically allocate you know a ton of GPU inference workers and then I'm going to hope that",
    "start": "1871580",
    "end": "1878539"
  },
  {
    "text": "my work queue utilizes all of them um it you you can often run into the",
    "start": "1878539",
    "end": "1883640"
  },
  {
    "text": "issues of oh oops I over allocated by basically like 2 3x um and ultimately I saw really poor like",
    "start": "1883640",
    "end": "1890960"
  },
  {
    "text": "aggregate GP utilization um and like like wasted a little bit of money so this is a pretty common feature",
    "start": "1890960",
    "end": "1897380"
  },
  {
    "text": "of like hey let's scale the the size of the pool with the uh with the work queue",
    "start": "1897380",
    "end": "1903140"
  },
  {
    "text": "so quick little show of how this works let's say you start off with having a single worker in the auto scaling pool",
    "start": "1903140",
    "end": "1909980"
  },
  {
    "text": "and then you're saying okay cool I'm going to do batch inference on the first little data block",
    "start": "1909980",
    "end": "1915980"
  },
  {
    "text": "awesome um and then concurrently you start up the second worker because you say oh the work Hues decently big I should probably",
    "start": "1915980",
    "end": "1923179"
  },
  {
    "text": "scale up the scale up the pool a little bit but starting these workers takes",
    "start": "1923179",
    "end": "1928220"
  },
  {
    "text": "time like if you're if you've ever tried to acquire GPU nodes from your favorite cloud provider you can often notice like",
    "start": "1928220",
    "end": "1935179"
  },
  {
    "text": "oh boy I don't have a lot of this particular GPU node available in this Zone",
    "start": "1935179",
    "end": "1940760"
  },
  {
    "text": "um you know or in this region and you might be waiting for a hot second for that GPU node to become available and",
    "start": "1940760",
    "end": "1946700"
  },
  {
    "text": "then also depending on what run time you're using for batch inference it can take a little bit of time to initialize",
    "start": "1946700",
    "end": "1952700"
  },
  {
    "text": "that runtime like for Ray actors it can take like a little bit of time to actually allocate a worker process and",
    "start": "1952700",
    "end": "1958760"
  },
  {
    "text": "then start up the reactor so here you have worker 2 is taking the second block",
    "start": "1958760",
    "end": "1964159"
  },
  {
    "text": "but before we can even start worker three we might say oh cool the first",
    "start": "1964159",
    "end": "1969500"
  },
  {
    "text": "block has been uh finished I'm doing batch inference on it cool so we can just send the third block to the first",
    "start": "1969500",
    "end": "1975440"
  },
  {
    "text": "worker and then as worker 3 starts up we're doing this parallel batch inference on worker one and worker two",
    "start": "1975440",
    "end": "1981799"
  },
  {
    "text": "worker three starts cool that means we can take the third block or sorry the final block send it to worker three and",
    "start": "1981799",
    "end": "1988640"
  },
  {
    "text": "ultimately that allows us to not over scale beyond the worker queue size or",
    "start": "1988640",
    "end": "1993919"
  },
  {
    "text": "beyond the work queue size and that actually is dynamic we're constantly looking at we're constantly looking at",
    "start": "1993919",
    "end": "1999620"
  },
  {
    "text": "readjusting the size of the pool based on the the amount of work that's left",
    "start": "1999620",
    "end": "2005380"
  },
  {
    "text": "um finally we combined this Auto scaling actor pool with the pipelining of data transfers with inference and with",
    "start": "2005380",
    "end": "2012399"
  },
  {
    "text": "fractional GP requests to get that oversubscription of gpus and what this",
    "start": "2012399",
    "end": "2017799"
  },
  {
    "text": "ultimately results in is we send multiple blocks to a single worker which",
    "start": "2017799",
    "end": "2022960"
  },
  {
    "text": "allows us to pipeline the uh the batch inference on one block with the data transfer of the other block and then we",
    "start": "2022960",
    "end": "2030460"
  },
  {
    "text": "also might have here two workers that are just basically like multiplexing on",
    "start": "2030460",
    "end": "2036039"
  },
  {
    "text": "top of a single GPU and so we are we have two workers submitting concurrent work requests to that GPU and that can",
    "start": "2036039",
    "end": "2043600"
  },
  {
    "text": "usually help you saturate a GPU if your model forward pass is pretty cheap so it's basically just multiple workers",
    "start": "2043600",
    "end": "2049898"
  },
  {
    "text": "parked on a single GPU and then finally the last scaling",
    "start": "2049899",
    "end": "2055118"
  },
  {
    "start": "2055000",
    "end": "2055000"
  },
  {
    "text": "challenge is per Epoch shuffling this can be pretty difficult in a distributed setting but data sets has invested a",
    "start": "2055119",
    "end": "2061599"
  },
  {
    "text": "good bit and a couple of really good distributed shuffling uh implementations this allows for either fully Global or",
    "start": "2061599",
    "end": "2068500"
  },
  {
    "text": "if you do your random shuffle after windowing only a per window uh",
    "start": "2068500",
    "end": "2074080"
  },
  {
    "text": "distributed Shuffle and this is just done well the first implementation is",
    "start": "2074080",
    "end": "2079960"
  },
  {
    "text": "just done using a simple mapreduce Shuffle so you have a couple of blocks you send one block to each each of the",
    "start": "2079960",
    "end": "2086080"
  },
  {
    "text": "blocks to a mapper the mapper chunks that input into a chunk per reducer and",
    "start": "2086080",
    "end": "2092560"
  },
  {
    "text": "then we just basically send each mapper sends one chunk to each reducer the",
    "start": "2092560",
    "end": "2098080"
  },
  {
    "text": "reducers concatenates those chunks and then shuffles them together at a row level and then ultimately at the end we",
    "start": "2098080",
    "end": "2104500"
  },
  {
    "text": "have two shuffled blocks and this actually amounts to a fully Global Shuffle that is equivalent to like row",
    "start": "2104500",
    "end": "2112060"
  },
  {
    "text": "levels sampling like perfect random sampling with replacement or sorry without replacement that's the one",
    "start": "2112060",
    "end": "2118720"
  },
  {
    "text": "um and then we recently added a different shuffling implementation which",
    "start": "2118720",
    "end": "2123760"
  },
  {
    "text": "is really really cool that actually there's a talk going on right now for this it's uh the EXO Shuffle",
    "start": "2123760",
    "end": "2129940"
  },
  {
    "text": "implementation and it's a push-based shuffle and I definitely encourage you to check out that talk unfortunately",
    "start": "2129940",
    "end": "2135940"
  },
  {
    "text": "since this talk is at the same time probably going to have to look at like the virtual recording but yeah it's a",
    "start": "2135940",
    "end": "2141099"
  },
  {
    "text": "great talk um in addition to distributed shuffling we also have a local shuffling option so",
    "start": "2141099",
    "end": "2147880"
  },
  {
    "text": "let's say your model isn't super Shuffle sensitive like it doesn't need fully Global shuffles but you still want to",
    "start": "2147880",
    "end": "2154000"
  },
  {
    "text": "have like a pretty good Shuffle uh that doesn't really affect your pre-processing throughput so much we",
    "start": "2154000",
    "end": "2161200"
  },
  {
    "text": "have something that uh uses what a common industry solution which is a local Shuffle buffer so you have a",
    "start": "2161200",
    "end": "2167560"
  },
  {
    "text": "buffer of some size some number of rows let's say like a million rows 10 million rows a billion rows and then all that",
    "start": "2167560",
    "end": "2174460"
  },
  {
    "text": "this buffer does is it accumulates samples mixes them together and then pops off random batches from that buffer",
    "start": "2174460",
    "end": "2182260"
  },
  {
    "text": "so to give a quick little example let's say you have a buffer size here of four so this is like the minimum buffer size",
    "start": "2182260",
    "end": "2189579"
  },
  {
    "text": "it must be that large before you start popping batches off and then you have a batch size of two so first we add a",
    "start": "2189579",
    "end": "2196300"
  },
  {
    "text": "block to the buffer and that's going to be a block is going to end up consisting of four rows so cool we've got a four",
    "start": "2196300",
    "end": "2202839"
  },
  {
    "text": "rows now in the buffer then we add another block like cool we've got eight rows awesome",
    "start": "2202839",
    "end": "2208079"
  },
  {
    "text": "now that it's beyond that minimum size we can start popping off those uh two",
    "start": "2208079",
    "end": "2214300"
  },
  {
    "text": "row GPU batches and so what happens is we're beyond the minimum size time to mix up the rows and then we pop off a",
    "start": "2214300",
    "end": "2221680"
  },
  {
    "text": "batch and then oh cool we're still above the minimum buffer size so we can",
    "start": "2221680",
    "end": "2227320"
  },
  {
    "text": "actually pop off another random batch and then so the trainer is doing uh is doing training on those random batches",
    "start": "2227320",
    "end": "2234460"
  },
  {
    "text": "now we're at we're back like at the local the minimum buffer size we can't",
    "start": "2234460",
    "end": "2239980"
  },
  {
    "text": "pop off a batch without going below the minimum buffer size so we are in the background pulling in more data to try",
    "start": "2239980",
    "end": "2246820"
  },
  {
    "text": "to fill up that shuffle buffer um cool we're above the minimum size Again mix it up and then pop a random",
    "start": "2246820",
    "end": "2252760"
  },
  {
    "text": "Batch off pop another random Batch off but notice that these samples from these different blocks are getting mixed over",
    "start": "2252760",
    "end": "2259359"
  },
  {
    "text": "time so it's not purely like let me take a couple of blocks from the beginning of the data set and then fully consume",
    "start": "2259359",
    "end": "2266380"
  },
  {
    "text": "those uh as GPU batches this is actually going to mix data throughout the entire",
    "start": "2266380",
    "end": "2272020"
  },
  {
    "text": "Epoch so you're going to have samples mixed from like the start of your data set all the way to the end of your data",
    "start": "2272020",
    "end": "2277660"
  },
  {
    "text": "set with some probability um so ultimately it's still a pretty good shuffling option and with our",
    "start": "2277660",
    "end": "2283060"
  },
  {
    "text": "implementation it's it's like within like 10 15 of uh throughput of no shuffling at all which is a good bit",
    "start": "2283060",
    "end": "2289839"
  },
  {
    "text": "better than the distributed shuffling um so yeah this ultimately results in a trade-off between the randomness of your",
    "start": "2289839",
    "end": "2295900"
  },
  {
    "text": "Shuffle and the throughput of the shuffle finally a few quick case studies I know by the way I'm at time so I'm going to go through this a little quickly in case",
    "start": "2295900",
    "end": "2302500"
  },
  {
    "text": "you all are hungry um we have uh uh one example is distributed training at credit base",
    "start": "2302500",
    "end": "2308200"
  },
  {
    "text": "which I think is over there what's up um yeah they uh they found their training to be ingest bound and their",
    "start": "2308200",
    "end": "2314619"
  },
  {
    "text": "models are Shuffle sensitive so they are wanting uh to try and uh they're wanting",
    "start": "2314619",
    "end": "2320260"
  },
  {
    "text": "to still try a high have a high quality Shuffle while also being able to still saturate their gpus they don't want",
    "start": "2320260",
    "end": "2325839"
  },
  {
    "text": "those pesky data prep stalls so the previous solution that they had was uh pandas based pre-processing then written",
    "start": "2325839",
    "end": "2333640"
  },
  {
    "text": "to S3 loaded with pedestorm and ingested in a horribad the ray-based solution was porting that",
    "start": "2333640",
    "end": "2340599"
  },
  {
    "text": "Panda space pre-processing the desk on Ray get rid of the S3 intermediate persistence and then ingest with Ray",
    "start": "2340599",
    "end": "2346780"
  },
  {
    "text": "data sets into raytrain and they ultimately saw eight times faster uh throughput even on small data and",
    "start": "2346780",
    "end": "2354099"
  },
  {
    "text": "cluster scales and so for example this was one example of like the small Benchmark with like a five gigabyte",
    "start": "2354099",
    "end": "2359500"
  },
  {
    "text": "subset of the New York City Taxi data set on a single instance even at small data scales you see a pretty good",
    "start": "2359500",
    "end": "2365920"
  },
  {
    "text": "performance Improvement and then this just gets larger at large data scales so for example here's one of those large",
    "start": "2365920",
    "end": "2371859"
  },
  {
    "text": "data scales we have worked with a large transport tech company who shall for now",
    "start": "2371859",
    "end": "2377079"
  },
  {
    "text": "remain nameless that they were suffering the same issue of ingest bound training",
    "start": "2377079",
    "end": "2382839"
  },
  {
    "text": "and their models are Shuffle sensitive their previous solution was data was an S3 let's load it with pedestorm into",
    "start": "2382839",
    "end": "2388780"
  },
  {
    "text": "horovod Ray's solution was still persisting the S3 because they had a large ETL job that they uh that could",
    "start": "2388780",
    "end": "2396040"
  },
  {
    "text": "run for quite a while so they still wanted that s three persistence and then in just three data sets and use rate",
    "start": "2396040",
    "end": "2402579"
  },
  {
    "text": "train for distributed training and the end result was that the rate data sets ingest was four times faster than pedestorm from S3 and even better",
    "start": "2402579",
    "end": "2409599"
  },
  {
    "text": "because of that high quality Shuffle they saw uh approximately one percent uh",
    "start": "2409599",
    "end": "2414640"
  },
  {
    "text": "reduction in mean absolute error and for this model it was actually pretty significant and they are now looking at",
    "start": "2414640",
    "end": "2421240"
  },
  {
    "text": "rolling this out to like a lot more of their internal models and uh yeah so we ultimately created a synthetic version",
    "start": "2421240",
    "end": "2428440"
  },
  {
    "text": "of their data set um and uh did basically like simulated their setup and we got 4X improved",
    "start": "2428440",
    "end": "2435820"
  },
  {
    "text": "Aggregate throughput and this is while doing a much more expensive Shuffle than uh pedestorm was doing this is doing a",
    "start": "2435820",
    "end": "2442900"
  },
  {
    "text": "distributed all to all Shuffle while pedestorm was just doing a local Shuffle so faster and also better Shuffle",
    "start": "2442900",
    "end": "2448599"
  },
  {
    "text": "quality you know everyone's happy and then finally another another example",
    "start": "2448599",
    "end": "2453820"
  },
  {
    "text": "from predabase they were also they also use data sets for distributed batch inference",
    "start": "2453820",
    "end": "2459400"
  },
  {
    "text": "um their issue was that it was not scalable and they were seeing a lot of idle gpus they were using pandas for",
    "start": "2459400",
    "end": "2465520"
  },
  {
    "text": "pre-processing and then torch for doing batch inference previously now they ported to desk on Ray Plus data sets and",
    "start": "2465520",
    "end": "2472180"
  },
  {
    "text": "torch for doing pre-processing and batch inference and it was 5x faster even on",
    "start": "2472180",
    "end": "2477700"
  },
  {
    "text": "small data and cluster scales and so here's another example in that same five gigabyte subset on a single instance as",
    "start": "2477700",
    "end": "2484420"
  },
  {
    "text": "you can see task on Ray plus Ray data sets is a good bit faster there we're showing throughput",
    "start": "2484420",
    "end": "2490780"
  },
  {
    "text": "and then finally a quick testimonial from an ml engineer that's actually using uh that was using Ray air as part",
    "start": "2490780",
    "end": "2497260"
  },
  {
    "start": "2491000",
    "end": "2491000"
  },
  {
    "text": "of like the alpha release of air primarily they uh have seen a huge",
    "start": "2497260",
    "end": "2502359"
  },
  {
    "text": "Improvement in their developer experience they think that the abstractions such as data sets and raytrain are super intuitive and the big",
    "start": "2502359",
    "end": "2508839"
  },
  {
    "text": "kicker was in two weeks they were able to recreate and outperform a data ingest pipeline that they had built bespoke by",
    "start": "2508839",
    "end": "2514960"
  },
  {
    "text": "hand over the course of six months so you're seeing an improvement in developer ux the time to actually",
    "start": "2514960",
    "end": "2521200"
  },
  {
    "text": "deliver a solution it's really quick to try out it's pretty quick to Port most like",
    "start": "2521200",
    "end": "2526480"
  },
  {
    "text": "existing solutions to Air and data sets and yeah and they achieved better performance than their existing solution",
    "start": "2526480",
    "end": "2533020"
  },
  {
    "text": "so reduction of tech debt reduction of uh of uh time to ingest everyone's once",
    "start": "2533020",
    "end": "2539320"
  },
  {
    "text": "again hopefully pretty happy and so a quick summary wrap up of the talk",
    "start": "2539320",
    "end": "2544599"
  },
  {
    "start": "2542000",
    "end": "2542000"
  },
  {
    "text": "um scaling ml pipelines is hard Ray air makes it pretty simple",
    "start": "2544599",
    "end": "2549660"
  },
  {
    "text": "ingest is a common bottleneck as data volume scales in particular leading to",
    "start": "2549660",
    "end": "2555520"
  },
  {
    "text": "idle gpus and then Ray air uses data sets to fix a few of these problems with features such as distributed data",
    "start": "2555520",
    "end": "2561940"
  },
  {
    "text": "sharding fast I O and pre-processing smart resilient data plane and then finally pipelining of CPU and GPU",
    "start": "2561940",
    "end": "2568599"
  },
  {
    "text": "compute oh yeah and also efficient and scalable pre-pack shuffling can't forget that one and uh yeah so this is a quick",
    "start": "2568599",
    "end": "2575079"
  },
  {
    "text": "teaser for ongoing and future work um we're working uh we're planning to work we're working on actively working",
    "start": "2575079",
    "end": "2580960"
  },
  {
    "text": "on all these things a few that I would call out is uh faster tablet pre-processing",
    "start": "2580960",
    "end": "2587020"
  },
  {
    "text": "um with distributed polars I saw 20x improvement over using Arrow as the",
    "start": "2587020",
    "end": "2592240"
  },
  {
    "text": "compute engine for for example for a group by and aggregations by switching to polars I saw 20x Improvement and",
    "start": "2592240",
    "end": "2598900"
  },
  {
    "text": "throughput on a bunch of different data scales which is exciting hoping to get that merged soon we're going to be adding support for joins for NLP and CV",
    "start": "2598900",
    "end": "2605619"
  },
  {
    "text": "preprocessors looking at data connectors looking at supporting elastic training a",
    "start": "2605619",
    "end": "2612280"
  },
  {
    "text": "graph no network training like pipelining device transfers as well",
    "start": "2612280",
    "end": "2617619"
  },
  {
    "text": "as the rest of pre-processing and then finally we're looking at an automatic ml pipeline optimization AKA like a query",
    "start": "2617619",
    "end": "2623560"
  },
  {
    "text": "pipeline for your entire end-to-end ml pipeline uh thank you that was my talk and uh",
    "start": "2623560",
    "end": "2630160"
  },
  {
    "text": "yeah I'm happy to stick around answer questions or we can talk outside uh but I won't blame you if you want to go get",
    "start": "2630160",
    "end": "2635800"
  },
  {
    "text": "lunch I know this went a little over but uh thank you [Applause]",
    "start": "2635800",
    "end": "2643289"
  }
]