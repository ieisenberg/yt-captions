[
  {
    "text": "[Applause] cool yeah let's get started um I'm sham I'm at any scale I'm a software engineer",
    "start": "3710",
    "end": "8880"
  },
  {
    "text": "this is Bruce and uh we're going to talk about kind of the architecture that we work with at any",
    "start": "8880",
    "end": "15039"
  },
  {
    "text": "scale so just to set some context at any scale we are managing millions of",
    "start": "15039",
    "end": "21480"
  },
  {
    "text": "concurrent cores CPUs and gpus every day it's over 10,000 Ray clusters thousands",
    "start": "21480",
    "end": "28279"
  },
  {
    "text": "of developers and we're doing this globally um on many different providers and many different Cloud regions um",
    "start": "28279",
    "end": "34840"
  },
  {
    "text": "today we're going to kind of talk about our different deployment architectures so we Deploy on cloud VMS we Deploy on",
    "start": "34840",
    "end": "40160"
  },
  {
    "text": "AWS gcp Azure a lot of different Cloud providers we Deploy on Prem and we also",
    "start": "40160",
    "end": "45760"
  },
  {
    "text": "Deploy on kubernetes so we have a lot of flexibility in how we deploy and we'll walk through the architecture that we've used to kind of build out these these",
    "start": "45760",
    "end": "52079"
  },
  {
    "text": "deployment schemes so to start out let's talk about our kind of first offering that we built",
    "start": "52079",
    "end": "58480"
  },
  {
    "text": "out which is this native cloud provider supports um and so this is directly leveraging Cloud BMS to kind of do our",
    "start": "58480",
    "end": "64239"
  },
  {
    "text": "orchestration and run Ray",
    "start": "64239",
    "end": "67680"
  },
  {
    "text": "clusters so kind of the most basic unit of compute that we get from these clouds is the VM right you can use the ec2 API",
    "start": "69560",
    "end": "76920"
  },
  {
    "text": "or the GC API and kind of acquire an instance and run whatever you want on the instance um and so we started with",
    "start": "76920",
    "end": "84240"
  },
  {
    "text": "this as our foundational architecture we kind of have an instance manager in our control plane which kind of runs an any",
    "start": "84240",
    "end": "90119"
  },
  {
    "text": "skills accounts and a uh demon in the data plane which runs on the instance when we launch the instance and the two",
    "start": "90119",
    "end": "97280"
  },
  {
    "text": "of these kind of work together such that we can run Ray clusters uh where the demon is essentially managing the",
    "start": "97280",
    "end": "102640"
  },
  {
    "text": "container and the instance manager is managing the instance directly working with VMS gives",
    "start": "102640",
    "end": "108640"
  },
  {
    "text": "us like really high throughput capabilities because we are only constrained by the cloud apis so you",
    "start": "108640",
    "end": "114439"
  },
  {
    "text": "know if you want to scale up like thousands of nodes and minutes we can do that as long as ec2 or GCE gives us capacity to do",
    "start": "114439",
    "end": "121719"
  },
  {
    "text": "this because we're working with the cloud apis we also get really intelligent failures so we can we can",
    "start": "121719",
    "end": "127640"
  },
  {
    "text": "kind of look at the error messages that the cloud apis are giving us so if it's like a you know out of capacity error or",
    "start": "127640",
    "end": "133319"
  },
  {
    "text": "this Zone doesn't have enough quoda or something like that we can kind of take these into account and make better",
    "start": "133319",
    "end": "138959"
  },
  {
    "text": "decisions in our autoscaling and our scheduling layers in the data plane we also working",
    "start": "138959",
    "end": "145480"
  },
  {
    "text": "directly with VMS can do things like really fine grain cgroup isolation so in Ray one of the most common kind of",
    "start": "145480",
    "end": "152319"
  },
  {
    "text": "issues that you run into at scale is that your workload kind of knocks out some of the ray system processes that you want to you know keep alive at all",
    "start": "152319",
    "end": "159159"
  },
  {
    "text": "times so this is like the ray worker process might knock out the rayet for example what we can do at any scale is",
    "start": "159159",
    "end": "165680"
  },
  {
    "text": "essentially set cgroups and protect our both the ray components and the any scale system components such that the",
    "start": "165680",
    "end": "171360"
  },
  {
    "text": "workload will never take down the kind of system components and orchestration can stay alive even if the application is",
    "start": "171360",
    "end": "177640"
  },
  {
    "text": "crushed another thing that we can do is is detect GPU failures we can you know",
    "start": "177640",
    "end": "182720"
  },
  {
    "text": "probe the gpus look for health checks make sure that we cycle them out when when those fail we can do dis pressure",
    "start": "182720",
    "end": "189040"
  },
  {
    "text": "eviction and spot preemption so we kind of have all of these features built out over you know over the years as we've seen different kinds of failures and and",
    "start": "189040",
    "end": "195159"
  },
  {
    "text": "failure cases and and we've dealt with all of",
    "start": "195159",
    "end": "199040"
  },
  {
    "text": "these so like I said before working directly with the VM API gives us like really high scaling capabilities and so",
    "start": "200640",
    "end": "207680"
  },
  {
    "text": "just this past few months we've been able to reach 800,000 CPU cores in a single Ray",
    "start": "207680",
    "end": "213879"
  },
  {
    "text": "cluster so that is like 8 to 10 8 to 10,000 nodes in a single Ray",
    "start": "213879",
    "end": "219319"
  },
  {
    "text": "cluster uh working with you know directly with the ray core team on on scaling GCS and scaling the ray",
    "start": "219319",
    "end": "224840"
  },
  {
    "text": "dashboard in addition to working with our control plane to kind of scale our instance management and our data plane capabilities kind of holistically",
    "start": "224840",
    "end": "233319"
  },
  {
    "text": "um and you know when you start hitting this kind of scale 10,000 nodes in a",
    "start": "233319",
    "end": "238680"
  },
  {
    "text": "single cluster you run into kind of different problems as well so one of these is that you have to go across multiple node types multiple zones",
    "start": "238680",
    "end": "245480"
  },
  {
    "text": "potentially Market types and some of our customers are opinionated about the",
    "start": "245480",
    "end": "250840"
  },
  {
    "text": "shapes of instances that work better in you know these workloads um but they do need to try many different types of",
    "start": "250840",
    "end": "256880"
  },
  {
    "text": "instances and so we have this group ranking kind of feature that we built out where autoscaler is able to take",
    "start": "256880",
    "end": "262639"
  },
  {
    "text": "into account user preferences when they're autoscaling and we can say you know always prefer the kind of St",
    "start": "262639",
    "end": "270080"
  },
  {
    "text": "standard uh High CPU options that gcp provides and when we've exhausted those",
    "start": "270080",
    "end": "276440"
  },
  {
    "text": "then fall back to like the disc the ones that come with attached discs which are more expensive slightly more expensive",
    "start": "276440",
    "end": "282000"
  },
  {
    "text": "but you know like at this kind of scale that few extra cents there on the dollar won't really make a difference and so",
    "start": "282000",
    "end": "288320"
  },
  {
    "text": "they want to fall back to those um kind of other instance",
    "start": "288320",
    "end": "293880"
  },
  {
    "text": "types and the other kind of really useful feature here is being able to go backwards you know when when those",
    "start": "293880",
    "end": "299080"
  },
  {
    "text": "cheaper in es open up or there's capacity for those we want to go back and evict the kind of like more",
    "start": "299080",
    "end": "304360"
  },
  {
    "text": "expensive ones and so we kind of do that as well another thing that we run into at",
    "start": "304360",
    "end": "312400"
  },
  {
    "text": "800,000 CPU scale is you know we have these kind of really Edge case uh ec2",
    "start": "312400",
    "end": "317840"
  },
  {
    "text": "failures that we see um and this you know GC has their own their own class of failures um but this is something like",
    "start": "317840",
    "end": "323880"
  },
  {
    "text": "the instance has some Hardware failure that you know it's not the user's fault that this happened it's just something",
    "start": "323880",
    "end": "329080"
  },
  {
    "text": "happened at the layer and you know we need to kind of surface this to the user",
    "start": "329080",
    "end": "334759"
  },
  {
    "text": "um and so this is something that we added recently where we monitor for these types of failures and we surface them to the ray application so that the",
    "start": "334759",
    "end": "341000"
  },
  {
    "text": "developer knows why did my Ray application crash why did my process",
    "start": "341000",
    "end": "347039"
  },
  {
    "text": "crash so that's kind of like the basic uh VM offering and recently we've also",
    "start": "351759",
    "end": "356960"
  },
  {
    "text": "expanded this into supporting multi-provider and multi-region deployments as",
    "start": "356960",
    "end": "363160"
  },
  {
    "text": "well so at Large Scale some of you may have seen that you you know a single region or a single zone may not be",
    "start": "364199",
    "end": "370639"
  },
  {
    "text": "enough and you need to go across the region boundary and potentially even across the provider boundary to find the",
    "start": "370639",
    "end": "376280"
  },
  {
    "text": "compute that you need and if you've heard of Sky pilot",
    "start": "376280",
    "end": "381400"
  },
  {
    "text": "that's kind of an open- Source uh solution to this they've been kind of working on this in the open source we've",
    "start": "381400",
    "end": "386800"
  },
  {
    "text": "also been working on this on the any scale product where we can kind of take this in in two",
    "start": "386800",
    "end": "392680"
  },
  {
    "text": "different uh approaches that we that we've we've done one is kind of the cloud and Resource Management side of",
    "start": "392680",
    "end": "398280"
  },
  {
    "text": "things so when you go from one region one provider to end regions and end providers you have to deal with a lot of cloud resources and so we've kind of",
    "start": "398280",
    "end": "404199"
  },
  {
    "text": "abstracted some of that away made it really easy for you to go from one to",
    "start": "404199",
    "end": "409520"
  },
  {
    "text": "many and at the Autos scaling layer we also need to be able to make Intelligent Decisions and you know find the regions",
    "start": "409560",
    "end": "416319"
  },
  {
    "text": "and providers that have capacity and you know have Global views of which ones do and we kind of manage that layer as",
    "start": "416319",
    "end": "424000"
  },
  {
    "text": "well and you know kind of going a little deeper on that sometimes like a single workload itself needs to scale across",
    "start": "430720",
    "end": "437720"
  },
  {
    "text": "the region boundary um let's say that you're running this really big job you're out of gpus in one region you",
    "start": "437720",
    "end": "443000"
  },
  {
    "text": "need to burst to multiple regions and so at this point you need to take the ray cluster itself and make that go across",
    "start": "443000",
    "end": "448319"
  },
  {
    "text": "the the boundary and so that's also something that we've done uh where you know your head no might be running in Us",
    "start": "448319",
    "end": "455840"
  },
  {
    "text": "East 2 on AWS and maybe you need the special type of GPU that's only available at the time in US West or Us",
    "start": "455840",
    "end": "461879"
  },
  {
    "text": "East 5 in in gcp um and so something that we can do is we can use overlay",
    "start": "461879",
    "end": "467280"
  },
  {
    "text": "networks uh which is this kind of networking abstraction over the VPC",
    "start": "467280",
    "end": "472400"
  },
  {
    "text": "to make the connectivity work between these nodes and you know if we tweak a",
    "start": "472400",
    "end": "477560"
  },
  {
    "text": "few other attributes at the ray Cor layer and the scheduling layer we can get this really nice capability that",
    "start": "477560",
    "end": "483400"
  },
  {
    "text": "allows us to burst Beyond a single VPC and a single private Network and go into multip multiple places and so two",
    "start": "483400",
    "end": "491199"
  },
  {
    "text": "particular strategic tweaks that we made when we did this is that we disabled head node scheduling which is really",
    "start": "491199",
    "end": "496479"
  },
  {
    "text": "important to essentially prevent a lot of inter inter region Network traffic and we also added more",
    "start": "496479",
    "end": "503199"
  },
  {
    "text": "intelligent kind of pack strategies in our Autos scaling and so one of the things that we do is we say you know",
    "start": "503199",
    "end": "508240"
  },
  {
    "text": "maybe this set of workers has to be packed together uh in a particular zone or a particular region you know but they",
    "start": "508240",
    "end": "515240"
  },
  {
    "text": "can be somewhere different than the head node they don't have to be in the same place as the head node because we know as as the application Builders we know",
    "start": "515240",
    "end": "520919"
  },
  {
    "text": "that this group of workers kind of has a lot of inter node connectivity and and we don't want to have that connectivity",
    "start": "520919",
    "end": "526800"
  },
  {
    "text": "go over the region boundary and so this gives us really good abilities to give people the power to try out multiple",
    "start": "526800",
    "end": "533760"
  },
  {
    "text": "accelerators and and kind of use gpus that are in lots of different places",
    "start": "533760",
    "end": "540560"
  },
  {
    "text": "and so when you cross the provider boundary you run into this other issue of you know like now these users are dealing with both AWS gcp potentially",
    "start": "540800",
    "end": "547839"
  },
  {
    "text": "other providers as well and you have all these instance types and so from a developer experience Point of View kind",
    "start": "547839",
    "end": "553680"
  },
  {
    "text": "of remembering that you know a1g on one provider versus another provider is",
    "start": "553680",
    "end": "559519"
  },
  {
    "text": "called something slightly different is kind of hard um and so we have this kind of accelerator driven autoscaling where",
    "start": "559519",
    "end": "565640"
  },
  {
    "text": "we have in the any scale UI you just check this box the Auto Select worker nodes box and under the hood we've",
    "start": "565640",
    "end": "571839"
  },
  {
    "text": "mapped you know a lot of accelerators and different kind of preferences for instances into the actual instance types",
    "start": "571839",
    "end": "576880"
  },
  {
    "text": "and the N users don't actually care about the underlying types we on the infra side of things kind of abstract",
    "start": "576880",
    "end": "582320"
  },
  {
    "text": "all of that",
    "start": "582320",
    "end": "584720"
  },
  {
    "text": "away and another thing that we've kind of seen kind of extending this multicloud multi- region",
    "start": "588640",
    "end": "593880"
  },
  {
    "text": "Universe recently is that some of our customers have on Prime deployments as well",
    "start": "593880",
    "end": "599920"
  },
  {
    "text": "so we kind of want to go into this territory so we built any skill machine",
    "start": "599920",
    "end": "605279"
  },
  {
    "text": "pools to kind of address this the motivation is pretty simple it's you know these businesses want to use like cheaper Hardware some sometimes",
    "start": "605279",
    "end": "612240"
  },
  {
    "text": "it's our customers sometimes it's ourselves internally you know our own development teams want to use h100s that",
    "start": "612240",
    "end": "617279"
  },
  {
    "text": "are on a third party provider or you know on Prem um",
    "start": "617279",
    "end": "622800"
  },
  {
    "text": "and they're saying how do we use these on any scale how do we integrate these with the rest of our Ray deployments you",
    "start": "622800",
    "end": "628120"
  },
  {
    "text": "know that are running in these different places and so when you kind of move into the on-prem territory you have to deal with",
    "start": "628120",
    "end": "635160"
  },
  {
    "text": "a separate set of infrastructure challenges that weren't kind of existing before in the VM world one of them is",
    "start": "635160",
    "end": "640880"
  },
  {
    "text": "this you know the main one is this life cycle management it's it's the the fact that you now go from having the cloud",
    "start": "640880",
    "end": "647079"
  },
  {
    "text": "provider take the instance and kind of reset all the state to us having to do that on the infrastructure layer so we",
    "start": "647079",
    "end": "653560"
  },
  {
    "text": "have to do all the State Management another is that when we run on top of customer bare metal where",
    "start": "653560",
    "end": "659959"
  },
  {
    "text": "there's a lot of security requirements that come by they say hey you can't run as root um you know so we have to have this root list support something like",
    "start": "659959",
    "end": "666839"
  },
  {
    "text": "that and when you are using physical Hardware that's kind of in the office or in the data center you have this",
    "start": "666839",
    "end": "673519"
  },
  {
    "text": "resource sharing issue you know there's a fixed number of boxes you can't really autoscale and so you want to essentially",
    "start": "673519",
    "end": "679959"
  },
  {
    "text": "create some priority preemption between different workloads so we've kind of built support for all three of these things",
    "start": "679959",
    "end": "687880"
  },
  {
    "text": "and the way that we kind of plug this into our existing architecture is relatively simple we took the kind of",
    "start": "689360",
    "end": "695839"
  },
  {
    "text": "cloud apis that the fundamental Cloud apis for VM management which is kind of like create node delete node and list",
    "start": "695839",
    "end": "702079"
  },
  {
    "text": "nodes and created a abstraction over the physical bare metal machine that allowed",
    "start": "702079",
    "end": "707920"
  },
  {
    "text": "us to implement this these abstractions with our own kind of management layer on",
    "start": "707920",
    "end": "713000"
  },
  {
    "text": "top of the on top of the machine itself",
    "start": "713000",
    "end": "717440"
  },
  {
    "text": "now I'll hand it over to Bruce to talk about our last deployment which is kind of on top of",
    "start": "720680",
    "end": "725839"
  },
  {
    "text": "kubernetes sence show so after ill has already successfully support the fancy",
    "start": "725839",
    "end": "731440"
  },
  {
    "text": "features we mentioned in BM deack and uh also for machine pool we start to explore another big area of the product",
    "start": "731440",
    "end": "738600"
  },
  {
    "text": "which is uh kuum stack so we started looking at the OSI solution which is cubr and uh people can run their cubr",
    "start": "738600",
    "end": "746440"
  },
  {
    "text": "operator in their kuber environments and can manage the reclass on top of it and we summarize several good advantages of",
    "start": "746440",
    "end": "753519"
  },
  {
    "text": "cubr first it's very easy to install an onboard you just need a h chart command",
    "start": "753519",
    "end": "758600"
  },
  {
    "text": "it will be set up and second is a native Comm support it's just a Comm operator",
    "start": "758600",
    "end": "763639"
  },
  {
    "text": "manage the CRS inside the Comm environment so third it can coexist with other resources other name space and",
    "start": "763639",
    "end": "770480"
  },
  {
    "text": "application together and the last cubr is installed in user data plane so it's",
    "start": "770480",
    "end": "775760"
  },
  {
    "text": "provide very like a good control and high security so we considering that we should extend all those advantages and",
    "start": "775760",
    "end": "782800"
  },
  {
    "text": "merge it with all the property features that ncope provide so here we introduce",
    "start": "782800",
    "end": "788440"
  },
  {
    "text": "Nale operator so on top of Nale operator we can Al we can it can manage hundreds",
    "start": "788440",
    "end": "793880"
  },
  {
    "text": "or thousands whatever number of re clusters and it combin it with all the",
    "start": "793880",
    "end": "799040"
  },
  {
    "text": "fancy features of NQ provide such as manage the log Matrix GP house check a",
    "start": "799040",
    "end": "806839"
  },
  {
    "text": "and all support for R dashboard and and for service manage the GCS for Torrance which can survive even a hand crash and",
    "start": "806839",
    "end": "814199"
  },
  {
    "text": "also reble with a lot of good features of let's say fast machine learning uh model download uh compact scheduling for",
    "start": "814199",
    "end": "822199"
  },
  {
    "text": "reserve and all those properity features provided by the retal and also very uh it's has better scale and better",
    "start": "822199",
    "end": "829000"
  },
  {
    "text": "performance an under nsco operator is powered by n platform it has all the",
    "start": "829000",
    "end": "834959"
  },
  {
    "text": "properity features like prisent log history and it's go ser is h r out andco",
    "start": "834959",
    "end": "841079"
  },
  {
    "text": "console UI SDK and CLI and also it has good ability include graphon of AR level",
    "start": "841079",
    "end": "848920"
  },
  {
    "text": "cloud level worklow level you can do cost management you can do admin control",
    "start": "848920",
    "end": "854040"
  },
  {
    "text": "you can do permission control you can do budget usage and everything so let's see",
    "start": "854040",
    "end": "859199"
  },
  {
    "text": "another figure so this one shows the cube Ray can be installed in any kinds of Comm environment which include eks",
    "start": "859199",
    "end": "865959"
  },
  {
    "text": "gke AKs and self hosed communities and C Operator just run in the namespace and",
    "start": "865959",
    "end": "872160"
  },
  {
    "text": "together work together with the R Auto scaler to manage the parts it coexists with third party applications and when",
    "start": "872160",
    "end": "878720"
  },
  {
    "text": "we introduce an operator it look like this it just replace the CU operator in",
    "start": "878720",
    "end": "884639"
  },
  {
    "text": "the same cting environment can run on top of any cting environment and what we",
    "start": "884639",
    "end": "889839"
  },
  {
    "text": "do is we also move the smart component like re a scaler to N scale control pin",
    "start": "889839",
    "end": "895000"
  },
  {
    "text": "which is the smarter part provide all the properity feature we just mentioned and an operator can also coexist with",
    "start": "895000",
    "end": "902480"
  },
  {
    "text": "all the other thirdparty applications let's take a deeper look first how can we install nsco operator",
    "start": "902480",
    "end": "909880"
  },
  {
    "text": "it's just a super easy two step first register an nsco Cloud it will give you",
    "start": "909880",
    "end": "915480"
  },
  {
    "text": "some output of the metadata and then you run the H command to install n operator",
    "start": "915480",
    "end": "922000"
  },
  {
    "text": "then it's done the hum command will create an assign the Nam space in your commes like the red figure show dco",
    "start": "922000",
    "end": "929920"
  },
  {
    "text": "operator will manage only the resources within the work workspace and it provide",
    "start": "929920",
    "end": "935199"
  },
  {
    "text": "very good isolation coexist with other names space and the resources and anco operator will start to have allb calls",
    "start": "935199",
    "end": "942319"
  },
  {
    "text": "to anesco control plan which provide all the smartness provide good operations",
    "start": "942319",
    "end": "947600"
  },
  {
    "text": "for cluster orchestration decisions and anco operator will excluse all the operations in the data pin and this",
    "start": "947600",
    "end": "954920"
  },
  {
    "text": "traffic is er only and even then the connection between control pin and nsco",
    "start": "954920",
    "end": "960319"
  },
  {
    "text": "operator is temporar broken it's fine nsco operator can keep managing all the resources in the data pin keep them",
    "start": "960319",
    "end": "966720"
  },
  {
    "text": "running as this and once the connection is re reestablished nsco operator can",
    "start": "966720",
    "end": "973120"
  },
  {
    "text": "consume can resume all the operations it need to do so now let's go to a lab demo",
    "start": "973120",
    "end": "979360"
  },
  {
    "text": "to see how the nsco operator can be installed and works so it has three steps first we will show how smooth to",
    "start": "979360",
    "end": "986680"
  },
  {
    "text": "instore andore operator and and then we will show you some customized config map that you can use to make it very",
    "start": "986680",
    "end": "993560"
  },
  {
    "text": "flexible and last we will show some real workload a half row out Canary service",
    "start": "993560",
    "end": "999519"
  },
  {
    "text": "with two versions and also a workspace with a stable diffusion workload in that",
    "start": "999519",
    "end": "1005000"
  },
  {
    "text": "so let's go to the demo so now assume I already have my aure AKs set up so here is the nsco UI",
    "start": "1005000",
    "end": "1013120"
  },
  {
    "text": "what it look like and here you can click and see the list of the cloud and then let's go to the terminal here today I",
    "start": "1013120",
    "end": "1019959"
  },
  {
    "text": "want to demo and create this as my new name uh new Cloud name so I will copy",
    "start": "1019959",
    "end": "1026319"
  },
  {
    "text": "the cloud register command to my terminal and just run it and during it",
    "start": "1026319",
    "end": "1032038"
  },
  {
    "text": "run let me explain what will happen so here the command first you need to",
    "start": "1032039",
    "end": "1037280"
  },
  {
    "text": "include the cloud name and then because it can support all kinds of environment so we say the cloud provider is generic",
    "start": "1037280",
    "end": "1044760"
  },
  {
    "text": "and this is for CET stack not VM stack and uh we will assign it a c namespace",
    "start": "1044760",
    "end": "1050600"
  },
  {
    "text": "the namespace is same as the cloud and I also put the IP address of my INR",
    "start": "1050600",
    "end": "1056120"
  },
  {
    "text": "controller which I used for excise my services and note it can be a private IP",
    "start": "1056120",
    "end": "1061840"
  },
  {
    "text": "in private VPC or public depends on your setup and once the command finish the",
    "start": "1061840",
    "end": "1067200"
  },
  {
    "text": "important output is this Cloud deployment ID so the second command I've",
    "start": "1067200",
    "end": "1072640"
  },
  {
    "text": "copy here is this home instore so I just uh replace",
    "start": "1072640",
    "end": "1079640"
  },
  {
    "text": "this placeholder with the cloud deployment ID output from the previous",
    "start": "1079640",
    "end": "1086760"
  },
  {
    "text": "command and just run it it will install the N operator and create a",
    "start": "1086760",
    "end": "1093039"
  },
  {
    "text": "namespace let for it to finish so here it's done it's super fast and then when",
    "start": "1093039",
    "end": "1099240"
  },
  {
    "text": "we see the command namespace we can see this is a new namespace we just a create and let's see what's inside",
    "start": "1099240",
    "end": "1107720"
  },
  {
    "text": "we can see there is already running the NCO operator and go to the UI page we can see this new cloud is appear and you",
    "start": "1109720",
    "end": "1117320"
  },
  {
    "text": "can switch to that and start use it and to save us time I already start the workloads in my the a cloud that create",
    "start": "1117320",
    "end": "1124760"
  },
  {
    "text": "this morning and in the UI you can see that NX provide you all the selection",
    "start": "1124760",
    "end": "1130240"
  },
  {
    "text": "for the types that you just defined and you can also do this Auto Select Auto Select noes I will demo it later so",
    "start": "1130240",
    "end": "1137640"
  },
  {
    "text": "let's go to my morning create a uh name space and see what's in there so I have a run script",
    "start": "1137640",
    "end": "1145159"
  },
  {
    "text": "to show uh the resources in this commes so first I want to show the instance",
    "start": "1145159",
    "end": "1150600"
  },
  {
    "text": "type config map so it's a place where you can Define all the instance shapes you have it's very similar to cubr part",
    "start": "1150600",
    "end": "1157159"
  },
  {
    "text": "shape but the cubr need Define it for any like a CR and here is a config map that is global reusable for example here",
    "start": "1157159",
    "end": "1165159"
  },
  {
    "text": "I add two special instance tab one for T4 which has has ACT type T4 and one for",
    "start": "1165159",
    "end": "1170880"
  },
  {
    "text": "atg so I want to specify this two uh two specific GPU type for me and then",
    "start": "1170880",
    "end": "1176679"
  },
  {
    "text": "another config map is the patches it give you very high flexibility that you can customize what nsq operator should",
    "start": "1176679",
    "end": "1183799"
  },
  {
    "text": "manage your Comm environment so let's just go to the two GPU Related Group so",
    "start": "1183799",
    "end": "1189720"
  },
  {
    "text": "first the semantics is you can write a selector it's just a like label selector that Kum provide so here I will say if I",
    "start": "1189720",
    "end": "1197320"
  },
  {
    "text": "go say this port is in at1g I want to add a node selector to put it in my node",
    "start": "1197320",
    "end": "1202480"
  },
  {
    "text": "pool which is the A10 node pool if it's T4 I want to put it in the T4 not poool",
    "start": "1202480",
    "end": "1208120"
  },
  {
    "text": "you can customize it in the UI or put it in the config map and then in my",
    "start": "1208120",
    "end": "1213760"
  },
  {
    "text": "namespace I already have three part running one for the workspace I want to demo and one for the primary version of",
    "start": "1213760",
    "end": "1219799"
  },
  {
    "text": "NX service and one for the canary version of NX service and uh we also set",
    "start": "1219799",
    "end": "1225039"
  },
  {
    "text": "up the Ingress for your service one for primary one for canary and three for the head note each one has one and then",
    "start": "1225039",
    "end": "1233080"
  },
  {
    "text": "let's go to the service page so this is a service I just show you uh it has two Port running each one for the headnote",
    "start": "1233080",
    "end": "1239679"
  },
  {
    "text": "and we can do the query and it will give you a URL that you can query it so here",
    "start": "1239679",
    "end": "1245640"
  },
  {
    "text": "I just copy it and see it gave me V2 results and V1",
    "start": "1245640",
    "end": "1250919"
  },
  {
    "text": "results it means it's from primary or from Canary and you can also put a special header which is Canary to say it",
    "start": "1250919",
    "end": "1257400"
  },
  {
    "text": "always go to the canary version is very useful when you test with cicd to verify the C version works as expected and",
    "start": "1257400",
    "end": "1264799"
  },
  {
    "text": "that's an EXO service then we go to demo for the workspace here I already start a",
    "start": "1264799",
    "end": "1271520"
  },
  {
    "text": "stable diffusion server and I just try to send a request to my server in the right side and uh when we go to the",
    "start": "1271520",
    "end": "1278840"
  },
  {
    "text": "dashboard we can see this stable diffusion actor is upscale from zero to one ricas and because I config hand node",
    "start": "1278840",
    "end": "1286120"
  },
  {
    "text": "with one GPU which is atg so can see the left side has a processing bar which is",
    "start": "1286120",
    "end": "1291240"
  },
  {
    "text": "generating the image with stable",
    "start": "1291240",
    "end": "1295039"
  },
  {
    "text": "diffusion and once it's finished we will see the figure just Auto",
    "start": "1296840",
    "end": "1303519"
  },
  {
    "text": "refreshed this is new generated image which with a cat on the Green Grass and",
    "start": "1304520",
    "end": "1310120"
  },
  {
    "text": "um also we want to demo that our Auto Select worker noes feature I already",
    "start": "1310120",
    "end": "1315840"
  },
  {
    "text": "configur my workspace say Auto Select workers so so what I want to do is I want to run a T4 task right now my",
    "start": "1315840",
    "end": "1321880"
  },
  {
    "text": "cluster does not have any T4 gpus and uh I already read a recode to",
    "start": "1321880",
    "end": "1326960"
  },
  {
    "text": "say it require a t so what happen if I just run it I run this Python and see it has some",
    "start": "1326960",
    "end": "1335919"
  },
  {
    "text": "log say oh right now we want to upscale some T4 machines and we can see it's",
    "start": "1335919",
    "end": "1341760"
  },
  {
    "text": "already launched on one node for T4 we can go to the read dashboard see",
    "start": "1341760",
    "end": "1348000"
  },
  {
    "text": "what happened here is the job running and you see it changed to two",
    "start": "1348000",
    "end": "1355120"
  },
  {
    "text": "GPU one for 8G one for T4 and you can also see the right side it become two",
    "start": "1355120",
    "end": "1360919"
  },
  {
    "text": "GPU and uh you can see it's running and idle because my job is very small and it",
    "start": "1360919",
    "end": "1366200"
  },
  {
    "text": "finished and if we wait for like about three or four minutes because the node is Idle it will auto down",
    "start": "1366200",
    "end": "1373520"
  },
  {
    "text": "scale um yeah so that's everything for the LA demo and let's go to the next",
    "start": "1373520",
    "end": "1381440"
  },
  {
    "text": "page so here we have we we will announce that NS is ready to supercharge your AI",
    "start": "1381679",
    "end": "1387960"
  },
  {
    "text": "platform we can support very well for VM stack on AWS and gcp we can also support",
    "start": "1387960",
    "end": "1393440"
  },
  {
    "text": "on premise machines with machine pools we also support all kinds of CR environments with NCO operator and very",
    "start": "1393440",
    "end": "1400480"
  },
  {
    "text": "smooth to instore and provide you even more powerful features than cubre so any",
    "start": "1400480",
    "end": "1406240"
  },
  {
    "text": "scale is a scalable and performance read cluster Management on any machine on any",
    "start": "1406240",
    "end": "1412600"
  },
  {
    "text": "platform um thanks everyone for joining the presentation and uh just want to highlight we also hering so if you're",
    "start": "1412600",
    "end": "1419400"
  },
  {
    "text": "interested in what we do in anale you can go to that URL and look for",
    "start": "1419400",
    "end": "1424520"
  },
  {
    "text": "positions all right we have five minutes for Q&A so if you have any question",
    "start": "1424520",
    "end": "1430360"
  },
  {
    "text": "please raise your hand and wait for me to bring the mic to you",
    "start": "1430360",
    "end": "1436000"
  },
  {
    "text": "oh uh I saw there was support for like a priority based uh preemption or or something like this um is that uh",
    "start": "1446320",
    "end": "1453400"
  },
  {
    "text": "preemption at uh whole job level or like at like node level can you like uh",
    "start": "1453400",
    "end": "1460360"
  },
  {
    "text": "borrow uh pieces of a of another job uh if necessary uh node level primarily at",
    "start": "1460360",
    "end": "1467080"
  },
  {
    "text": "least for our like infrastructure layer preemption sorry node level uh I don't know if my mic was on",
    "start": "1467080",
    "end": "1474360"
  },
  {
    "text": "um I believe we also are building in job level preemption as well for other shapes of workloads um but right now the",
    "start": "1474360",
    "end": "1481679"
  },
  {
    "text": "kind of most common use case we've seen has been at the note level cool",
    "start": "1481679",
    "end": "1487240"
  },
  {
    "text": "thanks any other",
    "start": "1488039",
    "end": "1491639"
  },
  {
    "text": "question uh do you require the control plane to be always accessible what happens if uh it's not reachable or",
    "start": "1494039",
    "end": "1501200"
  },
  {
    "text": "something like that and if it gets disconnected uh yeah for all Stacks the control plan is only for new operations",
    "start": "1501200",
    "end": "1508159"
  },
  {
    "text": "and if you have workflows running even if control plan is Crash the all the workflows will not be impact it's just",
    "start": "1508159",
    "end": "1513840"
  },
  {
    "text": "that you cannot submit new operations to your data pin for example on kubernetes if you have 10 R cluster running it's",
    "start": "1513840",
    "end": "1520120"
  },
  {
    "text": "keep running and it can like's say also execute and serve all the traffics and when the control plan come up you can",
    "start": "1520120",
    "end": "1527080"
  },
  {
    "text": "use ncco UI and SDK to maintain it maybe like say do the UPS scale down scale or",
    "start": "1527080",
    "end": "1532240"
  },
  {
    "text": "create new workflows yeah see we have next question hi um where the control PL is",
    "start": "1532240",
    "end": "1540320"
  },
  {
    "text": "deployed in which region can you repeat that uh where do",
    "start": "1540320",
    "end": "1546520"
  },
  {
    "text": "you deploy the where do we deoy yeah control plan um go ahead oh okay I mean",
    "start": "1546520",
    "end": "1551720"
  },
  {
    "text": "right now we deploying AWS um usb2 just like a",
    "start": "1551720",
    "end": "1557000"
  },
  {
    "text": "particular I chance uh but you know we have a kind of two phase control plan so we have like a",
    "start": "1557000",
    "end": "1563919"
  },
  {
    "text": "global control plane and we have a regional control plane so the regional control plan is something that we can deploy in any you know kind of region",
    "start": "1563919",
    "end": "1570520"
  },
  {
    "text": "across the globe so when we need to get closer to the actual customer we can do that as",
    "start": "1570520",
    "end": "1576080"
  },
  {
    "text": "well in whatever region full flexibility to deploy in whatever region uh yes I",
    "start": "1576080",
    "end": "1583120"
  },
  {
    "text": "the one thing I will say is that the control point right now is something that we deploy in any skilles account so it's not something that you would deploy",
    "start": "1583120",
    "end": "1588600"
  },
  {
    "text": "in euron but yeah we can deploy it in we can deploy our orchestration stuff in in different regions if we need to thank",
    "start": "1588600",
    "end": "1595640"
  },
  {
    "text": "you can take a few more questions without",
    "start": "1595640",
    "end": "1600520"
  },
  {
    "text": "here have you had any requests to uh deploy the uh any control plane in an air gapped",
    "start": "1601080",
    "end": "1608279"
  },
  {
    "text": "environment uh I will have to give that one to my product manager to answer D you want to take a question",
    "start": "1608279",
    "end": "1618120"
  },
  {
    "text": "have we gotten requests for it yes uh is it on our immediate road map no but we'd",
    "start": "1620039",
    "end": "1625399"
  },
  {
    "text": "love to have a conversation about the priorities and like what exactly uh would be involved",
    "start": "1625399",
    "end": "1631159"
  },
  {
    "text": "there uh so for the cross provider Ray architecture uh you mentioned that you disable head node uh scheduling is the",
    "start": "1636600",
    "end": "1642640"
  },
  {
    "text": "intent here really to partner to pair with the ray compile graph so that you basically avoid doing as much like head",
    "start": "1642640",
    "end": "1648360"
  },
  {
    "text": "worker communication is possible or like help me understand yeah yeah I think",
    "start": "1648360",
    "end": "1653440"
  },
  {
    "text": "disabling head node scheduling is something that we've been pushing for for a long time um especially with like",
    "start": "1653440",
    "end": "1659720"
  },
  {
    "text": "multi-node clusters and and and when you have like lots and lots of workers the main",
    "start": "1659720",
    "end": "1666720"
  },
  {
    "text": "goal without this cross provider stuff has typically been because of stability reasons like we want to keep GCS and the",
    "start": "1666720",
    "end": "1672840"
  },
  {
    "text": "ray dashboard very stable so we don't want to have worker processes interfering with them and the cross",
    "start": "1672840",
    "end": "1678440"
  },
  {
    "text": "provider case kind of the reason we do that is because the code has to run like",
    "start": "1678440",
    "end": "1683720"
  },
  {
    "text": "the the driver has to run somewhere and we have to be able to know what the workers need to to actually upscale them",
    "start": "1683720",
    "end": "1689320"
  },
  {
    "text": "and so we run the code on the head node like the the the actual like driver code um but the actual worker processes like",
    "start": "1689320",
    "end": "1695880"
  },
  {
    "text": "we want them to be all together and so we put them all into like the same region or the same same provider or whatever the constraint is on the on the",
    "start": "1695880",
    "end": "1702200"
  },
  {
    "text": "compute um and so that's kind of like why we just like we wouldn't want one worker to be over here and one worker to",
    "start": "1702200",
    "end": "1707240"
  },
  {
    "text": "be over there yeah um but in general like best practices are to disable head node scheduling in",
    "start": "1707240",
    "end": "1713760"
  },
  {
    "text": "multinode clusters like that's kind of what we suggest in our docs that's kind of what we're pushing for on any scale as well yeah got it thank",
    "start": "1713760",
    "end": "1721518"
  },
  {
    "text": "you um so hi thank you for the talk and can I assume like this is a uh kind of a",
    "start": "1721640",
    "end": "1729320"
  },
  {
    "text": "uh multicluster multi hybrate cloud my cluster and then I can uh you know",
    "start": "1729320",
    "end": "1734760"
  },
  {
    "text": "Federate my work clads ac across different types of uh uh like you know",
    "start": "1734760",
    "end": "1739840"
  },
  {
    "text": "uh uh uh uh like Cloud instances or like kubernetes environments yes okay so yeah yeah uh",
    "start": "1739840",
    "end": "1747679"
  },
  {
    "text": "and in in that case you know uh so for example if I can uh I can use it for",
    "start": "1747679",
    "end": "1753440"
  },
  {
    "text": "kind of the heterogeneous uh you know Cloud cases right uh in in that case is",
    "start": "1753440",
    "end": "1759080"
  },
  {
    "text": "that are you making open source version of this so we can adopt this in our own",
    "start": "1759080",
    "end": "1764440"
  },
  {
    "text": "uh Cloud setup uh maybe I should turn that to my product manager as well um I think Sky",
    "start": "1764440",
    "end": "1771399"
  },
  {
    "text": "pilot is probably like what a lot of people who are you know strongly into",
    "start": "1771399",
    "end": "1777000"
  },
  {
    "text": "the open source uh thing use um we uh yeah that's like I I would look at",
    "start": "1777000",
    "end": "1784799"
  },
  {
    "text": "sky pilot um yeah yeah",
    "start": "1784799",
    "end": "1788639"
  },
  {
    "text": "sure I mean any scale as a platform is designed to have a centralized control plan that can orchestrate Ray across",
    "start": "1791240",
    "end": "1798559"
  },
  {
    "text": "every cloud provider every location where you're running Ray uh ultimately our goal as a company is to provide the",
    "start": "1798559",
    "end": "1804840"
  },
  {
    "text": "best services for Ray that fits the workloads of of everyone uh if you love Ray we can do exist as a company though",
    "start": "1804840",
    "end": "1810679"
  },
  {
    "text": "so like we need something yeah I think the other thing I'll add is one of the",
    "start": "1810679",
    "end": "1816240"
  },
  {
    "text": "advantages of doing Global scheduling and Global Autos scaling is that we get Global Knowledge and so we know like",
    "start": "1816240",
    "end": "1823159"
  },
  {
    "text": "what capacity exists in what regions in a way that you can't really do if you have like separate open first deployments of Sky pallot across the",
    "start": "1823159",
    "end": "1829440"
  },
  {
    "text": "world um and so that's something that like we've actually found value in because like if we have two customers that are running in the same Cloud",
    "start": "1829440",
    "end": "1834679"
  },
  {
    "text": "region we know where there's capacity and where there isn't right we can take one more short",
    "start": "1834679",
    "end": "1842559"
  },
  {
    "text": "question by looks like we good okay if you're interested in trying out",
    "start": "1845720",
    "end": "1853000"
  },
  {
    "text": "any of these capabilities who do I reach out to this gentleman right here",
    "start": "1853000",
    "end": "1859880"
  },
  {
    "text": "all right if you have any more extra question you can ask sham and Bruce after the speak uh and thank you",
    "start": "1860840",
    "end": "1867279"
  },
  {
    "text": "everyone for coming please give now a round Applause for Bruce and Sh thank you",
    "start": "1867279",
    "end": "1874000"
  }
]