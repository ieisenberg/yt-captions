[
  {
    "text": "okay thank you thank you for the introduction uh Hey guys uh thanks for",
    "start": "2480",
    "end": "8000"
  },
  {
    "text": "making it to the last few sessions of the conference um I'm hu from Alibaba",
    "start": "8000",
    "end": "14040"
  },
  {
    "text": "cloud and today I'm more than happy to be here at race ammit U to introduce",
    "start": "14040",
    "end": "20080"
  },
  {
    "text": "project lnx and Report our experiences in optimizing llm inflence serving from",
    "start": "20080",
    "end": "26320"
  },
  {
    "text": "a cluster scheduling perspective okay let me begin with showing how LMS",
    "start": "26320",
    "end": "33280"
  },
  {
    "text": "are being served today from cluster perspective an LM a service is typically",
    "start": "33280",
    "end": "39040"
  },
  {
    "text": "deployed as multiple instances of the model run inside a certain INF engine like VM um with a request dispatcher top",
    "start": "39040",
    "end": "47440"
  },
  {
    "text": "routing the incoming requests to the instances uh for the infal layer we have",
    "start": "47440",
    "end": "54120"
  },
  {
    "text": "seen a lot of systems tailored for llms like VM or SG Lang and many other",
    "start": "54120",
    "end": "59440"
  },
  {
    "text": "systems and these systems shows you per performance inside a single",
    "start": "59440",
    "end": "64920"
  },
  {
    "text": "instance for the dispatcher people are still using systems or scheduling",
    "start": "64920",
    "end": "70200"
  },
  {
    "text": "policies that are inherited from the era of traditional DNS but not designed for",
    "start": "70200",
    "end": "76080"
  },
  {
    "text": "llms but the problem is LMS are different and require new design Philosophy for the scheduling",
    "start": "76080",
    "end": "83600"
  },
  {
    "text": "layer the first unique characteristic of LMS is that workloads are",
    "start": "83600",
    "end": "88640"
  },
  {
    "text": "heterogeneous because LMS are Universal models and can have diverse applications",
    "start": "88640",
    "end": "94280"
  },
  {
    "text": "and therefore heterogeneous requests first requests can have uh",
    "start": "94280",
    "end": "100159"
  },
  {
    "text": "different input and output lengths uh consider a simple intuitive example that",
    "start": "100159",
    "end": "105719"
  },
  {
    "text": "you want to use an LM to summarize an article or to to to write an article",
    "start": "105719",
    "end": "111200"
  },
  {
    "text": "from a simple prompt or to polish an article you've just written so these request will have drastic different",
    "start": "111200",
    "end": "117719"
  },
  {
    "text": "ranges of input and output length another aspect is latency SL slos for",
    "start": "117719",
    "end": "125079"
  },
  {
    "text": "example interactive applications like uh chatbot or uh P personal assistance",
    "start": "125079",
    "end": "131200"
  },
  {
    "text": "would expect lower latencies than um offline analysis tasks like summarize an",
    "start": "131200",
    "end": "137239"
  },
  {
    "text": "article a real product product example is that openi introduced a subscription",
    "start": "137239",
    "end": "143040"
  },
  {
    "text": "Plan called chat GPT plus uh which promises lower",
    "start": "143040",
    "end": "148360"
  },
  {
    "text": "latencies the the heterogeneity of requests uh becomes more challenging when combined with another",
    "start": "148720",
    "end": "155200"
  },
  {
    "text": "characteristic um namely the unpredictable Auto regressive execution",
    "start": "155200",
    "end": "160599"
  },
  {
    "text": "so during the auto regressive execution the final output lengths are not known a prior uh which also comes with GPU",
    "start": "160599",
    "end": "168000"
  },
  {
    "text": "memory demands of KV caches that dynamically grow with the sequences state of state-of-the-art",
    "start": "168000",
    "end": "175360"
  },
  {
    "text": "systems adopt page detention to enable dynamic memory allocation for KV caches",
    "start": "175360",
    "end": "181440"
  },
  {
    "text": "and combine it with preemptive scheduling because you know if you cannot know the final KV cach size in",
    "start": "181440",
    "end": "188239"
  },
  {
    "text": "the beginning then you might have to preempt certain quests when go when you run run out of the",
    "start": "188239",
    "end": "195560"
  },
  {
    "text": "memory these characteristics introduce a series of scheduling challenges uh that",
    "start": "195560",
    "end": "200840"
  },
  {
    "text": "affect the performance and efficiency of align Services the first is the performance",
    "start": "200840",
    "end": "207280"
  },
  {
    "text": "isolation um preemptions are inevitable when you use dynamic memory allocation",
    "start": "207280",
    "end": "212879"
  },
  {
    "text": "and it can lead to poor tail lences and SLO violations because the preempted",
    "start": "212879",
    "end": "218040"
  },
  {
    "text": "requests will have to go through another round of queueing even without preemptions",
    "start": "218040",
    "end": "224120"
  },
  {
    "text": "requests in the batch also have performance interference to each other so obviously we need load",
    "start": "224120",
    "end": "231239"
  },
  {
    "text": "balancing across instances to mitigate such issues but point is load balancing",
    "start": "231239",
    "end": "236560"
  },
  {
    "text": "via the one shot dispatching could be suboptimal because the sequence LS are not known in",
    "start": "236560",
    "end": "242319"
  },
  {
    "text": "the beginning that means we require continuous load balancing even after the",
    "start": "242319",
    "end": "248680"
  },
  {
    "text": "requests are dispatched is that enough no load",
    "start": "248680",
    "end": "255840"
  },
  {
    "text": "balancing also means memory fragmentation across instances because we tend to spread the cluster free",
    "start": "255840",
    "end": "262639"
  },
  {
    "text": "memory space over multiple instances actually this is a classic spreading o",
    "start": "262639",
    "end": "268800"
  },
  {
    "text": "versus packing tra PA off in scheduling problems fragmentation can lead to uh",
    "start": "268800",
    "end": "275240"
  },
  {
    "text": "worse queuing delays especially for the long inputs because the large space on one instance is needed for accommodating",
    "start": "275240",
    "end": "282520"
  },
  {
    "text": "the KV cach of the whole prompt to visualize this we did an experiment with",
    "start": "282520",
    "end": "287960"
  },
  {
    "text": "four instances using a spreading policy for request dispatching we plot the",
    "start": "287960",
    "end": "293440"
  },
  {
    "text": "memory demand uh of the head of line queueing request on each instance",
    "start": "293440",
    "end": "298560"
  },
  {
    "text": "against the total memory space in the cluster for most of time span the total",
    "start": "298560",
    "end": "304600"
  },
  {
    "text": "memory space can satisfy the queueing requests on at least three instances but",
    "start": "304600",
    "end": "310039"
  },
  {
    "text": "they are still queing because the free space is fragmented this shows that Beyond load",
    "start": "310039",
    "end": "317000"
  },
  {
    "text": "balancing we also need ability to defragment the cluster's free memory",
    "start": "317000",
    "end": "324240"
  },
  {
    "text": "space a third challenge is to satisfy different latency slos of requests",
    "start": "324600",
    "end": "330280"
  },
  {
    "text": "existing systems treat all requests equally where the Urgent requests could easily be interfered by normal ones um",
    "start": "330280",
    "end": "337680"
  },
  {
    "text": "for example the they may experience excessive queing delays or performance interference when collocated with normal",
    "start": "337680",
    "end": "344560"
  },
  {
    "text": "normal requests so we also need request priorities to systematically",
    "start": "344560",
    "end": "349960"
  },
  {
    "text": "differentiate request SL slos these challenges are fundamentally",
    "start": "349960",
    "end": "357199"
  },
  {
    "text": "a result of the dynamic execution nature of LMS so we solve it by making request",
    "start": "357199",
    "end": "363960"
  },
  {
    "text": "scheduling also Dynamic lumx is a system that features continuous rescheduling of LM requests",
    "start": "363960",
    "end": "371560"
  },
  {
    "text": "across multiple model instances it is combined with the initial dispatching of requests and",
    "start": "371560",
    "end": "377520"
  },
  {
    "text": "instance Auto scaling we show that rescheduling is a powerful weapon in various scheduling",
    "start": "377520",
    "end": "385440"
  },
  {
    "text": "scenarios the first is rescheduling for continuous load balancing to reduce",
    "start": "385440",
    "end": "390840"
  },
  {
    "text": "preemptions and and interference which which is complimentary uh to the",
    "start": "390840",
    "end": "396400"
  },
  {
    "text": "dispatch time load balancing to tame the trade-off between",
    "start": "396400",
    "end": "401680"
  },
  {
    "text": "load balancing and fragmentation lomnick also reschedules requests away from from",
    "start": "401680",
    "end": "407840"
  },
  {
    "text": "an instance to create free space for defragmentation to reduce queueing",
    "start": "407840",
    "end": "414800"
  },
  {
    "text": "delays the third scenario is rescheduling requests away from high priority requests to further reduce",
    "start": "414879",
    "end": "422400"
  },
  {
    "text": "performance interference and accelerate the execution of the high priority",
    "start": "422400",
    "end": "428960"
  },
  {
    "text": "requests finally um finally lumni also reschedules requests away from an",
    "start": "429280",
    "end": "435000"
  },
  {
    "text": "instance to create free space for oh sorry uh rescheduled request to satate",
    "start": "435000",
    "end": "441199"
  },
  {
    "text": "or Trin out instances more quickly during",
    "start": "441199",
    "end": "445840"
  },
  {
    "text": "autoscaling so the aim of lumni is to really utilize fully Dynamic scheduling",
    "start": "446520",
    "end": "451680"
  },
  {
    "text": "and make rescheduling the norm not the exception in llm serving to this end we",
    "start": "451680",
    "end": "457720"
  },
  {
    "text": "need to accomplish a series of design goals we need to maximize the efficiency",
    "start": "457720",
    "end": "463520"
  },
  {
    "text": "of the rescheduling so that we can uh reschedule much more aggressively and more",
    "start": "463520",
    "end": "469159"
  },
  {
    "text": "frequently we also need high scalability of cluster level scheduler and the",
    "start": "469159",
    "end": "474479"
  },
  {
    "text": "scheduling policy that maximizes the scheduling benefits of rescheduling for VAR scheduling",
    "start": "474479",
    "end": "481000"
  },
  {
    "text": "goals I'll first introduce the live migration mechanism in in",
    "start": "481000",
    "end": "486199"
  },
  {
    "text": "lumni so the major major concern with rescheduling is the KV cach States uh",
    "start": "486199",
    "end": "492240"
  },
  {
    "text": "you could either recompute or copy the KV cache to the new instance but these approaches will introduce downtimes to",
    "start": "492240",
    "end": "498960"
  },
  {
    "text": "the rescheduled requests for the recomputing or memory copying and also performance overhead due to the",
    "start": "498960",
    "end": "505400"
  },
  {
    "text": "Redundant computation of recomputing and more importantly these downtimes and",
    "start": "505400",
    "end": "510560"
  },
  {
    "text": "overheads will will will increase with sequence LS by contrast Lam's life migration",
    "start": "510560",
    "end": "517959"
  },
  {
    "text": "mechanism has near zero downtime and over overhead by Design the inspiration is virtual",
    "start": "517959",
    "end": "525200"
  },
  {
    "text": "machine live migration when you want to migrate VM you keep the source VM",
    "start": "525200",
    "end": "531120"
  },
  {
    "text": "online um during which you intuitively copy the dirty Pages namely the pages",
    "start": "531120",
    "end": "536760"
  },
  {
    "text": "that got changed to the destination VM when all the 30 pages got synced uh then",
    "start": "536760",
    "end": "543399"
  },
  {
    "text": "then Source VM stops the migration is committed and the new VM becomes",
    "start": "543399",
    "end": "549399"
  },
  {
    "text": "online well to apply this technique to LM serving we can liken the VMS to",
    "start": "549399",
    "end": "555200"
  },
  {
    "text": "requests and memory pages to the KV blocks now the question is what are the",
    "start": "555200",
    "end": "560800"
  },
  {
    "text": "30 pages so the key characteristic of llm",
    "start": "560800",
    "end": "566800"
  },
  {
    "text": "inference here is that K caches are a and only that means there are no dirty",
    "start": "566800",
    "end": "572640"
  },
  {
    "text": "Pages or dirty blocks and there are only incremental blocks that are appended to",
    "start": "572640",
    "end": "577680"
  },
  {
    "text": "the cache as the decoding proceeds the live migration in lumni",
    "start": "577680",
    "end": "582880"
  },
  {
    "text": "Works in multiple stages where we iteratively copy the incremental blocks generated in the last stage and for the",
    "start": "582880",
    "end": "590320"
  },
  {
    "text": "first stage there are blocks generated before the migration until we reach a stage that only produces the minimum",
    "start": "590320",
    "end": "596600"
  },
  {
    "text": "number of new blocks uh namely the stage end in the graph so we suspend the",
    "start": "596600",
    "end": "602519"
  },
  {
    "text": "request copy the last box and resume the request on new instance so this way the migration",
    "start": "602519",
    "end": "609720"
  },
  {
    "text": "downtime is only that of the final copying which is near near zero and constant to the sequence",
    "start": "609720",
    "end": "617360"
  },
  {
    "text": "L um with live migration being the mechanism Foundation I'll next show you",
    "start": "618040",
    "end": "623120"
  },
  {
    "text": "how lumx improve the scalability and scheduling benefits of",
    "start": "623120",
    "end": "628320"
  },
  {
    "text": "migration continuous rescheduling across instances also implies higher scheduling",
    "start": "628320",
    "end": "633839"
  },
  {
    "text": "pressure than in traditional schedules which could be a bottleneck if implemented in the centralized",
    "start": "633839",
    "end": "640720"
  },
  {
    "text": "component lumni uses a distributor scheduling architecture that employs a global scheduler for cross instance",
    "start": "640720",
    "end": "647680"
  },
  {
    "text": "scheduling decisions while using a set of distributed schedulers named the Lums",
    "start": "647680",
    "end": "652959"
  },
  {
    "text": "collocate with the model instances to handle intra instance scheduling in this uh in this",
    "start": "652959",
    "end": "659560"
  },
  {
    "text": "architecture the global scheduler makes all the scheduling decisions only based on the memory loads of the instances",
    "start": "659560",
    "end": "666800"
  },
  {
    "text": "reported by Lums it does not need to check the status of every single running",
    "start": "666800",
    "end": "672480"
  },
  {
    "text": "request which further reduces its pressure a key enabler of the global",
    "start": "672480",
    "end": "679760"
  },
  {
    "text": "schedule not caring about the specific running requests is our scheduling policy which translates the various",
    "start": "679760",
    "end": "686240"
  },
  {
    "text": "scheduling objectives to a simple instance load metric we achiev this with an abstraction",
    "start": "686240",
    "end": "692600"
  },
  {
    "text": "called the virtual usage the intuition is that the various goals are all trying to manipulate the",
    "start": "692600",
    "end": "699360"
  },
  {
    "text": "loads of the instances you either balance them or you create more free space on certain",
    "start": "699360",
    "end": "706600"
  },
  {
    "text": "instances and they can actually be unified into load balancing because for creating free space we can simply assume",
    "start": "706600",
    "end": "714360"
  },
  {
    "text": "a virtual load on that instance so distance will uh be virtually overloaded",
    "start": "714360",
    "end": "721000"
  },
  {
    "text": "and the load balancing policy will be triggered to migrate some running requests to other",
    "start": "721000",
    "end": "727560"
  },
  {
    "text": "instances so lumx uses a load balancing policy based on Virtual usages and",
    "start": "727560",
    "end": "732959"
  },
  {
    "text": "Define the rules for setting virtual usages in different cases uh for example",
    "start": "732959",
    "end": "738720"
  },
  {
    "text": "when we have a queueing request um lumx assumes a positive virtual usage of it",
    "start": "738720",
    "end": "744199"
  },
  {
    "text": "although the physical usage is actually zero so that we can migrate other requests away",
    "start": "744199",
    "end": "749680"
  },
  {
    "text": "which in effect is defragmentation and this approach also works for other scenarios um you can",
    "start": "749680",
    "end": "756560"
  },
  {
    "text": "refer to our research paper um published at osdi 24 for the",
    "start": "756560",
    "end": "762959"
  },
  {
    "text": "details uh lumni was originally a research project so we built a prot",
    "start": "763279",
    "end": "768399"
  },
  {
    "text": "prototype implementation to evaluate the effectiveness of our our idea it was",
    "start": "768399",
    "end": "774240"
  },
  {
    "text": "implemented on as a scheding layer on top of multiple instances of of",
    "start": "774240",
    "end": "779720"
  },
  {
    "text": "VM we evaluate it on the cluster with 16 A10 gpus using popular llama",
    "start": "779720",
    "end": "786560"
  },
  {
    "text": "models we compare the serving performance of lumni with infas a system",
    "start": "786560",
    "end": "792079"
  },
  {
    "text": "that implements explicit load balancing for dispatching lumx achieves up to 2.2",
    "start": "792079",
    "end": "798399"
  },
  {
    "text": "times and 5.5 times gains for mean and P99 first token lences or ttft via",
    "start": "798399",
    "end": "805839"
  },
  {
    "text": "defragmentation to reduce curing delays lumx also shows up to 1.3 times shorter",
    "start": "805839",
    "end": "813720"
  },
  {
    "text": "P99 per token generation latencies or TBT by reducing preemptions with",
    "start": "813720",
    "end": "820000"
  },
  {
    "text": "migration on average lumni reduces Laten penalties cost by preemptions by 73%",
    "start": "820000",
    "end": "827320"
  },
  {
    "text": "across these experiments um due to the time Li limit I'll refer you to our paper for other",
    "start": "827320",
    "end": "833480"
  },
  {
    "text": "results like benchmarks of the migration efficiency other traces priorities and",
    "start": "833480",
    "end": "841040"
  },
  {
    "text": "scaling okay now that you've got the main idea of lomnick and in the rest of",
    "start": "842199",
    "end": "847519"
  },
  {
    "text": "my talk let's talk about implementation over the past few months we have been working on the Practical",
    "start": "847519",
    "end": "855079"
  },
  {
    "text": "product level implementation to bring this idea into",
    "start": "855079",
    "end": "860199"
  },
  {
    "text": "production well the key requirement in implementing lumni is to enable fine",
    "start": "860639",
    "end": "865880"
  },
  {
    "text": "grain interaction between the scheduler and the instances and also interaction between each",
    "start": "865880",
    "end": "872240"
  },
  {
    "text": "instance the scheduler um in it it it does not only need to dispatch requests",
    "start": "872240",
    "end": "878839"
  },
  {
    "text": "but it also needs to uh it needs other Advanced interfaces from the engines like migrating requests or curing the",
    "start": "878839",
    "end": "886040"
  },
  {
    "text": "instance loads as defined by our scheduling policy for the instance uh",
    "start": "886040",
    "end": "891320"
  },
  {
    "text": "for the instances they also need the ability to talk to each other um to enable",
    "start": "891320",
    "end": "896560"
  },
  {
    "text": "migration however in existing serving structure it's difficult to realize our",
    "start": "896560",
    "end": "901720"
  },
  {
    "text": "goal the dispatcher does not have the advanced interfaces and instances are",
    "start": "901720",
    "end": "907040"
  },
  {
    "text": "totally isolated and cannot communicate with each other so conventional systems wi wisdom",
    "start": "907040",
    "end": "915120"
  },
  {
    "text": "tells us if you cannot find a proper layer then just add an interaction layer right so we've chosen to implement lumni",
    "start": "915120",
    "end": "923480"
  },
  {
    "text": "as an interaction layer between the external dispatcher and the backhand engines",
    "start": "923480",
    "end": "929199"
  },
  {
    "text": "lumx is compatible with external dispatchers because it exposes the same interfaces like inference engines namely",
    "start": "929199",
    "end": "937079"
  },
  {
    "text": "API end points alumx is collocated with inference engines in the ray cluster and",
    "start": "937079",
    "end": "943759"
  },
  {
    "text": "it manages components at re reactors so in this way um we can enable fine grain",
    "start": "943759",
    "end": "949440"
  },
  {
    "text": "communication between lumni and inference engines um in a simple and efficient",
    "start": "949440",
    "end": "956680"
  },
  {
    "text": "manner um lumx Pro Dev API endpoints via a component called the request",
    "start": "957120",
    "end": "963040"
  },
  {
    "text": "proxy uh in lumni a request might execute on different instances in its life cycle right because it could be",
    "start": "963040",
    "end": "970199"
  },
  {
    "text": "migrated but we need to ensure that um even even the request is migrated his",
    "start": "970199",
    "end": "975639"
  },
  {
    "text": "output uh his output tokens should be returned to users through the same API",
    "start": "975639",
    "end": "980880"
  },
  {
    "text": "endpoint so we bind each request to a proxy uh no matter which instance the",
    "start": "980880",
    "end": "986319"
  },
  {
    "text": "request is is running on and implement distribute token forwarding to send the",
    "start": "986319",
    "end": "991759"
  },
  {
    "text": "generated tokens back to the proxy the proxy provides a user",
    "start": "991759",
    "end": "997680"
  },
  {
    "text": "interface that is very similar to existing systems like VM so it is quite",
    "start": "997680",
    "end": "1002759"
  },
  {
    "text": "easy to migrate from an existing VM deployments to uh to lumx and you just",
    "start": "1002759",
    "end": "1008480"
  },
  {
    "text": "need to change the path of the API server command lumx is also compatible with",
    "start": "1008480",
    "end": "1016120"
  },
  {
    "text": "different backend engines using a a common abstraction in a lumet in principle the major Assumption",
    "start": "1016120",
    "end": "1023759"
  },
  {
    "text": "of Lum about the backends is the is continuous batching with Dynamic me",
    "start": "1023759",
    "end": "1029160"
  },
  {
    "text": "memory allocation like page tension which has been the def facto standard uh of inference",
    "start": "1029160",
    "end": "1035558"
  },
  {
    "text": "engines so we have abstracted the common interfaces from the backend engines and",
    "start": "1035559",
    "end": "1041199"
  },
  {
    "text": "Implement these interfaces uh with a wrapper for each back end so in this way",
    "start": "1041199",
    "end": "1046640"
  },
  {
    "text": "the lumet can integrate with different engines and currently lumni supports VM",
    "start": "1046640",
    "end": "1052520"
  },
  {
    "text": "and blade llm which is alibaba's internal inference",
    "start": "1052520",
    "end": "1058039"
  },
  {
    "text": "engine another important uh Topic in implementing alumni is for tolerance for",
    "start": "1058840",
    "end": "1065400"
  },
  {
    "text": "tolerance is non tral in our scenario uh when when the instances are all isolated",
    "start": "1065400",
    "end": "1071080"
  },
  {
    "text": "from each other then they are also naturally fult isolated but in lumni it",
    "start": "1071080",
    "end": "1076640"
  },
  {
    "text": "it is not the case because we introduce much much more fine grain communication between the each component so failure of",
    "start": "1076640",
    "end": "1084120"
  },
  {
    "text": "one component or one one instance um if not handled properly could impact other",
    "start": "1084120",
    "end": "1089799"
  },
  {
    "text": "components uh or even overall service availability um let's first talk about",
    "start": "1089799",
    "end": "1096760"
  },
  {
    "text": "schedule failures so uh currently the schedule is on the critical path of dispatching each request so if the",
    "start": "1096760",
    "end": "1104280"
  },
  {
    "text": "schedule fails alumic will temporarily enter a schedule by passing mode where",
    "start": "1104280",
    "end": "1110240"
  },
  {
    "text": "each proxy directly dispatches each request to lumet and the migration is",
    "start": "1110240",
    "end": "1115360"
  },
  {
    "text": "disabled so uh uh then the the scheduled actor will be restarted by Ray",
    "start": "1115360",
    "end": "1122240"
  },
  {
    "text": "automatically when a request proxy fails we will abort all requests that have",
    "start": "1122600",
    "end": "1127760"
  },
  {
    "text": "been associated with that proxy we note that uh these requests",
    "start": "1127760",
    "end": "1132919"
  },
  {
    "text": "could not only include those running in the lumet collocated with that proxy um but also those on other Lums because of",
    "start": "1132919",
    "end": "1141320"
  },
  {
    "text": "migration and similarly when the lumet fails we abort all the running requests",
    "start": "1141320",
    "end": "1146760"
  },
  {
    "text": "onet we also all the ongoing migrations on that lumet namely the",
    "start": "1146760",
    "end": "1152120"
  },
  {
    "text": "requests being migrated in or out on on that",
    "start": "1152120",
    "end": "1157279"
  },
  {
    "text": "plumet okay um one last topic I want to discuss is prefill decode",
    "start": "1158480",
    "end": "1165760"
  },
  {
    "text": "disaggregation I guess some of you guys might have already known this idea um actually many people are working on it",
    "start": "1165760",
    "end": "1172640"
  },
  {
    "text": "well basically PD disaggregation is to run prefill and decode computation on on",
    "start": "1172640",
    "end": "1178400"
  },
  {
    "text": "on separate instances and you will have prefill instances and decode instances",
    "start": "1178400",
    "end": "1183440"
  },
  {
    "text": "um this is mainly to eliminate potential interference between the two stages and also to utilize hetrogeneous Hardware",
    "start": "1183440",
    "end": "1190960"
  },
  {
    "text": "because the two stages have different uh resource requirements well today I'm not I'm not",
    "start": "1190960",
    "end": "1197640"
  },
  {
    "text": "going to discuss this idea itself but I'd like to share another view to consider this idea so how could we",
    "start": "1197640",
    "end": "1205480"
  },
  {
    "text": "Implement PD agregation one could implement it in inference engines right",
    "start": "1205480",
    "end": "1210679"
  },
  {
    "text": "you could label an instance as prefill or decode and execute different codes for different uh instance types but we",
    "start": "1210679",
    "end": "1218240"
  },
  {
    "text": "think uh that PD disaggregation is essentially a request scheduling policy",
    "start": "1218240",
    "end": "1224280"
  },
  {
    "text": "okay so forget about um the instance level Concepts like P instances or D",
    "start": "1224280",
    "end": "1229679"
  },
  {
    "text": "instances just think think of how a request is is scheduled on the PD",
    "start": "1229679",
    "end": "1235320"
  },
  {
    "text": "desegregation scheme U so a request first needs to be dispatched to a subset of instances",
    "start": "1235320",
    "end": "1242760"
  },
  {
    "text": "namely the so-called prefill instances and then it needs to be migrated to another set of of instances",
    "start": "1242760",
    "end": "1250280"
  },
  {
    "text": "uh namely the D instances after one step of execution that's it so in lumni we",
    "start": "1250280",
    "end": "1257480"
  },
  {
    "text": "implement the PD disaggregation policy simply using a special dispatching Rule",
    "start": "1257480",
    "end": "1263080"
  },
  {
    "text": "and a special migration Rule and we call it the scheduling defined PD",
    "start": "1263080",
    "end": "1269880"
  },
  {
    "text": "disaggregation what are the benefits well first this implementation reuses",
    "start": "1269919",
    "end": "1275520"
  },
  {
    "text": "most of the system level mechanisms of lumx that PD disaggregation also relies on for example the KV cach transfer um",
    "start": "1275520",
    "end": "1283799"
  },
  {
    "text": "the token forwarding because uh because in PD disaggregation you also need to",
    "start": "1283799",
    "end": "1288919"
  },
  {
    "text": "forward the tokens from the D instances to the initial API endpoint and also for Torance because",
    "start": "1288919",
    "end": "1295600"
  },
  {
    "text": "you need to handle P instances failures and D instance failures respectively and",
    "start": "1295600",
    "end": "1301000"
  },
  {
    "text": "all of these have been addressed by lumx with nearly no modifications",
    "start": "1301000",
    "end": "1306559"
  },
  {
    "text": "needed and another benefit is that it is nonintuitive to inference engines uh the",
    "start": "1306559",
    "end": "1312720"
  },
  {
    "text": "engines do not need to care about what type of instance it is and it just needs",
    "start": "1312720",
    "end": "1318200"
  },
  {
    "text": "to follow following the instruction from from alumx and the last and the most",
    "start": "1318200",
    "end": "1325039"
  },
  {
    "text": "important benefit is that PD disaggregation can similarly integrate with SK scheduling capabilities that",
    "start": "1325039",
    "end": "1331240"
  },
  {
    "text": "lumni already has for example for the initial dispatching we you need to",
    "start": "1331240",
    "end": "1336320"
  },
  {
    "text": "choose one p instance when migrating to the D instances you also need to choose",
    "start": "1336320",
    "end": "1341559"
  },
  {
    "text": "one D instance right and lumx already does this and also we can still use",
    "start": "1341559",
    "end": "1347960"
  },
  {
    "text": "lumni routine migration between these D instances because the problem we",
    "start": "1347960",
    "end": "1353080"
  },
  {
    "text": "mentioned before still exist in these D instances and M migration still works so",
    "start": "1353080",
    "end": "1359799"
  },
  {
    "text": "in lumni um the initial dispatching the initial migration and the routine",
    "start": "1359799",
    "end": "1365640"
  },
  {
    "text": "migration are naturally combined and can complement each other well we we are still uh currently",
    "start": "1365640",
    "end": "1372960"
  },
  {
    "text": "still active actively working on this feature and developing a benchmarking it",
    "start": "1372960",
    "end": "1380320"
  },
  {
    "text": "okay to conclude uh we believe that Dynamic work colds need Dynamic scheduling and LMS are no exception we",
    "start": "1380440",
    "end": "1388000"
  },
  {
    "text": "built lumx exactly following this principle the design of lumx draws",
    "start": "1388000",
    "end": "1393120"
  },
  {
    "text": "lessons from conventional systems wisdom um including definitions of classic scheduling goals in the new context of",
    "start": "1393120",
    "end": "1399760"
  },
  {
    "text": "llm serving um implementation of the rescheduling with the live migration",
    "start": "1399760",
    "end": "1405360"
  },
  {
    "text": "mechanism and fully continuous and dynamic DC rescheduling exploiting the",
    "start": "1405360",
    "end": "1411520"
  },
  {
    "text": "migration combined these techniques deliver better latencies and efficiency",
    "start": "1411520",
    "end": "1416760"
  },
  {
    "text": "of L&M serving lumni is open source and welcome to take a try okay I'll still stop here",
    "start": "1416760",
    "end": "1423919"
  },
  {
    "text": "and happy to take any questions okay thanks for the great talk",
    "start": "1423919",
    "end": "1429360"
  },
  {
    "text": "uh yeah",
    "start": "1429360",
    "end": "1433360"
  },
  {
    "text": "uh that was a great talk thank you um I'm curious when looking at the disaggregated serving um via dispatch",
    "start": "1439520",
    "end": "1447440"
  },
  {
    "text": "and migration how does a prefill instance",
    "start": "1447440",
    "end": "1453159"
  },
  {
    "text": "know the optimal decode instance to migrate to or how to kind of approach",
    "start": "1453159",
    "end": "1459520"
  },
  {
    "text": "that um so so the question is um when when we want to um migrate a request",
    "start": "1459520",
    "end": "1465760"
  },
  {
    "text": "from a prefill instance to a code instance uh how do we choose the the",
    "start": "1465760",
    "end": "1471840"
  },
  {
    "text": "optimal D instance right yes oh um uh okay that's a good question actually",
    "start": "1471840",
    "end": "1478520"
  },
  {
    "text": "when you think of this um when you migrate a request from a pref instance to a decode instance it's actually um",
    "start": "1478520",
    "end": "1487480"
  },
  {
    "text": "the same as you choose a initial dispatching instance when you do not",
    "start": "1487480",
    "end": "1493760"
  },
  {
    "text": "have the disagre disaggregation of p p and d right it's just another",
    "start": "1493760",
    "end": "1499480"
  },
  {
    "text": "dispatching so so so the logic is actually the same um when when we choose",
    "start": "1499480",
    "end": "1504640"
  },
  {
    "text": "a d instance we um normally choose the instance with the least least memory",
    "start": "1504640",
    "end": "1510440"
  },
  {
    "text": "load uh namely with the load balancing policy uh and we also have uh uh the",
    "start": "1510440",
    "end": "1517360"
  },
  {
    "text": "same uh same optimizations like defragmentation like I I I I I said",
    "start": "1517360",
    "end": "1522840"
  },
  {
    "text": "before for the D instances uh uh it's like um when when all the D",
    "start": "1522840",
    "end": "1529640"
  },
  {
    "text": "instances uh every instance has has has insufficient memory to to accommodate a",
    "start": "1529640",
    "end": "1536520"
  },
  {
    "text": "request from a p instance then we can we can trigger defragmentation we will try",
    "start": "1536520",
    "end": "1541600"
  },
  {
    "text": "to migrate some requests and uh make some room on One D instance and to accommodate the request from the P",
    "start": "1541600",
    "end": "1549200"
  },
  {
    "text": "instance I see and a related question um is is there an assumption that",
    "start": "1549200",
    "end": "1556120"
  },
  {
    "text": "instances have Total Access or unique access to like a device like a GPU and",
    "start": "1556120",
    "end": "1562919"
  },
  {
    "text": "it's not shared with other instances or if it is shared do you make decisions based on",
    "start": "1562919",
    "end": "1569440"
  },
  {
    "text": "um optimal location like staying on the same device versus transferring devices",
    "start": "1569440",
    "end": "1574600"
  },
  {
    "text": "versus transferring nodes Etc uh so so the question is uh the Assumption of the",
    "start": "1574600",
    "end": "1580600"
  },
  {
    "text": "inference engines to the hardware or gpus um actually that is um um it's like",
    "start": "1580600",
    "end": "1590559"
  },
  {
    "text": "uh lni a scheduling system and built on top inference engines and uh uh if the",
    "start": "1590559",
    "end": "1597200"
  },
  {
    "text": "if the INF engines need uh to access all the gpus then uh we do not add more",
    "start": "1597200",
    "end": "1604960"
  },
  {
    "text": "assumptions than the the the underlying inference engines so",
    "start": "1604960",
    "end": "1611559"
  },
  {
    "text": "yeah um yeah thanks a lot for the great talk kind of a related question I got the impression for the live migration",
    "start": "1612480",
    "end": "1618760"
  },
  {
    "text": "stuff you you do this sort of incremental transfer right where you can do the bulk while you're still",
    "start": "1618760",
    "end": "1624760"
  },
  {
    "text": "um generating decode tokens and then just when you get to the last one you then you say you minimize the sort of um",
    "start": "1624760",
    "end": "1632360"
  },
  {
    "text": "pause for a given sequence um you can't I mean you can't exploit that here",
    "start": "1632360",
    "end": "1637960"
  },
  {
    "text": "presumably for the prefill disaggregation because you have you got to transfer the whole thing you do you start decoding on the on the prefill um",
    "start": "1637960",
    "end": "1645919"
  },
  {
    "text": "instance and like and transfer while you're doing that or do you keep it exclusively prefill yes uh very good",
    "start": "1645919",
    "end": "1653200"
  },
  {
    "text": "question and actually uh in the current stage uh for for migrating a request",
    "start": "1653200",
    "end": "1658320"
  },
  {
    "text": "from a p to a d instance uh we justall a pause it uh and we do not apply the live",
    "start": "1658320",
    "end": "1665039"
  },
  {
    "text": "migration technique yeah and um actually we think that we can actually use live",
    "start": "1665039",
    "end": "1671840"
  },
  {
    "text": "migration because PD disaggregation does not need to be that strict um for",
    "start": "1671840",
    "end": "1678559"
  },
  {
    "text": "example um when you uh when you complete a a prefill step on the instance uh you",
    "start": "1678559",
    "end": "1685399"
  },
  {
    "text": "can just right you can just do another few rounds of decoding uh if if there are no",
    "start": "1685399",
    "end": "1693519"
  },
  {
    "text": "uh incoming requests so we are working on a policy that is smarter than that",
    "start": "1693519",
    "end": "1700279"
  },
  {
    "text": "can uh automatically uh turn uh turn between",
    "start": "1700279",
    "end": "1705440"
  },
  {
    "text": "the P and D modes for the same instance we think that would be better than than",
    "start": "1705440",
    "end": "1710720"
  },
  {
    "text": "the strict desegregation okay great thanks I had one other quick question um do you take like slos into account with",
    "start": "1710720",
    "end": "1717279"
  },
  {
    "text": "respect to your sort of Maximum latency so um schedule of being aware of the",
    "start": "1717279",
    "end": "1722480"
  },
  {
    "text": "batch size like sometimes you can you might pile uh more requests onto a",
    "start": "1722480",
    "end": "1728279"
  },
  {
    "text": "particular instance if they aren't latency sensitive right you to if you just those they care about throughput",
    "start": "1728279",
    "end": "1736760"
  },
  {
    "text": "um does that make sense um sorry I I I quite don't quite",
    "start": "1736760",
    "end": "1743320"
  },
  {
    "text": "understand um so you might have a mix of some requests might have a SLO where they they need a minimum sort of itl uh",
    "start": "1743320",
    "end": "1751039"
  },
  {
    "text": "and for that you might need to restrict the batch size so how many oh yes how many decode uh requests you have in a",
    "start": "1751039",
    "end": "1757480"
  },
  {
    "text": "batch on a particular instance and then others might not and so I just wondering if the scheduler can take those kind of",
    "start": "1757480",
    "end": "1762679"
  },
  {
    "text": "things into account um uh uh in our current implementation um we we provide",
    "start": "1762679",
    "end": "1769360"
  },
  {
    "text": "an interface for set setting uh the priority of each request um for example",
    "start": "1769360",
    "end": "1775279"
  },
  {
    "text": "if you mark a request as a high priority we will try to limit the batch size or",
    "start": "1775279",
    "end": "1781559"
  },
  {
    "text": "the the the to Total tokens of the running batch on that instance uh below",
    "start": "1781559",
    "end": "1787760"
  },
  {
    "text": "uh an upper limit uh that is one way to to to satisfy a certain level of SLO and",
    "start": "1787760",
    "end": "1795480"
  },
  {
    "text": "and certainly it can be improved okay thank you and thank you",
    "start": "1795480",
    "end": "1802640"
  },
  {
    "text": "just yeah I think we can have like one or two more questions I",
    "start": "1802640",
    "end": "1808760"
  },
  {
    "text": "think hi thank you thank you for the great insights uh I had a question about",
    "start": "1809000",
    "end": "1814840"
  },
  {
    "text": "scheduling Affinity so suppose you have a request which shares a prompt with a",
    "start": "1814840",
    "end": "1820440"
  },
  {
    "text": "subsequent request whose prefill will already already be done for the previous request does it have a routing affinity",
    "start": "1820440",
    "end": "1828320"
  },
  {
    "text": "where it will send a subsequent request which shares a significant amount of prompt to the same prefill server so",
    "start": "1828320",
    "end": "1835000"
  },
  {
    "text": "that it can reuse the KV cash oh so so you mean uh multi-term conversations",
    "start": "1835000",
    "end": "1840159"
  },
  {
    "text": "right yeah yeah yeah oh I see um yeah we actually we are we are now working on",
    "start": "1840159",
    "end": "1846559"
  },
  {
    "text": "this um that means when you uh because we this project is about a multi-",
    "start": "1846559",
    "end": "1853440"
  },
  {
    "text": "instance scheduling so uh in a multi- instance uh setup when you have enabled",
    "start": "1853440",
    "end": "1860120"
  },
  {
    "text": "uh prompt sharing then you need to consider this otherwise the The Prompt",
    "start": "1860120",
    "end": "1865159"
  },
  {
    "text": "caching prompt sharing could be uh could be useless you need to you need to know",
    "start": "1865159",
    "end": "1873919"
  },
  {
    "text": "uh where the prompt the KV cash of the previous request is on which instance",
    "start": "1873919",
    "end": "1880919"
  },
  {
    "text": "right um we are actually working on this um in in both dispatching requests and",
    "start": "1880919",
    "end": "1887399"
  },
  {
    "text": "also m requests uh yes I think it must be an important feature yeah so right now it's",
    "start": "1887399",
    "end": "1894480"
  },
  {
    "text": "stateless right then uh I'm sorry it's stat less as in it doesn't consider any",
    "start": "1894480",
    "end": "1900399"
  },
  {
    "text": "state from the previous request um I mean we are working on this",
    "start": "1900399",
    "end": "1906960"
  },
  {
    "text": "we are trying to uh consider the multi-term conversations and there are",
    "start": "1906960",
    "end": "1912080"
  },
  {
    "text": "Affinity uh in our scheduling policy uh does it answer your question",
    "start": "1912080",
    "end": "1917919"
  },
  {
    "text": "oh thank you okay uh our QA is kind of out of time but after the talk we can still like talk in person yeah thanks",
    "start": "1917919",
    "end": "1924559"
  },
  {
    "text": "for the great talk oh thank you guys [Applause]",
    "start": "1924559",
    "end": "1931190"
  }
]