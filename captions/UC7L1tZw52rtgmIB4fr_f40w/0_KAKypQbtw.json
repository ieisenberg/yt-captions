[
  {
    "text": "hi everyone uh my name is chany sh I'm a",
    "start": "3280",
    "end": "5799"
  },
  {
    "text": "machine learning engineer from Intel on",
    "start": "5799",
    "end": "8080"
  },
  {
    "text": "today I will introduce about running gen",
    "start": "8080",
    "end": "11200"
  },
  {
    "text": "AI workloads with Ray on Intel GDI",
    "start": "11200",
    "end": "16000"
  },
  {
    "text": "accelerator okay so there will be three",
    "start": "16600",
    "end": "19119"
  },
  {
    "text": "parts covered in this session uh firstly",
    "start": "19119",
    "end": "21760"
  },
  {
    "text": "I will go give a introduction of Intel",
    "start": "21760",
    "end": "25000"
  },
  {
    "text": "gy accelerator secondly I will present",
    "start": "25000",
    "end": "28279"
  },
  {
    "text": "how to enable Intel gy on Ray with Ray",
    "start": "28279",
    "end": "31400"
  },
  {
    "text": "cor Ray train and Ray serve then we will",
    "start": "31400",
    "end": "34600"
  },
  {
    "text": "dig into the details on how Intel gy",
    "start": "34600",
    "end": "37719"
  },
  {
    "text": "helps accelerate AI performance no",
    "start": "37719",
    "end": "40840"
  },
  {
    "text": "matter you are using you are doing a",
    "start": "40840",
    "end": "43440"
  },
  {
    "text": "training F tuning or serving with",
    "start": "43440",
    "end": "46039"
  },
  {
    "text": "reserve of",
    "start": "46039",
    "end": "48800"
  },
  {
    "text": "VM so let's start with the introduction",
    "start": "49719",
    "end": "53440"
  },
  {
    "text": "of",
    "start": "53440",
    "end": "54920"
  },
  {
    "text": "inteli so what is Intel gudi Intel gy AI",
    "start": "54920",
    "end": "59039"
  },
  {
    "text": "accelerator uh is designed from the grup",
    "start": "59039",
    "end": "62519"
  },
  {
    "text": "for AI training and inference so there",
    "start": "62519",
    "end": "65518"
  },
  {
    "text": "are three main subsystem on including",
    "start": "65519",
    "end": "69280"
  },
  {
    "text": "the computer units memory units and",
    "start": "69280",
    "end": "72280"
  },
  {
    "text": "networking so for the computer design",
    "start": "72280",
    "end": "75240"
  },
  {
    "text": "GDI is built with heterogeneous design",
    "start": "75240",
    "end": "78119"
  },
  {
    "text": "with mme and the TPC and I think if you",
    "start": "78119",
    "end": "82040"
  },
  {
    "text": "look at the page here uh it's about part",
    "start": "82040",
    "end": "85040"
  },
  {
    "text": "said gen engine which is mme and",
    "start": "85040",
    "end": "88560"
  },
  {
    "text": "underneath is a TPC so what is mme mme",
    "start": "88560",
    "end": "92159"
  },
  {
    "text": "is a matrix multiplication engine which",
    "start": "92159",
    "end": "95520"
  },
  {
    "text": "is responsible for all the linear",
    "start": "95520",
    "end": "97880"
  },
  {
    "text": "operations such as fully connected",
    "start": "97880",
    "end": "100360"
  },
  {
    "text": "layers or convolution layers and a TPC",
    "start": "100360",
    "end": "104159"
  },
  {
    "text": "is a programmable tensor processor core",
    "start": "104159",
    "end": "107200"
  },
  {
    "text": "for nonlinear operations such as like uh",
    "start": "107200",
    "end": "110840"
  },
  {
    "text": "spatial pooling or batch",
    "start": "110840",
    "end": "112920"
  },
  {
    "text": "normalization so the good side is that",
    "start": "112920",
    "end": "116119"
  },
  {
    "text": "on the software side user can also",
    "start": "116119",
    "end": "118680"
  },
  {
    "text": "develop their kernel runs on TPC and",
    "start": "118680",
    "end": "123200"
  },
  {
    "text": "also we provide a predefined TPC kernels",
    "start": "123200",
    "end": "126320"
  },
  {
    "text": "which is fully optimized with Intel's",
    "start": "126320",
    "end": "129440"
  },
  {
    "text": "GDI software suit I will mention later",
    "start": "129440",
    "end": "132680"
  },
  {
    "text": "for the memory design GDI provides SRAM",
    "start": "132680",
    "end": "136160"
  },
  {
    "text": "a TPC local memory and the hbm device",
    "start": "136160",
    "end": "139400"
  },
  {
    "text": "memory for the network G use Rocky",
    "start": "139400",
    "end": "142599"
  },
  {
    "text": "version 2 RDMA which allows linear",
    "start": "142599",
    "end": "145599"
  },
  {
    "text": "scalability both scale up and scale out",
    "start": "145599",
    "end": "151160"
  },
  {
    "text": "so released in quarter 3 2024 Intel",
    "start": "151959",
    "end": "155280"
  },
  {
    "text": "introduced the G 3 with the next level",
    "start": "155280",
    "end": "157959"
  },
  {
    "text": "of AI performance and the power",
    "start": "157959",
    "end": "160040"
  },
  {
    "text": "efficiency advancing from the previous",
    "start": "160040",
    "end": "162840"
  },
  {
    "text": "Intel gy 2 accelerator which is 7even",
    "start": "162840",
    "end": "165879"
  },
  {
    "text": "nanometer process the Intel G3",
    "start": "165879",
    "end": "168440"
  },
  {
    "text": "accelerator is manufactured in five",
    "start": "168440",
    "end": "170519"
  },
  {
    "text": "nanometer process which provides",
    "start": "170519",
    "end": "172840"
  },
  {
    "text": "improved area density and a power",
    "start": "172840",
    "end": "175760"
  },
  {
    "text": "efficiency so as you can see here for",
    "start": "175760",
    "end": "179080"
  },
  {
    "text": "the computer side we now have8 mme Compu",
    "start": "179080",
    "end": "183480"
  },
  {
    "text": "units and the 684 fifth generation TBC",
    "start": "183480",
    "end": "187879"
  },
  {
    "text": "units and for the memory uh we provide",
    "start": "187879",
    "end": "191480"
  },
  {
    "text": "eight H uh sorry 16 hbm which provides",
    "start": "191480",
    "end": "195920"
  },
  {
    "text": "Su up as",
    "start": "195920",
    "end": "197680"
  },
  {
    "text": "128 gigabyte device memory which is",
    "start": "197680",
    "end": "200599"
  },
  {
    "text": "super large you can even run a 70 bit",
    "start": "200599",
    "end": "203360"
  },
  {
    "text": "model if you quanti this as in8 and with",
    "start": "203360",
    "end": "206879"
  },
  {
    "text": "96 megab Sr for the network uh we",
    "start": "206879",
    "end": "211439"
  },
  {
    "text": "provide sum up as 24 multiply 200 GBS",
    "start": "211439",
    "end": "215319"
  },
  {
    "text": "per second Network",
    "start": "215319",
    "end": "218840"
  },
  {
    "text": "benit okay so now let's talk about the",
    "start": "221000",
    "end": "224760"
  },
  {
    "text": "software so we have this powerful",
    "start": "224760",
    "end": "227360"
  },
  {
    "text": "Hardware but it's not imedia right so we",
    "start": "227360",
    "end": "230360"
  },
  {
    "text": "need to provide a software layer which",
    "start": "230360",
    "end": "232640"
  },
  {
    "text": "will help users to enable the efficiency",
    "start": "232640",
    "end": "236400"
  },
  {
    "text": "mapping of neural network topologies",
    "start": "236400",
    "end": "239280"
  },
  {
    "text": "onto gy Hardware so on the top we",
    "start": "239280",
    "end": "242319"
  },
  {
    "text": "provide py torch Bridge which similarly",
    "start": "242319",
    "end": "245239"
  },
  {
    "text": "connects module executor codes onto the",
    "start": "245239",
    "end": "247799"
  },
  {
    "text": "Gia round time we for the round time we",
    "start": "247799",
    "end": "250519"
  },
  {
    "text": "provide like lazy mode eager mode and",
    "start": "250519",
    "end": "253120"
  },
  {
    "text": "torch compile mode which is really",
    "start": "253120",
    "end": "255319"
  },
  {
    "text": "popular this year and underneath the pie",
    "start": "255319",
    "end": "257919"
  },
  {
    "text": "chge is a graph compiler in around time",
    "start": "257919",
    "end": "261359"
  },
  {
    "text": "so this is responsible for the",
    "start": "261359",
    "end": "264080"
  },
  {
    "text": "pipelining between mme TBC and DME all",
    "start": "264080",
    "end": "267919"
  },
  {
    "text": "the computer units I just talk about",
    "start": "267919",
    "end": "269880"
  },
  {
    "text": "before uh as a hardware and also",
    "start": "269880",
    "end": "272560"
  },
  {
    "text": "responsible for the execution ex",
    "start": "272560",
    "end": "274720"
  },
  {
    "text": "scheduling graph sanitization note",
    "start": "274720",
    "end": "277840"
  },
  {
    "text": "extraction and kernel",
    "start": "277840",
    "end": "279680"
  },
  {
    "text": "optimization then the GDI uh Intel gudi",
    "start": "279680",
    "end": "282919"
  },
  {
    "text": "software suit also provides uh its own",
    "start": "282919",
    "end": "285720"
  },
  {
    "text": "communication libraries called hio hccl",
    "start": "285720",
    "end": "289919"
  },
  {
    "text": "and underneath are user mode driver and",
    "start": "289919",
    "end": "292600"
  },
  {
    "text": "a kernel mode",
    "start": "292600",
    "end": "295320"
  },
  {
    "text": "driver so let's talk about the",
    "start": "295440",
    "end": "297600"
  },
  {
    "text": "performance some of you may are already",
    "start": "297600",
    "end": "299960"
  },
  {
    "text": "work with Intel gy 2 which is released",
    "start": "299960",
    "end": "302320"
  },
  {
    "text": "one year before and on training and",
    "start": "302320",
    "end": "304960"
  },
  {
    "text": "inference workloads and since Intel gy3",
    "start": "304960",
    "end": "308160"
  },
  {
    "text": "accelerator comes up with more mme and",
    "start": "308160",
    "end": "311320"
  },
  {
    "text": "TPC Computing units large hbm bandwidth",
    "start": "311320",
    "end": "315320"
  },
  {
    "text": "and the capacity so allows Intel G",
    "start": "315320",
    "end": "318280"
  },
  {
    "text": "accelerator to achieve state of art",
    "start": "318280",
    "end": "320680"
  },
  {
    "text": "performance on gen AI training and",
    "start": "320680",
    "end": "323080"
  },
  {
    "text": "inference workloads so chart here",
    "start": "323080",
    "end": "325840"
  },
  {
    "text": "Compares uh Intel G 3 with Intel gy 2 so",
    "start": "325840",
    "end": "329800"
  },
  {
    "text": "on the right side uh there is a",
    "start": "329800",
    "end": "331639"
  },
  {
    "text": "performance Improvement for the the B",
    "start": "331639",
    "end": "334560"
  },
  {
    "text": "part is a training the the bottom part",
    "start": "334560",
    "end": "336919"
  },
  {
    "text": "is inference throughput and inference",
    "start": "336919",
    "end": "339199"
  },
  {
    "text": "latency so we observe that there are",
    "start": "339199",
    "end": "342600"
  },
  {
    "text": "1.75 performance Improvement on in the",
    "start": "342600",
    "end": "345840"
  },
  {
    "text": "training scenarios because training are",
    "start": "345840",
    "end": "349080"
  },
  {
    "text": "compute intensive so the increased",
    "start": "349080",
    "end": "352199"
  },
  {
    "text": "computer ratio provides intermediate uh",
    "start": "352199",
    "end": "355520"
  },
  {
    "text": "immediate performance gate and the",
    "start": "355520",
    "end": "358039"
  },
  {
    "text": "increased hbm bandwidth allows uh larger",
    "start": "358039",
    "end": "361880"
  },
  {
    "text": "computation and also the larger hbm",
    "start": "361880",
    "end": "365039"
  },
  {
    "text": "device memory which is uh increased from",
    "start": "365039",
    "end": "367680"
  },
  {
    "text": "96 GB to 128 gigabytes allows increased",
    "start": "367680",
    "end": "371880"
  },
  {
    "text": "batch size which means before maybe you",
    "start": "371880",
    "end": "374080"
  },
  {
    "text": "can do like batch size as 64 128 now",
    "start": "374080",
    "end": "377639"
  },
  {
    "text": "maybe you can do like a 526 and allows",
    "start": "377639",
    "end": "380759"
  },
  {
    "text": "avoiding recomputation of certain part",
    "start": "380759",
    "end": "383680"
  },
  {
    "text": "of the workload and also avoid the model",
    "start": "383680",
    "end": "386360"
  },
  {
    "text": "parallel",
    "start": "386360",
    "end": "387560"
  },
  {
    "text": "splits and the uh",
    "start": "387560",
    "end": "390160"
  },
  {
    "text": "in the inference part we Al also",
    "start": "390160",
    "end": "393000"
  },
  {
    "text": "observed a roughly 1.5 Improvement and",
    "start": "393000",
    "end": "397080"
  },
  {
    "text": "for the large models like llama 70",
    "start": "397080",
    "end": "399759"
  },
  {
    "text": "billion or Falcon 180 billion we see",
    "start": "399759",
    "end": "403560"
  },
  {
    "text": "Improvement more than two times the",
    "start": "403560",
    "end": "405840"
  },
  {
    "text": "larger Improvement is also provided by",
    "start": "405840",
    "end": "408440"
  },
  {
    "text": "the larger memory capacity which is aail",
    "start": "408440",
    "end": "411479"
  },
  {
    "text": "available on Intel G",
    "start": "411479",
    "end": "415160"
  },
  {
    "text": "3 okay horizontally we also compare",
    "start": "416599",
    "end": "419560"
  },
  {
    "text": "compare the Intel G distory uh with",
    "start": "419560",
    "end": "421840"
  },
  {
    "text": "Nvidia",
    "start": "421840",
    "end": "423000"
  },
  {
    "text": "h100 so the data here is collected on",
    "start": "423000",
    "end": "426319"
  },
  {
    "text": "March 28th 2024 Nvidia and per ml perf",
    "start": "426319",
    "end": "431280"
  },
  {
    "text": "dashboard so we compare token per second",
    "start": "431280",
    "end": "434479"
  },
  {
    "text": "and a test time training and observed",
    "start": "434479",
    "end": "437120"
  },
  {
    "text": "about 1 25 times up to 1.7 times better",
    "start": "437120",
    "end": "442599"
  },
  {
    "text": "performance compare with",
    "start": "442599",
    "end": "446120"
  },
  {
    "text": "h100 and on the INF side compare with",
    "start": "446440",
    "end": "449560"
  },
  {
    "text": "with h100 uh we compare with llama 7",
    "start": "449560",
    "end": "452919"
  },
  {
    "text": "bilon model 70 bilon model and the",
    "start": "452919",
    "end": "454879"
  },
  {
    "text": "Falcon 180 Bon model and I see average",
    "start": "454879",
    "end": "458240"
  },
  {
    "text": "1.5 time speed up in token per second",
    "start": "458240",
    "end": "461720"
  },
  {
    "text": "and a 1.4 times Improvement in average",
    "start": "461720",
    "end": "465159"
  },
  {
    "text": "power",
    "start": "465159",
    "end": "467639"
  },
  {
    "text": "efficiency okay so let's now go move on",
    "start": "470280",
    "end": "473840"
  },
  {
    "text": "to the next part which is how to enable",
    "start": "473840",
    "end": "475759"
  },
  {
    "text": "Intel Guardian Ray",
    "start": "475759",
    "end": "479560"
  },
  {
    "text": "so Intel GDI is natively supporting Ray",
    "start": "479800",
    "end": "483240"
  },
  {
    "text": "core since q1 2024 we added a new hpu",
    "start": "483240",
    "end": "487639"
  },
  {
    "text": "accelerator manager into the rord if you",
    "start": "487639",
    "end": "492159"
  },
  {
    "text": "uh if you be part of the uh this keynote",
    "start": "492159",
    "end": "496080"
  },
  {
    "text": "this morning uh you you should probably",
    "start": "496080",
    "end": "498199"
  },
  {
    "text": "know what is that is it's a it's a",
    "start": "498199",
    "end": "500319"
  },
  {
    "text": "computer resource manager underneath the",
    "start": "500319",
    "end": "503319"
  },
  {
    "text": "record and um and we also provide the",
    "start": "503319",
    "end": "506720"
  },
  {
    "text": "major PRS for HP support e this SL if",
    "start": "506720",
    "end": "509919"
  },
  {
    "text": "you are interested you can take a look",
    "start": "509919",
    "end": "511479"
  },
  {
    "text": "of that and enable Intel gy user only",
    "start": "511479",
    "end": "515120"
  },
  {
    "text": "need to specify the hpu resource through",
    "start": "515120",
    "end": "518518"
  },
  {
    "text": "Ray init or re Ray remote function it's",
    "start": "518519",
    "end": "521880"
  },
  {
    "text": "really simple and for Ray Trin Reserve",
    "start": "521880",
    "end": "525279"
  },
  {
    "text": "VM uh we also support Intel GDI through",
    "start": "525279",
    "end": "528680"
  },
  {
    "text": "the ray",
    "start": "528680",
    "end": "531160"
  },
  {
    "text": "cor so here is an example to just enable",
    "start": "531360",
    "end": "534959"
  },
  {
    "text": "Intel Guardian array as you can see here",
    "start": "534959",
    "end": "537560"
  },
  {
    "text": "the only thing you need to do is just to",
    "start": "537560",
    "end": "539440"
  },
  {
    "text": "to to um sorry so sry this is next page",
    "start": "539440",
    "end": "543800"
  },
  {
    "text": "so this one is just telling you how to",
    "start": "543800",
    "end": "546680"
  },
  {
    "text": "um bring up the drivers so we",
    "start": "546680",
    "end": "549480"
  },
  {
    "text": "consistently upgrade GDI software",
    "start": "549480",
    "end": "552200"
  },
  {
    "text": "through the doer image now we release",
    "start": "552200",
    "end": "555040"
  },
  {
    "text": "version",
    "start": "555040",
    "end": "556279"
  },
  {
    "text": "1.7 and the 1.18 release is just around",
    "start": "556279",
    "end": "559720"
  },
  {
    "text": "the corner and what you need to do to",
    "start": "559720",
    "end": "562560"
  },
  {
    "text": "launch G the container is just to",
    "start": "562560",
    "end": "565600"
  },
  {
    "text": "download the latest image and install",
    "start": "565600",
    "end": "568240"
  },
  {
    "text": "aray inside this loer furthermore uh we",
    "start": "568240",
    "end": "571600"
  },
  {
    "text": "recommend user to also install another",
    "start": "571600",
    "end": "574040"
  },
  {
    "text": "two optimization packages which is Opa",
    "start": "574040",
    "end": "576959"
  },
  {
    "text": "Habana this is a Hing phase Transformer",
    "start": "576959",
    "end": "580040"
  },
  {
    "text": "optimization kit and also the Habana AI",
    "start": "580040",
    "end": "583000"
  },
  {
    "text": "de",
    "start": "583000",
    "end": "585240"
  },
  {
    "text": "speed okay so how to enable rate W",
    "start": "586040",
    "end": "589839"
  },
  {
    "text": "record so it's pretty simple which is",
    "start": "589839",
    "end": "593040"
  },
  {
    "text": "when you specify the resource you you",
    "start": "593040",
    "end": "595720"
  },
  {
    "text": "assign to hpu in Ray in it or you use",
    "start": "595720",
    "end": "599440"
  },
  {
    "text": "array remote decorator to specify uh hpu",
    "start": "599440",
    "end": "603160"
  },
  {
    "text": "resource for different uh functions or",
    "start": "603160",
    "end": "608079"
  },
  {
    "text": "classes and for range train uh so hpu",
    "start": "609480",
    "end": "613360"
  },
  {
    "text": "resources can also be specified in",
    "start": "613360",
    "end": "616040"
  },
  {
    "text": "scaling config and this and the the HB",
    "start": "616040",
    "end": "619640"
  },
  {
    "text": "HCC heo uh communication Library can be",
    "start": "619640",
    "end": "623560"
  },
  {
    "text": "specified to the torch config then",
    "start": "623560",
    "end": "626600"
  },
  {
    "text": "provide both scaling config and the",
    "start": "626600",
    "end": "628720"
  },
  {
    "text": "torch fun",
    "start": "628720",
    "end": "629839"
  },
  {
    "text": "uh config to the toor trainer so you",
    "start": "629839",
    "end": "632440"
  },
  {
    "text": "will have your model trained on HBO and",
    "start": "632440",
    "end": "636079"
  },
  {
    "text": "you may ask if we also need to do we",
    "start": "636079",
    "end": "639440"
  },
  {
    "text": "need also need to do additional work in",
    "start": "639440",
    "end": "641399"
  },
  {
    "text": "the train function because that is the",
    "start": "641399",
    "end": "643279"
  },
  {
    "text": "most tricky part right the answer is yes",
    "start": "643279",
    "end": "646040"
  },
  {
    "text": "but it's not much I will explain what",
    "start": "646040",
    "end": "648720"
  },
  {
    "text": "need to change to your train code in the",
    "start": "648720",
    "end": "651720"
  },
  {
    "text": "later M llm training",
    "start": "651720",
    "end": "655560"
  },
  {
    "text": "part okay so for R race serve hpu",
    "start": "658000",
    "end": "662000"
  },
  {
    "text": "resource can be specified in the serve",
    "start": "662000",
    "end": "665639"
  },
  {
    "text": "development uh deployment decorator in",
    "start": "665639",
    "end": "669600"
  },
  {
    "text": "the inference code us user only need to",
    "start": "669600",
    "end": "672880"
  },
  {
    "text": "assign the model to",
    "start": "672880",
    "end": "674800"
  },
  {
    "text": "hpu one additional line highlighted here",
    "start": "674800",
    "end": "678399"
  },
  {
    "text": "is the wrapping model in hpu graph this",
    "start": "678399",
    "end": "682399"
  },
  {
    "text": "line actually is optional but it's a",
    "start": "682399",
    "end": "684720"
  },
  {
    "text": "very good um way to improve your",
    "start": "684720",
    "end": "687279"
  },
  {
    "text": "inference performance by keep entire",
    "start": "687279",
    "end": "690519"
  },
  {
    "text": "model operator execution on hpu instead",
    "start": "690519",
    "end": "694240"
  },
  {
    "text": "of having those uh CPU fullbacks and the",
    "start": "694240",
    "end": "697839"
  },
  {
    "text": "execution uh and the execution in lazy",
    "start": "697839",
    "end": "701200"
  },
  {
    "text": "mode with hpu graph will help you will",
    "start": "701200",
    "end": "704480"
  },
  {
    "text": "help the HB to capturing all entire",
    "start": "704480",
    "end": "707279"
  },
  {
    "text": "model and a replay so we can reduce",
    "start": "707279",
    "end": "711680"
  },
  {
    "text": "uh additional time to waiting for the",
    "start": "711680",
    "end": "714959"
  },
  {
    "text": "model to execute and in and Intel GDI",
    "start": "714959",
    "end": "718800"
  },
  {
    "text": "also provides other easy to use",
    "start": "718800",
    "end": "720920"
  },
  {
    "text": "optimization wrapper and environment",
    "start": "720920",
    "end": "723279"
  },
  {
    "text": "variables which help to accelerate model",
    "start": "723279",
    "end": "726560"
  },
  {
    "text": "during the inference uh I will share",
    "start": "726560",
    "end": "730720"
  },
  {
    "text": "later okay so um we still have like 70",
    "start": "731639",
    "end": "735560"
  },
  {
    "text": "minutes so I want to put more time for",
    "start": "735560",
    "end": "738320"
  },
  {
    "text": "here which is to dig into the details",
    "start": "738320",
    "end": "741480"
  },
  {
    "text": "about how to do um training serving uh",
    "start": "741480",
    "end": "745160"
  },
  {
    "text": "with Intel GDI ARR",
    "start": "745160",
    "end": "749560"
  },
  {
    "text": "okay so um here is a example for",
    "start": "750040",
    "end": "753160"
  },
  {
    "text": "training a llama model uh you can also",
    "start": "753160",
    "end": "755199"
  },
  {
    "text": "think it's a fine tuning llama model on",
    "start": "755199",
    "end": "757440"
  },
  {
    "text": "Intel g w r train so uh firstly we",
    "start": "757440",
    "end": "761079"
  },
  {
    "text": "import additional package called Optima",
    "start": "761079",
    "end": "764160"
  },
  {
    "text": "Habana which is a Kagen phase",
    "start": "764160",
    "end": "767000"
  },
  {
    "text": "Transformer GDI optimization",
    "start": "767000",
    "end": "769800"
  },
  {
    "text": "kit then we also need to import like gy",
    "start": "769800",
    "end": "773440"
  },
  {
    "text": "trainer gy config and gy training",
    "start": "773440",
    "end": "776760"
  },
  {
    "text": "arguments and also um adapt function",
    "start": "776760",
    "end": "780040"
  },
  {
    "text": "called adapt Transformers to GDI so that",
    "start": "780040",
    "end": "782959"
  },
  {
    "text": "is everything you need to do then uh as",
    "start": "782959",
    "end": "786199"
  },
  {
    "text": "mentioned before user only need to",
    "start": "786199",
    "end": "788959"
  },
  {
    "text": "specify hpu resource through the scaling",
    "start": "788959",
    "end": "791880"
  },
  {
    "text": "config and toch config and provide to",
    "start": "791880",
    "end": "794040"
  },
  {
    "text": "the to trainer so in the train code this",
    "start": "794040",
    "end": "797160"
  },
  {
    "text": "is the most important part because um",
    "start": "797160",
    "end": "799399"
  },
  {
    "text": "other things are just fine because um",
    "start": "799399",
    "end": "803160"
  },
  {
    "text": "all the effort we are doing is just to",
    "start": "803160",
    "end": "805320"
  },
  {
    "text": "write the training Co training function",
    "start": "805320",
    "end": "807560"
  },
  {
    "text": "right so for the training function um as",
    "start": "807560",
    "end": "810800"
  },
  {
    "text": "you can see there are only four line",
    "start": "810800",
    "end": "812320"
  },
  {
    "text": "changes the first one is uh we need to",
    "start": "812320",
    "end": "815240"
  },
  {
    "text": "call the adapt Transformer to GDI so",
    "start": "815240",
    "end": "818240"
  },
  {
    "text": "what does this line do this line",
    "start": "818240",
    "end": "820480"
  },
  {
    "text": "actually because we are providing",
    "start": "820480",
    "end": "822399"
  },
  {
    "text": "additional optimal Habana package which",
    "start": "822399",
    "end": "825560"
  },
  {
    "text": "can replace a original Transformer layer",
    "start": "825560",
    "end": "828639"
  },
  {
    "text": "with our op Habana layers and it",
    "start": "828639",
    "end": "832360"
  },
  {
    "text": "provides you better performance um so",
    "start": "832360",
    "end": "835680"
  },
  {
    "text": "this is what the adapted Transformer to",
    "start": "835680",
    "end": "838040"
  },
  {
    "text": "GDI do",
    "start": "838040",
    "end": "839560"
  },
  {
    "text": "and secondly uh you need to specify",
    "start": "839560",
    "end": "842360"
  },
  {
    "text": "training arguments and gy configuration",
    "start": "842360",
    "end": "845680"
  },
  {
    "text": "you you can also use in the default",
    "start": "845680",
    "end": "847240"
  },
  {
    "text": "option but this is to um a configuration",
    "start": "847240",
    "end": "851000"
  },
  {
    "text": "will help you to control more to your",
    "start": "851000",
    "end": "853199"
  },
  {
    "text": "cluster and to your model and then using",
    "start": "853199",
    "end": "856199"
  },
  {
    "text": "the gy trainer to train the Llama",
    "start": "856199",
    "end": "860160"
  },
  {
    "text": "model and the remaining part of the code",
    "start": "860959",
    "end": "864160"
  },
  {
    "text": "will stay the same as you write before",
    "start": "864160",
    "end": "867040"
  },
  {
    "text": "so you can see it's original tokenizer",
    "start": "867040",
    "end": "870000"
  },
  {
    "text": "it's a Reginal configuration you don't",
    "start": "870000",
    "end": "872040"
  },
  {
    "text": "need to change that",
    "start": "872040",
    "end": "873759"
  },
  {
    "text": "part and one thing need to mention is",
    "start": "873759",
    "end": "876519"
  },
  {
    "text": "that R train on Intel gy also support",
    "start": "876519",
    "end": "879839"
  },
  {
    "text": "DDP and the GDI optimized deep",
    "start": "879839",
    "end": "884639"
  },
  {
    "text": "speed and then let's look at this",
    "start": "886759",
    "end": "889440"
  },
  {
    "text": "example which is a serving on Intel G",
    "start": "889440",
    "end": "894839"
  },
  {
    "text": "westray so uh the the process is very",
    "start": "897120",
    "end": "900440"
  },
  {
    "text": "similar can be also do the serving for a",
    "start": "900440",
    "end": "903800"
  },
  {
    "text": "l model or a regular deep deep learning",
    "start": "903800",
    "end": "906800"
  },
  {
    "text": "model uh firstly you need to adapt",
    "start": "906800",
    "end": "909639"
  },
  {
    "text": "Transformers to GDI secondly assign the",
    "start": "909639",
    "end": "912759"
  },
  {
    "text": "model to hpu just as you did for assign",
    "start": "912759",
    "end": "916320"
  },
  {
    "text": "model to Cuda as well and then rep model",
    "start": "916320",
    "end": "919759"
  },
  {
    "text": "in hpu graph and the first and the third",
    "start": "919759",
    "end": "923440"
  },
  {
    "text": "one is optional only the second step is",
    "start": "923440",
    "end": "926839"
  },
  {
    "text": "necessary but the first and third step",
    "start": "926839",
    "end": "929160"
  },
  {
    "text": "will help you to improve the inference",
    "start": "929160",
    "end": "933079"
  },
  {
    "text": "performance okay so we talk lot about",
    "start": "936920",
    "end": "940120"
  },
  {
    "text": "the optim Habana and the torch Bridge",
    "start": "940120",
    "end": "943079"
  },
  {
    "text": "optimization so I also want to wrap in",
    "start": "943079",
    "end": "945519"
  },
  {
    "text": "here um when invoke the adapter adapt",
    "start": "945519",
    "end": "949880"
  },
  {
    "text": "Transformer to gy as I mentioned before",
    "start": "949880",
    "end": "952160"
  },
  {
    "text": "this is actually replace some known",
    "start": "952160",
    "end": "954600"
  },
  {
    "text": "layers from model to model with gy",
    "start": "954600",
    "end": "957920"
  },
  {
    "text": "optimizations version of layer and uh we",
    "start": "957920",
    "end": "961519"
  },
  {
    "text": "are consistently updating the optin",
    "start": "961519",
    "end": "963880"
  },
  {
    "text": "Habana to along with the Transformer",
    "start": "963880",
    "end": "966240"
  },
  {
    "text": "release so as you know last week Lama",
    "start": "966240",
    "end": "970199"
  },
  {
    "text": "3.2 model just released right and uh",
    "start": "970199",
    "end": "974160"
  },
  {
    "text": "along with the Transformer 4.4 45",
    "start": "974160",
    "end": "976959"
  },
  {
    "text": "release so there is a new model called",
    "start": "976959",
    "end": "980480"
  },
  {
    "text": "uh M lama multimodel lama which is very",
    "start": "980480",
    "end": "984000"
  },
  {
    "text": "different from other previous llama",
    "start": "984000",
    "end": "986319"
  },
  {
    "text": "model and now it's already been run",
    "start": "986319",
    "end": "988920"
  },
  {
    "text": "running on",
    "start": "988920",
    "end": "989839"
  },
  {
    "text": "hpu so that that is how easy to running",
    "start": "989839",
    "end": "994279"
  },
  {
    "text": "any model and any Transformer model on",
    "start": "994279",
    "end": "996480"
  },
  {
    "text": "hpu but we also working with Optima",
    "start": "996480",
    "end": "999560"
  },
  {
    "text": "Habana team to add additional",
    "start": "999560",
    "end": "1001519"
  },
  {
    "text": "optimization for Mama which is to adding",
    "start": "1001519",
    "end": "1004880"
  },
  {
    "text": "those uh automization layer uh to",
    "start": "1004880",
    "end": "1007800"
  },
  {
    "text": "replace Reginal Transformer layers and",
    "start": "1007800",
    "end": "1010959"
  },
  {
    "text": "then touch Bridge um there is also some",
    "start": "1010959",
    "end": "1014240"
  },
  {
    "text": "code for the user to can control their",
    "start": "1014240",
    "end": "1017639"
  },
  {
    "text": "uh model process C so I I did I provide",
    "start": "1017639",
    "end": "1021720"
  },
  {
    "text": "some examples here the first one is",
    "start": "1021720",
    "end": "1024038"
  },
  {
    "text": "wrapping hpu graph I introduced before",
    "start": "1024039",
    "end": "1026918"
  },
  {
    "text": "which is to uh capturing the entire",
    "start": "1026919",
    "end": "1029760"
  },
  {
    "text": "model as a graph and we capturing and",
    "start": "1029760",
    "end": "1032918"
  },
  {
    "text": "replay so it will be way faster than",
    "start": "1032919",
    "end": "1036199"
  },
  {
    "text": "before um and the second one is Mark",
    "start": "1036199",
    "end": "1039400"
  },
  {
    "text": "step so Mark step is like additional to",
    "start": "1039400",
    "end": "1042360"
  },
  {
    "text": "the rep in HP graph because when we are",
    "start": "1042360",
    "end": "1045079"
  },
  {
    "text": "wrapping a super large model it might",
    "start": "1045079",
    "end": "1048240"
  },
  {
    "text": "not fit in into the device memory right",
    "start": "1048240",
    "end": "1050480"
  },
  {
    "text": "so we want to use Mark stab to split",
    "start": "1050480",
    "end": "1053120"
  },
  {
    "text": "those big those big graphs into small",
    "start": "1053120",
    "end": "1056480"
  },
  {
    "text": "graphs and the synchronization is also",
    "start": "1056480",
    "end": "1059760"
  },
  {
    "text": "do the same thing it's like we can um we",
    "start": "1059760",
    "end": "1063240"
  },
  {
    "text": "can immediately to matze those lazy",
    "start": "1063240",
    "end": "1066200"
  },
  {
    "text": "tensor so we don't need to we don't want",
    "start": "1066200",
    "end": "1068320"
  },
  {
    "text": "to explore our device",
    "start": "1068320",
    "end": "1070919"
  },
  {
    "text": "memory and the torch Bridge also",
    "start": "1070919",
    "end": "1073760"
  },
  {
    "text": "provides a great number of optimizes",
    "start": "1073760",
    "end": "1077360"
  },
  {
    "text": "optimized kernels I put a example here",
    "start": "1077360",
    "end": "1080960"
  },
  {
    "text": "which is a fused sdpa and I will provide",
    "start": "1080960",
    "end": "1083840"
  },
  {
    "text": "more examples in our VM",
    "start": "1083840",
    "end": "1088200"
  },
  {
    "text": "introduction okay so besides all the",
    "start": "1089640",
    "end": "1092480"
  },
  {
    "text": "work we done in Ray train a r serve we",
    "start": "1092480",
    "end": "1096480"
  },
  {
    "text": "because VM is also built upon the ray",
    "start": "1096480",
    "end": "1099320"
  },
  {
    "text": "cord right uh for the distribution part",
    "start": "1099320",
    "end": "1101840"
  },
  {
    "text": "so we are we also did a lot of work in",
    "start": "1101840",
    "end": "1105280"
  },
  {
    "text": "VM on to support Intel gy",
    "start": "1105280",
    "end": "1109360"
  },
  {
    "text": "um so I be I believe everyone here is",
    "start": "1109360",
    "end": "1113159"
  },
  {
    "text": "quite familiar with VM at least we are",
    "start": "1113159",
    "end": "1115840"
  },
  {
    "text": "familiar with what it is and why it's so",
    "start": "1115840",
    "end": "1119000"
  },
  {
    "text": "popular because of all these features",
    "start": "1119000",
    "end": "1121200"
  },
  {
    "text": "which can improve the perform inference",
    "start": "1121200",
    "end": "1123400"
  },
  {
    "text": "performance so much",
    "start": "1123400",
    "end": "1124919"
  },
  {
    "text": "significantly so for Intel GDI we",
    "start": "1124919",
    "end": "1128080"
  },
  {
    "text": "currently provide a VM FK project uh",
    "start": "1128080",
    "end": "1131400"
  },
  {
    "text": "which can run VM on gy we also",
    "start": "1131400",
    "end": "1134240"
  },
  {
    "text": "submitting the pr to merging other the",
    "start": "1134240",
    "end": "1136799"
  },
  {
    "text": "CH other modification to the VM Main",
    "start": "1136799",
    "end": "1141280"
  },
  {
    "text": "upstreams and the ray is a default",
    "start": "1141280",
    "end": "1144080"
  },
  {
    "text": "multiprocess backand in V VM gy project",
    "start": "1144080",
    "end": "1148840"
  },
  {
    "text": "for either tensor parallel or pipeline",
    "start": "1148840",
    "end": "1152320"
  },
  {
    "text": "parallel and also uh all models",
    "start": "1152320",
    "end": "1155440"
  },
  {
    "text": "supported in VM are supported in VM GDI",
    "start": "1155440",
    "end": "1160320"
  },
  {
    "text": "already um and since the",
    "start": "1160320",
    "end": "1163080"
  },
  {
    "text": "z.6 uh V VM was just released a few",
    "start": "1163080",
    "end": "1167120"
  },
  {
    "text": "weeks ago there may be a small feature",
    "start": "1167120",
    "end": "1169919"
  },
  {
    "text": "Gap but uh we already sync our code to",
    "start": "1169919",
    "end": "1173720"
  },
  {
    "text": "the V uh V Point uh v0 point6 uh",
    "start": "1173720",
    "end": "1178080"
  },
  {
    "text": "6.2 and in enable h hpu on vrm user only",
    "start": "1178080",
    "end": "1183360"
  },
  {
    "text": "need to do is to First St using the VM",
    "start": "1183360",
    "end": "1186400"
  },
  {
    "text": "gy",
    "start": "1186400",
    "end": "1187400"
  },
  {
    "text": "project and then assign the device to",
    "start": "1187400",
    "end": "1190679"
  },
  {
    "text": "the hpu as I uh just highlight here so",
    "start": "1190679",
    "end": "1194440"
  },
  {
    "text": "this is only thing you should do to",
    "start": "1194440",
    "end": "1196360"
  },
  {
    "text": "enable uh running inference model on",
    "start": "1196360",
    "end": "1201200"
  },
  {
    "text": "hpu okay so don't don't be scared of",
    "start": "1202360",
    "end": "1205520"
  },
  {
    "text": "this diagram I know it's looks super",
    "start": "1205520",
    "end": "1207480"
  },
  {
    "text": "complex but I just want to show out what",
    "start": "1207480",
    "end": "1209799"
  },
  {
    "text": "is the modification we've done in VM to",
    "start": "1209799",
    "end": "1212760"
  },
  {
    "text": "support hpu so the diagram here uh is uh",
    "start": "1212760",
    "end": "1217840"
  },
  {
    "text": "everything we've been accomplished to",
    "start": "1217840",
    "end": "1220080"
  },
  {
    "text": "support hpu uh in VM project and as you",
    "start": "1220080",
    "end": "1224320"
  },
  {
    "text": "can see VM has a its own model codes so",
    "start": "1224320",
    "end": "1228799"
  },
  {
    "text": "it's not like other um many other",
    "start": "1228799",
    "end": "1231880"
  },
  {
    "text": "project which is directly using the",
    "start": "1231880",
    "end": "1233880"
  },
  {
    "text": "Transformer codes VM has its on model",
    "start": "1233880",
    "end": "1238000"
  },
  {
    "text": "execute folder which have all the",
    "start": "1238000",
    "end": "1240520"
  },
  {
    "text": "implementation of models layers in the v",
    "start": "1240520",
    "end": "1244840"
  },
  {
    "text": "project and so um because that because",
    "start": "1244840",
    "end": "1249080"
  },
  {
    "text": "of that we also need to do additional",
    "start": "1249080",
    "end": "1251240"
  },
  {
    "text": "work to change the original VM layers so",
    "start": "1251240",
    "end": "1255120"
  },
  {
    "text": "we can provide the fully optimized",
    "start": "1255120",
    "end": "1257480"
  },
  {
    "text": "kernel and the layers",
    "start": "1257480",
    "end": "1259240"
  },
  {
    "text": "uh in VM so this is the part on the top",
    "start": "1259240",
    "end": "1264080"
  },
  {
    "text": "sorry but right button the blue box is",
    "start": "1264080",
    "end": "1267880"
  },
  {
    "text": "that let's say this is a llama model so",
    "start": "1267880",
    "end": "1270480"
  },
  {
    "text": "we are are familiar with the Llama model",
    "start": "1270480",
    "end": "1273039"
  },
  {
    "text": "right there are some the major part is",
    "start": "1273039",
    "end": "1275640"
  },
  {
    "text": "the Transformer part in the middle and",
    "start": "1275640",
    "end": "1278440"
  },
  {
    "text": "what we need to do is we need to write",
    "start": "1278440",
    "end": "1281520"
  },
  {
    "text": "all those kernel in the white box which",
    "start": "1281520",
    "end": "1284559"
  },
  {
    "text": "we will P provide Habana page attention",
    "start": "1284559",
    "end": "1288240"
  },
  {
    "text": "um",
    "start": "1288240",
    "end": "1289679"
  },
  {
    "text": "uh hpu RMS Norm op operators and the hpu",
    "start": "1289679",
    "end": "1295039"
  },
  {
    "text": "rotary embedding uh function and also",
    "start": "1295039",
    "end": "1298679"
  },
  {
    "text": "let's say if it's this is a mixure model",
    "start": "1298679",
    "end": "1300960"
  },
  {
    "text": "then we also provide a fused Moe",
    "start": "1300960",
    "end": "1305520"
  },
  {
    "text": "implementation and then uh the other",
    "start": "1309480",
    "end": "1312240"
  },
  {
    "text": "thing I need to mention is above part",
    "start": "1312240",
    "end": "1314440"
  },
  {
    "text": "because VM is very Loosely coupled uh",
    "start": "1314440",
    "end": "1318120"
  },
  {
    "text": "design so it can provide all types of",
    "start": "1318120",
    "end": "1320960"
  },
  {
    "text": "accelerators because um user can provide",
    "start": "1320960",
    "end": "1324200"
  },
  {
    "text": "their own model Runners there are like",
    "start": "1324200",
    "end": "1326880"
  },
  {
    "text": "GPU Runner Mel Runners and other",
    "start": "1326880",
    "end": "1329799"
  },
  {
    "text": "accelerator model Runners and as well we",
    "start": "1329799",
    "end": "1332400"
  },
  {
    "text": "provide Habana uh model Runners so in a",
    "start": "1332400",
    "end": "1336360"
  },
  {
    "text": "Havana model Runner we Implement",
    "start": "1336360",
    "end": "1340159"
  },
  {
    "text": "um uh we we are adding a wrapping",
    "start": "1340159",
    "end": "1343840"
  },
  {
    "text": "function to the load loading weights",
    "start": "1343840",
    "end": "1346000"
  },
  {
    "text": "which will initialize the model as a the",
    "start": "1346000",
    "end": "1348679"
  },
  {
    "text": "beginning so we can have a better model",
    "start": "1348679",
    "end": "1351799"
  },
  {
    "text": "quantization function implementation and",
    "start": "1351799",
    "end": "1354600"
  },
  {
    "text": "other things in the loading weights",
    "start": "1354600",
    "end": "1357279"
  },
  {
    "text": "function and we also provide a model for",
    "start": "1357279",
    "end": "1360159"
  },
  {
    "text": "word wrapper uh which uh if you remember",
    "start": "1360159",
    "end": "1363360"
  },
  {
    "text": "before I I talk about the rapping hpu",
    "start": "1363360",
    "end": "1365720"
  },
  {
    "text": "graph function right so in the vrm we're",
    "start": "1365720",
    "end": "1368720"
  },
  {
    "text": "also doing that but it's already inside",
    "start": "1368720",
    "end": "1371440"
  },
  {
    "text": "the model for word function so you don't",
    "start": "1371440",
    "end": "1373440"
  },
  {
    "text": "need to worry about that and we also",
    "start": "1373440",
    "end": "1376240"
  },
  {
    "text": "provide Performance Tuning feuture",
    "start": "1376240",
    "end": "1378600"
  },
  {
    "text": "adaption uh fun uh implementations in",
    "start": "1378600",
    "end": "1382000"
  },
  {
    "text": "the input tensor preparations logest",
    "start": "1382000",
    "end": "1385520"
  },
  {
    "text": "processors and output sampling so with",
    "start": "1385520",
    "end": "1389200"
  },
  {
    "text": "above changes we can have vrm those",
    "start": "1389200",
    "end": "1393480"
  },
  {
    "text": "brilliant features such as speculative",
    "start": "1393480",
    "end": "1396640"
  },
  {
    "text": "decoding continuous batching multi-step",
    "start": "1396640",
    "end": "1399760"
  },
  {
    "text": "scheduling and also runs on hpu",
    "start": "1399760",
    "end": "1404640"
  },
  {
    "text": "effortless and for the ma memory",
    "start": "1407440",
    "end": "1409799"
  },
  {
    "text": "management I mentioned before now we",
    "start": "1409799",
    "end": "1412279"
  },
  {
    "text": "have 128 gab device memory so does that",
    "start": "1412279",
    "end": "1416760"
  },
  {
    "text": "what does that mean to the model serving",
    "start": "1416760",
    "end": "1419520"
  },
  {
    "text": "in VM um so as we know that there are",
    "start": "1419520",
    "end": "1423480"
  },
  {
    "text": "three major part major memory",
    "start": "1423480",
    "end": "1425840"
  },
  {
    "text": "consumption when you inference a model",
    "start": "1425840",
    "end": "1429000"
  },
  {
    "text": "in VM first thing is the most important",
    "start": "1429000",
    "end": "1431720"
  },
  {
    "text": "part which is a KV cach right because",
    "start": "1431720",
    "end": "1434080"
  },
  {
    "text": "this is how VM um showing so significant",
    "start": "1434080",
    "end": "1438120"
  },
  {
    "text": "performance performance Improvement than",
    "start": "1438120",
    "end": "1439880"
  },
  {
    "text": "other project which is a KV cache and",
    "start": "1439880",
    "end": "1442799"
  },
  {
    "text": "then the model weights so let's say if I",
    "start": "1442799",
    "end": "1445320"
  },
  {
    "text": "have a 70 billion model and I want to",
    "start": "1445320",
    "end": "1448559"
  },
  {
    "text": "just using bf16 which means there will",
    "start": "1448559",
    "end": "1450960"
  },
  {
    "text": "be around",
    "start": "1450960",
    "end": "1452840"
  },
  {
    "text": "130 gigabytes um memory consumption to",
    "start": "1452840",
    "end": "1458600"
  },
  {
    "text": "host this model and then uh we also need",
    "start": "1458600",
    "end": "1461760"
  },
  {
    "text": "to provide some device memory for hpu",
    "start": "1461760",
    "end": "1464320"
  },
  {
    "text": "graph compiled IRS it's very similar to",
    "start": "1464320",
    "end": "1467399"
  },
  {
    "text": "the Cuda graph",
    "start": "1467399",
    "end": "1468880"
  },
  {
    "text": "compiler and um so here uh let's say I",
    "start": "1468880",
    "end": "1472679"
  },
  {
    "text": "have this 7 bilm model and I can either",
    "start": "1472679",
    "end": "1476399"
  },
  {
    "text": "use two um two cards or four cards to",
    "start": "1476399",
    "end": "1479760"
  },
  {
    "text": "host this model but with uh G 3 this uh",
    "start": "1479760",
    "end": "1484480"
  },
  {
    "text": "with 128 gab memory capacity which means",
    "start": "1484480",
    "end": "1488520"
  },
  {
    "text": "I have at least 40 gigabyt on each card",
    "start": "1488520",
    "end": "1491880"
  },
  {
    "text": "for the K cach what does that mean that",
    "start": "1491880",
    "end": "1494320"
  },
  {
    "text": "means we can host at least 20K to uh to",
    "start": "1494320",
    "end": "1499320"
  },
  {
    "text": "20,000 uh cash tables with 128 blocks th",
    "start": "1499320",
    "end": "1504640"
  },
  {
    "text": "this is a very big c c cache so we can",
    "start": "1504640",
    "end": "1508399"
  },
  {
    "text": "um host a lot of previous um previous uh",
    "start": "1508399",
    "end": "1512600"
  },
  {
    "text": "attention uh K KV values so we don't",
    "start": "1512600",
    "end": "1515919"
  },
  {
    "text": "need to recomputation",
    "start": "1515919",
    "end": "1519360"
  },
  {
    "text": "that okay so what more um except for",
    "start": "1520760",
    "end": "1525520"
  },
  {
    "text": "just running vrm or running Reserve",
    "start": "1525520",
    "end": "1528640"
  },
  {
    "text": "there is a possibility that we can",
    "start": "1528640",
    "end": "1530840"
  },
  {
    "text": "combine these two project together",
    "start": "1530840",
    "end": "1533320"
  },
  {
    "text": "because um so the light blue part",
    "start": "1533320",
    "end": "1536120"
  },
  {
    "text": "underneath is VM and the currently is",
    "start": "1536120",
    "end": "1538919"
  },
  {
    "text": "pro it's using the fast API but race",
    "start": "1538919",
    "end": "1542559"
  },
  {
    "text": "serve actually it provides a whole",
    "start": "1542559",
    "end": "1545240"
  },
  {
    "text": "different um features above fast API",
    "start": "1545240",
    "end": "1549600"
  },
  {
    "text": "right so one one solution here is we can",
    "start": "1549600",
    "end": "1553480"
  },
  {
    "text": "also replace fast API with Reserve so it",
    "start": "1553480",
    "end": "1557679"
  },
  {
    "text": "can be",
    "start": "1557679",
    "end": "1558760"
  },
  {
    "text": "um deployed on R CU Ray and uh doing the",
    "start": "1558760",
    "end": "1563960"
  },
  {
    "text": "any skill scaling and the example is",
    "start": "1563960",
    "end": "1567880"
  },
  {
    "text": "also provided here uh which is the which",
    "start": "1567880",
    "end": "1571039"
  },
  {
    "text": "is contributed by one of my colleague um",
    "start": "1571039",
    "end": "1574039"
  },
  {
    "text": "and it's on the",
    "start": "1574039",
    "end": "1577000"
  },
  {
    "text": "raid.io okay that's all thank",
    "start": "1578520",
    "end": "1582279"
  },
  {
    "text": "you",
    "start": "1582279",
    "end": "1584320"
  },
  {
    "text": "um we have only three minutes",
    "start": "1584320",
    "end": "1589158"
  },
  {
    "text": "[Applause]",
    "start": "1590350",
    "end": "1595150"
  },
  {
    "text": "um do we have any questions yes does it",
    "start": "1595320",
    "end": "1600000"
  },
  {
    "text": "support uh eager mode",
    "start": "1600000",
    "end": "1603039"
  },
  {
    "text": "execution um yes yes yes but we actually",
    "start": "1603039",
    "end": "1609320"
  },
  {
    "text": "we sorry the question is does it support",
    "start": "1609320",
    "end": "1611520"
  },
  {
    "text": "uh eager mode p in pytorch the answer is",
    "start": "1611520",
    "end": "1614399"
  },
  {
    "text": "yes but uh we we don't recommend that",
    "start": "1614399",
    "end": "1619399"
  },
  {
    "text": "yes a couple apis that you mentioned",
    "start": "1619399",
    "end": "1622840"
  },
  {
    "text": "that modifies the",
    "start": "1622840",
    "end": "1625600"
  },
  {
    "text": "Transformer to support GOI could you",
    "start": "1625600",
    "end": "1628559"
  },
  {
    "text": "talk a bit more about what they what's",
    "start": "1628559",
    "end": "1630720"
  },
  {
    "text": "underneath these",
    "start": "1630720",
    "end": "1632240"
  },
  {
    "text": "functions oh sorry can you say that",
    "start": "1632240",
    "end": "1634440"
  },
  {
    "text": "question again there were a couple apis",
    "start": "1634440",
    "end": "1636159"
  },
  {
    "text": "you said we had to add for suring and",
    "start": "1636159",
    "end": "1638520"
  },
  {
    "text": "training yes but could you talk a bit",
    "start": "1638520",
    "end": "1640799"
  },
  {
    "text": "more about what those functions do the",
    "start": "1640799",
    "end": "1643919"
  },
  {
    "text": "modify adapt the adapt Transformers fun",
    "start": "1643919",
    "end": "1647679"
  },
  {
    "text": "okay um so the question is um what",
    "start": "1647679",
    "end": "1651640"
  },
  {
    "text": "actually the optim Habana do to uh repl",
    "start": "1651640",
    "end": "1655320"
  },
  {
    "text": "to optimize the Transformer layers I um",
    "start": "1655320",
    "end": "1659080"
  },
  {
    "text": "but answer is that I I can it's a it's",
    "start": "1659080",
    "end": "1661880"
  },
  {
    "text": "bit comp complex so I don't think I can",
    "start": "1661880",
    "end": "1664159"
  },
  {
    "text": "share here but I do provide a link uh",
    "start": "1664159",
    "end": "1667679"
  },
  {
    "text": "for you you can just go to the op Habana",
    "start": "1667679",
    "end": "1670720"
  },
  {
    "text": "GitHub it's very clear there um yeah",
    "start": "1670720",
    "end": "1677440"
  }
]