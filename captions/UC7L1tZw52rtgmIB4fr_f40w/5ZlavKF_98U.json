[
  {
    "text": "hello everyone thank you all for joining my name is uzukon I'm a PC student at UC",
    "start": "3240",
    "end": "9660"
  },
  {
    "text": "Berkeley working with Jan today I'm more than happy to present a recent project Bria 11 fast llm serving",
    "start": "9660",
    "end": "17100"
  },
  {
    "text": "with page attention this project is co-led by me and Juan here and our otherwise you're young",
    "start": "17100",
    "end": "25580"
  },
  {
    "text": "as we all know we are in the era of hero limbs the most Innovative applications today like Chachi PT and co-pilot are",
    "start": "26160",
    "end": "34020"
  },
  {
    "text": "powered via alums moreover the the whole field is still rapidly growing every week we see new",
    "start": "34020",
    "end": "41340"
  },
  {
    "text": "models and new breakthroughs that continue to expand the potential of of adult limbs",
    "start": "41340",
    "end": "47640"
  },
  {
    "text": "certainly L Ms are opening up a plaster of exciting opportunities",
    "start": "47640",
    "end": "53480"
  },
  {
    "text": "as a result we've seen a surge in applications and startups leveraging LMS",
    "start": "53820",
    "end": "59340"
  },
  {
    "text": "across various fields from chatting and programming to business operations",
    "start": "59340",
    "end": "66000"
  },
  {
    "text": "and an essential part of these applications we focus in this talk today it's serving Diego limbs",
    "start": "66000",
    "end": "72119"
  },
  {
    "text": "given these applications all heavily utilized at olymps the performance and cost of the",
    "start": "72119",
    "end": "78060"
  },
  {
    "text": "applications largely depend on the speed and cost of serving lens therefore how to serve adults fast and",
    "start": "78060",
    "end": "84960"
  },
  {
    "text": "cost effectively is becoming a tremend of the important problem these days",
    "start": "84960",
    "end": "90860"
  },
  {
    "text": "however we observe that serving elements is surprisingly slow and expensive even",
    "start": "92040",
    "end": "97860"
  },
  {
    "text": "on the best of free Hardware due to the large model size LMS often",
    "start": "97860",
    "end": "103020"
  },
  {
    "text": "run on high-end gpus like a media a100 despite that a single GPU with previous",
    "start": "103020",
    "end": "109500"
  },
  {
    "text": "systems could only serve a handful of weekends per second even less than one weekend per second",
    "start": "109500",
    "end": "116040"
  },
  {
    "text": "for a 13 billion model with moderate input sizes this means you will need a ton of gpus",
    "start": "116040",
    "end": "122540"
  },
  {
    "text": "if you actually build production skills services using LMS",
    "start": "122540",
    "end": "127799"
  },
  {
    "text": "obviously this has been a critical pain point for many large and small companies",
    "start": "127799",
    "end": "134720"
  },
  {
    "text": "to understand the problem let's recap the inference process of it alums first the user provides a prompt",
    "start": "136020",
    "end": "143160"
  },
  {
    "text": "consisting of multiple tokens the problem goes through the model",
    "start": "143160",
    "end": "149340"
  },
  {
    "text": "and then we get the next token to the prompt the next step we feed this means feed",
    "start": "149340",
    "end": "155700"
  },
  {
    "text": "this new token back to the model and get the second output",
    "start": "155700",
    "end": "161400"
  },
  {
    "text": "and this process is repeated until the sequence either reaches a",
    "start": "161400",
    "end": "166620"
  },
  {
    "text": "predefined maximum length say 2000 tokens or generates a certain token for",
    "start": "166620",
    "end": "171660"
  },
  {
    "text": "stopping this is basically how the inference process of LMS works",
    "start": "171660",
    "end": "178459"
  },
  {
    "text": "in the inference process LMS have a unique component which is often called KV cache in the literature",
    "start": "178680",
    "end": "186739"
  },
  {
    "text": "when processing a new token the model actually needs not only the representation of the current token",
    "start": "186959",
    "end": "192000"
  },
  {
    "text": "which is though in this particular example but it also needs the representations of",
    "start": "192000",
    "end": "197159"
  },
  {
    "text": "all the previous tokens so these states or previous tokens",
    "start": "197159",
    "end": "202500"
  },
  {
    "text": "should be kept in memory and they are called KV cache",
    "start": "202500",
    "end": "208099"
  },
  {
    "text": "the same for the next step except that the input token in the previous step which was though in this example was",
    "start": "209220",
    "end": "215459"
  },
  {
    "text": "newly appended to the KV cache as such the KV cache dynamically grows",
    "start": "215459",
    "end": "221640"
  },
  {
    "text": "and it also shrinks when the sequence finishes",
    "start": "221640",
    "end": "225920"
  },
  {
    "text": "here the key Insight in our project is that efficient management of the KV",
    "start": "227099",
    "end": "232260"
  },
  {
    "text": "cache is crucial for high throughput LM serving",
    "start": "232260",
    "end": "237540"
  },
  {
    "text": "let me give you an example let's say we run a 13 billion llm on a 40 gigabyte",
    "start": "237540",
    "end": "242640"
  },
  {
    "text": "GPU the model parameters take roughly 26 gigabytes of memory",
    "start": "242640",
    "end": "248340"
  },
  {
    "text": "in addition a small fraction of memory is reserved for workspace the rest of the memory which is about 20",
    "start": "248340",
    "end": "255239"
  },
  {
    "text": "gigabyte in this example can be used for the KV cache",
    "start": "255239",
    "end": "261320"
  },
  {
    "text": "our finding is that previous systems use KV cache inefficiently",
    "start": "261419",
    "end": "267300"
  },
  {
    "text": "and thus significantly limit the number of retests that can be batched together",
    "start": "267300",
    "end": "273360"
  },
  {
    "text": "in contrast our solution field of them manages KV cache much more efficiently",
    "start": "273360",
    "end": "279900"
  },
  {
    "text": "and thereby allows much larger batch size with the same amount of memory",
    "start": "279900",
    "end": "285740"
  },
  {
    "text": "and importantly the increase in batch size translates to the increase in throughput enhanced reduces the cost per",
    "start": "286680",
    "end": "294000"
  },
  {
    "text": "weakest so what are the memory inefficiencies in",
    "start": "294000",
    "end": "300300"
  },
  {
    "text": "the previous systems this is a snapshot of the KV cache when using a previous system where",
    "start": "300300",
    "end": "306660"
  },
  {
    "text": "we find three types of memory waste the first is internal fragmentation",
    "start": "306660",
    "end": "311940"
  },
  {
    "text": "which means the slots are located for a sequence but never used",
    "start": "311940",
    "end": "317639"
  },
  {
    "text": "this happens because we don't know the we don't know in advance how many tokens the model will generate",
    "start": "317639",
    "end": "325460"
  },
  {
    "text": "the second is reservation which means the slots better not used at the moment but will be used for the sequence in the",
    "start": "326160",
    "end": "332820"
  },
  {
    "text": "future here the three slots in the middle are reserved because they don't store any",
    "start": "332820",
    "end": "339060"
  },
  {
    "text": "token at the current step but we'll store output tokens in the subsequent steps",
    "start": "339060",
    "end": "345660"
  },
  {
    "text": "this is another kind of waste finally there is external fragmentation because different request A and B may",
    "start": "345660",
    "end": "353400"
  },
  {
    "text": "have different sequence links according to our profiling results the",
    "start": "353400",
    "end": "360120"
  },
  {
    "text": "memory waste was significant only 20 to 40 percent of the kvk space",
    "start": "360120",
    "end": "365340"
  },
  {
    "text": "was actually utilized to store the token States the rest of the memory was merely wasted",
    "start": "365340",
    "end": "371340"
  },
  {
    "text": "for these three reasons so how does vlm solve this",
    "start": "371340",
    "end": "380000"
  },
  {
    "text": "our solution is basically to employ the old idea of virtual memory and paging in operating systems",
    "start": "380280",
    "end": "386759"
  },
  {
    "text": "as we all know OS uses paging to reduce fragmentation any users virtual memory for efficient",
    "start": "386759",
    "end": "394740"
  },
  {
    "text": "and elegant space multiplexing between uh between processes",
    "start": "394740",
    "end": "400759"
  },
  {
    "text": "and what vlm does are basically the same it uses a similar kind of idea to",
    "start": "401100",
    "end": "406380"
  },
  {
    "text": "resolve the fragmentation in the KV test space and to enable efficient space sharing",
    "start": "406380",
    "end": "411900"
  },
  {
    "text": "between recast specifically this is enabled by our new technique page retention",
    "start": "411900",
    "end": "418680"
  },
  {
    "text": "let's dive into it okay to begin with we partition the KV cache into an array of KB blocks just",
    "start": "418680",
    "end": "426180"
  },
  {
    "text": "like paging a KV block is a fixed side chunk of",
    "start": "426180",
    "end": "431819"
  },
  {
    "text": "memory that can store token States from left to right",
    "start": "431819",
    "end": "437300"
  },
  {
    "text": "in this particular example the block size is 4 which means we can store four tokens in the block",
    "start": "437340",
    "end": "445099"
  },
  {
    "text": "on top of this we introduced page Attention our new implementation of attention mechanisms",
    "start": "446099",
    "end": "451440"
  },
  {
    "text": "we find that the fundamental limitation of the previous systems is that they",
    "start": "451440",
    "end": "456479"
  },
  {
    "text": "require all KV State servers for a sequence to be stored in a contiguous memory space",
    "start": "456479",
    "end": "463259"
  },
  {
    "text": "this is useful convention in traditional DNA workloads where the input and output shapes are static however it turns out",
    "start": "463259",
    "end": "470819"
  },
  {
    "text": "to be highly inefficient for LM inference where the sequence links are Dynamic and unknown a priority",
    "start": "470819",
    "end": "477918"
  },
  {
    "text": "pays attention directly addresses this limitation as shown in the animation page attention",
    "start": "478139",
    "end": "483419"
  },
  {
    "text": "operates on the KV States stored in non-contiguous blocks and it efficiently",
    "start": "483419",
    "end": "488819"
  },
  {
    "text": "patches the blocks located in arbitrary positions in the KV cache space",
    "start": "488819",
    "end": "494660"
  },
  {
    "text": "furthermore we virtualize the KV cache to logical and physical KB blocks let's",
    "start": "495660",
    "end": "501840"
  },
  {
    "text": "see how it works with an example request a its prompt is elementoring is a",
    "start": "501840",
    "end": "507000"
  },
  {
    "text": "computer and scientist in The Logical view the tokens are stored in a natural fashion they're",
    "start": "507000",
    "end": "513779"
  },
  {
    "text": "stored in in consecutive blocks where the order is preserved",
    "start": "513779",
    "end": "519500"
  },
  {
    "text": "in the physical view on the other hand the tokens in the same sequence may not be stored in adjacent blocks and in",
    "start": "520020",
    "end": "527160"
  },
  {
    "text": "their order between the the blocks can be arbitrary",
    "start": "527160",
    "end": "532399"
  },
  {
    "text": "and the mapping between The Logical blocks to the physical blocks is stored in a new data structure called block",
    "start": "533100",
    "end": "538440"
  },
  {
    "text": "table which which is analogous to page table",
    "start": "538440",
    "end": "543500"
  },
  {
    "text": "let's continue the example let's say the model has generated the next token end",
    "start": "544620",
    "end": "551180"
  },
  {
    "text": "the new token is first appended to the to the last logical block",
    "start": "551339",
    "end": "557399"
  },
  {
    "text": "and using the block table we also append the new token state to the corresponding physical block",
    "start": "557399",
    "end": "564259"
  },
  {
    "text": "the same for the Next Generation token mathematician and finally if the last plug is full",
    "start": "564360",
    "end": "571140"
  },
  {
    "text": "then we allocate a new physical block and store the token in the first slot of the block",
    "start": "571140",
    "end": "577940"
  },
  {
    "text": "importantly this means the blog is allocated on demand as opposed to being",
    "start": "577940",
    "end": "583200"
  },
  {
    "text": "pre-located like in the previous systems",
    "start": "583200",
    "end": "587540"
  },
  {
    "text": "so basically this is how we only manages the KV cache so far we've covered how it",
    "start": "588660",
    "end": "594180"
  },
  {
    "text": "works for a single recast a single sequence however I believe you will be able to",
    "start": "594180",
    "end": "599339"
  },
  {
    "text": "imagine how it's going to work for multiple request process simultaneously since its underlying principle is saying",
    "start": "599339",
    "end": "605399"
  },
  {
    "text": "is the same as the virtual memory in OS",
    "start": "605399",
    "end": "609560"
  },
  {
    "text": "let's quickly analyze the memory efficiency of viola first vlamp has minimal internal",
    "start": "610740",
    "end": "616019"
  },
  {
    "text": "fragmentation this is because the internal fragmentation only happens at the last",
    "start": "616019",
    "end": "621060"
  },
  {
    "text": "block of a sequence this means the the number of the wasted tokens per sequence is bounded by the",
    "start": "621060",
    "end": "628500"
  },
  {
    "text": "block size in practice we use block size 16 or 32",
    "start": "628500",
    "end": "634140"
  },
  {
    "text": "which means we store 16 or 32 token States per block and this is orders are",
    "start": "634140",
    "end": "639600"
  },
  {
    "text": "magnet is smaller than a typical sequence length therefore the internal fragmentation is very small",
    "start": "639600",
    "end": "645240"
  },
  {
    "text": "second vlm does not have external fragmentation since all blocks have the same size",
    "start": "645240",
    "end": "651920"
  },
  {
    "text": "putting these together real name only weighs four percent of the KV cast space in other words it improves the memory",
    "start": "652320",
    "end": "658500"
  },
  {
    "text": "utilization by three to five times compared to the previous systems this leads to significant increase in",
    "start": "658500",
    "end": "664860"
  },
  {
    "text": "batch size and eventually disturbing throughput okay from now on G1 we'll take over the",
    "start": "664860",
    "end": "670380"
  },
  {
    "text": "rest of the talk John okay um okay hello everyone this is Johan so",
    "start": "670380",
    "end": "676560"
  },
  {
    "text": "just now also talks about how the page memory or the dynamic block mapping reduces the memory waste and improves",
    "start": "676560",
    "end": "683100"
  },
  {
    "text": "the serving throughput okay and actually another big advantage of the dynamic",
    "start": "683100",
    "end": "688260"
  },
  {
    "text": "block mapping we have in BLM is that it enables sharing so for for example here",
    "start": "688260",
    "end": "694680"
  },
  {
    "text": "on the slide the parallel sampling is actually a very popular sampling method for air amps which which generates",
    "start": "694680",
    "end": "700620"
  },
  {
    "text": "multiple output from the same problem so as shown on the slide here we have one",
    "start": "700620",
    "end": "705959"
  },
  {
    "text": "input prompt the future of cloud computing is and we see this one input prompt into an llm and generate multiple",
    "start": "705959",
    "end": "713640"
  },
  {
    "text": "sample output from the lrm and in such a case the computation and the memory for",
    "start": "713640",
    "end": "719339"
  },
  {
    "text": "the prompt can be can be saved by sharing can be saved by sharing it between the",
    "start": "719339",
    "end": "724800"
  },
  {
    "text": "different parallel output samples so this scenario might be pretty rare for a",
    "start": "724800",
    "end": "729839"
  },
  {
    "text": "playing chatbot but it can be very helpful for cases like element-based programming assistance like the GitHub",
    "start": "729839",
    "end": "736079"
  },
  {
    "text": "copilot which we all use for for coding for coding like you can try to generate",
    "start": "736079",
    "end": "741180"
  },
  {
    "text": "multiple uh multiple candidates and select the one you want okay and",
    "start": "741180",
    "end": "748800"
  },
  {
    "text": "so vlm can naturally support this decoding scenario and using its blog tables so we use this we use the same",
    "start": "748800",
    "end": "755220"
  },
  {
    "text": "prompt the future of cloud computing is as the previous slide for example and here we generate two parallel samples so",
    "start": "755220",
    "end": "763079"
  },
  {
    "text": "for the prompt part because they share the same prompt these two samples share the same problem vrm will only do the",
    "start": "763079",
    "end": "769380"
  },
  {
    "text": "computation once and store only one copy of the KV cache in the physical blocks as shown in the slides here and because",
    "start": "769380",
    "end": "776760"
  },
  {
    "text": "the sequence A and B have the same prompt we'll your will will because they have the",
    "start": "776760",
    "end": "783360"
  },
  {
    "text": "same problem will only keep one copy but because both A and B they still have the same logical blocks so as shown on the",
    "start": "783360",
    "end": "790620"
  },
  {
    "text": "slide here and because each physical block can be mapped by multiple logical",
    "start": "790620",
    "end": "796920"
  },
  {
    "text": "blocks right naturally will keep a reference count for each physical blocks so in this case these two physical",
    "start": "796920",
    "end": "802560"
  },
  {
    "text": "blocks have a reference count of two and so as the next step these two",
    "start": "802560",
    "end": "808980"
  },
  {
    "text": "samples are begun to generate the output right and these two samples because they are different samples they will sample",
    "start": "808980",
    "end": "814560"
  },
  {
    "text": "for different outputs say here the sequence a generates the world price and the sequence B generates the word",
    "start": "814560",
    "end": "819660"
  },
  {
    "text": "interwine and both of them need to write the newly generated KV cache for these new words into the physical KV blocks",
    "start": "819660",
    "end": "826440"
  },
  {
    "text": "and now we Face an issue so where sequence a wants to write to the physical KV block it will check the",
    "start": "826440",
    "end": "833940"
  },
  {
    "text": "physical KV blocks reference count okay so it says see it's the reference count is two and then it will reduce this",
    "start": "833940",
    "end": "840899"
  },
  {
    "text": "reference count from two to one and do a copy on right and copy this physical block to a new block and then write the",
    "start": "840899",
    "end": "847920"
  },
  {
    "text": "corresponding KV cache of this World Bright to this new new bra and for",
    "start": "847920",
    "end": "853740"
  },
  {
    "text": "sequence B because it sees okay it checks that physical checks that physical block and since the",
    "start": "853740",
    "end": "860279"
  },
  {
    "text": "reference count is one it will directly write this newly January word interwind into this physical KV block and then the",
    "start": "860279",
    "end": "867480"
  },
  {
    "text": "generation continues with these new newly generated blocks so in this case we can see that that except for the very",
    "start": "867480",
    "end": "874620"
  },
  {
    "text": "last token block KV block for the prompt part all of the other token KV blocks",
    "start": "874620",
    "end": "880620"
  },
  {
    "text": "for the prompt can be shared across both sequences and this can greatly reduce the memory usage for the prom part",
    "start": "880620",
    "end": "887399"
  },
  {
    "text": "especially when your of your problem is very long and can release can also increase the throughput because of the",
    "start": "887399",
    "end": "892740"
  },
  {
    "text": "like memory reduction and uh but and vom does not only support parallel decoding",
    "start": "892740",
    "end": "899339"
  },
  {
    "text": "and VM also supports even more complicated decoding methods like beam search so beam search for people who are",
    "start": "899339",
    "end": "906360"
  },
  {
    "text": "not very familiar with beam search here so beam search is a decoding algorithm that is very popular for like machine",
    "start": "906360",
    "end": "912000"
  },
  {
    "text": "translation to find the most accurate translation and it can be also used in LMS say for example if you use LM for",
    "start": "912000",
    "end": "918660"
  },
  {
    "text": "translation and theme search is very similar to parallel sampling in the sense that the prompt part is also",
    "start": "918660",
    "end": "925079"
  },
  {
    "text": "shared by different beams so when different beams generates this prompt part can be shared across different",
    "start": "925079",
    "end": "931040"
  },
  {
    "text": "however beam search can can be more more Dynamic and more complex compared to",
    "start": "931040",
    "end": "937560"
  },
  {
    "text": "parallel sampling because the different beams can different beams can diverge",
    "start": "937560",
    "end": "943199"
  },
  {
    "text": "from another beam so for example here the new beam zero and beam one are all from the original beam zero and the new",
    "start": "943199",
    "end": "949680"
  },
  {
    "text": "beam 2 are from the original beam 2. so in this case this part of the of of the",
    "start": "949680",
    "end": "957000"
  },
  {
    "text": "KV Cache can also be shared across beam zero and beam one and the original like beam once KV Cache can be just deleted",
    "start": "957000",
    "end": "963480"
  },
  {
    "text": "and then as the next time step uh the the beam all the beam search result looks like this the B so beam zero is",
    "start": "963480",
    "end": "970500"
  },
  {
    "text": "from the original beam zero and the beam one and beam 2 are from the original beam 2. in this case this part can be",
    "start": "970500",
    "end": "976320"
  },
  {
    "text": "also be shared by the two beams beam one and beam two the you know that the sharing pattern is actually very Dynamic",
    "start": "976320",
    "end": "982680"
  },
  {
    "text": "and can change this over time and Via vrm dynamically supports supports this VR is a broad-based memory management",
    "start": "982680",
    "end": "990060"
  },
  {
    "text": "the page page memory and the block table mapping and this is for people who are familiar with operating system this is",
    "start": "990060",
    "end": "996180"
  },
  {
    "text": "like very similar for you for a fork tree so it's like when you keep calling Forks in your like operating system",
    "start": "996180",
    "end": "1002060"
  },
  {
    "text": "processes and how does the OS memory OS manages the memory it's very similar to",
    "start": "1002060",
    "end": "1007639"
  },
  {
    "text": "to this Fork 3 style and and this is also like this whole sharing",
    "start": "1007639",
    "end": "1013339"
  },
  {
    "text": "scenario is efficiently supported by vrm with this that with the dynamic block mapping and the copy on right mechanism",
    "start": "1013339",
    "end": "1020720"
  },
  {
    "text": "okay and uh yeah so two brief summary to briefly summary summarized so how does",
    "start": "1020720",
    "end": "1026660"
  },
  {
    "text": "page how do page attach NVM benefit ERM serving first as we success we can reduce memory fragmentation with paging",
    "start": "1026660",
    "end": "1033620"
  },
  {
    "text": "so this can reduce the memory waste and so we can fit more requests in the same batch so we can",
    "start": "1033620",
    "end": "1040220"
  },
  {
    "text": "improve the throughput and second is because of we can do more complicated sharing and we can further reduce the",
    "start": "1040220",
    "end": "1046760"
  },
  {
    "text": "memory usage and which further boosts us throughput and next let's dive a little bit into",
    "start": "1046760",
    "end": "1053600"
  },
  {
    "text": "the systems architecture and implementation of vlm so we'll have we have a we build Vim as an end-to-end LM",
    "start": "1053600",
    "end": "1061100"
  },
  {
    "text": "steering engine that includes a front-end distributed model executor and scheduler so we have a centralized",
    "start": "1061100",
    "end": "1067039"
  },
  {
    "text": "engine that manages the block table and at each iteration it sends the memory command to the GPU workers the cache",
    "start": "1067039",
    "end": "1073880"
  },
  {
    "text": "engine in the GPU worker will actually allocate the physical blocks and then execute the model sharps and more",
    "start": "1073880",
    "end": "1081500"
  },
  {
    "text": "specifically speaking so vrm is a mostly python project but we do have some cooler code for the page attention",
    "start": "1081500",
    "end": "1088220"
  },
  {
    "text": "kernel and we build on top of the popular stack of LMS like hugging face and hugging phase and Python and for",
    "start": "1088220",
    "end": "1095360"
  },
  {
    "text": "distribute Purity execution we use Megatron LM for the model parallel decoding and use Ray for the array for",
    "start": "1095360",
    "end": "1104120"
  },
  {
    "text": "the cluster management and the control plane communication here and uh yeah this is the performance results when we",
    "start": "1104120",
    "end": "1111320"
  },
  {
    "text": "are when we first announced vrm which we compete We compare ourselves with with",
    "start": "1111320",
    "end": "1118039"
  },
  {
    "text": "playing hugging phase.generate and another hugging phase inference solution hugging phase text generation inference",
    "start": "1118039",
    "end": "1125000"
  },
  {
    "text": "so compare with the most naive way of using LMS which is most of you might have used basically download a model",
    "start": "1125000",
    "end": "1131480"
  },
  {
    "text": "from hugging face and call model.generate we can achieve a 24x higher throughput compared to that naive",
    "start": "1131480",
    "end": "1137960"
  },
  {
    "text": "method and even compared with the hugging phase text generation in text generally inference this TGI framework",
    "start": "1137960",
    "end": "1145340"
  },
  {
    "text": "it's because it doesn't have a page attention back then you can achieve a up to 3.5 x higher throughput compared to",
    "start": "1145340",
    "end": "1152120"
  },
  {
    "text": "TGI and yeah and vrm is a open source project and it's open source on this",
    "start": "1152120",
    "end": "1158000"
  },
  {
    "text": "link and it's uh as we said it's mostly a python project so it's actually very easy to use you can just pip install it",
    "start": "1158000",
    "end": "1163880"
  },
  {
    "text": "and you can just start using it and the the we have documentation and we have got nearly 7K stars and",
    "start": "1163880",
    "end": "1171320"
  },
  {
    "text": "yeah for this open source project in the past several months months we have been working on intensively on model support",
    "start": "1171320",
    "end": "1177860"
  },
  {
    "text": "as we believe we are now supporting a very large set of the LMS basically most",
    "start": "1177860",
    "end": "1184039"
  },
  {
    "text": "of the I think 95 of the most popular airms are supported by via vrm right now",
    "start": "1184039",
    "end": "1189679"
  },
  {
    "text": "and you can pick your favorite one and you start start serving with vrm and our",
    "start": "1189679",
    "end": "1195740"
  },
  {
    "text": "first successful story of vrm is actually before our open source launch so we launched vrm on June 20th this",
    "start": "1195740",
    "end": "1203539"
  },
  {
    "text": "year and actually like two months is before that we will we have started serving serving the rikuna the famous",
    "start": "1203539",
    "end": "1210260"
  },
  {
    "text": "vikuna model and the chessboard Arena demo which is vrm and it has been deployed till the day it's like for like",
    "start": "1210260",
    "end": "1217580"
  },
  {
    "text": "about five months and it's the back end for llama based models and many most",
    "start": "1217580",
    "end": "1222740"
  },
  {
    "text": "info lava based models including vikuna alpaca koala and many more and",
    "start": "1222740",
    "end": "1227900"
  },
  {
    "text": "this demo receives a average traffic of 40K conversations per day and with vrm",
    "start": "1227900",
    "end": "1234320"
  },
  {
    "text": "we can first reduce the total number of gpus to serve by 50 and even with like",
    "start": "1234320",
    "end": "1239900"
  },
  {
    "text": "half of the gpus we can have we can serve 2x to 3x more requests per second",
    "start": "1239900",
    "end": "1245000"
  },
  {
    "text": "and and this game is only achieved when we",
    "start": "1245000",
    "end": "1251059"
  },
  {
    "text": "serve we serve vrm for a part of models and if you use VMS to serve all of your models and the game can be more",
    "start": "1251059",
    "end": "1257539"
  },
  {
    "text": "and as our product keeps growing we get many many adopters so so on the open",
    "start": "1257539",
    "end": "1264020"
  },
  {
    "text": "source side first we have fastjet as we just discussed and also we have Sky pilot so Sky pilot is another project",
    "start": "1264020",
    "end": "1270440"
  },
  {
    "text": "here we developed Skylab so we use Sky pilot during our development and also",
    "start": "1270440",
    "end": "1276679"
  },
  {
    "text": "you can easily launch a BLM cursor VM server with Sky pilot and also we",
    "start": "1276679",
    "end": "1283940"
  },
  {
    "text": "have many other like LM open source ERM Frameworks like open chat openings drug and with their LM they are all using",
    "start": "1283940",
    "end": "1290000"
  },
  {
    "text": "using vrm for for in first and we are talking to many companies and many companies actually we haven't talked to",
    "start": "1290000",
    "end": "1297080"
  },
  {
    "text": "some of them and they automatically start using vrm okay yeah and uh so you",
    "start": "1297080",
    "end": "1302659"
  },
  {
    "text": "might say okay talk is cheap show me the code so this is all of the code we can find on GitHub that these companies are",
    "start": "1302659",
    "end": "1308900"
  },
  {
    "text": "using via so so we want to show you okay vrm is actually a very mature project",
    "start": "1308900",
    "end": "1313940"
  },
  {
    "text": "and many people are using this video and vlm for their inference survey workload and yeah and more Beyond of vrm Beyond",
    "start": "1313940",
    "end": "1322700"
  },
  {
    "text": "of vrm the open source project from a more Ivy more like a massive perspective",
    "start": "1322700",
    "end": "1328220"
  },
  {
    "text": "the page attention algorithm has became the industrial standard the all of the other like uh other influence Frameworks",
    "start": "1328220",
    "end": "1335900"
  },
  {
    "text": "like like uh like built by others like fireworks or AI or or our Baseline just",
    "start": "1335900",
    "end": "1341720"
  },
  {
    "text": "now having for stgi and the recent Nvidia trm they are all using page attention in their in their serving",
    "start": "1341720",
    "end": "1348919"
  },
  {
    "text": "engine to to accelerate and lastly let me briefly dive into okay how you can",
    "start": "1348919",
    "end": "1355820"
  },
  {
    "text": "get started with vrm so VM API is actually very very simple so first we",
    "start": "1355820",
    "end": "1361159"
  },
  {
    "text": "show a case if you want to use vrm for offline batch batch influence",
    "start": "1361159",
    "end": "1366380"
  },
  {
    "text": "for offline battery inference so you can just import the lrm class from vrm and",
    "start": "1366380",
    "end": "1371840"
  },
  {
    "text": "and initialize the LM and feed in the prompt and that's it and vrm will",
    "start": "1371840",
    "end": "1377419"
  },
  {
    "text": "automatically batch all of the executions of all the prompts and give you a result so you can fit as many as",
    "start": "1377419",
    "end": "1383059"
  },
  {
    "text": "prompts as possible into this prompt list and vrm will automatically handle the batching for you and handle the",
    "start": "1383059",
    "end": "1389780"
  },
  {
    "text": "correct batch side and also manage the memory with page detention and next because we are a server we also",
    "start": "1389780",
    "end": "1396620"
  },
  {
    "text": "provide a open AI compatible server as a demo server so you can launch a open air",
    "start": "1396620",
    "end": "1401840"
  },
  {
    "text": "server open AI command server with a single command and then you can query the the API as if you are querying an",
    "start": "1401840",
    "end": "1409460"
  },
  {
    "text": "open AI API so yeah so let me Briefly summarize so in",
    "start": "1409460",
    "end": "1415640"
  },
  {
    "text": "conclusion vrm can improve the memory efficiency of of LM Serene by 2.5 x to",
    "start": "1415640",
    "end": "1422360"
  },
  {
    "text": "5x by reducing the fragmentation and enabling the sharing with page attention and vlm can outperform the state of the",
    "start": "1422360",
    "end": "1430159"
  },
  {
    "text": "art by 1.7 x to 4X in terms of the serving soup and we are building vrm as",
    "start": "1430159",
    "end": "1436460"
  },
  {
    "text": "a vibrant open source project that supports many models and decoding methods and vrm is getting widely",
    "start": "1436460",
    "end": "1442220"
  },
  {
    "text": "adopted so here we showed our GitHub link you can check out our code and also our",
    "start": "1442220",
    "end": "1447980"
  },
  {
    "text": "blog this is this is a very a good starting point for you to learn about vrm and the page attention technology",
    "start": "1447980",
    "end": "1454580"
  },
  {
    "text": "behind it and also we recently recently published our paper on in the sosp",
    "start": "1454580",
    "end": "1462620"
  },
  {
    "text": "conference so if you want to dive more into the page attention work so please take a look at the paper and finally if",
    "start": "1462620",
    "end": "1469520"
  },
  {
    "text": "you have any questions you can go to join this Discord Channel and just directly talk to us yeah and and yeah",
    "start": "1469520",
    "end": "1476840"
  },
  {
    "text": "very happy to be here and thanks for listening foreign",
    "start": "1476840",
    "end": "1482840"
  },
  {
    "text": "yes I think so because those those rnas uh okay so the question is about uh okay",
    "start": "1505000",
    "end": "1511940"
  },
  {
    "text": "there are many recent neural networks that are more like recurrent based like RN style neural network uh can with the",
    "start": "1511940",
    "end": "1518419"
  },
  {
    "text": "page attention technique still apply to those kinds of neural networks I believe the answer is yes so because even with",
    "start": "1518419",
    "end": "1525980"
  },
  {
    "text": "recurrent neural network you still need to manage the memory and you still don't know okay what's the output length is so you need to like like if you don't do",
    "start": "1525980",
    "end": "1533120"
  },
  {
    "text": "page paging in memory you still need to like reserve a big chunk of memory for like for those for like every request",
    "start": "1533120",
    "end": "1541520"
  },
  {
    "text": "and it will still waste a lot of memory so in this case like I think paging will also help on the RN side but like but",
    "start": "1541520",
    "end": "1550520"
  },
  {
    "text": "obviously like a paging will help but like page attention will not help if it's not using attention yeah",
    "start": "1550520",
    "end": "1558278"
  },
  {
    "text": "is",
    "start": "1563659",
    "end": "1566659"
  },
  {
    "text": "okay the the question is how we identify the the memory problem uh I think okay the project is started",
    "start": "1573380",
    "end": "1580820"
  },
  {
    "text": "by just uh building the LM serving system opens to the LM serving serving",
    "start": "1580820",
    "end": "1586220"
  },
  {
    "text": "system from scratch because back then there was no one available in in GitHub",
    "start": "1586220",
    "end": "1591380"
  },
  {
    "text": "in open source words and then what I think would be different is",
    "start": "1591380",
    "end": "1596960"
  },
  {
    "text": "I think the way we are different is that we focus on high school serving rather than like reducing the latency because",
    "start": "1596960",
    "end": "1603500"
  },
  {
    "text": "that one was more like effective in in reducing the cost per recast and then",
    "start": "1603500",
    "end": "1610640"
  },
  {
    "text": "like when you target The Heist group of serving then you you've you wanted to basically we wanted to increase the",
    "start": "1610640",
    "end": "1617059"
  },
  {
    "text": "batch size and then we found out the problem basically when you start implementing an",
    "start": "1617059",
    "end": "1622760"
  },
  {
    "text": "inference server once you reach that point you'll find this issue yeah the how you manage that memory is like all",
    "start": "1622760",
    "end": "1629299"
  },
  {
    "text": "the existing messages will not be elegant enough and will waste a lot of memory",
    "start": "1629299",
    "end": "1634900"
  },
  {
    "text": "go with it yeah from hands-on experience",
    "start": "1635840",
    "end": "1644799"
  },
  {
    "text": "you may be over there",
    "start": "1646120",
    "end": "1649779"
  },
  {
    "text": "oh yes that's a very good question so let me repeat the question so the question is about",
    "start": "1654080",
    "end": "1659260"
  },
  {
    "text": "like are there any like cash misses or something like that yeah I think the the answer is no the biggest reason is",
    "start": "1659260",
    "end": "1666500"
  },
  {
    "text": "because of the attention computations pattern it needs to uh for the to",
    "start": "1666500",
    "end": "1671539"
  },
  {
    "text": "generate a new world it needs to take a look at all of its like previous words so there's like at every step all of",
    "start": "1671539",
    "end": "1677299"
  },
  {
    "text": "these previous words need to be like looked at once so there's no misses something like that it's because of the",
    "start": "1677299",
    "end": "1683299"
  },
  {
    "text": "computation pattern so there will be that's the misses yeah okay",
    "start": "1683299",
    "end": "1689299"
  },
  {
    "text": "um yeah uh so you've mentioned a comparison with uh who face TGI I didn't you showed that",
    "start": "1689299",
    "end": "1696440"
  },
  {
    "text": "TGI is maybe incorporating uh paid attention so is there going to be any",
    "start": "1696440",
    "end": "1701960"
  },
  {
    "text": "benefits to using vllm directly over just using the pay attention mechanism",
    "start": "1701960",
    "end": "1707960"
  },
  {
    "text": "oh okay so the question is like what is the current benefit of using vlm as TGI",
    "start": "1707960",
    "end": "1714260"
  },
  {
    "text": "already Incorporated the page attention algorithm right so uh one thing we like",
    "start": "1714260",
    "end": "1720020"
  },
  {
    "text": "to highlight is that DJI is doing pretty good job in uh reducing the fragmentation using page attention but",
    "start": "1720020",
    "end": "1726380"
  },
  {
    "text": "they didn't like do a great job in like memory sharing that part is actually more like uh more complex to implement",
    "start": "1726380",
    "end": "1734419"
  },
  {
    "text": "so we have still have this performance benefits when the when for the complex",
    "start": "1734419",
    "end": "1740419"
  },
  {
    "text": "decoding methods like beam search and also uh",
    "start": "1740419",
    "end": "1745640"
  },
  {
    "text": "uh in the engineering side I think we did a pretty good job to I mean we did a",
    "start": "1745640",
    "end": "1750860"
  },
  {
    "text": "pretty good job on the optimizations also wanted to pay attention so we believe we are still have higher",
    "start": "1750860",
    "end": "1758480"
  },
  {
    "text": "performance than TGI yeah and we are on apache2 license yes they are like very",
    "start": "1758480",
    "end": "1764539"
  },
  {
    "text": "yeah yeah [Applause]",
    "start": "1764539",
    "end": "1771440"
  },
  {
    "text": "okay yeah",
    "start": "1771440",
    "end": "1774100"
  },
  {
    "text": "okay so the question is about okay we talk about memory sharing for Bing search and parallel decoding how about",
    "start": "1794240",
    "end": "1800000"
  },
  {
    "text": "some other cases like sharing prefix across different requests and yeah that's a great question actually we did",
    "start": "1800000",
    "end": "1806059"
  },
  {
    "text": "that experimenting in the paper so it actually works yes yes because there are some some small",
    "start": "1806059",
    "end": "1813500"
  },
  {
    "text": "engineering issues so we haven't merged that into the main branch but we will we will do it like eventually",
    "start": "1813500",
    "end": "1820159"
  },
  {
    "text": "yeah okay yeah yeah okay the first one okay I think the maybe last question",
    "start": "1820159",
    "end": "1825320"
  },
  {
    "text": "yeah so great work any question you talk a",
    "start": "1825320",
    "end": "1830960"
  },
  {
    "text": "lot about the throughput what about latency that too",
    "start": "1830960",
    "end": "1836559"
  },
  {
    "text": "okay I think for the latency okay so the question is about we talk a lot about Super and what about latency so I think",
    "start": "1836559",
    "end": "1843200"
  },
  {
    "text": "uh so at a very high level we focus on throughput is because throughput is actually what reduces the cost because",
    "start": "1843200",
    "end": "1849559"
  },
  {
    "text": "uh like reducing the latency like if you reduce the latency at the cost of",
    "start": "1849559",
    "end": "1855080"
  },
  {
    "text": "reducing throughput it's actually not good if you are serving a very big bigger bigger big big like service for",
    "start": "1855080",
    "end": "1861440"
  },
  {
    "text": "everyone and and more specifically I think we can categorize the the latency",
    "start": "1861440",
    "end": "1867440"
  },
  {
    "text": "of another techniques into two categories and one category is like it's just pure optimization it's optimized",
    "start": "1867440",
    "end": "1873679"
  },
  {
    "text": "latency and Source increased input and we will definitely incorporate these kind of techniques for example like",
    "start": "1873679",
    "end": "1879140"
  },
  {
    "text": "kernel Fusion things like that yeah we will definitely incorporate that in BLM in the future and uh and furthermore the",
    "start": "1879140",
    "end": "1887059"
  },
  {
    "text": "other side of the the work like like say trading off latency for throughput",
    "start": "1887059",
    "end": "1893840"
  },
  {
    "text": "something like that uh treatment of throughput for latency we will will be more cautious in that space there are",
    "start": "1893840",
    "end": "1900620"
  },
  {
    "text": "some techniques over there like speculative decoding uh that's one like",
    "start": "1900620",
    "end": "1905740"
  },
  {
    "text": "most very important and very impactful technique and we are thinking about it but we will be uh we'll implement the",
    "start": "1905740",
    "end": "1913220"
  },
  {
    "text": "first set of techniques first and then think about the second step and cool yeah thanks everyone yeah yeah",
    "start": "1913220",
    "end": "1920000"
  },
  {
    "text": "thank you everyone",
    "start": "1920000",
    "end": "1923600"
  }
]