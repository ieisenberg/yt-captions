[
  {
    "text": "[Music]",
    "start": "170",
    "end": "14799"
  },
  {
    "text": "hi",
    "start": "14799",
    "end": "15120"
  },
  {
    "text": "everyone my name is matt i'm a",
    "start": "15120",
    "end": "18320"
  },
  {
    "text": "researcher at google",
    "start": "18320",
    "end": "19520"
  },
  {
    "text": "on the brain team and today i'm going to",
    "start": "19520",
    "end": "21279"
  },
  {
    "text": "talk about jax",
    "start": "21279",
    "end": "23279"
  },
  {
    "text": "so to motivate jax we're going to start",
    "start": "23279",
    "end": "25439"
  },
  {
    "text": "with a question which is how might you",
    "start": "25439",
    "end": "27279"
  },
  {
    "text": "implement",
    "start": "27279",
    "end": "27920"
  },
  {
    "text": "a deep neural network from scratch in",
    "start": "27920",
    "end": "30080"
  },
  {
    "text": "python so forget about all the neural",
    "start": "30080",
    "end": "31840"
  },
  {
    "text": "network training libraries you know",
    "start": "31840",
    "end": "33440"
  },
  {
    "text": "um how would you do it from scratch uh",
    "start": "33440",
    "end": "35680"
  },
  {
    "text": "so when you're doing anything numerical",
    "start": "35680",
    "end": "37360"
  },
  {
    "text": "a good idea is to reach for",
    "start": "37360",
    "end": "38960"
  },
  {
    "text": "numpy and you can write a simple neural",
    "start": "38960",
    "end": "41440"
  },
  {
    "text": "network in numpy just in a few lines",
    "start": "41440",
    "end": "43440"
  },
  {
    "text": "uh calling functions like numpy dot and",
    "start": "43440",
    "end": "45520"
  },
  {
    "text": "doing some numpy",
    "start": "45520",
    "end": "47200"
  },
  {
    "text": "element-wise non-linearities um so",
    "start": "47200",
    "end": "49600"
  },
  {
    "text": "that'll give you a",
    "start": "49600",
    "end": "50719"
  },
  {
    "text": "fully connected uh neural network and",
    "start": "50719",
    "end": "52879"
  },
  {
    "text": "then you can write a loss function and",
    "start": "52879",
    "end": "54480"
  },
  {
    "text": "you know another few lines in this case",
    "start": "54480",
    "end": "55840"
  },
  {
    "text": "maybe a squared error loss function",
    "start": "55840",
    "end": "57600"
  },
  {
    "text": "and so you might ask you know if it's so",
    "start": "57600",
    "end": "59760"
  },
  {
    "text": "easy to express",
    "start": "59760",
    "end": "61680"
  },
  {
    "text": "the programs involved in neural networks",
    "start": "61680",
    "end": "65040"
  },
  {
    "text": "with just numpy you know what's missing",
    "start": "65040",
    "end": "67360"
  },
  {
    "text": "here what else do we need",
    "start": "67360",
    "end": "68960"
  },
  {
    "text": "so one thing we need is we need the",
    "start": "68960",
    "end": "70960"
  },
  {
    "text": "ability to use accelerator hardware",
    "start": "70960",
    "end": "72960"
  },
  {
    "text": "like gpus and tpus um that's just the",
    "start": "72960",
    "end": "76000"
  },
  {
    "text": "start",
    "start": "76000",
    "end": "76560"
  },
  {
    "text": "we also need things like efficient",
    "start": "76560",
    "end": "78400"
  },
  {
    "text": "automatic differentiation for",
    "start": "78400",
    "end": "80000"
  },
  {
    "text": "doing optimization and training we might",
    "start": "80000",
    "end": "82880"
  },
  {
    "text": "want to be able to stage out",
    "start": "82880",
    "end": "84240"
  },
  {
    "text": "to an optimizing compiler that can do",
    "start": "84240",
    "end": "86960"
  },
  {
    "text": "optimizations like fusion and memory",
    "start": "86960",
    "end": "89040"
  },
  {
    "text": "layout and and re-materialization um we",
    "start": "89040",
    "end": "92000"
  },
  {
    "text": "might want",
    "start": "92000",
    "end": "92479"
  },
  {
    "text": "you know for an advanced algorithm or",
    "start": "92479",
    "end": "94079"
  },
  {
    "text": "something like differential privacy",
    "start": "94079",
    "end": "95840"
  },
  {
    "text": "uh we might want things like automatic",
    "start": "95840",
    "end": "97600"
  },
  {
    "text": "vectorized batching of",
    "start": "97600",
    "end": "98880"
  },
  {
    "text": "operations and finally we might want to",
    "start": "98880",
    "end": "101040"
  },
  {
    "text": "parallelize over multiple accelerators",
    "start": "101040",
    "end": "103360"
  },
  {
    "text": "um so jax is about adding these missing",
    "start": "103360",
    "end": "106000"
  },
  {
    "text": "things it's about saying",
    "start": "106000",
    "end": "107119"
  },
  {
    "text": "you know numpy is such a great tool",
    "start": "107119",
    "end": "109280"
  },
  {
    "text": "let's let's uh you know keep",
    "start": "109280",
    "end": "110960"
  },
  {
    "text": "keep writing code with numpy api",
    "start": "110960",
    "end": "114159"
  },
  {
    "text": "but let's add in these extra features in",
    "start": "114159",
    "end": "116240"
  },
  {
    "text": "particular jax uses the xla",
    "start": "116240",
    "end": "118159"
  },
  {
    "text": "optimizing compiler to be able to you",
    "start": "118159",
    "end": "120719"
  },
  {
    "text": "know project",
    "start": "120719",
    "end": "121759"
  },
  {
    "text": "your code onto hardware accelerators and",
    "start": "121759",
    "end": "124240"
  },
  {
    "text": "then",
    "start": "124240",
    "end": "124799"
  },
  {
    "text": "the main action in jacks is actually in",
    "start": "124799",
    "end": "127119"
  },
  {
    "text": "these function transformations that are",
    "start": "127119",
    "end": "128560"
  },
  {
    "text": "at the bottom",
    "start": "128560",
    "end": "129280"
  },
  {
    "text": "of the screen here these are things like",
    "start": "129280",
    "end": "131280"
  },
  {
    "text": "automatic differentiation",
    "start": "131280",
    "end": "132800"
  },
  {
    "text": "uh or just-in-time optimized compilation",
    "start": "132800",
    "end": "136239"
  },
  {
    "text": "or things like vectorized batching so to",
    "start": "136239",
    "end": "138800"
  },
  {
    "text": "give you",
    "start": "138800",
    "end": "139440"
  },
  {
    "text": "more of an idea of what that actually",
    "start": "139440",
    "end": "141440"
  },
  {
    "text": "looks like we're going to jump into a",
    "start": "141440",
    "end": "143360"
  },
  {
    "text": "lightning fast",
    "start": "143360",
    "end": "144480"
  },
  {
    "text": "demo all right so here is our co-lab",
    "start": "144480",
    "end": "147680"
  },
  {
    "text": "notebook",
    "start": "147680",
    "end": "148400"
  },
  {
    "text": "and uh we're just going to take a look",
    "start": "148400",
    "end": "149840"
  },
  {
    "text": "at jax so the place to start",
    "start": "149840",
    "end": "151840"
  },
  {
    "text": "is numpy on running on a gpu or a tpu",
    "start": "151840",
    "end": "156000"
  },
  {
    "text": "in this case it's on a tpu but jax code",
    "start": "156000",
    "end": "157760"
  },
  {
    "text": "looks the same no matter what you're",
    "start": "157760",
    "end": "158800"
  },
  {
    "text": "running on",
    "start": "158800",
    "end": "159840"
  },
  {
    "text": "so let's start by importing jax.numpy as",
    "start": "159840",
    "end": "162160"
  },
  {
    "text": "jnp",
    "start": "162160",
    "end": "162879"
  },
  {
    "text": "and importing jacks and we'll just",
    "start": "162879",
    "end": "164400"
  },
  {
    "text": "generate a big random matrix so it's a",
    "start": "164400",
    "end": "166560"
  },
  {
    "text": "5000 by 5000 float32 matrix",
    "start": "166560",
    "end": "169360"
  },
  {
    "text": "and then we'll start doing numpy things",
    "start": "169360",
    "end": "171040"
  },
  {
    "text": "so we can do",
    "start": "171040",
    "end": "172720"
  },
  {
    "text": "matrix multiply we can print out the",
    "start": "172720",
    "end": "174640"
  },
  {
    "text": "first coefficient of the result",
    "start": "174640",
    "end": "176400"
  },
  {
    "text": "um you know we can do transposing of",
    "start": "176400",
    "end": "178400"
  },
  {
    "text": "course we can do",
    "start": "178400",
    "end": "180239"
  },
  {
    "text": "element we can do fancy indexing and all",
    "start": "180239",
    "end": "182640"
  },
  {
    "text": "these things sort of work",
    "start": "182640",
    "end": "183680"
  },
  {
    "text": "you might you know think that you're",
    "start": "183680",
    "end": "184959"
  },
  {
    "text": "using uh the original numpy one way you",
    "start": "184959",
    "end": "187040"
  },
  {
    "text": "can tell that you're not",
    "start": "187040",
    "end": "188080"
  },
  {
    "text": "though is if you do a simple timing",
    "start": "188080",
    "end": "189840"
  },
  {
    "text": "benchmark uh so",
    "start": "189840",
    "end": "191200"
  },
  {
    "text": "to multiply this 5000 by 5000 matrix by",
    "start": "191200",
    "end": "193840"
  },
  {
    "text": "itself",
    "start": "193840",
    "end": "194480"
  },
  {
    "text": "um this jackson umpire running on a on a",
    "start": "194480",
    "end": "196640"
  },
  {
    "text": "tpu doing it in 12 milliseconds",
    "start": "196640",
    "end": "198959"
  },
  {
    "text": "uh maybe you know an optimized numpy",
    "start": "198959",
    "end": "201040"
  },
  {
    "text": "build on a really fast cpu could do it",
    "start": "201040",
    "end": "202720"
  },
  {
    "text": "in 250 to 300 milliseconds so this is",
    "start": "202720",
    "end": "204720"
  },
  {
    "text": "running",
    "start": "204720",
    "end": "205280"
  },
  {
    "text": "you know quite quite quickly this block",
    "start": "205280",
    "end": "207599"
  },
  {
    "text": "console ready on the end is because",
    "start": "207599",
    "end": "209280"
  },
  {
    "text": "by default jacks will execute operations",
    "start": "209280",
    "end": "212239"
  },
  {
    "text": "asynchronously in the background",
    "start": "212239",
    "end": "213680"
  },
  {
    "text": "and only pull only block the sort of",
    "start": "213680",
    "end": "215760"
  },
  {
    "text": "main python thread or pull values back",
    "start": "215760",
    "end": "217519"
  },
  {
    "text": "to",
    "start": "217519",
    "end": "217840"
  },
  {
    "text": "the cpu if they're needed for printing",
    "start": "217840",
    "end": "220480"
  },
  {
    "text": "or plotting or python control flow or",
    "start": "220480",
    "end": "222080"
  },
  {
    "text": "saving to disk this sort of thing",
    "start": "222080",
    "end": "224239"
  },
  {
    "text": "and so for micro benchmarks we just add",
    "start": "224239",
    "end": "225680"
  },
  {
    "text": "this block until ready on the end",
    "start": "225680",
    "end": "227360"
  },
  {
    "text": "uh but yeah this is how we're you know",
    "start": "227360",
    "end": "229599"
  },
  {
    "text": "using jax numpy and running it on",
    "start": "229599",
    "end": "232159"
  },
  {
    "text": "a hardware accelerator um so that's",
    "start": "232159",
    "end": "234560"
  },
  {
    "text": "great but as i mentioned before the real",
    "start": "234560",
    "end": "236400"
  },
  {
    "text": "action with jax",
    "start": "236400",
    "end": "237519"
  },
  {
    "text": "comes in these function transformations",
    "start": "237519",
    "end": "239200"
  },
  {
    "text": "so the first one we'll look at",
    "start": "239200",
    "end": "240560"
  },
  {
    "text": "is for automatic differentiation that's",
    "start": "240560",
    "end": "242400"
  },
  {
    "text": "called grad so let's import grad",
    "start": "242400",
    "end": "244720"
  },
  {
    "text": "and write a simple python function here",
    "start": "244720",
    "end": "246560"
  },
  {
    "text": "so this python function has some",
    "start": "246560",
    "end": "248640"
  },
  {
    "text": "python control flow if x is greater than",
    "start": "248640",
    "end": "250400"
  },
  {
    "text": "zero return one thing",
    "start": "250400",
    "end": "251680"
  },
  {
    "text": "otherwise return this other thing grad",
    "start": "251680",
    "end": "254799"
  },
  {
    "text": "is a function transformation that takes",
    "start": "254799",
    "end": "257600"
  },
  {
    "text": "a python callable and gives you back a",
    "start": "257600",
    "end": "259120"
  },
  {
    "text": "python callable",
    "start": "259120",
    "end": "260320"
  },
  {
    "text": "if you give it a python callable it",
    "start": "260320",
    "end": "261600"
  },
  {
    "text": "evaluates a mathematical function f",
    "start": "261600",
    "end": "263440"
  },
  {
    "text": "it gives you back a python callable that",
    "start": "263440",
    "end": "265040"
  },
  {
    "text": "evaluates the gradient of f",
    "start": "265040",
    "end": "267199"
  },
  {
    "text": "and so we can just call grad of f and",
    "start": "267199",
    "end": "269680"
  },
  {
    "text": "run that we can exercise both sides of",
    "start": "269680",
    "end": "271280"
  },
  {
    "text": "the control flow and indeed we're just",
    "start": "271280",
    "end": "272479"
  },
  {
    "text": "getting the the gradient of this",
    "start": "272479",
    "end": "274080"
  },
  {
    "text": "simple scalar function back out one nice",
    "start": "274080",
    "end": "276160"
  },
  {
    "text": "thing about",
    "start": "276160",
    "end": "277120"
  },
  {
    "text": "this sort of function oriented view of",
    "start": "277120",
    "end": "279759"
  },
  {
    "text": "automatic differentiation is that it's",
    "start": "279759",
    "end": "281199"
  },
  {
    "text": "easy to do higher order differentiation",
    "start": "281199",
    "end": "282720"
  },
  {
    "text": "so in this case",
    "start": "282720",
    "end": "283520"
  },
  {
    "text": "we can just apply grad to its own output",
    "start": "283520",
    "end": "285840"
  },
  {
    "text": "apply it many times",
    "start": "285840",
    "end": "287280"
  },
  {
    "text": "and then we're doing higher order",
    "start": "287280",
    "end": "288479"
  },
  {
    "text": "differentiation so obviously that's a",
    "start": "288479",
    "end": "290880"
  },
  {
    "text": "very simple toy",
    "start": "290880",
    "end": "291840"
  },
  {
    "text": "example but we can also just go to that",
    "start": "291840",
    "end": "294160"
  },
  {
    "text": "neural network code that simple neural",
    "start": "294160",
    "end": "295600"
  },
  {
    "text": "network that we wrote on the first slide",
    "start": "295600",
    "end": "297919"
  },
  {
    "text": "here it is and here is just some simple",
    "start": "297919",
    "end": "300800"
  },
  {
    "text": "code to initialize it with",
    "start": "300800",
    "end": "302320"
  },
  {
    "text": "totally random parameters and and you",
    "start": "302320",
    "end": "304479"
  },
  {
    "text": "know random",
    "start": "304479",
    "end": "305759"
  },
  {
    "text": "input data so this is just all you know",
    "start": "305759",
    "end": "307840"
  },
  {
    "text": "a toy toy model",
    "start": "307840",
    "end": "309520"
  },
  {
    "text": "so we have our loss function we can",
    "start": "309520",
    "end": "311120"
  },
  {
    "text": "evaluate it on these randomly",
    "start": "311120",
    "end": "312479"
  },
  {
    "text": "initialized parameters and this",
    "start": "312479",
    "end": "313840"
  },
  {
    "text": "random data we get sum some value out",
    "start": "313840",
    "end": "316160"
  },
  {
    "text": "and we can run a simple gradient descent",
    "start": "316160",
    "end": "317919"
  },
  {
    "text": "loop so we're using",
    "start": "317919",
    "end": "319120"
  },
  {
    "text": "the gradient of the loss and we're",
    "start": "319120",
    "end": "320720"
  },
  {
    "text": "taking small steps in the",
    "start": "320720",
    "end": "322639"
  },
  {
    "text": "negative gradient direction to minimize",
    "start": "322639",
    "end": "325520"
  },
  {
    "text": "our our loss and in this case",
    "start": "325520",
    "end": "326800"
  },
  {
    "text": "you know our loss has indeed gone down",
    "start": "326800",
    "end": "329039"
  },
  {
    "text": "so that's great",
    "start": "329039",
    "end": "330000"
  },
  {
    "text": "um so jax has a lot of advanced auto",
    "start": "330000",
    "end": "332800"
  },
  {
    "text": "diff features this is you know maybe the",
    "start": "332800",
    "end": "334400"
  },
  {
    "text": "the",
    "start": "334400",
    "end": "335280"
  },
  {
    "text": "simplest demo but if you like autodiff",
    "start": "335280",
    "end": "337919"
  },
  {
    "text": "you should definitely take a look at jax",
    "start": "337919",
    "end": "339520"
  },
  {
    "text": "it has all kinds of fancy stuff that i",
    "start": "339520",
    "end": "341680"
  },
  {
    "text": "don't have time to go into",
    "start": "341680",
    "end": "343120"
  },
  {
    "text": "um but for now instead let's look at our",
    "start": "343120",
    "end": "345120"
  },
  {
    "text": "next function transformation so",
    "start": "345120",
    "end": "346960"
  },
  {
    "text": "the next one is end-to-end optimized",
    "start": "346960",
    "end": "349039"
  },
  {
    "text": "compilation",
    "start": "349039",
    "end": "350000"
  },
  {
    "text": "with the xla compiler using the jax",
    "start": "350000",
    "end": "352479"
  },
  {
    "text": "transformation jit",
    "start": "352479",
    "end": "353840"
  },
  {
    "text": "um so in this case again we have a",
    "start": "353840",
    "end": "356080"
  },
  {
    "text": "function that takes in a python callable",
    "start": "356080",
    "end": "357759"
  },
  {
    "text": "and gives you back a new python callable",
    "start": "357759",
    "end": "359440"
  },
  {
    "text": "so we're making the los jit version of",
    "start": "359440",
    "end": "362319"
  },
  {
    "text": "the loss function",
    "start": "362319",
    "end": "363120"
  },
  {
    "text": "and we can see that if we call it it has",
    "start": "363120",
    "end": "364800"
  },
  {
    "text": "the same input output behavior as our",
    "start": "364800",
    "end": "366639"
  },
  {
    "text": "loss from before in fact we're getting",
    "start": "366639",
    "end": "367919"
  },
  {
    "text": "the same numerical value out when we",
    "start": "367919",
    "end": "369360"
  },
  {
    "text": "call it",
    "start": "369360",
    "end": "370400"
  },
  {
    "text": "the difference is in timing so if we",
    "start": "370400",
    "end": "372160"
  },
  {
    "text": "time how long it takes to execute the",
    "start": "372160",
    "end": "374000"
  },
  {
    "text": "first",
    "start": "374000",
    "end": "374560"
  },
  {
    "text": "version of the loss because it was sort",
    "start": "374560",
    "end": "376319"
  },
  {
    "text": "of executing every operation on its own",
    "start": "376319",
    "end": "378800"
  },
  {
    "text": "um you know it's it's somewhat slower",
    "start": "378800",
    "end": "380720"
  },
  {
    "text": "the jit optimized version of our loss",
    "start": "380720",
    "end": "383039"
  },
  {
    "text": "function is dramatically faster",
    "start": "383039",
    "end": "384960"
  },
  {
    "text": "not only is this staging out of python",
    "start": "384960",
    "end": "386639"
  },
  {
    "text": "so there's no python overhead",
    "start": "386639",
    "end": "388080"
  },
  {
    "text": "but also xla is an end-to-end optimizing",
    "start": "388080",
    "end": "390319"
  },
  {
    "text": "compiler and so it's performing all",
    "start": "390319",
    "end": "392000"
  },
  {
    "text": "these optimizations like",
    "start": "392000",
    "end": "393440"
  },
  {
    "text": "optimizing memory layouts and and",
    "start": "393440",
    "end": "395520"
  },
  {
    "text": "performing fusions and these sorts of",
    "start": "395520",
    "end": "396960"
  },
  {
    "text": "things",
    "start": "396960",
    "end": "397680"
  },
  {
    "text": "um so can execute much faster we can do",
    "start": "397680",
    "end": "400639"
  },
  {
    "text": "the same thing to our gradient function",
    "start": "400639",
    "end": "402080"
  },
  {
    "text": "uh so we can",
    "start": "402080",
    "end": "403039"
  },
  {
    "text": "jit our our gradient function and indeed",
    "start": "403039",
    "end": "405120"
  },
  {
    "text": "that's uh you know very fast as we might",
    "start": "405120",
    "end": "407120"
  },
  {
    "text": "expect",
    "start": "407120",
    "end": "407759"
  },
  {
    "text": "and we can do another uh you know set of",
    "start": "407759",
    "end": "410560"
  },
  {
    "text": "steps of of gradient descent",
    "start": "410560",
    "end": "412319"
  },
  {
    "text": "and uh indeed this this version executed",
    "start": "412319",
    "end": "414720"
  },
  {
    "text": "much more quickly",
    "start": "414720",
    "end": "415599"
  },
  {
    "text": "and our loss is still going down uh so",
    "start": "415599",
    "end": "417360"
  },
  {
    "text": "that's great um so it's jit",
    "start": "417360",
    "end": "419360"
  },
  {
    "text": "and grad putting them together already",
    "start": "419360",
    "end": "421039"
  },
  {
    "text": "we can do some some nice neural network",
    "start": "421039",
    "end": "422639"
  },
  {
    "text": "training at pretty high performance",
    "start": "422639",
    "end": "424479"
  },
  {
    "text": "um i wanted to show one more",
    "start": "424479",
    "end": "426080"
  },
  {
    "text": "transformation though this is v map",
    "start": "426080",
    "end": "427759"
  },
  {
    "text": "vmap is actually oftentimes people's",
    "start": "427759",
    "end": "429759"
  },
  {
    "text": "favorite transformation when they start",
    "start": "429759",
    "end": "431039"
  },
  {
    "text": "to learn jacks",
    "start": "431039",
    "end": "432479"
  },
  {
    "text": "let's import it so what vmap does",
    "start": "432479",
    "end": "434639"
  },
  {
    "text": "semantically is very much like",
    "start": "434639",
    "end": "436000"
  },
  {
    "text": "a map or just sort of a loop over your",
    "start": "436000",
    "end": "438000"
  },
  {
    "text": "function",
    "start": "438000",
    "end": "439840"
  },
  {
    "text": "and so here we can say we're v-mapping a",
    "start": "439840",
    "end": "442240"
  },
  {
    "text": "function that just performs squaring",
    "start": "442240",
    "end": "444319"
  },
  {
    "text": "over an array and it's just as if we've",
    "start": "444319",
    "end": "446160"
  },
  {
    "text": "sort of mapped that function over the",
    "start": "446160",
    "end": "447440"
  },
  {
    "text": "array",
    "start": "447440",
    "end": "448720"
  },
  {
    "text": "what's different about vmap is how this",
    "start": "448720",
    "end": "450479"
  },
  {
    "text": "map is computed so",
    "start": "450479",
    "end": "452080"
  },
  {
    "text": "what vmap does is it vectorizes your",
    "start": "452080",
    "end": "454479"
  },
  {
    "text": "function",
    "start": "454479",
    "end": "455120"
  },
  {
    "text": "by plumbing the sort of extra mapped",
    "start": "455120",
    "end": "457199"
  },
  {
    "text": "dimension through",
    "start": "457199",
    "end": "458560"
  },
  {
    "text": "all of the operations in your code for",
    "start": "458560",
    "end": "460319"
  },
  {
    "text": "example turning matrix vector multiplies",
    "start": "460319",
    "end": "462240"
  },
  {
    "text": "into matrix matrix multiplies",
    "start": "462240",
    "end": "464080"
  },
  {
    "text": "and so this is exactly as if we had done",
    "start": "464080",
    "end": "466400"
  },
  {
    "text": "the batching by hand but we can do it",
    "start": "466400",
    "end": "468160"
  },
  {
    "text": "automatically",
    "start": "468160",
    "end": "469520"
  },
  {
    "text": "and so for example if we wanted to",
    "start": "469520",
    "end": "470960"
  },
  {
    "text": "compute per example gradients for exa",
    "start": "470960",
    "end": "472960"
  },
  {
    "text": "you know for differentially private",
    "start": "472960",
    "end": "474160"
  },
  {
    "text": "training we can do that we can compose",
    "start": "474160",
    "end": "476319"
  },
  {
    "text": "grad",
    "start": "476319",
    "end": "476800"
  },
  {
    "text": "with vmap and you know jit the whole",
    "start": "476800",
    "end": "478639"
  },
  {
    "text": "thing and then here we can compute",
    "start": "478639",
    "end": "480479"
  },
  {
    "text": "per example gradients here we have a",
    "start": "480479",
    "end": "481840"
  },
  {
    "text": "batch size of 128",
    "start": "481840",
    "end": "483680"
  },
  {
    "text": "those are the gradients with respect to",
    "start": "483680",
    "end": "485280"
  },
  {
    "text": "our first parameter there um",
    "start": "485280",
    "end": "487280"
  },
  {
    "text": "so those are just you know a few",
    "start": "487280",
    "end": "489199"
  },
  {
    "text": "transformations scratching the surface",
    "start": "489199",
    "end": "490560"
  },
  {
    "text": "of jacks",
    "start": "490560",
    "end": "491120"
  },
  {
    "text": "i don't have time to go into the details",
    "start": "491120",
    "end": "492800"
  },
  {
    "text": "but you can really gain a lot of power",
    "start": "492800",
    "end": "494400"
  },
  {
    "text": "by composing these things up",
    "start": "494400",
    "end": "495759"
  },
  {
    "text": "so for example the best way to compute",
    "start": "495759",
    "end": "498400"
  },
  {
    "text": "jacobian matrices dense jacobian",
    "start": "498400",
    "end": "500000"
  },
  {
    "text": "matrices and hessian matrices",
    "start": "500000",
    "end": "501840"
  },
  {
    "text": "is to compose these things together here",
    "start": "501840",
    "end": "503520"
  },
  {
    "text": "we're composing forward mode and reverse",
    "start": "503520",
    "end": "505280"
  },
  {
    "text": "mode and vmap twice",
    "start": "505280",
    "end": "507039"
  },
  {
    "text": "as well as jit and this is sort of the",
    "start": "507039",
    "end": "508800"
  },
  {
    "text": "most efficient way to compute dense",
    "start": "508800",
    "end": "510400"
  },
  {
    "text": "hessians so that's it for our first demo",
    "start": "510400",
    "end": "514479"
  },
  {
    "text": "i just want to tell you briefly about",
    "start": "514479",
    "end": "515919"
  },
  {
    "text": "some of the exciting research that's",
    "start": "515919",
    "end": "517200"
  },
  {
    "text": "been done with jax",
    "start": "517200",
    "end": "518399"
  },
  {
    "text": "so far so it's being used for research",
    "start": "518399",
    "end": "521279"
  },
  {
    "text": "on vision transformers",
    "start": "521279",
    "end": "522640"
  },
  {
    "text": "which is really exciting it's being used",
    "start": "522640",
    "end": "526480"
  },
  {
    "text": "for research on neural radiance fields",
    "start": "526480",
    "end": "529120"
  },
  {
    "text": "so we have these incredible",
    "start": "529120",
    "end": "530000"
  },
  {
    "text": "visualizations",
    "start": "530000",
    "end": "531279"
  },
  {
    "text": "that come out it's used also for things",
    "start": "531279",
    "end": "534399"
  },
  {
    "text": "like",
    "start": "534399",
    "end": "534720"
  },
  {
    "text": "molecular dynamics simulations so when",
    "start": "534720",
    "end": "536959"
  },
  {
    "text": "these folks are having fun they simulate",
    "start": "536959",
    "end": "538399"
  },
  {
    "text": "shooting a bullet at the jax logo",
    "start": "538399",
    "end": "540160"
  },
  {
    "text": "but when they're doing more serious work",
    "start": "540160",
    "end": "541760"
  },
  {
    "text": "they're doing things like optimizing",
    "start": "541760",
    "end": "543040"
  },
  {
    "text": "material properties",
    "start": "543040",
    "end": "544480"
  },
  {
    "text": "using using jacks it's being used to",
    "start": "544480",
    "end": "547279"
  },
  {
    "text": "great effect at deepmind",
    "start": "547279",
    "end": "548800"
  },
  {
    "text": "uh for example to solve uh the",
    "start": "548800",
    "end": "550640"
  },
  {
    "text": "schrodinger equation",
    "start": "550640",
    "end": "552000"
  },
  {
    "text": "uh much more efficiently um it's also",
    "start": "552000",
    "end": "554640"
  },
  {
    "text": "being used for",
    "start": "554640",
    "end": "555760"
  },
  {
    "text": "the next generation of work on uh",
    "start": "555760",
    "end": "557839"
  },
  {
    "text": "reinforcement learning agents",
    "start": "557839",
    "end": "559440"
  },
  {
    "text": "actually uh this is a research project",
    "start": "559440",
    "end": "561200"
  },
  {
    "text": "that i collaborated on",
    "start": "561200",
    "end": "562720"
  },
  {
    "text": "this is for neural odes making them fast",
    "start": "562720",
    "end": "565920"
  },
  {
    "text": "to solve and what was really exciting",
    "start": "565920",
    "end": "568880"
  },
  {
    "text": "about this is again we were able to",
    "start": "568880",
    "end": "570080"
  },
  {
    "text": "compose up a whole bunch of jax",
    "start": "570080",
    "end": "571440"
  },
  {
    "text": "transformations",
    "start": "571440",
    "end": "572560"
  },
  {
    "text": "to great effect so",
    "start": "572560",
    "end": "575760"
  },
  {
    "text": "that's you know some great research what",
    "start": "575760",
    "end": "577680"
  },
  {
    "text": "about scale this conference is all about",
    "start": "577680",
    "end": "579279"
  },
  {
    "text": "scale",
    "start": "579279",
    "end": "580000"
  },
  {
    "text": "so one thing we did to sort of push jax",
    "start": "580000",
    "end": "582240"
  },
  {
    "text": "at scale was we entered the ml perf",
    "start": "582240",
    "end": "584080"
  },
  {
    "text": "competition in in 2020",
    "start": "584080",
    "end": "585920"
  },
  {
    "text": "um this is sort of the big competition",
    "start": "585920",
    "end": "588240"
  },
  {
    "text": "for",
    "start": "588240",
    "end": "588959"
  },
  {
    "text": "uh training uh uh neural networks",
    "start": "588959",
    "end": "592000"
  },
  {
    "text": "at scale on on super computers there's",
    "start": "592000",
    "end": "594399"
  },
  {
    "text": "entries from",
    "start": "594399",
    "end": "595519"
  },
  {
    "text": "all sorts of different companies um and",
    "start": "595519",
    "end": "597839"
  },
  {
    "text": "so we made four submissions in jacks",
    "start": "597839",
    "end": "599760"
  },
  {
    "text": "and they were all one and a half to two",
    "start": "599760",
    "end": "602240"
  },
  {
    "text": "and a half times faster than the next",
    "start": "602240",
    "end": "603760"
  },
  {
    "text": "fastest non-google submission",
    "start": "603760",
    "end": "605519"
  },
  {
    "text": "um in particular what was really",
    "start": "605519",
    "end": "607360"
  },
  {
    "text": "exciting for one of my teammates who",
    "start": "607360",
    "end": "609200"
  },
  {
    "text": "worked on this",
    "start": "609200",
    "end": "610000"
  },
  {
    "text": "was this transformer uh benchmark where",
    "start": "610000",
    "end": "612800"
  },
  {
    "text": "jack set a clear world record",
    "start": "612800",
    "end": "614320"
  },
  {
    "text": "and for him personally it was a 50 000",
    "start": "614320",
    "end": "616160"
  },
  {
    "text": "time speed up over the work that he was",
    "start": "616160",
    "end": "618000"
  },
  {
    "text": "doing",
    "start": "618000",
    "end": "618560"
  },
  {
    "text": "on a sort of top of the line gpu rig uh",
    "start": "618560",
    "end": "621279"
  },
  {
    "text": "five years ago where it went from",
    "start": "621279",
    "end": "622880"
  },
  {
    "text": "taking seven to ten days to train to",
    "start": "622880",
    "end": "625839"
  },
  {
    "text": "taking 16 seconds to do this uh",
    "start": "625839",
    "end": "627920"
  },
  {
    "text": "uh training of a of a much more",
    "start": "627920",
    "end": "630240"
  },
  {
    "text": "sophisticated model a transformer model",
    "start": "630240",
    "end": "631920"
  },
  {
    "text": "on this",
    "start": "631920",
    "end": "632560"
  },
  {
    "text": "benchmark data set um so",
    "start": "632560",
    "end": "635920"
  },
  {
    "text": "that was for a benchmark competition but",
    "start": "635920",
    "end": "637600"
  },
  {
    "text": "what we want to do now is bring",
    "start": "637600",
    "end": "639360"
  },
  {
    "text": "this kind of scale to a lot more users",
    "start": "639360",
    "end": "641839"
  },
  {
    "text": "and one of the ways we're doing that",
    "start": "641839",
    "end": "643120"
  },
  {
    "text": "is by leveraging cloud tpus so cloud",
    "start": "643120",
    "end": "645760"
  },
  {
    "text": "cpus",
    "start": "645760",
    "end": "646640"
  },
  {
    "text": "are these accelerator chips um each",
    "start": "646640",
    "end": "648880"
  },
  {
    "text": "cloud tpu",
    "start": "648880",
    "end": "649920"
  },
  {
    "text": "has eight cores and you can think of it",
    "start": "649920",
    "end": "652079"
  },
  {
    "text": "as being attached to a single cpu host",
    "start": "652079",
    "end": "655680"
  },
  {
    "text": "uh together with lots of high-speed",
    "start": "655680",
    "end": "657360"
  },
  {
    "text": "interconnects between those accelerator",
    "start": "657360",
    "end": "658880"
  },
  {
    "text": "chips",
    "start": "658880",
    "end": "659440"
  },
  {
    "text": "and the xlite compiler is actually",
    "start": "659440",
    "end": "661360"
  },
  {
    "text": "designed for",
    "start": "661360",
    "end": "662480"
  },
  {
    "text": "targeting these things really",
    "start": "662480",
    "end": "663839"
  },
  {
    "text": "effectively now that was a single cloud",
    "start": "663839",
    "end": "666399"
  },
  {
    "text": "tpu but a cloud tpu pod",
    "start": "666399",
    "end": "668079"
  },
  {
    "text": "is many of these things hooked together",
    "start": "668079",
    "end": "669920"
  },
  {
    "text": "what's really interesting is that all of",
    "start": "669920",
    "end": "671200"
  },
  {
    "text": "those accelerator chips",
    "start": "671200",
    "end": "672560"
  },
  {
    "text": "have fast internship interconnect uh",
    "start": "672560",
    "end": "674880"
  },
  {
    "text": "between them actually in a sort of",
    "start": "674880",
    "end": "677040"
  },
  {
    "text": "toroidal uh topology and the xli",
    "start": "677040",
    "end": "679519"
  },
  {
    "text": "compiler is also really good at",
    "start": "679519",
    "end": "681040"
  },
  {
    "text": "targeting these big",
    "start": "681040",
    "end": "682160"
  },
  {
    "text": "super computers so the way to think",
    "start": "682160",
    "end": "684240"
  },
  {
    "text": "about how these machines look",
    "start": "684240",
    "end": "685760"
  },
  {
    "text": "is that we have all these accelerator",
    "start": "685760",
    "end": "687440"
  },
  {
    "text": "chips that are networked together",
    "start": "687440",
    "end": "688399"
  },
  {
    "text": "amongst",
    "start": "688399",
    "end": "689200"
  },
  {
    "text": "themselves and then we have these cpu",
    "start": "689200",
    "end": "691120"
  },
  {
    "text": "hosts off to the side",
    "start": "691120",
    "end": "692720"
  },
  {
    "text": "and what we need to do as programmers is",
    "start": "692720",
    "end": "694720"
  },
  {
    "text": "somehow launch",
    "start": "694720",
    "end": "696079"
  },
  {
    "text": "jack's python processes on all of these",
    "start": "696079",
    "end": "698880"
  },
  {
    "text": "hosts",
    "start": "698880",
    "end": "699760"
  },
  {
    "text": "and then use that to drive all these",
    "start": "699760",
    "end": "702320"
  },
  {
    "text": "accelerators that we'll talk",
    "start": "702320",
    "end": "704320"
  },
  {
    "text": "amongst each other among these on these",
    "start": "704320",
    "end": "706079"
  },
  {
    "text": "fast interchip interconnects",
    "start": "706079",
    "end": "708480"
  },
  {
    "text": "so to show you what that's like we",
    "start": "708480",
    "end": "710079"
  },
  {
    "text": "actually prepared a demo",
    "start": "710079",
    "end": "712800"
  },
  {
    "text": "so let's take a look at driving one of",
    "start": "712800",
    "end": "715440"
  },
  {
    "text": "these tpu supercomputers",
    "start": "715440",
    "end": "717120"
  },
  {
    "text": "uh using ray so we're ssh into",
    "start": "717120",
    "end": "720240"
  },
  {
    "text": "our uh our one of our worker nodes here",
    "start": "720240",
    "end": "723440"
  },
  {
    "text": "we're gonna import ray and just",
    "start": "723440",
    "end": "724720"
  },
  {
    "text": "initialize array cluster",
    "start": "724720",
    "end": "726560"
  },
  {
    "text": "so we already have ray running on uh all",
    "start": "726560",
    "end": "729360"
  },
  {
    "text": "these 128",
    "start": "729360",
    "end": "731279"
  },
  {
    "text": "tpu worker nodes so we're connected to",
    "start": "731279",
    "end": "734120"
  },
  {
    "text": "1024",
    "start": "734120",
    "end": "735600"
  },
  {
    "text": "uh tpu cores and we're just going to",
    "start": "735600",
    "end": "738000"
  },
  {
    "text": "write a simple ray function",
    "start": "738000",
    "end": "739200"
  },
  {
    "text": "that's going to import our ml perf",
    "start": "739200",
    "end": "740560"
  },
  {
    "text": "training code and and run it",
    "start": "740560",
    "end": "742880"
  },
  {
    "text": "we've set this up so that we'll call",
    "start": "742880",
    "end": "744000"
  },
  {
    "text": "this 128 times for 128 hosts",
    "start": "744000",
    "end": "746480"
  },
  {
    "text": "ray will balance that onto all the",
    "start": "746480",
    "end": "747920"
  },
  {
    "text": "different uh uh worker machines",
    "start": "747920",
    "end": "750320"
  },
  {
    "text": "and then uh we'll we'll start running",
    "start": "750320",
    "end": "752160"
  },
  {
    "text": "things so",
    "start": "752160",
    "end": "753600"
  },
  {
    "text": "actually initializing the cluster uh",
    "start": "753600",
    "end": "755600"
  },
  {
    "text": "takes about like two and a half minutes",
    "start": "755600",
    "end": "757360"
  },
  {
    "text": "and the compilation step",
    "start": "757360",
    "end": "758720"
  },
  {
    "text": "takes about one minute and so through",
    "start": "758720",
    "end": "760560"
  },
  {
    "text": "the magic of television we have sped up",
    "start": "760560",
    "end": "762399"
  },
  {
    "text": "those steps",
    "start": "762399",
    "end": "763120"
  },
  {
    "text": "but all this training right now is in",
    "start": "763120",
    "end": "765440"
  },
  {
    "text": "real time",
    "start": "765440",
    "end": "766399"
  },
  {
    "text": "um so this is exactly as if we're",
    "start": "766399",
    "end": "768240"
  },
  {
    "text": "running ml perf this is a smaller",
    "start": "768240",
    "end": "769680"
  },
  {
    "text": "machine than we ran for the for the",
    "start": "769680",
    "end": "770959"
  },
  {
    "text": "mlperf",
    "start": "770959",
    "end": "772320"
  },
  {
    "text": "but this is now running each epoch is",
    "start": "772320",
    "end": "774480"
  },
  {
    "text": "running this",
    "start": "774480",
    "end": "775440"
  },
  {
    "text": "wmt uh english german data set so about",
    "start": "775440",
    "end": "778480"
  },
  {
    "text": "4.4 million",
    "start": "778480",
    "end": "779839"
  },
  {
    "text": "uh parallel sentences",
    "start": "779839",
    "end": "782880"
  },
  {
    "text": "and with a global batch size of 2048 and",
    "start": "782880",
    "end": "785760"
  },
  {
    "text": "so we're running these steps very",
    "start": "785760",
    "end": "786959"
  },
  {
    "text": "quickly",
    "start": "786959",
    "end": "787519"
  },
  {
    "text": "and the ml perf target blue score is 25",
    "start": "787519",
    "end": "790959"
  },
  {
    "text": "or sort of a valid accuracy of 0.25",
    "start": "790959",
    "end": "793680"
  },
  {
    "text": "and here we can see we finished training",
    "start": "793680",
    "end": "795519"
  },
  {
    "text": "in just 38.35 seconds",
    "start": "795519",
    "end": "797839"
  },
  {
    "text": "using this tpu supercomputer and rey",
    "start": "797839",
    "end": "801200"
  },
  {
    "text": "so if you'd like a more detailed look at",
    "start": "801200",
    "end": "802959"
  },
  {
    "text": "that same demo check out this tweet",
    "start": "802959",
    "end": "804880"
  },
  {
    "text": "linked here and you can see uh you know",
    "start": "804880",
    "end": "807440"
  },
  {
    "text": "a bit more into the guts",
    "start": "807440",
    "end": "808720"
  },
  {
    "text": "of how all that works great so that was",
    "start": "808720",
    "end": "811600"
  },
  {
    "text": "a whirlwind tour",
    "start": "811600",
    "end": "812480"
  },
  {
    "text": "of jax and how it's you know a system",
    "start": "812480",
    "end": "815279"
  },
  {
    "text": "based on these composable function",
    "start": "815279",
    "end": "816639"
  },
  {
    "text": "transformations",
    "start": "816639",
    "end": "817519"
  },
  {
    "text": "meant for accelerating machine learning",
    "start": "817519",
    "end": "818880"
  },
  {
    "text": "research and we're really excited about",
    "start": "818880",
    "end": "820959"
  },
  {
    "text": "uh driving these this super computer",
    "start": "820959",
    "end": "822639"
  },
  {
    "text": "scale and and bringing it to more",
    "start": "822639",
    "end": "824560"
  },
  {
    "text": "people so thanks for your attention",
    "start": "824560",
    "end": "829839"
  }
]