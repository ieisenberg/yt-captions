[
  {
    "text": "hi hello everyone uh I'm Cody I'm a",
    "start": "4560",
    "end": "6520"
  },
  {
    "text": "software engineer at nll and is Philip",
    "start": "6520",
    "end": "8719"
  },
  {
    "text": "the CEO of any scale today we're going",
    "start": "8719",
    "end": "10679"
  },
  {
    "text": "to cover inference at any scale so this",
    "start": "10679",
    "end": "14719"
  },
  {
    "text": "will be the outline for today basically",
    "start": "14719",
    "end": "16358"
  },
  {
    "text": "we me cover two points one is NC open",
    "start": "16359",
    "end": "19039"
  },
  {
    "text": "source contributions and the second is",
    "start": "19039",
    "end": "20920"
  },
  {
    "text": "NC Preparatory inference capabilities so",
    "start": "20920",
    "end": "24119"
  },
  {
    "text": "let's start from the first point um we",
    "start": "24119",
    "end": "27000"
  },
  {
    "text": "know that VM is now one of the most",
    "start": "27000",
    "end": "29960"
  },
  {
    "text": "popular aent serving um engine in this",
    "start": "29960",
    "end": "32360"
  },
  {
    "text": "SW and has been adapted by many many",
    "start": "32360",
    "end": "34440"
  },
  {
    "text": "companies as their uh production use",
    "start": "34440",
    "end": "36719"
  },
  {
    "text": "cases and it's been over 500",
    "start": "36719",
    "end": "39480"
  },
  {
    "text": "contributors with 27k stars in their",
    "start": "39480",
    "end": "42280"
  },
  {
    "text": "GitHub repository and in the past 6 to 8",
    "start": "42280",
    "end": "45600"
  },
  {
    "text": "month N scale has spent a lot of efforts",
    "start": "45600",
    "end": "47600"
  },
  {
    "text": "investing on the open source vrm",
    "start": "47600",
    "end": "49719"
  },
  {
    "text": "especially on the performance",
    "start": "49719",
    "end": "52239"
  },
  {
    "text": "implementations for example N scale was",
    "start": "52239",
    "end": "54480"
  },
  {
    "text": "leading the following open source",
    "start": "54480",
    "end": "56079"
  },
  {
    "text": "features which are actually pretty",
    "start": "56079",
    "end": "58239"
  },
  {
    "text": "critical in the current VM performance",
    "start": "58239",
    "end": "60920"
  },
  {
    "text": "there is a APA support joined with the",
    "start": "60920",
    "end": "62800"
  },
  {
    "text": "neurom magic which give you gives you",
    "start": "62800",
    "end": "65239"
  },
  {
    "text": "the fast computation and last memory",
    "start": "65239",
    "end": "67320"
  },
  {
    "text": "footprint and it also contribute",
    "start": "67320",
    "end": "69320"
  },
  {
    "text": "scheduling with the trunk prefill which",
    "start": "69320",
    "end": "71439"
  },
  {
    "text": "is able to maintain the low inter toen",
    "start": "71439",
    "end": "73439"
  },
  {
    "text": "latency for long contact requests and",
    "start": "73439",
    "end": "76520"
  },
  {
    "text": "then we also contribute the speca",
    "start": "76520",
    "end": "77880"
  },
  {
    "text": "decoding framework which reduces the",
    "start": "77880",
    "end": "79640"
  },
  {
    "text": "inter toen latency by more than 2x for",
    "start": "79640",
    "end": "81960"
  },
  {
    "text": "the low uh low QPS serving cases I will",
    "start": "81960",
    "end": "85360"
  },
  {
    "text": "dive into all of those three feature uh",
    "start": "85360",
    "end": "87560"
  },
  {
    "text": "features in the rest of the talk and in",
    "start": "87560",
    "end": "90079"
  },
  {
    "text": "addition to that we also contribute many",
    "start": "90079",
    "end": "91960"
  },
  {
    "text": "many more features like multistep",
    "start": "91960",
    "end": "93799"
  },
  {
    "text": "scheduling to reduce the CPU overheads",
    "start": "93799",
    "end": "96159"
  },
  {
    "text": "and you also contribute efficient",
    "start": "96159",
    "end": "97880"
  },
  {
    "text": "Pipeline parison with the ray compiled",
    "start": "97880",
    "end": "100000"
  },
  {
    "text": "graphs that you will be hearing more",
    "start": "100000",
    "end": "102280"
  },
  {
    "text": "details in the following talk given by S",
    "start": "102280",
    "end": "104320"
  },
  {
    "text": "and morali and we also contribute the",
    "start": "104320",
    "end": "107119"
  },
  {
    "text": "visual language model support join with",
    "start": "107119",
    "end": "108640"
  },
  {
    "text": "the Roblox to bring the multimodality",
    "start": "108640",
    "end": "110719"
  },
  {
    "text": "model into vrn finally we also",
    "start": "110719",
    "end": "113600"
  },
  {
    "text": "contribute multi- low support so that",
    "start": "113600",
    "end": "116119"
  },
  {
    "text": "you can serve multiple low weights with",
    "start": "116119",
    "end": "118039"
  },
  {
    "text": "the same model engine",
    "start": "118039",
    "end": "120719"
  },
  {
    "text": "now let's dive into some critical",
    "start": "120719",
    "end": "122439"
  },
  {
    "text": "features um that's been used by VM",
    "start": "122439",
    "end": "125640"
  },
  {
    "text": "day-to-day the first one is the aps",
    "start": "125640",
    "end": "127920"
  },
  {
    "text": "support on Nvidia gpus again this is the",
    "start": "127920",
    "end": "129920"
  },
  {
    "text": "joint work with the neurom magic so",
    "start": "129920",
    "end": "132440"
  },
  {
    "text": "first of all why FP is critical in L",
    "start": "132440",
    "end": "135000"
  },
  {
    "text": "serving against fp16 first of all we can",
    "start": "135000",
    "end": "137400"
  },
  {
    "text": "see that the quality FPA is able to",
    "start": "137400",
    "end": "140120"
  },
  {
    "text": "achieve 99% quality against fp6 and it's",
    "start": "140120",
    "end": "144360"
  },
  {
    "text": "extremely easy to quantize so that you",
    "start": "144360",
    "end": "146560"
  },
  {
    "text": "can directly get the performance benefit",
    "start": "146560",
    "end": "148720"
  },
  {
    "text": "and the memory benefits",
    "start": "148720",
    "end": "150480"
  },
  {
    "text": "by just qu the model to fp8 and you can",
    "start": "150480",
    "end": "153720"
  },
  {
    "text": "see the Benchmark the right hand side by",
    "start": "153720",
    "end": "155879"
  },
  {
    "text": "benchmarking the Llama 3.17 model on",
    "start": "155879",
    "end": "158319"
  },
  {
    "text": "h100 you can get uh up to 1.27 ttft",
    "start": "158319",
    "end": "163040"
  },
  {
    "text": "Improvement time to the first token and",
    "start": "163040",
    "end": "165200"
  },
  {
    "text": "1.5x itl Inter toen latency improvements",
    "start": "165200",
    "end": "168599"
  },
  {
    "text": "so it's more like a almost the Freel",
    "start": "168599",
    "end": "170440"
  },
  {
    "text": "launch and lwh hand fruit to give you",
    "start": "170440",
    "end": "172159"
  },
  {
    "text": "the speed",
    "start": "172159",
    "end": "173040"
  },
  {
    "text": "ups but what's the challenge to support",
    "start": "173040",
    "end": "175800"
  },
  {
    "text": "FP in VM the first thing is to",
    "start": "175800",
    "end": "178319"
  },
  {
    "text": "scandalize FPA check Point format by the",
    "start": "178319",
    "end": "181120"
  },
  {
    "text": "time we want to support APA in the VM",
    "start": "181120",
    "end": "183200"
  },
  {
    "text": "APA is not that quite common in this SW",
    "start": "183200",
    "end": "186440"
  },
  {
    "text": "so we collaborated with the ne magic",
    "start": "186440",
    "end": "188720"
  },
  {
    "text": "Forks to figure out the reasonable FPA",
    "start": "188720",
    "end": "190879"
  },
  {
    "text": "checkpoint formats and support support",
    "start": "190879",
    "end": "193239"
  },
  {
    "text": "that in and unfortunately this format",
    "start": "193239",
    "end": "195440"
  },
  {
    "text": "has been widely used Now by many many",
    "start": "195440",
    "end": "197840"
  },
  {
    "text": "models so VN is able to load those",
    "start": "197840",
    "end": "200680"
  },
  {
    "text": "models and work just out of the box and",
    "start": "200680",
    "end": "204000"
  },
  {
    "text": "the second thing is we have to support",
    "start": "204000",
    "end": "205879"
  },
  {
    "text": "FP quantization framework in the v in",
    "start": "205879",
    "end": "208439"
  },
  {
    "text": "terms of how to load the model",
    "start": "208439",
    "end": "210319"
  },
  {
    "text": "cont on the fly or execute the model in",
    "start": "210319",
    "end": "212439"
  },
  {
    "text": "the efficient way and finally we of",
    "start": "212439",
    "end": "214760"
  },
  {
    "text": "course need uh efficient FPA Trident",
    "start": "214760",
    "end": "217439"
  },
  {
    "text": "kernels to fully utilize the FPA tensor",
    "start": "217439",
    "end": "220319"
  },
  {
    "text": "cores available on gpus and fortunately",
    "start": "220319",
    "end": "223720"
  },
  {
    "text": "now FPA is being widely used vrm and",
    "start": "223720",
    "end": "226480"
  },
  {
    "text": "become some of default options for large",
    "start": "226480",
    "end": "228760"
  },
  {
    "text": "models for example Lama 405b FAA uh",
    "start": "228760",
    "end": "232319"
  },
  {
    "text": "version becomes the one of the official",
    "start": "232319",
    "end": "234680"
  },
  {
    "text": "first release by",
    "start": "234680",
    "end": "237360"
  },
  {
    "text": "meta then second feature I want to",
    "start": "237360",
    "end": "239400"
  },
  {
    "text": "highlight is the trun prefill this is",
    "start": "239400",
    "end": "242079"
  },
  {
    "text": "also a critical feature used inan today",
    "start": "242079",
    "end": "245159"
  },
  {
    "text": "first of all why why trun prefill is",
    "start": "245159",
    "end": "247239"
  },
  {
    "text": "critical because trunk prefill can chunk",
    "start": "247239",
    "end": "249879"
  },
  {
    "text": "the long prompt into the small chunks",
    "start": "249879",
    "end": "251680"
  },
  {
    "text": "and run it in several batches by this",
    "start": "251680",
    "end": "254040"
  },
  {
    "text": "way we can limit the number of batch",
    "start": "254040",
    "end": "255760"
  },
  {
    "text": "tokens to maintain the loow low latency",
    "start": "255760",
    "end": "258560"
  },
  {
    "text": "which means you can improve your",
    "start": "258560",
    "end": "260320"
  },
  {
    "text": "interlocal latency and you can batch the",
    "start": "260320",
    "end": "263080"
  },
  {
    "text": "preview and decode requests together so",
    "start": "263080",
    "end": "265320"
  },
  {
    "text": "that your decoding won't be interrupt by",
    "start": "265320",
    "end": "267880"
  },
  {
    "text": "your prefi requests anymore",
    "start": "267880",
    "end": "270240"
  },
  {
    "text": "so here are some experimental results",
    "start": "270240",
    "end": "272000"
  },
  {
    "text": "you can see that simply enabling trun",
    "start": "272000",
    "end": "274400"
  },
  {
    "text": "field in the Llama 3.1 70b model in h100",
    "start": "274400",
    "end": "278360"
  },
  {
    "text": "you can give you can uh get up to 1.4x",
    "start": "278360",
    "end": "282320"
  },
  {
    "text": "itl Improvement by only paying 30% ttf",
    "start": "282320",
    "end": "286320"
  },
  {
    "text": "slowdown and this is expected because we",
    "start": "286320",
    "end": "289080"
  },
  {
    "text": "chunk The Prompt into multiple batches",
    "start": "289080",
    "end": "291720"
  },
  {
    "text": "but just 30% and you can give you uh",
    "start": "291720",
    "end": "294639"
  },
  {
    "text": "higher it uh better itl you can you can",
    "start": "294639",
    "end": "297840"
  },
  {
    "text": "likely meet your SLA in the better way",
    "start": "297840",
    "end": "301560"
  },
  {
    "text": "and again this is not a free launch to",
    "start": "301560",
    "end": "303479"
  },
  {
    "text": "support trunk prefill in the VM first of",
    "start": "303479",
    "end": "305759"
  },
  {
    "text": "all we have to integrate the append",
    "start": "305759",
    "end": "307160"
  },
  {
    "text": "kernels to the VM before that VN",
    "start": "307160",
    "end": "309759"
  },
  {
    "text": "attention kernel can only process the",
    "start": "309759",
    "end": "311560"
  },
  {
    "text": "prefill or decode but not mix of them",
    "start": "311560",
    "end": "314360"
  },
  {
    "text": "and we have to first integrate the",
    "start": "314360",
    "end": "315560"
  },
  {
    "text": "kernel to make it available and then we",
    "start": "315560",
    "end": "317400"
  },
  {
    "text": "have to refactor the request scheduler",
    "start": "317400",
    "end": "319639"
  },
  {
    "text": "and support prom chunking finally you",
    "start": "319639",
    "end": "321720"
  },
  {
    "text": "have to schedule both prefill and decode",
    "start": "321720",
    "end": "323960"
  },
  {
    "text": "request in the same batch and then been",
    "start": "323960",
    "end": "325479"
  },
  {
    "text": "work together this require a lot of",
    "start": "325479",
    "end": "327360"
  },
  {
    "text": "refactoring in the VN architecture and",
    "start": "327360",
    "end": "329720"
  },
  {
    "text": "we're glad that it's already landed and",
    "start": "329720",
    "end": "331600"
  },
  {
    "text": "already available in the uh recent VI",
    "start": "331600",
    "end": "333880"
  },
  {
    "text": "and release and it's now become the",
    "start": "333880",
    "end": "335759"
  },
  {
    "text": "default on option when you're serving",
    "start": "335759",
    "end": "338080"
  },
  {
    "text": "most of the",
    "start": "338080",
    "end": "340000"
  },
  {
    "text": "models and then I'll pass to philli to",
    "start": "340000",
    "end": "342520"
  },
  {
    "text": "introduce the Spectre decoding",
    "start": "342520",
    "end": "344720"
  },
  {
    "text": "efforts so speculative recording um",
    "start": "344720",
    "end": "347039"
  },
  {
    "text": "allows to significantly reduce the um",
    "start": "347039",
    "end": "349360"
  },
  {
    "text": "latency of requests and um let me",
    "start": "349360",
    "end": "352680"
  },
  {
    "text": "explain a little bit how this works so",
    "start": "352680",
    "end": "354199"
  },
  {
    "text": "um tpus are very good at um uh",
    "start": "354199",
    "end": "356919"
  },
  {
    "text": "processing batches of data in parallel",
    "start": "356919",
    "end": "358960"
  },
  {
    "text": "and actually to do agree that for",
    "start": "358960",
    "end": "360560"
  },
  {
    "text": "decoding it's often um the same",
    "start": "360560",
    "end": "363440"
  },
  {
    "text": "performance to decode a single token",
    "start": "363440",
    "end": "365400"
  },
  {
    "text": "versus decoding multiple tokens um so by",
    "start": "365400",
    "end": "369000"
  },
  {
    "text": "default you can't use that for decoding",
    "start": "369000",
    "end": "370800"
  },
  {
    "text": "at least not in a single request because",
    "start": "370800",
    "end": "372800"
  },
  {
    "text": "in order to get the future tokens you",
    "start": "372800",
    "end": "374280"
  },
  {
    "text": "have to first have the current token so",
    "start": "374280",
    "end": "376240"
  },
  {
    "text": "you can't really um um decode into the",
    "start": "376240",
    "end": "378880"
  },
  {
    "text": "future but specular of decoding allows",
    "start": "378880",
    "end": "380759"
  },
  {
    "text": "you to change that um so um it uses a",
    "start": "380759",
    "end": "383599"
  },
  {
    "text": "small draft model to um uh quickly",
    "start": "383599",
    "end": "388039"
  },
  {
    "text": "compute a bunch of tokens ahead and then",
    "start": "388039",
    "end": "390800"
  },
  {
    "text": "the whole um batch can be verified by",
    "start": "390800",
    "end": "393080"
  },
  {
    "text": "the big model so it will get get the",
    "start": "393080",
    "end": "395199"
  },
  {
    "text": "same um output that the big model would",
    "start": "395199",
    "end": "397319"
  },
  {
    "text": "give but if the big model agrees with",
    "start": "397319",
    "end": "399280"
  },
  {
    "text": "the small model then you get a decent",
    "start": "399280",
    "end": "400599"
  },
  {
    "text": "speed up so in this case for example we",
    "start": "400599",
    "end": "402400"
  },
  {
    "text": "would have like three tokens and two of",
    "start": "402400",
    "end": "403759"
  },
  {
    "text": "them got accepted so we got like an",
    "start": "403759",
    "end": "405440"
  },
  {
    "text": "almost 2x um speed up um so we have um",
    "start": "405440",
    "end": "409759"
  },
  {
    "text": "contributed specular of decoding um",
    "start": "409759",
    "end": "411520"
  },
  {
    "text": "implementation to um VM and we have also",
    "start": "411520",
    "end": "415639"
  },
  {
    "text": "been um um working on uh special specul",
    "start": "415639",
    "end": "420039"
  },
  {
    "text": "um models that are performing better",
    "start": "420039",
    "end": "421440"
  },
  {
    "text": "than open source models so like on the N",
    "start": "421440",
    "end": "423280"
  },
  {
    "text": "platform um you can just use those and",
    "start": "423280",
    "end": "425240"
  },
  {
    "text": "you can also F tune them on on your own",
    "start": "425240",
    "end": "426680"
  },
  {
    "text": "data um the challenges on supporting",
    "start": "426680",
    "end": "429240"
  },
  {
    "text": "spec decoding was um um changing the",
    "start": "429240",
    "end": "432120"
  },
  {
    "text": "block manager so it could pre-allocate",
    "start": "432120",
    "end": "434120"
  },
  {
    "text": "um the slots for the spec decoding and",
    "start": "434120",
    "end": "436599"
  },
  {
    "text": "then also integrating the draft model um",
    "start": "436599",
    "end": "438440"
  },
  {
    "text": "into the LM engine and then also there's",
    "start": "438440",
    "end": "441560"
  },
  {
    "text": "a decent amount of systemss overhead",
    "start": "441560",
    "end": "442800"
  },
  {
    "text": "that we needed to reduce to make the",
    "start": "442800",
    "end": "444520"
  },
  {
    "text": "speculative decoding fast um and here",
    "start": "444520",
    "end": "447360"
  },
  {
    "text": "are some results of this so on the left",
    "start": "447360",
    "end": "449360"
  },
  {
    "text": "hand side there's um a graph that shows",
    "start": "449360",
    "end": "452039"
  },
  {
    "text": "um spec of decoding in the context of",
    "start": "452039",
    "end": "453720"
  },
  {
    "text": "prompt lookup where um so this is the",
    "start": "453720",
    "end": "456120"
  },
  {
    "text": "case when you have um tasks like",
    "start": "456120",
    "end": "457800"
  },
  {
    "text": "summarization or rack and where during",
    "start": "457800",
    "end": "460560"
  },
  {
    "text": "the coding there's a decent amount of",
    "start": "460560",
    "end": "462120"
  },
  {
    "text": "repetition from The Prompt and then um",
    "start": "462120",
    "end": "464440"
  },
  {
    "text": "during the decoding um we do string",
    "start": "464440",
    "end": "466319"
  },
  {
    "text": "matching against the prompt and um and",
    "start": "466319",
    "end": "468919"
  },
  {
    "text": "if there's token overlap then um the",
    "start": "468919",
    "end": "471240"
  },
  {
    "text": "specul coding will be effective on the",
    "start": "471240",
    "end": "473560"
  },
  {
    "text": "right hand side you see um a more",
    "start": "473560",
    "end": "475240"
  },
  {
    "text": "generic setting where it's just",
    "start": "475240",
    "end": "476919"
  },
  {
    "text": "arbitrary prompts and there we use a all",
    "start": "476919",
    "end": "479680"
  },
  {
    "text": "LM to do the um predictions and then um",
    "start": "479680",
    "end": "483520"
  },
  {
    "text": "um get speed ups that way so typically",
    "start": "483520",
    "end": "486159"
  },
  {
    "text": "you can get speed UPS on the order of",
    "start": "486159",
    "end": "487560"
  },
  {
    "text": "like 1.5x to 2x using this",
    "start": "487560",
    "end": "490960"
  },
  {
    "text": "technique um I wanted to also say a few",
    "start": "490960",
    "end": "493599"
  },
  {
    "text": "words on um how we view the future of um",
    "start": "493599",
    "end": "495960"
  },
  {
    "text": "VM and what are the important things",
    "start": "495960",
    "end": "497800"
  },
  {
    "text": "here um so the reliability through the",
    "start": "497800",
    "end": "500840"
  },
  {
    "text": "power Mount so we've actually spent a",
    "start": "500840",
    "end": "502840"
  },
  {
    "text": "decent amount of time working together",
    "start": "502840",
    "end": "504199"
  },
  {
    "text": "with the Berkeley team to improve the",
    "start": "504199",
    "end": "505440"
  },
  {
    "text": "code structure but also really improve",
    "start": "505440",
    "end": "507800"
  },
  {
    "text": "the testing coverage to many more tests",
    "start": "507800",
    "end": "510680"
  },
  {
    "text": "and then also one of our Engineers",
    "start": "510680",
    "end": "512399"
  },
  {
    "text": "actually is maintaining the CI so um",
    "start": "512399",
    "end": "515159"
  },
  {
    "text": "making sure that um things will just get",
    "start": "515159",
    "end": "516919"
  },
  {
    "text": "more and more stable over time um one",
    "start": "516919",
    "end": "519640"
  },
  {
    "text": "thing that really gr about VM is the",
    "start": "519640",
    "end": "521279"
  },
  {
    "text": "comprehensive feature set so um um uh it",
    "start": "521279",
    "end": "525120"
  },
  {
    "text": "covers many of the features um some of",
    "start": "525120",
    "end": "527160"
  },
  {
    "text": "them that I um just um talked about but",
    "start": "527160",
    "end": "529279"
  },
  {
    "text": "also many different models and many use",
    "start": "529279",
    "end": "531920"
  },
  {
    "text": "cases multimodal support and things like",
    "start": "531920",
    "end": "533880"
  },
  {
    "text": "that um at the same time while um being",
    "start": "533880",
    "end": "537080"
  },
  {
    "text": "very comprehensive in the feature um",
    "start": "537080",
    "end": "538760"
  },
  {
    "text": "support it's very very important to be",
    "start": "538760",
    "end": "540839"
  },
  {
    "text": "um composable and have a minimal core",
    "start": "540839",
    "end": "542959"
  },
  {
    "text": "that is extended by the um by these",
    "start": "542959",
    "end": "546160"
  },
  {
    "text": "features um so that um everything stays",
    "start": "546160",
    "end": "548600"
  },
  {
    "text": "maintainable and testable and and stable",
    "start": "548600",
    "end": "551040"
  },
  {
    "text": "um so we've been working on refactoring",
    "start": "551040",
    "end": "552680"
  },
  {
    "text": "things so that um some of the things can",
    "start": "552680",
    "end": "554160"
  },
  {
    "text": "be plugged in like for example logic",
    "start": "554160",
    "end": "556600"
  },
  {
    "text": "processing and Chason processors or like",
    "start": "556600",
    "end": "559279"
  },
  {
    "text": "the exe executor or the schul things",
    "start": "559279",
    "end": "561839"
  },
  {
    "text": "like that um and then it's also um um",
    "start": "561839",
    "end": "565000"
  },
  {
    "text": "really great um to be open source and as",
    "start": "565000",
    "end": "567279"
  },
  {
    "text": "a big Community due to it um",
    "start": "567279",
    "end": "569880"
  },
  {
    "text": "Python and Pyon Roots it's very easy to",
    "start": "569880",
    "end": "571959"
  },
  {
    "text": "contribute things and to adapt new",
    "start": "571959",
    "end": "574279"
  },
  {
    "text": "models so it can always catch up with",
    "start": "574279",
    "end": "576160"
  },
  {
    "text": "the latest LM",
    "start": "576160",
    "end": "578160"
  },
  {
    "text": "Trends okay let me now switch gears and",
    "start": "578160",
    "end": "580680"
  },
  {
    "text": "talk a little bit more about the",
    "start": "580680",
    "end": "581880"
  },
  {
    "text": "propriety um inference capabilities",
    "start": "581880",
    "end": "583800"
  },
  {
    "text": "we've been developing um at any scale um",
    "start": "583800",
    "end": "586480"
  },
  {
    "text": "so first of all I'll give you an",
    "start": "586480",
    "end": "588000"
  },
  {
    "text": "overview of our inference stack so at",
    "start": "588000",
    "end": "590440"
  },
  {
    "text": "the bottom is the N scale platform that",
    "start": "590440",
    "end": "592240"
  },
  {
    "text": "is powered by Ray which is the AI",
    "start": "592240",
    "end": "594120"
  },
  {
    "text": "compute engine um um that um is",
    "start": "594120",
    "end": "596560"
  },
  {
    "text": "integrated with a comprehensive",
    "start": "596560",
    "end": "597880"
  },
  {
    "text": "ecosystem for all kinds of workloads um",
    "start": "597880",
    "end": "599959"
  },
  {
    "text": "including um unstructured data",
    "start": "599959",
    "end": "601760"
  },
  {
    "text": "processing um training and also um",
    "start": "601760",
    "end": "604440"
  },
  {
    "text": "inference um and um uh there's many in",
    "start": "604440",
    "end": "608200"
  },
  {
    "text": "uh features that connect um to the",
    "start": "608200",
    "end": "609920"
  },
  {
    "text": "underlying infrastructure to make it",
    "start": "609920",
    "end": "611440"
  },
  {
    "text": "better more automatic and reliable",
    "start": "611440",
    "end": "614120"
  },
  {
    "text": "scaling and also um we developed um we",
    "start": "614120",
    "end": "617240"
  },
  {
    "text": "spend a decent amount um of time um",
    "start": "617240",
    "end": "620079"
  },
  {
    "text": "investing into the development",
    "start": "620079",
    "end": "621320"
  },
  {
    "text": "experience um to make that really smooth",
    "start": "621320",
    "end": "623519"
  },
  {
    "text": "and then on top of that is the inference",
    "start": "623519",
    "end": "624880"
  },
  {
    "text": "engine and that is based on MVM and then",
    "start": "624880",
    "end": "627760"
  },
  {
    "text": "we have some proprietary features um",
    "start": "627760",
    "end": "629800"
  },
  {
    "text": "that I'll be talking about in a bit and",
    "start": "629800",
    "end": "632040"
  },
  {
    "text": "on top of that are the applications so",
    "start": "632040",
    "end": "633839"
  },
  {
    "text": "we support um online serving with Ray LM",
    "start": "633839",
    "end": "636440"
  },
  {
    "text": "and Ray serf and then offline batching",
    "start": "636440",
    "end": "638920"
  },
  {
    "text": "with ra and batch um we will talk about",
    "start": "638920",
    "end": "641639"
  },
  {
    "text": "that um in a little",
    "start": "641639",
    "end": "642959"
  },
  {
    "text": "bit so first I wanted to talk a little",
    "start": "642959",
    "end": "645000"
  },
  {
    "text": "bit about the accelerated inference",
    "start": "645000",
    "end": "646560"
  },
  {
    "text": "kernels that we've been developing so",
    "start": "646560",
    "end": "648560"
  },
  {
    "text": "this is really um a set a set of kernels",
    "start": "648560",
    "end": "652079"
  },
  {
    "text": "that um supports a more restricted um",
    "start": "652079",
    "end": "654519"
  },
  {
    "text": "set of features and models but really",
    "start": "654519",
    "end": "656720"
  },
  {
    "text": "the um um it's really optimized for",
    "start": "656720",
    "end": "658880"
  },
  {
    "text": "performance so we can do things that you",
    "start": "658880",
    "end": "660880"
  },
  {
    "text": "can't do with open source codes like",
    "start": "660880",
    "end": "662440"
  },
  {
    "text": "optimizing the um memory layout um doing",
    "start": "662440",
    "end": "665120"
  },
  {
    "text": "the optimal coing and then doing like",
    "start": "665120",
    "end": "667320"
  },
  {
    "text": "PTX level optimizations and in",
    "start": "667320",
    "end": "669720"
  },
  {
    "text": "particular um we can put all the",
    "start": "669720",
    "end": "672519"
  },
  {
    "text": "operations of the decoding into one",
    "start": "672519",
    "end": "674240"
  },
  {
    "text": "kernel um that's um something that open",
    "start": "674240",
    "end": "676560"
  },
  {
    "text": "SCE typically doesn't do um fusing",
    "start": "676560",
    "end": "678440"
  },
  {
    "text": "together all the operations like Matrix",
    "start": "678440",
    "end": "679680"
  },
  {
    "text": "multiplications fp8 conversions RMS Norm",
    "start": "679680",
    "end": "682760"
  },
  {
    "text": "rope celu and then keeping the data in",
    "start": "682760",
    "end": "685079"
  },
  {
    "text": "registers and shed memory as much as",
    "start": "685079",
    "end": "687000"
  },
  {
    "text": "possible and the code Bas is actually um",
    "start": "687000",
    "end": "689480"
  },
  {
    "text": "the same code can also run on different",
    "start": "689480",
    "end": "691160"
  },
  {
    "text": "Hardwares including um CPUs um with the",
    "start": "691160",
    "end": "694360"
  },
  {
    "text": "AVX and AMX instruction set and gpus",
    "start": "694360",
    "end": "697120"
  },
  {
    "text": "including H&G L4 and h100 um but um we",
    "start": "697120",
    "end": "701120"
  },
  {
    "text": "build it in such a way that it's um",
    "start": "701120",
    "end": "702920"
  },
  {
    "text": "extendable with plugins so um the same",
    "start": "702920",
    "end": "705120"
  },
  {
    "text": "code base can run on these and then",
    "start": "705120",
    "end": "706480"
  },
  {
    "text": "there are um there's plugins for the",
    "start": "706480",
    "end": "707920"
  },
  {
    "text": "different",
    "start": "707920",
    "end": "708600"
  },
  {
    "text": "Hardwares so here's some preliminary",
    "start": "708600",
    "end": "710880"
  },
  {
    "text": "results we are still improving them but",
    "start": "710880",
    "end": "712399"
  },
  {
    "text": "um at the moment we get like a 15 to 30%",
    "start": "712399",
    "end": "714839"
  },
  {
    "text": "speed up um for decoding so on the left",
    "start": "714839",
    "end": "716920"
  },
  {
    "text": "hand side there is the it Al for Lama",
    "start": "716920",
    "end": "719399"
  },
  {
    "text": "38b on h100s with fp8 and then on the",
    "start": "719399",
    "end": "722399"
  },
  {
    "text": "right hand side there is Lama 38b um",
    "start": "722399",
    "end": "725200"
  },
  {
    "text": "decoding on atgs with",
    "start": "725200",
    "end": "728480"
  },
  {
    "text": "fp16 um the next thing I wanted to talk",
    "start": "728480",
    "end": "730760"
  },
  {
    "text": "about is um realm for online serving so",
    "start": "730760",
    "end": "733560"
  },
  {
    "text": "this is um um a LM serving on top of",
    "start": "733560",
    "end": "737800"
  },
  {
    "text": "Reserve so it inherits all the great",
    "start": "737800",
    "end": "739920"
  },
  {
    "text": "properties of Reserve things like Model",
    "start": "739920",
    "end": "742240"
  },
  {
    "text": "Management um model multiplexing um on",
    "start": "742240",
    "end": "745440"
  },
  {
    "text": "so multiplexing multiple models on a",
    "start": "745440",
    "end": "746959"
  },
  {
    "text": "single GPU uh downscaling to zero um and",
    "start": "746959",
    "end": "751399"
  },
  {
    "text": "then also we have some specialized um",
    "start": "751399",
    "end": "753440"
  },
  {
    "text": "features that are specifically great for",
    "start": "753440",
    "end": "755519"
  },
  {
    "text": "llms like Fast model loading um fast",
    "start": "755519",
    "end": "758560"
  },
  {
    "text": "chasing mode um and and everything is",
    "start": "758560",
    "end": "761000"
  },
  {
    "text": "well integrated with the Nale platform",
    "start": "761000",
    "end": "763160"
  },
  {
    "text": "and so um if you sign up with Nale",
    "start": "763160",
    "end": "765399"
  },
  {
    "text": "there's an oneclick template um where",
    "start": "765399",
    "end": "767600"
  },
  {
    "text": "you can deploy this and it also um it's",
    "start": "767600",
    "end": "770240"
  },
  {
    "text": "great because it's very easy to get",
    "start": "770240",
    "end": "771320"
  },
  {
    "text": "started but it also has all the knobs um",
    "start": "771320",
    "end": "774000"
  },
  {
    "text": "um for configuration um exposed so um",
    "start": "774000",
    "end": "776720"
  },
  {
    "text": "you can uh tune it um for your",
    "start": "776720",
    "end": "778600"
  },
  {
    "text": "particular Bo",
    "start": "778600",
    "end": "780959"
  },
  {
    "text": "cool the next thing um is about um R bch",
    "start": "780959",
    "end": "784160"
  },
  {
    "text": "and Cody if going to talk about that",
    "start": "784160",
    "end": "786440"
  },
  {
    "text": "right so Philip just introduced Ray AR",
    "start": "786440",
    "end": "789279"
  },
  {
    "text": "this is the perfect solution for your",
    "start": "789279",
    "end": "790760"
  },
  {
    "text": "online serving to deploy your private",
    "start": "790760",
    "end": "792360"
  },
  {
    "text": "endpoint but on the other hand we want",
    "start": "792360",
    "end": "794760"
  },
  {
    "text": "to emphasize that the bat inference is",
    "start": "794760",
    "end": "797240"
  },
  {
    "text": "also an important workload especially in",
    "start": "797240",
    "end": "799399"
  },
  {
    "text": "the recent um months um why bat",
    "start": "799399",
    "end": "802480"
  },
  {
    "text": "inference because in the bat inference",
    "start": "802480",
    "end": "804600"
  },
  {
    "text": "you can actually minimize are and",
    "start": "804600",
    "end": "806440"
  },
  {
    "text": "serving cost by maximizing GPU",
    "start": "806440",
    "end": "808639"
  },
  {
    "text": "utilization and third puts so if you",
    "start": "808639",
    "end": "810800"
  },
  {
    "text": "have some Downstream text that you don't",
    "start": "810800",
    "end": "812680"
  },
  {
    "text": "really care about the intermediate",
    "start": "812680",
    "end": "814440"
  },
  {
    "text": "reaction in terms of latency then you",
    "start": "814440",
    "end": "817240"
  },
  {
    "text": "can try to use a and batch to save your",
    "start": "817240",
    "end": "819519"
  },
  {
    "text": "money for example if you want to process",
    "start": "819519",
    "end": "821720"
  },
  {
    "text": "a document summarize the document etc",
    "start": "821720",
    "end": "824000"
  },
  {
    "text": "etc then you can consider use batching",
    "start": "824000",
    "end": "827120"
  },
  {
    "text": "um and if you want to use batch uh",
    "start": "827120",
    "end": "829360"
  },
  {
    "text": "batching processing why you want to",
    "start": "829360",
    "end": "830920"
  },
  {
    "text": "consider Ray ER and batch because we",
    "start": "830920",
    "end": "833759"
  },
  {
    "text": "first provide the simple API with the",
    "start": "833759",
    "end": "835680"
  },
  {
    "text": "rate data I'm showing the Cod snip in",
    "start": "835680",
    "end": "837639"
  },
  {
    "text": "the right hand side you see that uh you",
    "start": "837639",
    "end": "839839"
  },
  {
    "text": "can easily Define your workload using",
    "start": "839839",
    "end": "841959"
  },
  {
    "text": "this data class the only thing you need",
    "start": "841959",
    "end": "844000"
  },
  {
    "text": "to do is to specify how the data will",
    "start": "844000",
    "end": "846240"
  },
  {
    "text": "loaded using the rate data API for",
    "start": "846240",
    "end": "848199"
  },
  {
    "text": "example your data is Json or other",
    "start": "848199",
    "end": "850000"
  },
  {
    "text": "formats and then you can choose how to",
    "start": "850000",
    "end": "852120"
  },
  {
    "text": "pass the data in any format you like and",
    "start": "852120",
    "end": "854240"
  },
  {
    "text": "then fit into the airm and after that",
    "start": "854240",
    "end": "857639"
  },
  {
    "text": "the only thing you need to do is to",
    "start": "857639",
    "end": "859399"
  },
  {
    "text": "provide this workload along with engine",
    "start": "859399",
    "end": "862279"
  },
  {
    "text": "configuration um and then that's it you",
    "start": "862279",
    "end": "864959"
  },
  {
    "text": "you are creating the ray Arn batch",
    "start": "864959",
    "end": "867240"
  },
  {
    "text": "Pipeline and then you can easily scale",
    "start": "867240",
    "end": "869360"
  },
  {
    "text": "out this pipeline by specifying the",
    "start": "869360",
    "end": "871160"
  },
  {
    "text": "number of replicas and then you can",
    "start": "871160",
    "end": "873079"
  },
  {
    "text": "change the best size to trade off",
    "start": "873079",
    "end": "875160"
  },
  {
    "text": "between the four tolerance and the",
    "start": "875160",
    "end": "876720"
  },
  {
    "text": "throughputs and the rest of thing will",
    "start": "876720",
    "end": "878839"
  },
  {
    "text": "be handled by R error and batch",
    "start": "878839",
    "end": "881959"
  },
  {
    "text": "specifically we will optimize the engine",
    "start": "881959",
    "end": "884440"
  },
  {
    "text": "configuration and then we'll hand we'll",
    "start": "884440",
    "end": "886480"
  },
  {
    "text": "deal with the scalability and the full",
    "start": "886480",
    "end": "888839"
  },
  {
    "text": "tolerance and again um Ray airn batch is",
    "start": "888839",
    "end": "892399"
  },
  {
    "text": "Al is already available on any scale",
    "start": "892399",
    "end": "894199"
  },
  {
    "text": "template so feel free to take a try and",
    "start": "894199",
    "end": "896839"
  },
  {
    "text": "read uh and do talk to us if you have",
    "start": "896839",
    "end": "898800"
  },
  {
    "text": "any feedback or you need any uh you need",
    "start": "898800",
    "end": "901120"
  },
  {
    "text": "any",
    "start": "901120",
    "end": "902320"
  },
  {
    "text": "supports and here are some um early",
    "start": "902320",
    "end": "904880"
  },
  {
    "text": "results from the r ER and batch you",
    "start": "904880",
    "end": "906639"
  },
  {
    "text": "probably already seen this figures",
    "start": "906639",
    "end": "908079"
  },
  {
    "text": "showing by ritual in the keynote this",
    "start": "908079",
    "end": "909800"
  },
  {
    "text": "morning specifically when benchmarking",
    "start": "909800",
    "end": "912240"
  },
  {
    "text": "Lama",
    "start": "912240",
    "end": "913320"
  },
  {
    "text": "3.1 um on l40 gpus with the 2,000 input",
    "start": "913320",
    "end": "918120"
  },
  {
    "text": "and 100 output we can see that with the",
    "start": "918120",
    "end": "920959"
  },
  {
    "text": "rare ear batch we can have 3X cost",
    "start": "920959",
    "end": "923920"
  },
  {
    "text": "saving compared to barrock um batch API",
    "start": "923920",
    "end": "927480"
  },
  {
    "text": "costing and then",
    "start": "927480",
    "end": "929759"
  },
  {
    "text": "um like Richard shown this morning",
    "start": "929759",
    "end": "931639"
  },
  {
    "text": "usually your prompt has lot of prefix",
    "start": "931639",
    "end": "933759"
  },
  {
    "text": "sharing so suppose your prompt has a",
    "start": "933759",
    "end": "935839"
  },
  {
    "text": "very long instruction that's being",
    "start": "935839",
    "end": "937480"
  },
  {
    "text": "shared across all the requests you have",
    "start": "937480",
    "end": "940040"
  },
  {
    "text": "and this is 80% of your total prompt",
    "start": "940040",
    "end": "942519"
  },
  {
    "text": "lens then you can actually get the",
    "start": "942519",
    "end": "945000"
  },
  {
    "text": "another 2x cost saving so you'll be 6X",
    "start": "945000",
    "end": "948360"
  },
  {
    "text": "compared to the Bedrock bashing API so",
    "start": "948360",
    "end": "952040"
  },
  {
    "text": "there are a lot of potential of the cost",
    "start": "952040",
    "end": "953759"
  },
  {
    "text": "saving using the batch inference and",
    "start": "953759",
    "end": "955680"
  },
  {
    "text": "it's really worklow dependent um and the",
    "start": "955680",
    "end": "958360"
  },
  {
    "text": "r ear is offering a customizable",
    "start": "958360",
    "end": "961560"
  },
  {
    "text": "solution for your particular use",
    "start": "961560",
    "end": "964680"
  },
  {
    "text": "cases and the next picture I want to",
    "start": "964680",
    "end": "966800"
  },
  {
    "text": "introduce is anyc Json mode so let me",
    "start": "966800",
    "end": "970040"
  },
  {
    "text": "first introduce the Json mode Json mode",
    "start": "970040",
    "end": "972560"
  },
  {
    "text": "actually enables a m to produce the Json",
    "start": "972560",
    "end": "975319"
  },
  {
    "text": "format responses and this is the",
    "start": "975319",
    "end": "977319"
  },
  {
    "text": "critical feature to integrate airm with",
    "start": "977319",
    "end": "979880"
  },
  {
    "text": "other systems that consume the airn",
    "start": "979880",
    "end": "981800"
  },
  {
    "text": "output um in other words this is",
    "start": "981800",
    "end": "984160"
  },
  {
    "text": "actually very critical if you want to",
    "start": "984160",
    "end": "985560"
  },
  {
    "text": "build AI agent with airm for example you",
    "start": "985560",
    "end": "988639"
  },
  {
    "text": "may want to ask LM to Output or analyze",
    "start": "988639",
    "end": "992040"
  },
  {
    "text": "a certain article or customer data and",
    "start": "992040",
    "end": "994480"
  },
  {
    "text": "you want LM to Output a certain Json",
    "start": "994480",
    "end": "996600"
  },
  {
    "text": "format with the analys results and you",
    "start": "996600",
    "end": "998639"
  },
  {
    "text": "want that result to be passed by another",
    "start": "998639",
    "end": "1000600"
  },
  {
    "text": "system component for further data",
    "start": "1000600",
    "end": "1002600"
  },
  {
    "text": "processing or whatever the action you",
    "start": "1002600",
    "end": "1004480"
  },
  {
    "text": "want to take so you have to let Ern to",
    "start": "1004480",
    "end": "1007959"
  },
  {
    "text": "strictly follow the Json grammar so that",
    "start": "1007959",
    "end": "1010160"
  },
  {
    "text": "your rest of the system components can",
    "start": "1010160",
    "end": "1012160"
  },
  {
    "text": "consume that without any",
    "start": "1012160",
    "end": "1013839"
  },
  {
    "text": "problem then how to achieve the Json",
    "start": "1013839",
    "end": "1016759"
  },
  {
    "text": "mode um intuitively most solutions out",
    "start": "1016759",
    "end": "1020480"
  },
  {
    "text": "there is actually embarassing the finest",
    "start": "1020480",
    "end": "1022839"
  },
  {
    "text": "the machine based solution uh",
    "start": "1022839",
    "end": "1025079"
  },
  {
    "text": "specifically you'll provide the JSM",
    "start": "1025079",
    "end": "1026600"
  },
  {
    "text": "schema and the Jason schema will be will",
    "start": "1026600",
    "end": "1029120"
  },
  {
    "text": "be converted to the regular expression",
    "start": "1029120",
    "end": "1031079"
  },
  {
    "text": "and based on the regular expression we",
    "start": "1031079",
    "end": "1032520"
  },
  {
    "text": "will construct a finance the machine",
    "start": "1032520",
    "end": "1034520"
  },
  {
    "text": "each state in this financial machine",
    "start": "1034520",
    "end": "1036880"
  },
  {
    "text": "basically says how what's the tokens is",
    "start": "1036880",
    "end": "1039959"
  },
  {
    "text": "available for your next Generation so",
    "start": "1039959",
    "end": "1042880"
  },
  {
    "text": "once you construct this finan the",
    "start": "1042880",
    "end": "1044280"
  },
  {
    "text": "machine during the decoding phase you're",
    "start": "1044280",
    "end": "1046240"
  },
  {
    "text": "just traversing this finan the machine",
    "start": "1046240",
    "end": "1048038"
  },
  {
    "text": "and tell the model",
    "start": "1048039",
    "end": "1049559"
  },
  {
    "text": "um at this moment you can only choose",
    "start": "1049559",
    "end": "1051679"
  },
  {
    "text": "from these three candidate tokens but",
    "start": "1051679",
    "end": "1053880"
  },
  {
    "text": "any but not any others um this solution",
    "start": "1053880",
    "end": "1056960"
  },
  {
    "text": "works well but you can imagine the major",
    "start": "1056960",
    "end": "1059440"
  },
  {
    "text": "problem of of solution is you have the",
    "start": "1059440",
    "end": "1061480"
  },
  {
    "text": "cold start especially if your regular",
    "start": "1061480",
    "end": "1063240"
  },
  {
    "text": "expression is extremely complex so in N",
    "start": "1063240",
    "end": "1066360"
  },
  {
    "text": "scale we took a another algorithm which",
    "start": "1066360",
    "end": "1069320"
  },
  {
    "text": "is called token Tree triers in short T3",
    "start": "1069320",
    "end": "1072200"
  },
  {
    "text": "instead of constructing entire financing",
    "start": "1072200",
    "end": "1074360"
  },
  {
    "text": "machine in advance we actually construct",
    "start": "1074360",
    "end": "1076720"
  },
  {
    "text": "and figure out the finan the machine on",
    "start": "1076720",
    "end": "1078440"
  },
  {
    "text": "the flight",
    "start": "1078440",
    "end": "1079360"
  },
  {
    "text": "so we will determine the next States",
    "start": "1079360",
    "end": "1081360"
  },
  {
    "text": "based on your current status in this way",
    "start": "1081360",
    "end": "1084280"
  },
  {
    "text": "we can support a a much much more",
    "start": "1084280",
    "end": "1086480"
  },
  {
    "text": "complex grammar in gbnf for example you",
    "start": "1086480",
    "end": "1089400"
  },
  {
    "text": "can even put the entire Python language",
    "start": "1089400",
    "end": "1091960"
  },
  {
    "text": "grammar in this adjacent mode or it's",
    "start": "1091960",
    "end": "1094679"
  },
  {
    "text": "not just mod anymore it's basically",
    "start": "1094679",
    "end": "1096159"
  },
  {
    "text": "python mode so yeah and then you can",
    "start": "1096159",
    "end": "1099080"
  },
  {
    "text": "imagine in this way you have to",
    "start": "1099080",
    "end": "1100880"
  },
  {
    "text": "construct this state on the fly so this",
    "start": "1100880",
    "end": "1103159"
  },
  {
    "text": "will bring additional overheads and we",
    "start": "1103159",
    "end": "1105039"
  },
  {
    "text": "solve this by introducing the",
    "start": "1105039",
    "end": "1106720"
  },
  {
    "text": "asynchronous token tree traversal so",
    "start": "1106720",
    "end": "1109120"
  },
  {
    "text": "we're generating this um this States",
    "start": "1109120",
    "end": "1111880"
  },
  {
    "text": "along with the model forward so keep",
    "start": "1111880",
    "end": "1114200"
  },
  {
    "text": "both CPU and GPU busy and purely high",
    "start": "1114200",
    "end": "1117799"
  },
  {
    "text": "this generation",
    "start": "1117799",
    "end": "1119000"
  },
  {
    "text": "latency and then of course we can",
    "start": "1119000",
    "end": "1121200"
  },
  {
    "text": "distribute this token tree processor",
    "start": "1121200",
    "end": "1123679"
  },
  {
    "text": "easily with rate and this feature is",
    "start": "1123679",
    "end": "1126400"
  },
  {
    "text": "also available on in scale and it will",
    "start": "1126400",
    "end": "1128440"
  },
  {
    "text": "will publish more details in our blog",
    "start": "1128440",
    "end": "1130919"
  },
  {
    "text": "post very soon so stay",
    "start": "1130919",
    "end": "1133760"
  },
  {
    "text": "tuned last but not least um the last",
    "start": "1133760",
    "end": "1136520"
  },
  {
    "text": "feature toce today is the fast model",
    "start": "1136520",
    "end": "1138840"
  },
  {
    "text": "loing again why fast model loading so",
    "start": "1138840",
    "end": "1141600"
  },
  {
    "text": "you imagine you're serving a private",
    "start": "1141600",
    "end": "1143880"
  },
  {
    "text": "endpoint um with certain replicas and",
    "start": "1143880",
    "end": "1146679"
  },
  {
    "text": "you has to make sure all the requests",
    "start": "1146679",
    "end": "1149280"
  },
  {
    "text": "are within certain sea let's say one uh",
    "start": "1149280",
    "end": "1151600"
  },
  {
    "text": "one second um you may calculate the",
    "start": "1151600",
    "end": "1154360"
  },
  {
    "text": "number of replicas you need over the",
    "start": "1154360",
    "end": "1156320"
  },
  {
    "text": "time but you may have some spike",
    "start": "1156320",
    "end": "1158200"
  },
  {
    "text": "traffics uh depending on special events",
    "start": "1158200",
    "end": "1160840"
  },
  {
    "text": "so in this case you will need Auto",
    "start": "1160840",
    "end": "1162280"
  },
  {
    "text": "scaling however if you just simply use",
    "start": "1162280",
    "end": "1165240"
  },
  {
    "text": "Autos scaling that will try to open more",
    "start": "1165240",
    "end": "1167840"
  },
  {
    "text": "replicas when there more traffics coming",
    "start": "1167840",
    "end": "1170120"
  },
  {
    "text": "in you may take 5 to 10 minutes to get a",
    "start": "1170120",
    "end": "1172880"
  },
  {
    "text": "new repli car's up and running because",
    "start": "1172880",
    "end": "1174840"
  },
  {
    "text": "every repli car will need to download",
    "start": "1174840",
    "end": "1176679"
  },
  {
    "text": "the model weights from your cloud",
    "start": "1176679",
    "end": "1178400"
  },
  {
    "text": "storage for example hog and face hop or",
    "start": "1178400",
    "end": "1180520"
  },
  {
    "text": "whatever storage you are having and load",
    "start": "1180520",
    "end": "1182760"
  },
  {
    "text": "them to the GPU memory and if you need",
    "start": "1182760",
    "end": "1185480"
  },
  {
    "text": "to take 5 to 10 minutes to make those",
    "start": "1185480",
    "end": "1187240"
  },
  {
    "text": "replicas available you may not be able",
    "start": "1187240",
    "end": "1189640"
  },
  {
    "text": "to efficiently respond to the request",
    "start": "1189640",
    "end": "1192760"
  },
  {
    "text": "traffic changes and in NC fast model",
    "start": "1192760",
    "end": "1195880"
  },
  {
    "text": "loading we actually embrace the optimal",
    "start": "1195880",
    "end": "1198679"
  },
  {
    "text": "model layout and the network utilization",
    "start": "1198679",
    "end": "1201080"
  },
  {
    "text": "so that you can load the model and let",
    "start": "1201080",
    "end": "1203640"
  },
  {
    "text": "the replica up and running with 4X and",
    "start": "1203640",
    "end": "1205840"
  },
  {
    "text": "5x speed up with the 7B model and 70 70b",
    "start": "1205840",
    "end": "1210159"
  },
  {
    "text": "model respectively so by this result we",
    "start": "1210159",
    "end": "1213240"
  },
  {
    "text": "can see every replica can be available",
    "start": "1213240",
    "end": "1216320"
  },
  {
    "text": "within 2 minutes so it's actually pretty",
    "start": "1216320",
    "end": "1218360"
  },
  {
    "text": "efficient and made Auto scaling more",
    "start": "1218360",
    "end": "1220120"
  },
  {
    "text": "practical in your business use cases and",
    "start": "1220120",
    "end": "1223159"
  },
  {
    "text": "then we do have the demo on the reground",
    "start": "1223159",
    "end": "1226280"
  },
  {
    "text": "and add over there so feel free to talk",
    "start": "1226280",
    "end": "1228440"
  },
  {
    "text": "to him if you want to try try out this",
    "start": "1228440",
    "end": "1231360"
  },
  {
    "text": "feature okay so lastly I want to",
    "start": "1231360",
    "end": "1233720"
  },
  {
    "text": "introduce two preview features which are",
    "start": "1233720",
    "end": "1236240"
  },
  {
    "text": "not available in the NC platform yet but",
    "start": "1236240",
    "end": "1238880"
  },
  {
    "text": "it will be available soon if you're",
    "start": "1238880",
    "end": "1240679"
  },
  {
    "text": "interested in those features uh feel",
    "start": "1240679",
    "end": "1242760"
  },
  {
    "text": "free to talk to us and we will introduce",
    "start": "1242760",
    "end": "1244400"
  },
  {
    "text": "more so the first the first one is the",
    "start": "1244400",
    "end": "1246960"
  },
  {
    "text": "custom draft model for specr decoding",
    "start": "1246960",
    "end": "1249280"
  },
  {
    "text": "Philip just introduced specular decoding",
    "start": "1249280",
    "end": "1251039"
  },
  {
    "text": "this sounds very attractive you can get",
    "start": "1251039",
    "end": "1252840"
  },
  {
    "text": "2X or even more itl reduction by",
    "start": "1252840",
    "end": "1256080"
  },
  {
    "text": "introducing a small draft model but in",
    "start": "1256080",
    "end": "1258919"
  },
  {
    "text": "order to achieve that speed up you",
    "start": "1258919",
    "end": "1261520"
  },
  {
    "text": "better have a customized draft model for",
    "start": "1261520",
    "end": "1264159"
  },
  {
    "text": "your particular Downstream taxs so that",
    "start": "1264159",
    "end": "1267280"
  },
  {
    "text": "means you have to train that small model",
    "start": "1267280",
    "end": "1269120"
  },
  {
    "text": "by yourself but in any scale we offer an",
    "start": "1269120",
    "end": "1271919"
  },
  {
    "text": "entrance solution in the r turbo um",
    "start": "1271919",
    "end": "1275000"
  },
  {
    "text": "specifically you will first have your",
    "start": "1275000",
    "end": "1277039"
  },
  {
    "text": "Target Model ready and then you can use",
    "start": "1277039",
    "end": "1279440"
  },
  {
    "text": "Ray AR batch to collect the output and",
    "start": "1279440",
    "end": "1282200"
  },
  {
    "text": "responses of this large Target Model and",
    "start": "1282200",
    "end": "1285120"
  },
  {
    "text": "then you can use a and Forge which is a",
    "start": "1285120",
    "end": "1287279"
  },
  {
    "text": "any scale fine-tuning solution to Trend",
    "start": "1287279",
    "end": "1290120"
  },
  {
    "text": "a small draft model with the cost",
    "start": "1290120",
    "end": "1292200"
  },
  {
    "text": "effective and again air and Forge is",
    "start": "1292200",
    "end": "1294559"
  },
  {
    "text": "available in the N scale both in the r",
    "start": "1294559",
    "end": "1296640"
  },
  {
    "text": "ground so feel free to check out and we",
    "start": "1296640",
    "end": "1298320"
  },
  {
    "text": "have the live demo there um so you will",
    "start": "1298320",
    "end": "1300640"
  },
  {
    "text": "show all the details of the air Forge",
    "start": "1300640",
    "end": "1303360"
  },
  {
    "text": "and after you get the finding draft",
    "start": "1303360",
    "end": "1305200"
  },
  {
    "text": "model your customized draft model can be",
    "start": "1305200",
    "end": "1307760"
  },
  {
    "text": "deployed along with the target model",
    "start": "1307760",
    "end": "1309640"
  },
  {
    "text": "using raym so this will become a",
    "start": "1309640",
    "end": "1312880"
  },
  {
    "text": "fantastic n2n solution provided by any",
    "start": "1312880",
    "end": "1315240"
  },
  {
    "text": "scale so do talk to us if you're",
    "start": "1315240",
    "end": "1317279"
  },
  {
    "text": "interested in this",
    "start": "1317279",
    "end": "1319200"
  },
  {
    "text": "in addition to that as I just mentioned",
    "start": "1319200",
    "end": "1321240"
  },
  {
    "text": "in the batch inference case because we",
    "start": "1321240",
    "end": "1323080"
  },
  {
    "text": "can optimize the engine configuration",
    "start": "1323080",
    "end": "1324919"
  },
  {
    "text": "based on your use cases the Tho can be",
    "start": "1324919",
    "end": "1327799"
  },
  {
    "text": "significantly different with and without",
    "start": "1327799",
    "end": "1329880"
  },
  {
    "text": "the optimization it can be as difference",
    "start": "1329880",
    "end": "1332159"
  },
  {
    "text": "as 2X or even more so in order to figure",
    "start": "1332159",
    "end": "1335360"
  },
  {
    "text": "out the best configuration particularly",
    "start": "1335360",
    "end": "1337360"
  },
  {
    "text": "for your workload we are offering uh the",
    "start": "1337360",
    "end": "1340559"
  },
  {
    "text": "engine engine configuration Optimizer",
    "start": "1340559",
    "end": "1343440"
  },
  {
    "text": "which will basically explore and",
    "start": "1343440",
    "end": "1345159"
  },
  {
    "text": "optimize the engine configurations to",
    "start": "1345159",
    "end": "1347480"
  },
  {
    "text": "minimize the cost only for your",
    "start": "1347480",
    "end": "1349120"
  },
  {
    "text": "workflows specifically you need to",
    "start": "1349120",
    "end": "1351360"
  },
  {
    "text": "provide the model you want to use and",
    "start": "1351360",
    "end": "1353320"
  },
  {
    "text": "the small set of the sample data and",
    "start": "1353320",
    "end": "1356120"
  },
  {
    "text": "then the engine inference engine",
    "start": "1356120",
    "end": "1357679"
  },
  {
    "text": "Optimizer will actually choose the best",
    "start": "1357679",
    "end": "1360120"
  },
  {
    "text": "GPU type the best perence strategy and",
    "start": "1360120",
    "end": "1363400"
  },
  {
    "text": "so on and then figure out the best",
    "start": "1363400",
    "end": "1365559"
  },
  {
    "text": "configuration and then you can just",
    "start": "1365559",
    "end": "1367600"
  },
  {
    "text": "simply feed this configuration file",
    "start": "1367600",
    "end": "1369840"
  },
  {
    "text": "along with the workflow You just defined",
    "start": "1369840",
    "end": "1371960"
  },
  {
    "text": "to the r batch and everything will just",
    "start": "1371960",
    "end": "1374559"
  },
  {
    "text": "work so again this is the preview",
    "start": "1374559",
    "end": "1376799"
  },
  {
    "text": "feature so if you're interested in that",
    "start": "1376799",
    "end": "1378520"
  },
  {
    "text": "do talk to us uh for more",
    "start": "1378520",
    "end": "1381880"
  },
  {
    "text": "information okay finally is a takeaway",
    "start": "1381880",
    "end": "1384600"
  },
  {
    "text": "then I'll head back to",
    "start": "1384600",
    "end": "1386440"
  },
  {
    "text": "fail so we showed you that we are very",
    "start": "1386440",
    "end": "1388760"
  },
  {
    "text": "invested in the open source Community to",
    "start": "1388760",
    "end": "1390720"
  },
  {
    "text": "make um um to contribute The Cutting um",
    "start": "1390720",
    "end": "1393360"
  },
  {
    "text": "edlm optimizations to open source um",
    "start": "1393360",
    "end": "1396320"
  },
  {
    "text": "that includes things like FP support",
    "start": "1396320",
    "end": "1397960"
  },
  {
    "text": "chunk prefill multi support decoding",
    "start": "1397960",
    "end": "1400880"
  },
  {
    "text": "multi step scheduling and many others um",
    "start": "1400880",
    "end": "1403400"
  },
  {
    "text": "and um at any scale we also innovate at",
    "start": "1403400",
    "end": "1405159"
  },
  {
    "text": "all levels of the stack um to make any",
    "start": "1405159",
    "end": "1406880"
  },
  {
    "text": "scale the best place to run um for LM",
    "start": "1406880",
    "end": "1409000"
  },
  {
    "text": "inference this includes inference level",
    "start": "1409000",
    "end": "1410679"
  },
  {
    "text": "optimizations to co-optimize what kind",
    "start": "1410679",
    "end": "1412960"
  },
  {
    "text": "of gpus are run with engine",
    "start": "1412960",
    "end": "1414880"
  },
  {
    "text": "configurations and to um do full",
    "start": "1414880",
    "end": "1416880"
  },
  {
    "text": "tolerance autoscaling to have very good",
    "start": "1416880",
    "end": "1419039"
  },
  {
    "text": "um monitoring observability and then",
    "start": "1419039",
    "end": "1420960"
  },
  {
    "text": "also we've been working on um a lot of",
    "start": "1420960",
    "end": "1423080"
  },
  {
    "text": "inference optimizations that make the",
    "start": "1423080",
    "end": "1424600"
  },
  {
    "text": "inference um even faster including",
    "start": "1424600",
    "end": "1426480"
  },
  {
    "text": "optimized kernels the Chason mode and",
    "start": "1426480",
    "end": "1428640"
  },
  {
    "text": "and then optimizations for batch",
    "start": "1428640",
    "end": "1429760"
  },
  {
    "text": "inference and online inference thank you",
    "start": "1429760",
    "end": "1431679"
  },
  {
    "text": "so much thank",
    "start": "1431679",
    "end": "1435080"
  },
  {
    "text": "you all right for Q&A please use this",
    "start": "1436640",
    "end": "1439760"
  },
  {
    "text": "microphone so we can record your",
    "start": "1439760",
    "end": "1443679"
  },
  {
    "text": "questions um yeah thank you for the",
    "start": "1446799",
    "end": "1448760"
  },
  {
    "text": "great present presentation here um I had",
    "start": "1448760",
    "end": "1451400"
  },
  {
    "text": "a question about um these inference",
    "start": "1451400",
    "end": "1454559"
  },
  {
    "text": "engines because which are cropping up",
    "start": "1454559",
    "end": "1456120"
  },
  {
    "text": "all over the place uh pine cone for",
    "start": "1456120",
    "end": "1458679"
  },
  {
    "text": "example has got an embedded inference",
    "start": "1458679",
    "end": "1460880"
  },
  {
    "text": "engine um any skill is kind of like",
    "start": "1460880",
    "end": "1463520"
  },
  {
    "text": "coming up so for the people who are uh",
    "start": "1463520",
    "end": "1465760"
  },
  {
    "text": "adopting this what is your take on this",
    "start": "1465760",
    "end": "1468679"
  },
  {
    "text": "like which way do we sway towards the",
    "start": "1468679",
    "end": "1471600"
  },
  {
    "text": "database side or is it more on the",
    "start": "1471600",
    "end": "1473559"
  },
  {
    "text": "compute",
    "start": "1473559",
    "end": "1476080"
  },
  {
    "text": "side um I'm not sure if I understand the",
    "start": "1480000",
    "end": "1483080"
  },
  {
    "text": "database analogy um are you asking like",
    "start": "1483080",
    "end": "1485360"
  },
  {
    "text": "what kind of inference engine will be",
    "start": "1485360",
    "end": "1486799"
  },
  {
    "text": "the best or whether will be proprietor",
    "start": "1486799",
    "end": "1488640"
  },
  {
    "text": "open source um I mean the way that we",
    "start": "1488640",
    "end": "1490600"
  },
  {
    "text": "view um the inference engine in terms of",
    "start": "1490600",
    "end": "1492360"
  },
  {
    "text": "propri in open source we do think that",
    "start": "1492360",
    "end": "1494240"
  },
  {
    "text": "there's a lot of value from the open",
    "start": "1494240",
    "end": "1495440"
  },
  {
    "text": "source community and and covering all",
    "start": "1495440",
    "end": "1497480"
  },
  {
    "text": "the different use cases with some",
    "start": "1497480",
    "end": "1499320"
  },
  {
    "text": "generic influence engine like VM is like",
    "start": "1499320",
    "end": "1501760"
  },
  {
    "text": "extremely good um but we also working on",
    "start": "1501760",
    "end": "1504200"
  },
  {
    "text": "proprietor optimizations for the best as",
    "start": "1504200",
    "end": "1506440"
  },
  {
    "text": "I showed um for I'm getting the best",
    "start": "1506440",
    "end": "1508520"
  },
  {
    "text": "performance um um for specific models",
    "start": "1508520",
    "end": "1511399"
  },
  {
    "text": "and specic specific features um so",
    "start": "1511399",
    "end": "1516880"
  },
  {
    "text": "yeah uh for the token tree traversal T3",
    "start": "1517240",
    "end": "1523960"
  },
  {
    "text": "uh for for the T3 traversal does it work",
    "start": "1523960",
    "end": "1527320"
  },
  {
    "text": "with speculative decoding can you",
    "start": "1527320",
    "end": "1529640"
  },
  {
    "text": "generate the three case steps ahead yeah",
    "start": "1529640",
    "end": "1532840"
  },
  {
    "text": "this is a good question so the question",
    "start": "1532840",
    "end": "1534320"
  },
  {
    "text": "is whether the Json mode with the T3 AR",
    "start": "1534320",
    "end": "1536600"
  },
  {
    "text": "is compatible with spectular decoding um",
    "start": "1536600",
    "end": "1538919"
  },
  {
    "text": "the short answer is no at this moment",
    "start": "1538919",
    "end": "1541039"
  },
  {
    "text": "but there's nothing fundamentally",
    "start": "1541039",
    "end": "1542520"
  },
  {
    "text": "prevent us from doing that specifically",
    "start": "1542520",
    "end": "1544520"
  },
  {
    "text": "what you need to do is you need to",
    "start": "1544520",
    "end": "1546000"
  },
  {
    "text": "Traverse you need to Traverse the arithm",
    "start": "1546000",
    "end": "1549000"
  },
  {
    "text": "um Traverse the tree and then along with",
    "start": "1549000",
    "end": "1551159"
  },
  {
    "text": "the specular tokens but you need to you",
    "start": "1551159",
    "end": "1553640"
  },
  {
    "text": "need to provide the capability of",
    "start": "1553640",
    "end": "1554760"
  },
  {
    "text": "falling back if that token was being",
    "start": "1554760",
    "end": "1556520"
  },
  {
    "text": "reject so this is definitely on a ro map",
    "start": "1556520",
    "end": "1559520"
  },
  {
    "text": "um so stay tuned um if we have anything",
    "start": "1559520",
    "end": "1562679"
  },
  {
    "text": "new to release um yeah we we'll let you",
    "start": "1562679",
    "end": "1566960"
  },
  {
    "text": "know so I have a question about that um",
    "start": "1566960",
    "end": "1570039"
  },
  {
    "text": "engine optimization that you had in the",
    "start": "1570039",
    "end": "1571720"
  },
  {
    "text": "previous slide like what what are the",
    "start": "1571720",
    "end": "1574120"
  },
  {
    "text": "hard slas that we can set in order to do",
    "start": "1574120",
    "end": "1577159"
  },
  {
    "text": "this optimization is what is the metric",
    "start": "1577159",
    "end": "1579679"
  },
  {
    "text": "for optimization is it dollar spent is",
    "start": "1579679",
    "end": "1582120"
  },
  {
    "text": "it time or can we set that is that",
    "start": "1582120",
    "end": "1585080"
  },
  {
    "text": "configurable yeah this is also a great",
    "start": "1585080",
    "end": "1586919"
  },
  {
    "text": "question so in short the question is",
    "start": "1586919",
    "end": "1588720"
  },
  {
    "text": "about uh what's the metric used in the",
    "start": "1588720",
    "end": "1591200"
  },
  {
    "text": "engine",
    "start": "1591200",
    "end": "1592120"
  },
  {
    "text": "optimization um so first of all the",
    "start": "1592120",
    "end": "1594960"
  },
  {
    "text": "objective function should be able to set",
    "start": "1594960",
    "end": "1596679"
  },
  {
    "text": "by users but the most reliable metrix",
    "start": "1596679",
    "end": "1599039"
  },
  {
    "text": "we're having right now is the cost per",
    "start": "1599039",
    "end": "1600960"
  },
  {
    "text": "medon token processing um so this will",
    "start": "1600960",
    "end": "1603520"
  },
  {
    "text": "consider the different uh GPU instances",
    "start": "1603520",
    "end": "1605640"
  },
  {
    "text": "price and the model you use and the",
    "start": "1605640",
    "end": "1607480"
  },
  {
    "text": "prompt you use so it's highly effective",
    "start": "1607480",
    "end": "1609640"
  },
  {
    "text": "by for example the length of your input",
    "start": "1609640",
    "end": "1611480"
  },
  {
    "text": "tokens and the length of output tokens",
    "start": "1611480",
    "end": "1613600"
  },
  {
    "text": "and how many uh shared tokens in your",
    "start": "1613600",
    "end": "1616399"
  },
  {
    "text": "input prompt andc Etc",
    "start": "1616399",
    "end": "1620240"
  },
  {
    "text": "yeah yeah it's also a good question so",
    "start": "1625120",
    "end": "1627320"
  },
  {
    "text": "the question is whether the engine",
    "start": "1627320",
    "end": "1628880"
  },
  {
    "text": "optimization will consider the model",
    "start": "1628880",
    "end": "1630679"
  },
  {
    "text": "quality if we enable quantization yeah",
    "start": "1630679",
    "end": "1633840"
  },
  {
    "text": "so this will be definitely uh another",
    "start": "1633840",
    "end": "1636120"
  },
  {
    "text": "metrix if we choose to enable certain",
    "start": "1636120",
    "end": "1638960"
  },
  {
    "text": "quantizations um so we will consider to",
    "start": "1638960",
    "end": "1641919"
  },
  {
    "text": "for example you can provide the set of",
    "start": "1641919",
    "end": "1643679"
  },
  {
    "text": "bench Benchmark set and then we will",
    "start": "1643679",
    "end": "1645720"
  },
  {
    "text": "first evaluate whether the certain",
    "start": "1645720",
    "end": "1647840"
  },
  {
    "text": "quantity is acceptable if so then we",
    "start": "1647840",
    "end": "1650200"
  },
  {
    "text": "will add it to the exploration space and",
    "start": "1650200",
    "end": "1654039"
  },
  {
    "text": "for most of the optimizations that we",
    "start": "1654039",
    "end": "1655880"
  },
  {
    "text": "work on um we try to really um get it",
    "start": "1655880",
    "end": "1659679"
  },
  {
    "text": "all and like um spend most of the time",
    "start": "1659679",
    "end": "1661720"
  },
  {
    "text": "on quation schemes that are as lossless",
    "start": "1661720",
    "end": "1664200"
  },
  {
    "text": "as possible like fp8 and then for the",
    "start": "1664200",
    "end": "1666480"
  },
  {
    "text": "inference optimizations like use as few",
    "start": "1666480",
    "end": "1668760"
  },
  {
    "text": "instructions as possible that makes it",
    "start": "1668760",
    "end": "1670480"
  },
  {
    "text": "both fast and and cost effective um but",
    "start": "1670480",
    "end": "1673159"
  },
  {
    "text": "sometimes they're tradeoffs like spec of",
    "start": "1673159",
    "end": "1674240"
  },
  {
    "text": "decoding for example if you activate",
    "start": "1674240",
    "end": "1675640"
  },
  {
    "text": "that you're going to pay more because",
    "start": "1675640",
    "end": "1677080"
  },
  {
    "text": "you will have some rejected ch",
    "start": "1677080",
    "end": "1678760"
  },
  {
    "text": "and like we need to run the draft model",
    "start": "1678760",
    "end": "1680440"
  },
  {
    "text": "so like that's where the flexibility",
    "start": "1680440",
    "end": "1681960"
  },
  {
    "text": "comes in right and like if the user",
    "start": "1681960",
    "end": "1683440"
  },
  {
    "text": "decides that they want to take these",
    "start": "1683440",
    "end": "1684559"
  },
  {
    "text": "thread offs then they can um but by",
    "start": "1684559",
    "end": "1686760"
  },
  {
    "text": "default we try to optimize as much as",
    "start": "1686760",
    "end": "1688200"
  },
  {
    "text": "possible um without too much",
    "start": "1688200",
    "end": "1691240"
  },
  {
    "text": "tradeoffs um yeah my question is about",
    "start": "1691240",
    "end": "1693640"
  },
  {
    "text": "the FP support so you included FBA",
    "start": "1693640",
    "end": "1697159"
  },
  {
    "text": "support as part of your open source",
    "start": "1697159",
    "end": "1699320"
  },
  {
    "text": "feature but it's also listed as a",
    "start": "1699320",
    "end": "1701519"
  },
  {
    "text": "proprietary feature and this morning uh",
    "start": "1701519",
    "end": "1704640"
  },
  {
    "text": "the speaker keynote mentioned that FBA",
    "start": "1704640",
    "end": "1707440"
  },
  {
    "text": "is actually part part of Turbo Ray that",
    "start": "1707440",
    "end": "1709799"
  },
  {
    "text": "is uh arguably um by default enabl on",
    "start": "1709799",
    "end": "1713039"
  },
  {
    "text": "any scale so my question is what is the",
    "start": "1713039",
    "end": "1715559"
  },
  {
    "text": "difference between the FPA that we have",
    "start": "1715559",
    "end": "1718200"
  },
  {
    "text": "for quantization in the open source",
    "start": "1718200",
    "end": "1720480"
  },
  {
    "text": "versus the one we have on any scale and",
    "start": "1720480",
    "end": "1722840"
  },
  {
    "text": "based on your answer probably I'll have",
    "start": "1722840",
    "end": "1724200"
  },
  {
    "text": "a follow up",
    "start": "1724200",
    "end": "1725760"
  },
  {
    "text": "there so um the N scale um version goes",
    "start": "1725760",
    "end": "1728840"
  },
  {
    "text": "a little bit farther in certain um um",
    "start": "1728840",
    "end": "1730960"
  },
  {
    "text": "cases where we can fuse together more",
    "start": "1730960",
    "end": "1733559"
  },
  {
    "text": "stuff due to how we write the code like",
    "start": "1733559",
    "end": "1735720"
  },
  {
    "text": "um with the open source stuff we go as",
    "start": "1735720",
    "end": "1737279"
  },
  {
    "text": "far as the open source Kels allow us but",
    "start": "1737279",
    "end": "1739559"
  },
  {
    "text": "like um there are some optimizations we",
    "start": "1739559",
    "end": "1740960"
  },
  {
    "text": "can do um um that that are just possible",
    "start": "1740960",
    "end": "1744039"
  },
  {
    "text": "because and actually um this is a",
    "start": "1744039",
    "end": "1746080"
  },
  {
    "text": "Larcher project that th has been leading",
    "start": "1746080",
    "end": "1747679"
  },
  {
    "text": "he sits in the back so if you have more",
    "start": "1747679",
    "end": "1749799"
  },
  {
    "text": "um um questions about that he will be",
    "start": "1749799",
    "end": "1751399"
  },
  {
    "text": "grand person to talk to um about that um",
    "start": "1751399",
    "end": "1754159"
  },
  {
    "text": "um it's it's actually very impressive",
    "start": "1754159",
    "end": "1755679"
  },
  {
    "text": "like it's uh basically rethinking how",
    "start": "1755679",
    "end": "1757720"
  },
  {
    "text": "the inference is being done and like um",
    "start": "1757720",
    "end": "1759760"
  },
  {
    "text": "writing everything from scratch and",
    "start": "1759760",
    "end": "1761279"
  },
  {
    "text": "that's a decent amount of optimizations",
    "start": "1761279",
    "end": "1762760"
  },
  {
    "text": "that we can do there that that are not",
    "start": "1762760",
    "end": "1763960"
  },
  {
    "text": "possible in open source",
    "start": "1763960",
    "end": "1766480"
  },
  {
    "text": "codes all right another round Applause",
    "start": "1766480",
    "end": "1768880"
  },
  {
    "text": "for Philip and Cody thank you",
    "start": "1768880",
    "end": "1773399"
  }
]