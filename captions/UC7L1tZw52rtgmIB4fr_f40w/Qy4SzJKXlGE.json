[
  {
    "start": "0",
    "end": "40000"
  },
  {
    "text": "[Music]",
    "start": "170",
    "end": "14480"
  },
  {
    "text": "hello welcome to my talk i'll present",
    "start": "14480",
    "end": "16240"
  },
  {
    "text": "deep comp which is our approach for",
    "start": "16240",
    "end": "18000"
  },
  {
    "text": "multi-cell selection in 5g and beyond",
    "start": "18000",
    "end": "20560"
  },
  {
    "text": "using reinforcement learning i'm stefan",
    "start": "20560",
    "end": "22960"
  },
  {
    "text": "schneider and this is john work together",
    "start": "22960",
    "end": "24480"
  },
  {
    "text": "with colleagues from paderborn",
    "start": "24480",
    "end": "25760"
  },
  {
    "text": "university",
    "start": "25760",
    "end": "26560"
  },
  {
    "text": "and from huawei germany so in this joint",
    "start": "26560",
    "end": "29199"
  },
  {
    "text": "research project",
    "start": "29199",
    "end": "30080"
  },
  {
    "text": "we used ray rlab to implement deploy and",
    "start": "30080",
    "end": "33200"
  },
  {
    "text": "evaluate our prototypes",
    "start": "33200",
    "end": "34719"
  },
  {
    "text": "but more in that later for now let's",
    "start": "34719",
    "end": "36719"
  },
  {
    "text": "start with the scenario and the",
    "start": "36719",
    "end": "38079"
  },
  {
    "text": "motivation of this work",
    "start": "38079",
    "end": "40079"
  },
  {
    "start": "40000",
    "end": "147000"
  },
  {
    "text": "so we're here in a use case where we",
    "start": "40079",
    "end": "42000"
  },
  {
    "text": "consider a wireless mobile scenario",
    "start": "42000",
    "end": "44239"
  },
  {
    "text": "where we have dense and partially",
    "start": "44239",
    "end": "46239"
  },
  {
    "text": "overlapping cells so here indicated by",
    "start": "46239",
    "end": "48000"
  },
  {
    "text": "these antennas and the",
    "start": "48000",
    "end": "49520"
  },
  {
    "text": "circles indicate the rough range of",
    "start": "49520",
    "end": "51120"
  },
  {
    "text": "these cells and then we have users",
    "start": "51120",
    "end": "53039"
  },
  {
    "text": "moving around these cells",
    "start": "53039",
    "end": "54480"
  },
  {
    "text": "represented by these smartphones here",
    "start": "54480",
    "end": "56719"
  },
  {
    "text": "and these users",
    "start": "56719",
    "end": "58239"
  },
  {
    "text": "want services like ar vr cloud gaming",
    "start": "58239",
    "end": "61039"
  },
  {
    "text": "video streaming",
    "start": "61039",
    "end": "62079"
  },
  {
    "text": "all things that require high and",
    "start": "62079",
    "end": "63680"
  },
  {
    "text": "reliable data rates and one option or",
    "start": "63680",
    "end": "66240"
  },
  {
    "text": "one way to achieve such high in reliable",
    "start": "66240",
    "end": "67920"
  },
  {
    "text": "data rates is using coordinated",
    "start": "67920",
    "end": "69360"
  },
  {
    "text": "multipoint or short comp",
    "start": "69360",
    "end": "71200"
  },
  {
    "text": "and here we focus on comp um coordinated",
    "start": "71200",
    "end": "74240"
  },
  {
    "text": "scheduling",
    "start": "74240",
    "end": "75040"
  },
  {
    "text": "and joint transmission where users can",
    "start": "75040",
    "end": "77200"
  },
  {
    "text": "connect to not just a single cell",
    "start": "77200",
    "end": "79200"
  },
  {
    "text": "but to multiple cells at once and then",
    "start": "79200",
    "end": "81439"
  },
  {
    "text": "receive data from these multiple cells",
    "start": "81439",
    "end": "83520"
  },
  {
    "text": "simultaneously",
    "start": "83520",
    "end": "84640"
  },
  {
    "text": "such that their effective effectively",
    "start": "84640",
    "end": "86799"
  },
  {
    "text": "received data rate",
    "start": "86799",
    "end": "87759"
  },
  {
    "text": "is significantly increased and so",
    "start": "87759",
    "end": "91280"
  },
  {
    "text": "as these users move around and connect",
    "start": "91280",
    "end": "93119"
  },
  {
    "text": "to the to the different cells",
    "start": "93119",
    "end": "95040"
  },
  {
    "text": "they of course compete for limited radio",
    "start": "95040",
    "end": "97040"
  },
  {
    "text": "resources for example",
    "start": "97040",
    "end": "98640"
  },
  {
    "text": "in the form of radio resource blocks",
    "start": "98640",
    "end": "101439"
  },
  {
    "text": "which are limited in",
    "start": "101439",
    "end": "102560"
  },
  {
    "text": "frequency and time domain and here we",
    "start": "102560",
    "end": "105360"
  },
  {
    "text": "explicitly don't make any assumptions",
    "start": "105360",
    "end": "106960"
  },
  {
    "text": "about",
    "start": "106960",
    "end": "107600"
  },
  {
    "text": "how these files allocate resources to",
    "start": "107600",
    "end": "109680"
  },
  {
    "text": "connected users",
    "start": "109680",
    "end": "111200"
  },
  {
    "text": "because this might be vendor specific or",
    "start": "111200",
    "end": "113759"
  },
  {
    "text": "proprietary and it might very well",
    "start": "113759",
    "end": "115520"
  },
  {
    "text": "happen that cell a",
    "start": "115520",
    "end": "116799"
  },
  {
    "text": "here has a different resource allocation",
    "start": "116799",
    "end": "118799"
  },
  {
    "text": "scheme than cell b and cell c",
    "start": "118799",
    "end": "120960"
  },
  {
    "text": "so we don't need to know about these two",
    "start": "120960",
    "end": "122799"
  },
  {
    "text": "allocation schemes we don't assume that",
    "start": "122799",
    "end": "124240"
  },
  {
    "text": "they're",
    "start": "124240",
    "end": "124719"
  },
  {
    "text": "homogeneous and so in this whole",
    "start": "124719",
    "end": "127520"
  },
  {
    "text": "scenario",
    "start": "127520",
    "end": "128080"
  },
  {
    "text": "our goal is to optimize dynamic",
    "start": "128080",
    "end": "130319"
  },
  {
    "text": "multi-cell selection so",
    "start": "130319",
    "end": "132319"
  },
  {
    "text": "for each user over time we want to",
    "start": "132319",
    "end": "134560"
  },
  {
    "text": "decide how many cells each user should",
    "start": "134560",
    "end": "136480"
  },
  {
    "text": "connect to",
    "start": "136480",
    "end": "137280"
  },
  {
    "text": "and which cells it should connect to and",
    "start": "137280",
    "end": "139920"
  },
  {
    "text": "the goal here is",
    "start": "139920",
    "end": "141200"
  },
  {
    "text": "to maximize the total quality of",
    "start": "141200",
    "end": "143040"
  },
  {
    "text": "experience or hear short q a",
    "start": "143040",
    "end": "145440"
  },
  {
    "text": "of all of our users and we model this",
    "start": "145440",
    "end": "148560"
  },
  {
    "start": "147000",
    "end": "216000"
  },
  {
    "text": "quality of experience",
    "start": "148560",
    "end": "149760"
  },
  {
    "text": "based on some previous studies as a",
    "start": "149760",
    "end": "151920"
  },
  {
    "text": "logarithmic function of the data rate",
    "start": "151920",
    "end": "154000"
  },
  {
    "text": "for each user",
    "start": "154000",
    "end": "155120"
  },
  {
    "text": "so if the data rate is zero then the",
    "start": "155120",
    "end": "157440"
  },
  {
    "text": "quality of experience is really bad",
    "start": "157440",
    "end": "158959"
  },
  {
    "text": "because",
    "start": "158959",
    "end": "159599"
  },
  {
    "text": "the service does not work and so if we",
    "start": "159599",
    "end": "161760"
  },
  {
    "text": "increase the data rate by just a little",
    "start": "161760",
    "end": "163680"
  },
  {
    "text": "bit",
    "start": "163680",
    "end": "164319"
  },
  {
    "text": "then the quality experience dramatically",
    "start": "164319",
    "end": "166319"
  },
  {
    "text": "improves because this service suddenly",
    "start": "166319",
    "end": "168160"
  },
  {
    "text": "starts to work",
    "start": "168160",
    "end": "169360"
  },
  {
    "text": "but then if we increase the data rate",
    "start": "169360",
    "end": "171120"
  },
  {
    "text": "more and more at some point we get",
    "start": "171120",
    "end": "172720"
  },
  {
    "text": "diminishing returns",
    "start": "172720",
    "end": "174400"
  },
  {
    "text": "and one nice thing about this utility",
    "start": "174400",
    "end": "176800"
  },
  {
    "text": "function is that it implicitly",
    "start": "176800",
    "end": "178319"
  },
  {
    "text": "encourages fairness",
    "start": "178319",
    "end": "180159"
  },
  {
    "text": "because if we want to maximize the total",
    "start": "180159",
    "end": "182000"
  },
  {
    "text": "quality of experience for all of our",
    "start": "182000",
    "end": "183599"
  },
  {
    "text": "users",
    "start": "183599",
    "end": "184239"
  },
  {
    "text": "we can do this most effectively by",
    "start": "184239",
    "end": "186080"
  },
  {
    "text": "improving the data rate of users that",
    "start": "186080",
    "end": "187599"
  },
  {
    "text": "currently have a",
    "start": "187599",
    "end": "188800"
  },
  {
    "text": "low data rate and what low quality of",
    "start": "188800",
    "end": "190400"
  },
  {
    "text": "experience because then we get this huge",
    "start": "190400",
    "end": "191920"
  },
  {
    "text": "gain in",
    "start": "191920",
    "end": "192800"
  },
  {
    "text": "qoe whereas improving the data rate of",
    "start": "192800",
    "end": "195200"
  },
  {
    "text": "users that already have a high rate",
    "start": "195200",
    "end": "196959"
  },
  {
    "text": "is not so helpful here but as i said",
    "start": "196959",
    "end": "200720"
  },
  {
    "text": "this is all based on previous studies",
    "start": "200720",
    "end": "202480"
  },
  {
    "text": "but it's just an example",
    "start": "202480",
    "end": "204159"
  },
  {
    "text": "we could also easily optimize any other",
    "start": "204159",
    "end": "206400"
  },
  {
    "text": "utility function here",
    "start": "206400",
    "end": "209280"
  },
  {
    "text": "so our goal is to maximize this total",
    "start": "209280",
    "end": "212000"
  },
  {
    "text": "quality of experience",
    "start": "212000",
    "end": "213360"
  },
  {
    "text": "and we want to do this with deep",
    "start": "213360",
    "end": "214640"
  },
  {
    "text": "reinforcement learning and for that we",
    "start": "214640",
    "end": "216959"
  },
  {
    "start": "216000",
    "end": "255000"
  },
  {
    "text": "proposed",
    "start": "216959",
    "end": "217519"
  },
  {
    "text": "three different approaches the first one",
    "start": "217519",
    "end": "220000"
  },
  {
    "text": "is called deep comp and it's a",
    "start": "220000",
    "end": "221840"
  },
  {
    "text": "centralized approach so we have a",
    "start": "221840",
    "end": "223200"
  },
  {
    "text": "centralized training procedure and we do",
    "start": "223200",
    "end": "224879"
  },
  {
    "text": "inference in a centralized",
    "start": "224879",
    "end": "226080"
  },
  {
    "text": "way then we have two multi-agent",
    "start": "226080",
    "end": "228640"
  },
  {
    "text": "approaches",
    "start": "228640",
    "end": "230400"
  },
  {
    "text": "d3 comp is completely distributed so it",
    "start": "230400",
    "end": "232640"
  },
  {
    "text": "uses",
    "start": "232640",
    "end": "233760"
  },
  {
    "text": "um distributed separate policy separate",
    "start": "233760",
    "end": "236640"
  },
  {
    "text": "neural networks during training",
    "start": "236640",
    "end": "238720"
  },
  {
    "text": "and also during inference whereas dd",
    "start": "238720",
    "end": "241120"
  },
  {
    "text": "comp is also multi-agent",
    "start": "241120",
    "end": "242720"
  },
  {
    "text": "but it uses a shared shared policy a",
    "start": "242720",
    "end": "244879"
  },
  {
    "text": "shared neural network during training",
    "start": "244879",
    "end": "247040"
  },
  {
    "text": "which can then be replicated for",
    "start": "247040",
    "end": "248400"
  },
  {
    "text": "distributed inference",
    "start": "248400",
    "end": "250080"
  },
  {
    "text": "so i'll walk through these different",
    "start": "250080",
    "end": "251360"
  },
  {
    "text": "approaches and compare their strengths",
    "start": "251360",
    "end": "253200"
  },
  {
    "text": "and weaknesses",
    "start": "253200",
    "end": "255040"
  },
  {
    "start": "255000",
    "end": "393000"
  },
  {
    "text": "let's start with deepcom the centralized",
    "start": "255040",
    "end": "257440"
  },
  {
    "text": "approach",
    "start": "257440",
    "end": "258400"
  },
  {
    "text": "deepcomp requires a global view and",
    "start": "258400",
    "end": "260799"
  },
  {
    "text": "global control of all of our users in",
    "start": "260799",
    "end": "262639"
  },
  {
    "text": "the area so",
    "start": "262639",
    "end": "263680"
  },
  {
    "text": "this is a bit challenging from a",
    "start": "263680",
    "end": "265360"
  },
  {
    "text": "technical perspective and it also means",
    "start": "265360",
    "end": "267360"
  },
  {
    "text": "that we get a",
    "start": "267360",
    "end": "268080"
  },
  {
    "text": "large observation on large action space",
    "start": "268080",
    "end": "270320"
  },
  {
    "text": "if we have many users",
    "start": "270320",
    "end": "271600"
  },
  {
    "text": "so quite high complexity and training",
    "start": "271600",
    "end": "273520"
  },
  {
    "text": "effort which is",
    "start": "273520",
    "end": "274880"
  },
  {
    "text": "a bit of the downside of this approach",
    "start": "274880",
    "end": "277040"
  },
  {
    "text": "but on the upside",
    "start": "277040",
    "end": "278400"
  },
  {
    "text": "it can learn really powerful and",
    "start": "278400",
    "end": "280240"
  },
  {
    "text": "fine-grained",
    "start": "280240",
    "end": "281520"
  },
  {
    "text": "coordination and cell selection policies",
    "start": "281520",
    "end": "284479"
  },
  {
    "text": "where",
    "start": "284479",
    "end": "284960"
  },
  {
    "text": "the different users cooperate with each",
    "start": "284960",
    "end": "286720"
  },
  {
    "text": "other because it does observe",
    "start": "286720",
    "end": "288479"
  },
  {
    "text": "and can simultaneously control all of",
    "start": "288479",
    "end": "290400"
  },
  {
    "text": "the users together",
    "start": "290400",
    "end": "293360"
  },
  {
    "text": "so to implement deep comp we need to",
    "start": "293360",
    "end": "295840"
  },
  {
    "text": "define the design the mark of decision",
    "start": "295840",
    "end": "297919"
  },
  {
    "text": "process or actually",
    "start": "297919",
    "end": "299120"
  },
  {
    "text": "partially observable markov decision",
    "start": "299120",
    "end": "301120"
  },
  {
    "text": "process and we defined observations for",
    "start": "301120",
    "end": "303600"
  },
  {
    "text": "that",
    "start": "303600",
    "end": "304720"
  },
  {
    "text": "based on the current connections of each",
    "start": "304720",
    "end": "307039"
  },
  {
    "text": "user to the different cells",
    "start": "307039",
    "end": "309120"
  },
  {
    "text": "based on the signal strength between the",
    "start": "309120",
    "end": "311120"
  },
  {
    "text": "different cells and users so this is the",
    "start": "311120",
    "end": "313280"
  },
  {
    "text": "signal to interference and noise ratio",
    "start": "313280",
    "end": "315120"
  },
  {
    "text": "or short sinr",
    "start": "315120",
    "end": "317199"
  },
  {
    "text": "and based on the user's current quality",
    "start": "317199",
    "end": "320080"
  },
  {
    "text": "of experience",
    "start": "320080",
    "end": "321280"
  },
  {
    "text": "and here we don't even need to know this",
    "start": "321280",
    "end": "323039"
  },
  {
    "text": "utility function they showed earlier",
    "start": "323039",
    "end": "324960"
  },
  {
    "text": "we simply need to approximate or",
    "start": "324960",
    "end": "326479"
  },
  {
    "text": "estimate the instantaneous quality of",
    "start": "326479",
    "end": "329199"
  },
  {
    "text": "experience for each user",
    "start": "329199",
    "end": "330560"
  },
  {
    "text": "and there are approaches out there to do",
    "start": "330560",
    "end": "332320"
  },
  {
    "text": "that",
    "start": "332320",
    "end": "334160"
  },
  {
    "text": "so given such an observation our deepcom",
    "start": "334160",
    "end": "336400"
  },
  {
    "text": "agent makes an action",
    "start": "336400",
    "end": "338000"
  },
  {
    "text": "and that action defines the cell",
    "start": "338000",
    "end": "340160"
  },
  {
    "text": "selection for all of our different users",
    "start": "340160",
    "end": "342639"
  },
  {
    "text": "and kind of the default action that we",
    "start": "342639",
    "end": "344880"
  },
  {
    "text": "have here is",
    "start": "344880",
    "end": "345919"
  },
  {
    "text": "to simply keep all connections as they",
    "start": "345919",
    "end": "347600"
  },
  {
    "text": "currently are so",
    "start": "347600",
    "end": "349600"
  },
  {
    "text": "not connecting to any new cell and also",
    "start": "349600",
    "end": "351600"
  },
  {
    "text": "not disconnecting from any cell",
    "start": "351600",
    "end": "353199"
  },
  {
    "text": "but of course the agent can also decide",
    "start": "353199",
    "end": "355600"
  },
  {
    "text": "that a user should connect to a new cell",
    "start": "355600",
    "end": "357280"
  },
  {
    "text": "or should disconnect from an existing",
    "start": "357280",
    "end": "359199"
  },
  {
    "text": "cell or from a connected cell",
    "start": "359199",
    "end": "361120"
  },
  {
    "text": "and we designed the action space here in",
    "start": "361120",
    "end": "363440"
  },
  {
    "text": "a way",
    "start": "363440",
    "end": "364319"
  },
  {
    "text": "that each user can make at most one",
    "start": "364319",
    "end": "366080"
  },
  {
    "text": "connection or disconnection per time",
    "start": "366080",
    "end": "367680"
  },
  {
    "text": "step",
    "start": "367680",
    "end": "368319"
  },
  {
    "text": "simply to limit protocol overhead and so",
    "start": "368319",
    "end": "372000"
  },
  {
    "text": "with these observations and actions the",
    "start": "372000",
    "end": "374240"
  },
  {
    "text": "agent",
    "start": "374240",
    "end": "375039"
  },
  {
    "text": "after each action receives a reward",
    "start": "375039",
    "end": "376800"
  },
  {
    "text": "which we define to simply be the sum of",
    "start": "376800",
    "end": "378720"
  },
  {
    "text": "all",
    "start": "378720",
    "end": "379199"
  },
  {
    "text": "users quality of experience because",
    "start": "379199",
    "end": "380800"
  },
  {
    "text": "that's exactly what we want to optimize",
    "start": "380800",
    "end": "384400"
  },
  {
    "text": "so that's the centralized deep comp",
    "start": "384400",
    "end": "386240"
  },
  {
    "text": "approach now the question is how can we",
    "start": "386240",
    "end": "388000"
  },
  {
    "text": "move far from the centralized approach",
    "start": "388000",
    "end": "390000"
  },
  {
    "text": "to the distributed",
    "start": "390000",
    "end": "391280"
  },
  {
    "text": "approaches and for that we need to make",
    "start": "391280",
    "end": "393440"
  },
  {
    "start": "393000",
    "end": "547000"
  },
  {
    "text": "some adjustments so",
    "start": "393440",
    "end": "394880"
  },
  {
    "text": "the main idea is that rather than one",
    "start": "394880",
    "end": "397199"
  },
  {
    "text": "centralized agent that observes and",
    "start": "397199",
    "end": "398720"
  },
  {
    "text": "controls everything",
    "start": "398720",
    "end": "399919"
  },
  {
    "text": "we have separate drlhens one for each",
    "start": "399919",
    "end": "402400"
  },
  {
    "text": "user and each drl agent",
    "start": "402400",
    "end": "404319"
  },
  {
    "text": "only has local observations of what the",
    "start": "404319",
    "end": "407199"
  },
  {
    "text": "own user does",
    "start": "407199",
    "end": "408160"
  },
  {
    "text": "and only controls the cell selection",
    "start": "408160",
    "end": "410479"
  },
  {
    "text": "locally off this",
    "start": "410479",
    "end": "411759"
  },
  {
    "text": "on user and that's of course a lot",
    "start": "411759",
    "end": "414240"
  },
  {
    "text": "simpler we have a much smaller",
    "start": "414240",
    "end": "416000"
  },
  {
    "text": "observation action space it's easier to",
    "start": "416000",
    "end": "418240"
  },
  {
    "text": "implement and it's a lot faster to train",
    "start": "418240",
    "end": "420160"
  },
  {
    "text": "so that's really great but",
    "start": "420160",
    "end": "422160"
  },
  {
    "text": "there's also a downside or challenging",
    "start": "422160",
    "end": "424400"
  },
  {
    "text": "part to it",
    "start": "424400",
    "end": "425440"
  },
  {
    "text": "which is that such an approach is quite",
    "start": "425440",
    "end": "428000"
  },
  {
    "text": "prone to",
    "start": "428000",
    "end": "428639"
  },
  {
    "text": "learning greedy behavior so if an agent",
    "start": "428639",
    "end": "432240"
  },
  {
    "text": "only observes their own user only",
    "start": "432240",
    "end": "434319"
  },
  {
    "text": "controls the action or the cell",
    "start": "434319",
    "end": "435919"
  },
  {
    "text": "selections of their own user",
    "start": "435919",
    "end": "437520"
  },
  {
    "text": "trying to maximize the qualitative",
    "start": "437520",
    "end": "438880"
  },
  {
    "text": "experience of only the own user",
    "start": "438880",
    "end": "440639"
  },
  {
    "text": "of course it learns a policy that",
    "start": "440639",
    "end": "442319"
  },
  {
    "text": "greedily maximizes their own user's",
    "start": "442319",
    "end": "443919"
  },
  {
    "text": "quality of experience",
    "start": "443919",
    "end": "445120"
  },
  {
    "text": "and this often means that these policies",
    "start": "445120",
    "end": "447039"
  },
  {
    "text": "are quite unfair",
    "start": "447039",
    "end": "448319"
  },
  {
    "text": "and really don't optimize our total",
    "start": "448319",
    "end": "450800"
  },
  {
    "text": "quality of experience",
    "start": "450800",
    "end": "451919"
  },
  {
    "text": "and that's what we want as an objective",
    "start": "451919",
    "end": "454080"
  },
  {
    "text": "here",
    "start": "454080",
    "end": "455039"
  },
  {
    "text": "so we need to make some adjustments and",
    "start": "455039",
    "end": "457759"
  },
  {
    "text": "first of all clearly here the",
    "start": "457759",
    "end": "459199"
  },
  {
    "text": "observations and actions are defined",
    "start": "459199",
    "end": "461440"
  },
  {
    "text": "per user for a single user corresponding",
    "start": "461440",
    "end": "464479"
  },
  {
    "text": "to the agent so not for all users like",
    "start": "464479",
    "end": "466319"
  },
  {
    "text": "in the centralized approach",
    "start": "466319",
    "end": "468240"
  },
  {
    "text": "but we also need some extra observation",
    "start": "468240",
    "end": "470160"
  },
  {
    "text": "to make these agents aware of what's",
    "start": "470160",
    "end": "472319"
  },
  {
    "text": "happening around them",
    "start": "472319",
    "end": "473360"
  },
  {
    "text": "and for that we introduce one extra",
    "start": "473360",
    "end": "475280"
  },
  {
    "text": "observation that indicates the number of",
    "start": "475280",
    "end": "478000"
  },
  {
    "text": "users per cell and that helps these",
    "start": "478000",
    "end": "480319"
  },
  {
    "text": "agents to",
    "start": "480319",
    "end": "481520"
  },
  {
    "text": "uh recognize if a cell is currently",
    "start": "481520",
    "end": "483280"
  },
  {
    "text": "unused and available or a facility cell",
    "start": "483280",
    "end": "485520"
  },
  {
    "text": "is highly congested",
    "start": "485520",
    "end": "486800"
  },
  {
    "text": "and then helps avoid congested cells",
    "start": "486800",
    "end": "490000"
  },
  {
    "text": "we also need to adjust our reward to not",
    "start": "490000",
    "end": "491840"
  },
  {
    "text": "just consider the quality of experience",
    "start": "491840",
    "end": "493440"
  },
  {
    "text": "of our own user",
    "start": "493440",
    "end": "494560"
  },
  {
    "text": "or of each agent's own user and uh",
    "start": "494560",
    "end": "498160"
  },
  {
    "text": "here in this distributed setting we",
    "start": "498160",
    "end": "499680"
  },
  {
    "text": "can't simply sum up overall",
    "start": "499680",
    "end": "501360"
  },
  {
    "text": "user's quality of experience because we",
    "start": "501360",
    "end": "502879"
  },
  {
    "text": "don't know them",
    "start": "502879",
    "end": "504400"
  },
  {
    "text": "but what we can do is we can average or",
    "start": "504400",
    "end": "506800"
  },
  {
    "text": "sum up in this case average",
    "start": "506800",
    "end": "508400"
  },
  {
    "text": "over the competing set and we define",
    "start": "508400",
    "end": "511120"
  },
  {
    "text": "this competing set as",
    "start": "511120",
    "end": "512479"
  },
  {
    "text": "all users that are connected to some of",
    "start": "512479",
    "end": "514320"
  },
  {
    "text": "the same base stations or to some of the",
    "start": "514320",
    "end": "516479"
  },
  {
    "text": "same cells as our own user",
    "start": "516479",
    "end": "518320"
  },
  {
    "text": "and here we assume that via these",
    "start": "518320",
    "end": "520080"
  },
  {
    "text": "connected or shared cells we can",
    "start": "520080",
    "end": "521839"
  },
  {
    "text": "exchange the quality of experience",
    "start": "521839",
    "end": "523919"
  },
  {
    "text": "of these users and so by using these",
    "start": "523919",
    "end": "527440"
  },
  {
    "text": "uh this average qoe over the computing",
    "start": "527440",
    "end": "529760"
  },
  {
    "text": "set we can avoid",
    "start": "529760",
    "end": "530720"
  },
  {
    "text": "greedily maximizing only our own users",
    "start": "530720",
    "end": "532720"
  },
  {
    "text": "quality of experience and that helps",
    "start": "532720",
    "end": "534640"
  },
  {
    "text": "encourage agents to use",
    "start": "534640",
    "end": "536080"
  },
  {
    "text": "free cells and to avoid competition with",
    "start": "536080",
    "end": "538000"
  },
  {
    "text": "other users or if competition is",
    "start": "538000",
    "end": "540480"
  },
  {
    "text": "necessary then rather compete with users",
    "start": "540480",
    "end": "542240"
  },
  {
    "text": "that already have a high quality",
    "start": "542240",
    "end": "543440"
  },
  {
    "text": "experience",
    "start": "543440",
    "end": "544240"
  },
  {
    "text": "where this competition is not as harmful",
    "start": "544240",
    "end": "547519"
  },
  {
    "start": "547000",
    "end": "610000"
  },
  {
    "text": "so based on this adjusted pr mdp we",
    "start": "547519",
    "end": "550160"
  },
  {
    "text": "designed and developed our two",
    "start": "550160",
    "end": "551760"
  },
  {
    "text": "distributed approaches first d3 com",
    "start": "551760",
    "end": "554480"
  },
  {
    "text": "which is fully distributed",
    "start": "554480",
    "end": "556000"
  },
  {
    "text": "where we have these independent drl",
    "start": "556000",
    "end": "557760"
  },
  {
    "text": "agents each with its own separate policy",
    "start": "557760",
    "end": "559839"
  },
  {
    "text": "and neural network",
    "start": "559839",
    "end": "561279"
  },
  {
    "text": "um and during training we don't need any",
    "start": "561279",
    "end": "563440"
  },
  {
    "text": "extra communication",
    "start": "563440",
    "end": "564480"
  },
  {
    "text": "for these sterile agents because they",
    "start": "564480",
    "end": "566160"
  },
  {
    "text": "are all trained independently",
    "start": "566160",
    "end": "568560"
  },
  {
    "text": "and one benefit here is that because",
    "start": "568560",
    "end": "570959"
  },
  {
    "text": "they all have their own policy that's",
    "start": "570959",
    "end": "572560"
  },
  {
    "text": "being trained independently",
    "start": "572560",
    "end": "573920"
  },
  {
    "text": "they can potentially learn heterogeneous",
    "start": "573920",
    "end": "575680"
  },
  {
    "text": "cell selection policies",
    "start": "575680",
    "end": "577040"
  },
  {
    "text": "which might be useful if our users are",
    "start": "577040",
    "end": "579120"
  },
  {
    "text": "very different from each other",
    "start": "579120",
    "end": "581839"
  },
  {
    "text": "then we also have dd comp which is also",
    "start": "581839",
    "end": "584800"
  },
  {
    "text": "a multi-agent approach",
    "start": "584800",
    "end": "586240"
  },
  {
    "text": "it also relies on local observations and",
    "start": "586240",
    "end": "588560"
  },
  {
    "text": "actions",
    "start": "588560",
    "end": "589279"
  },
  {
    "text": "but here these agents at least during",
    "start": "589279",
    "end": "591920"
  },
  {
    "text": "training use a shared policy a shared",
    "start": "591920",
    "end": "593600"
  },
  {
    "text": "neural network",
    "start": "593600",
    "end": "594480"
  },
  {
    "text": "and combine all of their and all the",
    "start": "594480",
    "end": "597040"
  },
  {
    "text": "agents combine their experience during",
    "start": "597040",
    "end": "598640"
  },
  {
    "text": "training such that we have more data",
    "start": "598640",
    "end": "600320"
  },
  {
    "text": "that we can leverage during training",
    "start": "600320",
    "end": "601760"
  },
  {
    "text": "from all the different users and it also",
    "start": "601760",
    "end": "603920"
  },
  {
    "text": "means that we can often learn better and",
    "start": "603920",
    "end": "605440"
  },
  {
    "text": "slightly more",
    "start": "605440",
    "end": "606640"
  },
  {
    "text": "robust policies than with the 3 comp",
    "start": "606640",
    "end": "610880"
  },
  {
    "start": "610000",
    "end": "645000"
  },
  {
    "text": "and so overall these are the three",
    "start": "610880",
    "end": "612640"
  },
  {
    "text": "different approaches that we propose",
    "start": "612640",
    "end": "614320"
  },
  {
    "text": "the centralized deep comp approach than",
    "start": "614320",
    "end": "616640"
  },
  {
    "text": "a fully distributed d3",
    "start": "616640",
    "end": "618000"
  },
  {
    "text": "comp approach where we have separate",
    "start": "618000",
    "end": "620079"
  },
  {
    "text": "policies for each agent",
    "start": "620079",
    "end": "621600"
  },
  {
    "text": "and then the ddcom approach where we",
    "start": "621600",
    "end": "623200"
  },
  {
    "text": "have a centralized training procedure",
    "start": "623200",
    "end": "624800"
  },
  {
    "text": "with the share policy",
    "start": "624800",
    "end": "626079"
  },
  {
    "text": "and then during the production",
    "start": "626079",
    "end": "627600"
  },
  {
    "text": "employment we can replicate this policy",
    "start": "627600",
    "end": "630000"
  },
  {
    "text": "replicate the neural network",
    "start": "630000",
    "end": "631279"
  },
  {
    "text": "and still perform inferior inference in",
    "start": "631279",
    "end": "633360"
  },
  {
    "text": "a distributed",
    "start": "633360",
    "end": "634399"
  },
  {
    "text": "manner so now that we have defined our",
    "start": "634399",
    "end": "637920"
  },
  {
    "text": "problem we have to assign our three",
    "start": "637920",
    "end": "639279"
  },
  {
    "text": "different approaches",
    "start": "639279",
    "end": "640320"
  },
  {
    "text": "we need to go and implement them and",
    "start": "640320",
    "end": "642399"
  },
  {
    "text": "that's exactly where ray our ellip comes",
    "start": "642399",
    "end": "644320"
  },
  {
    "text": "into play",
    "start": "644320",
    "end": "646079"
  },
  {
    "start": "645000",
    "end": "664000"
  },
  {
    "text": "so as a first part of the implementation",
    "start": "646079",
    "end": "648800"
  },
  {
    "text": "i",
    "start": "648800",
    "end": "649440"
  },
  {
    "text": "implemented the custom and configurable",
    "start": "649440",
    "end": "651440"
  },
  {
    "text": "mobile environment that i used for",
    "start": "651440",
    "end": "653040"
  },
  {
    "text": "training",
    "start": "653040",
    "end": "653680"
  },
  {
    "text": "and for evaluation of these drl",
    "start": "653680",
    "end": "655519"
  },
  {
    "text": "approaches and of course i had to",
    "start": "655519",
    "end": "657279"
  },
  {
    "text": "implement this openi gym interface here",
    "start": "657279",
    "end": "659839"
  },
  {
    "text": "such that i could connect different rl",
    "start": "659839",
    "end": "661440"
  },
  {
    "text": "approaches to this",
    "start": "661440",
    "end": "663360"
  },
  {
    "text": "environment so for a single agent for",
    "start": "663360",
    "end": "666240"
  },
  {
    "start": "664000",
    "end": "725000"
  },
  {
    "text": "the centralized approach the steep pump",
    "start": "666240",
    "end": "668000"
  },
  {
    "text": "approach",
    "start": "668000",
    "end": "669200"
  },
  {
    "text": "the code could look somewhat like this",
    "start": "669200",
    "end": "670800"
  },
  {
    "text": "it's a bit of a simplification here",
    "start": "670800",
    "end": "673040"
  },
  {
    "text": "but the real code is on github",
    "start": "673040",
    "end": "676079"
  },
  {
    "text": "so here i import jim and i implement a",
    "start": "676079",
    "end": "679200"
  },
  {
    "text": "gym",
    "start": "679200",
    "end": "679600"
  },
  {
    "text": "or i inherit the gym environment and i",
    "start": "679600",
    "end": "681360"
  },
  {
    "text": "implement a class mobile environment",
    "start": "681360",
    "end": "683360"
  },
  {
    "text": "that has a constructor get some",
    "start": "683360",
    "end": "685279"
  },
  {
    "text": "configuration and picked here",
    "start": "685279",
    "end": "687200"
  },
  {
    "text": "then it has these typical reset step and",
    "start": "687200",
    "end": "689600"
  },
  {
    "text": "reward",
    "start": "689600",
    "end": "690480"
  },
  {
    "text": "step and render functions that are",
    "start": "690480",
    "end": "692399"
  },
  {
    "text": "dictated by the gem interface",
    "start": "692399",
    "end": "694720"
  },
  {
    "text": "and i also have some functions to get",
    "start": "694720",
    "end": "696720"
  },
  {
    "text": "the current observation and current",
    "start": "696720",
    "end": "698160"
  },
  {
    "text": "reward",
    "start": "698160",
    "end": "698720"
  },
  {
    "text": "which are used inside the step function",
    "start": "698720",
    "end": "701120"
  },
  {
    "text": "and the step function also implements",
    "start": "701120",
    "end": "702480"
  },
  {
    "text": "all the dynamics and all the environment",
    "start": "702480",
    "end": "704000"
  },
  {
    "text": "logic",
    "start": "704000",
    "end": "705279"
  },
  {
    "text": "now the question is how to get from this",
    "start": "705279",
    "end": "708000"
  },
  {
    "text": "single agent approach or single agent",
    "start": "708000",
    "end": "710160"
  },
  {
    "text": "environment",
    "start": "710160",
    "end": "710959"
  },
  {
    "text": "to a multi-agent environment and",
    "start": "710959",
    "end": "714000"
  },
  {
    "text": "here ray rlab helps a lot and that's",
    "start": "714000",
    "end": "716079"
  },
  {
    "text": "actually the main reason why i switched",
    "start": "716079",
    "end": "717760"
  },
  {
    "text": "to radar and lib",
    "start": "717760",
    "end": "719040"
  },
  {
    "text": "uh in the first place because initially",
    "start": "719040",
    "end": "720720"
  },
  {
    "text": "i was using a different framework which",
    "start": "720720",
    "end": "722240"
  },
  {
    "text": "did not support multi-agent",
    "start": "722240",
    "end": "724639"
  },
  {
    "text": "but with rlab what i had to do is simply",
    "start": "724639",
    "end": "727279"
  },
  {
    "start": "725000",
    "end": "758000"
  },
  {
    "text": "import",
    "start": "727279",
    "end": "728560"
  },
  {
    "text": "from ray or ellip the multi-agent",
    "start": "728560",
    "end": "730160"
  },
  {
    "text": "environment",
    "start": "730160",
    "end": "731600"
  },
  {
    "text": "and then create a new class or inherit",
    "start": "731600",
    "end": "734079"
  },
  {
    "text": "first of all from my",
    "start": "734079",
    "end": "735519"
  },
  {
    "text": "single agent mobile environment and then",
    "start": "735519",
    "end": "737120"
  },
  {
    "text": "this multi-agent environment",
    "start": "737120",
    "end": "738639"
  },
  {
    "text": "and i only had to override here the",
    "start": "738639",
    "end": "740320"
  },
  {
    "text": "constructor to define this",
    "start": "740320",
    "end": "742480"
  },
  {
    "text": "adjusted observation in action space and",
    "start": "742480",
    "end": "744720"
  },
  {
    "text": "then also implement and overwrite the",
    "start": "744720",
    "end": "746880"
  },
  {
    "text": "get observation and get reward function",
    "start": "746880",
    "end": "748880"
  },
  {
    "text": "to also make the adjustments here but",
    "start": "748880",
    "end": "750560"
  },
  {
    "text": "for example i could simply use the",
    "start": "750560",
    "end": "752480"
  },
  {
    "text": "result the step and the render functions",
    "start": "752480",
    "end": "754800"
  },
  {
    "text": "which implement all the main logic of",
    "start": "754800",
    "end": "756480"
  },
  {
    "text": "this environment",
    "start": "756480",
    "end": "758800"
  },
  {
    "start": "758000",
    "end": "783000"
  },
  {
    "text": "what's also very handy is that rlab",
    "start": "758800",
    "end": "761120"
  },
  {
    "text": "supports",
    "start": "761120",
    "end": "761839"
  },
  {
    "text": "jim dick spaces so i can define my",
    "start": "761839",
    "end": "764639"
  },
  {
    "text": "observations here simply as i dict so",
    "start": "764639",
    "end": "767279"
  },
  {
    "text": "combining a multi-binary observation",
    "start": "767279",
    "end": "769279"
  },
  {
    "text": "space for the connected observation",
    "start": "769279",
    "end": "771519"
  },
  {
    "text": "and then a continuous box space and",
    "start": "771519",
    "end": "773360"
  },
  {
    "text": "another continuous box space for the",
    "start": "773360",
    "end": "774800"
  },
  {
    "text": "other observations",
    "start": "774800",
    "end": "775920"
  },
  {
    "text": "which is really useful and simple to do",
    "start": "775920",
    "end": "778639"
  },
  {
    "text": "but it's not supported by many other",
    "start": "778639",
    "end": "780320"
  },
  {
    "text": "frameworks so that's also a nice",
    "start": "780320",
    "end": "782160"
  },
  {
    "text": "extra here so now that the environment",
    "start": "782160",
    "end": "785360"
  },
  {
    "start": "783000",
    "end": "805000"
  },
  {
    "text": "is implemented",
    "start": "785360",
    "end": "786480"
  },
  {
    "text": "i of course also have to implement now",
    "start": "786480",
    "end": "788320"
  },
  {
    "text": "the rl agents",
    "start": "788320",
    "end": "790320"
  },
  {
    "text": "and here i chose the ppo implementation",
    "start": "790320",
    "end": "792720"
  },
  {
    "text": "which comes with radar lab",
    "start": "792720",
    "end": "794320"
  },
  {
    "text": "so that's again very nice that there are",
    "start": "794320",
    "end": "795920"
  },
  {
    "text": "so many high quality implementations of",
    "start": "795920",
    "end": "798160"
  },
  {
    "text": "different rl algorithms",
    "start": "798160",
    "end": "800240"
  },
  {
    "text": "and i chose the tensorflow 2 version but",
    "start": "800240",
    "end": "802399"
  },
  {
    "text": "there are also pi torch versions",
    "start": "802399",
    "end": "804800"
  },
  {
    "text": "and as i said for me the main reason to",
    "start": "804800",
    "end": "806959"
  },
  {
    "start": "805000",
    "end": "820000"
  },
  {
    "text": "switch to rlab was",
    "start": "806959",
    "end": "808320"
  },
  {
    "text": "this great support for moving from the",
    "start": "808320",
    "end": "811040"
  },
  {
    "text": "centralized approach to the multi-agent",
    "start": "811040",
    "end": "813120"
  },
  {
    "text": "approach and",
    "start": "813120",
    "end": "814480"
  },
  {
    "text": "that's not just true for the environment",
    "start": "814480",
    "end": "816480"
  },
  {
    "text": "but also for configuration and training",
    "start": "816480",
    "end": "818320"
  },
  {
    "text": "of these agents so",
    "start": "818320",
    "end": "819760"
  },
  {
    "text": "uh a bit oversimplified here when",
    "start": "819760",
    "end": "822560"
  },
  {
    "start": "820000",
    "end": "834000"
  },
  {
    "text": "configuring the",
    "start": "822560",
    "end": "823680"
  },
  {
    "text": "single h in the centralized approach i",
    "start": "823680",
    "end": "825839"
  },
  {
    "text": "simply set in the configuration the",
    "start": "825839",
    "end": "828079"
  },
  {
    "text": "multi-agent",
    "start": "828079",
    "end": "828959"
  },
  {
    "text": "option to be empty more or less so i",
    "start": "828959",
    "end": "831440"
  },
  {
    "text": "don't define any multi-agent policies",
    "start": "831440",
    "end": "833199"
  },
  {
    "text": "here",
    "start": "833199",
    "end": "833920"
  },
  {
    "text": "and then if i want to go to a",
    "start": "833920",
    "end": "835279"
  },
  {
    "start": "834000",
    "end": "863000"
  },
  {
    "text": "multi-agent approach and",
    "start": "835279",
    "end": "837199"
  },
  {
    "text": "for now let's look at the d3 comp",
    "start": "837199",
    "end": "838959"
  },
  {
    "text": "approach where i have separate policies",
    "start": "838959",
    "end": "840639"
  },
  {
    "text": "for each agent",
    "start": "840639",
    "end": "842720"
  },
  {
    "text": "i simply have to set the policy so here",
    "start": "842720",
    "end": "845199"
  },
  {
    "text": "i want to have separate policies for",
    "start": "845199",
    "end": "846800"
  },
  {
    "text": "each agent so iterate over all my",
    "start": "846800",
    "end": "848399"
  },
  {
    "text": "different agents",
    "start": "848399",
    "end": "849360"
  },
  {
    "text": "and for each agent i create a separate",
    "start": "849360",
    "end": "851120"
  },
  {
    "text": "policy based on the observation action",
    "start": "851120",
    "end": "852880"
  },
  {
    "text": "space here",
    "start": "852880",
    "end": "854160"
  },
  {
    "text": "now then i also have to define the",
    "start": "854160",
    "end": "855920"
  },
  {
    "text": "policy mapping function",
    "start": "855920",
    "end": "857440"
  },
  {
    "text": "and in this mapping function i simply",
    "start": "857440",
    "end": "859120"
  },
  {
    "text": "assign a separate policy for each agent",
    "start": "859120",
    "end": "861600"
  },
  {
    "text": "based on the id",
    "start": "861600",
    "end": "863600"
  },
  {
    "start": "863000",
    "end": "899000"
  },
  {
    "text": "it's also very simple to configure this",
    "start": "863600",
    "end": "866800"
  },
  {
    "text": "ddcomp approach where i have a shared",
    "start": "866800",
    "end": "868560"
  },
  {
    "text": "policy at least during training",
    "start": "868560",
    "end": "870480"
  },
  {
    "text": "and here instead of iterating over all",
    "start": "870480",
    "end": "873040"
  },
  {
    "text": "agents i simply define a single",
    "start": "873040",
    "end": "874800"
  },
  {
    "text": "policy that i called shared but i could",
    "start": "874800",
    "end": "876800"
  },
  {
    "text": "call it any other",
    "start": "876800",
    "end": "877920"
  },
  {
    "text": "i could use any other name here and then",
    "start": "877920",
    "end": "880160"
  },
  {
    "text": "in the policy mapping function",
    "start": "880160",
    "end": "882000"
  },
  {
    "text": "i map each agent to the share policy",
    "start": "882000",
    "end": "884560"
  },
  {
    "text": "such that all agents",
    "start": "884560",
    "end": "885920"
  },
  {
    "text": "use the same policy and you can imagine",
    "start": "885920",
    "end": "888160"
  },
  {
    "text": "that it's also",
    "start": "888160",
    "end": "889120"
  },
  {
    "text": "very easy to do something hybrid here",
    "start": "889120",
    "end": "891440"
  },
  {
    "text": "such that",
    "start": "891440",
    "end": "892240"
  },
  {
    "text": "some agents use a shared policy some",
    "start": "892240",
    "end": "894240"
  },
  {
    "text": "agents have their own policy",
    "start": "894240",
    "end": "895839"
  },
  {
    "text": "and that's really easy to configure here",
    "start": "895839",
    "end": "899279"
  },
  {
    "text": "and configuration training is easy",
    "start": "899279",
    "end": "902320"
  },
  {
    "text": "also inference is quite easy in this",
    "start": "902320",
    "end": "904240"
  },
  {
    "text": "single agenda case",
    "start": "904240",
    "end": "905920"
  },
  {
    "text": "this is the kind of typical approach",
    "start": "905920",
    "end": "907920"
  },
  {
    "text": "that you have where you get the",
    "start": "907920",
    "end": "909440"
  },
  {
    "text": "environment you get",
    "start": "909440",
    "end": "910560"
  },
  {
    "text": "reset it get the initial observation and",
    "start": "910560",
    "end": "912959"
  },
  {
    "text": "then",
    "start": "912959",
    "end": "913680"
  },
  {
    "text": "loop until the episode is done and in",
    "start": "913680",
    "end": "916000"
  },
  {
    "text": "each step you",
    "start": "916000",
    "end": "917199"
  },
  {
    "text": "pass the observation to your centralized",
    "start": "917199",
    "end": "919199"
  },
  {
    "text": "or to your single agent the agent",
    "start": "919199",
    "end": "920720"
  },
  {
    "text": "computes an action",
    "start": "920720",
    "end": "921920"
  },
  {
    "text": "that action is passed and applied to the",
    "start": "921920",
    "end": "923519"
  },
  {
    "text": "environment and then you get the next",
    "start": "923519",
    "end": "925199"
  },
  {
    "text": "observation reward until the episode is",
    "start": "925199",
    "end": "927120"
  },
  {
    "text": "done",
    "start": "927120",
    "end": "928160"
  },
  {
    "start": "928000",
    "end": "989000"
  },
  {
    "text": "now moving from this centralized single",
    "start": "928160",
    "end": "930880"
  },
  {
    "text": "agent inference to",
    "start": "930880",
    "end": "932079"
  },
  {
    "text": "multi-agent inference is also quite",
    "start": "932079",
    "end": "933920"
  },
  {
    "text": "simple so",
    "start": "933920",
    "end": "935199"
  },
  {
    "text": "uh not much more code here i again start",
    "start": "935199",
    "end": "938000"
  },
  {
    "text": "with the environment being resetted",
    "start": "938000",
    "end": "940079"
  },
  {
    "text": "and loop until the episode is done but",
    "start": "940079",
    "end": "942800"
  },
  {
    "text": "here the action and the observation",
    "start": "942800",
    "end": "945519"
  },
  {
    "text": "are not a single observation action but",
    "start": "945519",
    "end": "947440"
  },
  {
    "text": "they're addicts because i have no",
    "start": "947440",
    "end": "948720"
  },
  {
    "text": "actions and observations for each agent",
    "start": "948720",
    "end": "951120"
  },
  {
    "text": "and for that reason i iterate through",
    "start": "951120",
    "end": "952959"
  },
  {
    "text": "these observations",
    "start": "952959",
    "end": "954639"
  },
  {
    "text": "and for each agent i get the",
    "start": "954639",
    "end": "956560"
  },
  {
    "text": "corresponding policy using this policy",
    "start": "956560",
    "end": "958399"
  },
  {
    "text": "mapping function here",
    "start": "958399",
    "end": "960000"
  },
  {
    "text": "and that's nice because now i don't have",
    "start": "960000",
    "end": "962560"
  },
  {
    "text": "at this point to distinguish between the",
    "start": "962560",
    "end": "964240"
  },
  {
    "text": "dd comp and the d3 comp approach",
    "start": "964240",
    "end": "966320"
  },
  {
    "text": "because that's already defined here in",
    "start": "966320",
    "end": "967920"
  },
  {
    "text": "the policy mapping function",
    "start": "967920",
    "end": "969920"
  },
  {
    "text": "and so i get the policy whether it's not",
    "start": "969920",
    "end": "971839"
  },
  {
    "text": "a shared policy or a different policy",
    "start": "971839",
    "end": "973519"
  },
  {
    "text": "for each agent",
    "start": "973519",
    "end": "974959"
  },
  {
    "text": "and i then get the action from each",
    "start": "974959",
    "end": "977360"
  },
  {
    "text": "agent",
    "start": "977360",
    "end": "978399"
  },
  {
    "text": "based on the agent's observation and",
    "start": "978399",
    "end": "979920"
  },
  {
    "text": "based on the selected policy",
    "start": "979920",
    "end": "981680"
  },
  {
    "text": "and then once i have collected all the",
    "start": "981680",
    "end": "983040"
  },
  {
    "text": "actions i can pass them through the",
    "start": "983040",
    "end": "984480"
  },
  {
    "text": "environment",
    "start": "984480",
    "end": "985360"
  },
  {
    "text": "and then make the next step here",
    "start": "985360",
    "end": "988880"
  },
  {
    "text": "so as i said this support for",
    "start": "988880",
    "end": "991759"
  },
  {
    "start": "989000",
    "end": "1009000"
  },
  {
    "text": "multi-agent was for me the biggest",
    "start": "991759",
    "end": "993440"
  },
  {
    "text": "reason to switch to rla but there are a",
    "start": "993440",
    "end": "995199"
  },
  {
    "text": "lot of other nice features and benefits",
    "start": "995199",
    "end": "997040"
  },
  {
    "text": "so for example this full integration",
    "start": "997040",
    "end": "998720"
  },
  {
    "text": "with tensorboard",
    "start": "998720",
    "end": "999680"
  },
  {
    "text": "where there are lots of useful metrics",
    "start": "999680",
    "end": "1001600"
  },
  {
    "text": "supported by default that are great for",
    "start": "1001600",
    "end": "1003360"
  },
  {
    "text": "monitoring",
    "start": "1003360",
    "end": "1004160"
  },
  {
    "text": "training progress also for debugging or",
    "start": "1004160",
    "end": "1006320"
  },
  {
    "text": "for improving these approaches",
    "start": "1006320",
    "end": "1009120"
  },
  {
    "start": "1009000",
    "end": "1029000"
  },
  {
    "text": "it's quite simple to implement own",
    "start": "1009120",
    "end": "1011199"
  },
  {
    "text": "custom metrics so for example",
    "start": "1011199",
    "end": "1013199"
  },
  {
    "text": "you can add a metric for each step in",
    "start": "1013199",
    "end": "1014959"
  },
  {
    "text": "the episode or a metric",
    "start": "1014959",
    "end": "1016480"
  },
  {
    "text": "just on the end at the end of each",
    "start": "1016480",
    "end": "1018800"
  },
  {
    "text": "episode",
    "start": "1018800",
    "end": "1019600"
  },
  {
    "text": "and i use such a custom callback to keep",
    "start": "1019600",
    "end": "1022800"
  },
  {
    "text": "track of the total quality",
    "start": "1022800",
    "end": "1024000"
  },
  {
    "text": "of experience which is what i wanted to",
    "start": "1024000",
    "end": "1025839"
  },
  {
    "text": "optimize i added a custom metric here",
    "start": "1025839",
    "end": "1029760"
  },
  {
    "start": "1029000",
    "end": "1072000"
  },
  {
    "text": "one of the great things with ray is that",
    "start": "1029760",
    "end": "1031678"
  },
  {
    "text": "it's easy to go from local development",
    "start": "1031679",
    "end": "1033678"
  },
  {
    "text": "either",
    "start": "1033679",
    "end": "1034079"
  },
  {
    "text": "linux or windows to cluster deployment",
    "start": "1034079",
    "end": "1036880"
  },
  {
    "text": "in my case",
    "start": "1036880",
    "end": "1038240"
  },
  {
    "text": "i deployed this agent or trained them on",
    "start": "1038240",
    "end": "1041038"
  },
  {
    "text": "the university's local private cluster",
    "start": "1041039",
    "end": "1043520"
  },
  {
    "text": "which was really useful to speeding up",
    "start": "1043520",
    "end": "1045438"
  },
  {
    "text": "evaluation and scaling and",
    "start": "1045439",
    "end": "1047199"
  },
  {
    "text": "running lots of things in parallel and",
    "start": "1047199",
    "end": "1049919"
  },
  {
    "text": "lastly i have to say thanks for the",
    "start": "1049919",
    "end": "1052160"
  },
  {
    "text": "great support from the ray or from the",
    "start": "1052160",
    "end": "1054000"
  },
  {
    "text": "any scale team",
    "start": "1054000",
    "end": "1055120"
  },
  {
    "text": "and also from the very active community",
    "start": "1055120",
    "end": "1057120"
  },
  {
    "text": "so whenever i",
    "start": "1057120",
    "end": "1058160"
  },
  {
    "text": "had a question or hit a wall i could ask",
    "start": "1058160",
    "end": "1060640"
  },
  {
    "text": "on slack on github or on the disqus",
    "start": "1060640",
    "end": "1062960"
  },
  {
    "text": "forum",
    "start": "1062960",
    "end": "1063520"
  },
  {
    "text": "i would always get useful tips and",
    "start": "1063520",
    "end": "1065600"
  },
  {
    "text": "advice on how to",
    "start": "1065600",
    "end": "1067200"
  },
  {
    "text": "continue and that was really really",
    "start": "1067200",
    "end": "1069520"
  },
  {
    "text": "useful so thank you",
    "start": "1069520",
    "end": "1072559"
  },
  {
    "start": "1072000",
    "end": "1093000"
  },
  {
    "text": "okay so the implementation is done we",
    "start": "1072559",
    "end": "1074880"
  },
  {
    "text": "have our environment we have our agents",
    "start": "1074880",
    "end": "1077440"
  },
  {
    "text": "now of course the question is does any",
    "start": "1077440",
    "end": "1080080"
  },
  {
    "text": "of this really work",
    "start": "1080080",
    "end": "1081280"
  },
  {
    "text": "and for that let's start with an",
    "start": "1081280",
    "end": "1084559"
  },
  {
    "text": "untrained agent so an agent that's still",
    "start": "1084559",
    "end": "1087280"
  },
  {
    "text": "initially randomly",
    "start": "1087280",
    "end": "1089039"
  },
  {
    "text": "randomly initialized so it starts with",
    "start": "1089039",
    "end": "1090799"
  },
  {
    "text": "random actions",
    "start": "1090799",
    "end": "1092799"
  },
  {
    "text": "and what you'll see here is that this of",
    "start": "1092799",
    "end": "1095760"
  },
  {
    "start": "1093000",
    "end": "1110000"
  },
  {
    "text": "course does not work",
    "start": "1095760",
    "end": "1097280"
  },
  {
    "text": "well at all so here we have the",
    "start": "1097280",
    "end": "1099360"
  },
  {
    "text": "centralized deep comp approach that's",
    "start": "1099360",
    "end": "1101039"
  },
  {
    "text": "trained for 200 steps so",
    "start": "1101039",
    "end": "1102799"
  },
  {
    "text": "this counts as not trained at all",
    "start": "1102799",
    "end": "1104559"
  },
  {
    "text": "because it's so little training",
    "start": "1104559",
    "end": "1106559"
  },
  {
    "text": "and um we have",
    "start": "1106559",
    "end": "1110080"
  },
  {
    "start": "1110000",
    "end": "1158000"
  },
  {
    "text": "we have seven different cells here",
    "start": "1110080",
    "end": "1112160"
  },
  {
    "text": "indicated by these antennas",
    "start": "1112160",
    "end": "1113840"
  },
  {
    "text": "and each uh of these larger circles",
    "start": "1113840",
    "end": "1116400"
  },
  {
    "text": "indicates the rough range here and then",
    "start": "1116400",
    "end": "1117840"
  },
  {
    "text": "we have",
    "start": "1117840",
    "end": "1118640"
  },
  {
    "text": "uh the small points or circles",
    "start": "1118640",
    "end": "1122480"
  },
  {
    "text": "with the different colors that move",
    "start": "1122480",
    "end": "1123840"
  },
  {
    "text": "around and these represent the different",
    "start": "1123840",
    "end": "1125840"
  },
  {
    "text": "users and the color represents the",
    "start": "1125840",
    "end": "1127440"
  },
  {
    "text": "current quality of experience",
    "start": "1127440",
    "end": "1129120"
  },
  {
    "text": "of these users where red is really bad",
    "start": "1129120",
    "end": "1131679"
  },
  {
    "text": "so most of our users are very very",
    "start": "1131679",
    "end": "1133440"
  },
  {
    "text": "unhappy",
    "start": "1133440",
    "end": "1134080"
  },
  {
    "text": "because they're not connected to any",
    "start": "1134080",
    "end": "1135440"
  },
  {
    "text": "cell and just some of these users are by",
    "start": "1135440",
    "end": "1137760"
  },
  {
    "text": "chance",
    "start": "1137760",
    "end": "1139039"
  },
  {
    "text": "connected to some cell and range for",
    "start": "1139039",
    "end": "1141120"
  },
  {
    "text": "some reason always or often directly",
    "start": "1141120",
    "end": "1143039"
  },
  {
    "text": "disconnected again",
    "start": "1143039",
    "end": "1144240"
  },
  {
    "text": "and so overall with these random actions",
    "start": "1144240",
    "end": "1146240"
  },
  {
    "text": "of course you get a horrible cell",
    "start": "1146240",
    "end": "1147679"
  },
  {
    "text": "selection policy",
    "start": "1147679",
    "end": "1148880"
  },
  {
    "text": "the average qe for all of our users is",
    "start": "1148880",
    "end": "1150960"
  },
  {
    "text": "really really low and unstable",
    "start": "1150960",
    "end": "1152880"
  },
  {
    "text": "and that's a bad starting point but",
    "start": "1152880",
    "end": "1156000"
  },
  {
    "text": "if we just train for a while",
    "start": "1156000",
    "end": "1159120"
  },
  {
    "start": "1158000",
    "end": "1193000"
  },
  {
    "text": "sorry here for two million time steps",
    "start": "1159120",
    "end": "1161440"
  },
  {
    "text": "then you can see that the agent really",
    "start": "1161440",
    "end": "1162880"
  },
  {
    "text": "learns a",
    "start": "1162880",
    "end": "1163600"
  },
  {
    "text": "very useful cell selection policy so now",
    "start": "1163600",
    "end": "1166960"
  },
  {
    "text": "all of our users are connected",
    "start": "1166960",
    "end": "1170160"
  },
  {
    "text": "we have very smooth handovers between",
    "start": "1170160",
    "end": "1171919"
  },
  {
    "text": "the different cells the agent learns to",
    "start": "1171919",
    "end": "1174000"
  },
  {
    "text": "use compure",
    "start": "1174000",
    "end": "1175200"
  },
  {
    "text": "at this lh where it's really useful but",
    "start": "1175200",
    "end": "1177039"
  },
  {
    "text": "it doesn't use it excessively to avoid",
    "start": "1177039",
    "end": "1179039"
  },
  {
    "text": "unnecessary competition",
    "start": "1179039",
    "end": "1180720"
  },
  {
    "text": "and all of this was learned without any",
    "start": "1180720",
    "end": "1183200"
  },
  {
    "text": "human instruction or so",
    "start": "1183200",
    "end": "1185840"
  },
  {
    "text": "and the average qe here is in the end a",
    "start": "1185840",
    "end": "1188799"
  },
  {
    "text": "lot higher than in the beginning",
    "start": "1188799",
    "end": "1190720"
  },
  {
    "text": "quite good and very stable and so",
    "start": "1190720",
    "end": "1192559"
  },
  {
    "text": "overall our dural approaches learn",
    "start": "1192559",
    "end": "1194559"
  },
  {
    "start": "1193000",
    "end": "1203000"
  },
  {
    "text": "multi-cell selection very effectively",
    "start": "1194559",
    "end": "1196720"
  },
  {
    "text": "and they don't need any human",
    "start": "1196720",
    "end": "1197840"
  },
  {
    "text": "intervention or instruction or expert",
    "start": "1197840",
    "end": "1199520"
  },
  {
    "text": "knowledge",
    "start": "1199520",
    "end": "1200080"
  },
  {
    "text": "i think that's a big advantage over",
    "start": "1200080",
    "end": "1201520"
  },
  {
    "text": "existing approaches",
    "start": "1201520",
    "end": "1203520"
  },
  {
    "start": "1203000",
    "end": "1233000"
  },
  {
    "text": "of course we also did more systematic",
    "start": "1203520",
    "end": "1206159"
  },
  {
    "text": "experiments in evaluation so we",
    "start": "1206159",
    "end": "1208480"
  },
  {
    "text": "varied the resource allocation schemes",
    "start": "1208480",
    "end": "1210400"
  },
  {
    "text": "the number of users the number of cells",
    "start": "1210400",
    "end": "1212080"
  },
  {
    "text": "the distance between these cells",
    "start": "1212080",
    "end": "1213679"
  },
  {
    "text": "and in each of these scenarios we simply",
    "start": "1213679",
    "end": "1215760"
  },
  {
    "text": "retrained our agents but we did not give",
    "start": "1215760",
    "end": "1217600"
  },
  {
    "text": "them any extra information no",
    "start": "1217600",
    "end": "1219600"
  },
  {
    "text": "extra instructions no expert knowledge",
    "start": "1219600",
    "end": "1222400"
  },
  {
    "text": "and",
    "start": "1222400",
    "end": "1223039"
  },
  {
    "text": "they just trained and learned to adapt",
    "start": "1223039",
    "end": "1224799"
  },
  {
    "text": "to these different scenarios",
    "start": "1224799",
    "end": "1226159"
  },
  {
    "text": "and then each of these scenarios our",
    "start": "1226159",
    "end": "1228000"
  },
  {
    "text": "three drl approaches outperformed other",
    "start": "1228000",
    "end": "1229919"
  },
  {
    "text": "existing approaches here",
    "start": "1229919",
    "end": "1232720"
  },
  {
    "text": "and um yeah the two distributed the two",
    "start": "1232720",
    "end": "1236000"
  },
  {
    "text": "multi-agent approaches are quite similar",
    "start": "1236000",
    "end": "1237760"
  },
  {
    "text": "in terms of",
    "start": "1237760",
    "end": "1238640"
  },
  {
    "text": "performance uh but the centralized",
    "start": "1238640",
    "end": "1240720"
  },
  {
    "text": "deepcom approach is quite a bit better",
    "start": "1240720",
    "end": "1242159"
  },
  {
    "text": "than the two distributed approaches",
    "start": "1242159",
    "end": "1243440"
  },
  {
    "text": "simply because it does have this global",
    "start": "1243440",
    "end": "1245120"
  },
  {
    "text": "view and control of all users",
    "start": "1245120",
    "end": "1246880"
  },
  {
    "text": "and it's very close to optimal here",
    "start": "1246880",
    "end": "1250640"
  },
  {
    "text": "and so overall the drl agents learn to",
    "start": "1250640",
    "end": "1253520"
  },
  {
    "text": "self-adapt to",
    "start": "1253520",
    "end": "1254480"
  },
  {
    "text": "each given scenario and they outperform",
    "start": "1254480",
    "end": "1256480"
  },
  {
    "text": "existing approaches",
    "start": "1256480",
    "end": "1258240"
  },
  {
    "text": "now you might be wondering why do we",
    "start": "1258240",
    "end": "1260799"
  },
  {
    "text": "even bother with these",
    "start": "1260799",
    "end": "1262400"
  },
  {
    "text": "distributed approaches if they're always",
    "start": "1262400",
    "end": "1264000"
  },
  {
    "text": "worse than the centralized approach",
    "start": "1264000",
    "end": "1266000"
  },
  {
    "start": "1266000",
    "end": "1294000"
  },
  {
    "text": "and the reason is scalability so if we",
    "start": "1266000",
    "end": "1268880"
  },
  {
    "text": "look at scenarios with more and more",
    "start": "1268880",
    "end": "1270320"
  },
  {
    "text": "users",
    "start": "1270320",
    "end": "1271520"
  },
  {
    "text": "the action and observation space of the",
    "start": "1271520",
    "end": "1273280"
  },
  {
    "text": "centralized deep comp approach grows and",
    "start": "1273280",
    "end": "1274960"
  },
  {
    "text": "grows and the complexity",
    "start": "1274960",
    "end": "1276000"
  },
  {
    "text": "grows and the training and the",
    "start": "1276000",
    "end": "1277440"
  },
  {
    "text": "conversion times also grows",
    "start": "1277440",
    "end": "1279440"
  },
  {
    "text": "whereas for the distributed approaches",
    "start": "1279440",
    "end": "1281360"
  },
  {
    "text": "for example here dd comp",
    "start": "1281360",
    "end": "1282880"
  },
  {
    "text": "the observation in action space is",
    "start": "1282880",
    "end": "1284480"
  },
  {
    "text": "completely invariant from the number of",
    "start": "1284480",
    "end": "1286000"
  },
  {
    "text": "user",
    "start": "1286000",
    "end": "1286720"
  },
  {
    "text": "number of users and so in this example",
    "start": "1286720",
    "end": "1289120"
  },
  {
    "text": "here ddcom converges within here",
    "start": "1289120",
    "end": "1291280"
  },
  {
    "text": "200 000 training steps more or less",
    "start": "1291280",
    "end": "1293919"
  },
  {
    "text": "whereas the centralized deep comp",
    "start": "1293919",
    "end": "1295440"
  },
  {
    "text": "approach",
    "start": "1295440",
    "end": "1296080"
  },
  {
    "text": "needs much longer it needs more than one",
    "start": "1296080",
    "end": "1298400"
  },
  {
    "text": "million training steps to reach",
    "start": "1298400",
    "end": "1300080"
  },
  {
    "text": "a similar performance so more than five",
    "start": "1300080",
    "end": "1302000"
  },
  {
    "text": "times as long and that's a huge",
    "start": "1302000",
    "end": "1304159"
  },
  {
    "text": "benefit and advantage of these",
    "start": "1304159",
    "end": "1305600"
  },
  {
    "text": "distributed approaches",
    "start": "1305600",
    "end": "1307360"
  },
  {
    "text": "but if we keep on training and train for",
    "start": "1307360",
    "end": "1310000"
  },
  {
    "text": "long enough",
    "start": "1310000",
    "end": "1310640"
  },
  {
    "text": "ultimately the centralized approach",
    "start": "1310640",
    "end": "1312880"
  },
  {
    "text": "learns a slightly better policy than the",
    "start": "1312880",
    "end": "1314559"
  },
  {
    "text": "distributed approaches",
    "start": "1314559",
    "end": "1315760"
  },
  {
    "text": "simply because it does have this global",
    "start": "1315760",
    "end": "1317600"
  },
  {
    "text": "and centralized view and control of all",
    "start": "1317600",
    "end": "1319919"
  },
  {
    "text": "users",
    "start": "1319919",
    "end": "1321120"
  },
  {
    "text": "and so overall our distributed drl",
    "start": "1321120",
    "end": "1323840"
  },
  {
    "text": "approaches learn good",
    "start": "1323840",
    "end": "1325280"
  },
  {
    "text": "cell selection policies really quickly",
    "start": "1325280",
    "end": "1327200"
  },
  {
    "text": "whereas the centralized approach needs",
    "start": "1327200",
    "end": "1328720"
  },
  {
    "text": "some longer time to train",
    "start": "1328720",
    "end": "1329919"
  },
  {
    "text": "but then ultimately learns a policy",
    "start": "1329919",
    "end": "1331840"
  },
  {
    "text": "that's slightly better",
    "start": "1331840",
    "end": "1333440"
  },
  {
    "text": "and both of these approaches are",
    "start": "1333440",
    "end": "1335200"
  },
  {
    "text": "actually all three of them outperform",
    "start": "1335200",
    "end": "1336720"
  },
  {
    "text": "existing approaches",
    "start": "1336720",
    "end": "1339280"
  },
  {
    "start": "1339000",
    "end": "1426000"
  },
  {
    "text": "yeah and that already brings me to my",
    "start": "1339280",
    "end": "1340880"
  },
  {
    "text": "conclusion so i presented our three",
    "start": "1340880",
    "end": "1342720"
  },
  {
    "text": "different self-learning drl approaches",
    "start": "1342720",
    "end": "1344640"
  },
  {
    "text": "first of all",
    "start": "1344640",
    "end": "1345600"
  },
  {
    "text": "the centralized steep com approach",
    "start": "1345600",
    "end": "1347120"
  },
  {
    "text": "that's a bit slow to train",
    "start": "1347120",
    "end": "1348880"
  },
  {
    "text": "but ultimately learns highly optimized",
    "start": "1348880",
    "end": "1351039"
  },
  {
    "text": "cell selection policies",
    "start": "1351039",
    "end": "1352559"
  },
  {
    "text": "so this is really useful for quite",
    "start": "1352559",
    "end": "1354880"
  },
  {
    "text": "stable environments with not too many",
    "start": "1354880",
    "end": "1356640"
  },
  {
    "text": "users",
    "start": "1356640",
    "end": "1357520"
  },
  {
    "text": "and then the distributed dd comp and the",
    "start": "1357520",
    "end": "1359919"
  },
  {
    "text": "three comp approach",
    "start": "1359919",
    "end": "1361360"
  },
  {
    "text": "are trained a lot faster and they make",
    "start": "1361360",
    "end": "1363120"
  },
  {
    "text": "fast local multi-cell selection",
    "start": "1363120",
    "end": "1364880"
  },
  {
    "text": "decisions",
    "start": "1364880",
    "end": "1365679"
  },
  {
    "text": "and those approaches are really useful",
    "start": "1365679",
    "end": "1367200"
  },
  {
    "text": "for more practical scenarios",
    "start": "1367200",
    "end": "1368960"
  },
  {
    "text": "with a large scale many users or",
    "start": "1368960",
    "end": "1372320"
  },
  {
    "text": "faster fluctuations we developed and",
    "start": "1372320",
    "end": "1375360"
  },
  {
    "text": "deployed all of these approaches all",
    "start": "1375360",
    "end": "1377120"
  },
  {
    "text": "three of them",
    "start": "1377120",
    "end": "1377919"
  },
  {
    "text": "with radar lab and here as i said",
    "start": "1377919",
    "end": "1381039"
  },
  {
    "text": "really great was the support for",
    "start": "1381039",
    "end": "1382559"
  },
  {
    "text": "multi-agent uh but also all the other",
    "start": "1382559",
    "end": "1384960"
  },
  {
    "text": "features were really helpful here",
    "start": "1384960",
    "end": "1386880"
  },
  {
    "text": "and yeah ultimately we found that these",
    "start": "1386880",
    "end": "1390320"
  },
  {
    "text": "three approaches outperform existing",
    "start": "1390320",
    "end": "1392559"
  },
  {
    "text": "work existing approaches they work with",
    "start": "1392559",
    "end": "1394799"
  },
  {
    "text": "minimal realistically available",
    "start": "1394799",
    "end": "1396320"
  },
  {
    "text": "information",
    "start": "1396320",
    "end": "1397120"
  },
  {
    "text": "they self-adapt to varying different",
    "start": "1397120",
    "end": "1398720"
  },
  {
    "text": "scenarios their robust to sudden change",
    "start": "1398720",
    "end": "1400960"
  },
  {
    "text": "and they scale to large networks and so",
    "start": "1400960",
    "end": "1403280"
  },
  {
    "text": "i believe that",
    "start": "1403280",
    "end": "1404400"
  },
  {
    "text": "this approach is that this work is an",
    "start": "1404400",
    "end": "1406400"
  },
  {
    "text": "important step towards",
    "start": "1406400",
    "end": "1407840"
  },
  {
    "text": "self-adaptive and effective coordinated",
    "start": "1407840",
    "end": "1409919"
  },
  {
    "text": "multi-point in practice",
    "start": "1409919",
    "end": "1411440"
  },
  {
    "text": "and towards higher quality of experience",
    "start": "1411440",
    "end": "1413360"
  },
  {
    "text": "in 5g and beyond",
    "start": "1413360",
    "end": "1415679"
  },
  {
    "text": "so that's it thank you for your",
    "start": "1415679",
    "end": "1417200"
  },
  {
    "text": "attention if you're interested feel free",
    "start": "1417200",
    "end": "1419200"
  },
  {
    "text": "to check out the code that's publicly",
    "start": "1419200",
    "end": "1420799"
  },
  {
    "text": "available on github",
    "start": "1420799",
    "end": "1422400"
  },
  {
    "text": "thank you",
    "start": "1422400",
    "end": "1428240"
  }
]