[
  {
    "text": "thank you for joining us today very excited to talk about how to build a chat assistant or co-pilot fast using",
    "start": "5000",
    "end": "12480"
  },
  {
    "text": "canopy from Pine Cone and any scale endpoint my name is f Nan um I'm a",
    "start": "12480",
    "end": "18560"
  },
  {
    "text": "technical product marketing manager at N scale I'm very pleased to be joined uh by Roy Roy is part of the engineering",
    "start": "18560",
    "end": "26000"
  },
  {
    "text": "team at Pine con uh and k k solution architect at any",
    "start": "26000",
    "end": "33679"
  },
  {
    "text": "scale okay so for today uh this is the agenda we're going to be talking about",
    "start": "35559",
    "end": "41800"
  },
  {
    "text": "some of the challenges of building a a chat assistant or co-pilot uh Roy will",
    "start": "41800",
    "end": "47079"
  },
  {
    "text": "cover canopy a new framework um from Pine con as well as the application",
    "start": "47079",
    "end": "53079"
  },
  {
    "text": "architecture I touch on the latest feature of any scale endpoint and Cal is",
    "start": "53079",
    "end": "58600"
  },
  {
    "text": "going to uh you know perform a Live code uh walkth through or demo we'll make",
    "start": "58600",
    "end": "64360"
  },
  {
    "text": "sure to leave plenty of time for question and answers so if you have any question please use the Q&A function uh",
    "start": "64360",
    "end": "72040"
  },
  {
    "text": "in Zoom um and then at the end of the call or you know doing um the the presentation I'll make sure to surface",
    "start": "72040",
    "end": "78880"
  },
  {
    "text": "those questions so that we can answer them um on on the",
    "start": "78880",
    "end": "84880"
  },
  {
    "text": "webinar okay so what are the challenges of building a chat assistant right so as",
    "start": "86720",
    "end": "92640"
  },
  {
    "text": "you may know U grounding your p and your llm with relevant information is very",
    "start": "92640",
    "end": "98479"
  },
  {
    "text": "important in order to mitigate some hallucinations uh and uh you know optimize the accuracy of the responses",
    "start": "98479",
    "end": "105719"
  },
  {
    "text": "and prime your llm to be the most relevant and useful right so when you think about um you know providing the",
    "start": "105719",
    "end": "113759"
  },
  {
    "text": "context in the background there's many different consideration to be uh made um",
    "start": "113759",
    "end": "119000"
  },
  {
    "text": "if you think about a book I have many different chapters I have many different paragraph or many different sentences or",
    "start": "119000",
    "end": "125360"
  },
  {
    "text": "maybe different keywords So based on your use cases what is the granularity of the contact that you want to provide",
    "start": "125360",
    "end": "131840"
  },
  {
    "text": "and pass it onto the llm into your palm is quite important right so this is also",
    "start": "131840",
    "end": "137200"
  },
  {
    "text": "known as chunking so defining the size of your chunking and then using the right embedding model to represent that",
    "start": "137200",
    "end": "144879"
  },
  {
    "text": "into a vector space and retrieve it efficiently uh is all about you know context retrieval",
    "start": "144879",
    "end": "151239"
  },
  {
    "text": "when you provide a chat assistant or co-pilot to your user um it's very often going to be a multi-turn type of",
    "start": "151239",
    "end": "158000"
  },
  {
    "text": "experiences and so managing the chat history and being uh making sure that you pass the history onto your llm",
    "start": "158000",
    "end": "165720"
  },
  {
    "text": "appropriately is also U you know something that you need to account for when we talk about career",
    "start": "165720",
    "end": "172440"
  },
  {
    "text": "optimization there's many different algorithm the most common one is cosign similarities there's also other type of",
    "start": "172440",
    "end": "178480"
  },
  {
    "text": "algorithm to fetch the right uh or the most important vectors in your uh",
    "start": "178480",
    "end": "183760"
  },
  {
    "text": "database um and also when you get the result you may have multiple um query",
    "start": "183760",
    "end": "189959"
  },
  {
    "text": "result and so being able to filter R rank uh the result set so that you can",
    "start": "189959",
    "end": "195200"
  },
  {
    "text": "pass the most pertinent information onto your llm is quite important um so you",
    "start": "195200",
    "end": "201159"
  },
  {
    "text": "know today obviously we're covering how to fetch information from from Pine con and Vector database but there's",
    "start": "201159",
    "end": "206560"
  },
  {
    "text": "different ways on how you can augment the generation you can make use of function call Api in Json format so you",
    "start": "206560",
    "end": "214720"
  },
  {
    "text": "can interact with other system than databases um and you know allow you to query from multiple sources um you know",
    "start": "214720",
    "end": "222159"
  },
  {
    "text": "um synthesize the information um and ground your uh llm more",
    "start": "222159",
    "end": "228879"
  },
  {
    "text": "appropriately when we talk about serving llm efficiently um there's no one model",
    "start": "228879",
    "end": "235599"
  },
  {
    "text": "that is going to fit or serve all your use cases you obviously need to iterate an experiment across different model of",
    "start": "235599",
    "end": "242920"
  },
  {
    "text": "different sizes you may have to find tun your model based on you know type of use cases that you're trying to to",
    "start": "242920",
    "end": "249879"
  },
  {
    "text": "address agency is quite important if you want to you know provide a good user experience so the time it takes to",
    "start": "249879",
    "end": "256079"
  },
  {
    "text": "generate the first token and then you can stream the rest of the token is a consideration it can be quite important",
    "start": "256079",
    "end": "262919"
  },
  {
    "text": "uh but you know in some other use cases toput is more important and the end to end of the experience right so based on",
    "start": "262919",
    "end": "269759"
  },
  {
    "text": "your use cases you may want to optimize on latency versus cut and then of course cost is quite",
    "start": "269759",
    "end": "278039"
  },
  {
    "text": "important when you take all those uh you know different configurations in account and different resources that it is",
    "start": "278039",
    "end": "284639"
  },
  {
    "text": "needed to provide this experience you need to make sure that um you know uh it is cost efficient uh it can be",
    "start": "284639",
    "end": "290720"
  },
  {
    "text": "sustainable based on the value that you provide um on those PC prototype of production",
    "start": "290720",
    "end": "298360"
  },
  {
    "text": "work okay with that I'm going to hand it over to Bo we talk about",
    "start": "298360",
    "end": "304840"
  },
  {
    "text": "canopy thank you um so first can you hear me well yes perfect great um so",
    "start": "304840",
    "end": "313560"
  },
  {
    "text": "what is canopy um so we launched canopy couple of months ago",
    "start": "313560",
    "end": "320160"
  },
  {
    "text": "uh at Pine con and what we had in mind is that we are wanting we want to build",
    "start": "320160",
    "end": "327560"
  },
  {
    "text": "a rag framework and a context engine it will sit on top of pine con so you",
    "start": "327560",
    "end": "334360"
  },
  {
    "text": "probably know Pine con Vector database uh that can scale to billions of of",
    "start": "334360",
    "end": "340160"
  },
  {
    "text": "vectors uh and we wanted to have like a rag interace uh or a rag framework that will",
    "start": "340160",
    "end": "345720"
  },
  {
    "text": "enable people to build reliably uh AI application um so the rag framework is",
    "start": "345720",
    "end": "352639"
  },
  {
    "text": "basically you're going to see it in the demo later um this chat or natural",
    "start": "352639",
    "end": "358400"
  },
  {
    "text": "language interface um that combines an external knowledge",
    "start": "358400",
    "end": "364199"
  },
  {
    "text": "base um we also have and I don't know how much of it we'll cover today uh a",
    "start": "364199",
    "end": "370960"
  },
  {
    "text": "context engine package inside where basically um you are able to specify a",
    "start": "370960",
    "end": "378880"
  },
  {
    "text": "query but also a token budget to get to the relevant context so when you're building an application you're actually",
    "start": "378880",
    "end": "385639"
  },
  {
    "text": "able to control for the amount of tokens that are sent in the context and for",
    "start": "385639",
    "end": "390759"
  },
  {
    "text": "that control for the cost of your application also allowing you to work in multiple",
    "start": "390759",
    "end": "396360"
  },
  {
    "text": "tiers um everything here obviously works on top of pine con although we see a",
    "start": "396360",
    "end": "402199"
  },
  {
    "text": "future where uh we will support maybe other data bases or other uh knowledge",
    "start": "402199",
    "end": "409440"
  },
  {
    "text": "bases um yeah next slide please so what is it",
    "start": "409440",
    "end": "416039"
  },
  {
    "text": "actually um canopy offers Developers three layers uh within one package a library a",
    "start": "416039",
    "end": "424120"
  },
  {
    "text": "server and a CLI um so the lowest uh kind of layer is",
    "start": "424120",
    "end": "430879"
  },
  {
    "text": "the library it's a stack of uh the logic and the classes that could be configured",
    "start": "430879",
    "end": "438560"
  },
  {
    "text": "could be extended uh that takes care of the entire uh uh pipeline to do rag both",
    "start": "438560",
    "end": "445440"
  },
  {
    "text": "in and out so both the ETL of ingesting data in processing data chunking",
    "start": "445440",
    "end": "451360"
  },
  {
    "text": "encoding the data or embedding the data uh the retrieval part which uh is using",
    "start": "451360",
    "end": "457639"
  },
  {
    "text": "pine con for Vector search but also uh can do ranking and can do other uh",
    "start": "457639",
    "end": "462759"
  },
  {
    "text": "Second Stage uh operations uh but then before before the",
    "start": "462759",
    "end": "468960"
  },
  {
    "text": "context will go to the uh language model there's also everything around context",
    "start": "468960",
    "end": "475199"
  },
  {
    "text": "construction and and budgeting right as we specified before um so then the librar is",
    "start": "475199",
    "end": "483199"
  },
  {
    "text": "something that we actively develop and our friends here at any scale also contributed a lot of new",
    "start": "483199",
    "end": "489560"
  },
  {
    "text": "capabilities uh to canopy um when we actually want to use it or actually want to serve it uh kopy",
    "start": "489560",
    "end": "497000"
  },
  {
    "text": "offers a server so you know basically wsgi server uh that could be configured",
    "start": "497000",
    "end": "504080"
  },
  {
    "text": "externally so if you look at the right hand uh side of the screen you could see that I have a class an N SK llm class it",
    "start": "504080",
    "end": "512120"
  },
  {
    "text": "extends open an AI llm just because it has a similar",
    "start": "512120",
    "end": "517159"
  },
  {
    "text": "interface uh but does completely different things uh but then when I uh",
    "start": "517159",
    "end": "522880"
  },
  {
    "text": "want to configure my server uh so the lower part will show you that for my chat engine I would like to use an llm",
    "start": "522880",
    "end": "529959"
  },
  {
    "text": "of type any scale and the model that I would I want to use is Lama 7 billion",
    "start": "529959",
    "end": "537200"
  },
  {
    "text": "but could be any uh any other model that is uh uh available on uh any scale",
    "start": "537200",
    "end": "543240"
  },
  {
    "text": "endpoints finally uh kopy has a CLI I'm not sure if we have time to show",
    "start": "543240",
    "end": "549360"
  },
  {
    "text": "everything today uh but basically a CLI that enables to do utility operations right so managing indexes bugging mostly",
    "start": "549360",
    "end": "557760"
  },
  {
    "text": "or local chatting so when you develop you have a way to iterate",
    "start": "557760",
    "end": "564360"
  },
  {
    "text": "quickly yeah next slide",
    "start": "564760",
    "end": "570399"
  },
  {
    "text": "great yeah thanks by so yeah quickly anyc end point the latest features so um",
    "start": "570399",
    "end": "578560"
  },
  {
    "text": "uh Inc point is a serverless API uh you know we just announced the support of",
    "start": "578560",
    "end": "584200"
  },
  {
    "text": "Lama God uh which is the you know based on lamb 2 model uh and Lama God has been",
    "start": "584200",
    "end": "590040"
  },
  {
    "text": "um provided by meta that has been fine-tuned for Content moderation and and GW um as well as other models such",
    "start": "590040",
    "end": "597800"
  },
  {
    "text": "as mistol open or um and you know we're working on also having the mol um mixture expert um as",
    "start": "597800",
    "end": "605640"
  },
  {
    "text": "well very soon so any scale end point is actually free to try up until the first million token and then after that you",
    "start": "605640",
    "end": "612720"
  },
  {
    "text": "can provide your credit card and then pay as you go after that typically the pricing depends on the model size um I",
    "start": "612720",
    "end": "619680"
  },
  {
    "text": "think for the largest model the Llama 270b it's a dollar per million token um",
    "start": "619680",
    "end": "626880"
  },
  {
    "text": "and um you know it varies for the model size we also just recently announced uh",
    "start": "626880",
    "end": "632480"
  },
  {
    "text": "the support of embeddings API which we'll see in this demo as well as",
    "start": "632480",
    "end": "637720"
  },
  {
    "text": "function called API and Json format right so Inc endpoint is really the",
    "start": "637720",
    "end": "643240"
  },
  {
    "text": "lowest friction to allow you to experiment and prototype using open source model open weight model and um",
    "start": "643240",
    "end": "650040"
  },
  {
    "text": "you know including the eming API is an open uh weight model right um and allow",
    "start": "650040",
    "end": "655639"
  },
  {
    "text": "you to easily upgrade onto a private endpoint point right so if you have more",
    "start": "655639",
    "end": "660880"
  },
  {
    "text": "stringent uh data privacy or security you can bring your own cloud um ad or",
    "start": "660880",
    "end": "667519"
  },
  {
    "text": "gcp and will be deploying the endpoint into your account behind your VPC and behind your firewall and provide you the",
    "start": "667519",
    "end": "674320"
  },
  {
    "text": "same experience as you know any scale public",
    "start": "674320",
    "end": "679240"
  },
  {
    "text": "endpoint thanks so um I want to go kind of over the the architecture of a common",
    "start": "683040",
    "end": "690320"
  },
  {
    "text": "a application kind of show you how everything works together right how canopy and any scale endpoints with the",
    "start": "690320",
    "end": "696760"
  },
  {
    "text": "pine con database kind of make it really easy to uh to build an AI application so",
    "start": "696760",
    "end": "702680"
  },
  {
    "text": "let's uh let's say that I want to build like a chat a simple chat uh interface",
    "start": "702680",
    "end": "708200"
  },
  {
    "text": "over some data that I have um so actually the first thing I would",
    "start": "708200",
    "end": "713399"
  },
  {
    "text": "probably want to do is take my documents and upload those um so when I do that",
    "start": "713399",
    "end": "718800"
  },
  {
    "text": "what what basically happens is that knowledge base which is one of the internal classes within uh kopy uh so",
    "start": "718800",
    "end": "726040"
  },
  {
    "text": "the knowledge base will process the data chunk it according to the strategy that you uh",
    "start": "726040",
    "end": "731440"
  },
  {
    "text": "predefine uh we'll go to the any scale end points for embedding uh and encode",
    "start": "731440",
    "end": "737199"
  },
  {
    "text": "the data to a vector uh and then store this Vector on Pineo right uh it will do",
    "start": "737199",
    "end": "743880"
  },
  {
    "text": "it iteratively for large amount of documents uh and different formats",
    "start": "743880",
    "end": "749519"
  },
  {
    "text": "everything uh is going to managed within this knowledge mix uh then once I have my uh documents",
    "start": "749519",
    "end": "756360"
  },
  {
    "text": "ready uh and ready for uh quering basically my application would interact",
    "start": "756360",
    "end": "763440"
  },
  {
    "text": "with canopy chat engine similarly it would interact with an open AI uh model",
    "start": "763440",
    "end": "770519"
  },
  {
    "text": "or any scale vanilla model basically um providing the chat history as the input",
    "start": "770519",
    "end": "777480"
  },
  {
    "text": "the messages uh then what we are going to do is we're going to within the chat",
    "start": "777480",
    "end": "783399"
  },
  {
    "text": "engine usually uh kind of and it's also kind of configurable but usually what we",
    "start": "783399",
    "end": "789040"
  },
  {
    "text": "will see people doing is uh doing some query phrasal so basically taking the chat history and asking the llm to uh to",
    "start": "789040",
    "end": "799519"
  },
  {
    "text": "come up with the the query that needs to go to the knowledge base we will take this query embed it uh and send it to",
    "start": "799519",
    "end": "806880"
  },
  {
    "text": "the knowledge base the knowledge base will query fine uh fetch the relevant documents into the",
    "start": "806880",
    "end": "812320"
  },
  {
    "text": "context engine context engine will reconstruct the context according to the budget that was configured when the",
    "start": "812320",
    "end": "819639"
  },
  {
    "text": "application uh uh started and then send back this content to the llm via the",
    "start": "819639",
    "end": "827320"
  },
  {
    "text": "chat engine when the llm responses this response is going back to the user but",
    "start": "827320",
    "end": "833000"
  },
  {
    "text": "it was already all the contexts have been managed and sent to the llm so the uh answer is already a ragged",
    "start": "833000",
    "end": "841079"
  },
  {
    "text": "answer um so that's kind of the flow in its most simple um and I think if",
    "start": "841079",
    "end": "847360"
  },
  {
    "text": "questions we keep it to the end and I'll be happy to answer great thank you and I can see a",
    "start": "847360",
    "end": "854040"
  },
  {
    "text": "flowes of question coming in this is uh great um but let's go through the demo",
    "start": "854040",
    "end": "859399"
  },
  {
    "text": "and hopefully the demo will answer some of the question uh but you know we'll make sure to uh answer some of the",
    "start": "859399",
    "end": "864720"
  },
  {
    "text": "questions at the end um so with that uh K okay hi everybody uh my name is Kyle",
    "start": "864720",
    "end": "872480"
  },
  {
    "text": "Juan I'm a solution use architect with any scale so what I'm showing right now is N scale platform maybe some of you",
    "start": "872480",
    "end": "879040"
  },
  {
    "text": "haven't seen this before so basically uh this is the fully managed Ruster",
    "start": "879040",
    "end": "885360"
  },
  {
    "text": "provided by any skill uh we'll very quickly go through some very basic",
    "start": "885360",
    "end": "890480"
  },
  {
    "text": "functions of this console is that we provide recaster management and what you're seeing right now the workspace is",
    "start": "890480",
    "end": "897720"
  },
  {
    "text": "actually developer ID environment built on top of the r cluster you see that we",
    "start": "897720",
    "end": "904800"
  },
  {
    "text": "provide vs code and Jupiter notebook as common uh Pro programming tools we also",
    "start": "904800",
    "end": "911480"
  },
  {
    "text": "provide the read dashboard to monitor all the recluster usage um here you can",
    "start": "911480",
    "end": "917639"
  },
  {
    "text": "see that we only have a head note running with CPU and memory usage here",
    "start": "917639",
    "end": "922759"
  },
  {
    "text": "and today uh the Jupiter notebook uh we're going to run on this recluster",
    "start": "922759",
    "end": "929199"
  },
  {
    "text": "and I will walk you through the code how to using canopy with NC end point so uh",
    "start": "929199",
    "end": "936360"
  },
  {
    "text": "here is how you install canopy the name of the package is called canopy D",
    "start": "936360",
    "end": "941680"
  },
  {
    "text": "SDK after the installation uh of course you need to specify your basic Pine con",
    "start": "941680",
    "end": "948839"
  },
  {
    "text": "uh tokens and uh oh sorry this should be the N scale uh API",
    "start": "948839",
    "end": "955480"
  },
  {
    "text": "tokens uh with all those token I already run this uh sale to have the token set",
    "start": "955480",
    "end": "961759"
  },
  {
    "text": "now let's start reading some documents so we uh in this sample notebook there's",
    "start": "961759",
    "end": "967160"
  },
  {
    "text": "a public data set provided by uh Pine con uh you were reading",
    "start": "967160",
    "end": "974160"
  },
  {
    "text": "several uh nines and rows from this pine cone data sets so basically there the",
    "start": "974160",
    "end": "980120"
  },
  {
    "text": "the information scraped from the pine con documents",
    "start": "980120",
    "end": "985600"
  },
  {
    "text": "website the next step we are going to initial initialized tokenizer for the",
    "start": "985959",
    "end": "991480"
  },
  {
    "text": "canopy uh to use with any scale end point bace basically we're going to demo",
    "start": "991480",
    "end": "998040"
  },
  {
    "text": "how to using the open source llama model with canopy so the tokenizer is also one",
    "start": "998040",
    "end": "1004000"
  },
  {
    "text": "provided here called the Nama tokenizer so you should initialize the",
    "start": "1004000",
    "end": "1009279"
  },
  {
    "text": "tokenizer with this n tokenizer and you can do a very simple test that they can tokenize the world",
    "start": "1009279",
    "end": "1017079"
  },
  {
    "text": "properly the next next step we are going to create knowledge base to store our",
    "start": "1017079",
    "end": "1022480"
  },
  {
    "text": "data so knowledge base is the con the very first concept in canopy which will",
    "start": "1022480",
    "end": "1028438"
  },
  {
    "text": "store uh or your data set so to start with knowledge base uh you can initial",
    "start": "1028439",
    "end": "1036000"
  },
  {
    "text": "initialize it with a index name and you need to specify the record incoder to be",
    "start": "1036000",
    "end": "1043400"
  },
  {
    "text": "N scale record encoder so basically record encoder here is embedding mode",
    "start": "1043400",
    "end": "1048480"
  },
  {
    "text": "model for the uh data you're going to embedded and save as save in the uh",
    "start": "1048480",
    "end": "1055520"
  },
  {
    "text": "python Vector database if this index is not available",
    "start": "1055520",
    "end": "1060960"
  },
  {
    "text": "you can call the next Cale to create it I can start running the sale since this",
    "start": "1060960",
    "end": "1066720"
  },
  {
    "text": "one is already created uh so they won't go into this create NB",
    "start": "1066720",
    "end": "1072240"
  },
  {
    "text": "index uh sale here let me turn this one off so you can see more C clearly",
    "start": "1072240",
    "end": "1079840"
  },
  {
    "text": "so if this index is not exist so they will go to here to create it for you and",
    "start": "1079840",
    "end": "1087640"
  },
  {
    "text": "make sure that you choose Dimension as 1024 basically that's the uh Dimension",
    "start": "1087640",
    "end": "1093919"
  },
  {
    "text": "model anying scale supported for now next step you are going to Simply run",
    "start": "1093919",
    "end": "1099760"
  },
  {
    "text": "knowledge based connect to connect to the Pine con",
    "start": "1099760",
    "end": "1104960"
  },
  {
    "text": "database uh for the data set we download above we are going to",
    "start": "1104960",
    "end": "1110159"
  },
  {
    "text": "create them as document object and here's one example of the document",
    "start": "1110159",
    "end": "1117200"
  },
  {
    "text": "example in the sample data set you can see that we have a document ID to represent this file and here is a text",
    "start": "1117200",
    "end": "1124679"
  },
  {
    "text": "field will record what all the detail information stored in this document and",
    "start": "1124679",
    "end": "1131720"
  },
  {
    "text": "last we have some metadata showing that the created time and the title for this",
    "start": "1131720",
    "end": "1137320"
  },
  {
    "text": "document so you can call Knowledge based upsert to upsert",
    "start": "1137320",
    "end": "1143960"
  },
  {
    "text": "all those document up to uh the pine database so in the back end this uh",
    "start": "1143960",
    "end": "1151480"
  },
  {
    "text": "knowledge base will call the embedding model you specified earlier which is N",
    "start": "1151480",
    "end": "1157159"
  },
  {
    "text": "scale embedding to uh embedded all those document change them to Vector abser to",
    "start": "1157159",
    "end": "1163520"
  },
  {
    "text": "the vector base so I'm going to skip this step because I already done this for you after that you can see that in",
    "start": "1163520",
    "end": "1170840"
  },
  {
    "text": "the Pinon console you will see that a index called canopy Das Das N scale in",
    "start": "1170840",
    "end": "1177480"
  },
  {
    "text": "point a dash in Bedding was created here and inside this sample index we",
    "start": "1177480",
    "end": "1184880"
  },
  {
    "text": "create 840 vectors with dimension of 1024 to recorded all those",
    "start": "1184880",
    "end": "1192679"
  },
  {
    "text": "informations so here's the examples of one of the vectors",
    "start": "1192679",
    "end": "1200240"
  },
  {
    "text": "okay uh now the N base is created and a",
    "start": "1200679",
    "end": "1206039"
  },
  {
    "text": "Pome index was also inserted now you can start to curate this database directly",
    "start": "1206039",
    "end": "1213000"
  },
  {
    "text": "on the N based cury for example if you want to cury ATT Tex what's the P1 Port",
    "start": "1213000",
    "end": "1219600"
  },
  {
    "text": "capacity uh they will return all these uh CER result here they will show you",
    "start": "1219600",
    "end": "1227000"
  },
  {
    "text": "what's the document for this query what's the source of it and what's the",
    "start": "1227000",
    "end": "1232159"
  },
  {
    "text": "similarity score for it and when you do the cury of course",
    "start": "1232159",
    "end": "1237760"
  },
  {
    "text": "you can choose uh the top key K result by specifying the top key parameter if",
    "start": "1237760",
    "end": "1244600"
  },
  {
    "text": "we say this one only by two they will simply reply two result instead of",
    "start": "1244600",
    "end": "1249799"
  },
  {
    "text": "default uh about 10 I guess okay the next concept I'm going to",
    "start": "1249799",
    "end": "1256200"
  },
  {
    "text": "talk about is the context engine now you have a knowledge base The Next",
    "start": "1256200",
    "end": "1261559"
  },
  {
    "text": "Step you can do is create a context engine using this knowledge base with",
    "start": "1261559",
    "end": "1267280"
  },
  {
    "text": "this contact engine you can also do the query specify the query text and the top",
    "start": "1267280",
    "end": "1273760"
  },
  {
    "text": "result and then get a result here so this will be very similar to what we get above but just with the contact",
    "start": "1273760",
    "end": "1282279"
  },
  {
    "text": "engine here's the result of showing the top five result here",
    "start": "1282279",
    "end": "1289679"
  },
  {
    "text": "the next component in canopy is the chat engine that's really the beauty of it",
    "start": "1289679",
    "end": "1294720"
  },
  {
    "text": "the chat engine is built on top of context engine and you also can specify a llm provider here we're using uh the N",
    "start": "1294720",
    "end": "1303320"
  },
  {
    "text": "scale integrate with canopy called N scale llm to uh Power this chat engine",
    "start": "1303320",
    "end": "1311120"
  },
  {
    "text": "another thing you want to specify is the cury Builder we now we're using a very",
    "start": "1311120",
    "end": "1316600"
  },
  {
    "text": "uh basic Cur Builder called the last message contributor to a power this chat",
    "start": "1316600",
    "end": "1324039"
  },
  {
    "text": "Engine with all those components ready now you can really start your chat",
    "start": "1324039",
    "end": "1329679"
  },
  {
    "text": "assistant talk to the uh knowledge base you inserted",
    "start": "1329679",
    "end": "1335159"
  },
  {
    "text": "before so here let's say if you put this uh information uh this CER about what's",
    "start": "1335159",
    "end": "1341279"
  },
  {
    "text": "capacity of P1 Port into this chat engine what's his response",
    "start": "1341279",
    "end": "1346960"
  },
  {
    "text": "here so they will tell you based on the context the capacity of P1 part is about",
    "start": "1346960",
    "end": "1353480"
  },
  {
    "text": "1 million vectors with uh 768 Dimension and you can find this based on this",
    "start": "1353480",
    "end": "1360480"
  },
  {
    "text": "documents and this chat will also return the history of our",
    "start": "1360480",
    "end": "1366080"
  },
  {
    "text": "uh chat history now you can go to the next question still using the chat API",
    "start": "1366080",
    "end": "1372480"
  },
  {
    "text": "to see what's the latency requirement here and you can see they will response",
    "start": "1372480",
    "end": "1377679"
  },
  {
    "text": "with is the uh latency time less than 100 millisecond based on this",
    "start": "1377679",
    "end": "1383360"
  },
  {
    "text": "information and you can see the history is got updated with our first question",
    "start": "1383360",
    "end": "1389159"
  },
  {
    "text": "list here and a second question list there so now you already have a chatter",
    "start": "1389159",
    "end": "1394320"
  },
  {
    "text": "system build on top of both uh a pine cone and any",
    "start": "1394320",
    "end": "1400320"
  },
  {
    "text": "scale so this is a very quick go through of how you use canop API to build the uh",
    "start": "1400320",
    "end": "1407240"
  },
  {
    "text": "chat engine and next I want to show you how we can directly using kop cl to do",
    "start": "1407240",
    "end": "1414840"
  },
  {
    "text": "the similar work to using C we will create a yo file to have the",
    "start": "1414840",
    "end": "1421279"
  },
  {
    "text": "configurations set up for this uh CL usage uh from this one you can see here",
    "start": "1421279",
    "end": "1429640"
  },
  {
    "text": "uh we also need to specify what type of tokenizer we're going to use and what's the model used behind this nama",
    "start": "1429640",
    "end": "1436880"
  },
  {
    "text": "tokenizer for the challenging we're using any scale llm and we can also",
    "start": "1436880",
    "end": "1442240"
  },
  {
    "text": "specify which model uh you want to use from any scale public Point here we're",
    "start": "1442240",
    "end": "1448440"
  },
  {
    "text": "using 177 billion model and the last I want to show is for",
    "start": "1448440",
    "end": "1454919"
  },
  {
    "text": "the knowledge base as I mentioned before we're using the any scale recorder encoder which is our embedding model and",
    "start": "1454919",
    "end": "1461960"
  },
  {
    "text": "the model we are using is the GTE Das large model and the batch size",
    "start": "1461960",
    "end": "1468600"
  },
  {
    "text": "uh even though it's not directly used in the chat but later on if you want to insert your uh documents into the na",
    "start": "1468600",
    "end": "1476880"
  },
  {
    "text": "base uh they will be batched using this batch size value to work with it okay",
    "start": "1476880",
    "end": "1484039"
  },
  {
    "text": "now we have this configure file we can start a chat engine on top of it so",
    "start": "1484039",
    "end": "1491080"
  },
  {
    "text": "here's the command how you start canob be with this configure",
    "start": "1491080",
    "end": "1496360"
  },
  {
    "text": "file if you R this line they will connect to the uh Pine",
    "start": "1496360",
    "end": "1503720"
  },
  {
    "text": "con server and start a chat server",
    "start": "1503720",
    "end": "1510000"
  },
  {
    "text": "locally and this one is already connect to our a embedding Vector the base",
    "start": "1510000",
    "end": "1515799"
  },
  {
    "text": "online and now you can start the canopy chat",
    "start": "1515799",
    "end": "1521799"
  },
  {
    "text": "directly so now you already have a chat board",
    "start": "1526679",
    "end": "1533240"
  },
  {
    "text": "available for you to chat with so let's try the same question before what is the",
    "start": "1533240",
    "end": "1539440"
  },
  {
    "text": "P1 Port",
    "start": "1539440",
    "end": "1543278"
  },
  {
    "text": "capacity and Escape Follow by enter so this bot will directly uh do do",
    "start": "1549200",
    "end": "1557799"
  },
  {
    "text": "the r application and give you the answer here you can also keep what's the",
    "start": "1557799",
    "end": "1563520"
  },
  {
    "text": "latency since all the history was stored automatically so they will directly know",
    "start": "1563520",
    "end": "1569960"
  },
  {
    "text": "that what you were asking there and get the answer for you from there of course you can keep chatting with uh",
    "start": "1569960",
    "end": "1578520"
  },
  {
    "text": "this application to throw in more questions and this is all the answers uh",
    "start": "1578520",
    "end": "1586279"
  },
  {
    "text": "they will give to you based on the vector B database uh results",
    "start": "1586279",
    "end": "1593000"
  },
  {
    "text": "here uh so this is the uh example of this yo file and actually in the canopy",
    "start": "1593000",
    "end": "1600480"
  },
  {
    "text": "repo they provide uh the full configuration file here they will",
    "start": "1600480",
    "end": "1606120"
  },
  {
    "text": "specify all the relative informations you can configure H based on your application needs for example this",
    "start": "1606120",
    "end": "1614120"
  },
  {
    "text": "configure yo file you can basically modify what's the system prompt you want",
    "start": "1614120",
    "end": "1620000"
  },
  {
    "text": "to apply to the L model right so here is the default one using the following",
    "start": "1620000",
    "end": "1626120"
  },
  {
    "text": "Contex context to answer the queries in the next message uh etc etc and also the",
    "start": "1626120",
    "end": "1633960"
  },
  {
    "text": "query Builder prompt uh is BAS if you after getting your contacts and your",
    "start": "1633960",
    "end": "1639880"
  },
  {
    "text": "query they were using this uh quy Builder prompt to create a a very",
    "start": "1639880",
    "end": "1645600"
  },
  {
    "text": "helpful query for your next create to the engine and the tokenizer uh of course",
    "start": "1645600",
    "end": "1652880"
  },
  {
    "text": "open eyes is also supported you can using the open ey tokenizer specify which model you want to use and for chat",
    "start": "1652880",
    "end": "1661519"
  },
  {
    "text": "enging you see we can also play with what's the maximum length of the prompt",
    "start": "1661519",
    "end": "1666919"
  },
  {
    "text": "what's the maximum generate token context token etc etc and also how many",
    "start": "1666919",
    "end": "1673320"
  },
  {
    "text": "history you want to save for the chat engine open AI as a default engine here",
    "start": "1673320",
    "end": "1681120"
  },
  {
    "text": "uh this is the model you want to use uh for now any skill lii can be also",
    "start": "1681120",
    "end": "1688120"
  },
  {
    "text": "supplied here as LM provider cury engine contact engine and",
    "start": "1688120",
    "end": "1694600"
  },
  {
    "text": "knowledge base so there's a lot of parameters you can play with with this",
    "start": "1694600",
    "end": "1701480"
  },
  {
    "text": "one modified based on your needs then you can directly run the canopy COI to",
    "start": "1701480",
    "end": "1707000"
  },
  {
    "text": "start your chat application and and one thing I I'd like to add is this is pretty powerful in my",
    "start": "1707000",
    "end": "1714159"
  },
  {
    "text": "mind because you know I mentioned before you have all those different set of configuration that you need to iterate",
    "start": "1714159",
    "end": "1719720"
  },
  {
    "text": "based on your use case and your applications and so having everything store in a config yo file you can start",
    "start": "1719720",
    "end": "1726200"
  },
  {
    "text": "iterating and uh you know improving upon your Baseline config configuration for your rag application and maybe you can",
    "start": "1726200",
    "end": "1733080"
  },
  {
    "text": "have multiple you can store version and then you know quickly uh spin up a application based on your configuration",
    "start": "1733080",
    "end": "1740519"
  },
  {
    "text": "so to me I mean that's that's super cool that you know I can have multiple yo file based on the solution and rag",
    "start": "1740519",
    "end": "1746960"
  },
  {
    "text": "application that I want to build exactly so uh canopy is a open source",
    "start": "1746960",
    "end": "1754200"
  },
  {
    "text": "tool so for now they have support for uh you know different type of llm different",
    "start": "1754200",
    "end": "1760880"
  },
  {
    "text": "even the chunkers uh and I believe with more people contributed to it there will be",
    "start": "1760880",
    "end": "1766600"
  },
  {
    "text": "more component be supported and it will more easy for people to customize their",
    "start": "1766600",
    "end": "1773760"
  },
  {
    "text": "chat application and have a quick really quick iteration on that and build the application",
    "start": "1773760",
    "end": "1780640"
  },
  {
    "text": "out great awesome all right should we",
    "start": "1780640",
    "end": "1786240"
  },
  {
    "text": "um uh turn over to the links and then U maybe we can do some Q&A uh unless what",
    "start": "1786240",
    "end": "1793559"
  },
  {
    "text": "was there anything else you wanted to present uh that's all from me okay let's we can go to the Q&A",
    "start": "1793559",
    "end": "1801440"
  },
  {
    "text": "session great thank you all right so let me quickly share my",
    "start": "1801440",
    "end": "1807720"
  },
  {
    "text": "screen um all right so we've done this um so these are you know a couple of",
    "start": "1807720",
    "end": "1812919"
  },
  {
    "text": "links um if you wanted to try pine cone and incal point for free that's the",
    "start": "1812919",
    "end": "1818640"
  },
  {
    "text": "first link respectively for Pine in scale endpoint uh the example will be",
    "start": "1818640",
    "end": "1825320"
  },
  {
    "text": "pushed shortly onto the G github.com Pine con uh GitHub repo and if you're",
    "start": "1825320",
    "end": "1831360"
  },
  {
    "text": "interested in just more generic example using any scale endpoint we also have a repo with some Qui books um but let's",
    "start": "1831360",
    "end": "1840240"
  },
  {
    "text": "turn into the question I think there was some um very interesting questions and um you know Ro um or maybe you know you",
    "start": "1840240",
    "end": "1848000"
  },
  {
    "text": "can put provide some of your ideas so the first question is is canopy like L",
    "start": "1848000",
    "end": "1853039"
  },
  {
    "text": "Lang chain uh there's a couple of questions along those lines U maybe I can provide my take and then um you can",
    "start": "1853039",
    "end": "1858799"
  },
  {
    "text": "add some colors I think Lang chain has a bigger scope in Mission um and it try to",
    "start": "1858799",
    "end": "1864000"
  },
  {
    "text": "really Implement integrate many different tools I think by canopy is laser focus on building rag applications",
    "start": "1864000",
    "end": "1871120"
  },
  {
    "text": "um and you know you saw the example and that's really you know all it is um I think laser focus on um and not you know",
    "start": "1871120",
    "end": "1878559"
  },
  {
    "text": "trying to to be everything and do everything in the LM space um you know Roy what should",
    "start": "1878559",
    "end": "1884679"
  },
  {
    "text": "take so I I think that's a uh that's accurate um I think also one",
    "start": "1884679",
    "end": "1891279"
  },
  {
    "text": "difference that I see in in uh as we move forward is that um really for",
    "start": "1891279",
    "end": "1898159"
  },
  {
    "text": "canopy we are focusing on knowledge retrieval right this is what Pine con",
    "start": "1898159",
    "end": "1903240"
  },
  {
    "text": "does we're not trying to do uh any anything uh uh on the llms or on the uh",
    "start": "1903240",
    "end": "1910559"
  },
  {
    "text": "other component um and the way we see the future going as well is that a lot of",
    "start": "1910559",
    "end": "1916720"
  },
  {
    "text": "the heav lifting will go to the data so I don't know if you remember the early",
    "start": "1916720",
    "end": "1922559"
  },
  {
    "text": "days of machine learning uh it started off with the models it started off with",
    "start": "1922559",
    "end": "1927639"
  },
  {
    "text": "a lot of uh interesting stuff but ultimately it went uh down to the",
    "start": "1927639",
    "end": "1933679"
  },
  {
    "text": "ability of processing large amounts of data uh and storing large amounts of",
    "start": "1933679",
    "end": "1939480"
  },
  {
    "text": "data and and actually the quality of the data that you fed into your Learning",
    "start": "1939480",
    "end": "1945000"
  },
  {
    "text": "System right um and the way we build canopy is kind of with focus in this",
    "start": "1945000",
    "end": "1951399"
  },
  {
    "text": "area not trying to get everything out of everything ending up I feel like as kind",
    "start": "1951399",
    "end": "1957120"
  },
  {
    "text": "of the lowest common denominator we are trying to be really good at something really focused around brag right um",
    "start": "1957120",
    "end": "1964720"
  },
  {
    "text": "thank you all right another question for you Ro does pine con Vector database support",
    "start": "1964720",
    "end": "1971000"
  },
  {
    "text": "um flag per non Vector data it seems to me do is support metad data right",
    "start": "1971000",
    "end": "1978760"
  },
  {
    "text": "yeah so um we are supporting like uh arbitrary metadata uh I think that in",
    "start": "1978760",
    "end": "1985919"
  },
  {
    "text": "the future you would see uh um Innovation on this side both on Pine con side and on on canopy side of really",
    "start": "1985919",
    "end": "1993320"
  },
  {
    "text": "extending this not to just be data but to actually be knowledge right I think",
    "start": "1993320",
    "end": "1998559"
  },
  {
    "text": "data and knowledge are similar but not exactly the same thing so yeah great",
    "start": "1998559",
    "end": "2003760"
  },
  {
    "text": "thank you the next question which is you know quite popular um so clearly we're talking about text as a modality here",
    "start": "2003760",
    "end": "2011399"
  },
  {
    "text": "using landpoint but you know the pine con support other modality vave to wag",
    "start": "2011399",
    "end": "2017919"
  },
  {
    "text": "uh video image you know what uh how do people use pine for different",
    "start": "2017919",
    "end": "2024799"
  },
  {
    "text": "modalities so yeah so Pyon supports any modality right so with what common to",
    "start": "2024799",
    "end": "2031399"
  },
  {
    "text": "all of those uh modalities is that ultimately to to",
    "start": "2031399",
    "end": "2037080"
  },
  {
    "text": "store those or to represent those we're using some high dimensional uh uh Spa like high",
    "start": "2037080",
    "end": "2044440"
  },
  {
    "text": "dimensional semantic representation right this is the way that we use in order to retrieve any any type of kind",
    "start": "2044440",
    "end": "2050800"
  },
  {
    "text": "of unstructured multimodal data so Pon supports it naturally um right now",
    "start": "2050800",
    "end": "2056320"
  },
  {
    "text": "canopy is focused around text I think this is where we meet most of our uh uh",
    "start": "2056320",
    "end": "2061599"
  },
  {
    "text": "customers today uh But as time goes I think that we will definitely want to",
    "start": "2061599",
    "end": "2066638"
  },
  {
    "text": "see cannot be going multimodel also the models around multimodel models are",
    "start": "2066639",
    "end": "2073638"
  },
  {
    "text": "becoming more and more performant and more and more uh uh um available in the",
    "start": "2073639",
    "end": "2079480"
  },
  {
    "text": "open source um so hopefully yeah we'll see multimodality becoming a central",
    "start": "2079480",
    "end": "2085320"
  },
  {
    "text": "thing soon great thank you um all right an interesting one do canopy replace llama",
    "start": "2085320",
    "end": "2091720"
  },
  {
    "text": "Hub or is a competitor to llama index in my mind I mean there always always going",
    "start": "2091720",
    "end": "2097160"
  },
  {
    "text": "to be some overlapping features um naturally I think it's more complimentary to llama index the way I",
    "start": "2097160",
    "end": "2103920"
  },
  {
    "text": "think about Lama index is you know a way to manage the different pipeline from different sources to create embeddings I",
    "start": "2103920",
    "end": "2111079"
  },
  {
    "text": "don't believe that Lama index provide a vector database um or serve model so you",
    "start": "2111079",
    "end": "2116640"
  },
  {
    "text": "know that's why I think it's more complimentary than um competitive yeah and I think and one",
    "start": "2116640",
    "end": "2123880"
  },
  {
    "text": "comment here um I don't think I don't see ourselves as as the competitors of",
    "start": "2123880",
    "end": "2130040"
  },
  {
    "text": "of uh llama index or Lang chain I think that in the end of the day what we what",
    "start": "2130040",
    "end": "2135640"
  },
  {
    "text": "we as Pine want to see is developers succeeding in building AI application at scale right and by even even even if",
    "start": "2135640",
    "end": "2144119"
  },
  {
    "text": "you're using uh Lang chain along with any scale endpoint and pine conver the",
    "start": "2144119",
    "end": "2149880"
  },
  {
    "text": "database directly I'm happy for that as well so I think it's all about kind of meeting different kind of customers at a",
    "start": "2149880",
    "end": "2157079"
  },
  {
    "text": "different point in time 100% right the more options and choices um you can use and is available",
    "start": "2157079",
    "end": "2164760"
  },
  {
    "text": "for developer to BU the applications you know by all means okay can the embedding model be",
    "start": "2164760",
    "end": "2172400"
  },
  {
    "text": "fine tune um so we just announced uh the availability of the embedding model and",
    "start": "2172400",
    "end": "2178480"
  },
  {
    "text": "API um we don't support today fine-tuning the emitting model but certainly something that you know will",
    "start": "2178480",
    "end": "2184599"
  },
  {
    "text": "take in account uh as we uh fresh out I V that okay um someone is asking um is the",
    "start": "2184599",
    "end": "2195079"
  },
  {
    "text": "chat history being handled by the clients and not by the canopy or the server itself uh maybe while you can",
    "start": "2195079",
    "end": "2202400"
  },
  {
    "text": "shut some right there yeah so I think you saw two kind of interactions so when KY was kind of",
    "start": "2202400",
    "end": "2210079"
  },
  {
    "text": "walking through the library uh you saw Kyle kind of passing in the history so",
    "start": "2210079",
    "end": "2216000"
  },
  {
    "text": "we actually follow uh op open AI kind of uh",
    "start": "2216000",
    "end": "2221319"
  },
  {
    "text": "interface which I think is a a decision we made as a lot of the other community",
    "start": "2221319",
    "end": "2228240"
  },
  {
    "text": "Parts um for example verel and other kind of uh providers of different parts",
    "start": "2228240",
    "end": "2233880"
  },
  {
    "text": "of the stack have good integration uh and this one is kind of having the",
    "start": "2233880",
    "end": "2239440"
  },
  {
    "text": "history in the control of the user so the API itself has the history in the control of the user however you saw that",
    "start": "2239440",
    "end": "2246920"
  },
  {
    "text": "the CLI is already managing uh so the chat CLI is already managing this for you so building this is really really",
    "start": "2246920",
    "end": "2253000"
  },
  {
    "text": "simple and I think we will start adding more examples on how to do it however I think the interesting part is kind of",
    "start": "2253000",
    "end": "2258680"
  },
  {
    "text": "where the future is going right um so we saw a couple of uh uh weeks ago uh",
    "start": "2258680",
    "end": "2265839"
  },
  {
    "text": "assistance API and threads and other concept um definitely I think and you",
    "start": "2265839",
    "end": "2272280"
  },
  {
    "text": "can I think it's recorded so uh you can open up the recording at some point uh I",
    "start": "2272280",
    "end": "2277400"
  },
  {
    "text": "see us uh also going into kind of History management um whether uh in a form of a",
    "start": "2277400",
    "end": "2284800"
  },
  {
    "text": "thread or whether in a more complex form I think that if you think about it history is in",
    "start": "2284800",
    "end": "2292599"
  },
  {
    "text": "essence also data right it's also knowledge right um so I think it should",
    "start": "2292599",
    "end": "2298160"
  },
  {
    "text": "be treated this way and this is how we can a plan uh and hopefully we we will",
    "start": "2298160",
    "end": "2303880"
  },
  {
    "text": "uh provide some uh kind of public road map soon for canopy uh so you can see kind of where this fits in in our",
    "start": "2303880",
    "end": "2310839"
  },
  {
    "text": "plan perfect yeah the transition between State less to State full type of uh",
    "start": "2310839",
    "end": "2316280"
  },
  {
    "text": "experience uh makes perfect sense for database to to be managing that aspect",
    "start": "2316280",
    "end": "2321599"
  },
  {
    "text": "exactly great um okay there was a question where the chunking tokenizing",
    "start": "2321599",
    "end": "2326760"
  },
  {
    "text": "and embeddings take place um so in the example that uh car has shown the",
    "start": "2326760",
    "end": "2334000"
  },
  {
    "text": "embedding is being performed by the n scale uh endpoint obviously you can use",
    "start": "2334000",
    "end": "2339440"
  },
  {
    "text": "other means um you know similar to what you know open AI does um so you know that's how it was performed in the",
    "start": "2339440",
    "end": "2347920"
  },
  {
    "text": "demo okay what is the additional value of the context engine layer why can uh",
    "start": "2348000",
    "end": "2354880"
  },
  {
    "text": "the mod queries be made um so maybe why I know I think I just learned something",
    "start": "2354880",
    "end": "2360599"
  },
  {
    "text": "the the fact that you can limit the number of token and having the context engine manage that for you I think",
    "start": "2360599",
    "end": "2365680"
  },
  {
    "text": "that's super cool right but um yeah so canopy is still in the early",
    "start": "2365680",
    "end": "2373240"
  },
  {
    "text": "days um but I actually think that uh when we View kind of uh the stack I",
    "start": "2373240",
    "end": "2380119"
  },
  {
    "text": "think the context engine part is where all the Innovation will go in and I'll",
    "start": "2380119",
    "end": "2385319"
  },
  {
    "text": "try to explain um in the end of the day we see lots of development around models",
    "start": "2385319",
    "end": "2390720"
  },
  {
    "text": "right so new models every day I don't know how the team here at any scale is catching up with you know Vue model",
    "start": "2390720",
    "end": "2397079"
  },
  {
    "text": "coming out um so models are are becoming more available and and and have many so even",
    "start": "2397079",
    "end": "2404960"
  },
  {
    "text": "if you saw the newr models have different abilities already kind of models with some fused in Chain of",
    "start": "2404960",
    "end": "2412160"
  },
  {
    "text": "thoughts and other complex kind of or reasoning path right um for context",
    "start": "2412160",
    "end": "2419760"
  },
  {
    "text": "engine um I think that this is the this is the layer that actually helps you uh",
    "start": "2419760",
    "end": "2426119"
  },
  {
    "text": "in terms of uh uh uh your application side this is the this is the layer that",
    "start": "2426119",
    "end": "2432079"
  },
  {
    "text": "actually helps you go through a lot of information and figure out what is only",
    "start": "2432079",
    "end": "2439040"
  },
  {
    "text": "the right context so there was a question somewhere around can I tune top K so today you can tune top K it's",
    "start": "2439040",
    "end": "2445119"
  },
  {
    "text": "something that you can do um however in the future I would want this to disappear and and I'll try to explain in",
    "start": "2445119",
    "end": "2452200"
  },
  {
    "text": "the end of the day what you're actually expecting uh a knowledge base to do to be able to tell you hey given this query",
    "start": "2452200",
    "end": "2459119"
  },
  {
    "text": "I'm going to provide you with the relevant context and the relevant context only right llms are sensitive to",
    "start": "2459119",
    "end": "2467400"
  },
  {
    "text": "noise so we don't want to uh uh just chove in irrelevant context to our model",
    "start": "2467400",
    "end": "2474200"
  },
  {
    "text": "right so one query might have a thousand documents supporting it one query might",
    "start": "2474200",
    "end": "2480000"
  },
  {
    "text": "have one document supporting it in the end of the day the context engine layer",
    "start": "2480000",
    "end": "2485280"
  },
  {
    "text": "is the one that should simplify it for you as a developer this process of",
    "start": "2485280",
    "end": "2490359"
  },
  {
    "text": "actually needing to figure out what is the top K how do I make sure that I'm",
    "start": "2490359",
    "end": "2496079"
  },
  {
    "text": "not retrieving too much or too little so hope that's that makes sense",
    "start": "2496079",
    "end": "2501119"
  },
  {
    "text": "but that's a great question actually great thank you um this maybe should be",
    "start": "2501119",
    "end": "2506160"
  },
  {
    "text": "an easy one um they have multiple users that require different access control um",
    "start": "2506160",
    "end": "2513119"
  },
  {
    "text": "so how would you um you know create multitenant within pine cone can you create multiple indices with different",
    "start": "2513119",
    "end": "2520079"
  },
  {
    "text": "permission and controls so yeah so we have actually",
    "start": "2520079",
    "end": "2525640"
  },
  {
    "text": "couple of layers of of control um I think the two interesting one in terms of multi-tenancy",
    "start": "2525640",
    "end": "2531760"
  },
  {
    "text": "are indexes and name spaces right um so indexes are as as K shown",
    "start": "2531760",
    "end": "2540359"
  },
  {
    "text": "are uh uh object in space that have the same uh embedding space same metric",
    "start": "2540359",
    "end": "2548520"
  },
  {
    "text": "right um and a namespace is a sub of this index that allows you to have a",
    "start": "2548520",
    "end": "2556640"
  },
  {
    "text": "strict separation between data points so actually um given that in a reasonable",
    "start": "2556640",
    "end": "2562880"
  },
  {
    "text": "use case you will probably have a good setting of an llm that you chose and tuned or selected from uh uh from any",
    "start": "2562880",
    "end": "2571079"
  },
  {
    "text": "scale endpoint or an embedding model that fits your uh uh needs",
    "start": "2571079",
    "end": "2576800"
  },
  {
    "text": "uh and ultimately you have multiple uh uh tenants working with the same configuration so I think for",
    "start": "2576800",
    "end": "2583319"
  },
  {
    "text": "multi-tenancy what we see more common is namespaces and actually we're going to uh um have some more content around how",
    "start": "2583319",
    "end": "2590680"
  },
  {
    "text": "to use namespace and basically how to do multi- tendency with canopy great thank you another question",
    "start": "2590680",
    "end": "2598680"
  },
  {
    "text": "does any scale support cohere in Google model with the same API so you know if you think about the API is really",
    "start": "2598680",
    "end": "2605040"
  },
  {
    "text": "streamlined around the open AI SDK so you know you can build your own obstructions and Route",
    "start": "2605040",
    "end": "2610920"
  },
  {
    "text": "accordingly to different providers we are also thinking about a concept of a",
    "start": "2610920",
    "end": "2616200"
  },
  {
    "text": "router so think of it as uh you pass your palp and then based on the nature",
    "start": "2616200",
    "end": "2621480"
  },
  {
    "text": "of the pal you can you know pass it to different type of models of different sizes based on you know the nature and",
    "start": "2621480",
    "end": "2628079"
  },
  {
    "text": "complexity of your pond right so these router concept can be U you know uh be",
    "start": "2628079",
    "end": "2633880"
  },
  {
    "text": "used for the NH scale within the different model within any scale but um you know it could be also extended to uh",
    "start": "2633880",
    "end": "2641000"
  },
  {
    "text": "invoke different uh LM providers okay I think we are at time",
    "start": "2641000",
    "end": "2649440"
  },
  {
    "text": "thank you for all the questions um we'll make sure to uh push all the latest",
    "start": "2649440",
    "end": "2654720"
  },
  {
    "text": "example in the GitHub repo um and if you have any question you can reach out um",
    "start": "2654720",
    "end": "2659839"
  },
  {
    "text": "on a slack Channel or you know different Community uh on Pine and on any scale",
    "start": "2659839",
    "end": "2666920"
  },
  {
    "text": "okay thank you and thank you for joining us today thank",
    "start": "2666920",
    "end": "2673920"
  },
  {
    "text": "you",
    "start": "2676640",
    "end": "2679640"
  }
]