[
  {
    "text": "okay hi folks uh thanks for uh joining us today uh in today's webinar we will be talking with any scale um to that end",
    "start": "2639",
    "end": "10440"
  },
  {
    "text": "here's the here's the agenda for our talk today uh we'll start with an",
    "start": "10440",
    "end": "16278"
  },
  {
    "text": "introduction of the speakers and talk about how AWS is democratizing the AI",
    "start": "16279",
    "end": "21359"
  },
  {
    "text": "Revolution then we'll delve into our partnership with any scale uh then we'll",
    "start": "21359",
    "end": "27039"
  },
  {
    "text": "go into any scale's hosted SAS solution and the customers VPC deployment option",
    "start": "27039",
    "end": "32800"
  },
  {
    "text": "of the product my colleague will then discuss how any scale can manage the infrastructure for a",
    "start": "32800",
    "end": "38520"
  },
  {
    "text": "IML which allows you to keep costs in control then we'll show you a demo of",
    "start": "38520",
    "end": "43600"
  },
  {
    "text": "the N scale hosted SAS solution with a focus on cost managing cost management and this is followed by a Q&A",
    "start": "43600",
    "end": "51559"
  },
  {
    "text": "session and a call for Action um during the webinar if you have any questions please post them in the",
    "start": "51559",
    "end": "58519"
  },
  {
    "text": "chat session we have uh other folks from any scale uh that that will potentially",
    "start": "58519",
    "end": "64680"
  },
  {
    "text": "respond to your questions right in the chat or we can uh uh discuss those",
    "start": "64680",
    "end": "69920"
  },
  {
    "text": "questions at the end in the QI session uh to start with uh my name is",
    "start": "69920",
    "end": "75080"
  },
  {
    "text": "Art sadiki I am the partner ass architect with AWS um and I work closely with the any",
    "start": "75080",
    "end": "82360"
  },
  {
    "text": "scale team focusing on gen and uh um",
    "start": "82360",
    "end": "87600"
  },
  {
    "text": "machine learning Maran why don't you introduce yourself please hey folks I'm",
    "start": "87600",
    "end": "92960"
  },
  {
    "text": "Maran and I'm an AI engineer at any",
    "start": "92960",
    "end": "96720"
  },
  {
    "text": "scale so at uh I wanted to talk about some of our capabilities at AWS um at",
    "start": "98360",
    "end": "103759"
  },
  {
    "text": "AWS we we we are innovating on behalf of our customers to deliver the broadest",
    "start": "103759",
    "end": "108840"
  },
  {
    "text": "and the deepest set of machine learning capabilities for Builders of all levels of",
    "start": "108840",
    "end": "114280"
  },
  {
    "text": "expertise the goal here is to remove the undifferentiated heavy lifting so that",
    "start": "114280",
    "end": "119600"
  },
  {
    "text": "our customers can move faster over the past five years we've been investing in our own silicon to push the envelope on",
    "start": "119600",
    "end": "127079"
  },
  {
    "text": "performance and price performance for demanding workloads like ml ml training",
    "start": "127079",
    "end": "132280"
  },
  {
    "text": "and inference or AWS trainum and AWS INF frener chips offer the lowest cost for",
    "start": "132280",
    "end": "139519"
  },
  {
    "text": "training models and running inference in the cloud this ability to maximize",
    "start": "139519",
    "end": "144879"
  },
  {
    "text": "performance and control cost is why leading II startup such as any scale runs on a",
    "start": "144879",
    "end": "150400"
  },
  {
    "text": "us as you start to think about the future when you need to train and deploy models at scale in your application",
    "start": "150400",
    "end": "155840"
  },
  {
    "text": "across your organization most costs will be associated with running these models and doing inference the production",
    "start": "155840",
    "end": "162440"
  },
  {
    "text": "application can constantly be generating Productions and potentially generating Millions per hour the infia 2 instances",
    "start": "162440",
    "end": "171599"
  },
  {
    "text": "deliver up to 4X higher throughput and up to 10x lower latency compared to the",
    "start": "171599",
    "end": "178159"
  },
  {
    "text": "prior generations of infr based instances these capabilities deliver up",
    "start": "178159",
    "end": "184120"
  },
  {
    "text": "to 40% better inference price performance than other comparable Amazon",
    "start": "184120",
    "end": "189920"
  },
  {
    "text": "ec2 instances and the lowest cost for inference in the cloud in addition to Silicon AWS offers",
    "start": "189920",
    "end": "197799"
  },
  {
    "text": "a broad range of ml Frameworks and AI Services expert practitioners can develop in the Frameworks of their",
    "start": "197799",
    "end": "203879"
  },
  {
    "text": "choice data scientists can use endtoend capabilities of sagemaker business business analysts can",
    "start": "203879",
    "end": "210760"
  },
  {
    "text": "build their own models using Canvas OR application developers with no ml skills",
    "start": "210760",
    "end": "216280"
  },
  {
    "text": "can add intelligence to their application using our pre-train models",
    "start": "216280",
    "end": "221840"
  },
  {
    "text": "offered as managed Services these are some of the services that I mentioned but you can see it at top of the stack",
    "start": "221840",
    "end": "228000"
  },
  {
    "text": "uh you know it's it's uh labeled the AI AWS AI",
    "start": "228000",
    "end": "233720"
  },
  {
    "text": "Services the idea here is that at AWS you can innovate faster with the most comprehensive set of AIML Services let's",
    "start": "235560",
    "end": "243319"
  },
  {
    "text": "talk about our four key areas where AWS is helping deliver that set of",
    "start": "243319",
    "end": "248680"
  },
  {
    "text": "innovation first uh AWS enables AI powered use cases with hundreds of",
    "start": "248680",
    "end": "254200"
  },
  {
    "text": "pre-built algorithms models solutions for common use cases and",
    "start": "254200",
    "end": "260759"
  },
  {
    "text": "industries and delivers these at a faster time to Market with 23",
    "start": "260880",
    "end": "267400"
  },
  {
    "text": "pre-trained services like Amazon personalized Amazon KRA and Amazon monitoring secondly at AWS we're working",
    "start": "267400",
    "end": "274800"
  },
  {
    "text": "to democratize access to ml and make email accessible to more",
    "start": "274800",
    "end": "280120"
  },
  {
    "text": "people with Amazon Sage maker canvas AWS empowers any business analyst to make",
    "start": "280120",
    "end": "285720"
  },
  {
    "text": "predictions without writing any code with our AWS AI service you can easily",
    "start": "285720",
    "end": "291759"
  },
  {
    "text": "add AI capabilities to your applications with little or no machine learning",
    "start": "291759",
    "end": "297960"
  },
  {
    "text": "skills third can easily scale ml with AWS Amazon Sage allows you to build",
    "start": "297960",
    "end": "305199"
  },
  {
    "text": "train and deploy ml models for any use case as we continue to see the",
    "start": "305199",
    "end": "310639"
  },
  {
    "text": "exponential increase in sophistication of ml models with an increasing number of parameters you can build your own or",
    "start": "310639",
    "end": "318120"
  },
  {
    "text": "access open-source foundational models on AWS through Amazon sagemaker and last",
    "start": "318120",
    "end": "325240"
  },
  {
    "text": "we're committed to helping you grow your ml skills at your at your organization",
    "start": "325240",
    "end": "330840"
  },
  {
    "text": "and educating the next generation of ml developers with a whole host of resources from AWS training to AWS deep",
    "start": "330840",
    "end": "338919"
  },
  {
    "text": "racer to our AI ml scholarship program and AWS machine Learning",
    "start": "338919",
    "end": "344400"
  },
  {
    "text": "University you can also fur further accelerate your journey with AWS ml",
    "start": "344400",
    "end": "349800"
  },
  {
    "text": "Embark and AWS ml Solutions lab Amazon has been investing in Ai and",
    "start": "349800",
    "end": "356639"
  },
  {
    "text": "using ml for the past 20 years as some of the most sophisticated ml deployments in production we have",
    "start": "356639",
    "end": "363840"
  },
  {
    "text": "helped more than 100,000 customers achieve ml success and again these are some of the reasons why any scale",
    "start": "363840",
    "end": "370080"
  },
  {
    "text": "utilizes AWS for their platform to that end I wanted to take a",
    "start": "370080",
    "end": "375840"
  },
  {
    "text": "moment and give a brief introductions to Nale Nale platform is is the managed",
    "start": "375840",
    "end": "381880"
  },
  {
    "text": "version of The open- Source Ray product this can be used to scale and",
    "start": "381880",
    "end": "386919"
  },
  {
    "text": "manage distributed AI applications on AWS AI takes on the difficult it takes the",
    "start": "386919",
    "end": "393000"
  },
  {
    "text": "difficult task of instru infrastructure management of a distributed AI applications and it allows any skills",
    "start": "393000",
    "end": "400280"
  },
  {
    "text": "customers to focus on the model and the application itself to that end any",
    "start": "400280",
    "end": "405759"
  },
  {
    "text": "skilles platform has shown to reduce the cost of developing distributed AI applications and reduce the operational",
    "start": "405759",
    "end": "413000"
  },
  {
    "text": "expenses of running those applications by as much as 50% uh with that I'd like to hand over",
    "start": "413000",
    "end": "420080"
  },
  {
    "text": "to my colleague Marmon who will review and demonstrate how you can optimize cost and performance for modernday AI",
    "start": "420080",
    "end": "426960"
  },
  {
    "text": "applications on AWS Maran take it away thank you for",
    "start": "426960",
    "end": "432039"
  },
  {
    "text": "this uh nice introduction to any scale art uh let me just share my",
    "start": "432039",
    "end": "438280"
  },
  {
    "text": "screen uh so hopefully you see my screen now uh like art said I will be doing a",
    "start": "438280",
    "end": "446240"
  },
  {
    "text": "live demo of how you can uh Deploy on any scale and with a focus on uh cost",
    "start": "446240",
    "end": "452120"
  },
  {
    "text": "efficiency and scale but before I do that I'm going to take a closer look about how any scale is deployed on",
    "start": "452120",
    "end": "458879"
  },
  {
    "text": "AWS uh so this diagram is like a high level architecture diagram of any scale",
    "start": "458879",
    "end": "465280"
  },
  {
    "text": "um admittedly like there's a lot going on so let's break it down for you um so any scale employs like a control plane",
    "start": "465280",
    "end": "471919"
  },
  {
    "text": "data plane uh architecture so here on the right I have my control plane uh which is the zone that orchestrates the",
    "start": "471919",
    "end": "478400"
  },
  {
    "text": "compute and on the left is the data plane which is the Zone where the compute is running and where the data",
    "start": "478400",
    "end": "483720"
  },
  {
    "text": "lives so the core part of the data plane is the ray clusters that are running on AWS ec2 um and those ec2 instances will",
    "start": "483720",
    "end": "492639"
  },
  {
    "text": "have access to a shared cluster storage that's running on EFS and they will have um access to like a redis scache that's",
    "start": "492639",
    "end": "499960"
  },
  {
    "text": "um on a memory DB instance for high availability and fault tolerance uh additionally they have access to",
    "start": "499960",
    "end": "505800"
  },
  {
    "text": "external services like data that's sitting on S3 uh and can execute images",
    "start": "505800",
    "end": "510879"
  },
  {
    "text": "uh in the elastic container registry um you'll notice that all of these are deployed on public subnets across",
    "start": "510879",
    "end": "517479"
  },
  {
    "text": "different availability zones um within a VPC and so here uh let's talk about two",
    "start": "517479",
    "end": "523240"
  },
  {
    "text": "deployment models for any scale uh the first model is the VPC is in your Cloud",
    "start": "523240",
    "end": "528800"
  },
  {
    "text": "which we call it bring your own CL Cloud deployment model um there you have full control over your compute and your data",
    "start": "528800",
    "end": "535560"
  },
  {
    "text": "and you can uh bring in like any reserved capacity uh that you want uh",
    "start": "535560",
    "end": "541200"
  },
  {
    "text": "the other is like a fully SAS solution this option effectively means like zero infrastructure setup for you as a user",
    "start": "541200",
    "end": "548440"
  },
  {
    "text": "where the VPC will be inside any scales cloud and all you have to do is make use of uh any",
    "start": "548440",
    "end": "553640"
  },
  {
    "text": "scale uh so for any questions on this please you can post them in the Q&A and I have my colleagues uh from that can",
    "start": "553640",
    "end": "560480"
  },
  {
    "text": "answer them and we will pause later on for questions um so just a quick overview of",
    "start": "560480",
    "end": "566440"
  },
  {
    "text": "the AI challenges of today's world um the two main challenges are uh basically",
    "start": "566440",
    "end": "571880"
  },
  {
    "text": "cost and efficiency uh so if we take a look at the large language models of today both open source and closed Source",
    "start": "571880",
    "end": "578839"
  },
  {
    "text": "ones and especially the closed Source ones the large language models um you can uh easily extrapolate using the USD",
    "start": "578839",
    "end": "586320"
  },
  {
    "text": "per 1 million tokens that this will cost you on the order of hundreds of thousands of dollars to deploy these",
    "start": "586320",
    "end": "591600"
  },
  {
    "text": "models uh on a given year and um not only is it expensive but most of the",
    "start": "591600",
    "end": "596640"
  },
  {
    "text": "organizations do a bad job of like deploying their models um here I'm showing a survey conducted of 1,500",
    "start": "596640",
    "end": "603920"
  },
  {
    "text": "organizations where we see that most organizations do a poor job of utilizing their gpus so only 51 to 70% of their",
    "start": "603920",
    "end": "611279"
  },
  {
    "text": "gpus is being utilized by 53% of the organizations uh so today I'm going to",
    "start": "611279",
    "end": "619079"
  },
  {
    "text": "discuss a specific generative AI use case uh and that use case is that of deploying embedding models so um what",
    "start": "619079",
    "end": "626760"
  },
  {
    "text": "are embedding models embedding models are models that can encode unstructured data so like think of images text audio",
    "start": "626760",
    "end": "634880"
  },
  {
    "text": "uh into numerical vectors and so why might we want to use them like it turns out that they're very uh useful for",
    "start": "634880",
    "end": "642120"
  },
  {
    "text": "things like semantic search where like let's say you have a query you can encode that query into a numeric vector",
    "start": "642120",
    "end": "648320"
  },
  {
    "text": "and then you can return like the semantically similar um documents by Computing a similarity score and",
    "start": "648320",
    "end": "654480"
  },
  {
    "text": "fetching those retrieved documents um this same approach could be used with recommendation systems to find like",
    "start": "654480",
    "end": "660680"
  },
  {
    "text": "similar items or users and those retrieved documents could be passed to a large language model so that it can",
    "start": "660680",
    "end": "666920"
  },
  {
    "text": "generate a response based on a knowledge base we call this pattern uh retrieval augmented generation or rag for",
    "start": "666920",
    "end": "674000"
  },
  {
    "text": "short and um the challenges here are that we want to run uh at a perform",
    "start": "674000",
    "end": "680519"
  },
  {
    "text": "like we want to meet some performance slas given Dynamic traffic but we want to run in a cost-efficient",
    "start": "680519",
    "end": "686720"
  },
  {
    "text": "manner so uh to achieve this uh I'm we're going to make use of Ray serve and",
    "start": "686720",
    "end": "692040"
  },
  {
    "text": "we're going to deploy Ray serve on any scale so first like what is Ray serve um Ray serve is a easy easily scalable",
    "start": "692040",
    "end": "699160"
  },
  {
    "text": "framework for serving ml applications and um it's easy because it's python based and it it's like a",
    "start": "699160",
    "end": "706240"
  },
  {
    "text": "very nice abstraction that sits on top of Ray core which is the distributed computing platform um and it has Rich",
    "start": "706240",
    "end": "712760"
  },
  {
    "text": "Integrations with commonly used python libraries like Fast API um and so like the way Ray serve works is what I'm I'm",
    "start": "712760",
    "end": "719320"
  },
  {
    "text": "showing here in this diagram where like you will package your model uh into a deployment uh and it will run as",
    "start": "719320",
    "end": "725959"
  },
  {
    "text": "multiple replicas those replicas can run on multiple nodes and usually those replicas are",
    "start": "725959",
    "end": "731959"
  },
  {
    "text": "controlled um on a in a specific head node uh so we'll let's take a look at um",
    "start": "731959",
    "end": "738959"
  },
  {
    "text": "the first uh like how you configure your racer application so to conf you can",
    "start": "738959",
    "end": "745120"
  },
  {
    "text": "configure Rive application using something like yaml So for anybody who's to kubernetes before this is like a a",
    "start": "745120",
    "end": "752519"
  },
  {
    "text": "common pattern where you use yl to uh configure Your Service uh here what I'm",
    "start": "752519",
    "end": "757880"
  },
  {
    "text": "showing is that you can start uh scaling from to from zero and to zero so let's",
    "start": "757880",
    "end": "763839"
  },
  {
    "text": "just walk through the CML file uh this is my embedding service and",
    "start": "763839",
    "end": "769120"
  },
  {
    "text": "I'm uh configuring the embedding model deployment and I'm saying that I want to",
    "start": "769120",
    "end": "774440"
  },
  {
    "text": "start with an initial number of replicas of zero so this means that when I deploy the service I will only have my",
    "start": "774440",
    "end": "780360"
  },
  {
    "text": "controller or my load balancer running and I don't have any uh embedding model replicas running so I don't use gpus yet",
    "start": "780360",
    "end": "787760"
  },
  {
    "text": "until the first request comes in uh so that's when scaling will happen and then if I want to scale back down to zero I",
    "start": "787760",
    "end": "794600"
  },
  {
    "text": "can set my minimum replicas to zero the next thing that RAC serve uh uh",
    "start": "794600",
    "end": "801320"
  },
  {
    "text": "allows is for like a fractional gpus so it turns out like these embedding models don't take up too much GPU memory to",
    "start": "801320",
    "end": "808000"
  },
  {
    "text": "load um um so in this case like I want to run on an h10g uh accelerator but I",
    "start": "808000",
    "end": "814800"
  },
  {
    "text": "only want to use 1/10th of that a1g so I can specify my n gpus to be",
    "start": "814800",
    "end": "820360"
  },
  {
    "text": "0.1 and in this case Ray is going to perform a logical accounting of the available compute and so it will try to",
    "start": "820360",
    "end": "827800"
  },
  {
    "text": "like schedule nine other model replicas onto the same GPU effectively maximizing",
    "start": "827800",
    "end": "833199"
  },
  {
    "text": "the GP utilization uh and now when we deploy",
    "start": "833199",
    "end": "838440"
  },
  {
    "text": "Ray serve to any scale we can leverage a lot of cost optimizations thanks to our deep integration with AWS uh",
    "start": "838440",
    "end": "844959"
  },
  {
    "text": "specifically AWS ec2 spot instances so here what I'm showing is uh how I can",
    "start": "844959",
    "end": "851199"
  },
  {
    "text": "set the compute configuration for my any skill service so uh there's two parts to",
    "start": "851199",
    "end": "857560"
  },
  {
    "text": "to the to the to the racer um there's a there's the head node um which will host",
    "start": "857560",
    "end": "864120"
  },
  {
    "text": "the main controller which you can think of as the load balancer and then there's the worker nodes um where we we'll be",
    "start": "864120",
    "end": "870720"
  },
  {
    "text": "running our model uh our embedding model replicas so in this case I'm using U an",
    "start": "870720",
    "end": "877199"
  },
  {
    "text": "m5x large like a CPU for my head node and I'm using a g5x large like a GPU",
    "start": "877199",
    "end": "882759"
  },
  {
    "text": "instance for my worker nodes uh and I'm also specifying that I want to scale from zero nodes all the way up to four",
    "start": "882759",
    "end": "890120"
  },
  {
    "text": "and down here is where I'm um specifying my market type and I'm setting it to prefer spot so what this means is that",
    "start": "890120",
    "end": "897279"
  },
  {
    "text": "any scale is going to go and try to find find a spot instance for us but if there's no capacity then it will fall",
    "start": "897279",
    "end": "903240"
  },
  {
    "text": "back to On Demand but additionally what this also means is that periodically um",
    "start": "903240",
    "end": "908440"
  },
  {
    "text": "any scale is going to check for uh any spot instance availability and in the case spot instan has become available",
    "start": "908440",
    "end": "914320"
  },
  {
    "text": "any scale will safely migrate the replicas from on demand back to spot and what it also means is like if we're",
    "start": "914320",
    "end": "921000"
  },
  {
    "text": "running on spot and we have to like EV because the spot is no longer available",
    "start": "921000",
    "end": "926639"
  },
  {
    "text": "uh at the price that we took it at then any scale will safely migrate back to an on demand",
    "start": "926639",
    "end": "931720"
  },
  {
    "text": "instance um and additionally I should say like if you strictly want to run on spot we can change this Market type from",
    "start": "931720",
    "end": "938240"
  },
  {
    "text": "prefer spot to just spot uh there's many any scale cost and",
    "start": "938240",
    "end": "944160"
  },
  {
    "text": "performance op optimizations that's going to be enabled for me by default when I uh Deploy on any scale so here",
    "start": "944160",
    "end": "950160"
  },
  {
    "text": "I'm going to take the time to talk about one which is called replica compaction um so usually like when I'm",
    "start": "950160",
    "end": "956160"
  },
  {
    "text": "deploying uh to an any scale service I want to de multiple models so like in our case think of it like when I'm",
    "start": "956160",
    "end": "962000"
  },
  {
    "text": "deploying my embedding model I probably also want to add like a large language model to it for like a rag",
    "start": "962000",
    "end": "967360"
  },
  {
    "text": "application um in this diagram which I took from our blog um we're showing that we're deploying three large language",
    "start": "967360",
    "end": "973399"
  },
  {
    "text": "models model a uh which is a 70 billion parameter model B 13 billion and c and",
    "start": "973399",
    "end": "980480"
  },
  {
    "text": "so given the dynamic nature of the traffic what ends up happening is um we initially start with like a somewhat",
    "start": "980480",
    "end": "986279"
  },
  {
    "text": "optimal setup where like we''ve uh um bin packed the these replicas onto two",
    "start": "986279",
    "end": "991440"
  },
  {
    "text": "nodes uh to maximize utilization but then like uh the model A",
    "start": "991440",
    "end": "996519"
  },
  {
    "text": "will get Scaled down to zero let's say like no requests come in for Model A for a long for set period of time so now",
    "start": "996519",
    "end": "1003959"
  },
  {
    "text": "like we're left in a situation where our uh nodes are severely underutilized so",
    "start": "1003959",
    "end": "1010519"
  },
  {
    "text": "what uh replica compaction does it's like a periodic thing where like uh we will check the uh GP utilization and if",
    "start": "1010519",
    "end": "1018600"
  },
  {
    "text": "we can if any scale will start migrating these replicas from node 2 onto node one",
    "start": "1018600",
    "end": "1024880"
  },
  {
    "text": "uh and once once it's done it it will take down the second node um and saving us the cost of an additional GPU",
    "start": "1024880",
    "end": "1033160"
  },
  {
    "text": "instance so um I will switch now to like a live demo of all the features I've shown you so far but just high level",
    "start": "1033160",
    "end": "1041240"
  },
  {
    "text": "this is what I'm going going to demo for you so we're going to deploy an embedding model it's going to be the",
    "start": "1041240",
    "end": "1046520"
  },
  {
    "text": "nlpr GTE large model and um this is implemented using Ray",
    "start": "1046520",
    "end": "1051880"
  },
  {
    "text": "serve and fast API uh and we're deploying this on any skill Services uh",
    "start": "1051880",
    "end": "1057520"
  },
  {
    "text": "and for context you can think of this as like a component in an Enterprise QA application where traffic is dynamic and",
    "start": "1057520",
    "end": "1063559"
  },
  {
    "text": "practically zero and after work hours so I'm going to switch to my any",
    "start": "1063559",
    "end": "1069080"
  },
  {
    "text": "skill workspace and uh and so basically an any scale workspace is a you can think of",
    "start": "1069080",
    "end": "1076480"
  },
  {
    "text": "this as like an IDE like Visual Studio code uh where uh it's running against",
    "start": "1076480",
    "end": "1082400"
  },
  {
    "text": "array cluster so it's uh and and in this case I'm going to start off with the same yl file that I've been presenting",
    "start": "1082400",
    "end": "1089919"
  },
  {
    "text": "to you guys uh and here I'm just for the sake of time I'm going to show you how to like deploy this to any scale",
    "start": "1089919",
    "end": "1096200"
  },
  {
    "text": "services so I will run this command any scale service deploy and provide this yaml",
    "start": "1096200",
    "end": "1104280"
  },
  {
    "text": "file as as an option and so this will start my uh Service uh in the meantime",
    "start": "1104280",
    "end": "1111080"
  },
  {
    "text": "let's just talk through this again so uh what this is doing is um it's configuring my embedding",
    "start": "1111080",
    "end": "1117679"
  },
  {
    "text": "Service uh and it's importing the code from a main.py file uh specifically the",
    "start": "1117679",
    "end": "1123039"
  },
  {
    "text": "my app object and uh the current deployment is an embedding model",
    "start": "1123039",
    "end": "1128480"
  },
  {
    "text": "deployment where here's my autoscaling config where I can set certain things like how many requests um each replica",
    "start": "1128480",
    "end": "1135720"
  },
  {
    "text": "should handle before I start scaling to the next replica the minimum replicas initial replicas so",
    "start": "1135720",
    "end": "1141480"
  },
  {
    "text": "like I can start scaling from zero and scale down to zero the maximum number of",
    "start": "1141480",
    "end": "1147240"
  },
  {
    "text": "replicas so I don't keep scaling infinitely uh to control my costs uh and",
    "start": "1147240",
    "end": "1152400"
  },
  {
    "text": "any delays on scaling upscaling or down scaling so this this will really depend on the nature of your traffic uh and",
    "start": "1152400",
    "end": "1159880"
  },
  {
    "text": "additionally here like the N gpus is set to 0.1 because uh we want to use like",
    "start": "1159880",
    "end": "1165080"
  },
  {
    "text": "fractional gpus and we only need on10th of an A10 G uh and here's where I'm showing my head",
    "start": "1165080",
    "end": "1171840"
  },
  {
    "text": "node where I'm running a CPU instance of an m5x large and I'm",
    "start": "1171840",
    "end": "1177360"
  },
  {
    "text": "running uh a GPU G5 2x large instance for my worker notes so let's take a look",
    "start": "1177360",
    "end": "1183480"
  },
  {
    "text": "at the code my main.py file uh here's some imports I'm using fast API uh",
    "start": "1183480",
    "end": "1188720"
  },
  {
    "text": "sentence transform is to load the model and Ray serve so what I do is like you instantiate my fast API instance and",
    "start": "1188720",
    "end": "1196280"
  },
  {
    "text": "here's where I Define my serf deployment so serve deployment is usually a callable class where all of the",
    "start": "1196280",
    "end": "1203000"
  },
  {
    "text": "expensive state is handled in the Constructor so in this case we need to like when we launch our replica the",
    "start": "1203000",
    "end": "1208720"
  },
  {
    "text": "first thing we need to do is we need to go and load the model load this embedding model and place it on our Cuda",
    "start": "1208720",
    "end": "1214679"
  },
  {
    "text": "device and then uh the next thing is uh we specify like our endpoint so",
    "start": "1214679",
    "end": "1221520"
  },
  {
    "text": "like this is our encode endpoint where like we'll handle the HTTP request so here we're uh getting a name and we're",
    "start": "1221520",
    "end": "1228240"
  },
  {
    "text": "encoding it back into a vector which is a list of floats uh but one thing I'm showing here",
    "start": "1228240",
    "end": "1233840"
  },
  {
    "text": "is like Dynamic request batching with Ray serve so instead of doing this one name at a time I will dynamically batch",
    "start": "1233840",
    "end": "1241720"
  },
  {
    "text": "up to like 10 names uh and I will log that I'm starting my encoding I will uh",
    "start": "1241720",
    "end": "1247919"
  },
  {
    "text": "perform my encoding and return back vectors so now let's go take a look at my",
    "start": "1247919",
    "end": "1254240"
  },
  {
    "text": "service uh so the the any scale service will uh will show us like our deployment",
    "start": "1254240",
    "end": "1261120"
  },
  {
    "text": "which are embedding model deployment and it will give us um metrics and observability so I'm going to go to the",
    "start": "1261120",
    "end": "1266960"
  },
  {
    "text": "ray dashboard and take a look at my Ray serve um application and I can see like",
    "start": "1266960",
    "end": "1274240"
  },
  {
    "text": "I only have a deployment model with zero replicas right now and on the right hand side I can take a look at my compute",
    "start": "1274240",
    "end": "1280760"
  },
  {
    "text": "resources and I can see that I only have uh a head node running right now with no worker",
    "start": "1280760",
    "end": "1286120"
  },
  {
    "text": "nodes so I'm not making use of any GP use uh because I don't have any requests and this is like when the deployment",
    "start": "1286120",
    "end": "1292799"
  },
  {
    "text": "first started so now I'm going to mimic like sending a request uh to my service",
    "start": "1292799",
    "end": "1299799"
  },
  {
    "text": "uh to do that I will um call my query. py file and I will send in a single",
    "start": "1299799",
    "end": "1306840"
  },
  {
    "text": "request uh so now that I sent out this request uh if I go back here I will see",
    "start": "1306840",
    "end": "1312440"
  },
  {
    "text": "that now all of a sudden I'm upscaling from zero to one replicas and if I go to my logs uh and I",
    "start": "1312440",
    "end": "1319400"
  },
  {
    "text": "look at my service events I'll will see that oh look I'm upscaling from zero to one uh deployment replicas for my",
    "start": "1319400",
    "end": "1326159"
  },
  {
    "text": "embedding model uh G5 2x large and specifically uh in the US West 2A",
    "start": "1326159",
    "end": "1332840"
  },
  {
    "text": "availability Zone and it's a spot instance type and and it it just got",
    "start": "1332840",
    "end": "1338000"
  },
  {
    "text": "launched so here I'm showing that this Dynamic nature of uh the",
    "start": "1338000",
    "end": "1344000"
  },
  {
    "text": "autoscaling and if I go back to my dashboard uh I could see that my replica",
    "start": "1344000",
    "end": "1349200"
  },
  {
    "text": "is starting so uh what does that mean uh for the replica to start the node has to",
    "start": "1349200",
    "end": "1354440"
  },
  {
    "text": "be available and then once it starts um it needs to go and download the model uh",
    "start": "1354440",
    "end": "1361960"
  },
  {
    "text": "and then put the model on the GPU and then uh execute the request so the first thing is that we'll",
    "start": "1361960",
    "end": "1368559"
  },
  {
    "text": "see that the node is upscaling and anytime now that we'll start seeing some logs coming coming",
    "start": "1368559",
    "end": "1374480"
  },
  {
    "text": "through from the replica so here I start seeing my logs I'm initializing my replica and",
    "start": "1374480",
    "end": "1382240"
  },
  {
    "text": "downloading my model and now that I've downloaded the",
    "start": "1382240",
    "end": "1389400"
  },
  {
    "text": "model the tokenizer some configs uh I everything's done and then I finally",
    "start": "1389400",
    "end": "1395200"
  },
  {
    "text": "handle my uh handle my request so now if I go back here I should see that now I",
    "start": "1395200",
    "end": "1400520"
  },
  {
    "text": "finally got back my embedding Vector um",
    "start": "1400520",
    "end": "1405880"
  },
  {
    "text": "so we can send this we can send more requests to this but high level this is",
    "start": "1405880",
    "end": "1411320"
  },
  {
    "text": "the demo that I had in mind for you folks so uh I will pause here and start looking at",
    "start": "1411320",
    "end": "1418320"
  },
  {
    "text": "questions let's see",
    "start": "1427840",
    "end": "1432120"
  },
  {
    "text": "okay uh vak asked what if spot instances goes down so what happens uh if uh if a",
    "start": "1441320",
    "end": "1449039"
  },
  {
    "text": "spot instance uh wants to like get preempted uh this will send this will",
    "start": "1449039",
    "end": "1455440"
  },
  {
    "text": "send a pre preemption notice and then any scale will uh spin up a replica on",
    "start": "1455440",
    "end": "1461520"
  },
  {
    "text": "an on demand instance and uh now the traffic will start going to that on demand instance",
    "start": "1461520",
    "end": "1469320"
  },
  {
    "text": "um I noticed benchmarks missing for open AI there's a cost analysis for it um",
    "start": "1470360",
    "end": "1477440"
  },
  {
    "text": "okay I'm not exactly sure what you mean by that uh",
    "start": "1477440",
    "end": "1484480"
  },
  {
    "text": "we we have some cost analysis and benchmarks on the any scale uh blog I'd encourage you to go check those out",
    "start": "1484480",
    "end": "1493640"
  },
  {
    "text": "do we support observability and model performance yeah maybe this is a section I should showcase here is like if you go",
    "start": "1513000",
    "end": "1518720"
  },
  {
    "text": "to your metrics uh you'll start seeing uh Integrations with things like grafana",
    "start": "1518720",
    "end": "1523880"
  },
  {
    "text": "for observability uh in this case you could see things like your QPS uh your error",
    "start": "1523880",
    "end": "1531440"
  },
  {
    "text": "QPS um and you can get the further like Hardware utilization",
    "start": "1531440",
    "end": "1536960"
  },
  {
    "text": "metrics um that I could also show in",
    "start": "1536960",
    "end": "1543278"
  },
  {
    "text": "here uh things like how many nodes do I have running how many ongoing",
    "start": "1547480",
    "end": "1553679"
  },
  {
    "text": "requests um and if I go back to my rate dashboard I also get further",
    "start": "1555360",
    "end": "1561919"
  },
  {
    "text": "metrics so I could see my GPU memory usage right now uh there's still only",
    "start": "1568600",
    "end": "1574159"
  },
  {
    "text": "running one replica and this sort of shows the fractional usage of our GPU that we're not really using all of the",
    "start": "1574159",
    "end": "1580480"
  },
  {
    "text": "GPU memory that we could add more and more replicas to this",
    "start": "1580480",
    "end": "1586480"
  },
  {
    "text": "uh what is included inray open source versus the any skill service so the any skill Service uh will give you um",
    "start": "1595000",
    "end": "1602240"
  },
  {
    "text": "certain um High availability uh features um and what what that means is",
    "start": "1602240",
    "end": "1609120"
  },
  {
    "text": "like for and and like certain like ways of like production deployment so like let's say you want to do a canary rollup",
    "start": "1609120",
    "end": "1616200"
  },
  {
    "text": "uh then uh like you you've updated you have a new embedding model now and you want to go and deploy that uh you could",
    "start": "1616200",
    "end": "1623880"
  },
  {
    "text": "you could roll this out as a canary and uh switch a certain part of the traffic to that",
    "start": "1623880",
    "end": "1629919"
  },
  {
    "text": "new uh to that new model deployment uh and then only once you've",
    "start": "1629919",
    "end": "1635039"
  },
  {
    "text": "done testing can you switch all the traffic to that new deployment so uh you could do those things easily with any",
    "start": "1635039",
    "end": "1640960"
  },
  {
    "text": "skill Services versus with Ray open source you will have to go and figure out those deployments on your own",
    "start": "1640960",
    "end": "1649720"
  },
  {
    "text": "uh and then you you get versioning so these are all of the versions that I've deployed here uh you could easily roll",
    "start": "1649880",
    "end": "1655880"
  },
  {
    "text": "back to a previous version uh and so you get a lot of these production quality uh",
    "start": "1655880",
    "end": "1662279"
  },
  {
    "text": "features uh with any",
    "start": "1662279",
    "end": "1665640"
  },
  {
    "text": "skill can you use something like uh k for",
    "start": "1673720",
    "end": "1678799"
  },
  {
    "text": "kestra for building offline batch jobs using Ray plus any scale not exactly",
    "start": "1678799",
    "end": "1684200"
  },
  {
    "text": "sure uh but basically you should be able to integrate like with with Ry if you",
    "start": "1684200",
    "end": "1689519"
  },
  {
    "text": "can if you can integrate that code with Ray then you can run this on any scale jobs uh we did not showcase jobs uh as",
    "start": "1689519",
    "end": "1696919"
  },
  {
    "text": "part of this demo but any scale does offer you the ability to run batch jobs",
    "start": "1696919",
    "end": "1702760"
  },
  {
    "text": "um and get certain things like alerting for your jobs uh and automatic fre",
    "start": "1702760",
    "end": "1710320"
  },
  {
    "text": "tries um what about monitoring performance on test",
    "start": "1716840",
    "end": "1722158"
  },
  {
    "text": "data so yeah I maybe by performance like if you mean running evaluations on um",
    "start": "1722640",
    "end": "1730640"
  },
  {
    "text": "for your model then uh you can run these as like batch jobs uh and and integrate",
    "start": "1730640",
    "end": "1736559"
  },
  {
    "text": "with any of your favorite like experim tracking tools or performance tracking",
    "start": "1736559",
    "end": "1742600"
  },
  {
    "text": "tools um how's race serve help support to",
    "start": "1743240",
    "end": "1748399"
  },
  {
    "text": "deploy the models on custom inferencing server runtimes so we we can if if you want to",
    "start": "1748399",
    "end": "1756360"
  },
  {
    "text": "use any custom inference Library um for instance like anything like VM um or",
    "start": "1756360",
    "end": "1763039"
  },
  {
    "text": "Triton uh you could uh easily like integrate that with Ray serve um and as",
    "start": "1763039",
    "end": "1769120"
  },
  {
    "text": "a matter of fact we've done that extensively on any scale so",
    "start": "1769120",
    "end": "1775679"
  },
  {
    "text": "uh you basically the code that I showed you you guys would just change to make use of your favorite uh uh inferencing",
    "start": "1775679",
    "end": "1784320"
  },
  {
    "text": "library for uh large language models let's say",
    "start": "1784320",
    "end": "1789360"
  },
  {
    "text": "uh I'm seeing all this execution a scale can I do this in AWS because we don't have partnership any scale as of now so",
    "start": "1804159",
    "end": "1810080"
  },
  {
    "text": "so to clarify this is running in AWS and and this is running in an AWS Cloud so",
    "start": "1810080",
    "end": "1815240"
  },
  {
    "text": "I'm running in US West 2 uh so this will be if you want to make",
    "start": "1815240",
    "end": "1820880"
  },
  {
    "text": "use of your AWS Cloud uh and compute you can with art deployment model uh this is",
    "start": "1820880",
    "end": "1828159"
  },
  {
    "text": "just giving you the guey the the any skill guy on top with like optimized Ray features for you to run your ray",
    "start": "1828159",
    "end": "1836039"
  },
  {
    "text": "code yeah you will need to have an any scale account uh but we can we can help you get started um if you if you",
    "start": "1854000",
    "end": "1863039"
  },
  {
    "text": "want uh thank you so much for these",
    "start": "1863039",
    "end": "1870559"
  },
  {
    "text": "questions we can keep going a little",
    "start": "1870840",
    "end": "1875158"
  },
  {
    "text": "bit references for running offline rate jobs and its integration pattern yeah I",
    "start": "1882000",
    "end": "1887080"
  },
  {
    "text": "uh maybe one of the panelists can share some references uh for running",
    "start": "1887080",
    "end": "1894638"
  },
  {
    "text": "jobs so how about rid jobs for both batch and streaming I'm not seeing much talks for streaming so with Ray Ray",
    "start": "1908519",
    "end": "1915320"
  },
  {
    "text": "offers a library called gray data uh that enables you to do streaming uh it's",
    "start": "1915320",
    "end": "1920360"
  },
  {
    "text": "it it it's actually streaming execution by default and you can connect it to any of your favorite streaming uh setups uh",
    "start": "1920360",
    "end": "1929559"
  },
  {
    "text": "and uh execute jobs uh using",
    "start": "1929559",
    "end": "1936000"
  },
  {
    "text": "that I can deploy any scale into my VPC similar to the datab break deployment model VPC peering my data plane to your",
    "start": "1936320",
    "end": "1943080"
  },
  {
    "text": "any skill control plane yes exactly Chris that's you you got that exactly right",
    "start": "1943080",
    "end": "1949840"
  },
  {
    "text": "uh unfortunately I don't have a ray data example uh already but we have plenty of",
    "start": "1954799",
    "end": "1960360"
  },
  {
    "text": "examples as either as templates like immediately when you are uh when you get",
    "start": "1960360",
    "end": "1965639"
  },
  {
    "text": "to the uh I can show you that when you immediately get here uh you can get a",
    "start": "1965639",
    "end": "1971320"
  },
  {
    "text": "template for like running batch inference with llms or running batch jobs on any scale uh and this is like a",
    "start": "1971320",
    "end": "1978240"
  },
  {
    "text": "ready template where you can just launch it you'll have the code uh and you can generate you can run choose any of your",
    "start": "1978240",
    "end": "1986919"
  },
  {
    "text": "llms and uh generate data uh like in in",
    "start": "1986919",
    "end": "1992039"
  },
  {
    "text": "batch",
    "start": "1992039",
    "end": "1995039"
  },
  {
    "text": "okay uh how's race serve okay I think we know we've answered",
    "start": "2019960",
    "end": "2026278"
  },
  {
    "text": "this uh I think if there are no more questions",
    "start": "2035080",
    "end": "2040720"
  },
  {
    "text": "we can begin to wrap",
    "start": "2040720",
    "end": "2045158"
  },
  {
    "text": "uh okay thank you so much everyone uh I hope this demo was insightful for you to",
    "start": "2050280",
    "end": "2055320"
  },
  {
    "text": "Showcase how to make use of any skill in AWS uh for achieving scaling and cost",
    "start": "2055320",
    "end": "2060520"
  },
  {
    "text": "efficiency and hope to uh see you in future webinars thank you everyone",
    "start": "2060520",
    "end": "2068158"
  },
  {
    "text": "by",
    "start": "2068159",
    "end": "2071158"
  }
]