[
  {
    "start": "0",
    "end": "67000"
  },
  {
    "text": "uh thank you thank you uh so uh so today I'm going to talk about um a framework",
    "start": "3320",
    "end": "10040"
  },
  {
    "text": "called verel it's a flexible and efficient programming abstractions for irf research and",
    "start": "10040",
    "end": "16080"
  },
  {
    "text": "production so uh here is a brief overview so our first uh discuss some introduction and background about uh",
    "start": "16080",
    "end": "23279"
  },
  {
    "text": "what is irf especially today with larer langage models and uh how it's related",
    "start": "23279",
    "end": "28519"
  },
  {
    "text": "to the traditional RL and uh then I'll move to the motivation of how existing",
    "start": "28519",
    "end": "34200"
  },
  {
    "text": "reinforcement learning infrastructures can solve this problem and when it cannot by uh analyzing some of the",
    "start": "34200",
    "end": "40320"
  },
  {
    "text": "workloads of RF especially when you're dealing with large large language models",
    "start": "40320",
    "end": "45760"
  },
  {
    "text": "and based on those motivation and observations I will introduce our framework uh which is uh uh which is",
    "start": "45760",
    "end": "52640"
  },
  {
    "text": "called hybrid flow it's a it's a core technique used in our framework that combine somehow combines the uh single",
    "start": "52640",
    "end": "59000"
  },
  {
    "text": "controller programming after ction with the mon controller V Ray and I'll show you some programming apis and",
    "start": "59000",
    "end": "65840"
  },
  {
    "text": "examples okay so uh so when we move to the era of large langage models what we",
    "start": "65840",
    "end": "72119"
  },
  {
    "start": "67000",
    "end": "157000"
  },
  {
    "text": "do is that for example the the uh the uh CH gbt so how so it goes through three",
    "start": "72119",
    "end": "79960"
  },
  {
    "text": "process we first do a we first do pre-training that by crawling a bunch of",
    "start": "79960",
    "end": "85479"
  },
  {
    "text": "data from the website and then do paining while the next token prediction",
    "start": "85479",
    "end": "90520"
  },
  {
    "text": "and then we do supervise fine tuning by uh using a lot of prompts and ask a lot",
    "start": "90520",
    "end": "96000"
  },
  {
    "text": "of co-workers to write response and then we just fit the response using the prompt and there's an a last step which",
    "start": "96000",
    "end": "104200"
  },
  {
    "text": "is called the alignment that the goal of alignment is to align align the values",
    "start": "104200",
    "end": "110399"
  },
  {
    "text": "of the output of the large model to the human values so mathematically what does",
    "start": "110399",
    "end": "115880"
  },
  {
    "text": "it mean it means that we want to decrease the probability of generating answers the are incorrect that has",
    "start": "115880",
    "end": "122360"
  },
  {
    "text": "hallucination or it has some harmful to the humans and we want increase uh the",
    "start": "122360",
    "end": "128640"
  },
  {
    "text": "probability of answers that that are most likely Cho chosen by the human beings so here's the uh example that I",
    "start": "128640",
    "end": "136319"
  },
  {
    "text": "just asked chbt yesterday that how to make a bomb and so obviously chbt",
    "start": "136319",
    "end": "141680"
  },
  {
    "text": "shouldn't answer this question and saying that oh here I cannot assist you and know here's why and this is",
    "start": "141680",
    "end": "147319"
  },
  {
    "text": "obviously a model that is aligned with the values cuz I'm pretty sure that in the pre-training there's data on the",
    "start": "147319",
    "end": "153599"
  },
  {
    "text": "website on how to make a bomb okay and uh so so RF is one of the",
    "start": "153599",
    "end": "161760"
  },
  {
    "start": "157000",
    "end": "248000"
  },
  {
    "text": "uh uh approach of alignment so in in order to in order to talk about",
    "start": "161760",
    "end": "167800"
  },
  {
    "text": "alignment we have to talk about uh language model generation as an MTP so in order to use R in order to use RL in",
    "start": "167800",
    "end": "175840"
  },
  {
    "text": "this language model generation we have to model it as an mdp so uh here is another example that I asked chbd to",
    "start": "175840",
    "end": "183360"
  },
  {
    "text": "write a quick sort and here it is uh its response so actually this language model",
    "start": "183360",
    "end": "188920"
  },
  {
    "text": "generation is token by token it can be actually modeled as an mdp so the state",
    "start": "188920",
    "end": "194040"
  },
  {
    "text": "is The Prompt The Prompt is right quick sort and the current generated response it's so the response of the generated by",
    "start": "194040",
    "end": "201159"
  },
  {
    "text": "chbt is not uh appear Suddenly It's actually token by token so the state is",
    "start": "201159",
    "end": "206440"
  },
  {
    "text": "The Prompt plus the current generated response and the action space is simp is",
    "start": "206440",
    "end": "212080"
  },
  {
    "text": "very simple it's just the vocabulary size and the state transition is that we can candate the current state which is",
    "start": "212080",
    "end": "218760"
  },
  {
    "text": "the current promp plus the generated response and the action which is the next token and then this forms to the",
    "start": "218760",
    "end": "224560"
  },
  {
    "text": "next state and the reward in this when we train when we train language model is",
    "start": "224560",
    "end": "229920"
  },
  {
    "text": "actually computed by another language model called the reward model and the terminal state is when the",
    "start": "229920",
    "end": "236959"
  },
  {
    "text": "end of sequence token is generated and then that that's complet and then the chat gbt shows the output to the user to",
    "start": "236959",
    "end": "243760"
  },
  {
    "text": "the end users so this is a little maybe a little bit abstract so I will I will",
    "start": "243760",
    "end": "248959"
  },
  {
    "start": "248000",
    "end": "391000"
  },
  {
    "text": "show you a uh concrete example so so uh here's State we have R quick sort and",
    "start": "248959",
    "end": "256440"
  },
  {
    "text": "then we put it and then we just give it to the larg model it output a prop distribution of the next token and then",
    "start": "256440",
    "end": "263960"
  },
  {
    "text": "we sample from that distribution and the the next sample suppose is here and then we concatenate",
    "start": "263960",
    "end": "270919"
  },
  {
    "text": "the here to the previous state and becomes R quick sort uh here then we",
    "start": "270919",
    "end": "277960"
  },
  {
    "text": "just send it to the large model again and it output the proper dist distribution of the next token and then",
    "start": "277960",
    "end": "283800"
  },
  {
    "text": "we just keep sampling Until the End um the the last token should be the",
    "start": "283800",
    "end": "289680"
  },
  {
    "text": "end of sequence uh token and then this this sampling uh procedure ends so as we",
    "start": "289680",
    "end": "297160"
  },
  {
    "text": "can see that it's actually a trajectory rout using uh RMS and uh so this form the state the",
    "start": "297160",
    "end": "304919"
  },
  {
    "text": "action space the transition probability and uh what we can uh Define of the",
    "start": "304919",
    "end": "310320"
  },
  {
    "text": "reward is that so this is a so this reward is uh defined by this uh 2022",
    "start": "310320",
    "end": "317479"
  },
  {
    "text": "instructive paper written by open so for each of the intermediate State the the",
    "start": "317479",
    "end": "323440"
  },
  {
    "text": "the intermediate reward is the KR di KR Divergence between the current policy which is the that used to Ru out the",
    "start": "323440",
    "end": "330919"
  },
  {
    "text": "trajectory and the and the and the policy that is from the supervis fine tuning which at the beginning so uh so",
    "start": "330919",
    "end": "338479"
  },
  {
    "text": "in order to estimate them by sample so it's a minus beta * the log of the",
    "start": "338479",
    "end": "344840"
  },
  {
    "text": "probability uh difference and for each intermediate step it's this except from the last step",
    "start": "344840",
    "end": "353400"
  },
  {
    "text": "that we will have a reward reward R so so here there's a reward r minus the car",
    "start": "353400",
    "end": "360479"
  },
  {
    "text": "Divergence so this reward R comes from that comes from the reward model so the",
    "start": "360479",
    "end": "366479"
  },
  {
    "text": "r the reward model actually uh output a single Scala R and it's pluged into the",
    "start": "366479",
    "end": "373319"
  },
  {
    "text": "uh uh n of State uh state I'm sorry n of n of sequence State and this reference",
    "start": "373319",
    "end": "381720"
  },
  {
    "text": "and this Pi reference is the model weights with the supervis fine tuning okay so this is the forms the we defined",
    "start": "381720",
    "end": "388599"
  },
  {
    "text": "mdp right right so uh another thing that I want to talk about is the reward model",
    "start": "388599",
    "end": "395199"
  },
  {
    "start": "391000",
    "end": "447000"
  },
  {
    "text": "so reward model so the reward model is basically that saying that there's another large language model and I just",
    "start": "395199",
    "end": "401759"
  },
  {
    "text": "give the whole sentence to the RM and it outp put a single value of saying that",
    "start": "401759",
    "end": "407360"
  },
  {
    "text": "uh what's the score of this sentence and and it's actually trained by using",
    "start": "407360",
    "end": "413160"
  },
  {
    "text": "ranking loss um we just form a lot of different pairs and then send send it to",
    "start": "413160",
    "end": "418680"
  },
  {
    "text": "the model and the train using ranking loss and then the model will output a value saying that okay how humans prefer",
    "start": "418680",
    "end": "425919"
  },
  {
    "text": "this sentence and we want to uh and we want to optimize the trajectory which is",
    "start": "425919",
    "end": "431039"
  },
  {
    "text": "the language model generation such that the reward is maximized so this form a well defined Market decision process",
    "start": "431039",
    "end": "438280"
  },
  {
    "text": "that can be solved by uh any other algorithm such as uh PP which is done by the open are at the at the very",
    "start": "438280",
    "end": "445960"
  },
  {
    "text": "beginning okay so yes so uh as we can see that so lar lar language model",
    "start": "445960",
    "end": "453240"
  },
  {
    "start": "447000",
    "end": "770000"
  },
  {
    "text": "generation can be we can be modeled as a well defined mdp so uh obviously there are some existing RL distributed AR",
    "start": "453240",
    "end": "460120"
  },
  {
    "text": "infrastructures such as R lip that can actually solve this right so uh we what we",
    "start": "460120",
    "end": "465840"
  },
  {
    "text": "just so there's the algorithm P implemented in the r liip and can we directly use it to solve like RF and the",
    "start": "465840",
    "end": "473479"
  },
  {
    "text": "answer is absolutely yes we can do it so here's here's how we do it we just uh uh",
    "start": "473479",
    "end": "479840"
  },
  {
    "text": "so in the traditional classic ARL what we interacting with the environment is called the open G it's an real world",
    "start": "479840",
    "end": "486599"
  },
  {
    "text": "environment and we just need to change it to the auto regressive generation using language models and uh and replace",
    "start": "486599",
    "end": "494759"
  },
  {
    "text": "the mdp interactions with the Real Environment with language language model generation and also the reward function",
    "start": "494759",
    "end": "502400"
  },
  {
    "text": "computed by the model so uh R so language model generation can be modeled as a model based RL so here",
    "start": "502400",
    "end": "510599"
  },
  {
    "text": "yes we so as we can see that we can simply solve this with the existing",
    "start": "510599",
    "end": "515839"
  },
  {
    "text": "infrastructures but uh there are some problems with it so there are some many",
    "start": "515839",
    "end": "521440"
  },
  {
    "text": "challenges when we actually reply uh uh when we actually apply AR in real world",
    "start": "521440",
    "end": "526760"
  },
  {
    "text": "training so uh one of the fundamental challenge is that the model size skills",
    "start": "526760",
    "end": "532760"
  },
  {
    "text": "so for classic RL the model size is typically very small it's it's just a",
    "start": "532760",
    "end": "538519"
  },
  {
    "text": "maybe 1 million parameters or seven K or",
    "start": "538519",
    "end": "543560"
  },
  {
    "text": "hundreds of K parameters but when we talk about large langage model these days is about like Lama 8 B lama",
    "start": "543560",
    "end": "551560"
  },
  {
    "text": "3.28 lama 3.2 90 billion parameters it's huge parameters it cannot even be fit",
    "start": "551560",
    "end": "558079"
  },
  {
    "text": "onto a GPU so uh so this means that the the the this already implemented",
    "start": "558079",
    "end": "565320"
  },
  {
    "text": "distributed data paradism is actually not sufficient to train large models so we have to use some model paradism that",
    "start": "565320",
    "end": "572320"
  },
  {
    "text": "trunk the model parameters into different gpus so uh there are there are several ways existing there such uh the",
    "start": "572320",
    "end": "579680"
  },
  {
    "text": "first one is called tensor paradism and we have pipeline paradism pipeline",
    "start": "579680",
    "end": "585320"
  },
  {
    "text": "paradism uh and we also have the pyth native uh fully shed data parm uh",
    "start": "585320",
    "end": "591399"
  },
  {
    "text": "there's a reference so the tensor paradism and pipeline paradism originate from the Nvidia Megatron armm uh repo",
    "start": "591399",
    "end": "601320"
  },
  {
    "text": "okay so that's the first challenge so the Second Challenge is uh is that",
    "start": "601399",
    "end": "607680"
  },
  {
    "text": "um okay so this the Second Challenge is that uh when we see these uh previous uh",
    "start": "607680",
    "end": "614440"
  },
  {
    "text": "uh so what PB does is that so in the in the Learners and the environment Runners",
    "start": "614440",
    "end": "621040"
  },
  {
    "text": "there is a weight synchronization so uh so the runners and Learners are on different machines and each time the",
    "start": "621040",
    "end": "627600"
  },
  {
    "text": "Learners the runners just uh fetch the lat latest uh weights from the learner",
    "start": "627600",
    "end": "634079"
  },
  {
    "text": "it's is actually very natural when the weights is very small right so it just s send it through the r RPC and then",
    "start": "634079",
    "end": "641399"
  },
  {
    "text": "that's done but this is becomes intractable when the model becomes very very large",
    "start": "641399",
    "end": "646920"
  },
  {
    "text": "so uh this is very slow when the model is getting large so there are some numbers here so for example we if we run",
    "start": "646920",
    "end": "654320"
  },
  {
    "text": "on the uh nvidia's a100 RDM band which is 50 Gab gbes per second and then we",
    "start": "654320",
    "end": "660560"
  },
  {
    "text": "are training lar 4 five Bing parameters so so this takes like one one minute",
    "start": "660560",
    "end": "667480"
  },
  {
    "text": "just to transfer the weights so this is basically unacceptable in a real world",
    "start": "667480",
    "end": "673680"
  },
  {
    "text": "resistance because when you do weight transferring the GPU is basically",
    "start": "673680",
    "end": "679279"
  },
  {
    "text": "idle so the third challenge is that um we actually want want our framework that",
    "start": "679279",
    "end": "684399"
  },
  {
    "text": "is flexible for research and we actually noticed that nowadays people people",
    "start": "684399",
    "end": "689639"
  },
  {
    "text": "usually do uh uh direct direct preference optimization instead of ppos",
    "start": "689639",
    "end": "696200"
  },
  {
    "text": "uh because uh we believe that uh because the dpus is very simple to implement just by taking the sft procedure so we",
    "start": "696200",
    "end": "702880"
  },
  {
    "text": "believe that uh that's actually lack of infrastructures and uh when we building our infrastructures we are considering",
    "start": "702880",
    "end": "709279"
  },
  {
    "text": "that how how can we make our infrastructur infrastructures flexible for research so when we uh analyze the",
    "start": "709279",
    "end": "716959"
  },
  {
    "text": "nature of I research is that uh R research doesn't typically care about neuron net architecture so for classic",
    "start": "716959",
    "end": "723120"
  },
  {
    "text": "RL it's just a convolutional networks with rstm and of nowadays it becomes uh",
    "start": "723120",
    "end": "729360"
  },
  {
    "text": "uh uh Transformers and uh I research more care about IR data flows so so what",
    "start": "729360",
    "end": "736000"
  },
  {
    "text": "is AO data flow is like how do we sample trajectories is on policy or off policy and uh how do we Define loss",
    "start": "736000",
    "end": "743560"
  },
  {
    "text": "function is it a q loss is it policy gradient loss this kind of stuff and",
    "start": "743560",
    "end": "748760"
  },
  {
    "text": "then we want and we take this in mind that we want to feature this uh uh um we",
    "start": "748760",
    "end": "755480"
  },
  {
    "text": "want to want to build our framework that is flexible for this kind of",
    "start": "755480",
    "end": "760880"
  },
  {
    "text": "research uh meaning that new algorithms can be easily implemented and uh our Frameworks have to be uh",
    "start": "760880",
    "end": "768639"
  },
  {
    "text": "modulars okay so uh based on the challenges we we get some observations",
    "start": "768639",
    "end": "775600"
  },
  {
    "start": "770000",
    "end": "987000"
  },
  {
    "text": "and uh we'll introduce our methods so uh there's an there's a very high level",
    "start": "775600",
    "end": "782360"
  },
  {
    "text": "and abstracted uh comparisons between their two programming abstractions which",
    "start": "782360",
    "end": "788279"
  },
  {
    "text": "is called the single controller versus m controller so Ray is one of the greatest",
    "start": "788279",
    "end": "795079"
  },
  {
    "text": "D demo of single controller and uh single controller is also uh proposed by the Google's pathway",
    "start": "795079",
    "end": "803240"
  },
  {
    "text": "in 2022 that is building on their uh passway systems and all those their",
    "start": "803240",
    "end": "809399"
  },
  {
    "text": "gamma is actually trained using this passway systems and uh here I just show",
    "start": "809399",
    "end": "814880"
  },
  {
    "text": "some high level idea what is a single controller and what is the m controller means so in in single controller there",
    "start": "814880",
    "end": "822240"
  },
  {
    "text": "is a central control process and a list of workers so the central control process doesn't do actually computations",
    "start": "822240",
    "end": "828720"
  },
  {
    "text": "and the list and a list of workers do the computations each each one of them each one of them holds a uh",
    "start": "828720",
    "end": "836000"
  },
  {
    "text": "gpus hold a GPU and the Control process do give the workers instructions and data to compute so instead of the so so",
    "start": "836000",
    "end": "843959"
  },
  {
    "text": "the workers just have a set of computation apis just for example forward and backward and Optimizer step",
    "start": "843959",
    "end": "849480"
  },
  {
    "text": "and uh the Control process give them the data and instructions so uh here is the",
    "start": "849480",
    "end": "855759"
  },
  {
    "text": "example so which is actually very similar to the uh I liip so uh suppose I",
    "start": "855759",
    "end": "862199"
  },
  {
    "text": "have a single process controller and I have a bunch of op optimizers and",
    "start": "862199",
    "end": "867800"
  },
  {
    "text": "workers that uh to to to to play with so in order to",
    "start": "867800",
    "end": "873000"
  },
  {
    "text": "do one of the uh uh gradient optimizations what we do is that we first the single controller ERS this uh",
    "start": "873000",
    "end": "881399"
  },
  {
    "text": "give instructions to all of the workers that first set the optimizer gradient to zero so this is written in pyto and then",
    "start": "881399",
    "end": "889560"
  },
  {
    "text": "we trunk the data and then we send this data data sh to each worker and then each worker do does the forward and",
    "start": "889560",
    "end": "896240"
  },
  {
    "text": "backward and uh and the single controller Aggregates the gradients and then the the mo the main model on the",
    "start": "896240",
    "end": "902600"
  },
  {
    "text": "single controller applies the gradients so so that's the single so that's what a",
    "start": "902600",
    "end": "907880"
  },
  {
    "text": "single controller code looks like so the benefit of of this approach is that it's",
    "start": "907880",
    "end": "913639"
  },
  {
    "text": "very flexible because it's just every every program is defined by this single",
    "start": "913639",
    "end": "919120"
  },
  {
    "text": "process controller that I can do anything I don't need distributed things because the distributed things is is",
    "start": "919120",
    "end": "924279"
  },
  {
    "text": "handled by the workers I don't care about this and how how the uh how to",
    "start": "924279",
    "end": "929399"
  },
  {
    "text": "distribut the things of of the workers is done by the P but there's some problem with it is",
    "start": "929399",
    "end": "935920"
  },
  {
    "text": "that we I have to send the data and the com and the instructions to the remote so there's large dispatch",
    "start": "935920",
    "end": "942399"
  },
  {
    "text": "overhead so on the country uh mon what mon controller does is that I have a",
    "start": "942399",
    "end": "947440"
  },
  {
    "text": "list of workers and there's no Central Control process so and each workers execute the same program using different",
    "start": "947440",
    "end": "953120"
  },
  {
    "text": "trunk of the data in a batch like an spmd style and it's very fast but is",
    "start": "953120",
    "end": "959240"
  },
  {
    "text": "less less flexible because we have to each workers have to execute the same",
    "start": "959240",
    "end": "965240"
  },
  {
    "text": "same code with just different trunks of data so here is the example I just on each process what we do is optimize the",
    "start": "965240",
    "end": "972839"
  },
  {
    "text": "zero grad and then forward backward and then what we do and then how do we communicate different between different",
    "start": "972839",
    "end": "979000"
  },
  {
    "text": "workers is V uh nickel is a nvidia's uh uh Collective communication",
    "start": "979000",
    "end": "985880"
  },
  {
    "text": "Library Okay so based on this pre previous two key",
    "start": "985880",
    "end": "992160"
  },
  {
    "start": "987000",
    "end": "1085000"
  },
  {
    "text": "observations what we actually observe is that when we scaling traditional classic",
    "start": "992160",
    "end": "997360"
  },
  {
    "text": "RL to RL HF in which case the central difference is just the model model size",
    "start": "997360",
    "end": "1003160"
  },
  {
    "text": "skills uh significantly and then the and then the data flow problem of classic AR",
    "start": "1003160",
    "end": "1009240"
  },
  {
    "text": "becomes two level so we have the data flow of AR which is which is mainly care",
    "start": "1009240",
    "end": "1015480"
  },
  {
    "text": "about by the AR lip it controls how data is computed and exchanged among different components in the uh in AR",
    "start": "1015480",
    "end": "1023280"
  },
  {
    "text": "systems and then since the model becomes very large and distributed training is necessary and uh there's another level",
    "start": "1023280",
    "end": "1030600"
  },
  {
    "text": "of data flow which is called the commutation workload of the larg language models and uh we actually observe that",
    "start": "1030600",
    "end": "1037959"
  },
  {
    "text": "most open source RM computation workloads are implemented using spmd program which is Monti controller so due",
    "start": "1037959",
    "end": "1044038"
  },
  {
    "text": "to the because due to the uh uh performance performance uh issues",
    "start": "1044039",
    "end": "1050799"
  },
  {
    "text": "okay and uh and uh this workloads actually has three Primitives in in RF",
    "start": "1050799",
    "end": "1057400"
  },
  {
    "text": "uh first autog regressive generation just like what vrm does and SG does is just Auto regressive generate tokens and",
    "start": "1057400",
    "end": "1064679"
  },
  {
    "text": "also RM inference we just have a forward pass and also RM training just give them",
    "start": "1064679",
    "end": "1070679"
  },
  {
    "text": "a batch of data perform forward and then perform backward and then optimize the step so uh each of the each of the",
    "start": "1070679",
    "end": "1077400"
  },
  {
    "text": "Primitives is actually down uh in a distributed fashion okay uh then okay so",
    "start": "1077400",
    "end": "1085960"
  },
  {
    "start": "1085000",
    "end": "1171000"
  },
  {
    "text": "this this comes to our design which is called the hybrid controller so instead of uh so the over idea is that we want",
    "start": "1085960",
    "end": "1093600"
  },
  {
    "text": "to make the ARL data flow simple as as simple as possible and we don't want to uh use multi controller for uh many data",
    "start": "1093600",
    "end": "1101240"
  },
  {
    "text": "many pointto point uh uh more uh uh more to more Communications we want to use a",
    "start": "1101240",
    "end": "1106600"
  },
  {
    "text": "very simple single process to implement the whole RF data flows but we actually",
    "start": "1106600",
    "end": "1111799"
  },
  {
    "text": "want to use mon controller to implement the computation data flows like uh with",
    "start": "1111799",
    "end": "1117559"
  },
  {
    "text": "existing data with existing RM infrastructures such as vrm and pyto",
    "start": "1117559",
    "end": "1123520"
  },
  {
    "text": "fsdp and uh yeah we don't re reinvent the will so the core idea here here is",
    "start": "1123520",
    "end": "1130320"
  },
  {
    "text": "that we so the single controller has to manipulate the computations encapsulated",
    "start": "1130320",
    "end": "1136320"
  },
  {
    "text": "by a multiprocess controller as if it's a single process and this is the core of the the design so the idea is that the",
    "start": "1136320",
    "end": "1144760"
  },
  {
    "text": "computation is done by many many workers but their apis can be but their",
    "start": "1144760",
    "end": "1150960"
  },
  {
    "text": "computations is runs in an spmd fashion that can be encapsulated as if it's a single process and this single process",
    "start": "1150960",
    "end": "1158280"
  },
  {
    "text": "can be controlled by the IR by by the reinforcement data flow and I will show",
    "start": "1158280",
    "end": "1163600"
  },
  {
    "text": "you some examples later so uh and in order to implement that we need a the",
    "start": "1163600",
    "end": "1168799"
  },
  {
    "text": "core dat data structures okay so here comes to the implementations so uh we have to",
    "start": "1168799",
    "end": "1175440"
  },
  {
    "start": "1171000",
    "end": "1403000"
  },
  {
    "text": "introduce a uh a data structure called the worker group and uh in in inside the",
    "start": "1175440",
    "end": "1181679"
  },
  {
    "text": "worker group all the process runs the same program as PMD and expose apis that",
    "start": "1181679",
    "end": "1186799"
  },
  {
    "text": "can be called as if it's a single process so uh I give show you example so suppose I'm I'm writing a lar style FFN",
    "start": "1186799",
    "end": "1195280"
  },
  {
    "text": "a fully connected layer in the Transformers and I have four gpus and",
    "start": "1195280",
    "end": "1200360"
  },
  {
    "text": "I'm running tensor paradism Plus data paradism so there are four ranks and uh",
    "start": "1200360",
    "end": "1206320"
  },
  {
    "text": "I have input batch and then I dispatch so the input batch is on the single",
    "start": "1206320",
    "end": "1211400"
  },
  {
    "text": "controller and what what we does is that I want to chunk the input batch into into four of them and each of them stand",
    "start": "1211400",
    "end": "1218039"
  },
  {
    "text": "to each workers and then this workers uh do some commutations in spmd fashion and",
    "start": "1218039",
    "end": "1224640"
  },
  {
    "text": "then get some get the distributed outputs and then the single control does is it gathers output and form a new",
    "start": "1224640",
    "end": "1232440"
  },
  {
    "text": "batch so so and this is the internally is a worker group so um so from the outside",
    "start": "1232440",
    "end": "1241520"
  },
  {
    "text": "point of view it's it it doesn't really care about what's inside what we does is",
    "start": "1241520",
    "end": "1247440"
  },
  {
    "text": "we give it we give them a batch and we launch a uh some some apis for some",
    "start": "1247440",
    "end": "1252720"
  },
  {
    "text": "commutations that is done distributedly and then we get the output okay so and uh here is a code",
    "start": "1252720",
    "end": "1260240"
  },
  {
    "text": "that that can that can that can do this so we define a class called the FFN",
    "start": "1260240",
    "end": "1266039"
  },
  {
    "text": "workers and then there are uh three layers G it's a it's a llama style uh uh",
    "start": "1266039",
    "end": "1273559"
  },
  {
    "text": "uh f it has gate up and down and then it has infer function which is forward so",
    "start": "1273559",
    "end": "1279880"
  },
  {
    "text": "uh internally it is very very similar to a single uh process and",
    "start": "1279880",
    "end": "1285039"
  },
  {
    "text": "then and then the uh the letter code runs so the code after this runs on the",
    "start": "1285039",
    "end": "1291840"
  },
  {
    "text": "single controller process it's just the one process and the pre and the above and the and the remote code is actually",
    "start": "1291840",
    "end": "1298559"
  },
  {
    "text": "distributed there are four of them running the same code uh inside this re worker group and",
    "start": "1298559",
    "end": "1307720"
  },
  {
    "text": "uh so and so what the infer does is that we give them X and the x is",
    "start": "1307720",
    "end": "1313440"
  },
  {
    "text": "automatically trunk trunked into four parts and each part is sent to uh each",
    "start": "1313440",
    "end": "1318720"
  },
  {
    "text": "worker and the workers do them in a spmd fashion and after this is done uh the",
    "start": "1318720",
    "end": "1324960"
  },
  {
    "text": "answer is uh gathered uh from all this uh workers and",
    "start": "1324960",
    "end": "1330600"
  },
  {
    "text": "then uh aggregated on this single controller okay so um so here is the",
    "start": "1330600",
    "end": "1338840"
  },
  {
    "text": "process of data dispatch and aggregation so uh so the data dispatch and",
    "start": "1338840",
    "end": "1344440"
  },
  {
    "text": "aggregation is performed by the worker group so uh and we actually know all of",
    "start": "1344440",
    "end": "1350600"
  },
  {
    "text": "all of the information such as tensor paradism um uh size and the data parm size of the worker group so uh uh so in",
    "start": "1350600",
    "end": "1360120"
  },
  {
    "text": "large language model training so the data has to replicate among the TP groups and uh trunked in the batch size",
    "start": "1360120",
    "end": "1366880"
  },
  {
    "text": "of the of the of the DP groups and uh here that we can so X is some data on",
    "start": "1366880",
    "end": "1373880"
  },
  {
    "text": "the single controller and uh we just trunk it and then into a into a list and",
    "start": "1373880",
    "end": "1379240"
  },
  {
    "text": "then we just dispatch all of them to the distributed workers and then after that we just collect all of the data from the",
    "start": "1379240",
    "end": "1386080"
  },
  {
    "text": "distribute workers so so the so this idea is that this this",
    "start": "1386080",
    "end": "1392440"
  },
  {
    "text": "distributed commutations can be encapsulated and it can be invoked by a by a single API from the driver I mean",
    "start": "1392440",
    "end": "1399440"
  },
  {
    "text": "from the single process or or the driver okay now based on this idea we",
    "start": "1399440",
    "end": "1406000"
  },
  {
    "start": "1403000",
    "end": "1572000"
  },
  {
    "text": "just build a framework for irf uh it has uh it has two main components",
    "start": "1406000",
    "end": "1413120"
  },
  {
    "text": "the first is that we build a hybrid controller Library upon Ray by introducing some coord data structures",
    "start": "1413120",
    "end": "1419440"
  },
  {
    "text": "called uh workers and a ray worker group and each process is managed by a ray uh",
    "start": "1419440",
    "end": "1424720"
  },
  {
    "text": "remote process and uh different R remote process form a worker group and we use",
    "start": "1424720",
    "end": "1430240"
  },
  {
    "text": "this library to build a different R of components like actors row out critic",
    "start": "1430240",
    "end": "1435960"
  },
  {
    "text": "and uh reward models and I here is the basic apis",
    "start": "1435960",
    "end": "1441440"
  },
  {
    "text": "um so uh let's go through it so uh we have actor it has two it has three APS",
    "start": "1441440",
    "end": "1449440"
  },
  {
    "text": "uh computer log prop update policy and generate sequences and each one of them is actually running in spmd fashion",
    "start": "1449440",
    "end": "1456039"
  },
  {
    "text": "internally and it can be invoked uh uh by it can be invoked by the single",
    "start": "1456039",
    "end": "1462159"
  },
  {
    "text": "process externally okay and we have the same as the reward model and reference",
    "start": "1462159",
    "end": "1467279"
  },
  {
    "text": "policy and also critic okay so the underlying",
    "start": "1467279",
    "end": "1473200"
  },
  {
    "text": "implementation that can that can be fit into this APS is actually depends on the",
    "start": "1473200",
    "end": "1478240"
  },
  {
    "text": "distributed Frameworks for example this uh generation we we actually use the vrm",
    "start": "1478240",
    "end": "1483520"
  },
  {
    "text": "and for training and inference we you can use the maon ARs or uh fsdp P",
    "start": "1483520",
    "end": "1490840"
  },
  {
    "text": "fsdp okay so after building this after building all those computation components in spmd fashion uh we can",
    "start": "1491200",
    "end": "1498559"
  },
  {
    "text": "actually invoke this spmd uh uh invoke those computation components by using a",
    "start": "1498559",
    "end": "1504720"
  },
  {
    "text": "single process controller so what we does here is is is very simple this is done this is run on a single process so",
    "start": "1504720",
    "end": "1511880"
  },
  {
    "text": "what we do is like uh just for the data for each prom batching the data loader and then what we do is we we we just",
    "start": "1511880",
    "end": "1519120"
  },
  {
    "text": "generate a generate a sequence or trajectory results and then we compute the value compute the log of probability",
    "start": "1519120",
    "end": "1525679"
  },
  {
    "text": "compute the reward and then we compute the advantage and then we use the advantage to update the actors and then",
    "start": "1525679",
    "end": "1531360"
  },
  {
    "text": "the critics so this becomes very natural after uh we Define this uh worker",
    "start": "1531360",
    "end": "1538960"
  },
  {
    "text": "groups so uh so so from a single process controller point of view it doesn't",
    "start": "1538960",
    "end": "1544360"
  },
  {
    "text": "actually care about the how data is uh split or replicate the on Earth Hood the",
    "start": "1544360",
    "end": "1550960"
  },
  {
    "text": "the worker group has to uh take care of this B based on the model and data paradism",
    "start": "1550960",
    "end": "1556600"
  },
  {
    "text": "strategies so from from this high level point of view it's just a a single process that uh has has nothing",
    "start": "1556600",
    "end": "1565320"
  },
  {
    "text": "knowledge about the underlying uh data or data and model paradism",
    "start": "1565320",
    "end": "1571440"
  },
  {
    "text": "strategies okay so so next uh we want to highlight one",
    "start": "1571440",
    "end": "1576919"
  },
  {
    "start": "1572000",
    "end": "1921000"
  },
  {
    "text": "of our core design of uh of uh of uh in our HF systems that to to T to to tackle",
    "start": "1576919",
    "end": "1585080"
  },
  {
    "text": "the problems of weight transfer so the weight transfer between the rout and the training is is very heavy uh is very",
    "start": "1585080",
    "end": "1591520"
  },
  {
    "text": "timec consuming so we want to avoid as much as possible so here so here what we",
    "start": "1591520",
    "end": "1598159"
  },
  {
    "text": "do is that we actually want to share the weights of the ru out which is generating those uh sequences and the",
    "start": "1598159",
    "end": "1605760"
  },
  {
    "text": "weights that that is during training so what we do is that um we want to perform",
    "start": "1605760",
    "end": "1613039"
  },
  {
    "text": "a model parameter shotting uh resting during the run time so here example",
    "start": "1613039",
    "end": "1618240"
  },
  {
    "text": "example that we want if we training using fsdp 03 and then we want to like",
    "start": "1618240",
    "end": "1624240"
  },
  {
    "text": "if we do R out in vrm it's actually uh the the parameters is actually uh shed",
    "start": "1624240",
    "end": "1629720"
  },
  {
    "text": "in TP so there must so there has to be a data resorting process here uh and we",
    "start": "1629720",
    "end": "1636039"
  },
  {
    "text": "design actually design very simple apis for this uh we so it has a sharting",
    "start": "1636039",
    "end": "1641320"
  },
  {
    "text": "manager called the fstp to vrm shing manager what it does is um on the hood",
    "start": "1641320",
    "end": "1647240"
  },
  {
    "text": "it it performs weight restarting from the fsp uh uh from from the fsp form to",
    "start": "1647240",
    "end": "1655240"
  },
  {
    "text": "the uh tensor parel form and what it does on the hood is that it's just some pyto D tensor",
    "start": "1655240",
    "end": "1662279"
  },
  {
    "text": "redistribution so by using this simple apis the uh we can perform two different",
    "start": "1662279",
    "end": "1667440"
  },
  {
    "text": "computations using the same set of uh wave",
    "start": "1667440",
    "end": "1672720"
  },
  {
    "text": "parameters okay so uh as just give you some experimental results so by using",
    "start": "1673600",
    "end": "1680559"
  },
  {
    "text": "this apis we can easily Implement a bunch of pp variants such as Remax save RF or uh uh group group related policy",
    "start": "1680559",
    "end": "1690159"
  },
  {
    "text": "optimizations with a few lines of change um so for the grpo and Remax we just",
    "start": "1690159",
    "end": "1695440"
  },
  {
    "text": "simply remove the there's no critic and it estimate advantage using a different way so what",
    "start": "1695440",
    "end": "1702440"
  },
  {
    "text": "it does is that we can simply remove critic and then change how the advantage is computed and for Save RF we it",
    "start": "1702440",
    "end": "1709880"
  },
  {
    "text": "deploys multiple reward models and we can simply um add add reward model worker groups",
    "start": "1709880",
    "end": "1717440"
  },
  {
    "text": "for another set of reward models and then and then call the same API on this",
    "start": "1717440",
    "end": "1722559"
  },
  {
    "text": "on this single process and and then we can combine the rewards in some uh uh in",
    "start": "1722559",
    "end": "1728679"
  },
  {
    "text": "some way that can be uh that is in the research",
    "start": "1728679",
    "end": "1734039"
  },
  {
    "text": "paper Okay so uh uh we also compare the performance that is achieved by um some",
    "start": "1734039",
    "end": "1741519"
  },
  {
    "text": "open source implementations such as deep spe chat and open rhf it shows some uh a",
    "start": "1741519",
    "end": "1747039"
  },
  {
    "text": "great Improvement uh actually this is uh uh actually we we submit a paper to uh",
    "start": "1747039",
    "end": "1752919"
  },
  {
    "text": "our paper is actually accepted by the eurosis 25 and uh here is the archive link if you want to uh have some more uh",
    "start": "1752919",
    "end": "1760440"
  },
  {
    "text": "details and our code base we open source soon so uh it's undergoing the company's",
    "start": "1760440",
    "end": "1766080"
  },
  {
    "text": "internal review and uh uh and I know this topic is a little bit U uh abstract",
    "start": "1766080",
    "end": "1773080"
  },
  {
    "text": "and uh heavy so feel free to ask any",
    "start": "1773080",
    "end": "1778158"
  },
  {
    "text": "questions",
    "start": "1778559",
    "end": "1781559"
  },
  {
    "text": "yeah my question is how do you take care of environments the the where the",
    "start": "1784840",
    "end": "1791200"
  },
  {
    "text": "experimentation being run on let's say if you AR doing a so the so in the rhf",
    "start": "1791200",
    "end": "1796600"
  },
  {
    "text": "the the environment is basically Bally the language model itself so it's",
    "start": "1796600",
    "end": "1801919"
  },
  {
    "text": "generating the yeah so if we look at the very beginning there's a definition of",
    "start": "1801919",
    "end": "1809398"
  },
  {
    "text": "mdp so so uh I think I think my question",
    "start": "1810600",
    "end": "1816000"
  },
  {
    "text": "I think my question is is this I think my question is is this specific to can",
    "start": "1816000",
    "end": "1821240"
  },
  {
    "text": "it be only used for large language model this framework or is it can be generalized to some other use cases too",
    "start": "1821240",
    "end": "1828320"
  },
  {
    "text": "uh yeah it particularly solve uh uh reinforcement learning in larg language models cuz it it just presents a unique",
    "start": "1828320",
    "end": "1836760"
  },
  {
    "text": "challenges it cannot be used in U uh like classic RL G gaming or whatever",
    "start": "1836760",
    "end": "1843320"
  },
  {
    "text": "because the because in gaming the problem is how do we how do we accelerate the environment and uh and",
    "start": "1843320",
    "end": "1849840"
  },
  {
    "text": "the model there is very small and here the challenge is quite the opposite the environment is very simple but the model",
    "start": "1849840",
    "end": "1856039"
  },
  {
    "text": "is huge",
    "start": "1856039",
    "end": "1859240"
  },
  {
    "text": "yeah the later part of the equation cones to zero later part of the equations so so",
    "start": "1868440",
    "end": "1877519"
  },
  {
    "text": "which first part does it z uh uh actually this is a",
    "start": "1881080",
    "end": "1888639"
  },
  {
    "text": "per step reward it's not uh like it's not a equation so this is a pre-step",
    "start": "1888639",
    "end": "1895480"
  },
  {
    "text": "reward function but as it keeps on going from",
    "start": "1895480",
    "end": "1901240"
  },
  {
    "text": "s0 to S does it cones to Z oh this is the state so s0 is the uh",
    "start": "1901240",
    "end": "1909240"
  },
  {
    "text": "is the the current output from the language model it's a it's it's",
    "start": "1909240",
    "end": "1916480"
  },
  {
    "text": "output it's it's not it's not a number",
    "start": "1916480",
    "end": "1921600"
  }
]