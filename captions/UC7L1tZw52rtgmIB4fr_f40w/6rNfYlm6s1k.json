[
  {
    "text": "right so today uh we're going to talk",
    "start": "3560",
    "end": "5720"
  },
  {
    "text": "about uh how you can actually run",
    "start": "5720",
    "end": "8080"
  },
  {
    "text": "lowcost high performance uh training and",
    "start": "8080",
    "end": "10759"
  },
  {
    "text": "inference workloads on Amazon eks and uh",
    "start": "10759",
    "end": "13519"
  },
  {
    "text": "and the first thing we're going to touch",
    "start": "13519",
    "end": "14839"
  },
  {
    "text": "upon AWS neuron capabilities and then we",
    "start": "14839",
    "end": "18480"
  },
  {
    "text": "will delve into what it takes to",
    "start": "18480",
    "end": "20119"
  },
  {
    "text": "actually run these workloads on Amazon",
    "start": "20119",
    "end": "22199"
  },
  {
    "text": "eks and what components are required in",
    "start": "22199",
    "end": "24599"
  },
  {
    "text": "terms of plugins and infrastructure and",
    "start": "24599",
    "end": "27039"
  },
  {
    "text": "then we also delve into an end to endend",
    "start": "27039",
    "end": "29840"
  },
  {
    "text": "uh a model like llama 38b model",
    "start": "29840",
    "end": "32398"
  },
  {
    "text": "deploying on infer stances uh which is",
    "start": "32399",
    "end": "35520"
  },
  {
    "text": "comparable with the gpus and we also",
    "start": "35520",
    "end": "38160"
  },
  {
    "text": "have a small demo to show you how you",
    "start": "38160",
    "end": "39640"
  },
  {
    "text": "can actually scale these inference",
    "start": "39640",
    "end": "41120"
  },
  {
    "text": "workloads across multiple nodes using",
    "start": "41120",
    "end": "43480"
  },
  {
    "text": "Ray and aw's",
    "start": "43480",
    "end": "46199"
  },
  {
    "text": "neuron right so at AWS and this is one",
    "start": "46199",
    "end": "49640"
  },
  {
    "text": "thing that we consistently hear from the",
    "start": "49640",
    "end": "51719"
  },
  {
    "text": "customers uh the biggest concern is",
    "start": "51719",
    "end": "54719"
  },
  {
    "text": "around uh how do we actually reduce the",
    "start": "54719",
    "end": "57760"
  },
  {
    "text": "cost for the raising you know AI",
    "start": "57760",
    "end": "60879"
  },
  {
    "text": "workload demand in terms of both",
    "start": "60879",
    "end": "62440"
  },
  {
    "text": "training and influential um and the",
    "start": "62440",
    "end": "65478"
  },
  {
    "text": "second thing is about you know how to",
    "start": "65479",
    "end": "69119"
  },
  {
    "text": "actually overcome the challenge around",
    "start": "69119",
    "end": "71240"
  },
  {
    "text": "the scarcity of the gpus so with that",
    "start": "71240",
    "end": "74119"
  },
  {
    "text": "AWS actually built AWS silicon which is",
    "start": "74119",
    "end": "78320"
  },
  {
    "text": "which allows you to actually run uh",
    "start": "78320",
    "end": "80479"
  },
  {
    "text": "lowcost high performance uh training and",
    "start": "80479",
    "end": "83200"
  },
  {
    "text": "inference workloads on you know eks",
    "start": "83200",
    "end": "86479"
  },
  {
    "text": "Hemer or you know various other",
    "start": "86479",
    "end": "88479"
  },
  {
    "text": "platforms so starting with AWS tranium",
    "start": "88479",
    "end": "92640"
  },
  {
    "text": "so we launched the AWS tranium",
    "start": "92640",
    "end": "95320"
  },
  {
    "text": "specifically for running the training",
    "start": "95320",
    "end": "97439"
  },
  {
    "text": "workloads and this provides 50% cost",
    "start": "97439",
    "end": "100880"
  },
  {
    "text": "savings with a comparable ec2 instances",
    "start": "100880",
    "end": "104560"
  },
  {
    "text": "uh which is mainly designed for running",
    "start": "104560",
    "end": "106799"
  },
  {
    "text": "high performance uh training workloads",
    "start": "106799",
    "end": "110520"
  },
  {
    "text": "moving on to AWS inferentia 2 uh this is",
    "start": "110520",
    "end": "113560"
  },
  {
    "text": "mainly designed for running your large",
    "start": "113560",
    "end": "115560"
  },
  {
    "text": "language model inference and which",
    "start": "115560",
    "end": "118360"
  },
  {
    "text": "actually saves cost around 4 % with the",
    "start": "118360",
    "end": "121119"
  },
  {
    "text": "comparable ac2",
    "start": "121119",
    "end": "122560"
  },
  {
    "text": "instances and then the finally AWS",
    "start": "122560",
    "end": "125520"
  },
  {
    "text": "influencia uh which offers up to 70%",
    "start": "125520",
    "end": "128920"
  },
  {
    "text": "lower cost per inference uh compared to",
    "start": "128920",
    "end": "131599"
  },
  {
    "text": "the E other uh ec2 instances so um this",
    "start": "131599",
    "end": "136239"
  },
  {
    "text": "is mainly used for the Deep learning",
    "start": "136239",
    "end": "137840"
  },
  {
    "text": "models and a lot of our customers are",
    "start": "137840",
    "end": "140000"
  },
  {
    "text": "today are leveraging both ranium and",
    "start": "140000",
    "end": "141480"
  },
  {
    "text": "Frontier to run their workloads on",
    "start": "141480",
    "end": "143319"
  },
  {
    "text": "Amazon",
    "start": "143319",
    "end": "144599"
  },
  {
    "text": "eeks so let's talk about a little bit",
    "start": "144599",
    "end": "147200"
  },
  {
    "text": "what it takes to run these training and",
    "start": "147200",
    "end": "149280"
  },
  {
    "text": "inference work on eks right so when it",
    "start": "149280",
    "end": "151800"
  },
  {
    "text": "comes to the kubernetes you start to",
    "start": "151800",
    "end": "153400"
  },
  {
    "text": "think about you know it's too complex to",
    "start": "153400",
    "end": "155040"
  },
  {
    "text": "run these workloads but it's not the",
    "start": "155040",
    "end": "156800"
  },
  {
    "text": "case trust me on that so all that you",
    "start": "156800",
    "end": "159360"
  },
  {
    "text": "need with it deploying the Amazon eks",
    "start": "159360",
    "end": "162200"
  },
  {
    "text": "it's a kubernetes uh platform it's a",
    "start": "162200",
    "end": "164879"
  },
  {
    "text": "fully managed control plane more",
    "start": "164879",
    "end": "166879"
  },
  {
    "text": "efficient for running your training and",
    "start": "166879",
    "end": "168239"
  },
  {
    "text": "inference workloads and in terms of",
    "start": "168239",
    "end": "170640"
  },
  {
    "text": "scaling the workloads actual workloads",
    "start": "170640",
    "end": "173000"
  },
  {
    "text": "instances you can use Autos scaling like",
    "start": "173000",
    "end": "175440"
  },
  {
    "text": "Carpenter uh allows you to scale your",
    "start": "175440",
    "end": "177959"
  },
  {
    "text": "nodes and Layer Two",
    "start": "177959",
    "end": "180400"
  },
  {
    "text": "to deploy your workloads on kubernetes",
    "start": "180400",
    "end": "183120"
  },
  {
    "text": "and we highly recommend using Ray and",
    "start": "183120",
    "end": "185519"
  },
  {
    "text": "using Cube operator you can define a",
    "start": "185519",
    "end": "187799"
  },
  {
    "text": "small yaml and then you can deploy your",
    "start": "187799",
    "end": "190879"
  },
  {
    "text": "both distributed training and",
    "start": "190879",
    "end": "192040"
  },
  {
    "text": "distributed inference workloads using",
    "start": "192040",
    "end": "194040"
  },
  {
    "text": "Ray yaml and then in addition to that",
    "start": "194040",
    "end": "197400"
  },
  {
    "text": "using the VM which is most popular in",
    "start": "197400",
    "end": "200400"
  },
  {
    "text": "terms of batch inferencing for the high",
    "start": "200400",
    "end": "202280"
  },
  {
    "text": "performance and low uh so uh using VM",
    "start": "202280",
    "end": "207120"
  },
  {
    "text": "you can leverage uh your inference uh",
    "start": "207120",
    "end": "209799"
  },
  {
    "text": "much faster and to run to work with AWS",
    "start": "209799",
    "end": "213040"
  },
  {
    "text": "neuron inferentia instances we have",
    "start": "213040",
    "end": "215040"
  },
  {
    "text": "neuron SDK which is a software toolkit",
    "start": "215040",
    "end": "218080"
  },
  {
    "text": "helps you to actually work with both aw",
    "start": "218080",
    "end": "220920"
  },
  {
    "text": "strum and infront",
    "start": "220920",
    "end": "222920"
  },
  {
    "text": "instances and one layer down we have you",
    "start": "222920",
    "end": "226439"
  },
  {
    "text": "can run these workloads using gpus today",
    "start": "226439",
    "end": "229080"
  },
  {
    "text": "and you can run the both training and",
    "start": "229080",
    "end": "231120"
  },
  {
    "text": "inference workloads using trainum infer",
    "start": "231120",
    "end": "232920"
  },
  {
    "text": "as well but to run these workloads",
    "start": "232920",
    "end": "235879"
  },
  {
    "text": "specific to the train and frener you",
    "start": "235879",
    "end": "237840"
  },
  {
    "text": "need couple of plugins and to start with",
    "start": "237840",
    "end": "240040"
  },
  {
    "text": "neuron device plugin uh it's similar to",
    "start": "240040",
    "end": "242200"
  },
  {
    "text": "the kubernetes device plugin uh specific",
    "start": "242200",
    "end": "244720"
  },
  {
    "text": "to gpus and the neuron device plugin",
    "start": "244720",
    "end": "248079"
  },
  {
    "text": "allows you to uh expose your uh neuron",
    "start": "248079",
    "end": "251959"
  },
  {
    "text": "CES to kubernetes API server and and so",
    "start": "251959",
    "end": "255400"
  },
  {
    "text": "that your plus ports can be placed onto",
    "start": "255400",
    "end": "257720"
  },
  {
    "text": "either tranium or infan instances and",
    "start": "257720",
    "end": "260079"
  },
  {
    "text": "the second one is a neuron scheduler uh",
    "start": "260079",
    "end": "262199"
  },
  {
    "text": "which is mainly designed it's an",
    "start": "262199",
    "end": "263560"
  },
  {
    "text": "extension to the kubernetes Schuler uh",
    "start": "263560",
    "end": "265600"
  },
  {
    "text": "mainly designed to place more number of",
    "start": "265600",
    "end": "268280"
  },
  {
    "text": "parts in a larger instance of where you",
    "start": "268280",
    "end": "270720"
  },
  {
    "text": "really want to choose only fewer codes",
    "start": "270720",
    "end": "273039"
  },
  {
    "text": "than the available codes on the ec2",
    "start": "273039",
    "end": "275479"
  },
  {
    "text": "instance and the neuron node problem",
    "start": "275479",
    "end": "277880"
  },
  {
    "text": "detector is an X1 which is really cool",
    "start": "277880",
    "end": "279880"
  },
  {
    "text": "in this case uh it actually detects for",
    "start": "279880",
    "end": "282400"
  },
  {
    "text": "the hardware failures within your",
    "start": "282400",
    "end": "283840"
  },
  {
    "text": "trainum INF fren instances and then",
    "start": "283840",
    "end": "286919"
  },
  {
    "text": "restarts and creates uh spin up a new",
    "start": "286919",
    "end": "289039"
  },
  {
    "text": "node to replace your workloads and",
    "start": "289039",
    "end": "291400"
  },
  {
    "text": "finally the neuron monitor uh which",
    "start": "291400",
    "end": "293360"
  },
  {
    "text": "allows you to actually expose all the",
    "start": "293360",
    "end": "295160"
  },
  {
    "text": "metrics to either Prometheus or a",
    "start": "295160",
    "end": "297320"
  },
  {
    "text": "cloudwatch you can look at how the",
    "start": "297320",
    "end": "299080"
  },
  {
    "text": "training",
    "start": "299080",
    "end": "300160"
  },
  {
    "text": "is being used uh in terms of the the",
    "start": "300160",
    "end": "303160"
  },
  {
    "text": "course usage and finally the last one is",
    "start": "303160",
    "end": "306039"
  },
  {
    "text": "elastic fabric adapter especially when",
    "start": "306039",
    "end": "308560"
  },
  {
    "text": "you are thinking about running",
    "start": "308560",
    "end": "309759"
  },
  {
    "text": "distributed inference or a training you",
    "start": "309759",
    "end": "311960"
  },
  {
    "text": "really want um high performance and high",
    "start": "311960",
    "end": "315080"
  },
  {
    "text": "throughput uh which you can achieve by",
    "start": "315080",
    "end": "318039"
  },
  {
    "text": "enabling the EFA and distribute say for",
    "start": "318039",
    "end": "320800"
  },
  {
    "text": "example Lama 3.1 45b model you can't fit",
    "start": "320800",
    "end": "324120"
  },
  {
    "text": "into a single node but you can do the",
    "start": "324120",
    "end": "326039"
  },
  {
    "text": "distributed inference across four nodes",
    "start": "326039",
    "end": "329759"
  },
  {
    "text": "now let's talk about uh how you can",
    "start": "329759",
    "end": "331639"
  },
  {
    "text": "actually deploy llama 38b model uh when",
    "start": "331639",
    "end": "335360"
  },
  {
    "text": "it when you look at the stack it's so",
    "start": "335360",
    "end": "337160"
  },
  {
    "text": "simple uh you have the eks cluster on",
    "start": "337160",
    "end": "340199"
  },
  {
    "text": "the top and we have Carpenter autoscaler",
    "start": "340199",
    "end": "342520"
  },
  {
    "text": "which actually spins up the nodes but to",
    "start": "342520",
    "end": "344880"
  },
  {
    "text": "deploy once you deploy your uh inference",
    "start": "344880",
    "end": "347440"
  },
  {
    "text": "workload say in this case Lama 3B uh",
    "start": "347440",
    "end": "350080"
  },
  {
    "text": "instruct model 8B instruct model and",
    "start": "350080",
    "end": "352600"
  },
  {
    "text": "Cube operator spins up a ray headp pod",
    "start": "352600",
    "end": "355600"
  },
  {
    "text": "and Ray work part pods and when it",
    "start": "355600",
    "end": "358039"
  },
  {
    "text": "requests a ray headp pod as you can see",
    "start": "358039",
    "end": "359680"
  },
  {
    "text": "see it is landing on m5x large it's x86",
    "start": "359680",
    "end": "362120"
  },
  {
    "text": "instance but for the actual model is",
    "start": "362120",
    "end": "364720"
  },
  {
    "text": "deployed on one of the worker part on",
    "start": "364720",
    "end": "367120"
  },
  {
    "text": "INF 28x large so inf2 8X large come with",
    "start": "367120",
    "end": "370599"
  },
  {
    "text": "one neuron device and two neuron cores",
    "start": "370599",
    "end": "373400"
  },
  {
    "text": "which is sufficient to run your Lama 38b",
    "start": "373400",
    "end": "375639"
  },
  {
    "text": "model but then the question comes to the",
    "start": "375639",
    "end": "377880"
  },
  {
    "text": "auto scaling right when you have more",
    "start": "377880",
    "end": "380199"
  },
  {
    "text": "number of inference requests comes can",
    "start": "380199",
    "end": "382319"
  },
  {
    "text": "one instance can handle not necessary it",
    "start": "382319",
    "end": "384919"
  },
  {
    "text": "does but in this case Ray provides this",
    "start": "384919",
    "end": "388160"
  },
  {
    "text": "Autos scaling feature where it can",
    "start": "388160",
    "end": "389599"
  },
  {
    "text": "create multiple replicas so if you have",
    "start": "389599",
    "end": "391880"
  },
  {
    "text": "larger instances it can create the",
    "start": "391880",
    "end": "393720"
  },
  {
    "text": "replicas within the same instance but in",
    "start": "393720",
    "end": "396000"
  },
  {
    "text": "this case we're using 8X large so it",
    "start": "396000",
    "end": "399479"
  },
  {
    "text": "Carpenter automatically scales your",
    "start": "399479",
    "end": "401160"
  },
  {
    "text": "nodes based on your need and both Ray",
    "start": "401160",
    "end": "403880"
  },
  {
    "text": "and Carpenter works together to scale",
    "start": "403880",
    "end": "405479"
  },
  {
    "text": "your",
    "start": "405479",
    "end": "407199"
  },
  {
    "text": "replicas so with that I'm handing over",
    "start": "407199",
    "end": "409479"
  },
  {
    "text": "to ratnam who is going to talk about the",
    "start": "409479",
    "end": "411120"
  },
  {
    "text": "code",
    "start": "411120",
    "end": "412639"
  },
  {
    "text": "Snippets yeah sure so um in this one and",
    "start": "412639",
    "end": "416800"
  },
  {
    "text": "then again this is not the uh by any",
    "start": "416800",
    "end": "418879"
  },
  {
    "text": "means the full code snippet uh for that",
    "start": "418879",
    "end": "421440"
  },
  {
    "text": "we have a slide where you can basically",
    "start": "421440",
    "end": "423800"
  },
  {
    "text": "scan the QR code to get access to the",
    "start": "423800",
    "end": "425560"
  },
  {
    "text": "full good Bas but uh this is a snippet",
    "start": "425560",
    "end": "428520"
  },
  {
    "text": "of the racer vml so it is like as easy",
    "start": "428520",
    "end": "431520"
  },
  {
    "text": "as applying this CML file and there are",
    "start": "431520",
    "end": "434720"
  },
  {
    "text": "three sections highlighted here so the",
    "start": "434720",
    "end": "436520"
  },
  {
    "text": "first one is that um we are defining the",
    "start": "436520",
    "end": "439680"
  },
  {
    "text": "worker uh node group spec under the uh",
    "start": "439680",
    "end": "442520"
  },
  {
    "text": "Ray cluster config so depending on which",
    "start": "442520",
    "end": "444759"
  },
  {
    "text": "version of Ray you are using uh there",
    "start": "444759",
    "end": "446960"
  },
  {
    "text": "might be a slight variation of this but",
    "start": "446960",
    "end": "448800"
  },
  {
    "text": "for the most part",
    "start": "448800",
    "end": "450039"
  },
  {
    "text": "it will be like this only um one",
    "start": "450039",
    "end": "453039"
  },
  {
    "text": "important thing is for the uh horizontal",
    "start": "453039",
    "end": "455720"
  },
  {
    "text": "Autos scaling across nodes you need to",
    "start": "455720",
    "end": "458479"
  },
  {
    "text": "specify the underlying neuron cores",
    "start": "458479",
    "end": "461000"
  },
  {
    "text": "since you are using um in2 8X large",
    "start": "461000",
    "end": "464319"
  },
  {
    "text": "instance in this demo we are uh just",
    "start": "464319",
    "end": "466800"
  },
  {
    "text": "requesting for two neuron Coes and then",
    "start": "466800",
    "end": "470000"
  },
  {
    "text": "right below that we are making use of",
    "start": "470000",
    "end": "472039"
  },
  {
    "text": "the kubernetes uh constructs like node",
    "start": "472039",
    "end": "474639"
  },
  {
    "text": "selector and tolerations to make sure",
    "start": "474639",
    "end": "477720"
  },
  {
    "text": "that the the re worker parts actually",
    "start": "477720",
    "end": "480240"
  },
  {
    "text": "land on the in two instances and then on",
    "start": "480240",
    "end": "483000"
  },
  {
    "text": "the right hand side we do have the um",
    "start": "483000",
    "end": "486599"
  },
  {
    "text": "the bit for the um Auto scaling where in",
    "start": "486599",
    "end": "490440"
  },
  {
    "text": "this demo we are using Carpenter which",
    "start": "490440",
    "end": "492000"
  },
  {
    "text": "is a just in time autoscaler uh",
    "start": "492000",
    "end": "494520"
  },
  {
    "text": "equivalent to the cluster autoscaler so",
    "start": "494520",
    "end": "497520"
  },
  {
    "text": "um by looking at this resource and the",
    "start": "497520",
    "end": "499879"
  },
  {
    "text": "requests Carpenter is able to provision",
    "start": "499879",
    "end": "501960"
  },
  {
    "text": "new nodes for your ray",
    "start": "501960",
    "end": "505360"
  },
  {
    "text": "workloads so um when you are running",
    "start": "506800",
    "end": "509720"
  },
  {
    "text": "like inference workloads you want to",
    "start": "509720",
    "end": "511800"
  },
  {
    "text": "monitor the underlying neuron usage",
    "start": "511800",
    "end": "513959"
  },
  {
    "text": "right so within the neuron SDK there are",
    "start": "513959",
    "end": "516919"
  },
  {
    "text": "some useful tools for you that are",
    "start": "516919",
    "end": "518880"
  },
  {
    "text": "available to help you with that um",
    "start": "518880",
    "end": "521159"
  },
  {
    "text": "neuron LS will uh uh list the all the",
    "start": "521159",
    "end": "524279"
  },
  {
    "text": "devices underlying neuron devices and",
    "start": "524279",
    "end": "526200"
  },
  {
    "text": "then you can also monitor the runtime",
    "start": "526200",
    "end": "527839"
  },
  {
    "text": "utilization using something uh like a",
    "start": "527839",
    "end": "529959"
  },
  {
    "text": "very basic command like neuron",
    "start": "529959",
    "end": "533040"
  },
  {
    "text": "top so with that we will show you a",
    "start": "533040",
    "end": "536440"
  },
  {
    "text": "quick demo",
    "start": "536440",
    "end": "539720"
  },
  {
    "text": "so this is the um website that we have",
    "start": "543040",
    "end": "546360"
  },
  {
    "text": "uh under data on E project again the",
    "start": "546360",
    "end": "548399"
  },
  {
    "text": "link and the QR code will be in the",
    "start": "548399",
    "end": "549959"
  },
  {
    "text": "later slide so here we are housing all",
    "start": "549959",
    "end": "552480"
  },
  {
    "text": "of our inference and training related",
    "start": "552480",
    "end": "555000"
  },
  {
    "text": "patterns uh be it on gpus or or",
    "start": "555000",
    "end": "557399"
  },
  {
    "text": "inferentia so for this particular demo",
    "start": "557399",
    "end": "559880"
  },
  {
    "text": "we are um you know this is the",
    "start": "559880",
    "end": "561880"
  },
  {
    "text": "documentation so we are making use of",
    "start": "561880",
    "end": "563399"
  },
  {
    "text": "the VM with racer and um eks using",
    "start": "563399",
    "end": "567760"
  },
  {
    "text": "Carpenter on neuron so if you go through",
    "start": "567760",
    "end": "570200"
  },
  {
    "text": "this documentation so this is the",
    "start": "570200",
    "end": "571600"
  },
  {
    "text": "architecture that we just uh looked at",
    "start": "571600",
    "end": "574200"
  },
  {
    "text": "and um you know it's it's pretty simple",
    "start": "574200",
    "end": "576560"
  },
  {
    "text": "to deploy this blueprint it's all",
    "start": "576560",
    "end": "578880"
  },
  {
    "text": "terraform based so uh once you deploy",
    "start": "578880",
    "end": "581720"
  },
  {
    "text": "the cluster it not only creates the uh e",
    "start": "581720",
    "end": "584839"
  },
  {
    "text": "cluster but also U adds all of the uh",
    "start": "584839",
    "end": "587480"
  },
  {
    "text": "different uh add-ons that are needed and",
    "start": "587480",
    "end": "590200"
  },
  {
    "text": "this is the observability bit so uh",
    "start": "590200",
    "end": "592600"
  },
  {
    "text": "using Cloud W you can monitor the neuron",
    "start": "592600",
    "end": "594800"
  },
  {
    "text": "usage as well so let's now deploy a ray",
    "start": "594800",
    "end": "598000"
  },
  {
    "text": "cluster so on the terminal one we are",
    "start": "598000",
    "end": "600600"
  },
  {
    "text": "just applying the the racer yaml and",
    "start": "600600",
    "end": "603800"
  },
  {
    "text": "then um once we have done that then it",
    "start": "603800",
    "end": "606920"
  },
  {
    "text": "takes like depending on how much bigger",
    "start": "606920",
    "end": "609640"
  },
  {
    "text": "your container image is it takes a while",
    "start": "609640",
    "end": "611959"
  },
  {
    "text": "to get the rear worker ports up and",
    "start": "611959",
    "end": "613880"
  },
  {
    "text": "running once uh those are up and running",
    "start": "613880",
    "end": "616560"
  },
  {
    "text": "you can see in the terminal to and then",
    "start": "616560",
    "end": "619600"
  },
  {
    "text": "um because VM is compatible with open AI",
    "start": "619600",
    "end": "622320"
  },
  {
    "text": "we are making use of the open AI web UI",
    "start": "622320",
    "end": "625120"
  },
  {
    "text": "deployment and we are doing a port",
    "start": "625120",
    "end": "627519"
  },
  {
    "text": "forward to our local port and once you",
    "start": "627519",
    "end": "630279"
  },
  {
    "text": "do that then uh on the number four",
    "start": "630279",
    "end": "632760"
  },
  {
    "text": "terminal we can uh basically monitor the",
    "start": "632760",
    "end": "635000"
  },
  {
    "text": "neuron",
    "start": "635000",
    "end": "637320"
  },
  {
    "text": "usages",
    "start": "642240",
    "end": "645240"
  },
  {
    "text": "so yeah and um in the next uh so this is",
    "start": "646680",
    "end": "651360"
  },
  {
    "text": "so as you can see the neuron code",
    "start": "651360",
    "end": "653279"
  },
  {
    "text": "utilization right now is at zero because",
    "start": "653279",
    "end": "654920"
  },
  {
    "text": "we are not running an inference workload",
    "start": "654920",
    "end": "656639"
  },
  {
    "text": "now right um on the next screen here",
    "start": "656639",
    "end": "660160"
  },
  {
    "text": "this is the open web UI browser we are",
    "start": "660160",
    "end": "663760"
  },
  {
    "text": "basically selecting the model and um",
    "start": "663760",
    "end": "667120"
  },
  {
    "text": "after that I am writing a prompt here",
    "start": "667120",
    "end": "669880"
  },
  {
    "text": "and basically what's happening behind",
    "start": "669880",
    "end": "671720"
  },
  {
    "text": "the scene is this uh service from open",
    "start": "671720",
    "end": "674360"
  },
  {
    "text": "UI is talking to the underlying Ray",
    "start": "674360",
    "end": "677000"
  },
  {
    "text": "service behind the scenes and then it is",
    "start": "677000",
    "end": "679800"
  },
  {
    "text": "going to the inference endpoint which is",
    "start": "679800",
    "end": "681920"
  },
  {
    "text": "deployed on",
    "start": "681920",
    "end": "683040"
  },
  {
    "text": "eks so once I send the prompt we can see",
    "start": "683040",
    "end": "686200"
  },
  {
    "text": "the streaming output coming back from",
    "start": "686200",
    "end": "687720"
  },
  {
    "text": "the model and if we uh want to look at",
    "start": "687720",
    "end": "690760"
  },
  {
    "text": "the neuron utilization right now we can",
    "start": "690760",
    "end": "694040"
  },
  {
    "text": "see that you know right now as the",
    "start": "694040",
    "end": "696480"
  },
  {
    "text": "inference request is coming in the",
    "start": "696480",
    "end": "698040"
  },
  {
    "text": "neuron utilization is increasing right",
    "start": "698040",
    "end": "700760"
  },
  {
    "text": "and again we are using the INF to 8X",
    "start": "700760",
    "end": "703360"
  },
  {
    "text": "large instance so we have like two cores",
    "start": "703360",
    "end": "705079"
  },
  {
    "text": "and one device but depending on uh your",
    "start": "705079",
    "end": "707920"
  },
  {
    "text": "uh instance type you will see more cores",
    "start": "707920",
    "end": "709959"
  },
  {
    "text": "available",
    "start": "709959",
    "end": "712440"
  },
  {
    "text": "here so now the the output has been",
    "start": "712920",
    "end": "716079"
  },
  {
    "text": "completed and again this is open oi so",
    "start": "716079",
    "end": "718920"
  },
  {
    "text": "you can and uh basically modify",
    "start": "718920",
    "end": "720560"
  },
  {
    "text": "different parameters and you know modify",
    "start": "720560",
    "end": "722920"
  },
  {
    "text": "your prompts so this was a quick demo of",
    "start": "722920",
    "end": "726000"
  },
  {
    "text": "how easy it is to uh put together this",
    "start": "726000",
    "end": "728880"
  },
  {
    "text": "inference architecture using Ray Amazon",
    "start": "728880",
    "end": "731120"
  },
  {
    "text": "eks and VM on neuron thanks yeah thank",
    "start": "731120",
    "end": "735320"
  },
  {
    "text": "you very much everyone so if you want to",
    "start": "735320",
    "end": "736880"
  },
  {
    "text": "get started uh that's the portal that's",
    "start": "736880",
    "end": "739279"
  },
  {
    "text": "a QR code and uh you can you know it's a",
    "start": "739279",
    "end": "743000"
  },
  {
    "text": "few steps to deploy your workloads on",
    "start": "743000",
    "end": "744720"
  },
  {
    "text": "Amazon eks both for neuron and GPS as",
    "start": "744720",
    "end": "747000"
  },
  {
    "text": "well yeah that's it thank you very much",
    "start": "747000",
    "end": "751160"
  }
]