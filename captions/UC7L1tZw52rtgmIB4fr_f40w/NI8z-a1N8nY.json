[
  {
    "text": "uh when the first chat DBT came out near",
    "start": "3540",
    "end": "6299"
  },
  {
    "text": "the end of last year our team at Bento",
    "start": "6299",
    "end": "8400"
  },
  {
    "text": "ammo are all very excited but then we",
    "start": "8400",
    "end": "10620"
  },
  {
    "text": "are got a little bit worried as well",
    "start": "10620",
    "end": "12179"
  },
  {
    "text": "worried about his proprietary nature",
    "start": "12179",
    "end": "14219"
  },
  {
    "text": "such technology is only in the hands of",
    "start": "14219",
    "end": "17160"
  },
  {
    "text": "a few but then we thought to ourselves",
    "start": "17160",
    "end": "19440"
  },
  {
    "text": "if lln follows a path of operating",
    "start": "19440",
    "end": "22020"
  },
  {
    "text": "systems or relational databases there",
    "start": "22020",
    "end": "24720"
  },
  {
    "text": "will be a really chat really compelling",
    "start": "24720",
    "end": "27420"
  },
  {
    "text": "challenges coming from the open source",
    "start": "27420",
    "end": "29340"
  },
  {
    "text": "world coming to challenge the position",
    "start": "29340",
    "end": "31019"
  },
  {
    "text": "of a GPT in just a few years we were",
    "start": "31019",
    "end": "35399"
  },
  {
    "text": "completely wrong it wasn't a few years",
    "start": "35399",
    "end": "37020"
  },
  {
    "text": "so just a few months starting February",
    "start": "37020",
    "end": "39480"
  },
  {
    "text": "when llama was released by meta was",
    "start": "39480",
    "end": "43079"
  },
  {
    "text": "released for just research purposes but",
    "start": "43079",
    "end": "45420"
  },
  {
    "text": "then soon after within a week its ways",
    "start": "45420",
    "end": "48420"
  },
  {
    "text": "got leaked but that turned out to be",
    "start": "48420",
    "end": "50940"
  },
  {
    "text": "just a blessing in disguise it sparked a",
    "start": "50940",
    "end": "54180"
  },
  {
    "text": "series of Innovations like Iconia and",
    "start": "54180",
    "end": "57780"
  },
  {
    "text": "alpaca not only it's a train to follow",
    "start": "57780",
    "end": "60719"
  },
  {
    "text": "instructions but also it's a challenge",
    "start": "60719",
    "end": "63059"
  },
  {
    "text": "getting close closer to the performance",
    "start": "63059",
    "end": "65158"
  },
  {
    "text": "of gpt3 at the time and Lama is not the",
    "start": "65159",
    "end": "68520"
  },
  {
    "text": "only player in the game we also have npt",
    "start": "68520",
    "end": "70860"
  },
  {
    "text": "which is known for its large contact",
    "start": "70860",
    "end": "72659"
  },
  {
    "text": "sizes we also have",
    "start": "72659",
    "end": "75060"
  },
  {
    "text": "uh",
    "start": "75060",
    "end": "76680"
  },
  {
    "text": "um Falcon who briefly took the reign of",
    "start": "76680",
    "end": "79140"
  },
  {
    "text": "llama but then llama 2 release it",
    "start": "79140",
    "end": "82080"
  },
  {
    "text": "changed the game all over again the",
    "start": "82080",
    "end": "83939"
  },
  {
    "text": "cycle just repeats we have Innovation",
    "start": "83939",
    "end": "85500"
  },
  {
    "text": "coming out from llama two and what we",
    "start": "85500",
    "end": "88140"
  },
  {
    "text": "learned from here is that these open",
    "start": "88140",
    "end": "90060"
  },
  {
    "text": "source llms are really powerful and uh",
    "start": "90060",
    "end": "93240"
  },
  {
    "text": "the innovation in this space happens",
    "start": "93240",
    "end": "95280"
  },
  {
    "text": "really fast",
    "start": "95280",
    "end": "96540"
  },
  {
    "text": "so the question",
    "start": "96540",
    "end": "99320"
  },
  {
    "text": "is should you be running your own open",
    "start": "99420",
    "end": "101939"
  },
  {
    "text": "source LL lens and I am Sean I'm from",
    "start": "101939",
    "end": "104880"
  },
  {
    "text": "Bento ml I'm here to talk to you about",
    "start": "104880",
    "end": "107220"
  },
  {
    "text": "how to use bentron Mountain Ray to",
    "start": "107220",
    "end": "109200"
  },
  {
    "text": "productionize your llms open source LM",
    "start": "109200",
    "end": "112560"
  },
  {
    "text": "to be specific and the tape I think the",
    "start": "112560",
    "end": "115140"
  },
  {
    "text": "title itself is already giving away the",
    "start": "115140",
    "end": "117780"
  },
  {
    "text": "answer and yes we do think you should be",
    "start": "117780",
    "end": "120060"
  },
  {
    "text": "running your own open source llms and",
    "start": "120060",
    "end": "122280"
  },
  {
    "text": "for three reasons customizability data",
    "start": "122280",
    "end": "125640"
  },
  {
    "text": "privacy and cost efficiency and let's",
    "start": "125640",
    "end": "128459"
  },
  {
    "text": "address the most important problem in",
    "start": "128459",
    "end": "130800"
  },
  {
    "text": "the room it's the quality of Open Source",
    "start": "130800",
    "end": "133140"
  },
  {
    "text": "llns are they on par with gpt4 and with",
    "start": "133140",
    "end": "136379"
  },
  {
    "text": "the really Recent research done from any",
    "start": "136379",
    "end": "138900"
  },
  {
    "text": "skills the ray conference it suggests",
    "start": "138900",
    "end": "141660"
  },
  {
    "text": "that a llama 70b is actually getting the",
    "start": "141660",
    "end": "144420"
  },
  {
    "text": "factuality of gpt4 and Performing close",
    "start": "144420",
    "end": "147360"
  },
  {
    "text": "or exceeding the human level of",
    "start": "147360",
    "end": "149160"
  },
  {
    "text": "reasoning and far beyond the performance",
    "start": "149160",
    "end": "152580"
  },
  {
    "text": "of GPT 3.5",
    "start": "152580",
    "end": "154920"
  },
  {
    "text": "so with that out of the way uh we know",
    "start": "154920",
    "end": "157319"
  },
  {
    "text": "that our open source albums not only are",
    "start": "157319",
    "end": "159720"
  },
  {
    "text": "closing the Gap if the Gap is not",
    "start": "159720",
    "end": "161640"
  },
  {
    "text": "already closed let's talk about the",
    "start": "161640",
    "end": "163620"
  },
  {
    "text": "specific benefits first of all is",
    "start": "163620",
    "end": "166080"
  },
  {
    "text": "customizability or you can you provide",
    "start": "166080",
    "end": "168480"
  },
  {
    "text": "probably hear the word domain specific",
    "start": "168480",
    "end": "170340"
  },
  {
    "text": "model refinement so llms",
    "start": "170340",
    "end": "173760"
  },
  {
    "text": "are really good at handling domain",
    "start": "173760",
    "end": "175860"
  },
  {
    "text": "specific tasks to achieve this you can",
    "start": "175860",
    "end": "178620"
  },
  {
    "text": "fine tune your pre-train based model",
    "start": "178620",
    "end": "180180"
  },
  {
    "text": "with the data set of your own say for",
    "start": "180180",
    "end": "183120"
  },
  {
    "text": "text summarization you can fine tune it",
    "start": "183120",
    "end": "184980"
  },
  {
    "text": "to in the tone of Harry Potter books or",
    "start": "184980",
    "end": "188040"
  },
  {
    "text": "you you actually don't need a lot of",
    "start": "188040",
    "end": "189420"
  },
  {
    "text": "data to do this and there are techniques",
    "start": "189420",
    "end": "191040"
  },
  {
    "text": "you can only have a few hundred samples",
    "start": "191040",
    "end": "193560"
  },
  {
    "text": "data points you can get pretty quality",
    "start": "193560",
    "end": "195780"
  },
  {
    "text": "results and if you don't feel like",
    "start": "195780",
    "end": "197760"
  },
  {
    "text": "training your own data training your own",
    "start": "197760",
    "end": "199680"
  },
  {
    "text": "model there are plenty of pre-trained",
    "start": "199680",
    "end": "201480"
  },
  {
    "text": "models on hugging face there are over 20",
    "start": "201480",
    "end": "203459"
  },
  {
    "text": "000 pre-trained uh text generation model",
    "start": "203459",
    "end": "206220"
  },
  {
    "text": "they're available it's great growing at",
    "start": "206220",
    "end": "208019"
  },
  {
    "text": "a rapid pace",
    "start": "208019",
    "end": "209459"
  },
  {
    "text": "when should you be fine-tuning your own",
    "start": "209459",
    "end": "211440"
  },
  {
    "text": "model research showed that for tasks",
    "start": "211440",
    "end": "214739"
  },
  {
    "text": "that are required low reasoning",
    "start": "214739",
    "end": "217019"
  },
  {
    "text": "capabilities",
    "start": "217019",
    "end": "218599"
  },
  {
    "text": "fine-tuning actually finding different",
    "start": "218599",
    "end": "220680"
  },
  {
    "text": "tuning smaller parameter size model can",
    "start": "220680",
    "end": "223680"
  },
  {
    "text": "outperform GPT and in the second columns",
    "start": "223680",
    "end": "226620"
  },
  {
    "text": "it's a SQL generation against a specific",
    "start": "226620",
    "end": "229379"
  },
  {
    "text": "database schema SQL is well understood",
    "start": "229379",
    "end": "232220"
  },
  {
    "text": "within allo lamps so therefore",
    "start": "232220",
    "end": "235019"
  },
  {
    "text": "fine-tuned in the darker shade",
    "start": "235019",
    "end": "236519"
  },
  {
    "text": "represents the base model performance",
    "start": "236519",
    "end": "238319"
  },
  {
    "text": "and the lighter Shades represents the",
    "start": "238319",
    "end": "240299"
  },
  {
    "text": "performance after fine-tuning even with",
    "start": "240299",
    "end": "242700"
  },
  {
    "text": "the smaller model parameter sizes of 7",
    "start": "242700",
    "end": "245580"
  },
  {
    "text": "billion and 13 billion it can outperform",
    "start": "245580",
    "end": "247680"
  },
  {
    "text": "gpt4 however gpt4 outperforms the",
    "start": "247680",
    "end": "252180"
  },
  {
    "text": "smaller or the open source models on",
    "start": "252180",
    "end": "254099"
  },
  {
    "text": "tasks that require a lot of reasoning in",
    "start": "254099",
    "end": "257040"
  },
  {
    "text": "the third groups of columns which are",
    "start": "257040",
    "end": "259199"
  },
  {
    "text": "word problem from grade schools and",
    "start": "259199",
    "end": "261600"
  },
  {
    "text": "great GPT for outperform even the 70",
    "start": "261600",
    "end": "264780"
  },
  {
    "text": "billion parameter size of lamma",
    "start": "264780",
    "end": "269100"
  },
  {
    "text": "so with that out of the way we should we",
    "start": "269100",
    "end": "271740"
  },
  {
    "text": "think the first reason we think you",
    "start": "271740",
    "end": "273060"
  },
  {
    "text": "should run open source lamps is the",
    "start": "273060",
    "end": "275940"
  },
  {
    "text": "ability to fine-tuning on domain",
    "start": "275940",
    "end": "277979"
  },
  {
    "text": "specific tasks second is data privacy so",
    "start": "277979",
    "end": "281400"
  },
  {
    "text": "members from our community Franklin come",
    "start": "281400",
    "end": "283320"
  },
  {
    "text": "to us expressed the concern that they're",
    "start": "283320",
    "end": "285240"
  },
  {
    "text": "worried that their data will be used to",
    "start": "285240",
    "end": "287280"
  },
  {
    "text": "train for the next generation of llms",
    "start": "287280",
    "end": "289620"
  },
  {
    "text": "for the third-party model providers in a",
    "start": "289620",
    "end": "291960"
  },
  {
    "text": "way that their data will be leaked as",
    "start": "291960",
    "end": "293759"
  },
  {
    "text": "the model gets released",
    "start": "293759",
    "end": "296520"
  },
  {
    "text": "um so running your own model from uh",
    "start": "296520",
    "end": "300540"
  },
  {
    "text": "your own open source model within your",
    "start": "300540",
    "end": "302699"
  },
  {
    "text": "your virtual private Cloud won't have",
    "start": "302699",
    "end": "305160"
  },
  {
    "text": "this problem because your data will be",
    "start": "305160",
    "end": "307199"
  },
  {
    "text": "strictly contained within your VPC or",
    "start": "307199",
    "end": "309479"
  },
  {
    "text": "never leave your network boundary a lot",
    "start": "309479",
    "end": "311639"
  },
  {
    "text": "of the open source model providers",
    "start": "311639",
    "end": "313259"
  },
  {
    "text": "release their training data so you know",
    "start": "313259",
    "end": "316199"
  },
  {
    "text": "where the data come from you can",
    "start": "316199",
    "end": "317460"
  },
  {
    "text": "transparently pass down to your client",
    "start": "317460",
    "end": "318900"
  },
  {
    "text": "so you're not worried about potentially",
    "start": "318900",
    "end": "320880"
  },
  {
    "text": "leaking or generating copyright",
    "start": "320880",
    "end": "322979"
  },
  {
    "text": "infringing contents",
    "start": "322979",
    "end": "325199"
  },
  {
    "text": "lastly you have",
    "start": "325199",
    "end": "328580"
  }
]