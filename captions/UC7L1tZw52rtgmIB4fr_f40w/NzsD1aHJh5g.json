[
  {
    "text": "I'm Sergey uh very nice to meet you all I work at meta I'm a director of",
    "start": "3480",
    "end": "8639"
  },
  {
    "text": "engineering over there um and I work on llamas right now I'm leading the Lama 4",
    "start": "8639",
    "end": "14400"
  },
  {
    "text": "pre-training team prior to that I was leading Lama 2 and Lama 3 teams and",
    "start": "14400",
    "end": "20000"
  },
  {
    "text": "before all of this um gen um Revolution",
    "start": "20000",
    "end": "25080"
  },
  {
    "text": "started I was working on machine translation and speech so quite a bit of a journey um I'm a researcher and",
    "start": "25080",
    "end": "31679"
  },
  {
    "text": "engineer in at heart um and I don't particularly use those models so I know",
    "start": "31679",
    "end": "36719"
  },
  {
    "text": "a lot of people in this room probably have way more experience than I have in using these models um I I only train",
    "start": "36719",
    "end": "43840"
  },
  {
    "text": "them um but so when I was thinking about what should I talk to about to this audience",
    "start": "43840",
    "end": "51719"
  },
  {
    "text": "um I decided I'll share the story of building llamas um and some challenges",
    "start": "51719",
    "end": "57079"
  },
  {
    "text": "what we have and invent how you can help uh build better llamas in the future and",
    "start": "57079",
    "end": "62920"
  },
  {
    "text": "tomorrow there will be another talk from joose pisak who is actually a product director on our team uh and he will be",
    "start": "62920",
    "end": "69400"
  },
  {
    "text": "able to cover the real practical use cases of those models so if you have questions about how to use those models",
    "start": "69400",
    "end": "75640"
  },
  {
    "text": "Joe would be the right guy to answer those okay let me get started so the",
    "start": "75640",
    "end": "81119"
  },
  {
    "text": "evolution of llama it all started in February last year when we released llama one um which which was an",
    "start": "81119",
    "end": "88920"
  },
  {
    "text": "interesting release uh let me uh share why so first of all it was a researchon",
    "start": "88920",
    "end": "94399"
  },
  {
    "text": "model uh it was released with research only license meaning you Cann not build commercial applications on top of that",
    "start": "94399",
    "end": "101720"
  },
  {
    "text": "and it was not even built to be an llm meaning the team that was building this",
    "start": "101720",
    "end": "107079"
  },
  {
    "text": "model their main goal was not necessarily building the state of art Alm it was kind of a byproduct of the",
    "start": "107079",
    "end": "113000"
  },
  {
    "text": "other project that they were doing but we released it and it was great um people really liked it but when we",
    "start": "113000",
    "end": "120240"
  },
  {
    "text": "released four model sizes it was trained on 1.4 trillion tokens which sounds very",
    "start": "120240",
    "end": "125840"
  },
  {
    "text": "small these days back when it seemed pretty cool um and the context L of that",
    "start": "125840",
    "end": "130879"
  },
  {
    "text": "model was just 2,000 tokens oh and it was only pre-trained model so like",
    "start": "130879",
    "end": "136480"
  },
  {
    "text": "really really Bare Bones pretrain mean you can't really like talk to that model",
    "start": "136480",
    "end": "141519"
  },
  {
    "text": "um it it just it's only able to continue whatever sequence you input in it so we",
    "start": "141519",
    "end": "147560"
  },
  {
    "text": "released that model and it kind of loed so that's the picture from a paper",
    "start": "147560",
    "end": "153080"
  },
  {
    "text": "published in April last year so just two or three months down the road um and",
    "start": "153080",
    "end": "158280"
  },
  {
    "text": "they put a nice footnote here like hey due to a huge number of use cases of that model we just cannot put them all",
    "start": "158280",
    "end": "164640"
  },
  {
    "text": "on the same picture so people took llama U the original llama and we started using it for everything for medical use",
    "start": "164640",
    "end": "172239"
  },
  {
    "text": "cases for legal use cases for finance for Education people did multimodal",
    "start": "172239",
    "end": "177400"
  },
  {
    "text": "adaptations we figured out how to feed images into those models um and of course we did multilingual so the number",
    "start": "177400",
    "end": "183959"
  },
  {
    "text": "of use cases really really exploded at the time and then on our side we were looking into those beautiful curves so",
    "start": "183959",
    "end": "192400"
  },
  {
    "text": "that that's what we call training curves uh we show training lws uh on the Y AIS",
    "start": "192400",
    "end": "198560"
  },
  {
    "text": "versus uh the number of tokens you train on on the x-axis and typically the lower",
    "start": "198560",
    "end": "204159"
  },
  {
    "text": "is the better so what we see when we look at those curves is like hey you make the model bigger and it gets better",
    "start": "204159",
    "end": "211560"
  },
  {
    "text": "and you train it for longer and it also kind of gets better so those were the two very important access of",
    "start": "211560",
    "end": "217680"
  },
  {
    "text": "improvements that we immediately realized when we trained those four models and and released them and and and",
    "start": "217680",
    "end": "223120"
  },
  {
    "text": "we started to think hey how can we get them even better so the first follow was",
    "start": "223120",
    "end": "229120"
  },
  {
    "text": "Lama 2 which we released in July of the same year um and that model had some",
    "start": "229120",
    "end": "234840"
  },
  {
    "text": "substantial changes the most important one it was commercially available it was not research only and what it",
    "start": "234840",
    "end": "241640"
  },
  {
    "text": "practically meant when it happened is that we had to revisit the entire training data that we used to train this",
    "start": "241640",
    "end": "247159"
  },
  {
    "text": "model um and we actually ended up with more training data we trained it on two trillion tokens uh with uh 4,000 uh",
    "start": "247159",
    "end": "254480"
  },
  {
    "text": "contacts length and another most important thing is that we did not just release pre-trained model uh we also",
    "start": "254480",
    "end": "260680"
  },
  {
    "text": "released instruction fune model meaning the model you can actually speak to uh and have a meaningful conversation with",
    "start": "260680",
    "end": "268560"
  },
  {
    "text": "but when we looked at those Cur C again the curs I've showed you before but for now for for Lama 2 um and they show",
    "start": "268560",
    "end": "275720"
  },
  {
    "text": "exactly the same Trend they keep improving as you make the model bigger and they keep improving as you keep",
    "start": "275720",
    "end": "281240"
  },
  {
    "text": "training for longer and there is no sign of saturation here so it looks like you can just keep scaling and it's just",
    "start": "281240",
    "end": "286759"
  },
  {
    "text": "going to work um we did make the a lot of improvements to not improvements",
    "start": "286759",
    "end": "293320"
  },
  {
    "text": "advancements in our um training pipeline for those models for for Lama 2 um like",
    "start": "293320",
    "end": "299800"
  },
  {
    "text": "again for Lama one it was just a pre-trained model so whatever you see at the left bottom corner that's",
    "start": "299800",
    "end": "305800"
  },
  {
    "text": "pre-training that's kind of the dumb stage of the training of those models um",
    "start": "305800",
    "end": "311360"
  },
  {
    "text": "the the objective here is very simple you just tell the model to predict the next token in the sequence and you train",
    "start": "311360",
    "end": "317479"
  },
  {
    "text": "it on that and and you feed Gobles of data into that uh and you need a lot of gpus to do that so it's it's really",
    "start": "317479",
    "end": "324039"
  },
  {
    "text": "difficult engineering challenge but the optimization objective is quite simple however again the model that you get is",
    "start": "324039",
    "end": "330639"
  },
  {
    "text": "not really a chat model so to make it a chat model you need to align or fine-tune it um and and that's the next",
    "start": "330639",
    "end": "336400"
  },
  {
    "text": "stage that is shown on this picture um and and and it's broken down into multiple substages uh you first need to",
    "start": "336400",
    "end": "344080"
  },
  {
    "text": "feed a bunch of examples into the model of what you would expect it to answer if you ask it a question and then you want",
    "start": "344080",
    "end": "351280"
  },
  {
    "text": "to keep improving and iterating um like basically you ask this model to generate a bunch of examples of answers you ask",
    "start": "351280",
    "end": "358639"
  },
  {
    "text": "humans to rate those answers um you use those ratings to uh improve your model",
    "start": "358639",
    "end": "364759"
  },
  {
    "text": "and and you keep iterating um this process and that's where we found another scaling access for those models",
    "start": "364759",
    "end": "372000"
  },
  {
    "text": "so on this chart we show uh our subsequent iterations of rhf this is our",
    "start": "372000",
    "end": "378280"
  },
  {
    "text": "like model alignment fine tuning procedure and the more iterations we do",
    "start": "378280",
    "end": "383440"
  },
  {
    "text": "the model keeps just keeps getting better so that was also quite exciting to see and it's getting better across",
    "start": "383440",
    "end": "389160"
  },
  {
    "text": "multiple Dimension it it's becoming more helpful it's becoming um more safe so it",
    "start": "389160",
    "end": "395160"
  },
  {
    "text": "it just felt good to get there um so let's skip one year um this year we",
    "start": "395160",
    "end": "402560"
  },
  {
    "text": "actually did another major release we released Lama free it started in April um so in April we released 8B and 70b",
    "start": "402560",
    "end": "410000"
  },
  {
    "text": "70b models and then in July we released our Flagship the biggest model that we",
    "start": "410000",
    "end": "415240"
  },
  {
    "text": "have for a 5B and just a week ago we released uh our tiny baby llamas 1B and",
    "start": "415240",
    "end": "421039"
  },
  {
    "text": "3B but you can run on on device uh and we also released multimodel adapters for",
    "start": "421039",
    "end": "426360"
  },
  {
    "text": "those models so again uh just a quick recap here same commercially available",
    "start": "426360",
    "end": "433000"
  },
  {
    "text": "for for for for practical applications this time trained on 15 trillion tokens",
    "start": "433000",
    "end": "438759"
  },
  {
    "text": "of data that's an enormous amount of data uh that we had to collect uh to make it work and we also expanded the",
    "start": "438759",
    "end": "446599"
  },
  {
    "text": "the context size of those models making them useful for for um working with long",
    "start": "446599",
    "end": "451680"
  },
  {
    "text": "documents so uh again U I already talked about it so um two model sizes released",
    "start": "451680",
    "end": "458039"
  },
  {
    "text": "in April 8B 7B 70b 128k context length we added multilingual support uh for",
    "start": "458039",
    "end": "465280"
  },
  {
    "text": "this models um back with summer and we improved the tool use for them as well",
    "start": "465280",
    "end": "471400"
  },
  {
    "text": "and 405b is is definitely model of a class of its own um it's very",
    "start": "471400",
    "end": "476560"
  },
  {
    "text": "competitive uh let let me get uh to that the next slide but it's pretty big I'll",
    "start": "476560",
    "end": "483479"
  },
  {
    "text": "be honest just running it on your laptop is probably not really an option um just to get it running you and we released",
    "start": "483479",
    "end": "491240"
  },
  {
    "text": "quantized version that make it easier to ship this model but even with quantized version you need 8 h100 gpus which is a",
    "start": "491240",
    "end": "498520"
  },
  {
    "text": "lot uh just to run inference on this model so we envision these models to be",
    "start": "498520",
    "end": "503639"
  },
  {
    "text": "used for synthetic data generation F tuning and distillation for for any practical use cases and and and yes it",
    "start": "503639",
    "end": "510199"
  },
  {
    "text": "shows pretty good performance numbers we measured it on a lot of benchmarks like 150 benchmarks were all in our papers um",
    "start": "510199",
    "end": "518080"
  },
  {
    "text": "and and and it's pretty competitive of course it's better on some benchmarks uh worse than on other benchmarks it's",
    "start": "518080",
    "end": "524120"
  },
  {
    "text": "pretty competitive with other uh State ofart um llms including close Source ones and then of course we did human",
    "start": "524120",
    "end": "530720"
  },
  {
    "text": "evals um to make sure we are not just cheating on benchmarks um and it's also",
    "start": "530720",
    "end": "535920"
  },
  {
    "text": "pretty competitive overwar um it's pretty comparable with GPT 4 it's pretty",
    "start": "535920",
    "end": "541240"
  },
  {
    "text": "comparable with uh clo uh 3.5 Sonet it's a little bit behind on compared to GPT",
    "start": "541240",
    "end": "547399"
  },
  {
    "text": "for all now the topic of this talk was like hey what does it even take to train",
    "start": "547399",
    "end": "552480"
  },
  {
    "text": "those models so what does it take to train llama well as I mentioned I work on pre-training so that that's kind of",
    "start": "552480",
    "end": "558880"
  },
  {
    "text": "the most painful part for me uh close to my heart so I'll talk a little bit about pre-training well conceptually as I said",
    "start": "558880",
    "end": "566399"
  },
  {
    "text": "pre-training is very simple right you you have a very simple objective just hey model go ahead and predict the next",
    "start": "566399",
    "end": "572600"
  },
  {
    "text": "token in in sequence it's it's very very simple um but if you look what is",
    "start": "572600",
    "end": "578040"
  },
  {
    "text": "actually happening under the hood here it's pretty complex so the first paragraph here is a quote from our paper",
    "start": "578040",
    "end": "585399"
  },
  {
    "text": "and it basically describes how we do the initial stage of pre-training and it's",
    "start": "585399",
    "end": "590640"
  },
  {
    "text": "like hey yeah start with P learning rate do that number of steps when increased learning rate when a new learning rate",
    "start": "590640",
    "end": "596720"
  },
  {
    "text": "that number of steps on by the way you need to start with small badge size to make it stable and then do bigger batch",
    "start": "596720",
    "end": "602240"
  },
  {
    "text": "size so that you're efficient all of that stuff yeah the good thing is that we found that this recipe is stable",
    "start": "602240",
    "end": "609920"
  },
  {
    "text": "um but like just figuring out how to get to that recipe there is this there is enormous amount of knowledge and",
    "start": "609920",
    "end": "616000"
  },
  {
    "text": "experience that goes into that uh to figure that out and and that's not even everything um you will need to do way",
    "start": "616000",
    "end": "622560"
  },
  {
    "text": "more uh to to make those models good um like some things what we did is we did",
    "start": "622560",
    "end": "628240"
  },
  {
    "text": "uh data um set adjustments as we were training as we were training this model",
    "start": "628240",
    "end": "633320"
  },
  {
    "text": "we figured out that it's much better to start pre-training with lower quality data and then backload higher quality",
    "start": "633320",
    "end": "638680"
  },
  {
    "text": "data towards the end that helps um we had a long context expansion stage again",
    "start": "638680",
    "end": "644240"
  },
  {
    "text": "for efficiency reason when you start training those models you normally want to start them with smaller context",
    "start": "644240",
    "end": "649320"
  },
  {
    "text": "window and when you want to expand it towards the end that brings up a lot of new challenges uh with scaling all of",
    "start": "649320",
    "end": "656320"
  },
  {
    "text": "this um and then the most the stage of pre-training is a kneeling where you really want to squeeze the juices out of",
    "start": "656320",
    "end": "663040"
  },
  {
    "text": "the model and and push for performance and and and here you're doing it with the highest quality data that you can",
    "start": "663040",
    "end": "669160"
  },
  {
    "text": "possibly get so as simple as pre-training is is four stages of quite",
    "start": "669160",
    "end": "676240"
  },
  {
    "text": "complicated uh adjustments uh that need to be done now what challenges do you",
    "start": "676240",
    "end": "681440"
  },
  {
    "text": "see when uh when when you when you train those monsters when one thing that people don't talk about much is Just",
    "start": "681440",
    "end": "688680"
  },
  {
    "text": "Pure structural challenge so for llama 3 we trained them actually for llama 1 we",
    "start": "688680",
    "end": "694480"
  },
  {
    "text": "trained on 2,000 gpus that was relatively easy Lama 2 was 4,000 gpus it",
    "start": "694480",
    "end": "699600"
  },
  {
    "text": "was okay llama 3 was trained on 16,000 gpus and when you run such a massive",
    "start": "699600",
    "end": "705639"
  },
  {
    "text": "amount of gpus together things inevitably fail all the time it just Hardware is Hardware so in our practical",
    "start": "705639",
    "end": "713120"
  },
  {
    "text": "experience we had eight interruptions a day uh meaning eight times the whole thing kind of stopped and you have to",
    "start": "713120",
    "end": "719680"
  },
  {
    "text": "restart it um from from a checkpoint and in our paper we do report",
    "start": "719680",
    "end": "724720"
  },
  {
    "text": "the the collection of errors but we encountered and you can see that hey faulted gpus was a pretty big issue uh",
    "start": "724720",
    "end": "731800"
  },
  {
    "text": "where there issues with GPU memories where there software bags whereever issues with CPUs and network switches",
    "start": "731800",
    "end": "738120"
  },
  {
    "text": "pretty much everything that could possibly fail at that scale fails and that's just the beginning if",
    "start": "738120",
    "end": "744399"
  },
  {
    "text": "you think about what Lama 4 is supposed to be I'm not going to be able to tell how many gpus we're planning to train on",
    "start": "744399",
    "end": "750880"
  },
  {
    "text": "but it's going to be bigger number than 16,000 and if we had so many failures",
    "start": "750880",
    "end": "756000"
  },
  {
    "text": "back then how many we're going to have um a lot of those failures are inevitable so those are real practical",
    "start": "756000",
    "end": "762720"
  },
  {
    "text": "challenges of scaling those models the other interesting challenge that I haven't heard anyone uh talking",
    "start": "762720",
    "end": "769000"
  },
  {
    "text": "about is just the emotional component of it so when you get into this trining",
    "start": "769000",
    "end": "774560"
  },
  {
    "text": "round you're you're like about to burn like hundreds of millions of dollars maybe uh right and you pull the plug you",
    "start": "774560",
    "end": "781240"
  },
  {
    "text": "started and before the training run the biggest ablation that you ever had or the biggest experiment that you ever had",
    "start": "781240",
    "end": "787360"
  },
  {
    "text": "might be like 10x smaller so there's no way for you to know how good your run is going to be so you started on the first",
    "start": "787360",
    "end": "794360"
  },
  {
    "text": "day and you start watching some metrics in this case we watched",
    "start": "794360",
    "end": "799560"
  },
  {
    "text": "MML along long with FS but like we observed mlu closely it's a quite",
    "start": "799560",
    "end": "804680"
  },
  {
    "text": "popular metric and initially you see kind of a rocket ship so the model performance starts improving very very",
    "start": "804680",
    "end": "811160"
  },
  {
    "text": "quickly in in the first like two or 3 days you see dramatic Spike on performance you're excited uh it looks",
    "start": "811160",
    "end": "817320"
  },
  {
    "text": "great when it starts to flatten out uh very very quickly like closer to like",
    "start": "817320",
    "end": "822519"
  },
  {
    "text": "second week you're like okay it's not improving this fast anymore it's still clamping up but it's like not really",
    "start": "822519",
    "end": "828440"
  },
  {
    "text": "that fast and another challenge here is that it becomes really noisy so those",
    "start": "828440",
    "end": "833560"
  },
  {
    "text": "metrics were not like super super accurate uh there is a lot of noise when you measure them so What You observe on",
    "start": "833560",
    "end": "839920"
  },
  {
    "text": "a day-to-day basis is this like ups and downs ups and downs all the time and sometimes you don't see meaningful",
    "start": "839920",
    "end": "845800"
  },
  {
    "text": "improvements for weeks of training that is just draining um and and then like",
    "start": "845800",
    "end": "851920"
  },
  {
    "text": "you need to go all the way many months down the road sometimes to see the",
    "start": "851920",
    "end": "857120"
  },
  {
    "text": "ultimate performance results um and and it's pretty much impossible to predict",
    "start": "857120",
    "end": "862680"
  },
  {
    "text": "where where you going to be in vent so just the emotional component and of of this this exercise is very very hard to",
    "start": "862680",
    "end": "870759"
  },
  {
    "text": "describe and that's why you see researchers like looking like zombies sometimes when they train those",
    "start": "870759",
    "end": "876720"
  },
  {
    "text": "models now you may ask but hey with scaling laws and everything um like can't we use scaling laws to predict how",
    "start": "876720",
    "end": "883360"
  },
  {
    "text": "good they're going to be well let me tell you the story about scaling laws so these are like funny things again the",
    "start": "883360",
    "end": "891560"
  },
  {
    "text": "the the the scale of experiments but you can practically run on a daily basis is on the left side of this graph so this",
    "start": "891560",
    "end": "898800"
  },
  {
    "text": "graph shows your like compute budget how much flops you're going to train on or how many gpus and on where y AIS it's",
    "start": "898800",
    "end": "906800"
  },
  {
    "text": "some some Metric of performance doesn't matter which one um so the experiments",
    "start": "906800",
    "end": "912199"
  },
  {
    "text": "you run uh when you play with a model they are on on the very left side you",
    "start": "912199",
    "end": "918000"
  },
  {
    "text": "might have been lucky if you trained previous llama before like in our case it was llama 2 when you have some data",
    "start": "918000",
    "end": "924519"
  },
  {
    "text": "point somewhere closer to the middle but still pretty far and you were trying to",
    "start": "924519",
    "end": "929800"
  },
  {
    "text": "predict what's going to be on the right hand side of on the graph and by the way nothing here is linear so there is no",
    "start": "929800",
    "end": "936120"
  },
  {
    "text": "way to draw a straight line it just doesn't work so predicting the like what's going to be the output",
    "start": "936120",
    "end": "941920"
  },
  {
    "text": "performance is pretty much impossible we did some in in our paper",
    "start": "941920",
    "end": "947360"
  },
  {
    "text": "we published some skillin law experiments results and we did some work um towards getting like better ways of",
    "start": "947360",
    "end": "953399"
  },
  {
    "text": "predicting model performance so instead of like trying to predict it directly uh",
    "start": "953399",
    "end": "959120"
  },
  {
    "text": "measuring um model perplexity um and and plot it model perplexity on our test",
    "start": "959120",
    "end": "965199"
  },
  {
    "text": "sets and we plot it against the compute budget and that relationship is linear",
    "start": "965199",
    "end": "970560"
  },
  {
    "text": "relationship on the on the left hand side here um and then using that relationship you can map it back to the",
    "start": "970560",
    "end": "978279"
  },
  {
    "text": "model performance at your target metric in the end but still it's quite complicated exercise it's not super",
    "start": "978279",
    "end": "985519"
  },
  {
    "text": "reliable but it gives us some sense of uh where we are heading",
    "start": "985519",
    "end": "990920"
  },
  {
    "text": "now I'm going to the next challenge uh that we're going to face and and and for Lama 4 it's going to become even more",
    "start": "991279",
    "end": "997480"
  },
  {
    "text": "important so data um again we we have established pretty",
    "start": "997480",
    "end": "1004000"
  },
  {
    "text": "robust scaling laws on on the data site we know that if you want to increase where your compute budget and train your",
    "start": "1004000",
    "end": "1010519"
  },
  {
    "text": "models on let's say 100x more uh gpus or compute flops um that would require you",
    "start": "1010519",
    "end": "1017639"
  },
  {
    "text": "to roughly T next size and roughly tect the data size so if we train Lama free",
    "start": "1017639",
    "end": "1023720"
  },
  {
    "text": "on 15 trillion tokens then and if you want to Tex that that would require 150",
    "start": "1023720",
    "end": "1030199"
  },
  {
    "text": "trillion tokens of data and there is no place in the world where you can download that that just simply does not",
    "start": "1030199",
    "end": "1036160"
  },
  {
    "text": "exist so getting the training data is really really big challenge um and we we",
    "start": "1036160",
    "end": "1043000"
  },
  {
    "text": "have some ideas how to solve it but that that that one is a big one so",
    "start": "1043000",
    "end": "1049600"
  },
  {
    "text": "what we are trying to do to solve those challenges is we're trying to figure out how we can get more data efficient like",
    "start": "1049600",
    "end": "1054640"
  },
  {
    "text": "how can we train better models with smaller amount of data um we need to",
    "start": "1054640",
    "end": "1060559"
  },
  {
    "text": "figure out how to scale um synthetic data generation obviously like humans do",
    "start": "1060559",
    "end": "1066520"
  },
  {
    "text": "not produce vastly bigger amounts of data on a day-to-day basis as fast as we",
    "start": "1066520",
    "end": "1072000"
  },
  {
    "text": "need so can other models produce that data that's an interesting research",
    "start": "1072000",
    "end": "1077120"
  },
  {
    "text": "problem but nobody has really solved so far and we're trying to figure out how we can use multimodal",
    "start": "1077120",
    "end": "1083960"
  },
  {
    "text": "data um so let's move on a little bit to",
    "start": "1083960",
    "end": "1089360"
  },
  {
    "text": "post- trining um again the pre-training was relatively like objective was simple",
    "start": "1089360",
    "end": "1094559"
  },
  {
    "text": "and the the whole pipeline was relatively straightforward post training is a whole new level of complexity on",
    "start": "1094559",
    "end": "1100480"
  },
  {
    "text": "everything um instead of just like having one objective you suddenly have multiple objectives here um like you",
    "start": "1100480",
    "end": "1108240"
  },
  {
    "text": "want your model to be good at um math and good at reasoning and good in maybe",
    "start": "1108240",
    "end": "1114760"
  },
  {
    "text": "law domain like legal domain and and what not some of those objectives are actually competing with each other and",
    "start": "1114760",
    "end": "1121640"
  },
  {
    "text": "it becomes really really difficult to trade off between those one example I used to give before is um model",
    "start": "1121640",
    "end": "1129000"
  },
  {
    "text": "helpfulness and model safety they're pretty competing objective um like",
    "start": "1129000",
    "end": "1134760"
  },
  {
    "text": "naturally when you want to make model helpful you want it to be able to answer any question but there are some",
    "start": "1134760",
    "end": "1141320"
  },
  {
    "text": "questions that you don't really want model to answer uh for safety reasons right and that becomes really really",
    "start": "1141320",
    "end": "1147559"
  },
  {
    "text": "hard to trade off between those two objectives um anyway so our our post",
    "start": "1147559",
    "end": "1153080"
  },
  {
    "text": "training pipeline is quite complex uh we use heavily synthetic data uh we had to",
    "start": "1153080",
    "end": "1158440"
  },
  {
    "text": "train um multilingual and code experts to improve performance on those specific domains and we use DPO instead of Po",
    "start": "1158440",
    "end": "1165559"
  },
  {
    "text": "what we used for Lama to now some of the challenges um I already mentioned let me",
    "start": "1165559",
    "end": "1171480"
  },
  {
    "text": "cover them a little bit more here so for post training we really really need high quality data so that example here is an",
    "start": "1171480",
    "end": "1178360"
  },
  {
    "text": "example the first one is example of um funing data that is used for Lama 2 and",
    "start": "1178360",
    "end": "1184760"
  },
  {
    "text": "it is written by human so write a poem to help me remember the first 10 elements of periodic table given each",
    "start": "1184760",
    "end": "1191360"
  },
  {
    "text": "element its own line I don't know how about you I I cannot write such a poem",
    "start": "1191360",
    "end": "1196440"
  },
  {
    "text": "there is no way so and you need a lot of those examples for all sorts of domains",
    "start": "1196440",
    "end": "1201480"
  },
  {
    "text": "like you need examples in quantum physics you need examples in math but not it's very very difficult to get high",
    "start": "1201480",
    "end": "1207360"
  },
  {
    "text": "quality data and the quality here really matters so that that that one is a big challenge for post training um we also",
    "start": "1207360",
    "end": "1215360"
  },
  {
    "text": "need to carefully balance our objectives which I already mentioned um we don't",
    "start": "1215360",
    "end": "1221120"
  },
  {
    "text": "like if our scaling laws in pre-training are not super mature our Skilling laws and post training are like even less",
    "start": "1221120",
    "end": "1227799"
  },
  {
    "text": "mature so we don't know how much more post training we need to do how much",
    "start": "1227799",
    "end": "1233440"
  },
  {
    "text": "data we need to Fed into the model to achieve certain results so that that's another big Challenge and then finally",
    "start": "1233440",
    "end": "1240720"
  },
  {
    "text": "we sometimes don't even know how to measure model performance um like evaluation data sets let me go to them",
    "start": "1240720",
    "end": "1247360"
  },
  {
    "text": "um evaluation data sets is yet another challenge here so first of all the life",
    "start": "1247360",
    "end": "1252840"
  },
  {
    "text": "cycle of many benchmarks is very very short uh from the time a benchmark comes up and people people start using it and",
    "start": "1252840",
    "end": "1259600"
  },
  {
    "text": "broadly adopting it um to the time it becomes saturated it's usually just one Model cycle that's what we see there are",
    "start": "1259600",
    "end": "1266320"
  },
  {
    "text": "many reason many reasons for that some of them are good some of them are bad like there is obviously some draining",
    "start": "1266320",
    "end": "1272200"
  },
  {
    "text": "draining data pollution happening either intentional or unintentional where might",
    "start": "1272200",
    "end": "1277400"
  },
  {
    "text": "be some cheating happening as well with people like really really willing to push hard to get good numbers uh but",
    "start": "1277400",
    "end": "1284400"
  },
  {
    "text": "either way it is happening and this is something we have to recognize um some",
    "start": "1284400",
    "end": "1289760"
  },
  {
    "text": "things are just really really hard to measure especially if you go into multimodel domain if you move away from",
    "start": "1289760",
    "end": "1295360"
  },
  {
    "text": "text um like how do you measure the quality of generated images there's no automatic metric for that so you you",
    "start": "1295360",
    "end": "1302120"
  },
  {
    "text": "have to like a human has to look into that and that does not scale same with speech if you generate speech then how",
    "start": "1302120",
    "end": "1308400"
  },
  {
    "text": "do you measure the quality of that is is really really difficult but even on the text site how do you measure things like",
    "start": "1308400",
    "end": "1314240"
  },
  {
    "text": "hey what is the tone of your of the model response like is the model friendly is it chat is it like too conservative like it's",
    "start": "1314240",
    "end": "1321600"
  },
  {
    "text": "hard um and in general I think we experience the shortage of really really",
    "start": "1321600",
    "end": "1327360"
  },
  {
    "text": "good benchmarks for for all of those reasons so I promise to ask you how can",
    "start": "1327360",
    "end": "1334360"
  },
  {
    "text": "you help or like throwing some suggestions into this room so let me get",
    "start": "1334360",
    "end": "1339679"
  },
  {
    "text": "to that um I think we definitely need better benchmarks and I think Community",
    "start": "1339679",
    "end": "1345200"
  },
  {
    "text": "like people who actually use those models are best position to devel develop those benchmarks like pretty",
    "start": "1345200",
    "end": "1350559"
  },
  {
    "text": "much everyone who has a specific practical use case where we use those models I'm pretty sure you all have some",
    "start": "1350559",
    "end": "1358159"
  },
  {
    "text": "test sets what you use to decide whether a new model is good or not or how much",
    "start": "1358159",
    "end": "1363799"
  },
  {
    "text": "better it is so I think it would be very very important um at some point to get",
    "start": "1363799",
    "end": "1369840"
  },
  {
    "text": "those benchmarks out I would encourage you all to start releasing those",
    "start": "1369840",
    "end": "1375080"
  },
  {
    "text": "obviously keep some amount of them private um for obvious reasons you and and and because you don't want uh your",
    "start": "1375080",
    "end": "1382440"
  },
  {
    "text": "benchmark to be uh leaked into the training data so you want to keep some amount of them private but please do",
    "start": "1382440",
    "end": "1388640"
  },
  {
    "text": "release uh some public benchmarks that are relevant to you that show model performance on your specific use case",
    "start": "1388640",
    "end": "1395520"
  },
  {
    "text": "that would be a win-win to everyone because when we will be able to track performance against those benchmarks",
    "start": "1395520",
    "end": "1400840"
  },
  {
    "text": "when we train those models and when you can track a metric you can improve on that you will be getting better models",
    "start": "1400840",
    "end": "1406360"
  },
  {
    "text": "in the end because like once we train them hopefully we'll release them um and and you will get measures that are",
    "start": "1406360",
    "end": "1412679"
  },
  {
    "text": "relevant for you um the other thing that I want to encourage Community to do is",
    "start": "1412679",
    "end": "1417960"
  },
  {
    "text": "to invest more into post training one thing that people don't quite maybe realize that DPO the technique that we",
    "start": "1417960",
    "end": "1425080"
  },
  {
    "text": "used to Post train Lama free was not invented in a Big Industry lab it was invented in Stanford um and we didn't",
    "start": "1425080",
    "end": "1432279"
  },
  {
    "text": "have tens of thousands of gpus to do that so it's definitely possible to do significant improvements in in small",
    "start": "1432279",
    "end": "1439039"
  },
  {
    "text": "settings smaller settings um I think we need scaling loss for post training we need to figure out how to make uh do",
    "start": "1439039",
    "end": "1446080"
  },
  {
    "text": "Post training more efficiently and this is something that can also be done at a smaller scale and finally we need better",
    "start": "1446080",
    "end": "1452159"
  },
  {
    "text": "data um again I think a lot of people here fine tuning the models and and have",
    "start": "1452159",
    "end": "1458080"
  },
  {
    "text": "v data sets I understand the data is private for a reason it is private but",
    "start": "1458080",
    "end": "1463760"
  },
  {
    "text": "there may be other ways to make this data available for pre trining of those models uh like we can probably generate",
    "start": "1463760",
    "end": "1469919"
  },
  {
    "text": "some proxy synthetic data or what not we can figure it out um I do want to",
    "start": "1469919",
    "end": "1474960"
  },
  {
    "text": "encourage the community to think about those things um because that will really help make us next versions of llama much",
    "start": "1474960",
    "end": "1481159"
  },
  {
    "text": "much better thank you so much um if you want to learn more about llama llama.com our",
    "start": "1481159",
    "end": "1487600"
  },
  {
    "text": "new domain uh please please go there um it has a lot of recipes uh code",
    "start": "1487600",
    "end": "1493039"
  },
  {
    "text": "notebooks and and everything else and I have four minutes for questions thank",
    "start": "1493039",
    "end": "1498200"
  },
  {
    "text": "you [Applause]",
    "start": "1498200",
    "end": "1505200"
  },
  {
    "text": "he you have a microphone hi Mark Shipman um when you",
    "start": "1505200",
    "end": "1510679"
  },
  {
    "text": "talk about all the training times um and the cycles and everything what",
    "start": "1510679",
    "end": "1515960"
  },
  {
    "text": "percentage of that is hyperparameter tuning and is that just in the",
    "start": "1515960",
    "end": "1521480"
  },
  {
    "text": "pre-training you do a lot of parameter tuning in post training as well the post trining iteration cycle is much shorter",
    "start": "1521480",
    "end": "1528399"
  },
  {
    "text": "so you can do a lot more there actually uh pre-training iteration cycle is really really long and really really",
    "start": "1528399",
    "end": "1534520"
  },
  {
    "text": "expensive so for pre-training we do a lot of parameter tuning at much smaller scale and that's why we build all of",
    "start": "1534520",
    "end": "1540520"
  },
  {
    "text": "those scaling laws that allow us to actually tune those parameters at smaller scale and then extrapolate into",
    "start": "1540520",
    "end": "1547520"
  },
  {
    "text": "a larger scale um",
    "start": "1547520",
    "end": "1552240"
  },
  {
    "text": "uh thank you very much for this presentation uh I do have a question about a data pollution uh so one is do",
    "start": "1559919",
    "end": "1565240"
  },
  {
    "text": "you have existing mechanism or techniques kind of like handle that right now and another question is uh",
    "start": "1565240",
    "end": "1571320"
  },
  {
    "text": "since we're talking about intive data right so there I believe there could be data pollution around that as well uh is",
    "start": "1571320",
    "end": "1577039"
  },
  {
    "text": "there any mechanism to handle the data pollution as well so thank you yeah we",
    "start": "1577039",
    "end": "1582360"
  },
  {
    "text": "we do use a lot of techniques to kind of eliminate not eliminate to reduce data pollution like you you're looking for",
    "start": "1582360",
    "end": "1589080"
  },
  {
    "text": "similar examples in training data and try to remove them but in the end it's not always possible like for example",
    "start": "1589080",
    "end": "1595240"
  },
  {
    "text": "some of our benchmarks were constructed based on Wikipedia now what do you do about that you don't remove Wikipedia",
    "start": "1595240",
    "end": "1600760"
  },
  {
    "text": "from your training data really it's like the most high quality source so you you can't really do that uh but you do try",
    "start": "1600760",
    "end": "1607960"
  },
  {
    "text": "to remove things that exactly match the benchmarks or very very close to benchmarks those are the things we're",
    "start": "1607960",
    "end": "1613080"
  },
  {
    "text": "definitely doing on synthetic side M so far hasn't been an issue but I think it",
    "start": "1613080",
    "end": "1619640"
  },
  {
    "text": "really depends on what kind of techniques you're using for synthetic data generation like a lot of synthetic",
    "start": "1619640",
    "end": "1625080"
  },
  {
    "text": "data generation right now is on code site um where you can actually generate code and compile and run it validate and",
    "start": "1625080",
    "end": "1632919"
  },
  {
    "text": "that helps a lot thank you Sergey um can you tell us",
    "start": "1632919",
    "end": "1641399"
  },
  {
    "text": "uh what to expect in llama",
    "start": "1641399",
    "end": "1645679"
  },
  {
    "text": "4 so I have I have a tweet um with this like you know bigger and bigger monster",
    "start": "1647159",
    "end": "1653159"
  },
  {
    "text": "so um we expect Lama 4 to be much much bigger and much much stronger model but",
    "start": "1653159",
    "end": "1659440"
  },
  {
    "text": "it's very very difficult to give exact predictions on what kind of capabilities um you you should expect um from that",
    "start": "1659440",
    "end": "1666799"
  },
  {
    "text": "model um because some of them are emerging right uh they only show up at certain scale and so far no one really",
    "start": "1666799",
    "end": "1674279"
  },
  {
    "text": "has achieved that scale GPT 5 is not out um so we don't know what we're going to",
    "start": "1674279",
    "end": "1679720"
  },
  {
    "text": "find uh I guess we'll find out as we train and and",
    "start": "1679720",
    "end": "1685000"
  },
  {
    "text": "learn so I have a question about uh training data you mentioned multimodal",
    "start": "1685399",
    "end": "1691120"
  },
  {
    "text": "and ideas on how to use that do you mean like audio data or video or what kind of",
    "start": "1691120",
    "end": "1696559"
  },
  {
    "text": "ideas do you guys have in mind well we have many of ideas the question is what we can execute on um we want to use",
    "start": "1696559",
    "end": "1704120"
  },
  {
    "text": "everything ideally right so in in some perfect Ideal World you you would want a model that can consume all sorts of data",
    "start": "1704120",
    "end": "1712399"
  },
  {
    "text": "um and and train on it uh no matter where it's like no matter what modality it",
    "start": "1712399",
    "end": "1718360"
  },
  {
    "text": "is it might be that some modalities are less informational dense as text like",
    "start": "1718360",
    "end": "1724600"
  },
  {
    "text": "text has a lot of information at it images may be less I don't know about video so that might be the",
    "start": "1724600",
    "end": "1732960"
  },
  {
    "text": "challenge hey so I have a question uh so the first model were trained on uh human",
    "start": "1733919",
    "end": "1741240"
  },
  {
    "text": "data so what happens when more generative data is put into the internet and you train this model on this is",
    "start": "1741240",
    "end": "1749200"
  },
  {
    "text": "there like limitation or a corner to avoid this we don't know I mean in",
    "start": "1749200",
    "end": "1757200"
  },
  {
    "text": "research we sometimes call distillation when you use um synthetic data produced by a model to train another model I",
    "start": "1757200",
    "end": "1764279"
  },
  {
    "text": "guess it it is happening right now on on the web because there is so much synthetic data already there but we",
    "start": "1764279",
    "end": "1771559"
  },
  {
    "text": "don't know the impact of it I've been working on machine translation before llms and where it was already an issue",
    "start": "1771559",
    "end": "1777440"
  },
  {
    "text": "where a lot of data on the web um was automatically translated already so you train on it so I guess this is something",
    "start": "1777440",
    "end": "1785240"
  },
  {
    "text": "we will will have to figure out as we learn but it's going to be a challenge for sure okay we'll take one more",
    "start": "1785240",
    "end": "1790960"
  },
  {
    "text": "question um and then uh we're going to transition speakers so that folks can",
    "start": "1790960",
    "end": "1796000"
  },
  {
    "text": "catch uh Sergey in the in between space hi my question is um when do you",
    "start": "1796000",
    "end": "1803679"
  },
  {
    "text": "think that meta might pivot away from train training llama models do you think that um they'll ever stop creating new",
    "start": "1803679",
    "end": "1811039"
  },
  {
    "text": "llamas up until 100 that's a great question so far we",
    "start": "1811039",
    "end": "1818760"
  },
  {
    "text": "are seeing that their these models keep improving um as we train them for longer",
    "start": "1818760",
    "end": "1823919"
  },
  {
    "text": "on more compute and on more data but there are obvious bottlenecks uh some",
    "start": "1823919",
    "end": "1829200"
  },
  {
    "text": "walls what we are facing like data is one wall compute scaling compute going forward is probably also going to be",
    "start": "1829200",
    "end": "1835600"
  },
  {
    "text": "more and more challenging so we'll see uh what's going to happen when we hit those walls maybe we'll start scaling in",
    "start": "1835600",
    "end": "1842440"
  },
  {
    "text": "other dimensions like it's clearly that openi is doing some inference compute scaling so that's another interesting",
    "start": "1842440",
    "end": "1849519"
  },
  {
    "text": "Dimension to scale into who knows there might be more so",
    "start": "1849519",
    "end": "1854640"
  },
  {
    "text": "will the next models are going to be more and more pre-trained I don't know um but we'll keep improving as long as",
    "start": "1854640",
    "end": "1861559"
  },
  {
    "text": "we can all right let's give a round of applause thank you so much thank you",
    "start": "1861559",
    "end": "1869960"
  }
]