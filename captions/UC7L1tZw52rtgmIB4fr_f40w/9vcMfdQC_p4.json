[
  {
    "start": "0",
    "end": "99000"
  },
  {
    "text": "hello everyone uh my name is Smith K and I'm a senior machine learning engineer at clavio in this talk I'm going to be",
    "start": "2720",
    "end": "9599"
  },
  {
    "text": "sharing how CIO built a robust serving platform with Ray",
    "start": "9599",
    "end": "15240"
  },
  {
    "text": "serve so here's a quick agenda for what we're going to walk through I'll mostly be sharing about us using rayer in",
    "start": "15240",
    "end": "22119"
  },
  {
    "text": "production and sharing some insights that we learn along the way first let's talk about clavio",
    "start": "22119",
    "end": "30119"
  },
  {
    "text": "most importantly it's pronounced as clay I've heard a lot of",
    "start": "30119",
    "end": "35160"
  },
  {
    "text": "mispronunciations it's not clavio it's not anything else that you thought of um",
    "start": "35160",
    "end": "40559"
  },
  {
    "text": "it's clavio okay now that we know how CIO is",
    "start": "40559",
    "end": "46280"
  },
  {
    "text": "pronounced um here's a quick pitch so clavio Powers smarter digital",
    "start": "46280",
    "end": "51680"
  },
  {
    "text": "relationships we make it easy for b2c customers um to transform all of their",
    "start": "51680",
    "end": "57559"
  },
  {
    "text": "data into more valuable customer experience experiences across every touch point from email and SMS to the",
    "start": "57559",
    "end": "64320"
  },
  {
    "text": "web and reviews more than 151,000 businesses rely on clavio to",
    "start": "64320",
    "end": "69960"
  },
  {
    "text": "grow their revenue faster and more efficiently we're the only marketing",
    "start": "69960",
    "end": "75400"
  },
  {
    "text": "platform that is purpose-built for smarter digital relationships we help",
    "start": "75400",
    "end": "80759"
  },
  {
    "text": "businesses activate every single data point for the smartest customer interactions connect with customers at",
    "start": "80759",
    "end": "87920"
  },
  {
    "text": "every touch Point seamlessly and guide their marketing towards the best possible results also they can grow",
    "start": "87920",
    "end": "94799"
  },
  {
    "text": "their revenue faster and more efficiently let's move on to machine",
    "start": "94799",
    "end": "101520"
  },
  {
    "start": "99000",
    "end": "407000"
  },
  {
    "text": "learning at clayo and why we chose racer for our model serving",
    "start": "101520",
    "end": "106960"
  },
  {
    "text": "needs so up until a few years ago every model was deployed in its own",
    "start": "106960",
    "end": "112479"
  },
  {
    "text": "microservice using flask so that meant that every new ml feature at clavio that",
    "start": "112479",
    "end": "118960"
  },
  {
    "text": "had to be built you had to go in create a whole new service from scratch and",
    "start": "118960",
    "end": "124600"
  },
  {
    "text": "create all of the scaffoldings around it but that's not all you also needed to",
    "start": "124600",
    "end": "131120"
  },
  {
    "text": "set up all of the cicd along with it you had to build your Docker images set up code pipeline set up ECS and everything",
    "start": "131120",
    "end": "139200"
  },
  {
    "text": "that comes along with it and so using flask also meant that",
    "start": "139200",
    "end": "144519"
  },
  {
    "text": "there aren't any ml specific optimizations because flask isn't really",
    "start": "144519",
    "end": "150000"
  },
  {
    "text": "focused on machine learning and so all of that meant that adding new ml",
    "start": "150000",
    "end": "155120"
  },
  {
    "text": "features at clavio was a very very long process and so we wanted to make it make",
    "start": "155120",
    "end": "161599"
  },
  {
    "text": "this easier have a standardized ml platform and so one of the reasons we",
    "start": "161599",
    "end": "167720"
  },
  {
    "text": "were very excited about racer was that it was very focused on machine learning",
    "start": "167720",
    "end": "173120"
  },
  {
    "text": "use cases and had a lot of features that made sense for nml applications and on",
    "start": "173120",
    "end": "180519"
  },
  {
    "text": "top of that Reser didn't really care what type of ml model that you're deploying or if you're deploying an ml",
    "start": "180519",
    "end": "186640"
  },
  {
    "text": "model at all we looked at some other Frameworks and they had very different",
    "start": "186640",
    "end": "192799"
  },
  {
    "text": "experiences for example if you wanted to deploy a psychic learn model it was super easy but then if you wanted to",
    "start": "192799",
    "end": "197959"
  },
  {
    "text": "deploy something else that had some business a attached to it it was very",
    "start": "197959",
    "end": "204159"
  },
  {
    "text": "difficult and we also found the ray ecosystem to be very powerful Ray core",
    "start": "204159",
    "end": "210920"
  },
  {
    "text": "and other Ray libraries help unify The Experience",
    "start": "210920",
    "end": "216280"
  },
  {
    "text": "across the ml life cycle and when we were trying out racer we found the community and the",
    "start": "216280",
    "end": "222319"
  },
  {
    "text": "developers to be very engaged and very helpful my questions were answered",
    "start": "222319",
    "end": "228439"
  },
  {
    "text": "within a couple of hours and some of the bugs that I reported were fixed within a few days and so all of this got us very",
    "start": "228439",
    "end": "235840"
  },
  {
    "text": "excited to using racer but how does rer work so to",
    "start": "235840",
    "end": "241519"
  },
  {
    "text": "understand that let's go over the physical components of the rer system um",
    "start": "241519",
    "end": "247799"
  },
  {
    "text": "race serve runs on top of a ray cluster a ray cluster has a head node and a",
    "start": "247799",
    "end": "253640"
  },
  {
    "text": "bunch of worker nodes the controller that's running on the head node it",
    "start": "253640",
    "end": "259600"
  },
  {
    "text": "manages the life cycle of different Ray actors on The Ray cluster and it manages",
    "start": "259600",
    "end": "265639"
  },
  {
    "text": "autoscaling the HTTP and grpc proxies help you process and Route incoming",
    "start": "265639",
    "end": "271600"
  },
  {
    "text": "requests and the replica would then execute a code in response to a",
    "start": "271600",
    "end": "277840"
  },
  {
    "text": "request and to deploy Ray serve on kubernetes we use CU cubre is an open",
    "start": "277840",
    "end": "283919"
  },
  {
    "text": "source operator that manages Ray applications on kubernetes it provides you with a custom",
    "start": "283919",
    "end": "291000"
  },
  {
    "text": "resource called Ray service that allows you to run race serve on",
    "start": "291000",
    "end": "297120"
  },
  {
    "text": "kubernetes and cubra would manage all of the resources associated with the r",
    "start": "297120",
    "end": "303440"
  },
  {
    "text": "cluster such as the pods the service config maps and",
    "start": "303440",
    "end": "308600"
  },
  {
    "text": "Etc as the platform team we need to know all of this we need to know how the r",
    "start": "308600",
    "end": "313960"
  },
  {
    "text": "architecture looks like how we need to deploy it but our customers would be data scientists and they don't really",
    "start": "313960",
    "end": "320240"
  },
  {
    "text": "care of about any of them they care more about the logical components of the",
    "start": "320240",
    "end": "325440"
  },
  {
    "text": "system so let's quickly go through them um a deployment in racer is a central",
    "start": "325440",
    "end": "331639"
  },
  {
    "text": "concept you can think of it as a single piece of ml model or a business logic",
    "start": "331639",
    "end": "338080"
  },
  {
    "text": "that you want to execute and a replica would be a copy of that deployment that's",
    "start": "338080",
    "end": "343759"
  },
  {
    "text": "running an application can consist one or more deployments that are connected",
    "start": "343759",
    "end": "349520"
  },
  {
    "text": "together to handle an incoming request one of them would be an Ingress",
    "start": "349520",
    "end": "356000"
  },
  {
    "text": "deployment and the Ingress deployment would be the entry point for the request that's coming in the Ingress deployment",
    "start": "356000",
    "end": "362440"
  },
  {
    "text": "is then going to call the rest of the deployments it's going to execute its own logic and and then it's going to",
    "start": "362440",
    "end": "368759"
  },
  {
    "text": "respond to the request and the cool thing about rer is that you can run all of these deployments in",
    "start": "368759",
    "end": "376919"
  },
  {
    "text": "parallel um a quick note on the termin terminology for the rest of this talk to",
    "start": "379080",
    "end": "385039"
  },
  {
    "text": "avoid some confusion so there can be any number of R replicas on running on a",
    "start": "385039",
    "end": "390919"
  },
  {
    "text": "rain node a rain node is the exact same as a kubernetes pod I'll be using Ray",
    "start": "390919",
    "end": "397880"
  },
  {
    "text": "node and pod interchangeably and there can be any number of kubernetes PODS running on a",
    "start": "397880",
    "end": "404360"
  },
  {
    "text": "kubernetes node so with with all of this background",
    "start": "404360",
    "end": "409479"
  },
  {
    "start": "407000",
    "end": "476000"
  },
  {
    "text": "in mind let's move to using rayer in production so we built this platform",
    "start": "409479",
    "end": "416199"
  },
  {
    "text": "called Dart online um Dart stands for data science runtime this is the online",
    "start": "416199",
    "end": "422240"
  },
  {
    "text": "model serving platform at clavo this platform allows data scientists to focus on writing the",
    "start": "422240",
    "end": "429199"
  },
  {
    "text": "business logic and not really focus on building or managing any",
    "start": "429199",
    "end": "436039"
  },
  {
    "text": "infrastructure so here's how you would go about deploying a new model on D online so the first thing is you copy",
    "start": "436039",
    "end": "443080"
  },
  {
    "text": "this template You' create your pedantic request and response schemas and then",
    "start": "443080",
    "end": "448160"
  },
  {
    "text": "fill in your in it logic um you load your model and then you'd fill in this",
    "start": "448160",
    "end": "454199"
  },
  {
    "text": "predict function uh the predict function would be where you would receive a request and then you would send in a",
    "start": "454199",
    "end": "461280"
  },
  {
    "text": "response you can see this class inherits from a base class and That Base Class",
    "start": "461280",
    "end": "466560"
  },
  {
    "text": "adds a bunch of telemetry and monitoring and other utility function that makes it easier for you to run your application",
    "start": "466560",
    "end": "473720"
  },
  {
    "text": "in production then you copy this template and modify it for your own model and",
    "start": "473720",
    "end": "481639"
  },
  {
    "text": "then add it to a yaml file and of course you need to add test",
    "start": "481639",
    "end": "487199"
  },
  {
    "text": "so you'll obviously add unit tests for the class that you just created but we also make it easier for you to write",
    "start": "487199",
    "end": "493440"
  },
  {
    "text": "integration tests so to add an integration test you would inherit from this Base Class and",
    "start": "493440",
    "end": "500159"
  },
  {
    "text": "that class would provide you with a fixture that would start a r serve instance deploy your model and then give",
    "start": "500159",
    "end": "507120"
  },
  {
    "text": "you an endpoint that you can call to run your test and that's it so you take all of this u",
    "start": "507120",
    "end": "514120"
  },
  {
    "text": "pull up a PR merge it into production and then we would give you a client library that you can use to call your",
    "start": "514120",
    "end": "520680"
  },
  {
    "text": "model in production you can see metrics for your application on a grafana board and we'd",
    "start": "520680",
    "end": "528040"
  },
  {
    "text": "also get alerts if something is wrong with the app so let's talk about how Dot online",
    "start": "528040",
    "end": "535279"
  },
  {
    "text": "works on the back end so all of our race serve applications are deployed on the same Ray",
    "start": "535279",
    "end": "541480"
  },
  {
    "text": "cluster and we have two of them two identical Ray clusters for extra fall",
    "start": "541480",
    "end": "546600"
  },
  {
    "text": "tolerance and this is deployed on kubernetes using cub and then requests are then equally",
    "start": "546600",
    "end": "554120"
  },
  {
    "text": "routed through to these two clusters using Route 53 weed",
    "start": "554120",
    "end": "560720"
  },
  {
    "text": "routing there's also a health check service that makes sure that all of the",
    "start": "560760",
    "end": "566240"
  },
  {
    "text": "application on these two Ray clusters are running and healthy and we are also able",
    "start": "566240",
    "end": "572519"
  },
  {
    "text": "to reach the racers health check endpoint and this health check service",
    "start": "572519",
    "end": "578040"
  },
  {
    "text": "is then connected back to Route 53 and that makes it so that no no request is",
    "start": "578040",
    "end": "584519"
  },
  {
    "text": "sent to a unhealthy R cluster and so talking about request",
    "start": "584519",
    "end": "590959"
  },
  {
    "text": "routing let's talk about how a single request will go through the entire",
    "start": "590959",
    "end": "596519"
  },
  {
    "text": "system at a high level um when a client sends in a request rout 3 is going to",
    "start": "596519",
    "end": "603519"
  },
  {
    "text": "randomly send this request to one of the two Ray clusters the load balancer of",
    "start": "603519",
    "end": "608800"
  },
  {
    "text": "the ray cluster is going to assign that request to one of the ray nodes in a",
    "start": "608800",
    "end": "614839"
  },
  {
    "text": "round robin algorithm and then within the ray node an HTTP proxy is going to",
    "start": "614839",
    "end": "621200"
  },
  {
    "text": "receive the request and then it is going to process it and it's going to find a",
    "start": "621200",
    "end": "626240"
  },
  {
    "text": "replica to send that request to",
    "start": "626240",
    "end": "631320"
  },
  {
    "text": "so how does this proxy find a replica to assign the request to so it uses this",
    "start": "631920",
    "end": "639240"
  },
  {
    "text": "algorithm called the power of two choices schedular so in this example",
    "start": "639240",
    "end": "644360"
  },
  {
    "text": "let's say we have three different R three different models running on a ray cluster and they are SMS classifier and",
    "start": "644360",
    "end": "652519"
  },
  {
    "text": "email classifier and a product recommendation model and now we receive a request for the product recommendation",
    "start": "652519",
    "end": "659839"
  },
  {
    "text": "model and that gets assigned to the Head node the proxy on the head node is going to receive that request and then now it",
    "start": "659839",
    "end": "666480"
  },
  {
    "text": "needs to forward that request to one of the recommendation replicas so first it's going to look at",
    "start": "666480",
    "end": "674279"
  },
  {
    "text": "the look at its own Noe it's going to look at the head note and then see that",
    "start": "674279",
    "end": "680160"
  },
  {
    "text": "there is no replicas for that request then it's going to look at the rest of the cluster and here it found",
    "start": "680160",
    "end": "687560"
  },
  {
    "text": "three different replicas that can accept that incoming request and from those three it's going to randomly pick two of",
    "start": "687560",
    "end": "695600"
  },
  {
    "text": "them um and once it picks two of them it's going to look at the Q depth for",
    "start": "695600",
    "end": "702920"
  },
  {
    "text": "these two replicas and it's going to forward that request to the replica that",
    "start": "702920",
    "end": "709440"
  },
  {
    "text": "has the lowest QEP the replica is then going to take the request from the queue and then",
    "start": "709440",
    "end": "715279"
  },
  {
    "text": "process it and respond so here are some some key takeaways from what we've learned so far",
    "start": "715279",
    "end": "722120"
  },
  {
    "text": "um traffic to one endpoint can impact others if one of your endpoints suddenly",
    "start": "722120",
    "end": "728360"
  },
  {
    "text": "starts receiving too many requests with very large payloads it can slow down these proxies and because these proxies",
    "start": "728360",
    "end": "736120"
  },
  {
    "text": "are shared between all of your models you're going to see impact on",
    "start": "736120",
    "end": "743440"
  },
  {
    "text": "other models as well this traffic um",
    "start": "743440",
    "end": "749800"
  },
  {
    "text": "sorry the the second takeaway here is if your replicas aren't spread out enough",
    "start": "749800",
    "end": "755199"
  },
  {
    "text": "across these two replicas um most requests will need an extra hop to get",
    "start": "755199",
    "end": "760760"
  },
  {
    "text": "to the replica and this may or may not matter to you depending on your use case",
    "start": "760760",
    "end": "765920"
  },
  {
    "text": "but it is very important to understand how this system works so let's move on to fault Hance um",
    "start": "765920",
    "end": "773160"
  },
  {
    "text": "we use elastic hash rdus to make the ray cluster GCS fall tolerant GCS stands for",
    "start": "773160",
    "end": "780440"
  },
  {
    "text": "Global control service and that runs on the head node and that manages all of the cluster level metadata for the",
    "start": "780440",
    "end": "787839"
  },
  {
    "text": "cluster a high availability redis can be used to store this metadata and so in",
    "start": "787839",
    "end": "793360"
  },
  {
    "text": "case GCS restarts for any reason it can just load the metadata from realis and",
    "start": "793360",
    "end": "798920"
  },
  {
    "text": "prevent from the entire cluster going down but why do we use this extra fall",
    "start": "798920",
    "end": "806519"
  },
  {
    "start": "804000",
    "end": "956000"
  },
  {
    "text": "Hance of having two identical R clusters so the biggest reason is that",
    "start": "806519",
    "end": "813360"
  },
  {
    "text": "Ray serve can be unstable when encountering pod disruptions so what are these part",
    "start": "813360",
    "end": "820279"
  },
  {
    "text": "disruptions uh disruptions can be voluntary or involuntary voluntary disruptions include manually draining a",
    "start": "820279",
    "end": "828000"
  },
  {
    "text": "node to perform maintenance or running a kubernetes cluster upgrade which would",
    "start": "828000",
    "end": "834079"
  },
  {
    "text": "require a restart of all of your kubernetes nodes there could be",
    "start": "834079",
    "end": "839199"
  },
  {
    "text": "involuntary disruptions where a higher priority pod would kick off a lower",
    "start": "839199",
    "end": "844279"
  },
  {
    "text": "priority pod um and there could also be some node pressure evictions where a pod",
    "start": "844279",
    "end": "851160"
  },
  {
    "text": "exhausts some resource such as memory or FML storage and so kubernetes will provide",
    "start": "851160",
    "end": "858079"
  },
  {
    "text": "you a resource called a pod disruption budget or pdb for short and pdbs make",
    "start": "858079",
    "end": "864199"
  },
  {
    "text": "sure that any any such disruptions don't compromise the applications available",
    "start": "864199",
    "end": "869240"
  },
  {
    "text": "ility Beyond certain acceptable thresholds so for example this pdb that",
    "start": "869240",
    "end": "874440"
  },
  {
    "text": "you see here is going to make sure that at least 50% of your pods are always",
    "start": "874440",
    "end": "880120"
  },
  {
    "text": "running so to do that the assumption that pdb makes is all of your replicas",
    "start": "880120",
    "end": "885920"
  },
  {
    "text": "are running the same workloads so this way it doesn't matter which 50% of your",
    "start": "885920",
    "end": "891519"
  },
  {
    "text": "workloads 50% of your pods are running it just needs to make sure that any 50%",
    "start": "891519",
    "end": "896759"
  },
  {
    "text": "of your pods are running but when you deploy reserve on kues",
    "start": "896759",
    "end": "901959"
  },
  {
    "text": "using CU all pods aren't running the same workloads so for example in this",
    "start": "901959",
    "end": "907079"
  },
  {
    "text": "rate cluster you can see that replicas for deployment C are only present on two",
    "start": "907079",
    "end": "912320"
  },
  {
    "text": "of these um kubernetes SPS right um and so in this case if you have a pdb of",
    "start": "912320",
    "end": "921160"
  },
  {
    "text": "50% and and then suddenly two of these nodes go down you're still meeting this",
    "start": "921160",
    "end": "928240"
  },
  {
    "text": "pdb requirement but all of the replicas for deployment C are unavailable and that is a big",
    "start": "928240",
    "end": "937120"
  },
  {
    "text": "issue and so this is the biggest reason why we use the architecture we do uh we run regular maintenance on our kuet use",
    "start": "937120",
    "end": "944120"
  },
  {
    "text": "nodes to apply some security patches and to do this we need to manually drain our kubernetes nodes and sometimes this",
    "start": "944120",
    "end": "950959"
  },
  {
    "text": "causes disruptions with um some of our deployments so let's move on to some of",
    "start": "950959",
    "end": "957600"
  },
  {
    "start": "956000",
    "end": "1452000"
  },
  {
    "text": "our learnings from from using race Ser in production a big question that people",
    "start": "957600",
    "end": "964240"
  },
  {
    "text": "ask is should we use one rate cluster where we deplo all of our applications",
    "start": "964240",
    "end": "970959"
  },
  {
    "text": "together or should we use one rate cluster per application and so these are",
    "start": "970959",
    "end": "976319"
  },
  {
    "text": "some benefits for having multiple applications on a single Ray",
    "start": "976319",
    "end": "982160"
  },
  {
    "text": "cluster it's easier to manage multiple applications that are dependent on each",
    "start": "982160",
    "end": "987720"
  },
  {
    "text": "other um if an application calls another application Ray makes it easy for you to",
    "start": "987720",
    "end": "993240"
  },
  {
    "text": "call things running on the same Ray cluster you don't need to have a client or anything like that you can just use",
    "start": "993240",
    "end": "999279"
  },
  {
    "text": "Ray serves in buil features you also get better sharing of resources because Ray",
    "start": "999279",
    "end": "1005000"
  },
  {
    "text": "optimizes everything running on its own cluster and it can give you good sharing",
    "start": "1005000",
    "end": "1010680"
  },
  {
    "text": "of your gpus for example and it's also easier to manage",
    "start": "1010680",
    "end": "1016519"
  },
  {
    "text": "all of the infrastructure that surrounds a re cluster such as load balancers or am policies um things like that this",
    "start": "1016519",
    "end": "1025038"
  },
  {
    "text": "sometimes may not be a good thing um and lastly you get a nice R dashboard where",
    "start": "1025039",
    "end": "1031520"
  },
  {
    "text": "you can see all of your applications running in one place you can look at the logs for all of your applications you",
    "start": "1031520",
    "end": "1037438"
  },
  {
    "text": "can look at all of the metrics for the applications but if you have multiple clusters that that becomes",
    "start": "1037439",
    "end": "1045520"
  },
  {
    "text": "hard but there are also some important considerations to keep in mind when you before you go with the multi",
    "start": "1045520",
    "end": "1051520"
  },
  {
    "text": "applications approach a big one is that it makes all the applications less",
    "start": "1051520",
    "end": "1056799"
  },
  {
    "text": "isolated heavy traffic on one application can impact others and one",
    "start": "1056799",
    "end": "1062520"
  },
  {
    "text": "app failing to start up can block any new deployments to go down cu the ray cluster only becomes ready when all of",
    "start": "1062520",
    "end": "1068960"
  },
  {
    "text": "the applications start up also since cubet cubay does a blue",
    "start": "1068960",
    "end": "1074840"
  },
  {
    "text": "green deploy of Ray clusters when rolling out any new changes is you need",
    "start": "1074840",
    "end": "1079880"
  },
  {
    "text": "to make sure that you have twice as many resources available to deploy any changes and this can sometimes be a",
    "start": "1079880",
    "end": "1086960"
  },
  {
    "text": "challenge if you have a very large rate cluster using lots of gpus my personal recommendation with",
    "start": "1086960",
    "end": "1093440"
  },
  {
    "text": "this is for a few applications with low to medium traffic a single Ray cluster",
    "start": "1093440",
    "end": "1099919"
  },
  {
    "text": "is fine but if you need larger scale or lots of applications then go with a",
    "start": "1099919",
    "end": "1105320"
  },
  {
    "text": "single app or cluster approach so let's move on to some pitfalls um the",
    "start": "1105320",
    "end": "1111280"
  },
  {
    "text": "first Pitfall is sharing redis between multiple clusters so sharing means that you're",
    "start": "1111280",
    "end": "1117880"
  },
  {
    "text": "introducing a single point of failure into your system and in a scenario where red is were to go down it's going to",
    "start": "1117880",
    "end": "1124280"
  },
  {
    "text": "take down all of the rate clusters with it and this is because the head node won't have any information about the",
    "start": "1124280",
    "end": "1130840"
  },
  {
    "text": "rate cluster um since GCS stores all of his information in redis I also want to mention that the",
    "start": "1130840",
    "end": "1139159"
  },
  {
    "text": "Clusters won't immediately go down as soon as redis has like a 5sec blip um",
    "start": "1139159",
    "end": "1144280"
  },
  {
    "text": "the head node would only crash if it can connect to reddis for a certain period of time which you can configure via an",
    "start": "1144280",
    "end": "1150840"
  },
  {
    "text": "environment variable and because this redis that you need to use is high avability radus the situation is",
    "start": "1150840",
    "end": "1157280"
  },
  {
    "text": "unlikely but it can happen um we've learned this a hard",
    "start": "1157280",
    "end": "1163520"
  },
  {
    "text": "way the next Pitfall is installing packages at runtime so allows you to",
    "start": "1163640",
    "end": "1170000"
  },
  {
    "text": "specify a runtime environment for your deployments um so that each of your deployment can run within its own",
    "start": "1170000",
    "end": "1177360"
  },
  {
    "text": "environment and this can be amazing in situations where you have package conflicts between different deployments",
    "start": "1177360",
    "end": "1184520"
  },
  {
    "text": "and that's very common in uh the machine learning world and you want each of your",
    "start": "1184520",
    "end": "1189919"
  },
  {
    "text": "deployments to run in its own environment however these packages are going to be installed in the FML storage",
    "start": "1189919",
    "end": "1196520"
  },
  {
    "text": "of the kubernetes Pod when the recluster starts up and so if there are multiple",
    "start": "1196520",
    "end": "1201760"
  },
  {
    "text": "such deployments running on the same Ray node and each of them have their own",
    "start": "1201760",
    "end": "1209039"
  },
  {
    "text": "runtime environment the storage can fill up very quickly and so this increases",
    "start": "1209039",
    "end": "1214799"
  },
  {
    "text": "the chance of your pod being evicted because it ran out of FML storage so I",
    "start": "1214799",
    "end": "1220720"
  },
  {
    "text": "recommend you install all of your packages within the doer image of the ray",
    "start": "1220720",
    "end": "1227200"
  },
  {
    "text": "cluster the next Pitfall is running some workloads on the head node so let's say",
    "start": "1227280",
    "end": "1233000"
  },
  {
    "text": "you have a ray cluster where you're running some workloads on the head node and there is a small memory leak in your",
    "start": "1233000",
    "end": "1239240"
  },
  {
    "text": "system so over time it's going to fill up the memory used by your head node and",
    "start": "1239240",
    "end": "1245720"
  },
  {
    "text": "then the Pod would be evicted CU it exhausted the resource kubernetes or Cub is not going",
    "start": "1245720",
    "end": "1252120"
  },
  {
    "text": "to automatically restart this part and now because the head note is down it's",
    "start": "1252120",
    "end": "1257760"
  },
  {
    "text": "going to take down the the entire cluster with it and you won't be able to recover from this unless you manually go",
    "start": "1257760",
    "end": "1264480"
  },
  {
    "text": "in and restart your pod instead let's say this the same",
    "start": "1264480",
    "end": "1270520"
  },
  {
    "text": "thing happens on the working the autoscaler that's running on your head node is going to spin up a new",
    "start": "1270520",
    "end": "1277880"
  },
  {
    "text": "work node and you'll most likely be fine you should still fix the memory",
    "start": "1277880",
    "end": "1284799"
  },
  {
    "text": "leak so moving on to the next Pitfall um it's using very large R nodes so let's",
    "start": "1284799",
    "end": "1292640"
  },
  {
    "text": "say you have two very large worker nodes and each of them are running lots and",
    "start": "1292640",
    "end": "1297960"
  },
  {
    "text": "lots of replicas and one of them were to go down um you lost a lot of availability for",
    "start": "1297960",
    "end": "1304840"
  },
  {
    "text": "your application there's only a few pods left few replicas left that need to",
    "start": "1304840",
    "end": "1309960"
  },
  {
    "text": "handle all of your incoming requests and because you're using a large node that can sometimes take a while to spin",
    "start": "1309960",
    "end": "1317000"
  },
  {
    "text": "up so instead of if you have four worker noes your replicas are spread out a bit",
    "start": "1317000",
    "end": "1322600"
  },
  {
    "text": "more and if one of them were to go down again you haven't lost too much of your",
    "start": "1322600",
    "end": "1328000"
  },
  {
    "text": "capacity and it's likely that the rest will handle um the",
    "start": "1328000",
    "end": "1333200"
  },
  {
    "text": "load it's very important to note that there's a tradeoff between rain node sizes you shouldn't make your nodes too",
    "start": "1333200",
    "end": "1340559"
  },
  {
    "text": "small as well otherwise there's going to be an increased overhead of communication between the ray nodes Ray",
    "start": "1340559",
    "end": "1348279"
  },
  {
    "text": "pref first that you use larger nodes over smaller ones but this is a warning that you shouldn't make it too large as",
    "start": "1348279",
    "end": "1354000"
  },
  {
    "text": "well that one node going down is going to impact your",
    "start": "1354000",
    "end": "1359480"
  },
  {
    "text": "application so what's next for Dart online uh we're going to be moving to a",
    "start": "1360000",
    "end": "1365679"
  },
  {
    "text": "single cluster per applications approach cuz we need to deploy a lot more models",
    "start": "1365679",
    "end": "1370960"
  },
  {
    "text": "now and they need to be a lot more isolated we also want to have a more",
    "start": "1370960",
    "end": "1376000"
  },
  {
    "text": "abstract kubernetes manifest than what CU provides so it's easier for data",
    "start": "1376000",
    "end": "1381200"
  },
  {
    "text": "scientists to create and manage their own Ray clusters and that can spin up",
    "start": "1381200",
    "end": "1387480"
  },
  {
    "text": "all of the resources around it such as an Ingress Secrets config maps and all things like",
    "start": "1387480",
    "end": "1393000"
  },
  {
    "text": "that and we also want to have better integration with our other platforms",
    "start": "1393000",
    "end": "1398279"
  },
  {
    "text": "such as on demand and offline which lets you run ra jobs on demand and on a",
    "start": "1398279",
    "end": "1405200"
  },
  {
    "text": "schedule so to conclude you don't always need to use this extra fall tolerance",
    "start": "1405200",
    "end": "1410360"
  },
  {
    "text": "like we do for most non-critical workloads a single Ray cluster with high availability settings that is",
    "start": "1410360",
    "end": "1417159"
  },
  {
    "text": "recommended by Ray that's going to be enough for you it is important to understand the",
    "start": "1417159",
    "end": "1423039"
  },
  {
    "text": "inner workings of Ray that knowledge is crucial for debugging issues and",
    "start": "1423039",
    "end": "1428520"
  },
  {
    "text": "performance tuning third is don't share your Rus trust me at least don't share red for",
    "start": "1428520",
    "end": "1434960"
  },
  {
    "text": "critical workloads and lastly R serve is amazing amazing you should go use it we've been",
    "start": "1434960",
    "end": "1440120"
  },
  {
    "text": "using it for 2 years and we are very happy with it I want to thank everyone at cavio on",
    "start": "1440120",
    "end": "1446440"
  },
  {
    "text": "my team who has worked on Dart online over the years um and got to where it is",
    "start": "1446440",
    "end": "1452039"
  },
  {
    "start": "1452000",
    "end": "1885000"
  },
  {
    "text": "now and that's all I have for you today thank you all for",
    "start": "1452039",
    "end": "1457399"
  },
  {
    "text": "attending uh thank you Smith uh for Q&A please speak into this microphone because this talk is recorded So we want",
    "start": "1460159",
    "end": "1466840"
  },
  {
    "text": "to record your question as well",
    "start": "1466840",
    "end": "1470360"
  },
  {
    "text": "[Music] um hi uh my name is feder um so my",
    "start": "1474820",
    "end": "1481559"
  },
  {
    "text": "question is like you moved to single um application pure cluster right um so",
    "start": "1481559",
    "end": "1489440"
  },
  {
    "text": "with that one how you going to handle the uh P application communication like",
    "start": "1489440",
    "end": "1497279"
  },
  {
    "text": "in previous you can still do the remote calls right mhm so now what's what's",
    "start": "1497279",
    "end": "1504679"
  },
  {
    "text": "next yes yeah so there's two two ways to handle that um we we're still debating",
    "start": "1504679",
    "end": "1511279"
  },
  {
    "text": "we haven't moved to a single application per approach um but there's two ways to handle that one is you deploy the other",
    "start": "1511279",
    "end": "1517760"
  },
  {
    "text": "application on that cluster as well so you can do internal remote calls um but",
    "start": "1517760",
    "end": "1523799"
  },
  {
    "text": "if that doesn't make too much sense cuz the other application doesn't receive too much traffic",
    "start": "1523799",
    "end": "1528919"
  },
  {
    "text": "um you can just have a separate client and you can make a separate Network call across the other",
    "start": "1528919",
    "end": "1536600"
  },
  {
    "text": "system thank you uh and the next one is on a head note you said that uh when",
    "start": "1536600",
    "end": "1543760"
  },
  {
    "text": "radius is going to die and then uh everything is like the head note is is",
    "start": "1543760",
    "end": "1549640"
  },
  {
    "text": "not going to restart right yep is it something that leness and Readiness props should solve sorry can you repeat",
    "start": "1549640",
    "end": "1557480"
  },
  {
    "text": "uh kubernetes Readiness and liveness props is going to solve that right uh",
    "start": "1557480",
    "end": "1563080"
  },
  {
    "text": "I'm not too aware of those um but yeah",
    "start": "1563080",
    "end": "1568480"
  },
  {
    "text": "so generally if redis were to go down so the GCS that's running on the head node",
    "start": "1568480",
    "end": "1573760"
  },
  {
    "text": "it constantly talks to redis to fetch all of the cluster details um and so if",
    "start": "1573760",
    "end": "1581240"
  },
  {
    "text": "redis were to go down it won't connect and then it won't have any information on where all of the rest of the working",
    "start": "1581240",
    "end": "1586679"
  },
  {
    "text": "nodes are available and then it can connect to",
    "start": "1586679",
    "end": "1591240"
  },
  {
    "text": "anything hi I um I can't quite remember if you went over the observability piece",
    "start": "1599880",
    "end": "1604960"
  },
  {
    "text": "but I'm curious of what portion um of Ray do you get like observability metrics from and what do you have to",
    "start": "1604960",
    "end": "1610159"
  },
  {
    "text": "scaffold yourself and switching from microservices to Ray um did you get any benefit on that or was this still a lot",
    "start": "1610159",
    "end": "1616159"
  },
  {
    "text": "of it self-rolled yes so um Ray provides you with a lot of",
    "start": "1616159",
    "end": "1622039"
  },
  {
    "text": "metrics um we use Ray to have like you know it gives you application Health it",
    "start": "1622039",
    "end": "1628399"
  },
  {
    "text": "gives you how many requests you're you're getting it'll tell you how much time it's taking to process the request",
    "start": "1628399",
    "end": "1635320"
  },
  {
    "text": "um some things that it doesn't tell you very well I guess is",
    "start": "1635320",
    "end": "1641480"
  },
  {
    "text": "like are all of her replicas healthy at the same time and so that's why the health Che service that we have it",
    "start": "1641480",
    "end": "1646799"
  },
  {
    "text": "publishes metrics um because we want to know did one of the replica suddenly go",
    "start": "1646799",
    "end": "1652279"
  },
  {
    "text": "down even if the entire cluster is fine um so that's what we need to build ourselves that makes a lot of sense",
    "start": "1652279",
    "end": "1658840"
  },
  {
    "text": "thank you yeah hello um Narendra um so we don't uh",
    "start": "1658840",
    "end": "1665440"
  },
  {
    "text": "use uh Ray Ser at the moment but we use Ray job uh operator um so when you say",
    "start": "1665440",
    "end": "1672080"
  },
  {
    "text": "going to U cluster per application uh do you mean uh using Ray service um",
    "start": "1672080",
    "end": "1679919"
  },
  {
    "text": "operator yes yeah so for every ml application that you build so having one",
    "start": "1679919",
    "end": "1685320"
  },
  {
    "text": "Ray cluster for each ml application um racer supports you having like multiple",
    "start": "1685320",
    "end": "1691559"
  },
  {
    "text": "applications um so you could run like five and points five different models on the same rate cluster um so that's",
    "start": "1691559",
    "end": "1698320"
  },
  {
    "text": "that's the single single cluster for single cluster for all all of your applications approach um yeah the other",
    "start": "1698320",
    "end": "1704919"
  },
  {
    "text": "one is Race Service so you have multiple Race Service instances got it uh second question um regarding like latencies I",
    "start": "1704919",
    "end": "1712679"
  },
  {
    "text": "don't know if you talked about it but um um do you use um um race of right now",
    "start": "1712679",
    "end": "1718880"
  },
  {
    "text": "for batch inference as well as um um online inference yes so for batch",
    "start": "1718880",
    "end": "1725399"
  },
  {
    "text": "inference we use Ray jobs um similar to how you guys do it um yeah those two are",
    "start": "1725399",
    "end": "1730960"
  },
  {
    "text": "like very separate systems right now and we want to have like better Integrations okay sounds good thank you",
    "start": "1730960",
    "end": "1739080"
  },
  {
    "text": "um so I had a question about um sort of increasing like fault tolerance and I",
    "start": "1740760",
    "end": "1746200"
  },
  {
    "text": "was wondering have you thought about scaling up the number of head nodes um in a ray cluster sort of similar to how",
    "start": "1746200",
    "end": "1752799"
  },
  {
    "text": "a cuetes might scale up the number of nodes in a control plane for a managed kubernetes solution yeah I'm not aware",
    "start": "1752799",
    "end": "1759840"
  },
  {
    "text": "if you are able to scale up head nodes cuz cubra handles spinning up head nodes",
    "start": "1759840",
    "end": "1765840"
  },
  {
    "text": "and worker nodes we don't really have control over how many head no you how many working noes you have um so yeah",
    "start": "1765840",
    "end": "1773640"
  },
  {
    "text": "that's definitely a question for CU folks but I don't think that's possible at the",
    "start": "1773640",
    "end": "1779640"
  },
  {
    "text": "moment so I have a question you know I want to know your insights for some",
    "start": "1781200",
    "end": "1786679"
  },
  {
    "text": "particular applications especially when their response payload size is quite a big you know code R like a gig or two",
    "start": "1786679",
    "end": "1793880"
  },
  {
    "text": "gig do you have any suggestions you know in this resource SC how to handle that",
    "start": "1793880",
    "end": "1799240"
  },
  {
    "text": "and for having very large payloads payload yes yes um yeah if you have very",
    "start": "1799240",
    "end": "1805399"
  },
  {
    "text": "large payloads I think like reserve is going to handle it um but you should",
    "start": "1805399",
    "end": "1811440"
  },
  {
    "text": "make sure that it doesn't interfere with other um applications so if you have",
    "start": "1811440",
    "end": "1816880"
  },
  {
    "text": "like multiple end points within your same Ray cluster and if you get like large payloads HTTP proxies is going to",
    "start": "1816880",
    "end": "1824760"
  },
  {
    "text": "like slow down cuz it needs to process the entire payload before it forwards the",
    "start": "1824760",
    "end": "1830960"
  },
  {
    "text": "request hey there uh I was just wondering if you could briefly elaborate on your experience uh providing that",
    "start": "1831919",
    "end": "1837399"
  },
  {
    "text": "kind of like selfs serve um race serve deployments to data scientists sort of what went well and what didn't go well",
    "start": "1837399",
    "end": "1843840"
  },
  {
    "text": "yeah the thing that went really well is having a lot of examples um you so you",
    "start": "1843840",
    "end": "1850039"
  },
  {
    "text": "you saw that we have a template um the template is very useful they just need to copy it and like fill in whatever",
    "start": "1850039",
    "end": "1855639"
  },
  {
    "text": "they need um there are some other examples where you know you need to call another application um within that same",
    "start": "1855639",
    "end": "1863360"
  },
  {
    "text": "re cluster so providing a lot of examples have gone really well um and",
    "start": "1863360",
    "end": "1869080"
  },
  {
    "text": "yeah I guess minimizing the infrastructure components is is always good yeah all right thank",
    "start": "1869080",
    "end": "1878440"
  },
  {
    "text": "you Smith another round of applause for Smith thank you thank you",
    "start": "1878440",
    "end": "1885158"
  }
]