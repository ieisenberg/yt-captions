[
  {
    "text": "okay we'll get started um welcome everyone super super excited for all of you to join us from different parts of",
    "start": "160",
    "end": "6240"
  },
  {
    "text": "the world this is our second bay area ray meetup and we're really happy to get",
    "start": "6240",
    "end": "11280"
  },
  {
    "text": "this cadence going and we're going to be having this on a monthly basis so this is the second one since the last one",
    "start": "11280",
    "end": "16720"
  },
  {
    "text": "that we actually had in in in february um you know if you if you",
    "start": "16720",
    "end": "22240"
  },
  {
    "text": "want to talk at this particular meetup just just give me uh um get in touch with me my email is jules",
    "start": "22240",
    "end": "28960"
  },
  {
    "text": "at anyscale.com we would love to hear how you guys are using ray in your use",
    "start": "28960",
    "end": "34079"
  },
  {
    "text": "cases or in production and we would like to actually hear your stories to share that with the communities we're very",
    "start": "34079",
    "end": "39440"
  },
  {
    "text": "very very excited as you know you know community is central and core to the essence of any successful open source",
    "start": "39440",
    "end": "45039"
  },
  {
    "text": "project so this is no any different and we do love to love your participation and so spread",
    "start": "45039",
    "end": "50960"
  },
  {
    "text": "the word and spread the love for um for ray anyway we actually have a great",
    "start": "50960",
    "end": "56320"
  },
  {
    "text": "uh presentation today a couple of um introductions and announcements before we actually head into with the beefies",
    "start": "56320",
    "end": "63280"
  },
  {
    "text": "uh the first talk we're gonna actually have from our ml team uh led by will he's going to actually accept the",
    "start": "63280",
    "end": "69439"
  },
  {
    "text": "technical context he's gonna give us an introduction of why what's the motivation behind behind ray train and",
    "start": "69439",
    "end": "75920"
  },
  {
    "text": "what aspects of deep learning are some of the shortcomings that ray train tries to address and why you should actually",
    "start": "75920",
    "end": "82000"
  },
  {
    "text": "be using it and how easy it is for you and then matthew dang is going to somehow tie up all the things that that",
    "start": "82000",
    "end": "88960"
  },
  {
    "text": "that will is going to talk about in in in in a demo where you will actually get",
    "start": "88960",
    "end": "94000"
  },
  {
    "text": "to see how easy it is for you to use ray train and how intuitive the api so you get an intuition from that particular",
    "start": "94000",
    "end": "100640"
  },
  {
    "text": "aspects of it and then finally we have um as you know one of the great draws about about ray",
    "start": "100640",
    "end": "108000"
  },
  {
    "text": "from both the python ecosystem developers as well as from the ray ecosystem is the integration of all",
    "start": "108000",
    "end": "114479"
  },
  {
    "text": "these uh common and and popular ml libraries and i think today you're going",
    "start": "114479",
    "end": "119600"
  },
  {
    "text": "to hear from mark who is actually the software engineer with meta ai and partners engineering who's going to",
    "start": "119600",
    "end": "125280"
  },
  {
    "text": "share with us the latest torch x integration with ray where the pytorch developer community can now",
    "start": "125280",
    "end": "131520"
  },
  {
    "text": "actually take advantage of this particular new integration that we have today where they can actually submit their training jobs on the array cluster",
    "start": "131520",
    "end": "138480"
  },
  {
    "text": "why the new rescheduler which have which they have actually developed we get uh insight into that",
    "start": "138480",
    "end": "144480"
  },
  {
    "text": "now um we have a couple of announcements no meetups is without any particular announcements i just wanted to let you",
    "start": "144480",
    "end": "150560"
  },
  {
    "text": "know that in about a few weeks we actually have this production rl summit coming up is free of charge um so if you",
    "start": "150560",
    "end": "158560"
  },
  {
    "text": "want to um find out more about rl how it's actually been used in the in the community and in the real world we have",
    "start": "158560",
    "end": "165760"
  },
  {
    "text": "uh notable keynote speakers as well as some of the people who in the industry share with us how they're actually using",
    "start": "165760",
    "end": "171440"
  },
  {
    "text": "the the the reinforcement library so you actually get a chance to have uh idea",
    "start": "171440",
    "end": "177519"
  },
  {
    "text": "into this production workloads and then finally if you're interested to get some hands-on um",
    "start": "177519",
    "end": "183120"
  },
  {
    "text": "idea about about how really being used uh we have a tutorial that's going to be for the rest of the afternoon and it's",
    "start": "183120",
    "end": "189920"
  },
  {
    "text": "going to be led by sewen mika who is one of the lead maintainers so if you are in this particular meetup and you actually",
    "start": "189920",
    "end": "195040"
  },
  {
    "text": "do want to uh sign up for it you can actually use that particular url the slides we will share with you and you",
    "start": "195040",
    "end": "201680"
  },
  {
    "text": "can get a 50 discount uh and then finally we actually have another meetup coming up a month from",
    "start": "201680",
    "end": "207599"
  },
  {
    "text": "today which is going to be exclusively about how do you actually productionize ml models at scale using ratio and we'll",
    "start": "207599",
    "end": "214560"
  },
  {
    "text": "have people and software engineers and communities from the ray so team who's going to share us uh all about how you",
    "start": "214560",
    "end": "220400"
  },
  {
    "text": "actually productionize things at scale so i think that's all i had when with that uh i'm gonna stop sharing and",
    "start": "220400",
    "end": "228239"
  },
  {
    "text": "love to have will you just take it over all righty let's uh let's go for it um",
    "start": "228239",
    "end": "235120"
  },
  {
    "text": "cool so uh first talk uh and then we'll be talking about ray train and uh",
    "start": "235120",
    "end": "241920"
  },
  {
    "text": "we changed the title a little bit because we kind of wanted to reflect uh what it is you know it's a high level",
    "start": "241920",
    "end": "246959"
  },
  {
    "text": "library for deep learning training and we'll be talking about a few things and",
    "start": "246959",
    "end": "252640"
  },
  {
    "text": "um i think what i want to preface this by saying is that we do assume some kind of knowledge of rey but we will talk",
    "start": "252640",
    "end": "258160"
  },
  {
    "text": "about like why the benefits of ray are are here and how they you know come to bear and when we're",
    "start": "258160",
    "end": "263520"
  },
  {
    "text": "talking about deep learning but we'll we'll kind of be focusing more on that actual use case of like deep",
    "start": "263520",
    "end": "269199"
  },
  {
    "text": "learning training distributed clusters when your data gets big and you want to like do that kind of thing",
    "start": "269199",
    "end": "274720"
  },
  {
    "text": "uh and then at the end of it we'll have the fun part where matt will actually go through a demo and you'll get to see kind of things as they",
    "start": "274720",
    "end": "280840"
  },
  {
    "text": "are so what are we gonna watch so first uh let's get into it we're gonna talk about some problems in deep learning",
    "start": "280840",
    "end": "286960"
  },
  {
    "text": "training uh things that you know you guys may be familiar with but some that you know uh",
    "start": "286960",
    "end": "292240"
  },
  {
    "text": "may also be from a lot of the other use cases that we see uh then we'll start talking about ray train uh simple scaling flexibility the",
    "start": "292240",
    "end": "299120"
  },
  {
    "text": "high level api uh as well as the low level optimizations that are kind of the three goals that we'd like to center around",
    "start": "299120",
    "end": "305199"
  },
  {
    "text": "and have the library serve then we'll talk a little bit about the road map you know where things are going",
    "start": "305199",
    "end": "310880"
  },
  {
    "text": "um especially in the first half of 2022 and then we'll jump into the deep learning demo",
    "start": "310880",
    "end": "317680"
  },
  {
    "text": "so without further ado uh i think it's useful to talk about some of the problems that people have",
    "start": "317680",
    "end": "323759"
  },
  {
    "text": "when they're training these big models uh and they're using deep learning frameworks pie torch tensorflow um you",
    "start": "323759",
    "end": "329600"
  },
  {
    "text": "know so on and so forth uh and i think it's we're just like in a really uh lucky place where we get to",
    "start": "329600",
    "end": "335520"
  },
  {
    "text": "see a lot of these users that come in have all these different use cases and we try to build libraries around it both",
    "start": "335520",
    "end": "340960"
  },
  {
    "text": "generally in rey and then specifically for this in ray train that address those needs and so it's really um",
    "start": "340960",
    "end": "347120"
  },
  {
    "text": "it's just a really useful vantage point to see that and so i think it's useful to share that so some very obvious problems that you",
    "start": "347120",
    "end": "353840"
  },
  {
    "text": "know we see when you're like having a deep learning project is that you know training will take a long time there's",
    "start": "353840",
    "end": "359199"
  },
  {
    "text": "too many too much data to fit in one node if you really want to start having these models that that have",
    "start": "359199",
    "end": "365120"
  },
  {
    "text": "really powerful inference possible you're gonna need that kind of thing and you may even graduate to large",
    "start": "365120",
    "end": "370240"
  },
  {
    "text": "models that don't fit on one device particularly if you're saying the graphical networks or the",
    "start": "370240",
    "end": "375280"
  },
  {
    "text": "natural language processing field and these problems often are problems in themselves but then there's complexity",
    "start": "375280",
    "end": "381440"
  },
  {
    "text": "that comes with trying to code around them and like develop these uh you know distributed frameworks or use them in a",
    "start": "381440",
    "end": "388080"
  },
  {
    "text": "way that like makes your iteration go slower and makes the understanding of the code even harder for people to onboard and use it",
    "start": "388080",
    "end": "394400"
  },
  {
    "text": "so it's kind of a double whammy like you have these problems that are inherent in in the actual problem itself and then",
    "start": "394400",
    "end": "399840"
  },
  {
    "text": "kind of the resultant thing that comes from trying to work around them so the natural thing is to go distributed",
    "start": "399840",
    "end": "406560"
  },
  {
    "text": "right so you know you can't fit it all in one thing you want to go faster there's really only one solution because at some point you can't get a bigger",
    "start": "406560",
    "end": "412560"
  },
  {
    "text": "node or it's not economical or whatever so again this is kind of that second step right you have all these problems",
    "start": "412560",
    "end": "419199"
  },
  {
    "text": "but now you have to manage a new infrastack you may have to rewrite all of your training code you have to deal with the increased costs you may have to",
    "start": "419199",
    "end": "425039"
  },
  {
    "text": "set up these new optimizations or you want to use floating point 16 or deep speed or you know whatever it is",
    "start": "425039",
    "end": "431440"
  },
  {
    "text": "you want to set up nickel and then finally you may also want to tune hyper parameters and paralyze it over a bunch of nodes and understand",
    "start": "431440",
    "end": "437759"
  },
  {
    "text": "that you're not just throwing money away right um and so all of this stuff is really painful from like a development",
    "start": "437759",
    "end": "443599"
  },
  {
    "text": "perspective it's no longer you have all these problems that are not inherent to the actual thing you want to be doing",
    "start": "443599",
    "end": "448880"
  },
  {
    "text": "but just thinking about your model architecture you know the data waiting the cleaning that kind of thing that",
    "start": "448880",
    "end": "454080"
  },
  {
    "text": "really is is what we like to do as machine learning engineers so",
    "start": "454080",
    "end": "459440"
  },
  {
    "text": "our idea behind this of how we fix it is to have a more ideal solution you know pretend that it's perfectly ideal but",
    "start": "459440",
    "end": "465680"
  },
  {
    "text": "something that's easy to onboard abstracts away the infrastructure gives me that fast iteration speed",
    "start": "465680",
    "end": "472800"
  },
  {
    "text": "um allows me to use things like gpus spot instances is another huge one right as far as controlling costs",
    "start": "472800",
    "end": "479520"
  },
  {
    "text": "and but also something that's like portable from any cloud provider um you know being locked into one cloud and one",
    "start": "479520",
    "end": "485199"
  },
  {
    "text": "way that you're charged or even one particular system is just not as ideal as being able to run",
    "start": "485199",
    "end": "490840"
  },
  {
    "text": "anywhere um and then finally it integrates well with other things like if you have this great framework and it",
    "start": "490840",
    "end": "495919"
  },
  {
    "text": "does all these awesome things but it can't work uh sort of play nicely with others it doesn't really do you a lot of good",
    "start": "495919",
    "end": "501599"
  },
  {
    "text": "so you know the one way that we sort of like to frame this is there's something between uh say just a vanilla",
    "start": "501599",
    "end": "507599"
  },
  {
    "text": "like use of a framework like pytorch which is amazing it has all these great components and like this amazing developer community",
    "start": "507599",
    "end": "513760"
  },
  {
    "text": "um with some like very managed service like a sagemaker or something like that where it is very robust and there's",
    "start": "513760",
    "end": "519518"
  },
  {
    "text": "there should be some kind of middle ground that lets you take that infrastructure out of the picture",
    "start": "519519",
    "end": "524560"
  },
  {
    "text": "so again we see kind of a trade-off and you know no uh no talk would be complete",
    "start": "524560",
    "end": "529680"
  },
  {
    "text": "without a two-axis plot where you know we're up in the right-hand corner but you know that being said i think if you look at this in terms of like the ease",
    "start": "529680",
    "end": "536399"
  },
  {
    "text": "of development being kind of the y-axis and the production readiness you really can start grouping things at",
    "start": "536399",
    "end": "541680"
  },
  {
    "text": "least very um you know bluntly into these two categories of one thing that might be lightweight and it's it's maybe nimble",
    "start": "541680",
    "end": "548880"
  },
  {
    "text": "but it's like difficult to scale it's not that you can't um it just may not be the easiest or the quickest thing to do",
    "start": "548880",
    "end": "554160"
  },
  {
    "text": "it may not be the easiest developer experience and these also don't really include out",
    "start": "554160",
    "end": "559680"
  },
  {
    "text": "of the box ways to use the infrastructure like you kind of have to figure out how to use the infrastructure",
    "start": "559680",
    "end": "564800"
  },
  {
    "text": "yourself you're putting these things up containerizing you're trying to do all these things on the other hand",
    "start": "564800",
    "end": "570399"
  },
  {
    "text": "right on the bottom right we do have these heavyweight things that are sort of built out of the box to scale really well integrate with your",
    "start": "570399",
    "end": "577200"
  },
  {
    "text": "iem roles you know your security uh person your company is really happy but they're a little inflexible they're hard",
    "start": "577200",
    "end": "582720"
  },
  {
    "text": "to customize and so what we try to do is sort of trade those off with great train and make something that is production",
    "start": "582720",
    "end": "588399"
  },
  {
    "text": "ready is work workable with all these things but still allows you to sort of live and play with these these",
    "start": "588399",
    "end": "593519"
  },
  {
    "text": "frameworks that have such great developer communities and components to use",
    "start": "593519",
    "end": "598800"
  },
  {
    "text": "so what's our approach so really the the high level way to think about it is that you know ray",
    "start": "599839",
    "end": "605200"
  },
  {
    "text": "train is is a bridge right it's a bridge from deep learning frameworks uh the ones uh particularly",
    "start": "605200",
    "end": "611040"
  },
  {
    "text": "today like corabot tensorflow and pytorch over to the compute right to the compute",
    "start": "611040",
    "end": "616959"
  },
  {
    "text": "layer the infrastructure layer you know whether it's aws kubernetes on-prem azure you know it doesn't really it",
    "start": "616959",
    "end": "622399"
  },
  {
    "text": "shouldn't really matter um and with retrain you can do that because it's built on top of right and i would say that also there's a",
    "start": "622399",
    "end": "628320"
  },
  {
    "text": "couple components of like ray train also being working really well with the data processing component as well as model tuning and serving and",
    "start": "628320",
    "end": "634959"
  },
  {
    "text": "we'll talk more about that later but like that's also part of part of the value so really you know",
    "start": "634959",
    "end": "640880"
  },
  {
    "text": "like the the most the best crystallization i can do is that we try to make it flexible something that works with the frameworks",
    "start": "640880",
    "end": "646880"
  },
  {
    "text": "you you want it has battery but it also has batteries included in terms of you can mix and",
    "start": "646880",
    "end": "652240"
  },
  {
    "text": "match components if you need tuning great if you need serving great if you need data processing and loading also",
    "start": "652240",
    "end": "657360"
  },
  {
    "text": "great but you also get like a high level framework that is low level optimizations so there's sort of a",
    "start": "657360",
    "end": "662640"
  },
  {
    "text": "stable way that you can integrate around these things and develop features so that maybe you know the other data",
    "start": "662640",
    "end": "667680"
  },
  {
    "text": "scientists on your team or if you're building a platform or a library they can continue to use that same api",
    "start": "667680",
    "end": "673920"
  },
  {
    "text": "benefiting over time from like lower level optimizations that come through whether they be at the gpu level deep",
    "start": "673920",
    "end": "679600"
  },
  {
    "text": "speed backwards on so forth and then finally it should be easy to scale not just in terms of where you can run",
    "start": "679600",
    "end": "685519"
  },
  {
    "text": "it but also in terms of the actual code itself to integrate with at the end of the day the model code that you have",
    "start": "685519",
    "end": "691839"
  },
  {
    "text": "and we'll talk a little bit more about these in turn in total i think what we really really",
    "start": "691839",
    "end": "697839"
  },
  {
    "text": "try to like adhere to is like a division of labor you know we should do the things that we're good at right and deep",
    "start": "697839",
    "end": "703519"
  },
  {
    "text": "learning frameworks like pytorch horrorbot tensorflow do an amazing job at you know modules components patterns",
    "start": "703519",
    "end": "708800"
  },
  {
    "text": "training loops gradient communication protocols great developer community that's all perfect ray strengths right are managing compute",
    "start": "708800",
    "end": "716480"
  },
  {
    "text": "this anticipation and scheduling around data locality and constraints and moving data around and you know making sure",
    "start": "716480",
    "end": "722160"
  },
  {
    "text": "that the performance is great for these distributed applications and it's a way to distribute any python library you",
    "start": "722160",
    "end": "728560"
  },
  {
    "text": "want and not have to think about distributed systems so that's sort of the the marriage here",
    "start": "728560",
    "end": "734240"
  },
  {
    "text": "that's going on and that we hope to achieve so with that said we've kind of",
    "start": "734240",
    "end": "740399"
  },
  {
    "text": "discussed like the the main components that make uh you know this union uh between like",
    "start": "740399",
    "end": "746240"
  },
  {
    "text": "these great frameworks and apis for developers to use as well as the compute side um and so i",
    "start": "746240",
    "end": "752079"
  },
  {
    "text": "want to just kind of step through each of these components talk about how we achieve them or how we work towards them",
    "start": "752079",
    "end": "757680"
  },
  {
    "text": "and then we can kind of uh wrap the end and sort of look at it holistically",
    "start": "757680",
    "end": "763279"
  },
  {
    "text": "so first is simple scaling right and it should be easy right in our first uh problem we talked about how we wanted to",
    "start": "763279",
    "end": "769839"
  },
  {
    "text": "make training go faster we wanted to have an easier way to to do these things at larger scale but we didn't want to",
    "start": "769839",
    "end": "776959"
  },
  {
    "text": "incur all the developer overhead and costs that usually comes with trying to like achieve these solutions",
    "start": "776959",
    "end": "783279"
  },
  {
    "text": "so we really tried to make it as you know as dead simple as possible and you know matt is going to have a way better",
    "start": "783279",
    "end": "789040"
  },
  {
    "text": "example at the end where you'll actually get to see this in the live notebook but fundamentally right if you have step one",
    "start": "789040",
    "end": "795600"
  },
  {
    "text": "your your training code in one function great you just wrapped in a function the next step",
    "start": "795600",
    "end": "801760"
  },
  {
    "text": "you sort of prepare your model and data loader and essentially what this is doing under the hood is making sure that it can be distributed right",
    "start": "801760",
    "end": "808720"
  },
  {
    "text": "and these are like built-in functions you just do yeah in this case model train.torch.repairmodel afterwards",
    "start": "808720",
    "end": "814079"
  },
  {
    "text": "shot repair data loader awesome and then finally if you're familiar with like a multi-processing framework where",
    "start": "814079",
    "end": "819360"
  },
  {
    "text": "you set up kind of like a pool or something to run it it's a little similar what we call it is a trainer and",
    "start": "819360",
    "end": "824480"
  },
  {
    "text": "you simply specify the number of workers you know the resources you'd like to provide to it you do dot start you run",
    "start": "824480",
    "end": "830160"
  },
  {
    "text": "it and you shut it down and that should be as easy as it needs to be to like run these workflows and so",
    "start": "830160",
    "end": "836160"
  },
  {
    "text": "really all you have to do uh as a library maintainer or someone who's building a platform is expose these",
    "start": "836160",
    "end": "841360"
  },
  {
    "text": "trainers maybe you subclass around them but whatever user code needs to go into it they just have to provide a function",
    "start": "841360",
    "end": "847760"
  },
  {
    "text": "and they're kind of off the races again multi-node and multi-gpu is",
    "start": "847760",
    "end": "852880"
  },
  {
    "text": "extremely important to us so up at the top the first example is like how you might launch a cluster",
    "start": "852880",
    "end": "858480"
  },
  {
    "text": "right i'm going to do ray up my cluster config and then i might submit a job and my",
    "start": "858480",
    "end": "863839"
  },
  {
    "text": "it's you know python my script and in my script you know it could be a trainer right which has specifies again my back",
    "start": "863839",
    "end": "870240"
  },
  {
    "text": "end i could choose one worker i could do a hundred i could use gpus um it doesn't really matter it shouldn't",
    "start": "870240",
    "end": "876399"
  },
  {
    "text": "matter for the code that you write right your infrastructure shouldn't be this thing that's like weirdly entwined",
    "start": "876399",
    "end": "881839"
  },
  {
    "text": "with all of your code that you're trying to like you know design training around and stuff like that",
    "start": "881839",
    "end": "888000"
  },
  {
    "text": "so you should be able to move between laptop and the cluster as well uh and one way that we like you know",
    "start": "889360",
    "end": "894880"
  },
  {
    "text": "kind of pictorially show this is like you know you can do it iteratively with like the first method which many of you are familiar with right i can do",
    "start": "894880",
    "end": "901440"
  },
  {
    "text": "ray.init and i can give it array address you know my head node ip and port or if",
    "start": "901440",
    "end": "906959"
  },
  {
    "text": "i use any scale or any other provider you know i just provide a different url string in this case just any scale",
    "start": "906959",
    "end": "912800"
  },
  {
    "text": "cluster name the point is all the code that follows that should run on the cluster right and raytrain is no different it's",
    "start": "912800",
    "end": "918399"
  },
  {
    "text": "no exception it's not something different it works just like everything else in ray does",
    "start": "918399",
    "end": "923920"
  },
  {
    "text": "the second way is more about productionizing or if you're developing a platform around this you may want to have something that like you don't need",
    "start": "923920",
    "end": "929600"
  },
  {
    "text": "to keep your your laptop on overnight right you want to have like a programmatic way to do this great you",
    "start": "929600",
    "end": "934959"
  },
  {
    "text": "know you boot up your cluster you do array job submit and you submit your script to the cluster the driver",
    "start": "934959",
    "end": "940560"
  },
  {
    "text": "essentially happens on the cluster it runs and you can you know pull the logs from them and that sort of thing",
    "start": "940560",
    "end": "946560"
  },
  {
    "text": "so again you know it should be easy to scale that includes like where it runs how it runs but it also includes the",
    "start": "946560",
    "end": "952079"
  },
  {
    "text": "modality in which i run it like is this something where i package something up and run it in production is it iterative",
    "start": "952079",
    "end": "958160"
  },
  {
    "text": "your choice so again simple scaling that's important",
    "start": "958160",
    "end": "963759"
  },
  {
    "text": "but it also should be flexible right and what we try to do is make this as programmatic as possible again i",
    "start": "963759",
    "end": "969120"
  },
  {
    "text": "mentioned that functional interface where you just have your train function and you edit again uh you give it to the",
    "start": "969120",
    "end": "974399"
  },
  {
    "text": "cluster but also the about batteries included and we'll talk a little bit about some of the libraries that work really well with freight train and make it sort of a",
    "start": "974399",
    "end": "981199"
  },
  {
    "text": "more appealing thing to use so uh first of all i mean you guys know",
    "start": "981199",
    "end": "986720"
  },
  {
    "text": "this but like you know just to kind of spell it out i mean there there's really upstream and downstream things that happen as a result of machine learning",
    "start": "986720",
    "end": "992959"
  },
  {
    "text": "training right on the far left right you start and you gotta you gotta do some extraction some feature pre-processing",
    "start": "992959",
    "end": "998959"
  },
  {
    "text": "we call the sql land it may not necessarily be in sql but it's usually these kind of big processes which are",
    "start": "998959",
    "end": "1004000"
  },
  {
    "text": "like shoveling data transforming them in some way putting them down and you know being able to like rewrite them",
    "start": "1004000",
    "end": "1011519"
  },
  {
    "text": "there's also this last mile data ingest which we'll talk about in a second that's very important for performance",
    "start": "1011519",
    "end": "1016560"
  },
  {
    "text": "but you have uh you know machine learning in this case deep learning training and out of that might you might want to tune you might want to serve you",
    "start": "1016560",
    "end": "1022880"
  },
  {
    "text": "may want to monitor it and what we actually find is that there's a lot of gotchas in that process of like training",
    "start": "1022880",
    "end": "1028480"
  },
  {
    "text": "things being able to pull that data in a way where you actually saturate your gpus which data sets handles",
    "start": "1028480",
    "end": "1034079"
  },
  {
    "text": "um and being able to like connect to all the different things that you actually want to do with your model because after you",
    "start": "1034079",
    "end": "1039760"
  },
  {
    "text": "train it you know you want to use it um so these are like the kinds of things that are included and so you guys are",
    "start": "1039760",
    "end": "1046319"
  },
  {
    "text": "familiar with like the ray cake diagram if you've seen it before but really the way to think about trane is that it's kind of its own ecosystem in a sense so",
    "start": "1046319",
    "end": "1052960"
  },
  {
    "text": "at the bottom we have compute right you got to run it on vm somewhere uh you got to have a linux uh thing running",
    "start": "1052960",
    "end": "1058240"
  },
  {
    "text": "somewhere um though you can actually run brand windows but you know if you have a huge deployment probably unix and then you",
    "start": "1058240",
    "end": "1064320"
  },
  {
    "text": "have recore which is this like fundamental layer of like reads and writes it's kind of like the operating system array if you do ray.get right",
    "start": "1064320",
    "end": "1070799"
  },
  {
    "text": "output and then there's also obviously these distributed libraries like rlib racer and tune that ray works with but ray",
    "start": "1070799",
    "end": "1078000"
  },
  {
    "text": "train on top of it you know you can have these sort of um like your own library your own app um but it works well with",
    "start": "1078000",
    "end": "1084640"
  },
  {
    "text": "these other third-party libraries things like you know uh pie torch or tensorflow weights and",
    "start": "1084640",
    "end": "1090400"
  },
  {
    "text": "biases um you know you kind of name it these other things work seamlessly as a part of your",
    "start": "1090400",
    "end": "1096480"
  },
  {
    "text": "train funks and you don't have to like really think about it so we really see actually that's the big",
    "start": "1096480",
    "end": "1102000"
  },
  {
    "text": "pattern that we really see are people using ray train as a way to build platforms um and sort of be able to expose",
    "start": "1102000",
    "end": "1108640"
  },
  {
    "text": "something to their users in a way where like you don't have to worry about it the high level api stays the same you get",
    "start": "1108640",
    "end": "1114640"
  },
  {
    "text": "the low level benefits over time which will be our third point here in a second",
    "start": "1114640",
    "end": "1119919"
  },
  {
    "text": "again with hyper parameter tuning it's really the same sort of thing it's like the the way to think about it is very similar we have a training function",
    "start": "1119919",
    "end": "1126559"
  },
  {
    "text": "and that you know trainer two tuned trainable terrain funk and then you do tune out run and your config here is",
    "start": "1126559",
    "end": "1132160"
  },
  {
    "text": "where you actually hand in like your hybrid parameter grid you know data or you know number of splits or whatever",
    "start": "1132160",
    "end": "1138080"
  },
  {
    "text": "you might be wanting to tune over and the example here is very similar uh in terms of like",
    "start": "1138080",
    "end": "1144080"
  },
  {
    "text": "the way you think about a functional interface um you know i've got all the different searchers or schedulers i",
    "start": "1144080",
    "end": "1149120"
  },
  {
    "text": "might care about i can define a space uh and then i can run it and then i can load it from a checkpoint later and",
    "start": "1149120",
    "end": "1155600"
  },
  {
    "text": "cloud storage and all those kinds of nice things that you might want and pull it back down to your driver",
    "start": "1155600",
    "end": "1160720"
  },
  {
    "text": "no more ssh into nodes and like you know reading things and try to unpickle it and all that stuff",
    "start": "1160720",
    "end": "1166080"
  },
  {
    "text": "and you can of course track your training with tensorboard with raytrane it's important to know that your loss",
    "start": "1166080",
    "end": "1171120"
  },
  {
    "text": "curves are decreasing and stuff like that and of course something that's actually really new this is the first time we're",
    "start": "1171120",
    "end": "1177200"
  },
  {
    "text": "really talking about it in a presentation like this is um we now have a great integration with pytorch profiler shout out to the uh the pytorch",
    "start": "1177200",
    "end": "1184000"
  },
  {
    "text": "folks thanks a lot mark and this is a really great way to find bottlenecks in your code",
    "start": "1184000",
    "end": "1189120"
  },
  {
    "text": "and that's one of the top things we hear as a request is like i've got this thing running on 200 nodes i'm trying to",
    "start": "1189120",
    "end": "1194960"
  },
  {
    "text": "understand like why is it running slowly can it be running faster what's going on and then tools like these are invaluable",
    "start": "1194960",
    "end": "1200880"
  },
  {
    "text": "and so being able to use these as a part of raytran is really important to us",
    "start": "1200880",
    "end": "1207520"
  },
  {
    "text": "and so you know this is kind of like a you know don't believe us you can believe them there's a lot of people who",
    "start": "1207520",
    "end": "1213360"
  },
  {
    "text": "who kind of see the value of this flexibility and the connections to like different libraries that that are out there um and so all of these folks in",
    "start": "1213360",
    "end": "1220559"
  },
  {
    "text": "various different capacities are using are using ray itself and some of them using ray train",
    "start": "1220559",
    "end": "1227280"
  },
  {
    "text": "so third and final point and then uh we'll go quickly through the road map and get to the demo because i know we wanna we",
    "start": "1227360",
    "end": "1234080"
  },
  {
    "text": "wanna really see that um but you know i think it's important to realize that it's a high-level api with",
    "start": "1234080",
    "end": "1239200"
  },
  {
    "text": "low-level optimizations and the way that um you know it's first important to talk about is like",
    "start": "1239200",
    "end": "1244400"
  },
  {
    "text": "underneath the hood right ray train is essentially just an orchestrator the communication that actually happens for",
    "start": "1244400",
    "end": "1249840"
  },
  {
    "text": "example in a pie torch script is actually going on in pi torch land like we're not re-innovating or trying to",
    "start": "1249840",
    "end": "1256000"
  },
  {
    "text": "change the way that pi torch like passes gradients around like they're the experts they know a great way to do that",
    "start": "1256000",
    "end": "1261840"
  },
  {
    "text": "and that's really performance what does need to happen is that the data may need to be moved around or",
    "start": "1261840",
    "end": "1267520"
  },
  {
    "text": "loaded into these processes and there also may need to be orchestration in terms of like what runs where how it",
    "start": "1267520",
    "end": "1273360"
  },
  {
    "text": "runs how this job is started how nodes are added or subtracted over time and that's",
    "start": "1273360",
    "end": "1278640"
  },
  {
    "text": "really where the ray uh comes into the picture right again what we know best compute data locality moving things",
    "start": "1278640",
    "end": "1285360"
  },
  {
    "text": "around having that story around scaling and so the cool like uh you know uh",
    "start": "1285360",
    "end": "1291760"
  },
  {
    "text": "analogy that we like to draw is that you know if you have a high level api that's all that people have to worry about and",
    "start": "1291760",
    "end": "1296960"
  },
  {
    "text": "see there's a small surface area of complexity that users have to understand whereas underneath all of this right",
    "start": "1296960",
    "end": "1303600"
  },
  {
    "text": "you've got all these things that you really care about as as a developer or as someone who's building a platform or",
    "start": "1303600",
    "end": "1309120"
  },
  {
    "text": "library like you know multi gpu spot instances deep speed floating point 16",
    "start": "1309120",
    "end": "1314400"
  },
  {
    "text": "um zero copy reads uh data locality is another one like loading speed pipelining all this stuff",
    "start": "1314400",
    "end": "1320559"
  },
  {
    "text": "uh allows you to not only get that for free but in the future it's easy for us to add because we don't have to change that high level api the small thing at",
    "start": "1320559",
    "end": "1327520"
  },
  {
    "text": "the top that the user just needs to know to take their pi torch for their tensorflow or their horobod and just",
    "start": "1327520",
    "end": "1332559"
  },
  {
    "text": "plug it in and that's kind of the commitment so again the tensorflow tensor",
    "start": "1332559",
    "end": "1338559"
  },
  {
    "text": "communication all of that stuff that's all still handled by the framework they should be doing what they do best we're",
    "start": "1338559",
    "end": "1343600"
  },
  {
    "text": "just helping to orchestrate it and add all these things over time so again there's a division of labor",
    "start": "1343600",
    "end": "1349360"
  },
  {
    "text": "that's super important with that said i'm going to go really quickly to the roadmap and then we'll",
    "start": "1349360",
    "end": "1354559"
  },
  {
    "text": "just go to the you know the demo because again a code example is worth you know a thousand words or a thousand slides whatever",
    "start": "1354559",
    "end": "1362080"
  },
  {
    "text": "um just really briefly i mean the things that people the most asking for um you",
    "start": "1362080",
    "end": "1367360"
  },
  {
    "text": "know this is things that we're wrapping up in q1 things like elastic training better metrics and result handling uh we",
    "start": "1367360",
    "end": "1373440"
  },
  {
    "text": "want to have some examples around model parallelism and then that will lead us to think really through like how we support that long term in the best ways",
    "start": "1373440",
    "end": "1380000"
  },
  {
    "text": "because there's a lot of flavors and ways to do that deep speed floating point 16",
    "start": "1380000",
    "end": "1385440"
  },
  {
    "text": "and as well as some integrations around weights and biases the pi torch profiler is already out now the slide was made a",
    "start": "1385440",
    "end": "1391120"
  },
  {
    "text": "while ago as well as sort of a unified higher level ml api that will sort of tie together again all",
    "start": "1391120",
    "end": "1397440"
  },
  {
    "text": "these different libraries q2 will continue with better checkpoints advanced operations for like graph",
    "start": "1397440",
    "end": "1402799"
  },
  {
    "text": "neural networks parameter servers uh as well as moving into beta so if you have more questions about that definitely",
    "start": "1402799",
    "end": "1408640"
  },
  {
    "text": "happy for us to ask uh we just like to kind of get that out there and show people what's on the roadmap and if",
    "start": "1408640",
    "end": "1414240"
  },
  {
    "text": "there's anything we're missing uh please do point it out so with that said uh that's kind of the",
    "start": "1414240",
    "end": "1419919"
  },
  {
    "text": "motivation behind uh ray train i want to pass it off to matt for the actual demo",
    "start": "1419919",
    "end": "1425440"
  },
  {
    "text": "so i will shut up and stop my screen sharing and we can get to the good stuff",
    "start": "1425440",
    "end": "1431840"
  },
  {
    "text": "cool thank you will um can everyone see my screen can i get a thumbs up will you see it yep looks good to me",
    "start": "1434080",
    "end": "1440400"
  },
  {
    "text": "all right awesome um so today uh i'm going to show a quick and simple demo uh of training a resident 50",
    "start": "1440400",
    "end": "1448799"
  },
  {
    "text": "model on the c410 data set using raidtrain and this purposely uses a kind",
    "start": "1448799",
    "end": "1453840"
  },
  {
    "text": "of familiar model and uh and data set just so you could see how easy it is to use and of course uh if you were to try",
    "start": "1453840",
    "end": "1461039"
  },
  {
    "text": "this out yourself you could plug and play with whatever model and uh data set you'd like um so",
    "start": "1461039",
    "end": "1467440"
  },
  {
    "text": "in this demo uh if you want you could follow along i put it on my github which is linked here",
    "start": "1467440",
    "end": "1472559"
  },
  {
    "text": "but we're gonna go over three main steps uh in this demo and we're gonna showcase one how you would run your training",
    "start": "1472559",
    "end": "1478799"
  },
  {
    "text": "script locally uh and again this at this point it has nothing to do with ray or ray train uh but the second thing is we're going",
    "start": "1478799",
    "end": "1485520"
  },
  {
    "text": "to show you how to then convert this uh pytorch script to use raytrain and already start to take the value of it by",
    "start": "1485520",
    "end": "1493039"
  },
  {
    "text": "running it distributedly on your laptop and then finally after we've set that up",
    "start": "1493039",
    "end": "1498320"
  },
  {
    "text": "we'll then showcase what you could then do to then move this job to run on a",
    "start": "1498320",
    "end": "1503360"
  },
  {
    "text": "cluster scale it up run it on a distributed manager on gpus",
    "start": "1503360",
    "end": "1509279"
  },
  {
    "text": "okay so to start i'm going to go over just the vanilla pie torch training function",
    "start": "1509279",
    "end": "1515919"
  },
  {
    "text": "so in this case i'm just going to show you uh i'm just going to run it first and i've already like pre-loaded the results",
    "start": "1515919",
    "end": "1522400"
  },
  {
    "text": "here just to so that this demo is a little bit quicker but what you can see here is that i'm passing in two",
    "start": "1522400",
    "end": "1527600"
  },
  {
    "text": "configuration values to a training function first i have a test mode flag and this",
    "start": "1527600",
    "end": "1532640"
  },
  {
    "text": "is simply just uh specifying that this is gonna use a smaller data set just so that i can show",
    "start": "1532640",
    "end": "1537760"
  },
  {
    "text": "some results for this demo uh in this case i'm just gonna go over 64 records for training and 64 records for",
    "start": "1537760",
    "end": "1543600"
  },
  {
    "text": "validation similarly i'm going to set the number of epochs to 10 again just so that this runs a little faster",
    "start": "1543600",
    "end": "1549840"
  },
  {
    "text": "and what you can see is all i'm doing is passing it to a training function which i'll show in a second uh and what happens is that",
    "start": "1549840",
    "end": "1557679"
  },
  {
    "text": "you can see that my training function is loading some configuration values and printing out some data about its",
    "start": "1557679",
    "end": "1563440"
  },
  {
    "text": "training it's training over 10 epochs and as you can see uh its accuracy isn't",
    "start": "1563440",
    "end": "1569120"
  },
  {
    "text": "very good and it takes about two minutes to to run so",
    "start": "1569120",
    "end": "1574320"
  },
  {
    "text": "why don't we showcase uh what that training function actually first we'll just go through the results",
    "start": "1574320",
    "end": "1580159"
  },
  {
    "text": "um so what i've done in this training function is i've returned the model and what you can see here is i've",
    "start": "1580159",
    "end": "1586400"
  },
  {
    "text": "returned some additional metadata including the loss the accuracy and the model state dict",
    "start": "1586400",
    "end": "1591840"
  },
  {
    "text": "so what we could do with this is we could then verify whether or not we've actually successfully trained a",
    "start": "1591840",
    "end": "1598080"
  },
  {
    "text": "model in this case i have some additional utility functions that are also provided in this python script and",
    "start": "1598080",
    "end": "1603760"
  },
  {
    "text": "all i'm doing is loading uh 20 records of test data and testing my model on it",
    "start": "1603760",
    "end": "1610480"
  },
  {
    "text": "and as you can see what happens is that uh we have 20 different images but my model",
    "start": "1610480",
    "end": "1617520"
  },
  {
    "text": "thinks everything is a bird which isn't that great so by the end of this demo hopefully we can train a model that does",
    "start": "1617520",
    "end": "1623919"
  },
  {
    "text": "better than just predicting burps okay so the next thing that we want to do is",
    "start": "1623919",
    "end": "1629679"
  },
  {
    "text": "talk about how do we move this to raytrain and this is where we're going to start looking at the code",
    "start": "1629679",
    "end": "1634960"
  },
  {
    "text": "so i've switched over here to this python script which includes this training function that i already ran earlier in",
    "start": "1634960",
    "end": "1641440"
  },
  {
    "text": "the earlier step and as you can see all it does is take in a config dictionary",
    "start": "1641440",
    "end": "1648159"
  },
  {
    "text": "and then the rest of the training loop is defined so again this looks like a very normal",
    "start": "1648159",
    "end": "1653360"
  },
  {
    "text": "python uh training strip that might look very familiar to you and we'll see the changes that we had to make to make it",
    "start": "1653360",
    "end": "1658880"
  },
  {
    "text": "work with ray train so first of all since we're moving from a non-distributed to distributed setting",
    "start": "1658880",
    "end": "1664640"
  },
  {
    "text": "we'll have to make sure that our work is actually distributed across our workers so we split our worker batch size based",
    "start": "1664640",
    "end": "1669919"
  },
  {
    "text": "on the number of workers that we have which is provided by this train utility function",
    "start": "1669919",
    "end": "1675600"
  },
  {
    "text": "secondly we have our model through a utility function which just defines the regular",
    "start": "1675600",
    "end": "1680960"
  },
  {
    "text": "pi torch module and what we're doing is we're going to use this again another train utility function to prepare this",
    "start": "1680960",
    "end": "1688080"
  },
  {
    "text": "model which will end up wrapping it in distributed data parallel and mapping it",
    "start": "1688080",
    "end": "1693120"
  },
  {
    "text": "to the proper device if necessary similarly we'll set up our data loaders",
    "start": "1693120",
    "end": "1699039"
  },
  {
    "text": "uh with pi torch as you normally would and what we'll do is we'll use utility functions to wrap these as well and this",
    "start": "1699039",
    "end": "1706320"
  },
  {
    "text": "will also shard the data across your workers and move the the data to the",
    "start": "1706320",
    "end": "1711360"
  },
  {
    "text": "proper device again if necessary everything else should look pretty familiar you're creating your criteria",
    "start": "1711360",
    "end": "1717919"
  },
  {
    "text": "and optimizer and scheduler and you'll just define your regular training loop this should look very very familiar",
    "start": "1717919",
    "end": "1724159"
  },
  {
    "text": "lastly uh some optional things that you might want to do are to report your metrics so what this is going to do is",
    "start": "1724159",
    "end": "1730159"
  },
  {
    "text": "this is going to take your metrics from your distributed workers and send this back to your trainer",
    "start": "1730159",
    "end": "1736559"
  },
  {
    "text": "and what you could do on the trainer is you could define a set of callbacks to process these results and finally another optional thing you",
    "start": "1736559",
    "end": "1743200"
  },
  {
    "text": "could do is saving your checkpoints you could pass your checkpoints into this utility function and again it will save",
    "start": "1743200",
    "end": "1749200"
  },
  {
    "text": "it locally on your trainer uh so you can access it pythonically or",
    "start": "1749200",
    "end": "1754720"
  },
  {
    "text": "from your disk okay so let's go back to the the",
    "start": "1754720",
    "end": "1759760"
  },
  {
    "text": "the notebook so now that we've shown all the changes needed to make it raytrain uh compatible",
    "start": "1759760",
    "end": "1766559"
  },
  {
    "text": "what we want to do is actually run it so what you'll do is you'll import the trainer from raytrain and you'll set up",
    "start": "1766559",
    "end": "1773360"
  },
  {
    "text": "your trainer what you want to see here is what we're doing is we're just setting up our torch back end but again",
    "start": "1773360",
    "end": "1778799"
  },
  {
    "text": "if i were to change my training function to be a tensorflow training function for example i could just as easily swap this",
    "start": "1778799",
    "end": "1785279"
  },
  {
    "text": "out to be tensorflow and i could set the number of workers to describe the number of like uh distributed workers that i",
    "start": "1785279",
    "end": "1791840"
  },
  {
    "text": "want and i just i'll just pass a log view here so that my logs are are stored locally close to where i'm at right now",
    "start": "1791840",
    "end": "1799360"
  },
  {
    "text": "with that what we're going to do is we're going to start a trainer and as you can see uh some logs are printed out",
    "start": "1799360",
    "end": "1804640"
  },
  {
    "text": "where we have multi two workers each of them are aware of what rank they are and their total world size",
    "start": "1804640",
    "end": "1811200"
  },
  {
    "text": "and they're ready to train so similar to what we did with the local training function instead of calling train funk what i'm",
    "start": "1811200",
    "end": "1817760"
  },
  {
    "text": "actually going to do is just pass it to trainer.run and i'm actually going to pass in the same exact configuration value",
    "start": "1817760",
    "end": "1823440"
  },
  {
    "text": "and to see what the results look like here what you could see is that the logs will",
    "start": "1823440",
    "end": "1829520"
  },
  {
    "text": "print out some metadata um similarly you'll see that there's now twice as many logs because each worker is",
    "start": "1829520",
    "end": "1835520"
  },
  {
    "text": "printing out the logs for example the configurations and you'll also see that in this case",
    "start": "1835520",
    "end": "1840640"
  },
  {
    "text": "i'm moving my models to cpu and i'm still wrapping them in ddp as expected",
    "start": "1840640",
    "end": "1846480"
  },
  {
    "text": "so we'll look through the logs and see that we're training",
    "start": "1846480",
    "end": "1851760"
  },
  {
    "text": "and in this case it also took about two minutes to run so we didn't really gain anything from this but we're actually we",
    "start": "1851760",
    "end": "1857519"
  },
  {
    "text": "have been able to successfully split our work across two workers so now that i finished training on my",
    "start": "1857519",
    "end": "1863519"
  },
  {
    "text": "laptop uh what i want to do is i want to again test to see that this model actually works right to see that i",
    "start": "1863519",
    "end": "1868960"
  },
  {
    "text": "actually got to stay picked out of it and what i could do here is i could access some of the attributes to fetch",
    "start": "1868960",
    "end": "1874880"
  },
  {
    "text": "some data pythonically uh one thing i could do is i could look at the latest rundeer to see that all my logs are",
    "start": "1874880",
    "end": "1880000"
  },
  {
    "text": "stored locally uh i can see where all the checkpoints are stored and i can fetch the latest checkpoint",
    "start": "1880000",
    "end": "1885760"
  },
  {
    "text": "from this trainer and in this case this looks very similar to what we did earlier uh in which case",
    "start": "1885760",
    "end": "1891039"
  },
  {
    "text": "there's just a dictionary of information doing the same thing as we did in the first step uh we'll just use the same",
    "start": "1891039",
    "end": "1896960"
  },
  {
    "text": "utility functions on the same exact test data and in this case it still thinks almost everything is a",
    "start": "1896960",
    "end": "1902399"
  },
  {
    "text": "bird and the one thing that was a bird it actually didn't think it was a bird so it's still not doing great uh but at this point what we've been",
    "start": "1902399",
    "end": "1908960"
  },
  {
    "text": "able to do is we've been able to successfully distribute our training function to two workers on our laptop",
    "start": "1908960",
    "end": "1914960"
  },
  {
    "text": "but where we really want to go is we want to go distributed we want to scale this up we want to use gpus and we want",
    "start": "1914960",
    "end": "1921360"
  },
  {
    "text": "to train a successful model so here's how we'll do it uh the first thing we want to do is we",
    "start": "1921360",
    "end": "1927120"
  },
  {
    "text": "actually want to set up that cluster so that we can run our training function on it this is an example of how to set up",
    "start": "1927120",
    "end": "1932159"
  },
  {
    "text": "array cluster uh as you can see uh i have one cpu node and i'm going to",
    "start": "1932159",
    "end": "1938000"
  },
  {
    "text": "specify i'm going to create eight different gpu nodes with one gpu each and what you can see here is i'm",
    "start": "1938000",
    "end": "1943600"
  },
  {
    "text": "actually marking these as spot instances because i wanted one i want to save money and two i could use ray train's",
    "start": "1943600",
    "end": "1949200"
  },
  {
    "text": "built-in fault tolerance to resume my training session in case uh one of the nodes gets preempted",
    "start": "1949200",
    "end": "1956880"
  },
  {
    "text": "what i'll do to set up my cluster is then i'll just call the ray up command and",
    "start": "1956880",
    "end": "1962480"
  },
  {
    "text": "just to connect to this using the ray client i'll call ray attach uh to this cluster config",
    "start": "1962480",
    "end": "1968320"
  },
  {
    "text": "so that i could properly ssh and set up port forwarding and connect to my cluster here",
    "start": "1968320",
    "end": "1973440"
  },
  {
    "text": "but one thing to mention is that if i were to instead to set up my cluster on any scale what i could instead do uh is",
    "start": "1973440",
    "end": "1980240"
  },
  {
    "text": "i would just be able to connect directly to my cluster without having to go through this extra step to like set up the port forwarding and whatnot",
    "start": "1980240",
    "end": "1986720"
  },
  {
    "text": "and with that i'll initialize my ray cluster from my laptop i'll connect to it and i'll pass in a runtime",
    "start": "1986720",
    "end": "1993279"
  },
  {
    "text": "environment to specify that i want to pass in some of the code that's local to me such as the utilities file which i can",
    "start": "1993279",
    "end": "1999360"
  },
  {
    "text": "then access in my training function so the next thing i want to do is i want",
    "start": "1999360",
    "end": "2005039"
  },
  {
    "text": "to now set up a new trainer that can now train district like fully distributed on this cluster uh as you can see it looks",
    "start": "2005039",
    "end": "2010640"
  },
  {
    "text": "very similar but the two main changes that i made are that one i've now scaled up my number of workers to eight and two",
    "start": "2010640",
    "end": "2016960"
  },
  {
    "text": "i've now specified that i want to use gpu so if you remember a cluster configuration uh something you'll notice",
    "start": "2016960",
    "end": "2022880"
  },
  {
    "text": "is that i actually now want to set up one worker using one gpu per node and we'll see that if whether that works or",
    "start": "2022880",
    "end": "2029200"
  },
  {
    "text": "not as you can see the logs will still be loaded stored locally and when i call trainer.start",
    "start": "2029200",
    "end": "2036000"
  },
  {
    "text": "we'll see that we're now uh seeing that we have a world size of eight and we have eight",
    "start": "2036000",
    "end": "2041679"
  },
  {
    "text": "different workers and what you'll see is that these are actually coming from eight different ip addresses which correspond to those eight different gpu",
    "start": "2041679",
    "end": "2048560"
  },
  {
    "text": "nodes that i mentioned earlier so so far that's that's proving to work for us",
    "start": "2048560",
    "end": "2053679"
  },
  {
    "text": "okay but now for the interesting part for actually running this training function uh since we're scaling up to a much more",
    "start": "2053679",
    "end": "2060320"
  },
  {
    "text": "like production scalable level what we want to do is we want to pull in all the utilities right",
    "start": "2060320",
    "end": "2065440"
  },
  {
    "text": "uh so first of all what we want to do is we want to scale up so we'll change our configurations we'll increase our batch",
    "start": "2065440",
    "end": "2070560"
  },
  {
    "text": "size we'll change our learning rate and we'll use a higher number of epochs so we could train for longer second we're going to use some of ray",
    "start": "2070560",
    "end": "2077200"
  },
  {
    "text": "train's provided callbacks so that we could properly log some of the metadata of our training job so that we can",
    "start": "2077200",
    "end": "2082878"
  },
  {
    "text": "access and verify that the training's actually working or not and just like we did in the earlier",
    "start": "2082879",
    "end": "2088800"
  },
  {
    "text": "stage all we're going to do is call trainer.run with this configuration and these callbacks",
    "start": "2088800",
    "end": "2094638"
  },
  {
    "text": "as you can see our worker logs are now showing up for our eight different workers uh you can see that they're",
    "start": "2094639",
    "end": "2099920"
  },
  {
    "text": "properly moving to their cuda devices they're still getting wrapped in ddp and in this case they're downloading the",
    "start": "2099920",
    "end": "2105599"
  },
  {
    "text": "data set because they don't have them yet but after that they'll train for 100 epochs",
    "start": "2105599",
    "end": "2111760"
  },
  {
    "text": "so at this point what i want to show is so what happened with these loggers because",
    "start": "2112000",
    "end": "2117119"
  },
  {
    "text": "uh what happened with these callbacks so first of all with the json logger callback what we'll be able to see is",
    "start": "2117119",
    "end": "2122960"
  },
  {
    "text": "that they actually write results to a json file that's local to me um on my laptop",
    "start": "2122960",
    "end": "2128160"
  },
  {
    "text": "so you can see here that at every single epoch where i called train.report i'll be able to see the results that were reported",
    "start": "2128160",
    "end": "2135119"
  },
  {
    "text": "secondly uh with the tensorboard logger i could also see that uh the logs were",
    "start": "2135119",
    "end": "2140640"
  },
  {
    "text": "written locally to my laptop and i could create i could run this command to load",
    "start": "2140640",
    "end": "2146240"
  },
  {
    "text": "the tensorboard files and if i open them up right here what you can see is that i could",
    "start": "2146240",
    "end": "2151440"
  },
  {
    "text": "directly visualize all my metrics from my distributed training session you can see that it took uh from this",
    "start": "2151440",
    "end": "2158000"
  },
  {
    "text": "graph what we could see here is that it took about an hour and it trained to about 94 accuracy which is much better than what we were doing locally on our",
    "start": "2158000",
    "end": "2164000"
  },
  {
    "text": "laptop and i do want to stress that uh one of the big important parts here is",
    "start": "2164000",
    "end": "2169200"
  },
  {
    "text": "that all these logs are like while mean like sorry while the job is running in a",
    "start": "2169200",
    "end": "2174960"
  },
  {
    "text": "distributed manner across the gpu nodes on my aws cluster all of these logs are actually stored",
    "start": "2174960",
    "end": "2180240"
  },
  {
    "text": "locally on my laptop so i don't need a ssh into any of those nodes to grab the results or anything",
    "start": "2180240",
    "end": "2187280"
  },
  {
    "text": "and with that what we want to do is we want to verify that this actually worked that i've actually got to 94 accuracy uh",
    "start": "2187280",
    "end": "2193520"
  },
  {
    "text": "as you can see i could pythonically just fetch the latest checkpoint again for my trainer on my laptop",
    "start": "2193520",
    "end": "2199680"
  },
  {
    "text": "i could run it against the same utility functions to test and luckily we actually were able to confirm",
    "start": "2199680",
    "end": "2206160"
  },
  {
    "text": "that we got about a 90 accuracy so with that we've been able to successfully train the model",
    "start": "2206160",
    "end": "2213599"
  },
  {
    "text": "cool and that's the end of the demo",
    "start": "2213599",
    "end": "2218800"
  },
  {
    "text": "back to you jules brilliant that was seven minutes ahead of schedule i i i do",
    "start": "2221440",
    "end": "2227680"
  },
  {
    "text": "like that um let me see i actually went through a lot of a lot of questions actually came up most of them morge and",
    "start": "2227680",
    "end": "2233760"
  },
  {
    "text": "will and myself have answered it but one of the last one was is it possible to do customized callbacks and i was just",
    "start": "2233760",
    "end": "2240640"
  },
  {
    "text": "looking for a documentation where you can actually just inherit that from the from the base class so maybe maybe one",
    "start": "2240640",
    "end": "2245760"
  },
  {
    "text": "of you can say yes you can actually write your own customized callbacks by just inheriting uh the class anybody",
    "start": "2245760",
    "end": "2252000"
  },
  {
    "text": "wants to clarify a little more on that yeah that's exactly right um you can define your own callback and use it",
    "start": "2252000",
    "end": "2257280"
  },
  {
    "text": "directly we have a little section in the docs to explain how to do it yeah i was looking for that particular",
    "start": "2257280",
    "end": "2262640"
  },
  {
    "text": "particular page let me say um",
    "start": "2262640",
    "end": "2268240"
  },
  {
    "text": "do you actually have any questions in the q a i love red lightning project adam says",
    "start": "2268240",
    "end": "2274320"
  },
  {
    "text": "um is that plan to continue as well",
    "start": "2274320",
    "end": "2278800"
  },
  {
    "text": "yeah i can i can talk a little bit about that yeah yeah i mean i mean we do plan to continue rate lightning um i think the plans",
    "start": "2279760",
    "end": "2286400"
  },
  {
    "text": "there is actually to like have more um combine like have more synergy between ray landing and ray trains",
    "start": "2286400",
    "end": "2291839"
  },
  {
    "text": "um yeah but that project is not not being abandoned",
    "start": "2291839",
    "end": "2297359"
  },
  {
    "text": "brilliant another question from jimmy is if you use gpu equal to true",
    "start": "2297520",
    "end": "2303119"
  },
  {
    "text": "when we write the train function do we still need to do model to gpu uh",
    "start": "2303119",
    "end": "2308800"
  },
  {
    "text": "when we actually are are are converting either tensors or whatever to to that particular gpu device",
    "start": "2308800",
    "end": "2316320"
  },
  {
    "text": "right so if you use our prepare model and prepare data loader utility functions uh that's exactly what they're",
    "start": "2316320",
    "end": "2322640"
  },
  {
    "text": "doing under the hood for you so if you're switching from cpu to gpu you don't have to go through all your code",
    "start": "2322640",
    "end": "2328480"
  },
  {
    "text": "and change it at every single line okay yeah so you got the answer you don't have to do that because we do that",
    "start": "2328480",
    "end": "2334560"
  },
  {
    "text": "wouldn't do that with me for you all right let me just go through some of the question uh q a over here i think we",
    "start": "2334560",
    "end": "2340400"
  },
  {
    "text": "have almost answered all the questions that we actually had yeah just one more question matt do you know how how much",
    "start": "2340400",
    "end": "2346560"
  },
  {
    "text": "the like your demo costed or how long it took to run uh took an hour to run um i don't know",
    "start": "2346560",
    "end": "2352640"
  },
  {
    "text": "exactly how much the spot instances cost i didn't do the calculation but that demo took about an hour to run",
    "start": "2352640",
    "end": "2360320"
  },
  {
    "text": "i think one of the other questions was can you run this on your laptop and i was wondering well if it took an hour to do on a cluster probably laptop won't",
    "start": "2360320",
    "end": "2366400"
  },
  {
    "text": "work as much yeah so you could definitely run the first two steps yes",
    "start": "2366400",
    "end": "2371599"
  },
  {
    "text": "yes yes definitely um question from tinky liu",
    "start": "2371599",
    "end": "2378720"
  },
  {
    "text": "can we connect to a ray cluster that is set up on our local hpc cluster uh it",
    "start": "2378720",
    "end": "2384240"
  },
  {
    "text": "was not as necessary as writing the config file so is he's wondering just like the way",
    "start": "2384240",
    "end": "2390880"
  },
  {
    "text": "we actually connected to the any clay cluster or the way we use ray init is there any way to connect that to a red",
    "start": "2390880",
    "end": "2397440"
  },
  {
    "text": "cluster running on hpc yes you should be able to connect to uh the htc cluster as long as as long as",
    "start": "2397440",
    "end": "2404400"
  },
  {
    "text": "you're able to connect to it like regularly okay yeah so just by just by providing the headnode address on the",
    "start": "2404400",
    "end": "2410240"
  },
  {
    "text": "hpc we can actually do that brilliant yeah so generally speaking you can use the ray cluster launcher like ray up uh",
    "start": "2410240",
    "end": "2416480"
  },
  {
    "text": "and that is really easy to provision on say like a cloud provider um if you're using especially something like slurm we",
    "start": "2416480",
    "end": "2422560"
  },
  {
    "text": "have a bunch of examples of how to do that but that's usually more of like a manual process of say starting up the",
    "start": "2422560",
    "end": "2428240"
  },
  {
    "text": "ray daemon with ray start on each node and then uh pointing them to the head node but again if you use",
    "start": "2428240",
    "end": "2434640"
  },
  {
    "text": "ray up on like a classical cloud provider you don't have to worry about any of that it's all taken care of for you",
    "start": "2434640",
    "end": "2440720"
  },
  {
    "text": "and then the swear asks i think we'll take the last question um uh also does",
    "start": "2440720",
    "end": "2445760"
  },
  {
    "text": "the cluster set up script also support cleanup of aws machines",
    "start": "2445760",
    "end": "2450880"
  },
  {
    "text": "um presumably he actually means that after you have done that when you do a shutdown does it actually shut down or",
    "start": "2450880",
    "end": "2457359"
  },
  {
    "text": "you have to do ray ray uh stop to stop all the machines",
    "start": "2457359",
    "end": "2463520"
  },
  {
    "text": "uh yeah i think i'm against this with ray down okay yeah with the way down you can actually do that okay i think that's that's almost all",
    "start": "2464079",
    "end": "2470800"
  },
  {
    "text": "the questions that we actually answered thanks a lot will and thanks a lot moog and mate i mean and matt for all the uh",
    "start": "2470800",
    "end": "2477280"
  },
  {
    "text": "the great demo and i think i think people actually got an idea about about you know how we actually set the context and and what's under the hood so i think",
    "start": "2477280",
    "end": "2484160"
  },
  {
    "text": "this is actually a good segue to get into the next uh great presentation from our pie torch uh contributor and and uh",
    "start": "2484160",
    "end": "2492720"
  },
  {
    "text": "and and uh torsek's um integration with ray from mark and he's one of the",
    "start": "2492720",
    "end": "2498079"
  },
  {
    "text": "software engineer at python engineering group and he's going to talk about you know the the new torch integration with with ray and how",
    "start": "2498079",
    "end": "2504400"
  },
  {
    "text": "they're gonna how park torch developers can actually take advantage of that so uh mark you ready",
    "start": "2504400",
    "end": "2510480"
  },
  {
    "text": "yeah let's do it thank you so much let me quickly share my screen",
    "start": "2510480",
    "end": "2516400"
  },
  {
    "text": "actually okay",
    "start": "2520800",
    "end": "2526079"
  },
  {
    "text": "all good yep okay perfect um yeah so uh so hi everyone uh i'm mark",
    "start": "2526079",
    "end": "2533359"
  },
  {
    "text": "uh i i work on the fightridge team and i'm sort of here to tell you today about like uh i think a pretty compelling",
    "start": "2533359",
    "end": "2539599"
  },
  {
    "text": "story we have for how to make it easier for people to train like large-scale jobs in open source",
    "start": "2539599",
    "end": "2546480"
  },
  {
    "text": "uh way while using like something like george jackson right so",
    "start": "2546480",
    "end": "2551599"
  },
  {
    "text": "just like a bit about me um so like most of my day job i actually spend like",
    "start": "2551599",
    "end": "2556720"
  },
  {
    "text": "maintaining torch serve which is a tool for model inferencing i also contribute like now and then to projects like torch",
    "start": "2556720",
    "end": "2563359"
  },
  {
    "text": "x and corepy torch uh most of my my time i i spent really worried about is it easier for people to",
    "start": "2563359",
    "end": "2570079"
  },
  {
    "text": "deploy models in production the reason why this is important is because for a lot of like you know",
    "start": "2570079",
    "end": "2575359"
  },
  {
    "text": "internet giants i think companies like meta or google et cetera like there's already like quite robust infrastructure",
    "start": "2575359",
    "end": "2580800"
  },
  {
    "text": "to do this internally but it's very difficult to open source that infrastructure externally um also some people think i pretty much",
    "start": "2580800",
    "end": "2587760"
  },
  {
    "text": "spend most of my day tweeting so i'm here to compare i probably spent about 12 hours a day just like on twitter uh",
    "start": "2587760",
    "end": "2593040"
  },
  {
    "text": "so that's like you know definitely if you're interested in ml up stuff like make sure to reach out to me there dm me if you have any questions about this",
    "start": "2593040",
    "end": "2598480"
  },
  {
    "text": "stuff so uh before we get to the the meat of the talk like these were sort of the",
    "start": "2598480",
    "end": "2605440"
  },
  {
    "text": "problems that have been nagging me in a couple of my peers working on production and fight church so the first one is you",
    "start": "2605440",
    "end": "2612560"
  },
  {
    "text": "know setting up an infrastructure it's it's one of those things where if if someone knows how to do it great",
    "start": "2612560",
    "end": "2619440"
  },
  {
    "text": "like you basically have to hire someone that knows like all the cube ctl commands that's like you know that's",
    "start": "2619440",
    "end": "2624560"
  },
  {
    "text": "like a devops expert um and then you're like great like you know i have this thing now and you know i",
    "start": "2624560",
    "end": "2629920"
  },
  {
    "text": "have this infrastructure that i can use the next problem is like well how do i sort of submit jobs against it and",
    "start": "2629920",
    "end": "2635119"
  },
  {
    "text": "ideally how do i submit jobs against it without having to modify my code too much because the more i have to modify",
    "start": "2635119",
    "end": "2641440"
  },
  {
    "text": "it the more this is something you know i don't care about doing that i need to do um and and then like you know the third",
    "start": "2641440",
    "end": "2648480"
  },
  {
    "text": "one is just like well you deployed a job great like where is it you know what's happening",
    "start": "2648480",
    "end": "2653920"
  },
  {
    "text": "where are the logs uh for it and the final one is okay great like you deployed like let's say a",
    "start": "2653920",
    "end": "2659599"
  },
  {
    "text": "toy train dot ui script like how can you deploy now like a system like like a running system that needs to like or",
    "start": "2659599",
    "end": "2666160"
  },
  {
    "text": "services um so fundamentally like you know all of these problems boil down to this is that",
    "start": "2666160",
    "end": "2672800"
  },
  {
    "text": "like like a data scientist is not necessarily an infrastructure engineer",
    "start": "2672800",
    "end": "2678000"
  },
  {
    "text": "but the tooling doesn't reflect this like the tooling assumes that if you want to deploy something to fraud you",
    "start": "2678000",
    "end": "2684400"
  },
  {
    "text": "need to go ahead and become a domain expert at both of those very distinct skill sets before you can start doing something",
    "start": "2684400",
    "end": "2690720"
  },
  {
    "text": "like really useful uh so this is where like we started working with uh with the ray team and",
    "start": "2690720",
    "end": "2697040"
  },
  {
    "text": "you know collaborative the door checks seemed to build something that was a bit saner so this is really what i'm here to",
    "start": "2697040",
    "end": "2702160"
  },
  {
    "text": "talk about today so i guess like before we get to the specifics i like i do want to",
    "start": "2702160",
    "end": "2707680"
  },
  {
    "text": "acknowledge that like you know even though i'm the one giving the talk here like this was like this this work",
    "start": "2707680",
    "end": "2713359"
  },
  {
    "text": "wouldn't have been possible without like john who really developed the original interface for this uh tristan who helped",
    "start": "2713359",
    "end": "2720079"
  },
  {
    "text": "me unblock a lot of ci issues alex for like really rigorous code reviews gita",
    "start": "2720079",
    "end": "2725119"
  },
  {
    "text": "diamond for leadership guidance and also from the any scale side like i mean i i spent like countless like like so so",
    "start": "2725119",
    "end": "2732160"
  },
  {
    "text": "much time like bugging a mug over random rate questions john who built the right job api which we'll talk about jules for writing most",
    "start": "2732160",
    "end": "2739119"
  },
  {
    "text": "of the blog posts which isn't out yet but very soon and richard also prefer like his leadership guidance",
    "start": "2739119",
    "end": "2745119"
  },
  {
    "text": "so okay so we talked about setting up an infrastructure and this is sort of what the experience looks like at this point",
    "start": "2745119",
    "end": "2751920"
  },
  {
    "text": "like thanks to ray so let's say you're using something like aws but you know it could be any club provider you type in",
    "start": "2751920",
    "end": "2757760"
  },
  {
    "text": "aws configure have your access key your your secret and then you can just say call like ray up on a clustering ammo",
    "start": "2757760",
    "end": "2764400"
  },
  {
    "text": "and this sets up a cluster for you uh personally like before i went through this like i sort of went through things",
    "start": "2764400",
    "end": "2770800"
  },
  {
    "text": "a bit from a first principle standpoint and i tried to do this like in in kubeflow i tried to do it in like raw",
    "start": "2770800",
    "end": "2776960"
  },
  {
    "text": "kubernetes uh definitely i i would say like like the ray cluster yama was one of the far more like human readable ones",
    "start": "2776960",
    "end": "2783920"
  },
  {
    "text": "that that i experienced and it sort of looks like exactly what you would expect right it's like well like you know you'll have information",
    "start": "2783920",
    "end": "2790400"
  },
  {
    "text": "like what's your cloud provider like what are your machine types like are using like i don't know like p3 gpus or",
    "start": "2790400",
    "end": "2796240"
  },
  {
    "text": "like c5 or m6i cpus which which docker image are you using",
    "start": "2796240",
    "end": "2803040"
  },
  {
    "text": "and at this point like like because of this it's not like entirely possible to like",
    "start": "2803040",
    "end": "2808880"
  },
  {
    "text": "base everything i'm covering today if you go to the short url you can set up an infrastructure submit a job against",
    "start": "2808880",
    "end": "2816000"
  },
  {
    "text": "it get the job status get the logs back from within like a google collab notebook so pretty much like there's",
    "start": "2816000",
    "end": "2822640"
  },
  {
    "text": "almost like no difference between local and and remote as far as as far as your user experience is",
    "start": "2822640",
    "end": "2827920"
  },
  {
    "text": "concerned as long as you have aws credentials you can just like go ahead and click on the slick and get started",
    "start": "2827920",
    "end": "2834640"
  },
  {
    "text": "um yeah so well let's like so sort of briefly talk about like tour checks a bit so i really",
    "start": "2834960",
    "end": "2842000"
  },
  {
    "text": "haven't explained what it is right so so fundamentally torch x is a tool to like submit jobs right and though",
    "start": "2842000",
    "end": "2849280"
  },
  {
    "text": "like the way this traditionally worked as well we had a slurm scheduler we had a kubernetes scheduler uh and then you",
    "start": "2849280",
    "end": "2855280"
  },
  {
    "text": "would submit jobs and they could work like across like various cloud providers uh and now just like there's another option like we have is right and and ray",
    "start": "2855280",
    "end": "2862720"
  },
  {
    "text": "like with with their new job api has made it also trivial not just to submit a job but also to",
    "start": "2862720",
    "end": "2867839"
  },
  {
    "text": "monitor it and and and get the logs back so this is what we'll talk about",
    "start": "2867839",
    "end": "2873119"
  },
  {
    "text": "oh wow this is really bad resolution okay so uh so but fundamentally in torch checks",
    "start": "2873119",
    "end": "2879040"
  },
  {
    "text": "like your user experience will involve you writing like a couple of components so you can think of this as",
    "start": "2879040",
    "end": "2884400"
  },
  {
    "text": "configuration in python uh then once you have that configuration you can run it and then you can also compose it so",
    "start": "2884400",
    "end": "2890960"
  },
  {
    "text": "we'll talk about like pipelines in a second so like i said like this feature is",
    "start": "2890960",
    "end": "2896960"
  },
  {
    "text": "right like hot off the press like uh torch 1.11 is coming out uh",
    "start": "2896960",
    "end": "2902720"
  },
  {
    "text": "probably sometime next week or so i believe i'm not mistaken uh and so the torch release will probably quickly",
    "start": "2902720",
    "end": "2908160"
  },
  {
    "text": "follow then but if you want to use this feature uh you know make sure to install like the the nightly binaries",
    "start": "2908160",
    "end": "2914480"
  },
  {
    "text": "uh submitting a job is now like really as simple as this like you would say like tortex run like dash s for",
    "start": "2914480",
    "end": "2920319"
  },
  {
    "text": "scheduler ray right and then the configuration here is like well there's a specific dashboard",
    "start": "2920319",
    "end": "2925920"
  },
  {
    "text": "address where we're submitting jobs against uh you're working there so if you want to upload like data like you",
    "start": "2925920",
    "end": "2931040"
  },
  {
    "text": "want to put some data some helper files your trainer ui uh and then the component is really like",
    "start": "2931040",
    "end": "2936880"
  },
  {
    "text": "the the core logic and we'll talk about we'll talk about this in a second",
    "start": "2936880",
    "end": "2942960"
  },
  {
    "text": "um so i talked about components and torches briefly and you can think of them really as like",
    "start": "2942960",
    "end": "2949920"
  },
  {
    "text": "yaml but in python uh so so so this will have like stuff like you know what's the",
    "start": "2949920",
    "end": "2955280"
  },
  {
    "text": "entry point for your script like what's your script called uh which docker image are you using how many resources you",
    "start": "2955280",
    "end": "2960319"
  },
  {
    "text": "have like cpus gpus like what's the number of like what what's the total number of ram that you're allocating for them uh what's the number of replicas uh",
    "start": "2960319",
    "end": "2967760"
  },
  {
    "text": "so all of these are sort of like very standard things that you have but like the benefit of having this like within",
    "start": "2967760",
    "end": "2972960"
  },
  {
    "text": "within python is that it's more easily shareable like someone could basically get this component inherit it compose it",
    "start": "2972960",
    "end": "2979440"
  },
  {
    "text": "into other components uh so it makes it like much much easier like if you work with other people that",
    "start": "2979440",
    "end": "2984880"
  },
  {
    "text": "are also building components you can sort of end up with components that you can leverage that other people built and",
    "start": "2984880",
    "end": "2989920"
  },
  {
    "text": "so this is very much like internally what the story looks like for us now with torch x and hopefully over time",
    "start": "2989920",
    "end": "2995200"
  },
  {
    "text": "like that it will be the same and open source so what does like our trainer.py look like",
    "start": "2995200",
    "end": "3001119"
  },
  {
    "text": "here so so i have the script called compute world size let me just sort of go this uh go go this in a second",
    "start": "3001119",
    "end": "3007119"
  },
  {
    "text": "so i i know this is like more code than a lot of these like demos weren't but",
    "start": "3007119",
    "end": "3012240"
  },
  {
    "text": "but i didn't think it would be interesting to go over like what does really a hello world look like for distributed training in pythons",
    "start": "3012240",
    "end": "3018640"
  },
  {
    "text": "right so there are sort of two like you know words that you may or may not be aware of at least i was confused of when",
    "start": "3018640",
    "end": "3024800"
  },
  {
    "text": "i first heard of them so those are like like world size and rank so world size is the number of nodes that you have in",
    "start": "3024800",
    "end": "3030400"
  },
  {
    "text": "a cluster and rank is a specific idea of a node on that cluster so when you're starting a distributed script",
    "start": "3030400",
    "end": "3036400"
  },
  {
    "text": "essentially every uh every process needs to know uh what's the mass report what's the master",
    "start": "3036400",
    "end": "3042480"
  },
  {
    "text": "address what's my world size what's my rank um and then you initialize them and so what we're going to do here and this is",
    "start": "3042480",
    "end": "3048640"
  },
  {
    "text": "why this is a hello world script we're basically just going to get all of the different like workers to agree to what",
    "start": "3048640",
    "end": "3054400"
  },
  {
    "text": "the world sizes so this is what this already use is doing here and we're just going to return this",
    "start": "3054400",
    "end": "3060480"
  },
  {
    "text": "world size fundamentally if you're doing uh anything else like let's say a ddp",
    "start": "3060480",
    "end": "3065599"
  },
  {
    "text": "script with resnet like we saw like like we saw earlier or if you're doing like a python",
    "start": "3065599",
    "end": "3070720"
  },
  {
    "text": "lightning script uh or pretty much other any other ddp way any other distributed data parallel",
    "start": "3070720",
    "end": "3076240"
  },
  {
    "text": "way of doing of working with pytorch like they all fundamentally use the same idea which is why even though i'm",
    "start": "3076240",
    "end": "3081440"
  },
  {
    "text": "showing you the script here uh it works just fine if you want to give it like a fighter's lightning script or you want",
    "start": "3081440",
    "end": "3087359"
  },
  {
    "text": "to give it like a full like like like a more like real trainer script works the exact same way this is just purely like",
    "start": "3087359",
    "end": "3093440"
  },
  {
    "text": "an educational example just to know what these words mean so once you've done this",
    "start": "3093440",
    "end": "3100000"
  },
  {
    "text": "uh you get it like so so once you've called like tortex run you're gonna get a job id back and so this job id you can",
    "start": "3100000",
    "end": "3107760"
  },
  {
    "text": "then use it to say something like torch x describe or torch x log and that's it",
    "start": "3107760",
    "end": "3113280"
  },
  {
    "text": "so this is what you would do from like you know the comfort of your notebook or your terminal and the nice thing about this is that",
    "start": "3113280",
    "end": "3119920"
  },
  {
    "text": "log for example would take care of aggregating logs over multiple machines and rendering them in the standard out",
    "start": "3119920",
    "end": "3126400"
  },
  {
    "text": "of your local machine so you don't have to worry about like where are the logs or what's happening as far as you're",
    "start": "3126400",
    "end": "3131920"
  },
  {
    "text": "concerned like you're programming like against a single machine so just to be a bit more specific like",
    "start": "3131920",
    "end": "3137920"
  },
  {
    "text": "you know when you look at something like tortex describe uh so so for example like this is the name",
    "start": "3137920",
    "end": "3143599"
  },
  {
    "text": "we we we use to be able to uh like get the status or the logs but",
    "start": "3143599",
    "end": "3148960"
  },
  {
    "text": "it also show you some other stuff like well how many times have we retried the script like was the total number of replicas uh what is the retry policy so",
    "start": "3148960",
    "end": "3157040"
  },
  {
    "text": "this is one of the benefits of using torch x is that it gives you a lot of flexibility to configure this kind of",
    "start": "3157040",
    "end": "3162240"
  },
  {
    "text": "behavior you know and pretty much like all in python you would just like set up a",
    "start": "3162240",
    "end": "3167359"
  },
  {
    "text": "component and then you could you could do all this stuff uh torchlight's log on the other hand and so again this is just be calling",
    "start": "3167359",
    "end": "3173760"
  },
  {
    "text": "torchic's log with the with the id so here what this is doing is like like i said what what the script will do is",
    "start": "3173760",
    "end": "3179680"
  },
  {
    "text": "compute the world size right so it's asking each node uh what is your wall size so we're seeing here there's this",
    "start": "3179680",
    "end": "3184880"
  },
  {
    "text": "the first command actor like this this process id and a different process id uh both agreeing to the same world size",
    "start": "3184880",
    "end": "3192000"
  },
  {
    "text": "uh but again like if this was like a training loop uh you would pretty much see like what the training loop that",
    "start": "3192000",
    "end": "3197520"
  },
  {
    "text": "everyone else is seeing then there's like an all reduce that happens you have some weights that are synchronized and",
    "start": "3197520",
    "end": "3202880"
  },
  {
    "text": "then you know the processor so it's sort of the exact same logic uh but this is sort of the the the the beginner's",
    "start": "3202880",
    "end": "3209200"
  },
  {
    "text": "version of it and probably like again the more helpful education",
    "start": "3209200",
    "end": "3214079"
  },
  {
    "text": "so how does this work under the hood right so so because i figured this was a great talk like you clear",
    "start": "3214640",
    "end": "3220720"
  },
  {
    "text": "and i do want to sort of highlight some stuff that was interesting about ray that made our job very easy to build",
    "start": "3220720",
    "end": "3227119"
  },
  {
    "text": "this kind of experience so the first one was sort of this this new experience called the right job sdk",
    "start": "3227119",
    "end": "3232720"
  },
  {
    "text": "and i believe like uh like like matt sort of hinted at it uh you can say something like i forget it's",
    "start": "3232720",
    "end": "3238720"
  },
  {
    "text": "a great great job or something like race of my job i forgot what the exact commentator was but regardless if you",
    "start": "3238720",
    "end": "3244160"
  },
  {
    "text": "look at like the like like from a python library standpoint that looks like this where you're giving it an entry point",
    "start": "3244160",
    "end": "3249440"
  },
  {
    "text": "you're giving it a working directory and then maybe you're giving it like a bunch of like dependent like python dependencies so this would be the",
    "start": "3249440",
    "end": "3255599"
  },
  {
    "text": "typical stuff like i don't know like torch sure requests pandas et cetera so you could just like",
    "start": "3255599",
    "end": "3262000"
  },
  {
    "text": "dump all of those requirements and then it would run the script right and also like let's say for something",
    "start": "3262000",
    "end": "3267920"
  },
  {
    "text": "like getting the job status is very similar right like we like we have like this job status function and like let's",
    "start": "3267920",
    "end": "3274000"
  },
  {
    "text": "say if you want to do something like waiting until you finish the script like really it's as simple as like writing the simple polling function you check",
    "start": "3274000",
    "end": "3281040"
  },
  {
    "text": "the status periodically and as soon as the status is either status like succeeded stop failed return it",
    "start": "3281040",
    "end": "3288240"
  },
  {
    "text": "because otherwise it means that it's queued which isn't particularly helpful you just know like you know like the the",
    "start": "3288240",
    "end": "3293680"
  },
  {
    "text": "benefit of this right is that uh if you sort of set up the infrastructure with the area of yourself",
    "start": "3293680",
    "end": "3299280"
  },
  {
    "text": "you could do that for yourself other people could leverage the same infrastructure and then when you're all done you can tear it down so really this",
    "start": "3299280",
    "end": "3305359"
  },
  {
    "text": "whole idea i think behind the like differentiating personas around like data scientists versus versus",
    "start": "3305359",
    "end": "3312079"
  },
  {
    "text": "infrastructure engineers is kind of largely artificial and really at least showed me personally that there's like",
    "start": "3312079",
    "end": "3317520"
  },
  {
    "text": "some serious like two gaps here that are helpful to address so let's talk a bit about like the raid",
    "start": "3317520",
    "end": "3324480"
  },
  {
    "text": "driver right so so how does this work exactly right we we we're calling torchex",
    "start": "3324480",
    "end": "3329599"
  },
  {
    "text": "with like a ddp script and then this is running like an array cluster right so how does this work exactly",
    "start": "3329599",
    "end": "3336240"
  },
  {
    "text": "so typically if you've used ray before like they there's like this experience with the",
    "start": "3336240",
    "end": "3341920"
  },
  {
    "text": "ray driver script so this is like the script that you're writing that goes and does stuff so specifically as far as ddp is",
    "start": "3341920",
    "end": "3348319"
  },
  {
    "text": "concerned what this stuff really is is every actor like ray actor is going",
    "start": "3348319",
    "end": "3353920"
  },
  {
    "text": "to set its enviro it's it's important environment variables so there's some",
    "start": "3353920",
    "end": "3358960"
  },
  {
    "text": "that are shared like like the master address import so those are the chain so those are the same across all of your notes but then",
    "start": "3358960",
    "end": "3365440"
  },
  {
    "text": "there's some stuff that will change like specifically the rank uh will change so we basically just create this like",
    "start": "3365440",
    "end": "3371680"
  },
  {
    "text": "command actor and by using this command actor we can aggregate like all of this like useful like all these useful",
    "start": "3371680",
    "end": "3377680"
  },
  {
    "text": "environment variables right and so this is sort of that sets up the the setup logic as far as edp is",
    "start": "3377680",
    "end": "3383520"
  },
  {
    "text": "concerned uh the next step is like well you can create these command actors and those",
    "start": "3383520",
    "end": "3388880"
  },
  {
    "text": "command actors would be on placement groups uh and have have resource constraints",
    "start": "3388880",
    "end": "3394799"
  },
  {
    "text": "right so the idea is like you like if you set up an infrastructure for someone",
    "start": "3394799",
    "end": "3399839"
  },
  {
    "text": "and everyone has access to the full thing that kind of doesn't really make sense like right like you want to",
    "start": "3399839",
    "end": "3405200"
  },
  {
    "text": "ultimately say well well i i like let's say i have a thousand nodes but people should be",
    "start": "3405200",
    "end": "3410319"
  },
  {
    "text": "able to use at most like five by default or or like you know maybe this sort of like in a more multi-tenant setting",
    "start": "3410319",
    "end": "3418240"
  },
  {
    "text": "so being able to set the resource requirements per actor is like super critical uh again assuming that like",
    "start": "3418240",
    "end": "3424559"
  },
  {
    "text": "like you don't want people to worry too much about top stuff this uh makes things much much simpler so this is this",
    "start": "3424559",
    "end": "3430079"
  },
  {
    "text": "line here around the placement group number of cpu's number of gpus so",
    "start": "3430079",
    "end": "3435839"
  },
  {
    "text": "once this is done like what so this is just like the actor setup the actual core logic is really just as simple as",
    "start": "3435839",
    "end": "3442000"
  },
  {
    "text": "this we're basically just gonna like trigger all of these like remote actors",
    "start": "3442000",
    "end": "3447440"
  },
  {
    "text": "and we're going to keep track of which ones are completed and the ones that like aren't completed",
    "start": "3447440",
    "end": "3452720"
  },
  {
    "text": "for whatever reason you throw an exception but you know so this is really here the",
    "start": "3452720",
    "end": "3457839"
  },
  {
    "text": "this array weight uh and then these ray object references would get you like would get you back the actual like uh",
    "start": "3457839",
    "end": "3465520"
  },
  {
    "text": "like uh the what would actually trigger the actor to start running the ddp the dp script",
    "start": "3465520",
    "end": "3472000"
  },
  {
    "text": "uh so yeah i mean like uh like like honestly like if you look at this and if you've messed around with ray like so",
    "start": "3472000",
    "end": "3477760"
  },
  {
    "text": "sort of very similar even if you go to their like ray beginners tutorial uh around like how to do this and even",
    "start": "3477760",
    "end": "3483119"
  },
  {
    "text": "just sort of using these simple ideas like will help us and build something like kind of a powerful integration without like necessarily doing uh you",
    "start": "3483119",
    "end": "3490160"
  },
  {
    "text": "know really crazy stuff right like just like the subtraction of an actor is helpful and so we definitely were able",
    "start": "3490160",
    "end": "3495520"
  },
  {
    "text": "to leverage it to do something like a seamless script without having people worry about",
    "start": "3495520",
    "end": "3500720"
  },
  {
    "text": "hey maybe i need to start this node then let me export these environment variables around the world and rank size",
    "start": "3500720",
    "end": "3506960"
  },
  {
    "text": "because that has sort of like historically been people's experience and why they've uh like leaned on solutions like let's",
    "start": "3506960",
    "end": "3513839"
  },
  {
    "text": "say like grid ai or like other alternatives the benefit here really uh is that this is all open source right",
    "start": "3513839",
    "end": "3519680"
  },
  {
    "text": "like i mean source checks is open source raise open source uh so this is like stuff that's available to public you know you could",
    "start": "3519680",
    "end": "3526000"
  },
  {
    "text": "use all this stuff and not pay either company a penny uh you can extend it like however you want right so this is",
    "start": "3526000",
    "end": "3531440"
  },
  {
    "text": "like sort of a huge benefit of open source and you know strategically when when we think of a lot of stuff for pytorch like we you know we we don't",
    "start": "3531440",
    "end": "3538400"
  },
  {
    "text": "sell by torch right like we don't we don't make money from pie torch but we do want people's",
    "start": "3538400",
    "end": "3543440"
  },
  {
    "text": "experience in open source for abroad to be uh pretty good and so this is definitely one of the projects that i think is hitting on that mission",
    "start": "3543440",
    "end": "3552000"
  },
  {
    "text": "so great so so this sort of now thought so all of this was to say like you know we",
    "start": "3553680",
    "end": "3558880"
  },
  {
    "text": "you can run a you can run a torch extra like you can run it you can run a ddp job with torch x on top of a ray cluster",
    "start": "3558880",
    "end": "3565280"
  },
  {
    "text": "just like a couple of command line arguments the actual integration itself is like fairly simple if you understand",
    "start": "3565280",
    "end": "3570400"
  },
  {
    "text": "like how array works but you know now like what's really the benefit of this right so i sort of hinted at this like",
    "start": "3570400",
    "end": "3576720"
  },
  {
    "text": "where targets really provides like a structured way to like write like ml applications so ml",
    "start": "3576720",
    "end": "3583200"
  },
  {
    "text": "applications may involve a lot of stuff so it can involve stuff like model serving maybe like you know setting",
    "start": "3583200",
    "end": "3589040"
  },
  {
    "text": "configurations for an elastic job hyper parameter optimization like metric logging config config management maybe",
    "start": "3589040",
    "end": "3596240"
  },
  {
    "text": "it has a trainer loop something like python lightning with callbacks and the benefit of this now is that you could",
    "start": "3596240",
    "end": "3602000"
  },
  {
    "text": "take infrastructure or like basically pipelines that we know work pretty well uh internally at meta and you can just",
    "start": "3602000",
    "end": "3609040"
  },
  {
    "text": "like run them now and open source on top of array cluster so you could run like torch serve on top",
    "start": "3609040",
    "end": "3615119"
  },
  {
    "text": "of array cluster you could also run race serve as a component on torch x on a ray cluster so sort of there's like a lot of",
    "start": "3615119",
    "end": "3622079"
  },
  {
    "text": "like potentially like interesting integrations like that could happen here and you know depending on i guess your",
    "start": "3622079",
    "end": "3627359"
  },
  {
    "text": "feedback on this feature uh you know let us know because it definitely helps us prioritize like what to do next",
    "start": "3627359",
    "end": "3633680"
  },
  {
    "text": "um and so if you don't believe me right like this is really uh like when i say you can take like an infrastructure that",
    "start": "3633680",
    "end": "3640799"
  },
  {
    "text": "was built for meta and now run it on array cluster like i do mean it like",
    "start": "3640799",
    "end": "3645920"
  },
  {
    "text": "let's say you had something here like code and we have a lot of this code internally that says something like scheduler like i don't know slurm or",
    "start": "3645920",
    "end": "3652960"
  },
  {
    "text": "kubernetes or whatever if you just change that to uh ray",
    "start": "3652960",
    "end": "3658079"
  },
  {
    "text": "everything still just like works right and so you can sort out like after your components and your pipelines and torch",
    "start": "3658079",
    "end": "3664559"
  },
  {
    "text": "checks and let ray worry about like the infrastructure setup and the scalability",
    "start": "3664559",
    "end": "3670400"
  },
  {
    "text": "um so thank you uh you know this this was fun i'm happy to take any questions in the meantime you know feel free to",
    "start": "3670400",
    "end": "3676160"
  },
  {
    "text": "give us a star on tour checks or ray uh troy checks like is is a new project like it just came out like late last",
    "start": "3676160",
    "end": "3682640"
  },
  {
    "text": "year at python so if you have any feedback on what we should be doing with it please let us know but our goal there is to make it",
    "start": "3682640",
    "end": "3689280"
  },
  {
    "text": "really easy for people to do ml ops without needing to work at a trillion dollar company i think we used to be a",
    "start": "3689280",
    "end": "3696079"
  },
  {
    "text": "trillion dollar company not anymore so yeah i guess half a trillion dollar company uh thank you so thank you so much everyone",
    "start": "3696079",
    "end": "3702160"
  },
  {
    "text": "well thank you mark uh for the great explanation of this joint engineering effort between the ml uh team",
    "start": "3702160",
    "end": "3708559"
  },
  {
    "text": "in the scale and and the pytorch team i think this is this is a great contribution and now the the pie church",
    "start": "3708559",
    "end": "3714960"
  },
  {
    "text": "ecosystem or developers actually now have an ability to to train their models using using torch checks and submit the",
    "start": "3714960",
    "end": "3721039"
  },
  {
    "text": "jobs i think one one question that that came up was that um you you included the",
    "start": "3721039",
    "end": "3727200"
  },
  {
    "text": "short url for the google lab uh what is the minimum thing will i need for that",
    "start": "3727200",
    "end": "3732319"
  },
  {
    "text": "to run endlessly do i actually have to have an aws cluster set up and all that accounts or",
    "start": "3732319",
    "end": "3737839"
  },
  {
    "text": "or is is that google google lab is going to run automatically for me yeah so so really the only thing that",
    "start": "3737839",
    "end": "3744880"
  },
  {
    "text": "you would need is like an aws account so uh it doesn't even have to be like a",
    "start": "3744880",
    "end": "3750720"
  },
  {
    "text": "real account like it could just be like an account you create like in 10 minutes uh but but you will need it so that like",
    "start": "3750720",
    "end": "3756160"
  },
  {
    "text": "real instances get deployed um yeah i mean it's still possible to do",
    "start": "3756160",
    "end": "3761440"
  },
  {
    "text": "all of this like purely locally uh but the benefit wouldn't be as obvious so so",
    "start": "3761440",
    "end": "3766480"
  },
  {
    "text": "that's why it's not really there in the co-op but again the only dependency you really need uh is that and everything else like is",
    "start": "3766480",
    "end": "3773200"
  },
  {
    "text": "is there natively in the notebook um so there's another question from swear",
    "start": "3773200",
    "end": "3779599"
  },
  {
    "text": "shaw is it easy or possible to set up say a fair sig a fair sequence training",
    "start": "3779599",
    "end": "3786000"
  },
  {
    "text": "job using torch x usually i found i've i found it a bit difficult to use such leaves like",
    "start": "3786000",
    "end": "3792880"
  },
  {
    "text": "frameworks unless you're working with the raw pycharg code so you just want to see that does this does the torch x make",
    "start": "3792880",
    "end": "3798880"
  },
  {
    "text": "it easier with fair scheduler uh interesting yeah so so i haven't personally used the first scheduler it",
    "start": "3798880",
    "end": "3804880"
  },
  {
    "text": "kind of all depends on like if they sort of have a similar experience where they",
    "start": "3804880",
    "end": "3809920"
  },
  {
    "text": "expect you to ssh into a bunch of nodes and export and then export a bunch of environment",
    "start": "3809920",
    "end": "3815680"
  },
  {
    "text": "variables before you run it uh then then yes uh you know it would work just fine",
    "start": "3815680",
    "end": "3822240"
  },
  {
    "text": "but you know do let me know i mean if we can sort of help you scale that like i'm happy to take a closer look uh but at",
    "start": "3822240",
    "end": "3828640"
  },
  {
    "text": "least like a lot of the famous distributed training rappers all operate the same way because they all basically",
    "start": "3828640",
    "end": "3835839"
  },
  {
    "text": "like you can think of it as fighters distributed that involves like some boilerplate which which makes you right",
    "start": "3835839",
    "end": "3841280"
  },
  {
    "text": "like here's my rank here's my world size uh change this reduce to an already used sort of thing and some frameworks either",
    "start": "3841280",
    "end": "3848319"
  },
  {
    "text": "do that for you or they expect or they expect you to do it and then you do it uh so",
    "start": "3848319",
    "end": "3854799"
  },
  {
    "text": "same rules really apply because it's kind of difficult to introspect on a training script and make it",
    "start": "3854799",
    "end": "3859920"
  },
  {
    "text": "automatically uh distributed but like i think that's generally not the case for arbitrary code although people will",
    "start": "3859920",
    "end": "3866480"
  },
  {
    "text": "produce examples and you know claim that it works arbitrary good i think another question was from a",
    "start": "3866480",
    "end": "3871520"
  },
  {
    "text": "developer's point of view if i were to describe the dev the pi torch developer journey",
    "start": "3871520",
    "end": "3877520"
  },
  {
    "text": "would that be sort of like a two mode one i can actually use torch x as a command line to go ahead and submit my",
    "start": "3877520",
    "end": "3884240"
  },
  {
    "text": "my dot py file but then can i actually do the equivalence thing in the jupyter notebook or or do i have to use torch",
    "start": "3884240",
    "end": "3891280"
  },
  {
    "text": "exclusively from the command line does this make sense uh yeah so so that that's a great question so i mean like",
    "start": "3891280",
    "end": "3897680"
  },
  {
    "text": "you can um so so torch x like has a command line like really as a helper",
    "start": "3897680",
    "end": "3904319"
  },
  {
    "text": "uh but you can do everything uh like with just like pure python like from the library standpoint like that works just",
    "start": "3904319",
    "end": "3909920"
  },
  {
    "text": "fine that said uh like the way i set up the notebook was you can like run shell commands in a",
    "start": "3909920",
    "end": "3916160"
  },
  {
    "text": "jupyter notebooks i would just say like yeah exclamation mark or check subscribe and it works just fine",
    "start": "3916160",
    "end": "3921440"
  },
  {
    "text": "but one thing i really want to stress on is that like i really like this first comment around like a two mode sort of",
    "start": "3921440",
    "end": "3927680"
  },
  {
    "text": "thing so like one thing we've observed with a lot of people making their journey to prod is like they're sort of like like",
    "start": "3927680",
    "end": "3935039"
  },
  {
    "text": "they know how to use pytorch like they they know how a trainer loop works they've set up their code and now",
    "start": "3935039",
    "end": "3940160"
  },
  {
    "text": "they're like okay i want to go deploy it and now you're telling me i need to go learn like kubernetes or i need to go learn",
    "start": "3940160",
    "end": "3947039"
  },
  {
    "text": "like maybe ray or i need to go to and that's just like not fun for if especially if it's your first time doing",
    "start": "3947039",
    "end": "3953200"
  },
  {
    "text": "this like and if you're a time crunch right like it's just it's its own thing and you need to invest time learning it one of the things we're really proud of",
    "start": "3953200",
    "end": "3959599"
  },
  {
    "text": "in tor checks is that because because changing schedulers or cloud",
    "start": "3959599",
    "end": "3964799"
  },
  {
    "text": "providers is really just a flag it becomes much easier to almost have like a free market approach right like",
    "start": "3964799",
    "end": "3970880"
  },
  {
    "text": "where you just sort of benchmark and see like what did you prefer like what was really your preference for your model",
    "start": "3970880",
    "end": "3976880"
  },
  {
    "text": "and then you can sort of like hone in on that when you're actually deploying something uh so so yeah i mean at least uh",
    "start": "3976880",
    "end": "3982960"
  },
  {
    "text": "personally and i think this is true for a bunch of my other peers at meta uh like trojan is sort of a great",
    "start": "3982960",
    "end": "3988160"
  },
  {
    "text": "experimentation library for knowing what to do in production uh you know that's been sort of like i",
    "start": "3988160",
    "end": "3995280"
  },
  {
    "text": "don't think we do a good enough job of advertising that in the docs but at least my my personal experience with it",
    "start": "3995280",
    "end": "4000640"
  },
  {
    "text": "uh like that's been that's been a huge uh huge productivity boost for myself i mean what would you what would you",
    "start": "4000640",
    "end": "4006799"
  },
  {
    "text": "tell the pie choice developers if they were to to use two checks would they use uh torch torch x sdk programmatically or",
    "start": "4006799",
    "end": "4014240"
  },
  {
    "text": "would they just use the command line to submit the jobs what would you do yeah so so so i would say like it really",
    "start": "4014240",
    "end": "4021599"
  },
  {
    "text": "depends on like your level of expertise but i think if you want to use like so",
    "start": "4021599",
    "end": "4027359"
  },
  {
    "text": "think of it this way right like you remember all these components i showed so the thing is that there's a lot of built-in ones so for the most part like",
    "start": "4027359",
    "end": "4034079"
  },
  {
    "text": "if you're doing like simple like stuff that everyone does like stuff like i don't know trying a resnet in a dp way",
    "start": "4034079",
    "end": "4039839"
  },
  {
    "text": "you probably don't need to write any code actually you can probably pick up like those components and run them with the command line",
    "start": "4039839",
    "end": "4045359"
  },
  {
    "text": "that said if you if you're doing something like a bit more unique you're probably going to need to write your own component but like i showed you it's",
    "start": "4045359",
    "end": "4051599"
  },
  {
    "text": "really just a python configuration that looks like yaml but at least it's programmable unlike camel",
    "start": "4051599",
    "end": "4057839"
  },
  {
    "text": "uh you know i i don't particularly enjoy programming like yaml i i think a few people do but regardless like you know",
    "start": "4057839",
    "end": "4064720"
  },
  {
    "text": "you so so so once you've done that you can still like just trigger those components and use them",
    "start": "4064720",
    "end": "4070880"
  },
  {
    "text": "uh from the command line so so really the experience is like what do you prefer personally i've used the",
    "start": "4070880",
    "end": "4076559"
  },
  {
    "text": "the command line more but uh other members on the team pretty much just use the core interface and the core",
    "start": "4076559",
    "end": "4082880"
  },
  {
    "text": "libraries and they're like again the command line's not magic right it just purely uses all the core library",
    "start": "4082880",
    "end": "4088160"
  },
  {
    "text": "functions where you can do everything like running describing uh and actually and if you go to the",
    "start": "4088160",
    "end": "4093280"
  },
  {
    "text": "torch x and you like basically if you go to tor checks schedulers you can like literally see how the code",
    "start": "4093280",
    "end": "4099278"
  },
  {
    "text": "is implemented it's really straightforward because all the schedulers are going to have a described function they're going to have a run",
    "start": "4099279",
    "end": "4104400"
  },
  {
    "text": "function they're going to have a log function right and so you can see like like how simple they actually are so you can just",
    "start": "4104400",
    "end": "4110560"
  },
  {
    "text": "call those functions directly that's actually how our unit tests work actually now that i think about it so i've actually used both",
    "start": "4110560",
    "end": "4116880"
  },
  {
    "text": "really yeah so we actually have the ability to use programmatically as well as using the cli correct um",
    "start": "4116880",
    "end": "4122000"
  },
  {
    "text": "let me see if there's any more questions coming in i think we have answered almost everything um",
    "start": "4122000",
    "end": "4128080"
  },
  {
    "text": "i don't see any more questions coming in but since i hold the mic i might as well my mother drop it and ask you one",
    "start": "4128080",
    "end": "4134080"
  },
  {
    "text": "question out of levity mark are you ready for this go for it yeah okay well richard liu wanted to know",
    "start": "4134080",
    "end": "4140640"
  },
  {
    "text": "it is that you claimed you are on twitter 24 7. so if i send you a dm message at 1 30 am in the morning are",
    "start": "4140640",
    "end": "4146640"
  },
  {
    "text": "you going to respond probably probably unfortunately well there you go my girlfriend's not too happy about that",
    "start": "4146640",
    "end": "4152880"
  },
  {
    "text": "okay there you go richard he's going to answer if you send him a dm message at 1 30 1 31 am in the morning so that's",
    "start": "4152880",
    "end": "4158480"
  },
  {
    "text": "that's one way to find out well i think i think that really concludes today's great two presentations from our team um",
    "start": "4158480",
    "end": "4166798"
  },
  {
    "text": "wonderful discussions wonderful discourse this is a great collaboration from the community and i",
    "start": "4166799",
    "end": "4172159"
  },
  {
    "text": "think as i said earlier and i'm going to repeat day and day after because it's tattooed under my ears community is at",
    "start": "4172159",
    "end": "4178000"
  },
  {
    "text": "the heart of all the open source projects so we do want you to give us a lot of feedback both on this particular",
    "start": "4178000",
    "end": "4183520"
  },
  {
    "text": "integration as well as on the ray train we would love to hear your feedback and again if you actually want to give a",
    "start": "4183520",
    "end": "4189359"
  },
  {
    "text": "talk at this particular meetup um send me email at jules j-u-l-j-u-l-e-s",
    "start": "4189359",
    "end": "4195760"
  },
  {
    "text": "note like family jewels but j-u-l-e-s at any scale.com and we can somehow set up",
    "start": "4195760",
    "end": "4202560"
  },
  {
    "text": "uh uh uh an abstract for you other than that um thanks a lot for",
    "start": "4202560",
    "end": "4207679"
  },
  {
    "text": "coming stay healthy and hopefully one of these days we'll be able to meet all in in person",
    "start": "4207679",
    "end": "4214239"
  },
  {
    "text": "at the next few meetups and again stay healthy and good night and thanks a",
    "start": "4214239",
    "end": "4220159"
  },
  {
    "text": "lot for joining us cheers guys",
    "start": "4220159",
    "end": "4224679"
  }
]