[
  {
    "text": "yeah thank you okay cool yeah we're excited to be here today um yeah just a quick intro um",
    "start": "3840",
    "end": "11559"
  },
  {
    "text": "so yeah I'm Stephanie I'm one of the creators of Ray I'm also now a professor at University of Washington and this is",
    "start": "11559",
    "end": "18199"
  },
  {
    "text": "Ry uh he's one of the members of the ray core team and a lead developer on the compiled graph system which we'll be",
    "start": "18199",
    "end": "24560"
  },
  {
    "text": "talking about a lot more later in this talk okay so for today we're going to go",
    "start": "24560",
    "end": "29800"
  },
  {
    "text": "through through a quick overview of some of the major features um that have been released in Ray cor since the last Ray",
    "start": "29800",
    "end": "36280"
  },
  {
    "text": "Summit and then we'll spend the rest of the talk uh actually doing a deep dive of this system called Ray compiled",
    "start": "36280",
    "end": "42960"
  },
  {
    "text": "graphs uh which you may have seen in the keynote and this is essentially a new backend for the ray core system um and",
    "start": "42960",
    "end": "50039"
  },
  {
    "text": "so we'll do an overview of kind of the reasons why we built this and how you can use it",
    "start": "50039",
    "end": "56039"
  },
  {
    "text": "yourself so for those of you who aren't super familiar with Ray core uh Ray cor",
    "start": "56039",
    "end": "61399"
  },
  {
    "text": "is basically the AI compute engine that underpins all of the distributed libraries in the ray ecosystem so all",
    "start": "61399",
    "end": "68560"
  },
  {
    "text": "the things that you you know all the libraries that you love using in Ray um from Ray train to Ray data uh what",
    "start": "68560",
    "end": "75080"
  },
  {
    "text": "underlies all of these is the same set of core Primitives and this is remote functions",
    "start": "75080",
    "end": "81280"
  },
  {
    "text": "or tasks and remote classes or Actors and The backend for this API is",
    "start": "81280",
    "end": "88000"
  },
  {
    "text": "Ray cor um so this is a general purpose distributed execution system and you know anytime you use a ray Library",
    "start": "88000",
    "end": "94600"
  },
  {
    "text": "you're really using Ray core so there have been a ton of different features uh released since the",
    "start": "94600",
    "end": "100960"
  },
  {
    "text": "last Ray Summit so I just want to highlight a few of them here we now have weekly releases which is a really",
    "start": "100960",
    "end": "106880"
  },
  {
    "text": "exciting development uh going from 6 weeks I think on average per release",
    "start": "106880",
    "end": "111960"
  },
  {
    "text": "before we now also have support for structured logging um so this supports log format and also Json logging on any",
    "start": "111960",
    "end": "119360"
  },
  {
    "text": "scale we have a ton of different stability bug fixes and a lot of these are related to execution on spot instances so you can",
    "start": "119360",
    "end": "126880"
  },
  {
    "text": "expect to see better reliability in these cases I'll also do a little bit more of a tech Deep dive on two features",
    "start": "126880",
    "end": "134000"
  },
  {
    "text": "uh Ray generators and some exciting developments in scalability of Ray clusters going from 4,000 nodes uh at",
    "start": "134000",
    "end": "141360"
  },
  {
    "text": "the last Ray Summit to now over 8,000 nodes today so uh Ray streaming generators",
    "start": "141360",
    "end": "148280"
  },
  {
    "text": "what is this um this is basically the idea of taking a python generator and",
    "start": "148280",
    "end": "153959"
  },
  {
    "text": "now being able to execute it as array remote task and what's new now is that",
    "start": "153959",
    "end": "159159"
  },
  {
    "text": "we support streaming Behavior Uh meaning that you can actually consume object refs while the task is still",
    "start": "159159",
    "end": "165879"
  },
  {
    "text": "executing and this pattern is actually being used today already in libraries like Ray data and Ray serve so one",
    "start": "165879",
    "end": "172760"
  },
  {
    "text": "example in Ray serve is for LM generation where you want to be able to consume some partial results while the",
    "start": "172760",
    "end": "179400"
  },
  {
    "text": "the model is still decoding the rest of the sequence and so just to give a quick illustration of this the way that this",
    "start": "179400",
    "end": "186959"
  },
  {
    "text": "works is that we'll have a caller process and the worker process that actually executes the task and you can",
    "start": "186959",
    "end": "193720"
  },
  {
    "text": "submit a generator task using the normal. remote syntax and you get back",
    "start": "193720",
    "end": "198959"
  },
  {
    "text": "this generator of object refs so you can call rad. get on these object refs and",
    "start": "198959",
    "end": "204519"
  },
  {
    "text": "this will block as usual until the result becomes ready so on the worker",
    "start": "204519",
    "end": "209599"
  },
  {
    "text": "side will start executing this task asynchronously and once this yields the",
    "start": "209599",
    "end": "215280"
  },
  {
    "text": "first result we can now unblock the color and the caller can then do the same thing to get the next reference and",
    "start": "215280",
    "end": "222120"
  },
  {
    "text": "this will execute in parallel with the worker so H and so on and so in this way",
    "start": "222120",
    "end": "227400"
  },
  {
    "text": "you can see how you can actually put some work on the color and that will actually overlap now with the rate worker",
    "start": "227400",
    "end": "234680"
  },
  {
    "text": "execution uh the other major development that I wanted to talk more about is scalability to 8,000",
    "start": "235360",
    "end": "241920"
  },
  {
    "text": "noes and so this is kind of a high level overview of the ray architecture uh typically array cluster will consist of",
    "start": "241920",
    "end": "249200"
  },
  {
    "text": "some pool of worker nodes which are homogeneous they all look the same um and then one head node which runs some",
    "start": "249200",
    "end": "255560"
  },
  {
    "text": "kind of specialized cluster services and these include the global control service",
    "start": "255560",
    "end": "261000"
  },
  {
    "text": "so this is a centralized service that runs a bunch of kind of cluster uh metadata management tasks um so that",
    "start": "261000",
    "end": "268360"
  },
  {
    "text": "includes things like actor schedule scheding placement group scheduling and node resource availability and we also have the",
    "start": "268360",
    "end": "275800"
  },
  {
    "text": "dashboard server uh which is maybe a lesser known process but equally important so this is actually used for a",
    "start": "275800",
    "end": "281840"
  },
  {
    "text": "bunch of auxiliary application functions um not just hosting dashboard metrics but also things like streaming uh logs",
    "start": "281840",
    "end": "288880"
  },
  {
    "text": "from the rest of the cluster and supporting the ray jobs API so you can imagine that because",
    "start": "288880",
    "end": "295800"
  },
  {
    "text": "these two services are centralized if you design them naively",
    "start": "295800",
    "end": "300840"
  },
  {
    "text": "and you try to scale out a cluster um you could end up you know sort of linearly increasing the load on these",
    "start": "300840",
    "end": "307000"
  },
  {
    "text": "two Services right um so through some great engineering work on the event Loop",
    "start": "307000",
    "end": "312759"
  },
  {
    "text": "that kind of underpins these two Services uh We've now made it so that even when the load on these uh Services",
    "start": "312759",
    "end": "320000"
  },
  {
    "text": "increases with the size of the cluster it won't affect the availability of the services um and so this is one of the",
    "start": "320000",
    "end": "326960"
  },
  {
    "text": "major uh features that has led to this improved scale avability",
    "start": "326960",
    "end": "332039"
  },
  {
    "text": "okay so I'm going to switch gears now and talk a bit more about the ray",
    "start": "332080",
    "end": "337120"
  },
  {
    "text": "compiled graph system and so I want to first talk about why we actually built this system and so so to do that we're",
    "start": "337120",
    "end": "344840"
  },
  {
    "text": "going to contrast it with kind of the classic Ray cor back end that you may be more familiar with um so one way I like",
    "start": "344840",
    "end": "351560"
  },
  {
    "text": "to think about Ray cor is as kind of this programmable infrastructure that you can use for any kind of distributed",
    "start": "351560",
    "end": "358160"
  },
  {
    "text": "compute intensive tasks right so that includes things like data training and inference um but",
    "start": "358160",
    "end": "364360"
  },
  {
    "text": "historically the performance of Ray core at millisecond scale becomes a problem",
    "start": "364360",
    "end": "369800"
  },
  {
    "text": "um and so here what I'm talking about is if you have compute tasks that run in tens of milliseconds or",
    "start": "369800",
    "end": "376000"
  },
  {
    "text": "less and so compiled graphs is a way of giving you a ray core like",
    "start": "376000",
    "end": "381960"
  },
  {
    "text": "API but with much better performance for cases where you repeatedly execute the",
    "start": "381960",
    "end": "387440"
  },
  {
    "text": "same task graph and also Al native support for GPU GPU communication which",
    "start": "387440",
    "end": "393240"
  },
  {
    "text": "again is something that users have actually requested uh over and over again in Ray cor and we'll talk a little",
    "start": "393240",
    "end": "398680"
  },
  {
    "text": "bit about why that's been difficult to support in the past so today we'll talk about uh do more of an overview of why",
    "start": "398680",
    "end": "406280"
  },
  {
    "text": "we actually built this system uh what the API is and how you can use it and",
    "start": "406280",
    "end": "411400"
  },
  {
    "text": "also some exciting use cases that you can actually build out with compiled graphs so one of the major takeaways is",
    "start": "411400",
    "end": "418599"
  },
  {
    "text": "that Ray compiled graph s are 10 to 20 times faster than Ray tasks run on the",
    "start": "418599",
    "end": "423879"
  },
  {
    "text": "classic Ray core backend and this is specifically for static task graphs and",
    "start": "423879",
    "end": "429240"
  },
  {
    "text": "you know you might be wondering what are these cases uh well a lot of multi-gpu workloads actually can be expressed with",
    "start": "429240",
    "end": "436199"
  },
  {
    "text": "completely static control flow and so later in this talk we'll give some examples here in multi-gpu LM inference",
    "start": "436199",
    "end": "443479"
  },
  {
    "text": "as well as some cases in distributed training and if you're interested in more of these workloads I'd also",
    "start": "443479",
    "end": "448919"
  },
  {
    "text": "encourage you to to attend these talks later in Ray Summit which will do a deeper dive on both inference and",
    "start": "448919",
    "end": "455599"
  },
  {
    "text": "training okay so that I'm going to show uh a little bit more about why we",
    "start": "455599",
    "end": "461400"
  },
  {
    "text": "actually built this system by first taking a look at why there are these overheads in the classic Ray cor back",
    "start": "461400",
    "end": "467360"
  },
  {
    "text": "end so this is going to be just a quick example where we have uh we're going to create these two Echo actors which just",
    "start": "467360",
    "end": "474680"
  },
  {
    "text": "take in a message and then Echo back the same message so we'll create the two worker",
    "start": "474680",
    "end": "480000"
  },
  {
    "text": "the two actors here and then we're going to create uh just two tasks so we're",
    "start": "480000",
    "end": "485319"
  },
  {
    "text": "going to pass the message hello to actor a and then pass that result to actor B",
    "start": "485319",
    "end": "491199"
  },
  {
    "text": "and for just for the sake of this example I'll also have the object store here and so what this looks like is that",
    "start": "491199",
    "end": "497360"
  },
  {
    "text": "we're going to pass this message hello to actor a by creating an object in the object store and we're going to pass the",
    "start": "497360",
    "end": "504800"
  },
  {
    "text": "reference to that object to actor a which will'll then read from that object and we have to repeat this same process",
    "start": "504800",
    "end": "512440"
  },
  {
    "text": "when actor a creates its return value so we'll return back the reference to the driver which can then pass it to the",
    "start": "512440",
    "end": "518839"
  },
  {
    "text": "actor B and so on and so you can see here that there's",
    "start": "518839",
    "end": "524159"
  },
  {
    "text": "actually quite a few different arrows moving back and forth in this picture um and that's only considering the ones",
    "start": "524159",
    "end": "529800"
  },
  {
    "text": "that are kind of on the critical path of execution and so you can see very clearly here that the rpcs especially to",
    "start": "529800",
    "end": "536680"
  },
  {
    "text": "the driver can become a bottleneck and so this is exactly why we built",
    "start": "536680",
    "end": "542760"
  },
  {
    "text": "compiled graphs um when we think about rayor classic API one of the really great things about it is that it lets",
    "start": "542760",
    "end": "548959"
  },
  {
    "text": "you create tasks dynamically but that also comes with a cost right so it means that you have to",
    "start": "548959",
    "end": "555560"
  },
  {
    "text": "pay the cost of doing these rpcs at runtime to execute the task you often have to do some kind of dynamic memory",
    "start": "555560",
    "end": "561959"
  },
  {
    "text": "allocation to pass around arguments and return values and it's also very difficult to support peer-to-peer",
    "start": "561959",
    "end": "567560"
  },
  {
    "text": "protocols like nvidia's Nick protocol and that's because these kinds",
    "start": "567560",
    "end": "573399"
  },
  {
    "text": "of communications uh communication patterns require you to actually uh",
    "start": "573399",
    "end": "578720"
  },
  {
    "text": "allocate resources upfront on all peers and so in compiled graphs uh the",
    "start": "578720",
    "end": "584640"
  },
  {
    "text": "idea is to take static task graphs which you can express with a ray cor like API but instead of allocating resources",
    "start": "584640",
    "end": "592880"
  },
  {
    "text": "dynamically we can just allocate them once and then reuse them for multiple executions",
    "start": "592880",
    "end": "599440"
  },
  {
    "text": "and the other great thing about this is that it now also lets us declare these kinds of peer-to-peer communication",
    "start": "599440",
    "end": "604680"
  },
  {
    "text": "edges up front and that way we can schedule them before the execution actually",
    "start": "604680",
    "end": "610399"
  },
  {
    "text": "happens and so just to give you this picture what we want is to go from something like this to something more",
    "start": "610399",
    "end": "617959"
  },
  {
    "text": "like this where the only communication that's happening is kind of along the path of the",
    "start": "617959",
    "end": "624160"
  },
  {
    "text": "data so to show you the API and how this works I mentioned there are two steps",
    "start": "624160",
    "end": "630000"
  },
  {
    "text": "right so first we need to compile the graph and so we're going to now from the user side we're going to declare what",
    "start": "630000",
    "end": "636639"
  },
  {
    "text": "the task graph looks like and this is actually the same kind of task graph that we had earlier except now we're",
    "start": "636639",
    "end": "642760"
  },
  {
    "text": "going to bind to an input placeholder instead of binding to a physical",
    "start": "642760",
    "end": "648000"
  },
  {
    "text": "input next what we can do is add this new compile command and under the hood",
    "start": "648000",
    "end": "653800"
  },
  {
    "text": "this is going to statically allocate the resources so we allocate some static buffers in the object store that can now",
    "start": "653800",
    "end": "660920"
  },
  {
    "text": "be reused across multiple executions instead of just once and we're also going to start an",
    "start": "660920",
    "end": "667360"
  },
  {
    "text": "execution loop on each involved actor that will know basically the tasks to execute at",
    "start": "667360",
    "end": "673839"
  },
  {
    "text": "runtime and that way during execution we can kind of avoid those expensive rpcs",
    "start": "673839",
    "end": "679560"
  },
  {
    "text": "so now when we actually call execute on the graph uh we're just passing in the input from the driver we can also use",
    "start": "679560",
    "end": "686800"
  },
  {
    "text": "the normal rad. get API and what this looks like during execution is basically just the path of",
    "start": "686800",
    "end": "693240"
  },
  {
    "text": "the data another nice thing that we can do",
    "start": "693240",
    "end": "698279"
  },
  {
    "text": "in this API is support uh exception handling that gives you a very similar experience to normal Ray core so the",
    "start": "698279",
    "end": "705480"
  },
  {
    "text": "idea here is that for application Level exceptions um let's say actor a throws maybe a python value error what we can",
    "start": "705480",
    "end": "713000"
  },
  {
    "text": "do is store that result in the object store and that way we can actually throw it to Downstream tasks and even into the",
    "start": "713000",
    "end": "719639"
  },
  {
    "text": "driver and this is just like normal Ray cor for system level exceptions so you",
    "start": "719639",
    "end": "726079"
  },
  {
    "text": "know there are certain cases that we can't recover from so let's say this actor a dies in that case we actually",
    "start": "726079",
    "end": "732279"
  },
  {
    "text": "can't produce results anymore right so what we can do instead is just tear down the dag resources so we can clear away",
    "start": "732279",
    "end": "739839"
  },
  {
    "text": "the objects in the object store and also stop these execution loops and one",
    "start": "739839",
    "end": "745120"
  },
  {
    "text": "really nice thing about this is that you're kind of just back in normal Ray corand you can use these actors that are",
    "start": "745120",
    "end": "750839"
  },
  {
    "text": "remaining and potentially compile a new graph all right so I'm going to hand it over to Ray to talk about some of the",
    "start": "750839",
    "end": "757480"
  },
  {
    "text": "Primitives that are specific to GPU programming hi uh thanks Stephanie next",
    "start": "757480",
    "end": "764920"
  },
  {
    "text": "I'm going to talk about how to use uh R compiled graphs to program across multiple",
    "start": "764920",
    "end": "770760"
  },
  {
    "text": "gpus um suppose we have a sender actor uh that runs on gpu0 and we have a",
    "start": "770760",
    "end": "777040"
  },
  {
    "text": "receiver actor that runs on GPU 1 and the sender needs to send the tensor to the",
    "start": "777040",
    "end": "783000"
  },
  {
    "text": "receiver uh well um compelled graphs could send this through shareed memory just like",
    "start": "783000",
    "end": "788760"
  },
  {
    "text": "recore um but it also adds the support to uh for direct GPU to GPU transfer",
    "start": "788760",
    "end": "795360"
  },
  {
    "text": "through ni and the way to achieve this is really simple uh as we can see from this code",
    "start": "795360",
    "end": "803600"
  },
  {
    "text": "snippit uh the we can just take the output of the send method and annotate",
    "start": "803600",
    "end": "808680"
  },
  {
    "text": "with a type hint uh that is the torch tensor type and we specify transport",
    "start": "808680",
    "end": "813720"
  },
  {
    "text": "equals ni and then we pass this to the receiver and that's it uh the rest is",
    "start": "813720",
    "end": "819680"
  },
  {
    "text": "the same uh as Stephanie just talked about we compel and then",
    "start": "819680",
    "end": "825000"
  },
  {
    "text": "execute uh other than the pointto point APS we just talked about uh compel",
    "start": "825680",
    "end": "831839"
  },
  {
    "text": "graphs also support Collective apis um to illustrate we will use all to",
    "start": "831839",
    "end": "838480"
  },
  {
    "text": "all pattern as an example uh that is all reduce so in this example we have a few",
    "start": "838480",
    "end": "844680"
  },
  {
    "text": "workers and each worker has its partial data and we have this we put there this",
    "start": "844680",
    "end": "850600"
  },
  {
    "text": "data the references to this data to uh list called parts and then we can simply",
    "start": "850600",
    "end": "856480"
  },
  {
    "text": "call read. collect uh read. Collective do all reduce on the",
    "start": "856480",
    "end": "862399"
  },
  {
    "text": "parts uh and that's it here uh we also use transport equals ni group handle",
    "start": "862399",
    "end": "870000"
  },
  {
    "text": "instead of just the string ni and this allows us to use a previous previously",
    "start": "870000",
    "end": "875800"
  },
  {
    "text": "defined uh ni group for the communication instead of using the default",
    "start": "875800",
    "end": "881399"
  },
  {
    "text": "one we also support other um patterns like all to one and one to all uh they",
    "start": "881399",
    "end": "887680"
  },
  {
    "text": "have a very similar API uh a common problem for GPU to GPO",
    "start": "887680",
    "end": "894360"
  },
  {
    "text": "communication is the possibility of a deadlock um this is because Nico",
    "start": "894360",
    "end": "900240"
  },
  {
    "text": "operations are blocking and uh it could delog if execution has circular",
    "start": "900240",
    "end": "907000"
  },
  {
    "text": "dependencies let's look at an example uh say we have two actors which use uh ni",
    "start": "907000",
    "end": "913519"
  },
  {
    "text": "to communicate between the T between their tasks underneath uh compell graphs adds",
    "start": "913519",
    "end": "919920"
  },
  {
    "text": "this ni send receive operations to the corresponding tasks and we can see that there is the",
    "start": "919920",
    "end": "926920"
  },
  {
    "text": "circular dependency uh circular data dependency between the actors um as shown in with the red arrows and this",
    "start": "926920",
    "end": "934680"
  },
  {
    "text": "could lead to De logs during excusion our solution is to generate a",
    "start": "934680",
    "end": "940680"
  },
  {
    "text": "schedule uh at compel time uh the compelled graph generates the schedules for all the actors with global ordering",
    "start": "940680",
    "end": "948800"
  },
  {
    "text": "uh with topological sort and then it schedules the Nico operation pairs for",
    "start": "948800",
    "end": "954360"
  },
  {
    "text": "example the send and receive together when there are no other data dependencies",
    "start": "954360",
    "end": "960920"
  },
  {
    "text": "compell graph supports overlapping GPU communication and computation for better",
    "start": "961480",
    "end": "967639"
  },
  {
    "text": "performance suppose we have a trainer actor that first runs a forward task and",
    "start": "967639",
    "end": "972800"
  },
  {
    "text": "then runs the backward task um you know this task use Nico",
    "start": "972800",
    "end": "978240"
  },
  {
    "text": "receive and send Communications so we have these operations the default um the naive",
    "start": "978240",
    "end": "986959"
  },
  {
    "text": "approach would just run the them uh with a single uh single Coda stream but uh",
    "start": "986959",
    "end": "994160"
  },
  {
    "text": "compell graphs optimized this by using multiple CL streams and leveraging a scheduling algorithm to overlap the",
    "start": "994160",
    "end": "1000920"
  },
  {
    "text": "communication and computation as a result uh a receive",
    "start": "1000920",
    "end": "1007000"
  },
  {
    "text": "stream a computation stream and a send stream are used and these operations run on different streams and can be",
    "start": "1007000",
    "end": "1014079"
  },
  {
    "text": "overlapped as we can see that the receive operation and the forward can be overlap",
    "start": "1014079",
    "end": "1019360"
  },
  {
    "text": "and backward and sand can be overlapped and this reduces the time for to finish",
    "start": "1019360",
    "end": "1025240"
  },
  {
    "text": "the execution we evaluated the performance",
    "start": "1025240",
    "end": "1030280"
  },
  {
    "text": "of recompiled graphs compared to recore uh for a simple um run trip",
    "start": "1030280",
    "end": "1036760"
  },
  {
    "text": "Benchmark compelled graphs is 17x faster on a single note and 2.8x faster on",
    "start": "1036760",
    "end": "1043918"
  },
  {
    "text": "Multi noes and the difference is larger if",
    "start": "1043919",
    "end": "1048960"
  },
  {
    "text": "multiple tasks are involved for example if we do scatter and gather of 32",
    "start": "1048960",
    "end": "1054480"
  },
  {
    "text": "tasks uh compelled graphs is 18x faster and if we chain 32 tasks the compelled",
    "start": "1054480",
    "end": "1061679"
  },
  {
    "text": "graphs is 21x faster so to summarize compell graphs is",
    "start": "1061679",
    "end": "1067120"
  },
  {
    "text": "2.8 to 21x faster as for Nico",
    "start": "1067120",
    "end": "1074120"
  },
  {
    "text": "operations compelled graphs has no overhead for static shape the",
    "start": "1074120",
    "end": "1079640"
  },
  {
    "text": "tensor and uh for dynamic shifted tensor it has uh roughly .5 milliseconds",
    "start": "1079640",
    "end": "1086039"
  },
  {
    "text": "overhead because uh compiled graphs needs to send additional uh metadata transfer through share man",
    "start": "1086039",
    "end": "1093039"
  },
  {
    "text": "channel uh but we are working to improve",
    "start": "1093039",
    "end": "1097600"
  },
  {
    "text": "this so after talking about apis uh what can you actually build with compar",
    "start": "1098240",
    "end": "1103559"
  },
  {
    "text": "graphs I'm going to talk about a few examples and we will start from",
    "start": "1103559",
    "end": "1109159"
  },
  {
    "text": "distributed inference specifically optimizing the vrm control",
    "start": "1109159",
    "end": "1114640"
  },
  {
    "text": "plane uh this diagram shows the VM control plane architecture so there's a",
    "start": "1114640",
    "end": "1120320"
  },
  {
    "text": "driver that runs the llm engine which sends the request to multiple workers",
    "start": "1120320",
    "end": "1126320"
  },
  {
    "text": "that execute the different model sharts and previously when Ray was used",
    "start": "1126320",
    "end": "1132400"
  },
  {
    "text": "the overhead was 2 to 3 milliseconds for the communication and the serialization",
    "start": "1132400",
    "end": "1139799"
  },
  {
    "text": "when we use compelled graphs the overhead is about 100 to 200",
    "start": "1140080",
    "end": "1145880"
  },
  {
    "text": "microc and with compelled graphs we have reduced the latency for distributed",
    "start": "1145880",
    "end": "1152360"
  },
  {
    "text": "inference we also implemented tensor and P parallel parallelism with compared",
    "start": "1152360",
    "end": "1158559"
  },
  {
    "text": "graphs in vrm as you can see from the core",
    "start": "1158559",
    "end": "1163679"
  },
  {
    "text": "implementation um the code is really uh short",
    "start": "1163679",
    "end": "1169600"
  },
  {
    "text": "and from high level what this does is you know the compile graphs takes the",
    "start": "1169600",
    "end": "1175159"
  },
  {
    "text": "request as the graph input and then for each tensor Parallel Group the workers just execute the model",
    "start": "1175159",
    "end": "1183400"
  },
  {
    "text": "and their output are transferred to the corresponding workers in the next PP",
    "start": "1183400",
    "end": "1190480"
  },
  {
    "text": "stage we Meed the vrm performance when using compelled graphs uh for online",
    "start": "1192320",
    "end": "1198960"
  },
  {
    "text": "inference where the latency is important we achieved a 10 to 20%",
    "start": "1198960",
    "end": "1204640"
  },
  {
    "text": "reduction uh compared with the original architecture and for offline inference",
    "start": "1204640",
    "end": "1210120"
  },
  {
    "text": "where the throughput is important we achieved the 10%",
    "start": "1210120",
    "end": "1215520"
  },
  {
    "text": "Improvement we also achieved the performance performance parity in various other setups uh note that this",
    "start": "1215520",
    "end": "1222840"
  },
  {
    "text": "comparison is between a very simple compelled graph implementation and the",
    "start": "1222840",
    "end": "1228080"
  },
  {
    "text": "continuously improved vrm implementation so this highlights the compelled graph",
    "start": "1228080",
    "end": "1233840"
  },
  {
    "text": "has uh provid good performance even with uh manual",
    "start": "1233840",
    "end": "1239919"
  },
  {
    "text": "optimization compelled graphs also provide great support for heterogenous training uh so in general there are two",
    "start": "1241039",
    "end": "1248159"
  },
  {
    "text": "types of distributed training one is homogenous training that every process executes the same program and examples",
    "start": "1248159",
    "end": "1256080"
  },
  {
    "text": "include data parallel fully shed data par parallel and tensor parallel training uh for heterogenous training",
    "start": "1256080",
    "end": "1263760"
  },
  {
    "text": "different processes execute different programs examples include p and parallel",
    "start": "1263760",
    "end": "1269960"
  },
  {
    "text": "and when we use heterogen clusters re compel de uh lets you easily",
    "start": "1269960",
    "end": "1276919"
  },
  {
    "text": "express and scale the heterogeneous training so for p parallel training um",
    "start": "1276919",
    "end": "1285080"
  },
  {
    "text": "compare graphs allows you to express various pipeline schedules in less than",
    "start": "1285080",
    "end": "1290720"
  },
  {
    "text": "100 lines of code as for um multimodel training um compelled grph",
    "start": "1290720",
    "end": "1299360"
  },
  {
    "text": "allow you to run different uh training components for example the vision and text in coders on different types of",
    "start": "1299360",
    "end": "1306080"
  },
  {
    "text": "gpus which can help to improve GPU utilization uh to summarize uh we",
    "start": "1306080",
    "end": "1313600"
  },
  {
    "text": "covered two classes of applications for compelled graphs uh uh one is",
    "start": "1313600",
    "end": "1319279"
  },
  {
    "text": "distributed infant and another is hyrogen training we covered this in in",
    "start": "1319279",
    "end": "1325080"
  },
  {
    "text": "um from a high level but there are two additional talks for this one is the multi-gpu inference with VM the other is",
    "start": "1325080",
    "end": "1333320"
  },
  {
    "text": "beyond fsdp talk uh the first one is today at 4 p.m. the other one is tomorrow at 1 p.m. and I highly",
    "start": "1333320",
    "end": "1340600"
  },
  {
    "text": "recommend attending these two talks for more details of how we exactly um achieve you know Implement them with",
    "start": "1340600",
    "end": "1347559"
  },
  {
    "text": "compelled graphs uh next I'm going to hand this back to Stephanie to talk about what's",
    "start": "1347559",
    "end": "1355559"
  },
  {
    "text": "next okay cool thanks Ray um so yeah for Ray compiled graphs the next step is to",
    "start": "1355559",
    "end": "1362080"
  },
  {
    "text": "release the beta version so keep an eye out for that some of the major features that are in progress for this right now",
    "start": "1362080",
    "end": "1369559"
  },
  {
    "text": "we actually already mentioned a couple of these but a lot of these are around improving support for GPU tasks um",
    "start": "1369559",
    "end": "1375600"
  },
  {
    "text": "especially improving performance and these will be directly leveraged in the end use cases um including VM where",
    "start": "1375600",
    "end": "1382919"
  },
  {
    "text": "we're currently working towards an architecture migration for vm's control plane um to make Ray compiled graphs the",
    "start": "1382919",
    "end": "1390159"
  },
  {
    "text": "default for Ray uh VM on Ray in the future as well as exploring some more",
    "start": "1390159",
    "end": "1395200"
  },
  {
    "text": "complex architectures like prefill dis aration for on the training side uh",
    "start": "1395200",
    "end": "1401000"
  },
  {
    "text": "we're going to continue kind of building on the heterogeneous use cases that we've seen so far and just further try",
    "start": "1401000",
    "end": "1406440"
  },
  {
    "text": "to improve have an excellent usability experience in addition to Performance so uh what's next for Ray",
    "start": "1406440",
    "end": "1414279"
  },
  {
    "text": "core at large um so we talked a lot about Ray compiled graphs and how that builds on top of the ray core",
    "start": "1414279",
    "end": "1420039"
  },
  {
    "text": "Primitives underlying all of this is Ray clusters right so you often we'll have a",
    "start": "1420039",
    "end": "1425480"
  },
  {
    "text": "cluster of VMS that actually execute uh the ray core",
    "start": "1425480",
    "end": "1430559"
  },
  {
    "text": "system now uh historically it's been true that whenever you build a ray",
    "start": "1430559",
    "end": "1435679"
  },
  {
    "text": "application you often need to create its own Ray cluster just to host that application uh which can really be a",
    "start": "1435679",
    "end": "1442440"
  },
  {
    "text": "problem especially if you're talking about expensive GPU resources or you might want to share these gpus across",
    "start": "1442440",
    "end": "1448880"
  },
  {
    "text": "different Ray applications right and so to better support this uh we're excited",
    "start": "1448880",
    "end": "1454159"
  },
  {
    "text": "to announce a new feature um this you know a new feature in the uh in the",
    "start": "1454159",
    "end": "1459400"
  },
  {
    "text": "future of Ray cor uh which is called virtual clusters and the idea here is to",
    "start": "1459400",
    "end": "1464679"
  },
  {
    "text": "basically allow you to take a single physical Ray cluster so just one head node",
    "start": "1464679",
    "end": "1470120"
  },
  {
    "text": "here and be able to slice it into multiple virtual clusters and so basically what this will let you do is",
    "start": "1470120",
    "end": "1476880"
  },
  {
    "text": "actually use all of the normal Ray scheduling Primitives but within kind of this now virtual container of Ray",
    "start": "1476880",
    "end": "1483399"
  },
  {
    "text": "resources and so hopefully in the future U this will now allow us to efficiently share resources between jobs while also",
    "start": "1483399",
    "end": "1490480"
  },
  {
    "text": "enforcing resource isolation between those jobs okay so uh today I talked about uh",
    "start": "1490480",
    "end": "1497520"
  },
  {
    "text": "we talked about some of of the exciting developments in Ray cor uh towards you know its both present and future as an",
    "start": "1497520",
    "end": "1504559"
  },
  {
    "text": "AI compute engine um and yeah we're happy to take any",
    "start": "1504559",
    "end": "1509919"
  },
  {
    "text": "[Applause]",
    "start": "1512180",
    "end": "1517919"
  },
  {
    "text": "questions sure yeah oh yeah you can regarding the Benchmark between red core",
    "start": "1517919",
    "end": "1524279"
  },
  {
    "text": "and comp it seems like the single not significantly",
    "start": "1524279",
    "end": "1530000"
  },
  {
    "text": "better compared to the m scenario uh what's the can you explain what was the intu behind it and there any plan to",
    "start": "1530000",
    "end": "1537640"
  },
  {
    "text": "improve them scenario sure yeah the question was about the benchmarks for um",
    "start": "1537640",
    "end": "1544120"
  },
  {
    "text": "Ray compiled graphs versus Ray core okay maybe I was going to see if I could pull up the slides but I think it's too hard",
    "start": "1544120",
    "end": "1550120"
  },
  {
    "text": "um but yeah so the there was this kind of big gap on single node performance and then for multi- node performance the",
    "start": "1550120",
    "end": "1556360"
  },
  {
    "text": "Gap was not as large um so the question was why um do you do you want to take a shot for okay well yeah so basically",
    "start": "1556360",
    "end": "1563320"
  },
  {
    "text": "right now we've actually Focus primarily on single node performance um especially thinking about kind of multi-gpu",
    "start": "1563320",
    "end": "1569520"
  },
  {
    "text": "inference use cases where you often have multiple devices on a single node um and so basically what we've done is kind of",
    "start": "1569520",
    "end": "1575880"
  },
  {
    "text": "Leverage the shared memory Object Store to really improve that case and so essentially what you can do is kind of",
    "start": "1575880",
    "end": "1582200"
  },
  {
    "text": "eliminate all rpcs on the critical path uh for single node execution uh for multi-node whenever we cross a node",
    "start": "1582200",
    "end": "1589760"
  },
  {
    "text": "boundary we still have to use rpcs right now and we're just using kind of vanill RPC for that that's definitely something",
    "start": "1589760",
    "end": "1596279"
  },
  {
    "text": "we would like to improve in the future so you know you can imagine actually using something like RDMA for instance",
    "start": "1596279",
    "end": "1601919"
  },
  {
    "text": "uh to further improve that yeah um I think maybe you are",
    "start": "1601919",
    "end": "1607919"
  },
  {
    "text": "next questions the first one is how does this",
    "start": "1607919",
    "end": "1613278"
  },
  {
    "text": "interact Ray tune you said yeah yeah so the question was how does this interact with Ray tune and is there integration",
    "start": "1613760",
    "end": "1621520"
  },
  {
    "text": "with it um so today there's no integration with Ray tune uh or actually",
    "start": "1621520",
    "end": "1626720"
  },
  {
    "text": "with any of the other Ray libraries for that matter um so in the future we'll definitely think about how we can",
    "start": "1626720",
    "end": "1632080"
  },
  {
    "text": "actually integrate these directly into Ray libraries but for now we're kind of focusing on sort of the multi-gpu use",
    "start": "1632080",
    "end": "1638640"
  },
  {
    "text": "cases that have not been supported well yet by rayor and those are specifically",
    "start": "1638640",
    "end": "1643720"
  },
  {
    "text": "VM and some of these kinds of heterogeneous training cases question was um maybe different use case",
    "start": "1643720",
    "end": "1651440"
  },
  {
    "text": "that we care a lot about and you think be the",
    "start": "1651440",
    "end": "1658279"
  },
  {
    "text": "to to that you have TR yeah so the question was about reproducibility and",
    "start": "1658279",
    "end": "1665039"
  },
  {
    "text": "what we can do in compiled graphs to better support that um I think that's a great Point actually yeah I think one of",
    "start": "1665039",
    "end": "1671519"
  },
  {
    "text": "the values of the compiled graph API we hope is that you get actually better predictability right so you know if you",
    "start": "1671519",
    "end": "1677720"
  },
  {
    "text": "run the graph once you know that you're going to get the same performance for every kind of future execution and yeah",
    "start": "1677720",
    "end": "1683559"
  },
  {
    "text": "I can definitely imagine taking that further and actually being able to kind of save the compiled graph as an",
    "start": "1683559",
    "end": "1688600"
  },
  {
    "text": "artifact yeah um yeah maybe you yeah",
    "start": "1688600",
    "end": "1698039"
  },
  {
    "text": "dire that is going",
    "start": "1708519",
    "end": "1711519"
  },
  {
    "text": "to yeah so the question was how this relates to uh llm training Frameworks",
    "start": "1713720",
    "end": "1719440"
  },
  {
    "text": "like Megatron or deep speed um do you want I can yeah sure so I think uh we",
    "start": "1719440",
    "end": "1726120"
  },
  {
    "text": "have actually in our applications we actually uh use compare Dax to compare with deep speed specifically and we",
    "start": "1726120",
    "end": "1733640"
  },
  {
    "text": "achieved U performance gains because of our you know uh the performance optimizations in V uh in uh compar",
    "start": "1733640",
    "end": "1741159"
  },
  {
    "text": "graphs and actually there's a talk like I pointed out that talk that we'll have",
    "start": "1741159",
    "end": "1746600"
  },
  {
    "text": "a deep dive on that uh spe specifically I recommend you going to that talk to",
    "start": "1746600",
    "end": "1751799"
  },
  {
    "text": "for more details yeah I think uh I'll also add that as of today there's no kind of end",
    "start": "1751799",
    "end": "1758919"
  },
  {
    "text": "to-end training solution built on top of compiled graphs um that is something that we're kind of thinking about for",
    "start": "1758919",
    "end": "1764720"
  },
  {
    "text": "the future but you know at the moment the goal is actually to take kind of the cases that we don't think are supported",
    "start": "1764720",
    "end": "1770480"
  },
  {
    "text": "as well in Megatron and deep speed and Frameworks like that so especially these kinds of heterogeneous use cases and try",
    "start": "1770480",
    "end": "1776760"
  },
  {
    "text": "to see you know what performance we can gain in those scenarios yeah going",
    "start": "1776760",
    "end": "1785919"
  },
  {
    "text": "back yeah so so the the question was about the failure case where a node dies",
    "start": "1796640",
    "end": "1802000"
  },
  {
    "text": "uh what does the end user see so again this is a little bit of a work in progress uh but the thing that we would",
    "start": "1802000",
    "end": "1808640"
  },
  {
    "text": "like to support is basically to be able to get a system level exception on the driver as you would in any case we",
    "start": "1808640",
    "end": "1815200"
  },
  {
    "text": "already have that today actually um but what you don't get as good of an experience on is kind of the root cause",
    "start": "1815200",
    "end": "1821399"
  },
  {
    "text": "um so you know you can imagine it' be great to know actually that the root cause was that this particular actor",
    "start": "1821399",
    "end": "1826440"
  },
  {
    "text": "died um maybe over there yeah yes mentioned",
    "start": "1826440",
    "end": "1833960"
  },
  {
    "text": "that you [Music] have but doesn't this compiled job profile with artifacts",
    "start": "1833960",
    "end": "1843760"
  },
  {
    "text": "actually have little bit ofing problem with the virtu",
    "start": "1843760",
    "end": "1849640"
  },
  {
    "text": "clust that because you're",
    "start": "1849640",
    "end": "1853480"
  },
  {
    "text": "having oh so there the question was are are you asking about scheduling overhead",
    "start": "1856159",
    "end": "1861760"
  },
  {
    "text": "yeah so I think the question was about if we introduce this kind of virtual cluster concept won't there be",
    "start": "1861760",
    "end": "1866799"
  },
  {
    "text": "performance overhead from that yeah so that's probably a topic of further discussion you can find me offline but",
    "start": "1866799",
    "end": "1872559"
  },
  {
    "text": "basically we think that we can kind of avoid that by doing it and kind of you know it's basically what you can do is",
    "start": "1872559",
    "end": "1878799"
  },
  {
    "text": "just support using the same scheduling mechanisms that we have today but essentially you just add a filter first",
    "start": "1878799",
    "end": "1884960"
  },
  {
    "text": "uh to filter down to the virtual resources yeah",
    "start": "1884960",
    "end": "1889919"
  },
  {
    "text": "yeah yeah I think one thing you can do with execution is about you can do a lot",
    "start": "1892200",
    "end": "1898919"
  },
  {
    "text": "optimization because you know what extion look like but those strategy are",
    "start": "1898919",
    "end": "1904519"
  },
  {
    "text": "highly depend on the different scenario so uh the compil world will do some",
    "start": "1904519",
    "end": "1910600"
  },
  {
    "text": "general strategy for the um so I think the question was about",
    "start": "1910600",
    "end": "1917039"
  },
  {
    "text": "what optimization we can apply in a general scenario yeah so I think I mean this is",
    "start": "1917039",
    "end": "1923159"
  },
  {
    "text": "still I think an open question of like yeah what you know basically like how how specialized can we be to the",
    "start": "1923159",
    "end": "1929720"
  },
  {
    "text": "application but we think that there are certain optimizations that are really common um like you know array talked",
    "start": "1929720",
    "end": "1935799"
  },
  {
    "text": "about overlapping uh compute and communication and these are things that you can apply actually without really",
    "start": "1935799",
    "end": "1941519"
  },
  {
    "text": "needing to know much about the application um so those are the the kind of cases we'll Target first",
    "start": "1941519",
    "end": "1949799"
  },
  {
    "text": "yeah is the compil graph system uh engine agnostic um and yeah so right now",
    "start": "1960480",
    "end": "1967600"
  },
  {
    "text": "we are working primarily with VM um and that's because VM was already built on",
    "start": "1967600",
    "end": "1973120"
  },
  {
    "text": "top of Ray and they were seeing issues with Ray core and the performance overheads from that um but it it's not",
    "start": "1973120",
    "end": "1979559"
  },
  {
    "text": "you know there's nothing about it that is VM specific um so definitely you could integrate it with other LM engines",
    "start": "1979559",
    "end": "1986200"
  },
  {
    "text": "like SGL um I think really the requirement would be that right now at least like you need to be using python",
    "start": "1986200",
    "end": "1993200"
  },
  {
    "text": "um yeah I I should also say we're supporting nickel um just as the kind of",
    "start": "1993200",
    "end": "1998240"
  },
  {
    "text": "First Step but actually you know Ray uh actually already kind of has support for",
    "start": "1998240",
    "end": "2003720"
  },
  {
    "text": "being able to do these custom communicators and so we're you know our our goal is also not to be kind of uh",
    "start": "2003720",
    "end": "2010519"
  },
  {
    "text": "specific to a particular Hardware",
    "start": "2010519",
    "end": "2014120"
  },
  {
    "text": "platform sorry what was the question yeah what is the current",
    "start": "2019480",
    "end": "2025799"
  },
  {
    "text": "support for RDMA uh currently there is no support but that's definitely something we're interested in supporting",
    "start": "2025799",
    "end": "2032039"
  },
  {
    "text": "in the future especially for these kinds of control plane messages yeah",
    "start": "2032039",
    "end": "2038398"
  },
  {
    "text": "s oh yeah do you have option comp you have",
    "start": "2043720",
    "end": "2049800"
  },
  {
    "text": "option to yeah is there an option to compile",
    "start": "2049800",
    "end": "2056200"
  },
  {
    "text": "for throughput versus latency uh that's an interesting question uh right now we don't have anything I would say that's",
    "start": "2056200",
    "end": "2063000"
  },
  {
    "text": "specific to those two regimes um and actually you know we're using the exact same same system for both training and",
    "start": "2063000",
    "end": "2069280"
  },
  {
    "text": "inference but yeah I mean like in the future there might be cases where you would want to do something different at",
    "start": "2069280",
    "end": "2074760"
  },
  {
    "text": "compile time yeah yeah like optimization strategy",
    "start": "2074760",
    "end": "2080800"
  },
  {
    "text": "like data caching",
    "start": "2080800",
    "end": "2084440"
  },
  {
    "text": "like uh do we have any optimization strategies Based on data caching in the object store is that the question um so",
    "start": "2088040",
    "end": "2096839"
  },
  {
    "text": "I guess in a way the system actually is kind of designed around that concept because we use the same sort of uh Shar",
    "start": "2096839",
    "end": "2104320"
  },
  {
    "text": "like the same physical memory over and over again um so you can write different data to it but uh you'll use the same",
    "start": "2104320",
    "end": "2110720"
  },
  {
    "text": "physical memory um but we don't have anything related to kind of like a higher level concept of data caching",
    "start": "2110720",
    "end": "2119839"
  },
  {
    "text": "yeah yeah",
    "start": "2121880",
    "end": "2125880"
  },
  {
    "text": "yeah uh do we think that Ray data is back end can continue uh could be replaced by Ray compiled graphs or will",
    "start": "2133680",
    "end": "2139760"
  },
  {
    "text": "it continue to use Ray core um do you want to yeah I think I think that's",
    "start": "2139760",
    "end": "2146760"
  },
  {
    "text": "definitely an interesting question and we are exploring I think uh right now like there's um you know just some early",
    "start": "2146760",
    "end": "2153960"
  },
  {
    "text": "work but it's not decided yet fully yeah I think also you know it's like our",
    "start": "2153960",
    "end": "2159640"
  },
  {
    "text": "goal is actually for this to work well together with Ray cor because there are definitely things about this that you",
    "start": "2159640",
    "end": "2165040"
  },
  {
    "text": "know uh Ray data is actually an extremely Dynamic workload in many cases",
    "start": "2165040",
    "end": "2170119"
  },
  {
    "text": "um if you're using it for batch inference there can be kind of static parts to it and those could definitely",
    "start": "2170119",
    "end": "2175640"
  },
  {
    "text": "benefit from this but by and large a lot of it is actually Dynamic so ideally you would want I think some combination of",
    "start": "2175640",
    "end": "2183799"
  },
  {
    "text": "both C",
    "start": "2185440",
    "end": "2189440"
  },
  {
    "text": "yeah potentially you could imagine a future where you have some kind of hybrid graph and parts of it are Dynamic",
    "start": "2196160",
    "end": "2201720"
  },
  {
    "text": "and some parts of it are static yeah yeah but we're we're probably a bit aways from that right now",
    "start": "2201720",
    "end": "2209599"
  },
  {
    "text": "yeah oh okay okay I think uh we are out of time",
    "start": "2209599",
    "end": "2216319"
  },
  {
    "text": "sorry okay",
    "start": "2216319",
    "end": "2219520"
  }
]