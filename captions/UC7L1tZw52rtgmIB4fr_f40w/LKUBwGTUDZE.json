[
  {
    "text": "hi everyone uh I'm o I'm the engineering lead on uh racer and with me I have Ed",
    "start": "3879",
    "end": "9840"
  },
  {
    "text": "who's uh behind the creation of racer and been leading racer since the beginning um so quick show of fans how",
    "start": "9840",
    "end": "16118"
  },
  {
    "text": "many of you are using racer already or have at least tried it out cool and how",
    "start": "16119",
    "end": "22600"
  },
  {
    "text": "many of you have made it to production all right uh it's great to see like the more more and more people",
    "start": "22600",
    "end": "30000"
  },
  {
    "text": "take R serve all the way through um so today we'll be talking about fast and reliable model serving with r serve and",
    "start": "30000",
    "end": "36360"
  },
  {
    "text": "any scale um so since a lot of you uh I saw are still new to racer we'll go a",
    "start": "36360",
    "end": "42480"
  },
  {
    "text": "little bit over the overview and motivation behind uh building RAC serve and also some of the cool things we've",
    "start": "42480",
    "end": "48399"
  },
  {
    "text": "been doing in 2024 uh then I'll talk about why RAC serve is particularly well suited for",
    "start": "48399",
    "end": "54719"
  },
  {
    "text": "Gen applications and then finally Ed will uh talk about some of the extensions we've been doing to make",
    "start": "54719",
    "end": "60760"
  },
  {
    "text": "racer really great on any scale and any scale really the most complete and uh",
    "start": "60760",
    "end": "65960"
  },
  {
    "text": "performant way for model surveying so let's get right into it um",
    "start": "65960",
    "end": "73119"
  },
  {
    "text": "so this was uh how we came about building racer so if you guys are",
    "start": "73119",
    "end": "79320"
  },
  {
    "text": "building ml applications uh you're typically doing more than just building one model so let's imagine a case of a",
    "start": "79320",
    "end": "87240"
  },
  {
    "text": "recommendation system or a computer vision system so uh in the case of a CV application like",
    "start": "87240",
    "end": "92920"
  },
  {
    "text": "our friends at samsara did uh they had a bunch of different models maybe some for segmentation some for classification and",
    "start": "92920",
    "end": "99920"
  },
  {
    "text": "then some additional business Logic on top to uh actually round out their complete service now the most typical",
    "start": "99920",
    "end": "107399"
  },
  {
    "text": "way of deploying it is using microservices uh and a lot of you are still in this like scary land where",
    "start": "107399",
    "end": "113560"
  },
  {
    "text": "every component is its own microservice um and then each of these are like scaling independently and so on",
    "start": "113560",
    "end": "121360"
  },
  {
    "text": "uh but there are some drawbacks to this one it's really hard to innovate there's one single application that is now split",
    "start": "121360",
    "end": "128399"
  },
  {
    "text": "across many different configs images you have to set up separate cicd pipelines",
    "start": "128399",
    "end": "134680"
  },
  {
    "text": "separate monitoring for each of these indiv individual microservices and then when you're upgrading them you have to",
    "start": "134680",
    "end": "140640"
  },
  {
    "text": "make sure things are backwards compatible uh so this can really slow down and add a lot of operational",
    "start": "140640",
    "end": "147319"
  },
  {
    "text": "burden secondly you can't really share resources that easily so uh let's take",
    "start": "147319",
    "end": "152879"
  },
  {
    "text": "our CV example uh where you could do some business logic just on CPUs and maybe your uh segmentation model needs",
    "start": "152879",
    "end": "159920"
  },
  {
    "text": "to run on some kind of GPU let's say a1g and your classification needs to run on a different kind of GPU this is hard to",
    "start": "159920",
    "end": "167480"
  },
  {
    "text": "express inside one single microservice and then uh get the most out of your",
    "start": "167480",
    "end": "172599"
  },
  {
    "text": "hardware and uh this is especially harder as you go to llms where you're",
    "start": "172599",
    "end": "178319"
  },
  {
    "text": "deploying multiple where deploying models across multiple",
    "start": "178319",
    "end": "183360"
  },
  {
    "text": "gpus so this was the motivation behind building racer we wanted a flexible scalable and efficient framework for",
    "start": "183360",
    "end": "190599"
  },
  {
    "text": "online inference so let's break that down the first aspect was simplicity so everyone uses python for",
    "start": "190599",
    "end": "198239"
  },
  {
    "text": "ML development so this had to be a framework that was python native and with a simple decorator you could go to",
    "start": "198239",
    "end": "205480"
  },
  {
    "text": "production uh so with Ray serve you can just add the serve. deployment decorator on top of your business logic and ml",
    "start": "205480",
    "end": "212519"
  },
  {
    "text": "model classes uh and that's really all it takes for any anyone whether it's a",
    "start": "212519",
    "end": "218239"
  },
  {
    "text": "data scientist or um an ml platform engineer to take this model to",
    "start": "218239",
    "end": "225200"
  },
  {
    "text": "production then we wanted to add first class support for many model inference so in our previous CV case you could",
    "start": "225239",
    "end": "233200"
  },
  {
    "text": "compose each of these as separate rer deployments each with their own resource needs and they can scale independently",
    "start": "233200",
    "end": "239799"
  },
  {
    "text": "within the same service and you can also do things like fractional gpus so let's",
    "start": "239799",
    "end": "245159"
  },
  {
    "text": "say one of your models only needs a quarter or a half of a GPU you can express that and they can scale",
    "start": "245159",
    "end": "250319"
  },
  {
    "text": "independently within the same service and there's also another pattern in racer that's popularly use called Model",
    "start": "250319",
    "end": "256959"
  },
  {
    "text": "multiplexing where you might have like thousands of fine tune models one per customer and you can deploy these very",
    "start": "256959",
    "end": "263840"
  },
  {
    "text": "efficiently on one pool of uh replicas and one one one deployment",
    "start": "263840",
    "end": "270919"
  },
  {
    "text": "and finally we wanted to make it very flexible so you can uh use you can deploy any gen models any application on",
    "start": "271240",
    "end": "278440"
  },
  {
    "text": "top of racer and get the most out of your underlying hardware and all of this needed to be production ready uh and",
    "start": "278440",
    "end": "285680"
  },
  {
    "text": "this this is available on both kubernetes in open source as well as any",
    "start": "285680",
    "end": "291280"
  },
  {
    "text": "scale here are some of the really cool success stories we've seen over the years so samsara saved 50% of their",
    "start": "291280",
    "end": "299160"
  },
  {
    "text": "annual inference cost by moving to Ray serve so they were deploying this kind of monolitic architecture and when they",
    "start": "299160",
    "end": "305919"
  },
  {
    "text": "had a more expressive framework with RAC serve they could really like get better Hardware utilization and financial",
    "start": "305919",
    "end": "313039"
  },
  {
    "text": "really took Ray serve to the next scale by building a 4,000 GPU node online serving cluster for recommendation",
    "start": "313039",
    "end": "319160"
  },
  {
    "text": "systems and then Clary on any scale improved performance by over 20% by using model multiplexing with RAC serve",
    "start": "319160",
    "end": "327280"
  },
  {
    "text": "and we see this like graph on the right hand side which kind of shows the exponential growth Trend since we",
    "start": "327280",
    "end": "333840"
  },
  {
    "text": "launched race Ser back in 2022 and just since last race Summit we've seen Forex",
    "start": "333840",
    "end": "339360"
  },
  {
    "text": "growth in clusters and users uh so this really speaks to kind of the adoption",
    "start": "339360",
    "end": "344680"
  },
  {
    "text": "and racer across the community so let's talk about some of",
    "start": "344680",
    "end": "350479"
  },
  {
    "text": "the things we've been doing in 2024 uh so there's three main areas of focus for us one is keep pushing this cost and",
    "start": "350479",
    "end": "358199"
  },
  {
    "text": "performance optimization in racer make it more flexible for Gen applications",
    "start": "358199",
    "end": "364319"
  },
  {
    "text": "and then thirdly uh keep pushing the production Readiness so you can go to",
    "start": "364319",
    "end": "369800"
  },
  {
    "text": "production with racer very reliably here are some of the main features there's a lot of other things",
    "start": "369800",
    "end": "376000"
  },
  {
    "text": "that happened but I'll just list out some top four first we revamped our Auto scaling",
    "start": "376000",
    "end": "382599"
  },
  {
    "text": "and load Management in racer so we heard some feedback from the community for Autos scaling to be more responsive and",
    "start": "382599",
    "end": "388880"
  },
  {
    "text": "also have capabilities for load shedding for better safety in production so these were added in racer in",
    "start": "388880",
    "end": "397120"
  },
  {
    "text": "2024 in 2023 we launched multiple application support so this is where you",
    "start": "397120",
    "end": "402599"
  },
  {
    "text": "can deploy multiple separate ml applications with their own upgrade life cycles in the same race serve cluster uh",
    "start": "402599",
    "end": "411360"
  },
  {
    "text": "and one piece of feedback that the community had first they loved the feature and a lot of people started",
    "start": "411360",
    "end": "416599"
  },
  {
    "text": "using it because it enabled better Hardware utilization uh but they wanted to separate out the",
    "start": "416599",
    "end": "422960"
  },
  {
    "text": "dependencies for these with ML packages getting really bloated these days this was important as you're deploying",
    "start": "422960",
    "end": "429039"
  },
  {
    "text": "different model different types of models uh so we added support for separate container images within the",
    "start": "429039",
    "end": "435599"
  },
  {
    "text": "same Race Service and this was one I was really",
    "start": "435599",
    "end": "441240"
  },
  {
    "text": "excited about because this reflects how serious race serve is becoming across the community this was a community",
    "start": "441240",
    "end": "447759"
  },
  {
    "text": "contributed feature for running race serve on any hardware accelerator so folks from Google and Amazon and Intel",
    "start": "447759",
    "end": "455280"
  },
  {
    "text": "contributed um to Ray to run racer on whether it was tpus on Google Cloud",
    "start": "455280",
    "end": "461680"
  },
  {
    "text": "influential on Amazon Cloud and Habana chips on Intel so you can run racer really on any type of",
    "start": "461680",
    "end": "469560"
  },
  {
    "text": "accelerator and finally we've been partnering with other companies to also add support for the best --class",
    "start": "469560",
    "end": "475000"
  },
  {
    "text": "Frameworks for uh online inference so Nvidia Triton who um who we partnered with now supports an",
    "start": "475000",
    "end": "482560"
  },
  {
    "text": "integration where you can get the performance benefits of Triton on top of racer um and then there's also tensor RT",
    "start": "482560",
    "end": "489840"
  },
  {
    "text": "llm and VM support cool now let's look into why Ray",
    "start": "489840",
    "end": "496000"
  },
  {
    "text": "serve is really great for Gen applications and gen comes with a lot of unique",
    "start": "496000",
    "end": "502639"
  },
  {
    "text": "challenges so we'll do that with a case study on serving llms on top of Ray",
    "start": "502639",
    "end": "507720"
  },
  {
    "text": "serve so first big challenge is models are absolutely huge these days there's um 7B",
    "start": "507720",
    "end": "515440"
  },
  {
    "text": "models that were commonly used in the beginning that were 14 gig but now we have like 70b and 405b models which are",
    "start": "515440",
    "end": "522959"
  },
  {
    "text": "approaching a terabyte and gpus are extremely expensive and the availability doesn't",
    "start": "522959",
    "end": "528760"
  },
  {
    "text": "seem to be getting that much better uh so you can see h10g which has uh 24 gigs",
    "start": "528760",
    "end": "534800"
  },
  {
    "text": "of memory is already pretty pricey and then when you get to h100 the cost of deploying these systems becomes really",
    "start": "534800",
    "end": "542120"
  },
  {
    "text": "really high so that means you really want to get as much out of your Hardware as",
    "start": "542120",
    "end": "548519"
  },
  {
    "text": "possible so at any scale we were U looking earlier this year on how can we",
    "start": "548519",
    "end": "554279"
  },
  {
    "text": "make a great simple experience for people to deploy llms and these were some of the use cases people came to us",
    "start": "554279",
    "end": "561480"
  },
  {
    "text": "with first this is really table Stakes you have to do multi-gpu um these models are often not",
    "start": "561480",
    "end": "569560"
  },
  {
    "text": "enough to fit in one GPU memory uh and now we also see multi node so you have to uh enable these 1 TB models to run",
    "start": "569560",
    "end": "578160"
  },
  {
    "text": "across multiple nodes high performance serving is extremely critical um as I mentioned",
    "start": "578160",
    "end": "585440"
  },
  {
    "text": "earlier the cost of these gpus is really high so you have to get the best inclass",
    "start": "585440",
    "end": "591160"
  },
  {
    "text": "performance that you can another common pattern with uh that",
    "start": "591160",
    "end": "597240"
  },
  {
    "text": "people are doing with llms is adap it to their own domains and they might do it separately for different customers as",
    "start": "597240",
    "end": "603279"
  },
  {
    "text": "well so people want to serve many different fine tune models inside the same",
    "start": "603279",
    "end": "610160"
  },
  {
    "text": "cluster and then a common pattern is also serving many different base model",
    "start": "610160",
    "end": "615800"
  },
  {
    "text": "so imagine you're building a chatbot where some queries could be handled with a 7B model but some need some more",
    "start": "615800",
    "end": "622200"
  },
  {
    "text": "advanced reasoning where you need to route that query to a 70b",
    "start": "622200",
    "end": "627640"
  },
  {
    "text": "model and then of course CMS are kind of like any other model in in which you can",
    "start": "627640",
    "end": "634320"
  },
  {
    "text": "just take a model behind an endpoint and call it an application you need to build more components around it uh so",
    "start": "634320",
    "end": "640040"
  },
  {
    "text": "retrieval augmented generation is extremely common and then this is a really unique",
    "start": "640040",
    "end": "647880"
  },
  {
    "text": "and cool requirement where you need to be able to go hybrid Cloud so a lot of times people can get the gpus they need",
    "start": "647880",
    "end": "654040"
  },
  {
    "text": "on just AWS or Google cloud and they'll procure gpus from a vendor like Lambda Labs or corve or one of these other",
    "start": "654040",
    "end": "660760"
  },
  {
    "text": "third party providers and you want to be able to use all your gpus",
    "start": "660760",
    "end": "667000"
  },
  {
    "text": "efficiently so this is how we went about solving it in racer this is kind of what",
    "start": "667800",
    "end": "672839"
  },
  {
    "text": "the very high level architecture of this Library looks like first we have one",
    "start": "672839",
    "end": "678000"
  },
  {
    "text": "racer deployment that can handle all the openai apis uh and business logic and",
    "start": "678000",
    "end": "683560"
  },
  {
    "text": "this is critical for um integrating with any of your other llm tooling like Lang",
    "start": "683560",
    "end": "688600"
  },
  {
    "text": "chain Lama index and so on and these can run on CPU only and then behind the",
    "start": "688600",
    "end": "694639"
  },
  {
    "text": "scenes you can have many different llms that can be easily set up with configuration files and um you can",
    "start": "694639",
    "end": "701160"
  },
  {
    "text": "deploy an 8B model or a 70b model as well as embedding model so let's look at how IT addresses some of our use",
    "start": "701160",
    "end": "708560"
  },
  {
    "text": "cases model parallel is uh very easy to set up and this is one of the core",
    "start": "708560",
    "end": "714040"
  },
  {
    "text": "strengths of Ray is managing and orchestrating python processes across different gpus",
    "start": "714040",
    "end": "720079"
  },
  {
    "text": "in fact even pipeline parallelism is now possible with the new compiled graphs API that we just launched and we've been",
    "start": "720079",
    "end": "726120"
  },
  {
    "text": "experimenting with that internally high performance uh so here",
    "start": "726120",
    "end": "732200"
  },
  {
    "text": "we support VM tensor RT llm really any inference engine under the hood that you",
    "start": "732200",
    "end": "737440"
  },
  {
    "text": "want uh but we've also been working on our own proprietary kernels to really uh",
    "start": "737440",
    "end": "742839"
  },
  {
    "text": "speed up some of the execution multi- Lowa or multiple",
    "start": "742839",
    "end": "748480"
  },
  {
    "text": "fine-tuning models is possible with RAC serves multiplexing support so the way",
    "start": "748480",
    "end": "753560"
  },
  {
    "text": "it's set up is we use a rer multiplex deployment and that can efficiently Swap",
    "start": "753560",
    "end": "759000"
  },
  {
    "text": "and U load in the Lowa weights as needed and then use vlm support for running",
    "start": "759000",
    "end": "764480"
  },
  {
    "text": "inference on many luras in the same batch multimodel inference so doing 8B",
    "start": "764480",
    "end": "770720"
  },
  {
    "text": "and 70b ac across one service is um also possible to set up with this Library",
    "start": "770720",
    "end": "776680"
  },
  {
    "text": "quite easily um and you can upgrade the engine in one",
    "start": "776680",
    "end": "782440"
  },
  {
    "text": "go and finally hybrid cloud is also possible so something that we did",
    "start": "782440",
    "end": "787560"
  },
  {
    "text": "internally as well as our customers are doing are is so taking like this example",
    "start": "787560",
    "end": "793320"
  },
  {
    "text": "of a 70b model that's running on an h100 some of it is running on AWS nodes but",
    "start": "793320",
    "end": "798959"
  },
  {
    "text": "it's also has the ability to burst into Lambda Labs capacity or some people are going the other way where they might",
    "start": "798959",
    "end": "804760"
  },
  {
    "text": "have a lot of Reserve capacity on a third party cloud and then they burst into the cloud when they need extra",
    "start": "804760",
    "end": "811920"
  },
  {
    "text": "capacity and let's look at how you would set up a retrieval augmented generation system so these uh this is very commonly",
    "start": "811920",
    "end": "819560"
  },
  {
    "text": "used to introduce more context into an llms response uh so this on a high level",
    "start": "819560",
    "end": "825560"
  },
  {
    "text": "consists of two parts once you get the user query you're going to do the retrieval uh which itself con contains",
    "start": "825560",
    "end": "832800"
  },
  {
    "text": "generating and embedding and doing a vector search over your vector database",
    "start": "832800",
    "end": "838120"
  },
  {
    "text": "and then converting that into the context and prompt that you can then send to your llm and then send the final",
    "start": "838120",
    "end": "845040"
  },
  {
    "text": "response to the user um I talked about Ros model composition earlier and that",
    "start": "845040",
    "end": "850560"
  },
  {
    "text": "is perfectly suited for the rag pattern where you can set up all of these as separate R serve deployments that can",
    "start": "850560",
    "end": "856800"
  },
  {
    "text": "use different resources and scale up independently now I'll hand it off to Ed",
    "start": "856800",
    "end": "862079"
  },
  {
    "text": "to talk about some of the awesome extensions we've been doing with racer on any",
    "start": "862079",
    "end": "867320"
  },
  {
    "text": "scale okay thank Q hopefully everybody can hear me um yeah so I'm going to talk a little bit about uh some of the cool",
    "start": "867320",
    "end": "873800"
  },
  {
    "text": "features we've been building to make RAC serve uh work great on any",
    "start": "873800",
    "end": "878959"
  },
  {
    "text": "scale okay um so for people who aren't too familiar with the any scale platform uh I think the way to think about it is",
    "start": "879199",
    "end": "884959"
  },
  {
    "text": "that it's really like an extension of Ray so Ray is like the AI compute engine",
    "start": "884959",
    "end": "890079"
  },
  {
    "text": "right um it makes it really easy to write distributed programs do all of the cool things for online serving applications that aay mentioned but",
    "start": "890079",
    "end": "896519"
  },
  {
    "text": "there are a lot of other things around that like cost tracking Administration features observability production",
    "start": "896519",
    "end": "902959"
  },
  {
    "text": "Readiness that don't really fit into the scope of Ray but you need if you want to really go to production and have like an",
    "start": "902959",
    "end": "908560"
  },
  {
    "text": "end to-end platform um so any scale is is our attempt at you know building a round Ray something that's really well",
    "start": "908560",
    "end": "915000"
  },
  {
    "text": "integrated into it and does offer you this like endtoend AI platform um so for",
    "start": "915000",
    "end": "920279"
  },
  {
    "text": "race serve there are a couple of kind of key areas that we focus on the first is",
    "start": "920279",
    "end": "926279"
  },
  {
    "text": "um adding really good support for like end to-end obs ability and debugging this is especially important for online",
    "start": "926279",
    "end": "932480"
  },
  {
    "text": "serving applications um you know it's really crucial that you can understand uh you know if you have a latency Spike",
    "start": "932480",
    "end": "938759"
  },
  {
    "text": "why is that happening if things go wrong you know where did they go wrong um and have monitoring alerting uh I'm sure",
    "start": "938759",
    "end": "945079"
  },
  {
    "text": "many people are familiar with this uh in a similar vein we also focus a lot on making sure that everything is",
    "start": "945079",
    "end": "951399"
  },
  {
    "text": "like hardened and production ready and um any scale also has a lot of unique",
    "start": "951399",
    "end": "956639"
  },
  {
    "text": "opportunities because it is this like uh full stack all the way from kind of managing the hardware to the you know",
    "start": "956639",
    "end": "962839"
  },
  {
    "text": "Ray layer to the library layer to the um application code we have some uh great",
    "start": "962839",
    "end": "968160"
  },
  {
    "text": "optimiz or opportunities to make full stack optimizations and improve performance and save cost so let's talk",
    "start": "968160",
    "end": "975199"
  },
  {
    "text": "about each of these in a little bit more detail so the first is observability and debugging um so any scale comes uh kind",
    "start": "975199",
    "end": "981800"
  },
  {
    "text": "of packaged with um an integrated UI that shows you like status at a glance",
    "start": "981800",
    "end": "987240"
  },
  {
    "text": "and historical uh information and also has a lot of collaboration features built in uh and we also have solutions",
    "start": "987240",
    "end": "993399"
  },
  {
    "text": "for logs and metrics out of the box um and uh I didn't mention here but we also have an integration for tracing",
    "start": "993399",
    "end": "999720"
  },
  {
    "text": "distributed tracing support so let's take a closer look at what it looks like when you run um a race serve application",
    "start": "999720",
    "end": "1006399"
  },
  {
    "text": "on any scale so this is um maybe a little bit small to see but this is basically just the the dashboard that",
    "start": "1006399",
    "end": "1012240"
  },
  {
    "text": "you see for uh an any skill service um so here you can see that this service on",
    "start": "1012240",
    "end": "1017519"
  },
  {
    "text": "the top left is currently the rolling out state which means that uh it's incrementally moving towards a new",
    "start": "1017519",
    "end": "1023079"
  },
  {
    "text": "version because it got deployed for each of the versions We can see all of the key metadata like the image that was",
    "start": "1023079",
    "end": "1029120"
  },
  {
    "text": "used uh the compute configuration that was used what version it was who deployed it when um so it gives you this",
    "start": "1029120",
    "end": "1035360"
  },
  {
    "text": "like added glance view of you know what is going on with my uh application and in addition to this we",
    "start": "1035360",
    "end": "1042000"
  },
  {
    "text": "also have um metrics that are built directly into the platform um so here I just picked uh two at random uh one is",
    "start": "1042000",
    "end": "1049000"
  },
  {
    "text": "like a utilization metric and another is a QPS metric um but we have a host of uh",
    "start": "1049000",
    "end": "1054640"
  },
  {
    "text": "like built-in dashboards that are useful out of the box and you can also customize your own grafana dashboards",
    "start": "1054640",
    "end": "1059720"
  },
  {
    "text": "and add alerts uh as needed one of the things that I'm really excited that we uh sort of launched this",
    "start": "1059720",
    "end": "1066200"
  },
  {
    "text": "year into the platform is built-in support for uh log aggregation and search so when you launch a service on",
    "start": "1066200",
    "end": "1072559"
  },
  {
    "text": "any scale you now get a view like what I have at the center of the screen here um where it Aggregates all of the logs not",
    "start": "1072559",
    "end": "1078919"
  },
  {
    "text": "only across all the nodes of one cluster but even across multiple clusters over time as you're um like deploying and",
    "start": "1078919",
    "end": "1085880"
  },
  {
    "text": "upgrading and so forth um and you can filter based on the version the component like what application it is",
    "start": "1085880",
    "end": "1092400"
  },
  {
    "text": "and uh you can even search so for example here I'm searching for a given request ID and I can see what happened",
    "start": "1092400",
    "end": "1098559"
  },
  {
    "text": "for that request ID across the system between you know race serve level logs and application logs so this is just",
    "start": "1098559",
    "end": "1104960"
  },
  {
    "text": "like an absolutely invaluable tool and it's uh it's awesome to have it just work out of the",
    "start": "1104960",
    "end": "1110840"
  },
  {
    "text": "box um and then one other thing I want to mention is that uh one of the things we're announcing at Ray Summit is that",
    "start": "1110840",
    "end": "1116000"
  },
  {
    "text": "any scale now works on kubernetes as like the compute back end basically any kubernetes cluster that you might run on",
    "start": "1116000",
    "end": "1122400"
  },
  {
    "text": "um so you can kind of get the best The Best of Both Worlds where you can run on your existing compute and integrate with other applications you have on",
    "start": "1122400",
    "end": "1128320"
  },
  {
    "text": "kubernetes but get all of these features and the other ones I'll talk about in The Talk um in addition to",
    "start": "1128320",
    "end": "1135679"
  },
  {
    "text": "that uh okay so that's just like a a short overview of the observability and debugging tools um let's talk a little",
    "start": "1135679",
    "end": "1141400"
  },
  {
    "text": "bit about the improved production Readiness so uh another feature that we have on any scale is um head node fault",
    "start": "1141400",
    "end": "1148159"
  },
  {
    "text": "tolerance out of the box um so Ray has this concept of a head node and by default it stores thing things in memory",
    "start": "1148159",
    "end": "1155480"
  },
  {
    "text": "um so while you know generally there aren't any issues if you do have like a hardware failure or you know anything",
    "start": "1155480",
    "end": "1160840"
  },
  {
    "text": "unexpected goes wrong uh it can cause some downtime in the application you can",
    "start": "1160840",
    "end": "1166159"
  },
  {
    "text": "address this by enabling head node fault tolerance but it adds add more kind of operational uh burden because you need",
    "start": "1166159",
    "end": "1172000"
  },
  {
    "text": "external storage um and on any scale we configure that for you by default and",
    "start": "1172000",
    "end": "1178000"
  },
  {
    "text": "handle the entire recovery process um uh automatically so if the head node does",
    "start": "1178000",
    "end": "1183280"
  },
  {
    "text": "go down for any reason you won't drop any requests and uh it will get",
    "start": "1183280",
    "end": "1188520"
  },
  {
    "text": "recovered we also um have support for uh rollouts like you saw in the UI page and",
    "start": "1188520",
    "end": "1194640"
  },
  {
    "text": "one of the things that I wanted to talk about is we've also extended this recently to support incremental roll",
    "start": "1194640",
    "end": "1199799"
  },
  {
    "text": "outs so let's talk about uh what that is and and why it's important so here I have a diagram of",
    "start": "1199799",
    "end": "1207600"
  },
  {
    "text": "what it looks like when you're running a service on any scale um so this any scale control plane which kind of manages the a load balancer as well as",
    "start": "1207600",
    "end": "1214960"
  },
  {
    "text": "uh some Ray clusters that are running applications in this case I have uh you know one Ray serve cluster and as OA",
    "start": "1214960",
    "end": "1222400"
  },
  {
    "text": "mentioned this could um this could consist of multiple deployments multiple models that are composed together and",
    "start": "1222400",
    "end": "1228240"
  },
  {
    "text": "you know pretty complex topology but here I've simplified it down to just call it a cluster and say that it uses 10 gpus this is in steady state so all",
    "start": "1228240",
    "end": "1236280"
  },
  {
    "text": "of the requests are um getting sent to this to this cluster and they'd be load balanced across all of the nodes in the",
    "start": "1236280",
    "end": "1243080"
  },
  {
    "text": "cluster when we want to do an upgrade um a a user would issue a deploy command",
    "start": "1243080",
    "end": "1248720"
  },
  {
    "text": "with a new configuration and the any scale control plane would first create a new version of the cluster and then it",
    "start": "1248720",
    "end": "1255159"
  },
  {
    "text": "would update the load balancer to uh slowly do a staged roll roll out for example starting with 10% of the traffic",
    "start": "1255159",
    "end": "1262000"
  },
  {
    "text": "and then proceeding assuming that all of the um you know status and health checks look",
    "start": "1262000",
    "end": "1267280"
  },
  {
    "text": "good but there's a pretty pretty big problem here uh in this picture which is that now when you're upgrading your uh",
    "start": "1267280",
    "end": "1274159"
  },
  {
    "text": "application this requires 20 gpus and you'll notice that uh only 10% of the",
    "start": "1274159",
    "end": "1279520"
  },
  {
    "text": "traffic is going to the second cluster so a lot of these GPU resources are are probably going to be idle um and this is",
    "start": "1279520",
    "end": "1286400"
  },
  {
    "text": "something that's a little bit of a unique problem for RAC serve because you can deploy these like complex topologies",
    "start": "1286400",
    "end": "1291760"
  },
  {
    "text": "on one cluster uh something like a a standard kubernetes deployment roll out",
    "start": "1291760",
    "end": "1297080"
  },
  {
    "text": "doesn't quite fit into the model um so this is an area that uh we had a few customers um uh that had issues with it",
    "start": "1297080",
    "end": "1305039"
  },
  {
    "text": "and we wanted to uh basically solve it and give you the best of both worlds so let's talk about what any scale supports",
    "start": "1305039",
    "end": "1311559"
  },
  {
    "text": "now so the key difference is instead of spinning up the full version of the new application uh or of the new cluster",
    "start": "1311559",
    "end": "1318200"
  },
  {
    "text": "well in instead create basically a minimal version of it um that uses the the fewest resources possible um in this",
    "start": "1318200",
    "end": "1324919"
  },
  {
    "text": "case maybe it only needs one GPU or you know maybe if they're two models it would just be two gpus and then uh we",
    "start": "1324919",
    "end": "1331640"
  },
  {
    "text": "will incrementally scale the old cluster down and the new cluster up while also",
    "start": "1331640",
    "end": "1338120"
  },
  {
    "text": "coordinating the traffic split so that each cluster is only getting as much traffic as it can currently handle and",
    "start": "1338120",
    "end": "1344600"
  },
  {
    "text": "in that way what we can do is incrementally um scale up so for for example uh we would eventually reach a",
    "start": "1344600",
    "end": "1350960"
  },
  {
    "text": "50/50 split and each cluster is about half the size and then the new cluster would be approaching the the full size",
    "start": "1350960",
    "end": "1358320"
  },
  {
    "text": "and then finally we would be able to finish the roll out and this entire process would only have required sort of",
    "start": "1358320",
    "end": "1363919"
  },
  {
    "text": "n plus one gpus so in this case only 11 instead of the 20 that we had with the naive",
    "start": "1363919",
    "end": "1369440"
  },
  {
    "text": "solution so this is a pretty simple uh feature in concept but obviously it's a",
    "start": "1369440",
    "end": "1374600"
  },
  {
    "text": "huge Improvement and it's also something that requires very careful coordination between the control plane the load",
    "start": "1374600",
    "end": "1380279"
  },
  {
    "text": "balancer and the um and the Clusters so this is something that uh you know we were able to build because we have a",
    "start": "1380279",
    "end": "1386840"
  },
  {
    "text": "control plane that is managing all of those components in tandem uh okay so the last thing I want",
    "start": "1386840",
    "end": "1393240"
  },
  {
    "text": "to talk about the last major point about race serve on any scale is the full stack optimization work that we've been",
    "start": "1393240",
    "end": "1398919"
  },
  {
    "text": "doing so one optimization that we added this year was um safe replica migration",
    "start": "1398919",
    "end": "1404880"
  },
  {
    "text": "um so basically if we detect that uh your the cluster is in kind of a suboptimal uh configuration we will move",
    "start": "1404880",
    "end": "1412200"
  },
  {
    "text": "replicas from one node to another in order to free up instances to be given back to the cloud provider um so this is",
    "start": "1412200",
    "end": "1419200"
  },
  {
    "text": "a a cost-saving mechanism that's really useful if you have U multiple models that are uh autoscaling up and down in",
    "start": "1419200",
    "end": "1426720"
  },
  {
    "text": "tandem we've also added support for serving on spot instances without dropping requests so if a spot instance",
    "start": "1426720",
    "end": "1432840"
  },
  {
    "text": "gets preempted any scale will um handle that by spinning up a new copy and um",
    "start": "1432840",
    "end": "1438559"
  },
  {
    "text": "Shifting the traffic over and then the really cool thing that we do is we can also when spot instances become",
    "start": "1438559",
    "end": "1444640"
  },
  {
    "text": "available again we'll shift back from on demand back to spot in order to save",
    "start": "1444640",
    "end": "1450480"
  },
  {
    "text": "money and then finally a big paino that we see across um uh not just any scale",
    "start": "1450480",
    "end": "1456200"
  },
  {
    "text": "customers but also open source users and people doing model serving in general is that autoscaling can get really slow",
    "start": "1456200",
    "end": "1461679"
  },
  {
    "text": "especially for large models so we've been working a lot on uh improving autoscaling",
    "start": "1461679",
    "end": "1466960"
  },
  {
    "text": "speeds so let's talk about that in a little more detail as o mentioned earlier um models",
    "start": "1466960",
    "end": "1474000"
  },
  {
    "text": "are have just gotten absolutely huge um and this leads to uh some pretty serious",
    "start": "1474000",
    "end": "1481120"
  },
  {
    "text": "problems so I think a lot of people have probably seen this classic XKCD comic uh where you know people are slacking off",
    "start": "1481120",
    "end": "1486840"
  },
  {
    "text": "waiting for code to compile but I think we need to update it for the uh you know the modern AI driven world and we're",
    "start": "1486840",
    "end": "1493520"
  },
  {
    "text": "still doing the same thing but now our excuse is that models are loading instead of uh code compiling",
    "start": "1493520",
    "end": "1499679"
  },
  {
    "text": "so this really does lead to some serious problems so in development I'm sure basically everyone in this room has",
    "start": "1499679",
    "end": "1505840"
  },
  {
    "text": "experienced the productivity drain of just sitting there waiting for your kubernetes cluster to Auto scale and",
    "start": "1505840",
    "end": "1511480"
  },
  {
    "text": "then the container to pull and then your you know giant python code to start up and then massive model weights to load",
    "start": "1511480",
    "end": "1518279"
  },
  {
    "text": "it's uh you're just like sitting around and waiting and not actually testing or developing or or um uh or doing your",
    "start": "1518279",
    "end": "1525840"
  },
  {
    "text": "job and in production this slow scaling often leads to over-provisioning if you",
    "start": "1525840",
    "end": "1531360"
  },
  {
    "text": "can't autoscale quickly in response to increased traffic then what you end up having to do is basically have more",
    "start": "1531360",
    "end": "1537559"
  },
  {
    "text": "Headroom and have more idle resources which means that you're just wasting money on very expensive",
    "start": "1537559",
    "end": "1544520"
  },
  {
    "text": "gpus uh so we ran a benchmark to uh kind of see what a typical setup of VM",
    "start": "1544640",
    "end": "1550640"
  },
  {
    "text": "running on top of uh kubernetes would look like um oh and my slides are out of",
    "start": "1550640",
    "end": "1557159"
  },
  {
    "text": "order okay I'm going to just go backwards so we we ran a benchmark here um to see how long it would take for VM",
    "start": "1557159",
    "end": "1564440"
  },
  {
    "text": "running on kubernetes to um spin up a new node start a new pod pull the",
    "start": "1564440",
    "end": "1569840"
  },
  {
    "text": "container and load the model weights so here I have it uh on the plot",
    "start": "1569840",
    "end": "1574919"
  },
  {
    "text": "on the right for a 7db model it takes almost 10 minutes end to end and about half of the time is spent starting up a",
    "start": "1574919",
    "end": "1581720"
  },
  {
    "text": "node and pulling a container image and the other half of the time is spent waiting for VM to start up and to",
    "start": "1581720",
    "end": "1587520"
  },
  {
    "text": "download the model and then to load the model into um GPU memory and this is for",
    "start": "1587520",
    "end": "1592679"
  },
  {
    "text": "fp16 so the model weights are about 140 gigabytes",
    "start": "1592679",
    "end": "1598240"
  },
  {
    "text": "total um so any scale has a couple of key optimizations here uh the first is",
    "start": "1598240",
    "end": "1604840"
  },
  {
    "text": "we have a custom container image format which makes it uh possible to pull large",
    "start": "1604840",
    "end": "1610120"
  },
  {
    "text": "images much much faster than using just like a default Docker container de client um so what this looks like is is",
    "start": "1610120",
    "end": "1619080"
  },
  {
    "text": "when you start a cluster on any scale you can provide any sort of generic container image uh so this could be",
    "start": "1619080",
    "end": "1624320"
  },
  {
    "text": "hosted in ECR or like any container registry um it's basically just array image with custom dependencies very",
    "start": "1624320",
    "end": "1630000"
  },
  {
    "text": "similar to what you would use with uh Cube Ray and the first time that you use this",
    "start": "1630000",
    "end": "1635360"
  },
  {
    "text": "image we will pull the image from the container registry and then automatically optimize it into uh like a",
    "start": "1635360",
    "end": "1642080"
  },
  {
    "text": "custom image format and then cache it in a per region bucket so anytime you use",
    "start": "1642080",
    "end": "1647360"
  },
  {
    "text": "the same image app after that the startup speed will be dramatically faster so we ran an even simpler",
    "start": "1647360",
    "end": "1654120"
  },
  {
    "text": "Benchmark just measuring the time that it takes to pull two images one's 6 gbt it's just like a ray base image and one",
    "start": "1654120",
    "end": "1660720"
  },
  {
    "text": "is 13 gbt and that one is basically Ray plus VM and its dependencies and we compared uh eks",
    "start": "1660720",
    "end": "1667519"
  },
  {
    "text": "versus any scale and here you can see that any scale is dramatically faster for pulling the image and starting the",
    "start": "1667519",
    "end": "1673120"
  },
  {
    "text": "container um so eks and this is after quite a bit of optimization effort takes",
    "start": "1673120",
    "end": "1678399"
  },
  {
    "text": "almost or over 100 seconds to pull the 6 GB image and for the 13 gig image it's",
    "start": "1678399",
    "end": "1683960"
  },
  {
    "text": "like 230 seconds whereas any scale is much much faster only 3 and a half and",
    "start": "1683960",
    "end": "1689480"
  },
  {
    "text": "16 seconds respectively um the other optimization we have is uh",
    "start": "1689480",
    "end": "1697519"
  },
  {
    "text": "optimize model loading to help with the uh the very large model",
    "start": "1697519",
    "end": "1702640"
  },
  {
    "text": "weights so the common solution that we see many of our customers do is something like this so they have their",
    "start": "1702640",
    "end": "1708200"
  },
  {
    "text": "model weights stored in AWS S3 or another equivalent remote storage um and",
    "start": "1708200",
    "end": "1713360"
  },
  {
    "text": "they'll first download from Storage to disk and then load the disc uh into the GPU but under the hood this has some",
    "start": "1713360",
    "end": "1720480"
  },
  {
    "text": "problems um because you basically have like three synchronous steps you're first downloading from S3 to local disk",
    "start": "1720480",
    "end": "1727600"
  },
  {
    "text": "then you're loading from local dis to CPU then you're loading from CPU to GPU and when model weights are really big",
    "start": "1727600",
    "end": "1733120"
  },
  {
    "text": "each one of these steps can be very slow so any scale has a a safe tensor",
    "start": "1733120",
    "end": "1738640"
  },
  {
    "text": "compatible client that you can directly point at model weights that are in remote storage and load them uh directly",
    "start": "1738640",
    "end": "1744799"
  },
  {
    "text": "to a GPU under the hood this basically takes advantage of pipeline parallelism and it",
    "start": "1744799",
    "end": "1751519"
  },
  {
    "text": "will fetch the tensors chunk by chunk and stream them directly onto the GPU and avoid these synchronous copy",
    "start": "1751519",
    "end": "1758640"
  },
  {
    "text": "steps this results in a much faster model download times so in an experiment where we're",
    "start": "1758640",
    "end": "1765600"
  },
  {
    "text": "just testing the VM startup and the model loading ass ECT for a 7B and a 70b model um you can see that for the 7 7B",
    "start": "1765600",
    "end": "1773399"
  },
  {
    "text": "model on the left the any scale client is about twice as fast as uh using the the Baseline and for a 70b model it's",
    "start": "1773399",
    "end": "1781200"
  },
  {
    "text": "almost F uh yeah it's about five times as fast uh so if you put these things",
    "start": "1781200",
    "end": "1788480"
  },
  {
    "text": "together end to end any scale is able to scale up a new replica of a 7db model for serving over five times faster than",
    "start": "1788480",
    "end": "1796240"
  },
  {
    "text": "running basically the same set set up on",
    "start": "1796240",
    "end": "1800640"
  },
  {
    "text": "kubernetes um yeah so with that I'll wrap up uh and just wanted to highlight once again that um you know any scale",
    "start": "1801480",
    "end": "1807039"
  },
  {
    "text": "offers a lot of great extensions to really make racer part of an endtoend AI platform including observability improve",
    "start": "1807039",
    "end": "1813760"
  },
  {
    "text": "production Readiness and some really cool optimizations and with that I think uh",
    "start": "1813760",
    "end": "1819679"
  },
  {
    "text": "we're pretty much at time um but o and I will be around to answer any questions that you might have",
    "start": "1819679",
    "end": "1826470"
  },
  {
    "text": "[Applause]",
    "start": "1826470",
    "end": "1830690"
  }
]