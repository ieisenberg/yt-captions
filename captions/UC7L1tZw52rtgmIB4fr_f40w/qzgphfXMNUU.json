[
  {
    "text": "all right awesome so I'd like to get",
    "start": "3540",
    "end": "6060"
  },
  {
    "text": "this started by asking you all a",
    "start": "6060",
    "end": "7440"
  },
  {
    "text": "question how many of you all do offline",
    "start": "7440",
    "end": "9540"
  },
  {
    "text": "batch processing of some sort",
    "start": "9540",
    "end": "12420"
  },
  {
    "text": "okay awesome well I'm glad you all are",
    "start": "12420",
    "end": "14639"
  },
  {
    "text": "here because today we're talking about",
    "start": "14639",
    "end": "16020"
  },
  {
    "text": "how you can do faster and cheaper batch",
    "start": "16020",
    "end": "18420"
  },
  {
    "text": "inference on Ray",
    "start": "18420",
    "end": "20279"
  },
  {
    "text": "so to get this started we're going to be",
    "start": "20279",
    "end": "22320"
  },
  {
    "text": "talking about what is batch inference",
    "start": "22320",
    "end": "23939"
  },
  {
    "text": "and why you should care",
    "start": "23939",
    "end": "25680"
  },
  {
    "text": "next we'll talk about some of the",
    "start": "25680",
    "end": "27300"
  },
  {
    "text": "challenges you might experience and what",
    "start": "27300",
    "end": "29160"
  },
  {
    "text": "current Solutions look like in the space",
    "start": "29160",
    "end": "30599"
  },
  {
    "text": "and in particular we'll look at three",
    "start": "30599",
    "end": "32640"
  },
  {
    "text": "solutions sagemaker batch transform",
    "start": "32640",
    "end": "34800"
  },
  {
    "text": "Apache spark and Ray data",
    "start": "34800",
    "end": "37800"
  },
  {
    "text": "finally we'll talk about some features",
    "start": "37800",
    "end": "39660"
  },
  {
    "text": "that are only available on any scale as",
    "start": "39660",
    "end": "42420"
  },
  {
    "text": "well as some success stories",
    "start": "42420",
    "end": "45360"
  },
  {
    "text": "cool so the first question is what",
    "start": "45360",
    "end": "47219"
  },
  {
    "text": "exactly is offline batch inference right",
    "start": "47219",
    "end": "48960"
  },
  {
    "text": "so I think most of you guys like well",
    "start": "48960",
    "end": "50460"
  },
  {
    "text": "most of you raise your hand so you",
    "start": "50460",
    "end": "51719"
  },
  {
    "text": "probably already know but just to rehash",
    "start": "51719",
    "end": "53520"
  },
  {
    "text": "uh it's ultimately a very simple",
    "start": "53520",
    "end": "55320"
  },
  {
    "text": "workload right you have a large data set",
    "start": "55320",
    "end": "57420"
  },
  {
    "text": "consisting of terabytes or possibly",
    "start": "57420",
    "end": "59039"
  },
  {
    "text": "petabytes of data you have a pre-trained",
    "start": "59039",
    "end": "61079"
  },
  {
    "text": "model and what you want to do is take",
    "start": "61079",
    "end": "62879"
  },
  {
    "text": "each data point you have pass it through",
    "start": "62879",
    "end": "64559"
  },
  {
    "text": "the pre-trained model to get a",
    "start": "64559",
    "end": "65760"
  },
  {
    "text": "prediction and optional optionally you",
    "start": "65760",
    "end": "67740"
  },
  {
    "text": "may have some pre-processing that you do",
    "start": "67740",
    "end": "69659"
  },
  {
    "text": "before feeding in the data to the model",
    "start": "69659",
    "end": "71220"
  },
  {
    "text": "so overall it seems like a very simple",
    "start": "71220",
    "end": "73260"
  },
  {
    "text": "workload but there's a few challenges",
    "start": "73260",
    "end": "75240"
  },
  {
    "text": "really that particularly that we've seen",
    "start": "75240",
    "end": "76979"
  },
  {
    "text": "uh when you actually want to scale this",
    "start": "76979",
    "end": "78659"
  },
  {
    "text": "out so scale this out to you know",
    "start": "78659",
    "end": "80340"
  },
  {
    "text": "hundreds of gpus and really a terabyte",
    "start": "80340",
    "end": "82439"
  },
  {
    "text": "scale data",
    "start": "82439",
    "end": "84060"
  },
  {
    "text": "um so let's go through some of these",
    "start": "84060",
    "end": "85080"
  },
  {
    "text": "challenges so the first one really is",
    "start": "85080",
    "end": "86820"
  },
  {
    "text": "how you can manage your compute",
    "start": "86820",
    "end": "88200"
  },
  {
    "text": "infrastructure so when you're dealing",
    "start": "88200",
    "end": "89759"
  },
  {
    "text": "with lots of data obviously you need a",
    "start": "89759",
    "end": "91320"
  },
  {
    "text": "lot of compute in order to efficiently",
    "start": "91320",
    "end": "92820"
  },
  {
    "text": "process and do inference and all that",
    "start": "92820",
    "end": "94439"
  },
  {
    "text": "data so you need some system to be able",
    "start": "94439",
    "end": "96720"
  },
  {
    "text": "to create clusters and create like GPU",
    "start": "96720",
    "end": "99299"
  },
  {
    "text": "clusters for you",
    "start": "99299",
    "end": "100680"
  },
  {
    "text": "and in particular you may need to manage",
    "start": "100680",
    "end": "102479"
  },
  {
    "text": "heterogeneous compute infrastructure and",
    "start": "102479",
    "end": "104640"
  },
  {
    "text": "what this means is that if you have a",
    "start": "104640",
    "end": "106259"
  },
  {
    "text": "cluster consisting of both GPU nodes and",
    "start": "106259",
    "end": "108960"
  },
  {
    "text": "CPU nodes so we'll go into a little bit",
    "start": "108960",
    "end": "111240"
  },
  {
    "text": "more detail later about why this type of",
    "start": "111240",
    "end": "113040"
  },
  {
    "text": "heterogeneous heterogeneous clusters can",
    "start": "113040",
    "end": "115439"
  },
  {
    "text": "be useful for performance but uh really",
    "start": "115439",
    "end": "118439"
  },
  {
    "text": "if you're if your workload if your batch",
    "start": "118439",
    "end": "120060"
  },
  {
    "text": "inference workload is bottlenecked on",
    "start": "120060",
    "end": "121979"
  },
  {
    "text": "expensive CPU pre-processing or",
    "start": "121979",
    "end": "123899"
  },
  {
    "text": "bottleneck non-expensive like or",
    "start": "123899",
    "end": "125759"
  },
  {
    "text": "bottlenecked on CPU or Ram you may want",
    "start": "125759",
    "end": "127860"
  },
  {
    "text": "to add additional CPU only nodes which",
    "start": "127860",
    "end": "129720"
  },
  {
    "text": "are cheaper than GPU nodes to your",
    "start": "129720",
    "end": "131520"
  },
  {
    "text": "cluster so have a mix of both GPU plus",
    "start": "131520",
    "end": "133980"
  },
  {
    "text": "CPU nodes",
    "start": "133980",
    "end": "135480"
  },
  {
    "text": "the Second Challenge is how you can",
    "start": "135480",
    "end": "136980"
  },
  {
    "text": "actually utilize all the resources in",
    "start": "136980",
    "end": "138540"
  },
  {
    "text": "this cluster so once you have a system",
    "start": "138540",
    "end": "140400"
  },
  {
    "text": "to create your clusters for you how can",
    "start": "140400",
    "end": "142800"
  },
  {
    "text": "you actually take your job and have it",
    "start": "142800",
    "end": "144660"
  },
  {
    "text": "use all the resources so making sure",
    "start": "144660",
    "end": "146520"
  },
  {
    "text": "your gpus are fully saturated making",
    "start": "146520",
    "end": "148620"
  },
  {
    "text": "sure your CPUs are utilized the reason",
    "start": "148620",
    "end": "151080"
  },
  {
    "text": "this is important obviously is because",
    "start": "151080",
    "end": "152459"
  },
  {
    "text": "you want to make sure that when you're",
    "start": "152459",
    "end": "153959"
  },
  {
    "text": "paying for these uh this compute you're",
    "start": "153959",
    "end": "156360"
  },
  {
    "text": "you're using them right you don't want",
    "start": "156360",
    "end": "157560"
  },
  {
    "text": "CPUs or gpus sitting idly and not being",
    "start": "157560",
    "end": "159599"
  },
  {
    "text": "used because you're paying for them so",
    "start": "159599",
    "end": "161640"
  },
  {
    "text": "for cost-effective batch processing or",
    "start": "161640",
    "end": "163620"
  },
  {
    "text": "batch inference you need to make sure",
    "start": "163620",
    "end": "164640"
  },
  {
    "text": "you're utilizing the resources at any",
    "start": "164640",
    "end": "166319"
  },
  {
    "text": "given point in time",
    "start": "166319",
    "end": "168180"
  },
  {
    "text": "third you want efficient data transfer",
    "start": "168180",
    "end": "170220"
  },
  {
    "text": "from your storage where you're saving",
    "start": "170220",
    "end": "172260"
  },
  {
    "text": "your data like S3 to CPU Ram or you want",
    "start": "172260",
    "end": "175140"
  },
  {
    "text": "to do possibly do CPU pre-processing and",
    "start": "175140",
    "end": "177360"
  },
  {
    "text": "then do GPU Ram where you actually do",
    "start": "177360",
    "end": "179280"
  },
  {
    "text": "model inference so again having",
    "start": "179280",
    "end": "180840"
  },
  {
    "text": "efficient data transfer is really",
    "start": "180840",
    "end": "182280"
  },
  {
    "text": "important to maximize performance and",
    "start": "182280",
    "end": "183720"
  },
  {
    "text": "making sure that you're fully saturating",
    "start": "183720",
    "end": "185519"
  },
  {
    "text": "your gpus",
    "start": "185519",
    "end": "187379"
  },
  {
    "text": "and the last challenge that we see is a",
    "start": "187379",
    "end": "189540"
  },
  {
    "text": "really undeveloper experience so you",
    "start": "189540",
    "end": "191459"
  },
  {
    "text": "don't go right when you're developing",
    "start": "191459",
    "end": "192659"
  },
  {
    "text": "these types of processing pipelines you",
    "start": "192659",
    "end": "194459"
  },
  {
    "text": "don't go directly to production you",
    "start": "194459",
    "end": "196560"
  },
  {
    "text": "probably want to start off with working",
    "start": "196560",
    "end": "198300"
  },
  {
    "text": "on a smaller data set making sure your",
    "start": "198300",
    "end": "199680"
  },
  {
    "text": "pipeline is working well so you need a",
    "start": "199680",
    "end": "202080"
  },
  {
    "text": "good developer experience so you can",
    "start": "202080",
    "end": "203159"
  },
  {
    "text": "iterate quickly to join a smaller data",
    "start": "203159",
    "end": "205200"
  },
  {
    "text": "set during development and then have low",
    "start": "205200",
    "end": "206640"
  },
  {
    "text": "friction when you move that code when",
    "start": "206640",
    "end": "208860"
  },
  {
    "text": "you built during development over into",
    "start": "208860",
    "end": "210120"
  },
  {
    "text": "production so these are the four",
    "start": "210120",
    "end": "211500"
  },
  {
    "text": "challenges that we really use we see for",
    "start": "211500",
    "end": "213360"
  },
  {
    "text": "what what maybe can considered a simple",
    "start": "213360",
    "end": "215819"
  },
  {
    "text": "workload like batch processing or batch",
    "start": "215819",
    "end": "217379"
  },
  {
    "text": "inference but becomes much more complex",
    "start": "217379",
    "end": "220019"
  },
  {
    "text": "when you scale it out so these are the",
    "start": "220019",
    "end": "221819"
  },
  {
    "text": "four challenges that we see",
    "start": "221819",
    "end": "223920"
  },
  {
    "text": "so now let's go into like what really",
    "start": "223920",
    "end": "225720"
  },
  {
    "text": "does the industry recommend right so",
    "start": "225720",
    "end": "227640"
  },
  {
    "text": "what is the best solution that people",
    "start": "227640",
    "end": "229560"
  },
  {
    "text": "are recommending for addressing these",
    "start": "229560",
    "end": "231720"
  },
  {
    "text": "four challenges I think the best uh well",
    "start": "231720",
    "end": "234299"
  },
  {
    "text": "like this is one tweet that I saw about",
    "start": "234299",
    "end": "236519"
  },
  {
    "text": "uh so this is Ben Hammer who's one of",
    "start": "236519",
    "end": "238260"
  },
  {
    "text": "the who's one of the creators for kaggle",
    "start": "238260",
    "end": "240180"
  },
  {
    "text": "um so you just pose this question on",
    "start": "240180",
    "end": "241319"
  },
  {
    "text": "Twitter saying you know I want to spin",
    "start": "241319",
    "end": "243180"
  },
  {
    "text": "up a bunch of GPU spot instances to run",
    "start": "243180",
    "end": "245400"
  },
  {
    "text": "batch inference jobs what is everyone's",
    "start": "245400",
    "end": "247440"
  },
  {
    "text": "favorite approach and we can take a look",
    "start": "247440",
    "end": "249180"
  },
  {
    "text": "at some of the responses to really see",
    "start": "249180",
    "end": "250439"
  },
  {
    "text": "like how the industry is thinking about",
    "start": "250439",
    "end": "251700"
  },
  {
    "text": "this right so the first response is just",
    "start": "251700",
    "end": "253739"
  },
  {
    "text": "like very definitively is like just just",
    "start": "253739",
    "end": "255720"
  },
  {
    "text": "use AWS batch",
    "start": "255720",
    "end": "257880"
  },
  {
    "text": "um you know you can use Moto Labs as one",
    "start": "257880",
    "end": "259680"
  },
  {
    "text": "option lightning AI like pie torch",
    "start": "259680",
    "end": "262139"
  },
  {
    "text": "lightning also saying they'll do like",
    "start": "262139",
    "end": "263639"
  },
  {
    "text": "offline batch processing you can also",
    "start": "263639",
    "end": "266520"
  },
  {
    "text": "roll out your own with your Cloud vendor",
    "start": "266520",
    "end": "268259"
  },
  {
    "text": "of choice someone's using kubeflow",
    "start": "268259",
    "end": "271259"
  },
  {
    "text": "someone's using Azure batch compute this",
    "start": "271259",
    "end": "273840"
  },
  {
    "text": "person is using like a race serve for",
    "start": "273840",
    "end": "275460"
  },
  {
    "text": "offline batch inference but overall it's",
    "start": "275460",
    "end": "277800"
  },
  {
    "text": "very uh disjoint set of uh solutions",
    "start": "277800",
    "end": "280380"
  },
  {
    "text": "that people are looking into right",
    "start": "280380",
    "end": "281580"
  },
  {
    "text": "varying from using like Cloud vendor",
    "start": "281580",
    "end": "283740"
  },
  {
    "text": "solutions to even just like doing it",
    "start": "283740",
    "end": "285540"
  },
  {
    "text": "yourself",
    "start": "285540",
    "end": "286680"
  },
  {
    "text": "um so what we want to do is really",
    "start": "286680",
    "end": "288479"
  },
  {
    "text": "categorize the different solutions that",
    "start": "288479",
    "end": "290280"
  },
  {
    "text": "people are looking into or people have",
    "start": "290280",
    "end": "291960"
  },
  {
    "text": "tried out for offline batch inference",
    "start": "291960",
    "end": "293880"
  },
  {
    "text": "and we've determined like three",
    "start": "293880",
    "end": "295139"
  },
  {
    "text": "different categories that the solutions",
    "start": "295139",
    "end": "296759"
  },
  {
    "text": "fall into",
    "start": "296759",
    "end": "298800"
  },
  {
    "text": "so the first approach uh for first",
    "start": "298800",
    "end": "301020"
  },
  {
    "text": "category of solutions is what we call",
    "start": "301020",
    "end": "302580"
  },
  {
    "text": "like batch services so something like",
    "start": "302580",
    "end": "304139"
  },
  {
    "text": "AWS batch or gcp or Azure batch",
    "start": "304139",
    "end": "307680"
  },
  {
    "text": "so how these work is that you can go to",
    "start": "307680",
    "end": "310080"
  },
  {
    "text": "your cloud provider of choice create a",
    "start": "310080",
    "end": "312540"
  },
  {
    "text": "cluster right so you can request on what",
    "start": "312540",
    "end": "314460"
  },
  {
    "text": "this many instances and then your the",
    "start": "314460",
    "end": "317340"
  },
  {
    "text": "solution will like create that cluster",
    "start": "317340",
    "end": "318960"
  },
  {
    "text": "for you and then you can deploy your",
    "start": "318960",
    "end": "320580"
  },
  {
    "text": "code or your software to run on these",
    "start": "320580",
    "end": "322500"
  },
  {
    "text": "instances so this partially handles",
    "start": "322500",
    "end": "324479"
  },
  {
    "text": "challenge number one of being able to",
    "start": "324479",
    "end": "326460"
  },
  {
    "text": "have like a managed infrastructure so",
    "start": "326460",
    "end": "327840"
  },
  {
    "text": "easily creating clusters",
    "start": "327840",
    "end": "329660"
  },
  {
    "text": "some of these don't don't handle",
    "start": "329660",
    "end": "332400"
  },
  {
    "text": "heterogeneous clusters but but it does",
    "start": "332400",
    "end": "335220"
  },
  {
    "text": "partially handle challenge number one",
    "start": "335220",
    "end": "336660"
  },
  {
    "text": "but really these Solutions don't handle",
    "start": "336660",
    "end": "338400"
  },
  {
    "text": "the remainder of the challenges right so",
    "start": "338400",
    "end": "339960"
  },
  {
    "text": "developer experience is pretty uh not",
    "start": "339960",
    "end": "342960"
  },
  {
    "text": "not great when you're dealing directly",
    "start": "342960",
    "end": "344639"
  },
  {
    "text": "with like ec2 instances and in",
    "start": "344639",
    "end": "346620"
  },
  {
    "text": "particular the stuff about having",
    "start": "346620",
    "end": "348120"
  },
  {
    "text": "efficient data transfer and utilizing",
    "start": "348120",
    "end": "350460"
  },
  {
    "text": "your resources that's all on you as the",
    "start": "350460",
    "end": "352500"
  },
  {
    "text": "user to make sure that the software you",
    "start": "352500",
    "end": "354360"
  },
  {
    "text": "are writing is properly processing your",
    "start": "354360",
    "end": "357060"
  },
  {
    "text": "data in an efficient way and is actually",
    "start": "357060",
    "end": "358860"
  },
  {
    "text": "utilizing the resources on these ec2",
    "start": "358860",
    "end": "361259"
  },
  {
    "text": "instances for example",
    "start": "361259",
    "end": "362699"
  },
  {
    "text": "so these Solutions really handle the",
    "start": "362699",
    "end": "364680"
  },
  {
    "text": "managed infrastructure part but they",
    "start": "364680",
    "end": "365940"
  },
  {
    "text": "don't handle anything about the software",
    "start": "365940",
    "end": "367320"
  },
  {
    "text": "aspect or how you can actually use or",
    "start": "367320",
    "end": "369780"
  },
  {
    "text": "build a efficient data processing",
    "start": "369780",
    "end": "371639"
  },
  {
    "text": "pipeline",
    "start": "371639",
    "end": "373139"
  },
  {
    "text": "something like Moto Labs is another",
    "start": "373139",
    "end": "374759"
  },
  {
    "text": "option",
    "start": "374759",
    "end": "375780"
  },
  {
    "text": "um so they have a much nicer developer",
    "start": "375780",
    "end": "377820"
  },
  {
    "text": "experience but again they don't have a",
    "start": "377820",
    "end": "379919"
  },
  {
    "text": "software written necessarily for",
    "start": "379919",
    "end": "381600"
  },
  {
    "text": "actually processing large amounts of",
    "start": "381600",
    "end": "383340"
  },
  {
    "text": "data and making sure you're fully",
    "start": "383340",
    "end": "384720"
  },
  {
    "text": "utilizing all the resources in your",
    "start": "384720",
    "end": "386039"
  },
  {
    "text": "cluster",
    "start": "386039",
    "end": "387000"
  },
  {
    "text": "so this is the first category of",
    "start": "387000",
    "end": "388500"
  },
  {
    "text": "solutions that we see so bad services",
    "start": "388500",
    "end": "390840"
  },
  {
    "text": "the other category of solutions that",
    "start": "390840",
    "end": "392400"
  },
  {
    "text": "that people they also use for offline",
    "start": "392400",
    "end": "394740"
  },
  {
    "text": "inference is actually just shoehorning",
    "start": "394740",
    "end": "396000"
  },
  {
    "text": "an online inference solution for the",
    "start": "396000",
    "end": "398039"
  },
  {
    "text": "offline case so using something like",
    "start": "398039",
    "end": "399900"
  },
  {
    "text": "Bento LL or array serve or sagemaker",
    "start": "399900",
    "end": "402600"
  },
  {
    "text": "batch transform so these Solutions you",
    "start": "402600",
    "end": "405060"
  },
  {
    "text": "know they're going to abstract away",
    "start": "405060",
    "end": "406139"
  },
  {
    "text": "infrastructure complexities they most of",
    "start": "406139",
    "end": "410220"
  },
  {
    "text": "them are pretty well integrated with the",
    "start": "410220",
    "end": "411539"
  },
  {
    "text": "rest of the ml ecosystem and they have",
    "start": "411539",
    "end": "413639"
  },
  {
    "text": "good abstractions for model Packaging",
    "start": "413639",
    "end": "415620"
  },
  {
    "text": "but the problem where you try to force",
    "start": "415620",
    "end": "417300"
  },
  {
    "text": "in like an online inference solution to",
    "start": "417300",
    "end": "419340"
  },
  {
    "text": "the offline case is that you can add",
    "start": "419340",
    "end": "421139"
  },
  {
    "text": "like additional complexities additional",
    "start": "421139",
    "end": "423060"
  },
  {
    "text": "overhead for the offline case",
    "start": "423060",
    "end": "425520"
  },
  {
    "text": "so usually this includes things like",
    "start": "425520",
    "end": "427319"
  },
  {
    "text": "starting out of starting up an HTTP",
    "start": "427319",
    "end": "429060"
  },
  {
    "text": "server and sending requests over a",
    "start": "429060",
    "end": "430680"
  },
  {
    "text": "network",
    "start": "430680",
    "end": "431819"
  },
  {
    "text": "um sometimes something like sagemaker",
    "start": "431819",
    "end": "433380"
  },
  {
    "text": "batch transform has a max payload you",
    "start": "433380",
    "end": "435240"
  },
  {
    "text": "can send for each request so like these",
    "start": "435240",
    "end": "437340"
  },
  {
    "text": "additional complexities really make it",
    "start": "437340",
    "end": "438780"
  },
  {
    "text": "hard to saturate gpus for what doesn't",
    "start": "438780",
    "end": "441180"
  },
  {
    "text": "need you know doesn't really need an",
    "start": "441180",
    "end": "443160"
  },
  {
    "text": "HTTP server right you're not building an",
    "start": "443160",
    "end": "444599"
  },
  {
    "text": "online application so for something like",
    "start": "444599",
    "end": "446580"
  },
  {
    "text": "offline batch inference or offline",
    "start": "446580",
    "end": "448380"
  },
  {
    "text": "processing these online Solutions don't",
    "start": "448380",
    "end": "450900"
  },
  {
    "text": "necessarily work well to the point where",
    "start": "450900",
    "end": "452520"
  },
  {
    "text": "even the mental ml integrates with spark",
    "start": "452520",
    "end": "454500"
  },
  {
    "text": "as a back end for offline inference",
    "start": "454500",
    "end": "456840"
  },
  {
    "text": "rather than using their existing back",
    "start": "456840",
    "end": "458280"
  },
  {
    "text": "end for for the offline inference case",
    "start": "458280",
    "end": "461699"
  },
  {
    "text": "and the third category solutions that we",
    "start": "461699",
    "end": "463440"
  },
  {
    "text": "see is distributed Data Systems so these",
    "start": "463440",
    "end": "465780"
  },
  {
    "text": "are things like spark and Ray data and",
    "start": "465780",
    "end": "468240"
  },
  {
    "text": "these Solutions are the ones that are",
    "start": "468240",
    "end": "469560"
  },
  {
    "text": "actually well designed and well built to",
    "start": "469560",
    "end": "471419"
  },
  {
    "text": "be able to process large data sets so",
    "start": "471419",
    "end": "473460"
  },
  {
    "text": "something like offline batch inference",
    "start": "473460",
    "end": "474780"
  },
  {
    "text": "really is just a map operation you have",
    "start": "474780",
    "end": "477360"
  },
  {
    "text": "a pre-trained model and you just want to",
    "start": "477360",
    "end": "478680"
  },
  {
    "text": "map that model over a bunch of data so",
    "start": "478680",
    "end": "481139"
  },
  {
    "text": "these are where distributed data",
    "start": "481139",
    "end": "482400"
  },
  {
    "text": "processing systems really shine so they",
    "start": "482400",
    "end": "484680"
  },
  {
    "text": "can easily scale across your entire",
    "start": "484680",
    "end": "486479"
  },
  {
    "text": "cluster to have support for data",
    "start": "486479",
    "end": "488460"
  },
  {
    "text": "partitioning and be able to batch your",
    "start": "488460",
    "end": "490080"
  },
  {
    "text": "data effectively and they have a really",
    "start": "490080",
    "end": "492240"
  },
  {
    "text": "good a built-in i o layer so you can",
    "start": "492240",
    "end": "493800"
  },
  {
    "text": "connect the different data sources and",
    "start": "493800",
    "end": "495060"
  },
  {
    "text": "load in data very quickly with these",
    "start": "495060",
    "end": "497580"
  },
  {
    "text": "Solutions",
    "start": "497580",
    "end": "498660"
  },
  {
    "text": "so these are the three categories of",
    "start": "498660",
    "end": "500340"
  },
  {
    "text": "solutions that we generally see for",
    "start": "500340",
    "end": "501900"
  },
  {
    "text": "offline batch inference using just batch",
    "start": "501900",
    "end": "503819"
  },
  {
    "text": "Services trying to use online solution",
    "start": "503819",
    "end": "506400"
  },
  {
    "text": "for the offline case and then actually",
    "start": "506400",
    "end": "508139"
  },
  {
    "text": "using distributed Data Systems so let's",
    "start": "508139",
    "end": "510240"
  },
  {
    "text": "take a look we're going to do a deep",
    "start": "510240",
    "end": "511440"
  },
  {
    "text": "dive into three candidates out of this",
    "start": "511440",
    "end": "513419"
  },
  {
    "text": "entire solution space",
    "start": "513419",
    "end": "515339"
  },
  {
    "text": "so what we did is we actually ran like",
    "start": "515339",
    "end": "517380"
  },
  {
    "text": "an image classification Benchmark",
    "start": "517380",
    "end": "518820"
  },
  {
    "text": "comparing sagemaker batch transform",
    "start": "518820",
    "end": "520919"
  },
  {
    "text": "Apache spark and Ray data",
    "start": "520919",
    "end": "525060"
  },
  {
    "text": "so the setup here is that we have a",
    "start": "525060",
    "end": "526560"
  },
  {
    "text": "pre-trained resnet 50 model and we want",
    "start": "526560",
    "end": "528420"
  },
  {
    "text": "to do batch inference on imagenet data",
    "start": "528420",
    "end": "531300"
  },
  {
    "text": "so the pipeline is very simple it's just",
    "start": "531300",
    "end": "533760"
  },
  {
    "text": "you want to read images from S3 you want",
    "start": "533760",
    "end": "535740"
  },
  {
    "text": "to do some simple CPU preprocessing such",
    "start": "535740",
    "end": "537899"
  },
  {
    "text": "as resizing cropping and normalizing",
    "start": "537899",
    "end": "540060"
  },
  {
    "text": "those images and finally you want to do",
    "start": "540060",
    "end": "541920"
  },
  {
    "text": "model inference on gpus and you run this",
    "start": "541920",
    "end": "544260"
  },
  {
    "text": "on both 10 gigabytes and 300 gigabytes",
    "start": "544260",
    "end": "547800"
  },
  {
    "text": "so we can take a look at the results",
    "start": "547800",
    "end": "549180"
  },
  {
    "text": "that we we got",
    "start": "549180",
    "end": "551220"
  },
  {
    "text": "um so we tried a few different spark",
    "start": "551220",
    "end": "552420"
  },
  {
    "text": "configurations also sagemaker about",
    "start": "552420",
    "end": "554339"
  },
  {
    "text": "transform and then finally with Ray data",
    "start": "554339",
    "end": "556140"
  },
  {
    "text": "sets so you can see the array data",
    "start": "556140",
    "end": "558120"
  },
  {
    "text": "outperforms sagemaker batch transform",
    "start": "558120",
    "end": "559920"
  },
  {
    "text": "quite considerably and is also around",
    "start": "559920",
    "end": "561959"
  },
  {
    "text": "two times faster or two times more",
    "start": "561959",
    "end": "564000"
  },
  {
    "text": "throughput than what the best spark",
    "start": "564000",
    "end": "566399"
  },
  {
    "text": "configuration",
    "start": "566399",
    "end": "568500"
  },
  {
    "text": "so we can take a look at like diving",
    "start": "568500",
    "end": "570480"
  },
  {
    "text": "deep more deeply into kind of why we see",
    "start": "570480",
    "end": "572880"
  },
  {
    "text": "Ray data having better performance than",
    "start": "572880",
    "end": "574620"
  },
  {
    "text": "some of these other solutions for this",
    "start": "574620",
    "end": "576120"
  },
  {
    "text": "type of workload",
    "start": "576120",
    "end": "578640"
  },
  {
    "text": "so first about sagemaker batch transform",
    "start": "578640",
    "end": "580980"
  },
  {
    "text": "so this is one of the solutions that",
    "start": "580980",
    "end": "582779"
  },
  {
    "text": "falls into category number two which is",
    "start": "582779",
    "end": "584640"
  },
  {
    "text": "using online inference solutions for the",
    "start": "584640",
    "end": "586980"
  },
  {
    "text": "offline case",
    "start": "586980",
    "end": "588120"
  },
  {
    "text": "so again this addresses the challenge",
    "start": "588120",
    "end": "590339"
  },
  {
    "text": "number one partially it abstracts away",
    "start": "590339",
    "end": "592440"
  },
  {
    "text": "the infrastructure management but with",
    "start": "592440",
    "end": "594959"
  },
  {
    "text": "regards to the rest of the challenges it",
    "start": "594959",
    "end": "596700"
  },
  {
    "text": "doesn't really work well sagemaker batch",
    "start": "596700",
    "end": "598860"
  },
  {
    "text": "transform using architecture for online",
    "start": "598860",
    "end": "600600"
  },
  {
    "text": "serving so it starts in HTTP server",
    "start": "600600",
    "end": "603060"
  },
  {
    "text": "deploys a model as an endpoint and each",
    "start": "603060",
    "end": "605760"
  },
  {
    "text": "image is sent as like one request to the",
    "start": "605760",
    "end": "607920"
  },
  {
    "text": "server to get predictions back",
    "start": "607920",
    "end": "609839"
  },
  {
    "text": "there's also no support for batching",
    "start": "609839",
    "end": "611640"
  },
  {
    "text": "across multiple files and the max",
    "start": "611640",
    "end": "613920"
  },
  {
    "text": "payload size for each request is 100",
    "start": "613920",
    "end": "615420"
  },
  {
    "text": "megabytes so a lot of these additional",
    "start": "615420",
    "end": "617339"
  },
  {
    "text": "complexities which may benefit the",
    "start": "617339",
    "end": "619500"
  },
  {
    "text": "online inference case make it very hard",
    "start": "619500",
    "end": "621839"
  },
  {
    "text": "to actually saturate gpus on the offline",
    "start": "621839",
    "end": "623940"
  },
  {
    "text": "case for the offline case if you have a",
    "start": "623940",
    "end": "626100"
  },
  {
    "text": "bunch of data you want to like make sure",
    "start": "626100",
    "end": "627420"
  },
  {
    "text": "your batch sizes are large as possible",
    "start": "627420",
    "end": "628980"
  },
  {
    "text": "you don't want any payload limits when",
    "start": "628980",
    "end": "631260"
  },
  {
    "text": "you're sending requests so that's where",
    "start": "631260",
    "end": "633720"
  },
  {
    "text": "we see that sagemaker batch transform is",
    "start": "633720",
    "end": "635399"
  },
  {
    "text": "not having the best as good performance",
    "start": "635399",
    "end": "637260"
  },
  {
    "text": "compared to like spark array data",
    "start": "637260",
    "end": "639420"
  },
  {
    "text": "add to that that uh sagemaker bash",
    "start": "639420",
    "end": "641580"
  },
  {
    "text": "transform just generally doesn't really",
    "start": "641580",
    "end": "642959"
  },
  {
    "text": "have a good developer ux and more",
    "start": "642959",
    "end": "645180"
  },
  {
    "text": "difficult to debug so",
    "start": "645180",
    "end": "646860"
  },
  {
    "text": "hard to iterate as quickly as you can",
    "start": "646860",
    "end": "648779"
  },
  {
    "text": "with something like spark or Ray data",
    "start": "648779",
    "end": "652519"
  },
  {
    "text": "so now let's take a look comparing Ray",
    "start": "656399",
    "end": "657839"
  },
  {
    "text": "data and Spark",
    "start": "657839",
    "end": "659279"
  },
  {
    "text": "so really the differences here is that",
    "start": "659279",
    "end": "661380"
  },
  {
    "text": "both both can effectively scale out too",
    "start": "661380",
    "end": "664100"
  },
  {
    "text": "many different gpus across your entire",
    "start": "664100",
    "end": "666300"
  },
  {
    "text": "cluster but the difference here is",
    "start": "666300",
    "end": "667920"
  },
  {
    "text": "really how you can actually utilize all",
    "start": "667920",
    "end": "670440"
  },
  {
    "text": "the resources in the cluster effectively",
    "start": "670440",
    "end": "672000"
  },
  {
    "text": "with both Ray data and Spark",
    "start": "672000",
    "end": "675060"
  },
  {
    "text": "and one of the Key Properties about",
    "start": "675060",
    "end": "676560"
  },
  {
    "text": "offline batch inference workloads",
    "start": "676560",
    "end": "678420"
  },
  {
    "text": "especially for when you're working with",
    "start": "678420",
    "end": "680160"
  },
  {
    "text": "deep learning models is that they",
    "start": "680160",
    "end": "681899"
  },
  {
    "text": "consists of both CPU and GPU processing",
    "start": "681899",
    "end": "684660"
  },
  {
    "text": "so you have some pre-processing steps",
    "start": "684660",
    "end": "686880"
  },
  {
    "text": "that you do on CPU and the actual model",
    "start": "686880",
    "end": "688920"
  },
  {
    "text": "inference takes place on GPU so you'll",
    "start": "688920",
    "end": "691079"
  },
  {
    "text": "need to leverage both sets of Hardware",
    "start": "691079",
    "end": "692760"
  },
  {
    "text": "effectively for maximal performance",
    "start": "692760",
    "end": "697100"
  },
  {
    "text": "the key difference in performance here",
    "start": "697320",
    "end": "698820"
  },
  {
    "text": "between spark and rate data is their",
    "start": "698820",
    "end": "700800"
  },
  {
    "text": "underlying execution engine so this is",
    "start": "700800",
    "end": "702839"
  },
  {
    "text": "like this GIF kind of explains the",
    "start": "702839",
    "end": "704880"
  },
  {
    "text": "difference between spark and Ray data so",
    "start": "704880",
    "end": "707160"
  },
  {
    "text": "spark uses underneath basically",
    "start": "707160",
    "end": "708660"
  },
  {
    "text": "underneath how spark works is that it",
    "start": "708660",
    "end": "710040"
  },
  {
    "text": "uses a bulk based execution",
    "start": "710040",
    "end": "711839"
  },
  {
    "text": "uh so both the CPU stage the",
    "start": "711839",
    "end": "714540"
  },
  {
    "text": "pre-processing and the GPU stages which",
    "start": "714540",
    "end": "716579"
  },
  {
    "text": "is the model inference are all fused",
    "start": "716579",
    "end": "718320"
  },
  {
    "text": "together so how it works is that you",
    "start": "718320",
    "end": "720060"
  },
  {
    "text": "take your first two partitions of data",
    "start": "720060",
    "end": "721860"
  },
  {
    "text": "you pre-process them then do inference",
    "start": "721860",
    "end": "724200"
  },
  {
    "text": "then save them and only then do you",
    "start": "724200",
    "end": "726720"
  },
  {
    "text": "start processing the next set of data",
    "start": "726720",
    "end": "729180"
  },
  {
    "text": "Ray data on the other hand has a",
    "start": "729180",
    "end": "731339"
  },
  {
    "text": "underlying streaming based execution",
    "start": "731339",
    "end": "733019"
  },
  {
    "text": "engine so if you came to one of the",
    "start": "733019",
    "end": "735120"
  },
  {
    "text": "talks earlier today about Ray data or",
    "start": "735120",
    "end": "736680"
  },
  {
    "text": "the talk yesterday about Ray data I went",
    "start": "736680",
    "end": "738480"
  },
  {
    "text": "into more detail about the streaming",
    "start": "738480",
    "end": "739860"
  },
  {
    "text": "execution engine but how it works is",
    "start": "739860",
    "end": "741660"
  },
  {
    "text": "that it can effectively pipeline all",
    "start": "741660",
    "end": "743519"
  },
  {
    "text": "your data across all the different",
    "start": "743519",
    "end": "745380"
  },
  {
    "text": "resources resource types in your cluster",
    "start": "745380",
    "end": "747300"
  },
  {
    "text": "so while you're doing model inference on",
    "start": "747300",
    "end": "749399"
  },
  {
    "text": "one basically unlike one set of batches",
    "start": "749399",
    "end": "751560"
  },
  {
    "text": "at the same time you could be doing",
    "start": "751560",
    "end": "753420"
  },
  {
    "text": "pre-processing on the next set of",
    "start": "753420",
    "end": "754800"
  },
  {
    "text": "batches",
    "start": "754800",
    "end": "755760"
  },
  {
    "text": "so you can effectively pipeline all your",
    "start": "755760",
    "end": "757680"
  },
  {
    "text": "data through all the different",
    "start": "757680",
    "end": "758940"
  },
  {
    "text": "operations because these operations are",
    "start": "758940",
    "end": "761040"
  },
  {
    "text": "utilizing different have different",
    "start": "761040",
    "end": "762420"
  },
  {
    "text": "resource requirements and in between",
    "start": "762420",
    "end": "764459"
  },
  {
    "text": "these stages you can buffer the data in",
    "start": "764459",
    "end": "766860"
  },
  {
    "text": "the ray shared memory Object Store",
    "start": "766860",
    "end": "769139"
  },
  {
    "text": "so as you can see the ray data is able",
    "start": "769139",
    "end": "770940"
  },
  {
    "text": "to because it's streaming the data",
    "start": "770940",
    "end": "773040"
  },
  {
    "text": "throughout it can more it can basically",
    "start": "773040",
    "end": "775860"
  },
  {
    "text": "has higher throughput so you can process",
    "start": "775860",
    "end": "778260"
  },
  {
    "text": "and then do inference on the data much",
    "start": "778260",
    "end": "780360"
  },
  {
    "text": "faster than you can with spark the other",
    "start": "780360",
    "end": "782220"
  },
  {
    "text": "Advantage with this approach is that you",
    "start": "782220",
    "end": "783660"
  },
  {
    "text": "can actually independently scale out",
    "start": "783660",
    "end": "785040"
  },
  {
    "text": "each of the stages",
    "start": "785040",
    "end": "787019"
  },
  {
    "text": "um so with spark your your parallelism",
    "start": "787019",
    "end": "789480"
  },
  {
    "text": "is limited by the number of gpus in the",
    "start": "789480",
    "end": "791700"
  },
  {
    "text": "cluster but with the ray data you can",
    "start": "791700",
    "end": "793320"
  },
  {
    "text": "scale out your CPU pre-processing",
    "start": "793320",
    "end": "795060"
  },
  {
    "text": "independently from the GPU inference so",
    "start": "795060",
    "end": "797700"
  },
  {
    "text": "making sure you're actually fully",
    "start": "797700",
    "end": "798899"
  },
  {
    "text": "utilizing all the resources in your",
    "start": "798899",
    "end": "800339"
  },
  {
    "text": "cluster effectively",
    "start": "800339",
    "end": "803120"
  },
  {
    "text": "one of the other advantages that we have",
    "start": "803160",
    "end": "804779"
  },
  {
    "text": "with them",
    "start": "804779",
    "end": "806040"
  },
  {
    "text": "with the ray data is that you could",
    "start": "806040",
    "end": "807360"
  },
  {
    "text": "actually work with heterogeneous",
    "start": "807360",
    "end": "808260"
  },
  {
    "text": "clusters so this is very important for",
    "start": "808260",
    "end": "811440"
  },
  {
    "text": "cases where your batch processing is",
    "start": "811440",
    "end": "813800"
  },
  {
    "text": "bottlenecked on memory so when you're",
    "start": "813800",
    "end": "815940"
  },
  {
    "text": "working with things like images or",
    "start": "815940",
    "end": "817320"
  },
  {
    "text": "videos or audios or a lot of the new",
    "start": "817320",
    "end": "819300"
  },
  {
    "text": "type of unstructured data that we're",
    "start": "819300",
    "end": "820740"
  },
  {
    "text": "seeing with the viewer AI workloads",
    "start": "820740",
    "end": "822480"
  },
  {
    "text": "these are bottlenecked on memory so",
    "start": "822480",
    "end": "825060"
  },
  {
    "text": "rather than adding additional GPU nodes",
    "start": "825060",
    "end": "827700"
  },
  {
    "text": "which are more expensive if your",
    "start": "827700",
    "end": "829260"
  },
  {
    "text": "bottleneck is on CPU member you can",
    "start": "829260",
    "end": "830820"
  },
  {
    "text": "actually add CPU only nodes to your",
    "start": "830820",
    "end": "833160"
  },
  {
    "text": "array cluster and have a cluster",
    "start": "833160",
    "end": "834839"
  },
  {
    "text": "consisting of both GPU and CPU nodes",
    "start": "834839",
    "end": "837839"
  },
  {
    "text": "so you can do pre-processing across both",
    "start": "837839",
    "end": "840560"
  },
  {
    "text": "across the CPUs of your entire cluster",
    "start": "840560",
    "end": "843660"
  },
  {
    "text": "and then the intermediate data is",
    "start": "843660",
    "end": "845519"
  },
  {
    "text": "buffered in the shared memory Object",
    "start": "845519",
    "end": "847320"
  },
  {
    "text": "Store",
    "start": "847320",
    "end": "848700"
  },
  {
    "text": "and then when it's time for inference",
    "start": "848700",
    "end": "850200"
  },
  {
    "text": "some of the nodes don't have gpus some",
    "start": "850200",
    "end": "852120"
  },
  {
    "text": "of them are only CPUs but Ray can handle",
    "start": "852120",
    "end": "854220"
  },
  {
    "text": "that effectively and will move all the",
    "start": "854220",
    "end": "856500"
  },
  {
    "text": "data from all the pre-process data to",
    "start": "856500",
    "end": "859079"
  },
  {
    "text": "the appropriate nodes in order to do",
    "start": "859079",
    "end": "860459"
  },
  {
    "text": "inference",
    "start": "860459",
    "end": "861660"
  },
  {
    "text": "so this way we can effectively uh",
    "start": "861660",
    "end": "863760"
  },
  {
    "text": "overcome our memory bottlenecks without",
    "start": "863760",
    "end": "866399"
  },
  {
    "text": "needing to spend more money to get GPU",
    "start": "866399",
    "end": "869160"
  },
  {
    "text": "nodes you can do it with CPU only nodes",
    "start": "869160",
    "end": "873319"
  },
  {
    "text": "so that was Challenge number two and how",
    "start": "873540",
    "end": "875220"
  },
  {
    "text": "you can effectively utilize our",
    "start": "875220",
    "end": "876360"
  },
  {
    "text": "resources in your cluster for hybrid CPU",
    "start": "876360",
    "end": "878700"
  },
  {
    "text": "plus GPU workloads",
    "start": "878700",
    "end": "880320"
  },
  {
    "text": "Challenge number three is how you can",
    "start": "880320",
    "end": "881639"
  },
  {
    "text": "actually do efficient data transfer and",
    "start": "881639",
    "end": "883380"
  },
  {
    "text": "the key property of this workload is",
    "start": "883380",
    "end": "884820"
  },
  {
    "text": "that we're working with",
    "start": "884820",
    "end": "885360"
  },
  {
    "text": "multi-dimensional tensors so again with",
    "start": "885360",
    "end": "887639"
  },
  {
    "text": "something like images it's unlike",
    "start": "887639",
    "end": "889440"
  },
  {
    "text": "tabular data each image is represented",
    "start": "889440",
    "end": "891779"
  },
  {
    "text": "as a large multi-dimensional tensor and",
    "start": "891779",
    "end": "894360"
  },
  {
    "text": "this is where something like Ray did a",
    "start": "894360",
    "end": "896040"
  },
  {
    "text": "really shines because it uses it's",
    "start": "896040",
    "end": "898260"
  },
  {
    "text": "basically essentially built entirely in",
    "start": "898260",
    "end": "900000"
  },
  {
    "text": "numpy plus pi Arrow so something like",
    "start": "900000",
    "end": "902699"
  },
  {
    "text": "the jvm to Pi Arrow overhead you don't",
    "start": "902699",
    "end": "905579"
  },
  {
    "text": "get that you don't get that overhead",
    "start": "905579",
    "end": "906660"
  },
  {
    "text": "when you're working with the fully",
    "start": "906660",
    "end": "907920"
  },
  {
    "text": "python based system like Ray data and",
    "start": "907920",
    "end": "910139"
  },
  {
    "text": "there's no pandas involved which incurs",
    "start": "910139",
    "end": "911880"
  },
  {
    "text": "a copy between pyro to pandas so the",
    "start": "911880",
    "end": "914100"
  },
  {
    "text": "rated you can entirely work with numpy",
    "start": "914100",
    "end": "916079"
  },
  {
    "text": "and Pyro which are built for tensors",
    "start": "916079",
    "end": "919980"
  },
  {
    "text": "and then finally this fourth challenge",
    "start": "919980",
    "end": "921420"
  },
  {
    "text": "is on developer experience I think one",
    "start": "921420",
    "end": "924000"
  },
  {
    "text": "of the advantages of Ray data has here",
    "start": "924000",
    "end": "925440"
  },
  {
    "text": "is that it's fully python native so you",
    "start": "925440",
    "end": "927060"
  },
  {
    "text": "get better stack traces and a lot of the",
    "start": "927060",
    "end": "929339"
  },
  {
    "text": "basic debugging techniques that you use",
    "start": "929339",
    "end": "931620"
  },
  {
    "text": "for python continue to work with Ray",
    "start": "931620",
    "end": "933480"
  },
  {
    "text": "data",
    "start": "933480",
    "end": "935839"
  },
  {
    "text": "so that was for 10 gigabytes uh let's",
    "start": "935839",
    "end": "938339"
  },
  {
    "text": "take a look at 300 gigabytes and this",
    "start": "938339",
    "end": "939959"
  },
  {
    "text": "one we can do this live so I'll have a",
    "start": "939959",
    "end": "943139"
  },
  {
    "text": "side by side here of a spark versus uh",
    "start": "943139",
    "end": "946199"
  },
  {
    "text": "Ray data and then we can take a look at",
    "start": "946199",
    "end": "948180"
  },
  {
    "text": "this",
    "start": "948180",
    "end": "948959"
  },
  {
    "text": "um",
    "start": "948959",
    "end": "949800"
  },
  {
    "text": "okay I think you guys can see this",
    "start": "949800",
    "end": "951240"
  },
  {
    "text": "hopefully so the left-hand side we have",
    "start": "951240",
    "end": "953160"
  },
  {
    "text": "Ray running on the any scale platform",
    "start": "953160",
    "end": "954839"
  },
  {
    "text": "and then on the right side we have spark",
    "start": "954839",
    "end": "956579"
  },
  {
    "text": "running on databricks",
    "start": "956579",
    "end": "958560"
  },
  {
    "text": "and so again this is the same workload",
    "start": "958560",
    "end": "961079"
  },
  {
    "text": "that we talked about the image",
    "start": "961079",
    "end": "961920"
  },
  {
    "text": "classification",
    "start": "961920",
    "end": "963300"
  },
  {
    "text": "so three steps is that one we want to",
    "start": "963300",
    "end": "965459"
  },
  {
    "text": "read images from S3",
    "start": "965459",
    "end": "967199"
  },
  {
    "text": "two we want to do pre-processing on CPU",
    "start": "967199",
    "end": "969420"
  },
  {
    "text": "and three you want to do model inference",
    "start": "969420",
    "end": "971279"
  },
  {
    "text": "on GPU so I'm going to run both of these",
    "start": "971279",
    "end": "973500"
  },
  {
    "text": "right now and then we can take a look at",
    "start": "973500",
    "end": "975000"
  },
  {
    "text": "the throughput once they finish",
    "start": "975000",
    "end": "976860"
  },
  {
    "text": "so this is the raid data code",
    "start": "976860",
    "end": "979380"
  },
  {
    "text": "so as you can see we want to use like",
    "start": "979380",
    "end": "981720"
  },
  {
    "text": "the ray data streamy executor and we",
    "start": "981720",
    "end": "983519"
  },
  {
    "text": "have these images stored in S3 bucket so",
    "start": "983519",
    "end": "985800"
  },
  {
    "text": "there's a single line to read those",
    "start": "985800",
    "end": "987720"
  },
  {
    "text": "images which are stored as parquet files",
    "start": "987720",
    "end": "989279"
  },
  {
    "text": "into a data set then we Define our",
    "start": "989279",
    "end": "991079"
  },
  {
    "text": "pre-processing function which does",
    "start": "991079",
    "end": "992339"
  },
  {
    "text": "resizing cropping and normalization and",
    "start": "992339",
    "end": "994740"
  },
  {
    "text": "then map that pre-processing function",
    "start": "994740",
    "end": "996360"
  },
  {
    "text": "over the images that we read from S3",
    "start": "996360",
    "end": "998699"
  },
  {
    "text": "finally we define a class where we",
    "start": "998699",
    "end": "1001100"
  },
  {
    "text": "actually do our inference so as part of",
    "start": "1001100",
    "end": "1003560"
  },
  {
    "text": "the state of the class we load in a",
    "start": "1003560",
    "end": "1005180"
  },
  {
    "text": "pre-trained model and then we can do",
    "start": "1005180",
    "end": "1006980"
  },
  {
    "text": "inference on batches just passing the",
    "start": "1006980",
    "end": "1009139"
  },
  {
    "text": "batches through the model to get",
    "start": "1009139",
    "end": "1010279"
  },
  {
    "text": "predictions and because we have 16 gpus",
    "start": "1010279",
    "end": "1012380"
  },
  {
    "text": "in our cluster we can specify you want",
    "start": "1012380",
    "end": "1014060"
  },
  {
    "text": "to use 16 gpus and we stitch the entire",
    "start": "1014060",
    "end": "1016339"
  },
  {
    "text": "thing as a ray data pipeline so as you",
    "start": "1016339",
    "end": "1018860"
  },
  {
    "text": "can see it's turning away doing",
    "start": "1018860",
    "end": "1020120"
  },
  {
    "text": "concurrently doing both the",
    "start": "1020120",
    "end": "1021560"
  },
  {
    "text": "pre-processing on CPUs as well as the",
    "start": "1021560",
    "end": "1024260"
  },
  {
    "text": "inference on gpus",
    "start": "1024260",
    "end": "1027380"
  },
  {
    "text": "so now we can go to the spark side which",
    "start": "1027380",
    "end": "1029178"
  },
  {
    "text": "uh the workload was pretty much exactly",
    "start": "1029179",
    "end": "1031160"
  },
  {
    "text": "the same",
    "start": "1031160",
    "end": "1033319"
  },
  {
    "text": "so again here is a first thing we want",
    "start": "1033319",
    "end": "1035900"
  },
  {
    "text": "to do is we want to load in our",
    "start": "1035900",
    "end": "1036918"
  },
  {
    "text": "pre-trained model so we loaded the",
    "start": "1036919",
    "end": "1038480"
  },
  {
    "text": "pre-train model and then broadcast that",
    "start": "1038480",
    "end": "1039798"
  },
  {
    "text": "to all the spark workers then we read",
    "start": "1039799",
    "end": "1042079"
  },
  {
    "text": "the same exact Bucket from S3 to load in",
    "start": "1042079",
    "end": "1044780"
  },
  {
    "text": "our images we Define a preprocessing",
    "start": "1044780",
    "end": "1046880"
  },
  {
    "text": "function it's a little bit more complex",
    "start": "1046880",
    "end": "1048919"
  },
  {
    "text": "because spark doesn't have native",
    "start": "1048919",
    "end": "1050660"
  },
  {
    "text": "support for multi-dimensional tensors so",
    "start": "1050660",
    "end": "1052760"
  },
  {
    "text": "you have to flatten the tensors and",
    "start": "1052760",
    "end": "1054980"
  },
  {
    "text": "unflatten them as needed",
    "start": "1054980",
    "end": "1056780"
  },
  {
    "text": "then we do our prediction which again is",
    "start": "1056780",
    "end": "1058580"
  },
  {
    "text": "just passing our data through the",
    "start": "1058580",
    "end": "1060320"
  },
  {
    "text": "pre-trained model and getting the",
    "start": "1060320",
    "end": "1062059"
  },
  {
    "text": "prediction results back",
    "start": "1062059",
    "end": "1063620"
  },
  {
    "text": "so you're going to put this all together",
    "start": "1063620",
    "end": "1065360"
  },
  {
    "text": "and Spark pipeline essentially and then",
    "start": "1065360",
    "end": "1067640"
  },
  {
    "text": "run the whole thing so as you can see",
    "start": "1067640",
    "end": "1069559"
  },
  {
    "text": "with the right data we're getting around",
    "start": "1069559",
    "end": "1070700"
  },
  {
    "text": "3000 images per second and again just",
    "start": "1070700",
    "end": "1073400"
  },
  {
    "text": "skip to the end here for spark",
    "start": "1073400",
    "end": "1076900"
  },
  {
    "text": "and then we see we're getting like 684",
    "start": "1078620",
    "end": "1081260"
  },
  {
    "text": "images per second with spark so 300",
    "start": "1081260",
    "end": "1084679"
  },
  {
    "text": "gigabytes from across 16 gpus we're",
    "start": "1084679",
    "end": "1087740"
  },
  {
    "text": "getting much better performance Ray data",
    "start": "1087740",
    "end": "1089299"
  },
  {
    "text": "and this is primarily because what we",
    "start": "1089299",
    "end": "1090500"
  },
  {
    "text": "talked about that Ray data uses a",
    "start": "1090500",
    "end": "1092179"
  },
  {
    "text": "streaming execution backend which is",
    "start": "1092179",
    "end": "1094160"
  },
  {
    "text": "really gets you much better performance",
    "start": "1094160",
    "end": "1096140"
  },
  {
    "text": "for these hybrid CPU plus GPU workloads",
    "start": "1096140",
    "end": "1098240"
  },
  {
    "text": "compared to something like spark which",
    "start": "1098240",
    "end": "1099500"
  },
  {
    "text": "is all bulk based",
    "start": "1099500",
    "end": "1102520"
  },
  {
    "text": "okay sorry yeah and then so so let's see",
    "start": "1106400",
    "end": "1109340"
  },
  {
    "text": "how radiator scales up to 10 terabytes",
    "start": "1109340",
    "end": "1111440"
  },
  {
    "text": "so we talked about 10 gigabytes 200",
    "start": "1111440",
    "end": "1113419"
  },
  {
    "text": "gigabytes 10 terabytes again we're using",
    "start": "1113419",
    "end": "1115700"
  },
  {
    "text": "a 40 GPU cluster and we're getting over",
    "start": "1115700",
    "end": "1118039"
  },
  {
    "text": "90 GPU utilization throughout the entire",
    "start": "1118039",
    "end": "1120380"
  },
  {
    "text": "run so we really are saturating our GPU",
    "start": "1120380",
    "end": "1122660"
  },
  {
    "text": "is pretty and almost close to maximal as",
    "start": "1122660",
    "end": "1125480"
  },
  {
    "text": "we can get in this case we are using",
    "start": "1125480",
    "end": "1127220"
  },
  {
    "text": "heterogeneous cluster so a mix of CPUs",
    "start": "1127220",
    "end": "1129140"
  },
  {
    "text": "plus gpus to better uh improve our",
    "start": "1129140",
    "end": "1131600"
  },
  {
    "text": "performance",
    "start": "1131600",
    "end": "1134080"
  },
  {
    "text": "so just to summarize a comparison",
    "start": "1134120",
    "end": "1135740"
  },
  {
    "text": "between sagemaker batch transform spark",
    "start": "1135740",
    "end": "1138020"
  },
  {
    "text": "and Ray data and across the four",
    "start": "1138020",
    "end": "1141080"
  },
  {
    "text": "different dimensions of the the",
    "start": "1141080",
    "end": "1142400"
  },
  {
    "text": "challenges that we see for large-scale",
    "start": "1142400",
    "end": "1144740"
  },
  {
    "text": "bash processing so all three of them can",
    "start": "1144740",
    "end": "1146780"
  },
  {
    "text": "really help with managing your compute",
    "start": "1146780",
    "end": "1148280"
  },
  {
    "text": "infrastructure but when you want",
    "start": "1148280",
    "end": "1150080"
  },
  {
    "text": "efficient resource utilization and",
    "start": "1150080",
    "end": "1151640"
  },
  {
    "text": "efficient data transfer specifically for",
    "start": "1151640",
    "end": "1153740"
  },
  {
    "text": "these hybrid workloads and also when",
    "start": "1153740",
    "end": "1155720"
  },
  {
    "text": "you're working with unstructured data",
    "start": "1155720",
    "end": "1157100"
  },
  {
    "text": "Ray data really gets you the best",
    "start": "1157100",
    "end": "1158720"
  },
  {
    "text": "performance there plus it's like fully",
    "start": "1158720",
    "end": "1160760"
  },
  {
    "text": "python native so developer user",
    "start": "1160760",
    "end": "1162679"
  },
  {
    "text": "experience is much better you can the",
    "start": "1162679",
    "end": "1164960"
  },
  {
    "text": "same code that you can work with on a",
    "start": "1164960",
    "end": "1166940"
  },
  {
    "text": "smaller scale can then easily be",
    "start": "1166940",
    "end": "1168500"
  },
  {
    "text": "transferred to when you're actually",
    "start": "1168500",
    "end": "1169580"
  },
  {
    "text": "moving this to production large scale",
    "start": "1169580",
    "end": "1173000"
  },
  {
    "text": "cool so emoj just told you about how Ray",
    "start": "1173000",
    "end": "1175760"
  },
  {
    "text": "data performs for image classification",
    "start": "1175760",
    "end": "1177620"
  },
  {
    "text": "now I'd like to tell you about how rate",
    "start": "1177620",
    "end": "1179900"
  },
  {
    "text": "up data performs for stable diffusion",
    "start": "1179900",
    "end": "1182539"
  },
  {
    "text": "and in particular also how it Compares",
    "start": "1182539",
    "end": "1184760"
  },
  {
    "text": "against spark to understand this we ran",
    "start": "1184760",
    "end": "1186980"
  },
  {
    "text": "a benchmark with 16 gpus and 300",
    "start": "1186980",
    "end": "1189620"
  },
  {
    "text": "gigabytes of data this corresponds to",
    "start": "1189620",
    "end": "1191600"
  },
  {
    "text": "about 500 000 images what we've found is",
    "start": "1191600",
    "end": "1194780"
  },
  {
    "text": "that Ray data performs almost one and a",
    "start": "1194780",
    "end": "1196820"
  },
  {
    "text": "half times faster than Spark",
    "start": "1196820",
    "end": "1200120"
  },
  {
    "text": "can we do better than this",
    "start": "1200120",
    "end": "1202100"
  },
  {
    "text": "probably not what we've found is that we",
    "start": "1202100",
    "end": "1204740"
  },
  {
    "text": "achieved nearly 100 GPU utilization on",
    "start": "1204740",
    "end": "1207980"
  },
  {
    "text": "this task",
    "start": "1207980",
    "end": "1210280"
  },
  {
    "text": "of course now we saw how where offline",
    "start": "1211340",
    "end": "1213679"
  },
  {
    "text": "batch difference is important for both",
    "start": "1213679",
    "end": "1214880"
  },
  {
    "text": "image classification as well as a newer",
    "start": "1214880",
    "end": "1216620"
  },
  {
    "text": "type of workload for general image",
    "start": "1216620",
    "end": "1218059"
  },
  {
    "text": "generation with stable diffusion how",
    "start": "1218059",
    "end": "1220100"
  },
  {
    "text": "does offline veteran friends work for",
    "start": "1220100",
    "end": "1221840"
  },
  {
    "text": "llm application development for example",
    "start": "1221840",
    "end": "1223880"
  },
  {
    "text": "it's a pretty it's a pretty critical",
    "start": "1223880",
    "end": "1225799"
  },
  {
    "text": "piece and in particular really where",
    "start": "1225799",
    "end": "1227840"
  },
  {
    "text": "it's most useful is when you're scaling",
    "start": "1227840",
    "end": "1230059"
  },
  {
    "text": "embedding pipelines for rag applications",
    "start": "1230059",
    "end": "1232580"
  },
  {
    "text": "so if you're not familiar with rag",
    "start": "1232580",
    "end": "1233960"
  },
  {
    "text": "applications the way it works is that",
    "start": "1233960",
    "end": "1235340"
  },
  {
    "text": "you have a bunch you want to build for",
    "start": "1235340",
    "end": "1236840"
  },
  {
    "text": "example a q a system over a set of data",
    "start": "1236840",
    "end": "1239179"
  },
  {
    "text": "that you have you process that data you",
    "start": "1239179",
    "end": "1241640"
  },
  {
    "text": "embed that data and store it in some",
    "start": "1241640",
    "end": "1243260"
  },
  {
    "text": "Vector database then when a query comes",
    "start": "1243260",
    "end": "1245539"
  },
  {
    "text": "in like a user question comes in you can",
    "start": "1245539",
    "end": "1247700"
  },
  {
    "text": "look at the embedding similarity between",
    "start": "1247700",
    "end": "1250220"
  },
  {
    "text": "the user's question as well as your data",
    "start": "1250220",
    "end": "1252320"
  },
  {
    "text": "to pull in the relevant context to feed",
    "start": "1252320",
    "end": "1254840"
  },
  {
    "text": "it to your llm which will then produce a",
    "start": "1254840",
    "end": "1257000"
  },
  {
    "text": "response back to the user but the first",
    "start": "1257000",
    "end": "1258919"
  },
  {
    "text": "part of this really is just a batch",
    "start": "1258919",
    "end": "1260120"
  },
  {
    "text": "inference problem you have a bunch of",
    "start": "1260120",
    "end": "1261799"
  },
  {
    "text": "data a bunch of documents you need to do",
    "start": "1261799",
    "end": "1264140"
  },
  {
    "text": "some processing on CPU to split them and",
    "start": "1264140",
    "end": "1265940"
  },
  {
    "text": "chunk them and then you need to embed",
    "start": "1265940",
    "end": "1267799"
  },
  {
    "text": "them which means doing inference with",
    "start": "1267799",
    "end": "1269960"
  },
  {
    "text": "the embedding model on GPU so using Ray",
    "start": "1269960",
    "end": "1272960"
  },
  {
    "text": "data can really help scale out this",
    "start": "1272960",
    "end": "1275059"
  },
  {
    "text": "entire process and make it run much",
    "start": "1275059",
    "end": "1277100"
  },
  {
    "text": "faster",
    "start": "1277100",
    "end": "1277940"
  },
  {
    "text": "so something that would take several",
    "start": "1277940",
    "end": "1279740"
  },
  {
    "text": "hours like if you have a bunch of PDFs",
    "start": "1279740",
    "end": "1281720"
  },
  {
    "text": "and you're just indexing them one by one",
    "start": "1281720",
    "end": "1283700"
  },
  {
    "text": "it could now be done several minutes by",
    "start": "1283700",
    "end": "1285380"
  },
  {
    "text": "indexing indexing them in parallel so if",
    "start": "1285380",
    "end": "1288200"
  },
  {
    "text": "you want to build llm applications that",
    "start": "1288200",
    "end": "1289940"
  },
  {
    "text": "are very real time or you have data",
    "start": "1289940",
    "end": "1291620"
  },
  {
    "text": "that's often changing and you need to",
    "start": "1291620",
    "end": "1292880"
  },
  {
    "text": "keep on indexing them as they change you",
    "start": "1292880",
    "end": "1295159"
  },
  {
    "text": "want to make sure that indexing pipeline",
    "start": "1295159",
    "end": "1297080"
  },
  {
    "text": "can run very fast and very efficiently",
    "start": "1297080",
    "end": "1301059"
  },
  {
    "text": "cool so hopefully we've convinced you",
    "start": "1301159",
    "end": "1303440"
  },
  {
    "text": "now that open source rate data is a",
    "start": "1303440",
    "end": "1305360"
  },
  {
    "text": "great solution for offline batch",
    "start": "1305360",
    "end": "1306740"
  },
  {
    "text": "processing now I'd like to tell you",
    "start": "1306740",
    "end": "1309020"
  },
  {
    "text": "about some features that are coming only",
    "start": "1309020",
    "end": "1310700"
  },
  {
    "text": "to any scale",
    "start": "1310700",
    "end": "1312620"
  },
  {
    "text": "we've seen an increasing number of users",
    "start": "1312620",
    "end": "1314720"
  },
  {
    "text": "work with unstructured data like video",
    "start": "1314720",
    "end": "1316640"
  },
  {
    "text": "and audio so we've introduced apis",
    "start": "1316640",
    "end": "1319460"
  },
  {
    "text": "specifically for streaming this sort of",
    "start": "1319460",
    "end": "1321320"
  },
  {
    "text": "data in addition cost concern continues",
    "start": "1321320",
    "end": "1324200"
  },
  {
    "text": "to be a concern for our users so we're",
    "start": "1324200",
    "end": "1326480"
  },
  {
    "text": "enabling spot instant support by default",
    "start": "1326480",
    "end": "1328640"
  },
  {
    "text": "on any scale",
    "start": "1328640",
    "end": "1330200"
  },
  {
    "text": "first let me tell you about the video",
    "start": "1330200",
    "end": "1332120"
  },
  {
    "text": "support feature and what sort of",
    "start": "1332120",
    "end": "1333620"
  },
  {
    "text": "performance you can expect to get",
    "start": "1333620",
    "end": "1335780"
  },
  {
    "text": "here's what the API looks like the video",
    "start": "1335780",
    "end": "1337940"
  },
  {
    "text": "data source API lets you stream from",
    "start": "1337940",
    "end": "1340159"
  },
  {
    "text": "cloud storage or local disk in a",
    "start": "1340159",
    "end": "1341900"
  },
  {
    "text": "streaming fashion",
    "start": "1341900",
    "end": "1343640"
  },
  {
    "text": "to understand the performance we ran a",
    "start": "1343640",
    "end": "1345500"
  },
  {
    "text": "benchmark with 100 gpus in almost 1 000",
    "start": "1345500",
    "end": "1348080"
  },
  {
    "text": "hours of footage",
    "start": "1348080",
    "end": "1349820"
  },
  {
    "text": "in comparison to running this on a",
    "start": "1349820",
    "end": "1351440"
  },
  {
    "text": "single machine you can run your entire",
    "start": "1351440",
    "end": "1353360"
  },
  {
    "text": "batch inference job in just four hours",
    "start": "1353360",
    "end": "1357520"
  },
  {
    "text": "can you do better uh almost not you can",
    "start": "1357620",
    "end": "1361100"
  },
  {
    "text": "get almost 100 GPU utilization again",
    "start": "1361100",
    "end": "1363559"
  },
  {
    "text": "with this video object detection task",
    "start": "1363559",
    "end": "1366799"
  },
  {
    "text": "next I'd like to tell you about the",
    "start": "1366799",
    "end": "1368539"
  },
  {
    "text": "audio support feature",
    "start": "1368539",
    "end": "1370760"
  },
  {
    "text": "similar to the video data source API",
    "start": "1370760",
    "end": "1372440"
  },
  {
    "text": "we've introduced an audio data source",
    "start": "1372440",
    "end": "1374659"
  },
  {
    "text": "class",
    "start": "1374659",
    "end": "1375799"
  },
  {
    "text": "this lets you stream data again from",
    "start": "1375799",
    "end": "1377900"
  },
  {
    "text": "cloud storage or local disk",
    "start": "1377900",
    "end": "1380299"
  },
  {
    "text": "what sort of performance can you get we",
    "start": "1380299",
    "end": "1382700"
  },
  {
    "text": "ran a similar Benchmark with 100 gpus",
    "start": "1382700",
    "end": "1384740"
  },
  {
    "text": "and one terabyte of audio data",
    "start": "1384740",
    "end": "1387980"
  },
  {
    "text": "in order to transcribe this data using",
    "start": "1387980",
    "end": "1389960"
  },
  {
    "text": "open AI whisper it just took less than",
    "start": "1389960",
    "end": "1392179"
  },
  {
    "text": "one hour",
    "start": "1392179",
    "end": "1393380"
  },
  {
    "text": "what sort of GPU utilization can you",
    "start": "1393380",
    "end": "1395240"
  },
  {
    "text": "expect about 60 percent this might not",
    "start": "1395240",
    "end": "1398419"
  },
  {
    "text": "look great but if you run the same model",
    "start": "1398419",
    "end": "1401000"
  },
  {
    "text": "in a loop on a single machine you see",
    "start": "1401000",
    "end": "1403340"
  },
  {
    "text": "that you get almost as good of GP",
    "start": "1403340",
    "end": "1405020"
  },
  {
    "text": "utilization as you can possibly get",
    "start": "1405020",
    "end": "1407360"
  },
  {
    "text": "finally I'd like to tell you about spot",
    "start": "1407360",
    "end": "1409159"
  },
  {
    "text": "instant support on any scale",
    "start": "1409159",
    "end": "1411200"
  },
  {
    "text": "raid data is Fault tolerant and this",
    "start": "1411200",
    "end": "1413780"
  },
  {
    "text": "means that you can use spot instances",
    "start": "1413780",
    "end": "1415340"
  },
  {
    "text": "what sort of cost savings can you expect",
    "start": "1415340",
    "end": "1417440"
  },
  {
    "text": "to achieve",
    "start": "1417440",
    "end": "1418760"
  },
  {
    "text": "remember the video Benchmark I told you",
    "start": "1418760",
    "end": "1420559"
  },
  {
    "text": "about with open source Ray this",
    "start": "1420559",
    "end": "1423200"
  },
  {
    "text": "Benchmark would cost 500 to run whereas",
    "start": "1423200",
    "end": "1426320"
  },
  {
    "text": "with spot instances on any scale the AWS",
    "start": "1426320",
    "end": "1428720"
  },
  {
    "text": "cost is only 192 dollars",
    "start": "1428720",
    "end": "1432679"
  },
  {
    "text": "cool so we went over why offline batch",
    "start": "1432679",
    "end": "1435020"
  },
  {
    "text": "numbers is important how it's a critical",
    "start": "1435020",
    "end": "1436940"
  },
  {
    "text": "workload and whenever some of the",
    "start": "1436940",
    "end": "1438440"
  },
  {
    "text": "candidate solutions for doing high",
    "start": "1438440",
    "end": "1440659"
  },
  {
    "text": "performance offline match inference for",
    "start": "1440659",
    "end": "1442520"
  },
  {
    "text": "both image classification and for",
    "start": "1442520",
    "end": "1443840"
  },
  {
    "text": "generative AI talked about nlm",
    "start": "1443840",
    "end": "1446000"
  },
  {
    "text": "applications and biology went over some",
    "start": "1446000",
    "end": "1448100"
  },
  {
    "text": "of the additional features we have on",
    "start": "1448100",
    "end": "1449840"
  },
  {
    "text": "any scale I think I'm going to wrap this",
    "start": "1449840",
    "end": "1451400"
  },
  {
    "text": "up with some case studies that we've",
    "start": "1451400",
    "end": "1452600"
  },
  {
    "text": "seen for people using Rey and using any",
    "start": "1452600",
    "end": "1454400"
  },
  {
    "text": "scale for offline batching friends and",
    "start": "1454400",
    "end": "1456320"
  },
  {
    "text": "some some of the value they get out of",
    "start": "1456320",
    "end": "1458240"
  },
  {
    "text": "these uh out of what what we talked",
    "start": "1458240",
    "end": "1460400"
  },
  {
    "text": "about today",
    "start": "1460400",
    "end": "1461900"
  },
  {
    "text": "so the first one is bite dance which is",
    "start": "1461900",
    "end": "1463880"
  },
  {
    "text": "the creators of tick tock so their use",
    "start": "1463880",
    "end": "1465679"
  },
  {
    "text": "case is really really cool and very",
    "start": "1465679",
    "end": "1467240"
  },
  {
    "text": "Advanced",
    "start": "1467240",
    "end": "1468260"
  },
  {
    "text": "um so they have 200 terabytes of both",
    "start": "1468260",
    "end": "1470539"
  },
  {
    "text": "image and Text data and they want to",
    "start": "1470539",
    "end": "1472820"
  },
  {
    "text": "pass both of them in to them through a",
    "start": "1472820",
    "end": "1475100"
  },
  {
    "text": "model to generate embeddings in the same",
    "start": "1475100",
    "end": "1476780"
  },
  {
    "text": "embedding space for both the images and",
    "start": "1476780",
    "end": "1478940"
  },
  {
    "text": "Text data one of the key characteristics",
    "start": "1478940",
    "end": "1481220"
  },
  {
    "text": "about this workload is the model doesn't",
    "start": "1481220",
    "end": "1482780"
  },
  {
    "text": "fit on a single GPU right so they have",
    "start": "1482780",
    "end": "1485299"
  },
  {
    "text": "to Shard the model across multiple gpus",
    "start": "1485299",
    "end": "1488440"
  },
  {
    "text": "and they actually chose Ray data over",
    "start": "1488440",
    "end": "1490940"
  },
  {
    "text": "spark for this type of workload because",
    "start": "1490940",
    "end": "1492559"
  },
  {
    "text": "of rate data's expressivity and its",
    "start": "1492559",
    "end": "1495320"
  },
  {
    "text": "performance so with array data you could",
    "start": "1495320",
    "end": "1497000"
  },
  {
    "text": "simply easily create this pipeline where",
    "start": "1497000",
    "end": "1500000"
  },
  {
    "text": "you're doing reading pre-processing of",
    "start": "1500000",
    "end": "1502880"
  },
  {
    "text": "your data passing the data through the",
    "start": "1502880",
    "end": "1505100"
  },
  {
    "text": "first stage which is the first few",
    "start": "1505100",
    "end": "1506539"
  },
  {
    "text": "layers of the model then to the second",
    "start": "1506539",
    "end": "1508280"
  },
  {
    "text": "stage and then to the search third stage",
    "start": "1508280",
    "end": "1510020"
  },
  {
    "text": "each of these stages consisting some",
    "start": "1510020",
    "end": "1511760"
  },
  {
    "text": "layers in the model and then finally",
    "start": "1511760",
    "end": "1513020"
  },
  {
    "text": "writing it out to a data source so with",
    "start": "1513020",
    "end": "1515960"
  },
  {
    "text": "the rate data you can actually stream",
    "start": "1515960",
    "end": "1517280"
  },
  {
    "text": "your stream this entire pipeline so data",
    "start": "1517280",
    "end": "1520580"
  },
  {
    "text": "can be streamed all the way through",
    "start": "1520580",
    "end": "1521720"
  },
  {
    "text": "across all the different model stages",
    "start": "1521720",
    "end": "1523580"
  },
  {
    "text": "versus something like spark that's not",
    "start": "1523580",
    "end": "1525559"
  },
  {
    "text": "the case",
    "start": "1525559",
    "end": "1527600"
  },
  {
    "text": "so another use case we want to talk",
    "start": "1527600",
    "end": "1529520"
  },
  {
    "text": "about is a company called sewer AI which",
    "start": "1529520",
    "end": "1531860"
  },
  {
    "text": "is a they what they do is they basically",
    "start": "1531860",
    "end": "1533900"
  },
  {
    "text": "send robots into like sewers to analyze",
    "start": "1533900",
    "end": "1536059"
  },
  {
    "text": "damage or any any other problems that",
    "start": "1536059",
    "end": "1538400"
  },
  {
    "text": "maybe maybe occurring in sewer pipelines",
    "start": "1538400",
    "end": "1540380"
  },
  {
    "text": "so they have a bunch of videos like",
    "start": "1540380",
    "end": "1541820"
  },
  {
    "text": "thousands of videos that they wanted to",
    "start": "1541820",
    "end": "1543559"
  },
  {
    "text": "process uh frequently and they do like",
    "start": "1543559",
    "end": "1546200"
  },
  {
    "text": "object detection on different frames of",
    "start": "1546200",
    "end": "1547700"
  },
  {
    "text": "the video so previously they were using",
    "start": "1547700",
    "end": "1549320"
  },
  {
    "text": "sagemaker instead they adopted Rey at",
    "start": "1549320",
    "end": "1552740"
  },
  {
    "text": "any scale and they see around 75 cost",
    "start": "1552740",
    "end": "1554840"
  },
  {
    "text": "reduction in their offline batch",
    "start": "1554840",
    "end": "1556520"
  },
  {
    "text": "inference workload when when using array",
    "start": "1556520",
    "end": "1558860"
  },
  {
    "text": "and how you scale",
    "start": "1558860",
    "end": "1561400"
  },
  {
    "text": "cool so to wrap this up here's what we",
    "start": "1561980",
    "end": "1565039"
  },
  {
    "text": "talked about batch inference is a",
    "start": "1565039",
    "end": "1566840"
  },
  {
    "text": "critical workload especially for",
    "start": "1566840",
    "end": "1568159"
  },
  {
    "text": "emerging applications like generative Ai",
    "start": "1568159",
    "end": "1570260"
  },
  {
    "text": "and LMS and hopefully we've convinced",
    "start": "1570260",
    "end": "1573020"
  },
  {
    "text": "you that Ray data is the best open",
    "start": "1573020",
    "end": "1575059"
  },
  {
    "text": "source solution to batch inference for",
    "start": "1575059",
    "end": "1576980"
  },
  {
    "text": "developer productivity cost and",
    "start": "1576980",
    "end": "1579620"
  },
  {
    "text": "performance and finally we're",
    "start": "1579620",
    "end": "1581900"
  },
  {
    "text": "introducing new features on any scale to",
    "start": "1581900",
    "end": "1584179"
  },
  {
    "text": "further enhance batch inference",
    "start": "1584179",
    "end": "1586820"
  },
  {
    "text": "and the takeout message here is that if",
    "start": "1586820",
    "end": "1589279"
  },
  {
    "text": "you want faster and cheaper batch",
    "start": "1589279",
    "end": "1590779"
  },
  {
    "text": "inference get in touch with us thank you",
    "start": "1590779",
    "end": "1595539"
  },
  {
    "text": "okay maybe you have one questions",
    "start": "1596720",
    "end": "1600460"
  },
  {
    "text": "uh how does Ray handle fault tolerance",
    "start": "1603500",
    "end": "1606260"
  },
  {
    "text": "like if you're two hours into that four",
    "start": "1606260",
    "end": "1608299"
  },
  {
    "text": "hour long job",
    "start": "1608299",
    "end": "1610600"
  },
  {
    "text": "hello my question is about fault",
    "start": "1610600",
    "end": "1613400"
  },
  {
    "text": "tolerance so if you're halfway through",
    "start": "1613400",
    "end": "1615500"
  },
  {
    "text": "one of those really big jobs",
    "start": "1615500",
    "end": "1617779"
  },
  {
    "text": "um I I haven't used Ray that much so",
    "start": "1617779",
    "end": "1619640"
  },
  {
    "text": "maybe it's a really simple answer but um",
    "start": "1619640",
    "end": "1621860"
  },
  {
    "text": "how does it handle basically like",
    "start": "1621860",
    "end": "1623480"
  },
  {
    "text": "something failed really drastically and",
    "start": "1623480",
    "end": "1625700"
  },
  {
    "text": "you need to restart the job can you pick",
    "start": "1625700",
    "end": "1627740"
  },
  {
    "text": "up where you left off are there like",
    "start": "1627740",
    "end": "1629480"
  },
  {
    "text": "checkpoints or is that totally dependent",
    "start": "1629480",
    "end": "1631700"
  },
  {
    "text": "on exactly the program you wrote um so",
    "start": "1631700",
    "end": "1634880"
  },
  {
    "text": "sorry for the somewhat vague question",
    "start": "1634880",
    "end": "1636559"
  },
  {
    "text": "yeah no that's a great question so uh so",
    "start": "1636559",
    "end": "1639559"
  },
  {
    "text": "you don't have to restart the job from",
    "start": "1639559",
    "end": "1640880"
  },
  {
    "text": "the very beginning right so Rey has",
    "start": "1640880",
    "end": "1642380"
  },
  {
    "text": "basically built an object reconstruction",
    "start": "1642380",
    "end": "1644960"
  },
  {
    "text": "so if a node fails and there's some data",
    "start": "1644960",
    "end": "1647720"
  },
  {
    "text": "or some state that the node is holding",
    "start": "1647720",
    "end": "1649279"
  },
  {
    "text": "what we can do is we can actually",
    "start": "1649279",
    "end": "1650900"
  },
  {
    "text": "regenerate those objects so if you lose",
    "start": "1650900",
    "end": "1653480"
  },
  {
    "text": "some of your intermediate data we can",
    "start": "1653480",
    "end": "1654860"
  },
  {
    "text": "re-run the pre-processing steps for just",
    "start": "1654860",
    "end": "1657320"
  },
  {
    "text": "that data that you lost",
    "start": "1657320",
    "end": "1659179"
  },
  {
    "text": "so if you're basically for any data",
    "start": "1659179",
    "end": "1661460"
  },
  {
    "text": "that's not that's not uh that's totally",
    "start": "1661460",
    "end": "1663980"
  },
  {
    "text": "fine not touched by any node failure",
    "start": "1663980",
    "end": "1665600"
  },
  {
    "text": "that data is all there so you don't have",
    "start": "1665600",
    "end": "1667340"
  },
  {
    "text": "to restart from the very beginning you",
    "start": "1667340",
    "end": "1668360"
  },
  {
    "text": "can essentially only reconstruct the",
    "start": "1668360",
    "end": "1670580"
  },
  {
    "text": "data that's lost on a node failure",
    "start": "1670580",
    "end": "1674059"
  },
  {
    "text": "okay another round of applause for a",
    "start": "1674059",
    "end": "1676700"
  },
  {
    "text": "mark and balachi thank you so much",
    "start": "1676700",
    "end": "1678490"
  },
  {
    "text": "[Applause]",
    "start": "1678490",
    "end": "1682960"
  }
]