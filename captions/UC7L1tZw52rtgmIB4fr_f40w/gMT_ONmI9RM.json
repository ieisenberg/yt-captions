[
  {
    "text": "good morning everyone my name is shuning I'm from Uber I'm an engineering manager at Uber's machine learning platform also",
    "start": "3120",
    "end": "9780"
  },
  {
    "text": "known as Michelangelo you might have heard about it together with me uh du and Michael mui these are all engineers",
    "start": "9780",
    "end": "17160"
  },
  {
    "text": "in the machine learning platform team at Uber at Uber we've been using Rey to power",
    "start": "17160",
    "end": "23400"
  },
  {
    "text": "some of the machine learning workflow for quite a while we're collaborating with any scale today I want to take this",
    "start": "23400",
    "end": "29519"
  },
  {
    "text": "opportunity to share with all of you our journey in using Rey and some of the",
    "start": "29519",
    "end": "34620"
  },
  {
    "text": "interesting stuff we developed and our topic is a large-scale deep learning training and tuning with Ray",
    "start": "34620",
    "end": "42500"
  },
  {
    "text": "just a quick introduction about for ourselves I've been at Uber for more",
    "start": "42920",
    "end": "48300"
  },
  {
    "text": "than seven years and for the past four years I have been on a journey to uh",
    "start": "48300",
    "end": "54420"
  },
  {
    "text": "bring deep learning as a capability to Uber's machine learning platform D here is specializes in the ray-based",
    "start": "54420",
    "end": "63120"
  },
  {
    "text": "Deep learning infrastructure and Michael has a lot of experience in building",
    "start": "63120",
    "end": "68939"
  },
  {
    "text": "distributed training infrastructure hyper parameter optimization and model representation evaluation",
    "start": "68939",
    "end": "76700"
  },
  {
    "text": "so in today's talk it's only be 30 minutes but it's packed with content first I'll be giving a a brief uh brief",
    "start": "78000",
    "end": "86580"
  },
  {
    "text": "talk about the the journey uh we have over the multiple years developing machine learning platform at Uber as you",
    "start": "86580",
    "end": "93180"
  },
  {
    "text": "know Michelangelo is a one of the early Pioneer uh concept developer of this",
    "start": "93180",
    "end": "98939"
  },
  {
    "text": "machine learning as a platform after that D will come in and talk about",
    "start": "98939",
    "end": "105320"
  },
  {
    "text": "large-scale deep learning Power by by Ray and then Michael will talk about",
    "start": "105320",
    "end": "110640"
  },
  {
    "text": "evolving this high performance search which we initially have at one platform and then we migrate into array using",
    "start": "110640",
    "end": "116640"
  },
  {
    "text": "retune finally I will give a closing remark and somebody and also point you",
    "start": "116640",
    "end": "121979"
  },
  {
    "text": "guys to uh some of the resources online resources and blog posts that we have written in the past in order for you to",
    "start": "121979",
    "end": "128459"
  },
  {
    "text": "Deep dive so um first let me just give a overview of",
    "start": "128459",
    "end": "135000"
  },
  {
    "text": "Michelangelo Michelangelo is a end-to-end machine learning platform that basically uh",
    "start": "135000",
    "end": "142200"
  },
  {
    "text": "integrates well with Uber's machine Uber's data infrastructure to help our",
    "start": "142200",
    "end": "148260"
  },
  {
    "text": "machine learning Engineers develop their uh develop and productionize their models quickly",
    "start": "148260",
    "end": "153840"
  },
  {
    "text": "at Uber all our data is stored in the data data store in this uh in this slide",
    "start": "153840",
    "end": "163140"
  },
  {
    "text": "is the basis feature store but we also take a lot of streaming data and we do",
    "start": "163140",
    "end": "169739"
  },
  {
    "text": "processing and those become a batch feature store in real time future store why do we do that this is because for",
    "start": "169739",
    "end": "176760"
  },
  {
    "text": "any model we have contextual data like data that only is related to the model itself but across different models we",
    "start": "176760",
    "end": "184500"
  },
  {
    "text": "all have a lot of shared data shared data pipelines share features that's used across all the many of these models",
    "start": "184500",
    "end": "191159"
  },
  {
    "text": "that's why I build this online offline consistent data store in the middle there's a model training module which is",
    "start": "191159",
    "end": "198720"
  },
  {
    "text": "a generic module that executes the the training we support a traditional machine learning models such as linear",
    "start": "198720",
    "end": "206040"
  },
  {
    "text": "linear or logistic regression to this to the boosted trees and then of course the",
    "start": "206040",
    "end": "212580"
  },
  {
    "text": "Deep learning models after model is trained I store in our local internal",
    "start": "212580",
    "end": "217739"
  },
  {
    "text": "model store where we manage is like the checkpoint we manage life cycle of these models and give our users the",
    "start": "217739",
    "end": "223200"
  },
  {
    "text": "operational panel to deploy we have two ways to deploy the models one is the RPC",
    "start": "223200",
    "end": "228780"
  },
  {
    "text": "this is very similar to what you have seen in today's demo by Robert we also",
    "start": "228780",
    "end": "236040"
  },
  {
    "text": "have the batch prediction jobs where we can kick off a batch large scale",
    "start": "236040",
    "end": "242180"
  },
  {
    "text": "protection job on a data on the data table that on the hive and then and then",
    "start": "242180",
    "end": "247739"
  },
  {
    "text": "write the output back to the data lay in a different table um as I said Michelangelo has been in a",
    "start": "247739",
    "end": "254879"
  },
  {
    "text": "multiple Journey a memorial year journey I started in 2016. that's our first",
    "start": "254879",
    "end": "260519"
  },
  {
    "text": "generation uh our first generation is basically a pure spark based uh uh",
    "start": "260519",
    "end": "265880"
  },
  {
    "text": "application more like a monolithic spark based application and we only uh support",
    "start": "265880",
    "end": "271380"
  },
  {
    "text": "the algorithm that comes with it in a spark ml lip and then in 2018 we start",
    "start": "271380",
    "end": "276840"
  },
  {
    "text": "to have a increasingly larger Demand on deep learning now with the existing spark system what we did was boat on the",
    "start": "276840",
    "end": "284820"
  },
  {
    "text": "Deep learning capabilities on top of spark and then uh into since 2021 we",
    "start": "284820",
    "end": "291120"
  },
  {
    "text": "have been in this new journey of like taking uh Ray and using Rey as the unify uh compute framework to power both",
    "start": "291120",
    "end": "298800"
  },
  {
    "text": "machine learning and deep learning workload here at Uber in the next couple",
    "start": "298800",
    "end": "303900"
  },
  {
    "text": "minutes I would just talk about this in much more detail",
    "start": "303900",
    "end": "309020"
  },
  {
    "text": "so uh this is our first generation machine learning platform this is basically one big spark application on",
    "start": "309120",
    "end": "316139"
  },
  {
    "text": "the left hand side we basically do a spark SQL join between our batch",
    "start": "316139",
    "end": "321259"
  },
  {
    "text": "basis table which is the model specific feature and then also the batch feature",
    "start": "321259",
    "end": "328139"
  },
  {
    "text": "store which is the common features across different models and then this state John table gets passed through a",
    "start": "328139",
    "end": "334620"
  },
  {
    "text": "first pipeline of a series of Transformers such as standard scaling such as one hot encoding or string",
    "start": "334620",
    "end": "341400"
  },
  {
    "text": "indexers so these are very very standard spark operations after that the data",
    "start": "341400",
    "end": "347639"
  },
  {
    "text": "frame is passed through the second second pipeline which is the model fitting Post train transform and",
    "start": "347639",
    "end": "352979"
  },
  {
    "text": "evaluation right so of these are very straightforward spot application style they resulted",
    "start": "352979",
    "end": "359300"
  },
  {
    "text": "artifact here is in the yellow box which is a spark servable spark pipeline that",
    "start": "359300",
    "end": "366780"
  },
  {
    "text": "contains multiple feature Church of multiple Transformers like feature",
    "start": "366780",
    "end": "371940"
  },
  {
    "text": "Transformers and scoring models and then this uh several model is going",
    "start": "371940",
    "end": "377880"
  },
  {
    "text": "to be used in both real-time serving and the batch serving now in order for for",
    "start": "377880",
    "end": "384539"
  },
  {
    "text": "us to have a high performance uh real-time prediction service we actually",
    "start": "384539",
    "end": "390660"
  },
  {
    "text": "make our own customization to spark a spark Pipeline and uh we we open source",
    "start": "390660",
    "end": "397740"
  },
  {
    "text": "that as a part as one of the spark ticket it's like a right now it's in a patch format a patch status",
    "start": "397740",
    "end": "404819"
  },
  {
    "text": "but the idea is that we we don't need a spark context to use these Transformers we just like use we can expose the right",
    "start": "404819",
    "end": "412199"
  },
  {
    "text": "interface and use the internal algorithms um since 2018 like I said we",
    "start": "412199",
    "end": "418800"
  },
  {
    "text": "um uh Ubers basically a model uh which is in classical machine learning models",
    "start": "418800",
    "end": "424620"
  },
  {
    "text": "are getting saturated and then we we basically all the teams are starting to look at uh deep learning now deep",
    "start": "424620",
    "end": "430680"
  },
  {
    "text": "learning presents a lot of new challenges on this existing spot system notably uh we were using spark one point",
    "start": "430680",
    "end": "438479"
  },
  {
    "text": "attack and I think remember it's a 2.x and we have a GPU executor problem so",
    "start": "438479",
    "end": "443880"
  },
  {
    "text": "basically like we have some work that needs to be done in CPU some work needs to be done in GPU um and then the the spark data frame is",
    "start": "443880",
    "end": "452220"
  },
  {
    "text": "also not very in conducive to this micro mini batch Shuffle that is very typical use in deep learning training",
    "start": "452220",
    "end": "459780"
  },
  {
    "text": "um and then we need to also to all reduce basically a distributed job training Uber we developed a",
    "start": "459780",
    "end": "465840"
  },
  {
    "text": "um a open source software called Harvard and I'm sure many of you might have heard of it uh it's open source library",
    "start": "465840",
    "end": "472259"
  },
  {
    "text": "for distributed deep learning um so now the the best way to execute",
    "start": "472259",
    "end": "477360"
  },
  {
    "text": "Hardware job is actually a standalone distributed container job that will launched a class launched a cluster now",
    "start": "477360",
    "end": "484259"
  },
  {
    "text": "how do we marry that with with the spot pipeline well what we did was also open",
    "start": "484259",
    "end": "489479"
  },
  {
    "text": "source it's called spark Hardware spot estimators so essentially this is a spot estimator syntax but that can be",
    "start": "489479",
    "end": "496979"
  },
  {
    "text": "launched and executed within spark but then like uh it will do the horrible thing so let me just illustrate you uh",
    "start": "496979",
    "end": "504360"
  },
  {
    "text": "illustrate to you in this side um in this world what happened is",
    "start": "504360",
    "end": "509759"
  },
  {
    "text": "basically we have the first spot application that's the feature preparation and transform and then we",
    "start": "509759",
    "end": "515279"
  },
  {
    "text": "will call the horrible estimators fit function the the fifth function is going to write the data uh into into hdfs and",
    "start": "515279",
    "end": "523500"
  },
  {
    "text": "then kick off the uh distributed GPU job the GPU job will run on on its own uh",
    "start": "523500",
    "end": "529440"
  },
  {
    "text": "we'll stream the data from the hdfs and then do the do the training all right",
    "start": "529440",
    "end": "534480"
  },
  {
    "text": "and and the resulted model is gonna it's be wrapped in a custom spark Transformer and uh and send that to a spark context",
    "start": "534480",
    "end": "541620"
  },
  {
    "text": "so we can construct a pipeline now uh at by 2021 we are basically at a",
    "start": "541620",
    "end": "550980"
  },
  {
    "text": "point where most of the most impactful Machinery models that Uber are using deep learning",
    "start": "550980",
    "end": "556680"
  },
  {
    "text": "um and they are increasing demand bigger and larger models training with lots of more data and being more performant uh",
    "start": "556680",
    "end": "564420"
  },
  {
    "text": "our our pipe our like stitched together uh architecture is a it's pretty complex",
    "start": "564420",
    "end": "570360"
  },
  {
    "text": "and increasingly becomes fragile um so this is when we turn to Rey uh",
    "start": "570360",
    "end": "576420"
  },
  {
    "text": "what we found is that Ray provides very powerful and simple uh Primitives for",
    "start": "576420",
    "end": "581760"
  },
  {
    "text": "distributed computing such as remote tasks and it supports like well like um",
    "start": "581760",
    "end": "587300"
  },
  {
    "text": "Hardware aware of this uh task scheduling so that let us be able to",
    "start": "587300",
    "end": "592980"
  },
  {
    "text": "like carefully decide what needs to be executed on CPU and what needs to be executed on GPU",
    "start": "592980",
    "end": "599700"
  },
  {
    "text": "so here's where we are um we have uh right now we are in a on a",
    "start": "599700",
    "end": "605940"
  },
  {
    "text": "journey to replace the majority of the components of deep learning of machine learning Pipeline with with spark uh we",
    "start": "605940",
    "end": "612180"
  },
  {
    "text": "have this first step which will replace the training module uh oh sorry with Ray uh where the first first step",
    "start": "612180",
    "end": "620160"
  },
  {
    "text": "um we are expanding this array uh coverage from model training uh to next",
    "start": "620160",
    "end": "626820"
  },
  {
    "text": "step is to transform and then to to feature generation now so this is our",
    "start": "626820",
    "end": "631980"
  },
  {
    "text": "roadmap with that I'm going to hand over to D who is going to talk about the large-scale Deep learning training with",
    "start": "631980",
    "end": "637740"
  },
  {
    "text": "uh Ray all right hello everyone my name is d uh I work at Uber in the Michelangelo AI",
    "start": "637740",
    "end": "645540"
  },
  {
    "text": "team I'm glad to have this opportunity to talk to you about how we actually uh",
    "start": "645540",
    "end": "651839"
  },
  {
    "text": "integrate array into our training pipeline so basically this is a very high level view of how our training",
    "start": "651839",
    "end": "659760"
  },
  {
    "text": "pipeline looks like we have a bunch of feature preparation a transformation stage to get the data ready those are",
    "start": "659760",
    "end": "667920"
  },
  {
    "text": "based on spark executors and then when it comes to the model training we bring",
    "start": "667920",
    "end": "673320"
  },
  {
    "text": "up a back-end based on Rey which can efficiently and very conveniently train",
    "start": "673320",
    "end": "679260"
  },
  {
    "text": "the models and then for later stage we go back to the spark executor",
    "start": "679260",
    "end": "686160"
  },
  {
    "text": "um so uh my talk will be focused on the red part of the diagram",
    "start": "686160",
    "end": "691860"
  },
  {
    "text": "um basically how we use Ray open source to enable offline trainer uh running in",
    "start": "691860",
    "end": "698220"
  },
  {
    "text": "scale uh we actually had a uh already evolved",
    "start": "698220",
    "end": "704220"
  },
  {
    "text": "the two generations of array this is our first generation basically I want to",
    "start": "704220",
    "end": "709620"
  },
  {
    "text": "show how we actually bring up the ray cluster on the left side you can see the",
    "start": "709620",
    "end": "714980"
  },
  {
    "text": "trainer main process actually will reach a point where it costs a array of",
    "start": "714980",
    "end": "722160"
  },
  {
    "text": "command which the command will be sent to the job Gateway Michelangelo job Gateway the microl engine's job Gateway",
    "start": "722160",
    "end": "728940"
  },
  {
    "text": "will actually parse request and then send the um call to a scheduler we have",
    "start": "728940",
    "end": "735899"
  },
  {
    "text": "our backend running on on-premises data center which is using Palatine as a",
    "start": "735899",
    "end": "743820"
  },
  {
    "text": "scheduler or we can run on Google Cloud which is a kubernetes so this scheduler",
    "start": "743820",
    "end": "749760"
  },
  {
    "text": "will actually provision the ray cluster basically in a manner that it bring up",
    "start": "749760",
    "end": "755640"
  },
  {
    "text": "the header node and the worker node of the array all together and then I don't",
    "start": "755640",
    "end": "760740"
  },
  {
    "text": "know I'm working on will use a timeout strategy to synchronize and get render walls once array cluster is ready you go",
    "start": "760740",
    "end": "769019"
  },
  {
    "text": "back the the train main process on the left side wait for the return of this",
    "start": "769019",
    "end": "776399"
  },
  {
    "text": "call and then once it gets the return it knows the rate class is ready it will",
    "start": "776399",
    "end": "781620"
  },
  {
    "text": "create the array client link to the cluster and start running jobs on this",
    "start": "781620",
    "end": "787560"
  },
  {
    "text": "Ray cluster so the once the recaster is provisioned",
    "start": "787560",
    "end": "796380"
  },
  {
    "text": "the overall architecture is a relatively simple we have header note I just draw",
    "start": "796380",
    "end": "801720"
  },
  {
    "text": "one working out here the header note working are identical in terms of Hardware",
    "start": "801720",
    "end": "808700"
  },
  {
    "text": "specification they both have a GPU on them we run the training process on the",
    "start": "808700",
    "end": "815100"
  },
  {
    "text": "GPU and they both have large memory CPU we run the data loading and shuffling",
    "start": "815100",
    "end": "821639"
  },
  {
    "text": "process on them so basically this kind of force the one-to-one match of data",
    "start": "821639",
    "end": "827880"
  },
  {
    "text": "loading of versus the training processes um the header note itself also need to",
    "start": "827880",
    "end": "834480"
  },
  {
    "text": "take care of the master management so this is a architecture of a first generation we call it a rigid rate",
    "start": "834480",
    "end": "842000"
  },
  {
    "text": "for rigid Ray I want to compare the pros and cons of the setup basically the uh",
    "start": "842000",
    "end": "850980"
  },
  {
    "text": "it's very straightforward to implement it's a getting us started with a working",
    "start": "850980",
    "end": "860360"
  },
  {
    "text": "architecture basically at the moment the emergency majority of our deep learning",
    "start": "860360",
    "end": "866100"
  },
  {
    "text": "workflow running on such configuration but very quickly we discovered the limitation of this configuration",
    "start": "866100",
    "end": "872760"
  },
  {
    "text": "basically the i o and compute process has to be one to one ratio the",
    "start": "872760",
    "end": "879860"
  },
  {
    "text": "cluster size once provisioned is kind of fixed it lacks the uh elasticity and the fault",
    "start": "879860",
    "end": "888420"
  },
  {
    "text": "tolerance which actually provided come with a very open source so we very",
    "start": "888420",
    "end": "894480"
  },
  {
    "text": "quickly evolved into our second generation heterogeneous reclasser the provisioning step slightly different",
    "start": "894480",
    "end": "900540"
  },
  {
    "text": "here the kicks on are still the same the train main process sends Mario upcoming",
    "start": "900540",
    "end": "907800"
  },
  {
    "text": "to our job Gateway the job gateways and the provisioning request to our scheduler uh the difference are when the",
    "start": "907800",
    "end": "915420"
  },
  {
    "text": "scheduler actually bring up the instance for the new generation we actually just",
    "start": "915420",
    "end": "921060"
  },
  {
    "text": "bring up the head now and then we leverage the auto scalar capability provided by Ray open source to",
    "start": "921060",
    "end": "927959"
  },
  {
    "text": "um directly bring up the rest of the classer so basically we just need to provide a configuration file to the hat",
    "start": "927959",
    "end": "935339"
  },
  {
    "text": "and the hat node will automatically bring up the rest of the red cluster and",
    "start": "935339",
    "end": "940500"
  },
  {
    "text": "then the synchronization and wait for the ray class to come up will pretty much still be the same",
    "start": "940500",
    "end": "946920"
  },
  {
    "text": "so once the cluster is up and running we can have",
    "start": "946920",
    "end": "952079"
  },
  {
    "text": "for example a configuration like this basically we have a dedicated had no this time the head note doesn't have GPU",
    "start": "952079",
    "end": "959519"
  },
  {
    "text": "process on them um they that doesn't run the training or",
    "start": "959519",
    "end": "964560"
  },
  {
    "text": "data loading anymore it only manages the classer it take care of the auto scaling",
    "start": "964560",
    "end": "970079"
  },
  {
    "text": "and also collecting uh premises metrics collecting the logs the trainer now can be specified with",
    "start": "970079",
    "end": "979519"
  },
  {
    "text": "arbitrary number of nodes arbitrary number of gpus on each node I draw",
    "start": "979519",
    "end": "985320"
  },
  {
    "text": "example of two GPU worker here basically the trainer node will be dedicated for training it doesn't",
    "start": "985320",
    "end": "992480"
  },
  {
    "text": "doesn't have to do the data processing the shuffling anymore we decouple the",
    "start": "992480",
    "end": "1000139"
  },
  {
    "text": "data shuffling and data reading to the data nodes which is uh the uh in the",
    "start": "1000139",
    "end": "1005839"
  },
  {
    "text": "middle column three of them I draw three of them which are pretty much a very uh",
    "start": "1005839",
    "end": "1011779"
  },
  {
    "text": "compute intense memory intense um it needs um we use the rate data",
    "start": "1011779",
    "end": "1018579"
  },
  {
    "text": "provided as a module in the array open source software to run these data",
    "start": "1018579",
    "end": "1025699"
  },
  {
    "text": "trainers so the benefit of having the heterogeneous classes",
    "start": "1025699",
    "end": "1030938"
  },
  {
    "text": "we have a uh we um we're able to kind of isolate the i o",
    "start": "1030939",
    "end": "1039260"
  },
  {
    "text": "part and the compute part of the model to different cluster and then that allows us to flexibly configure",
    "start": "1039260",
    "end": "1046280"
  },
  {
    "text": "different um i o to compute ratio to match the",
    "start": "1046280",
    "end": "1051820"
  },
  {
    "text": "characteristic of each model because each model can be different some models",
    "start": "1051820",
    "end": "1057020"
  },
  {
    "text": "are more complicated need more compute but some models need to read more data which needs more IO",
    "start": "1057020",
    "end": "1063799"
  },
  {
    "text": "um and then we leverage the object store provided by Ray so that we can do uh",
    "start": "1063799",
    "end": "1070580"
  },
  {
    "text": "like a semi a global Shuffle basically the shuffling the data shuffling will be",
    "start": "1070580",
    "end": "1075860"
  },
  {
    "text": "carried out in a much larger scale and also by isolating the um data reading process and the training",
    "start": "1075860",
    "end": "1084140"
  },
  {
    "text": "process we are much easier to say diagnose what's happening inside the",
    "start": "1084140",
    "end": "1089960"
  },
  {
    "text": "array if something if something went out of memory we will be able to know exactly which process cost it so this",
    "start": "1089960",
    "end": "1097700"
  },
  {
    "text": "decoupling helps us with debugging and monitoring the recluster as well",
    "start": "1097700",
    "end": "1103280"
  },
  {
    "text": "uh and also we can leverage the native elasticity of Ray um which is a very uh powerful feature",
    "start": "1103280",
    "end": "1110900"
  },
  {
    "text": "as well um so I want to give a concrete example of",
    "start": "1110900",
    "end": "1117500"
  },
  {
    "text": "how the performance actually gets better when we switch to heterogeneous cluster",
    "start": "1117500",
    "end": "1123380"
  },
  {
    "text": "this is used a training of a i o intense model basically uh",
    "start": "1123380",
    "end": "1130700"
  },
  {
    "text": "you can see even if the node number looks more each node we can have less",
    "start": "1130700",
    "end": "1137780"
  },
  {
    "text": "resource assigned to them so the overall which is the force line total CPU core memory use are",
    "start": "1137780",
    "end": "1146000"
  },
  {
    "text": "reduced with heterogeneous recluster however we were able to actually improve the performance because now we decouple",
    "start": "1146000",
    "end": "1153380"
  },
  {
    "text": "train and uh and data reading each of the CPU or GPU can be carried out at",
    "start": "1153380",
    "end": "1162020"
  },
  {
    "text": "their Peak capacity so we have a savings on GPU CPU memories but also we have a",
    "start": "1162020",
    "end": "1168919"
  },
  {
    "text": "throughput Improvement so after this I'm gonna hand my talk to",
    "start": "1168919",
    "end": "1174020"
  },
  {
    "text": "Michael",
    "start": "1174020",
    "end": "1176500"
  },
  {
    "text": "sorry let me quickly test this okay uh oh sorry",
    "start": "1180440",
    "end": "1186799"
  },
  {
    "text": "uh okay hey everyone so my name is Michael and today I want to kind of just share a little bit about kind of our",
    "start": "1186799",
    "end": "1193160"
  },
  {
    "text": "journey of evolving hyper parameter optimization at Uber uh starting with",
    "start": "1193160",
    "end": "1198440"
  },
  {
    "text": "you know how we build our first hyper parameter search service to kind of tune our models uh some of the challenges we",
    "start": "1198440",
    "end": "1204200"
  },
  {
    "text": "actually observe along the way as we try to tune larger and larger models and how bringing Ray actually help us",
    "start": "1204200",
    "end": "1210980"
  },
  {
    "text": "tackle some of these challenges so uh to start just let's just do a",
    "start": "1210980",
    "end": "1217160"
  },
  {
    "text": "quick recap right so hyper parameter search as you know many of you know is the process of choosing a set of optimal",
    "start": "1217160",
    "end": "1224299"
  },
  {
    "text": "values uh that maximizes your model's performance right but unfortunately our resources are finite so uh we cannot",
    "start": "1224299",
    "end": "1231860"
  },
  {
    "text": "just naively kind of try out every single combination of them uh so we started with you know the",
    "start": "1231860",
    "end": "1237200"
  },
  {
    "text": "standard grid search and random search but move but very quickly we move towards more Basin based approaches as",
    "start": "1237200",
    "end": "1242660"
  },
  {
    "text": "search base starts to become more complex and evaluate evaluating each hyper parameter combination which we",
    "start": "1242660",
    "end": "1248960"
  },
  {
    "text": "often call it a trial within a study uh becomes more and more expensive so essentially we have to train and",
    "start": "1248960",
    "end": "1255260"
  },
  {
    "text": "evaluate a model for every every trial which means that you know as we continue to scale our model uh this process is",
    "start": "1255260",
    "end": "1262220"
  },
  {
    "text": "only going to be uh you know more and more costly both in terms of time and computes fast forward to 2018 uh we built",
    "start": "1262220",
    "end": "1269299"
  },
  {
    "text": "autotune as our first black box optimization uh service to kind of scalably guide all the tuning process at",
    "start": "1269299",
    "end": "1276140"
  },
  {
    "text": "Uber more intelligently and on a high level our first autotune",
    "start": "1276140",
    "end": "1284059"
  },
  {
    "text": "workflow kind of looks a bit like this right so I want to tune a job so I'll add I'll go to Michelangelo kick off a",
    "start": "1284059",
    "end": "1291020"
  },
  {
    "text": "hybrid parameter search job which will then create a study autotune will then return a bunch of a trial suggestions",
    "start": "1291020",
    "end": "1298580"
  },
  {
    "text": "which are essentially just hyper parameter values for us to try my current job would then take those",
    "start": "1298580",
    "end": "1303919"
  },
  {
    "text": "values to train the model evaluate it and send the results back to autotune to basically let it know whether you know",
    "start": "1303919",
    "end": "1310220"
  },
  {
    "text": "this the previous suggestion was like what's a good recommendation or not right and based on that auto-tune will",
    "start": "1310220",
    "end": "1315919"
  },
  {
    "text": "send more suggestions and then we'll keep repeating this process until we find the best candidates",
    "start": "1315919",
    "end": "1321759"
  },
  {
    "text": "but the underlying control plane of the system is quite similar but there's actually a few differences or important",
    "start": "1321919",
    "end": "1328880"
  },
  {
    "text": "details that I would like to call out right so first uh after Michelangelo kind of creates a study the request",
    "start": "1328880",
    "end": "1335360"
  },
  {
    "text": "actually gets rerouted to our workflow service which will then materialize a static logical and physical plan of how",
    "start": "1335360",
    "end": "1341840"
  },
  {
    "text": "the trial should be scheduled and executed different trials would then be scheduled",
    "start": "1341840",
    "end": "1347600"
  },
  {
    "text": "and launched on different distributed clusters and depending on our logical plan we might uh fuse one or two trials",
    "start": "1347600",
    "end": "1355039"
  },
  {
    "text": "within a single uh to be executed on the same cluster another thing we want to call out is",
    "start": "1355039",
    "end": "1361340"
  },
  {
    "text": "that to ensure resource isolation and loose coupling between the autotune service and the library we the auditing",
    "start": "1361340",
    "end": "1368720"
  },
  {
    "text": "workers actually will launch Docker containers on different resource pools uh in order to generate suggestions",
    "start": "1368720",
    "end": "1376059"
  },
  {
    "text": "so uh so that was the first version of our auditing service right and that was actually enough to get us to our first",
    "start": "1376520",
    "end": "1382640"
  },
  {
    "text": "tens of millions of Trials uh but we quickly hit some bottlenecks so in the next version we we added a few",
    "start": "1382640",
    "end": "1388880"
  },
  {
    "text": "improvements uh first we added a dynamic control flow and just-in-time compilation to our static ml workflows",
    "start": "1388880",
    "end": "1395539"
  },
  {
    "text": "this allows us to dynamically change the study execution based on runtime data evaluation",
    "start": "1395539",
    "end": "1401900"
  },
  {
    "text": "uh this is super helpful actually for us to say introduce some kind of custom retry Logic for a specific failed trial",
    "start": "1401900",
    "end": "1408320"
  },
  {
    "text": "or some early stopping mechanism uh to to globally stop study if it's no longer",
    "start": "1408320",
    "end": "1413600"
  },
  {
    "text": "improving and combined with some simple meta learning strategies on historical studies to kind of extrapolate and and",
    "start": "1413600",
    "end": "1420919"
  },
  {
    "text": "warm start future studies we're actually able to dramatically reduce improve the",
    "start": "1420919",
    "end": "1426320"
  },
  {
    "text": "runtime efficiency of all our Studies by about 50 percent uh another thing we actually wanted to",
    "start": "1426320",
    "end": "1432919"
  },
  {
    "text": "improve was actually the latency of General at the end-to-end latency of generating kind of each trial suggestion",
    "start": "1432919",
    "end": "1439220"
  },
  {
    "text": "so so basically we uh we gradually move away from scheduling and polling uh",
    "start": "1439220",
    "end": "1444860"
  },
  {
    "text": "auditing Dockers towards more inline parallel execution uh this actually dramatically reduced",
    "start": "1444860",
    "end": "1451039"
  },
  {
    "text": "the number of dangling studies that we have at each single time and it's super actually like useful for users who want",
    "start": "1451039",
    "end": "1456679"
  },
  {
    "text": "to say just spin up 100 trials locally on their machine um and and just do try out their",
    "start": "1456679",
    "end": "1462200"
  },
  {
    "text": "workflow quickly before pushing that to the remote cluster but again as we start to",
    "start": "1462200",
    "end": "1469720"
  },
  {
    "text": "tune larger and more complex models especially for deep learning models",
    "start": "1469720",
    "end": "1474799"
  },
  {
    "text": "um new challenges arise right so first we real as each trial takes longer to",
    "start": "1474799",
    "end": "1480679"
  },
  {
    "text": "train we realize we actually need an even more aggressive early stopping than before right so um so instead of",
    "start": "1480679",
    "end": "1487220"
  },
  {
    "text": "actually waiting for each trial to actually complete before we make more trial suggestions they're actually",
    "start": "1487220",
    "end": "1492980"
  },
  {
    "text": "successive having techniques that can monitor the progress of every ongoing parallel trials after every",
    "start": "1492980",
    "end": "1498260"
  },
  {
    "text": "accelerations and then prune the candidates that actually are less promising and this actually gave us a",
    "start": "1498260",
    "end": "1504140"
  },
  {
    "text": "way to say guarantee to finish a study within a fixed budget",
    "start": "1504140",
    "end": "1509860"
  },
  {
    "text": "um and uh and next I think for dynamic schedules we realized that for deep",
    "start": "1509960",
    "end": "1516559"
  },
  {
    "text": "learning models specifically is super important for us to actually change the hyper parameter values throughout the",
    "start": "1516559",
    "end": "1522440"
  },
  {
    "text": "training process in order for us to get the state-of-the-art performance what this means for the hybrid parameter",
    "start": "1522440",
    "end": "1527600"
  },
  {
    "text": "search process now is that we actually have to find a schedule of how the each",
    "start": "1527600",
    "end": "1532940"
  },
  {
    "text": "of the higher parameter values need to change after every iteration instead of just finding a fixed set of",
    "start": "1532940",
    "end": "1538820"
  },
  {
    "text": "optimal values that can be used throughout the entire training process and and finally we also want to make",
    "start": "1538820",
    "end": "1546020"
  },
  {
    "text": "sure that we can um elastically scale our workers in Auto scaling environments to maximize our",
    "start": "1546020",
    "end": "1552740"
  },
  {
    "text": "training throughputs uh convergence and minimize costs right so for example this could be if we want to gradually",
    "start": "1552740",
    "end": "1559039"
  },
  {
    "text": "increase the batch size over time how many workers can we actually allow to freely kind of enter and exit the",
    "start": "1559039",
    "end": "1564860"
  },
  {
    "text": "cluster without actually impacting the training throughput and then obviously we also need some",
    "start": "1564860",
    "end": "1570500"
  },
  {
    "text": "kind of like compute framework that allows for that flexibility so for our next version of our autotune",
    "start": "1570500",
    "end": "1577700"
  },
  {
    "text": "I think one core piece that we actually have to incorporate is some kind of like Central orchestrator that has full visibility and granular control over uh",
    "start": "1577700",
    "end": "1585799"
  },
  {
    "text": "how each of the trial needs to actually happen right um so uh and while actually maintaining",
    "start": "1585799",
    "end": "1593000"
  },
  {
    "text": "very very minimal overhead this is super important because we want to make sure that each child suggestion still happens",
    "start": "1593000",
    "end": "1599360"
  },
  {
    "text": "super quickly so that a users can actually quickly try out try their their workflow locally and push it to the",
    "start": "1599360",
    "end": "1606020"
  },
  {
    "text": "remote cluster without much friction um and uh another thing is that we want",
    "start": "1606020",
    "end": "1612860"
  },
  {
    "text": "to make sure that the we decouple kind of the the interface and the logic and the abstractions between a child",
    "start": "1612860",
    "end": "1618620"
  },
  {
    "text": "generation and suggestion so again we can quickly try out new techniques like Asha and population Based training",
    "start": "1618620",
    "end": "1625700"
  },
  {
    "text": "and as it turns out these are actually essentially problems that region can help us address",
    "start": "1625700",
    "end": "1630740"
  },
  {
    "text": "but before we can actually incorporate retune uh we actually have to first move our the common uh trainers distributed",
    "start": "1630740",
    "end": "1637940"
  },
  {
    "text": "trainers at Uber like actually boost or deal Frameworks on horovod onto Ray as a universe compute layer",
    "start": "1637940",
    "end": "1644659"
  },
  {
    "text": "uh and we we introduced elasticity uh to actually boost and Harvard and Ray uh to",
    "start": "1644659",
    "end": "1650059"
  },
  {
    "text": "allow distributor training to kind of scale the number of workers uh throughout the training process and as part of this initial work we kind",
    "start": "1650059",
    "end": "1656960"
  },
  {
    "text": "of solve the problem of fusing the computer environment between spark and Ray and for a deal specifically we also",
    "start": "1656960",
    "end": "1664580"
  },
  {
    "text": "have to build integration between Harvard and retune so that you can start",
    "start": "1664580",
    "end": "1669919"
  },
  {
    "text": "trials using reactors with each reactor actually spinning up elastic Hardware workers",
    "start": "1669919",
    "end": "1675980"
  },
  {
    "text": "so we shifted the autotunes kind of internal State machine logic more towards into Ray and updated our",
    "start": "1675980",
    "end": "1682279"
  },
  {
    "text": "workflow compilation logic to have a generate a unified workflow for our training and tuning workflow",
    "start": "1682279",
    "end": "1688460"
  },
  {
    "text": "we can now basically essentially run an entire study uh within a single array cluster which will help us avoid the",
    "start": "1688460",
    "end": "1694700"
  },
  {
    "text": "need to kind of like maintain multiple spark clusters for parallelism avoid the you know the scheduling the load and as",
    "start": "1694700",
    "end": "1700880"
  },
  {
    "text": "well as the data loading and shuffling costs into each distributed workers for each trial um",
    "start": "1700880",
    "end": "1706159"
  },
  {
    "text": "and uh we also connected Ray tune uh to our checkpoint manager to have efficient",
    "start": "1706159",
    "end": "1711380"
  },
  {
    "text": "trial recovery and we actually expanded our cross-study services within our auditing service to better warm start",
    "start": "1711380",
    "end": "1718159"
  },
  {
    "text": "every single rating session uh so retune can now actually allows it can efficiently manage the end-to-end oh",
    "start": "1718159",
    "end": "1724640"
  },
  {
    "text": "sorry I actually didn't click uh rating can actually manage the end-to-end kind",
    "start": "1724640",
    "end": "1729860"
  },
  {
    "text": "of execution of every single study but across different",
    "start": "1729860",
    "end": "1735740"
  },
  {
    "text": "studies our warm start services our cross-study services were actually kind of manage intelligence across studies",
    "start": "1735740",
    "end": "1743659"
  },
  {
    "text": "and this is super important as we start to kind of pair our retuning strategies with stuff like incremental training and",
    "start": "1743659",
    "end": "1749120"
  },
  {
    "text": "transfer learning as we continue to scale deep learning so now let's uh take a look at some",
    "start": "1749120",
    "end": "1755659"
  },
  {
    "text": "results right so uh compared to our previous uh autotune as a baseline the",
    "start": "1755659",
    "end": "1761120"
  },
  {
    "text": "first row actually highlights the efficiency efficiency improvements simply from adopting some aggressive",
    "start": "1761120",
    "end": "1767600"
  },
  {
    "text": "more aggressive early stocking strategies like Asha so this is roughly measured by the total number of iterations and trials that we",
    "start": "1767600",
    "end": "1774080"
  },
  {
    "text": "can prune from the algorithm and from this we already see a fork speed up on",
    "start": "1774080",
    "end": "1779360"
  },
  {
    "text": "top of our previous early stopping strategies and on top of that on the second row uh",
    "start": "1779360",
    "end": "1784940"
  },
  {
    "text": "just for by running everything on a single array cluster and saving the cost of managing multiple spark cluster data",
    "start": "1784940",
    "end": "1790220"
  },
  {
    "text": "shuffling and loading we will see another 2x speed up on top of just using uh at least a more aggressive early",
    "start": "1790220",
    "end": "1796880"
  },
  {
    "text": "stopping and we're able to actually achieve all these results without actually compromising uh the performance of the",
    "start": "1796880",
    "end": "1804080"
  },
  {
    "text": "models right which is actually kind of important for for tuning tuning job uh the model improvement with us is",
    "start": "1804080",
    "end": "1810799"
  },
  {
    "text": "actually a bit lower which is fully understandable because we're actually applying a very very aggressive early stopping so essentially what we're doing",
    "start": "1810799",
    "end": "1817580"
  },
  {
    "text": "here is trading off you know full Explorations versus uh you know some kind of like tuning efficiency",
    "start": "1817580",
    "end": "1825278"
  },
  {
    "text": "and other than performance because of how Ray tune can actually decouple and",
    "start": "1826159",
    "end": "1831440"
  },
  {
    "text": "then expose the different logic of trial generation suggestion uh we actually now",
    "start": "1831440",
    "end": "1836539"
  },
  {
    "text": "can inject a custom acquisition functions to say uh talk to our feature",
    "start": "1836539",
    "end": "1841880"
  },
  {
    "text": "store or model libraries to kind of expand the the and parametrize kind of",
    "start": "1841880",
    "end": "1847760"
  },
  {
    "text": "the the search base Beyond just hyper parameters into architectures techniques and mod and feature transformations",
    "start": "1847760",
    "end": "1854600"
  },
  {
    "text": "I'm also very excited by Rey's new AI runtime which also included a new tunable data set concept which is very",
    "start": "1854600",
    "end": "1860960"
  },
  {
    "text": "very relevant for this joint exploration work and finally just to wrap up a couple of",
    "start": "1860960",
    "end": "1867260"
  },
  {
    "text": "things we're also actively exploring and would like to share some results soon is how to do parallel X4 exploits with",
    "start": "1867260",
    "end": "1873679"
  },
  {
    "text": "Dynamic resource allocation which means that we can potentially go even faster by just diverting and redirecting",
    "start": "1873679",
    "end": "1880220"
  },
  {
    "text": "resources to more promising candidates along the way we also want to continue to improve our",
    "start": "1880220",
    "end": "1885980"
  },
  {
    "text": "cross study services to better use stuff like learning curve extrapolations to extrapolate from our like the 100",
    "start": "1885980",
    "end": "1892700"
  },
  {
    "text": "millions of trials that we have done already to better guide our future tuning studies and finally we want to",
    "start": "1892700",
    "end": "1898940"
  },
  {
    "text": "kind of extend uh you know just abstract out all the good stuff from retune to generically support other complex",
    "start": "1898940",
    "end": "1904580"
  },
  {
    "text": "training patterns that Uber like you know model partitioning or modern sampling uh thank you and uh I'll hand it back to",
    "start": "1904580",
    "end": "1911480"
  },
  {
    "text": "Shu for closing remarks",
    "start": "1911480",
    "end": "1914620"
  },
  {
    "text": "so I'll be real quick here so number one is that the as you can see over the past",
    "start": "1917240",
    "end": "1923120"
  },
  {
    "text": "six years we went to three generations uh this is because the ml platform technology involved really really",
    "start": "1923120",
    "end": "1929059"
  },
  {
    "text": "rapidly and then with the technology we have evolved our business use case also",
    "start": "1929059",
    "end": "1935179"
  },
  {
    "text": "is they are increasing demanding in our experience we really find that Ray provides the best sweet spot in terms of",
    "start": "1935179",
    "end": "1942559"
  },
  {
    "text": "giving us the flexibility and also the the strong ml ecosystem for us to move",
    "start": "1942559",
    "end": "1949279"
  },
  {
    "text": "forward and finally I think this is the most important point is that the rating is really really awesome we have great",
    "start": "1949279",
    "end": "1956299"
  },
  {
    "text": "collaboration with the ray team and especially Richard uh Chen Chris Kai Jen",
    "start": "1956299",
    "end": "1962360"
  },
  {
    "text": "and J I think without these folks working with us along the way we will",
    "start": "1962360",
    "end": "1967820"
  },
  {
    "text": "encounter much more challenging before we part I also want to point out these are the blog posts and Conference",
    "start": "1967820",
    "end": "1974779"
  },
  {
    "text": "appearance we had in the past about Michelangelo and this go way deeper than the 30 minutes hot we have here so",
    "start": "1974779",
    "end": "1981559"
  },
  {
    "text": "please check it out and do a deep dive I'm happy to chat off offline with any",
    "start": "1981559",
    "end": "1986659"
  },
  {
    "text": "of all of you thank you [Applause]",
    "start": "1986659",
    "end": "1996858"
  }
]