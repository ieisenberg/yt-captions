[
  {
    "text": "thanks thanks for the introduction M hi",
    "start": "3480",
    "end": "5799"
  },
  {
    "text": "everyone welcome this is cha soft",
    "start": "5799",
    "end": "8200"
  },
  {
    "text": "engineer in Uber micro team in this talk",
    "start": "8200",
    "end": "11679"
  },
  {
    "text": "we will briefly introduce how Uber micro",
    "start": "11679",
    "end": "14120"
  },
  {
    "text": "platform Embrace large Lang model apps",
    "start": "14120",
    "end": "16880"
  },
  {
    "text": "with support of R this work is currently",
    "start": "16880",
    "end": "19480"
  },
  {
    "text": "being actively developed Uber by mango",
    "start": "19480",
    "end": "22039"
  },
  {
    "text": "team engineer including both myself",
    "start": "22039",
    "end": "24800"
  },
  {
    "text": "Anette and",
    "start": "24800",
    "end": "26199"
  },
  {
    "text": " firstly I'll introduce how we bridge",
    "start": "26199",
    "end": "29679"
  },
  {
    "text": "the gap between mangio and large Lang",
    "start": "29679",
    "end": "32200"
  },
  {
    "text": "model Ops from a high",
    "start": "32200",
    "end": "33800"
  },
  {
    "text": "level so let's look at how mangio looks",
    "start": "33800",
    "end": "36800"
  },
  {
    "text": "like before large Lang models micr is",
    "start": "36800",
    "end": "39600"
  },
  {
    "text": "Uber's internal n machine learning",
    "start": "39600",
    "end": "41440"
  },
  {
    "text": "platform it enables machine learning",
    "start": "41440",
    "end": "43480"
  },
  {
    "text": "developers to prepare data TR and",
    "start": "43480",
    "end": "46239"
  },
  {
    "text": "evaluate models deploy models make",
    "start": "46239",
    "end": "48840"
  },
  {
    "text": "predictions set AO ret Pipeline and",
    "start": "48840",
    "end": "51640"
  },
  {
    "text": "monitor model performance in",
    "start": "51640",
    "end": "54359"
  },
  {
    "text": "production so M started to integrate R",
    "start": "54359",
    "end": "58519"
  },
  {
    "text": "since 2021 and as of today both deep",
    "start": "58519",
    "end": "62120"
  },
  {
    "text": "learning training jobs and ex training",
    "start": "62120",
    "end": "64119"
  },
  {
    "text": "jobs are running on clusters in",
    "start": "64119",
    "end": "66280"
  },
  {
    "text": "production we have seen a few important",
    "start": "66280",
    "end": "68560"
  },
  {
    "text": "benefits of soltion three such as we",
    "start": "68560",
    "end": "71479"
  },
  {
    "text": "provides powerful prelimi lives for",
    "start": "71479",
    "end": "73560"
  },
  {
    "text": "distrib computing including decorators",
    "start": "73560",
    "end": "76439"
  },
  {
    "text": "for remote task and API for distributed",
    "start": "76439",
    "end": "79280"
  },
  {
    "text": "memory store and also r support",
    "start": "79280",
    "end": "81240"
  },
  {
    "text": "hetrogeneous cluster cluster Auto",
    "start": "81240",
    "end": "83240"
  },
  {
    "text": "skating and Lifey and life cycle",
    "start": "83240",
    "end": "85400"
  },
  {
    "text": "management which is very critical for",
    "start": "85400",
    "end": "87119"
  },
  {
    "text": "tuning workflows and also a lot of",
    "start": "87119",
    "end": "89320"
  },
  {
    "text": "popular learning library have we",
    "start": "89320",
    "end": "91200"
  },
  {
    "text": "integrate into the ecosystem and this",
    "start": "91200",
    "end": "93560"
  },
  {
    "text": "allows Mico to pick up the lowest",
    "start": "93560",
    "end": "95759"
  },
  {
    "text": "handing fruits uh for we also provides",
    "start": "95759",
    "end": "99200"
  },
  {
    "text": "first class kubernetes support this",
    "start": "99200",
    "end": "101200"
  },
  {
    "text": "aligns with wuber ongoing efforts of",
    "start": "101200",
    "end": "103520"
  },
  {
    "text": "moving our Computing resources into",
    "start": "103520",
    "end": "105320"
  },
  {
    "text": "kubernetes cluster comparing with r and",
    "start": "105320",
    "end": "108320"
  },
  {
    "text": "before 2021 we have used and explore",
    "start": "108320",
    "end": "111000"
  },
  {
    "text": "other Alternatives in the past such as",
    "start": "111000",
    "end": "113159"
  },
  {
    "text": "MPR and Spark for NPI it is",
    "start": "113159",
    "end": "115600"
  },
  {
    "text": "straightforward to spawn data",
    "start": "115600",
    "end": "117119"
  },
  {
    "text": "parallelism jobs from machine learning",
    "start": "117119",
    "end": "118960"
  },
  {
    "text": "workflows but it lacks support for",
    "start": "118960",
    "end": "121000"
  },
  {
    "text": "resource management for Michelangelo was",
    "start": "121000",
    "end": "123600"
  },
  {
    "text": "doing spark machine learning pipelines",
    "start": "123600",
    "end": "125479"
  },
  {
    "text": "for training before 2021 and the spark",
    "start": "125479",
    "end": "128239"
  },
  {
    "text": "estimator class is consistent for",
    "start": "128239",
    "end": "130640"
  },
  {
    "text": "offline training jobs and online",
    "start": "130640",
    "end": "132319"
  },
  {
    "text": "prediction jobs but we encounter a few",
    "start": "132319",
    "end": "134720"
  },
  {
    "text": "challenges like separating CPU GPU jobs",
    "start": "134720",
    "end": "137360"
  },
  {
    "text": "and life cycle management and also mro",
    "start": "137360",
    "end": "140319"
  },
  {
    "text": "team has spent a lot of effort to make",
    "start": "140319",
    "end": "142120"
  },
  {
    "text": "deep learning framework like tenso and",
    "start": "142120",
    "end": "144680"
  },
  {
    "text": "pyto be compatible with spark estimator",
    "start": "144680",
    "end": "147200"
  },
  {
    "text": "class this part of work has been open",
    "start": "147200",
    "end": "149400"
  },
  {
    "text": "source to hard G rle on spark",
    "start": "149400",
    "end": "153080"
  },
  {
    "text": "folder large L model have recently gain",
    "start": "153080",
    "end": "156480"
  },
  {
    "text": "a lot of attention since the release of",
    "start": "156480",
    "end": "159040"
  },
  {
    "text": "chat GPT around the end of 2022 Uber has",
    "start": "159040",
    "end": "162440"
  },
  {
    "text": "started to apply large langage models to",
    "start": "162440",
    "end": "164599"
  },
  {
    "text": "a few important use cases including both",
    "start": "164599",
    "end": "167239"
  },
  {
    "text": "pre tuning and fine tuning scenarios uh",
    "start": "167239",
    "end": "170120"
  },
  {
    "text": "these geni Ed cases including internal",
    "start": "170120",
    "end": "172800"
  },
  {
    "text": "developer facing case like code C pilot",
    "start": "172800",
    "end": "175360"
  },
  {
    "text": "and external customer facing case like C",
    "start": "175360",
    "end": "177920"
  },
  {
    "text": "customer service chatbot development",
    "start": "177920",
    "end": "180879"
  },
  {
    "text": "platform Engineers are developing coding",
    "start": "180879",
    "end": "183120"
  },
  {
    "text": "assistance to by pre-training open",
    "start": "183120",
    "end": "185640"
  },
  {
    "text": "source model like Star coder and cod",
    "start": "185640",
    "end": "187480"
  },
  {
    "text": "Lama using Uber's internal code repos",
    "start": "187480",
    "end": "190640"
  },
  {
    "text": "such coding assistant can provide",
    "start": "190640",
    "end": "192640"
  },
  {
    "text": "autofield suggesting as well as well as",
    "start": "192640",
    "end": "195280"
  },
  {
    "text": "test suggesting to improve developer",
    "start": "195280",
    "end": "198640"
  },
  {
    "text": "productivity custom Obsession team have",
    "start": "198640",
    "end": "201000"
  },
  {
    "text": "launched generative AI to support three",
    "start": "201000",
    "end": "203440"
  },
  {
    "text": "cases contact summarization summarizing",
    "start": "203440",
    "end": "206720"
  },
  {
    "text": "contacts faster and more accurate to",
    "start": "206720",
    "end": "208760"
  },
  {
    "text": "save agents time fnq chat Bard response",
    "start": "208760",
    "end": "212159"
  },
  {
    "text": "question in a conversational manner",
    "start": "212159",
    "end": "214040"
  },
  {
    "text": "instead of just giving help articles",
    "start": "214040",
    "end": "216840"
  },
  {
    "text": "response generation provide more",
    "start": "216840",
    "end": "218680"
  },
  {
    "text": "personalized and Inc contact responses",
    "start": "218680",
    "end": "221000"
  },
  {
    "text": "instead of using static save the",
    "start": "221000",
    "end": "224040"
  },
  {
    "text": "templates in order to arding gen cases",
    "start": "224040",
    "end": "227799"
  },
  {
    "text": "to my it require M to support large",
    "start": "227799",
    "end": "230640"
  },
  {
    "text": "language model Ops to Encompass the",
    "start": "230640",
    "end": "232840"
  },
  {
    "text": "practices techniques and tools used in",
    "start": "232840",
    "end": "235480"
  },
  {
    "text": "production environments the common",
    "start": "235480",
    "end": "237560"
  },
  {
    "text": "components of large language model Ops",
    "start": "237560",
    "end": "239760"
  },
  {
    "text": "including exploratory data analysis data",
    "start": "239760",
    "end": "242840"
  },
  {
    "text": "preparation uh to note that data",
    "start": "242840",
    "end": "245079"
  },
  {
    "text": "preparation is one of the most",
    "start": "245079",
    "end": "246439"
  },
  {
    "text": "challenging stages our uh model",
    "start": "246439",
    "end": "249239"
  },
  {
    "text": "scientist have encountered today when",
    "start": "249239",
    "end": "250760"
  },
  {
    "text": "applying large language models proper",
    "start": "250760",
    "end": "252879"
  },
  {
    "text": "engineering model pruning model fine",
    "start": "252879",
    "end": "255400"
  },
  {
    "text": "tuning model review of governance model",
    "start": "255400",
    "end": "257680"
  },
  {
    "text": "inference and model monitoring with",
    "start": "257680",
    "end": "259919"
  },
  {
    "text": "human feedback on one hand users can",
    "start": "259919",
    "end": "262479"
  },
  {
    "text": "choose to use close Source large n model",
    "start": "262479",
    "end": "265000"
  },
  {
    "text": "Ops platform and onetop solution on the",
    "start": "265000",
    "end": "267840"
  },
  {
    "text": "other hand user can also use popular",
    "start": "267840",
    "end": "270120"
  },
  {
    "text": "open source Solutions such as Ray",
    "start": "270120",
    "end": "272240"
  },
  {
    "text": "hugging face deep speed to fill their",
    "start": "272240",
    "end": "274960"
  },
  {
    "text": "own",
    "start": "274960",
    "end": "276039"
  },
  {
    "text": "needs both open source and cl Source",
    "start": "276039",
    "end": "278840"
  },
  {
    "text": "Solutions have their own advantages and",
    "start": "278840",
    "end": "281120"
  },
  {
    "text": "disadvantages in common sense open",
    "start": "281120",
    "end": "283560"
  },
  {
    "text": "source Solutions have good Community",
    "start": "283560",
    "end": "285199"
  },
  {
    "text": "Support the implementation is",
    "start": "285199",
    "end": "286880"
  },
  {
    "text": "transparent because the code accessible",
    "start": "286880",
    "end": "289000"
  },
  {
    "text": "and the cost usually is free but it come",
    "start": "289000",
    "end": "291680"
  },
  {
    "text": "with disant advantages such as lack of",
    "start": "291680",
    "end": "294320"
  },
  {
    "text": "technical support the Community Driven",
    "start": "294320",
    "end": "296759"
  },
  {
    "text": "approach can lead to faster development",
    "start": "296759",
    "end": "299000"
  },
  {
    "text": "and innovation ation but it also can",
    "start": "299000",
    "end": "301039"
  },
  {
    "text": "lead to lack of quality control it has",
    "start": "301039",
    "end": "303520"
  },
  {
    "text": "fragmentation issues since it's hard to",
    "start": "303520",
    "end": "306280"
  },
  {
    "text": "uh ensure everyone is using the same",
    "start": "306280",
    "end": "308039"
  },
  {
    "text": "version and with the same features uh I",
    "start": "308039",
    "end": "310360"
  },
  {
    "text": "want to C here this is common sense for",
    "start": "310360",
    "end": "312479"
  },
  {
    "text": "open source Solutions uh with the an",
    "start": "312479",
    "end": "315320"
  },
  {
    "text": "scale team support with r we haven't run",
    "start": "315320",
    "end": "317360"
  },
  {
    "text": "into any of these issues with yet uh",
    "start": "317360",
    "end": "320520"
  },
  {
    "text": "meanwhile closer solution are good at",
    "start": "320520",
    "end": "322720"
  },
  {
    "text": "quality control providing technical",
    "start": "322720",
    "end": "324800"
  },
  {
    "text": "support and better security check but it",
    "start": "324800",
    "end": "327400"
  },
  {
    "text": "Miss the transparency issue Miss trans",
    "start": "327400",
    "end": "329560"
  },
  {
    "text": "cury because the code is not accessible",
    "start": "329560",
    "end": "331600"
  },
  {
    "text": "and it requires to pay for a license to",
    "start": "331600",
    "end": "333680"
  },
  {
    "text": "use to use the software and The Limited",
    "start": "333680",
    "end": "336280"
  },
  {
    "text": "feature that provided by the owners by",
    "start": "336280",
    "end": "338440"
  },
  {
    "text": "consider Uber's own business needs and",
    "start": "338440",
    "end": "340440"
  },
  {
    "text": "Engineering requirements M will chooses",
    "start": "340440",
    "end": "343000"
  },
  {
    "text": "open source Solutions build lar model",
    "start": "343000",
    "end": "346080"
  },
  {
    "text": "apps so we are facing a few challenges",
    "start": "346080",
    "end": "348840"
  },
  {
    "text": "supporting large language mod model Ops",
    "start": "348840",
    "end": "351120"
  },
  {
    "text": "on Mango including existing internal",
    "start": "351120",
    "end": "353680"
  },
  {
    "text": "limitations and ex external integration",
    "start": "353680",
    "end": "356800"
  },
  {
    "text": "challenges the in internal limitations",
    "start": "356800",
    "end": "360720"
  },
  {
    "text": "is about how to Ard large number model",
    "start": "360720",
    "end": "362880"
  },
  {
    "text": "on M M lacks the model paradism support",
    "start": "362880",
    "end": "366639"
  },
  {
    "text": "which limits the number of the model",
    "start": "366639",
    "end": "368160"
  },
  {
    "text": "parameter by the GPU physical memory M",
    "start": "368160",
    "end": "370960"
  },
  {
    "text": "also don't have a good endtoend support",
    "start": "370960",
    "end": "372960"
  },
  {
    "text": "for huging face py models especially",
    "start": "372960",
    "end": "375680"
  },
  {
    "text": "tokenization part in the past we have",
    "start": "375680",
    "end": "378199"
  },
  {
    "text": "observed a few issues blocking huging",
    "start": "378199",
    "end": "380479"
  },
  {
    "text": "face model to be servable in online",
    "start": "380479",
    "end": "382319"
  },
  {
    "text": "service environments we're also we're",
    "start": "382319",
    "end": "385039"
  },
  {
    "text": "also facing a few inter integration",
    "start": "385039",
    "end": "387039"
  },
  {
    "text": "challenges with Uber's computer",
    "start": "387039",
    "end": "388880"
  },
  {
    "text": "architecture such as the storage part we",
    "start": "388880",
    "end": "391800"
  },
  {
    "text": "want how to connect Uber internal",
    "start": "391800",
    "end": "393880"
  },
  {
    "text": "storage like Hado and terab when",
    "start": "393880",
    "end": "396240"
  },
  {
    "text": "training opensource SL models ter here",
    "start": "396240",
    "end": "399599"
  },
  {
    "text": "is a Uber centralized solution to St",
    "start": "399599",
    "end": "401680"
  },
  {
    "text": "blob data and the hardware part how to",
    "start": "401680",
    "end": "404520"
  },
  {
    "text": "distribute a large L model training job",
    "start": "404520",
    "end": "406440"
  },
  {
    "text": "to most powerful gpus inhouse on the",
    "start": "406440",
    "end": "409000"
  },
  {
    "text": "cloud the software part how to integrate",
    "start": "409000",
    "end": "411800"
  },
  {
    "text": "multiple open source libraries with wber",
    "start": "411800",
    "end": "414120"
  },
  {
    "text": "production containers Beyond this since",
    "start": "414120",
    "end": "416639"
  },
  {
    "text": "lar language model training involves",
    "start": "416639",
    "end": "419080"
  },
  {
    "text": "massive Computing resources and hours",
    "start": "419080",
    "end": "421479"
  },
  {
    "text": "and aile development environment to",
    "start": "421479",
    "end": "423680"
  },
  {
    "text": "accelerate both development and deing",
    "start": "423680",
    "end": "426199"
  },
  {
    "text": "purpose is",
    "start": "426199",
    "end": "427360"
  },
  {
    "text": "desired so we list a few open source",
    "start": "427360",
    "end": "429759"
  },
  {
    "text": "Library here as our major building",
    "start": "429759",
    "end": "432160"
  },
  {
    "text": "blocks for large language model op",
    "start": "432160",
    "end": "433960"
  },
  {
    "text": "Journey at Uber including Ray hugging",
    "start": "433960",
    "end": "437280"
  },
  {
    "text": "phas pyos deep speed TT and Tron INF",
    "start": "437280",
    "end": "441199"
  },
  {
    "text": "server for serving and B and and bit and",
    "start": "441199",
    "end": "443720"
  },
  {
    "text": "bites for",
    "start": "443720",
    "end": "445360"
  },
  {
    "text": "quantization in general Uber's end to",
    "start": "445360",
    "end": "448039"
  },
  {
    "text": "end machine learning Ops are by",
    "start": "448039",
    "end": "449879"
  },
  {
    "text": "integration R based large language model",
    "start": "449879",
    "end": "452440"
  },
  {
    "text": "training framework into micro Studio",
    "start": "452440",
    "end": "454840"
  },
  {
    "text": "micro studio is over the onstop solution",
    "start": "454840",
    "end": "457400"
  },
  {
    "text": "to build train deploy Monitor and debar",
    "start": "457400",
    "end": "460199"
  },
  {
    "text": "models many features to micr Studio can",
    "start": "460199",
    "end": "463080"
  },
  {
    "text": "be reused here and the rest of talk we",
    "start": "463080",
    "end": "465800"
  },
  {
    "text": "focus on the design of R Based training",
    "start": "465800",
    "end": "469000"
  },
  {
    "text": "framework uh our lar language model",
    "start": "469000",
    "end": "471360"
  },
  {
    "text": "training architecture is as show here",
    "start": "471360",
    "end": "473360"
  },
  {
    "text": "from top level ring API to bottom level",
    "start": "473360",
    "end": "476520"
  },
  {
    "text": "level Lial Library ring provider s API",
    "start": "476520",
    "end": "479720"
  },
  {
    "text": "to perform distribut training in pyto on",
    "start": "479720",
    "end": "481960"
  },
  {
    "text": "recluster in particular ring handle the",
    "start": "481960",
    "end": "484879"
  },
  {
    "text": "setup of distributed process group in",
    "start": "484879",
    "end": "486879"
  },
  {
    "text": "pyto enable user to run their training",
    "start": "486879",
    "end": "489360"
  },
  {
    "text": "script in data parallelism fashion",
    "start": "489360",
    "end": "491639"
  },
  {
    "text": "huging phas Transformer provides API to",
    "start": "491639",
    "end": "493960"
  },
  {
    "text": "downloads and train State off open",
    "start": "493960",
    "end": "496919"
  },
  {
    "text": "source Transformer based open source",
    "start": "496919",
    "end": "498720"
  },
  {
    "text": "models this capacity allows Uber",
    "start": "498720",
    "end": "501080"
  },
  {
    "text": "developers to efficiently work with the",
    "start": "501080",
    "end": "503800"
  },
  {
    "text": "most recent open source model with",
    "start": "503800",
    "end": "505599"
  },
  {
    "text": "minimum effort deep speed and a popular",
    "start": "505599",
    "end": "508599"
  },
  {
    "text": "deep learning optimizing software",
    "start": "508599",
    "end": "510240"
  },
  {
    "text": "supporting model parallelism for pyto",
    "start": "510240",
    "end": "512240"
  },
  {
    "text": "models we choose pyto as a deep learning",
    "start": "512240",
    "end": "514640"
  },
  {
    "text": "framework because most of us s open",
    "start": "514640",
    "end": "517479"
  },
  {
    "text": "source large langage models and",
    "start": "517479",
    "end": "519000"
  },
  {
    "text": "techniques are implementing in py niiko",
    "start": "519000",
    "end": "521640"
  },
  {
    "text": "implements the multi-gpu and multi node",
    "start": "521640",
    "end": "523919"
  },
  {
    "text": "communication optimization for NVIDIA",
    "start": "523919",
    "end": "526160"
  },
  {
    "text": "gpus and",
    "start": "526160",
    "end": "527640"
  },
  {
    "text": "networking so today large lry models are",
    "start": "527640",
    "end": "530399"
  },
  {
    "text": "train hour in a100 clusters as below",
    "start": "530399",
    "end": "533680"
  },
  {
    "text": "each host is equipped with 4 180 GB gpus",
    "start": "533680",
    "end": "537720"
  },
  {
    "text": "and two Intel I CP Gus in future we are",
    "start": "537720",
    "end": "540800"
  },
  {
    "text": "planning to extend this training to",
    "start": "540800",
    "end": "542480"
  },
  {
    "text": "Cloud provided",
    "start": "542480",
    "end": "544200"
  },
  {
    "text": "gpus we provision the recluster on a 100",
    "start": "544200",
    "end": "547720"
  },
  {
    "text": "GPU using my CL job controller as show",
    "start": "547720",
    "end": "550279"
  },
  {
    "text": "on the right of the slides feder R",
    "start": "550279",
    "end": "552600"
  },
  {
    "text": "scheduling scheme send job request to",
    "start": "552600",
    "end": "554880"
  },
  {
    "text": "multiple kubernetes cluster the job",
    "start": "554880",
    "end": "557160"
  },
  {
    "text": "request is a GPC call generated from end",
    "start": "557160",
    "end": "559760"
  },
  {
    "text": "to end workflows notebooks and CI",
    "start": "559760",
    "end": "562279"
  },
  {
    "text": "commands a dedicated cluster receiv",
    "start": "562279",
    "end": "565160"
  },
  {
    "text": "request and Spins up the r cluster using",
    "start": "565160",
    "end": "567320"
  },
  {
    "text": "open source KU operator in addition",
    "start": "567320",
    "end": "570320"
  },
  {
    "text": "elastic resource sharing is imp that so",
    "start": "570320",
    "end": "573079"
  },
  {
    "text": "we can reallocate available resources to",
    "start": "573079",
    "end": "575600"
  },
  {
    "text": "high demand resource Pro as show on the",
    "start": "575600",
    "end": "577440"
  },
  {
    "text": "left of the",
    "start": "577440",
    "end": "579120"
  },
  {
    "text": "slides we have built an interactive",
    "start": "579120",
    "end": "581880"
  },
  {
    "text": "development environment on recluster a",
    "start": "581880",
    "end": "584279"
  },
  {
    "text": "drup left environment is enabled this",
    "start": "584279",
    "end": "586560"
  },
  {
    "text": "allows our developers our model",
    "start": "586560",
    "end": "588399"
  },
  {
    "text": "scientists to add a source code and the",
    "start": "588399",
    "end": "590640"
  },
  {
    "text": "model configuration file on the Fly and",
    "start": "590640",
    "end": "592680"
  },
  {
    "text": "ruse the same Rec cluster to run",
    "start": "592680",
    "end": "594560"
  },
  {
    "text": "multiple jobs R dashboard is also enable",
    "start": "594560",
    "end": "597320"
  },
  {
    "text": "so we have better loging and system",
    "start": "597320",
    "end": "599120"
  },
  {
    "text": "monitoring",
    "start": "599120",
    "end": "600480"
  },
  {
    "text": "support now I'm giving the rest of time",
    "start": "600480",
    "end": "603000"
  },
  {
    "text": "to B to talk about the r Based training",
    "start": "603000",
    "end": "606200"
  },
  {
    "text": "design and integration in",
    "start": "606200",
    "end": "608519"
  },
  {
    "text": "details yeah",
    "start": "608519",
    "end": "611079"
  },
  {
    "text": "thanks yeah hi everyone uh my name is B",
    "start": "611079",
    "end": "614000"
  },
  {
    "text": "also from mangelo uh over AI team uh now",
    "start": "614000",
    "end": "617640"
  },
  {
    "text": "I will talk about uh um re uh based",
    "start": "617640",
    "end": "622000"
  },
  {
    "text": "trainer design and the integration with",
    "start": "622000",
    "end": "623920"
  },
  {
    "text": "a mechan existing",
    "start": "623920",
    "end": "627480"
  },
  {
    "text": "component so yeah so first of all let's",
    "start": "628040",
    "end": "631800"
  },
  {
    "text": "talk about the training pipeline design",
    "start": "631800",
    "end": "633839"
  },
  {
    "text": "so at the high level our training",
    "start": "633839",
    "end": "635560"
  },
  {
    "text": "pipeline design containing three stages",
    "start": "635560",
    "end": "638920"
  },
  {
    "text": "uh data loading distributed training and",
    "start": "638920",
    "end": "642000"
  },
  {
    "text": "the model uh checkpoint management so",
    "start": "642000",
    "end": "644720"
  },
  {
    "text": "let me first F uh on the distri training",
    "start": "644720",
    "end": "648040"
  },
  {
    "text": "part so we use to uh r t trainer uh to",
    "start": "648040",
    "end": "653079"
  },
  {
    "text": "start multiple reactors and uh in each",
    "start": "653079",
    "end": "656880"
  },
  {
    "text": "reactor uh we make sure the to process",
    "start": "656880",
    "end": "660120"
  },
  {
    "text": "uh uh group already configured for just",
    "start": "660120",
    "end": "662399"
  },
  {
    "text": "training and each reactor will do its",
    "start": "662399",
    "end": "665519"
  },
  {
    "text": "own uh training Loop and uh using hen",
    "start": "665519",
    "end": "669839"
  },
  {
    "text": "face trainer and hen data set and in hen",
    "start": "669839",
    "end": "673800"
  },
  {
    "text": "trainer we rely on keep speed to do",
    "start": "673800",
    "end": "676000"
  },
  {
    "text": "model paraly them and we rely on hiking",
    "start": "676000",
    "end": "679240"
  },
  {
    "text": "face accelerator to manager the",
    "start": "679240",
    "end": "681720"
  },
  {
    "text": "partition model States and do gradient",
    "start": "681720",
    "end": "686440"
  },
  {
    "text": "updates now let me talk about how we do",
    "start": "686440",
    "end": "689320"
  },
  {
    "text": "that distribute data loading so first of",
    "start": "689320",
    "end": "692000"
  },
  {
    "text": "all we ensure the uh uh each re actor",
    "start": "692000",
    "end": "696440"
  },
  {
    "text": "only work on its own data set including",
    "start": "696440",
    "end": "699040"
  },
  {
    "text": "train and Evel so the train and EV data",
    "start": "699040",
    "end": "701680"
  },
  {
    "text": "set do not mess with each other across",
    "start": "701680",
    "end": "705399"
  },
  {
    "text": "reactors uh so for uh data source we",
    "start": "705399",
    "end": "708480"
  },
  {
    "text": "support reading the larg data set from",
    "start": "708480",
    "end": "711240"
  },
  {
    "text": "Hadoop which very important for pre",
    "start": "711240",
    "end": "713760"
  },
  {
    "text": "training because pre training data set",
    "start": "713760",
    "end": "715519"
  },
  {
    "text": "is very large we also support reading",
    "start": "715519",
    "end": "717959"
  },
  {
    "text": "data set from blob usually for small",
    "start": "717959",
    "end": "720639"
  },
  {
    "text": "data set that's uh very important for",
    "start": "720639",
    "end": "723079"
  },
  {
    "text": "supervisor fine tune because supervisor",
    "start": "723079",
    "end": "725120"
  },
  {
    "text": "fine tune data set is usually small we",
    "start": "725120",
    "end": "727519"
  },
  {
    "text": "also support reading data set from Hing",
    "start": "727519",
    "end": "729600"
  },
  {
    "text": "phase that's very important for our",
    "start": "729600",
    "end": "732120"
  },
  {
    "text": "developers to testing the code and do",
    "start": "732120",
    "end": "735320"
  },
  {
    "text": "active uh development in terms of file",
    "start": "735320",
    "end": "738320"
  },
  {
    "text": "format we support pit because uh pqu",
    "start": "738320",
    "end": "741399"
  },
  {
    "text": "most time is a default format on hyy",
    "start": "741399",
    "end": "744279"
  },
  {
    "text": "system we also support uh Jon list uh",
    "start": "744279",
    "end": "747880"
  },
  {
    "text": "which most time using ter block uh for",
    "start": "747880",
    "end": "750880"
  },
  {
    "text": "tokenization we support streaming",
    "start": "750880",
    "end": "753000"
  },
  {
    "text": "tokenization basically tokenizing on the",
    "start": "753000",
    "end": "755040"
  },
  {
    "text": "fly during training that's important",
    "start": "755040",
    "end": "757320"
  },
  {
    "text": "because for large language model PR",
    "start": "757320",
    "end": "759279"
  },
  {
    "text": "training the data set usually is very",
    "start": "759279",
    "end": "761320"
  },
  {
    "text": "large people want to see how the",
    "start": "761320",
    "end": "763839"
  },
  {
    "text": "training goes on the Fly we also support",
    "start": "763839",
    "end": "766800"
  },
  {
    "text": "pre uh pre talization and local cach",
    "start": "766800",
    "end": "770800"
  },
  {
    "text": "that's most of the time used off of in",
    "start": "770800",
    "end": "773680"
  },
  {
    "text": "the supervis fine tune when the data set",
    "start": "773680",
    "end": "776079"
  },
  {
    "text": "is small and on ter blow",
    "start": "776079",
    "end": "780680"
  },
  {
    "text": "so let me talk about our ongoing effort",
    "start": "780680",
    "end": "783480"
  },
  {
    "text": "our model uh checkpoint management and",
    "start": "783480",
    "end": "785440"
  },
  {
    "text": "for tolerance so uh you know uh usually",
    "start": "785440",
    "end": "789320"
  },
  {
    "text": "the large L of training is very costly",
    "start": "789320",
    "end": "791240"
  },
  {
    "text": "and also time consuming you need to use",
    "start": "791240",
    "end": "793160"
  },
  {
    "text": "hundreds of gpus and train weeks to",
    "start": "793160",
    "end": "795920"
  },
  {
    "text": "months to get the final output so if",
    "start": "795920",
    "end": "798760"
  },
  {
    "text": "anything fail in the middle our",
    "start": "798760",
    "end": "800880"
  },
  {
    "text": "developer definitely don't want to",
    "start": "800880",
    "end": "802279"
  },
  {
    "text": "stting from uh everything from scratch",
    "start": "802279",
    "end": "805639"
  },
  {
    "text": "so instead they really want to find uh",
    "start": "805639",
    "end": "808160"
  },
  {
    "text": "the fail and resume from the uh",
    "start": "808160",
    "end": "810959"
  },
  {
    "text": "corresponding",
    "start": "810959",
    "end": "812360"
  },
  {
    "text": "checkpoint to uh uh Implement that we uh",
    "start": "812360",
    "end": "817000"
  },
  {
    "text": "periodically push the data snapshot and",
    "start": "817000",
    "end": "819760"
  },
  {
    "text": "the model checkpoint to our remote",
    "start": "819760",
    "end": "822519"
  },
  {
    "text": "storage on T blop and when the user want",
    "start": "822519",
    "end": "825760"
  },
  {
    "text": "to resume from the fail point we ensure",
    "start": "825760",
    "end": "829000"
  },
  {
    "text": "each reactor could download the data",
    "start": "829000",
    "end": "831279"
  },
  {
    "text": "snapshot information and the model",
    "start": "831279",
    "end": "833480"
  },
  {
    "text": "checkpoint including model uh model",
    "start": "833480",
    "end": "835680"
  },
  {
    "text": "parameter and the checkpoint then the",
    "start": "835680",
    "end": "838759"
  },
  {
    "text": "they can Contin uh",
    "start": "838759",
    "end": "842079"
  },
  {
    "text": "training so now let me talk about our",
    "start": "842079",
    "end": "844839"
  },
  {
    "text": "loging and profiling system so mechul",
    "start": "844839",
    "end": "847560"
  },
  {
    "text": "already have a loging system with",
    "start": "847560",
    "end": "849480"
  },
  {
    "text": "comment so we are able to integrate L",
    "start": "849480",
    "end": "852920"
  },
  {
    "text": "Lang model training with Mech existing",
    "start": "852920",
    "end": "856079"
  },
  {
    "text": "logging framework comment uh so this",
    "start": "856079",
    "end": "859000"
  },
  {
    "text": "actually very important because during",
    "start": "859000",
    "end": "861000"
  },
  {
    "text": "developing our developers really want to",
    "start": "861000",
    "end": "863880"
  },
  {
    "text": "see how the training goes during the",
    "start": "863880",
    "end": "866759"
  },
  {
    "text": "training not the end of the training so",
    "start": "866759",
    "end": "869480"
  },
  {
    "text": "they can monitor the uh metrix",
    "start": "869480",
    "end": "871720"
  },
  {
    "text": "especially the loss monitor monitoring",
    "start": "871720",
    "end": "874000"
  },
  {
    "text": "the uh the resource usage so they can",
    "start": "874000",
    "end": "876720"
  },
  {
    "text": "adjust the batch size and other hyper",
    "start": "876720",
    "end": "879399"
  },
  {
    "text": "parameters we also enable the torch",
    "start": "879399",
    "end": "882040"
  },
  {
    "text": "profiler for advanced developers so they",
    "start": "882040",
    "end": "885519"
  },
  {
    "text": "can debug under the current level and",
    "start": "885519",
    "end": "888519"
  },
  {
    "text": "they can do more model",
    "start": "888519",
    "end": "891959"
  },
  {
    "text": "optimization so with all those developer",
    "start": "891959",
    "end": "894959"
  },
  {
    "text": "uh developed we can uh show our premium",
    "start": "894959",
    "end": "898040"
  },
  {
    "text": "result so first of all we can train Lama",
    "start": "898040",
    "end": "900399"
  },
  {
    "text": "2 70 billion on 32",
    "start": "900399",
    "end": "903360"
  },
  {
    "text": "gpus secondly is we we we find the",
    "start": "903360",
    "end": "907560"
  },
  {
    "text": "linear scalability when we train Lama 30",
    "start": "907560",
    "end": "909880"
  },
  {
    "text": "bilon models when we increasing from",
    "start": "909880",
    "end": "912199"
  },
  {
    "text": "single host to multiple host that's",
    "start": "912199",
    "end": "914639"
  },
  {
    "text": "actually is very cool for the linear",
    "start": "914639",
    "end": "918160"
  },
  {
    "text": "scalability uh last one we actually",
    "start": "918160",
    "end": "920320"
  },
  {
    "text": "support the pre-training which is",
    "start": "920320",
    "end": "922399"
  },
  {
    "text": "concept in larg ler model supervisor",
    "start": "922399",
    "end": "925079"
  },
  {
    "text": "fine tune also parameter efficient fine",
    "start": "925079",
    "end": "928000"
  },
  {
    "text": "tune which is a lower and Q",
    "start": "928000",
    "end": "930920"
  },
  {
    "text": "lower and people only need to change a",
    "start": "930920",
    "end": "933759"
  },
  {
    "text": "few lines of code to switch between all",
    "start": "933759",
    "end": "937199"
  },
  {
    "text": "these cases here I give a a a plot shows",
    "start": "937199",
    "end": "941480"
  },
  {
    "text": "how uh uh our evaluation metric uh the",
    "start": "941480",
    "end": "945319"
  },
  {
    "text": "loss metric looks like the key learning",
    "start": "945319",
    "end": "947680"
  },
  {
    "text": "from here is for fine tune always gives",
    "start": "947680",
    "end": "950920"
  },
  {
    "text": "you the best",
    "start": "950920",
    "end": "952600"
  },
  {
    "text": "results you can see from the Lama 2 70",
    "start": "952600",
    "end": "955160"
  },
  {
    "text": "billion and L 2 uh uh 30 billion result",
    "start": "955160",
    "end": "958639"
  },
  {
    "text": "out and also uh four uh four even though",
    "start": "958639",
    "end": "963360"
  },
  {
    "text": "the four time uh four parameter fine",
    "start": "963360",
    "end": "965519"
  },
  {
    "text": "tune is the best but in terms of",
    "start": "965519",
    "end": "967519"
  },
  {
    "text": "resources and time is actually based on",
    "start": "967519",
    "end": "970399"
  },
  {
    "text": "our Benchmark is four to eight times",
    "start": "970399",
    "end": "972880"
  },
  {
    "text": "slower than Laur and Q",
    "start": "972880",
    "end": "975920"
  },
  {
    "text": "low more than that you need more",
    "start": "975920",
    "end": "978480"
  },
  {
    "text": "resources for example Lama to 70 billion",
    "start": "978480",
    "end": "981720"
  },
  {
    "text": "you need at least 32 gpus to train if",
    "start": "981720",
    "end": "985000"
  },
  {
    "text": "you want to train with a four parameter",
    "start": "985000",
    "end": "986639"
  },
  {
    "text": "update but the on the k you only need a",
    "start": "986639",
    "end": "989399"
  },
  {
    "text": "single GPU to train",
    "start": "989399",
    "end": "992720"
  },
  {
    "text": "it okay after your model training what",
    "start": "992959",
    "end": "995639"
  },
  {
    "text": "do you want to do next you want to do",
    "start": "995639",
    "end": "997720"
  },
  {
    "text": "evaluation and decide whether to put",
    "start": "997720",
    "end": "999880"
  },
  {
    "text": "online for serving so in order to do",
    "start": "999880",
    "end": "1002480"
  },
  {
    "text": "that uh we also using R to do offline",
    "start": "1002480",
    "end": "1005720"
  },
  {
    "text": "model prediction so we build a very",
    "start": "1005720",
    "end": "1008040"
  },
  {
    "text": "simple apis which is input data PR the",
    "start": "1008040",
    "end": "1012120"
  },
  {
    "text": "model output destination we integrate",
    "start": "1012120",
    "end": "1015160"
  },
  {
    "text": "all this with",
    "start": "1015160",
    "end": "1016399"
  },
  {
    "text": "Ray and uh uh across different reactors",
    "start": "1016399",
    "end": "1021079"
  },
  {
    "text": "we use a data paralyzer but inside each",
    "start": "1021079",
    "end": "1024438"
  },
  {
    "text": "reactor we use pipeline paraly if the",
    "start": "1024439",
    "end": "1027600"
  },
  {
    "text": "model is too big to fit a single GPU we",
    "start": "1027600",
    "end": "1030000"
  },
  {
    "text": "need to use multiple GPU in each",
    "start": "1030000",
    "end": "1033520"
  },
  {
    "text": "reactors so after after the uh model",
    "start": "1033520",
    "end": "1036520"
  },
  {
    "text": "prediction done you want to do",
    "start": "1036520",
    "end": "1038038"
  },
  {
    "text": "evaluation we integrate larg language",
    "start": "1038039",
    "end": "1040558"
  },
  {
    "text": "model evaluation with a mechan to",
    "start": "1040559",
    "end": "1043558"
  },
  {
    "text": "existing evaluation framework so our",
    "start": "1043559",
    "end": "1046520"
  },
  {
    "text": "customer can view the accurated",
    "start": "1046520",
    "end": "1049320"
  },
  {
    "text": "to decide how model uh how good is a",
    "start": "1049320",
    "end": "1053240"
  },
  {
    "text": "model we also support the instance level",
    "start": "1053240",
    "end": "1056960"
  },
  {
    "text": "metrix here I just give an example of",
    "start": "1056960",
    "end": "1059799"
  },
  {
    "text": "our customers uh evaluation metric which",
    "start": "1059799",
    "end": "1062679"
  },
  {
    "text": "is for the chat summarization which CH",
    "start": "1062679",
    "end": "1065720"
  },
  {
    "text": "mentioned",
    "start": "1065720",
    "end": "1068120"
  },
  {
    "text": "before okay after evaluation people find",
    "start": "1068440",
    "end": "1071320"
  },
  {
    "text": "okay my model is good what to do next",
    "start": "1071320",
    "end": "1073440"
  },
  {
    "text": "they want to deploy it so we reuse",
    "start": "1073440",
    "end": "1075919"
  },
  {
    "text": "mangelo existing deployment pipeline",
    "start": "1075919",
    "end": "1078400"
  },
  {
    "text": "plan which including model validation to",
    "start": "1078400",
    "end": "1080880"
  },
  {
    "text": "see whether your model missing anything",
    "start": "1080880",
    "end": "1083080"
  },
  {
    "text": "right model packaging to see okay if I",
    "start": "1083080",
    "end": "1085360"
  },
  {
    "text": "want to model uh T ID for the model you",
    "start": "1085360",
    "end": "1087320"
  },
  {
    "text": "need to do packaging model uploading the",
    "start": "1087320",
    "end": "1089640"
  },
  {
    "text": "model deployment in model deployment we",
    "start": "1089640",
    "end": "1092919"
  },
  {
    "text": "use tra on server and TT to serice the",
    "start": "1092919",
    "end": "1096080"
  },
  {
    "text": "larger ler model once the model deploys",
    "start": "1096080",
    "end": "1099320"
  },
  {
    "text": "our caller will call our services",
    "start": "1099320",
    "end": "1101960"
  },
  {
    "text": "through mangelo API Gateway the mangelo",
    "start": "1101960",
    "end": "1106200"
  },
  {
    "text": "Gen AI Gateway basically root the larg",
    "start": "1106200",
    "end": "1109280"
  },
  {
    "text": "langage model service to the",
    "start": "1109280",
    "end": "1111200"
  },
  {
    "text": "corresponding larg language model uh uh",
    "start": "1111200",
    "end": "1114559"
  },
  {
    "text": "server and then larg language model",
    "start": "1114559",
    "end": "1116760"
  },
  {
    "text": "server basically log in the input and",
    "start": "1116760",
    "end": "1119360"
  },
  {
    "text": "output text to Uber M3 storage for",
    "start": "1119360",
    "end": "1123559"
  },
  {
    "text": "monitoring and",
    "start": "1123559",
    "end": "1126679"
  },
  {
    "text": "debugging so this is just a snapshot",
    "start": "1126679",
    "end": "1129600"
  },
  {
    "text": "show show you how easy the deployment of",
    "start": "1129600",
    "end": "1133080"
  },
  {
    "text": "lar language model to be because we",
    "start": "1133080",
    "end": "1135240"
  },
  {
    "text": "already integrated with a m m existing",
    "start": "1135240",
    "end": "1138360"
  },
  {
    "text": "model deployment uh API in M Studio",
    "start": "1138360",
    "end": "1142159"
  },
  {
    "text": "people only need create create",
    "start": "1142159",
    "end": "1144320"
  },
  {
    "text": "deployment button then their model will",
    "start": "1144320",
    "end": "1146720"
  },
  {
    "text": "online so after their model deployed",
    "start": "1146720",
    "end": "1149919"
  },
  {
    "text": "they can do live test say okay how is my",
    "start": "1149919",
    "end": "1152840"
  },
  {
    "text": "model looks like anything",
    "start": "1152840",
    "end": "1155919"
  },
  {
    "text": "wrong with all that I will hand over to",
    "start": "1155919",
    "end": "1159360"
  },
  {
    "text": "tra for takeaways and our",
    "start": "1159360",
    "end": "1162520"
  },
  {
    "text": "learnings thanks both for the details of",
    "start": "1162520",
    "end": "1164960"
  },
  {
    "text": "our training Pipeline and the evaluation",
    "start": "1164960",
    "end": "1166960"
  },
  {
    "text": "part yeah there are few",
    "start": "1166960",
    "end": "1168840"
  },
  {
    "text": "valuable takeway we have learned from",
    "start": "1168840",
    "end": "1170840"
  },
  {
    "text": "this project this project just started",
    "start": "1170840",
    "end": "1173280"
  },
  {
    "text": "in June it's just uh in current shape",
    "start": "1173280",
    "end": "1175400"
  },
  {
    "text": "it's only takes after less than three",
    "start": "1175400",
    "end": "1177559"
  },
  {
    "text": "months so for firstly we are leveraging",
    "start": "1177559",
    "end": "1180880"
  },
  {
    "text": "the extensible micral framework to",
    "start": "1180880",
    "end": "1183200"
  },
  {
    "text": "support large language model cases with",
    "start": "1183200",
    "end": "1185280"
  },
  {
    "text": "a new rebased large L model training",
    "start": "1185280",
    "end": "1187280"
  },
  {
    "text": "framework we didn't start everything",
    "start": "1187280",
    "end": "1189280"
  },
  {
    "text": "from scratch we use everything possible",
    "start": "1189280",
    "end": "1191240"
  },
  {
    "text": "already existing M secondly we show that",
    "start": "1191240",
    "end": "1194960"
  },
  {
    "text": "an endend large Lang mod model Ops is",
    "start": "1194960",
    "end": "1197840"
  },
  {
    "text": "buil possible with open source Solutions",
    "start": "1197840",
    "end": "1200520"
  },
  {
    "text": "and also we recall all the re benefits",
    "start": "1200520",
    "end": "1202799"
  },
  {
    "text": "we have we highlighted in the beginning",
    "start": "1202799",
    "end": "1204360"
  },
  {
    "text": "of the slides it's already show up again",
    "start": "1204360",
    "end": "1206400"
  },
  {
    "text": "in our new lar language model training",
    "start": "1206400",
    "end": "1209919"
  },
  {
    "text": "framework so last but not least I would",
    "start": "1209919",
    "end": "1212919"
  },
  {
    "text": "like to give acknowledgement to both",
    "start": "1212919",
    "end": "1214480"
  },
  {
    "text": "Uber and N skill team so without a great",
    "start": "1214480",
    "end": "1217159"
  },
  {
    "text": "collaboration it is impossible to make",
    "start": "1217159",
    "end": "1219000"
  },
  {
    "text": "this project in current shape within",
    "start": "1219000",
    "end": "1220679"
  },
  {
    "text": "three months I would like to thank you",
    "start": "1220679",
    "end": "1223039"
  },
  {
    "text": "for the quick response and support from",
    "start": "1223039",
    "end": "1224960"
  },
  {
    "text": "any skill folk especially giving credit",
    "start": "1224960",
    "end": "1227039"
  },
  {
    "text": "to massive D to help out fixing a few",
    "start": "1227039",
    "end": "1229640"
  },
  {
    "text": "pyth distribution issues on on R open",
    "start": "1229640",
    "end": "1231919"
  },
  {
    "text": "source repo I I would also like to",
    "start": "1231919",
    "end": "1234679"
  },
  {
    "text": "thanks Uber M team for uning upstream",
    "start": "1234679",
    "end": "1238039"
  },
  {
    "text": "and downstream stages model Ops we",
    "start": "1238039",
    "end": "1241360"
  },
  {
    "text": "cannot be here today without other",
    "start": "1241360",
    "end": "1243120"
  },
  {
    "text": "support above yeah that's all for our",
    "start": "1243120",
    "end": "1245559"
  },
  {
    "text": "talk today yeah I'll leave the time for",
    "start": "1245559",
    "end": "1247360"
  },
  {
    "text": "the",
    "start": "1247360",
    "end": "1249520"
  },
  {
    "text": "question cool thank you so much um yes",
    "start": "1252559",
    "end": "1255480"
  },
  {
    "text": "so questions uh we'll start over here",
    "start": "1255480",
    "end": "1260120"
  },
  {
    "text": "thanks for presentation quick question",
    "start": "1265240",
    "end": "1267320"
  },
  {
    "text": "that uh on your serving uh piece I know",
    "start": "1267320",
    "end": "1270799"
  },
  {
    "text": "that using Triton server for serving",
    "start": "1270799",
    "end": "1273320"
  },
  {
    "text": "your LS yes can you talk a little bit",
    "start": "1273320",
    "end": "1275320"
  },
  {
    "text": "about the motivation behind that uh one",
    "start": "1275320",
    "end": "1278840"
  },
  {
    "text": "thing is Lear because in production",
    "start": "1278840",
    "end": "1280480"
  },
  {
    "text": "today uh Uber's model differently model",
    "start": "1280480",
    "end": "1282919"
  },
  {
    "text": "already served in Tron inference server",
    "start": "1282919",
    "end": "1285080"
  },
  {
    "text": "so we see very good performance and we",
    "start": "1285080",
    "end": "1287000"
  },
  {
    "text": "have very collab ation with Nvidia and",
    "start": "1287000",
    "end": "1290039"
  },
  {
    "text": "Nvidia just taking one step further they",
    "start": "1290039",
    "end": "1292000"
  },
  {
    "text": "are preparing the 10m rapo and want to",
    "start": "1292000",
    "end": "1294760"
  },
  {
    "text": "optimize the performance with uh intense",
    "start": "1294760",
    "end": "1298200"
  },
  {
    "text": "are specific model so that's our",
    "start": "1298200",
    "end": "1300799"
  },
  {
    "text": "decision goes right now",
    "start": "1300799",
    "end": "1304639"
  },
  {
    "text": "yeah I have a quick question for the uh",
    "start": "1306640",
    "end": "1309520"
  },
  {
    "text": "evaluation part and go back to the slide",
    "start": "1309520",
    "end": "1311799"
  },
  {
    "text": "like in terms of the score that you",
    "start": "1311799",
    "end": "1313120"
  },
  {
    "text": "calculated how how did you come with the",
    "start": "1313120",
    "end": "1315120"
  },
  {
    "text": "scores how do we come this scores yes so",
    "start": "1315120",
    "end": "1318640"
  },
  {
    "text": "first of all uh this the evaluation of",
    "start": "1318640",
    "end": "1321200"
  },
  {
    "text": "larg Lang model is very different from",
    "start": "1321200",
    "end": "1323679"
  },
  {
    "text": "traditional machine learning model",
    "start": "1323679",
    "end": "1325279"
  },
  {
    "text": "because most of time traditional machine",
    "start": "1325279",
    "end": "1327200"
  },
  {
    "text": "Li basically binary ranking or linear",
    "start": "1327200",
    "end": "1329919"
  },
  {
    "text": "regression you have standardized Matrix",
    "start": "1329919",
    "end": "1332039"
  },
  {
    "text": "so for this particular case which is",
    "start": "1332039",
    "end": "1334080"
  },
  {
    "text": "chest summarization we care about how",
    "start": "1334080",
    "end": "1336919"
  },
  {
    "text": "your summarize reflect your original",
    "start": "1336919",
    "end": "1339000"
  },
  {
    "text": "chess between agents and the customer so",
    "start": "1339000",
    "end": "1341840"
  },
  {
    "text": "those metric I defined uh in the stand",
    "start": "1341840",
    "end": "1345000"
  },
  {
    "text": "for I forget R Rog raple basically",
    "start": "1345000",
    "end": "1348840"
  },
  {
    "text": "covers that uh how accurate your",
    "start": "1348840",
    "end": "1351000"
  },
  {
    "text": "summarization it is and how concise it",
    "start": "1351000",
    "end": "1354120"
  },
  {
    "text": "is and there another one I forget so",
    "start": "1354120",
    "end": "1356720"
  },
  {
    "text": "basically basically that's how a",
    "start": "1356720",
    "end": "1358520"
  },
  {
    "text": "customer can define those metrics the",
    "start": "1358520",
    "end": "1360600"
  },
  {
    "text": "nice thing about our evaluation is we",
    "start": "1360600",
    "end": "1363520"
  },
  {
    "text": "allow the customer to Define their own",
    "start": "1363520",
    "end": "1367159"
  },
  {
    "text": "instant level metrix M only manager The",
    "start": "1367159",
    "end": "1371720"
  },
  {
    "text": "aggregated Matrix and and the final M",
    "start": "1371720",
    "end": "1376720"
  },
  {
    "text": "display so so as long as the user uh our",
    "start": "1376720",
    "end": "1380799"
  },
  {
    "text": "customer write their own uh instant",
    "start": "1380799",
    "end": "1383279"
  },
  {
    "text": "level metrix in certain format input",
    "start": "1383279",
    "end": "1386799"
  },
  {
    "text": "output then mechan can take over it and",
    "start": "1386799",
    "end": "1390000"
  },
  {
    "text": "display and do aggregation for them yeah",
    "start": "1390000",
    "end": "1392240"
  },
  {
    "text": "is that answer your question is that b",
    "start": "1392240",
    "end": "1395240"
  },
  {
    "text": "like human human based oh you you're",
    "start": "1395240",
    "end": "1397679"
  },
  {
    "text": "saying how do we get the response right",
    "start": "1397679",
    "end": "1400279"
  },
  {
    "text": "oh yeah yeah yeah so there are two",
    "start": "1400279",
    "end": "1401559"
  },
  {
    "text": "things we first of all rely on human",
    "start": "1401559",
    "end": "1404039"
  },
  {
    "text": "human level data secondly we can we also",
    "start": "1404039",
    "end": "1406840"
  },
  {
    "text": "use the uh cpt4 to gener compare with",
    "start": "1406840",
    "end": "1409799"
  },
  {
    "text": "cpt4",
    "start": "1409799",
    "end": "1412399"
  },
  {
    "text": "yeah thanks uh I might take it wrong but",
    "start": "1417840",
    "end": "1421080"
  },
  {
    "text": "it seems you've reused a lot of the",
    "start": "1421080",
    "end": "1423320"
  },
  {
    "text": "mongal uh like features already except",
    "start": "1423320",
    "end": "1426640"
  },
  {
    "text": "the training and fine tun of the LM just",
    "start": "1426640",
    "end": "1429279"
  },
  {
    "text": "wonder uh if that's correct and if",
    "start": "1429279",
    "end": "1431120"
  },
  {
    "text": "that's correct uh yes uh what takes you",
    "start": "1431120",
    "end": "1433919"
  },
  {
    "text": "to this decision and if you have any",
    "start": "1433919",
    "end": "1436400"
  },
  {
    "text": "plan to uh integrate the LM training and",
    "start": "1436400",
    "end": "1439760"
  },
  {
    "text": "fine tuning into",
    "start": "1439760",
    "end": "1441720"
  },
  {
    "text": "Mulo so this is already part of Mulo so",
    "start": "1441720",
    "end": "1447640"
  },
  {
    "text": "so so some of you might see the previous",
    "start": "1447640",
    "end": "1450679"
  },
  {
    "text": "year the resign me talk Uber have last",
    "start": "1450679",
    "end": "1453200"
  },
  {
    "text": "year so the major important part is the",
    "start": "1453200",
    "end": "1456000"
  },
  {
    "text": "model training part so Uber has own",
    "start": "1456000",
    "end": "1458600"
  },
  {
    "text": "framework on Ray but it's running on",
    "start": "1458600",
    "end": "1460400"
  },
  {
    "text": "hardvard for data parad support that's",
    "start": "1460400",
    "end": "1463919"
  },
  {
    "text": "the things currently in production for",
    "start": "1463919",
    "end": "1466320"
  },
  {
    "text": "recommendation models and simple NP",
    "start": "1466320",
    "end": "1468559"
  },
  {
    "text": "cases but however we found that",
    "start": "1468559",
    "end": "1470480"
  },
  {
    "text": "framework cannot fit into the case here",
    "start": "1470480",
    "end": "1472919"
  },
  {
    "text": "for two blockers list here one is the",
    "start": "1472919",
    "end": "1475320"
  },
  {
    "text": "model parallelism because hardvard don't",
    "start": "1475320",
    "end": "1477480"
  },
  {
    "text": "support model parallelism second is the",
    "start": "1477480",
    "end": "1480120"
  },
  {
    "text": "huging ph and support we found in we",
    "start": "1480120",
    "end": "1483640"
  },
  {
    "text": "found our Ser model Sero pip PL in",
    "start": "1483640",
    "end": "1486520"
  },
  {
    "text": "existing micro they have a few blockers",
    "start": "1486520",
    "end": "1488799"
  },
  {
    "text": "for serving hugging face organizers so",
    "start": "1488799",
    "end": "1491559"
  },
  {
    "text": "instead of fixing the things on the",
    "start": "1491559",
    "end": "1494840"
  },
  {
    "text": "outside we found that very convenient to",
    "start": "1494840",
    "end": "1497840"
  },
  {
    "text": "taking the usage of R API starting from",
    "start": "1497840",
    "end": "1500880"
  },
  {
    "text": "scratch adding the support to the new L",
    "start": "1500880",
    "end": "1503640"
  },
  {
    "text": "remot framework but for your questioning",
    "start": "1503640",
    "end": "1506240"
  },
  {
    "text": "how to integration back we are working",
    "start": "1506240",
    "end": "1508080"
  },
  {
    "text": "on that so this work today is most our",
    "start": "1508080",
    "end": "1511080"
  },
  {
    "text": "users they are",
    "start": "1511080",
    "end": "1513120"
  },
  {
    "text": "using uh most of users and model",
    "start": "1513120",
    "end": "1516279"
  },
  {
    "text": "developers they're using this UI like",
    "start": "1516279",
    "end": "1518720"
  },
  {
    "text": "this DB interactive DB La environment to",
    "start": "1518720",
    "end": "1521760"
  },
  {
    "text": "edit a file to edit a model",
    "start": "1521760",
    "end": "1523640"
  },
  {
    "text": "configuration file some job but we are",
    "start": "1523640",
    "end": "1526200"
  },
  {
    "text": "integrating things back to mic studio so",
    "start": "1526200",
    "end": "1529240"
  },
  {
    "text": "this will become a new trainer called",
    "start": "1529240",
    "end": "1531640"
  },
  {
    "text": "large L model trainer and the user can",
    "start": "1531640",
    "end": "1533480"
  },
  {
    "text": "click a button on the mic studio and K",
    "start": "1533480",
    "end": "1536200"
  },
  {
    "text": "user don't have to go to this",
    "start": "1536200",
    "end": "1538120"
  },
  {
    "text": "interactive environment around R the job",
    "start": "1538120",
    "end": "1541240"
  },
  {
    "text": "yeah I can add a few points so Mech is a",
    "start": "1541240",
    "end": "1543840"
  },
  {
    "text": "very general so it has a high level API",
    "start": "1543840",
    "end": "1546880"
  },
  {
    "text": "level UniFi API level and also have on",
    "start": "1546880",
    "end": "1549559"
  },
  {
    "text": "the bottom has something each component",
    "start": "1549559",
    "end": "1552679"
  },
  {
    "text": "and at uh under each component it also",
    "start": "1552679",
    "end": "1555720"
  },
  {
    "text": "have some orgm so so for larger L model",
    "start": "1555720",
    "end": "1560200"
  },
  {
    "text": "training basically we kind of using uh",
    "start": "1560200",
    "end": "1564080"
  },
  {
    "text": "Ray and open source model to have in the",
    "start": "1564080",
    "end": "1567960"
  },
  {
    "text": "or part to replace the trainer part but",
    "start": "1567960",
    "end": "1571159"
  },
  {
    "text": "at the top level in terms of the uh",
    "start": "1571159",
    "end": "1575159"
  },
  {
    "text": "component and also at a high level UniFi",
    "start": "1575159",
    "end": "1578080"
  },
  {
    "text": "API because we Define the protocol the",
    "start": "1578080",
    "end": "1580919"
  },
  {
    "text": "protocol is General enough such as any",
    "start": "1580919",
    "end": "1583240"
  },
  {
    "text": "kind of very low level development can",
    "start": "1583240",
    "end": "1585679"
  },
  {
    "text": "easily plug in back to Mech",
    "start": "1585679",
    "end": "1588919"
  },
  {
    "text": "yeah",
    "start": "1588919",
    "end": "1591919"
  },
  {
    "text": "answer uh thanks so much for the",
    "start": "1598399",
    "end": "1600240"
  },
  {
    "text": "presentation um I'm wondering if you can",
    "start": "1600240",
    "end": "1601840"
  },
  {
    "text": "speak more to the API um specifically",
    "start": "1601840",
    "end": "1606240"
  },
  {
    "text": "you know I'm a little unclear on you",
    "start": "1606240",
    "end": "1608559"
  },
  {
    "text": "what Michelangelo abstracts away from",
    "start": "1608559",
    "end": "1610799"
  },
  {
    "text": "the user like for just concretely would",
    "start": "1610799",
    "end": "1613679"
  },
  {
    "text": "the user need to know Ray in order to",
    "start": "1613679",
    "end": "1616279"
  },
  {
    "text": "use uh you know Michelangelo for this",
    "start": "1616279",
    "end": "1619240"
  },
  {
    "text": "purpose uh no user so so let's say if",
    "start": "1619240",
    "end": "1623360"
  },
  {
    "text": "user knows what kind of data data source",
    "start": "1623360",
    "end": "1625799"
  },
  {
    "text": "he needs what kind of feature he needs",
    "start": "1625799",
    "end": "1629440"
  },
  {
    "text": "so he just named the data source name of",
    "start": "1629440",
    "end": "1631520"
  },
  {
    "text": "the feature and user if he want to pick",
    "start": "1631520",
    "end": "1634200"
  },
  {
    "text": "up one of the open source model for R he",
    "start": "1634200",
    "end": "1636720"
  },
  {
    "text": "can specify that in the yo and kick out",
    "start": "1636720",
    "end": "1639520"
  },
  {
    "text": "the end to end workflow without knowing",
    "start": "1639520",
    "end": "1642159"
  },
  {
    "text": "Ray M long will take care of lunching",
    "start": "1642159",
    "end": "1645200"
  },
  {
    "text": "Ray pipelining the training job with",
    "start": "1645200",
    "end": "1647720"
  },
  {
    "text": "other data preparation and transforming",
    "start": "1647720",
    "end": "1649559"
  },
  {
    "text": "stages internally and what's user facing",
    "start": "1649559",
    "end": "1653679"
  },
  {
    "text": "is just this micr studio UI this is only",
    "start": "1653679",
    "end": "1657320"
  },
  {
    "text": "our customer facing so they don't have",
    "start": "1657320",
    "end": "1659760"
  },
  {
    "text": "to know but today the initial version of",
    "start": "1659760",
    "end": "1662360"
  },
  {
    "text": "our lonic model training framework is we",
    "start": "1662360",
    "end": "1664840"
  },
  {
    "text": "first presented this Jupiter based",
    "start": "1664840",
    "end": "1667840"
  },
  {
    "text": "interactive interactive environment to",
    "start": "1667840",
    "end": "1670000"
  },
  {
    "text": "our like arpa users so because they are",
    "start": "1670000",
    "end": "1673159"
  },
  {
    "text": "kind of really know how to write a code",
    "start": "1673159",
    "end": "1674760"
  },
  {
    "text": "how to configure file so they first KY",
    "start": "1674760",
    "end": "1677000"
  },
  {
    "text": "out their model training with like by",
    "start": "1677000",
    "end": "1679640"
  },
  {
    "text": "writing a code but eventually user don't",
    "start": "1679640",
    "end": "1682120"
  },
  {
    "text": "have to user will be on pure on the data",
    "start": "1682120",
    "end": "1684360"
  },
  {
    "text": "science level specifying data source the",
    "start": "1684360",
    "end": "1687679"
  },
  {
    "text": "model want to use and the",
    "start": "1687679",
    "end": "1690880"
  },
  {
    "text": "fates um I was I was wondering how did",
    "start": "1698519",
    "end": "1701600"
  },
  {
    "text": "you get that Ray dashboard at the bottom",
    "start": "1701600",
    "end": "1703600"
  },
  {
    "text": "of your uh Jupiter okay environment was",
    "start": "1703600",
    "end": "1706399"
  },
  {
    "text": "that a custom widget or um is that was",
    "start": "1706399",
    "end": "1709679"
  },
  {
    "text": "that available from the Ray it's good",
    "start": "1709679",
    "end": "1712279"
  },
  {
    "text": "question it's not on the same address so",
    "start": "1712279",
    "end": "1714320"
  },
  {
    "text": "jup lab has its own part red dashboard",
    "start": "1714320",
    "end": "1716880"
  },
  {
    "text": "has another part so it's not like this",
    "start": "1716880",
    "end": "1719399"
  },
  {
    "text": "actually two screenshots yeah so you",
    "start": "1719399",
    "end": "1721960"
  },
  {
    "text": "Photoshopped it I see Catch yeah you",
    "start": "1721960",
    "end": "1725120"
  },
  {
    "text": "tricked me",
    "start": "1725120",
    "end": "1728158"
  },
  {
    "text": "yeah cool we have time for one more",
    "start": "1729200",
    "end": "1731360"
  },
  {
    "text": "question there are",
    "start": "1731360",
    "end": "1734440"
  },
  {
    "text": "any",
    "start": "1736399",
    "end": "1739399"
  },
  {
    "text": "um the uh batch scheduler seem like a",
    "start": "1742799",
    "end": "1745159"
  },
  {
    "text": "really cool addition to the stack are",
    "start": "1745159",
    "end": "1747320"
  },
  {
    "text": "there any plans to um maybe open source",
    "start": "1747320",
    "end": "1750519"
  },
  {
    "text": "that at some point it seems like a good",
    "start": "1750519",
    "end": "1751960"
  },
  {
    "text": "alternative to like volcano or some of",
    "start": "1751960",
    "end": "1753799"
  },
  {
    "text": "the other kubernetes batch schedulers",
    "start": "1753799",
    "end": "1756039"
  },
  {
    "text": "out",
    "start": "1756039",
    "end": "1757240"
  },
  {
    "text": "there uh I think it's possible we don't",
    "start": "1757240",
    "end": "1759679"
  },
  {
    "text": "close the door I think the micr",
    "start": "1759679",
    "end": "1761880"
  },
  {
    "text": "concerning about open source the whole",
    "start": "1761880",
    "end": "1764200"
  },
  {
    "text": "uh micro repost to partty I think this",
    "start": "1764200",
    "end": "1766880"
  },
  {
    "text": "part is job schedule will be part of it",
    "start": "1766880",
    "end": "1769159"
  },
  {
    "text": "because we call this ma job schedule I",
    "start": "1769159",
    "end": "1772159"
  },
  {
    "text": "think Travis If are interested we can we",
    "start": "1772159",
    "end": "1774360"
  },
  {
    "text": "talk with the mean offline the principal",
    "start": "1774360",
    "end": "1775720"
  },
  {
    "text": "engineer in",
    "start": "1775720",
    "end": "1778240"
  },
  {
    "text": "mure",
    "start": "1780120",
    "end": "1783120"
  }
]