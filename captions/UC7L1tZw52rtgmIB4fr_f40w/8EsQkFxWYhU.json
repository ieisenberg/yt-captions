[
  {
    "text": "welcome everyone from lunch uh I'm Wesley Kerr I lead tech research at Riot",
    "start": "5359",
    "end": "10559"
  },
  {
    "text": "games I'm here today to talk about how we're using large-scale deep learning to augment our production RL workloads",
    "start": "10559",
    "end": "17100"
  },
  {
    "text": "a quick run through of the agenda first we're going to talk a little bit about who Riot games is uh we'll dive into one",
    "start": "17100",
    "end": "22920"
  },
  {
    "text": "specific game team fight tactics uh frame the reinforcement learning problem and then how we're using supervised",
    "start": "22920",
    "end": "28380"
  },
  {
    "text": "learning and then conclude with some remarks so rag games is the developer and",
    "start": "28380",
    "end": "33840"
  },
  {
    "text": "publisher of League of Legends over the past few years we've expanded our portfolio we've included new games so we",
    "start": "33840",
    "end": "39600"
  },
  {
    "text": "have League of Legends wild Rift which is our mobile version of League of Legends we have Team fight tactics which",
    "start": "39600",
    "end": "45000"
  },
  {
    "text": "is Riot's take on the auto battler genre and valorent which is our our version of",
    "start": "45000",
    "end": "50520"
  },
  {
    "text": "a tactical first person shooter and then Legends runeter which is our Trading Card Game",
    "start": "50520",
    "end": "56039"
  },
  {
    "text": "uh today's talk is primarily focused on team fight tactics as I mentioned team fight tactics is Riot's take on the auto",
    "start": "56039",
    "end": "61980"
  },
  {
    "text": "battler genre it is set in the League of Legends Universe with our Champions items and Monsters uh team team fight",
    "start": "61980",
    "end": "69900"
  },
  {
    "text": "tactics is a relatively new game for Riot and therefore also a relatively new game to balance and design around",
    "start": "69900",
    "end": "76619"
  },
  {
    "text": "uh so team fight tactics you play play takes place in rounds so every round you",
    "start": "76619",
    "end": "81780"
  },
  {
    "text": "are building up your board your board consists of Champions that you buy and place on your board",
    "start": "81780",
    "end": "87840"
  },
  {
    "text": "and uh Browns compete you get a random selection of Champions that you start",
    "start": "87840",
    "end": "93360"
  },
  {
    "text": "from uh down shown here down here on the bottom of your screen you have gold every Champion costs gold every round",
    "start": "93360",
    "end": "98880"
  },
  {
    "text": "you get more gold uh it's an eight-person game and your job is to survive longer than all the others every",
    "start": "98880",
    "end": "104880"
  },
  {
    "text": "round your board wins you do damage to your opponent every round you lose your opponent does damage to you",
    "start": "104880",
    "end": "111799"
  },
  {
    "text": "there are a bunch of different Champions to choose from every Champion costs different amounts of gold their abilities sort of scale with their price",
    "start": "112259",
    "end": "118560"
  },
  {
    "text": "and gold you get gold again every round and you can save save up to get better champions",
    "start": "118560",
    "end": "124740"
  },
  {
    "text": "you get three of the same Champions that champion levels up to level two uh level two Champions have more abilities are",
    "start": "124740",
    "end": "131220"
  },
  {
    "text": "stronger you get three level twos and you now have a level three Champion again uh leveling them up making them",
    "start": "131220",
    "end": "137400"
  },
  {
    "text": "more powerful for your board uh throughout the game you'll also get items so items drop in PVE rounds in",
    "start": "137400",
    "end": "144900"
  },
  {
    "text": "other places those items you can attach to a champion that gives the champion better stats they can do more damage",
    "start": "144900",
    "end": "150840"
  },
  {
    "text": "take more damage you can combine items to make more powerful items",
    "start": "150840",
    "end": "155879"
  },
  {
    "text": "um and here we have a needlessly large rod and a recurve bow",
    "start": "155879",
    "end": "161400"
  },
  {
    "text": "combining to create a Ginsu rage blade which gives your uh attacker more damage",
    "start": "161400",
    "end": "166500"
  },
  {
    "text": "attack speed uh that stacks and so you can place that on your favorite yordle uh here Tristana and Tristana will do",
    "start": "166500",
    "end": "172260"
  },
  {
    "text": "that much more damage uh in in each battle uh Champions also have classes and",
    "start": "172260",
    "end": "178500"
  },
  {
    "text": "Origins so here we have Ezreal who is a tempest Swift shot and Ash who is a Jade",
    "start": "178500",
    "end": "183720"
  },
  {
    "text": "Dragon Manser Swift shot if you have two Champions that uh when placed on the",
    "start": "183720",
    "end": "188760"
  },
  {
    "text": "board with similar classes or origins or with the same class Origins you can get a Synergy which will also give you",
    "start": "188760",
    "end": "194580"
  },
  {
    "text": "additional bonus stats for your board again making that board stronger in the next battle so that you can win and",
    "start": "194580",
    "end": "201720"
  },
  {
    "text": "continue to outlive your opponents and so here we have two switch shots on the board we gain the Synergy Swift shot",
    "start": "201720",
    "end": "208620"
  },
  {
    "text": "which gives us plus ten percent attack speed okay so this is around actually playing",
    "start": "208620",
    "end": "214379"
  },
  {
    "text": "out so uh Champions you do not control them once they're on your board they fight each other uh on their own",
    "start": "214379",
    "end": "220980"
  },
  {
    "text": "um here you can see I'm gonna lose uh but uh at the end of it I'll take damage",
    "start": "220980",
    "end": "226260"
  },
  {
    "text": "and don't pay attention too much to my synergies here I was way more worried about getting a good video than I was",
    "start": "226260",
    "end": "231959"
  },
  {
    "text": "about winning this game um and so that's why I ended up losing um probably not the only reason but",
    "start": "231959",
    "end": "239040"
  },
  {
    "text": "um and so here towards the end you'll see I'm going to lose damage is done to me my health drops lower and again over",
    "start": "239040",
    "end": "245940"
  },
  {
    "text": "time you try to survive and Outlast your opponents so we spent a lot of time that's TFT in",
    "start": "245940",
    "end": "251939"
  },
  {
    "text": "a very short nutshell um you're still lots more to learn lots more depth there to learn as you go to get become a",
    "start": "251939",
    "end": "258660"
  },
  {
    "text": "master of strategy and team fight tactics and I bring that all up now to really set up it's important to",
    "start": "258660",
    "end": "265080"
  },
  {
    "text": "understand the game that we're playing in order to understand how we're going to use reinforcement learning to solve it",
    "start": "265080",
    "end": "271020"
  },
  {
    "text": "so the reinforcement learning problem that we're looking at is we want to build an AI that learns to play TFT at the highest levels it's not there just",
    "start": "271020",
    "end": "277440"
  },
  {
    "text": "to stomp on our our opponents we actually want to learn so that we can do things like metagame balance we can",
    "start": "277440",
    "end": "282960"
  },
  {
    "text": "unlock other player features that are very valuable for our players uh there's been incredible progress for",
    "start": "282960",
    "end": "289320"
  },
  {
    "text": "reinforcement learning and games over the past few years from the early uh papers and where we had human level",
    "start": "289320",
    "end": "294840"
  },
  {
    "text": "achievements in Atari to later work by deepmind and open AI where they played top tier matches against Starcraft and",
    "start": "294840",
    "end": "301680"
  },
  {
    "text": "Dota 2 teams respectively we hope to stand on the shoulders of these Giants and make our AI that learns",
    "start": "301680",
    "end": "307860"
  },
  {
    "text": "to play the games in similar ways that they did but we're going to take it to the next step which is further which is how do we bring that value of an agent",
    "start": "307860",
    "end": "315120"
  },
  {
    "text": "that plays really well back to features that will impact our players and make our games better and we've begun to improve those game",
    "start": "315120",
    "end": "321240"
  },
  {
    "text": "experiences using deep reinforcement learning now I'm not going to dive deep into those successes in this talk in fact I'm going to point you to an",
    "start": "321240",
    "end": "327240"
  },
  {
    "text": "excellent talk by Ben Casper sitting right over there at the any scale RL Summit where he talks about how we've",
    "start": "327240",
    "end": "333240"
  },
  {
    "text": "been begun to improve those experiences uh using reinforcement learning instead here I'm going to focus on at",
    "start": "333240",
    "end": "340500"
  },
  {
    "text": "the end of the talk he talks about how we we are been doing more with TFT and we're going to dive into how we've been",
    "start": "340500",
    "end": "345780"
  },
  {
    "text": "doing that with TFT so in uh 2020 the team had the audacious",
    "start": "345780",
    "end": "352500"
  },
  {
    "text": "goal to build AI that plays at the highest level but we encountered some challenges along the way",
    "start": "352500",
    "end": "357539"
  },
  {
    "text": "uh so TFT launched in 2019 uh this new genre Auto Battlers came into existence",
    "start": "357539",
    "end": "362940"
  },
  {
    "text": "then uh and very quickly Riot wanted to create our experience of an auto battler",
    "start": "362940",
    "end": "368039"
  },
  {
    "text": "set in our world with our players and champions and so the team moved extremely fast to",
    "start": "368039",
    "end": "374460"
  },
  {
    "text": "launch Team fight tactics and along the way they didn't really stop and say hey you know what maybe later we're going to want to hook up an AI to this and learn",
    "start": "374460",
    "end": "380820"
  },
  {
    "text": "to play so the integration doesn't exist for us to work with the game servers",
    "start": "380820",
    "end": "386039"
  },
  {
    "text": "TFT constantly evolves part of the fun of this game is solving the puzzle figuring out what best Champions are",
    "start": "386039",
    "end": "391500"
  },
  {
    "text": "what the best synergies are what items to put on who and so to keep that fresh we launch new sets where we changed the",
    "start": "391500",
    "end": "398160"
  },
  {
    "text": "mechanics we change the Dynamics and really sort of keep players excited to come back and play so that means our AI",
    "start": "398160",
    "end": "404280"
  },
  {
    "text": "is going to need to learn on something that's constantly evolving over time",
    "start": "404280",
    "end": "409500"
  },
  {
    "text": "and then lastly write games's mission is to be the most player focused game company in the world and we can't",
    "start": "409500",
    "end": "414660"
  },
  {
    "text": "achieve that emission unless we're focused on delivering as much player value as possible this can make it really difficult to tackle multi-year",
    "start": "414660",
    "end": "421199"
  },
  {
    "text": "projects like build an agent that learns to play TFT when you have to offset of what player value is going to come out",
    "start": "421199",
    "end": "426840"
  },
  {
    "text": "of it and so because of these challenges we actually went out and built our own version of TFT so here's the programmer",
    "start": "426840",
    "end": "433740"
  },
  {
    "text": "art for for our game it turns out that building a lot of the the metagame uh",
    "start": "433740",
    "end": "439199"
  },
  {
    "text": "the buying the selling the putting items on placing items on your board is pretty easy to code so we programmed it up in",
    "start": "439199",
    "end": "445199"
  },
  {
    "text": "Python pretty quickly here you can see the the same hexagonal board that you saw uh before you have your bench you",
    "start": "445199",
    "end": "451800"
  },
  {
    "text": "have your buy menu and then the battles are taking place over on the right",
    "start": "451800",
    "end": "457199"
  },
  {
    "text": "now one thing that I'm not going to bury the lead on this is that it turns out that",
    "start": "457199",
    "end": "462419"
  },
  {
    "text": "the hardest part of building that game is actually programming all the champion abilities for those battles so you saw",
    "start": "462419",
    "end": "467520"
  },
  {
    "text": "the Champions running around attacking each other that's the complex part um and we're scientists and researchers",
    "start": "467520",
    "end": "473039"
  },
  {
    "text": "and we don't want to spend our time writing games we have game teams to do that and so what we set out to do was",
    "start": "473039",
    "end": "478919"
  },
  {
    "text": "could we take a supervised learning algorithm that could predict outcomes for the battle that way we don't",
    "start": "478919",
    "end": "484740"
  },
  {
    "text": "actually have to train up or write all the code for the individual agents and so what that looks like is actually",
    "start": "484740",
    "end": "490680"
  },
  {
    "text": "here so here you have two boards uh we'll call them home and away for Simplicity the Champions are varying",
    "start": "490680",
    "end": "496380"
  },
  {
    "text": "levels so there are some level three Champions some level two Champions uh they have different synergies uh that",
    "start": "496380",
    "end": "501840"
  },
  {
    "text": "are currently active given the the board um and then they're gonna fight uh and we have that data and the damage of this",
    "start": "501840",
    "end": "508919"
  },
  {
    "text": "particular outcome is is negative 12. so the home team took 12 damage now on the right what you actually see is the",
    "start": "508919",
    "end": "515459"
  },
  {
    "text": "x-axis represents all the different damages that are possible and the y-axis is the probability of",
    "start": "515459",
    "end": "520800"
  },
  {
    "text": "that damage occurring given this specific board and so what's really at play here is that outcomes are not",
    "start": "520800",
    "end": "527459"
  },
  {
    "text": "deterministic so depending on what how the agent decided to Target how they decided to move in that round you can",
    "start": "527459",
    "end": "533760"
  },
  {
    "text": "get very widely different outcomes and so here we need a model that can actually not just predict a single",
    "start": "533760",
    "end": "538980"
  },
  {
    "text": "scalar value we need a distribution over all the possible values",
    "start": "538980",
    "end": "544279"
  },
  {
    "text": "luckily for us we've had lots and lots of people playing our game we have lots of these boards to draw from and there's",
    "start": "544380",
    "end": "550800"
  },
  {
    "text": "just a whole lot of information in all of these there's different Champions there's different items there are",
    "start": "550800",
    "end": "556200"
  },
  {
    "text": "different star levels some boards have very few Champions so you have a three star down here against another three",
    "start": "556200",
    "end": "562080"
  },
  {
    "text": "star and then one extra malphite that's a really a late game comp but",
    "start": "562080",
    "end": "567180"
  },
  {
    "text": "someone for whatever reason these two players were kind of goofing off um and some of you may be sitting there",
    "start": "567180",
    "end": "574080"
  },
  {
    "text": "going I've got a bad feeling about this because what we have is a bunch of player data that we're going to build a supervised",
    "start": "574080",
    "end": "579779"
  },
  {
    "text": "learning model on we're going to use that to feed an RL Loop and train on that and so there's this problem of uh",
    "start": "579779",
    "end": "587100"
  },
  {
    "text": "distributional shift or domain adaptation or app domain adaption that",
    "start": "587100",
    "end": "592380"
  },
  {
    "text": "we have to worry about in this case we are 100 certain that our rrl agent will encounter areas of the state space that",
    "start": "592380",
    "end": "599700"
  },
  {
    "text": "our model has not been trained on because the RL agent is playing more randomly than any soup and then our",
    "start": "599700",
    "end": "606420"
  },
  {
    "text": "players are our players are very are falling to very specific strategies and are doing very specific things",
    "start": "606420",
    "end": "612240"
  },
  {
    "text": "and so this distributional shift is is going to be a challenge for us and is a big concern uh expressed by the team of",
    "start": "612240",
    "end": "619260"
  },
  {
    "text": "like how are we going to encounter this and get agents that we train on this supervised learning policy that are",
    "start": "619260",
    "end": "625260"
  },
  {
    "text": "going to work in the real game and so we took our player data and that",
    "start": "625260",
    "end": "630540"
  },
  {
    "text": "represents the majority of the training data that we have so our players are really great and that they play lots of",
    "start": "630540",
    "end": "636000"
  },
  {
    "text": "games we get lots of boards we get lots of data then we worked with the game team to create a combat simulator so the",
    "start": "636000",
    "end": "642660"
  },
  {
    "text": "simulator here is actually able to take two boards simulate the outcome and we",
    "start": "642660",
    "end": "648600"
  },
  {
    "text": "can then add that data to augment our training data set uh and so what we do is we we grab a",
    "start": "648600",
    "end": "654720"
  },
  {
    "text": "bunch of data just randomly generate boards that were much likely more likely to encounter in the RL space and we send",
    "start": "654720",
    "end": "660660"
  },
  {
    "text": "them through the combat simulator now the simulator is too slow to hook into the RL Loop it takes about 30 seconds to",
    "start": "660660",
    "end": "666660"
  },
  {
    "text": "do one battle um so you imagine if you're trying to build an RL training algorithm on that you're going to be waiting days",
    "start": "666660",
    "end": "673260"
  },
  {
    "text": "um and so we do that with the random and then we also augment player data so imagine here is that players may always",
    "start": "673260",
    "end": "679620"
  },
  {
    "text": "play Tanki champions in the front and soft squeeze squishy champions in the back we will randomize where they play",
    "start": "679620",
    "end": "687060"
  },
  {
    "text": "just so that we have better training data to train our supervised learning model",
    "start": "687060",
    "end": "693079"
  },
  {
    "text": "the other thing that we wanted to tackle was many different model architectures so here we want to quickly iterate on",
    "start": "693240",
    "end": "698640"
  },
  {
    "text": "different ones to find the best one that is going to lead to the best possible outcome for our reinforcement learning",
    "start": "698640",
    "end": "704940"
  },
  {
    "text": "and so here we have a pretty standard ml training Loop we have a bunch of battles we transform the data into the neural",
    "start": "704940",
    "end": "711720"
  },
  {
    "text": "network input we run our train Loop and then we get out a model",
    "start": "711720",
    "end": "716899"
  },
  {
    "text": "the types of models that we want to run through this we could have just a simple feed forward neural network so here are",
    "start": "716940",
    "end": "722880"
  },
  {
    "text": "the inputs we've thrown away the position information we've thrown away the item information and in fact we're",
    "start": "722880",
    "end": "728339"
  },
  {
    "text": "just sending what Champions you have on the board we look at this as our Baseline so this is the floor of how well we can do predicting the damage",
    "start": "728339",
    "end": "734640"
  },
  {
    "text": "probabilities for those battles if we start in bringing in more additional information so here we have",
    "start": "734640",
    "end": "740760"
  },
  {
    "text": "positional information coming in now we can leverage convolutions to sort of better understand how one player's",
    "start": "740760",
    "end": "746100"
  },
  {
    "text": "position affects a one Champions position affects another champion and we get a better model out of this",
    "start": "746100",
    "end": "751440"
  },
  {
    "text": "architecture and then lastly we can go full Transformers on it and bring in uh",
    "start": "751440",
    "end": "756540"
  },
  {
    "text": "all the champion information position now is encoded as an integer and the model can learn to attend to where it",
    "start": "756540",
    "end": "763019"
  },
  {
    "text": "wants to and pay attention to the champion matchups that are important now one key thing on all three of these",
    "start": "763019",
    "end": "769440"
  },
  {
    "text": "is if you notice all of them have very different inputs and that's going to be a theme that we talk a little bit more about as we go forward",
    "start": "769440",
    "end": "777240"
  },
  {
    "text": "so here we have uh I'm going to focus on the first part of that training Loop which is really just have battles in",
    "start": "777240",
    "end": "782820"
  },
  {
    "text": "transform it into neural network input and what that looks like for us is we have a bunch of battles and sort of a",
    "start": "782820",
    "end": "787920"
  },
  {
    "text": "standard Json format we run it through a small bit of python to encode that battle into the correct neural network",
    "start": "787920",
    "end": "794399"
  },
  {
    "text": "input so this code is associated with each architecture each architecture",
    "start": "794399",
    "end": "799860"
  },
  {
    "text": "knows how to take a battle and turn it into the stuff that it needs for training and the output is then a",
    "start": "799860",
    "end": "804899"
  },
  {
    "text": "dictionary of inputs to numpy arrays so a very simple function but a very",
    "start": "804899",
    "end": "810480"
  },
  {
    "text": "important one that we can use to feed into these large-scale distributed systems",
    "start": "810480",
    "end": "816360"
  },
  {
    "text": "so we set out first with the spark workflow and here we have two jobs uh",
    "start": "816360",
    "end": "822180"
  },
  {
    "text": "the first job will read the battles in run the transform step which is actually just doing that map function I alluded",
    "start": "822180",
    "end": "828000"
  },
  {
    "text": "to earlier and it's going to write out TF records files so we are a tensorflow shop we use TF records and TF data in",
    "start": "828000",
    "end": "836220"
  },
  {
    "text": "order to keep the GPU humming and this right is done in spark where we're",
    "start": "836220",
    "end": "843180"
  },
  {
    "text": "basically going right to TF record and it's inferring the schema along the way so that write scheme is going to be",
    "start": "843180",
    "end": "848339"
  },
  {
    "text": "important we have a second job who reads these TF records using TF data runs it through",
    "start": "848339",
    "end": "854639"
  },
  {
    "text": "the standard Cash training Loop and will write out the Keras model then solving our problem",
    "start": "854639",
    "end": "859920"
  },
  {
    "text": "now this read Loop you have to define the schema that you're going to read so that it can parse the TF record file and",
    "start": "859920",
    "end": "866040"
  },
  {
    "text": "generate the right data set to train on and what we found was quite often the",
    "start": "866040",
    "end": "871260"
  },
  {
    "text": "inference process in spark would pick the wrong type for a some data being",
    "start": "871260",
    "end": "876540"
  },
  {
    "text": "written and so your schema for right Miss was did not match up with your schema from read the errors you get of",
    "start": "876540",
    "end": "882779"
  },
  {
    "text": "that are pretty opaque we lost a lot of time debugging these things and it really slowed down our ability to",
    "start": "882779",
    "end": "888540"
  },
  {
    "text": "iterate on these different model architectures so we built array workflow that",
    "start": "888540",
    "end": "893760"
  },
  {
    "text": "leveraged uh Ray data rate train and Ray tune and here we're reading the battle",
    "start": "893760",
    "end": "898920"
  },
  {
    "text": "we're doing the transform as part of raid data then we do the training loop as part of raytrain and it's going to",
    "start": "898920",
    "end": "904380"
  },
  {
    "text": "write out that same model now nothing's ever free here we recognize that the other problem we had",
    "start": "904380",
    "end": "911579"
  },
  {
    "text": "in the previous workflow is that we also serialized a lot of different data sets now you have a governance problem where",
    "start": "911579",
    "end": "917040"
  },
  {
    "text": "you need to go back and clean them all up but you could reuse the same data sets",
    "start": "917040",
    "end": "922079"
  },
  {
    "text": "if it was valuable in the ray case we don't serialize out all those data sets which means we may need to recompute",
    "start": "922079",
    "end": "928740"
  },
  {
    "text": "that in code battle every time that's a trade-off that I'm willing to make because I'm here I'm optimizing for scientist iteration time not",
    "start": "928740",
    "end": "937440"
  },
  {
    "text": "um uh not how much compute that we're solving or not doing",
    "start": "937440",
    "end": "943820"
  },
  {
    "text": "a little bit of the ray workflow itself so here's the skeleton of the source code for the the training file We have",
    "start": "944040",
    "end": "950040"
  },
  {
    "text": "basically four functions we'll go through our encode battle here we have a model registry which keeps track of all",
    "start": "950040",
    "end": "956160"
  },
  {
    "text": "of our architectures those architectures know how to parse battles and generate the right neural network inputs so this",
    "start": "956160",
    "end": "963300"
  },
  {
    "text": "encode battle function just allows us to do that in a and as part of a map function",
    "start": "963300",
    "end": "968459"
  },
  {
    "text": "the data set here is we're just reading Json compressed Json we're re-partitioning it turns out the data",
    "start": "968459",
    "end": "975000"
  },
  {
    "text": "set that we're running when it's uh fully hydrated it's a it's about two",
    "start": "975000",
    "end": "980100"
  },
  {
    "text": "terabytes and so here we want to split it up into many different smaller batches so that we can run the map",
    "start": "980100",
    "end": "985620"
  },
  {
    "text": "function far more efficiently we do that for both training data and validation data and return that back",
    "start": "985620",
    "end": "992880"
  },
  {
    "text": "the train function is the biggest function although it's not very there's not a lot of complexity in here and I'm",
    "start": "992880",
    "end": "998639"
  },
  {
    "text": "really excited for the ray air additions to simplify this even further so here we have the same model registry",
    "start": "998639",
    "end": "1005360"
  },
  {
    "text": "we need to load the model we need to prepare it for training we check to see if there's any checkpoints out there if",
    "start": "1005360",
    "end": "1011420"
  },
  {
    "text": "they are we override the weights with the previous checkpoint and then we gather our data for validation we're",
    "start": "1011420",
    "end": "1017600"
  },
  {
    "text": "only going to randomize once at the beginning we're going to convert it to TF records and here this two TF records",
    "start": "1017600",
    "end": "1024199"
  },
  {
    "text": "which was causing this problem before with the schemas is much easier because it's fully encapsulated inside that",
    "start": "1024199",
    "end": "1030380"
  },
  {
    "text": "model so here we have the output signature the model knows what it's going to Output it also knows what input",
    "start": "1030380",
    "end": "1036020"
  },
  {
    "text": "features it's interested in and their types and so we can Define that one place it's all together these",
    "start": "1036020",
    "end": "1041480"
  },
  {
    "text": "architectures are all self-contained then we start the for Loop over our epochs so this is our just our standard",
    "start": "1041480",
    "end": "1047240"
  },
  {
    "text": "training Loop uh hard-coded 100 for fun we randomize our training data convert the training data to TF records as well",
    "start": "1047240",
    "end": "1053600"
  },
  {
    "text": "and then just call the regular Keras fit after each Epoch we save a checkpoint",
    "start": "1053600",
    "end": "1059419"
  },
  {
    "text": "and Report how well we've done on the validation data the tune function is our orchestrator so",
    "start": "1059419",
    "end": "1066860"
  },
  {
    "text": "its job is to make sure all the data is loaded and ready we create the raytrain",
    "start": "1066860",
    "end": "1071900"
  },
  {
    "text": "trainer again we said we're tensorflow we're using gpus in this case and then we call tune run and it orchestrates",
    "start": "1071900",
    "end": "1078740"
  },
  {
    "text": "everything for us so some of the benefits we get with Ray",
    "start": "1078740",
    "end": "1084980"
  },
  {
    "text": "is our iteration time is greatly reduced we can make changes without needing to re-materialize those TF records data",
    "start": "1084980",
    "end": "1090500"
  },
  {
    "text": "sets and this is sort of a standard data science Loop where you have the problem definition your ETL feature extraction",
    "start": "1090500",
    "end": "1097100"
  },
  {
    "text": "some training and then once you're happy with your model you put it into deployment the red line there is really for us to",
    "start": "1097100",
    "end": "1103940"
  },
  {
    "text": "iterate much more quickly as you saw we have different architectures we have different things there and I really want that Loop to be as tight and as quick as",
    "start": "1103940",
    "end": "1110539"
  },
  {
    "text": "possible going from laptop to distributed requires less steps so here I can spin",
    "start": "1110539",
    "end": "1116480"
  },
  {
    "text": "up a cluster in any scale we don't manage our own infrastructure again we're a small team of scientists I want",
    "start": "1116480",
    "end": "1122660"
  },
  {
    "text": "scientists focus on the the research I don't want them worried about spinning up infrastructure tearing it down and",
    "start": "1122660",
    "end": "1128539"
  },
  {
    "text": "all of those things so we we leverage any scale to do that going from our laptop to the cluster is quite easy and",
    "start": "1128539",
    "end": "1135140"
  },
  {
    "text": "quite fast and then going from single GPU to multiple GPU as we'll see some of these",
    "start": "1135140",
    "end": "1140960"
  },
  {
    "text": "training later in some of the slides you'll see how long it takes to train um we're at days now for like uh",
    "start": "1140960",
    "end": "1148100"
  },
  {
    "text": "multiple epochs of some of the Transformer work and so the plan here is we want to be able to go quickly to",
    "start": "1148100",
    "end": "1153799"
  },
  {
    "text": "multiple gpus and bring that down and make iteration time faster there too and then lastly we get integration with",
    "start": "1153799",
    "end": "1160280"
  },
  {
    "text": "Ray tune so we can do hyper parameter searches and find the best possible parameters for that model so that we can",
    "start": "1160280",
    "end": "1166340"
  },
  {
    "text": "we can really do the our reinforcement learning on the best model possible",
    "start": "1166340",
    "end": "1171679"
  },
  {
    "text": "so I set up an experiment this is not very scientific uh but it at least again here I'm optimizing for our scientists",
    "start": "1171679",
    "end": "1179299"
  },
  {
    "text": "ability to iterate so if Rey is in the ballpark of spark that's a win because",
    "start": "1179299",
    "end": "1184460"
  },
  {
    "text": "we don't have to materialize all the data sets we don't have the additional debugging Steps From the TF records and",
    "start": "1184460",
    "end": "1189740"
  },
  {
    "text": "so what I did was just generated multiple data sets from a million to 100 million battles we split you'll see",
    "start": "1189740",
    "end": "1196340"
  },
  {
    "text": "reported splitting in the transform time training time and then we reported both our spark time and our raid time to do",
    "start": "1196340",
    "end": "1202460"
  },
  {
    "text": "this training was done on a Nvidia T4 GPU and Cascade Lake CPUs",
    "start": "1202460",
    "end": "1209480"
  },
  {
    "text": "again our infrastructure we don't want to do it so I hand off spark provisioning to databricks Ray cluster",
    "start": "1209480",
    "end": "1215360"
  },
  {
    "text": "provisioning to any scale and let them worry about optimizing all of that for me",
    "start": "1215360",
    "end": "1221559"
  },
  {
    "text": "all right so this graph highlights uh some of the training so one row here is",
    "start": "1221780",
    "end": "1227840"
  },
  {
    "text": "a single model this model is our feed forward Network the four different graphs are different data set sizes so",
    "start": "1227840",
    "end": "1233480"
  },
  {
    "text": "we have one million on the left and 100 million on the right and then we have",
    "start": "1233480",
    "end": "1239179"
  },
  {
    "text": "two for each plot there's this timing it took in minutes for spark and the timing it took in minutes for Ray",
    "start": "1239179",
    "end": "1245539"
  },
  {
    "text": "uh what we can see a couple things and then blue is transform time and the red is training time well we can see on the",
    "start": "1245539",
    "end": "1251720"
  },
  {
    "text": "data set size from the uh the biggest data set is that Ray is actually performing faster than spark again not",
    "start": "1251720",
    "end": "1257600"
  },
  {
    "text": "terribly scientific because I didn't actually do multiple runs to get some confidence intervals around that but",
    "start": "1257600",
    "end": "1262880"
  },
  {
    "text": "what I wanted to see is are we in the same ballpark um and and the large data sets Ray is doing",
    "start": "1262880",
    "end": "1268640"
  },
  {
    "text": "better on the smaller data sets it's hit or miss and Spark and Spark tends to be",
    "start": "1268640",
    "end": "1274100"
  },
  {
    "text": "a little faster as we make a more uh complex model so",
    "start": "1274100",
    "end": "1279620"
  },
  {
    "text": "here we have our convolutional model the charts look very similar but they are different uh subtly and again we see the",
    "start": "1279620",
    "end": "1286460"
  },
  {
    "text": "same pattern where on the larger data sets rate it's doing better for us on smaller data sets they're at least very",
    "start": "1286460",
    "end": "1292220"
  },
  {
    "text": "comparable and then lastly our Transformer I mentioned uh takes much longer here all",
    "start": "1292220",
    "end": "1298280"
  },
  {
    "text": "of our time is spent training I want to call out that we have not engaged with our machine learning Engineers yet to",
    "start": "1298280",
    "end": "1303799"
  },
  {
    "text": "optimize this this is scientist code scientist iteration code so I think there are tons of room for improvement",
    "start": "1303799",
    "end": "1309860"
  },
  {
    "text": "there but the key thing is that our performance and our ability to iterate with one job versus two uh is is the",
    "start": "1309860",
    "end": "1317360"
  },
  {
    "text": "same so the takeaways Ray allows us faster",
    "start": "1317360",
    "end": "1323000"
  },
  {
    "text": "iteration on deep learning architectures at our scale going from laptop to distributed required less steps for us",
    "start": "1323000",
    "end": "1329140"
  },
  {
    "text": "our code was more concise we could pull a lot more together and really button it",
    "start": "1329140",
    "end": "1334280"
  },
  {
    "text": "up in nice ways and the speed is on par with what we got out of spark ntf data and then finally",
    "start": "1334280",
    "end": "1341360"
  },
  {
    "text": "using Ray unlocks these other features that are going to be valuable for us as we mentioned one Ray tune and eventually",
    "start": "1341360",
    "end": "1346880"
  },
  {
    "text": "Ray serve now some of you may be going back to the",
    "start": "1346880",
    "end": "1352039"
  },
  {
    "text": "original title of this talk and and had come in expecting to hear about how",
    "start": "1352039",
    "end": "1357080"
  },
  {
    "text": "we're bootstrapping RL with supervised learning through different mechanisms of behavior cloning or imitation learning",
    "start": "1357080",
    "end": "1362419"
  },
  {
    "text": "or are these other techniques to make our RL better and may feel a little Hoodwinked or Bamboozled and I apologize",
    "start": "1362419",
    "end": "1368179"
  },
  {
    "text": "for that that wasn't the purpose but I do want to just sort of walk through why this was valuable so the team set out to do deep",
    "start": "1368179",
    "end": "1375140"
  },
  {
    "text": "reinforcement learning for TFT that was the balloon that we were after and we were looking to hug",
    "start": "1375140",
    "end": "1380299"
  },
  {
    "text": "along the way uh we ran into some challenges so we did not have game server access and so we had to find a",
    "start": "1380299",
    "end": "1386840"
  },
  {
    "text": "way around that so we built a custom game and again that felt pretty good but then we had to code every Champion's",
    "start": "1386840",
    "end": "1393559"
  },
  {
    "text": "ability again setting us back and so so it was like okay now let's bring in a deep supervised learning model here",
    "start": "1393559",
    "end": "1400700"
  },
  {
    "text": "and then we started using spark and we hit other debugging challenges that slowed down our ability to iterate",
    "start": "1400700",
    "end": "1406039"
  },
  {
    "text": "and then we brought in Ray and sort of simplified that part of the training workload so we're in a good spot here",
    "start": "1406039",
    "end": "1412340"
  },
  {
    "text": "um and so I think the the key takeaways for me here is that training AI or deep reinforcement",
    "start": "1412340",
    "end": "1419120"
  },
  {
    "text": "learning for TFT is hard it's a multi-year project and we believe the",
    "start": "1419120",
    "end": "1424700"
  },
  {
    "text": "way we've set this up learning to play this game where we model the outcomes will transfer to the full game of TFT",
    "start": "1424700",
    "end": "1431480"
  },
  {
    "text": "now even if it doesn't transfer we have done we are able to work in",
    "start": "1431480",
    "end": "1437360"
  },
  {
    "text": "parallel with the game team so we're able to make progress on this really hard problem without impacting the",
    "start": "1437360",
    "end": "1442760"
  },
  {
    "text": "team's roadmap thereby allowing us to show the value of a good model at this game and what it's capable of as well as",
    "start": "1442760",
    "end": "1450380"
  },
  {
    "text": "make progress on how we would model the state model the action model all the different things in order to solve this",
    "start": "1450380",
    "end": "1456140"
  },
  {
    "text": "game so that when the game team does come and say we're ready for this let's do it on the real game we're ready and",
    "start": "1456140",
    "end": "1462020"
  },
  {
    "text": "and well ahead of the game there and I think the last takeaway is really that this path where we have this custom",
    "start": "1462020",
    "end": "1469039"
  },
  {
    "text": "game to transfer over allowed us to move very quickly um and get make progress on hard",
    "start": "1469039",
    "end": "1474919"
  },
  {
    "text": "reinforcement learning problems and so with that uh I thank you for your time and I think we",
    "start": "1474919",
    "end": "1482120"
  },
  {
    "text": "have time for one or two questions",
    "start": "1482120",
    "end": "1485679"
  },
  {
    "text": "[Applause] all right if we have questions for West I'll run over give you the mic",
    "start": "1487820",
    "end": "1495399"
  },
  {
    "text": "foreign with spark and uh Ray did you also like",
    "start": "1497539",
    "end": "1504679"
  },
  {
    "text": "make sure that the errors the training accuracy was same in both the cases or oh",
    "start": "1504679",
    "end": "1510260"
  },
  {
    "text": "um yeah so it's a good question and yeah the the training uh the end result is",
    "start": "1510260",
    "end": "1515780"
  },
  {
    "text": "performance on par with each other I mean it's not identical because of the same gradient pass but they reach uh the",
    "start": "1515780",
    "end": "1521539"
  },
  {
    "text": "same performance right and and one other question was so you set out to train an RL agent but you",
    "start": "1521539",
    "end": "1528200"
  },
  {
    "text": "ended up training a supervised learning agent that predicts the damage is that the cusp of that well no the rl8 we we",
    "start": "1528200",
    "end": "1536120"
  },
  {
    "text": "do RL at the core so we are playing our custom game and the supervised learning",
    "start": "1536120",
    "end": "1542059"
  },
  {
    "text": "is all about being able to circumvent or sidestep the the real gameplay mechanics so that we can make progress on the RL",
    "start": "1542059",
    "end": "1548539"
  },
  {
    "text": "Loop okay and and the goal of RL agent is to predict find strategies it's to play the",
    "start": "1548539",
    "end": "1554539"
  },
  {
    "text": "game at top tier levels um if you when you build an RL agent to play at that level you can then leverage",
    "start": "1554539",
    "end": "1560720"
  },
  {
    "text": "that agent in other ways so we can look at it for metagame balance are there strategies that are degenerate and that",
    "start": "1560720",
    "end": "1565940"
  },
  {
    "text": "we need to balance for um are is the meta interesting so the players want to actually play the game",
    "start": "1565940",
    "end": "1571460"
  },
  {
    "text": "uh so there's a whole lot of those questions we can answer with it we can also uh dumb it down and let it play",
    "start": "1571460",
    "end": "1577400"
  },
  {
    "text": "against players so that there's an easier onboarding experience we can also leverage it to do coaching so you have",
    "start": "1577400",
    "end": "1584240"
  },
  {
    "text": "an agent that now knows how to play the game at the highest levels you can actually ask it what they should have",
    "start": "1584240",
    "end": "1590299"
  },
  {
    "text": "done at this step instead of what they did and then you could use that as a coaching moment",
    "start": "1590299",
    "end": "1596360"
  },
  {
    "text": "thanks hi uh so I wanted to ask um the three",
    "start": "1596360",
    "end": "1604340"
  },
  {
    "text": "models that you were talking about um how good or what's the trade-off between",
    "start": "1604340",
    "end": "1610220"
  },
  {
    "text": "the three of them the fully connected Network versus the Transformer and how well do they perform like approximately",
    "start": "1610220",
    "end": "1617539"
  },
  {
    "text": "relative to the true simulator um they get up words of",
    "start": "1617539",
    "end": "1623720"
  },
  {
    "text": "so we have different as I mentioned it's learning a distribution and so measuring our losses on that have been fun",
    "start": "1623720",
    "end": "1630860"
  },
  {
    "text": "um and a challenge and so it's doing well enough that we can make forward progress on the RL task so",
    "start": "1630860",
    "end": "1638659"
  },
  {
    "text": "bad would look like it just abusing the system and not learning good strategies the things that are coming out of RL are",
    "start": "1638659",
    "end": "1644779"
  },
  {
    "text": "good and therefore we believe that the combat simulator is there and like each jump from like uh so the Baseline is",
    "start": "1644779",
    "end": "1654200"
  },
  {
    "text": "like in the 70 accuracy and then we added the convolutions and got to like 75 and then the Transformers bring us up",
    "start": "1654200",
    "end": "1661820"
  },
  {
    "text": "over 80 um but it the accuracy again is kind of tricky to measure but that kind of gives",
    "start": "1661820",
    "end": "1667820"
  },
  {
    "text": "you an idea of where we're at there's still room to go um but yeah we're doing pretty well",
    "start": "1667820",
    "end": "1673039"
  },
  {
    "text": "one other quick question um you mentioned that the TFT game itself is you know continuously changing and being",
    "start": "1673039",
    "end": "1678799"
  },
  {
    "text": "developed so um how are you how do you incorporate that into training your like model",
    "start": "1678799",
    "end": "1685100"
  },
  {
    "text": "whichever of the three you're using um like are you retraining from scratch every time you have a major change or",
    "start": "1685100",
    "end": "1691760"
  },
  {
    "text": "are you planning to do some sort of like lifelong or continual the plan is more lifelong learning",
    "start": "1691760",
    "end": "1697820"
  },
  {
    "text": "um so we've had success with other games where uh we're able to adapt to a new",
    "start": "1697820",
    "end": "1703460"
  },
  {
    "text": "set by just sort of relaxing our learning rate at the beginning and sort of continuing",
    "start": "1703460",
    "end": "1710000"
  },
  {
    "text": "from where we left off and give it a several hundred thousand more games to sort of figure out new sets",
    "start": "1710000",
    "end": "1715039"
  },
  {
    "text": "um that's our plan if it doesn't work then we'll adapt and figure out something else uh trying to avoid going",
    "start": "1715039",
    "end": "1720799"
  },
  {
    "text": "from scratch every time uh and really simplify our Cycles",
    "start": "1720799",
    "end": "1726400"
  },
  {
    "text": "so how are you guys encoding uh interactions uh between players within rounds like adding items or are is all",
    "start": "1732260",
    "end": "1738559"
  },
  {
    "text": "your data just from the initial board State uh for the supervised learning it's those boards so the items are",
    "start": "1738559",
    "end": "1744320"
  },
  {
    "text": "already on there from the RL game there are different phases of the game that we've sort of built our game to contain",
    "start": "1744320",
    "end": "1750080"
  },
  {
    "text": "so uh before the the battle phase there is a a round where the actions that are",
    "start": "1750080",
    "end": "1756500"
  },
  {
    "text": "available are put items on Champions and you can shoot the AI can choose to do that those actions",
    "start": "1756500",
    "end": "1762860"
  },
  {
    "text": "I understand there are some mechanics like in in previous where you can actually like take action in the middle",
    "start": "1762860",
    "end": "1768500"
  },
  {
    "text": "of like fights right so are you like encoding every time step",
    "start": "1768500",
    "end": "1774460"
  },
  {
    "text": "um so in those cases uh it's rare uh I think",
    "start": "1775520",
    "end": "1781520"
  },
  {
    "text": "that set will break this model um or those types of those types of sets",
    "start": "1781520",
    "end": "1787340"
  },
  {
    "text": "will and we'll need to adapt for that and leverage a a different representation but for now the sets that",
    "start": "1787340",
    "end": "1794000"
  },
  {
    "text": "we've been using for this one I've not had those mechanics in them so we haven't had to worry about them",
    "start": "1794000",
    "end": "1800379"
  },
  {
    "text": "uh for TFT especially at the highest level you typically have players that",
    "start": "1805340",
    "end": "1810380"
  },
  {
    "text": "look at the other players board so that you can counter them and stuff like that does that also get encoded to the inputs",
    "start": "1810380",
    "end": "1815960"
  },
  {
    "text": "today or is that is encoded into the RL side um not so we've done it in different",
    "start": "1815960",
    "end": "1822380"
  },
  {
    "text": "ways um trying to respect a player's ability to jump around to the different boards",
    "start": "1822380",
    "end": "1827720"
  },
  {
    "text": "and how much they can take in um and so yeah we we are offering up at",
    "start": "1827720",
    "end": "1833720"
  },
  {
    "text": "least counts and different things for them to for the agent to reason about what's likely to be on the other team's board",
    "start": "1833720",
    "end": "1842200"
  },
  {
    "text": "all right that's all the time we have questions for now if you have more and if Wes has time feel free to come ask him",
    "start": "1845539",
    "end": "1851000"
  },
  {
    "text": "um but yeah thanks again Wes yep thank you",
    "start": "1851000",
    "end": "1855278"
  }
]