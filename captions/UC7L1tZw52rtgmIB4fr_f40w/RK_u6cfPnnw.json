[
  {
    "text": "as you just introduced uh Winston I'm",
    "start": "3240",
    "end": "5339"
  },
  {
    "text": "product manager in Google cloud with",
    "start": "5339",
    "end": "7140"
  },
  {
    "text": "Richard I'm a senior engineer on Google",
    "start": "7140",
    "end": "9900"
  },
  {
    "text": "kubernetes engine cool yeah thanks for",
    "start": "9900",
    "end": "12599"
  },
  {
    "text": "joining our session we'll talk about",
    "start": "12599",
    "end": "13799"
  },
  {
    "text": "using tpus through kubrey",
    "start": "13799",
    "end": "18139"
  },
  {
    "text": "um so as we're all witnessing we're",
    "start": "18480",
    "end": "20100"
  },
  {
    "text": "definitely in a new Evolution of AI",
    "start": "20100",
    "end": "22140"
  },
  {
    "text": "where large ml models are demonstrating",
    "start": "22140",
    "end": "24420"
  },
  {
    "text": "human-like capabilities this is opening",
    "start": "24420",
    "end": "27060"
  },
  {
    "text": "up a whole new",
    "start": "27060",
    "end": "29640"
  },
  {
    "text": "opportunities to apply ai ai in real",
    "start": "29640",
    "end": "32758"
  },
  {
    "text": "world use cases that weren't available",
    "start": "32759",
    "end": "34140"
  },
  {
    "text": "before",
    "start": "34140",
    "end": "36180"
  },
  {
    "text": "one of the dimensions driving those new",
    "start": "36180",
    "end": "38640"
  },
  {
    "text": "capabilities is the expanding size of",
    "start": "38640",
    "end": "41160"
  },
  {
    "text": "these ml models over the past five years",
    "start": "41160",
    "end": "43920"
  },
  {
    "text": "or so they've been growing 10 acts year",
    "start": "43920",
    "end": "46140"
  },
  {
    "text": "over year that's an order of magnitude",
    "start": "46140",
    "end": "47879"
  },
  {
    "text": "part of that is driven by AI compute",
    "start": "47879",
    "end": "51840"
  },
  {
    "text": "accelerated capabilities but in turn",
    "start": "51840",
    "end": "53879"
  },
  {
    "text": "it's also driving increased demand for",
    "start": "53879",
    "end": "56340"
  },
  {
    "text": "that accelerated compute for decades",
    "start": "56340",
    "end": "59039"
  },
  {
    "text": "Google has been a leader in machine",
    "start": "59039",
    "end": "61260"
  },
  {
    "text": "learning and we've been serving these",
    "start": "61260",
    "end": "64378"
  },
  {
    "text": "type of models inside YouTube Maps and",
    "start": "64379",
    "end": "66900"
  },
  {
    "text": "search all the way from the original",
    "start": "66900",
    "end": "69180"
  },
  {
    "text": "Innovation that is the seed for all of",
    "start": "69180",
    "end": "71460"
  },
  {
    "text": "gen AI with the Transformer to do so we",
    "start": "71460",
    "end": "74640"
  },
  {
    "text": "had to Custom Design our own chips to",
    "start": "74640",
    "end": "77280"
  },
  {
    "text": "deliver this global scale performance",
    "start": "77280",
    "end": "79619"
  },
  {
    "text": "so enter left tpus",
    "start": "79619",
    "end": "84140"
  },
  {
    "text": "as we just announced that next that we",
    "start": "84479",
    "end": "87240"
  },
  {
    "text": "now have tpu's v5e in preview and so",
    "start": "87240",
    "end": "91979"
  },
  {
    "text": "this further enables customers to pursue",
    "start": "91979",
    "end": "94080"
  },
  {
    "text": "these transformative opportunities in",
    "start": "94080",
    "end": "96479"
  },
  {
    "text": "AIML the same way Google has done for",
    "start": "96479",
    "end": "98520"
  },
  {
    "text": "our own products and services",
    "start": "98520",
    "end": "101539"
  },
  {
    "text": "the two main benefits that we expect",
    "start": "101700",
    "end": "103979"
  },
  {
    "text": "customers to enjoy either using tpus is",
    "start": "103979",
    "end": "107579"
  },
  {
    "text": "one efficiency and two scalability",
    "start": "107579",
    "end": "112460"
  },
  {
    "text": "so the first one efficiency oh I can't",
    "start": "113640",
    "end": "116220"
  },
  {
    "text": "see this chart there are sorry so here",
    "start": "116220",
    "end": "118619"
  },
  {
    "text": "we have a chart demonstrating the",
    "start": "118619",
    "end": "120060"
  },
  {
    "text": "performance of the recent TPU v5e over",
    "start": "120060",
    "end": "123659"
  },
  {
    "text": "the previous generation on the y-axis",
    "start": "123659",
    "end": "126299"
  },
  {
    "text": "you have the relative inferences per",
    "start": "126299",
    "end": "128700"
  },
  {
    "text": "dollar by relative the V4 is normalized",
    "start": "128700",
    "end": "131400"
  },
  {
    "text": "to one and then we show the Improvement",
    "start": "131400",
    "end": "133440"
  },
  {
    "text": "performance with the v5e and across the",
    "start": "133440",
    "end": "136860"
  },
  {
    "text": "horizontal axis you can see it",
    "start": "136860",
    "end": "138300"
  },
  {
    "text": "demonstrated across a variety of the",
    "start": "138300",
    "end": "140580"
  },
  {
    "text": "different gen AI models and so you see a",
    "start": "140580",
    "end": "142980"
  },
  {
    "text": "range of actually a 1.7 to 2.5 x",
    "start": "142980",
    "end": "145620"
  },
  {
    "text": "Improvement in the relative inferences",
    "start": "145620",
    "end": "148020"
  },
  {
    "text": "per dollar and so you know like many",
    "start": "148020",
    "end": "150900"
  },
  {
    "text": "things in these days it's really about",
    "start": "150900",
    "end": "152640"
  },
  {
    "text": "that performance per cost and optimizing",
    "start": "152640",
    "end": "155099"
  },
  {
    "text": "what you can deliver there",
    "start": "155099",
    "end": "157920"
  },
  {
    "text": "uh the second aspect is scalability so",
    "start": "157920",
    "end": "161280"
  },
  {
    "text": "tpus are designed with a special",
    "start": "161280",
    "end": "164459"
  },
  {
    "text": "topology and high-speed interconnect",
    "start": "164459",
    "end": "166260"
  },
  {
    "text": "such that each chip can expand and act",
    "start": "166260",
    "end": "169379"
  },
  {
    "text": "in unison that allows you to serve from",
    "start": "169379",
    "end": "172319"
  },
  {
    "text": "medium models to super large the largest",
    "start": "172319",
    "end": "175440"
  },
  {
    "text": "of models as you can see in the table",
    "start": "175440",
    "end": "177720"
  },
  {
    "text": "there one TPU V5 V chip can serve a 13",
    "start": "177720",
    "end": "180599"
  },
  {
    "text": "billion parameter model and then you can",
    "start": "180599",
    "end": "183120"
  },
  {
    "text": "seamlessly scale all the way up to 256",
    "start": "183120",
    "end": "185580"
  },
  {
    "text": "chips which can go on the order of a 2",
    "start": "185580",
    "end": "187860"
  },
  {
    "text": "trillion parameter model so you know the",
    "start": "187860",
    "end": "190080"
  },
  {
    "text": "largest of the large",
    "start": "190080",
    "end": "191540"
  },
  {
    "text": "furthermore we also have multi-slice",
    "start": "191540",
    "end": "194519"
  },
  {
    "text": "technology which enables near linear",
    "start": "194519",
    "end": "197099"
  },
  {
    "text": "scaling all the way up to tens of",
    "start": "197099",
    "end": "198959"
  },
  {
    "text": "thousands of chips",
    "start": "198959",
    "end": "200940"
  },
  {
    "text": "great so now that you understand some of",
    "start": "200940",
    "end": "203280"
  },
  {
    "text": "the benefits our customers are",
    "start": "203280",
    "end": "204599"
  },
  {
    "text": "appreciating by leveraging tpus Richard",
    "start": "204599",
    "end": "207060"
  },
  {
    "text": "can you tell us about some of the",
    "start": "207060",
    "end": "208200"
  },
  {
    "text": "differences in using tpus",
    "start": "208200",
    "end": "210480"
  },
  {
    "text": "all right thanks Wesley so uh as we just",
    "start": "210480",
    "end": "213360"
  },
  {
    "text": "learned from innocent uh tpus give you",
    "start": "213360",
    "end": "216659"
  },
  {
    "text": "high levels of efficiency and",
    "start": "216659",
    "end": "219680"
  },
  {
    "text": "scalability so I'm going to explain a",
    "start": "219680",
    "end": "222720"
  },
  {
    "text": "few ways that tpus are different from",
    "start": "222720",
    "end": "225120"
  },
  {
    "text": "traditional GPU workloads so when we",
    "start": "225120",
    "end": "228480"
  },
  {
    "text": "think of",
    "start": "228480",
    "end": "229580"
  },
  {
    "text": "GPU workloads right gpus are individual",
    "start": "229580",
    "end": "233040"
  },
  {
    "text": "devices attached to VMS",
    "start": "233040",
    "end": "235319"
  },
  {
    "text": "so what that means is you can have a VM",
    "start": "235319",
    "end": "238260"
  },
  {
    "text": "and a VM can have some number of gpus",
    "start": "238260",
    "end": "241319"
  },
  {
    "text": "attached to it like up to 16 gpus right",
    "start": "241319",
    "end": "244980"
  },
  {
    "text": "so",
    "start": "244980",
    "end": "246659"
  },
  {
    "text": "what does that mean when you're trying",
    "start": "246659",
    "end": "248459"
  },
  {
    "text": "to schedule workloads kubernetes",
    "start": "248459",
    "end": "250439"
  },
  {
    "text": "workloads on the gpus what you do is you",
    "start": "250439",
    "end": "254760"
  },
  {
    "text": "you have a worker parts that reserve",
    "start": "254760",
    "end": "258660"
  },
  {
    "text": "some number of gpus that are on that VM",
    "start": "258660",
    "end": "262440"
  },
  {
    "text": "and if you're using time sharing or",
    "start": "262440",
    "end": "264660"
  },
  {
    "text": "something like that you can even share a",
    "start": "264660",
    "end": "267479"
  },
  {
    "text": "GPU across multiple different workloads",
    "start": "267479",
    "end": "272419"
  },
  {
    "text": "for tpus it's a little bit different so",
    "start": "272419",
    "end": "275520"
  },
  {
    "text": "we have a really simple diagram here so",
    "start": "275520",
    "end": "279360"
  },
  {
    "text": "imagine you're looking inside at the TPU",
    "start": "279360",
    "end": "281759"
  },
  {
    "text": "right inside each one of those boxes you",
    "start": "281759",
    "end": "285000"
  },
  {
    "text": "will see",
    "start": "285000",
    "end": "285979"
  },
  {
    "text": "four chips connected to a single host",
    "start": "285979",
    "end": "288979"
  },
  {
    "text": "connected by ICI which is an internship",
    "start": "288979",
    "end": "291780"
  },
  {
    "text": "interconnect",
    "start": "291780",
    "end": "293639"
  },
  {
    "text": "and in kubernetes when you're scheduling",
    "start": "293639",
    "end": "296340"
  },
  {
    "text": "workloads each of your workloads is",
    "start": "296340",
    "end": "299820"
  },
  {
    "text": "going to be on that part and the part is",
    "start": "299820",
    "end": "301740"
  },
  {
    "text": "going to reserve all the TPU chips",
    "start": "301740",
    "end": "303960"
  },
  {
    "text": "attached to your host",
    "start": "303960",
    "end": "306479"
  },
  {
    "text": "and uh",
    "start": "306479",
    "end": "308100"
  },
  {
    "text": "we heard about Vincent that you can use",
    "start": "308100",
    "end": "310500"
  },
  {
    "text": "a multi-slides to connect together tpus",
    "start": "310500",
    "end": "313560"
  },
  {
    "text": "right so what does that look like so in",
    "start": "313560",
    "end": "316320"
  },
  {
    "text": "this picture you can see that we have",
    "start": "316320",
    "end": "318600"
  },
  {
    "text": "several tpus connected together with a",
    "start": "318600",
    "end": "321180"
  },
  {
    "text": "high bandwidth memory and that's how I",
    "start": "321180",
    "end": "323400"
  },
  {
    "text": "was able to achieve that uh high",
    "start": "323400",
    "end": "325380"
  },
  {
    "text": "throughput",
    "start": "325380",
    "end": "326539"
  },
  {
    "text": "so in Google kubernetes engine each of",
    "start": "326539",
    "end": "330960"
  },
  {
    "text": "these corresponds to a node pool",
    "start": "330960",
    "end": "333960"
  },
  {
    "text": "and when you create a snowball you must",
    "start": "333960",
    "end": "336960"
  },
  {
    "text": "allocate the snowball with a specific",
    "start": "336960",
    "end": "339539"
  },
  {
    "text": "topology type for example in this case",
    "start": "339539",
    "end": "341880"
  },
  {
    "text": "where we're seeing as a two by two by",
    "start": "341880",
    "end": "345539"
  },
  {
    "text": "four which is the total number of chips",
    "start": "345539",
    "end": "348900"
  },
  {
    "text": "and TPU hosts",
    "start": "348900",
    "end": "352500"
  },
  {
    "text": "and so",
    "start": "352500",
    "end": "355259"
  },
  {
    "text": "this also means that each of your TPU",
    "start": "355259",
    "end": "358259"
  },
  {
    "text": "hosts must be aware of its own",
    "start": "358259",
    "end": "360419"
  },
  {
    "text": "environment like its own index and then",
    "start": "360419",
    "end": "363660"
  },
  {
    "text": "host names of all the other hosts in the",
    "start": "363660",
    "end": "366780"
  },
  {
    "text": "TPU slides",
    "start": "366780",
    "end": "368460"
  },
  {
    "text": "so we'll come back to this point in a",
    "start": "368460",
    "end": "372000"
  },
  {
    "text": "little bit",
    "start": "372000",
    "end": "374360"
  },
  {
    "text": "so we went over the architectural",
    "start": "375360",
    "end": "377880"
  },
  {
    "text": "difference architecture model right",
    "start": "377880",
    "end": "380340"
  },
  {
    "text": "another major difference is the",
    "start": "380340",
    "end": "382919"
  },
  {
    "text": "programming pattern",
    "start": "382919",
    "end": "384300"
  },
  {
    "text": "so typically typically gpus access chips",
    "start": "384300",
    "end": "388319"
  },
  {
    "text": "through low-level libraries like Cuda",
    "start": "388319",
    "end": "390919"
  },
  {
    "text": "for tpus we access chips through a",
    "start": "390919",
    "end": "395340"
  },
  {
    "text": "higher level compiler like xl8",
    "start": "395340",
    "end": "397740"
  },
  {
    "text": "and using LTE comes with a bit of a",
    "start": "397740",
    "end": "401100"
  },
  {
    "text": "trade-off right so the advantage is that",
    "start": "401100",
    "end": "404759"
  },
  {
    "text": "it does a lot of optimizations for you",
    "start": "404759",
    "end": "407460"
  },
  {
    "text": "the disadvantage is that it's less",
    "start": "407460",
    "end": "410100"
  },
  {
    "text": "customized customizable",
    "start": "410100",
    "end": "413539"
  },
  {
    "text": "in addition tpus mostly run with a",
    "start": "413539",
    "end": "417000"
  },
  {
    "text": "multi-controller model which means that",
    "start": "417000",
    "end": "419460"
  },
  {
    "text": "if you imagine what you don't have a",
    "start": "419460",
    "end": "422280"
  },
  {
    "text": "workload orchestrator what that means is",
    "start": "422280",
    "end": "424860"
  },
  {
    "text": "that you must SSH into each of your",
    "start": "424860",
    "end": "427860"
  },
  {
    "text": "hosts and make sure that the same",
    "start": "427860",
    "end": "429180"
  },
  {
    "text": "workload is running",
    "start": "429180",
    "end": "432440"
  },
  {
    "text": "and finally we have the a difference in",
    "start": "432539",
    "end": "435240"
  },
  {
    "text": "the ecosystem so gpus have been around a",
    "start": "435240",
    "end": "438660"
  },
  {
    "text": "lot longer so as a consequence it has a",
    "start": "438660",
    "end": "441660"
  },
  {
    "text": "more comprehensive ecosystem of ml",
    "start": "441660",
    "end": "444360"
  },
  {
    "text": "Frameworks",
    "start": "444360",
    "end": "446699"
  },
  {
    "text": "so those are a few ways in which tpus",
    "start": "446699",
    "end": "449759"
  },
  {
    "text": "are different from GPU workloads today",
    "start": "449759",
    "end": "452699"
  },
  {
    "text": "now what we're trying to solve is to",
    "start": "452699",
    "end": "454860"
  },
  {
    "text": "make the TPU experience mirror gpus as",
    "start": "454860",
    "end": "457560"
  },
  {
    "text": "much as possible",
    "start": "457560",
    "end": "458880"
  },
  {
    "text": "uh so now Winston is going to talk about",
    "start": "458880",
    "end": "462300"
  },
  {
    "text": "a few ways that uh we can make that",
    "start": "462300",
    "end": "464940"
  },
  {
    "text": "happen awesome so we heard about some of",
    "start": "464940",
    "end": "467039"
  },
  {
    "text": "the benefits that customers to get out",
    "start": "467039",
    "end": "468419"
  },
  {
    "text": "of tpus but that does come with some of",
    "start": "468419",
    "end": "470699"
  },
  {
    "text": "the differences in managing it so you",
    "start": "470699",
    "end": "472259"
  },
  {
    "text": "get that optimized performance you get",
    "start": "472259",
    "end": "474180"
  },
  {
    "text": "the scale but it's a slightly more",
    "start": "474180",
    "end": "475860"
  },
  {
    "text": "complex topology kubernetes helps you",
    "start": "475860",
    "end": "478319"
  },
  {
    "text": "manage complex deployments it's a",
    "start": "478319",
    "end": "481020"
  },
  {
    "text": "trusted standard for deploying operating",
    "start": "481020",
    "end": "483180"
  },
  {
    "text": "and monitoring really any type of",
    "start": "483180",
    "end": "484440"
  },
  {
    "text": "application furthermore it has a robust",
    "start": "484440",
    "end": "487259"
  },
  {
    "text": "ecosystem of OSS tools that are both",
    "start": "487259",
    "end": "490919"
  },
  {
    "text": "mature and really apply to any workload",
    "start": "490919",
    "end": "492960"
  },
  {
    "text": "which you know obviously where Ray comes",
    "start": "492960",
    "end": "494880"
  },
  {
    "text": "in Ray just offers a delightful",
    "start": "494880",
    "end": "497520"
  },
  {
    "text": "experience data scientists love it",
    "start": "497520",
    "end": "499680"
  },
  {
    "text": "provides that simple interface such that",
    "start": "499680",
    "end": "501840"
  },
  {
    "text": "you can build deploy and manage your ml",
    "start": "501840",
    "end": "504479"
  },
  {
    "text": "workloads across distributed compute",
    "start": "504479",
    "end": "506460"
  },
  {
    "text": "right it abstracts away the resources so",
    "start": "506460",
    "end": "508259"
  },
  {
    "text": "you just don't need to worry about that",
    "start": "508259",
    "end": "509720"
  },
  {
    "text": "and ultimately we see this in almost",
    "start": "509720",
    "end": "511919"
  },
  {
    "text": "three layers right you have your",
    "start": "511919",
    "end": "513120"
  },
  {
    "text": "infrastructure your tpus so strong",
    "start": "513120",
    "end": "515159"
  },
  {
    "text": "performance there and then kubernetes to",
    "start": "515159",
    "end": "517740"
  },
  {
    "text": "handle your scalable orchestration and",
    "start": "517740",
    "end": "519899"
  },
  {
    "text": "reduce complexity in your deployment",
    "start": "519899",
    "end": "521940"
  },
  {
    "text": "then lastly Rey to really simplify and",
    "start": "521940",
    "end": "524700"
  },
  {
    "text": "give you that great experience to",
    "start": "524700",
    "end": "528180"
  },
  {
    "text": "distribute your applications",
    "start": "528180",
    "end": "531800"
  },
  {
    "text": "uh and so how do you actually run the",
    "start": "531839",
    "end": "535800"
  },
  {
    "text": "ray onto tpus",
    "start": "535800",
    "end": "538440"
  },
  {
    "text": "so uh this is sort of a simple mental",
    "start": "538440",
    "end": "541860"
  },
  {
    "text": "picture of uh what a rayon tpus cluster",
    "start": "541860",
    "end": "546600"
  },
  {
    "text": "should look like so here we have a",
    "start": "546600",
    "end": "549300"
  },
  {
    "text": "number of uh four array workers each is",
    "start": "549300",
    "end": "553200"
  },
  {
    "text": "on a different kubernetes part and as",
    "start": "553200",
    "end": "556500"
  },
  {
    "text": "mentioned earlier each part is deployed",
    "start": "556500",
    "end": "559800"
  },
  {
    "text": "directly on a TPU host which means that",
    "start": "559800",
    "end": "563040"
  },
  {
    "text": "they Reserve all the TPU chips that are",
    "start": "563040",
    "end": "565320"
  },
  {
    "text": "on their host so",
    "start": "565320",
    "end": "568080"
  },
  {
    "text": "as we mentioned earlier if we have a",
    "start": "568080",
    "end": "572820"
  },
  {
    "text": "multiple host topology",
    "start": "572820",
    "end": "575480"
  },
  {
    "text": "each of the workers must first of all",
    "start": "575480",
    "end": "578940"
  },
  {
    "text": "they must have a unique index and second",
    "start": "578940",
    "end": "581580"
  },
  {
    "text": "they must know the host names of all the",
    "start": "581580",
    "end": "584880"
  },
  {
    "text": "other hosts that are in the same",
    "start": "584880",
    "end": "586260"
  },
  {
    "text": "topology so here we have an example of",
    "start": "586260",
    "end": "590640"
  },
  {
    "text": "this uh this last worker in this uh this",
    "start": "590640",
    "end": "594779"
  },
  {
    "text": "TPU cluster it has a worker ID3 set to",
    "start": "594779",
    "end": "598140"
  },
  {
    "text": "it and it should also be initialized",
    "start": "598140",
    "end": "601740"
  },
  {
    "text": "with the host names of all the other",
    "start": "601740",
    "end": "604080"
  },
  {
    "text": "hosts in the same topology",
    "start": "604080",
    "end": "607019"
  },
  {
    "text": "and we'll see you later how cubery makes",
    "start": "607019",
    "end": "609899"
  },
  {
    "text": "a",
    "start": "609899",
    "end": "610860"
  },
  {
    "text": "make all this possible",
    "start": "610860",
    "end": "614540"
  },
  {
    "text": "so here's a sample workload of how you",
    "start": "615360",
    "end": "619680"
  },
  {
    "text": "run uh you use a ray to schedule",
    "start": "619680",
    "end": "622880"
  },
  {
    "text": "workloads on tpus",
    "start": "622880",
    "end": "625760"
  },
  {
    "text": "so as mentioned earlier",
    "start": "625760",
    "end": "628100"
  },
  {
    "text": "TPU uses multi-controller model where",
    "start": "628100",
    "end": "631260"
  },
  {
    "text": "the same program must be running on all",
    "start": "631260",
    "end": "633720"
  },
  {
    "text": "the hosts so instead of SSH into each",
    "start": "633720",
    "end": "637380"
  },
  {
    "text": "host and running the same code we can",
    "start": "637380",
    "end": "639779"
  },
  {
    "text": "just have a single driver program here",
    "start": "639779",
    "end": "642959"
  },
  {
    "text": "is us we Define a simple function that",
    "start": "642959",
    "end": "646079"
  },
  {
    "text": "uses Jax and each that function Reserves",
    "start": "646079",
    "end": "651899"
  },
  {
    "text": "a number of TPU resources and we can",
    "start": "651899",
    "end": "654959"
  },
  {
    "text": "send non-blocking calls to each worker",
    "start": "654959",
    "end": "657600"
  },
  {
    "text": "to schedule the workload on other",
    "start": "657600",
    "end": "659700"
  },
  {
    "text": "workers and then this would produce the",
    "start": "659700",
    "end": "662100"
  },
  {
    "text": "uh the output that we want",
    "start": "662100",
    "end": "665899"
  },
  {
    "text": "so now we're going to walk through of",
    "start": "669060",
    "end": "671220"
  },
  {
    "text": "how this works end to end",
    "start": "671220",
    "end": "673740"
  },
  {
    "text": "this is a very simple terraform snippet",
    "start": "673740",
    "end": "678959"
  },
  {
    "text": "um this uh talks to Google kubernetes",
    "start": "678959",
    "end": "682380"
  },
  {
    "text": "engine to provision the TPU node pool",
    "start": "682380",
    "end": "685079"
  },
  {
    "text": "with a specific topology type",
    "start": "685079",
    "end": "687899"
  },
  {
    "text": "behind the scenes Google kubernetes",
    "start": "687899",
    "end": "690360"
  },
  {
    "text": "engine will spin up the VMS create the",
    "start": "690360",
    "end": "693779"
  },
  {
    "text": "dopool and installs the TPU drivers and",
    "start": "693779",
    "end": "697079"
  },
  {
    "text": "device plugin on the worker containers",
    "start": "697079",
    "end": "702019"
  },
  {
    "text": "so next we're going to use kubray to",
    "start": "703380",
    "end": "706560"
  },
  {
    "text": "make the user experience for",
    "start": "706560",
    "end": "708060"
  },
  {
    "text": "provisioning a cluster as a",
    "start": "708060",
    "end": "709860"
  },
  {
    "text": "straightforward as possible",
    "start": "709860",
    "end": "712320"
  },
  {
    "text": "so here what we see is a a ray worker",
    "start": "712320",
    "end": "716279"
  },
  {
    "text": "spec so this is just a part of the the",
    "start": "716279",
    "end": "719279"
  },
  {
    "text": "full cube Ray spec",
    "start": "719279",
    "end": "721760"
  },
  {
    "text": "in this snippet we can see that we're",
    "start": "721760",
    "end": "725640"
  },
  {
    "text": "initializing the worker by declaring the",
    "start": "725640",
    "end": "729060"
  },
  {
    "text": "number of kbu resources that's available",
    "start": "729060",
    "end": "732000"
  },
  {
    "text": "on each worker",
    "start": "732000",
    "end": "734240"
  },
  {
    "text": "so after the next raid release we are",
    "start": "734240",
    "end": "739320"
  },
  {
    "text": "planning to make tpus and natively",
    "start": "739320",
    "end": "741480"
  },
  {
    "text": "supported resource so this step should",
    "start": "741480",
    "end": "744740"
  },
  {
    "text": "in the future ideally this should no",
    "start": "744740",
    "end": "747000"
  },
  {
    "text": "longer be necessary",
    "start": "747000",
    "end": "749720"
  },
  {
    "text": "here what we're doing is we're the",
    "start": "751980",
    "end": "755459"
  },
  {
    "text": "distribute look familiar if you have",
    "start": "755459",
    "end": "757320"
  },
  {
    "text": "used QB so instead of reserving the",
    "start": "757320",
    "end": "760440"
  },
  {
    "text": "number of gpus we just say that the",
    "start": "760440",
    "end": "762779"
  },
  {
    "text": "number of TPU chips that we're",
    "start": "762779",
    "end": "765480"
  },
  {
    "text": "requesting for each worker",
    "start": "765480",
    "end": "767880"
  },
  {
    "text": "and finally since we are deploying this",
    "start": "767880",
    "end": "771180"
  },
  {
    "text": "on a specific topology we need to",
    "start": "771180",
    "end": "773639"
  },
  {
    "text": "specify the topology type in the uh the",
    "start": "773639",
    "end": "777959"
  },
  {
    "text": "node selector so this will ensure that",
    "start": "777959",
    "end": "780839"
  },
  {
    "text": "the workload will get scheduled on no",
    "start": "780839",
    "end": "783540"
  },
  {
    "text": "pool with a specified topology type as",
    "start": "783540",
    "end": "786420"
  },
  {
    "text": "well as the accelerator type",
    "start": "786420",
    "end": "790339"
  },
  {
    "text": "so when your cursor is up and running",
    "start": "791639",
    "end": "794660"
  },
  {
    "text": "you can just submit a simple workload",
    "start": "794660",
    "end": "799380"
  },
  {
    "text": "and specify the number of tpus that you",
    "start": "799380",
    "end": "802200"
  },
  {
    "text": "need in this case we have a stable",
    "start": "802200",
    "end": "804240"
  },
  {
    "text": "diffusion this is just basically just a",
    "start": "804240",
    "end": "807180"
  },
  {
    "text": "code snippet and you can use a ray serve",
    "start": "807180",
    "end": "811760"
  },
  {
    "text": "to declare that the actor options and",
    "start": "811760",
    "end": "816420"
  },
  {
    "text": "specify the number of replicas as well",
    "start": "816420",
    "end": "818940"
  },
  {
    "text": "as the amount of resources required for",
    "start": "818940",
    "end": "821760"
  },
  {
    "text": "each replica",
    "start": "821760",
    "end": "824839"
  },
  {
    "text": "so if you have follow along we mentioned",
    "start": "825000",
    "end": "828000"
  },
  {
    "text": "earlier that the we are in in the",
    "start": "828000",
    "end": "832740"
  },
  {
    "text": "multi-host environment the worker hosts",
    "start": "832740",
    "end": "835440"
  },
  {
    "text": "must be initialized with the worker",
    "start": "835440",
    "end": "839040"
  },
  {
    "text": "index as well as the host names",
    "start": "839040",
    "end": "841079"
  },
  {
    "text": "so what do you do if you're trying to",
    "start": "841079",
    "end": "843540"
  },
  {
    "text": "run this on gke",
    "start": "843540",
    "end": "846139"
  },
  {
    "text": "So currently we will we publish this",
    "start": "846139",
    "end": "849899"
  },
  {
    "text": "little code snippet that would help you",
    "start": "849899",
    "end": "851940"
  },
  {
    "text": "initialize uh the idea here is that uh",
    "start": "851940",
    "end": "855839"
  },
  {
    "text": "before you initialize the Jax",
    "start": "855839",
    "end": "858180"
  },
  {
    "text": "environment you should get the to send",
    "start": "858180",
    "end": "862380"
  },
  {
    "text": "some code to get the hostname for each",
    "start": "862380",
    "end": "864240"
  },
  {
    "text": "TPU worker and then use uh just use some",
    "start": "864240",
    "end": "868320"
  },
  {
    "text": "some python code to set the environment",
    "start": "868320",
    "end": "870540"
  },
  {
    "text": "variables and then after this point uh",
    "start": "870540",
    "end": "873720"
  },
  {
    "text": "you should you can initialize Jacks and",
    "start": "873720",
    "end": "876899"
  },
  {
    "text": "delete the TPU",
    "start": "876899",
    "end": "879060"
  },
  {
    "text": "libraries will be initialized with the",
    "start": "879060",
    "end": "881579"
  },
  {
    "text": "correct environment settings",
    "start": "881579",
    "end": "885079"
  },
  {
    "text": "so now I want to shift gears a little",
    "start": "886139",
    "end": "888420"
  },
  {
    "text": "bit so we've talked a bit about how to",
    "start": "888420",
    "end": "892320"
  },
  {
    "text": "run a kubray cluster with tpus right but",
    "start": "892320",
    "end": "895980"
  },
  {
    "text": "if you're running a if you're trying to",
    "start": "895980",
    "end": "898620"
  },
  {
    "text": "operationalize a machine learning",
    "start": "898620",
    "end": "900839"
  },
  {
    "text": "workload in the in your environment most",
    "start": "900839",
    "end": "904380"
  },
  {
    "text": "likely you have a number of uh General",
    "start": "904380",
    "end": "907560"
  },
  {
    "text": "issues to solve right for example",
    "start": "907560",
    "end": "910740"
  },
  {
    "text": "if you're",
    "start": "910740",
    "end": "912360"
  },
  {
    "text": "if you're using this uh in your own",
    "start": "912360",
    "end": "916139"
  },
  {
    "text": "environment you may want to enable",
    "start": "916139",
    "end": "918019"
  },
  {
    "text": "authentication to protect your ray",
    "start": "918019",
    "end": "920639"
  },
  {
    "text": "cluster endpoints right",
    "start": "920639",
    "end": "922560"
  },
  {
    "text": "you may want to monitor what's going on",
    "start": "922560",
    "end": "925560"
  },
  {
    "text": "in your deployment somehow like check",
    "start": "925560",
    "end": "927779"
  },
  {
    "text": "out the logging like you see if you need",
    "start": "927779",
    "end": "930959"
  },
  {
    "text": "to debug a job",
    "start": "930959",
    "end": "933380"
  },
  {
    "text": "maybe as a system administrator maybe",
    "start": "933380",
    "end": "937260"
  },
  {
    "text": "you want to look at the system from a a",
    "start": "937260",
    "end": "940980"
  },
  {
    "text": "broader point of view and look at the",
    "start": "940980",
    "end": "943980"
  },
  {
    "text": "metrics General metrics like what what",
    "start": "943980",
    "end": "946019"
  },
  {
    "text": "are my resource utilizations what's",
    "start": "946019",
    "end": "949339"
  },
  {
    "text": "what's the memory consumption",
    "start": "949339",
    "end": "952560"
  },
  {
    "text": "right",
    "start": "952560",
    "end": "953880"
  },
  {
    "text": "and finally as a ml researcher or",
    "start": "953880",
    "end": "957839"
  },
  {
    "text": "engineer",
    "start": "957839",
    "end": "959120"
  },
  {
    "text": "you want some ways to run experiments or",
    "start": "959120",
    "end": "963360"
  },
  {
    "text": "just just to experiment with your array",
    "start": "963360",
    "end": "966000"
  },
  {
    "text": "clusters right so there's a number of",
    "start": "966000",
    "end": "968000"
  },
  {
    "text": "Integrations that you may need",
    "start": "968000",
    "end": "970620"
  },
  {
    "text": "so to make this process easier",
    "start": "970620",
    "end": "974339"
  },
  {
    "text": "we have published a set of terraform",
    "start": "974339",
    "end": "977880"
  },
  {
    "text": "templates that will help the users get",
    "start": "977880",
    "end": "982019"
  },
  {
    "text": "started with rayon gke so this comes out",
    "start": "982019",
    "end": "985860"
  },
  {
    "text": "of the box with the installations for a",
    "start": "985860",
    "end": "989399"
  },
  {
    "text": "GPU or TPU enabled gke cluster",
    "start": "989399",
    "end": "994139"
  },
  {
    "text": "um it comes with a cubery operator that",
    "start": "994139",
    "end": "996779"
  },
  {
    "text": "you can use to deploy the workloads that",
    "start": "996779",
    "end": "999600"
  },
  {
    "text": "we have just described",
    "start": "999600",
    "end": "1001940"
  },
  {
    "text": "there is a IAP enabled endpoint IAP",
    "start": "1001940",
    "end": "1005839"
  },
  {
    "text": "stands for identity aware proxy so this",
    "start": "1005839",
    "end": "1008660"
  },
  {
    "text": "enables you to authenticate to your rare",
    "start": "1008660",
    "end": "1012980"
  },
  {
    "text": "cluster using your Google credentials",
    "start": "1012980",
    "end": "1017540"
  },
  {
    "text": "there is a built-in support for cloud",
    "start": "1017779",
    "end": "1021320"
  },
  {
    "text": "logging so you can go to the cloud cloud",
    "start": "1021320",
    "end": "1023720"
  },
  {
    "text": "console to view your logs and there's a",
    "start": "1023720",
    "end": "1028058"
  },
  {
    "text": "Prometheus monitoring to help look at",
    "start": "1028059",
    "end": "1031520"
  },
  {
    "text": "the metrics",
    "start": "1031520",
    "end": "1034160"
  },
  {
    "text": "and in addition we have installed a",
    "start": "1034160",
    "end": "1036319"
  },
  {
    "text": "jupyter notebook server to make",
    "start": "1036319",
    "end": "1038798"
  },
  {
    "text": "experimenting with the kubray cluster",
    "start": "1038799",
    "end": "1042160"
  },
  {
    "text": "fairly straightforward",
    "start": "1042160",
    "end": "1045400"
  },
  {
    "text": "so yeah you can check out the user guide",
    "start": "1045400",
    "end": "1048980"
  },
  {
    "text": "on this GitHub link yeah obviously you",
    "start": "1048980",
    "end": "1052220"
  },
  {
    "text": "can't click on the link if you just did",
    "start": "1052220",
    "end": "1053660"
  },
  {
    "text": "like Google search GitHub rayon gke",
    "start": "1053660",
    "end": "1055820"
  },
  {
    "text": "you'll pop into a GitHub site that is",
    "start": "1055820",
    "end": "1058520"
  },
  {
    "text": "all our Ai and gke templates best",
    "start": "1058520",
    "end": "1061039"
  },
  {
    "text": "practices guides and things like that so",
    "start": "1061039",
    "end": "1062600"
  },
  {
    "text": "definitely encourage you to check it out",
    "start": "1062600",
    "end": "1063860"
  },
  {
    "text": "and reach out to us through there",
    "start": "1063860",
    "end": "1066740"
  },
  {
    "text": "cool so presentation is no fun without a",
    "start": "1066740",
    "end": "1070220"
  },
  {
    "text": "demo what we're going to demo is stable",
    "start": "1070220",
    "end": "1072500"
  },
  {
    "text": "diffusion which is a if you're not",
    "start": "1072500",
    "end": "1074059"
  },
  {
    "text": "familiar a popular text to image",
    "start": "1074059",
    "end": "1076820"
  },
  {
    "text": "generation algorithm we're using a",
    "start": "1076820",
    "end": "1079039"
  },
  {
    "text": "stable diffusion this version v14 from",
    "start": "1079039",
    "end": "1081679"
  },
  {
    "text": "hugging face so hopefully this will",
    "start": "1081679",
    "end": "1084260"
  },
  {
    "text": "switch seamlessly",
    "start": "1084260",
    "end": "1087200"
  },
  {
    "text": "all right now",
    "start": "1087200",
    "end": "1089480"
  },
  {
    "text": "where is it",
    "start": "1089480",
    "end": "1091100"
  },
  {
    "text": "does it play",
    "start": "1091100",
    "end": "1093760"
  },
  {
    "text": "let's see the little mouse cursor am I",
    "start": "1102200",
    "end": "1103820"
  },
  {
    "text": "there you got it right there",
    "start": "1103820",
    "end": "1105799"
  },
  {
    "text": "there all right Richard's gonna offer a",
    "start": "1105799",
    "end": "1108140"
  },
  {
    "text": "voiceover yeah so I'll just narrate this",
    "start": "1108140",
    "end": "1110380"
  },
  {
    "text": "so this is uh done using the terraform",
    "start": "1110380",
    "end": "1113840"
  },
  {
    "text": "templates that I just introduced so",
    "start": "1113840",
    "end": "1116600"
  },
  {
    "text": "we're just going to",
    "start": "1116600",
    "end": "1117799"
  },
  {
    "text": "run this deployment",
    "start": "1117799",
    "end": "1120200"
  },
  {
    "text": "I hope this still works",
    "start": "1120200",
    "end": "1122360"
  },
  {
    "text": "oh it's a recording",
    "start": "1122360",
    "end": "1125440"
  },
  {
    "text": "all right",
    "start": "1126380",
    "end": "1127820"
  },
  {
    "text": "um",
    "start": "1127820",
    "end": "1128539"
  },
  {
    "text": "so now we're going to uh clearly",
    "start": "1128539",
    "end": "1131059"
  },
  {
    "text": "unedited so this is real yeah",
    "start": "1131059",
    "end": "1133340"
  },
  {
    "text": "definitely real",
    "start": "1133340",
    "end": "1134960"
  },
  {
    "text": "so yeah this this will deploy a one Ray",
    "start": "1134960",
    "end": "1138440"
  },
  {
    "text": "cluster with a single worker",
    "start": "1138440",
    "end": "1141879"
  },
  {
    "text": "and that's how we could save capacity",
    "start": "1144320",
    "end": "1145940"
  },
  {
    "text": "for customers",
    "start": "1145940",
    "end": "1148600"
  },
  {
    "text": "oh so the here we're looking at the IP",
    "start": "1150740",
    "end": "1153799"
  },
  {
    "text": "for the Jupiter notebook server so we",
    "start": "1153799",
    "end": "1156679"
  },
  {
    "text": "can use it to experiment there we go",
    "start": "1156679",
    "end": "1159559"
  },
  {
    "text": "so first we're going to pip install the",
    "start": "1159559",
    "end": "1162679"
  },
  {
    "text": "Ray in the Jupiter notebook",
    "start": "1162679",
    "end": "1165740"
  },
  {
    "text": "this should take about a minute",
    "start": "1165740",
    "end": "1169419"
  },
  {
    "text": "yeah there we go",
    "start": "1172760",
    "end": "1175280"
  },
  {
    "text": "okay now we're going to scroll down",
    "start": "1175280",
    "end": "1178539"
  },
  {
    "text": "scroll down see",
    "start": "1178640",
    "end": "1181059"
  },
  {
    "text": "so in the next cell we're uh so notice",
    "start": "1181059",
    "end": "1185000"
  },
  {
    "text": "how we're talking to a kubernetes",
    "start": "1185000",
    "end": "1187520"
  },
  {
    "text": "service endpoint this is because this",
    "start": "1187520",
    "end": "1189799"
  },
  {
    "text": "notebook server is deployed in the same",
    "start": "1189799",
    "end": "1191720"
  },
  {
    "text": "kubernetes cluster",
    "start": "1191720",
    "end": "1193280"
  },
  {
    "text": "we're going to install some a bunch of",
    "start": "1193280",
    "end": "1196580"
  },
  {
    "text": "libraries on the reworkers",
    "start": "1196580",
    "end": "1198860"
  },
  {
    "text": "next we're going to this is a Ingress",
    "start": "1198860",
    "end": "1201500"
  },
  {
    "text": "class",
    "start": "1201500",
    "end": "1203000"
  },
  {
    "text": "so this is this basically can run on any",
    "start": "1203000",
    "end": "1206299"
  },
  {
    "text": "infrastructure",
    "start": "1206299",
    "end": "1209080"
  },
  {
    "text": "so this is where we specify the actors",
    "start": "1209900",
    "end": "1212900"
  },
  {
    "text": "for the uh for the replica",
    "start": "1212900",
    "end": "1217419"
  },
  {
    "text": "and uh we're going to yeah this is just",
    "start": "1220640",
    "end": "1223700"
  },
  {
    "text": "Stars the serve instance",
    "start": "1223700",
    "end": "1226899"
  },
  {
    "text": "so now we're going to take a look at the",
    "start": "1227720",
    "end": "1230600"
  },
  {
    "text": "the jobs from the uh Ray dashboard you",
    "start": "1230600",
    "end": "1234020"
  },
  {
    "text": "can see that this job is",
    "start": "1234020",
    "end": "1236419"
  },
  {
    "text": "running now",
    "start": "1236419",
    "end": "1239139"
  },
  {
    "text": "so yeah everything looks good you can",
    "start": "1247160",
    "end": "1249740"
  },
  {
    "text": "also look at the logs if we need but",
    "start": "1249740",
    "end": "1254080"
  },
  {
    "text": "okay so we have the serve running",
    "start": "1256160",
    "end": "1258860"
  },
  {
    "text": "so now we're going to actually send the",
    "start": "1258860",
    "end": "1261440"
  },
  {
    "text": "request you can see that because we're",
    "start": "1261440",
    "end": "1265280"
  },
  {
    "text": "we're using the same instance we're just",
    "start": "1265280",
    "end": "1267980"
  },
  {
    "text": "going to send the inference request to",
    "start": "1267980",
    "end": "1269900"
  },
  {
    "text": "the same endpoint",
    "start": "1269900",
    "end": "1272240"
  },
  {
    "text": "and you can see below that we're uh this",
    "start": "1272240",
    "end": "1275480"
  },
  {
    "text": "is a list of prompts that we're going to",
    "start": "1275480",
    "end": "1276860"
  },
  {
    "text": "generate",
    "start": "1276860",
    "end": "1279220"
  },
  {
    "text": "and set",
    "start": "1281419",
    "end": "1283880"
  },
  {
    "text": "it's going to finish in a second because",
    "start": "1283880",
    "end": "1285440"
  },
  {
    "text": "this",
    "start": "1285440",
    "end": "1287980"
  },
  {
    "text": "there we go",
    "start": "1289460",
    "end": "1292240"
  },
  {
    "text": "so yeah so we're going to look at the",
    "start": "1293480",
    "end": "1295220"
  },
  {
    "text": "results",
    "start": "1295220",
    "end": "1297400"
  },
  {
    "text": "yeah these are the pictures that we've",
    "start": "1300140",
    "end": "1302720"
  },
  {
    "text": "generated from from our prompts",
    "start": "1302720",
    "end": "1307000"
  },
  {
    "text": "cool yeah with that we'll open this",
    "start": "1311059",
    "end": "1313400"
  },
  {
    "text": "floor to questions",
    "start": "1313400",
    "end": "1314900"
  },
  {
    "text": "don't really want to jump back to the",
    "start": "1314900",
    "end": "1316340"
  },
  {
    "text": "presentation but it's basically q a",
    "start": "1316340",
    "end": "1319779"
  },
  {
    "text": "maybe I will jump back to this",
    "start": "1326780",
    "end": "1330340"
  },
  {
    "text": "oh the the usage yeah",
    "start": "1333260",
    "end": "1336940"
  },
  {
    "text": "not yet",
    "start": "1336980",
    "end": "1340000"
  },
  {
    "text": "can you go back to those",
    "start": "1346100",
    "end": "1348039"
  },
  {
    "text": "the next coming next feature is next yes",
    "start": "1348039",
    "end": "1352280"
  },
  {
    "text": "yeah just I'll just quickly talk about",
    "start": "1352280",
    "end": "1354260"
  },
  {
    "text": "it",
    "start": "1354260",
    "end": "1356320"
  },
  {
    "text": "yeah just a quick question for your TPU",
    "start": "1356320",
    "end": "1359299"
  },
  {
    "text": "instances do you also have something",
    "start": "1359299",
    "end": "1361400"
  },
  {
    "text": "similar to what ec2 does like spot and",
    "start": "1361400",
    "end": "1364480"
  },
  {
    "text": "reserved and you know at different price",
    "start": "1364480",
    "end": "1366440"
  },
  {
    "text": "points",
    "start": "1366440",
    "end": "1368919"
  },
  {
    "text": "so yeah is it different price points for",
    "start": "1369020",
    "end": "1370940"
  },
  {
    "text": "different TPU shapes no no like spot",
    "start": "1370940",
    "end": "1373580"
  },
  {
    "text": "spot oh yes they're spot available for",
    "start": "1373580",
    "end": "1376159"
  },
  {
    "text": "TV can you look ahead and reserve and",
    "start": "1376159",
    "end": "1378440"
  },
  {
    "text": "you can do reservations yep okay",
    "start": "1378440",
    "end": "1379880"
  },
  {
    "text": "absolutely",
    "start": "1379880",
    "end": "1382600"
  },
  {
    "text": "and just curious will ever be TPU",
    "start": "1383840",
    "end": "1386780"
  },
  {
    "text": "shortage on gcp",
    "start": "1386780",
    "end": "1390760"
  },
  {
    "text": "um let me follow up with you on that",
    "start": "1391220",
    "end": "1394780"
  },
  {
    "text": "hey um so comparison to gpus uh for",
    "start": "1397240",
    "end": "1401360"
  },
  {
    "text": "training and for serving do you have you",
    "start": "1401360",
    "end": "1403340"
  },
  {
    "text": "compared the performance with tpus",
    "start": "1403340",
    "end": "1406220"
  },
  {
    "text": "and simple answer is yes but a lot of it",
    "start": "1406220",
    "end": "1408080"
  },
  {
    "text": "will come different to models so it's",
    "start": "1408080",
    "end": "1410659"
  },
  {
    "text": "not like it's one is necessarily better",
    "start": "1410659",
    "end": "1412400"
  },
  {
    "text": "than the other for every single model",
    "start": "1412400",
    "end": "1413600"
  },
  {
    "text": "and then a lot of it is like what we",
    "start": "1413600",
    "end": "1416120"
  },
  {
    "text": "talked about is price per or performance",
    "start": "1416120",
    "end": "1418159"
  },
  {
    "text": "per price",
    "start": "1418159",
    "end": "1419600"
  },
  {
    "text": "um so I would say like it's going to",
    "start": "1419600",
    "end": "1421100"
  },
  {
    "text": "come down to different use cases and and",
    "start": "1421100",
    "end": "1422960"
  },
  {
    "text": "what you're doing",
    "start": "1422960",
    "end": "1424100"
  },
  {
    "text": "but you know happy to follow up and work",
    "start": "1424100",
    "end": "1425840"
  },
  {
    "text": "with you to compare what your exact use",
    "start": "1425840",
    "end": "1427460"
  },
  {
    "text": "case is cool thanks",
    "start": "1427460",
    "end": "1431020"
  },
  {
    "text": "thanks um have you what's the largest uh",
    "start": "1437840",
    "end": "1440419"
  },
  {
    "text": "pre-training model you've done on TPU",
    "start": "1440419",
    "end": "1443000"
  },
  {
    "text": "just as part of your experimentation",
    "start": "1443000",
    "end": "1445280"
  },
  {
    "text": "I mean so we don't specifically do the",
    "start": "1445280",
    "end": "1447799"
  },
  {
    "text": "AML we enable customers and also",
    "start": "1447799",
    "end": "1449960"
  },
  {
    "text": "internally do AML",
    "start": "1449960",
    "end": "1451640"
  },
  {
    "text": "um internally I think publicly we've",
    "start": "1451640",
    "end": "1453559"
  },
  {
    "text": "announced 1.6 trillion often but I think",
    "start": "1453559",
    "end": "1456799"
  },
  {
    "text": "there was one paper that was like 2.1",
    "start": "1456799",
    "end": "1458780"
  },
  {
    "text": "trillion",
    "start": "1458780",
    "end": "1460340"
  },
  {
    "text": "um publicly that's what it is and I'll",
    "start": "1460340",
    "end": "1462140"
  },
  {
    "text": "say internally there's always more",
    "start": "1462140",
    "end": "1465460"
  },
  {
    "text": "I think we're trying to serve one of the",
    "start": "1466039",
    "end": "1467720"
  },
  {
    "text": "Llama models",
    "start": "1467720",
    "end": "1470659"
  },
  {
    "text": "we mean for us well so obviously with GK",
    "start": "1470659",
    "end": "1473780"
  },
  {
    "text": "we are enabling customers with",
    "start": "1473780",
    "end": "1475400"
  },
  {
    "text": "production workloads and I'd offered",
    "start": "1475400",
    "end": "1477679"
  },
  {
    "text": "that in many cases the largest of the",
    "start": "1477679",
    "end": "1479600"
  },
  {
    "text": "large model isn't quite there to",
    "start": "1479600",
    "end": "1481220"
  },
  {
    "text": "production but we definitely scale we",
    "start": "1481220",
    "end": "1484280"
  },
  {
    "text": "have the largest scale of publicly known",
    "start": "1484280",
    "end": "1487280"
  },
  {
    "text": "size of clusters for size of models I",
    "start": "1487280",
    "end": "1490760"
  },
  {
    "text": "think there's blog posts out there and",
    "start": "1490760",
    "end": "1491960"
  },
  {
    "text": "being very careful with words if not",
    "start": "1491960",
    "end": "1493400"
  },
  {
    "text": "they will be there soon",
    "start": "1493400",
    "end": "1496299"
  }
]