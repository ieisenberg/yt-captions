[
  {
    "text": "all right hi everyone uh I'm Alexander I'm a senior staff uh engineer at Cruz",
    "start": "5940",
    "end": "13019"
  },
  {
    "text": "and today I'm very humbled and excited to present to you uh together with Ann",
    "start": "13019",
    "end": "18119"
  },
  {
    "text": "uh our staff engineer uh our cruise data system which is a new data set processing pipeline for Cruz ml",
    "start": "18119",
    "end": "25619"
  },
  {
    "text": "and here's our agenda for today I will start from discussing what are generic needs for data processing for",
    "start": "25619",
    "end": "32340"
  },
  {
    "text": "machine learning and for ML practitioners what existing open source systems available to them and what are",
    "start": "32340",
    "end": "38700"
  },
  {
    "text": "artists for improvements in that status quo and then n is going to work his guys through Cruise data solution and last",
    "start": "38700",
    "end": "45660"
  },
  {
    "text": "but not least how array makes it effortless to build so let's start",
    "start": "45660",
    "end": "51440"
  },
  {
    "text": "these are General data processing needs for machine learning uh you have mainly",
    "start": "51440",
    "end": "57360"
  },
  {
    "text": "two kinds of data Transformations uh one kind of transformations in the context of this presentation we will be calling",
    "start": "57360",
    "end": "64619"
  },
  {
    "text": "offline Transformations so data processor step would normally take a few data sets in and produce one or few data",
    "start": "64619",
    "end": "72659"
  },
  {
    "text": "sets out and uh there is another kind of transformation so we'll be calling",
    "start": "72659",
    "end": "78479"
  },
  {
    "text": "online that happen during training again people might be calling things online in",
    "start": "78479",
    "end": "83880"
  },
  {
    "text": "different contexts in different way but here in the context of this presentation online Transformations will be Transformations that run during training",
    "start": "83880",
    "end": "91320"
  },
  {
    "text": "and in there you also would be taking a few data sets in or maybe just one and",
    "start": "91320",
    "end": "96479"
  },
  {
    "text": "doing something with them like combining a certain way and then having this unified data stream and feeding into",
    "start": "96479",
    "end": "102240"
  },
  {
    "text": "your model training Loop and in the end training gives you a model and there is generally no clear rule for",
    "start": "102240",
    "end": "109380"
  },
  {
    "text": "ML Engineers where to put those transformations in online setting or in offline setting majority of",
    "start": "109380",
    "end": "115259"
  },
  {
    "text": "Transformations actually can be put in any place things like concatenation uh",
    "start": "115259",
    "end": "120360"
  },
  {
    "text": "interleaving of data sets and any other forms of sampling uh they can be also",
    "start": "120360",
    "end": "125399"
  },
  {
    "text": "Parallel level sampling applying transforms on a parallel basis such as just a map function filtering runs",
    "start": "125399",
    "end": "131039"
  },
  {
    "text": "batching rows shuffling your data set or segments of data sets and so on and it's",
    "start": "131039",
    "end": "137459"
  },
  {
    "text": "not just confusing where to put that logic but also once you do that it can",
    "start": "137459",
    "end": "142680"
  },
  {
    "text": "be also challenging to change the choice and we will look at more detail for the",
    "start": "142680",
    "end": "149040"
  },
  {
    "text": "reasons why on the following slides so what is our opportunity for",
    "start": "149040",
    "end": "154080"
  },
  {
    "text": "improvement here first of all we have fragmented offline and online data transformation systems and there are",
    "start": "154080",
    "end": "159360"
  },
  {
    "text": "just too many data sets to manage and on top of that we have inefficient data reading and loading and training",
    "start": "159360",
    "end": "165780"
  },
  {
    "text": "that I think many teams in the industry suffer from uh generally people trying",
    "start": "165780",
    "end": "171120"
  },
  {
    "text": "to use gpus for training they would really expect uh for forward for Loop in",
    "start": "171120",
    "end": "177360"
  },
  {
    "text": "training to be getting next mini batch of data as soon as it wants to have that",
    "start": "177360",
    "end": "182400"
  },
  {
    "text": "mini batch so you really need to hide latencies of data reading and data Transformations and in order to do that",
    "start": "182400",
    "end": "188400"
  },
  {
    "text": "you would be maybe uh prone to let's say let's just throw capacity problems so we'll try to",
    "start": "188400",
    "end": "195900"
  },
  {
    "text": "use a host with more CPUs with more memory try to hide that necessarily well like you would need some large memory",
    "start": "195900",
    "end": "202140"
  },
  {
    "text": "buffers potentially for that and uh actually what we discovered is that by",
    "start": "202140",
    "end": "207659"
  },
  {
    "text": "trying to hide latencies we end up spending more money on CPU and memory",
    "start": "207659",
    "end": "213720"
  },
  {
    "text": "than on gpus which can be quite counterintuitive finding to many people who know hey gpus are expensive but if",
    "start": "213720",
    "end": "220080"
  },
  {
    "text": "you actually try to leverage well and you don't have very efficient data loading pipeline actually the cost",
    "start": "220080",
    "end": "226640"
  },
  {
    "text": "dominance shifts to CPU and memory and on top of that if you don't go to more",
    "start": "226640",
    "end": "232980"
  },
  {
    "text": "complex data load infrastructure where your data loads would loaders will be separate in a more simplistic case there",
    "start": "232980",
    "end": "239879"
  },
  {
    "text": "is also limit to how much memory is available to you so you would be seeing let's say frequent ooms and generally",
    "start": "239879",
    "end": "245459"
  },
  {
    "text": "speaking of reliability it's hard to debug a training data loading failures",
    "start": "245459",
    "end": "250980"
  },
  {
    "text": "another sort of problems kind of problems is there is no party guarantee between Transformations that happen",
    "start": "250980",
    "end": "257160"
  },
  {
    "text": "offline and those that happen in online training setting or maybe in one case",
    "start": "257160",
    "end": "263280"
  },
  {
    "text": "you pre-process your data and save it in a data set and another case you just decided to do it on the Fly and assume",
    "start": "263280",
    "end": "270000"
  },
  {
    "text": "the logic is the same but maybe there was a small bug and in the end it would be just really challenging to debug most",
    "start": "270000",
    "end": "276120"
  },
  {
    "text": "likely it would be misled in your experimentation and take wrong conclusions it can be really costly and",
    "start": "276120",
    "end": "281880"
  },
  {
    "text": "cost maybe several months of development that would be a big problem and of course there is more traditional machine",
    "start": "281880",
    "end": "288720"
  },
  {
    "text": "learning challenge of parity between offline transformations in general that is a happen",
    "start": "288720",
    "end": "294780"
  },
  {
    "text": "as pre-processing stage before training or during training and online inference Transformations for example feature",
    "start": "294780",
    "end": "301020"
  },
  {
    "text": "extraction priority so these are our areas for improvement and I will be looking later in the presentation how",
    "start": "301020",
    "end": "307560"
  },
  {
    "text": "our cruise data system attempts to solve those on top of this all there are some use",
    "start": "307560",
    "end": "313320"
  },
  {
    "text": "case specific challenges that are not really specific to autonomous vehicles but they're more dominant in a common",
    "start": "313320",
    "end": "318900"
  },
  {
    "text": "vehicle space so first of all we're dealing with a high resolution sensor regulator and this leads to larger data",
    "start": "318900",
    "end": "326220"
  },
  {
    "text": "sets larger memory consumption during data loading and um many people might be familiar with uh",
    "start": "326220",
    "end": "333440"
  },
  {
    "text": "training regime where you would be seeing every example just one time this is popular in large uh internet",
    "start": "333440",
    "end": "340320"
  },
  {
    "text": "companies with abundance of label data but in our case we need to do",
    "start": "340320",
    "end": "345660"
  },
  {
    "text": "multi-ebook training with traveling and data is read and transformed every time uh you go through the data and every",
    "start": "345660",
    "end": "353520"
  },
  {
    "text": "time it's done in different order so these things they really make naive",
    "start": "353520",
    "end": "359460"
  },
  {
    "text": "approaches to question challenging you can't just say hey every distribute",
    "start": "359460",
    "end": "364919"
  },
  {
    "text": "training work is going to store a pre-processed version of data set and",
    "start": "364919",
    "end": "370440"
  },
  {
    "text": "make it really easy available to read and uh",
    "start": "370440",
    "end": "376259"
  },
  {
    "text": "as a result uh caching just ends up being infeasible in a simple form and we",
    "start": "376259",
    "end": "381960"
  },
  {
    "text": "will discuss more complex versions of caching that actually work in a later part file presentation",
    "start": "381960",
    "end": "387300"
  },
  {
    "text": "and last but not least there is this so-called need for analytical queries or",
    "start": "387300",
    "end": "393060"
  },
  {
    "text": "SQL like queries into data sets and for example people might want to check distributions of their data they want uh",
    "start": "393060",
    "end": "399360"
  },
  {
    "text": "to to mine some interesting examples some scenarios where models don't do as well and typically data formats that are",
    "start": "399360",
    "end": "408360"
  },
  {
    "text": "meant for efficient data loading during training they end up being not friendly to those kind of analytical queries so",
    "start": "408360",
    "end": "414780"
  },
  {
    "text": "people end up storing two versions of those data sets the issue here is that there are no system to keep them consistent so again you have this",
    "start": "414780",
    "end": "421800"
  },
  {
    "text": "priority challenge so let's first quickly look at existing",
    "start": "421800",
    "end": "427979"
  },
  {
    "text": "online systems that would be used during training so generally these systems they",
    "start": "427979",
    "end": "433620"
  },
  {
    "text": "uh are easy for ML Engineers to use and test locally as they promote a local debugging flows notebooks visualization",
    "start": "433620",
    "end": "440039"
  },
  {
    "text": "and so on and in pytorch world that would be ecosystem of data pipes data",
    "start": "440039",
    "end": "445680"
  },
  {
    "text": "load and torch data for those familiar with the second system a neuter's data package which which was",
    "start": "445680",
    "end": "451620"
  },
  {
    "text": "introduced within last year I think uh brings much more structure to traditionally less structured python",
    "start": "451620",
    "end": "458220"
  },
  {
    "text": "system but it still uses pytorch data load with multiprocessing to paralyze reads and Transformations which prevents",
    "start": "458220",
    "end": "465780"
  },
  {
    "text": "buffers reuse in readers and this generally leads to higher costs and out",
    "start": "465780",
    "end": "471840"
  },
  {
    "text": "of memory error chances and most Engineers I've worked with they just uh",
    "start": "471840",
    "end": "478080"
  },
  {
    "text": "have hard time debugging profile python multi-processing implications there are some people who can be really efficient",
    "start": "478080",
    "end": "483840"
  },
  {
    "text": "in this but like just even like Linux tooling is just not really good at it and if you're dealing with a single",
    "start": "483840",
    "end": "490259"
  },
  {
    "text": "process with multiple threats all those tasks become much thinner now",
    "start": "490259",
    "end": "496440"
  },
  {
    "text": "additionally it might seem appealing that in pytological system in data",
    "start": "496440",
    "end": "501539"
  },
  {
    "text": "loading you can put arbitrary python in your data loading logic but what we discovered for autonomous vehicle",
    "start": "501539",
    "end": "507780"
  },
  {
    "text": "research and development and production uh ml Engineers actually putting some small guard layers in place that would",
    "start": "507780",
    "end": "516599"
  },
  {
    "text": "actually enable them to experiment faster and get results uh done in a small amount of time because they end up",
    "start": "516599",
    "end": "523680"
  },
  {
    "text": "running training in more reliable and in a faster fashion and uh there is also this interesting",
    "start": "523680",
    "end": "531600"
  },
  {
    "text": "detail we discovered with spytosh data loaders and the way it enforces deterministic data loading in training",
    "start": "531600",
    "end": "537720"
  },
  {
    "text": "it is actually having very significant performance cliffs in some rare scenarios and sometimes we've been",
    "start": "537720",
    "end": "543600"
  },
  {
    "text": "sufficiently unlucky to hit those rare scenarios the way readers and their latencies align with each other",
    "start": "543600",
    "end": "550440"
  },
  {
    "text": "uh now in the tensorflow ecosystem side there is a package called TF data from",
    "start": "550440",
    "end": "555899"
  },
  {
    "text": "performance uh and efficiency of the library perspective it's very mature one and it has many knobs for example it has",
    "start": "555899",
    "end": "562740"
  },
  {
    "text": "deterministic and non-deterministic modes of loading and it connects really good with tensorflow system but doesn't",
    "start": "562740",
    "end": "569339"
  },
  {
    "text": "connect that well with spy toshica system and this is kind of deal breaker for us and uh people might say hey maybe",
    "start": "569339",
    "end": "575399"
  },
  {
    "text": "you just use both together but for people who try to combine pytorch and tensorflow in a single process",
    "start": "575399",
    "end": "581820"
  },
  {
    "text": "especially in a more unified build settings I know it's kind of can be very challenging and just expensive to",
    "start": "581820",
    "end": "587820"
  },
  {
    "text": "maintain uh support for two deploying Frameworks within a single deployment",
    "start": "587820",
    "end": "593279"
  },
  {
    "text": "and last but not least uh python Transformations are not executed as efficiently in a tensorflow world as",
    "start": "593279",
    "end": "600779"
  },
  {
    "text": "they would be hitting Global interpreter log uh in that multi-processor multi-stranded executors that they have",
    "start": "600779",
    "end": "607320"
  },
  {
    "text": "at least last time I looked at it was the case and in offline systems Switching gears",
    "start": "607320",
    "end": "613740"
  },
  {
    "text": "to those we have things like Apache beam and Spark and they're really flexible and can be very",
    "start": "613740",
    "end": "621300"
  },
  {
    "text": "performant and scalable but they may require a lot of boilerplate for simple things such as uh",
    "start": "621300",
    "end": "629220"
  },
  {
    "text": "like writing a pipeline and writing some pre-processing stage uh that creates a new data set and then feeding that into",
    "start": "629220",
    "end": "634680"
  },
  {
    "text": "trainer and they usually not very well known by ml Engineers ml practitioners",
    "start": "634680",
    "end": "640500"
  },
  {
    "text": "familiar with tensorflow or pytorch and there is this practical consideration uh in many cases",
    "start": "640500",
    "end": "648480"
  },
  {
    "text": "deployments of such systems don't have GPU access so what is our idios scenario before we",
    "start": "648480",
    "end": "655019"
  },
  {
    "text": "go into solution phase we would like to be able to prototype in online setting",
    "start": "655019",
    "end": "660300"
  },
  {
    "text": "uh to be able to perform Transformations during training and once ready we should",
    "start": "660300",
    "end": "666420"
  },
  {
    "text": "be able to move parts of transformation graph that is movable to offline to",
    "start": "666420",
    "end": "672600"
  },
  {
    "text": "offline setting things like filtering and Gymnastic map operations are good candidates to be moved to offline world",
    "start": "672600",
    "end": "678300"
  },
  {
    "text": "but we will keep some operations online such as all sort of sampling operations",
    "start": "678300",
    "end": "683640"
  },
  {
    "text": "sampling can be done on level of data sets on a level of rows and people just",
    "start": "683640",
    "end": "688800"
  },
  {
    "text": "like to change parameters in the data distribution and experiment actively with that regenerating data set every",
    "start": "688800",
    "end": "694620"
  },
  {
    "text": "time you want to change some sampling configurations usually is just not efficient for malpractitioners and",
    "start": "694620",
    "end": "700440"
  },
  {
    "text": "things like batching random data augmentation and transforms exploring the data size they better to be kept",
    "start": "700440",
    "end": "706560"
  },
  {
    "text": "online as well and last but not least we would like to have inference parity so all",
    "start": "706560",
    "end": "712079"
  },
  {
    "text": "transformations that happen in offline pre-processing or during training would",
    "start": "712079",
    "end": "717180"
  },
  {
    "text": "like to be able to take them and package together with the model automatically and use that for online inference so we",
    "start": "717180",
    "end": "723779"
  },
  {
    "text": "solve that online in transparity problem once before and for all as well and now I'm going to let Ann talk about",
    "start": "723779",
    "end": "731220"
  },
  {
    "text": "our cruise data solution to the set of problems yes thanks Alex uh now comes to",
    "start": "731220",
    "end": "736920"
  },
  {
    "text": "the exciting part I'd like to introduce you our cruise dot data solution and I",
    "start": "736920",
    "end": "742440"
  },
  {
    "text": "will work you the in-depth how it solves the problem that I just mentioned so",
    "start": "742440",
    "end": "747660"
  },
  {
    "text": "there are some specialty of cruise use case first of all we are dealing with very heavy columns so any redundant copy",
    "start": "747660",
    "end": "755339"
  },
  {
    "text": "of those columns can result in unstable memory Peak during training and it can lead to ohms and fail the job secondly",
    "start": "755339",
    "end": "763440"
  },
  {
    "text": "uh our users they do need a lot of flexibility and different ops to support",
    "start": "763440",
    "end": "768959"
  },
  {
    "text": "their data transformation logic especially for dealing with the AV case we need to have a lot of support around",
    "start": "768959",
    "end": "776279"
  },
  {
    "text": "sampling so that they can have better data distribution and it's critical for",
    "start": "776279",
    "end": "781440"
  },
  {
    "text": "the model performance therefore we develop a bunch of Ops for our user to",
    "start": "781440",
    "end": "787740"
  },
  {
    "text": "author a data pipeline they can use different combinations of the Ops to describe what they want to do with their",
    "start": "787740",
    "end": "794399"
  },
  {
    "text": "input data and the output of the data pipeline is just a graph it's not",
    "start": "794399",
    "end": "799680"
  },
  {
    "text": "executed yet so this is our DSL layer in Python so we also provide a stable graph",
    "start": "799680",
    "end": "807060"
  },
  {
    "text": "intermediate representation layer the purpose is to separate the Declaration",
    "start": "807060",
    "end": "812399"
  },
  {
    "text": "from the execution so our execution is all on C plus percent so that we can",
    "start": "812399",
    "end": "818040"
  },
  {
    "text": "maintain the efficiency for memory and also the throughput this layer can transmit the graph from python to C plus",
    "start": "818040",
    "end": "825480"
  },
  {
    "text": "plus other than that we have good tools to visualize our graph to our user we",
    "start": "825480",
    "end": "830639"
  },
  {
    "text": "also leverage this layer so there are two executors for the online part",
    "start": "830639",
    "end": "836360"
  },
  {
    "text": "currently we leverage the python or the pi torch data loader to provide multiprocessing to make it run more",
    "start": "836360",
    "end": "842940"
  },
  {
    "text": "efficiently underneath we have a loader as I mentioned it's implemented in C",
    "start": "842940",
    "end": "848100"
  },
  {
    "text": "plus so we can leverage the regular threads to do that okay then re-helps",
    "start": "848100",
    "end": "853800"
  },
  {
    "text": "our ecosystem for two parts one is called graph graph graph optimization the other part is reserves as the",
    "start": "853800",
    "end": "861600"
  },
  {
    "text": "backend for our offline executors both I will go in depth",
    "start": "861600",
    "end": "866820"
  },
  {
    "text": "uh before going into that I would like to give a shout out to our amazing orchestration and compute team they are",
    "start": "866820",
    "end": "874620"
  },
  {
    "text": "able to make us life much easier to develop a re-job we just need to apply a",
    "start": "874620",
    "end": "880019"
  },
  {
    "text": "python decorator by applying by providing the results back and this job",
    "start": "880019",
    "end": "885540"
  },
  {
    "text": "will work seamlessly with the other jobs in our user pipeline those other jobs can be with different backend it doesn't",
    "start": "885540",
    "end": "892740"
  },
  {
    "text": "need to be ready so here's the graph optimization problem I just mentioned before the purpose of",
    "start": "892740",
    "end": "899940"
  },
  {
    "text": "that is to rewrite the graph so that our execution can be more efficiently",
    "start": "899940",
    "end": "905579"
  },
  {
    "text": "so they are usually uh multiple iterations for graph rewriting so we can focus some purpose for different",
    "start": "905579",
    "end": "911820"
  },
  {
    "text": "iterations there are some cheap uh and easy graph optimization process for",
    "start": "911820",
    "end": "918300"
  },
  {
    "text": "example if such pattern is detected in user-defined pipeline we can replace it",
    "start": "918300",
    "end": "923699"
  },
  {
    "text": "with a more efficient op for example if user defines Shuffle Then followed by a",
    "start": "923699",
    "end": "929880"
  },
  {
    "text": "batch we can replace that with a fuse Shuffle and batch basically underneath we can have better memory management",
    "start": "929880",
    "end": "936540"
  },
  {
    "text": "with that new new OP and some graph rewriting is very expensive for example",
    "start": "936540",
    "end": "942959"
  },
  {
    "text": "going through all the rows all the files to build the read index or for the for",
    "start": "942959",
    "end": "949199"
  },
  {
    "text": "some very uh Advanced sampling algorithm we need to go through all the rules to",
    "start": "949199",
    "end": "954899"
  },
  {
    "text": "generate weight so that we can properly sample them there are some commonalities about those",
    "start": "954899",
    "end": "961440"
  },
  {
    "text": "expensive graph optimization process they all need to go through millions of rows with terabytes of data and they all",
    "start": "961440",
    "end": "969480"
  },
  {
    "text": "need to finish very quickly because the graph optimization belongs to our data loader initialization stage if running",
    "start": "969480",
    "end": "976920"
  },
  {
    "text": "with training our expensive GPU nodes are set idle to weight that phase to",
    "start": "976920",
    "end": "982320"
  },
  {
    "text": "finish so here we set a SLA at five minutes to finish the graph optimization",
    "start": "982320",
    "end": "988560"
  },
  {
    "text": "no matter how large the data set is for some extreme case if we for example",
    "start": "988560",
    "end": "994800"
  },
  {
    "text": "expand our data set by 100 x we probably cannot run graph optimization on the Fly",
    "start": "994800",
    "end": "1000920"
  },
  {
    "text": "then it should be very easy for us to run it offline and cached luckily we have solids or even more",
    "start": "1000920",
    "end": "1008060"
  },
  {
    "text": "files so we can map this problem to a embarrassing priority Problem where we",
    "start": "1008060",
    "end": "1013699"
  },
  {
    "text": "use rate to speed it up that's what we did and we do see phenomenal Improvement",
    "start": "1013699",
    "end": "1019459"
  },
  {
    "text": "by applying rate to make this job run much quicker you can see with a",
    "start": "1019459",
    "end": "1025220"
  },
  {
    "text": "medium-sized PR we managed to improve our performance by 100x and the cluster",
    "start": "1025220",
    "end": "1030678"
  },
  {
    "text": "is still not very large that's amazing right what's even more amazing is like I",
    "start": "1030679",
    "end": "1035780"
  },
  {
    "text": "mentioned have a red job defined as separately and wrong independently is",
    "start": "1035780",
    "end": "1041240"
  },
  {
    "text": "super easy for us so if we need it we can run it offline",
    "start": "1041240",
    "end": "1046459"
  },
  {
    "text": "other than graph optimization now comes to the offline executor part so we use",
    "start": "1046459",
    "end": "1052040"
  },
  {
    "text": "Ray as our backend and work with the data loader I mentioned before for the",
    "start": "1052040",
    "end": "1057080"
  },
  {
    "text": "offline execution it brings us a lot of benefits first of all no parity issue at",
    "start": "1057080",
    "end": "1062419"
  },
  {
    "text": "all because we are using the same data loader secondly there's less engineer",
    "start": "1062419",
    "end": "1067760"
  },
  {
    "text": "time invested for developing a feature because once a feature is added you",
    "start": "1067760",
    "end": "1072980"
  },
  {
    "text": "don't need to modify two places to add the same support it can be leveraged for both passes and for the offline offload",
    "start": "1072980",
    "end": "1081760"
  },
  {
    "text": "computation part if a user defines a very complex data pipeline they can",
    "start": "1081760",
    "end": "1087140"
  },
  {
    "text": "start with that for training and decide to say I want to offload this part to run offline and for the online it should",
    "start": "1087140",
    "end": "1094400"
  },
  {
    "text": "be much light weighted so with this offline executor it works very well Ray",
    "start": "1094400",
    "end": "1099799"
  },
  {
    "text": "enables us to run a scale it solves the data imbalance the issue for us if we",
    "start": "1099799",
    "end": "1105500"
  },
  {
    "text": "have many more tasks than the physical node second delay Ray help us to speed",
    "start": "1105500",
    "end": "1111020"
  },
  {
    "text": "up some of the user-defined mapper op that needs GPU acceleration",
    "start": "1111020",
    "end": "1117620"
  },
  {
    "text": "this is a high level and simple version diagram of our offline executor and you",
    "start": "1117620",
    "end": "1122840"
  },
  {
    "text": "will understand why we call it materialization so basically for the user input data",
    "start": "1122840",
    "end": "1128299"
  },
  {
    "text": "pipeline for each task we append a chart node at the end so each task is dealing",
    "start": "1128299",
    "end": "1134480"
  },
  {
    "text": "with a portion of data the data is loaded and transferred based on the user",
    "start": "1134480",
    "end": "1139880"
  },
  {
    "text": "definition and it's written out immediately there's no data moving copying involved because the data is",
    "start": "1139880",
    "end": "1146780"
  },
  {
    "text": "written out immediately and you know those data copying and moving can be",
    "start": "1146780",
    "end": "1152179"
  },
  {
    "text": "very very expensive that's why without those our offline executor runs very",
    "start": "1152179",
    "end": "1158000"
  },
  {
    "text": "efficiently and secondly because each job they are independent from each other",
    "start": "1158000",
    "end": "1163660"
  },
  {
    "text": "so it enables us to reliably retry a failed job and provide this very good",
    "start": "1163660",
    "end": "1170539"
  },
  {
    "text": "reliability to our end users okay there are some ongoing and future",
    "start": "1170539",
    "end": "1176660"
  },
  {
    "text": "work we plan uh to improve our crew start data solution we will currently we",
    "start": "1176660",
    "end": "1182600"
  },
  {
    "text": "support our touch scriptable mapper to running multi-threaded executor we want",
    "start": "1182600",
    "end": "1188000"
  },
  {
    "text": "to expand that to more python functions so it's more user friendly for our end user we may explore touch deployee for",
    "start": "1188000",
    "end": "1195500"
  },
  {
    "text": "that purpose and currently once we release the crew start data to our users",
    "start": "1195500",
    "end": "1200539"
  },
  {
    "text": "they are very excited about that and they come up a bunch of great ideas though so they are sending us requests",
    "start": "1200539",
    "end": "1207080"
  },
  {
    "text": "at the more Ops in our ecosystem and we are also keeping improving our",
    "start": "1207080",
    "end": "1212299"
  },
  {
    "text": "performance that's the core of cruise.data and we also plan to revamp",
    "start": "1212299",
    "end": "1217460"
  },
  {
    "text": "our current production pipeline to replace the old solution with the new",
    "start": "1217460",
    "end": "1223480"
  },
  {
    "text": "cruise.data solution for example bulk inference can be one good candidate we",
    "start": "1223480",
    "end": "1228799"
  },
  {
    "text": "no longer need another infer to do bulk inference we can use the user input data",
    "start": "1228799",
    "end": "1234500"
  },
  {
    "text": "pipeline by appending a map to predict on the model and leverage our offline executor to run it at scale of course we",
    "start": "1234500",
    "end": "1243080"
  },
  {
    "text": "will keep improving our data loading logic to make our training more efficiently we've heard the ray data set",
    "start": "1243080",
    "end": "1250039"
  },
  {
    "text": "work uh like in these two days and we are very excited about the potential",
    "start": "1250039",
    "end": "1255460"
  },
  {
    "text": "opportunity we can collaborate to realize some of great ideas we already have and the last but not least we are",
    "start": "1255460",
    "end": "1263059"
  },
  {
    "text": "very interested in open sourcing this solution to more users who are using pytorch and users who are interested in",
    "start": "1263059",
    "end": "1271280"
  },
  {
    "text": "efficient data loading okay in summary we are trying to build a",
    "start": "1271280",
    "end": "1277640"
  },
  {
    "text": "normal system for ML data processing we want this solution to also enable the",
    "start": "1277640",
    "end": "1283520"
  },
  {
    "text": "flexible efficient and faster data loading so it provides the parity as I",
    "start": "1283520",
    "end": "1289280"
  },
  {
    "text": "mentioned for cache and the only flight transformation for both mode it needs",
    "start": "1289280",
    "end": "1294440"
  },
  {
    "text": "the backend to enable this distributed computing to enable it run fast and",
    "start": "1294440",
    "end": "1300080"
  },
  {
    "text": "skill so Ray help us to enable this back-end efficiently and in a matter of",
    "start": "1300080",
    "end": "1307580"
  },
  {
    "text": "weeks not monthses or quarters okay thank you",
    "start": "1307580",
    "end": "1313630"
  },
  {
    "text": "[Applause]",
    "start": "1313630",
    "end": "1319660"
  },
  {
    "text": "oh any question okay do we have a mic for questions",
    "start": "1321140",
    "end": "1329620"
  },
  {
    "text": "[Laughter]",
    "start": "1332030",
    "end": "1335160"
  },
  {
    "text": "how do you handle like versioning of data sets and like metadata metadata and",
    "start": "1337039",
    "end": "1342740"
  },
  {
    "text": "discovery of the data sets and versions um I can take it go ahead yeah so from",
    "start": "1342740",
    "end": "1349220"
  },
  {
    "text": "the library uh for the reader and writer we don't care about the data version as far as you can provide us the absolute",
    "start": "1349220",
    "end": "1355580"
  },
  {
    "text": "path of your data set we can ingest it or write it out from our higher level we",
    "start": "1355580",
    "end": "1361640"
  },
  {
    "text": "do have a versioning system for managing the data so we produce uh so we built",
    "start": "1361640",
    "end": "1367100"
  },
  {
    "text": "the semantic meaning for the data set and the versioning for those data sets we have the data set back end uh sorry",
    "start": "1367100",
    "end": "1373220"
  },
  {
    "text": "data set metadata backend to register and manage those data sets but I would",
    "start": "1373220",
    "end": "1379460"
  },
  {
    "text": "say it's all slope of the Pew Library the reader and the writer library but it",
    "start": "1379460",
    "end": "1385820"
  },
  {
    "text": "does support it in our system maybe to adjust that on top of what Ann said uh",
    "start": "1385820",
    "end": "1391460"
  },
  {
    "text": "we have that orchestration system that we leveraged and mentioned and you can",
    "start": "1391460",
    "end": "1396919"
  },
  {
    "text": "think of uh let's say index Builder or materialization as a step in our orchestration stage and that system can",
    "start": "1396919",
    "end": "1404960"
  },
  {
    "text": "really uh cache all those functions uh there is concept of stable sterilizations that enables",
    "start": "1404960",
    "end": "1411039"
  },
  {
    "text": "understanding if that function was ever executed with the same parameters or not so in this case we can know that this",
    "start": "1411039",
    "end": "1417260"
  },
  {
    "text": "monitorization was already done so Library would be sufficiently intelligent to reduce the work or not",
    "start": "1417260",
    "end": "1422659"
  },
  {
    "text": "reduce the work so this doesn't replace some kind of high level metadata management for data sets like nice drop",
    "start": "1422659",
    "end": "1428720"
  },
  {
    "text": "downs with UI like hey these are my data sets so this is as NSAID would be a different layer of abstraction",
    "start": "1428720",
    "end": "1434780"
  },
  {
    "text": "cool thanks and are and you're able to like track lineage of the of like the",
    "start": "1434780",
    "end": "1440600"
  },
  {
    "text": "data as it flows through training and whatever is that part of this system so the graph uh is kind of layered as",
    "start": "1440600",
    "end": "1447620"
  },
  {
    "text": "undescribed data source graph you can add more items to it and now uh some of",
    "start": "1447620",
    "end": "1454100"
  },
  {
    "text": "the data processors that you might want to drag lineage of uh that they would be just adding doing straightforward",
    "start": "1454100",
    "end": "1460760"
  },
  {
    "text": "operation just adding something to the graph not even executing it that's kind of the beauty of a stable IR photograph",
    "start": "1460760",
    "end": "1467179"
  },
  {
    "text": "and then lineage is being tracked by orchestration system that we use thanks",
    "start": "1467179",
    "end": "1474399"
  },
  {
    "text": "so when do you use cruise.data and when would you use Ray data sets would I ever",
    "start": "1475520",
    "end": "1480679"
  },
  {
    "text": "use Ray data sets if you open source this uh I would say like those two are from",
    "start": "1480679",
    "end": "1486919"
  },
  {
    "text": "like different layers so the mission of crucial data one is to support uh",
    "start": "1486919",
    "end": "1493159"
  },
  {
    "text": "reading data like single data super efficiently with low memory footprint and the second delay we support a bunch",
    "start": "1493159",
    "end": "1500780"
  },
  {
    "text": "of operators based on some common needs for data processing as I mentioned like",
    "start": "1500780",
    "end": "1507020"
  },
  {
    "text": "sampling or batching or Shuffle those are like uh in-house like we Implement",
    "start": "1507020",
    "end": "1513380"
  },
  {
    "text": "those Ops on our side uh Ray data side to me is like it's a even more powerful",
    "start": "1513380",
    "end": "1519860"
  },
  {
    "text": "like touch data loader you can scale your data reading in another layer like for example you want to run our reader",
    "start": "1519860",
    "end": "1528020"
  },
  {
    "text": "underneath and then use free data set to distribute the workload across the",
    "start": "1528020",
    "end": "1533120"
  },
  {
    "text": "cluster so I don't see any like confliction of using those two together",
    "start": "1533120",
    "end": "1539600"
  },
  {
    "text": "yeah that's also like the collaboration opportunity I'm looking for",
    "start": "1539600",
    "end": "1546020"
  },
  {
    "text": "what what is interesting is also sometimes people try to go to this broad infrastructure or where they would",
    "start": "1546020",
    "end": "1552440"
  },
  {
    "text": "separate data loading from let's say trainers I think we haven't exhausted optimization opportunities with an",
    "start": "1552440",
    "end": "1558380"
  },
  {
    "text": "existing training cluster plus caching that is just completely separate and once we exhaust those opportunities and",
    "start": "1558380",
    "end": "1564380"
  },
  {
    "text": "we see a need to actually distribute data loaders in life while training then that's where Ray datasets can come with",
    "start": "1564380",
    "end": "1570919"
  },
  {
    "text": "offended so can I interpret this cruise.data as like map functions for",
    "start": "1570919",
    "end": "1576679"
  },
  {
    "text": "array data set that's highly optimized and then you can also optimize graphs of map functions locally using this graph",
    "start": "1576679",
    "end": "1583340"
  },
  {
    "text": "optimizer that's I think uh for cruise.data",
    "start": "1583340",
    "end": "1588440"
  },
  {
    "text": "currently we focus on optimizing the map function as you mentioned but based on our user need we may also build like",
    "start": "1588440",
    "end": "1595820"
  },
  {
    "text": "some aggregation or reduction capability on top of it but we want to at least for",
    "start": "1595820",
    "end": "1601640"
  },
  {
    "text": "our end user if we want to use Ray underneath just like materialization we",
    "start": "1601640",
    "end": "1607400"
  },
  {
    "text": "want to treat it as an implementation details instead of array as the user",
    "start": "1607400",
    "end": "1613419"
  },
  {
    "text": "surface the API so we will then decide if we want to enable some reduce at some",
    "start": "1613419",
    "end": "1620539"
  },
  {
    "text": "reduce or aggregation capability to our library we will definitely explore Ray as a back end but yeah we will treat",
    "start": "1620539",
    "end": "1627500"
  },
  {
    "text": "that as implementation details",
    "start": "1627500",
    "end": "1631000"
  },
  {
    "text": "I saw that auto memory was an issue um what what did you just use to like",
    "start": "1633620",
    "end": "1639200"
  },
  {
    "text": "overcome that was it raised specifically or something else could you repeat the question we",
    "start": "1639200",
    "end": "1645500"
  },
  {
    "text": "couldn't see you if you could walk in in the middle how about you yeah so I saw that out of",
    "start": "1645500",
    "end": "1651919"
  },
  {
    "text": "Maverick was an issue in one of your slides uh did you use Rey to like overcome that or something else",
    "start": "1651919",
    "end": "1658520"
  },
  {
    "text": "so Rey is used for speeding up some of the",
    "start": "1658520",
    "end": "1664000"
  },
  {
    "text": "hay also used for our server as our backend for the offline executor as I",
    "start": "1666200",
    "end": "1672080"
  },
  {
    "text": "mentioned uh your question is do we encounter any issue by using Ray",
    "start": "1672080",
    "end": "1678100"
  },
  {
    "text": "oh I see because for the materialization the current implementation is you're",
    "start": "1679700",
    "end": "1685400"
  },
  {
    "text": "reading the data and you write it out immediately so we limit the uh you know",
    "start": "1685400",
    "end": "1690799"
  },
  {
    "text": "temp object in memory for our implementation",
    "start": "1690799",
    "end": "1696440"
  },
  {
    "text": "that's how we uh I think uh yeah that's how we make uh offline executor to work",
    "start": "1696679",
    "end": "1703039"
  },
  {
    "text": "with any large data sets uh second delay because our implementation uh avoid",
    "start": "1703039",
    "end": "1709520"
  },
  {
    "text": "copying that's the commission of our library so we do see more stable and",
    "start": "1709520",
    "end": "1714860"
  },
  {
    "text": "like for the peak memory during usage that also helps the out of memory issue",
    "start": "1714860",
    "end": "1720740"
  },
  {
    "text": "so it wasn't like a resolution rather it's an implementation detail",
    "start": "1720740",
    "end": "1725779"
  },
  {
    "text": "oh yeah that's implementation details how we use Ray we pay extra attention to",
    "start": "1725779",
    "end": "1731480"
  },
  {
    "text": "the objects that lives in the memory and also comparing let's say to",
    "start": "1731480",
    "end": "1737240"
  },
  {
    "text": "pytological system which have relies on multi-processing because we go with a",
    "start": "1737240",
    "end": "1742460"
  },
  {
    "text": "stable graph representations that can be executed in C plus plus we can use threads and then I'll say render objects",
    "start": "1742460",
    "end": "1749600"
  },
  {
    "text": "or they don't have independent buffers we can potentially like have a read a buffer shared with Shuffle buffer there",
    "start": "1749600",
    "end": "1756320"
  },
  {
    "text": "could be a lot of optimization between just if you live in a single process that you don't get with a Python's data",
    "start": "1756320",
    "end": "1762620"
  },
  {
    "text": "loader as an example I had a quick question as you're",
    "start": "1762620",
    "end": "1768140"
  },
  {
    "text": "pipelining your Transformations are you running these Transformations exclusively on CPU nodes and feeding",
    "start": "1768140",
    "end": "1773779"
  },
  {
    "text": "them to GPU nodes or are you also running them on gpus and yeah it can be configurable so some of the processing",
    "start": "1773779",
    "end": "1781039"
  },
  {
    "text": "depends on if the process the user defined function can benefit from GPU",
    "start": "1781039",
    "end": "1787039"
  },
  {
    "text": "like if we treat inference as a map of a map of of course it can benefit from the",
    "start": "1787039",
    "end": "1792559"
  },
  {
    "text": "GPU uh wrong and if the Opera is just to you know rotate a graph maybe it's not",
    "start": "1792559",
    "end": "1798740"
  },
  {
    "text": "so it's configurable and is that configurable because of the ray scheduler or are you doing that",
    "start": "1798740",
    "end": "1804860"
  },
  {
    "text": "separately yeah for only fly uh so if things are running on line only",
    "start": "1804860",
    "end": "1811159"
  },
  {
    "text": "currently we all use CPU for the GPO part we do leverage rate for the offline",
    "start": "1811159",
    "end": "1817580"
  },
  {
    "text": "executor just just to clarify the user code has to know like let's say user puts a UDF",
    "start": "1817580",
    "end": "1824000"
  },
  {
    "text": "that UDF needs to know how to use gpus and I think about our system let's say again comparing to pytor's data load",
    "start": "1824000",
    "end": "1830120"
  },
  {
    "text": "could just doesn't work could a memory doesn't work well with multi-processing and memory sharing that is uh being a",
    "start": "1830120",
    "end": "1837140"
  },
  {
    "text": "case in Python's data order setting again single process much easier to share memory under the hood we're using",
    "start": "1837140",
    "end": "1842899"
  },
  {
    "text": "pytorch High values to represent our objects and they are friendly to GPU but",
    "start": "1842899",
    "end": "1848120"
  },
  {
    "text": "then user code has to know how to use it we are not writing business logic of",
    "start": "1848120",
    "end": "1853520"
  },
  {
    "text": "Transformations where writing kind of shells so users can plug in their logic in there",
    "start": "1853520",
    "end": "1859960"
  },
  {
    "text": "cool um I think we're about at time so we can continue the conversation between the",
    "start": "1860600",
    "end": "1866840"
  },
  {
    "text": "between the talks uh thanks for the uh great talk [Applause]",
    "start": "1866840",
    "end": "1875520"
  }
]