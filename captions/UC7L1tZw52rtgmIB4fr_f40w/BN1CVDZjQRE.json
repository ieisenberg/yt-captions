[
  {
    "start": "0",
    "end": "38000"
  },
  {
    "text": "uh good afternoon everyone welcome uh",
    "start": "2960",
    "end": "5480"
  },
  {
    "text": "I'm way from the machine learning",
    "start": "5480",
    "end": "7279"
  },
  {
    "text": "platform team at ROBLOX and joining me",
    "start": "7279",
    "end": "9679"
  },
  {
    "text": "today are my awesome colleagues uh Steve",
    "start": "9679",
    "end": "12360"
  },
  {
    "text": "and ichin today we are going to share",
    "start": "12360",
    "end": "14879"
  },
  {
    "text": "how we leverage rate to efficiently",
    "start": "14879",
    "end": "17000"
  },
  {
    "text": "scale batch inference at",
    "start": "17000",
    "end": "19800"
  },
  {
    "text": "ROBLOX so here's what we will cover",
    "start": "19800",
    "end": "22199"
  },
  {
    "text": "today we will start with a brief",
    "start": "22199",
    "end": "24320"
  },
  {
    "text": "introduction to Roblox and uh followed",
    "start": "24320",
    "end": "27599"
  },
  {
    "text": "by an overview of our machine learning",
    "start": "27599",
    "end": "29560"
  },
  {
    "text": "platform then we will deep dive into a",
    "start": "29560",
    "end": "31960"
  },
  {
    "text": "few Ray use cases and then discuss the",
    "start": "31960",
    "end": "35040"
  },
  {
    "text": "key components of our Ray drob",
    "start": "35040",
    "end": "38480"
  },
  {
    "text": "platform so for those of you who aren't",
    "start": "38480",
    "end": "41280"
  },
  {
    "text": "familiar with roox roox is a platform",
    "start": "41280",
    "end": "44600"
  },
  {
    "text": "that millions of people come every day",
    "start": "44600",
    "end": "46840"
  },
  {
    "text": "to create play and connect with each",
    "start": "46840",
    "end": "49480"
  },
  {
    "text": "other in all kinds of virtual experience",
    "start": "49480",
    "end": "52680"
  },
  {
    "text": "created by our Global community of",
    "start": "52680",
    "end": "56800"
  },
  {
    "text": "creators so AI have been deeply embeded",
    "start": "56800",
    "end": "60000"
  },
  {
    "text": "in Roblox especially when it comes to",
    "start": "60000",
    "end": "62960"
  },
  {
    "text": "safety which is essential to everything",
    "start": "62960",
    "end": "65518"
  },
  {
    "text": "we do so we leverage AI for um realtime",
    "start": "65519",
    "end": "70240"
  },
  {
    "text": "part detection voice and tex moderation",
    "start": "70240",
    "end": "73920"
  },
  {
    "text": "and uh safety review of user generated",
    "start": "73920",
    "end": "77119"
  },
  {
    "text": "content uh in terms of like uh images",
    "start": "77119",
    "end": "80920"
  },
  {
    "text": "video audio 3D model and even code we",
    "start": "80920",
    "end": "84920"
  },
  {
    "text": "will make sure everything is carefully",
    "start": "84920",
    "end": "87119"
  },
  {
    "text": "reviewed before make them available on",
    "start": "87119",
    "end": "89360"
  },
  {
    "text": "our platform",
    "start": "89360",
    "end": "90799"
  },
  {
    "text": "for creators we introduced a bunch of uh",
    "start": "90799",
    "end": "94159"
  },
  {
    "text": "very cool gen eye features uh to speed",
    "start": "94159",
    "end": "97240"
  },
  {
    "text": "up their creative process I will demo a",
    "start": "97240",
    "end": "100320"
  },
  {
    "text": "few in the following slides for players",
    "start": "100320",
    "end": "103920"
  },
  {
    "text": "AI Powers our recommendation and search",
    "start": "103920",
    "end": "106920"
  },
  {
    "text": "systems helping them to find the um",
    "start": "106920",
    "end": "110799"
  },
  {
    "text": "right experience and when they click the",
    "start": "110799",
    "end": "113719"
  },
  {
    "text": "play button our um U matchmaking",
    "start": "113719",
    "end": "117119"
  },
  {
    "text": "algorithm will help them to find the has",
    "start": "117119",
    "end": "120159"
  },
  {
    "text": "available",
    "start": "120159",
    "end": "122479"
  },
  {
    "text": "servers so here is some examples that uh",
    "start": "122479",
    "end": "126479"
  },
  {
    "text": "we released earlier this year um the",
    "start": "126479",
    "end": "130039"
  },
  {
    "text": "first is the uh texture generator so uh",
    "start": "130039",
    "end": "134280"
  },
  {
    "text": "creators can simply use text prompt to",
    "start": "134280",
    "end": "137800"
  },
  {
    "text": "quickly customize how a 3D object looks",
    "start": "137800",
    "end": "141959"
  },
  {
    "text": "so the texture will automatic conform to",
    "start": "141959",
    "end": "144400"
  },
  {
    "text": "the shape of the 3D object so it save",
    "start": "144400",
    "end": "147360"
  },
  {
    "text": "the Creator tons of time and effort",
    "start": "147360",
    "end": "151840"
  },
  {
    "text": "another one is Avatar Auto setup so uh",
    "start": "151879",
    "end": "155879"
  },
  {
    "text": "the creators can just simply upload a a",
    "start": "155879",
    "end": "158800"
  },
  {
    "text": "3D model and then this twool can help",
    "start": "158800",
    "end": "162159"
  },
  {
    "text": "them to automatically create the um um",
    "start": "162159",
    "end": "167280"
  },
  {
    "text": "uh Avatar measures and also uh um the",
    "start": "167280",
    "end": "171040"
  },
  {
    "text": "auto ring and then they can preview the",
    "start": "171040",
    "end": "174840"
  },
  {
    "text": "animation the uh skin tones the closing",
    "start": "174840",
    "end": "178840"
  },
  {
    "text": "and also preview the Avatar bodies",
    "start": "178840",
    "end": "182280"
  },
  {
    "text": "everything right inside the robox studio",
    "start": "182280",
    "end": "185120"
  },
  {
    "text": "so this will help uh the Avatar creation",
    "start": "185120",
    "end": "189120"
  },
  {
    "text": "uh from hours to just",
    "start": "189120",
    "end": "192959"
  },
  {
    "text": "minutes last just in uh last month we",
    "start": "192959",
    "end": "197440"
  },
  {
    "text": "announced a new uh AI incubation project",
    "start": "197440",
    "end": "201280"
  },
  {
    "text": "to develop a multimodel 3D Foundation",
    "start": "201280",
    "end": "204280"
  },
  {
    "text": "model for future generative creation",
    "start": "204280",
    "end": "209920"
  },
  {
    "text": "so uh to uh keep up with uh the rapid",
    "start": "210959",
    "end": "214840"
  },
  {
    "text": "demand of ml in the past few years we",
    "start": "214840",
    "end": "218519"
  },
  {
    "text": "develop a a UniFi machine learning",
    "start": "218519",
    "end": "222439"
  },
  {
    "text": "platform this is built on top of uh many",
    "start": "222439",
    "end": "225519"
  },
  {
    "text": "open source uh Technologies we use qflow",
    "start": "225519",
    "end": "229760"
  },
  {
    "text": "to um for its uh core ml components such",
    "start": "229760",
    "end": "234200"
  },
  {
    "text": "as pipelines and notebook servers",
    "start": "234200",
    "end": "237120"
  },
  {
    "text": "including Jupiter servers and V",
    "start": "237120",
    "end": "240280"
  },
  {
    "text": "servers and uh we use uh K Ser with the",
    "start": "240280",
    "end": "245400"
  },
  {
    "text": "Nvidia Triton inference server to",
    "start": "245400",
    "end": "248480"
  },
  {
    "text": "support multiple ml uh Frameworks",
    "start": "248480",
    "end": "251680"
  },
  {
    "text": "running on CPU and GPU across our",
    "start": "251680",
    "end": "254760"
  },
  {
    "text": "multiple data center and in the",
    "start": "254760",
    "end": "258160"
  },
  {
    "text": "crowd and uh on top of this we develop a",
    "start": "258160",
    "end": "261639"
  },
  {
    "text": "Roblox ml a python SDK to make it super",
    "start": "261639",
    "end": "265960"
  },
  {
    "text": "easy for our users to bring their",
    "start": "265960",
    "end": "268479"
  },
  {
    "text": "prototype ideas all the way to",
    "start": "268479",
    "end": "270840"
  },
  {
    "text": "production so user can just run",
    "start": "270840",
    "end": "273840"
  },
  {
    "text": "experiment in their notebook and then",
    "start": "273840",
    "end": "276840"
  },
  {
    "text": "they can launch it into a pipeline",
    "start": "276840",
    "end": "279080"
  },
  {
    "text": "without needing to build the darker",
    "start": "279080",
    "end": "281120"
  },
  {
    "text": "images and uh because our SDK will",
    "start": "281120",
    "end": "284880"
  },
  {
    "text": "automat help them to snapshot the",
    "start": "284880",
    "end": "286960"
  },
  {
    "text": "runtime environment in the death",
    "start": "286960",
    "end": "289840"
  },
  {
    "text": "cruster and we when they need to uh make",
    "start": "289840",
    "end": "293520"
  },
  {
    "text": "it to production we have the cicd system",
    "start": "293520",
    "end": "296759"
  },
  {
    "text": "for the pipeline and the model",
    "start": "296759",
    "end": "298360"
  },
  {
    "text": "deployment and user can simply use the",
    "start": "298360",
    "end": "301880"
  },
  {
    "text": "GitHub command to trigger the uh",
    "start": "301880",
    "end": "304800"
  },
  {
    "text": "automatic load testing and the",
    "start": "304800",
    "end": "306800"
  },
  {
    "text": "currentness",
    "start": "306800",
    "end": "308960"
  },
  {
    "text": "testing and with our uh model registry",
    "start": "308960",
    "end": "312919"
  },
  {
    "text": "user can use roox ml to upload and",
    "start": "312919",
    "end": "316199"
  },
  {
    "text": "download",
    "start": "316199",
    "end": "317319"
  },
  {
    "text": "models and this will be automat version",
    "start": "317319",
    "end": "320479"
  },
  {
    "text": "to make sure the model artifacts are",
    "start": "320479",
    "end": "323240"
  },
  {
    "text": "reproducible and traceable which is very",
    "start": "323240",
    "end": "326039"
  },
  {
    "text": "important for the",
    "start": "326039",
    "end": "328400"
  },
  {
    "text": "production and earlier this year we also",
    "start": "328400",
    "end": "331880"
  },
  {
    "text": "took one step further we heavily in",
    "start": "331880",
    "end": "335120"
  },
  {
    "text": "invested in vrm and directly contribute",
    "start": "335120",
    "end": "338720"
  },
  {
    "text": "to vrm open source project and we built",
    "start": "338720",
    "end": "342600"
  },
  {
    "text": "our uh ml Gate Weight to uh efficiently",
    "start": "342600",
    "end": "347600"
  },
  {
    "text": "support large uh",
    "start": "347600",
    "end": "350880"
  },
  {
    "text": "models but uh as Ai and ml continues to",
    "start": "352000",
    "end": "357319"
  },
  {
    "text": "evolve um especially with more and more",
    "start": "357319",
    "end": "360479"
  },
  {
    "text": "gen use cases and a huge uh unstructural",
    "start": "360479",
    "end": "364520"
  },
  {
    "text": "data needs we realize our existing",
    "start": "364520",
    "end": "367919"
  },
  {
    "text": "solution cannot keep up with this uh",
    "start": "367919",
    "end": "370400"
  },
  {
    "text": "rapid",
    "start": "370400",
    "end": "371680"
  },
  {
    "text": "demand so one area is batch",
    "start": "371680",
    "end": "375800"
  },
  {
    "text": "inference uh traditionally our users has",
    "start": "375800",
    "end": "379039"
  },
  {
    "text": "been used spark or leverage the online",
    "start": "379039",
    "end": "382039"
  },
  {
    "text": "inference service or use pipeline to run",
    "start": "382039",
    "end": "384840"
  },
  {
    "text": "their bat inference so spark we all know",
    "start": "384840",
    "end": "388680"
  },
  {
    "text": "that is very good for structural data",
    "start": "388680",
    "end": "390840"
  },
  {
    "text": "processing but it's not so efficient for",
    "start": "390840",
    "end": "393599"
  },
  {
    "text": "the ml task and for real time uh",
    "start": "393599",
    "end": "397199"
  },
  {
    "text": "inference Service A lot of times it",
    "start": "397199",
    "end": "400440"
  },
  {
    "text": "cannot fully utilize the computer",
    "start": "400440",
    "end": "402520"
  },
  {
    "text": "resource and causing uh GPU starvation",
    "start": "402520",
    "end": "406280"
  },
  {
    "text": "and Steve will talk about this later in",
    "start": "406280",
    "end": "408520"
  },
  {
    "text": "the use case Deep dive for pipeline ruls",
    "start": "408520",
    "end": "412360"
  },
  {
    "text": "it requires our Engineers write their",
    "start": "412360",
    "end": "415479"
  },
  {
    "text": "own loger to do uh data partitioning and",
    "start": "415479",
    "end": "418440"
  },
  {
    "text": "also error handling",
    "start": "418440",
    "end": "420160"
  },
  {
    "text": "which is very time consuming and also uh",
    "start": "420160",
    "end": "423479"
  },
  {
    "text": "error PR so to overcome all this",
    "start": "423479",
    "end": "427440"
  },
  {
    "text": "limitations we return to",
    "start": "427440",
    "end": "429639"
  },
  {
    "text": "Ray um it can make it super easy to uh",
    "start": "429639",
    "end": "434400"
  },
  {
    "text": "efficiently uh scale patch inference",
    "start": "434400",
    "end": "437520"
  },
  {
    "text": "workloads and",
    "start": "437520",
    "end": "439520"
  },
  {
    "text": "um as you can see uh in this diagram we",
    "start": "439520",
    "end": "444120"
  },
  {
    "text": "integrate Ray with our existing",
    "start": "444120",
    "end": "447039"
  },
  {
    "text": "ecosystem and we also add the batch API",
    "start": "447039",
    "end": "450800"
  },
  {
    "text": "to support uh large language model uh",
    "start": "450800",
    "end": "454560"
  },
  {
    "text": "batch inference which uh make it uh",
    "start": "454560",
    "end": "457840"
  },
  {
    "text": "easier for users to run the lrm",
    "start": "457840",
    "end": "460919"
  },
  {
    "text": "inference uh with optimal",
    "start": "460919",
    "end": "464360"
  },
  {
    "text": "setups and uh this R we have seen um",
    "start": "464360",
    "end": "469080"
  },
  {
    "text": "optimal uh improve uh computer resource",
    "start": "469080",
    "end": "472720"
  },
  {
    "text": "utilization and enable multistage",
    "start": "472720",
    "end": "475680"
  },
  {
    "text": "processing in henus uh Computer",
    "start": "475680",
    "end": "478879"
  },
  {
    "text": "Resources",
    "start": "478879",
    "end": "480639"
  },
  {
    "text": "and we also enable task periodism with",
    "start": "480639",
    "end": "483599"
  },
  {
    "text": "easier F tolerance and we have seen",
    "start": "483599",
    "end": "486840"
  },
  {
    "text": "greater efficient gain this",
    "start": "486840",
    "end": "490639"
  },
  {
    "text": "rate so here is a few uh key results",
    "start": "490639",
    "end": "495759"
  },
  {
    "text": "from our uh batch inference use cases",
    "start": "495759",
    "end": "499520"
  },
  {
    "text": "and since launching R we have run a r",
    "start": "499520",
    "end": "502759"
  },
  {
    "text": "job uh daily next uh let's have St to",
    "start": "502759",
    "end": "507240"
  },
  {
    "text": "give some deep dive of our use cases",
    "start": "507240",
    "end": "510280"
  },
  {
    "text": "yeah thank you way uh so I want to talk",
    "start": "510280",
    "end": "512440"
  },
  {
    "text": "about some use cases of Ray data at",
    "start": "512440",
    "end": "514320"
  },
  {
    "text": "roblocks that really justify this place",
    "start": "514320",
    "end": "516039"
  },
  {
    "start": "515000",
    "end": "819000"
  },
  {
    "text": "in our ml platform so first we have clip",
    "start": "516039",
    "end": "518440"
  },
  {
    "text": "PCH inference so as you probably know",
    "start": "518440",
    "end": "520440"
  },
  {
    "text": "clip is an open a model that Maps both",
    "start": "520440",
    "end": "522479"
  },
  {
    "text": "images and text to the same embeding",
    "start": "522479",
    "end": "523880"
  },
  {
    "text": "space so we can tell how well a given",
    "start": "523880",
    "end": "525720"
  },
  {
    "text": "text and image fit together so we use",
    "start": "525720",
    "end": "527839"
  },
  {
    "text": "this for semantic search for 3D assets",
    "start": "527839",
    "end": "530560"
  },
  {
    "text": "which helps uh developers to find 3D",
    "start": "530560",
    "end": "532399"
  },
  {
    "text": "models in their inventory without",
    "start": "532399",
    "end": "534120"
  },
  {
    "text": "needing to remember exactly what they're",
    "start": "534120",
    "end": "535560"
  },
  {
    "text": "called uh we use rate data to back fill",
    "start": "535560",
    "end": "537800"
  },
  {
    "text": "these embeddings we also use clip for",
    "start": "537800",
    "end": "540000"
  },
  {
    "text": "clickbait detection we have millions of",
    "start": "540000",
    "end": "542399"
  },
  {
    "text": "experiences on Roblox and inevitably",
    "start": "542399",
    "end": "544560"
  },
  {
    "text": "some developers would try to copy logos",
    "start": "544560",
    "end": "546680"
  },
  {
    "text": "from popular experiences and put them on",
    "start": "546680",
    "end": "548200"
  },
  {
    "text": "their own thumbnails to bait users no",
    "start": "548200",
    "end": "550160"
  },
  {
    "text": "clicking them so to detect this we're",
    "start": "550160",
    "end": "552399"
  },
  {
    "text": "using YOLO to segment the Lo the logos",
    "start": "552399",
    "end": "554480"
  },
  {
    "text": "from the thumbnails and then we're",
    "start": "554480",
    "end": "556079"
  },
  {
    "text": "comparing their clipping medings against",
    "start": "556079",
    "end": "557640"
  },
  {
    "text": "the medings of those protected logos",
    "start": "557640",
    "end": "560079"
  },
  {
    "text": "again Ray was used to backfill these",
    "start": "560079",
    "end": "563320"
  },
  {
    "text": "medings so why do we choose to use Ray",
    "start": "563320",
    "end": "565640"
  },
  {
    "text": "data for these tasks well first it's",
    "start": "565640",
    "end": "568360"
  },
  {
    "text": "because our workload is off offline",
    "start": "568360",
    "end": "569880"
  },
  {
    "text": "second is multistage and third is",
    "start": "569880",
    "end": "571839"
  },
  {
    "text": "heterogeneous and let's break this",
    "start": "571839",
    "end": "573959"
  },
  {
    "text": "down first we have an offline batch",
    "start": "573959",
    "end": "576320"
  },
  {
    "text": "inference job so compared to our",
    "start": "576320",
    "end": "578240"
  },
  {
    "text": "existing solution of deploying online",
    "start": "578240",
    "end": "579959"
  },
  {
    "text": "server and calling it in the loop uh we",
    "start": "579959",
    "end": "583040"
  },
  {
    "text": "are able to like using a dedicated batch",
    "start": "583040",
    "end": "585680"
  },
  {
    "text": "uh batch solution has many benefits so",
    "start": "585680",
    "end": "587800"
  },
  {
    "text": "for one we're able to use zero copy",
    "start": "587800",
    "end": "589640"
  },
  {
    "text": "memory sharing to communicate between",
    "start": "589640",
    "end": "591000"
  },
  {
    "text": "processes instead of having to set up a",
    "start": "591000",
    "end": "593120"
  },
  {
    "text": "uh HTP server and sending requests to it",
    "start": "593120",
    "end": "595880"
  },
  {
    "text": "we're also able to uh instead of like",
    "start": "595880",
    "end": "598320"
  },
  {
    "text": "having to balance between latency and",
    "start": "598320",
    "end": "599920"
  },
  {
    "text": "throughput in online setting for example",
    "start": "599920",
    "end": "602000"
  },
  {
    "text": "by using Dynamic batching uh in offline",
    "start": "602000",
    "end": "604120"
  },
  {
    "text": "solution we can purely optimize for",
    "start": "604120",
    "end": "605959"
  },
  {
    "text": "throughput uh we can always saturate the",
    "start": "605959",
    "end": "608079"
  },
  {
    "text": "GPU memory and finally when you're",
    "start": "608079",
    "end": "610720"
  },
  {
    "text": "sending HTTP requests you have to handle",
    "start": "610720",
    "end": "612880"
  },
  {
    "text": "the errors yourself for example if the",
    "start": "612880",
    "end": "615120"
  },
  {
    "text": "model is auto scaling up and there's",
    "start": "615120",
    "end": "617440"
  },
  {
    "text": "some requests that are timing out we",
    "start": "617440",
    "end": "619240"
  },
  {
    "text": "have to deal with that manually and",
    "start": "619240",
    "end": "620800"
  },
  {
    "text": "sometimes the requests are canceled by",
    "start": "620800",
    "end": "622440"
  },
  {
    "text": "clients but it's still being process on",
    "start": "622440",
    "end": "623720"
  },
  {
    "text": "the server so it leads to some Wast of",
    "start": "623720",
    "end": "625600"
  },
  {
    "text": "computation on the other hand ray runs",
    "start": "625600",
    "end": "628480"
  },
  {
    "text": "in the local cluster and and it's fa",
    "start": "628480",
    "end": "629720"
  },
  {
    "text": "tolerant so even if a node fails rate",
    "start": "629720",
    "end": "631760"
  },
  {
    "text": "can still",
    "start": "631760",
    "end": "632839"
  },
  {
    "text": "recover because of these reasons and the",
    "start": "632839",
    "end": "635279"
  },
  {
    "text": "fact that our existing solution was now",
    "start": "635279",
    "end": "636639"
  },
  {
    "text": "the most optimized we're able to achieve",
    "start": "636639",
    "end": "638360"
  },
  {
    "text": "a 10x Improvement when moving to Ray and",
    "start": "638360",
    "end": "640680"
  },
  {
    "text": "it's pretty easy to see why just by",
    "start": "640680",
    "end": "642320"
  },
  {
    "text": "looking at this GP utilization chart",
    "start": "642320",
    "end": "643839"
  },
  {
    "text": "before the",
    "start": "643839",
    "end": "645000"
  },
  {
    "text": "after so the second point I want to make",
    "start": "645000",
    "end": "647399"
  },
  {
    "text": "is that our job is multi-stage and to",
    "start": "647399",
    "end": "649240"
  },
  {
    "text": "see why you can actually see this uh",
    "start": "649240",
    "end": "651440"
  },
  {
    "text": "core logic in our clip pipeline so we",
    "start": "651440",
    "end": "654040"
  },
  {
    "text": "first read the park file and then map a",
    "start": "654040",
    "end": "656399"
  },
  {
    "text": "CPU processing function to it and then",
    "start": "656399",
    "end": "658760"
  },
  {
    "text": "we use the G for clip inference and",
    "start": "658760",
    "end": "660760"
  },
  {
    "text": "finally we save it out to S3 notice that",
    "start": "660760",
    "end": "663800"
  },
  {
    "text": "we have a mix of CPU and GPU resources",
    "start": "663800",
    "end": "666480"
  },
  {
    "text": "hence this is hog genius so let's keep",
    "start": "666480",
    "end": "669320"
  },
  {
    "text": "this diagram in mind and let's see how",
    "start": "669320",
    "end": "670959"
  },
  {
    "text": "Ray is able to execute this efficiently",
    "start": "670959",
    "end": "673079"
  },
  {
    "text": "using streaming and",
    "start": "673079",
    "end": "675079"
  },
  {
    "text": "pipelining so as the so first Ray breaks",
    "start": "675079",
    "end": "677959"
  },
  {
    "text": "the data set into partitions and the",
    "start": "677959",
    "end": "679800"
  },
  {
    "text": "steps into stages and as the execution",
    "start": "679800",
    "end": "683120"
  },
  {
    "text": "starts the first partition gets executed",
    "start": "683120",
    "end": "685279"
  },
  {
    "text": "in the first stage and after that's done",
    "start": "685279",
    "end": "687240"
  },
  {
    "text": "it's passed to the GPU inference stage",
    "start": "687240",
    "end": "689360"
  },
  {
    "text": "in the meantime there's a free CPU for",
    "start": "689360",
    "end": "691320"
  },
  {
    "text": "stage one so Ray starts executing the",
    "start": "691320",
    "end": "693240"
  },
  {
    "text": "second partition of the data set so",
    "start": "693240",
    "end": "695680"
  },
  {
    "text": "basically the data streams through the",
    "start": "695680",
    "end": "697440"
  },
  {
    "text": "pipeline and all the stages are",
    "start": "697440",
    "end": "700079"
  },
  {
    "text": "interleaved together to maximize the",
    "start": "700079",
    "end": "702920"
  },
  {
    "text": "utilization and this is in contrast to a",
    "start": "702920",
    "end": "705360"
  },
  {
    "text": "framewor like spark which essentially",
    "start": "705360",
    "end": "707639"
  },
  {
    "text": "executes one stage after the other Ray",
    "start": "707639",
    "end": "709839"
  },
  {
    "text": "is a lot more granular and is able to",
    "start": "709839",
    "end": "711440"
  },
  {
    "text": "use the resource more",
    "start": "711440",
    "end": "713880"
  },
  {
    "text": "efficiently so if your CPU preprocessing",
    "start": "713880",
    "end": "716360"
  },
  {
    "text": "is particularly intensive you can also",
    "start": "716360",
    "end": "718279"
  },
  {
    "text": "easily add in addition CPU node to help",
    "start": "718279",
    "end": "720920"
  },
  {
    "text": "and R will take care of bringing in the",
    "start": "720920",
    "end": "723320"
  },
  {
    "text": "objects from the CPU node to the GPU",
    "start": "723320",
    "end": "724920"
  },
  {
    "text": "node for the second step of batch",
    "start": "724920",
    "end": "727440"
  },
  {
    "text": "inference so you can actually see this",
    "start": "727440",
    "end": "729480"
  },
  {
    "text": "in action in this timeline trace of our",
    "start": "729480",
    "end": "731519"
  },
  {
    "text": "clip job so I wanted to show you this",
    "start": "731519",
    "end": "733199"
  },
  {
    "text": "because I think it's pretty neat uh the",
    "start": "733199",
    "end": "735040"
  },
  {
    "text": "way you read this is that each row is a",
    "start": "735040",
    "end": "736800"
  },
  {
    "text": "separate worker process and um so you",
    "start": "736800",
    "end": "739800"
  },
  {
    "text": "can see the CPU node on top and the GPU",
    "start": "739800",
    "end": "741360"
  },
  {
    "text": "node on the bottom we're using pretty",
    "start": "741360",
    "end": "743440"
  },
  {
    "text": "intensive pre-processing logic so we're",
    "start": "743440",
    "end": "745079"
  },
  {
    "text": "bringing in an additional node almost",
    "start": "745079",
    "end": "747199"
  },
  {
    "text": "all the rows you see are CPU workers",
    "start": "747199",
    "end": "749720"
  },
  {
    "text": "except for the third row to the bottom",
    "start": "749720",
    "end": "751199"
  },
  {
    "text": "which is the only GPU worker we have in",
    "start": "751199",
    "end": "752680"
  },
  {
    "text": "the cluster So reading from left to",
    "start": "752680",
    "end": "755240"
  },
  {
    "text": "right we first download the model on the",
    "start": "755240",
    "end": "757160"
  },
  {
    "text": "GP worker and then we start executing",
    "start": "757160",
    "end": "760040"
  },
  {
    "text": "like all the CP workers on both the CPU",
    "start": "760040",
    "end": "761680"
  },
  {
    "text": "and the GPU node start running in start",
    "start": "761680",
    "end": "763760"
  },
  {
    "text": "running pre-processing tasks on the",
    "start": "763760",
    "end": "765240"
  },
  {
    "text": "first batch of batches of data and after",
    "start": "765240",
    "end": "767680"
  },
  {
    "text": "they're done Ray will automatically",
    "start": "767680",
    "end": "769519"
  },
  {
    "text": "Aggregates the batches together and",
    "start": "769519",
    "end": "771639"
  },
  {
    "text": "start the clip inference on those",
    "start": "771639",
    "end": "773079"
  },
  {
    "text": "results and the clip inference takes in",
    "start": "773079",
    "end": "775040"
  },
  {
    "text": "a large batch size to saturate the vram",
    "start": "775040",
    "end": "777199"
  },
  {
    "text": "of the GPU so at the meantime the CPUs",
    "start": "777199",
    "end": "780360"
  },
  {
    "text": "start preprocessing the next next next",
    "start": "780360",
    "end": "782399"
  },
  {
    "text": "batch of data and it doesn't take a",
    "start": "782399",
    "end": "784519"
  },
  {
    "text": "break at all in the middle so this is uh",
    "start": "784519",
    "end": "787760"
  },
  {
    "text": "the ability of pipelining to take",
    "start": "787760",
    "end": "789320"
  },
  {
    "text": "advantage of the resources you have so",
    "start": "789320",
    "end": "791839"
  },
  {
    "text": "in this control experiment we're able to",
    "start": "791839",
    "end": "793760"
  },
  {
    "text": "achieve a 58% cost saving compared to",
    "start": "793760",
    "end": "795959"
  },
  {
    "text": "our optimized online",
    "start": "795959",
    "end": "798680"
  },
  {
    "text": "solution Ray is also able to keep the",
    "start": "798680",
    "end": "800800"
  },
  {
    "text": "memory usage down by throttling steps",
    "start": "800800",
    "end": "803120"
  },
  {
    "text": "that are producing too many outputs for",
    "start": "803120",
    "end": "804560"
  },
  {
    "text": "the next step and we actually saw a huge",
    "start": "804560",
    "end": "806680"
  },
  {
    "text": "Improvement of this in rate data 2.10 in",
    "start": "806680",
    "end": "809279"
  },
  {
    "text": "which the scheduler is able to",
    "start": "809279",
    "end": "810720"
  },
  {
    "text": "predictively restrict the parallelism of",
    "start": "810720",
    "end": "812800"
  },
  {
    "text": "tasks in order to avoid um spilling",
    "start": "812800",
    "end": "816199"
  },
  {
    "text": "memory into dis so shout out to the r",
    "start": "816199",
    "end": "818279"
  },
  {
    "text": "data",
    "start": "818279",
    "end": "819480"
  },
  {
    "start": "819000",
    "end": "1064000"
  },
  {
    "text": "team so moving on to the second use case",
    "start": "819480",
    "end": "821720"
  },
  {
    "text": "we have which is Al batch",
    "start": "821720",
    "end": "823800"
  },
  {
    "text": "API I Roblox were fully embracing the",
    "start": "823800",
    "end": "826720"
  },
  {
    "text": "Innovations in llms and while we have",
    "start": "826720",
    "end": "829160"
  },
  {
    "text": "many online use cases we also have a",
    "start": "829160",
    "end": "831480"
  },
  {
    "text": "huge demand for having a simple to use",
    "start": "831480",
    "end": "833160"
  },
  {
    "text": "offline batch API we use it for",
    "start": "833160",
    "end": "835440"
  },
  {
    "text": "translation in which we trans uh",
    "start": "835440",
    "end": "837320"
  },
  {
    "text": "summarize content for Roblox experiences",
    "start": "837320",
    "end": "839600"
  },
  {
    "text": "and translate user documentation in",
    "start": "839600",
    "end": "841480"
  },
  {
    "text": "batches and we also use it for safety so",
    "start": "841480",
    "end": "844600"
  },
  {
    "text": "as we mentioned uh at ROBLOX we take",
    "start": "844600",
    "end": "846399"
  },
  {
    "text": "safety very seriously we have an an",
    "start": "846399",
    "end": "848440"
  },
  {
    "text": "amazing team of thousands of moderators",
    "start": "848440",
    "end": "851000"
  },
  {
    "text": "who are always conducting safety reviews",
    "start": "851000",
    "end": "852720"
  },
  {
    "text": "on chat messages and assets on the",
    "start": "852720",
    "end": "854440"
  },
  {
    "text": "platform and when we have to change our",
    "start": "854440",
    "end": "857320"
  },
  {
    "text": "policy we have to essentially validate",
    "start": "857320",
    "end": "859199"
  },
  {
    "text": "it by retraining the moderators and",
    "start": "859199",
    "end": "861240"
  },
  {
    "text": "having them label some as some samples",
    "start": "861240",
    "end": "863399"
  },
  {
    "text": "and then uh comparing the results",
    "start": "863399",
    "end": "864920"
  },
  {
    "text": "against our expectations and if it's not",
    "start": "864920",
    "end": "867519"
  },
  {
    "text": "good then bad to the drawing board to",
    "start": "867519",
    "end": "869199"
  },
  {
    "text": "repeat this whole process again so we",
    "start": "869199",
    "end": "871560"
  },
  {
    "text": "can essentially speed up this inner loop",
    "start": "871560",
    "end": "873279"
  },
  {
    "text": "by using llms to label the samples in",
    "start": "873279",
    "end": "876079"
  },
  {
    "text": "batches according to the new policy and",
    "start": "876079",
    "end": "878040"
  },
  {
    "text": "this will streamline the policy drafting",
    "start": "878040",
    "end": "879920"
  },
  {
    "text": "process and increase the iteration speed",
    "start": "879920",
    "end": "882880"
  },
  {
    "text": "we also eventually want to develop our",
    "start": "882880",
    "end": "884199"
  },
  {
    "text": "own adaptable and policy aware am models",
    "start": "884199",
    "end": "886480"
  },
  {
    "text": "for automated abuse detection we have",
    "start": "886480",
    "end": "888920"
  },
  {
    "text": "many other use cases such as uh game",
    "start": "888920",
    "end": "891240"
  },
  {
    "text": "journ labeling and search query",
    "start": "891240",
    "end": "893320"
  },
  {
    "text": "relevancy detection and so on so for the",
    "start": "893320",
    "end": "896199"
  },
  {
    "text": "batch API this is our architecture uh we",
    "start": "896199",
    "end": "898440"
  },
  {
    "text": "want to keep keep the online and offline",
    "start": "898440",
    "end": "900279"
  },
  {
    "text": "API in the same unified API so we're",
    "start": "900279",
    "end": "903199"
  },
  {
    "text": "using we're putting this behind ML gway",
    "start": "903199",
    "end": "905720"
  },
  {
    "text": "and when the user Mak a request the MAA",
    "start": "905720",
    "end": "907959"
  },
  {
    "text": "forwards it to the ray backend service",
    "start": "907959",
    "end": "909680"
  },
  {
    "text": "to create a ray job in the user's name",
    "start": "909680",
    "end": "911600"
  },
  {
    "text": "space and then qra is responsible for",
    "start": "911600",
    "end": "914320"
  },
  {
    "text": "reconciling the cluster renting the code",
    "start": "914320",
    "end": "917000"
  },
  {
    "text": "and uh writing the outputs to S3 we also",
    "start": "917000",
    "end": "919399"
  },
  {
    "text": "have a front end to show the user",
    "start": "919399",
    "end": "921120"
  },
  {
    "text": "exactly what the progress",
    "start": "921120",
    "end": "923560"
  },
  {
    "text": "is compared to an online endpoint with a",
    "start": "923560",
    "end": "926160"
  },
  {
    "text": "bat API we're able to improve the",
    "start": "926160",
    "end": "927639"
  },
  {
    "text": "throughput by efficiently handling large",
    "start": "927639",
    "end": "929759"
  },
  {
    "text": "volumes of data sometimes we have more",
    "start": "929759",
    "end": "931680"
  },
  {
    "text": "than 10 billion tokens in a single job",
    "start": "931680",
    "end": "933800"
  },
  {
    "text": "and it takes like maybe 10 to 20 hours",
    "start": "933800",
    "end": "935360"
  },
  {
    "text": "to run we can also reduce processing",
    "start": "935360",
    "end": "937720"
  },
  {
    "text": "time by funing multiple requests into",
    "start": "937720",
    "end": "939480"
  },
  {
    "text": "the same same call since all the",
    "start": "939480",
    "end": "942040"
  },
  {
    "text": "requests in a batch job are consistently",
    "start": "942040",
    "end": "944279"
  },
  {
    "text": "run in the same cluster we're also to",
    "start": "944279",
    "end": "946240"
  },
  {
    "text": "ensure we're also able to ensure that",
    "start": "946240",
    "end": "947920"
  },
  {
    "text": "related requests are processed together",
    "start": "947920",
    "end": "950360"
  },
  {
    "text": "taking advantage of prefix caching we",
    "start": "950360",
    "end": "952560"
  },
  {
    "text": "can also deliver more consistent and",
    "start": "952560",
    "end": "953800"
  },
  {
    "text": "reliable outputs and I just want to",
    "start": "953800",
    "end": "955880"
  },
  {
    "text": "highlight prefix prefix caching is a",
    "start": "955880",
    "end": "957880"
  },
  {
    "text": "huge factor in our throughput increase",
    "start": "957880",
    "end": "960600"
  },
  {
    "text": "because in the safety example for",
    "start": "960600",
    "end": "962000"
  },
  {
    "text": "example uh we have we always put the",
    "start": "962000",
    "end": "964360"
  },
  {
    "text": "same safety policy in front of every",
    "start": "964360",
    "end": "965959"
  },
  {
    "text": "prompt so being able to cach the prefix",
    "start": "965959",
    "end": "968120"
  },
  {
    "text": "in the KV storage um in vlm has really",
    "start": "968120",
    "end": "970920"
  },
  {
    "text": "helped us to speed it up we also have",
    "start": "970920",
    "end": "973800"
  },
  {
    "text": "better cost efficiency by having lower",
    "start": "973800",
    "end": "975560"
  },
  {
    "text": "cost per token and being able to use the",
    "start": "975560",
    "end": "977720"
  },
  {
    "text": "GPU instances when they're uh not being",
    "start": "977720",
    "end": "979560"
  },
  {
    "text": "used and event uh finally we also want",
    "start": "979560",
    "end": "982399"
  },
  {
    "text": "to reserve the online endpoint so we can",
    "start": "982399",
    "end": "984240"
  },
  {
    "text": "avoid overwhelming it it's kind of like",
    "start": "984240",
    "end": "985519"
  },
  {
    "text": "how you want to separate your olap and",
    "start": "985519",
    "end": "987079"
  },
  {
    "text": "ltp databases",
    "start": "987079",
    "end": "989079"
  },
  {
    "text": "so this is our API um it actually is",
    "start": "989079",
    "end": "991600"
  },
  {
    "text": "very similar to the open AI bat API but",
    "start": "991600",
    "end": "993839"
  },
  {
    "text": "we did have to expose a lot more",
    "start": "993839",
    "end": "995319"
  },
  {
    "text": "parameters for example we let users",
    "start": "995319",
    "end": "997319"
  },
  {
    "text": "customize all the VM parameters so they",
    "start": "997319",
    "end": "999600"
  },
  {
    "text": "can like deploy their own they can use",
    "start": "999600",
    "end": "1001440"
  },
  {
    "text": "their own models uh if they want we also",
    "start": "1001440",
    "end": "1005120"
  },
  {
    "text": "uh behind the scenes we also use the",
    "start": "1005120",
    "end": "1007040"
  },
  {
    "text": "same logic as we did for the clipping",
    "start": "1007040",
    "end": "1008800"
  },
  {
    "text": "example but in this case we're using VM",
    "start": "1008800",
    "end": "1011399"
  },
  {
    "text": "as the batch INF",
    "start": "1011399",
    "end": "1013399"
  },
  {
    "text": "engine we ran into numerous challenges",
    "start": "1013399",
    "end": "1016240"
  },
  {
    "text": "When developing this uh first is that",
    "start": "1016240",
    "end": "1019279"
  },
  {
    "text": "LMS are quite large and sometimes it",
    "start": "1019279",
    "end": "1020920"
  },
  {
    "text": "take takes a long time to download them",
    "start": "1020920",
    "end": "1023000"
  },
  {
    "text": "so sometimes up to 30 minutes which",
    "start": "1023000",
    "end": "1025199"
  },
  {
    "text": "really uh lowers our utilization of the",
    "start": "1025199",
    "end": "1027678"
  },
  {
    "text": "of the nodes and our solution was",
    "start": "1027679",
    "end": "1029600"
  },
  {
    "text": "actually to use the mvme storage as a",
    "start": "1029600",
    "end": "1031400"
  },
  {
    "text": "way to cach the weights and also use a",
    "start": "1031400",
    "end": "1033400"
  },
  {
    "text": "more optimized downloader that's able to",
    "start": "1033400",
    "end": "1035240"
  },
  {
    "text": "achieve a speed of 5.6 gbytes per second",
    "start": "1035240",
    "end": "1037640"
  },
  {
    "text": "on a P4 instance we also want to",
    "start": "1037640",
    "end": "1040319"
  },
  {
    "text": "allocate the GPU instances more",
    "start": "1040319",
    "end": "1042000"
  },
  {
    "text": "efficiently and E while actually talk",
    "start": "1042000",
    "end": "1043918"
  },
  {
    "text": "more about like our custom GPU scheduler",
    "start": "1043919",
    "end": "1046640"
  },
  {
    "text": "to like use the spare GPU instances and",
    "start": "1046640",
    "end": "1049160"
  },
  {
    "text": "also bin pack the GPU noes and finally",
    "start": "1049160",
    "end": "1052200"
  },
  {
    "text": "we want to have a multi tendency support",
    "start": "1052200",
    "end": "1054520"
  },
  {
    "text": "uh we want to prevent a single user from",
    "start": "1054520",
    "end": "1056080"
  },
  {
    "text": "just overwhelming the entire cluster and",
    "start": "1056080",
    "end": "1058160"
  },
  {
    "text": "we do this by essentially defining a job",
    "start": "1058160",
    "end": "1060200"
  },
  {
    "text": "in the users community's namespace and",
    "start": "1060200",
    "end": "1062640"
  },
  {
    "text": "we have a resource Coda on each",
    "start": "1062640",
    "end": "1064640"
  },
  {
    "start": "1064000",
    "end": "1083000"
  },
  {
    "text": "namespace so now that we've established",
    "start": "1064640",
    "end": "1066880"
  },
  {
    "text": "that Ray was very helpful for us uh I",
    "start": "1066880",
    "end": "1068919"
  },
  {
    "text": "want to pass it to eting to talk about",
    "start": "1068919",
    "end": "1070280"
  },
  {
    "text": "how we build a platform around",
    "start": "1070280",
    "end": "1073120"
  },
  {
    "text": "it hey thanks uh Steve for talking about",
    "start": "1073120",
    "end": "1076080"
  },
  {
    "text": "the use case of uh Ray job at ROBLOX",
    "start": "1076080",
    "end": "1079559"
  },
  {
    "text": "so now let me talk about the r out",
    "start": "1079559",
    "end": "1081360"
  },
  {
    "text": "platform at",
    "start": "1081360",
    "end": "1082760"
  },
  {
    "text": "ROBLOX so since the launch of our Ral",
    "start": "1082760",
    "end": "1085559"
  },
  {
    "text": "platform uh we have seen numerous team",
    "start": "1085559",
    "end": "1087679"
  },
  {
    "text": "withing Roblox uh quickly adopting it to",
    "start": "1087679",
    "end": "1090520"
  },
  {
    "text": "run their Compu intensive workloads so",
    "start": "1090520",
    "end": "1093640"
  },
  {
    "text": "actually we are seeing hundred of jobs",
    "start": "1093640",
    "end": "1095600"
  },
  {
    "text": "being scheduled daily um just to uh",
    "start": "1095600",
    "end": "1099240"
  },
  {
    "text": "support a wide range of use case at",
    "start": "1099240",
    "end": "1103120"
  },
  {
    "start": "1103000",
    "end": "1174000"
  },
  {
    "text": "ROBLOX and this is our architecture of",
    "start": "1103159",
    "end": "1105960"
  },
  {
    "text": "our RAR platform and we build it on top",
    "start": "1105960",
    "end": "1109159"
  },
  {
    "text": "of the open source kubra and kubra API",
    "start": "1109159",
    "end": "1111559"
  },
  {
    "text": "server so when we design it we really",
    "start": "1111559",
    "end": "1114400"
  },
  {
    "text": "focus on these four key points so first",
    "start": "1114400",
    "end": "1117400"
  },
  {
    "text": "of all it needs to work simless with the",
    "start": "1117400",
    "end": "1120200"
  },
  {
    "text": "existing MTI tendency and GPU scheduling",
    "start": "1120200",
    "end": "1123240"
  },
  {
    "text": "setup and secondly we provide simplified",
    "start": "1123240",
    "end": "1126400"
  },
  {
    "text": "API with sensible defaults so which",
    "start": "1126400",
    "end": "1129360"
  },
  {
    "text": "helps our user to safely ignore the",
    "start": "1129360",
    "end": "1131679"
  },
  {
    "text": "rayad complex configuration and really",
    "start": "1131679",
    "end": "1134760"
  },
  {
    "text": "focus on what matter most to their",
    "start": "1134760",
    "end": "1136840"
  },
  {
    "text": "business needs and we also provide",
    "start": "1136840",
    "end": "1139840"
  },
  {
    "text": "preconfig compute templates to help",
    "start": "1139840",
    "end": "1142799"
  },
  {
    "text": "streamline the setup process uh which I",
    "start": "1142799",
    "end": "1145520"
  },
  {
    "text": "will show you later in the following",
    "start": "1145520",
    "end": "1147320"
  },
  {
    "text": "slides and we also provide deep",
    "start": "1147320",
    "end": "1149919"
  },
  {
    "text": "integration with C uh coup flow and this",
    "start": "1149919",
    "end": "1153120"
  },
  {
    "text": "helps our user to use Ray along with the",
    "start": "1153120",
    "end": "1156159"
  },
  {
    "text": "other ml Stacks at",
    "start": "1156159",
    "end": "1158600"
  },
  {
    "text": "ROBLOX and lastly we also automatically",
    "start": "1158600",
    "end": "1161919"
  },
  {
    "text": "process the logs and metrics uh using",
    "start": "1161919",
    "end": "1164720"
  },
  {
    "text": "the company Loki and premisus and we",
    "start": "1164720",
    "end": "1167320"
  },
  {
    "text": "believe this uh give the user and",
    "start": "1167320",
    "end": "1170039"
  },
  {
    "text": "comprehensive observability and",
    "start": "1170039",
    "end": "1173799"
  },
  {
    "text": "insights so this is the front end of our",
    "start": "1173799",
    "end": "1176840"
  },
  {
    "start": "1174000",
    "end": "1208000"
  },
  {
    "text": "Ray job platform as we can see from this",
    "start": "1176840",
    "end": "1179360"
  },
  {
    "text": "screenshot for this namespace we show",
    "start": "1179360",
    "end": "1182880"
  },
  {
    "text": "the uh all the ray jobs uh R job status",
    "start": "1182880",
    "end": "1186760"
  },
  {
    "text": "creation timestamp and the grafana uh",
    "start": "1186760",
    "end": "1189840"
  },
  {
    "text": "the corresponding grafana links and uh a",
    "start": "1189840",
    "end": "1193120"
  },
  {
    "text": "job message and the user can also create",
    "start": "1193120",
    "end": "1196480"
  },
  {
    "text": "a new R job by click the create eight",
    "start": "1196480",
    "end": "1199320"
  },
  {
    "text": "job uh button so that uh prevent them",
    "start": "1199320",
    "end": "1203280"
  },
  {
    "text": "from deal with the uh the complex yam",
    "start": "1203280",
    "end": "1206159"
  },
  {
    "text": "manufacturs of",
    "start": "1206159",
    "end": "1208360"
  },
  {
    "start": "1208000",
    "end": "1242000"
  },
  {
    "text": "kubra and this is our panel to create",
    "start": "1208360",
    "end": "1211679"
  },
  {
    "text": "new R job so as we can see from here the",
    "start": "1211679",
    "end": "1214679"
  },
  {
    "text": "user needs to specify the ray job name",
    "start": "1214679",
    "end": "1218000"
  },
  {
    "text": "um the base Docker image uh they can",
    "start": "1218000",
    "end": "1220760"
  },
  {
    "text": "also specify the entry point uh for the",
    "start": "1220760",
    "end": "1223559"
  },
  {
    "text": "computer template they can either select",
    "start": "1223559",
    "end": "1226000"
  },
  {
    "text": "the preconfig computer templates they",
    "start": "1226000",
    "end": "1228840"
  },
  {
    "text": "also specify their own custom uh",
    "start": "1228840",
    "end": "1231360"
  },
  {
    "text": "computer templates and of course they",
    "start": "1231360",
    "end": "1233240"
  },
  {
    "text": "can store the custom computer templates",
    "start": "1233240",
    "end": "1235480"
  },
  {
    "text": "for future use and this is the overall",
    "start": "1235480",
    "end": "1238520"
  },
  {
    "text": "UI of the uh job creation",
    "start": "1238520",
    "end": "1241600"
  },
  {
    "text": "panel so now let me talk about the GPU",
    "start": "1241600",
    "end": "1245200"
  },
  {
    "start": "1242000",
    "end": "1370000"
  },
  {
    "text": "scheduler so we used to have some",
    "start": "1245200",
    "end": "1247880"
  },
  {
    "text": "problems um so for example the team",
    "start": "1247880",
    "end": "1251080"
  },
  {
    "text": "requires uh the team own specific notes",
    "start": "1251080",
    "end": "1253880"
  },
  {
    "text": "and they need GPU type specific",
    "start": "1253880",
    "end": "1256840"
  },
  {
    "text": "quarters and secondly the teams uh",
    "start": "1256840",
    "end": "1260320"
  },
  {
    "text": "expect immediate GPU allocation even",
    "start": "1260320",
    "end": "1263200"
  },
  {
    "text": "though U they don't use them",
    "start": "1263200",
    "end": "1266720"
  },
  {
    "text": "24/7 and we also experienced resource",
    "start": "1266720",
    "end": "1270080"
  },
  {
    "text": "fragmentation which makes the ports pots",
    "start": "1270080",
    "end": "1273120"
  },
  {
    "text": "hard to get scheduled and we also",
    "start": "1273120",
    "end": "1275840"
  },
  {
    "text": "observe that user spend a lot of time uh",
    "start": "1275840",
    "end": "1278559"
  },
  {
    "text": "figuring out which nodes are",
    "start": "1278559",
    "end": "1281120"
  },
  {
    "text": "available so here are our",
    "start": "1281120",
    "end": "1284279"
  },
  {
    "text": "Solutions uh so we are migrating from uh",
    "start": "1284279",
    "end": "1287760"
  },
  {
    "text": "allocating the specific nodes to the",
    "start": "1287760",
    "end": "1290559"
  },
  {
    "text": "teams to a system where all the nodes",
    "start": "1290559",
    "end": "1293120"
  },
  {
    "text": "are",
    "start": "1293120",
    "end": "1294120"
  },
  {
    "text": "interchangeable so we have a rapo to",
    "start": "1294120",
    "end": "1296760"
  },
  {
    "text": "manage quot as code um so that the user",
    "start": "1296760",
    "end": "1300600"
  },
  {
    "text": "now owns a unicorn queue instead of the",
    "start": "1300600",
    "end": "1303559"
  },
  {
    "text": "specific",
    "start": "1303559",
    "end": "1304600"
  },
  {
    "text": "nodes and we also allow the user to",
    "start": "1304600",
    "end": "1307559"
  },
  {
    "text": "input a range of GPU type or tiers so",
    "start": "1307559",
    "end": "1310720"
  },
  {
    "text": "that we can get them the best available",
    "start": "1310720",
    "end": "1312840"
  },
  {
    "text": "option like",
    "start": "1312840",
    "end": "1314600"
  },
  {
    "text": "automatically and we also Implement",
    "start": "1314600",
    "end": "1317000"
  },
  {
    "text": "unicorn uh for advaned Q management we",
    "start": "1317000",
    "end": "1320760"
  },
  {
    "text": "also build our custom web hook uh as The",
    "start": "1320760",
    "end": "1323640"
  },
  {
    "text": "Meta scheduler to integrate with the",
    "start": "1323640",
    "end": "1325600"
  },
  {
    "text": "Unicorn que so that we can provide the",
    "start": "1325600",
    "end": "1328320"
  },
  {
    "text": "optimal uh queue uh for parts and also",
    "start": "1328320",
    "end": "1332720"
  },
  {
    "text": "uh we also make sure the CPU and memory",
    "start": "1332720",
    "end": "1335799"
  },
  {
    "text": "are proportional to the GPU",
    "start": "1335799",
    "end": "1338799"
  },
  {
    "text": "account so uh as we can see from this uh",
    "start": "1338799",
    "end": "1342760"
  },
  {
    "text": "this two diagram the one on the top",
    "start": "1342760",
    "end": "1344880"
  },
  {
    "text": "showcase the previous situation where",
    "start": "1344880",
    "end": "1347279"
  },
  {
    "text": "all the teams own sup speciic notes so",
    "start": "1347279",
    "end": "1350000"
  },
  {
    "text": "as we can see from the diagram uh all",
    "start": "1350000",
    "end": "1352600"
  },
  {
    "text": "the parts are not really schedu",
    "start": "1352600",
    "end": "1355200"
  },
  {
    "text": "efficiently to the GPU nodes and now we",
    "start": "1355200",
    "end": "1358520"
  },
  {
    "text": "move to a new system uh as we can see",
    "start": "1358520",
    "end": "1361159"
  },
  {
    "text": "from here uh like all the GPU nodes are",
    "start": "1361159",
    "end": "1365200"
  },
  {
    "text": "being used pretty efficiently and we",
    "start": "1365200",
    "end": "1367679"
  },
  {
    "text": "have more available",
    "start": "1367679",
    "end": "1370360"
  },
  {
    "start": "1370000",
    "end": "1383000"
  },
  {
    "text": "nodes and this is the real uh word",
    "start": "1370360",
    "end": "1373159"
  },
  {
    "text": "screenshot of how our noes get scheduled",
    "start": "1373159",
    "end": "1376640"
  },
  {
    "text": "as we can see from here almost all the",
    "start": "1376640",
    "end": "1378600"
  },
  {
    "text": "GPU nodes being uh used uh",
    "start": "1378600",
    "end": "1382799"
  },
  {
    "start": "1383000",
    "end": "1455000"
  },
  {
    "text": "fully and this uh slides illustrate the",
    "start": "1383200",
    "end": "1388000"
  },
  {
    "text": "uh the authentication and authorization",
    "start": "1388000",
    "end": "1390320"
  },
  {
    "text": "flow for Ray job so uh we follow the",
    "start": "1390320",
    "end": "1394640"
  },
  {
    "text": "coup uh coup flow architecture and we",
    "start": "1394640",
    "end": "1397200"
  },
  {
    "text": "support both oidc for human and service",
    "start": "1397200",
    "end": "1400039"
  },
  {
    "text": "account for",
    "start": "1400039",
    "end": "1401279"
  },
  {
    "text": "services so for users uh the oidc off",
    "start": "1401279",
    "end": "1405360"
  },
  {
    "text": "service communicate with the oidc",
    "start": "1405360",
    "end": "1407640"
  },
  {
    "text": "provider",
    "start": "1407640",
    "end": "1409120"
  },
  {
    "text": "uh to verify its identity and for",
    "start": "1409120",
    "end": "1412080"
  },
  {
    "text": "service the service uh account name is",
    "start": "1412080",
    "end": "1415080"
  },
  {
    "text": "been added to the request",
    "start": "1415080",
    "end": "1417400"
  },
  {
    "text": "header so uh and one so once after this",
    "start": "1417400",
    "end": "1422919"
  },
  {
    "text": "the request being passed to the backend",
    "start": "1422919",
    "end": "1426279"
  },
  {
    "text": "um which handles the authorization using",
    "start": "1426279",
    "end": "1429279"
  },
  {
    "text": "the kubernetes rback",
    "start": "1429279",
    "end": "1432559"
  },
  {
    "text": "resources uh so basically the backend uh",
    "start": "1432559",
    "end": "1436159"
  },
  {
    "text": "checked the permission of the job to",
    "start": "1436159",
    "end": "1438679"
  },
  {
    "text": "check whether it's have permission to",
    "start": "1438679",
    "end": "1440320"
  },
  {
    "text": "create a ray job or not and throughout",
    "start": "1440320",
    "end": "1442799"
  },
  {
    "text": "this process the kubernetes rback",
    "start": "1442799",
    "end": "1445960"
  },
  {
    "text": "resources uh uh managed permissions and",
    "start": "1445960",
    "end": "1450080"
  },
  {
    "text": "check the uh to ensure a secure access",
    "start": "1450080",
    "end": "1453760"
  },
  {
    "text": "to",
    "start": "1453760",
    "end": "1455640"
  },
  {
    "start": "1455000",
    "end": "1487000"
  },
  {
    "text": "resources and this is our notification",
    "start": "1455640",
    "end": "1458400"
  },
  {
    "text": "client so as we can see from here we",
    "start": "1458400",
    "end": "1461360"
  },
  {
    "text": "Implement our own notification client",
    "start": "1461360",
    "end": "1463679"
  },
  {
    "text": "that attached to the kubra job",
    "start": "1463679",
    "end": "1465360"
  },
  {
    "text": "submission pod so once a job status",
    "start": "1465360",
    "end": "1468480"
  },
  {
    "text": "become deterministic it send an events",
    "start": "1468480",
    "end": "1471039"
  },
  {
    "text": "to the",
    "start": "1471039",
    "end": "1472080"
  },
  {
    "text": "ccom then the K native notification",
    "start": "1472080",
    "end": "1475159"
  },
  {
    "text": "service pull the events from Kafka then",
    "start": "1475159",
    "end": "1478120"
  },
  {
    "text": "send an email or slack to the end user",
    "start": "1478120",
    "end": "1481600"
  },
  {
    "text": "and it will automatically scale down",
    "start": "1481600",
    "end": "1483480"
  },
  {
    "text": "once there are no more events in",
    "start": "1483480",
    "end": "1487120"
  },
  {
    "start": "1487000",
    "end": "1520000"
  },
  {
    "text": "kfka and we also Implement our own",
    "start": "1487520",
    "end": "1490640"
  },
  {
    "text": "Roblox ml asdk to support Ray job as we",
    "start": "1490640",
    "end": "1495039"
  },
  {
    "text": "can see from the code here we support uh",
    "start": "1495039",
    "end": "1497840"
  },
  {
    "text": "some job get job list job and delete job",
    "start": "1497840",
    "end": "1501480"
  },
  {
    "text": "and we also provide the SDK to help our",
    "start": "1501480",
    "end": "1505159"
  },
  {
    "text": "developer to sync or snapshot their",
    "start": "1505159",
    "end": "1507919"
  },
  {
    "text": "local working directory to the remote",
    "start": "1507919",
    "end": "1510080"
  },
  {
    "text": "Ray cluster to help them easier to",
    "start": "1510080",
    "end": "1512880"
  },
  {
    "text": "iterate their Ray",
    "start": "1512880",
    "end": "1515000"
  },
  {
    "text": "code and of course we also provide the",
    "start": "1515000",
    "end": "1517679"
  },
  {
    "text": "rest API for Ray",
    "start": "1517679",
    "end": "1520960"
  },
  {
    "start": "1520000",
    "end": "1751000"
  },
  {
    "text": "jobs and this is our future work so",
    "start": "1520960",
    "end": "1524399"
  },
  {
    "text": "first of all right now we use the open",
    "start": "1524399",
    "end": "1526559"
  },
  {
    "text": "source kubra API server we want to",
    "start": "1526559",
    "end": "1529000"
  },
  {
    "text": "improve it by uh implementing our own",
    "start": "1529000",
    "end": "1531880"
  },
  {
    "text": "API server so that we can better",
    "start": "1531880",
    "end": "1534080"
  },
  {
    "text": "integration with our company system and",
    "start": "1534080",
    "end": "1536840"
  },
  {
    "text": "have a flexible",
    "start": "1536840",
    "end": "1538520"
  },
  {
    "text": "iteration we also want to improve our uh",
    "start": "1538520",
    "end": "1541679"
  },
  {
    "text": "development workflow in addition to the",
    "start": "1541679",
    "end": "1544200"
  },
  {
    "text": "uh",
    "start": "1544200",
    "end": "1545399"
  },
  {
    "text": "SDK and after that right now we have",
    "start": "1545399",
    "end": "1548200"
  },
  {
    "text": "cicd integration uh for model serving",
    "start": "1548200",
    "end": "1552200"
  },
  {
    "text": "and ml pipeline we also want to have a",
    "start": "1552200",
    "end": "1554799"
  },
  {
    "text": "cicd integration for Ray job",
    "start": "1554799",
    "end": "1558480"
  },
  {
    "text": "and uh we also want to have cost",
    "start": "1558480",
    "end": "1560880"
  },
  {
    "text": "estimation before even we submit the r",
    "start": "1560880",
    "end": "1563600"
  },
  {
    "text": "batch job uh right now we have a really",
    "start": "1563600",
    "end": "1566039"
  },
  {
    "text": "good dashboard to track the finished Ray",
    "start": "1566039",
    "end": "1569240"
  },
  {
    "text": "job and uh finally we want to uh have a",
    "start": "1569240",
    "end": "1572799"
  },
  {
    "text": "way to expose the ray dashboard securely",
    "start": "1572799",
    "end": "1575440"
  },
  {
    "text": "because we all know that the ray",
    "start": "1575440",
    "end": "1577080"
  },
  {
    "text": "dashboard process uh does have right",
    "start": "1577080",
    "end": "1579559"
  },
  {
    "text": "permission so we want to uh make sure we",
    "start": "1579559",
    "end": "1581840"
  },
  {
    "text": "handle this",
    "start": "1581840",
    "end": "1584520"
  },
  {
    "text": "properly uh that's it so now we want to",
    "start": "1584679",
    "end": "1588360"
  },
  {
    "text": "have some time for",
    "start": "1588360",
    "end": "1590120"
  },
  {
    "text": "Q&A so for Q&A please use this",
    "start": "1590120",
    "end": "1592880"
  },
  {
    "text": "microphone",
    "start": "1592880",
    "end": "1595679"
  },
  {
    "text": "and hi um I have a question about the",
    "start": "1601279",
    "end": "1605520"
  },
  {
    "text": "the the UI dashboard and the",
    "start": "1605520",
    "end": "1607640"
  },
  {
    "text": "notifications you talk about in the reg",
    "start": "1607640",
    "end": "1610240"
  },
  {
    "text": "is that something part of the the open",
    "start": "1610240",
    "end": "1613480"
  },
  {
    "text": "source array or is it something you",
    "start": "1613480",
    "end": "1615159"
  },
  {
    "text": "build custom and does it it is it tied",
    "start": "1615159",
    "end": "1619000"
  },
  {
    "text": "to specific Ray cluster or is it Global",
    "start": "1619000",
    "end": "1621840"
  },
  {
    "text": "across all Ray clusters so uh so first",
    "start": "1621840",
    "end": "1624720"
  },
  {
    "text": "of all we Implement that in house",
    "start": "1624720",
    "end": "1627559"
  },
  {
    "text": "it's uh so first of all we Implement",
    "start": "1627559",
    "end": "1629960"
  },
  {
    "text": "that in house it's not part of the open",
    "start": "1629960",
    "end": "1632159"
  },
  {
    "text": "source uh we we write a lot of front end",
    "start": "1632159",
    "end": "1635360"
  },
  {
    "text": "code to make sure it integrate well with",
    "start": "1635360",
    "end": "1637279"
  },
  {
    "text": "the coup flow and uh the same for the",
    "start": "1637279",
    "end": "1640200"
  },
  {
    "text": "notification client we also Implement",
    "start": "1640200",
    "end": "1642360"
  },
  {
    "text": "our own notification client um does it",
    "start": "1642360",
    "end": "1645880"
  },
  {
    "text": "answer your",
    "start": "1645880",
    "end": "1647000"
  },
  {
    "text": "question yes yeah there was a second",
    "start": "1647000",
    "end": "1649520"
  },
  {
    "text": "yeah it's Global like it's our entire",
    "start": "1649520",
    "end": "1651399"
  },
  {
    "text": "platform is like this is enabled on the",
    "start": "1651399",
    "end": "1653640"
  },
  {
    "text": "whole platform doesn't matter which",
    "start": "1653640",
    "end": "1655320"
  },
  {
    "text": "cluster it is thank",
    "start": "1655320",
    "end": "1658559"
  },
  {
    "text": "you um so I have a second uh another",
    "start": "1658559",
    "end": "1661440"
  },
  {
    "text": "question so uh thank you for the talk by",
    "start": "1661440",
    "end": "1663480"
  },
  {
    "text": "the way um uh so basically do you have",
    "start": "1663480",
    "end": "1666360"
  },
  {
    "text": "like a federation for you know R cluster",
    "start": "1666360",
    "end": "1669159"
  },
  {
    "text": "over multiple comes clusters and if you",
    "start": "1669159",
    "end": "1671760"
  },
  {
    "text": "do and how uh you know any kind of",
    "start": "1671760",
    "end": "1674600"
  },
  {
    "text": "challenges for",
    "start": "1674600",
    "end": "1676440"
  },
  {
    "text": "that so uh we have a lot of Ray clusters",
    "start": "1676440",
    "end": "1682120"
  },
  {
    "text": "uh within the same kubernetes cluster",
    "start": "1682120",
    "end": "1685519"
  },
  {
    "text": "for each name space uh that's our setup",
    "start": "1685519",
    "end": "1688840"
  },
  {
    "text": "okay so uh there's no like kind of",
    "start": "1688840",
    "end": "1690880"
  },
  {
    "text": "federation across the different tic",
    "start": "1690880",
    "end": "1693760"
  },
  {
    "text": "clusters uh so we actually have a lot of",
    "start": "1693760",
    "end": "1696559"
  },
  {
    "text": "different environments and all those",
    "start": "1696559",
    "end": "1698240"
  },
  {
    "text": "environment are isolated oh got it so uh",
    "start": "1698240",
    "end": "1701480"
  },
  {
    "text": "uh now how about like you know",
    "start": "1701480",
    "end": "1703320"
  },
  {
    "text": "heterogeneous R clusters for for example",
    "start": "1703320",
    "end": "1706440"
  },
  {
    "text": "workers with different specs do you uh",
    "start": "1706440",
    "end": "1709320"
  },
  {
    "text": "have support for that yes yes we do we",
    "start": "1709320",
    "end": "1711799"
  },
  {
    "text": "can support for example uh on like we",
    "start": "1711799",
    "end": "1714919"
  },
  {
    "text": "actually support it in many different",
    "start": "1714919",
    "end": "1716320"
  },
  {
    "text": "way so for example the user can use the",
    "start": "1716320",
    "end": "1719159"
  },
  {
    "text": "SDK to specify what kind of the resource",
    "start": "1719159",
    "end": "1721760"
  },
  {
    "text": "they need uh they can also use the",
    "start": "1721760",
    "end": "1723840"
  },
  {
    "text": "frontend like job creation panel to",
    "start": "1723840",
    "end": "1726559"
  },
  {
    "text": "create their own computer template and",
    "start": "1726559",
    "end": "1728760"
  },
  {
    "text": "the compute template is the namespace",
    "start": "1728760",
    "end": "1731039"
  },
  {
    "text": "scope so they can really store it for",
    "start": "1731039",
    "end": "1733120"
  },
  {
    "text": "future use and of course they can for",
    "start": "1733120",
    "end": "1736279"
  },
  {
    "text": "the really mature user they can use the",
    "start": "1736279",
    "end": "1738600"
  },
  {
    "text": "crd like directly as well",
    "start": "1738600",
    "end": "1741640"
  },
  {
    "text": "yeah all right a round of applause for",
    "start": "1741640",
    "end": "1744600"
  },
  {
    "text": "Steve way and D thank you",
    "start": "1744600",
    "end": "1749679"
  }
]