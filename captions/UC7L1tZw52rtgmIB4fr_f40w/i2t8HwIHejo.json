[
  {
    "start": "0",
    "end": "61000"
  },
  {
    "text": "can you hear me I'm Arun and I am a computer vision engineer at right cell",
    "start": "5880",
    "end": "11160"
  },
  {
    "text": "uh so we are working on a product called Nemo as in Finding Nemo basically",
    "start": "11160",
    "end": "16198"
  },
  {
    "text": "building a search engine for automotive data we'll collect a bunch of different types of sensor data like camera lidar",
    "start": "16199",
    "end": "22859"
  },
  {
    "text": "Radars and stuff and we build a query engine that",
    "start": "22859",
    "end": "27960"
  },
  {
    "text": "extract scenarios that we would like to or that people want to extract so in today's talk I will be focusing on",
    "start": "27960",
    "end": "35700"
  },
  {
    "text": "a few challenges in training machine learning pipelines specifically focusing",
    "start": "35700",
    "end": "41940"
  },
  {
    "text": "on hyper parameter tuning and uh how we overcome them using Ray so at the",
    "start": "41940",
    "end": "48899"
  },
  {
    "text": "beginning I'll talk about some of the the Basics and briefly walk through walk you",
    "start": "48899",
    "end": "55620"
  },
  {
    "text": "through internal pipeline that we have and also at the end show a demo hopefully the demo works",
    "start": "55620",
    "end": "61980"
  },
  {
    "start": "61000",
    "end": "386000"
  },
  {
    "text": "so uh yeah uh",
    "start": "61980",
    "end": "66140"
  },
  {
    "text": "so what is hyper parameter tuning right right so and why do we need to do that and the figure on your right as you",
    "start": "67799",
    "end": "74460"
  },
  {
    "text": "could see uh some of the machine learning models that has been built over the past 72 years",
    "start": "74460",
    "end": "81119"
  },
  {
    "text": "um the y-axis shows the number of petaflops used for training each one of",
    "start": "81119",
    "end": "87360"
  },
  {
    "text": "those models a petaflop is basically 10 to the power 15 floating Point operations",
    "start": "87360",
    "end": "93420"
  },
  {
    "text": "so now as you could see the the number of computation the amount of computation",
    "start": "93420",
    "end": "98460"
  },
  {
    "text": "required for training these models have been growing quite exponentially over the last 70 years and with dealing with",
    "start": "98460",
    "end": "104820"
  },
  {
    "text": "hundreds of millions of peraflops of operations in the uh for for training pretty recent machine learning models",
    "start": "104820",
    "end": "112200"
  },
  {
    "text": "so to learn a robust AI model we need to let these algorithms learn an optimal",
    "start": "112200",
    "end": "118140"
  },
  {
    "text": "set of ways or parameters whereas hyper parameters are like think of them as meta parameters which uh which",
    "start": "118140",
    "end": "125579"
  },
  {
    "text": "determines or controls how the learning of parameters is achieved like how fast",
    "start": "125579",
    "end": "130920"
  },
  {
    "text": "can we learn where do we start what are the initial weights and where do we stop how many iterations of data should you",
    "start": "130920",
    "end": "138060"
  },
  {
    "text": "go through so those are basically hyper parameters now this is for mostly non-technical folks if you guys are technical folks you probably already",
    "start": "138060",
    "end": "144120"
  },
  {
    "text": "know these things so uh coming to the challenges what are the challenges like",
    "start": "144120",
    "end": "151040"
  },
  {
    "text": "so uh here are some of the uh a simple graph where we're trying to achieve a",
    "start": "151500",
    "end": "158280"
  },
  {
    "text": "higher accuracy and each uh line uh represents an experiment so basically",
    "start": "158280",
    "end": "165599"
  },
  {
    "text": "over the number of iterations we achieve uh some some higher accuracy based on the different",
    "start": "165599",
    "end": "172080"
  },
  {
    "text": "types of initialization used so in this case one thing to notice is that uh we uh each of the hyper parameter basically",
    "start": "172080",
    "end": "179400"
  },
  {
    "text": "determines the outcome of the experiment largely so if I have to train a new a",
    "start": "179400",
    "end": "185280"
  },
  {
    "text": "machine learning model with that takes hundreds of millions of better flops I'm",
    "start": "185280",
    "end": "191159"
  },
  {
    "text": "dealing with a grid search of these hyper parameters and each one of those experiment is going to consume such such",
    "start": "191159",
    "end": "196920"
  },
  {
    "text": "a extensive amount of memory and compute cost that means if I'm dealing with a",
    "start": "196920",
    "end": "202500"
  },
  {
    "text": "like half a dozen hyper parameters when the grid search through that would mean that I'd be dealing with hundreds of uh",
    "start": "202500",
    "end": "209519"
  },
  {
    "text": "or thousands of experiments so that's been basically thousands of times of training these models each one itself is",
    "start": "209519",
    "end": "216000"
  },
  {
    "text": "a each individual trial itself is expensive sometimes it can go up to three to five days and cost thousands or",
    "start": "216000",
    "end": "221580"
  },
  {
    "text": "millions of dollars sometimes and this scale is really uh quite exorbitant so",
    "start": "221580",
    "end": "228239"
  },
  {
    "text": "how do we optimize this is the question and I see the slide missing okay",
    "start": "228239",
    "end": "235440"
  },
  {
    "text": "so the focus is on trial distribution the goal is to minimize the number of trials that we spin off for uh during",
    "start": "235440",
    "end": "243120"
  },
  {
    "text": "hyper parameters and also achieve some early stopping techniques where we don't have to train each of the trial to the",
    "start": "243120",
    "end": "250680"
  },
  {
    "text": "end to figure out that this didn't work we could figure out that this didn't work as early as we could and then stop",
    "start": "250680",
    "end": "256560"
  },
  {
    "text": "them that way those resources can be used or allocated for uh for uh other",
    "start": "256560",
    "end": "262260"
  },
  {
    "text": "trials that are actually performing well so that is basically the the",
    "start": "262260",
    "end": "267300"
  },
  {
    "text": "overview of the trial distribution and before jumping into the uh",
    "start": "267300",
    "end": "272880"
  },
  {
    "text": "parameter tuning will go through some of the basics uh logging uh basically",
    "start": "272880",
    "end": "277979"
  },
  {
    "text": "retune has a extensive support for other logging libraries like tensorboard and",
    "start": "277979",
    "end": "283380"
  },
  {
    "text": "then mlflow ways and biases some of the commonly used loggers and the need for logging libraries is pretty uh extensive",
    "start": "283380",
    "end": "290220"
  },
  {
    "text": "because as I mentioned these experiments individual trials could be quite expensive and if you lose track of",
    "start": "290220",
    "end": "295979"
  },
  {
    "text": "experiments or we confuse one experiment for the other or we repeat experiments uh basically that's going to be even",
    "start": "295979",
    "end": "303240"
  },
  {
    "text": "more costlier so proper logging is kind of like a quite critical for these hyper",
    "start": "303240",
    "end": "308460"
  },
  {
    "text": "parameter search tasks and yeah as I mentioned poor logging often needs to",
    "start": "308460",
    "end": "313560"
  },
  {
    "text": "pretty exorbitant amount of money spent on um doing hyper parameter tuning",
    "start": "313560",
    "end": "319500"
  },
  {
    "text": "and finally parameter search Ray again has this great tune provides uh or",
    "start": "319500",
    "end": "325680"
  },
  {
    "text": "allows integration with some of the most commonly used parameter search libraries like uh hyperopter Neville grad and",
    "start": "325680",
    "end": "332699"
  },
  {
    "text": "Optima Etc so these parameter search libraries basically uh does optimize the",
    "start": "332699",
    "end": "338880"
  },
  {
    "text": "grid search process so as I mentioned we have a few hyper parameters maybe half a dozen hyper parameters that we want to",
    "start": "338880",
    "end": "344639"
  },
  {
    "text": "search across and there is a range for each hyper parameters those parameters could be learning rate where we have a",
    "start": "344639",
    "end": "351120"
  },
  {
    "text": "continuous range or could be a type of Optimizer which is more of a categorical so these uh these parameter",
    "start": "351120",
    "end": "359639"
  },
  {
    "text": "search libraries allows us to allow us to do efficiency talk about those in",
    "start": "359639",
    "end": "365100"
  },
  {
    "text": "detail in the next few slides so the core of these parameter search libraries are schedulers so in this case",
    "start": "365100",
    "end": "372900"
  },
  {
    "text": "the most commonly used two types of schedulers are asynchronous hyperband schedulers and median stopping rule so",
    "start": "372900",
    "end": "379860"
  },
  {
    "text": "asynchronous hyperband scheduler I don't want to jump too much in detail into technicalities of that but basically",
    "start": "379860",
    "end": "385080"
  },
  {
    "text": "it's it's uh it's a hyperband optimization algorithm that performs a",
    "start": "385080",
    "end": "390120"
  },
  {
    "start": "386000",
    "end": "433000"
  },
  {
    "text": "technique called successive halving so think about it this way if we start with 100 initial trials to search half a",
    "start": "390120",
    "end": "396300"
  },
  {
    "text": "dozen hyper parameters and then within a specified number of iterations we're",
    "start": "396300",
    "end": "402060"
  },
  {
    "text": "going to cut that by half like say for example only 50 of them goes through and then another within the repetitively we",
    "start": "402060",
    "end": "408479"
  },
  {
    "text": "just get 25 and then 12 of them and then six of them so ultimately we get that one perform best performing model that",
    "start": "408479",
    "end": "414600"
  },
  {
    "text": "actually has optimal hyper parameters so that's basically the asynchronous hyperband scheduler and median stopping",
    "start": "414600",
    "end": "420660"
  },
  {
    "text": "rule does a similar algorithm I mean does work similarly but basically it identifies uh what is a median",
    "start": "420660",
    "end": "426479"
  },
  {
    "text": "performing Trail in terms terms of some Metric like accuracy and it stops everything below that so basically it's",
    "start": "426479",
    "end": "433500"
  },
  {
    "text": "it's in a way successive halving but uh does it does sit in a slightly different way",
    "start": "433500",
    "end": "440120"
  },
  {
    "text": "and yeah gray supports a bunch of",
    "start": "440160",
    "end": "447680"
  },
  {
    "text": "development Frameworks like tensorflow by torch HD boost Keras and everything so it's pretty easy to integrate uh I",
    "start": "447680",
    "end": "455099"
  },
  {
    "text": "mean seamless I mean irrespective of uh whatever the libraries are using it's it's basically race minimally invasive",
    "start": "455099",
    "end": "461880"
  },
  {
    "text": "and easy to integrate uh into existing architectures so we found that true as",
    "start": "461880",
    "end": "468300"
  },
  {
    "text": "well in our use case and",
    "start": "468300",
    "end": "474319"
  },
  {
    "text": "so uh I'll talk briefly about an internal use case that we have and uh",
    "start": "475680",
    "end": "482039"
  },
  {
    "text": "yeah so in our product we have this a bunch of neural networks so we we",
    "start": "482039",
    "end": "487440"
  },
  {
    "text": "basically deal with Drive data think about a dash cam or a set of rear view",
    "start": "487440",
    "end": "492900"
  },
  {
    "text": "camera mirrors I mean cameras placed on rear view mirrors that are facing",
    "start": "492900",
    "end": "499379"
  },
  {
    "text": "frontward so the goal is to uh extract interesting scenarios like say",
    "start": "499379",
    "end": "505440"
  },
  {
    "text": "for example a pedestrian Crossing into the road or some car overtaking the ego vehicle the such interesting scenarios",
    "start": "505440",
    "end": "513060"
  },
  {
    "text": "might be occurring in the in the in the while the dry data is being collected but it is hard to query uh such",
    "start": "513060",
    "end": "519180"
  },
  {
    "text": "instances so that's what that's where we come in that's basically the product that we are developing so we have Lane",
    "start": "519180",
    "end": "524520"
  },
  {
    "text": "detection algorithm that basically identifies Lane pixels and classify them as like okay hey this is a continuous y",
    "start": "524520",
    "end": "531779"
  },
  {
    "text": "I mean like a a white lane or yellow lane or a dotted lanes and so on and also we have object detection algorithms",
    "start": "531779",
    "end": "537959"
  },
  {
    "text": "which basically sometimes 3D object detection algorithms sometimes 2D detection combined with depth prediction",
    "start": "537959",
    "end": "543180"
  },
  {
    "text": "we identify the 3D position model interactions and stuff tracking algorithms so bunch of neural networks",
    "start": "543180",
    "end": "550260"
  },
  {
    "text": "and we have to do have a parameter tuning for each of them for the algorithm to perform efficiently at the",
    "start": "550260",
    "end": "555779"
  },
  {
    "text": "entrance time here's the architecture uh basically a",
    "start": "555779",
    "end": "562140"
  },
  {
    "text": "bunch of perception networks that extract some semantic information that involves modeling the actors and",
    "start": "562140",
    "end": "569959"
  },
  {
    "text": "identifying them as a scenario that way at query time we can search say hey for example give me all the instances of an",
    "start": "569959",
    "end": "577200"
  },
  {
    "text": "accident that involves a pedestrian or give me all instances of a cut in that involves a a an SUV",
    "start": "577200",
    "end": "586440"
  },
  {
    "text": "something like that and that is basically the the runtime and I mean inference time application",
    "start": "586440",
    "end": "593399"
  },
  {
    "text": "so how we use Ray basically as I mentioned Rays quite minimally invasive",
    "start": "593399",
    "end": "599100"
  },
  {
    "text": "and fairly simple to integrate into existing Library so the core of this uh",
    "start": "599100",
    "end": "604140"
  },
  {
    "text": "Ray tune algorithm is the call method called tune.run where we have a bunch of",
    "start": "604140",
    "end": "609779"
  },
  {
    "text": "parameters starting with your training function that basically is a python function that that calls you training a",
    "start": "609779",
    "end": "615720"
  },
  {
    "text": "model and then your metric what you want to log say for example I want to achieve higher accuracy that's basically a",
    "start": "615720",
    "end": "622019"
  },
  {
    "text": "metric and then like a what is a type of thing you want to minimize the the loss or maximize the accuracy something like",
    "start": "622019",
    "end": "628019"
  },
  {
    "text": "that that basically is a mode and then some configuration the parameters like okay what are the parameters I want to",
    "start": "628019",
    "end": "633540"
  },
  {
    "text": "search learning rate or initial learning rate momentum uh weights and Etc and a",
    "start": "633540",
    "end": "639420"
  },
  {
    "text": "scheduler so as I mentioned like we have been playing around with a couple of schedulers one is one is hyperban",
    "start": "639420",
    "end": "645000"
  },
  {
    "text": "scheduler and the other one is like a median stopping rule but in this experiment we have been in this demo we",
    "start": "645000",
    "end": "650640"
  },
  {
    "text": "will be focusing on the hyperbatch scheduler and also wherever you define the",
    "start": "650640",
    "end": "657779"
  },
  {
    "text": "training function which was the first parameter of your tune dot run you call this tune.report that's basically where",
    "start": "657779",
    "end": "663660"
  },
  {
    "text": "you tell which metric in this case the accuracy which is the metric uh we call",
    "start": "663660",
    "end": "669060"
  },
  {
    "text": "them the variable to maximize to be maximized so these are basically the minimal code you need to modify in your",
    "start": "669060",
    "end": "675959"
  },
  {
    "text": "training pipeline to actually have rate tuned set up there and a bunch of advice I mean other",
    "start": "675959",
    "end": "682740"
  },
  {
    "text": "things that you could do is resource handling so I have a a GPU cluster with",
    "start": "682740",
    "end": "687839"
  },
  {
    "text": "uh like 96 gpus and uh some 200 CPUs and and so on so how do I optimize this",
    "start": "687839",
    "end": "694019"
  },
  {
    "text": "thing how do I maximize the resource utilization and how do I kind of like uh load balance it do I do manually or like",
    "start": "694019",
    "end": "702360"
  },
  {
    "text": "that's a major challenge right because sometimes the it's resource handling that takes most of the effort so Ray",
    "start": "702360",
    "end": "708720"
  },
  {
    "text": "takes care of some of I mean most of these uh things efficiently so basically you can you can mention the number of",
    "start": "708720",
    "end": "714300"
  },
  {
    "text": "CPU cores and GPU cores that you want you can even mention fractional ones like okay if it is if the GPU course is",
    "start": "714300",
    "end": "721260"
  },
  {
    "text": "like 0.5 it's going to basically you use half of the course and if the CPU course is facts fractional basically it's going",
    "start": "721260",
    "end": "726779"
  },
  {
    "text": "to cut the process into two and queue them so it's basically fairly straightforward actually so uh yeah this",
    "start": "726779",
    "end": "733260"
  },
  {
    "text": "just by passing a variable called resource portrayal allows us to to handle it more efficiently and more",
    "start": "733260",
    "end": "739920"
  },
  {
    "text": "elegantly so now coming to tracking where the some",
    "start": "739920",
    "end": "746279"
  },
  {
    "text": "of the major thing I mean advantages of VC with race it's it's a",
    "start": "746279",
    "end": "751980"
  },
  {
    "text": "it allows seamless integration with other tracking to other logging tools and other uh parameter research tools so",
    "start": "751980",
    "end": "758940"
  },
  {
    "text": "one of the logging tools that we use here is the ml flow so using ml flow is again relatively easy with Ray you just",
    "start": "758940",
    "end": "765720"
  },
  {
    "text": "have to basically get the tracking URI which which tells us where exactly are we logging this uh data and call these",
    "start": "765720",
    "end": "774120"
  },
  {
    "text": "three methods basically log metrics params and artifacts so metrics what are the accuracy or validation laws or",
    "start": "774120",
    "end": "781680"
  },
  {
    "text": "training laws etcetera these are metrics basically you want to track over time that changes over every iteration",
    "start": "781680",
    "end": "787579"
  },
  {
    "text": "parameters are something like initial learning rate okay what was it what was the initial learning rate used to",
    "start": "787579",
    "end": "794000"
  },
  {
    "text": "start this trial what was the initial momentum used to start this Optimizer how many epochs were it initialized for",
    "start": "794000",
    "end": "801180"
  },
  {
    "text": "so these are some of the parameters that we could log and artifacts are basically the trained models so okay best trained",
    "start": "801180",
    "end": "807120"
  },
  {
    "text": "models saved somewhere so everything think of them as like a single entry in large interactive table I'll show that",
    "start": "807120",
    "end": "814320"
  },
  {
    "text": "in the demo as well how ml flow works so this allows efficient experiment tracking for these large-scale parameter",
    "start": "814320",
    "end": "822360"
  },
  {
    "text": "research experiments and up to you now the parameter Search",
    "start": "822360",
    "end": "827820"
  },
  {
    "text": "tool that we use here so again integrating option now with the ray pipeline is fairly straightforward so",
    "start": "827820",
    "end": "834300"
  },
  {
    "text": "basically there is a method defined by run optional so the defined by run allows you to allows us to search across",
    "start": "834300",
    "end": "841680"
  },
  {
    "text": "categories like say for example which Optimizer should be used is it like a gradient descent or atom Optimizer or",
    "start": "841680",
    "end": "847800"
  },
  {
    "text": "any any of the types of Optimizer available and also we can search across",
    "start": "847800",
    "end": "855180"
  },
  {
    "text": "a range of uh inputs we say for example what would be the initial learning rate so we can",
    "start": "855180",
    "end": "861720"
  },
  {
    "text": "mention a range so in in line four where we could suggest the the range of each of the trial for initial learning rate",
    "start": "861720",
    "end": "868740"
  },
  {
    "text": "and in line seven we have the learning rate momentum similarly uh suggested by using a range and also it's not just for",
    "start": "868740",
    "end": "876180"
  },
  {
    "text": "continuous ranges this would work for categories as well in line two where we can actually give a bunch of optimizers",
    "start": "876180",
    "end": "882120"
  },
  {
    "text": "and it's going to search basically it's going to do a grid search not exhaustively but more efficiently and",
    "start": "882120",
    "end": "888360"
  },
  {
    "text": "try to find the optimal set of what Optimizer to use what is the ideal",
    "start": "888360",
    "end": "893399"
  },
  {
    "text": "starting learning rate what would be the ideal momentum and everything will show I'll show them in the demo",
    "start": "893399",
    "end": "898740"
  },
  {
    "text": "actually in a more uh remote player when doing the demo and uh yeah that's basically what I",
    "start": "898740",
    "end": "906420"
  },
  {
    "text": "actually mentioned like okay how do we uh how the suggest float and such as category",
    "start": "906420",
    "end": "912839"
  },
  {
    "text": "operations I mean allow us to search across categories or ranges and stuff",
    "start": "912839",
    "end": "917880"
  },
  {
    "text": "so finally uh the demo where",
    "start": "917880",
    "end": "923760"
  },
  {
    "start": "918000",
    "end": "1346000"
  },
  {
    "text": "so basically uh yeah it's a fairly straightforward mnist uh training uh",
    "start": "932399",
    "end": "938760"
  },
  {
    "text": "Ms classifier which we have for demo we just need to run across a few uh",
    "start": "938760",
    "end": "946339"
  },
  {
    "text": "import libraries and stuff and then like this is our convolution new network a very simple quick to train one we",
    "start": "946560",
    "end": "952680"
  },
  {
    "text": "download the data sets here and then basically this is our training pipeline where we lock our metric here so as I",
    "start": "952680",
    "end": "958680"
  },
  {
    "text": "mentioned log metric is one of the ml flow Library I mean like a function call among the three commonly used one metric",
    "start": "958680",
    "end": "965300"
  },
  {
    "text": "log parameters and log artifacts so metric basically models a training law",
    "start": "965300",
    "end": "970800"
  },
  {
    "text": "so in this case this is this training loss varies across each I mean it keeps updating for uh with respect to each",
    "start": "970800",
    "end": "976740"
  },
  {
    "text": "feed forward pass so logging the metric basically is taken care of by the ml flow automatically and uh here so this",
    "start": "976740",
    "end": "984360"
  },
  {
    "text": "is our validation Pipeline and uh and so this is where we actually have",
    "start": "984360",
    "end": "990240"
  },
  {
    "text": "the uh whole uh ml flow pipeline combined with the",
    "start": "990240",
    "end": "996899"
  },
  {
    "text": "network where we initialize the network and then like uh call the ml for ML flow run and then log all the data so",
    "start": "996899",
    "end": "1002660"
  },
  {
    "text": "basically this is a logger and uh the trainer as well so uh this basically is",
    "start": "1002660",
    "end": "1008180"
  },
  {
    "text": "where we combine the parameter search Library I mean we Define the parameter research as I mentioned earlier in a",
    "start": "1008180",
    "end": "1015500"
  },
  {
    "text": "couple of slides ago where we identify where we in this case search across two of the optimizers and a few it ranges",
    "start": "1015500",
    "end": "1023360"
  },
  {
    "text": "for initial learning rates and uh in case of SGD I mean using momentum uh",
    "start": "1023360",
    "end": "1030339"
  },
  {
    "text": "have an initial range average for momentum as well so finally we combine",
    "start": "1030339",
    "end": "1035780"
  },
  {
    "text": "all here so ml flow for logger and then up to enough for parameter search with array Library so as I mentioned uh the",
    "start": "1035780",
    "end": "1043520"
  },
  {
    "text": "simplest ways to call tune dot run which has basically changed to tune dot tuner in 2.0 which was released today so uh",
    "start": "1043520",
    "end": "1052299"
  },
  {
    "text": "in this case yeah we we passed the the we combined the app tuner as well as ml",
    "start": "1052299",
    "end": "1060020"
  },
  {
    "text": "flow and then like uh called the tune dot run so uh I have already run it it takes a a while to actually get this",
    "start": "1060020",
    "end": "1067220"
  },
  {
    "text": "thing so I'm not going to run it again so basically if you could see uh we can it searches across different optimizers",
    "start": "1067220",
    "end": "1074179"
  },
  {
    "text": "and different iterations and then different initial momentum uh different",
    "start": "1074179",
    "end": "1080059"
  },
  {
    "text": "initialization for momentum and here different initialization for learning rates and stuff and it these are the",
    "start": "1080059",
    "end": "1086059"
  },
  {
    "text": "accuracies obtained for these so each of these runs so uh and we could see that",
    "start": "1086059",
    "end": "1092600"
  },
  {
    "text": "in the logger that we have here okay",
    "start": "1092600",
    "end": "1098080"
  },
  {
    "text": "so uh give me one minute",
    "start": "1104200",
    "end": "1109059"
  },
  {
    "text": "okay so while it loads I I can talk about a few other things",
    "start": "1115880",
    "end": "1121760"
  },
  {
    "text": "um so yeah one more problem that we had was that uh storing artifacts so we are generating where we are training we are",
    "start": "1121760",
    "end": "1128120"
  },
  {
    "text": "running a bunch of experiments here like a bunch of Trials where we don't know which artifact we are going to need so",
    "start": "1128120",
    "end": "1133280"
  },
  {
    "text": "if we are storing hundreds of uh checkpoints for each of the trial we if",
    "start": "1133280",
    "end": "1139039"
  },
  {
    "text": "you're running like 200 epochs we'll be dealing with uh thousands and thousands of models which is going to log a lot of",
    "start": "1139039",
    "end": "1144260"
  },
  {
    "text": "space so hug a lot of space in the in the in the cloud so what we did was we actually run all the search I mean like",
    "start": "1144260",
    "end": "1149299"
  },
  {
    "text": "we identify initial set of optimal parameters and just store them as a Json file I mean uh so and then finally we",
    "start": "1149299",
    "end": "1155840"
  },
  {
    "text": "just pick the the the optimal hyper parameter uh combination and rerun the",
    "start": "1155840",
    "end": "1160880"
  },
  {
    "text": "training with that to generate the the efficient I mean the the uh High accurate high high accuracy model",
    "start": "1160880",
    "end": "1169220"
  },
  {
    "text": "basically so that way we don't really save each of the artifact or model or checkpoint uh across all the iterations",
    "start": "1169220",
    "end": "1175700"
  },
  {
    "text": "we just save the combination and then we just log the experiment information and",
    "start": "1175700",
    "end": "1181039"
  },
  {
    "text": "avoid saving the checkpoint and do that at the end that way we save memory and",
    "start": "1181039",
    "end": "1186140"
  },
  {
    "text": "uh yeah coming back to the hammerflow experiments so these are some",
    "start": "1186140",
    "end": "1191419"
  },
  {
    "text": "of the experiments I just ran uh before the presentation and then uh so ml flow is basically so the uh it allows us to",
    "start": "1191419",
    "end": "1198740"
  },
  {
    "text": "compare these experiments uh like uh as we could see here",
    "start": "1198740",
    "end": "1205580"
  },
  {
    "text": "it's a bit slow sorry about that",
    "start": "1205580",
    "end": "1209080"
  },
  {
    "text": "so here are some of the Run details and then like what is a",
    "start": "1210919",
    "end": "1215740"
  },
  {
    "text": "what are the parameters used and what is the training loss achieved for each of them and uh validation loss and F1 score",
    "start": "1216559",
    "end": "1223520"
  },
  {
    "text": "achieved for each of the experiments so it's kind of like organized by the experiment and then the parameters used for the grid search and also it provides",
    "start": "1223520",
    "end": "1230660"
  },
  {
    "text": "interactive graphs so this graph basically means that uh to we can backtrack this from validation F1 score",
    "start": "1230660",
    "end": "1236780"
  },
  {
    "text": "so what achieved the most highest validation is basically the atom Optimizer with the momentum initialized",
    "start": "1236780",
    "end": "1242960"
  },
  {
    "text": "at 0.0.87 and learning rate of 0.0045 probably so we just initialized",
    "start": "1242960",
    "end": "1250400"
  },
  {
    "text": "the when we started the experiment we just gave this bounds basically so 0.001",
    "start": "1250400",
    "end": "1256039"
  },
  {
    "text": "to 0.006 and similarly we just gave a range for momentum and a bunch of a",
    "start": "1256039",
    "end": "1261080"
  },
  {
    "text": "couple of optimizers so this basically is a grid search that it performed and that gave us okay we have now the the",
    "start": "1261080",
    "end": "1268400"
  },
  {
    "text": "highest validation iPhone score and we can backtrack which are the uh which are",
    "start": "1268400",
    "end": "1274220"
  },
  {
    "text": "the parameters or which are the optimizers and exact uh values in the within the range that contributed to",
    "start": "1274220",
    "end": "1280280"
  },
  {
    "text": "this one so it's pretty interactive and uh easy to uh understand and also",
    "start": "1280280",
    "end": "1286520"
  },
  {
    "text": "let me just show the so here is how the optimizer is working",
    "start": "1286520",
    "end": "1293840"
  },
  {
    "text": "so basically you can see the successive halfing or the the hyperband optimization algorithm in bulk so we",
    "start": "1293840",
    "end": "1300080"
  },
  {
    "text": "initialized a bunch of experiments and after a few iterations some of those experiments have been stopped because",
    "start": "1300080",
    "end": "1305240"
  },
  {
    "text": "they didn't do just as good as others and so uh so only the uh so these",
    "start": "1305240",
    "end": "1310580"
  },
  {
    "text": "experiments are stopped basically and then those resources were allocated back to the the the better performing",
    "start": "1310580",
    "end": "1317059"
  },
  {
    "text": "experiments or trials and they can let continue and to at the end we just get",
    "start": "1317059",
    "end": "1322100"
  },
  {
    "text": "that that one most high performing experiment so this is basically the how how the successive halfing or the",
    "start": "1322100",
    "end": "1327740"
  },
  {
    "text": "optimizer works so it doesn't do an exhaustive grid search it instead uh",
    "start": "1327740",
    "end": "1332840"
  },
  {
    "text": "starts with exhaustive grid search and tries to stop trials that are basically",
    "start": "1332840",
    "end": "1338419"
  },
  {
    "text": "not going to give us good results and yeah that's uh",
    "start": "1338419",
    "end": "1345200"
  },
  {
    "text": "that's the that's mostly about the demo and uh yeah it's it's a this is a",
    "start": "1345200",
    "end": "1350419"
  },
  {
    "start": "1346000",
    "end": "1526000"
  },
  {
    "text": "basically our setup where we use Rey with a bunch of other libraries that is supported by Ray and uh",
    "start": "1350419",
    "end": "1357679"
  },
  {
    "text": "train a neural network and achieve a hyper parameter tuning um for that so",
    "start": "1357679",
    "end": "1363500"
  },
  {
    "text": "yeah questions okay sorry go ahead",
    "start": "1363500",
    "end": "1371679"
  },
  {
    "text": "it's Michael so yeah as far as the resource",
    "start": "1371960",
    "end": "1378080"
  },
  {
    "text": "allocation so with it just to understand that that scheduler algorithm correctly so when you are killing off some",
    "start": "1378080",
    "end": "1385460"
  },
  {
    "text": "experiments that aren't doing well do you just basically get more you let's say you have gpus or CPUs you just",
    "start": "1385460",
    "end": "1391640"
  },
  {
    "text": "allocate those to the distributed training on the other experiments or can",
    "start": "1391640",
    "end": "1397039"
  },
  {
    "text": "you actually use those to start up because I'm assuming you have a lot of these uh hyper parameter jobs running",
    "start": "1397039",
    "end": "1403700"
  },
  {
    "text": "concurrently so how do you like manage that so yeah basically I mean in our case what we found out was that it",
    "start": "1403700",
    "end": "1409159"
  },
  {
    "text": "basically assigns new experiments so it it like okay if we have 200 experiments",
    "start": "1409159",
    "end": "1414679"
  },
  {
    "text": "initialized and we have only uh 20 gpus allocated and each experiment takes one GPU so it starts with the 20 experiments",
    "start": "1414679",
    "end": "1421880"
  },
  {
    "text": "and basically stops experiments that are uh that are not performing well and then",
    "start": "1421880",
    "end": "1426919"
  },
  {
    "text": "assigns newer experiments we haven't really seen how how much of a existing experiments resource being",
    "start": "1426919",
    "end": "1434659"
  },
  {
    "text": "expanded and stuff but it basically it allows new experiments to be created and and run new trials to be begin yeah",
    "start": "1434659",
    "end": "1444159"
  },
  {
    "text": "into this and then the other question is kind of just like is there any way to sort of like optimize this so that you can actually",
    "start": "1444320",
    "end": "1451280"
  },
  {
    "text": "make it more efficient not just across a single hyper parameter tuning search but",
    "start": "1451280",
    "end": "1456440"
  },
  {
    "text": "over like you have multiple models that already using the same resources probably in kubernetes or whatever you're running sure at this point we are",
    "start": "1456440",
    "end": "1463760"
  },
  {
    "text": "uh just to retrace the question so basically assassin can we just uh your question is uh can we just uh uh",
    "start": "1463760",
    "end": "1471159"
  },
  {
    "text": "optimize it for uh in a way that okay multiple such experiments or like parameter searches can be happening at",
    "start": "1471159",
    "end": "1477140"
  },
  {
    "text": "the same time and uh can this be can this setup be extended for that is that your question right",
    "start": "1477140",
    "end": "1482780"
  },
  {
    "text": "yeah so uh that actually is uh in a way we are controlling that manually so far",
    "start": "1482780",
    "end": "1488659"
  },
  {
    "text": "so initially when we have the resources we are we're using uh we'll be allocating okay hey depth prediction",
    "start": "1488659",
    "end": "1493880"
  },
  {
    "text": "takes a third of the resources and then we kind of like upper bound it I mean like in a way and so far we haven't",
    "start": "1493880",
    "end": "1499039"
  },
  {
    "text": "really tried around uh playing with the uh automating that part and I don't think I'm not sure whether",
    "start": "1499039",
    "end": "1505159"
  },
  {
    "text": "Rey supports that but at this point we are doing it one at a time like we we initialize experiments a number of",
    "start": "1505159",
    "end": "1510799"
  },
  {
    "text": "Trials and then allocate some resources and uh the resources are efficiently used within that particular bunch of",
    "start": "1510799",
    "end": "1516559"
  },
  {
    "text": "experiments but we haven't really tried across multiple Network optimizing multiple networks at the same",
    "start": "1516559",
    "end": "1522740"
  },
  {
    "text": "time so yeah I hope that answers your question thank you",
    "start": "1522740",
    "end": "1528080"
  },
  {
    "text": "if you have a question",
    "start": "1528080",
    "end": "1530919"
  },
  {
    "text": "my question is do you have to reserve resources for other tasks rather than a",
    "start": "1535220",
    "end": "1540500"
  },
  {
    "text": "10. run because I've seen I've used this before but I see um there are a lot of",
    "start": "1540500",
    "end": "1547100"
  },
  {
    "text": "times that the the trials will be pending like all the time so I'm I'm not sure if if this is a resource leaking",
    "start": "1547100",
    "end": "1553760"
  },
  {
    "text": "thing or it's just you have to set some buffers for other tasks when you integrate with other packages like MF",
    "start": "1553760",
    "end": "1559880"
  },
  {
    "text": "flow so at this point uh we kind of like okay",
    "start": "1559880",
    "end": "1565220"
  },
  {
    "text": "when we run an expert say for example we have a segmentation Network and we are trying to run a parameter search for",
    "start": "1565220",
    "end": "1570860"
  },
  {
    "text": "that we kind of set the upper bound on available resources for now and then we",
    "start": "1570860",
    "end": "1575900"
  },
  {
    "text": "run just that uh parameters I mean like search experiments and then within that we often have it automatically queues we",
    "start": "1575900",
    "end": "1583640"
  },
  {
    "text": "often have a much larger number of X trials that we want to run but lesser resource in a way okay 20 gpus available",
    "start": "1583640",
    "end": "1589640"
  },
  {
    "text": "whereas 20 200 experiments to be run so it tries it starts with 20 and then like okay keep stopping experiments and",
    "start": "1589640",
    "end": "1596080"
  },
  {
    "text": "respawning new ones and basically at the end it identifies some of the best performing experiments so we haven't",
    "start": "1596080",
    "end": "1602779"
  },
  {
    "text": "really worked on ah queuing anything manually or we haven't really focused on",
    "start": "1602779",
    "end": "1608779"
  },
  {
    "text": "uh I hope that answers your question or like yeah for example for this example",
    "start": "1608779",
    "end": "1614179"
  },
  {
    "text": "how large is this great cluster for this uh notebook environment",
    "start": "1614179",
    "end": "1619880"
  },
  {
    "text": "how large is what like how much CPUs and gpus for this Ray cluster so in this case we just have one CPU and one GPU",
    "start": "1619880",
    "end": "1626539"
  },
  {
    "text": "which is for demo actually yeah okay um so if you are running 200 experiments so basically one CPU and one GPU are",
    "start": "1626539",
    "end": "1632659"
  },
  {
    "text": "utilized in full if you use half a GPU and one one CPU so one experiment is hogging the GPU and then not only half",
    "start": "1632659",
    "end": "1640100"
  },
  {
    "text": "of the GPU is being utilized so it's kind of like yeah we initialize it in a way that okay uh each exp I mean we can",
    "start": "1640100",
    "end": "1645500"
  },
  {
    "text": "assign them for 20 gpus and 20 CPUs where each experiment can take one CPU and one GPU core and run and then if you",
    "start": "1645500",
    "end": "1651980"
  },
  {
    "text": "have 200 experiments after the first few experiments are stopped automatically Ray uh passes on the next full set of",
    "start": "1651980",
    "end": "1657980"
  },
  {
    "text": "experiments that keeps running basically so yeah I see kind of thanks",
    "start": "1657980",
    "end": "1663700"
  },
  {
    "text": "your question where I think okay there are two questions uh",
    "start": "1665059",
    "end": "1671360"
  },
  {
    "text": "here you who first okay who was it sorry about that one minute yeah",
    "start": "1671360",
    "end": "1676940"
  },
  {
    "text": "it's gonna be a quick one uh how do you set the ranges for that you search over",
    "start": "1676940",
    "end": "1682340"
  },
  {
    "text": "for your algorithms and different optimizers so uh yeah we actually it is",
    "start": "1682340",
    "end": "1688700"
  },
  {
    "text": "basically in a way hyper hyper parameter I would say we just uh start because we we",
    "start": "1688700",
    "end": "1696260"
  },
  {
    "text": "initiate okay in our case we play around with a few experiments before running the parameter search so we have some",
    "start": "1696260",
    "end": "1701779"
  },
  {
    "text": "some range in mind at least to start with some okay we have some baseline now the range could be around the Baseline",
    "start": "1701779",
    "end": "1708020"
  },
  {
    "text": "is what we assume and then okay if the learning rate is 0.0005 now we can actually start from zero zero one two",
    "start": "1708020",
    "end": "1714440"
  },
  {
    "text": "zero zero zero uh uh one or something like that so that's how we actually initialize the learning rate it is more",
    "start": "1714440",
    "end": "1721039"
  },
  {
    "text": "of like at this point intuitive we don't really know uh I mean it's there is a bunch of limitation to how much we could",
    "start": "1721039",
    "end": "1726919"
  },
  {
    "text": "search right because the it could be uh we want to limit the number of experiments or trials as well so yeah",
    "start": "1726919",
    "end": "1733100"
  },
  {
    "text": "that's how we we initialize it so we initialize it around the Baseline that we estimated before the experiment",
    "start": "1733100",
    "end": "1738140"
  },
  {
    "text": "actually that answers oh that answers your question and then one last question who had their",
    "start": "1738140",
    "end": "1745100"
  },
  {
    "text": "hand okay uh he has a question yeah I will come back to you yeah yeah so how",
    "start": "1745100",
    "end": "1751039"
  },
  {
    "text": "does your actual training which you do in the company uh look different from like just running a python notebook and",
    "start": "1751039",
    "end": "1758059"
  },
  {
    "text": "in terms of like the output of one workflow goes to another and how do sharing of phrase resources happen is",
    "start": "1758059",
    "end": "1764539"
  },
  {
    "text": "there sure um um in our case as I mentioned we have a",
    "start": "1764539",
    "end": "1770000"
  },
  {
    "text": "basically Lane detection and object detection networks so though at inference time we have a workflow where",
    "start": "1770000",
    "end": "1777200"
  },
  {
    "text": "one Network's output is fed to the other uh and why so I mean and so on in a more sequential manner at training Time Each",
    "start": "1777200",
    "end": "1783980"
  },
  {
    "text": "network has defined uh I mean like a loss and accuracy Matrix that we need to achieve so at training time we do not",
    "start": "1783980",
    "end": "1790340"
  },
  {
    "text": "really uh for I mean like a kind of Leverage The the I mean are use the",
    "start": "1790340",
    "end": "1795740"
  },
  {
    "text": "dependence uh training time basically we have said okay say for example if it is a lane detection algorithm which would",
    "start": "1795740",
    "end": "1801080"
  },
  {
    "text": "be used for object tracking or like a object model interaction modeling but at training time we have a hey this is",
    "start": "1801080",
    "end": "1806899"
  },
  {
    "text": "Elaine here's a segmentation here's a classification and we have to achieve F1 score of this and training and",
    "start": "1806899",
    "end": "1812659"
  },
  {
    "text": "validation accuracy of this this and stuff so that's basically a requirement so at training time for hyper parameter",
    "start": "1812659",
    "end": "1818000"
  },
  {
    "text": "tuning uh or like the parameter search we do not really focus on or we do not need to focus on the the sequence or the",
    "start": "1818000",
    "end": "1824960"
  },
  {
    "text": "how the the this network works with the other or within the pipeline it's kind of like a set we have a set of",
    "start": "1824960",
    "end": "1832399"
  },
  {
    "text": "requirements that we need to achieve for this particular Network and then we'll just try to achieve that at inference",
    "start": "1832399",
    "end": "1838039"
  },
  {
    "text": "time basically we just wrap things around so once each network is efficiently working or like optimized",
    "start": "1838039",
    "end": "1843559"
  },
  {
    "text": "well then we assume that the whole inference pipeline is optimized that's how we work at this point yeah",
    "start": "1843559",
    "end": "1849440"
  },
  {
    "text": "so yeah these are great questions um we have lunch now so if you have further questions feel free to come talk to Arun",
    "start": "1849440",
    "end": "1854960"
  },
  {
    "text": "um thank you all for attending thank you",
    "start": "1854960",
    "end": "1858940"
  }
]