[
  {
    "text": "so model serving is being able to uh you",
    "start": "3419",
    "end": "8280"
  },
  {
    "text": "know send your models to uh you know not",
    "start": "8280",
    "end": "11099"
  },
  {
    "text": "not just training your models but",
    "start": "11099",
    "end": "12840"
  },
  {
    "text": "sending them off to the uh cluster to be",
    "start": "12840",
    "end": "15299"
  },
  {
    "text": "able to use for uh you know production",
    "start": "15299",
    "end": "18180"
  },
  {
    "text": "for clients and stuff and K server",
    "start": "18180",
    "end": "21000"
  },
  {
    "text": "allows you to serve your models it's uh",
    "start": "21000",
    "end": "23939"
  },
  {
    "text": "you know runs on kubernetes so it's",
    "start": "23939",
    "end": "25500"
  },
  {
    "text": "platform independent doesn't lock you",
    "start": "25500",
    "end": "27539"
  },
  {
    "text": "into any particular customer",
    "start": "27539",
    "end": "30119"
  },
  {
    "text": "um it has standardized protocols it's",
    "start": "30119",
    "end": "32279"
  },
  {
    "text": "serverless it has pluggable production",
    "start": "32279",
    "end": "34680"
  },
  {
    "text": "serving it does Advanced deployments",
    "start": "34680",
    "end": "38579"
  },
  {
    "text": "um",
    "start": "38579",
    "end": "39600"
  },
  {
    "text": "and uh it supports both model serving",
    "start": "39600",
    "end": "42960"
  },
  {
    "text": "and model mesh so model serving it with",
    "start": "42960",
    "end": "46320"
  },
  {
    "text": "case Serve by default it just does one",
    "start": "46320",
    "end": "48239"
  },
  {
    "text": "model per pod but if you use the model",
    "start": "48239",
    "end": "51239"
  },
  {
    "text": "mesh it allows you to do multiple models",
    "start": "51239",
    "end": "53520"
  },
  {
    "text": "in one pod so it allows you to do very",
    "start": "53520",
    "end": "55980"
  },
  {
    "text": "high scalability",
    "start": "55980",
    "end": "58500"
  },
  {
    "text": "um case serve it supports multiple",
    "start": "58500",
    "end": "61320"
  },
  {
    "text": "runtime models uh Triton torch serve ml",
    "start": "61320",
    "end": "65518"
  },
  {
    "text": "serving TF serving and model serve and",
    "start": "65519",
    "end": "68700"
  },
  {
    "text": "then there's different ways you can",
    "start": "68700",
    "end": "69659"
  },
  {
    "text": "communicate with those runtimes you can",
    "start": "69659",
    "end": "72000"
  },
  {
    "text": "use the the most easy way which is crds",
    "start": "72000",
    "end": "74880"
  },
  {
    "text": "you can also use HTTP and rest method",
    "start": "74880",
    "end": "77700"
  },
  {
    "text": "and then there's the Google RPC or grpc",
    "start": "77700",
    "end": "80700"
  },
  {
    "text": "now I'm going to hand it off to Ted to",
    "start": "80700",
    "end": "82920"
  },
  {
    "text": "do uh racer",
    "start": "82920",
    "end": "86420"
  },
  {
    "text": "first time",
    "start": "87619",
    "end": "90799"
  },
  {
    "text": "yeah thanks Jim",
    "start": "93560",
    "end": "95939"
  },
  {
    "text": "um",
    "start": "95939",
    "end": "96540"
  },
  {
    "text": "now we have some basic understanding of",
    "start": "96540",
    "end": "99439"
  },
  {
    "text": "uh model serving with K Surf and",
    "start": "99439",
    "end": "102299"
  },
  {
    "text": "kubernetes now let's look at",
    "start": "102299",
    "end": "105180"
  },
  {
    "text": "what is racer and how to integrate",
    "start": "105180",
    "end": "107460"
  },
  {
    "text": "cancer with racer for parallel",
    "start": "107460",
    "end": "109799"
  },
  {
    "text": "inferencing",
    "start": "109799",
    "end": "111899"
  },
  {
    "text": "um",
    "start": "111899",
    "end": "112619"
  },
  {
    "text": "so let's understand from very high level",
    "start": "112619",
    "end": "115079"
  },
  {
    "text": "what Reserve is so it Reserve is",
    "start": "115079",
    "end": "118380"
  },
  {
    "text": "scalable and programmable serving",
    "start": "118380",
    "end": "120960"
  },
  {
    "text": "framework built on top of Ray the ray",
    "start": "120960",
    "end": "123299"
  },
  {
    "text": "serve is framework agnostic that means",
    "start": "123299",
    "end": "126240"
  },
  {
    "text": "you can program rate to serve deep",
    "start": "126240",
    "end": "128280"
  },
  {
    "text": "learning model build with any framework",
    "start": "128280",
    "end": "131540"
  },
  {
    "text": "such as Python tensorflow and SK",
    "start": "131540",
    "end": "134280"
  },
  {
    "text": "learning Reserve also allows you to",
    "start": "134280",
    "end": "136980"
  },
  {
    "text": "build a complex inference service",
    "start": "136980",
    "end": "139260"
  },
  {
    "text": "uh that's composed of multiple models",
    "start": "139260",
    "end": "142980"
  },
  {
    "text": "and business logic or in Python code",
    "start": "142980",
    "end": "146879"
  },
  {
    "text": "um so let's look at the k-serv",
    "start": "146879",
    "end": "148379"
  },
  {
    "text": "architecture uh Reserve is built on top",
    "start": "148379",
    "end": "151140"
  },
  {
    "text": "of Ray so what you're looking at is a",
    "start": "151140",
    "end": "153360"
  },
  {
    "text": "ray cluster with one hat node and a few",
    "start": "153360",
    "end": "156239"
  },
  {
    "text": "worker nodes so race surf is just an",
    "start": "156239",
    "end": "158819"
  },
  {
    "text": "instance of a rate cluster running",
    "start": "158819",
    "end": "160500"
  },
  {
    "text": "additional rate actors uh so for example",
    "start": "160500",
    "end": "165060"
  },
  {
    "text": "the HD proxy actor accept incoming",
    "start": "165060",
    "end": "168720"
  },
  {
    "text": "requests and forward them to the",
    "start": "168720",
    "end": "171060"
  },
  {
    "text": "replicas and then each replica will",
    "start": "171060",
    "end": "173640"
  },
  {
    "text": "process an individual requests and then",
    "start": "173640",
    "end": "177000"
  },
  {
    "text": "once complete it returned the result and",
    "start": "177000",
    "end": "181379"
  },
  {
    "text": "the controller actor is actually",
    "start": "181379",
    "end": "184019"
  },
  {
    "text": "responsible for managing the other",
    "start": "184019",
    "end": "186720"
  },
  {
    "text": "actors",
    "start": "186720",
    "end": "189260"
  },
  {
    "text": "um so let's look like how does the",
    "start": "191519",
    "end": "194519"
  },
  {
    "text": "parallel inferencing work with k-serv",
    "start": "194519",
    "end": "197099"
  },
  {
    "text": "and Ray serve so by default when you",
    "start": "197099",
    "end": "199140"
  },
  {
    "text": "launch a case of custom model serving",
    "start": "199140",
    "end": "201480"
  },
  {
    "text": "runtime",
    "start": "201480",
    "end": "202400"
  },
  {
    "text": "it would load the model and run the",
    "start": "202400",
    "end": "204959"
  },
  {
    "text": "prediction",
    "start": "204959",
    "end": "205920"
  },
  {
    "text": "uh in the same process as the HTTP",
    "start": "205920",
    "end": "210120"
  },
  {
    "text": "server and also the inference happened",
    "start": "210120",
    "end": "212700"
  },
  {
    "text": "inside the same process as well",
    "start": "212700",
    "end": "215220"
  },
  {
    "text": "and so that's why we need to enable",
    "start": "215220",
    "end": "218040"
  },
  {
    "text": "Reserve so when you enable the racer the",
    "start": "218040",
    "end": "221280"
  },
  {
    "text": "k-serve would actually launch a Razer",
    "start": "221280",
    "end": "223980"
  },
  {
    "text": "instance so with this separation",
    "start": "223980",
    "end": "228500"
  },
  {
    "text": "the models are deployed into the reserve",
    "start": "228840",
    "end": "231599"
  },
  {
    "text": "as multiple replicas and then so the",
    "start": "231599",
    "end": "235440"
  },
  {
    "text": "parallel inference can be performed uh",
    "start": "235440",
    "end": "238560"
  },
  {
    "text": "when the serving body and when multiple",
    "start": "238560",
    "end": "241799"
  },
  {
    "text": "requests when serving multiple requests",
    "start": "241799",
    "end": "245220"
  },
  {
    "text": "so let's look at uh",
    "start": "245220",
    "end": "250500"
  },
  {
    "text": "let's look at some code uh code snippet",
    "start": "250500",
    "end": "253920"
  },
  {
    "text": "how do we enable Reserve with k-serv so",
    "start": "253920",
    "end": "260600"
  },
  {
    "text": "this is a code snippet created for",
    "start": "260600",
    "end": "264060"
  },
  {
    "text": "k-serv custom model semi runtime",
    "start": "264060",
    "end": "266660"
  },
  {
    "text": "for case serve right usually you just",
    "start": "266660",
    "end": "269400"
  },
  {
    "text": "need to implement two cases model",
    "start": "269400",
    "end": "271979"
  },
  {
    "text": "handlers for example the load function",
    "start": "271979",
    "end": "275580"
  },
  {
    "text": "you would implement low function to load",
    "start": "275580",
    "end": "277860"
  },
  {
    "text": "a model from somewhere into the memory",
    "start": "277860",
    "end": "280139"
  },
  {
    "text": "and then you also need to implement a",
    "start": "280139",
    "end": "282419"
  },
  {
    "text": "predict this predict would process your",
    "start": "282419",
    "end": "285960"
  },
  {
    "text": "logic and then return the prediction",
    "start": "285960",
    "end": "287880"
  },
  {
    "text": "result",
    "start": "287880",
    "end": "288900"
  },
  {
    "text": "and this is how you write a basic custom",
    "start": "288900",
    "end": "292919"
  },
  {
    "text": "model semi runtime in case of and I have",
    "start": "292919",
    "end": "296100"
  },
  {
    "text": "a QR code which you can download the",
    "start": "296100",
    "end": "297720"
  },
  {
    "text": "full version of this code and you can",
    "start": "297720",
    "end": "299699"
  },
  {
    "text": "run this as a just any regular python",
    "start": "299699",
    "end": "302880"
  },
  {
    "text": "service and you can also send requests",
    "start": "302880",
    "end": "305240"
  },
  {
    "text": "uh to this custom model saving runtime",
    "start": "305240",
    "end": "308460"
  },
  {
    "text": "and get some result back and now let's",
    "start": "308460",
    "end": "311460"
  },
  {
    "text": "look at how we enable racer uh with",
    "start": "311460",
    "end": "315000"
  },
  {
    "text": "k-serv so basically there is some simple",
    "start": "315000",
    "end": "317220"
  },
  {
    "text": "code pattern which you can follow uh",
    "start": "317220",
    "end": "320160"
  },
  {
    "text": "three steps right so first you need to",
    "start": "320160",
    "end": "322139"
  },
  {
    "text": "enable uh",
    "start": "322139",
    "end": "324240"
  },
  {
    "text": "Ray",
    "start": "324240",
    "end": "325560"
  },
  {
    "text": "and then second you need you need to",
    "start": "325560",
    "end": "328080"
  },
  {
    "text": "import rate and then rate it Reserve",
    "start": "328080",
    "end": "330780"
  },
  {
    "text": "right so second you need to decorate uh",
    "start": "330780",
    "end": "334380"
  },
  {
    "text": "your custom model with the as a reserve",
    "start": "334380",
    "end": "337800"
  },
  {
    "text": "deployment notice here you can also",
    "start": "337800",
    "end": "340380"
  },
  {
    "text": "specify the number of replicas right the",
    "start": "340380",
    "end": "343080"
  },
  {
    "text": "more replicas the more concurrent uh",
    "start": "343080",
    "end": "345600"
  },
  {
    "text": "requests they were able to process",
    "start": "345600",
    "end": "349199"
  },
  {
    "text": "and third we would need to start start",
    "start": "349199",
    "end": "352020"
  },
  {
    "text": "the model server a little different with",
    "start": "352020",
    "end": "354300"
  },
  {
    "text": "a different parameter basically using a",
    "start": "354300",
    "end": "357479"
  },
  {
    "text": "dictionary to start this so I also have",
    "start": "357479",
    "end": "360900"
  },
  {
    "text": "a full version of this code you can scan",
    "start": "360900",
    "end": "363600"
  },
  {
    "text": "the QR code and get a full version of it",
    "start": "363600",
    "end": "365280"
  },
  {
    "text": "and run it and run a python run as a",
    "start": "365280",
    "end": "367860"
  },
  {
    "text": "python process process and you can",
    "start": "367860",
    "end": "371820"
  },
  {
    "text": "experiment the concurrency uh you know",
    "start": "371820",
    "end": "375300"
  },
  {
    "text": "you can send multiple requests at once",
    "start": "375300",
    "end": "377580"
  },
  {
    "text": "and experience and experiment with the",
    "start": "377580",
    "end": "380100"
  },
  {
    "text": "difference between the two",
    "start": "380100",
    "end": "383120"
  },
  {
    "text": "thank you",
    "start": "383699",
    "end": "385080"
  },
  {
    "text": "now uh when we talk about caser uh",
    "start": "385080",
    "end": "390660"
  },
  {
    "text": "we also need to talk about a case of",
    "start": "390660",
    "end": "392639"
  },
  {
    "text": "model match right case of model match is",
    "start": "392639",
    "end": "394800"
  },
  {
    "text": "uh",
    "start": "394800",
    "end": "395940"
  },
  {
    "text": "another you know is another component in",
    "start": "395940",
    "end": "398880"
  },
  {
    "text": "in case serve which is designed for high",
    "start": "398880",
    "end": "401819"
  },
  {
    "text": "scale high density and frequent changing",
    "start": "401819",
    "end": "404819"
  },
  {
    "text": "model scenario",
    "start": "404819",
    "end": "406319"
  },
  {
    "text": "so it it has its own intelligent uh cash",
    "start": "406319",
    "end": "411199"
  },
  {
    "text": "that loads and unload model",
    "start": "411199",
    "end": "414500"
  },
  {
    "text": "uh so you could actually serve hundreds",
    "start": "414500",
    "end": "417720"
  },
  {
    "text": "or thousands of models in a very small",
    "start": "417720",
    "end": "419880"
  },
  {
    "text": "cluster",
    "start": "419880",
    "end": "422880"
  },
  {
    "text": "and",
    "start": "422880",
    "end": "424560"
  },
  {
    "text": "we actually we actually have a case of",
    "start": "424560",
    "end": "428160"
  },
  {
    "text": "model mesh",
    "start": "428160",
    "end": "430220"
  },
  {
    "text": "included as one of the open source",
    "start": "430220",
    "end": "433500"
  },
  {
    "text": "Technologies in openshift Ai and",
    "start": "433500",
    "end": "437819"
  },
  {
    "text": "also IBS next-gen uh AI platform is",
    "start": "437819",
    "end": "441900"
  },
  {
    "text": "built on top of the openshift AI and we",
    "start": "441900",
    "end": "445139"
  },
  {
    "text": "have a few talks tomorrow regarding",
    "start": "445139",
    "end": "447180"
  },
  {
    "text": "openshift Ai and some of those",
    "start": "447180",
    "end": "449460"
  },
  {
    "text": "technology stacked in the chart just in",
    "start": "449460",
    "end": "452759"
  },
  {
    "text": "case you you are interested",
    "start": "452759",
    "end": "456199"
  },
  {
    "text": "um so yeah that's it for our talk thank",
    "start": "456240",
    "end": "459060"
  },
  {
    "text": "you very much",
    "start": "459060",
    "end": "461419"
  }
]