[
  {
    "start": "0",
    "end": "71000"
  },
  {
    "text": "[Applause] hello everyone thank you and welcome to the talk second to last Talk of the",
    "start": "2720",
    "end": "10679"
  },
  {
    "text": "summit um today wewe and I are going to talk about elastic GPU management for Ray achieving scalability and efficiency",
    "start": "10679",
    "end": "18960"
  },
  {
    "text": "forl workloads um so first the introduction",
    "start": "18960",
    "end": "24519"
  },
  {
    "text": "slides um I have worked on or with the ray community at app and before that at",
    "start": "24519",
    "end": "31439"
  },
  {
    "text": "LinkedIn um and then we do you want to talk about yourself a little bit yeah hi everyone my name is we way and I'm",
    "start": "31439",
    "end": "37960"
  },
  {
    "text": "working the data and a infr infrastructure in in apple and basically help the team to build the uh",
    "start": "37960",
    "end": "44399"
  },
  {
    "text": "infrastructure for running both data processing and the Machine learning workloads on on multiple",
    "start": "44399",
    "end": "50760"
  },
  {
    "text": "clouds so today we will give you a little bit of an overview of Ray uh",
    "start": "53079",
    "end": "58840"
  },
  {
    "text": "we'll discuss the scaling challenges with GPU workloads and some of the",
    "start": "58840",
    "end": "65040"
  },
  {
    "text": "techniques that we have tried uh for optimizing GPU capacity management and",
    "start": "65040",
    "end": "71479"
  },
  {
    "start": "71000",
    "end": "110000"
  },
  {
    "text": "utilization so why why use Ray Ray provides a unified Computing API for",
    "start": "71680",
    "end": "78200"
  },
  {
    "text": "data processing um training tuning serving and uh for us uh one of its um",
    "start": "78200",
    "end": "87360"
  },
  {
    "text": "kind of positives is that it provides this kind of seamless API that works on",
    "start": "87360",
    "end": "93240"
  },
  {
    "text": "user's laptop all the way to the data center um users can kind of try out",
    "start": "93240",
    "end": "98399"
  },
  {
    "text": "small small experiments that they can then uh migrate to the Clusters without many",
    "start": "98399",
    "end": "104719"
  },
  {
    "text": "changes um it has great support for model serving um including",
    "start": "104719",
    "end": "110680"
  },
  {
    "start": "110000",
    "end": "156000"
  },
  {
    "text": "VM um users use our R platform in three different ways um notebooks um are used",
    "start": "110680",
    "end": "118840"
  },
  {
    "text": "for early iteration um this is where they can try out early experiments uh they can try out as if",
    "start": "118840",
    "end": "124960"
  },
  {
    "text": "they're on their laptop it's in interactive mode um and then they can use the same apis for a full scale large",
    "start": "124960",
    "end": "132760"
  },
  {
    "text": "scale workload in the data center um and then in addition to",
    "start": "132760",
    "end": "137959"
  },
  {
    "text": "notebooks users always demand like mature sdks for programmatic job submission and model deployment um and",
    "start": "137959",
    "end": "145239"
  },
  {
    "text": "then um there's portals to support ux based model deployment and also for",
    "start": "145239",
    "end": "151000"
  },
  {
    "text": "observability which is like a very important part in any productionize",
    "start": "151000",
    "end": "156680"
  },
  {
    "start": "156000",
    "end": "206000"
  },
  {
    "text": "workloads while Ray has many features there are also quite a few challenges",
    "start": "156680",
    "end": "161840"
  },
  {
    "text": "using Ray one is resource sizing so there's too many options when it comes to the",
    "start": "161840",
    "end": "169400"
  },
  {
    "text": "kind of resource the users can request and that is not at the top of the ml pra",
    "start": "169400",
    "end": "175760"
  },
  {
    "text": "practitioner mind when they're trying to execute a batch INF job or deploying a",
    "start": "175760",
    "end": "181360"
  },
  {
    "text": "model um autoscaling kind of attempts to help it but then also kind of brings its",
    "start": "181360",
    "end": "187319"
  },
  {
    "text": "own challenges that we have seen um and then lastly GPU utilization is a is a",
    "start": "187319",
    "end": "194120"
  },
  {
    "text": "problem that both the ml practitioner and the infra Engineers try to solve",
    "start": "194120",
    "end": "200440"
  },
  {
    "text": "because of the high demand for gpus and the and the the cost",
    "start": "200440",
    "end": "206360"
  },
  {
    "start": "206000",
    "end": "266000"
  },
  {
    "text": "involved so going deeper into this with the with the different infr resource",
    "start": "206360",
    "end": "212080"
  },
  {
    "text": "choices that users have so they they have to choose the kind of the in nodes",
    "start": "212080",
    "end": "219760"
  },
  {
    "text": "or machines based on the number of cards on the nodes the GPU memory they have",
    "start": "219760",
    "end": "226400"
  },
  {
    "text": "the um the SRAM versus Dam uh the intra GPU and Inter GPU connectivity um the",
    "start": "226400",
    "end": "234599"
  },
  {
    "text": "network congestion in a particular geographic region they're choosing um",
    "start": "234599",
    "end": "239879"
  },
  {
    "text": "the Region's overall capacity and congestion um a lot of choices um that",
    "start": "239879",
    "end": "245840"
  },
  {
    "text": "that are too hard to specify uh for a for a human being while they're",
    "start": "245840",
    "end": "251640"
  },
  {
    "text": "launching a fairly large workload or uh or productionizing a model deployment so",
    "start": "251640",
    "end": "260000"
  },
  {
    "text": "our observation has there been there has been that kind of pairing down the choices uh and providing some kind of",
    "start": "260000",
    "end": "267759"
  },
  {
    "start": "266000",
    "end": "301000"
  },
  {
    "text": "standard uh standard computer options um are helpful so think like small medium",
    "start": "267759",
    "end": "273800"
  },
  {
    "text": "large um and kind of provide abstractions that users can kind of talk to and then they can choose that okay I",
    "start": "273800",
    "end": "281320"
  },
  {
    "text": "I need kind of this and uh the our platform then provides uh Ray the the",
    "start": "281320",
    "end": "289479"
  },
  {
    "text": "particular kind know resource configuration based on the choices that the users make uh while this doesn't optimize for",
    "start": "289479",
    "end": "297240"
  },
  {
    "text": "every case it sort of covers 80% of our cases um one intermediate step that we",
    "start": "297240",
    "end": "304479"
  },
  {
    "text": "took on the way to kind of figuring out how to optimize GPU cases and how to",
    "start": "304479",
    "end": "309639"
  },
  {
    "text": "allow users to kind of provide the correct kind of nodes or machines they need is to kind of come up with no",
    "start": "309639",
    "end": "317320"
  },
  {
    "text": "specific cues and no specific clusters um and that didn't work out well so it",
    "start": "317320",
    "end": "324240"
  },
  {
    "text": "kind of resulted in while the users so we could kind of give them these choices and um it ended up with um kind of even",
    "start": "324240",
    "end": "333160"
  },
  {
    "text": "more fragmentation and lower utilization that we had uh before that so then we",
    "start": "333160",
    "end": "340639"
  },
  {
    "text": "sort of uh backed out of that and then um worked on multi-level scheduling so",
    "start": "340639",
    "end": "347919"
  },
  {
    "text": "where the node level scheduling could handle multiple types of nodes and instances and kind of can fall back on",
    "start": "347919",
    "end": "356240"
  },
  {
    "text": "different uh uh options when the primary the user primary asks are not",
    "start": "356240",
    "end": "363319"
  },
  {
    "start": "364000",
    "end": "412000"
  },
  {
    "text": "available another important part of optimizing the GP utilization is to support autoscaling because the resource",
    "start": "364800",
    "end": "372720"
  },
  {
    "text": "utilization of large workloads both for batch and for real time doesn't um stay",
    "start": "372720",
    "end": "380000"
  },
  {
    "text": "static it varies with the life Lifetime with the of the workload um we heard in",
    "start": "380000",
    "end": "385479"
  },
  {
    "text": "earlier in the summit um Serge from meta talking about how llama needed to grow",
    "start": "385479",
    "end": "391919"
  },
  {
    "text": "its bat size during the training um so we've observed similar things and uh",
    "start": "391919",
    "end": "398080"
  },
  {
    "text": "fortunately Ray has native support for autoscaling both for uh batch and for",
    "start": "398080",
    "end": "403759"
  },
  {
    "text": "real-time workloads um raise the way ra raise autoscaler uh works is",
    "start": "403759",
    "end": "412680"
  },
  {
    "start": "412000",
    "end": "456000"
  },
  {
    "text": "um wait so the way raise autoscaler works is like it um kind of watches the",
    "start": "412919",
    "end": "419599"
  },
  {
    "text": "metrics uh of the workers and and patches the the the crds based on that",
    "start": "419599",
    "end": "426720"
  },
  {
    "text": "um and then we have this we use um U Apache",
    "start": "426720",
    "end": "432400"
  },
  {
    "text": "unicorn which watches the uh the modifications that the raise autoscaler",
    "start": "432400",
    "end": "437759"
  },
  {
    "text": "makes and um then cues up pods and workloads based on the additional needs",
    "start": "437759",
    "end": "445440"
  },
  {
    "text": "of the of the autoscaler um and then it does the same on the downscaling side and tries to",
    "start": "445440",
    "end": "451800"
  },
  {
    "text": "reconcile and Bin pack the workloads",
    "start": "451800",
    "end": "456360"
  },
  {
    "start": "456000",
    "end": "626000"
  },
  {
    "text": "together um and then but with successful autoscaling um there's some challenges",
    "start": "457479",
    "end": "464159"
  },
  {
    "text": "that come with it um so first is um for very large workloads",
    "start": "464159",
    "end": "472639"
  },
  {
    "text": "uh we want to like we want to make a choice whether we want to get limited by",
    "start": "472639",
    "end": "478000"
  },
  {
    "text": "geographical regions or do we want to go across geographical regions but then if we want to go across regions then",
    "start": "478000",
    "end": "484319"
  },
  {
    "text": "there's um data transfer cost to um kind of consider um and then the next problem",
    "start": "484319",
    "end": "491159"
  },
  {
    "text": "that occurs is like even with the single region uh workloads can be okay with",
    "start": "491159",
    "end": "496960"
  },
  {
    "text": "upscaling um scaling up but many workloads are not okay with downscaling um so this is what we saw in",
    "start": "496960",
    "end": "504280"
  },
  {
    "text": "a lot of our kind of uh stateful workloads where downscaling cost",
    "start": "504280",
    "end": "510120"
  },
  {
    "text": "failur um so for so guard against that",
    "start": "510120",
    "end": "515880"
  },
  {
    "text": "was to kind of make sure that okay uh workloads our Ray Ray workers cannot be",
    "start": "515880",
    "end": "522240"
  },
  {
    "text": "evicted so they can be upscaled but they cannot be downscaled now that resulted",
    "start": "522240",
    "end": "527360"
  },
  {
    "text": "in lower utilization because now we have kind of created this um uh workload that",
    "start": "527360",
    "end": "533800"
  },
  {
    "text": "cannot be shrunk um so then with that to kind of",
    "start": "533800",
    "end": "539720"
  },
  {
    "text": "try to resolve that was to kind of focus more on checkpointing um um but there to support",
    "start": "539720",
    "end": "546720"
  },
  {
    "text": "better checkpointing we had to introduce better apis from a platform that users",
    "start": "546720",
    "end": "551839"
  },
  {
    "text": "could easily use to kind of have standard checkpointing across all different use cases um and then when the",
    "start": "551839",
    "end": "561240"
  },
  {
    "text": "check uh so when we resolve that problem the next problem was like Fast",
    "start": "561240",
    "end": "566480"
  },
  {
    "text": "autoscaling um f fast upscaling kind of creates its own problems where node",
    "start": "566480",
    "end": "572880"
  },
  {
    "text": "initialization um starts surfacing different issues such as like maybe the surface and the certificate management",
    "start": "572880",
    "end": "579040"
  },
  {
    "text": "is not quite ready uh the nodes are not quite uh the IP address availability is",
    "start": "579040",
    "end": "585040"
  },
  {
    "text": "not quite there um so all of those problems kind of started showing up when autoscaling became like you widely",
    "start": "585040",
    "end": "592240"
  },
  {
    "text": "usable and the users could Autos scale up fast so uh to resolve that uh we had",
    "start": "592240",
    "end": "599200"
  },
  {
    "text": "to kind of work on keeping Headroom uh in so to make sure that there's always",
    "start": "599200",
    "end": "604360"
  },
  {
    "text": "some um additional nodes available uh which has trade-offs with uh the kind of",
    "start": "604360",
    "end": "611160"
  },
  {
    "text": "the utilization um none of these uh kind of steps would be possible without like our",
    "start": "611160",
    "end": "617920"
  },
  {
    "text": "scheduler support So now um deep di a deep dive on the schedule will provided",
    "start": "617920",
    "end": "623079"
  },
  {
    "text": "by way wa uh thank you uping um so so uh I will talk about the",
    "start": "623079",
    "end": "631560"
  },
  {
    "start": "626000",
    "end": "652000"
  },
  {
    "text": "GPU management and uh GPU resource preemption uh which we we use a lot I think I I think in the use case in IPO",
    "start": "631560",
    "end": "638839"
  },
  {
    "text": "is huge that we have so many organizations teams want to use Ray and how to really um offer The Limited GPU",
    "start": "638839",
    "end": "647040"
  },
  {
    "text": "to all these teams organizations is the key issue that we want to solve",
    "start": "647040",
    "end": "652920"
  },
  {
    "start": "652000",
    "end": "744000"
  },
  {
    "text": "um and why we're choosing approach unicorn so uh approach unicorn um is a",
    "start": "652920",
    "end": "660000"
  },
  {
    "text": "kubernetes resource schedular and uh because when we run Ron kubernetes we we we really see a lot of challenges uh",
    "start": "660000",
    "end": "666760"
  },
  {
    "text": "like the uh one of the most important things that resource fragmentation that we see um different GPU nodes are not",
    "start": "666760",
    "end": "673920"
  },
  {
    "text": "fully utilized because such of fragmentations happens here and there and um also it's very hard to manage GPU",
    "start": "673920",
    "end": "680760"
  },
  {
    "text": "codas imagine that you have a lot of different uh type of instances and you want to share those instances with",
    "start": "680760",
    "end": "687279"
  },
  {
    "text": "different teams is extremely hard to to manage uh the quota for different type of gpus and also the GPU utilization is",
    "start": "687279",
    "end": "696120"
  },
  {
    "text": "being a problem is always being a problem and and and we know that people are complaining about not getting enough",
    "start": "696120",
    "end": "702000"
  },
  {
    "text": "gpus but at the same time actually sometimes GPU is Idle we don't want to see that happen and also uh it's a lack",
    "start": "702000",
    "end": "710040"
  },
  {
    "text": "of uh observability um I think that is one one casing that are missing in a lot of places we trying to build this really",
    "start": "710040",
    "end": "716920"
  },
  {
    "text": "well um so aach unicorn actually help us to solve a lot of issues it highs the complexity of the infrastructure the",
    "start": "716920",
    "end": "724240"
  },
  {
    "text": "resources from the ray so let R to focus on the application side of things and",
    "start": "724240",
    "end": "729760"
  },
  {
    "text": "unicorn will handle the resource management scheduling underneath uh it's",
    "start": "729760",
    "end": "734839"
  },
  {
    "text": "really really uh for us is really bring the mar tendency ready platform to our end users and also Pro provides the huge",
    "start": "734839",
    "end": "742040"
  },
  {
    "text": "cost savings along the way and just a quick recap about Unicorn",
    "start": "742040",
    "end": "747600"
  },
  {
    "start": "744000",
    "end": "789000"
  },
  {
    "text": "it's a resource scheduler on kubernetes and it offers a lot of um scheduling",
    "start": "747600",
    "end": "753560"
  },
  {
    "text": "capabilities and it provides U uh really high efficiency and uh improved sras and",
    "start": "753560",
    "end": "761279"
  },
  {
    "text": "there are some highlights about the features unicorn provides uh such as uh resource cues that is essential for for",
    "start": "761279",
    "end": "768040"
  },
  {
    "text": "Mar tency and uh also offers the resource fairness between different cues",
    "start": "768040",
    "end": "774920"
  },
  {
    "text": "and offers the gun scheding which is very very important for for machine learning clad and uh also the resource",
    "start": "774920",
    "end": "781880"
  },
  {
    "text": "pretty sophisticate uh resource premion I will explain more in this uh later session and we use that a",
    "start": "781880",
    "end": "789480"
  },
  {
    "start": "789000",
    "end": "838000"
  },
  {
    "text": "lot and Cate with unicor integration the good news is that uh in the past uh uh",
    "start": "789480",
    "end": "795720"
  },
  {
    "text": "two months where unicorn Community is working with cuay Upstream that contributing the Unicorn integration to",
    "start": "795720",
    "end": "801720"
  },
  {
    "text": "the Upstream it will be soon available uh in the Le cuber release and uh with",
    "start": "801720",
    "end": "806880"
  },
  {
    "text": "this um integration uh it's very simple to enable unicorn uh for for Ray uh you",
    "start": "806880",
    "end": "812079"
  },
  {
    "text": "just need to set a flag batch scheduler uh set a name as unicorn you have unicorn installed then um then you can",
    "start": "812079",
    "end": "819680"
  },
  {
    "text": "easily you use this all these unicorn features and the way to use it is basically you can set it the labels uh",
    "start": "819680",
    "end": "826199"
  },
  {
    "text": "very simple labels um maybe one is the app ID and the other is a q name to basically tell unicorn uh which queue",
    "start": "826199",
    "end": "832880"
  },
  {
    "text": "you want to run your recluster re job on then it will work out of the out of the box",
    "start": "832880",
    "end": "839519"
  },
  {
    "text": "so just a quick example um you rather read jobs back you give that label to Define what is the uh app ID for this",
    "start": "839519",
    "end": "846399"
  },
  {
    "text": "application you define which Q you want to submit this uh job to then uh once",
    "start": "846399",
    "end": "851480"
  },
  {
    "text": "you submited this back to cuay the cuay operator will basically uh create a kuet",
    "start": "851480",
    "end": "856720"
  },
  {
    "text": "job and also the recluster with the hiend workers and um because of the",
    "start": "856720",
    "end": "861959"
  },
  {
    "text": "integration we have done then unic can basically see all this workers and hat",
    "start": "861959",
    "end": "867399"
  },
  {
    "text": "is coming from this application and they will put it put it uh cluster into a uh",
    "start": "867399",
    "end": "873160"
  },
  {
    "text": "queue uh where we manage the uh GP resources and it will apply all those",
    "start": "873160",
    "end": "878720"
  },
  {
    "text": "policies or codas uh in place for this workload and when you run reob or",
    "start": "878720",
    "end": "886560"
  },
  {
    "start": "883000",
    "end": "978000"
  },
  {
    "text": "reclustering cues it will be very different than running in uh lative kubernetes uh in name spaces um because",
    "start": "886560",
    "end": "893959"
  },
  {
    "text": "each of the cues are providing nice Concepts about um resource quas and it",
    "start": "893959",
    "end": "900759"
  },
  {
    "text": "offers the concept about guaranteed and the max quoda in two Dimensions uh so",
    "start": "900759",
    "end": "907000"
  },
  {
    "text": "really make these qes are elastic and uh when we have different Rec clusters running the queue uh it supports the",
    "start": "907000",
    "end": "914040"
  },
  {
    "text": "priority so it also supports f for ordering and other policies in order to",
    "start": "914040",
    "end": "920399"
  },
  {
    "text": "order the application instead of the queue uh to to make sure that right application got a GPU first and also",
    "start": "920399",
    "end": "926920"
  },
  {
    "text": "between across these cues we're doing the resource fairness that is extremely important uh because uh we we we have a",
    "start": "926920",
    "end": "933880"
  },
  {
    "text": "lot of cases where people are competing for for gpus we don't want to um make anyone unhappy but we just want to",
    "start": "933880",
    "end": "940639"
  },
  {
    "text": "maintain a fairness um policy there to make sure each of team can still get some GPU to use even in the peak hours",
    "start": "940639",
    "end": "948240"
  },
  {
    "text": "of our system and underneath it's a large shared GPU pool uh the pr",
    "start": "948240",
    "end": "954000"
  },
  {
    "text": "principle we're trying to get here is basically uh we want to make sure the GPU pool can be sh",
    "start": "954000",
    "end": "959680"
  },
  {
    "text": "as much as possible for for all these teams um in some cases we we still do",
    "start": "959680",
    "end": "965199"
  },
  {
    "text": "like isolations in some extreme cases but in most of the cases we want to use",
    "start": "965199",
    "end": "970279"
  },
  {
    "text": "a large GPU pool to share to everybody and use unicorn to matage all those GPU Ty uh quas that works very well for",
    "start": "970279",
    "end": "978839"
  },
  {
    "start": "978000",
    "end": "1108000"
  },
  {
    "text": "us and here's a slide about talking about the features we using uh in Ray uh",
    "start": "978839",
    "end": "985000"
  },
  {
    "text": "from unicorn um so first one is the GPU C management and it it is just it can",
    "start": "985000",
    "end": "991120"
  },
  {
    "text": "manage a lot of the different resource tabs GPU is just one of it and second one is the over subscription as it's",
    "start": "991120",
    "end": "996880"
  },
  {
    "text": "think this is the key that we are able to over subscribe our cues to utilize",
    "start": "996880",
    "end": "1002279"
  },
  {
    "text": "more idle gpus so we can essentially we can put a lot of uh cues uh share",
    "start": "1002279",
    "end": "1007920"
  },
  {
    "text": "underneath GPU cluster to drive the cluster utilization to really high and Q",
    "start": "1007920",
    "end": "1013360"
  },
  {
    "text": "hierarchy is something that we um I I think it it makes a lot of sense for um",
    "start": "1013360",
    "end": "1018440"
  },
  {
    "text": "for if you have a pretty complex organization structure so we can use that to map to the organization",
    "start": "1018440",
    "end": "1025038"
  },
  {
    "text": "structure to make sure that we have a proper Management on different layers of uh organization from the from from large",
    "start": "1025039",
    "end": "1032640"
  },
  {
    "text": "business live business to small team or individuals and uh resource f is",
    "start": "1032640",
    "end": "1039000"
  },
  {
    "text": "something that we we use on not actually it's uh coming by default uh for all the",
    "start": "1039000",
    "end": "1044558"
  },
  {
    "text": "gpus and the guaranteed and Max Capacity is the key that we can offer different",
    "start": "1044559",
    "end": "1049799"
  },
  {
    "text": "tiers of GPU for our customers uh like guaranteed is are some gpus that you can always available the max is kind of a",
    "start": "1049799",
    "end": "1056760"
  },
  {
    "text": "back effort Bean packing is the mechanism that we really make sure our nodes are fully utilized um there are",
    "start": "1056760",
    "end": "1064720"
  },
  {
    "text": "two parts of be packing I talk about later but that is the cas how we can really make uh good use of uh no",
    "start": "1064720",
    "end": "1071640"
  },
  {
    "text": "resources um and the queing is U is um uh something fundamentally provided by",
    "start": "1071640",
    "end": "1078039"
  },
  {
    "text": "the queue when you are running out of capacity we are not going to reject your applications just wait in the queue",
    "start": "1078039",
    "end": "1084039"
  },
  {
    "text": "until there's someone or some workloads release gpus and also uh priority",
    "start": "1084039",
    "end": "1090120"
  },
  {
    "text": "supported so we make sure that we we do consider the priority both in scheduling",
    "start": "1090120",
    "end": "1095400"
  },
  {
    "text": "and also the premion and the last is the premion which we use a lot to ensure um",
    "start": "1095400",
    "end": "1101400"
  },
  {
    "text": "we oversell the CPUs but at the same time when we need them back we have a proper way to reclaim the gpus",
    "start": "1101400",
    "end": "1109360"
  },
  {
    "start": "1108000",
    "end": "1136000"
  },
  {
    "text": "here's a very simple example um uh on the right hand side is the example configuration you can conf config uh",
    "start": "1109360",
    "end": "1116559"
  },
  {
    "text": "very easy you can set up a different uh several cues with guaranteed Max GPU Coda and there are some extra features",
    "start": "1116559",
    "end": "1124120"
  },
  {
    "text": "provided by by this cues uh such as that you can configure Dynamic cues or or",
    "start": "1124120",
    "end": "1129320"
  },
  {
    "text": "static cues based on your needs and also the Q hierarchy supported um there's",
    "start": "1129320",
    "end": "1134880"
  },
  {
    "text": "ACLS as well and in interesting part is that we",
    "start": "1134880",
    "end": "1140120"
  },
  {
    "start": "1136000",
    "end": "1209000"
  },
  {
    "text": "are kind of going to the direction of manage U mul type gpus in the past is very hard for us because uh even we have",
    "start": "1140120",
    "end": "1147320"
  },
  {
    "text": "a mixed um Diversified the GPU instance types um the resources using in the GPU",
    "start": "1147320",
    "end": "1152880"
  },
  {
    "text": "is still um the the the the single type um that for us is not very convenient uh",
    "start": "1152880",
    "end": "1160120"
  },
  {
    "text": "because in some cases some high NG GPS we want to limit the usage for for teams",
    "start": "1160120",
    "end": "1166240"
  },
  {
    "text": "and we want them to to use more you know cheaper instances if possible so we",
    "start": "1166240",
    "end": "1171520"
  },
  {
    "text": "don't have a really good story to manage different type of gpus uh we are exploring a way to use a modified GPU um",
    "start": "1171520",
    "end": "1178600"
  },
  {
    "text": "GPU driver to expose different GPU types as resources then you can really manage",
    "start": "1178600",
    "end": "1183880"
  },
  {
    "text": "by unicorn then we have a last chart uh this is from the metrix uh last chart",
    "start": "1183880",
    "end": "1189120"
  },
  {
    "text": "about uh what is the guaranteed GPU per Q what is the max gpq and what's the current utilization this way um we can",
    "start": "1189120",
    "end": "1196840"
  },
  {
    "text": "easily track um different type of gpus distribution uh do you want to reshuffle",
    "start": "1196840",
    "end": "1201919"
  },
  {
    "text": "them uh do you want to change the layout so it's totally un last so this is very convenient this is something we're",
    "start": "1201919",
    "end": "1209720"
  },
  {
    "start": "1209000",
    "end": "1303000"
  },
  {
    "text": "exploring so pin packing um as mentioned this is one of the key ke feature we're",
    "start": "1210159",
    "end": "1216120"
  },
  {
    "text": "using to uh solve the re GPU fragmentation issue which is really a serious problem happening uh lot of",
    "start": "1216120",
    "end": "1223360"
  },
  {
    "text": "times we see that uh cluster utilization is not that high but the coming GPU",
    "start": "1223360",
    "end": "1228640"
  },
  {
    "text": "workows cannot get a GPU because there's just not enough space on each of the node um it's just because there are",
    "start": "1228640",
    "end": "1234400"
  },
  {
    "text": "fragmentations on here and there um we wasting our GPU hours so the bean",
    "start": "1234400",
    "end": "1241799"
  },
  {
    "text": "packing we have two things one is like U uh using the scheduling phase beIN packing provided by unicorn uh this is a",
    "start": "1241799",
    "end": "1249159"
  },
  {
    "text": "policy you can config and it will basically uh during the scheduling for each of the parts allocation try to find",
    "start": "1249159",
    "end": "1255559"
  },
  {
    "text": "out I'll try to pack Parts more uh increase the density on the noes make",
    "start": "1255559",
    "end": "1260840"
  },
  {
    "text": "sure the nodes are fully utilized before moving to the next one so this is a schedule phe but this is not going to",
    "start": "1260840",
    "end": "1266640"
  },
  {
    "text": "solve all the problem because after some some time you will see the fragmentation issue come popups again and then we will",
    "start": "1266640",
    "end": "1274640"
  },
  {
    "text": "need to some post scheduling um imp packing mechanism currently we're using Carpenter to do that and that basically",
    "start": "1274640",
    "end": "1281640"
  },
  {
    "text": "scan the uh P pods uh uh distributions on your cluster and trying to move the",
    "start": "1281640",
    "end": "1287919"
  },
  {
    "text": "pods around and make it more pack so we can freeze some of the nodes uh easier to scale down or leave for larger you",
    "start": "1287919",
    "end": "1295679"
  },
  {
    "text": "know work uh larger GPU requests with this two we are able to uh",
    "start": "1295679",
    "end": "1301279"
  },
  {
    "text": "reduce the fragmentation a lot and G scheduling is a very important",
    "start": "1301279",
    "end": "1306480"
  },
  {
    "start": "1303000",
    "end": "1355000"
  },
  {
    "text": "feature and also this will be coming is the um up Upstream C Ray offering with",
    "start": "1306480",
    "end": "1312400"
  },
  {
    "text": "the Unicorn integration and very simple you just need a private label on your CR",
    "start": "1312400",
    "end": "1318000"
  },
  {
    "text": "um telling uh with the R iio Gun schedule enabled was true then uh it",
    "start": "1318000",
    "end": "1323799"
  },
  {
    "text": "will give the Unicorn schedule head up so I want to enable gun for this application this is a per application",
    "start": "1323799",
    "end": "1331200"
  },
  {
    "text": "configuration you can apply and once you have that enable uh what unicorn does",
    "start": "1331200",
    "end": "1336919"
  },
  {
    "text": "will basically res Reserve all the resource ahead of time before actually uh schedule it so uh this way we can",
    "start": "1336919",
    "end": "1345279"
  },
  {
    "text": "really avoid the situation while uh some of the recluster only gets the partial resources and the now the workers are",
    "start": "1345279",
    "end": "1351679"
  },
  {
    "text": "work making progress and this is very helpful then uh I will spent some time",
    "start": "1351679",
    "end": "1358799"
  },
  {
    "text": "to talk about the GPU premion and this is a uh quite new feature we are uh",
    "start": "1358799",
    "end": "1364159"
  },
  {
    "text": "using lately to improve the utilization so um because we're supporting so many",
    "start": "1364159",
    "end": "1369760"
  },
  {
    "text": "different use cases they latency sensitive or non-sensitive workloads they um interactive sessions realtime",
    "start": "1369760",
    "end": "1377200"
  },
  {
    "text": "sessions and they also some first iteration experiments user just want to get the results back really quick so um",
    "start": "1377200",
    "end": "1385520"
  },
  {
    "text": "among all these different use cases users just want very simple just want to have their job done in XYZ hours um",
    "start": "1385520",
    "end": "1394120"
  },
  {
    "text": "makes sense and but on the other hand uh administrator want to really make the",
    "start": "1394120",
    "end": "1399679"
  },
  {
    "text": "cluster utilization High GPU is so expensive we don't want to waste a lot of money on there so there's a conflict",
    "start": "1399679",
    "end": "1407240"
  },
  {
    "text": "um how do you solve this is we uh leverage unicorn qes guaranteed at Max Capacity what it what it means is",
    "start": "1407240",
    "end": "1414240"
  },
  {
    "text": "basically we can Define the guaranteed capacity for each of the queue that is the capacity the schedule will always",
    "start": "1414240",
    "end": "1421520"
  },
  {
    "text": "satisfy in any uh situation even every every queue is competing for gpus those",
    "start": "1421520",
    "end": "1428039"
  },
  {
    "text": "guaranteed capacity will still be available and then Max is the way you can scale your que if other teams are",
    "start": "1428039",
    "end": "1434799"
  },
  {
    "text": "not using uh uh gpus some gpus are available on the class cluster um those",
    "start": "1434799",
    "end": "1439840"
  },
  {
    "text": "GPO will be available up to your L your Max uh this way we put a lot of cues in",
    "start": "1439840",
    "end": "1445559"
  },
  {
    "text": "the same cluster we set the guarantee and Max Capacity based on the customer needs um then we're able to make them",
    "start": "1445559",
    "end": "1452760"
  },
  {
    "text": "really elastic and in the idle time that we make sure we drive the utilization",
    "start": "1452760",
    "end": "1458640"
  },
  {
    "text": "because certain cues still have a uh outstanding requests but in the busy hours we do the premion to make sure the",
    "start": "1458640",
    "end": "1464760"
  },
  {
    "text": "guarantee can still still uh satisfi I will just give a quick um um",
    "start": "1464760",
    "end": "1471440"
  },
  {
    "start": "1468000",
    "end": "1621000"
  },
  {
    "text": "demonstration about how this works uh imagine we have two CES very simple two CES right now cluster is empty cues are",
    "start": "1471440",
    "end": "1477559"
  },
  {
    "text": "empty uh the utilization are both 0% um so now let's see we have some",
    "start": "1477559",
    "end": "1483600"
  },
  {
    "text": "workloads the r clusters running in the first queue um it uses 30% so both cues",
    "start": "1483600",
    "end": "1489559"
  },
  {
    "text": "are um we set a Max up to the cluster Max uh but the first queue we give 60%",
    "start": "1489559",
    "end": "1496159"
  },
  {
    "text": "of the guaranteed gpus to and the second queue only gets 40 40 uh 20%",
    "start": "1496159",
    "end": "1502679"
  },
  {
    "text": "40% then once we submitted um rate clusters to the first queue it start to",
    "start": "1502679",
    "end": "1508279"
  },
  {
    "text": "consume resources and now we uh because there's no work Clow in the second queue",
    "start": "1508279",
    "end": "1514159"
  },
  {
    "text": "it consumes 50% of gpus that's fine we can go even higher that uh because this",
    "start": "1514159",
    "end": "1519880"
  },
  {
    "text": "que is really busy we add it scale to 100% that's that's fine and we are happy because now the all the gpus are",
    "start": "1519880",
    "end": "1526720"
  },
  {
    "text": "utilized but what if there's a another recluster submitted to the second que",
    "start": "1526720",
    "end": "1532480"
  },
  {
    "text": "and now there's no available gpus uh in the cluster the pods will be pending",
    "start": "1532480",
    "end": "1538000"
  },
  {
    "text": "there and at this time unicorn will basically start to trigger the premion",
    "start": "1538000",
    "end": "1544399"
  },
  {
    "text": "process because right now what happens is that the first queue uses more and guaranteed capacity but the second queue",
    "start": "1544399",
    "end": "1551640"
  },
  {
    "text": "needs some more resource to to reach his guaranteed capacity so um unicorn will",
    "start": "1551640",
    "end": "1557600"
  },
  {
    "text": "basically say oh I need to move some of the GPU from over utilized cues to",
    "start": "1557600",
    "end": "1562720"
  },
  {
    "text": "underutilized cues and it will basically honor the guaranteed the capacity and also when when you do the prion you will",
    "start": "1562720",
    "end": "1569520"
  },
  {
    "text": "consider the application priority and also task priority and also how long",
    "start": "1569520",
    "end": "1574679"
  },
  {
    "text": "your task are running uh imagine if the task priority the same then uh you",
    "start": "1574679",
    "end": "1580640"
  },
  {
    "text": "basically select two postive print a very very simple example then you will reclaim the gpus and reallocate the gpus",
    "start": "1580640",
    "end": "1587120"
  },
  {
    "text": "to the second Que then the second que will will get a resource to run and but",
    "start": "1587120",
    "end": "1593000"
  },
  {
    "text": "imagine if you at this time if you submit more workloads to the second que because we are still under 100%",
    "start": "1593000",
    "end": "1598840"
  },
  {
    "text": "utilization so the second will not be able to get a gpus this is the contract we give to our users and uh what I found",
    "start": "1598840",
    "end": "1607200"
  },
  {
    "text": "is is uh it's pretty straightfor for user to understand how this works and that they're able to come up with some",
    "start": "1607200",
    "end": "1613320"
  },
  {
    "text": "some um design on their how they want to operate on their guaranteed or Max Capacity",
    "start": "1613320",
    "end": "1619080"
  },
  {
    "text": "um works pretty well for us and uh here's the simulation just to",
    "start": "1619080",
    "end": "1625399"
  },
  {
    "start": "1621000",
    "end": "1717000"
  },
  {
    "text": "mimic how it works and because we have this less metrics about track tracking the utilizations on different cues um we",
    "start": "1625399",
    "end": "1634679"
  },
  {
    "text": "we are able to virtualize like basically see how this works um this is a very common uh production use case uh for",
    "start": "1634679",
    "end": "1642279"
  },
  {
    "text": "example when we start a cluster we start five qes one of the que have a lot of",
    "start": "1642279",
    "end": "1648240"
  },
  {
    "text": "more workload than others this is very common and we let it to use more resources as you can say most of the",
    "start": "1648240",
    "end": "1654919"
  },
  {
    "text": "gpus are being utilized by the first queue and it is good so but what if",
    "start": "1654919",
    "end": "1661399"
  },
  {
    "text": "other cues are start to uh requesting for gpus at this time because where uh",
    "start": "1661399",
    "end": "1667320"
  },
  {
    "text": "gpus are all used then we were consider to make sure that each of the queue can",
    "start": "1667320",
    "end": "1672360"
  },
  {
    "text": "still get the um guaranteed capacity so if you look at the uh that that that",
    "start": "1672360",
    "end": "1680000"
  },
  {
    "text": "moment the prion kicks in will start to reallocate GPU from the first que to",
    "start": "1680000",
    "end": "1685360"
  },
  {
    "text": "other cues if you compare the two charts the first que uses 400 plus gpus but",
    "start": "1685360",
    "end": "1692559"
  },
  {
    "text": "after prion over time all the cues were exactly g a guaranteed capacity as um as",
    "start": "1692559",
    "end": "1698320"
  },
  {
    "text": "a pre previous defined so um this is how we make it work on production just to um",
    "start": "1698320",
    "end": "1705840"
  },
  {
    "text": "to give the guaranteed Capac Max Capacity for each of the team we we",
    "start": "1705840",
    "end": "1711279"
  },
  {
    "text": "managed to use uh something like this uh for production clusters and works very",
    "start": "1711279",
    "end": "1717919"
  },
  {
    "start": "1717000",
    "end": "1818000"
  },
  {
    "text": "well um so quick summary about the session um we are we were running a lot",
    "start": "1719039",
    "end": "1725000"
  },
  {
    "text": "of running into a lot of issues in order to scale Ray uh in apple and I I think I",
    "start": "1725000",
    "end": "1731000"
  },
  {
    "text": "think uh what we want to you to take away from this session is that uh first",
    "start": "1731000",
    "end": "1736200"
  },
  {
    "text": "informal re clusters is is um is um is needed for Mar tendency environment and",
    "start": "1736200",
    "end": "1742159"
  },
  {
    "text": "also to make reliable autoc scanning is the key to for efficiency there are a lot of different uh challenges that have",
    "start": "1742159",
    "end": "1748760"
  },
  {
    "text": "been like I've been mentioned to make Autos skinning really reliable and also",
    "start": "1748760",
    "end": "1755039"
  },
  {
    "text": "try to use Diversified loot pools and uh um we're trying to use as",
    "start": "1755039",
    "end": "1760720"
  },
  {
    "text": "diser diversifi notus as much as possible um just want to make sure uh",
    "start": "1760720",
    "end": "1766600"
  },
  {
    "text": "all the our cues can access the different type of gpus very easily and",
    "start": "1766600",
    "end": "1772840"
  },
  {
    "text": "also use Bean packing to reduce the fragmentation both in the scheduling phase and the post scheduling phase and",
    "start": "1772840",
    "end": "1779799"
  },
  {
    "text": "also leverage so in our case we leverage unicorn cues for Mar tendency how to plan the resources for each of the teams",
    "start": "1779799",
    "end": "1787080"
  },
  {
    "text": "and how to make sure they can share uh limited GPU resources um and also um",
    "start": "1787080",
    "end": "1794360"
  },
  {
    "text": "balance the GP utilization Improvement and the good user experience actually very hard sometimes there's a treat off",
    "start": "1794360",
    "end": "1800960"
  },
  {
    "text": "so it really depends on how you define your system s and how how you how much",
    "start": "1800960",
    "end": "1806840"
  },
  {
    "text": "you care about the GP utilization so you need to find the right balance there then Define the then design the",
    "start": "1806840",
    "end": "1813720"
  },
  {
    "text": "infrastructure to to uh make sure that can happen um yeah thanks then that's that's",
    "start": "1813720",
    "end": "1821760"
  },
  {
    "start": "1818000",
    "end": "1840000"
  },
  {
    "text": "it from our session we can take some questions um unfortunately I think we're out of time but uh thank thank you we",
    "start": "1821760",
    "end": "1828720"
  },
  {
    "text": "and auin I think you you'll hang hang out around the back so if you have questions feel free to find them there but thank you every for coming thank you",
    "start": "1828720",
    "end": "1835399"
  },
  {
    "text": "yeah",
    "start": "1835399",
    "end": "1838399"
  }
]