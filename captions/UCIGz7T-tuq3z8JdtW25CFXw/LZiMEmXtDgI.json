[
  {
    "text": "[Music]",
    "start": "310",
    "end": "12800"
  },
  {
    "text": "good morning everyone uh thanks for being here uh my name is Vu Hu i'm from",
    "start": "12800",
    "end": "18400"
  },
  {
    "text": "uh Second Sate uh Second Sate is the creator and maintainer of the CNCF was",
    "start": "18400",
    "end": "23760"
  },
  {
    "text": "magic project oh",
    "start": "23760",
    "end": "29220"
  },
  {
    "text": "[Music] um hello everyone uh my name is Michael Yang and I'm also from Second State and",
    "start": "29220",
    "end": "35680"
  },
  {
    "text": "many of you guys know me and I'm the um uh maintainer of the WMage project as",
    "start": "35680",
    "end": "41440"
  },
  {
    "text": "well okay uh let's get started so uh I'm",
    "start": "41440",
    "end": "48320"
  },
  {
    "text": "going to talk about uh was language agnostic but I'm not talking about the",
    "start": "48320",
    "end": "55039"
  },
  {
    "text": "programming language i'm talking about a WM based AI agent it can translate any",
    "start": "55039",
    "end": "61600"
  },
  {
    "text": "video in any language and uh with very high accuracy so it's uh the name is",
    "start": "61600",
    "end": "68640"
  },
  {
    "text": "called the agent name is called u video langua uh you can go to video langua.com",
    "start": "68640",
    "end": "73840"
  },
  {
    "text": "and upload a video and translate the video to your desired language so I think we can",
    "start": "73840",
    "end": "82360"
  },
  {
    "text": "uh let's have a live demo so I will go to um video langua.com and",
    "start": "82360",
    "end": "89200"
  },
  {
    "text": "right now we support uh English to Chinese English to Japanese English to",
    "start": "89200",
    "end": "94479"
  },
  {
    "text": "Korean and Chinese to English and Japanese to English so I will love uh",
    "start": "94479",
    "end": "101040"
  },
  {
    "text": "upload a video to translate uh from English to",
    "start": "101040",
    "end": "108880"
  },
  {
    "text": "Chinese uh the video I'm uh uploading is um after movie uh for was my old 2024 i",
    "start": "108880",
    "end": "116720"
  },
  {
    "text": "have seen many familiar uh friends in this",
    "start": "116720",
    "end": "121360"
  },
  {
    "text": "video it takes some time to upload",
    "start": "128360",
    "end": "133399"
  },
  {
    "text": "okay so uh the video is uploaded so here I can choose to add the uh Chinese voice",
    "start": "140160",
    "end": "147200"
  },
  {
    "text": "over to the video or to add the Chinese uh subtitles and uh I can also u choose the",
    "start": "147200",
    "end": "154560"
  },
  {
    "text": "if I choose Chinese voice over I can choose whether I want um a male voice or",
    "start": "154560",
    "end": "160480"
  },
  {
    "text": "a female voice but I will choose a Chinese subtitle here and you can uh when you choose the ch uh when you",
    "start": "160480",
    "end": "166640"
  },
  {
    "text": "choose subtitle you can u customize the uh the font style uh I will use the default one and we also have several",
    "start": "166640",
    "end": "174080"
  },
  {
    "text": "translation prompts uh for different content type so we have uh tag AI rust",
    "start": "174080",
    "end": "181120"
  },
  {
    "text": "cloud native uh maybe I should add a web simulate later and um uh you can also",
    "start": "181120",
    "end": "187440"
  },
  {
    "text": "write the whistle prompt but I don't think I I need here so I will and this",
    "start": "187440",
    "end": "193200"
  },
  {
    "text": "is a translation prompt um I will add I can customize the u translation prompt",
    "start": "193200",
    "end": "200000"
  },
  {
    "text": "for this video because I know uh someone will talk about was myo and was myo",
    "start": "200000",
    "end": "205120"
  },
  {
    "text": "shouldn't be translated so I will add this rule to the translation prompt",
    "start": "205120",
    "end": "212400"
  },
  {
    "text": "yeah so just to add to it you know so the whisper pal is to um teach the model",
    "start": "212400",
    "end": "218400"
  },
  {
    "text": "uh how to do the um voice to text so for instance a lot of those videos has",
    "start": "218400",
    "end": "224480"
  },
  {
    "text": "specific terms so for instance if you we say wasome you know was said a lot in this in this meeting right in in the in",
    "start": "224480",
    "end": "231120"
  },
  {
    "text": "a video that produced by this meeting but if you just give it a general language model and ask to say wasome and",
    "start": "231120",
    "end": "237760"
  },
  {
    "text": "it wouldn't be able to be able to transcribe it to correctly it gonna misar into something else right it gonna",
    "start": "237760",
    "end": "243760"
  },
  {
    "text": "hear awesome it's going to hear something else you know so to put uh so you can put those things special words",
    "start": "243760",
    "end": "249120"
  },
  {
    "text": "in whist so another example is in in CNCF videos we say Kubernetes a lot you",
    "start": "249120",
    "end": "254400"
  },
  {
    "text": "know and you know a lot of those um you know uh text to uh voice to uh to text models have difficulties understanding",
    "start": "254400",
    "end": "260880"
  },
  {
    "text": "those unusual words right so you can put those into the uh so the west prompt really is to instruct the model um",
    "start": "260880",
    "end": "269040"
  },
  {
    "text": "special words you need to pay attention to or here and then the translation promp is for the large language model so",
    "start": "269040",
    "end": "274800"
  },
  {
    "text": "it's on the back it's open source large language model as we were going to talk about in a minute So you can see there's",
    "start": "274800",
    "end": "281199"
  },
  {
    "text": "all kind of instructions you know translate this to that and uh pay attention to this pay attention to that",
    "start": "281199",
    "end": "286479"
  },
  {
    "text": "so all those are in the uh translation template so I think this is a general template right if you select AI or",
    "start": "286479",
    "end": "293520"
  },
  {
    "text": "crypto it going to have a different set of rules uh yes so we also have some uh",
    "start": "293520",
    "end": "300400"
  },
  {
    "text": "translation examples in this in this prompt like we say uh we shouldn't",
    "start": "300400",
    "end": "306320"
  },
  {
    "text": "translate uh API so we should keep API as API and there are some uh host so uh",
    "start": "306320",
    "end": "314960"
  },
  {
    "text": "whole sentence translation example so uh after do that I can uh input my email",
    "start": "314960",
    "end": "321039"
  },
  {
    "text": "and uh submit the task and we also have a uh status page",
    "start": "321039",
    "end": "328560"
  },
  {
    "text": "it uh shows um which steps um which steps the agents are working on right",
    "start": "328560",
    "end": "334240"
  },
  {
    "text": "now we have the uh um the whisper model to recognize the video content because",
    "start": "334240",
    "end": "340960"
  },
  {
    "text": "the video will be processed by multiple light language models uh whisper uh TDS",
    "start": "340960",
    "end": "347600"
  },
  {
    "text": "so it will take uh some time so I will go back to my slides",
    "start": "347600",
    "end": "354919"
  },
  {
    "text": "yeah this is uh you know when you hear the term inference scaling or scaling the inference this is what's happening",
    "start": "356160",
    "end": "362320"
  },
  {
    "text": "here you know it's uh the longer it takes the better the quality would be because what it does is pass through the",
    "start": "362320",
    "end": "368800"
  },
  {
    "text": "model multiple times translate once and have the model critic itself is did it",
    "start": "368800",
    "end": "373840"
  },
  {
    "text": "get anything wrong translate it again transcribe it again so we go through multiples um uh steps so you know we uh",
    "start": "373840",
    "end": "381680"
  },
  {
    "text": "the system like that could do almost a real-time translation you know meaning as soon as I finish a sentence large",
    "start": "381680",
    "end": "387759"
  },
  {
    "text": "language model would come back with a with a translation but uh the longer it takes at inference time the more",
    "start": "387759",
    "end": "394720"
  },
  {
    "text": "accurate it going to be you know we see it again and again with open eyes 01 03",
    "start": "394720",
    "end": "399759"
  },
  {
    "text": "R1 R3 models right you know make it sync and it gonna do a lot better",
    "start": "399759",
    "end": "405280"
  },
  {
    "text": "so uh here is a translated video that I uh translate yesterday changed and some",
    "start": "405280",
    "end": "412479"
  },
  {
    "text": "things the the image is generated by uh ch new product",
    "start": "412479",
    "end": "418639"
  },
  {
    "text": "and this video is generated by AI as well so we added the uh Korean subtitle",
    "start": "418639",
    "end": "424720"
  },
  {
    "text": "to the video but I don't know u Korean but it seems that everything is fine the lands of",
    "start": "424720",
    "end": "431840"
  },
  {
    "text": "darkness what must I do the ring must be destroyed cast back",
    "start": "431840",
    "end": "438880"
  },
  {
    "text": "into the fires of Mount Doom i know what hunts you they're coming",
    "start": "438880",
    "end": "444800"
  },
  {
    "text": "they will find the ring and kill the one who cares",
    "start": "444800",
    "end": "450759"
  },
  {
    "text": "you have my sword my bow and my axe you carry the fate of his all little one",
    "start": "451039",
    "end": "458000"
  },
  {
    "text": "this task was appointed to you and if you do not find the way no one will",
    "start": "458000",
    "end": "464639"
  },
  {
    "text": "let us hope that our presence may go unnoticed",
    "start": "464639",
    "end": "469400"
  },
  {
    "text": "they are coming",
    "start": "472319",
    "end": "475720"
  },
  {
    "text": "we must have shall not pass",
    "start": "477520",
    "end": "487800"
  },
  {
    "text": "you will face evil and you will defeat it even the smallest person can change the",
    "start": "491360",
    "end": "498400"
  },
  {
    "text": "course of the future i'm glad you're with me",
    "start": "498400",
    "end": "505400"
  },
  {
    "text": "[Music]",
    "start": "507130",
    "end": "514059"
  },
  {
    "text": "you see something really interesting here is that it has a very loud background music and it detects that it",
    "start": "515360",
    "end": "522560"
  },
  {
    "text": "completely ignores the music so it only has caption when the when there are people speaking there right",
    "start": "522560",
    "end": "531160"
  },
  {
    "text": "uh this is the same video but I add a Korean voice over to it uh I assume that",
    "start": "532880",
    "end": "539680"
  },
  {
    "text": "none of us can understand the Korean so I just play for several seconds",
    "start": "539680",
    "end": "547720"
  },
  {
    "text": "okay and that's what I want to show you or we can um add a subtitle and a voice",
    "start": "578080",
    "end": "583200"
  },
  {
    "text": "over to the uh video so uh next I'm going to talk about how",
    "start": "583200",
    "end": "590560"
  },
  {
    "text": "we achieve high uh accuracy so uh the first one is that we have agent",
    "start": "590560",
    "end": "596720"
  },
  {
    "text": "collaboration across u multiple AI models so firstly we will have the",
    "start": "596720",
    "end": "602240"
  },
  {
    "text": "whisper model it will transcribe the video to text with the time steps and",
    "start": "602240",
    "end": "608000"
  },
  {
    "text": "then we will have llama 3.18 billion model uh firstly it will combine all the",
    "start": "608000",
    "end": "615360"
  },
  {
    "text": "uh all the transcript text uh together and then it will break up the text to",
    "start": "615360",
    "end": "622000"
  },
  {
    "text": "make sure each time step contains exactly one sentence so the translation",
    "start": "622000",
    "end": "627680"
  },
  {
    "text": "will be correct and uh then according to the uh the target language we will use",
    "start": "627680",
    "end": "633600"
  },
  {
    "text": "different light language models to translate the video so if you choose Chinese we will use the uh Q1",
    "start": "633600",
    "end": "640760"
  },
  {
    "text": "2.572 billion model uh because the Q1 model is developed by Alibaba i think",
    "start": "640760",
    "end": "646640"
  },
  {
    "text": "it's it is the uh best Chinese open source model and if you choose Korea we",
    "start": "646640",
    "end": "652880"
  },
  {
    "text": "will use the exa models it's released by uh LG AI",
    "start": "652880",
    "end": "658760"
  },
  {
    "text": "research it's a special Korean model it's good at Korean translation so when",
    "start": "658760",
    "end": "665279"
  },
  {
    "text": "you do translation we also have a system prompts as I just before it will uh contains examples on how to translate",
    "start": "665279",
    "end": "672880"
  },
  {
    "text": "some domain specific words and we also will put the whole uh uh the whole",
    "start": "672880",
    "end": "680160"
  },
  {
    "text": "transcripts into the system prompts so when the uh when the light language",
    "start": "680160",
    "end": "685200"
  },
  {
    "text": "model translates the sentence it will uh have full context it will make sure uh",
    "start": "685200",
    "end": "693120"
  },
  {
    "text": "uh the translation is uh is suitable for the context and um after we got the",
    "start": "693120",
    "end": "699839"
  },
  {
    "text": "translation results we will have um a small model uh llama 3.2 3 billion model",
    "start": "699839",
    "end": "705839"
  },
  {
    "text": "it will do uh it will do cross check the uh translation results uh it will check",
    "start": "705839",
    "end": "713360"
  },
  {
    "text": "um if the translation result contains notes or explanation about the trans uh",
    "start": "713360",
    "end": "719839"
  },
  {
    "text": "translation for example if you uh if the video uh uh if the video uh uh says uh",
    "start": "719839",
    "end": "727279"
  },
  {
    "text": "how do you think a web assembly um it may cause the light langu translate this question and we",
    "start": "727279",
    "end": "735519"
  },
  {
    "text": "will also to check if the uh uh if the translation result uh contains",
    "start": "735519",
    "end": "741519"
  },
  {
    "text": "non-Chinese uh for this are to make sure the u translation",
    "start": "741519",
    "end": "747120"
  },
  {
    "text": "result is consequence and uh If you choose to uh finally if you choose to",
    "start": "747120",
    "end": "752800"
  },
  {
    "text": "add a voice over to the to the video we will use a TTI model to generate the uh",
    "start": "752800",
    "end": "759040"
  },
  {
    "text": "target language uh soundtrack and this is the uh agent",
    "start": "759040",
    "end": "764480"
  },
  {
    "text": "collaboration we also have human agit u collaboration we support human is loop",
    "start": "764480",
    "end": "771519"
  },
  {
    "text": "uh as I showed before is can um we can customize the w promise the whisper",
    "start": "771519",
    "end": "776800"
  },
  {
    "text": "prompts we can customize the translation prompts And um when the video translation is",
    "start": "776800",
    "end": "782959"
  },
  {
    "text": "done you can also review the output so I think the video is",
    "start": "782959",
    "end": "788040"
  },
  {
    "text": "translated let me see yeah uh it's done so",
    "start": "788040",
    "end": "796639"
  },
  {
    "text": "uh we have the time steps and the subtitles and you",
    "start": "796639",
    "end": "803120"
  },
  {
    "text": "can uh you can edit the Samsung and u",
    "start": "803120",
    "end": "808160"
  },
  {
    "text": "edit the subtitle is that okay",
    "start": "808160",
    "end": "812920"
  },
  {
    "text": "this is such a nent ecosystem something that's about to explode into life",
    "start": "815040",
    "end": "822600"
  },
  {
    "text": "by the way that was last [Music]",
    "start": "822720",
    "end": "830349"
  },
  {
    "text": "year feels like we're at the beginning of something",
    "start": "832600",
    "end": "837920"
  },
  {
    "text": "amazing i think that this conference is a great place for new contributors and even experienced folks in the web",
    "start": "839880",
    "end": "846160"
  },
  {
    "text": "assembly community to come together and describe the different ideas and the adoption that we're seeing in web assembly",
    "start": "846160",
    "end": "852480"
  },
  {
    "text": "it's absolutely amazing to be with everyone in the community uh to see the amazing things that folks are building",
    "start": "852480",
    "end": "859920"
  },
  {
    "text": "i have learned a lot by having the opportunity to talk to people that are building this ecosystem",
    "start": "859920",
    "end": "867360"
  },
  {
    "text": "you're getting to build the future with people but you're also getting into the nitty-gritty of what that means and what the next concrete step forward is for a",
    "start": "867360",
    "end": "874240"
  },
  {
    "text": "lot of these projects you're not going to find an experience anywhere else like this",
    "start": "874240",
    "end": "882370"
  },
  {
    "text": "[Music] okay I think the translation is good so",
    "start": "882370",
    "end": "888000"
  },
  {
    "text": "I don't need to uh add it here but if I want I can add it here to make it more",
    "start": "888000",
    "end": "896800"
  },
  {
    "text": "accurate so uh we are in a tech conference so I'm I'm going to talk more",
    "start": "904839",
    "end": "910399"
  },
  {
    "text": "about the uh agent collaboration so as you can see we have uh three different",
    "start": "910399",
    "end": "915920"
  },
  {
    "text": "um types of uh AI models to uh to do the translation task together so we have a",
    "start": "915920",
    "end": "922800"
  },
  {
    "text": "whisper uh we have the light language models and we also have the TTS model",
    "start": "922800",
    "end": "928320"
  },
  {
    "text": "each model has its own task so the uh so we use some we use web was",
    "start": "928320",
    "end": "938240"
  },
  {
    "text": "to run on those models so why we use w was but I think before we answer this",
    "start": "938240",
    "end": "944079"
  },
  {
    "text": "question the fundamental question is that why we need some why we not just use open eye why we not just use the s",
    "start": "944079",
    "end": "951120"
  },
  {
    "text": "inference providers so uh here are some uh some reasons um so the model publish",
    "start": "951120",
    "end": "958399"
  },
  {
    "text": "the models published by open eye is one size fits all that means that we can't",
    "start": "958399",
    "end": "964000"
  },
  {
    "text": "optimize the model for the specific tasks or specific language u we can't",
    "start": "964000",
    "end": "970399"
  },
  {
    "text": "optimize the open model open models for the Korean uh Korean language or",
    "start": "970399",
    "end": "975920"
  },
  {
    "text": "Japanese language and secondly open lacks privacy and uh data uh",
    "start": "975920",
    "end": "981279"
  },
  {
    "text": "transparency And uh lastly u the automated goal of",
    "start": "981279",
    "end": "987120"
  },
  {
    "text": "the translation service is to handle voice in real time so if we do a",
    "start": "987120",
    "end": "993040"
  },
  {
    "text": "realtime translation the latency is very important if we use a s service we may",
    "start": "993040",
    "end": "999519"
  },
  {
    "text": "have too much latency um we um in the real time use case we want to the device",
    "start": "999519",
    "end": "1005839"
  },
  {
    "text": "is in this conference not in the u cloud",
    "start": "1005839",
    "end": "1012600"
  },
  {
    "text": "and uh we also have several challenges in local uh the first one is the uh uh",
    "start": "1012720",
    "end": "1020079"
  },
  {
    "text": "is complexity so uh we have three different kinds of models and each model",
    "start": "1020079",
    "end": "1025280"
  },
  {
    "text": "requires its own runtime we can use the TTS model runtime to run on to run light",
    "start": "1025280",
    "end": "1031839"
  },
  {
    "text": "language model and uh different models require uh different hardwares",
    "start": "1031839",
    "end": "1038160"
  },
  {
    "text": "uh because uh as uh as far as I know the latest version of GP so uh can only run",
    "start": "1038160",
    "end": "1046400"
  },
  {
    "text": "on CUDA uh it's can't run on mic or or CPU and the other question is that some",
    "start": "1046400",
    "end": "1054400"
  },
  {
    "text": "u um uh is resource intensity Because if we want to build realtime",
    "start": "1054400",
    "end": "1061039"
  },
  {
    "text": "application or and edge application it will demand u lightweight solutions and",
    "start": "1061039",
    "end": "1068080"
  },
  {
    "text": "the current text sacks are too heavy for uh for those contexts for example the",
    "start": "1068080",
    "end": "1073919"
  },
  {
    "text": "PyTorch uh docker the docker the pytor docker image will be 4 gabytes and we",
    "start": "1073919",
    "end": "1080880"
  },
  {
    "text": "and we know here some was file the size of the wom file is only several meabytes",
    "start": "1080880",
    "end": "1087679"
  },
  {
    "text": "so um was portable and lightweight so we use w was to run the uh the light",
    "start": "1087679",
    "end": "1094640"
  },
  {
    "text": "language models so how how uh how how did we run those",
    "start": "1094640",
    "end": "1100919"
  },
  {
    "text": "models so there is a um um one thing that's uh the load of the ring is the",
    "start": "1100919",
    "end": "1108160"
  },
  {
    "text": "only ring to rule out all and I want to say was magic is one time to rule all AI",
    "start": "1108160",
    "end": "1113760"
  },
  {
    "text": "models on any hardware so uh based on was magic we",
    "start": "1113760",
    "end": "1120400"
  },
  {
    "text": "have was um project called lage u it's um uh it's a framework to run different",
    "start": "1120400",
    "end": "1127039"
  },
  {
    "text": "types of light language models uh it's lightweight uh the was m runtime plus",
    "start": "1127039",
    "end": "1133039"
  },
  {
    "text": "the uh API server was uh API server was file is less than uh 30 uh meabytes more",
    "start": "1133039",
    "end": "1139760"
  },
  {
    "text": "importantly there is no as no dependency and there is no python u package it's",
    "start": "1139760",
    "end": "1146320"
  },
  {
    "text": "easy to set up and it's a high performance because magic will automate",
    "start": "1146320",
    "end": "1152280"
  },
  {
    "text": "automatically use the device uh use the device local hardware and software",
    "start": "1152280",
    "end": "1157480"
  },
  {
    "text": "acceleration And um uh it supports multiple models it supports light",
    "start": "1157480",
    "end": "1163520"
  },
  {
    "text": "language models it supports TTI uh ST and the image generation like stable",
    "start": "1163520",
    "end": "1169679"
  },
  {
    "text": "diffusion and flex and also the uh the vision model so we can use the one",
    "start": "1169679",
    "end": "1174799"
  },
  {
    "text": "runtime to uh to run all the models and uh it's also portable the WM file is",
    "start": "1174799",
    "end": "1181919"
  },
  {
    "text": "portable and the W runtime is portable you can only write one build once and",
    "start": "1181919",
    "end": "1187440"
  },
  {
    "text": "run it on any hardares so here is the uh the link uh the link of LA the docs link",
    "start": "1187440",
    "end": "1195679"
  },
  {
    "text": "of LA you if you're interesting you can go to this link yeah so if I may add you",
    "start": "1195679",
    "end": "1201760"
  },
  {
    "text": "know this um portability is um uh a key feature of WASM and it's also really",
    "start": "1201760",
    "end": "1207840"
  },
  {
    "text": "important for us because if you look at GPU pricing on uh on the cloud you know",
    "start": "1207840",
    "end": "1213280"
  },
  {
    "text": "a lot of those uh you know they are by far the most expensive thing you can buy as compute you know so cost thousands of",
    "start": "1213280",
    "end": "1220320"
  },
  {
    "text": "dollars a month however you know one way to really uh uh save money is to buy",
    "start": "1220320",
    "end": "1226080"
  },
  {
    "text": "from the from the spot market right you know when as those GPUs become available however uh in the spot market especially",
    "start": "1226080",
    "end": "1233440"
  },
  {
    "text": "on lot there are lots of marketplaces you get GPUs even if it's all Nvidia you",
    "start": "1233440",
    "end": "1238480"
  },
  {
    "text": "sometime get different types of GPUs you sometime get different drivers you know it could be koda koda 11 or kuda 12 it",
    "start": "1238480",
    "end": "1245120"
  },
  {
    "text": "just depend on the time of day you know you get a different type of device and uh so to have a runtime that's can be",
    "start": "1245120",
    "end": "1252400"
  },
  {
    "text": "entirely portable and agnostic to those runtime your application sees all the underlying GPUs whether AMD or Huawei or",
    "start": "1252400",
    "end": "1259919"
  },
  {
    "text": "CUDA or the Mac exactly the same there's no binary change at any level you know so it allows us to build a system that",
    "start": "1259919",
    "end": "1267600"
  },
  {
    "text": "is uh that is more compatible with uh the vision of cloud native right you know so we have a we have a Kubernetes",
    "start": "1267600",
    "end": "1273600"
  },
  {
    "text": "cluster that's uh that that monitors the spot prices and uh if AMD prices become",
    "start": "1273600",
    "end": "1278799"
  },
  {
    "text": "low we just buy AMD and use that to run the large language model or to run whisper so everything um except the",
    "start": "1278799",
    "end": "1286640"
  },
  {
    "text": "orchestration piece all the workload is running on WASM so you know so I think that's one of the um more interesting",
    "start": "1286640",
    "end": "1292720"
  },
  {
    "text": "thing because uh you can't do this with Docker because um you know Docker has no",
    "start": "1292720",
    "end": "1298000"
  },
  {
    "text": "visibility to the GPUs right you you have to have the GPU driver installed into the Docker host in order to do in",
    "start": "1298000",
    "end": "1304320"
  },
  {
    "text": "order to do things like that right but with W wasam it's uh like we said it's you know get a new machine download a",
    "start": "1304320",
    "end": "1310080"
  },
  {
    "text": "certain megabytes package and uh um nothing else to install and then download the WAM binary application and",
    "start": "1310080",
    "end": "1316480"
  },
  {
    "text": "then start to go right so I just want to add this portability piece is really important for us operationally you know",
    "start": "1316480",
    "end": "1322480"
  },
  {
    "text": "to provide this service because we are open source project we uh the company raised some money but it's you know we",
    "start": "1322480",
    "end": "1328799"
  },
  {
    "text": "we can't afford the GPUs you know that's so so we buy GPUs from the spot market yeah go ahead",
    "start": "1328799",
    "end": "1336640"
  },
  {
    "text": "so we will do another uh demo on running um running whisper on your own machine",
    "start": "1336799",
    "end": "1343679"
  },
  {
    "text": "so let's go to my terminal",
    "start": "1343679",
    "end": "1349080"
  },
  {
    "text": "so to uh to run the W model the first thing is that we need to uh download",
    "start": "1361039",
    "end": "1366240"
  },
  {
    "text": "some uh the was my runtime so so this is I download the was my",
    "start": "1366240",
    "end": "1375240"
  },
  {
    "text": "runtime and then uh we will need to download the uh the whisper plugin uh",
    "start": "1375240",
    "end": "1381200"
  },
  {
    "text": "from was mag because we didn't optimize the installer so uh you need to do it",
    "start": "1381200",
    "end": "1387760"
  },
  {
    "text": "manually but we have a uh we have a RFX mentorship uh project are working on",
    "start": "1387760",
    "end": "1395000"
  },
  {
    "text": "this and then we need to download the wave the whisper API server uh file it's",
    "start": "1395000",
    "end": "1401600"
  },
  {
    "text": "a WM file uh you can see it's only uh",
    "start": "1401600",
    "end": "1406960"
  },
  {
    "text": "it's it's less than 4 megabytes and then we need to download the uh the W per",
    "start": "1406960",
    "end": "1413720"
  },
  {
    "text": "model and the model is huge the model is like 1.4 bit uh gigabytes right so if you",
    "start": "1413720",
    "end": "1421840"
  },
  {
    "text": "look at the API server the API server is a WM file so written written in Rust it's already compiled into WASM so what",
    "start": "1421840",
    "end": "1428400"
  },
  {
    "text": "it does is provide HTTP server it provides four open eye compatible API server and on the other end it runs",
    "start": "1428400",
    "end": "1434799"
  },
  {
    "text": "inference uh on this model so all all this thing together everything together is 3 megabytes right you know just",
    "start": "1434799",
    "end": "1441360"
  },
  {
    "text": "imagine if you want to use docker with python you know the pietorch docker image is 4 gigabytes okay so you know so",
    "start": "1441360",
    "end": "1448480"
  },
  {
    "text": "if you want to use docker to run and and pytorch to run this you are looking at three order magnitude bigger in",
    "start": "1448480",
    "end": "1454240"
  },
  {
    "text": "footprint yeah and uh there is a tip so if you want to run whisper model you should use",
    "start": "1454240",
    "end": "1461360"
  },
  {
    "text": "version two not the version three because version three is is not",
    "start": "1461360",
    "end": "1468320"
  },
  {
    "text": "Then we can use uh this command line to run the uh the whisper model and we",
    "start": "1468720",
    "end": "1474880"
  },
  {
    "text": "already have the uh the whisper the runtime and the whisper model and the",
    "start": "1474880",
    "end": "1480000"
  },
  {
    "text": "API server so let's do it so uh the the whisper uh API server",
    "start": "1480000",
    "end": "1489440"
  },
  {
    "text": "uh has started is listening on the 80 uh 880 port so I will do a little",
    "start": "1489440",
    "end": "1496360"
  },
  {
    "text": "test so uh this is um uh this is a very",
    "start": "1496360",
    "end": "1501840"
  },
  {
    "text": "simple uh simple uh wh file so we can",
    "start": "1501840",
    "end": "1507840"
  },
  {
    "text": "ask whisper to transcribe this file u by the way as Michael um",
    "start": "1507840",
    "end": "1513840"
  },
  {
    "text": "mentioned this um uh the API uh the the API format is open compatible so you you",
    "start": "1513840",
    "end": "1521760"
  },
  {
    "text": "you just uh you just need to um to to learn um one uh so you just need to",
    "start": "1521760",
    "end": "1528720"
  },
  {
    "text": "follow open",
    "start": "1528720",
    "end": "1531278"
  },
  {
    "text": "eye so this is the output from whisper",
    "start": "1535080",
    "end": "1540240"
  },
  {
    "text": "this is the test record for u whisper whisper.cpp actually we can we can we",
    "start": "1540240",
    "end": "1547200"
  },
  {
    "text": "can hear the the the test wav fail",
    "start": "1547200",
    "end": "1554760"
  },
  {
    "text": "this is a test record for whisperc so as you can see the whisper",
    "start": "1576880",
    "end": "1584799"
  },
  {
    "text": "AP serum this this",
    "start": "1584799",
    "end": "1590158"
  },
  {
    "text": "text okay I have show this and um uh we have talk about is web",
    "start": "1594039",
    "end": "1601760"
  },
  {
    "text": "assembly enterprise uh ready uh uh uh yesterday so I want to share um an",
    "start": "1601760",
    "end": "1608799"
  },
  {
    "text": "enterprise use case um from video",
    "start": "1608799",
    "end": "1613840"
  },
  {
    "text": "langu based AI agent so we have um one uh there is one of our um customer it's",
    "start": "1613880",
    "end": "1622960"
  },
  {
    "text": "the it's a short video platform in US uh it's called clapper it use was m and um",
    "start": "1622960",
    "end": "1630000"
  },
  {
    "text": "kubernetes to deploy the u the inference workloads uh it's it's use lag to run",
    "start": "1630000",
    "end": "1638240"
  },
  {
    "text": "the wer model oh sorry I should add more context so they are using was magic to",
    "start": "1638240",
    "end": "1645200"
  },
  {
    "text": "was magic wer model and um um Llama 3.18 billion model to add the caption to",
    "start": "1645200",
    "end": "1652480"
  },
  {
    "text": "their uh to their video and uh uh add a summarization for the for each video and",
    "start": "1652480",
    "end": "1658960"
  },
  {
    "text": "because the uh the platform um each day the platform uh the uh the users will",
    "start": "1658960",
    "end": "1666760"
  },
  {
    "text": "upload 400,000 videos to this platform so if they using um open eye uh it will",
    "start": "1666760",
    "end": "1674799"
  },
  {
    "text": "be very expensive so they are using uh so the price uh in comparison price we",
    "start": "1674799",
    "end": "1680000"
  },
  {
    "text": "got from openi was $80,000 per day right you know that's the the the price they quoted us so running this I think we are",
    "start": "1680000",
    "end": "1688799"
  },
  {
    "text": "$100 a day something like that right yeah they yeah yes so uh so they choose",
    "start": "1688799",
    "end": "1696320"
  },
  {
    "text": "uh they choose to uh self-host the solution with their um um",
    "start": "1696320",
    "end": "1702760"
  },
  {
    "text": "uh with magic and uh and the Kubernetes so what's magic is um uh is runtime to",
    "start": "1702760",
    "end": "1711600"
  },
  {
    "text": "run um to run different models like whisper and llama 3.1 model and u",
    "start": "1711600",
    "end": "1717919"
  },
  {
    "text": "kubernet since the water is uh is cloud ready uh it use kubernetes to",
    "start": "1717919",
    "end": "1724399"
  },
  {
    "text": "autoscaling um the service if the traffic ses let's",
    "start": "1724399",
    "end": "1730000"
  },
  {
    "text": "say if the tik tok is shut down this video platform will have much more traffic than euro So yeah for the",
    "start": "1730000",
    "end": "1738399"
  },
  {
    "text": "Americans here you know when the uh when the Tik Tok was about to shut down this application become the number one in the",
    "start": "1738399",
    "end": "1743600"
  },
  {
    "text": "in the US app store you know it's uh you know we all know that there's another app called uh a little red book who who",
    "start": "1743600",
    "end": "1750080"
  },
  {
    "text": "was number one for a while but this one was too you know so uh at that time there's a huge surge of uh traffic and",
    "start": "1750080",
    "end": "1756720"
  },
  {
    "text": "it's normally you know app like that has a very clear traffic pattern that's also one of the things I thought was very",
    "start": "1756720",
    "end": "1762399"
  },
  {
    "text": "interesting is that uh um it has periods of the day especially uh daytime during",
    "start": "1762399",
    "end": "1768240"
  },
  {
    "text": "the US it has peak traffic so you really need to provision you more servers at that time so that goes again to my",
    "start": "1768240",
    "end": "1774799"
  },
  {
    "text": "earlier point about buying spot instances of those GPUs right you know so when there's a huge demand we have to",
    "start": "1774799",
    "end": "1780960"
  },
  {
    "text": "buy more spot instances and uh we get those instances from different places different hardware different drivers so",
    "start": "1780960",
    "end": "1787440"
  },
  {
    "text": "a crossplatform runtime becomes really important to manage this otherwise I would have to if you look at um",
    "start": "1787440",
    "end": "1793360"
  },
  {
    "text": "whisper.cbp's CBP's build you know they they have over a hundred different builds for each of the GPU plus driver",
    "start": "1793360",
    "end": "1800320"
  },
  {
    "text": "combination right so for Nvidia they have like 20 different builds for different versions of CUDA right so you",
    "start": "1800320",
    "end": "1805360"
  },
  {
    "text": "know that's uh but for us it's just one one version one was file yeah",
    "start": "1805360",
    "end": "1811760"
  },
  {
    "text": "okay I think uh that's all thank",
    "start": "1811760",
    "end": "1816080"
  },
  {
    "text": "Yeah uh so if you have any questions I think we still have some time",
    "start": "1823320",
    "end": "1831880"
  },
  {
    "text": "hello um thank you great talk um I'm curious to learn more um you mentioned",
    "start": "1838799",
    "end": "1844000"
  },
  {
    "text": "like you're using different kind of models and combine them together and some of them are pretty small like 3",
    "start": "1844000",
    "end": "1849440"
  },
  {
    "text": "billion 8 billion so how you usually test this model suits great for the use",
    "start": "1849440",
    "end": "1855279"
  },
  {
    "text": "case that you want to to run um so maybe I answer that so you know",
    "start": "1855279",
    "end": "1861120"
  },
  {
    "text": "it's uh so it's in the product development you have this trial and",
    "start": "1861120",
    "end": "1866320"
  },
  {
    "text": "error right you know so um we have the so that's a great thing about open source model because we can choose a lot",
    "start": "1866320",
    "end": "1871760"
  },
  {
    "text": "of them you know so um uh so we choose the smallest model that matches what we",
    "start": "1871760",
    "end": "1877840"
  },
  {
    "text": "need right you know so if we can run 3B model we would never run eight uh 7B model you know that's a so one of the",
    "start": "1877840",
    "end": "1884559"
  },
  {
    "text": "interesting thing is that you know the for instance the Korean language model was just released a couple days ago it",
    "start": "1884559",
    "end": "1890080"
  },
  {
    "text": "was one of the um you know a lot of excitement in the Korean market you know because um that model is based on",
    "start": "1890080",
    "end": "1896320"
  },
  {
    "text": "Chenwin but it's fine-tuned with a lot of Korean data right before that we were using chairman to generate Korean And uh",
    "start": "1896320",
    "end": "1902320"
  },
  {
    "text": "that's requires a lot more post-processing because it would have Chinese and English mixed in there because it's just not that well",
    "start": "1902320",
    "end": "1908559"
  },
  {
    "text": "fine-tuned for the Korean language but since we changed to this model all the proc postprocessing goes away you know",
    "start": "1908559",
    "end": "1913919"
  },
  {
    "text": "because you know it just to generate correct form in Korean sentences you know uh at every turn so yeah that's uh",
    "start": "1913919",
    "end": "1920159"
  },
  {
    "text": "you know so that's what I thought you know uh people say uh would the product like this go away when the model become",
    "start": "1920159",
    "end": "1926480"
  },
  {
    "text": "agi i would say yes you know when open eye figured out how to do this perfectly",
    "start": "1926480",
    "end": "1931600"
  },
  {
    "text": "at every turn then you know we would no longer exist but for now we still you",
    "start": "1931600",
    "end": "1937279"
  },
  {
    "text": "know the market still need us you know cool thank",
    "start": "1937279",
    "end": "1942799"
  },
  {
    "text": "you okay thank you",
    "start": "1951880",
    "end": "1958360"
  }
]