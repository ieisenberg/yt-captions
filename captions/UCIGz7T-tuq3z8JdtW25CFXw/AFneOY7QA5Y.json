[
  {
    "text": "[Music]",
    "start": "290",
    "end": "12799"
  },
  {
    "text": "all right um so hello everyone um Welcome to our session on privacy first",
    "start": "12799",
    "end": "19000"
  },
  {
    "text": "building LM powered uh web apps with client side web assembly uh my name is San par I'm working as principal",
    "start": "19000",
    "end": "25119"
  },
  {
    "text": "developer Advocate at lobs Loft labs and I'm joined by I'm Shai so I work at as a",
    "start": "25119",
    "end": "30560"
  },
  {
    "text": "developer relations engineer or developer evangelist at couchbase and I'm also a core contributor to the vet",
    "start": "30560",
    "end": "37680"
  },
  {
    "text": "project awesome uh so let's get things started uh set the context first uh with",
    "start": "37680",
    "end": "43440"
  },
  {
    "text": "the AI ecosystem uh so it's it's basically not something which is very new so if you have been searching on",
    "start": "43440",
    "end": "50079"
  },
  {
    "text": "Google when you type uh what is then it tries to give you some search results which are there uh so means the",
    "start": "50079",
    "end": "57399"
  },
  {
    "text": "prediction of what comes next as the word or what you might be searching based on your geographical location and",
    "start": "57399",
    "end": "63800"
  },
  {
    "text": "what not stuff it has already been providing you certain stuff um and you",
    "start": "63800",
    "end": "69560"
  },
  {
    "text": "you know booking airbnbs uh watching movies uh getting recommendations for that everything in our day-to-day lives",
    "start": "69560",
    "end": "76720"
  },
  {
    "text": "was already kind of having this but uh for everyone it got mainstream for",
    "start": "76720",
    "end": "82720"
  },
  {
    "text": "everyone building application around AI the whole funding ecosystem around AI boomed when Chad GPT came into uh",
    "start": "82720",
    "end": "89600"
  },
  {
    "text": "picture because it kind of became a next Revolution where a person whether in",
    "start": "89600",
    "end": "95159"
  },
  {
    "text": "Tech or in non-tech everyone just knows about chat GPD or AI which is a very big",
    "start": "95159",
    "end": "101159"
  },
  {
    "text": "thing so that's how the kind of I term it as a uh you know the AI revion that",
    "start": "101159",
    "end": "106560"
  },
  {
    "text": "started after but before that um when the GPT model was there and it was not",
    "start": "106560",
    "end": "112079"
  },
  {
    "text": "for the public the researchers were pretty excited uh at that particular point of time because they knew the",
    "start": "112079",
    "end": "117320"
  },
  {
    "text": "potential that it would be having um so right now what's the current state",
    "start": "117320",
    "end": "122560"
  },
  {
    "text": "Uh current state is that I mean we are not sleeping current state is that today",
    "start": "122560",
    "end": "128200"
  },
  {
    "text": "if we kind of go to sleep and we wake up in the morning we wake up to reality",
    "start": "128200",
    "end": "134120"
  },
  {
    "text": "there's a new model out there so that's how the current state is like uh it's the llms or the models which are",
    "start": "134120",
    "end": "142560"
  },
  {
    "text": "coming is at a much much faster Pace than anyone anticipated in the past so",
    "start": "142560",
    "end": "148120"
  },
  {
    "text": "you can see pre 2022 till 2023 you can count them but 2023 till date it's very",
    "start": "148120",
    "end": "156640"
  },
  {
    "text": "difficult to even count them and yeah we we all know some of the you know cool ones out there that gets hyped up on",
    "start": "156640",
    "end": "162959"
  },
  {
    "text": "xplatform or whatever that is like you have llama you have uh deep seek you have Falcon all these are the popular",
    "start": "162959",
    "end": "169480"
  },
  {
    "text": "models but there are a lot of other models targeted for specific task like image table diffusion research uh deep",
    "start": "169480",
    "end": "177000"
  },
  {
    "text": "research and all that stuff so that's the current state which is uh very crazy out there in the AI",
    "start": "177000",
    "end": "182720"
  },
  {
    "text": "ecosystem and also how you usually use like today most common way of consuming the generative AI that we call is via",
    "start": "182720",
    "end": "190080"
  },
  {
    "text": "the web so we have all these web applications and we are the end users use these web applications to do our",
    "start": "190080",
    "end": "195920"
  },
  {
    "text": "day-to-day task and interact with that how do we build these applications building these applications is is easy",
    "start": "195920",
    "end": "202879"
  },
  {
    "text": "right because you have apis you can call those apis with whatever programming language uh of your choice so you can",
    "start": "202879",
    "end": "209959"
  },
  {
    "text": "see uh you have JavaScript you can just import uh and give the model name uh gp4",
    "start": "209959",
    "end": "215120"
  },
  {
    "text": "or you can have Python and you can give uh deep seek whatever that is you will be able to connect to the model and get",
    "start": "215120",
    "end": "223040"
  },
  {
    "text": "uh the response so like a user writes the application and uh uh goes to the",
    "start": "223040",
    "end": "228920"
  },
  {
    "text": "model and the response comes back in but there's a lot happening in that particular space in both the spaces one",
    "start": "228920",
    "end": "236120"
  },
  {
    "text": "writing the application two uh how the model is kind of doing so how the model is doing we'll talk a a bit later let's",
    "start": "236120",
    "end": "243840"
  },
  {
    "text": "talk about the application first and also what about privacy because the talk",
    "start": "243840",
    "end": "251480"
  },
  {
    "text": "is about that we have to talk about it right so what about privacy since we are calling these models which are available",
    "start": "251480",
    "end": "257479"
  },
  {
    "text": "online that are very powerful that gives you that solves a particular task gives you a lot of good information sometimes",
    "start": "257479",
    "end": "263320"
  },
  {
    "text": "hallucinates uh Hallucination is also just a model error because it's",
    "start": "263320",
    "end": "268639"
  },
  {
    "text": "everything is based on Metric multiplication and Mathematics there's no like human only makes hallucinations",
    "start": "268639",
    "end": "274479"
  },
  {
    "text": "so it's it's a user that passes data files images sensitive information but",
    "start": "274479",
    "end": "280400"
  },
  {
    "text": "is it suitable for very critical Industries is it suitable for healthcare where you have patients the data is",
    "start": "280400",
    "end": "286120"
  },
  {
    "text": "there very critical military military Secrets yes it might solve some of the",
    "start": "286120",
    "end": "291280"
  },
  {
    "text": "problems in that space But is it worth it compromising the military Secrets",
    "start": "291280",
    "end": "296400"
  },
  {
    "text": "over there and sensitive information secrets and stuff why I can say that uh this is scary because of all these news",
    "start": "296400",
    "end": "305039"
  },
  {
    "text": "headlines that are there so you can see Samsung uh meeting notes and source code were leaked with Chad GPD Chad GPD was",
    "start": "305039",
    "end": "312280"
  },
  {
    "text": "offline because people were able to see their each other's chats uh grock uh for",
    "start": "312280",
    "end": "319319"
  },
  {
    "text": "misusing the AI data deep seek since the time it has come people have been saying that it uses anything that you just give",
    "start": "319319",
    "end": "325720"
  },
  {
    "text": "it for retraining and it happens right um nobody tells you that whatever you",
    "start": "325720",
    "end": "330880"
  },
  {
    "text": "are sending the data you are sending they would not use it to make improve the model or improve what they are",
    "start": "330880",
    "end": "337360"
  },
  {
    "text": "predicting so this is concerning so one thing is the Privacy issue of using all",
    "start": "337360",
    "end": "343240"
  },
  {
    "text": "these available things uh the next thing is the cost so what about the costs which are there uh we should not forget",
    "start": "343240",
    "end": "349880"
  },
  {
    "text": "about the cost because the output per million token uh price for every model is like it's very expensive when you do",
    "start": "349880",
    "end": "357560"
  },
  {
    "text": "it from the apis so cost is fun Challenge and then you know how to deploy it how to deploy it in production",
    "start": "357560",
    "end": "364000"
  },
  {
    "text": "how to deploy it on existing um infrastructure like kubernetes and stuff it's a whole new story with its own uh",
    "start": "364000",
    "end": "371199"
  },
  {
    "text": "set of best practices and stuff but uh we'll come back later that but how to build these the the question now is how",
    "start": "371199",
    "end": "377880"
  },
  {
    "text": "to how we can build this because now right now we are focusing on that application layer how we can build these",
    "start": "377880",
    "end": "382960"
  },
  {
    "text": "web apps in a secure and cost effective way so uh what to do is basically there",
    "start": "382960",
    "end": "388599"
  },
  {
    "text": "is a project called AMA there are a lot of models on hugging face so you can have Ama to get up and running with",
    "start": "388599",
    "end": "395319"
  },
  {
    "text": "models and you can have your own server your own model deployed over there with your own trust privacy that nothing is",
    "start": "395319",
    "end": "401880"
  },
  {
    "text": "going on uh to the cloud apis which are there and this acts as a backend web",
    "start": "401880",
    "end": "407919"
  },
  {
    "text": "service that you can call within your application so how to do it uh natively in the browser like how we can do all of",
    "start": "407919",
    "end": "414880"
  },
  {
    "text": "these in the browser let's take a step back and let's understand three things over here one is uh",
    "start": "414880",
    "end": "421160"
  },
  {
    "text": "webg uh so web GL is basically a uh graphical abstraction layer on top of uh",
    "start": "421160",
    "end": "426720"
  },
  {
    "text": "the GPU and that graphical language will then do the computation or the",
    "start": "426720",
    "end": "431800"
  },
  {
    "text": "calculations um on the GPU it's it works it's a bit slower because it's",
    "start": "431800",
    "end": "437080"
  },
  {
    "text": "abstraction layer on top of GPU it doesn't do it directly um so for you",
    "start": "437080",
    "end": "442599"
  },
  {
    "text": "know not optimized for parallel compute tasks in the AI ecosystem then is",
    "start": "442599",
    "end": "448160"
  },
  {
    "text": "obviously web assembly for what what everyone is here in this particular conference uh so let's try to understand",
    "start": "448160",
    "end": "453400"
  },
  {
    "text": "it's slow but the only thing here is that this works only on the CPU architecture if you only use web assembly so it's a performance- wise",
    "start": "453400",
    "end": "459840"
  },
  {
    "text": "it's it's a bit lesser than uh the webg but the execution flow is interesting so",
    "start": "459840",
    "end": "465319"
  },
  {
    "text": "there are a few steps here like if you see uh there's a model preparation uh so you can have there's a",
    "start": "465319",
    "end": "471319"
  },
  {
    "text": "there's a framework called tensorflow.js that uh Shiva will be talking in a a bit detail later on so you have these uh",
    "start": "471319",
    "end": "478840"
  },
  {
    "text": "Frameworks that convert these trained models to the browser compatible format",
    "start": "478840",
    "end": "483919"
  },
  {
    "text": "and then they are quantized uh so because you have these billion parameters uh you know 120 GB 150 GB",
    "start": "483919",
    "end": "492400"
  },
  {
    "text": "models you cannot run those in in the browser so you need something uh how we can quantize it so that you are able to",
    "start": "492400",
    "end": "498599"
  },
  {
    "text": "run these in the browsers so that's how the models are quantized and then there is the backend initialization for the uh",
    "start": "498599",
    "end": "505879"
  },
  {
    "text": "web assembly uh using again the tensorflow JS and stuff and then comes the inference space where the browser uh",
    "start": "505879",
    "end": "512080"
  },
  {
    "text": "captures the data using the Dom and using you know all the stuff over there converts that into JS and uh do the proc",
    "start": "512080",
    "end": "520200"
  },
  {
    "text": "and then again it is processed using the web assembly uh module so the model the",
    "start": "520200",
    "end": "526040"
  },
  {
    "text": "module executes the compiled uh model operations directly on the CPU so this one is on the uh CPU",
    "start": "526040",
    "end": "534480"
  },
  {
    "text": "itself interestingly we have something which is called Web GPU so when you combine web assembly with web GPU this",
    "start": "534480",
    "end": "541519"
  },
  {
    "text": "becomes powerful so you can like uh web GPU is kind of a successor or you can say now most widely used than webgl",
    "start": "541519",
    "end": "549519"
  },
  {
    "text": "because it runs directly um uh it is able to do the parallel compute on the",
    "start": "549519",
    "end": "554640"
  },
  {
    "text": "gpus itself so web GPU and web assembly becomes a very good uh combination where",
    "start": "554640",
    "end": "559920"
  },
  {
    "text": "web assembly handles the logic data uh the pre-processing and web GPU provides the lower level GPU access for parallel",
    "start": "559920",
    "end": "566839"
  },
  {
    "text": "Computing um so web assembly will be PR processing the data web GPU um is",
    "start": "566839",
    "end": "572320"
  },
  {
    "text": "executing the compute for ML inference and the results are then rendered via the webg canvas um so this combination",
    "start": "572320",
    "end": "580360"
  },
  {
    "text": "is pretty interesting for heavy kind of application like if you if you have the applications where you are uh you are",
    "start": "580360",
    "end": "586800"
  },
  {
    "text": "having a camera you are inputting images or you are continuously seeing from the camera taking the frames and analyzing",
    "start": "586800",
    "end": "593279"
  },
  {
    "text": "it on the fly so this works really well in that high performance um you know giving you the high performance uh",
    "start": "593279",
    "end": "599720"
  },
  {
    "text": "output and also it opens up new possibilities of doing web assembly at the edge uh but we have so many things",
    "start": "599720",
    "end": "607279"
  },
  {
    "text": "um which are there uh so we have uh the we have webg we have web assembly we",
    "start": "607279",
    "end": "613600"
  },
  {
    "text": "know the privacy and the cost things that those those two problems which are there um there are so many devices which",
    "start": "613600",
    "end": "621079"
  },
  {
    "text": "are there uh you have CPU only you have GPU uh so how to select from a particular back end uh on a user's",
    "start": "621079",
    "end": "628839"
  },
  {
    "text": "device um so let's try to understand in a bit deeper way um on how how it works yeah",
    "start": "628839",
    "end": "635360"
  },
  {
    "text": "so as Sam kind of mentioned that um you know we are talking about having these different type of backends and when we",
    "start": "635360",
    "end": "641880"
  },
  {
    "text": "say a backend it's not like a server right we are talking about the supported backends that are running inside of your",
    "start": "641880",
    "end": "647839"
  },
  {
    "text": "browser so we saw that we have the support from webg web assembly and the combination of web assembly and web GPU",
    "start": "647839",
    "end": "655079"
  },
  {
    "text": "now in order to automatically identify that what is the type of device backend",
    "start": "655079",
    "end": "660440"
  },
  {
    "text": "that should be chosen based on what environment your code is being run on based on the user because you might have",
    "start": "660440",
    "end": "667519"
  },
  {
    "text": "a user using the application on an iPad or someone might have an HP laptop that's 10 years old or someone might",
    "start": "667519",
    "end": "673800"
  },
  {
    "text": "have the latest Mac so there you can leverage this browser spec which is the web newal Network API that automatically",
    "start": "673800",
    "end": "680320"
  },
  {
    "text": "is able to select the type of backend that is the most compatible so that you",
    "start": "680320",
    "end": "685440"
  },
  {
    "text": "as the user don't have to manually change that back end now of course when you are writing your application logic",
    "start": "685440",
    "end": "691680"
  },
  {
    "text": "you should include uh the capability to select these different type of backends so that whatever model that you're",
    "start": "691680",
    "end": "698440"
  },
  {
    "text": "running uh usually it's not just magic that your machine learning model will directly work in the browser there is a",
    "start": "698440",
    "end": "705600"
  },
  {
    "text": "process that actually you have to convert the existing model that might be written in Python uh with Transformers",
    "start": "705600",
    "end": "712240"
  },
  {
    "text": "uh you need to transform that and make it compatible to run inside of the browser and usually how that happens is",
    "start": "712240",
    "end": "718040"
  },
  {
    "text": "that whatever uh application code that you have written you'll need to write custom Ops so these are machine learning",
    "start": "718040",
    "end": "724320"
  },
  {
    "text": "operations that take place whether it could be the convolutional or mult because ultimately whatever kind of",
    "start": "724320",
    "end": "730399"
  },
  {
    "text": "operations are happening in your uh machine learning background these are all mathematical operations so we need to write custom Ops supported for web",
    "start": "730399",
    "end": "737320"
  },
  {
    "text": "GPU web assembly so all of that can be handled with the help of the web neural network API and if you look at the",
    "start": "737320",
    "end": "744160"
  },
  {
    "text": "overall architecture so the entire premises that whether you have a model that's coming from pytor or tlow or even",
    "start": "744160",
    "end": "752279"
  },
  {
    "text": "onx based models all of them will be supported with the help of the web end and API and whether uh you use a backend",
    "start": "752279",
    "end": "760279"
  },
  {
    "text": "which is supported with web GPU with web assembly uh or depending on what type of device architecture you're using because",
    "start": "760279",
    "end": "766639"
  },
  {
    "text": "an Android device will use uh something else uh on mostly on iOS you have metal",
    "start": "766639",
    "end": "772639"
  },
  {
    "text": "uh then of course if you're using Linux you'll be using open Vino so depending on the type of application or the",
    "start": "772639",
    "end": "778360"
  },
  {
    "text": "platform on which you running it it will automatically be able to select it and of course uh because we are going to be",
    "start": "778360",
    "end": "783920"
  },
  {
    "text": "using if you're using a very large model we'll need some sort of Hardware accelerator so in that case you could",
    "start": "783920",
    "end": "789320"
  },
  {
    "text": "use either CPU GPU or for that matter if there are some really hyp specialized",
    "start": "789320",
    "end": "794880"
  },
  {
    "text": "Processing Unit like such such as a TPU uh then even those could also come into picture but you as the end user",
    "start": "794880",
    "end": "802240"
  },
  {
    "text": "developing these applications don't have to worry when it comes to choosing the right type of",
    "start": "802240",
    "end": "807720"
  },
  {
    "text": "model uh now of course uh we spoke about tflow JS now I didn't want to like have",
    "start": "807720",
    "end": "812839"
  },
  {
    "text": "a dedicated slide on that because it is kind of in a maintenance mode now there's no active development going on",
    "start": "812839",
    "end": "820160"
  },
  {
    "text": "when it comes to uh uh tens fls I mean there are still a lot of AI models that you can still run it's still supported a",
    "start": "820160",
    "end": "827240"
  },
  {
    "text": "lot of companies like uh Adobe and then uh you know a lot of startups are also",
    "start": "827240",
    "end": "832680"
  },
  {
    "text": "actually using uh tflow JS in production so for example uh this uh Adobe",
    "start": "832680",
    "end": "838120"
  },
  {
    "text": "Photoshop web uses a lot of machine learning models to do realtime editing of images directly into the browser or",
    "start": "838120",
    "end": "845279"
  },
  {
    "text": "if you you might have probably used Google meet uh the virtual background filter is being uh is using the media",
    "start": "845279",
    "end": "851800"
  },
  {
    "text": "pipe API so you're seeing that these are already in production serving millions",
    "start": "851800",
    "end": "856839"
  },
  {
    "text": "and potentially like billions of users throughout the entire web but the natural progression or the natural uh",
    "start": "856839",
    "end": "864320"
  },
  {
    "text": "you know transformation of the T flows library was with Transformers which is",
    "start": "864320",
    "end": "869360"
  },
  {
    "text": "the biggest open-source library today for the web uh it's of course built on top or or based on top of the uh of",
    "start": "869360",
    "end": "876920"
  },
  {
    "text": "course a lot of us might have heard of the Transformers Library so in this case what happens is that whenever there's a",
    "start": "876920",
    "end": "882639"
  },
  {
    "text": "new model that has to be ported to the web uh there'll be a machine learning model that's written in Python uh it",
    "start": "882639",
    "end": "888480"
  },
  {
    "text": "will be first converted into the Onyx format and then the Onyx format has bindings with web assembly and web GPU",
    "start": "888480",
    "end": "895480"
  },
  {
    "text": "that allows you to execute that particular code inside of your JavaScript application so the",
    "start": "895480",
    "end": "900839"
  },
  {
    "text": "transformers. JS is definitely the most capable library that is out there that all of you can also use in fact I think",
    "start": "900839",
    "end": "908120"
  },
  {
    "text": "I was just speaking with the original uh author of this particular Library called",
    "start": "908120",
    "end": "913279"
  },
  {
    "text": "Joshua and he said that there are over Bill a million plus downloads happening on a weekly basis right now and a lot of",
    "start": "913279",
    "end": "920600"
  },
  {
    "text": "companies are also using this particular library in production right now so one of the key things I wanted to",
    "start": "920600",
    "end": "926880"
  },
  {
    "text": "show was uh of course this particular uh example will be having a lot of demos",
    "start": "926880",
    "end": "932040"
  },
  {
    "text": "as well I'll show the main demonstration with when we talk about llms but before that I wanted to show one more",
    "start": "932040",
    "end": "938199"
  },
  {
    "text": "additional demo that I was already running even you without realizing so this demo was with the help of whisper",
    "start": "938199",
    "end": "943959"
  },
  {
    "text": "model how many of you have heard of The Whisper model so it's a open- source model that was released by open yes open",
    "start": "943959",
    "end": "950800"
  },
  {
    "text": "does release some of the open source models uh but it allows you to do realtime transcription of your audio and",
    "start": "950800",
    "end": "957399"
  },
  {
    "text": "it's actually one of Hands-On one of the best models that is available today in the market so in this case when you",
    "start": "957399",
    "end": "963800"
  },
  {
    "text": "compile it or when you combine it with uh transformers. JS what you do is that you load or you have of course taken a",
    "start": "963800",
    "end": "971120"
  },
  {
    "text": "base trans uh or a base whisper model and you've converted into the Onyx",
    "start": "971120",
    "end": "977120"
  },
  {
    "text": "format with the help of the transformer. JS model now what you'll do is that initially when you run this inside of",
    "start": "977120",
    "end": "983600"
  },
  {
    "text": "your web application you would download that model usually it's about 1 GB in size but once you have downloaded it uh",
    "start": "983600",
    "end": "990199"
  },
  {
    "text": "we'll use a web worker that is actually doing the inference now as I'm speaking",
    "start": "990199",
    "end": "995399"
  },
  {
    "text": "it will take the audio chunk by chunk and in each and every chunk will be converted and will be transcribed by the",
    "start": "995399",
    "end": "1002560"
  },
  {
    "text": "uh by the uh whisper model and then it will load that into my web UI so",
    "start": "1002560",
    "end": "1008279"
  },
  {
    "text": "hopefully if everything was working fine I should have already been captured a lot of this particular uh thing and you",
    "start": "1008279",
    "end": "1015959"
  },
  {
    "text": "can probably see um probably what happened was that my laptop crashed a",
    "start": "1015959",
    "end": "1021240"
  },
  {
    "text": "little but you could see that I was already running it when I had started uh this so you can probably see we talk",
    "start": "1021240",
    "end": "1026640"
  },
  {
    "text": "about developer relations and we had this kicked off our session so again I'm not saying it's 100% accurate but",
    "start": "1026640",
    "end": "1034319"
  },
  {
    "text": "because it is running a very small model our browser is very capable but it cannot run like a 70 billion parameter",
    "start": "1034319",
    "end": "1040959"
  },
  {
    "text": "model and looks like the application crashed which is okay I mean but you can",
    "start": "1040959",
    "end": "1046640"
  },
  {
    "text": "still see that uh for those folks who were there at the beginning you can probably see that yes it did a good job",
    "start": "1046640",
    "end": "1053919"
  },
  {
    "text": "uh it's just that I'm not sure how optimized it is but this is how it basically looks like so initially you",
    "start": "1053919",
    "end": "1059520"
  },
  {
    "text": "would load the model it'll take a few seconds to load it and then we can start to actually do live trans transcription",
    "start": "1059520",
    "end": "1066240"
  },
  {
    "text": "so probably a demo hey everyone I'm joining in here with say uh doing a live",
    "start": "1066240",
    "end": "1071760"
  },
  {
    "text": "transcription demo with the help of the whisper model so you can see that it's",
    "start": "1071760",
    "end": "1076799"
  },
  {
    "text": "wonderful right there's no back back in fact if I also turn off my internet which I'll probably do it later it is",
    "start": "1076799",
    "end": "1084480"
  },
  {
    "text": "doing it I'm not using a web service I'm not paying openai I'm not uh sending",
    "start": "1084480",
    "end": "1089840"
  },
  {
    "text": "this private information to anyone else so that's the beauty of being able to run these type of models on the browser",
    "start": "1089840",
    "end": "1096679"
  },
  {
    "text": "but of course we have a lot more nice demos coming up later so don't get too excited because then otherwise you'll",
    "start": "1096679",
    "end": "1103320"
  },
  {
    "text": "lose your energy but anyways so of course uh you know today's entire session was",
    "start": "1103320",
    "end": "1109360"
  },
  {
    "text": "um how can we run llms in the browser right because today all of these native",
    "start": "1109360",
    "end": "1115720"
  },
  {
    "text": "models are great but llms are a completely different Beast because they",
    "start": "1115720",
    "end": "1120799"
  },
  {
    "text": "are trained upon so much amount of data and you'll probably see that they can be anywhere between 1 GB to 500 GB in size",
    "start": "1120799",
    "end": "1128880"
  },
  {
    "text": "a lot of these models like the 40 model or if you talk about even open source models like the Deep seek model you",
    "start": "1128880",
    "end": "1134120"
  },
  {
    "text": "cannot just even run them on your uh Hardware on your laptop because they have such huge amount of weight right so",
    "start": "1134120",
    "end": "1141200"
  },
  {
    "text": "what do you do like how can we run them in a cost effective manner because we saw that if you want to have security if",
    "start": "1141200",
    "end": "1147000"
  },
  {
    "text": "you want to have uh privacy how can we run them and execute them in the browser so that's the most interesting part so",
    "start": "1147000",
    "end": "1153080"
  },
  {
    "text": "don't just leave yet so of course we spoke about llms",
    "start": "1153080",
    "end": "1159000"
  },
  {
    "text": "right uh essentially when you take some input text and convert it into the output text what's happening behind the",
    "start": "1159000",
    "end": "1165039"
  },
  {
    "text": "scenes you're tokenizing your input and the output and finally then converting it into the text now behind the scenes",
    "start": "1165039",
    "end": "1171840"
  },
  {
    "text": "we are also of course using an AI model so of course these llms are nothing but",
    "start": "1171840",
    "end": "1177240"
  },
  {
    "text": "a series of more complex neural networks now if you understand that for",
    "start": "1177240",
    "end": "1183880"
  },
  {
    "text": "example if I'm giving the input as write an essay about a GPU and the llm is",
    "start": "1183880",
    "end": "1189360"
  },
  {
    "text": "providing us that output now usually there will be a multiple different type of architectures or run times or model",
    "start": "1189360",
    "end": "1195600"
  },
  {
    "text": "run times that are capable of being able to do so now the ones that you are specifically seeing are the ones that",
    "start": "1195600",
    "end": "1202720"
  },
  {
    "text": "allow the capability to run these llms directly into the browser if we talk about non-browser environments you have",
    "start": "1202720",
    "end": "1208960"
  },
  {
    "text": "things like Transformers you have py TOS all of these different type or hugging face they allow you to run these",
    "start": "1208960",
    "end": "1215159"
  },
  {
    "text": "particular llms in your python environment but we when we talk about how they run uh exclusively in your",
    "start": "1215159",
    "end": "1222240"
  },
  {
    "text": "browser and we'll understand how that actually happens behind the scene just a short while but technically will run",
    "start": "1222240",
    "end": "1229039"
  },
  {
    "text": "them with either kasas with tflow JS or the Onyx format which I showed you with",
    "start": "1229039",
    "end": "1234400"
  },
  {
    "text": "the uh you know the whisper model so that will be using transformers. JS or if you have your own llm for example a",
    "start": "1234400",
    "end": "1241720"
  },
  {
    "text": "Gemma 2 billion parameter model you can use applications like mlc or media pipe",
    "start": "1241720",
    "end": "1246799"
  },
  {
    "text": "that can convert that particular model format to be well suited for running it directly into the browser so if we go a",
    "start": "1246799",
    "end": "1254360"
  },
  {
    "text": "bit further into understanding this from an architecture perspective so what what's happening behind the scene is",
    "start": "1254360",
    "end": "1260360"
  },
  {
    "text": "that we are using this framework called as mlc it's uh an Apache project what it",
    "start": "1260360",
    "end": "1265720"
  },
  {
    "text": "basically does is that it takes your original code that you might have written in Python and it will convert it",
    "start": "1265720",
    "end": "1272960"
  },
  {
    "text": "into the vosm and the web GPU binary so uh you'll basically compile your",
    "start": "1272960",
    "end": "1278840"
  },
  {
    "text": "original model code into web assembly binary and the web sem binary will then expose the set of web GPU apis because",
    "start": "1278840",
    "end": "1286360"
  },
  {
    "text": "webs has that correlation or that Bridge with web GPU so that it can leverage",
    "start": "1286360",
    "end": "1291400"
  },
  {
    "text": "those set of web GPU apis for doing the more harder compute because as we know that webm can only do the compute on CPU",
    "start": "1291400",
    "end": "1298520"
  },
  {
    "text": "so when it comes to more of uh the internal mathematics that's going on behind an llm that will be taken care of",
    "start": "1298520",
    "end": "1305760"
  },
  {
    "text": "with the help of the GPU now of course as say mentioned earlier that you cannot just take a 70 billion parameter model",
    "start": "1305760",
    "end": "1312600"
  },
  {
    "text": "and run it because if you think about it like something like a llama 2 model even when you have like a 7 billion parameter",
    "start": "1312600",
    "end": "1319440"
  },
  {
    "text": "model that will be 130 GB in size if you're taking like let's say a 64bit",
    "start": "1319440",
    "end": "1324480"
  },
  {
    "text": "Precision which means that you'll have to Store 130 GB in your browser which is not feasible right so usually you'll",
    "start": "1324480",
    "end": "1331039"
  },
  {
    "text": "employ a technique of converting it into a smaller format which with the help of something that we call as quantization",
    "start": "1331039",
    "end": "1337360"
  },
  {
    "text": "which essentially means that the total number of bytes that you're giving to the model uh we reducing that Precision",
    "start": "1337360",
    "end": "1343880"
  },
  {
    "text": "that means from 64 it will be like 4 bytes now while that does lead to reduction in your overall accuracy of",
    "start": "1343880",
    "end": "1351120"
  },
  {
    "text": "the model it's still decent enough that you can do a lot of basic tasks such as",
    "start": "1351120",
    "end": "1356880"
  },
  {
    "text": "summarization text generation that we'll see shortly or even use it for applications like retrieval augmented",
    "start": "1356880",
    "end": "1363200"
  },
  {
    "text": "generation so in this case what we are going to be doing is we are going to be using the mlc framework where we will",
    "start": "1363200",
    "end": "1370360"
  },
  {
    "text": "take the original uh model that you have taken a quantied model converted into",
    "start": "1370360",
    "end": "1376240"
  },
  {
    "text": "web assembly and web GPU and then we'll have that model available to us with the",
    "start": "1376240",
    "end": "1381279"
  },
  {
    "text": "help of the apis that are provided by mlc or media pipe that you can now start to use in your application so here is an",
    "start": "1381279",
    "end": "1388039"
  },
  {
    "text": "example uh of course you can try out what we call as the web llm that's uh the framework that allows you to convert",
    "start": "1388039",
    "end": "1394880"
  },
  {
    "text": "the model into that format it's very easy you start by creating the mlc",
    "start": "1394880",
    "end": "1399919"
  },
  {
    "text": "engine inside of your web application and then you define what model you're using and you'll see that it has a open",
    "start": "1399919",
    "end": "1406960"
  },
  {
    "text": "AI compliant architecture so if you might have used something like ch. completions API it's very similar right",
    "start": "1406960",
    "end": "1412960"
  },
  {
    "text": "we are not changing the developer experience at all when it comes to interacting with these llm based",
    "start": "1412960",
    "end": "1418640"
  },
  {
    "text": "applications but of course the magic is that this llm is completely running in the browser thanks to web GPU and web",
    "start": "1418640",
    "end": "1426320"
  },
  {
    "text": "assembly and in fact you can also bring your own model so web llm or mlc allows",
    "start": "1426320",
    "end": "1432320"
  },
  {
    "text": "you to convert your own model so if you don't want to use like a Gemma model you can also use something like a Lama 2",
    "start": "1432320",
    "end": "1438640"
  },
  {
    "text": "model llama 3 Model that's optimized to run in the browser and mlc will help you",
    "start": "1438640",
    "end": "1443760"
  },
  {
    "text": "to convert it into the format that runs in the browser and you can execute it now let's take a look of inside of a",
    "start": "1443760",
    "end": "1451240"
  },
  {
    "text": "real life application right and I also just want to be uh in short of the time but uh let's go into this application so",
    "start": "1451240",
    "end": "1458679"
  },
  {
    "text": "uh I work at cloudspace it's a uh Vector so it's a no SQL company but we also",
    "start": "1458679",
    "end": "1463840"
  },
  {
    "text": "have Vector search right so I do a lot of day in day out work with rag or with just called as retrieval augmented",
    "start": "1463840",
    "end": "1469520"
  },
  {
    "text": "generation which means that if you uh if your llm has not been trained upon some private data you can still actually uh",
    "start": "1469520",
    "end": "1477399"
  },
  {
    "text": "augment it with some external knowledge that usually comes from retrieving it with appropriate Knowledge from the",
    "start": "1477399",
    "end": "1483039"
  },
  {
    "text": "vector store so in this case uh the application is I'll upload a PDF document and it will chunk the PDF",
    "start": "1483039",
    "end": "1490480"
  },
  {
    "text": "document generate embeddings and store them directly inside of my browser and then it will use the Gemma model uh to",
    "start": "1490480",
    "end": "1497399"
  },
  {
    "text": "do the augmentation and gener generate the result and then I also Show You by also turning off my Wi-Fi that it",
    "start": "1497399",
    "end": "1504080"
  },
  {
    "text": "actually is working so in this case I'll just upload a PDF document in this case I'll upload a",
    "start": "1504080",
    "end": "1511559"
  },
  {
    "text": "document uh from my previous presentation that I gave a couple of weeks back so in this case what I did",
    "start": "1511559",
    "end": "1517240"
  },
  {
    "text": "was that automatically I had a PDF reader or a JS PDF reader that automatically extracted all of the text",
    "start": "1517240",
    "end": "1523880"
  },
  {
    "text": "from my PDF document uh another thing I want to show is I'll probably retry this",
    "start": "1523880",
    "end": "1528919"
  },
  {
    "text": "because I want to show you what's going on behind the scenes because that's like one of the key things to basically cover",
    "start": "1528919",
    "end": "1534320"
  },
  {
    "text": "so I'll again let the model load and I'll choose that file again now what",
    "start": "1534320",
    "end": "1540320"
  },
  {
    "text": "you're going to be seeing is that it says added 220 entries in three chunks because I have a on device Vector",
    "start": "1540320",
    "end": "1547200"
  },
  {
    "text": "database that's basically running here uh that's going to be using this zenova",
    "start": "1547200",
    "end": "1552279"
  },
  {
    "text": "all mini L6 embedding model and that's also completely running in the browser so it's not only extract all of the text",
    "start": "1552279",
    "end": "1559320"
  },
  {
    "text": "converting it into the embeddings and storing it inside of the indexed DB that's a local DB that's running inside",
    "start": "1559320",
    "end": "1565240"
  },
  {
    "text": "of the browser now once I've done that I can go ahead and ask a question about this particular uh about this particular",
    "start": "1565240",
    "end": "1573000"
  },
  {
    "text": "uh PDF so let's say I ask it what is flight which is the context of this",
    "start": "1573000",
    "end": "1579440"
  },
  {
    "text": "particular uh document so it says according to the document flight is a",
    "start": "1579440",
    "end": "1584840"
  },
  {
    "text": "tool okay I mean not probably a very good answer but we can probably ask it",
    "start": "1584840",
    "end": "1590000"
  },
  {
    "text": "uh what is uh prompt enging right or probably if the audience wants to ask a question we can do like a live demo like",
    "start": "1590000",
    "end": "1596480"
  },
  {
    "text": "we have a bunch of resources over here if you want to ask some question related to the PDF probably let's let's go to a random",
    "start": "1596480",
    "end": "1604480"
  },
  {
    "text": "section and try that where we have a I think a decent amount of uh you know any",
    "start": "1604480",
    "end": "1611080"
  },
  {
    "text": "suggestions anyone wants to uh we can probably ask it uh let's",
    "start": "1611080",
    "end": "1617120"
  },
  {
    "text": "see um probably what is prompt tuning you can probably ask it what is prompt ining yeah yeah okay let's ask it what",
    "start": "1617120",
    "end": "1625399"
  },
  {
    "text": "is prompt",
    "start": "1625399",
    "end": "1628520"
  },
  {
    "text": "enging so I mean it gives you the idea and what you're what you're also seeing",
    "start": "1632600",
    "end": "1637840"
  },
  {
    "text": "is that it's giving you the sources so the idea behind drag is that when we",
    "start": "1637840",
    "end": "1643159"
  },
  {
    "text": "perform the vector search it finds the most relevant set of or semantically relevant set of docum ments that are",
    "start": "1643159",
    "end": "1648760"
  },
  {
    "text": "there so in this case I have the source and it automatically goes to that particular source so in this case",
    "start": "1648760",
    "end": "1654000"
  },
  {
    "text": "because it performed the vector search and it also generated the result now I can also turn off the internet and",
    "start": "1654000",
    "end": "1659440"
  },
  {
    "text": "you'll see that it will still execute so I can ask it a new question so I can probably ask it again what is flight",
    "start": "1659440",
    "end": "1665840"
  },
  {
    "text": "right so let's see yeah perfect so this time it says",
    "start": "1665840",
    "end": "1672039"
  },
  {
    "text": "that it's used to refer to a system for machine learning model orchestration and you can see that it is completely",
    "start": "1672039",
    "end": "1677519"
  },
  {
    "text": "running in an offline manner because the the reason behind that is if you look at the network tab if I go over here and",
    "start": "1677519",
    "end": "1684919"
  },
  {
    "text": "look at my network Tab and probably reload the model you'll see that there is this ort",
    "start": "1684919",
    "end": "1693640"
  },
  {
    "text": "wasum simd right so that's the actual uh web Sly module for my llm and when I",
    "start": "1693640",
    "end": "1700080"
  },
  {
    "text": "start to execute it uses the web GPU behind the scenes to do the execution and give you the inference that's",
    "start": "1700080",
    "end": "1706799"
  },
  {
    "text": "happening again in a completely offline manner so that's one of the key aspects and if I go directly to my code base I",
    "start": "1706799",
    "end": "1713640"
  },
  {
    "text": "can probably show you um the magic that's happening over here so by the way like uh this particular code base will",
    "start": "1713640",
    "end": "1719559"
  },
  {
    "text": "be open sourced so if you want to have discussion I I know we are running slightly out of time but I would uh be",
    "start": "1719559",
    "end": "1726679"
  },
  {
    "text": "happy to share the code base with you but trust me like it's working as you saw right so I'll go and turn my",
    "start": "1726679",
    "end": "1732760"
  },
  {
    "text": "internet back up again um but that was a quick thing about the web GPU and how",
    "start": "1732760",
    "end": "1738320"
  },
  {
    "text": "you use it for doing on device inference now another great thing is that a lot of the browsers are coming up with their",
    "start": "1738320",
    "end": "1744679"
  },
  {
    "text": "own set of API and tooling that allows you to do llm inferencing so Chrome uh",
    "start": "1744679",
    "end": "1749880"
  },
  {
    "text": "they last year announced the Gemini Nano that can run completely in your browser and in fact they provide a set of",
    "start": "1749880",
    "end": "1756080"
  },
  {
    "text": "different apis that you can use trans like doing real-time translation directly in your browser prompt apis so",
    "start": "1756080",
    "end": "1763120"
  },
  {
    "text": "this works well if you want to use the native web app web apis and also then",
    "start": "1763120",
    "end": "1768360"
  },
  {
    "text": "combine them very well with the API that are provided uh to you with the help of uh these models in fact like Mozilla",
    "start": "1768360",
    "end": "1774880"
  },
  {
    "text": "also just a couple of months back has added support for uh transformers. JS to",
    "start": "1774880",
    "end": "1779919"
  },
  {
    "text": "run AI directly inside of the extensions within Mozilla",
    "start": "1779919",
    "end": "1785120"
  },
  {
    "text": "Firefox um in fact you can also run eii agents because you can run llms so we",
    "start": "1785120",
    "end": "1791559"
  },
  {
    "text": "don't have enough time but I'll recommend uh this was supposed to be a demo where uh Jason ma who is one of the",
    "start": "1791559",
    "end": "1797720"
  },
  {
    "text": "web AI leads at Google he built this agent Tech application where he's using tool calling to generate a Spotify",
    "start": "1797720",
    "end": "1804120"
  },
  {
    "text": "playlist on the Fly based on your mood so you define your mod to the application and it runs it completely",
    "start": "1804120",
    "end": "1810279"
  },
  {
    "text": "again in inside of your browser you just provide your Spotify API key and it it does that for you so",
    "start": "1810279",
    "end": "1818080"
  },
  {
    "text": "probably say if you want to talk about Are We There Yet yeah so um yeah it's it's basically we have models but we",
    "start": "1818080",
    "end": "1826000"
  },
  {
    "text": "need to quantize them if we want to run in the browsers so smaller more efficient models with similar results uh",
    "start": "1826000",
    "end": "1832080"
  },
  {
    "text": "Hardware is something that uh obviously we want it to be run in a private mode uh the Privacy first and which is where",
    "start": "1832080",
    "end": "1839559"
  },
  {
    "text": "affordable Hardware will continue to improve so we'll have more and more uh chips we have more and more",
    "start": "1839559",
    "end": "1844919"
  },
  {
    "text": "announcements from Nvidia and different organizations on how they are coming up with uh better gpus better accelerators",
    "start": "1844919",
    "end": "1852480"
  },
  {
    "text": "within the hardware day-to-day Hardware that you are using and then access to AI",
    "start": "1852480",
    "end": "1857679"
  },
  {
    "text": "op ised Hardware web NN like um uh he was showing in one of the diagrams where you have webn that automatically does um",
    "start": "1857679",
    "end": "1864440"
  },
  {
    "text": "you know whether you are using web GPU web assembly combination and stuff like that and yeah AI is AI is evolving",
    "start": "1864440",
    "end": "1871159"
  },
  {
    "text": "Hardware is evolving and we'll see more and more of these things and I think the idea of this particular VM IO is Are We",
    "start": "1871159",
    "end": "1879519"
  },
  {
    "text": "There Yet like is web assembly ready for production the first session if you remember from day one uh is it ready for",
    "start": "1879519",
    "end": "1885760"
  },
  {
    "text": "production so I think the app appliation that was shown makes sense because it's",
    "start": "1885760",
    "end": "1890960"
  },
  {
    "text": "the application that matters behind the scenes yes it is running web assembly and it is using that uh architectures",
    "start": "1890960",
    "end": "1896519"
  },
  {
    "text": "and stuff which is good that's why it is behaving in that manner but for the end users for selling for the Enterprises",
    "start": "1896519",
    "end": "1903200"
  },
  {
    "text": "it's the application that needs to be sold rather than the technology now because now everyone is aware that yes",
    "start": "1903200",
    "end": "1910480"
  },
  {
    "text": "web assembly is a solid technology it works and we have the practitioners in the Enterprises who are uh experts",
    "start": "1910480",
    "end": "1917039"
  },
  {
    "text": "enough to use use web assembly in their organization so a and the briser A and the browsers",
    "start": "1917039",
    "end": "1923919"
  },
  {
    "text": "yeah the benefits uh are already there like no drivers no installs data stays on the client and U scales indefinitely",
    "start": "1923919",
    "end": "1931279"
  },
  {
    "text": "yeah so I mean you you saw that with my code I didn't have to pay openi I didn't have to worry about my privacy I didn't",
    "start": "1931279",
    "end": "1938120"
  },
  {
    "text": "have to uh you know think about privacy or latency at all because everything was running inside of the browser so that is",
    "start": "1938120",
    "end": "1945240"
  },
  {
    "text": "why like I have been very bullish about uh AI capabilities running in the browser ever since 2020 when I first",
    "start": "1945240",
    "end": "1951880"
  },
  {
    "text": "took part my my hackathon and I literally like w 15 hackathons straight in in a row because I was the only one",
    "start": "1951880",
    "end": "1957639"
  },
  {
    "text": "who was doing this and I understood that and I think like um we'll see more and more applications that will take",
    "start": "1957639",
    "end": "1964240"
  },
  {
    "text": "advantage of being able to run in device or on device Ai and I think like web",
    "start": "1964240",
    "end": "1970120"
  },
  {
    "text": "assembly is a huge proponent to that and hopefully by next year we'll have many more examples of production use cases",
    "start": "1970120",
    "end": "1977600"
  },
  {
    "text": "for for web assembly because we always question that is web assembly ready I think the browser is the perfect example",
    "start": "1977600",
    "end": "1983600"
  },
  {
    "text": "because web assembly started with the browser and even today it still you know of course like say comes from a v",
    "start": "1983600",
    "end": "1990880"
  },
  {
    "text": "cluster background so he deals with a lot of uh you know comunities you can probably have a chat with him on how to",
    "start": "1990880",
    "end": "1996000"
  },
  {
    "text": "do that more efficiently with we cluster uh running AI workloads webs workloads over there uh I come with a lot of",
    "start": "1996000",
    "end": "2002840"
  },
  {
    "text": "database background and you know you can do a lot of stuffs with that but the thing that excites me more is definitely",
    "start": "2002840",
    "end": "2008440"
  },
  {
    "text": "the browser so with that in mind I think we'll conclude our presentation but if there are any questions we would love to",
    "start": "2008440",
    "end": "2014200"
  },
  {
    "text": "hear and uh yes I'll be sticking around uh to discuss more about the codebase because I know that we ran slightly over",
    "start": "2014200",
    "end": "2020559"
  },
  {
    "text": "time but I'll be more than happy to show you how everything is working behind the scenes as well thank you so much thank you",
    "start": "2020559",
    "end": "2028360"
  }
]