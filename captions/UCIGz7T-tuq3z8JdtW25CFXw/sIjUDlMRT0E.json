[
  {
    "start": "0",
    "end": "50000"
  },
  {
    "text": "[Music]",
    "start": "1490",
    "end": "7109"
  },
  {
    "text": "hi hello everyone uh I'm M and uh uh it's my great honor to be here you today",
    "start": "10480",
    "end": "16198"
  },
  {
    "text": "and I'm the maintainer of the cncf was mag project um hello everyone uh my name",
    "start": "16199",
    "end": "22760"
  },
  {
    "text": "is Michael Yan I work at the second state and uh also the founder and maintainer of the W project so we want",
    "start": "22760",
    "end": "28599"
  },
  {
    "text": "to talk about you know uh a new use case we have seen a lot of traction since",
    "start": "28599",
    "end": "33640"
  },
  {
    "text": "wasam iio last year so we came here last I I gave a talk about at the time chbt",
    "start": "33640",
    "end": "38840"
  },
  {
    "text": "just came out we talked about running chbt extensions with wasam and you know things like that we've seen a lot of use",
    "start": "38840",
    "end": "44680"
  },
  {
    "text": "cases with wasm in the large language model um throughout this year so we want to talk about that please U so today U",
    "start": "44680",
    "end": "52280"
  },
  {
    "start": "50000",
    "end": "232000"
  },
  {
    "text": "Michael and I will talk about how to create ctive agents and extensions for",
    "start": "52280",
    "end": "57840"
  },
  {
    "text": "lmm using web ums has been um very popular and a",
    "start": "57840",
    "end": "64600"
  },
  {
    "text": "powerful tool for our daily life and work so as a new kind of workload ourm",
    "start": "64600",
    "end": "71360"
  },
  {
    "text": "apps raise some new demands in the cloud native Tech stack so uh we will talk",
    "start": "71360",
    "end": "76720"
  },
  {
    "text": "about the current Tex stack for LM apps and its limitations and then we will talk about how web simply can solve the",
    "start": "76720",
    "end": "85079"
  },
  {
    "text": "problems uh during the uh the talk we will have uh two demos if we we have",
    "start": "85079",
    "end": "90280"
  },
  {
    "text": "enough time okay let's start so uh the first is uh the current Tex de fors is",
    "start": "90280",
    "end": "96840"
  },
  {
    "text": "dominated by python we use Python to do inference we use launching and python to",
    "start": "96840",
    "end": "103119"
  },
  {
    "text": "do to develop RM agent R exensions however python it also has its",
    "start": "103119",
    "end": "109399"
  },
  {
    "text": "problems the first one is that python is uh is high with because it has lots of",
    "start": "109399",
    "end": "117119"
  },
  {
    "text": "dependencies it also make it difficult for developers to install uh on the left",
    "start": "117119",
    "end": "123960"
  },
  {
    "text": "image Chris album he is the uh engineer director of Wikipedia and he has entered",
    "start": "123960",
    "end": "132080"
  },
  {
    "text": "the problem that um uh he has entered some problems when he want to install",
    "start": "132080",
    "end": "137800"
  },
  {
    "text": "python on his new Macbook and um the right the Right image is gr brokman is C",
    "start": "137800",
    "end": "145560"
  },
  {
    "text": "or open Ai and he also believes that U dealing with python dependencies is the",
    "start": "145560",
    "end": "151640"
  },
  {
    "text": "most important work in machine learning engineering so U from the from the view",
    "start": "151640",
    "end": "158519"
  },
  {
    "text": "of RM agents um python is too heav with and um with lots of comp complex",
    "start": "158519",
    "end": "166080"
  },
  {
    "text": "dependences so for the um inference the bigger problem is that the air apps is",
    "start": "166080",
    "end": "171720"
  },
  {
    "text": "not posable we have lots kinds of um GPU",
    "start": "171720",
    "end": "177000"
  },
  {
    "text": "device so we need to repel our IP again to match the new hardware um because the",
    "start": "177000",
    "end": "184680"
  },
  {
    "text": "A app is not importable so we have as result we are forc to use APS Z but APS",
    "start": "184680",
    "end": "191760"
  },
  {
    "text": "however API Z is not flexible and performance for example if we want to uh",
    "start": "191760",
    "end": "199040"
  },
  {
    "text": "develop an LM app with multiple models the um open AI API is not sufficient so",
    "start": "199040",
    "end": "207840"
  },
  {
    "text": "there is a need that we need to write our inference directly and moreover even",
    "start": "207840",
    "end": "213439"
  },
  {
    "text": "python itself is not cross device portable so in summary the current St",
    "start": "213439",
    "end": "219439"
  },
  {
    "text": "Tex stack for LM apps is not possible and is heavy with so we think uh we",
    "start": "219439",
    "end": "227319"
  },
  {
    "text": "think WM can solve those problems well so how does WM solve this",
    "start": "227319",
    "end": "235040"
  },
  {
    "start": "232000",
    "end": "607000"
  },
  {
    "text": "problem you know I I heard a lot of talk today um that talked about the history of computing from Assembly Language to",
    "start": "235040",
    "end": "240879"
  },
  {
    "text": "Java you know so one of the old issues we thought we have solved 20 years ago",
    "start": "240879",
    "end": "246799"
  },
  {
    "text": "30 years ago with Java has not came back it's how do we write once wrong anywhere",
    "start": "246799",
    "end": "252680"
  },
  {
    "text": "you know um Java s solve this problem you know when you compile with CPUs right but today if you look at there's a",
    "start": "252680",
    "end": "260519"
  },
  {
    "text": "um marage has his own GPU um the um Nvidia has it own GPU MD has it own GPU",
    "start": "260519",
    "end": "267680"
  },
  {
    "text": "every single cloud provider now have their own G gpus right so it's no longer the case that I can write application on",
    "start": "267680",
    "end": "274080"
  },
  {
    "text": "my ma on my laptop compile and expect to run on the Cloud Server I will have to",
    "start": "274080",
    "end": "280000"
  },
  {
    "text": "recompile it for the Cloud Server every single time again when I use tools like container tools and kubernetes I can no",
    "start": "280000",
    "end": "286400"
  },
  {
    "text": "longer rely on them to distribute my binary artifacts because if I have a",
    "start": "286400",
    "end": "291560"
  },
  {
    "text": "binary that is compiled for the mac and I accidentally deployed on an nedia device it would not run there so",
    "start": "291560",
    "end": "297280"
  },
  {
    "text": "portability has really come back with the Vengeance you know you know is that how do we solve the prob portability",
    "start": "297280",
    "end": "302759"
  },
  {
    "text": "problem all over again not for CPUs this time but for gpus you know that's a that",
    "start": "302759",
    "end": "308360"
  },
  {
    "text": "actually is one of the most interesting challenge we see how people use WM to address those issues so in a",
    "start": "308360",
    "end": "315720"
  },
  {
    "text": "way I don't know why [Laughter] it's in the way that WM solved this um",
    "start": "315720",
    "end": "322759"
  },
  {
    "text": "you know we're going to talk about in a minute how wasum solve this problem but the second I think equally important is",
    "start": "322759",
    "end": "327960"
  },
  {
    "text": "that wasum U when you use WM to write GPU applications or AI inference applications the size is very small the",
    "start": "327960",
    "end": "335680"
  },
  {
    "text": "application size is only like 2 megab 3 megabytes the wrong time itself is 30",
    "start": "335680",
    "end": "340919"
  },
  {
    "text": "megabytes 140 megabytes we're talking about water mat here but all the other wrong times are the same if you compare that we say the non-portable P python py",
    "start": "340919",
    "end": "348600"
  },
  {
    "text": "torch wrong time can you guess how how large is a p is a py torch is a py torch",
    "start": "348600",
    "end": "353800"
  },
  {
    "text": "based stalker image it's 4 gabes right it's 4,000 megabytes so it's 100 times",
    "start": "353800",
    "end": "358880"
  },
  {
    "text": "difference so so we're talking about two orders of magnitude difference in the lightweightness and uh also the the",
    "start": "358880",
    "end": "364360"
  },
  {
    "text": "speed is much faster than Python and also it is um uh it is truly portable at",
    "start": "364360",
    "end": "370000"
  },
  {
    "text": "least we can make it truly pable right so how do we do that there's a a specification that is uh um that grew up",
    "start": "370000",
    "end": "378400"
  },
  {
    "text": "I think um from Intel almost three years ago it's called wasi and wasi neuro Network it is now being adapted into",
    "start": "378400",
    "end": "385199"
  },
  {
    "text": "component model and wasi preview too but you know um so when the when wasi first",
    "start": "385199",
    "end": "391960"
  },
  {
    "text": "came out it was mostly to run you know uh smaller models on the edge so for instance py T of flow models that are uh",
    "start": "391960",
    "end": "399560"
  },
  {
    "text": "on camera devices and you know things like that you youve heard about those stories today as well so um but I think",
    "start": "399560",
    "end": "406440"
  },
  {
    "text": "wasi NN is now being adapted to run very large models like large language models",
    "start": "406440",
    "end": "411520"
  },
  {
    "text": "and also stable diffusion and models like that so the specific thing that we",
    "start": "411520",
    "end": "417319"
  },
  {
    "text": "did and we were hoping to standardize that as well is to build Wy extensions",
    "start": "417319",
    "end": "422639"
  },
  {
    "text": "to Wy andn meaning the the the way Wyn works is like how every single Wy spec",
    "start": "422639",
    "end": "429120"
  },
  {
    "text": "works is that there's host functions being defined in the wum side and then native functions on the on the host side",
    "start": "429120",
    "end": "437039"
  },
  {
    "text": "right you know so there's a um so that provides opportunity to provide a different back end so for instance W",
    "start": "437039",
    "end": "443280"
  },
  {
    "text": "itself you can make it around on Linux you can read it on Windows you can have different lipy imple implementations for",
    "start": "443280",
    "end": "448879"
  },
  {
    "text": "that but for the WN we now have large language model backends for WN so that",
    "start": "448879",
    "end": "455120"
  },
  {
    "text": "all the inference all the heavyweight compute um actually take place as native",
    "start": "455120",
    "end": "460960"
  },
  {
    "text": "applications on that wrap around in the in under underneath the W wrong time",
    "start": "460960",
    "end": "466280"
  },
  {
    "text": "right so you know so you would have you can still use Cuda you can still use Mac mental you know so the net effect really",
    "start": "466280",
    "end": "473159"
  },
  {
    "text": "is that people uh developers can standardize on the wasn um um uh",
    "start": "473159",
    "end": "478759"
  },
  {
    "text": "interface so you write application not to Cuda not to metal not to the uh the",
    "start": "478759",
    "end": "484240"
  },
  {
    "text": "AMD romc you know not to any of those um you know uh GPU specifications but you",
    "start": "484240",
    "end": "489560"
  },
  {
    "text": "write application to W and you compile to Wasa and as long as as then you",
    "start": "489560",
    "end": "495159"
  },
  {
    "text": "deploy to a to a machine that has wasi large language model backhand installed it would automatically translate all",
    "start": "495159",
    "end": "501400"
  },
  {
    "text": "your wasm function CS into the Cuda function Cs on that machine or the metal metal function Cs on the on the Macbook",
    "start": "501400",
    "end": "508520"
  },
  {
    "text": "so that's how it works so I think that is a rather elegant solution for the portability problem you know so it's the",
    "start": "508520",
    "end": "514200"
  },
  {
    "text": "the same thing that you need to install different jvms on different CPUs but at the Java bod level it's right one right",
    "start": "514200",
    "end": "521719"
  },
  {
    "text": "one wrong anywhere so you know we have we would have a lot more demos later that's uh in know in a workshop but you know just to",
    "start": "521719",
    "end": "530399"
  },
  {
    "text": "summarize what do we get you know so we have a project called llama Ed you know so it's a it's it's a derivative of the",
    "start": "530399",
    "end": "536519"
  },
  {
    "text": "wasm Ed project and we have numerous back end to it so we have the Apple metal back end to it we have the um you",
    "start": "536519",
    "end": "543120"
  },
  {
    "text": "know uh the Nvidia back end to it we have the um run the arm CPUs R Intel CPUs we have uh four Linux Foundation",
    "start": "543120",
    "end": "550519"
  },
  {
    "text": "interns uh those are graduate students um that's working that working with us at this moment to add new um uh backend",
    "start": "550519",
    "end": "558240"
  },
  {
    "text": "to it so for instance we we wanted to run um better on Advanced CPUs on the uh",
    "start": "558240",
    "end": "564160"
  },
  {
    "text": "you know new generation of Intel CPUs which has simd uh simd and AVX enabled you know so those are there's lot of",
    "start": "564160",
    "end": "571000"
  },
  {
    "text": "hardware and you know lowlevel Innovation that that is going on that we can use W as envelope to to wrap around",
    "start": "571000",
    "end": "578760"
  },
  {
    "text": "those interfaces so that application developers only need to worry about compiling their application to was and",
    "start": "578760",
    "end": "584480"
  },
  {
    "text": "to was them they do not need to worry about all the strange high performance",
    "start": "584480",
    "end": "589959"
  },
  {
    "text": "uh you know uh native code based back end that's that's that the wrong SL models underneath that right",
    "start": "589959",
    "end": "596000"
  },
  {
    "text": "so so um I ask we going to come back and do the first demo you know use a uh to",
    "start": "596000",
    "end": "601360"
  },
  {
    "text": "to write a crossplatform application for the B to for LM",
    "start": "601360",
    "end": "607000"
  },
  {
    "start": "607000",
    "end": "1115000"
  },
  {
    "text": "inference yes uh so for the demo I will first create uh an LM web service on my",
    "start": "607000",
    "end": "614360"
  },
  {
    "text": "mybook and then I will run it on um a media device",
    "start": "614360",
    "end": "621959"
  },
  {
    "text": "so okay",
    "start": "626240",
    "end": "630240"
  },
  {
    "text": "so source code for the LM service is in the Lama Edge Ripple it's all written in",
    "start": "636000",
    "end": "643160"
  },
  {
    "text": "Russ so uh the first step is that we need to get compare R to the W",
    "start": "643160",
    "end": "650880"
  },
  {
    "text": "Banner uh I I have already get cloned the rle so we",
    "start": "650880",
    "end": "656839"
  },
  {
    "text": "just and use Caro build Target W 32 was it released to compare the r source",
    "start": "656839",
    "end": "664800"
  },
  {
    "text": "code and compelling Russ will take some time I believe that everyone has have",
    "start": "664800",
    "end": "671120"
  },
  {
    "text": "will go through this",
    "start": "671120",
    "end": "675160"
  },
  {
    "text": "so I want to emphasize this is compiling on the Mac and uh using all the standard",
    "start": "699399",
    "end": "704560"
  },
  {
    "text": "rust libraries there's no um compiling to the Mac GPU there no step like that",
    "start": "704560",
    "end": "710120"
  },
  {
    "text": "you know so it's all um very standard if you look at there there's a was was",
    "start": "710120",
    "end": "715560"
  },
  {
    "text": "which is um you know our extension was and we hope to make it uh to incorporate into standard later you know go yeah",
    "start": "715560",
    "end": "722800"
  },
  {
    "text": "okay uh so uh next step I will U move this uh the compell wasn't fil to my uh",
    "start": "722800",
    "end": "729519"
  },
  {
    "text": "root directory because I also downloaded U the light language models",
    "start": "729519",
    "end": "736399"
  },
  {
    "text": "here so um the Lama uh API zero",
    "start": "744160",
    "end": "749920"
  },
  {
    "text": "WM is the uh the was F we just compiled um the the time here is is a Singapore",
    "start": "749920",
    "end": "756920"
  },
  {
    "text": "time because I just came from Singapore uh okay uh so let's use um",
    "start": "756920",
    "end": "764320"
  },
  {
    "text": "next we will use was mag to to uh to run the Lama 2",
    "start": "764320",
    "end": "771920"
  },
  {
    "text": "model uh okay uh this is the command line to run on the LI language models",
    "start": "777920",
    "end": "783839"
  },
  {
    "text": "you can use any L language models that in GUI format so the the API server is",
    "start": "783839",
    "end": "790800"
  },
  {
    "text": "listening on um Local Host it's uh s0 a z let's open it in my uh",
    "start": "790800",
    "end": "800320"
  },
  {
    "text": "browser so uh let's ask um a simple question",
    "start": "801120",
    "end": "807560"
  },
  {
    "text": "WR a hollow World program in",
    "start": "812519",
    "end": "819079"
  },
  {
    "text": "Ras the first interaction takes longer because it needs to load the whole 5 GB",
    "start": "822360",
    "end": "828079"
  },
  {
    "text": "model into the memory but now it come back you know so it's it runs entirely locally you know",
    "start": "828079",
    "end": "835279"
  },
  {
    "text": "so um we could have turned off the Wi-Fi here and this would still around you know because uh the large language model",
    "start": "835279",
    "end": "842040"
  },
  {
    "text": "is been down is has been downloaded into the into this machine right you know so",
    "start": "842040",
    "end": "847160"
  },
  {
    "text": "it's a so if you look at here it's a um it's a local server so it's a local host",
    "start": "847160",
    "end": "852839"
  },
  {
    "text": "that it connects to the to the to to to the uh HTTP server which also runs",
    "start": "852839",
    "end": "859759"
  },
  {
    "text": "inside wasm and uh um using the wasm socket API right and then you know it's",
    "start": "859759",
    "end": "865720"
  },
  {
    "text": "a um it connects to the local server that runs the w uh the large language model and then the front hand does that",
    "start": "865720",
    "end": "872279"
  },
  {
    "text": "yeah so uh so here is the uh program but but",
    "start": "872279",
    "end": "878440"
  },
  {
    "text": "I don't think it's is is it right yeah okay so I'm not a",
    "start": "878440",
    "end": "886680"
  },
  {
    "text": "r okay so uh now we have um um compar the was file on my MacBook next step I",
    "start": "886680",
    "end": "893920"
  },
  {
    "text": "will move the uh the the uh Lama API Z was filed to",
    "start": "893920",
    "end": "899600"
  },
  {
    "text": "to the uh Jon orang device so by the way that that device",
    "start": "899600",
    "end": "906000"
  },
  {
    "text": "sitting in our office in Tai so we have an engineering office in Tai the device is about this big it's a Jetson Orin it",
    "start": "906000",
    "end": "912399"
  },
  {
    "text": "has a um it was designed to using robotic applications it has a small",
    "start": "912399",
    "end": "917800"
  },
  {
    "text": "Nvidia um uh gpus in it so we put that in our office as a coding assistant so",
    "start": "917800",
    "end": "923920"
  },
  {
    "text": "we we run a large uh you know llama code llama model using wasm Edge on that device so that we hook up with the with",
    "start": "923920",
    "end": "930360"
  },
  {
    "text": "all the IDS in the office right so what she's doing that we are copying this from Barcelona to Tai and then um in a",
    "start": "930360",
    "end": "938000"
  },
  {
    "text": "device that has entirely different CPU entirely different GPU and uh oh by the",
    "start": "938000",
    "end": "943920"
  },
  {
    "text": "way the performance they have just seen on uh on that on that chat interface is",
    "start": "943920",
    "end": "949040"
  },
  {
    "text": "running on the GPU on the Mac because if not on GPU it wouldn't be that fast it would be very it would speak very very",
    "start": "949040",
    "end": "956000"
  },
  {
    "text": "slowly you know much slower than I speak now but you know if",
    "start": "956000",
    "end": "961800"
  },
  {
    "text": "you but it runs on the Mac CPU now she's copied that file into the into the um",
    "start": "961800",
    "end": "968440"
  },
  {
    "text": "into the Jetson device in Ty a right so so now she's logging into the Jetson device yeah I have logg into the Jets",
    "start": "968440",
    "end": "975279"
  },
  {
    "text": "and device and uh you can see",
    "start": "975279",
    "end": "981759"
  },
  {
    "text": "so so here is the Lama AP serm we just",
    "start": "981759",
    "end": "986920"
  },
  {
    "text": "compiled",
    "start": "986920",
    "end": "989920"
  },
  {
    "text": "okay uh we no now we can use the exactly the same command line to start the um to",
    "start": "998240",
    "end": "1004600"
  },
  {
    "text": "start an API Z for the Lama 27b models so I just copy the common l i just use",
    "start": "1004600",
    "end": "1014480"
  },
  {
    "text": "okay so uh we can use the code to test the API server let's send um API",
    "start": "1021959",
    "end": "1030199"
  },
  {
    "text": "request uh so this is the uh the API",
    "start": "1040679",
    "end": "1045959"
  },
  {
    "text": "requir format to um to to interact with the model um the the question is write a",
    "start": "1045959",
    "end": "1053039"
  },
  {
    "text": "hollow World program in Python yeah so what's interesting here",
    "start": "1053039",
    "end": "1059559"
  },
  {
    "text": "is that we started API server the API server we just show you the API server has a web front end but it also has a",
    "start": "1059559",
    "end": "1066919"
  },
  {
    "text": "API back end uh API endpoint that's compatible with open AI now it's already",
    "start": "1066919",
    "end": "1072200"
  },
  {
    "text": "came back because you know you can see the media TPU is faster than the Mac TPU you know so now it's uh",
    "start": "1072200",
    "end": "1080840"
  },
  {
    "text": "did it come back did it yes okay this is a simple uh Hall World programming in",
    "start": "1082200",
    "end": "1088360"
  },
  {
    "text": "Python okay okay um so um from this Amo we can see that with what magic we can",
    "start": "1088360",
    "end": "1096080"
  },
  {
    "text": "develop and test apps on my on our local machine and then deploy it on a on other",
    "start": "1096080",
    "end": "1102919"
  },
  {
    "text": "machines without recompiling so this is the uh first s",
    "start": "1102919",
    "end": "1110600"
  },
  {
    "text": "okay so yeah that's uh um so we have shown open a compatible API server you know that's a very you know so um",
    "start": "1116000",
    "end": "1124480"
  },
  {
    "text": "because it supports open ey style um um web services end points you can actually connect a lot of tools with that so so",
    "start": "1124480",
    "end": "1130960"
  },
  {
    "text": "we talked about you can do like land chain with it you know because you can use a lot of tools to hook up Vector",
    "start": "1130960",
    "end": "1136360"
  },
  {
    "text": "database to manipulate the prom you know there are lot of advanced stuffff that people do with large language models",
    "start": "1136360",
    "end": "1141440"
  },
  {
    "text": "this days um however um we don't want",
    "start": "1141440",
    "end": "1146480"
  },
  {
    "text": "python in there as well you know because you know um one of the things that we learn is that you know um we have spent",
    "start": "1146480",
    "end": "1153280"
  },
  {
    "text": "so much time in Cloud native and with optimized go application and rust application now we are going all the way",
    "start": "1153280",
    "end": "1158960"
  },
  {
    "text": "back to python to develop to deploy all all those applications and the and in fact I personally don't want that so we",
    "start": "1158960",
    "end": "1165679"
  },
  {
    "text": "want use uh rust and wasm to do the tooling that around the the WM server as",
    "start": "1165679",
    "end": "1172400"
  },
  {
    "text": "well to of the as well so this is one of the topics that Daniel mentioned earlier",
    "start": "1172400",
    "end": "1178760"
  },
  {
    "text": "in his talk about the new Microsoft project about using wasum as a prompt right you know so basically they were uh",
    "start": "1178760",
    "end": "1185760"
  },
  {
    "text": "they were uh adding wasm to their application server and to their large language model server so that you can",
    "start": "1185760",
    "end": "1192120"
  },
  {
    "text": "use you can write a a program in Rust to manipulate the prom to insert stuff into the prom and then to execute the the",
    "start": "1192120",
    "end": "1198760"
  },
  {
    "text": "program or the all the outcome that that come out of the armm to select for instance select a better answer or if",
    "start": "1198760",
    "end": "1205480"
  },
  {
    "text": "you ask it to program in Python you to run the Python program that's in your um uh in your uh your own application right",
    "start": "1205480",
    "end": "1212360"
  },
  {
    "text": "you know so what we want to do is that we want um we um uh have seen a lot of",
    "start": "1212360",
    "end": "1218720"
  },
  {
    "text": "demand and we will do do do something that is similar you know that's a lot of people have the same ideas you know is that we use wasum to interface with the",
    "start": "1218720",
    "end": "1226360"
  },
  {
    "text": "wasm server we have just built right we have built wasm a server but we want to use wasm to manage the pipelines well",
    "start": "1226360",
    "end": "1232400"
  },
  {
    "text": "essentially provide a l chain alternative using rust and the wome",
    "start": "1232400",
    "end": "1239520"
  },
  {
    "text": "um oh yeah so you know that's uh um the reason for that is really you know",
    "start": "1239520",
    "end": "1245559"
  },
  {
    "text": "that's uh because those are extensions and plugins we talk about those all the time you know wasm the one of the best",
    "start": "1245559",
    "end": "1251240"
  },
  {
    "text": "use cases WM is Serv functions and and and plugins and now we have a you know a",
    "start": "1251240",
    "end": "1256600"
  },
  {
    "text": "large language models that's that definitely require those features but now we are uh going back to use Python",
    "start": "1256600",
    "end": "1262000"
  },
  {
    "text": "to do it you know that's that's to me this is um you know this is definitely going backwards so we want to go forward",
    "start": "1262000",
    "end": "1267280"
  },
  {
    "text": "and using um R and wasm to do that so um",
    "start": "1267280",
    "end": "1272880"
  },
  {
    "text": "yeah so the specific tool that we want to uh we want to show is one of the other projects that's based on was m is",
    "start": "1272880",
    "end": "1278919"
  },
  {
    "text": "called float Network you know what it does is that it's a serverless function you know there's lot of things like that",
    "start": "1278919",
    "end": "1284640"
  },
  {
    "text": "it's a serverless function platform where you can just upload your WM and it would execute that for you one thing it",
    "start": "1284640",
    "end": "1289960"
  },
  {
    "text": "does is that it it now can act as your server you don't have to start your own server hence it's called serverless right so we would come come on again and",
    "start": "1289960",
    "end": "1297559"
  },
  {
    "text": "talk about how we create a wasm application for for for Discord part um",
    "start": "1297559",
    "end": "1303120"
  },
  {
    "text": "and uh to connect that to the large language model service we have just built yeah yeah uh the the second demo",
    "start": "1303120",
    "end": "1309159"
  },
  {
    "start": "1307000",
    "end": "1757000"
  },
  {
    "text": "is to in to demonstrate that wasm is the um Bice runtime for uh I extensions so",
    "start": "1309159",
    "end": "1316720"
  },
  {
    "text": "for network is also a platform parted by W mag so uh we can",
    "start": "1316720",
    "end": "1324480"
  },
  {
    "text": "let's do uh do our second demo so uh here is the uh Source uh",
    "start": "1324480",
    "end": "1332520"
  },
  {
    "text": "source code for building a Discord board with open source LMS uh it's also",
    "start": "1332520",
    "end": "1338279"
  },
  {
    "text": "written in Russ so we will need to compare the Russ code to WM again but um",
    "start": "1338279",
    "end": "1345039"
  },
  {
    "text": "instead of compelling on my own Ma book some uh the platform on FL network will",
    "start": "1345039",
    "end": "1351679"
  },
  {
    "text": "compile the the rust code for us okay so let's go to flows Network and",
    "start": "1351679",
    "end": "1360240"
  },
  {
    "text": "um click on create flow and uh click on um import the flow",
    "start": "1360240",
    "end": "1367679"
  },
  {
    "text": "function from GitHub reple U Flo Network use get Ops so we",
    "start": "1367679",
    "end": "1375279"
  },
  {
    "text": "will import the source code from uh from have Ripple not not compiling the uh",
    "start": "1375279",
    "end": "1381320"
  },
  {
    "text": "source code uh in your own",
    "start": "1381320",
    "end": "1385080"
  },
  {
    "text": "machine the folder is Discord and um now we need to add some",
    "start": "1387600",
    "end": "1394240"
  },
  {
    "text": "configurations for this LM agent because it invol involves uh Discord we will",
    "start": "1394240",
    "end": "1400640"
  },
  {
    "text": "need a Discord B token U the Discord the Discord bot toen can be gu",
    "start": "1400640",
    "end": "1406799"
  },
  {
    "text": "from Discord developer uh portal and click on new application and name it",
    "start": "1406799",
    "end": "1415279"
  },
  {
    "text": "was click on on both and then we will get the the discount",
    "start": "1421919",
    "end": "1428559"
  },
  {
    "text": "token and then copy and paste the Discord token",
    "start": "1434600",
    "end": "1442400"
  },
  {
    "text": "here let's continue set up our Discord bot U we will turn on the um price the",
    "start": "1451919",
    "end": "1459360"
  },
  {
    "text": "prices intent the U server members intent and M content intent and click on",
    "start": "1459360",
    "end": "1466679"
  },
  {
    "text": "Save change and then we will go",
    "start": "1466679",
    "end": "1473440"
  },
  {
    "text": "to or us to to give some permissions for our Discord B and then um inv is an",
    "start": "1476240",
    "end": "1484799"
  },
  {
    "text": "invitation URL will be generated automatically we will copy this URL and",
    "start": "1484799",
    "end": "1490200"
  },
  {
    "text": "uh open in the browser and let's invite let's invite",
    "start": "1490200",
    "end": "1496440"
  },
  {
    "text": "the V my2 on 24 and bought to my own Discord",
    "start": "1496440",
    "end": "1503600"
  },
  {
    "text": "server so we can see that here is um b b called W my 24 24 uh it's offline right",
    "start": "1513679",
    "end": "1523240"
  },
  {
    "text": "now but um it but don't worry it will be online um quickly so let's go back to",
    "start": "1523240",
    "end": "1528760"
  },
  {
    "text": "Flo Network and um um continue the continue setting up some configurations",
    "start": "1528760",
    "end": "1535120"
  },
  {
    "text": "the first is I am",
    "start": "1535120",
    "end": "1538880"
  },
  {
    "text": "Endo uh I have used Ang to um um to make",
    "start": "1543480",
    "end": "1548720"
  },
  {
    "text": "to turn my Local Host l z l z to be an HTV as a link so we can use this link",
    "start": "1548720",
    "end": "1559600"
  },
  {
    "text": "the API server is open a compatible so we will add we1 at the end of the link",
    "start": "1568919",
    "end": "1576559"
  },
  {
    "text": "and the next is we will set some uh simple system prompts for the",
    "start": "1576559",
    "end": "1584360"
  },
  {
    "text": "B um let's just say you are and a help",
    "start": "1585279",
    "end": "1591919"
  },
  {
    "text": "for",
    "start": "1593679",
    "end": "1596679"
  },
  {
    "text": "assistant okay now let's build this um build this function and uh click on U",
    "start": "1603000",
    "end": "1612200"
  },
  {
    "text": "deploy so you can see the the rust code is um uh is building now",
    "start": "1612200",
    "end": "1619760"
  },
  {
    "text": "this is U building U this is the building log for building a Russ program",
    "start": "1622240",
    "end": "1628159"
  },
  {
    "text": "into WM oh",
    "start": "1628159",
    "end": "1633919"
  },
  {
    "text": "sorry again it will uh take some time because compan R is oh okay oh it's it's",
    "start": "1634799",
    "end": "1642679"
  },
  {
    "text": "it's online now so we can go back to uh our Discord server and you can see that",
    "start": "1642679",
    "end": "1648840"
  },
  {
    "text": "the warm I24 B is online now no we can oh sorry one more",
    "start": "1648840",
    "end": "1655440"
  },
  {
    "text": "thing uh I need",
    "start": "1655440",
    "end": "1659799"
  },
  {
    "text": "to I will use U was magic to uh start the uh API server for our Discord Bard",
    "start": "1668679",
    "end": "1676840"
  },
  {
    "text": "so this is the same commment line in the first demo so I think now we can talk to",
    "start": "1676840",
    "end": "1684960"
  },
  {
    "text": "the",
    "start": "1684960",
    "end": "1687600"
  },
  {
    "text": "bot so in the terminal we can see the uh see the coners the conversation history",
    "start": "1698559",
    "end": "1704600"
  },
  {
    "text": "I just send hi and I think not support has response to me",
    "start": "1704600",
    "end": "1712320"
  },
  {
    "text": "um I I think uh this demo is uh is done",
    "start": "1712320",
    "end": "1717399"
  },
  {
    "text": "I'm not going to ask some complex complex questions since our time",
    "start": "1717399",
    "end": "1723760"
  },
  {
    "text": "is all right okay so maybe you can put the link up there so that uh you know if anyone's",
    "start": "1723760",
    "end": "1730320"
  },
  {
    "text": "interested you can add this part to to own GitHub and Discord server yeah",
    "start": "1730320",
    "end": "1737640"
  },
  {
    "text": "yeah so this is um our agent demo and uh",
    "start": "1737640",
    "end": "1742760"
  },
  {
    "text": "uh you can also use we also develop some agent based on CH gbt that time uh you",
    "start": "1742760",
    "end": "1750519"
  },
  {
    "text": "can go to flow Network that page to um to check to check",
    "start": "1750519",
    "end": "1756960"
  },
  {
    "text": "them so besid besides Michael and I have discussed and demonstrat what my U",
    "start": "1756960",
    "end": "1764840"
  },
  {
    "start": "1757000",
    "end": "1883000"
  },
  {
    "text": "continually want to um improve the uh app development experience so in the",
    "start": "1764840",
    "end": "1771679"
  },
  {
    "text": "future we will add more am related backends for the W in plug in uh it will",
    "start": "1771679",
    "end": "1777519"
  },
  {
    "text": "have MX it's for the apple chips and we will have uh int Intel extension for",
    "start": "1777519",
    "end": "1785080"
  },
  {
    "text": "Transformers it's for Intel chips and we we will have more more models like open",
    "start": "1785080",
    "end": "1791480"
  },
  {
    "text": "eyes whisper is it Con is convert text to voice and we will also have a um burn",
    "start": "1791480",
    "end": "1798279"
  },
  {
    "text": "uh it's similar to it's similar to uh lama lama CVP and we will also add a",
    "start": "1798279",
    "end": "1806360"
  },
  {
    "text": "support for embedding models which is important to develop rug applications",
    "start": "1806360",
    "end": "1813120"
  },
  {
    "text": "and we will also figure out some ways to do U WM container Management on on GPU",
    "start": "1813120",
    "end": "1820320"
  },
  {
    "text": "so this this is our our plan and um this is the resource that um uh we we use in",
    "start": "1820320",
    "end": "1827640"
  },
  {
    "text": "uh in this talk including the was in proposal so was mag was create and the Lage and S",
    "start": "1827640",
    "end": "1837120"
  },
  {
    "text": "code and also the U the world mostess platform FL",
    "start": "1837120",
    "end": "1843279"
  },
  {
    "text": "Network so uh in the after oh sorry in the afternoon Michael will have a workshop on um bu on building um um I'm",
    "start": "1843279",
    "end": "1852960"
  },
  {
    "text": "inference on the ID Cloud which will introduce more uh um I I'm a features in",
    "start": "1852960",
    "end": "1859679"
  },
  {
    "text": "the inward match uh if I you if you are interested in you are",
    "start": "1859679",
    "end": "1865360"
  },
  {
    "text": "welcome all right thank you yeah that's um um you know I think we are right on time so you know we won't hold up for",
    "start": "1865360",
    "end": "1873080"
  },
  {
    "text": "for lunch you know so we hope to see you after lunch that's your Workshop",
    "start": "1873080",
    "end": "1878810"
  },
  {
    "text": "[Applause] [Music]",
    "start": "1878810",
    "end": "1885779"
  }
]