[
  {
    "text": "[Music]",
    "start": "350",
    "end": "11920"
  },
  {
    "text": "hello everyone I hope you all are doing good I am Saloni Naral I am Docker captain CNCF",
    "start": "11920",
    "end": "18960"
  },
  {
    "text": "ambassador and co-founder of Cube Simplify Uh there is lot to be discussed",
    "start": "18960",
    "end": "24000"
  },
  {
    "text": "today So let's get right into it So the title of our panel is running AI",
    "start": "24000",
    "end": "29920"
  },
  {
    "text": "workloads in vasom Is it production ready today we have Shiva from couch",
    "start": "29920",
    "end": "34960"
  },
  {
    "text": "base Michael Yuan from second stage and the famous Wajam Wazam edge runtime",
    "start": "34960",
    "end": "41280"
  },
  {
    "text": "project Sai principal developer advocate at Loft Labs and creator of Wazam course",
    "start": "41280",
    "end": "48640"
  },
  {
    "text": "Then we have Radu from Firmion who are creators of spin and spin cube",
    "start": "48640",
    "end": "55719"
  },
  {
    "text": "So remember the first talk of the day who all",
    "start": "55719",
    "end": "61399"
  },
  {
    "text": "remember okay well this is Wazam IO and it's about Wazam ready for enterprise",
    "start": "61399",
    "end": "68720"
  },
  {
    "text": "and Adardo and Andrea ported a beautiful um story over there this morning and I",
    "start": "68720",
    "end": "75200"
  },
  {
    "text": "want to take things in continuation with that but with the AI angle to it Well we",
    "start": "75200",
    "end": "81840"
  },
  {
    "text": "need to have the AI angle isn't it so we have witnessed a rapid rise in adoption",
    "start": "81840",
    "end": "88400"
  },
  {
    "text": "of generative AI across industries pushing the boundaries of what's possible in machine learning and beyond",
    "start": "88400",
    "end": "95360"
  },
  {
    "text": "Today we'll explore the current state of WASM in this ecosystem how it's enabling",
    "start": "95360",
    "end": "100560"
  },
  {
    "text": "efficient scalable and sustainable AI inference whether it's truly a gamecher",
    "start": "100560",
    "end": "106640"
  },
  {
    "text": "for production workloads We'll cover its application in edge computing with projects like llama edge",
    "start": "106640",
    "end": "113920"
  },
  {
    "text": "ga nodes kubernetesbased orchestration with spin cube browserbased inference",
    "start": "113920",
    "end": "120079"
  },
  {
    "text": "with web llm and even its potential in secure multi-tenant deployments plus",
    "start": "120079",
    "end": "126079"
  },
  {
    "text": "we'll peak into the future of AI agents and vasam's evolving now moving to the",
    "start": "126079",
    "end": "133160"
  },
  {
    "text": "equations I would first uh like to ask Sam and Shibai about the current",
    "start": "133160",
    "end": "139440"
  },
  {
    "text": "generative AI landscape What is happening and how are you keeping up so",
    "start": "139440",
    "end": "145840"
  },
  {
    "text": "let's um start with Sam first Okay Um so well the introduction is on me for",
    "start": "145840",
    "end": "153280"
  },
  {
    "text": "setting the context which is fine but setting the context we actually did I hope you see the picture uh behind and",
    "start": "153280",
    "end": "159760"
  },
  {
    "text": "that's the Gibli style image is is the new trend right and how can we miss it at at Wasam.io as well U so yeah if",
    "start": "159760",
    "end": "168319"
  },
  {
    "text": "anybody doesn't know AI is artificial intelligence uh just in case and generative AI means it is able to",
    "start": "168319",
    "end": "174800"
  },
  {
    "text": "generate new content text image code out of the input that you provide to um the",
    "start": "174800",
    "end": "181239"
  },
  {
    "text": "models So the genai ecosystem why it became popular well it's it's well known",
    "start": "181239",
    "end": "187519"
  },
  {
    "text": "kind of now you know when chat GPD came in uh GPT like generative uh pre-trained",
    "start": "187519",
    "end": "193200"
  },
  {
    "text": "transformer So generative is predicting the next word on the pre-trained set of",
    "start": "193200",
    "end": "198640"
  },
  {
    "text": "LLMs use using the large uh the text that it is trained on and then transfer",
    "start": "198640",
    "end": "205040"
  },
  {
    "text": "transformer being the algorithm over there So with that there has been a",
    "start": "205040",
    "end": "210720"
  },
  {
    "text": "surge in people like this is this is the first time I have seen not the first time but very rarely we see a technology",
    "start": "210720",
    "end": "216879"
  },
  {
    "text": "that everyone uh who is in tech and non- tech using it So that's why it became so",
    "start": "216879",
    "end": "223280"
  },
  {
    "text": "you know widely spread and there is no single conference where we do not talk about uh the AI ecosystem about AI Every",
    "start": "223280",
    "end": "230959"
  },
  {
    "text": "product out there is trying to figure out how they can cater to the needs of AI or you know provide the application",
    "start": "230959",
    "end": "238239"
  },
  {
    "text": "uh to the AI ecosystem So yes AI is riding you sleep you wake up in the morning and there's a new model out",
    "start": "238239",
    "end": "244319"
  },
  {
    "text": "there So that's how the AI ecosystem is being developed So we were on the on the way to Wazam.io and the Gibli style",
    "start": "244319",
    "end": "251760"
  },
  {
    "text": "images are trending suddenly on the internet because there is a new revolution in the image generation the",
    "start": "251760",
    "end": "258239"
  },
  {
    "text": "text to image is has gone to next level So just a few days back if you went on",
    "start": "258239",
    "end": "264720"
  },
  {
    "text": "chat GBT and tried you know uh give me put this text on this particular image it would fail miserably but now if you",
    "start": "264720",
    "end": "271600"
  },
  {
    "text": "do it like it's doing really well job even uh capturing the details from an",
    "start": "271600",
    "end": "277520"
  },
  {
    "text": "existing image which is very very interesting how fast the landscape is evolving and I think whether you are uh",
    "start": "277520",
    "end": "284880"
  },
  {
    "text": "in the and that's that's what I believe like whether you are in the ML ecosystem or in the non-machine learning ecosystem",
    "start": "284880",
    "end": "291680"
  },
  {
    "text": "or background machine knowing about this technology from your own area of",
    "start": "291680",
    "end": "297840"
  },
  {
    "text": "expertise is important like let me give you my example I come from the cloud native and the uh Kubernetes background",
    "start": "297840",
    "end": "303919"
  },
  {
    "text": "for me infrastructure is something which is very important so I in um although I",
    "start": "303919",
    "end": "309360"
  },
  {
    "text": "use AI for all my stuff uh you know helping out productivity and all those that everybody is doing but for me from",
    "start": "309360",
    "end": "315600"
  },
  {
    "text": "then from an infra person I believe that I need to know how stuff works how I can",
    "start": "315600",
    "end": "321360"
  },
  {
    "text": "make the models the inferencing available to the machine learning engineers How I can provide the",
    "start": "321360",
    "end": "326400"
  },
  {
    "text": "best-in-class infrastructure with Kubernetes to the AI engineers so that uh the infra level layer is something",
    "start": "326400",
    "end": "333440"
  },
  {
    "text": "that I focus on and how AI uh I cater to those AI workloads So like this everyone",
    "start": "333440",
    "end": "340479"
  },
  {
    "text": "kind of does this in their own way and how I keep up like with the landscape is",
    "start": "340479",
    "end": "346840"
  },
  {
    "text": "pretty pretty simple like uh since again uh same repetitive stuff like I'm from the cloud native background so there's a",
    "start": "346840",
    "end": "353280"
  },
  {
    "text": "cloud native AI group uh there are social media uh there are some newsletters that you can follow for all",
    "start": "353280",
    "end": "358800"
  },
  {
    "text": "the stuff that is happening in the AI ecosystem uh but yeah I think it's important to kind of get updated and",
    "start": "358800",
    "end": "365840"
  },
  {
    "text": "stay upto date in the AI ecosystem Yeah and I think just to kind of piggyback on what say mentioned uh",
    "start": "365840",
    "end": "371919"
  },
  {
    "text": "another growing trend that we are seeing is with aentic AI right so we are seeing that now it's not just these LLMs that",
    "start": "371919",
    "end": "378800"
  },
  {
    "text": "were trained on this data and they were only able to actually generate text but we are now seeing that when you add the",
    "start": "378800",
    "end": "385199"
  },
  {
    "text": "capability of being able to actually give tool calling to the LLMs where now they're able to access databases or",
    "start": "385199",
    "end": "391600"
  },
  {
    "text": "they're able to access external APIs the kind of functionality that you can actually get out of these particular",
    "start": "391600",
    "end": "398240"
  },
  {
    "text": "agent systems is pretty tremendous and we'll see specifically throughout",
    "start": "398240",
    "end": "403360"
  },
  {
    "text": "today's particular panel that how uh when it comes to making more efficient",
    "start": "403360",
    "end": "408880"
  },
  {
    "text": "compute because ultimately we know that uh whenever you're trying to execute any AI workload there is a lot of compute at",
    "start": "408880",
    "end": "415120"
  },
  {
    "text": "play and when you're trying to execute these particular workloads in systems that are typically not as performant",
    "start": "415120",
    "end": "422479"
  },
  {
    "text": "like such as the browser or the edge uh how we can actually do that more efficiently ly and also in a more",
    "start": "422479",
    "end": "428400"
  },
  {
    "text": "sustainable manner with the help of web assembly So uh I don't want to take up too much of the time because you know uh",
    "start": "428400",
    "end": "434080"
  },
  {
    "text": "the crux of what we want to cover is basically what we'll uh talk about later but you'll see these trends around",
    "start": "434080",
    "end": "441280"
  },
  {
    "text": "agentic AI lms and how we are doing them more efficiently with the help of web assembly across a diverse set of uh use",
    "start": "441280",
    "end": "448240"
  },
  {
    "text": "cases So whether you are working in an edge company or you're working uh building applications securely in the",
    "start": "448240",
    "end": "453759"
  },
  {
    "text": "browser how web assembly is being used there to run these AI workloads Well",
    "start": "453759",
    "end": "459520"
  },
  {
    "text": "those were some great insights As we all know now most developers prefer to use Python AI ML for AI and ML So my next",
    "start": "459520",
    "end": "467759"
  },
  {
    "text": "question is to Michael and Radu So what are your thoughts on the languages used in AIM ML space what does",
    "start": "467759",
    "end": "475919"
  },
  {
    "text": "the landscape looks right now is Python still dominant or most uh",
    "start": "475919",
    "end": "483000"
  },
  {
    "text": "performant is it most efficient language for the geni workloads or there are",
    "start": "483000",
    "end": "488560"
  },
  {
    "text": "another languages frameworks which can be used if yes then tell us how and",
    "start": "488560",
    "end": "494240"
  },
  {
    "text": "where Let's start with Michael Oh thank you Yeah So um I have um uh",
    "start": "494240",
    "end": "502639"
  },
  {
    "text": "conflicted feelings about Python you know that's u because I I think I'm a little bit known you know one of the mo",
    "start": "502639",
    "end": "508879"
  },
  {
    "text": "most popular articles I read I wrote on medium who has that has over I think half a million view and thousands of",
    "start": "508879",
    "end": "516000"
  },
  {
    "text": "comments is I say the language of a AGI is rust that's actually not my quote that Elon Musk said that you know so um",
    "start": "516000",
    "end": "523839"
  },
  {
    "text": "one of the key point that we want to make is although Python is very popular uh all the models are released as Py",
    "start": "523839",
    "end": "529120"
  },
  {
    "text": "Python as PyTorch waits initially um it actually not that popular in real time",
    "start": "529120",
    "end": "536640"
  },
  {
    "text": "in the real uh production workloads in production workloads when people run um",
    "start": "536640",
    "end": "542640"
  },
  {
    "text": "I see 90% of them are either running lama.cpp CPP or VIM or SGLAN all three",
    "start": "542640",
    "end": "549279"
  },
  {
    "text": "of them are C C++ based frameworks nothing to do with Python Python is just used as a front end you know so um",
    "start": "549279",
    "end": "557680"
  },
  {
    "text": "because all those um today's AI um inference workload and another data",
    "start": "557680",
    "end": "563360"
  },
  {
    "text": "point I want to give you is that if you see a new open source model released on hugging face you can go look at the",
    "start": "563360",
    "end": "569519"
  },
  {
    "text": "download counts the llama.cpp's CP's format which is GGUF format those files",
    "start": "569519",
    "end": "575839"
  },
  {
    "text": "are downloaded 10 times more than Python format the day in a week after it's",
    "start": "575839",
    "end": "580959"
  },
  {
    "text": "released So I I thought that was very um you know uh uh enlightening is that you",
    "start": "580959",
    "end": "587040"
  },
  {
    "text": "know although Python get a lot of mentions and a lot of mind shares but I assume when it comes to inference most",
    "start": "587040",
    "end": "594000"
  },
  {
    "text": "people don't use Python to do inference most people actually use one of the CC++ based frameworks and I think that is one",
    "start": "594000",
    "end": "601440"
  },
  {
    "text": "of the drivers that opens us up for uh using WASM to run CC++ code you know",
    "start": "601440",
    "end": "607519"
  },
  {
    "text": "because it's we know it's hard to run Python code especially the Python ecosystem in Wom but CBC C++ is an",
    "start": "607519",
    "end": "614079"
  },
  {
    "text": "entirely different story So you know so that's uh uh you know what I would say is the origin story of why we use WM to",
    "start": "614079",
    "end": "620720"
  },
  {
    "text": "do was edge to do AI inference Yeah I I think I' I'd split the question",
    "start": "620720",
    "end": "627959"
  },
  {
    "text": "into what do most people reach for for the first time when they're trying something out and what's the most like",
    "start": "627959",
    "end": "634079"
  },
  {
    "text": "accessible language out there and what do people use when it's time to move to production and what are the new kinds of",
    "start": "634079",
    "end": "639279"
  },
  {
    "text": "inferencing engines using and for when when talking about accessible languages",
    "start": "639279",
    "end": "644640"
  },
  {
    "text": "and tools that make it easy for anyone to like start prototyping something Python I don't think is going away",
    "start": "644640",
    "end": "650480"
  },
  {
    "text": "anytime soon uh like the simplicity of being able to fetch a a built-in library and just use",
    "start": "650480",
    "end": "658079"
  },
  {
    "text": "it to generate inferencing I think that's largely going to to remain and it's an awesome language and it's",
    "start": "658079",
    "end": "664800"
  },
  {
    "text": "something that uh yeah people from high school all the way through uh through companies can can learn and use and in",
    "start": "664800",
    "end": "671120"
  },
  {
    "text": "and real things uh one of the things that that has been happening over the last I would say three years or so is I",
    "start": "671120",
    "end": "678959"
  },
  {
    "text": "see a lot of inferencing engines built in Rust rather than Python Uh and I",
    "start": "678959",
    "end": "684320"
  },
  {
    "text": "think for some of the same reasons that that Michael was talking about CPP C++ being used um the the memory safety of",
    "start": "684320",
    "end": "691760"
  },
  {
    "text": "that Rust brings to the table is something that's very very inspiring confidence when running in in production",
    "start": "691760",
    "end": "699200"
  },
  {
    "text": "Uh and so if you look at some of the the new frameworks that are coming out of",
    "start": "699200",
    "end": "704320"
  },
  {
    "text": "hugging face candle is is a very very popular uh rust framework for building",
    "start": "704320",
    "end": "710079"
  },
  {
    "text": "uh machine learning applications And I think a couple of weeks ago Nvidia announced another inferencing server also built in Rust",
    "start": "710079",
    "end": "717760"
  },
  {
    "text": "And so I'm kind of biased when it comes to Rust It's one of my favorite programming languages and we build a lot",
    "start": "717760",
    "end": "723600"
  },
  {
    "text": "of things in Rust But I I see a lot of people doing Python when it comes to prototyping and I see a lot of big",
    "start": "723600",
    "end": "730000"
  },
  {
    "text": "companies putting their their eggs in the rust basket for real production grade systems",
    "start": "730000",
    "end": "736560"
  },
  {
    "text": "Yeah So well now we all know that generative AI landscape is massive and there is room for different programming",
    "start": "736560",
    "end": "742880"
  },
  {
    "text": "languages based on the different use cases Time for what you have been waiting for web assembly Let's",
    "start": "742880",
    "end": "750160"
  },
  {
    "text": "understand where web assembly comes into the picture when we talk about genai So",
    "start": "750160",
    "end": "755680"
  },
  {
    "text": "Michael coming back to you to understand what when it comes to AI or AI inference",
    "start": "755680",
    "end": "761839"
  },
  {
    "text": "which means when the end user is trying to access a particular model where wom comes into the play",
    "start": "761839",
    "end": "770360"
  },
  {
    "text": "Well ironically I think Wam's role plays here is very similar to Python role",
    "start": "771880",
    "end": "777120"
  },
  {
    "text": "played in the ecosystem is to make two abstractions One is to abstract away the",
    "start": "777120",
    "end": "783279"
  },
  {
    "text": "hardware because one thing unique about AI inference is that there are so many GPUs so many C um you know accelerators",
    "start": "783279",
    "end": "791279"
  },
  {
    "text": "and all that stuff right for developers to target each platform is very difficult So you really need a truly",
    "start": "791279",
    "end": "797040"
  },
  {
    "text": "write once run anywhere solution meaning that I write my inference code once and",
    "start": "797040",
    "end": "802639"
  },
  {
    "text": "I expect it to run on you know I don't have on AMD on media on the Mac I don't",
    "start": "802639",
    "end": "809120"
  },
  {
    "text": "have to tell it it's a CUDA 11.5 versus CUDA 12.4 four they should be all the",
    "start": "809120",
    "end": "814880"
  },
  {
    "text": "same code there shouldn't be any difference you know Python handles it very well but if we don't have Python we",
    "start": "814880",
    "end": "820079"
  },
  {
    "text": "go to the C++ land all this has to be hold coded again so was I think uh on",
    "start": "820079",
    "end": "825920"
  },
  {
    "text": "one hand solve this abstraction so it provide the right ones run anywhere across GPU architectures especially on",
    "start": "825920",
    "end": "832240"
  },
  {
    "text": "the edge right on the second abstraction is the model abstraction the model abstraction is also what Python solved",
    "start": "832240",
    "end": "838880"
  },
  {
    "text": "but C++ doesn't solve is to say I have different model architecture You know",
    "start": "838880",
    "end": "843920"
  },
  {
    "text": "some are large language model some large language model can can handle pictures like you know like the one that we just",
    "start": "843920",
    "end": "849519"
  },
  {
    "text": "saw right uh can handle videos some are texttospech speechto text image",
    "start": "849519",
    "end": "854560"
  },
  {
    "text": "generation and all those models I need a consistent way to run all of them and",
    "start": "854560",
    "end": "859760"
  },
  {
    "text": "provide and uh you know so I don't have to handcraft the the code to manipulate",
    "start": "859760",
    "end": "865519"
  },
  {
    "text": "the tensor and do that you know So again you know the traditional PyTorch framework solved this problem very well",
    "start": "865519",
    "end": "871760"
  },
  {
    "text": "but not the CC++ framework in order for uh you know I think high performance",
    "start": "871760",
    "end": "877360"
  },
  {
    "text": "inference on the edge that is um that is more friendly to developers I",
    "start": "877360",
    "end": "882959"
  },
  {
    "text": "think we need to have those two abstractions abstract away the GPU and uh the drivers and all that stuff",
    "start": "882959",
    "end": "889279"
  },
  {
    "text": "hardware related stuff and abstract away the model difference And I think WASM in",
    "start": "889279",
    "end": "894399"
  },
  {
    "text": "this um in in those two particular areas and WASM does it very well you know so",
    "start": "894399",
    "end": "899440"
  },
  {
    "text": "because it is a intermediate runtime framework that's set between hardware and software right you know so you know",
    "start": "899440",
    "end": "906560"
  },
  {
    "text": "it's sort of like the old days of the JVM right you know you you have a bite code format that's uh you know that",
    "start": "906560",
    "end": "912800"
  },
  {
    "text": "looks the same to developers you just compile it once and at runtime the was runtime figure out which GPU it's",
    "start": "912800",
    "end": "919440"
  },
  {
    "text": "running on what version of CUDA what version of you know the Mac metal framework and what model is running and",
    "start": "919440",
    "end": "925680"
  },
  {
    "text": "then translate it you know uh accordingly right you know so I think that's uh you know uh uh at least from",
    "start": "925680",
    "end": "934480"
  },
  {
    "text": "my point of view that's the biggest reason why people why our developers use wom is you know to have a consistent",
    "start": "934480",
    "end": "940560"
  },
  {
    "text": "developer user experience but in languages like C and Rust",
    "start": "940560",
    "end": "945760"
  },
  {
    "text": "so I would uh go back again to Radu and I will ask can you also provide why vom",
    "start": "945760",
    "end": "951839"
  },
  {
    "text": "for AI inference is the go-to choice Yeah I think the one one thing that I'd",
    "start": "951839",
    "end": "958480"
  },
  {
    "text": "start with is I'm not necessarily old enough but I' i've worked within uh with with machine",
    "start": "958480",
    "end": "965839"
  },
  {
    "text": "learning well before it was the AI hype around that that we see today And one of",
    "start": "965839",
    "end": "971440"
  },
  {
    "text": "the the very first things that I did with web assembly was building an inferencing engine for classical machine",
    "start": "971440",
    "end": "977199"
  },
  {
    "text": "learning models And the the reason there was there were these machine learning",
    "start": "977199",
    "end": "983279"
  },
  {
    "text": "models that needed to be executed independently of CPU architectures and operating systems and to to Michael's",
    "start": "983279",
    "end": "990120"
  },
  {
    "text": "point abstract both the the the model itself but also more importantly the hardware behind the scenes and and so I",
    "start": "990120",
    "end": "998959"
  },
  {
    "text": "that is really the the big draw that that I see people come to to Web Assembly when it comes to inferencing is",
    "start": "998959",
    "end": "1006079"
  },
  {
    "text": "yeah uh making sure that they and run reliably and consistently uh a",
    "start": "1006079",
    "end": "1011560"
  },
  {
    "text": "program across various operating systems CPU architectures but also GPU uh",
    "start": "1011560",
    "end": "1017120"
  },
  {
    "text": "implementations as well And this to probably touch upon what Rahu mentioned If you look at one classic example of a",
    "start": "1017120",
    "end": "1023600"
  },
  {
    "text": "mobile net model uh you'll if you look at the size of the model and the inference times when you're comparing it",
    "start": "1023600",
    "end": "1029520"
  },
  {
    "text": "with running it in classic Python in a Docker container or you're looking at running it with NodeJS you'll",
    "start": "1029520",
    "end": "1034558"
  },
  {
    "text": "consistently find that because of the ahead of time compilation uh strategies",
    "start": "1034559",
    "end": "1040000"
  },
  {
    "text": "and of course the smaller bundle size that you get with Web Assembly it'll consistently be not only the quickest in",
    "start": "1040000",
    "end": "1046240"
  },
  {
    "text": "terms of execution uh where you'll probably find like it's uh even running faster than native Python but also have",
    "start": "1046240",
    "end": "1053360"
  },
  {
    "text": "a much smaller size So I think those are some of the capabilities that also enable you to package your models in a",
    "start": "1053360",
    "end": "1059679"
  },
  {
    "text": "more efficient manner and run them in these smaller edge compute whereas if you were just running them with",
    "start": "1059679",
    "end": "1065360"
  },
  {
    "text": "traditional Python it would have taken a lot more uh space as well So I think that's also one consideration especially",
    "start": "1065360",
    "end": "1071679"
  },
  {
    "text": "if you're trying to run machine learning inference across different types of device architectures where now you're",
    "start": "1071679",
    "end": "1077600"
  },
  {
    "text": "not just concerned about the underlying GP or the CPU architecture but also from a compute u speed and how small these",
    "start": "1077600",
    "end": "1086640"
  },
  {
    "text": "models are So WAM is right now proving to be very efficient choice for the AI",
    "start": "1086640",
    "end": "1092320"
  },
  {
    "text": "inference with all the benefits it provides Let's switch the gears to production Now we have seen that WAMUM",
    "start": "1092320",
    "end": "1099120"
  },
  {
    "text": "is stable use in production for many of the use cases like last year Google also",
    "start": "1099120",
    "end": "1104640"
  },
  {
    "text": "presented how they are using WAM in all their products AI products basically So",
    "start": "1104640",
    "end": "1109840"
  },
  {
    "text": "let's discuss some of the use cases for WAM in production with the AI",
    "start": "1109840",
    "end": "1115720"
  },
  {
    "text": "angle for this one Let's spend some time with everyone of you and let's start with Michael first",
    "start": "1115720",
    "end": "1123520"
  },
  {
    "text": "Me again Okay Well so you know",
    "start": "1123520",
    "end": "1129360"
  },
  {
    "text": "um so we have a lot of customers or you know I wouldn't say you know as a",
    "start": "1130120",
    "end": "1135760"
  },
  {
    "text": "company we have customers but open source project we have a lot of users that is using um so the primary use case",
    "start": "1135760",
    "end": "1142160"
  },
  {
    "text": "really is to use was to run AI locally on your own machine right you know so",
    "start": "1142160",
    "end": "1147200"
  },
  {
    "text": "you would ask you know what's are the use cases for that so there's lots you know so from the um you know hobist or",
    "start": "1147200",
    "end": "1156160"
  },
  {
    "text": "or you know um developer point of view you know I often there's new open source model that",
    "start": "1156160",
    "end": "1162640"
  },
  {
    "text": "come out that I need to run it myself you know that's a so which is you know there are lots of tools like that a lot",
    "start": "1162640",
    "end": "1168240"
  },
  {
    "text": "of people use llama edge but there's also o lama and you know things like that where you run on your machine right",
    "start": "1168240",
    "end": "1173520"
  },
  {
    "text": "but I think um some of the more interesting use cases in in in our observation are like um on the edge for",
    "start": "1173520",
    "end": "1179120"
  },
  {
    "text": "instance there's there's one example that I talked about before you know last year we had an event where one of our",
    "start": "1179120",
    "end": "1184640"
  },
  {
    "text": "users uh at UC Berkeley camp and uh what they did is that they have a laptop and",
    "start": "1184640",
    "end": "1189679"
  },
  {
    "text": "their drone So they use large language model to control their drone So how it works is that you know um so you know",
    "start": "1189679",
    "end": "1195600"
  },
  {
    "text": "large language model when you talk when you speak natural language to it it will come back with natural language right",
    "start": "1195600",
    "end": "1200880"
  },
  {
    "text": "but they fine-tune the model so that it doesn't respond in natural language it respond in python right so the python is",
    "start": "1200880",
    "end": "1206880"
  },
  {
    "text": "using is built with the drone SDK so it h can automatically generate python and controls the drone so I I have to tell",
    "start": "1206880",
    "end": "1213360"
  },
  {
    "text": "the the u the drone um you know um tell me what's behind that wall and it would",
    "start": "1213360",
    "end": "1219200"
  },
  {
    "text": "generate the python code to say take off by 10 mters ahead turn around take a",
    "start": "1219200",
    "end": "1225280"
  },
  {
    "text": "picture and come back right you know that's uh so it would be the response large language model give right for use",
    "start": "1225280",
    "end": "1231280"
  },
  {
    "text": "cases like that is all local it's all edge and uh you know so that's would be one of the you know um interesting use",
    "start": "1231280",
    "end": "1238320"
  },
  {
    "text": "case for um you know why would you want to run large language model use was engine right in some of those edge",
    "start": "1238320",
    "end": "1244080"
  },
  {
    "text": "devices and today you know lama age runs on say you know um the the media jet um",
    "start": "1244080",
    "end": "1250640"
  },
  {
    "text": "you know uh chipset it runs on raspberry pi you know that's um using the accelerator you know so it runs all",
    "start": "1250640",
    "end": "1257360"
  },
  {
    "text": "kinds of different models and uh there's many many other use cases I I don't think we have full time to to to to",
    "start": "1257360",
    "end": "1263280"
  },
  {
    "text": "discuss but personal coding assistant is one of the other ones right you know that's uh you use the large language",
    "start": "1263280",
    "end": "1268400"
  },
  {
    "text": "mark you use you run like chairman coder with uh your own um you know codebase or",
    "start": "1268400",
    "end": "1273840"
  },
  {
    "text": "your private code base and have it answer your questions and plug into cursor and you know things like that so",
    "start": "1273840",
    "end": "1279120"
  },
  {
    "text": "yeah there's uh um we we also have fairly large use cases where one of the",
    "start": "1279120",
    "end": "1284240"
  },
  {
    "text": "um uh um one of the leading short video platforms in America wants to um you",
    "start": "1284240",
    "end": "1290080"
  },
  {
    "text": "know um have uh a transcription for all their video you know for the reason for",
    "start": "1290080",
    "end": "1296159"
  },
  {
    "text": "that is for recommendation engine you know because recommendation engine work off text so you need to know what's the",
    "start": "1296159",
    "end": "1301200"
  },
  {
    "text": "text in the video or is music or you know things like that so um we build a customized uh lama age pipeline for them",
    "start": "1301200",
    "end": "1307840"
  },
  {
    "text": "it's it's uh it's handling 700,000 videos per day you know to have all those videos come in and have u two or",
    "start": "1307840",
    "end": "1314720"
  },
  {
    "text": "three models work on extracting the audio extracting the feature breaking into sentences and then do the",
    "start": "1314720",
    "end": "1320640"
  },
  {
    "text": "transcription and use a large language model to check the transcription to see if it's done correctly and then assemble them back together You know it's an",
    "start": "1320640",
    "end": "1326799"
  },
  {
    "text": "entire pipeline of multiple models multiple applications already written in Rust and uh run the um you know uh WM",
    "start": "1326799",
    "end": "1334880"
  },
  {
    "text": "platform I can go on but but I think I should stop you that's",
    "start": "1334880",
    "end": "1339919"
  },
  {
    "text": "yes Yeah Uh so if if we talk when we talk about um like coming from the last",
    "start": "1339919",
    "end": "1346240"
  },
  {
    "text": "year so last year um the firm folks and some of the open source community they",
    "start": "1346240",
    "end": "1351360"
  },
  {
    "text": "launched spin cube So spin cube is a project that lets you run uh you know create those spin applications and run",
    "start": "1351360",
    "end": "1358080"
  },
  {
    "text": "it on kubernetes clusters So we know the benefits of web assembly is something that you all have been hearing you know",
    "start": "1358080",
    "end": "1363919"
  },
  {
    "text": "the universal bite code write once run anywhere kind of things and kubernetes",
    "start": "1363919",
    "end": "1369039"
  },
  {
    "text": "is a base platform it has become so stable and uh some people call it boring",
    "start": "1369039",
    "end": "1374080"
  },
  {
    "text": "as well because when a tech becomes boring and standard it becomes stable that that's that what it means so now",
    "start": "1374080",
    "end": "1379840"
  },
  {
    "text": "you have kubernetes as a base layer and now the in the web assembly ecosystem you have this um uh you know path where",
    "start": "1379840",
    "end": "1387760"
  },
  {
    "text": "you can have your web assembly the um applications of web assembly functions that are created and you can run it on",
    "start": "1387760",
    "end": "1393440"
  },
  {
    "text": "those uh Kubernetes clusters Now with AI ecosystem it makes even more sense Now",
    "start": "1393440",
    "end": "1399760"
  },
  {
    "text": "uh if you remember the first talk it's not about replacing the containers It it's about actually running them side by",
    "start": "1399760",
    "end": "1405120"
  },
  {
    "text": "side It's about you have your base Kubernetes cluster you have your infrastructure with you of running",
    "start": "1405120",
    "end": "1410640"
  },
  {
    "text": "anything that you want And uh Kubernetes is not now a container orchestrator only",
    "start": "1410640",
    "end": "1415840"
  },
  {
    "text": "It can orchestrate VMs It can orchestrate any resource based on the lot of CNCF projects like crossplane and",
    "start": "1415840",
    "end": "1421440"
  },
  {
    "text": "stuff But with the tools like open source tooling like spin cube uh which is also part of CNCF um it lets you",
    "start": "1421440",
    "end": "1429520"
  },
  {
    "text": "easily create those workloads have those access the inference uh the AI models to inference that um and you know having",
    "start": "1429520",
    "end": "1437360"
  },
  {
    "text": "that privacy as well like because Vasom runs in those isolated um sandboxes uh you define uh how much access has to be",
    "start": "1437360",
    "end": "1444640"
  },
  {
    "text": "given based on uh the components you know the level of access that you define within those uh web assembly modules and",
    "start": "1444640",
    "end": "1452080"
  },
  {
    "text": "then you run it on your Kubernetes cluster So now you have a platform so it leads to uh the adoption barrier like",
    "start": "1452080",
    "end": "1458559"
  },
  {
    "text": "there is less adoption barrier now because you don't have to change the infrastructure that you are having You",
    "start": "1458559",
    "end": "1463760"
  },
  {
    "text": "are still using the same one and AI is coming in uh there are tools within the web assembly ecosystem that everyone",
    "start": "1463760",
    "end": "1470159"
  },
  {
    "text": "wants to use for AI inferencing as we heard from uh Michael and Radu as well So now you can use them because you",
    "start": "1470159",
    "end": "1476640"
  },
  {
    "text": "don't need a new infrastructure You have your clusters already in place Now you just need to annotate them have the uh",
    "start": "1476640",
    "end": "1483440"
  },
  {
    "text": "uh the shims the new shims onto that with the uh vasame or the another runtime that you're having so that when",
    "start": "1483440",
    "end": "1490400"
  },
  {
    "text": "you are running it you can use the new runtime class specified in your YAML specification and that will use um the",
    "start": "1490400",
    "end": "1497640"
  },
  {
    "text": "new uh shim that's the power of containerd and then with the AI",
    "start": "1497640",
    "end": "1502960"
  },
  {
    "text": "inferencing power it can use something uh which is running like you know the GBs of model access that which are preh",
    "start": "1502960",
    "end": "1509200"
  },
  {
    "text": "bought um and get those inference I think Radu last year gave uh this talk",
    "start": "1509200",
    "end": "1514640"
  },
  {
    "text": "and a demo where we had a Kubernetes cluster spend up and we created the new application connected to the spin cloud",
    "start": "1514640",
    "end": "1520559"
  },
  {
    "text": "and it already had uh those uh large language models out there and it is able to um give you use the GPUs attached and",
    "start": "1520559",
    "end": "1528960"
  },
  {
    "text": "give you the uh output immediately So I think uh that's where the infra kubernetes role play important role in",
    "start": "1528960",
    "end": "1536320"
  },
  {
    "text": "production using some of the toolings that the vasom ecosystem has created Yeah and think like probably keeping in",
    "start": "1536320",
    "end": "1542960"
  },
  {
    "text": "time as well I'd like to also probably just quickly touch upon some of the other use cases on the browser So as you",
    "start": "1542960",
    "end": "1549120"
  },
  {
    "text": "know that most of today's generative AI applications or in general most of the",
    "start": "1549120",
    "end": "1554400"
  },
  {
    "text": "applications that we interface are either on your mobile but mostly they're on the web and one of the biggest issues",
    "start": "1554400",
    "end": "1560400"
  },
  {
    "text": "that we have to deal with is privacy right so if there are companies that are wanting to uh probably it could be",
    "start": "1560400",
    "end": "1566799"
  },
  {
    "text": "healthcare or insurance companies that don't want to send their priv privacy data to uh these servers or these openi",
    "start": "1566799",
    "end": "1573919"
  },
  {
    "text": "models or perhaps their use case is very simple where having the cost of a hardware accelerated with a server it it",
    "start": "1573919",
    "end": "1581600"
  },
  {
    "text": "doesn't doesn't justify So today the the thing is that now over the past 5 to 6",
    "start": "1581600",
    "end": "1587279"
  },
  {
    "text": "years the browser ecosystem has evolved so much that we have ecosystem within",
    "start": "1587279",
    "end": "1592799"
  },
  {
    "text": "the help of web assembly and technology like web GPU which can allow you to run your AI workloads directly into the",
    "start": "1592799",
    "end": "1599279"
  },
  {
    "text": "browser and there are already companies like as big as some something like Photoshop where Adobe Photoshop uses",
    "start": "1599279",
    "end": "1606080"
  },
  {
    "text": "TensorFlowJS project under the hood for doing realtime editing capabilities of images directly in the browser where the",
    "start": "1606080",
    "end": "1613440"
  },
  {
    "text": "model will get downloaded into your browser and with the help of mcripton where it allows you to translate your",
    "start": "1613440",
    "end": "1620080"
  },
  {
    "text": "C++ code into web assembly and then run it inside of your browser environment with the help of JavaScript uh to a lot",
    "start": "1620080",
    "end": "1626720"
  },
  {
    "text": "of other smaller startups also actually adopting it So if you're using something like Google meet it uses um web assembly",
    "start": "1626720",
    "end": "1634159"
  },
  {
    "text": "for doing the background removal or adding a virtual background or there are healthcare companies that where you can",
    "start": "1634159",
    "end": "1639840"
  },
  {
    "text": "do like online physotherapy by leveraging the online webcam and the great thing about the web ecosystem",
    "start": "1639840",
    "end": "1645760"
  },
  {
    "text": "specifically is that you have such a huge ecosystem of tooling that you can technically also build your own entire",
    "start": "1645760",
    "end": "1652080"
  },
  {
    "text": "rack pipeline by running vector databases directly into the browser running uh web lm today which uses a",
    "start": "1652080",
    "end": "1659279"
  },
  {
    "text": "combin combination of your web assembly and web GPU because of course while we know that web assembly is really",
    "start": "1659279",
    "end": "1664640"
  },
  {
    "text": "optimized for CPU based workloads and a lot of these uh simple machine learning",
    "start": "1664640",
    "end": "1669760"
  },
  {
    "text": "models like because today in Genaii ecosystem we tend to forget that you have your traditional models as well",
    "start": "1669760",
    "end": "1676080"
  },
  {
    "text": "which can run very effectively just on your CPU but if you are trying to run more of LLMs uh then you can very easily",
    "start": "1676080",
    "end": "1682640"
  },
  {
    "text": "handoff uh because of the good interrop between web assembly and web GPU where",
    "start": "1682640",
    "end": "1687840"
  },
  {
    "text": "most of your tasks or the tensor processing for your machine learning workloads can be handled with the cmd uh",
    "start": "1687840",
    "end": "1694399"
  },
  {
    "text": "which is this supported by web assembly but any other high compute for llm inferencing can then be taken up by the",
    "start": "1694399",
    "end": "1701440"
  },
  {
    "text": "web GPU So by using a combination you can actually just run uh simple you know",
    "start": "1701440",
    "end": "1707520"
  },
  {
    "text": "1 billion parameter model or two billion parameter based gemma models directly into the browser and that gives you the",
    "start": "1707520",
    "end": "1714720"
  },
  {
    "text": "capability of running it completely offline it's uh you're not you're not running any into any cost because it's",
    "start": "1714720",
    "end": "1720720"
  },
  {
    "text": "on your device Uh the latency is something that you uh are also able to save on So these capabilities allow you",
    "start": "1720720",
    "end": "1727840"
  },
  {
    "text": "to give a lot of flexibility to customers who are wanting privacy first applications and that is why like not",
    "start": "1727840",
    "end": "1735520"
  },
  {
    "text": "just the web ecosystem but you now we're also seeing it extended to MCP So of course a lot of you might have probably",
    "start": "1735520",
    "end": "1741520"
  },
  {
    "text": "heard of the model context protocol So folks from DIP so they showed it during their initial keynote presentation as",
    "start": "1741520",
    "end": "1748080"
  },
  {
    "text": "well with the mcp.run where they have built this entire ecosystem of being able to bring in MCP server compliant",
    "start": "1748080",
    "end": "1755039"
  },
  {
    "text": "applications and registering them and serving them as web assembly executables in a more secure manner uh where you can",
    "start": "1755039",
    "end": "1761919"
  },
  {
    "text": "just build any sort of application that can interface with these LLMs and do efficient tool calling So we what we're",
    "start": "1761919",
    "end": "1768799"
  },
  {
    "text": "seeing is that whether you talk about running secure applications on the web or you're talking about uh running",
    "start": "1768799",
    "end": "1775120"
  },
  {
    "text": "edgentic applications um web assembly is the go-to choice when it comes to whether edge compute or browser and",
    "start": "1775120",
    "end": "1781760"
  },
  {
    "text": "probably I know like we we are running a bit out of time if Radu if you want to add to it",
    "start": "1781760",
    "end": "1788640"
  },
  {
    "text": "Yeah so we are running out of time so Radu can you please um uh merge the use",
    "start": "1788640",
    "end": "1793919"
  },
  {
    "text": "cases and closing thought together for our audience Uh sure I I think the the",
    "start": "1793919",
    "end": "1799039"
  },
  {
    "text": "one of the most important things to keep in mind is why would you use web assembly for something and to me the the",
    "start": "1799039",
    "end": "1804399"
  },
  {
    "text": "immediate answer to that is well you need something that runs in a sandbox in an isolation context And Sam's point",
    "start": "1804399",
    "end": "1811120"
  },
  {
    "text": "about about model context protocol and agents in general is a really really good topic I think I'd add one more one",
    "start": "1811120",
    "end": "1818080"
  },
  {
    "text": "more uh idea that hasn't been uh said so far here which is there's a lot of",
    "start": "1818080",
    "end": "1823760"
  },
  {
    "text": "serverless needs for things like retrieval augmented generation and glue code if you want to call it that way",
    "start": "1823760",
    "end": "1830320"
  },
  {
    "text": "where you actually have to run something very fast and execute sandbox code as well and so really if you need not even",
    "start": "1830320",
    "end": "1837279"
  },
  {
    "text": "just for inferencing where uh Michael's been work on on llama edge is uh encompassing a lot of the generate",
    "start": "1837279",
    "end": "1843919"
  },
  {
    "text": "inferencing from web assembly but even outside uh whenever you need to execute something in a sandbox environment web",
    "start": "1843919",
    "end": "1850240"
  },
  {
    "text": "assembly is becoming a really really good choice there Okay So well uh as I had more questions",
    "start": "1850240",
    "end": "1859200"
  },
  {
    "text": "than I'm sure you all must be having So they are all around here You can ask your questions from them So thank you",
    "start": "1859200",
    "end": "1866480"
  },
  {
    "text": "all uh for this panel and um see you",
    "start": "1866480",
    "end": "1871919"
  },
  {
    "text": "Thank you Thank you",
    "start": "1871919",
    "end": "1876080"
  }
]